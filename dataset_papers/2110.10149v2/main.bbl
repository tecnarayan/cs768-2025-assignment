\begin{thebibliography}{89}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amos et~al.(2017)Amos, Xu, and Kolter]{amos2017input}
Amos, B., Xu, L., and Kolter, J.~Z.
\newblock Input convex neural networks.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Baker, Chociej, Jozefowicz,
  McGrew, Pachocki, Petron, Plappert, Powell, Ray,
  et~al.]{andrychowicz2020learning}
Andrychowicz, O.~M., Baker, B., Chociej, M., Jozefowicz, R., McGrew, B.,
  Pachocki, J., Petron, A., Plappert, M., Powell, G., Ray, A., et~al.
\newblock Learning dexterous in-hand manipulation.
\newblock \emph{The International Journal of Robotics Research}, 2020.

\bibitem[Asadi et~al.(2021)Asadi, Parikh, Parr, Konidaris, and
  Littman]{asadi2021deep}
Asadi, K., Parikh, N., Parr, R.~E., Konidaris, G.~D., and Littman, M.~L.
\newblock Deep radial-basis value functions for continuous control.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2021.

\bibitem[Barto et~al.(2013)Barto, Mirolli, and Baldassarre]{barto2013novelty}
Barto, A., Mirolli, M., and Baldassarre, G.
\newblock Novelty or surprise?
\newblock \emph{Frontiers in psychology}, 2013.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{bellemare2013arcade}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 2013.

\bibitem[Bellman(1957)]{bellman}
Bellman, R.
\newblock A markovian decision process.
\newblock \emph{Indiana University Mathematics Journal}, 1957.

\bibitem[Bellman et~al.(1956)Bellman, Glicksberg, and Gross]{bellman1956bang}
Bellman, R., Glicksberg, I., and Gross, O.
\newblock On the “bang-bang” control problem.
\newblock \emph{Quarterly of Applied Mathematics}, 1956.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Dkbiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., Dkbiak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bertsekas(2000)]{bertsekas2000dynamic}
Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}.
\newblock Athena scientific Belmont, 2000.

\bibitem[Bishop(1994)]{bishop1994mixture}
Bishop, C.~M.
\newblock Mixture density networks.
\newblock 1994.

\bibitem[Brantley et~al.(2019)Brantley, Sun, and
  Henaff]{brantley2019disagreement}
Brantley, K., Sun, W., and Henaff, M.
\newblock Disagreement-regularized imitation learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Browne et~al.(2012)Browne, Powley, Whitehouse, Lucas, Cowling,
  Rohlfshagen, Tavener, Perez, Samothrakis, and Colton]{browne2012survey}
Browne, C.~B., Powley, E., Whitehouse, D., Lucas, S.~M., Cowling, P.~I.,
  Rohlfshagen, P., Tavener, S., Perez, D., Samothrakis, S., and Colton, S.
\newblock A survey of monte carlo tree search methods.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in
  games}, 2012.

\bibitem[Bushaw(1953)]{bushaw1952differential}
Bushaw, D.~W.
\newblock Differential equations with a discontinuous forcing term.
\newblock Technical report, Stevens Inst Of Tech Hoboken NJ Experimental Towing
  Tank, 1953.

\bibitem[Calinon \& Billard(2007)Calinon and Billard]{calinon2007incremental}
Calinon, S. and Billard, A.
\newblock Incremental learning of gestures by imitation in a humanoid robot.
\newblock In \emph{ACM/IEEE international conference on Human-robot
  interaction}, 2007.

\bibitem[Chernova \& Veloso(2007)Chernova and Veloso]{chernova2007confidence}
Chernova, S. and Veloso, M.
\newblock Confidence-based policy learning from demonstration using gaussian
  mixture models.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent
  Systems}, 2007.

\bibitem[Dadashi et~al.(2021)Dadashi, Hussenot, Geist, and
  Pietquin]{dadashi2020primal}
Dadashi, R., Hussenot, L., Geist, M., and Pietquin, O.
\newblock Primal wasserstein imitation learning.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Ding et~al.(2019)Ding, Florensa, Phielipp, and Abbeel]{ding2019goal}
Ding, Y., Florensa, C., Phielipp, M., and Abbeel, P.
\newblock Goal-conditioned imitation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Dulac-Arnold et~al.(2015)Dulac-Arnold, Evans, van Hasselt, Sunehag,
  Lillicrap, Hunt, Mann, Weber, Degris, and Coppin]{dulac2015deep}
Dulac-Arnold, G., Evans, R., van Hasselt, H., Sunehag, P., Lillicrap, T., Hunt,
  J., Mann, T., Weber, T., Degris, T., and Coppin, B.
\newblock Deep reinforcement learning in large discrete action spaces.
\newblock \emph{arXiv preprint arXiv:1512.07679}, 2015.

\bibitem[Flamary et~al.(2021)Flamary, Courty, Gramfort, Alaya, Boisbunon,
  Chambon, Chapel, Corenflos, Fatras, Fournier, Gautheron, Gayraud, Janati,
  Rakotomamonjy, Redko, Rolet, Schutz, Seguy, Sutherland, Tavenard, Tong, and
  Vayer]{flamary2021pot}
Flamary, R., Courty, N., Gramfort, A., Alaya, M.~Z., Boisbunon, A., Chambon,
  S., Chapel, L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L.,
  Gayraud, N.~T., Janati, H., Rakotomamonjy, A., Redko, I., Rolet, A., Schutz,
  A., Seguy, V., Sutherland, D.~J., Tavenard, R., Tong, A., and Vayer, T.
\newblock Pot: Python optimal transport.
\newblock \emph{Journal of Machine Learning Research}, 2021.

\bibitem[Florence et~al.(2021)Florence, Lynch, Zeng, Ramirez, Wahid, Downs,
  Wong, Lee, Mordatch, and Tompson]{florence2021implicit}
Florence, P., Lynch, C., Zeng, A., Ramirez, O., Wahid, A., Downs, L., Wong, A.,
  Lee, J., Mordatch, I., and Tompson, J.
\newblock Implicit behavioral cloning.
\newblock \emph{arXiv preprint arXiv:2109.00137}, 2021.

\bibitem[Fox et~al.(2019)Fox, Berenstein, Stoica, and Goldberg]{fox2019multi}
Fox, R., Berenstein, R., Stoica, I., and Goldberg, K.
\newblock Multi-task hierarchical imitation learning for home automation.
\newblock In \emph{International Conference on Automation Science and
  Engineering (CASE)}. IEEE, 2019.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2106.06860}, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Fujimoto, S., Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Ghasemipour et~al.(2019)Ghasemipour, Zemel, and
  Gu]{ghasemipour2019divergence}
Ghasemipour, S. K.~S., Zemel, R., and Gu, S.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock \emph{Conference on Robot Learning}, 2019.

\bibitem[Gu et~al.(2016)Gu, Lillicrap, Sutskever, and Levine]{gu2016continuous}
Gu, S., Lillicrap, T., Sutskever, I., and Levine, S.
\newblock Continuous deep q-learning with model-based acceleration.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Gupta et~al.(2019)Gupta, Kumar, Lynch, Levine, and
  Hausman]{lynch2019play}
Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K.
\newblock Relay policy learning: Solving long horizon tasks via imitation and
  reinforcement learning.
\newblock \emph{Conference on Robot Learning (CoRL)}, 2019.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning},
  2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft2}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Hoffman et~al.(2020)Hoffman, Shahriari, Aslanides, Barth-Maron,
  Behbahani, Norman, Abdolmaleki, Cassirer, Yang, Baumli,
  et~al.]{hoffman2020acme}
Hoffman, M., Shahriari, B., Aslanides, J., Barth-Maron, G., Behbahani, F.,
  Norman, T., Abdolmaleki, A., Cassirer, A., Yang, F., Baumli, K., et~al.
\newblock Acme: A research framework for distributed reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.00979}, 2020.

\bibitem[Hussenot et~al.(2021)Hussenot, Andrychowicz, Vincent, Dadashi,
  Raichuk, Ramos, Momchev, Girgin, Marinier, Stafiniak,
  et~al.]{hussenot2021hyperparameter}
Hussenot, L., Andrychowicz, M., Vincent, D., Dadashi, R., Raichuk, A., Ramos,
  S., Momchev, N., Girgin, S., Marinier, R., Stafiniak, L., et~al.
\newblock Hyperparameter selection for imitation learning.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Jarrett et~al.(2020)Jarrett, Bica, and van~der
  Schaar]{jarrett2020strictly}
Jarrett, D., Bica, I., and van~der Schaar, M.
\newblock Strictly batch imitation learning by energy-based distribution
  matching.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, et~al.]{kalashnikov2018qt}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E.,
  Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., et~al.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock \emph{Conference on Robot Learning}, 2018.

\bibitem[Kannan et~al.(2021)Kannan, Hafner, Finn, and
  Erhan]{kannan2021robodesk}
Kannan, H., Hafner, D., Finn, C., and Erhan, D.
\newblock Robodesk: A multi-task reinforcement learning benchmark.
\newblock \url{https://github.com/google-research/robodesk}, 2021.

\bibitem[Kipf et~al.(2019)Kipf, Li, Dai, Zambaldi, Sanchez-Gonzalez,
  Grefenstette, Kohli, and Battaglia]{kipf2019compile}
Kipf, T., Li, Y., Dai, H., Zambaldi, V., Sanchez-Gonzalez, A., Grefenstette,
  E., Kohli, P., and Battaglia, P.
\newblock Compile: Compositional imitation learning and execution.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Konda \& Tsitsiklis(2000)Konda and Tsitsiklis]{konda2000actor}
Konda, V.~R. and Tsitsiklis, J.~N.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2000.

\bibitem[Kostrikov et~al.(2019)Kostrikov, Agrawal, Dwibedi, Levine, and
  Tompson]{kostrikov2018discriminator}
Kostrikov, I., Agrawal, K.~K., Dwibedi, D., Levine, S., and Tompson, J.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward
  bias in adversarial imitation learning.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Krishnan et~al.(2017)Krishnan, Fox, Stoica, and
  Goldberg]{krishnan2017ddco}
Krishnan, S., Fox, R., Stoica, I., and Goldberg, K.
\newblock Ddco: Discovery of deep continuous options for robot learning from
  demonstrations.
\newblock In \emph{Conference on Robot Learning}, 2017.

\bibitem[Kroemer et~al.(2015)Kroemer, Daniel, Neumann, Van~Hoof, and
  Peters]{kroemer2015towards}
Kroemer, O., Daniel, C., Neumann, G., Van~Hoof, H., and Peters, J.
\newblock Towards learning hierarchical skills for multi-phase manipulation
  tasks.
\newblock In \emph{International Conference on Robotics and Automation}. IEEE,
  2015.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Le et~al.(2018)Le, Jiang, Agarwal, Dud{\'\i}k, Yue, and
  Daum{\'e}]{le2018hierarchical}
Le, H., Jiang, N., Agarwal, A., Dud{\'\i}k, M., Yue, Y., and Daum{\'e}, H.
\newblock Hierarchical imitation and reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Lim et~al.(2018)Lim, Joseph, Le, Pan, and White]{lim2018actor}
Lim, S., Joseph, A., Le, L., Pan, Y., and White, M.
\newblock Actor-expert: A framework for using q-learning in continuous action
  spaces.
\newblock \emph{arXiv preprint arXiv:1810.09103}, 2018.

\bibitem[Lynch et~al.(2020)Lynch, Khansari, Xiao, Kumar, Tompson, Levine, and
  Sermanet]{lynch2020learning}
Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine, S., and
  Sermanet, P.
\newblock Learning latent plans from play.
\newblock In \emph{Conference on Robot Learning}, 2020.

\bibitem[Mandlekar et~al.(2021)Mandlekar, Xu, Wong, Nasiriany, Wang, Kulkarni,
  Fei-Fei, Savarese, Zhu, and Mart{\'\i}n-Mart{\'\i}n]{mandlekar2021matters}
Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang, C., Kulkarni, R.,
  Fei-Fei, L., Savarese, S., Zhu, Y., and Mart{\'\i}n-Mart{\'\i}n, R.
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock \emph{arXiv preprint arXiv:2108.03298}, 2021.

\bibitem[Manschitz et~al.(2015)Manschitz, Kober, Gienger, and
  Peters]{manschitz2015learning}
Manschitz, S., Kober, J., Gienger, M., and Peters, J.
\newblock Learning movement primitive attractor goals and sequential skills
  from kinesthetic demonstrations.
\newblock \emph{Robotics and Autonomous Systems}, 2015.

\bibitem[Metz et~al.(2017)Metz, Ibarz, Jaitly, and Davidson]{metz2017discrete}
Metz, L., Ibarz, J., Jaitly, N., and Davidson, J.
\newblock Discrete sequential prediction of continuous actions for deep rl.
\newblock \emph{arXiv preprint arXiv:1705.05035}, 2017.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 2015.

\bibitem[Neunert et~al.(2020)Neunert, Abdolmaleki, Wulfmeier, Lampe,
  Springenberg, Hafner, Romano, Buchli, Heess, and
  Riedmiller]{neunert2020continuous}
Neunert, M., Abdolmaleki, A., Wulfmeier, M., Lampe, T., Springenberg, T.,
  Hafner, R., Romano, F., Buchli, J., Heess, N., and Riedmiller, M.
\newblock Continuous-discrete reinforcement learning for hybrid control in
  robotics.
\newblock In \emph{Conference on Robot Learning}, 2020.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{International Conference on Machine Learning}, 1999.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S.~J., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2000.

\bibitem[Orsini et~al.(2021)Orsini, Raichuk, Hussenot, Vincent, Dadashi,
  Girgin, Geist, Bachem, Pietquin, and Andrychowicz]{orsini2021matters}
Orsini, M., Raichuk, A., Hussenot, L., Vincent, D., Dadashi, R., Girgin, S.,
  Geist, M., Bachem, O., Pietquin, O., and Andrychowicz, M.
\newblock What matters for adversarial imitation learning?
\newblock \emph{arXiv preprint arXiv:2106.00672}, 2021.

\bibitem[Pastor et~al.(2009)Pastor, Hoffmann, Asfour, and
  Schaal]{pastor2009learning}
Pastor, P., Hoffmann, H., Asfour, T., and Schaal, S.
\newblock Learning and generalization of motor skills by learning from
  demonstration.
\newblock In \emph{International Conference on Robotics and Automation}. IEEE,
  2009.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Pomerleau, D.~A.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural computation}, 1991.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rahmatizadeh et~al.(2018)Rahmatizadeh, Abolghasemi, Behal, and
  B{\"o}l{\"o}ni]{rahmatizadeh2018virtual}
Rahmatizadeh, R., Abolghasemi, P., Behal, A., and B{\"o}l{\"o}ni, L.
\newblock From virtual demonstration to real-world manipulation using lstm and
  mdn.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Vezzani, Schulman,
  Todorov, and Levine]{rajeswaran2017learning}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E.,
  and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement
  learning and demonstrations.
\newblock \emph{Robotics: Science and Systems}, 2017.

\bibitem[Ryu et~al.(2020)Ryu, Chow, Anderson, Tjandraatmadja, and
  Boutilier]{ryu2019caql}
Ryu, M., Chow, Y., Anderson, R., Tjandraatmadja, C., and Boutilier, C.
\newblock Caql: Continuous action q-learning.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sakryukin et~al.(2020)Sakryukin, Raissi, and
  Kankanhalli]{pmlr-v119-sakryukin20a}
Sakryukin, A., Raissi, C., and Kankanhalli, M.
\newblock Inferring {DQN} structure for high-dimensional continuous control.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Seyde et~al.(2021)Seyde, Gilitschenski, Schwarting, Stellato,
  Riedmiller, Wulfmeier, and Rus]{seyde2021bang}
Seyde, T., Gilitschenski, I., Schwarting, W., Stellato, B., Riedmiller, M.,
  Wulfmeier, M., and Rus, D.
\newblock Is bang-bang control all you need? solving continuous control with
  bernoulli policies.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Shankar et~al.(2019)Shankar, Tulsiani, Pinto, and
  Gupta]{shankar2019discovering}
Shankar, T., Tulsiani, S., Pinto, L., and Gupta, A.
\newblock Discovering motor programs by recomposing demonstrations.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 2016.

\bibitem[Simmons-Edler et~al.(2019)Simmons-Edler, Eisner, Mitchell, Seung, and
  Lee]{simmons2019q}
Simmons-Edler, R., Eisner, B., Mitchell, E., Seung, S., and Lee, D.
\newblock Q-learning for continuous actions with cross-entropy guided policies.
\newblock \emph{arXiv preprint arXiv:1903.10605}, 2019.

\bibitem[Singh et~al.(2020)Singh, Liu, Zhou, Yu, Rhinehart, and
  Levine]{singh2020parrot}
Singh, A., Liu, H., Zhou, G., Yu, A., Rhinehart, N., and Levine, S.
\newblock Parrot: Data-driven behavioral priors for reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2000.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Chen, Duan, Schulman,
  De~Turck, and Abbeel]{tang2017exploration}
Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan, Y., Schulman,
  J., De~Turck, F., and Abbeel, P.
\newblock \# exploration: A study of count-based exploration for deep
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Tang \& Agrawal(2020)Tang and Agrawal]{tang2020discretizing}
Tang, Y. and Agrawal, S.
\newblock Discretizing continuous action space for on-policy optimization.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2020.

\bibitem[Tassa et~al.(2020)Tassa, Tunyasuvunakool, Muldal, Doron, Liu, Bohez,
  Merel, Erez, Lillicrap, and Heess]{tassa2020dmcontrol}
Tassa, Y., Tunyasuvunakool, S., Muldal, A., Doron, Y., Liu, S., Bohez, S.,
  Merel, J., Erez, T., Lillicrap, T., and Heess, N.
\newblock dm\_control: Software and tasks for continuous control, 2020.

\bibitem[Tavakoli et~al.(2018)Tavakoli, Pardo, and
  Kormushev]{tavakoli2018action}
Tavakoli, A., Pardo, F., and Kormushev, P.
\newblock Action branching architectures for deep reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Tavakoli et~al.(2021)Tavakoli, Fatemi, and
  Kormushev]{tavakoli2020learning}
Tavakoli, A., Fatemi, M., and Kormushev, P.
\newblock Learning to represent action values as a hypergraph on the action
  vertices.
\newblock \emph{Internation Conference in Learning Representations}, 2021.

\bibitem[Tessler et~al.(2019)Tessler, Tennenholtz, and
  Mannor]{tessler2019distributional}
Tessler, C., Tennenholtz, G., and Mannor, S.
\newblock Distributional policy optimization: An alternative approach for
  continuous control.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot,
  Heess, Roth{\"o}rl, Lampe, and Riedmiller]{vecerik2017leveraging}
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess,
  N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics
  problems with sparse rewards.
\newblock \emph{arXiv preprint arXiv:1707.08817}, 2017.

\bibitem[Vieillard et~al.(2020)Vieillard, Pietquin, and
  Geist]{vieillard2020munchausen}
Vieillard, N., Pietquin, O., and Geist, M.
\newblock Munchausen reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Vieillard et~al.(2021)Vieillard, Andrychowicz, Raichuk, Pietquin, and
  Geist]{vieillard2021implicitly}
Vieillard, N., Andrychowicz, M., Raichuk, A., Pietquin, O., and Geist, M.
\newblock Implicitly regularized rl with implicit q-values.
\newblock \emph{arXiv preprint arXiv:2108.07041}, 2021.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 2019.

\bibitem[Wang et~al.(2019)Wang, Ciliberto, Amadori, and
  Demiris]{wang2019random}
Wang, R., Ciliberto, C., Amadori, P.~V., and Demiris, Y.
\newblock Random expert distillation: Imitation learning via expert policy
  support estimation.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Watkins \& Dayan(1992)Watkins and Dayan]{watkins1992q}
Watkins, C.~J. and Dayan, P.
\newblock Q-learning.
\newblock \emph{Machine learning}, 1992.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 1992.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Yu et~al.(2018)Yu, Finn, Xie, Dasari, Zhang, Abbeel, and
  Levine]{yu2018one}
Yu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., and Levine, S.
\newblock One-shot imitation from observing humans via domain-adaptive
  meta-learning.
\newblock \emph{Robotics: Science and Systems}, 2018.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2008.

\end{thebibliography}
