@inproceedings{Marzoev:20,
title="Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data",
author=" Marzoev, Alana and  Madden, Samuel and Kaashoek, M. Frans and Cafarella, Michael  and Andreas, Jacob",
year = 2020,
booktitle={Proceedings of the First Workshop on Natural Language Interfaces (NLI)},
url={https://arxiv.org/pdf/2004.13645.pdf}
}
@misc{tran2020generating,
      title={Generating Synthetic Data for Task-Oriented Semantic Parsing with Hierarchical Representations}, 
      author={Ke Tran and Ming Tan},
      year={2020},
      eprint={2011.02050},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@inproceedings{berant-liang-2014-semantic,
    title = "Semantic Parsing via Paraphrasing",
    author = "Berant, Jonathan  and
      Liang, Percy",
    booktitle = "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P14-1133",
    doi = "10.3115/v1/P14-1133",
    pages = "1415--1425",
}

@misc{helm,
  doi = {10.48550/ARXIV.2211.09110},
  
  url = {https://arxiv.org/abs/2211.09110},
  
  author = {Liang, Percy and Bommasani, Rishi and Lee, Tony and Tsipras, Dimitris and Soylu, Dilara and Yasunaga, Michihiro and Zhang, Yian and Narayanan, Deepak and Wu, Yuhuai and Kumar, Ananya and Newman, Benjamin and Yuan, Binhang and Yan, Bobby and Zhang, Ce and Cosgrove, Christian and Manning, Christopher D. and Ré, Christopher and Acosta-Navas, Diana and Hudson, Drew A. and Zelikman, Eric and Durmus, Esin and Ladhak, Faisal and Rong, Frieda and Ren, Hongyu and Yao, Huaxiu and Wang, Jue and Santhanam, Keshav and Orr, Laurel and Zheng, Lucia and Yuksekgonul, Mert and Suzgun, Mirac and Kim, Nathan and Guha, Neel and Chatterji, Niladri and Khattab, Omar and Henderson, Peter and Huang, Qian and Chi, Ryan and Xie, Sang Michael and Santurkar, Shibani and Ganguli, Surya and Hashimoto, Tatsunori and Icard, Thomas and Zhang, Tianyi and Chaudhary, Vishrav and Wang, William and Li, Xuechen and Mai, Yifan and Zhang, Yuhui and Koreeda, Yuta},
  
  keywords = {Computation and Language (cs.CL), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Holistic Evaluation of Language Models},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@inproceedings{platanios-etal-2021-value,
    title = "Value-Agnostic Conversational Semantic Parsing",
    author = "Platanios, Emmanouil Antonios  and
      Pauls, Adam  and
      Roy, Subhro  and
      Zhang, Yuchen  and
      Kyte, Alexander  and
      Guo, Alan  and
      Thomson, Sam  and
      Krishnamurthy, Jayant  and
      Wolfe, Jason  and
      Andreas, Jacob  and
      Klein, Dan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.284",
    doi = "10.18653/v1/2021.acl-long.284",
    pages = "3666--3681",
    abstract = "Conversational semantic parsers map user utterances to executable programs given dialogue histories composed of previous utterances, programs, and system responses. Existing parsers typically condition on rich representations of history that include the complete set of values and computations previously discussed. We propose a model that abstracts over values to focus prediction on type- and function-level context. This approach provides a compact encoding of dialogue histories and predicted programs, improving generalization and computational efficiency. Our model incorporates several other components, including an atomic span copy operation and structural enforcement of well-formedness constraints on predicted programs, that are particularly advantageous in the low-data regime. Trained on the SMCalFlow and TreeDST datasets, our model outperforms prior work by 7.3{\%} and 10.6{\%} respectively in terms of absolute accuracy. Trained on only a thousand examples from each dataset, it outperforms strong baselines by 12.4{\%} and 6.4{\%}. These results indicate that simple representations are key to effective generalization in conversational semantic parsing.",
}

@article{andreas-etal-2020-task,
    title = "Task-Oriented Dialogue as Dataflow Synthesis",
    author = "Andreas, Jacob  and
      Bufe, John  and
      Burkett, David  and
      Chen, Charles  and
      Clausman, Josh  and
      Crawford, Jean  and
      Crim, Kate  and
      DeLoach, Jordan  and
      Dorner, Leah  and
      Eisner, Jason  and
      Fang, Hao  and
      Guo, Alan  and
      Hall, David  and
      Hayes, Kristin  and
      Hill, Kellie  and
      Ho, Diana  and
      Iwaszuk, Wendy  and
      Jha, Smriti  and
      Klein, Dan  and
      Krishnamurthy, Jayant  and
      Lanman, Theo  and
      Liang, Percy  and
      Lin, Christopher H.  and
      Lintsbakh, Ilya  and
      McGovern, Andy  and
      Nisnevich, Aleksandr  and
      Pauls, Adam  and
      Petters, Dmitrij  and
      Read, Brent  and
      Roth, Dan  and
      Roy, Subhro  and
      Rusak, Jesse  and
      Short, Beth  and
      Slomin, Div  and
      Snyder, Ben  and
      Striplin, Stephon  and
      Su, Yu  and
      Tellman, Zachary  and
      Thomson, Sam  and
      Vorobev, Andrei  and
      Witoszko, Izabela  and
      Wolfe, Jason  and
      Wray, Abby  and
      Zhang, Yuchen  and
      Zotov, Alexander",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "8",
    year = "2020",
    url = "https://www.aclweb.org/anthology/2020.tacl-1.36",
    doi = "10.1162/tacl_a_00333",
    pages = "556--571",
    abstract = "We describe an approach to task-oriented dialogue in which dialogue state is represented as a dataflow graph. A dialogue agent maps each user utterance to a program that extends this graph. Programs include metacomputation operators for reference and revision that reuse dataflow fragments from previous turns. Our graph-based state enables the expression and manipulation of complex user intents, and explicit metacomputation makes these intents easier for learned models to predict. We introduce a new dataset, SMCalFlow, featuring complex dialogues about events, weather, places, and people. Experiments show that dataflow graphs and metacomputation substantially improve representability and predictability in these natural dialogues. Additional experiments on the MultiWOZ dataset show that our dataflow representation enables an otherwise off-the-shelf sequence-to-sequence model to match the best existing task-specific state tracking model. The SMCalFlow dataset, code for replicating experiments, and a public leaderboard are available at https://www.microsoft.com/en-us/research/project/dataflow-based-dialogue-semantic-machines.",
}

@inproceedings{zhang-etal-2019-amr,
    title = "{AMR} Parsing as Sequence-to-Graph Transduction",
    author = "Zhang, Sheng  and
      Ma, Xutai  and
      Duh, Kevin  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1009",
    doi = "10.18653/v1/P19-1009",
    pages = "80--94",
    abstract = "We propose an attention-based model that treats AMR parsing as sequence-to-graph transduction. Unlike most AMR parsers that rely on pre-trained aligners, external semantic resources, or data augmentation, our proposed parser is aligner-free, and it can be effectively trained with limited amounts of labeled AMR data. Our experimental results outperform all previously reported SMATCH scores, on both AMR 2.0 (76.3{\%} on LDC2017T10) and AMR 1.0 (70.2{\%} on LDC2014T12).",
}
@article{Brown:2020:gpt-3,
    title = {Language Models Are Few-Shot Learners},
    author = {Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year = {2020},
    journal={Computing Research Repository},
    volume ={arXiv:2005.14165},
}

@inproceedings{Devlin:2019:bert,
    title = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
    author = {Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina},
    booktitle = {Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    month = {jun},
    year = {2019},
    address = {Minneapolis, Minnesota},
    OPTpublisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/N19-1423},
    doi = {10.18653/v1/N19-1423},
    pages = {4171--4186},
}

@inproceedings{Murray:2018:length-normalization,
    title = {Correcting Length Bias in Neural Machine Translation},
    author = {Murray, Kenton  and
      Chiang, David},
    booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
    month = {oct},
    year = {2018},
    address = {Brussels, Belgium},
    OPTpublisher = {Association for Computational Linguistics},
    url = {https://www.aclweb.org/anthology/W18-6322},
    doi = {10.18653/v1/W18-6322},
    pages = {212--223},
}

@techreport{Radford:2019:gpt-2,
    title = {Language Models Are Unsupervised Multitask Learners},
    author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    institution ={OpenAI},
    year = {2019},
    url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
}

@article{Raffel:2020:t5,
    author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
    title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
    journal = {Journal of Machine Learning Research},
    year = {2020},
    volume = {21},
    number = {140},
    pages = {1--67},
    url = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{suhr-etal-2019-corpus,
    title = "A Corpus for Reasoning about Natural Language Grounded in Photographs",
    author = "Suhr, Alane  and
      Zhou, Stephanie  and
      Zhang, Ally  and
      Zhang, Iris  and
      Bai, Huajun  and
      Artzi, Yoav",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1644",
    doi = "10.18653/v1/P19-1644",
    pages = "6418--6428",
    abstract = "We introduce a new dataset for joint reasoning about natural language and images, with a focus on semantic diversity, compositionality, and visual reasoning challenges. The data contains 107,292 examples of English sentences paired with web photographs. The task is to determine whether a natural language caption is true about a pair of photographs. We crowdsource the data using sets of visually rich images and a compare-and-contrast task to elicit linguistically diverse language. Qualitative analysis shows the data requires compositional joint reasoning, including about quantities, comparisons, and relations. Evaluation using state-of-the-art visual reasoning methods shows the data presents a strong challenge.",
}
@InProceedings{Johnson_2017_CVPR,
author = {Johnson, Justin and Hariharan, Bharath and van der Maaten, Laurens and Li, Fei-Fei and Lawrence Zitnick, C. and Girshick, Ross},
title = {CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017},
url = {https://www.aclweb.org/anthology/N19-1058},
}

@inproceedings{abujabal-etal-2019-comqa,
    title = "{C}om{QA}: A Community-sourced Dataset for Complex Factoid Question Answering with Paraphrase Clusters",
    author = "Abujabal, Abdalghani  and
      Saha Roy, Rishiraj  and
      Yahya, Mohamed  and
      Weikum, Gerhard",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1027",
    doi = "10.18653/v1/N19-1027",
    pages = "307--317",
    abstract = "To bridge the gap between the capabilities of the state-of-the-art in factoid question answering (QA) and what users ask, we need large datasets of real user questions that capture the various question phenomena users are interested in, and the diverse ways in which these questions are formulated. We introduce ComQA, a large dataset of real user questions that exhibit different challenging aspects such as compositionality, temporal reasoning, and comparisons. ComQA questions come from the WikiAnswers community QA platform, which typically contains questions that are not satisfactorily answerable by existing search engine technology. Through a large crowdsourcing effort, we clean the question dataset, group questions into paraphrase clusters, and annotate clusters with their answers. ComQA contains 11,214 questions grouped into 4,834 paraphrase clusters. We detail the process of constructing ComQA, including the measures taken to ensure its high quality while making effective use of crowdsourcing. We also present an extensive analysis of the dataset and the results achieved by state-of-the-art systems on ComQA, demonstrating that our dataset can be a driver of future research on QA.",
}

@inproceedings{dua-etal-2019-drop,
    title = "{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs",
    author = "Dua, Dheeru  and
      Wang, Yizhong  and
      Dasigi, Pradeep  and
      Stanovsky, Gabriel  and
      Singh, Sameer  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1246",
    doi = "10.18653/v1/N19-1246",
    pages = "2368--2378",
    abstract = "Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 55k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs, as they remove the paraphrase-and-entity-typing shortcuts available in prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 38.4{\%} F1 on our generalized accuracy metric, while expert human performance is 96{\%}. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 51{\%} F1.",
}

@inproceedings{yang-etal-2018-hotpotqa,
    title = "{H}otpot{QA}: A Dataset for Diverse, Explainable Multi-hop Question Answering",
    author = "Yang, Zhilin  and
      Qi, Peng  and
      Zhang, Saizheng  and
      Bengio, Yoshua  and
      Cohen, William  and
      Salakhutdinov, Ruslan  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1259",
    doi = "10.18653/v1/D18-1259",
    pages = "2369--2380",
    abstract = "Existing question answering (QA) datasets fail to train QA systems to perform complex reasoning and provide explanations for answers. We introduce HotpotQA, a new dataset with 113k Wikipedia-based question-answer pairs with four key features: (1) the questions require finding and reasoning over multiple supporting documents to answer; (2) the questions are diverse and not constrained to any pre-existing knowledge bases or knowledge schemas; (3) we provide sentence-level supporting facts required for reasoning, allowing QA systems to reason with strong supervision and explain the predictions; (4) we offer a new type of factoid comparison questions to test QA systems{'} ability to extract relevant facts and perform necessary comparison. We show that HotpotQA is challenging for the latest QA systems, and the supporting facts enable models to improve performance and make explainable predictions.",
}

@inproceedings{andreas-etal-2013-semantic,
    title = "Semantic Parsing as Machine Translation",
    author = "Andreas, Jacob  and
      Vlachos, Andreas  and
      Clark, Stephen",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P13-2009",
    pages = "47--52",
}
@inproceedings{jia-liang-2016-data,
    title = "Data Recombination for Neural Semantic Parsing",
    author = "Jia, Robin  and
      Liang, Percy",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1002",
    doi = "10.18653/v1/P16-1002",
    pages = "12--22",
}
@inproceedings{zhang-etal-2018-cross,
    title = "Cross-lingual Decompositional Semantic Parsing",
    author = "Zhang, Sheng  and
      Ma, Xutai  and
      Rudinger, Rachel  and
      Duh, Kevin  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1194",
    doi = "10.18653/v1/D18-1194",
    pages = "1664--1675",
    abstract = "We introduce the task of cross-lingual decompositional semantic parsing: mapping content provided in a source language into a decompositional semantic analysis based on a target language. We present: (1) a form of decompositional semantic analysis designed to allow systems to target varying levels of structural complexity (shallow to deep analysis), (2) an evaluation metric to measure the similarity between system output and reference semantic analysis, (3) an end-to-end model with a novel annotating mechanism that supports intra-sentential coreference, and (4) an evaluation dataset on which our model outperforms strong baselines by at least 1.75 F1 score.",
}
@inproceedings{zhang-etal-2019-broad,
    title = "Broad-Coverage Semantic Parsing as Transduction",
    author = "Zhang, Sheng  and
      Ma, Xutai  and
      Duh, Kevin  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1392",
    doi = "10.18653/v1/D19-1392",
    pages = "3786--3798",
    abstract = "We unify different broad-coverage semantic parsing tasks into a transduction parsing paradigm, and propose an attention-based neural transducer that incrementally builds meaning representation via a sequence of semantic relations. By leveraging multiple attention mechanisms, the neural transducer can be effectively trained without relying on a pre-trained aligner. Experiments separately conducted on three broad-coverage semantic parsing tasks {--} AMR, SDP and UCCA {--} demonstrate that our attention-based neural transducer improves the state of the art on both AMR and UCCA, and is competitive with the state of the art on SDP.",
}
@inproceedings{talmor-berant-2018-web,
    title = "The Web as a Knowledge-Base for Answering Complex Questions",
    author = "Talmor, Alon  and
      Berant, Jonathan",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1059",
    doi = "10.18653/v1/N18-1059",
    pages = "641--651",
    abstract = "Answering complex questions is a time-consuming activity for humans that requires reasoning and integration of information. Recent work on reading comprehension made headway in answering simple questions, but tackling complex questions is still an ongoing research challenge. Conversely, semantic parsers have been successful at handling compositionality, but only when the information resides in a target knowledge-base. In this paper, we present a novel framework for answering broad and complex questions, assuming answering simple questions is possible using a search engine and a reading comprehension model. We propose to decompose complex questions into a sequence of simple questions, and compute the final answer from the sequence of answers. To illustrate the viability of our approach, we create a new dataset of complex questions, ComplexWebQuestions, and present a model that decomposes questions and interacts with the web to compute an answer. We empirically demonstrate that question decomposition improves performance from 20.8 precision@1 to 27.5 precision@1 on this new dataset.",
}

@inproceedings{yu-etal-2018-spider,
    title = "{S}pider: A Large-Scale Human-Labeled Dataset for Complex and Cross-Domain Semantic Parsing and Text-to-{SQL} Task",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Yang, Kai  and
      Yasunaga, Michihiro  and
      Wang, Dongxu  and
      Li, Zifan  and
      Ma, James  and
      Li, Irene  and
      Yao, Qingning  and
      Roman, Shanelle  and
      Zhang, Zilin  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1425",
    doi = "10.18653/v1/D18-1425",
    pages = "3911--3921",
    abstract = "We present \textit{Spider}, a large-scale complex and cross-domain semantic parsing and text-to-SQL dataset annotated by 11 college students. It consists of 10,181 questions and 5,693 unique complex SQL queries on 200 databases with multiple tables covering 138 different domains. We define a new complex and cross-domain semantic parsing and text-to-SQL task so that different complicated SQL queries and databases appear in train and test sets. In this way, the task requires the model to generalize well to both new SQL queries and new database schemas. Therefore, Spider is distinct from most of the previous semantic parsing tasks because they all use a single database and have the exact same program in the train set and the test set. We experiment with various state-of-the-art models and the best model achieves only 9.7{\%} exact matching accuracy on a database split setting. This shows that Spider presents a strong challenge for future research. Our dataset and task with the most recent updates are publicly available at \url{https://yale-lily.github.io/seq2sql/spider}.",
}

@InProceedings{zelle:aaai96,
title={Learning to Parse Database Queries using Inductive Logic Programming},
author={John M. Zelle and Raymond J. Mooney},
booktitle={AAAI'96: Proceedings of the Thirteenth National Conference on Artificial Intelligence},
volume=2,
pages={1050--1055},
year={1996},
month=aug,
address={Portland, OR},
OPTpublisher={AAAI Press/MIT Press},
url="https://dl.acm.org/doi/10.5555/1864519.1864543",
}

@inproceedings{Li-nalir:2014,
    author = {Fei Li and H. V. Jagadish},
    title = {{NaLIR}: An Interactive Natural Language Interface for Querying Relational Databases},
    year = {2014},
    booktitle={International Conference on Management of Data, SIGMOD},
    url={https://dl.acm.org/doi/10.1145/2588555.2594519},
}
@inproceedings{price-1990-evaluation,
    title = "Evaluation of Spoken Language Systems: the {ATIS} Domain",
    author = "Price, P. J.",
    booktitle = "Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",
    year = "1990",
    url = "https://www.aclweb.org/anthology/H90-1020",
}
@inproceedings{moore-etal-1997-commandtalk,
    title = "{C}ommand{T}alk: A Spoken-Language Interface for Battlefield Simulations",
    author = "Moore, R. C.  and
      Dowding, J.  and
      Bratt, H.  and
      Gawron, J. M.  and
      Gorfu, Y.  and
      Cheyer, A.",
    booktitle = "Fifth Conference on Applied Natural Language Processing",
    month = mar,
    year = "1997",
    address = "Washington, DC, USA",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/A97-1001",
    doi = "10.3115/974557.974558",
    pages = "1--7",
}

@inproceedings{ijcai2020-502,
  title     = {Guided Generation of Cause and Effect},
  author    = {Li, Zhongyang and Ding, Xiao and Liu, Ting and Hu, J. Edward and Van Durme, Benjamin},
  booktitle = {Proceedings of the Twenty-Ninth International Joint Conference on
               Artificial Intelligence, {IJCAI-20}},
  OPTpublisher = {International Joint Conferences on Artificial Intelligence Organization},             
  editor    = {Christian Bessiere},	
  pages     = {3629--3636},
  year      = {2020},
  month     = {7},
  note      = {Main track},
  doi       = {10.24963/ijcai.2020/502},
  url       = {https://doi.org/10.24963/ijcai.2020/502},
}


@inproceedings{hu-etal-2019-improved,
    title = "Improved Lexically Constrained Decoding for Translation and Monolingual Rewriting",
    author = "Hu, J. Edward  and
      Khayrallah, Huda  and
      Culkin, Ryan  and
      Xia, Patrick  and
      Chen, Tongfei  and
      Post, Matt  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1090",
    doi = "10.18653/v1/N19-1090",
    pages = "839--850",
    abstract = "Lexically-constrained sequence decoding allows for explicit positive or negative phrase-based constraints to be placed on target output strings in generation tasks such as machine translation or monolingual text rewriting. We describe vectorized dynamic beam allocation, which extends work in lexically-constrained decoding to work with batching, leading to a five-fold improvement in throughput when working with positive constraints. Faster decoding enables faster exploration of constraint strategies: we illustrate this via data augmentation experiments with a monolingual rewriter applied to the tasks of natural language inference, question answering and machine translation, showing improvements in all three.",
}

@article{weston-15,
    author = {Jason Weston and Antoine Bordes and Sumit Chopra and Rush, Alexander M and van Merrienboer, Bart and Armand Joulin and Tomas Mikolov},
    year = 2015,
    title = {Towards {AI}-Complete Question Answering: A Set of Prerequisite Toy Tasks},
    journal={Computing Research Repository},
    volume = {arXiv:1502.05698},
    url = {https://arxiv.org/abs/1502.05698},
}
@inproceedings{lake-baroni-18,
author = "Brenden Lake and Marco Baroni",
year = 2018,
title = "Generalization Without Systematicity: On the Compositional Skills
of Sequence-to-Sequence Recurrent Networks",
booktitle = "International Conference on Machine Learning",
url={https://arxiv.org/abs/1711.00350},
}
@inproceedings{campagna-etal-2020-zero,
    title = "Zero-Shot Transfer Learning with Synthesized Data for Multi-Domain Dialogue State Tracking",
    author = "Campagna, Giovanni  and
      Foryciarz, Agata  and
      Moradshahi, Mehrad  and
      Lam, Monica",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.12",
    doi = "10.18653/v1/2020.acl-main.12",
    pages = "122--132",
    abstract = "Zero-shot transfer learning for multi-domain dialogue state tracking can allow us to handle new domains without incurring the high cost of data acquisition. This paper proposes new zero-short transfer learning technique for dialogue state tracking where the in-domain training data are all synthesized from an abstract dialogue model and the ontology of the domain. We show that data augmentation through synthesized data can improve the accuracy of zero-shot learning for both the TRADE model and the BERT-based SUMBT model on the MultiWOZ 2.1 dataset. We show training with only synthesized in-domain data on the SUMBT model can reach about 2/3 of the accuracy obtained with the full training dataset. We improve the zero-shot learning state of the art on average across domains by 21{\%}.",
}
@inproceedings{campagna-19,
title="Genie: A Generator of Natural Language Semantic Parsers for Virtual Assistant Commands",
author = "Giovanni Campagna and Silei Xu and Mehrad Moradshahi and Richard Socher and Lam,  Monica S.",
booktitle ="Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",
year = 2019,
url = {https://dl.acm.org/doi/10.1145/3314221.3314594},
}
@inproceedings{dbpal20,
author={Nathaniel Weir and Prasetya Utama and Alex Galakatos and Andrew Crotty and Amir Ilkhechi and Shekar Ramaswamy and Rohin Bhushan and Nadja Geisler and Benjamin Hattasch and Steffen Eger and Carsten Binnig and Ugur Cetintemel},
title={{DBPal}: A Fully Pluggable {NL2SQL} Training Pipeline},
booktitle={Proceedings of SIGMOD},
year={2020},
url={https://dl.acm.org/doi/10.1145/3318464.3380589},
}
@inproceedings{dbpal18,
author= {Fuat Basik, Benjamin Hattasch, Amir Ilkhechi, Arif Usta, Shekar Ramaswamy, Prasetya Utama, Nathaniel Weir, Carsten Binnig, Ugur Cetintemel},
title={DBPal: A Learned NL-Interface for Databases},
booktitle={Proceedings of SIGMOD (demo)},
year = 2018}

@article{yu2020grappa,
    title={GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing}, 
    author={Tao Yu and Chien-Sheng Wu and Xi Victoria Lin and Bailin Wang and Yi Chern Tan and Xinyi Yang and Dragomir Radev and Richard Socher and Caiming Xiong},
    year={2020},
    journal={Computing Research Repository},
    volume={arXiv:2009.13845},
    url={https://arxiv.org/abs/2009.13845},
}
@inproceedings{anderson-etal-2017-guided,
    title = "Guided Open Vocabulary Image Captioning with Constrained Beam Search",
    author = "Anderson, Peter  and
      Fernando, Basura  and
      Johnson, Mark  and
      Gould, Stephen",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1098",
    doi = "10.18653/v1/D17-1098",
    pages = "936--945",
    abstract = "Existing image captioning models do not generalize well to out-of-domain images containing novel scenes or objects. This limitation severely hinders the use of these models in real world applications dealing with images in the wild. We address this problem using a flexible approach that enables existing deep captioning architectures to take advantage of image taggers at test time, without re-training. Our method uses constrained beam search to force the inclusion of selected tag words in the output, and fixed, pretrained word embeddings to facilitate vocabulary expansion to previously unseen tag words. Using this approach we achieve state of the art results for out-of-domain captioning on MSCOCO (and improved results for in-domain captioning). Perhaps surprisingly, our results significantly outperform approaches that incorporate the same tag predictions into the learning algorithm. We also show that we can significantly improve the quality of generated ImageNet captions by leveraging ground-truth labels.",
}

@inproceedings{hokamp-liu-2017-lexically,
    title = "Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search",
    author = "Hokamp, Chris  and
      Liu, Qun",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1141",
    doi = "10.18653/v1/P17-1141",
    pages = "1535--1546",
    abstract = "We present Grid Beam Search (GBS), an algorithm which extends beam search to allow the inclusion of pre-specified lexical constraints. The algorithm can be used with any model which generates sequences token by token. Lexical constraints take the form of phrases or words that must be present in the output sequence. This is a very general way to incorporate auxillary knowledge into a model{'}s output without requiring any modification of the parameters or training data. We demonstrate the feasibility and flexibility of Lexically Constrained Decoding by conducting experiments on Neural Interactive-Predictive Translation, as well as Domain Adaptation for Neural Machine Translation. Experiments show that GBS can provide large improvements in translation quality in interactive scenarios, and that, even without any user input, GBS can be used to achieve significant gains in performance in domain adaptation scenarios.",
}

@inproceedings{post-vilar-2018-fast,
    title = "Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation",
    author = "Post, Matt  and
      Vilar, David",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1119",
    doi = "10.18653/v1/N18-1119",
    pages = "1314--1324",
    abstract = "The end-to-end nature of neural machine translation (NMT) removes many ways of manually guiding the translation process that were available in older paradigms. Recent work, however, has introduced a new capability: lexically constrained or guided decoding, a modification to beam search that forces the inclusion of pre-specified words and phrases in the output. However, while theoretically sound, existing approaches have computational complexities that are either linear (Hokamp and Liu, 2017) or exponential (Anderson et al., 2017) in the number of constraints. We present a algorithm for lexically constrained decoding with a complexity of O(1) in the number of constraints. We demonstrate the algorithm{'}s remarkable ability to properly place these constraints, and use it to explore the shaky relationship between model and BLEU scores. Our implementation is available as part of Sockeye.",
}
@inproceedings{moore_removing_2000,
    title = "Removing Left Recursion from Context-Free Grammars",
    author = "Moore, Robert C.",
    booktitle = "1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2000",
    url = "https://www.aclweb.org/anthology/A00-2033",
}

@INPROCEEDINGS{rosenkrantz_deterministic_1970,
  author={D. J. {Rosenkrantz} and P. M. {Lewis}},
  booktitle={11th Annual Symposium on Switching and Automata Theory (SWAT 1970)}, 
  title={Deterministic left corner parsing}, 
  year={1970},
  volume={},
  number={},
  pages={139--152},
  doi={10.1109/SWAT.1970.5}
}

@inproceedings{johnson_finite_state_1998,
	title = {Finite-State Approximation of Constraint-Based Grammars Using Left-Corner Grammar Transforms},
	url = {https://www.aclweb.org/anthology/C98-1098},
	urldate = {2021-01-13},
	booktitle = {Proceedings of COLING: The 17th International Conference
                  on Computational Linguistics},
        volume = 1,
	author = {Johnson, Mark},
	year = {1998},
}

@article{brown2020language,
    title={Language Models Are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    eprint={2005.14165},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2005.14165}
}

@inproceedings{wu-etal-2021-paraphrasing,
    title = "From Paraphrasing to Semantic Parsing: Unsupervised Semantic Parsing via Synchronous Semantic Decoding",
    author = "Wu, Shan  and
      Chen, Bo  and
      Xin, Chunlei  and
      Han, Xianpei  and
      Sun, Le  and
      Zhang, Weipeng  and
      Chen, Jiansong  and
      Yang, Fan  and
      Cai, Xunliang",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.397",
    doi = "10.18653/v1/2021.acl-long.397",
    pages = "5110--5121",
    abstract = "Semantic parsing is challenging due to the structure gap and the semantic gap between utterances and logical forms. In this paper, we propose an unsupervised semantic parsing method - Synchronous Semantic Decoding (SSD), which can simultaneously resolve the semantic gap and the structure gap by jointly leveraging paraphrasing and grammar-constrained decoding. Specifically, we reformulate semantic parsing as a constrained paraphrasing problem: given an utterance, our model synchronously generates its canonical utterancel and meaning representation. During synchronously decoding: the utterance paraphrasing is constrained by the structure of the logical form, therefore the canonical utterance can be paraphrased controlledly; the semantic decoding is guided by the semantics of the canonical utterance, therefore its logical form can be generated unsupervisedly. Experimental results show that SSD is a promising approach and can achieve state-of-the-art unsupervised semantic parsing performance on multiple datasets.",
}


@article{schick2020exploiting,
      title={Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference}, 
      author={Timo Schick and Hinrich Schütze},
      year={2020},
      journal={Computing Research Repository},
      volume={arXiv:2001.07676},
      url={https://arxiv.org/abs/2001.07676}
}
@incollection{nagao-1984,
  author = 	 {Makoto Nagao},
  editor = 	 {A. Elithorn and R. Banerji},
  title = 	 {{A Framework of Mechanical Translation between Japanese and English by Analogy Principle}},
  booktitle= {ARTIFICIAL AND HUMAN INTELLIGENCE},
  chapter = 	 {11},
  publisher = 	 {Elsevier Science Publishers},
  year = 	 1984,
}
@article{artetxe-schwenk-2019-massively,
    title = "Massively Multilingual Sentence Embeddings for Zero-Shot Cross-Lingual Transfer and Beyond",
    author = "Artetxe, Mikel  and
      Schwenk, Holger",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1038",
    doi = "10.1162/tacl_a_00288",
    pages = "597--610",
    abstract = "We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.",
}
@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1410",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
    abstract = "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.",
}
@inproceedings{lewis-etal-2020-bart,
    title = "{BART}: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
    author = "Lewis, Mike  and
      Liu, Yinhan  and
      Goyal, Naman  and
      Ghazvininejad, Marjan  and
      Mohamed, Abdelrahman  and
      Levy, Omer  and
      Stoyanov, Veselin  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.703",
    doi = "10.18653/v1/2020.acl-main.703",
    pages = "7871--7880",
    abstract = "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.",
}
@inproceedings{yao-etal-2020-imitation,
    title = "An Imitation Game for Learning Semantic Parsers from User Interaction",
    author = "Yao, Ziyu  and
      Tang, Yiqi  and
      Yih, Wen-tau  and
      Sun, Huan  and
      Su, Yu",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.559",
    doi = "10.18653/v1/2020.emnlp-main.559",
    pages = "6883--6902",
    abstract = "Despite the widely successful applications, bootstrapping and fine-tuning semantic parsers are still a tedious process with challenges such as costly data annotation and privacy risks. In this paper, we suggest an alternative, human-in-the-loop methodology for learning semantic parsers directly from users. A semantic parser should be introspective of its uncertainties and prompt for user demonstrations when uncertain. In doing so it also gets to imitate the user behavior and continue improving itself autonomously with the hope that eventually it may become as good as the user in interpreting their questions. To combat the sparsity of demonstrations, we propose a novel annotation-efficient imitation learning algorithm, which iteratively collects new datasets by mixing demonstrated states and confident predictions and retrains the semantic parser in a Dataset Aggregation fashion (Ross et al., 2011). We provide a theoretical analysis of its cost bound and also empirically demonstrate its promising performance on the text-to-SQL problem. Code will be available at https://github.com/sunlab-osu/MISP.",
}
@inproceedings{chen-etal-2020-low,
    title = "Low-Resource Domain Adaptation for Compositional Task-Oriented Semantic Parsing",
    author = "Chen, Xilun  and
      Ghoshal, Asish  and
      Mehdad, Yashar  and
      Zettlemoyer, Luke  and
      Gupta, Sonal",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.413",
    doi = "10.18653/v1/2020.emnlp-main.413",
    pages = "5090--5100",
    abstract = "Task-oriented semantic parsing is a critical component of virtual assistants, which is responsible for understanding the user{'}s intents (set reminder, play music, etc.). Recent advances in deep learning have enabled several approaches to successfully parse more complex queries (Gupta et al., 2018; Rongali et al.,2020), but these models require a large amount of annotated training data to parse queries on new domains (e.g. reminder, music). In this paper, we focus on adapting task-oriented semantic parsers to low-resource domains, and propose a novel method that outperforms a supervised neural model at a 10-fold data reduction. In particular, we identify two fundamental factors for low-resource domain adaptation: better representation learning and better training techniques. Our representation learning uses BART (Lewis et al., 2019) to initialize our model which outperforms encoder-only pre-trained representations used in previous work. Furthermore, we train with optimization-based meta-learning (Finn et al., 2017) to improve generalization to low-resource domains. This approach significantly outperforms all baseline methods in the experiments on a newly collected multi-domain task-oriented semantic parsing dataset (TOPv2), which we release to the public.",
}
@inproceedings{hashimoto-18,
    title={A Retrieve-and-Edit Framework for Predicting Structured Outputs},
    author={Tatsunori B. Hashimoto and Kelvin Guu and Yonatan Oren and Percy Liang},
    year={2018},
    booktitle={32nd Conference on Neural Information Processing Systems}}

@techreport{radford2018improving,
      title={Improving Language Understanding by Generative Pre-Training},
      author={Alec Radford and Karthik Narasimhan and Tim Salimans and and Ilya Sutskever},
      year={2018},
      url={https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf}
}
@inproceedings{kocisky-etal-2016-semantic,
    title = "Semantic Parsing with Semi-Supervised Sequential Autoencoders",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward  and
      Dyer, Chris  and
      Ling, Wang  and
      Blunsom, Phil  and
      Hermann, Karl Moritz",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1116",
    doi = "10.18653/v1/D16-1116",
    pages = "1078--1087",
}
@inproceedings{krishnamurthy-etal-2017-neural,
    title = "Neural Semantic Parsing with Type Constraints for Semi-Structured Tables",
    author = "Krishnamurthy, Jayant  and
      Dasigi, Pradeep  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1160",
    doi = "10.18653/v1/D17-1160",
    pages = "1516--1526",
    abstract = "We present a new semantic parsing model for answering compositional questions on semi-structured Wikipedia tables. Our parser is an encoder-decoder neural network with two key technical innovations: (1) a grammar for the decoder that only generates well-typed logical forms; and (2) an entity embedding and linking module that identifies entity mentions while generalizing across tables. We also introduce a novel method for training our neural model with question-answer supervision. On the WikiTableQuestions data set, our parser achieves a state-of-the-art accuracy of 43.3{\%} for a single model and 45.9{\%} for a 5-model ensemble, improving on the best prior score of 38.7{\%} set by a 15-model ensemble. These results suggest that type constraints and entity linking are valuable components to incorporate in neural semantic parsers.",
}
@inproceedings{yin-neubig-2017-syntactic,
    title = "A Syntactic Neural Model for General-Purpose Code Generation",
    author = "Yin, Pengcheng  and
      Neubig, Graham",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1041",
    doi = "10.18653/v1/P17-1041",
    pages = "440--450",
    abstract = "We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.",
}
@inproceedings{wong-mooney-2006-learning,
    title = "Learning for Semantic Parsing with Statistical Machine Translation",
    author = "Wong, Yuk Wah  and
      Mooney, Raymond",
    booktitle = "Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",
    month = jun,
    year = "2006",
    address = "New York City, USA",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N06-1056",
    pages = "439--446",
}
@inproceedings{dong-lapata-2016-language,
    title = "Language to Logical Form with Neural Attention",
    author = "Dong, Li  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1004",
    doi = "10.18653/v1/P16-1004",
    pages = "33--43",
}
@article{liu2021makes,
      title={What Makes Good In-Context Examples for {GPT}-3?}, 
      author={Jiachang Liu and Dinghan Shen and Yizhe Zhang and Bill Dolan and Lawrence Carin and Weizhu Chen},
      year={2021},
    journal = {Computing Research Repository},
    volume ={arXiv:2101.06804},
    url={https://arxiv.org/abs/2101.06804},
}
@article{gao2020making,
      title={Making Pre-trained Language Models Better Few-shot Learners}, 
      author={Tianyu Gao and Adam Fisch and Danqi Chen},
      year={2020},
      journal={Computing Research Repository},
      volume={arXiv:2012.15723},
      url={https://arxiv.org/abs/2012.15723},
}

@misc{schick2020its,
      title={It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners}, 
      author={Timo Schick and Hinrich Schütze},
      year={2020},
      eprint={2009.07118},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Wolfson2020Break,
  title={Break It Down: A Question Understanding Benchmark},
  author={Wolfson, Tomer and Geva, Mor and Gupta, Ankit and Gardner, Matt and Goldberg, Yoav and Deutch, Daniel and Berant, Jonathan},
  journal={Transactions of the Association for Computational Linguistics},
  volume=8,
  pages={183--198},
  year={2020},
  url={https://www.mitpressjournals.org/doi/full/10.1162/tacl_a_00309},
}

@inproceedings{wang-etal-2015-building,
    title = "Building a Semantic Parser Overnight",
    author = "Wang, Yushi  and
      Berant, Jonathan  and
      Liang, Percy",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1129",
    doi = "10.3115/v1/P15-1129",
    pages = "1332--1342",
}

@article{xu-etal-2016-optimizing,
    title = "Optimizing Statistical Machine Translation for Text Simplification",
    author = "Xu, Wei  and
      Napoles, Courtney  and
      Pavlick, Ellie  and
      Chen, Quanze  and
      Callison-Burch, Chris",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "4",
    year = "2016",
    url = "https://www.aclweb.org/anthology/Q16-1029",
    doi = "10.1162/tacl_a_00107",
    pages = "401--415",
    abstract = "Most recent sentence simplification systems use basic machine translation models to learn lexical and syntactic paraphrases from a manually simplified parallel corpus. These methods are limited by the quality and quantity of manually simplified corpora, which are expensive to build. In this paper, we conduct an in-depth adaptation of statistical machine translation to perform text simplification, taking advantage of large-scale paraphrases learned from bilingual texts and a small amount of manual simplifications with multiple references. Our work is the first to design automatic metrics that are effective for tuning and evaluating simplification systems, which will facilitate iterative development for this task.",
}

@inproceedings{xu-etal-2020-autoqa,
    title = "{A}uto{QA}: From Databases To {QA} Semantic Parsers With Only Synthetic Training Data",
    author = "Xu, Silei  and
      Semnani, Sina  and
      Campagna, Giovanni  and
      Lam, Monica",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    OPTpublisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.31",
    doi = "10.18653/v1/2020.emnlp-main.31",
    pages = "422--434",
    abstract = "We propose AutoQA, a methodology and toolkit to generate semantic parsers that answer questions on databases, with no manual effort. Given a database schema and its data, AutoQA automatically generates a large set of high-quality questions for training that covers different database operations. It uses automatic paraphrasing combined with template-based parsing to find alternative expressions of an attribute in different parts of speech. It also uses a novel filtered auto-paraphraser to generate correct paraphrases of entire sentences. We apply AutoQA to the Schema2QA dataset and obtain an average logical form accuracy of 62.9{\%} when tested on natural questions, which is only 6.4{\%} lower than a model trained with expert natural language annotations and paraphrase data collected from crowdworkers. To demonstrate the generality of AutoQA, we also apply it to the Overnight dataset. AutoQA achieves 69.8{\%} answer accuracy, 16.4{\%} higher than the state-of-the-art zero-shot models and only 5.2{\%} lower than the same model trained with human data.",
}


@article{Andreas:2020:dataflow,
    author = {{Semantic Machines} and Andreas, Jacob and Bufe, John and Burkett, David and Chen, Charles and Clausman, Josh and Crawford, Jean and Crim, Kate and DeLoach, Jordan and Dorner, Leah and Eisner, Jason and Fang, Hao and Guo, Alan and Hall, David and Hayes, Kristin and Hill, Kellie and Ho, Diana and Iwaszuk, Wendy and Jha, Smriti and Klein, Dan and Krishnamurthy, Jayant and Lanman, Theo and Liang, Percy and Lin, Christopher H. and Lintsbakh, Ilya and McGovern, Andy and Nisnevich, Aleksandr and Pauls, Adam and Petters, Dmitrij and Read, Brent and Roth, Dan and Roy, Subhro and Rusak, Jesse and Short, Beth and Slomin, Div and Snyder, Ben and Striplin, Stephon and Su, Yu and Tellman, Zachary and Thomson, Sam and Vorobev, Andrei and Witoszko, Izabela and Wolfe, Jason and Wray, Abby and Zhang, Yuchen and Zotov, Alexander},
    title = {Task-Oriented Dialogue as Dataflow Synthesis},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {8},
    number = {},
    pages = {556--571},
    year = {2020},
    doi = {10.1162/tacl\_a\_00333},
    url = {https://doi.org/10.1162/tacl_a_00333},
    eprint = {https://doi.org/10.1162/tacl_a_00333},
}

@inproceedings{cao-etal-2019-semantic,
    title = "Semantic Parsing with Dual Learning",
    author = "Cao, Ruisheng  and
      Zhu, Su  and
      Liu, Chen  and
      Li, Jieyu  and
      Yu, Kai",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1007",
    doi = "10.18653/v1/P19-1007",
    pages = "51--64",
    abstract = "Semantic parsing converts natural language queries into structured logical forms. The lack of training data is still one of the most serious problems in this area. In this work, we develop a semantic parsing framework with the dual learning algorithm, which enables a semantic parser to make full use of data (labeled and even unlabeled) through a dual-learning game. This game between a primal model (semantic parsing) and a dual model (logical form to query) forces them to regularize each other, and can achieve feedback signals from some prior-knowledge. By utilizing the prior-knowledge of logical form structures, we propose a novel reward signal at the surface and semantic levels which tends to generate complete and reasonable logical forms. Experimental results show that our approach achieves new state-of-the-art performance on ATIS dataset and gets competitive performance on OVERNIGHT dataset.",
}
@misc{liu2021gpt,
      title={GPT Understands, Too}, 
      author={Xiao Liu and Yanan Zheng and Zhengxiao Du and Ming Ding and Yujie Qian and Zhilin Yang and Jie Tang},
      year={2021},
      eprint={2103.10385},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{li2021prefixtuning,
      title={Prefix-Tuning: Optimizing Continuous Prompts for Generation}, 
      author={Xiang Lisa Li and Percy Liang},
      year={2021},
      eprint={2101.00190},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{wu2016googles,
      title={Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}, 
      author={Yonghui Wu and Mike Schuster and Zhifeng Chen and Quoc V. Le and Mohammad Norouzi and Wolfgang Macherey and Maxim Krikun and Yuan Cao and Qin Gao and Klaus Macherey and Jeff Klingner and Apurva Shah and Melvin Johnson and Xiaobing Liu and Łukasz Kaiser and Stephan Gouws and Yoshikiyo Kato and Taku Kudo and Hideto Kazawa and Keith Stevens and George Kurian and Nishant Patil and Wei Wang and Cliff Young and Jason Smith and Jason Riesa and Alex Rudnick and Oriol Vinyals and Greg Corrado and Macduff Hughes and Jeffrey Dean},
      year={2016},
      eprint={1609.08144},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{desai2021lowresource,
      title={Low-Resource Task-Oriented Semantic Parsing via Intrinsic Modeling}, 
      author={Shrey Desai and Akshat Shrivastava and Alexander Zotov and Ahmed Aly},
      year={2021},
      eprint={2104.07224},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{duan-etal-2016-generating,
    title = "Generating Disambiguating Paraphrases for Structurally Ambiguous Sentences",
    author = "Duan, Manjuan  and
      Hill, Ethan  and
      White, Michael",
    booktitle = "Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with {ACL} 2016 ({LAW}-X 2016)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W16-1718",
    doi = "10.18653/v1/W16-1718",
    pages = "160--170",
}

@misc{herzig2021unlocking,
      title={Unlocking Compositional Generalization in Pre-trained Models Using Intermediate Representations}, 
      author={Jonathan Herzig and Peter Shaw and Ming-Wei Chang and Kelvin Guu and Panupong Pasupat and Yuan Zhang},
      year={2021},
      eprint={2104.07478},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{earley70,
  author    = {Jay Earley},
  title     = {An Efficient Context-Free Parsing Algorithm},
  journal   = {Communications of the ACM},
  volume    = {13},
  number    = {2},
  pages     = {94--102},
  year      = {1970},
  url       = {https://doi.org/10.1145/362007.362035},
  doi       = {10.1145/362007.362035},
  timestamp = {Wed, 14 Nov 2018 10:22:30 +0100},
  biburl    = {https://dblp.org/rec/journals/cacm/Earley70.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{elgohary2021nl-edit,
author = {Elgohary, Ahmed and Meek, Chris and Richardson, Matthew and Fourney, Adam and Ramos, Gonzalo and Awadallah, Ahmed H.},
title = {{NL-EDIT}: Correcting Semantic Parse Errors through Natural Language Interaction},
booktitle = {NAACL 2021},
year = {2021},
month = {June},
url = {https://www.microsoft.com/en-us/research/publication/nl-edit-correcting-semantic-parse-errors-through-natural-language-interaction/},
}


@inproceedings{yao_model-based_2019,
	author = {Yao, Ziyu and Su, Yu and Sun, Huan and Yih, Wen-tau},
	title = {Model-based {Interactive} {Semantic} {Parsing}: {A} {Unified} {Framework} and {A} {Text}-to-{SQL} {Case} {Study}},
	address = {Hong Kong, China},
	shorttitle = {Model-based {Interactive} {Semantic} {Parsing}},
	url = {https://www.aclweb.org/anthology/D19-1547},
	doi = {10.18653/v1/D19-1547},
	urldate = {2021-05-17},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	month = nov,
	year = {2019},
	pages = {5447--5458},
}

@inproceedings{he-etal-2016-human,
    title = "Human-in-the-Loop Parsing",
    author = "He, Luheng  and
      Michael, Julian  and
      Lewis, Mike  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D16-1258",
    doi = "10.18653/v1/D16-1258",
    pages = "2337--2342",
}

@inproceedings{shin-etal-2021-constrained,
    title = "Constrained Language Models Yield Few-Shot Semantic Parsers",
    author = "Shin, Richard  and
      Lin, Christopher  and
      Thomson, Sam  and
      Chen, Charles  and
      Roy, Subhro  and
      Platanios, Emmanouil Antonios  and
      Pauls, Adam  and
      Klein, Dan  and
      Eisner, Jason  and
      Van Durme, Benjamin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.608",
    doi = "10.18653/v1/2021.emnlp-main.608",
    pages = "7699--7715",
    abstract = "We explore the use of large pretrained language models as few-shot semantic parsers. The goal in semantic parsing is to generate a structured meaning representation given a natural language input. However, language models are trained to generate natural language. To bridge the gap, we use language models to paraphrase inputs into a controlled sublanguage resembling English that can be automatically mapped to a target meaning representation. Our results demonstrate that with only a small amount of data and very little code to convert into English-like representations, our blueprint for rapidly bootstrapping semantic parsers leads to surprisingly effective performance on multiple community tasks, greatly exceeding baseline methods also trained on the same limited data.",
}

@inproceedings{scholak-etal-2021-picard,
    title = "{PICARD}: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models",
    author = "Scholak, Torsten  and
      Schucher, Nathan  and
      Bahdanau, Dzmitry",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.779",
    doi = "10.18653/v1/2021.emnlp-main.779",
    pages = "9895--9901",
    abstract = "Large pre-trained language models for textual data have an unconstrained output space; at each decoding step, they can produce any of 10,000s of sub-word tokens. When fine-tuned to target constrained formal languages like SQL, these models often generate invalid code, rendering it unusable. We propose PICARD (code available at https://github.com/ElementAI/picard), a method for constraining auto-regressive decoders of language models through incremental parsing. PICARD helps to find valid output sequences by rejecting inadmissible tokens at each decoding step. On the challenging Spider and CoSQL text-to-SQL translation tasks, we show that PICARD transforms fine-tuned T5 models with passable performance into state-of-the-art solutions.",
}

@inproceedings{wang-etal-2018-glue,
    title = "{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding",
    author = "Wang, Alex  and
      Singh, Amanpreet  and
      Michael, Julian  and
      Hill, Felix  and
      Levy, Omer  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-5446",
    doi = "10.18653/v1/W18-5446",
    pages = "353--355",
    abstract = "Human ability to understand language is \textit{general, flexible, and robust}. In contrast, most NLU models above the word level are designed for a specific task and struggle with out-of-domain data. If we aspire to develop models with understanding beyond the detection of superficial correspondences between inputs and outputs, then it is critical to develop a unified model that can execute a range of linguistic tasks across different domains. To facilitate research in this direction, we present the General Language Understanding Evaluation (GLUE, gluebenchmark.com): a benchmark of nine diverse NLU tasks, an auxiliary dataset for probing models for understanding of specific linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions.",
}

@inproceedings{wang-etal-2019-superglue,
 author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
 url = {https://proceedings.neurips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{cheng-etal-2020-conversational,
    title = "Conversational Semantic Parsing for Dialog State Tracking",
    author = "Cheng, Jianpeng  and
      Agrawal, Devang  and
      Mart{\'\i}nez Alonso, H{\'e}ctor  and
      Bhargava, Shruti  and
      Driesen, Joris  and
      Flego, Federico  and
      Kaplan, Dain  and
      Kartsaklis, Dimitri  and
      Li, Lin  and
      Piraviperumal, Dhivya  and
      Williams, Jason D.  and
      Yu, Hong  and
      {\'O} S{\'e}aghdha, Diarmuid  and
      Johannsen, Anders",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.651",
    doi = "10.18653/v1/2020.emnlp-main.651",
    pages = "8107--8117",
    abstract = "We consider a new perspective on dialog state tracking (DST), the task of estimating a user{'}s goal through the course of a dialog. By formulating DST as a semantic parsing task over hierarchical representations, we can incorporate semantic compositionality, cross-domain knowledge sharing and co-reference. We present TreeDST, a dataset of 27k conversations annotated with tree-structured dialog states and system acts. We describe an encoder-decoder framework for DST with hierarchical representations, which leads to {\textasciitilde}20{\%} improvement over state-of-the-art DST approaches that operate on a flat meaning space of slot-value pairs.",
}

@article{roberta-liu-2019,
    title = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    author = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and
              Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and
              Luke Zettlemoyer and Veselin Stoyanov},
    journal={arXiv preprint arXiv:1907.11692},
    year = {2019},
}

@article{t5-raffel-2020,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1-67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@article{codex-chen-2021,
  author    = {Mark Chen and
               Jerry Tworek and
               Heewoo Jun and
               Qiming Yuan and
               Henrique Ponde de Oliveira Pinto and
               Jared Kaplan and
               Harrison Edwards and
               Yuri Burda and
               Nicholas Joseph and
               Greg Brockman and
               Alex Ray and
               Raul Puri and
               Gretchen Krueger and
               Michael Petrov and
               Heidy Khlaaf and
               Girish Sastry and
               Pamela Mishkin and
               Brooke Chan and
               Scott Gray and
               Nick Ryder and
               Mikhail Pavlov and
               Alethea Power and
               Lukasz Kaiser and
               Mohammad Bavarian and
               Clemens Winter and
               Philippe Tillet and
               Felipe Petroski Such and
               Dave Cummings and
               Matthias Plappert and
               Fotios Chantzis and
               Elizabeth Barnes and
               Ariel Herbert{-}Voss and
               William Hebgen Guss and
               Alex Nichol and
               Alex Paino and
               Nikolas Tezak and
               Jie Tang and
               Igor Babuschkin and
               Suchir Balaji and
               Shantanu Jain and
               William Saunders and
               Christopher Hesse and
               Andrew N. Carr and
               Jan Leike and
               Joshua Achiam and
               Vedant Misra and
               Evan Morikawa and
               Alec Radford and
               Matthew Knight and
               Miles Brundage and
               Mira Murati and
               Katie Mayer and
               Peter Welinder and
               Bob McGrew and
               Dario Amodei and
               Sam McCandlish and
               Ilya Sutskever and
               Wojciech Zaremba},
  title     = {Evaluating Large Language Models Trained on Code},
  journal   = {CoRR},
  volume    = {abs/2107.03374},
  year      = {2021},
  url       = {https://arxiv.org/abs/2107.03374},
  eprinttype = {arXiv},
  eprint    = {2107.03374},
  timestamp = {Tue, 20 Jul 2021 15:08:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2107-03374.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{sanh2021multitask,
      title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
      author={Victor Sanh and Albert Webson and Colin Raffel and Stephen H. Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Teven Le Scao and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Stella Biderman and Leo Gao and Tali Bers and Thomas Wolf and Alexander M. Rush},
      year={2021},
      eprint={2110.08207},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{deberta-v3-he-2021,
  author    = {Pengcheng He and
               Jianfeng Gao and
               Weizhu Chen},
  title     = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with
               Gradient-Disentangled Embedding Sharing},
  journal   = {CoRR},
  volume    = {abs/2111.09543},
  year      = {2021},
  url       = {https://arxiv.org/abs/2111.09543},
  eprinttype = {arXiv},
  eprint    = {2111.09543},
  timestamp = {Mon, 22 Nov 2021 16:44:07 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2111-09543.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{zero-label-wang-2021,
  author    = {Zirui Wang and
               Adams Wei Yu and
               Orhan Firat and
               Yuan Cao},
  title     = {Towards Zero-Label Language Learning},
  journal   = {CoRR},
  volume    = {abs/2109.09193},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.09193},
  eprinttype = {arXiv},
  eprint    = {2109.09193},
  timestamp = {Mon, 27 Sep 2021 15:21:05 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-09193.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{schucher-2021,
  author    = {Nathan Schucher and
               Siva Reddy and
               Harm de Vries},
  title     = {The Power of Prompt Tuning for Low-Resource Semantic Parsing},
  journal   = {CoRR},
  volume    = {abs/2110.08525},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.08525},
  eprinttype = {arXiv},
  eprint    = {2110.08525},
  timestamp = {Fri, 22 Oct 2021 13:33:09 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-08525.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mtop-li-2021,
    title = "{MTOP}: A Comprehensive Multilingual Task-Oriented Semantic Parsing Benchmark",
    author = "Li, Haoran  and
      Arora, Abhinav  and
      Chen, Shuohui  and
      Gupta, Anchit  and
      Gupta, Sonal  and
      Mehdad, Yashar",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.257",
    doi = "10.18653/v1/2021.eacl-main.257",
    pages = "2950--2962",
    abstract = "Scaling semantic parsing models for task-oriented dialog systems to new languages is often expensive and time-consuming due to the lack of available datasets. Available datasets suffer from several shortcomings: a) they contain few languages b) they contain small amounts of labeled examples per language c) they are based on the simple intent and slot detection paradigm for non-compositional queries. In this paper, we present a new multilingual dataset, called MTOP, comprising of 100k annotated utterances in 6 languages across 11 domains. We use this dataset and other publicly available datasets to conduct a comprehensive benchmarking study on using various state-of-the-art multilingual pre-trained models for task-oriented semantic parsing. We achieve an average improvement of +6.3 points on Slot F1 for the two existing multilingual datasets, over best results reported in their experiments. Furthermore, we demonstrate strong zero-shot performance using pre-trained models combined with automatic translation and alignment, and a proposed distant supervision method to reduce the noise in slot label projection.",
}

@inproceedings{yu-etal-2019-cosql,
    title = "{C}o{SQL}: A Conversational Text-to-{SQL} Challenge Towards Cross-Domain Natural Language Interfaces to Databases",
    author = "Yu, Tao  and
      Zhang, Rui  and
      Er, Heyang  and
      Li, Suyi  and
      Xue, Eric  and
      Pang, Bo  and
      Lin, Xi Victoria  and
      Tan, Yi Chern  and
      Shi, Tianze  and
      Li, Zihan  and
      Jiang, Youxuan  and
      Yasunaga, Michihiro  and
      Shim, Sungrok  and
      Chen, Tao  and
      Fabbri, Alexander  and
      Li, Zifan  and
      Chen, Luyao  and
      Zhang, Yuwen  and
      Dixit, Shreya  and
      Zhang, Vincent  and
      Xiong, Caiming  and
      Socher, Richard  and
      Lasecki, Walter  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1204",
    doi = "10.18653/v1/D19-1204",
    pages = "1962--1979",
    abstract = "We present CoSQL, a corpus for building cross-domain, general-purpose database (DB) querying dialogue systems. It consists of 30k+ turns plus 10k+ annotated SQL queries, obtained from a Wizard-of-Oz (WOZ) collection of 3k dialogues querying 200 complex DBs spanning 138 domains. Each dialogue simulates a real-world DB query scenario with a crowd worker as a user exploring the DB and a SQL expert retrieving answers with SQL, clarifying ambiguous questions, or otherwise informing of unanswerable questions. When user questions are answerable by SQL, the expert describes the SQL and execution results to the user, hence maintaining a natural interaction flow. CoSQL introduces new challenges compared to existing task-oriented dialogue datasets: (1) the dialogue states are grounded in SQL, a domain-independent executable representation, instead of domain-specific slot value pairs, and (2) because testing is done on unseen databases, success requires generalizing to new domains. CoSQL includes three tasks: SQL-grounded dialogue state tracking, response generation from query results, and user dialogue act prediction. We evaluate a set of strong baselines for each task and show that CoSQL presents significant challenges for future research. The dataset, baselines, and leaderboard will be released at https://yale-lily.github.io/cosql.",
}

@inproceedings{ram2021fewshotqa,
  author = {Ori Ram and Yuval Kirstain and Jonathan Berant and Amir Globerson and Omer Levy},
  booktitle = {Association for Computational Linguistics (ACL)},
  title = {Few-Shot Question Answering by Pretraining Span Selection},
  year = {2021},
}

@article{rubin-2021-in-context-learning,
  author    = {Ohad Rubin and
               Jonathan Herzig and
               Jonathan Berant},
  title     = {Learning To Retrieve Prompts for In-Context Learning},
  journal   = {CoRR},
  volume    = {abs/2112.08633},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.08633},
  eprinttype = {arXiv},
  eprint    = {2112.08633},
  timestamp = {Mon, 03 Jan 2022 15:45:35 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-08633.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{pasupat-etal-2021-controllable,
    title = "Controllable Semantic Parsing via Retrieval Augmentation",
    author = "Pasupat, Panupong  and
      Zhang, Yuan  and
      Guu, Kelvin",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.607",
    doi = "10.18653/v1/2021.emnlp-main.607",
    pages = "7683--7698",
    abstract = "In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model.",
}

@InProceedings{pmlr-v80-shazeer18a,
  title = 	 {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  author =       {Shazeer, Noam and Stern, Mitchell},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {4596--4604},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/shazeer18a/shazeer18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/shazeer18a.html},
  abstract = 	 {In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves.}
}

@misc{antlr-2022,
  author = {antlr},
  title = {grammars-v4},
  year = {2022},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/antlr/grammars-v4}},
  commit = {7abdd63092631bbf02136fb19a4add3cb9c9499f}
}

@article{marcus-etal-1993-ptb,
    title = "Building a Large Annotated Corpus of {E}nglish: The {P}enn {T}reebank",
    author = "Marcus, Mitchell P.  and
      Santorini, Beatrice  and
      Marcinkiewicz, Mary Ann",
    journal = "Computational Linguistics",
    volume = "19",
    number = "2",
    year = "1993",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/J93-2004",
    pages = "313--330",
}

@inproceedings{nivre-etal-2016-universal,
    title = "{U}niversal {D}ependencies v1: A Multilingual Treebank Collection",
    author = "Nivre, Joakim  and
      de Marneffe, Marie-Catherine  and
      Ginter, Filip  and
      Goldberg, Yoav  and
      Haji{\v{c}}, Jan  and
      Manning, Christopher D.  and
      McDonald, Ryan  and
      Petrov, Slav  and
      Pyysalo, Sampo  and
      Silveira, Natalia  and
      Tsarfaty, Reut  and
      Zeman, Daniel",
    booktitle = "Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16)",
    month = may,
    year = "2016",
    address = "Portoro{\v{z}}, Slovenia",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://aclanthology.org/L16-1262",
    pages = "1659--1666",
    abstract = "Cross-linguistically consistent annotation is necessary for sound comparative evaluation and cross-lingual learning experiments. It is also useful for multilingual system development and comparative linguistic studies. Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. In this paper, we describe v1 of the universal guidelines, the underlying design principles, and the currently available treebanks for 33 languages.",
}

 @misc{univ-deps-211,
 title = {Universal Dependencies 2.11},
 author = {Zeman, Daniel and others},
 url = {http://hdl.handle.net/11234/1-4923},
 note = {{LINDAT}/{CLARIAH}-{CZ} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Licence Universal Dependencies v2.11},
 year = {2022} }

@inproceedings{banarescu-etal-2013-AMR,
    title = "{A}bstract {M}eaning {R}epresentation for Sembanking",
    author = "Banarescu, Laura  and
      Bonial, Claire  and
      Cai, Shu  and
      Georgescu, Madalina  and
      Griffitt, Kira  and
      Hermjakob, Ulf  and
      Knight, Kevin  and
      Koehn, Philipp  and
      Palmer, Martha  and
      Schneider, Nathan",
    booktitle = "Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-2322",
    pages = "178--186",
}

@article{UnifiedSKG,
      title={UnifiedSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models}, 
      author={Tianbao Xie and Chen Henry Wu and Peng Shi and Ruiqi Zhong and Torsten Scholak and Michihiro Yasunaga and Chien-Sheng Wu and Ming Zhong and Pengcheng Yin and Sida I. Wang and Victor Zhong and Bailin Wang and Chengzu Li and Connor Boyle and Ansong Ni and Ziyu Yao and Dragomir Radev and Caiming Xiong and Lingpeng Kong and Rui Zhang and Noah A. Smith and Luke Zettlemoyer and Tao Yu},
      journal={EMNLP},
      year={2022},
}

@article{mohammadshahi-henderson-2021-recursive,
    title = "Recursive Non-Autoregressive Graph-to-Graph Transformer for Dependency Parsing with Iterative Refinement",
    author = "Mohammadshahi, Alireza  and
      Henderson, James",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.8",
    doi = "10.1162/tacl_a_00358",
    pages = "120--138",
    abstract = "We propose the Recursive Non-autoregressive Graph-to-Graph Transformer architecture (RNGTr) for the iterative refinement of arbitrary graphs through the recursive application of a non-autoregressive Graph-to-Graph Transformer and apply it to syntactic dependency parsing. We demonstrate the power and effectiveness of RNGTr on several dependency corpora, using a refinement model pre-trained with BERT. We also introduce Syntactic Transformer (SynTr), a non-recursive parser similar to our refinement model. RNGTr can improve the accuracy of a variety of initial parsers on 13 languages from the Universal Dependencies Treebanks, English and Chinese Penn Treebanks, and the German CoNLL2009 corpus, even improving over the new state-of-the-art results achieved by SynTr, significantly improving the state-of-the-art for all corpora tested.",
}

@inproceedings{tian-etal-2020-improving,
    title = "Improving Constituency Parsing with Span Attention",
    author = "Tian, Yuanhe  and
      Song, Yan  and
      Xia, Fei  and
      Zhang, Tong",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2020",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.findings-emnlp.153",
    doi = "10.18653/v1/2020.findings-emnlp.153",
    pages = "1691--1703",
    abstract = "Constituency parsing is a fundamental and important task for natural language understanding, where a good representation of contextual information can help this task. N-grams, which is a conventional type of feature for contextual information, have been demonstrated to be useful in many tasks, and thus could also be beneficial for constituency parsing if they are appropriately modeled. In this paper, we propose span attention for neural chart-based constituency parsing to leverage n-gram information. Considering that current chart-based parsers with Transformer-based encoder represent spans by subtraction of the hidden states at the span boundaries, which may cause information loss especially for long spans, we incorporate n-grams into span representations by weighting them according to their contributions to the parsing process. Moreover, we propose categorical span attention to further enhance the model by weighting n-grams within different length categories, and thus benefit long-sentence parsing. Experimental results on three widely used benchmark datasets demonstrate the effectiveness of our approach in parsing Arabic, Chinese, and English, where state-of-the-art performance is obtained by our approach on all of them.",
}

@inproceedings{bai-etal-2022-graph,
    title = "Graph Pre-training for {AMR} Parsing and Generation",
    author = "Bai, Xuefeng  and
      Chen, Yulong  and
      Zhang, Yue",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.415",
    doi = "10.18653/v1/2022.acl-long.415",
    pages = "6001--6015",
    abstract = "Abstract meaning representation (AMR) highlights the core semantic information of text in a graph structure.Recently, pre-trained language models (PLMs) have advanced tasks of AMR parsing and AMR-to-text generation, respectively.However, PLMs are typically pre-trained on textual data, thus are sub-optimal for modeling structural knowledge.To this end, we investigate graph self-supervised training to improve the structure awareness of PLMs over AMR graphs.In particular, we introduce two graph auto-encoding strategies for graph-to-graph pre-training and four tasks to integrate text and graph information during pre-training.We further design a unified framework to bridge the gap between pre-training and fine-tuning tasks.Experiments on both AMR parsing and AMR-to-text generation show the superiority of our model.To our knowledge, we are the first to consider pre-training on semantic graphs.",
}

@inproceedings{cai-lam-2020-amr,
    title = "{AMR} Parsing via Graph-Sequence Iterative Inference",
    author = "Cai, Deng  and
      Lam, Wai",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.119",
    doi = "10.18653/v1/2020.acl-main.119",
    pages = "1290--1301",
    abstract = "We propose a new end-to-end model that treats AMR parsing as a series of dual decisions on the input sequence and the incrementally constructed graph. At each time step, our model performs multiple rounds of attention, reasoning, and composition that aim to answer two critical questions: (1) which part of the input \textit{sequence} to abstract; and (2) where in the output \textit{graph} to construct the new concept. We show that the answers to these two questions are mutually causalities. We design a model based on iterative inference that helps achieve better answers in both perspectives, leading to greatly improved parsing accuracy. Our experimental results significantly outperform all previously reported Smatch scores by large margins. Remarkably, without the help of any large-scale pre-trained language model (e.g., BERT), our model already surpasses previous state-of-the-art using BERT. With the help of BERT, we can push the state-of-the-art results to 80.2{\%} on LDC2017T10 (AMR 2.0) and 75.4{\%} on LDC2014T12 (AMR 1.0).",
}

@inproceedings{yang-tu-2022-bottom,
    title = "Bottom-Up Constituency Parsing and Nested Named Entity Recognition with Pointer Networks",
    author = "Yang, Songlin  and
      Tu, Kewei",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.171",
    doi = "10.18653/v1/2022.acl-long.171",
    pages = "2403--2416",
    abstract = "Constituency parsing and nested named entity recognition (NER) are similar tasks since they both aim to predict a collection of nested and non-crossing spans. In this work, we cast nested NER to constituency parsing and propose a novel pointing mechanism for bottom-up parsing to tackle both tasks. The key idea is based on the observation that if we traverse a constituency tree in post-order, i.e., visiting a parent after its children, then two consecutively visited spans would share a boundary. Our model tracks the shared boundaries and predicts the next boundary at each step by leveraging a pointer network. As a result, it needs only linear steps to parse and thus is efficient. It also maintains a parsing configuration for structural consistency, i.e., always outputting valid trees. Experimentally, our model achieves the state-of-the-art performance on PTB among all BERT-based models (96.01 F1 score) and competitive performance on CTB7 in constituency parsing; and it also achieves strong performance on three benchmark datasets of nested NER: ACE2004, ACE2005, and GENIA. Our code will be available at \url{https://github.com/xxxxx}.",
}

@inproceedings{kitaev-klein-2018-constituency,
    title = "Constituency Parsing with a Self-Attentive Encoder",
    author = "Kitaev, Nikita  and
      Klein, Dan",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P18-1249",
    doi = "10.18653/v1/P18-1249",
    pages = "2676--2686",
    abstract = "We demonstrate that replacing an LSTM encoder with a self-attentive architecture can lead to improvements to a state-of-the-art discriminative constituency parser. The use of attention makes explicit the manner in which information is propagated between different locations in the sentence, which we use to both analyze our model and propose potential improvements. For example, we find that separating positional and content information in the encoder can lead to improved parsing accuracy. Additionally, we evaluate different approaches for lexical representation. Our parser achieves new state-of-the-art results for single models trained on the Penn Treebank: 93.55 F1 without the use of any external data, and 95.13 F1 when using pre-trained word representations. Our parser also outperforms the previous best-published accuracy figures on 8 of the 9 languages in the SPMRL dataset.",
}

@inproceedings{zhang-etal-2017-stack,
    title = "Stack-based Multi-layer Attention for Transition-based Dependency Parsing",
    author = "Zhang, Zhirui  and
      Liu, Shujie  and
      Li, Mu  and
      Zhou, Ming  and
      Chen, Enhong",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1175",
    doi = "10.18653/v1/D17-1175",
    pages = "1677--1682",
    abstract = "Although sequence-to-sequence (seq2seq) network has achieved significant success in many NLP tasks such as machine translation and text summarization, simply applying this approach to transition-based dependency parsing cannot yield a comparable performance gain as in other state-of-the-art methods, such as stack-LSTM and head selection. In this paper, we propose a stack-based multi-layer attention model for seq2seq learning to better leverage structural linguistics information. In our method, two binary vectors are used to track the decoding stack in transition-based parsing, and multi-layer attention is introduced to capture multiple word dependencies in partial trees. We conduct experiments on PTB and CTB datasets, and the results show that our proposed model achieves state-of-the-art accuracy and significant improvement in labeled precision with respect to the baseline seq2seq model.",
}

@article{liu-zhang-transition-2017,
    author = {Liu, Jiangming and Zhang, Yue},
    title = "{In-Order Transition-based Constituent Parsing}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {5},
    pages = {413-424},
    year = {2017},
    month = {11},
    abstract = "{Both bottom-up and top-down strategies have been used for neural transition-based
                    constituent parsing. The parsing strategies differ in terms of the order in
                    which they recognize productions in the derivation tree, where bottom-up
                    strategies and top-down strategies take post-order and pre-order traversal over
                    trees, respectively. Bottom-up parsers benefit from rich features from readily
                    built partial parses, but lack lookahead guidance in the parsing process;
                    top-down parsers benefit from non-local guidance for local decisions, but rely
                    on a strong encoder over the input to predict a constituent hierarchy before its
                    construction. To mitigate both issues, we propose a novel parsing system based
                    on in-order traversal over syntactic trees, designing a set of transition
                    actions to find a compromise between bottom-up constituent information and
                    top-down lookahead information. Based on stack-LSTM, our psycholinguistically
                    motivated constituent parsing system achieves 91.8 F1 on the WSJ
                    benchmark. Furthermore, the system achieves 93.6 F1 with supervised
                    reranking and 94.2 F1 with semi-supervised reranking, which are the
                    best results on the WSJ benchmark.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00070},
    url = {https://doi.org/10.1162/tacl\_a\_00070},
}

@inproceedings{fernandez-gonzalez-gomez-rodriguez-2020-enriched,
    title = "Enriched In-Order Linearization for Faster Sequence-to-Sequence Constituent Parsing",
    author = "Fern{\'a}ndez-Gonz{\'a}lez, Daniel  and
      G{\'o}mez-Rodr{\'\i}guez, Carlos",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.376",
    doi = "10.18653/v1/2020.acl-main.376",
    pages = "4092--4099",
    abstract = "Sequence-to-sequence constituent parsing requires a linearization to represent trees as sequences. Top-down tree linearizations, which can be based on brackets or shift-reduce actions, have achieved the best accuracy to date. In this paper, we show that these results can be improved by using an in-order linearization instead. Based on this observation, we implement an enriched in-order shift-reduce linearization inspired by Vinyals et al. (2015){'}s approach, achieving the best accuracy to date on the English PTB dataset among fully-supervised single-model sequence-to-sequence constituent parsers. Finally, we apply deterministic attention mechanisms to match the speed of state-of-the-art transition-based parsers, thus showing that sequence-to-sequence models can match them, not only in accuracy, but also in speed.",
}

@inproceedings{li-etal-2018-seq2seq,
    title = "Seq2seq Dependency Parsing",
    author = "Li, Zuchao  and
      Cai, Jiaxun  and
      He, Shexia  and
      Zhao, Hai",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/C18-1271",
    pages = "3203--3214",
    abstract = "This paper presents a sequence to sequence (seq2seq) dependency parser by directly predicting the relative position of head for each given word, which therefore results in a truly end-to-end seq2seq dependency parser for the first time. Enjoying the advantage of seq2seq modeling, we enrich a series of embedding enhancement, including firstly introduced subword and node2vec augmentation. Meanwhile, we propose a beam search decoder with tree constraint and subroot decomposition over the sequence to furthermore enhance our seq2seq parser. Our parser is evaluated on benchmark treebanks, being on par with the state-of-the-art parsers by achieving 94.11{\%} UAS on PTB and 88.78{\%} UAS on CTB, respectively.",
}

@inproceedings{wiseman-rush-2016-sequence,
    title = "Sequence-to-Sequence Learning as Beam-Search Optimization",
    author = "Wiseman, Sam  and
      Rush, Alexander M.",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1137",
    doi = "10.18653/v1/D16-1137",
    pages = "1296--1306",
}

@inproceedings{wang-etal-2021-codet5,
    title = "{C}ode{T}5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation",
    author = "Wang, Yue  and
      Wang, Weishi  and
      Joty, Shafiq  and
      Hoi, Steven C.H.",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.685",
    doi = "10.18653/v1/2021.emnlp-main.685",
    pages = "8696--8708",
    abstract = "Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5.",
}

@inproceedings{van-noord-bos-2017-dealing,
    title = "Dealing with Co-reference in Neural Semantic Parsing",
    author = "van Noord, Rik  and
      Bos, Johan",
    booktitle = "Proceedings of the 2nd Workshop on Semantic Deep Learning ({S}em{D}eep-2)",
    month = sep,
    year = "2017",
    address = "Montpellier, France",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-7306",
    pages = "41--49",
}

@article{van2017neural,
  title={Neural Semantic Parsing by Character-based Translation: Experiments with Abstract Meaning Representations},
  author={van Noord, Rik and Bos, Johan},
  journal={Computational Linguistics in the Netherlands Journal},
  volume={7},
  pages={93--108},
  year={2017}
}

@InProceedings{ruiqi20,
  author =  {Ruiqi Zhong and Tao Yu and Dan Klein},
  title =   {Semantic Evaluation for Text-to-SQL with Distilled Test Suite},
  year =    {2020},
  booktitle =   {The 2020 Conference on Empirical Methods in Natural Language Processing},
  publisher = {Association for Computational Linguistics},
}

@inproceedings{cai-knight-2013-smatch,
    title = "{S}match: an Evaluation Metric for Semantic Feature Structures",
    author = "Cai, Shu and Knight, Kevin",
    booktitle = "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P13-2131",
    pages = "748--752",
}

@article{sekine1997evalb,
  title={Evalb bracket scoring program},
  author={Sekine, Satoshi and Collins, Michael},
  journal={URL: http://www. cs. nyu. edu/cs/projects/proteus/evalb},
  year={1997}
}

@inproceedings{reimers-2019-sentence-bert,
  title = "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
  author = "Reimers, Nils and Gurevych, Iryna",
  booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing",
  month = "11",
  year = "2019",
  publisher = "Association for Computational Linguistics",
  url = "https://arxiv.org/abs/1908.10084",
}

@inproceedings{ni-etal-2022-sentence-t5,
    title = "Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models",
    author = "Ni, Jianmo  and
      Hernandez Abrego, Gustavo  and
      Constant, Noah  and
      Ma, Ji  and
      Hall, Keith  and
      Cer, Daniel  and
      Yang, Yinfei",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2022",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-acl.146",
    doi = "10.18653/v1/2022.findings-acl.146",
    pages = "1864--1874",
    abstract = "We provide the first exploration of sentence embeddings from text-to-text transformers (T5) including the effects of scaling up sentence encoders to 11B parameters. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods to construct Sentence-T5 (ST5) models: two utilize only the T5 encoder and one using the full T5 encoder-decoder. We establish a new sentence representation transfer benchmark, SentGLUE, which extends the SentEval toolkit to nine tasks from the GLUE benchmark. Our encoder-only models outperform the previous best models on both SentEval and SentGLUE transfer tasks, including semantic textual similarity (STS). Scaling up ST5 from millions to billions of parameters shown to consistently improve performance. Finally, our encoder-decoder method achieves a new state-of-the-art on STS when using sentence embeddings.",
}

@inproceedings{koo2015grammar,
  title={Grammar as a foreign language},
  author={Koo, Petrov and Petrov, S and Sutskever, I and Hinton, GE and Vinyals, O and Kaiser, L},
  booktitle={Advances in Neural Information Processing Systems},
  year={2015}
}

@article{bigbench,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Aarohi Srivastava and Abhinav Rastogi and others},
  journal={ArXiv},
  year={2022},
  volume={abs/2206.04615}
}

@article{llama2,
  author       = {Hugo Touvron and
                  Louis Martin and
                  Kevin Stone and
                  Peter Albert and
                  Amjad Almahairi and
                  Yasmine Babaei and
                  Nikolay Bashlykov and
                  Soumya Batra and
                  Prajjwal Bhargava and
                  Shruti Bhosale and
                  Dan Bikel and
                  Lukas Blecher and
                  Cristian Canton{-}Ferrer and
                  Moya Chen and
                  Guillem Cucurull and
                  David Esiobu and
                  Jude Fernandes and
                  Jeremy Fu and
                  Wenyin Fu and
                  Brian Fuller and
                  Cynthia Gao and
                  Vedanuj Goswami and
                  Naman Goyal and
                  Anthony Hartshorn and
                  Saghar Hosseini and
                  Rui Hou and
                  Hakan Inan and
                  Marcin Kardas and
                  Viktor Kerkez and
                  Madian Khabsa and
                  Isabel Kloumann and
                  Artem Korenev and
                  Punit Singh Koura and
                  Marie{-}Anne Lachaux and
                  Thibaut Lavril and
                  Jenya Lee and
                  Diana Liskovich and
                  Yinghai Lu and
                  Yuning Mao and
                  Xavier Martinet and
                  Todor Mihaylov and
                  Pushkar Mishra and
                  Igor Molybog and
                  Yixin Nie and
                  Andrew Poulton and
                  Jeremy Reizenstein and
                  Rashi Rungta and
                  Kalyan Saladi and
                  Alan Schelten and
                  Ruan Silva and
                  Eric Michael Smith and
                  Ranjan Subramanian and
                  Xiaoqing Ellen Tan and
                  Binh Tang and
                  Ross Taylor and
                  Adina Williams and
                  Jian Xiang Kuan and
                  Puxin Xu and
                  Zheng Yan and
                  Iliyan Zarov and
                  Yuchen Zhang and
                  Angela Fan and
                  Melanie Kambadur and
                  Sharan Narang and
                  Aur{\'{e}}lien Rodriguez and
                  Robert Stojnic and
                  Sergey Edunov and
                  Thomas Scialom},
  title        = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  journal      = {CoRR},
  volume       = {abs/2307.09288},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2307.09288},
  doi          = {10.48550/arXiv.2307.09288},
  eprinttype    = {arXiv},
  eprint       = {2307.09288},
}

@inproceedings{vllm,
  author       = {Woosuk Kwon and
                  Zhuohan Li and
                  Siyuan Zhuang and
                  Ying Sheng and
                  Lianmin Zheng and
                  Cody Hao Yu and
                  Joseph Gonzalez and
                  Hao Zhang and
                  Ion Stoica},
  editor       = {Jason Flinn and
                  Margo I. Seltzer and
                  Peter Druschel and
                  Antoine Kaufmann and
                  Jonathan Mace},
  title        = {Efficient Memory Management for Large Language Model Serving with
                  PagedAttention},
  booktitle    = {Proceedings of the 29th Symposium on Operating Systems Principles,
                  {SOSP} 2023, Koblenz, Germany, October 23-26, 2023},
  pages        = {611--626},
  publisher    = {{ACM}},
  year         = {2023},
  url          = {https://doi.org/10.1145/3600006.3613165},
  doi          = {10.1145/3600006.3613165},
  timestamp    = {Mon, 16 Oct 2023 14:05:50 +0200},
  biburl       = {https://dblp.org/rec/conf/sosp/KwonLZ0ZY0ZS23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}