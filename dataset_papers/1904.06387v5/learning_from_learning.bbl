\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the 21st international conference on Machine
  learning}, 2004.

\bibitem[Akrour et~al.(2011)Akrour, Schoenauer, and
  Sebag]{akrour2011preference}
Akrour, R., Schoenauer, M., and Sebag, M.
\newblock Preference-based policy learning.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  12--27. Springer, 2011.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and
  Man{\'e}, D.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Argall et~al.(2009)Argall, Chernova, Veloso, and Browning]{Argall2009}
Argall, B.~D., Chernova, S., Veloso, M., and Browning, B.
\newblock A survey of robot learning from demonstration.
\newblock \emph{Robotics and autonomous systems}, 57\penalty0 (5):\penalty0
  469--483, 2009.

\bibitem[Arora \& Doshi(2018)Arora and Doshi]{arora2018survey}
Arora, S. and Doshi, P.
\newblock A survey of inverse reinforcement learning: Challenges, methods and
  progress.
\newblock \emph{arXiv preprint arXiv:1806.06877}, 2018.

\bibitem[Aytar et~al.(2018)Aytar, Pfaff, Budden, Paine, Wang, and
  de~Freitas]{imitationyoutube}
Aytar, Y., Pfaff, T., Budden, D., Paine, T.~L., Wang, Z., and de~Freitas, N.
\newblock Playing hard exploration games by watching youtube.
\newblock \emph{arXiv preprint arXiv:1805.11592}, 2018.

\bibitem[Boularias et~al.(2011)Boularias, Kober, and
  Peters]{boularias2011relative}
Boularias, A., Kober, J., and Peters, J.
\newblock Relative entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  182--189, 2011.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Bradley, R.~A. and Terry, M.~E.
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Burchfiel et~al.(2016)Burchfiel, Tomasi, and Parr]{dmirl}
Burchfiel, B., Tomasi, C., and Parr, R.
\newblock Distance minimization for reward learning from scored trajectories.
\newblock In \emph{AAAI}, pp.\  3330--3336, 2016.

\bibitem[Choi et~al.(2019)Choi, Lee, and Oh]{choi2019robust}
Choi, S., Lee, K., and Oh, S.
\newblock Robust learning from demonstrations with mixed qualities using
  leveraged gaussian processes.
\newblock \emph{IEEE Transactions on Robotics}, 2019.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4299--4307, 2017.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{baselines}
Dhariwal, P., Hesse, C., Klimov, O., Nichol, A., Plappert, M., Radford, A.,
  Schulman, J., Sidor, S., Wu, Y., and Zhokhov, P.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[El~Asri et~al.(2016)El~Asri, Piot, Geist, Laroche, and
  Pietquin]{scorebasedirl}
El~Asri, L., Piot, B., Geist, M., Laroche, R., and Pietquin, O.
\newblock Score-based inverse reinforcement learning.
\newblock In \emph{Proceedings of the 2016 International Conference on
  Autonomous Agents \& Multiagent Systems}, pp.\  457--465. International
  Foundation for Autonomous Agents and Multiagent Systems, 2016.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International Conference on Machine Learning}, 2016.

\bibitem[Fu et~al.(2017)Fu, Luo, and Levine]{airl}
Fu, J., Luo, K., and Levine, S.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1710.11248}, 2017.

\bibitem[Gao et~al.(2012)Gao, Peters, Tsourdos, Zhifei, and
  Meng~Joo]{gao2012survey}
Gao, Y., Peters, J., Tsourdos, A., Zhifei, S., and Meng~Joo, E.
\newblock A survey of inverse reinforcement learning techniques.
\newblock \emph{International Journal of Intelligent Computing and
  Cybernetics}, 5\penalty0 (3):\penalty0 293--311, 2012.

\bibitem[Gao et~al.(2018)Gao, Lin, Yu, Levine, Darrell,
  et~al.]{gao2018reinforcement}
Gao, Y., Lin, J., Yu, F., Levine, S., Darrell, T., et~al.
\newblock Reinforcement learning from imperfect demonstrations.
\newblock \emph{arXiv preprint arXiv:1802.05313}, 2018.

\bibitem[Goo \& Niekum(2019)Goo and Niekum]{goo2019one}
Goo, W. and Niekum, S.
\newblock One-shot learning of multi-step tasks from observation via activity
  localization in auxiliary video.
\newblock In \emph{2019 IEEE International Conference on Robotics and
  Automation (ICRA)}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Greydanus et~al.(2018)Greydanus, Koul, Dodge, and
  Fern]{greydanus2018visualizing}
Greydanus, S., Koul, A., Dodge, J., and Fern, A.
\newblock Visualizing and understanding atari agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1787--1796, 2018.

\bibitem[Grollman \& Billard(2011)Grollman and Billard]{grollman2011donut}
Grollman, D.~H. and Billard, A.
\newblock Donut as i do: Learning from failed demonstrations.
\newblock In \emph{Robotics and Automation (ICRA), 2011 IEEE International
  Conference on}, pp.\  3804--3809. IEEE, 2011.

\bibitem[Henderson et~al.(2018)Henderson, Chang, Bacon, Meger, Pineau, and
  Precup]{henderson2018optiongan}
Henderson, P., Chang, W.-D., Bacon, P.-L., Meger, D., Pineau, J., and Precup,
  D.
\newblock Optiongan: Learning joint reward-policy options using generative
  adversarial inverse reinforcement learning.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Hester et~al.(2017)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Dulac-Arnold, et~al.]{hester2017deep}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B.,
  Horgan, D., Quan, J., Sendonaris, A., Dulac-Arnold, G., et~al.
\newblock Deep q-learning from demonstrations.
\newblock \emph{arXiv preprint arXiv:1704.03732}, 2017.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4565--4573, 2016.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and
  Amodei]{ibarz2018reward}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock Reward learning from human preferences and demonstrations in atari.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8022--8034, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kober \& Peters(2009)Kober and Peters]{kober2009policy}
Kober, J. and Peters, J.~R.
\newblock Policy search for motor primitives in robotics.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  849--856, 2009.

\bibitem[Kurin et~al.(2017)Kurin, Nowozin, Hofmann, Beyer, and
  Leibe]{kurin2017atari}
Kurin, V., Nowozin, S., Hofmann, K., Beyer, L., and Leibe, B.
\newblock The atari grand challenge dataset.
\newblock \emph{arXiv preprint arXiv:1705.10998}, 2017.

\bibitem[Liu et~al.(2018)Liu, Gupta, Abbeel, and Levine]{liu2018imitation}
Liu, Y., Gupta, A., Abbeel, P., and Levine, S.
\newblock Imitation from observation: Learning to imitate behaviors from raw
  video via context translation.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  1118--1125. IEEE, 2018.

\bibitem[Luce(2012)]{luce2012individual}
Luce, R.~D.
\newblock \emph{Individual choice behavior: A theoretical analysis}.
\newblock Courier Corporation, 2012.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{ICML}, volume~99, pp.\  278--287, 1999.

\bibitem[Osa et~al.(2018)Osa, Pajarinen, Neumann, Bagnell, Abbeel, Peters,
  et~al.]{osa2018algorithmic}
Osa, T., Pajarinen, J., Neumann, G., Bagnell, J.~A., Abbeel, P., Peters, J.,
  et~al.
\newblock An algorithmic perspective on imitation learning.
\newblock \emph{Foundations and Trends{\textregistered} in Robotics},
  7\penalty0 (1-2):\penalty0 1--179, 2018.

\bibitem[Pomerleau(1991)]{pomerleau1991efficient}
Pomerleau, D.~A.
\newblock Efficient training of artificial neural networks for autonomous
  navigation.
\newblock \emph{Neural Computation}, 3\penalty0 (1):\penalty0 88--97, 1991.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Qureshi \& Yip(2018)Qureshi and Yip]{qureshi2018adversarial}
Qureshi, A.~H. and Yip, M.~C.
\newblock Adversarial imitation via variational inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1809.06404}, 2018.

\bibitem[Ramachandran \& Amir(2007)Ramachandran and
  Amir]{ramachandran2007bayesian}
Ramachandran, D. and Amir, E.
\newblock Bayesian inverse reinforcement learning.
\newblock In \emph{Proceedings of the 20th International Joint Conference on
  Artifical intelligence}, pp.\  2586--2591, 2007.

\bibitem[Ross et~al.(2011)Ross, Gordon, and Bagnell]{ross2011reduction}
Ross, S., Gordon, G., and Bagnell, D.
\newblock A reduction of imitation learning and structured prediction to
  no-regret online learning.
\newblock In \emph{Proceedings of the fourteenth international conference on
  artificial intelligence and statistics}, pp.\  627--635, 2011.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sermanet et~al.(2018)Sermanet, Lynch, Chebotar, Hsu, Jang, Schaal,
  Levine, and Brain]{sermanet2018time}
Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal, S., Levine,
  S., and Brain, G.
\newblock Time-contrastive networks: Self-supervised learning from video.
\newblock In \emph{2018 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  1134--1141. IEEE, 2018.

\bibitem[Shiarlis et~al.(2016)Shiarlis, Messias, and
  Whiteson]{shiarlis2016inverse}
Shiarlis, K., Messias, J., and Whiteson, S.
\newblock Inverse reinforcement learning from failure.
\newblock In \emph{Proceedings of the 2016 International Conference on
  Autonomous Agents \& Multiagent Systems}, pp.\  1060--1068. International
  Foundation for Autonomous Agents and Multiagent Systems, 2016.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Meguro, and Minami]{irldialog}
Sugiyama, H., Meguro, T., and Minami, Y.
\newblock Preference-learning based inverse reinforcement learning for dialog
  control.
\newblock In \emph{INTERSPEECH}, pp.\  222--225, 2012.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998introduction}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Introduction to reinforcement learning}, volume 135.
\newblock MIT press Cambridge, 1998.

\bibitem[Syed \& Schapire(2008)Syed and Schapire]{syed2008game}
Syed, U. and Schapire, R.~E.
\newblock A game-theoretic approach to apprenticeship learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1449--1456, 2008.

\bibitem[Taylor et~al.(2011)Taylor, Suay, and Chernova]{taylor2011integrating}
Taylor, M.~E., Suay, H.~B., and Chernova, S.
\newblock Integrating reinforcement learning with human demonstrations of
  varying ability.
\newblock In \emph{The 10th International Conference on Autonomous Agents and
  Multiagent Systems-Volume 2}, pp.\  617--624. International Foundation for
  Autonomous Agents and Multiagent Systems, 2011.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Torabi et~al.(2018{\natexlab{a}})Torabi, Warnell, and
  Stone]{torabi2018behavioral}
Torabi, F., Warnell, G., and Stone, P.
\newblock Behavioral cloning from observation.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence (IJCAI)}, July 2018{\natexlab{a}}.

\bibitem[Torabi et~al.(2018{\natexlab{b}})Torabi, Warnell, and
  Stone]{torabi2018generative}
Torabi, F., Warnell, G., and Stone, P.
\newblock Generative adversarial imitation from observation.
\newblock \emph{arXiv preprint arXiv:1807.06158}, 2018{\natexlab{b}}.

\bibitem[Tucker et~al.(2018)Tucker, Gleave, and Russell]{irlvideogames}
Tucker, A., Gleave, A., and Russell, S.
\newblock Inverse reinforcement learning for video games.
\newblock In \emph{Proceedings of the Workshop on Deep Reinforcement Learning
  at NeurIPS}, 2018.

\bibitem[Wirth et~al.(2016)Wirth, F{\"u}rnkranz, and Neumann]{pbirl}
Wirth, C., F{\"u}rnkranz, J., and Neumann, G.
\newblock Model-free preference-based reinforcement learning.
\newblock In \emph{Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, pp.\  2222--2228. AAAI Press, 2016.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and
  F{{{\"u}}}rnkranz]{preferencesurvey}
Wirth, C., Akrour, R., Neumann, G., and F{{{\"u}}}rnkranz, J.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{Journal of Machine Learning Research}, 18\penalty0
  (136):\penalty0 1--46, 2017.
\newblock URL \url{http://jmlr.org/papers/v18/16-634.html}.

\bibitem[Wulfmeier et~al.(2015)Wulfmeier, Ondruska, and
  Posner]{wulfmeier2015maximum}
Wulfmeier, M., Ondruska, P., and Posner, I.
\newblock Maximum entropy deep inverse reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1507.04888}, 2015.

\bibitem[Yu et~al.(2018)Yu, Finn, Xie, Dasari, Zhang, Abbeel, and
  Levine]{yu2018one}
Yu, T., Finn, C., Xie, A., Dasari, S., Zhang, T., Abbeel, P., and Levine, S.
\newblock One-shot imitation from observing humans via domain-adaptive
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1802.01557}, 2018.

\bibitem[Zheng et~al.(2014)Zheng, Liu, and Ni]{zheng2014robust}
Zheng, J., Liu, S., and Ni, L.~M.
\newblock Robust bayesian inverse reinforcement learning with sparse behavior
  noise.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  2198--2205, 2014.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the 23rd AAAI Conference on Artificial
  Intelligence}, pp.\  1433--1438, 2008.

\end{thebibliography}
