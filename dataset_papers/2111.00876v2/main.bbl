\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel and Ng.(2004)]{abbeel2004apprenticeship}
Pieter Abbeel and Andrew~Y. Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  learning}, 2004.

\bibitem[Ackley and Littman(1992)]{ackley1992interactions}
David Ackley and Michael~L. Littman.
\newblock Interactions between learning and evolution.
\newblock \emph{Artificial Life II}, 1992.

\bibitem[Akshay et~al.(2013)Akshay, Bertrand, Haddad, and
  Helouet]{akshay2013steady}
Sundararaman Akshay, Nathalie Bertrand, Serge Haddad, and Loic Helouet.
\newblock The steady-state control problem for {M}arkov decision processes.
\newblock In \emph{Proceedings of the International Conference on Quantitative
  Evaluation of Systems}, 2013.

\bibitem[Amin et~al.(2017)Amin, Jiang, and Singh]{amin2017repeated}
Kareem Amin, Nan Jiang, and Satinder Singh.
\newblock Repeated inverse reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G. Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[Christian(2021)]{christian2021alignment}
Brian Christian.
\newblock \emph{The Alignment Problem: Machine Learning and Human Values},
  pages 130--131.
\newblock Atlantic Books, 2021.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Paul~F. Christiano, Jan Leike, Tom~B. Brown, Miljan Martic, Shane Legg, and
  Dario Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Debreu(1954)]{debreu1954representation}
Gerard Debreu.
\newblock Representation of a preference ordering by a numerical function.
\newblock \emph{Decision Processes}, 3:\penalty0 159--165, 1954.

\bibitem[Dewey(2014)]{dewey2014reinforcement}
Daniel Dewey.
\newblock Reinforcement learning and the reward engineering principle.
\newblock In \emph{Proceedings of the AAAI Spring Symposium Series}, 2014.

\bibitem[Everitt et~al.(2017)Everitt, Krakovna, Orseau, Hutter, and
  Legg]{everitt2017reinforcement}
Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, and Shane Legg.
\newblock Reinforcement learning with a corrupted reward channel.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence}, 2017.

\bibitem[Fedus et~al.(2019)Fedus, Gelada, Bengio, Bellemare, and
  Larochelle]{fedus2019hyperbolic}
William Fedus, Carles Gelada, Yoshua Bengio, Marc~G. Bellemare, and Hugo
  Larochelle.
\newblock Hyperbolic discounting and learning over multiple horizons.
\newblock \emph{arXiv preprint arXiv:1902.06865}, 2019.

\bibitem[Friston(2010)]{friston2010free}
Karl~J. Friston.
\newblock The free-energy principle: a unified brain theory?
\newblock \emph{Nature reviews neuroscience}, 11\penalty0 (2):\penalty0
  127--138, 2010.

\bibitem[Friston et~al.(2009)Friston, Daunizeau, and
  Kiebel]{friston2009reinforcement}
Karl~J. Friston, Jean Daunizeau, and Stefan~J. Kiebel.
\newblock Reinforcement learning or active inference?
\newblock \emph{PloS One}, 4\penalty0 (7):\penalty0 e6421, 2009.

\bibitem[Hadfield-Menell et~al.(2016)Hadfield-Menell, Dragan, Abbeel, and
  Russell]{hadfield2016cooperative}
Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell.
\newblock Cooperative inverse reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Hadfield-Menell et~al.(2017{\natexlab{a}})Hadfield-Menell, Dragan,
  Abbeel, and Russell]{hadfield2016off}
Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, and Stuart Russell.
\newblock The off-switch game.
\newblock In \emph{Proceedings of the International Joint Conference on
  Artificial Intelligence}, 2017{\natexlab{a}}.

\bibitem[Hadfield-Menell et~al.(2017{\natexlab{b}})Hadfield-Menell, Milli,
  Abbeel, Russell, and Dragan]{hadfield2017inverse}
Dylan Hadfield-Menell, Smitha Milli, Pieter Abbeel, Stuart Russell, and Anca
  Dragan.
\newblock Inverse reward design.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2017{\natexlab{b}}.

\bibitem[Hafner et~al.(2020)Hafner, Ortega, Ba, Parr, Friston, and
  Heess]{hafner2020action}
Danijar Hafner, Pedro~A. Ortega, Jimmy Ba, Thomas Parr, Karl~J. Friston, and
  Nicolas Heess.
\newblock Action and perception as divergence minimization.
\newblock \emph{arXiv preprint arXiv:2009.01791}, 2020.

\bibitem[Hammond et~al.(2021)Hammond, Abate, Gutierrez, and
  Wooldridge]{hammond2021multi}
Lewis Hammond, Alessandro Abate, Julian Gutierrez, and Michael Wooldridge.
\newblock Multi-agent reinforcement learning with temporal logic
  specifications.
\newblock In \emph{Proceedings of the International Conference on Autonomous
  Agents and Multiagent Systems}, 2021.

\bibitem[Icarte et~al.(2018)Icarte, Klassen, Valenzano, and
  McIlraith]{icarte2018using}
Rodrigo~Toro Icarte, Toryn Klassen, Richard Valenzano, and Sheila McIlraith.
\newblock Using reward machines for high-level task specification and
  decomposition in reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2018.

\bibitem[Jeon et~al.(2020)Jeon, Milli, and Dragan]{jeon2020reward}
Hong~Jun Jeon, Smitha Milli, and Anca Dragan.
\newblock Reward-rational (implicit) choice: A unifying formalism for reward
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Jothimurugan et~al.(2020)Jothimurugan, Alur, and
  Bastani]{jothimurugan2020composable}
Kishor Jothimurugan, Rajeev Alur, and Osbert Bastani.
\newblock A composable specification language for reinforcement learning tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Karmarkar(1984)]{karmarkar1984new}
Narendra Karmarkar.
\newblock A new polynomial-time algorithm for linear programming.
\newblock In \emph{Proceedings of the Annual ACM Symposium on Theory of
  Computing}, 1984.

\bibitem[Knox and Stone(2009)]{knox2009interactively}
W.~Bradley Knox and Peter Stone.
\newblock Interactively shaping agents via human reinforcement: The {TAMER}
  framework.
\newblock In \emph{Proceedings of the International Conference on Knowledge
  Capture}, 2009.

\bibitem[Koopmans(1960)]{koopmans1960stationary}
Tjalling~C. Koopmans.
\newblock Stationary ordinal utility and impatience.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pages
  287--309, 1960.

\bibitem[Kreps(1988)]{kreps1988notes}
David Kreps.
\newblock \emph{Notes on the Theory of Choice}.
\newblock Westview Press, 1988.

\bibitem[Krishna~Gottipati et~al.(2020)Krishna~Gottipati, Pathak, Nuttall,
  Chunduru, Touati, Ganapathi~Subramanian, Taylor, and
  Chandar]{krishna2020maximum}
Sai Krishna~Gottipati, Yashaswi Pathak, Rohan Nuttall, Raviteja Chunduru, Ahmed
  Touati, Sriram Ganapathi~Subramanian, Matthew~E. Taylor, and Sarath Chandar.
\newblock Maximum reward formulation in reinforcement learning.
\newblock In \emph{NeurIPS Workshop on Deep Reinforcement Learning}, 2020.

\bibitem[Kumar et~al.(2020)Kumar, Uesato, Ngo, Everitt, Krakovna, and
  Legg]{kumar2020realab}
Ramana Kumar, Jonathan Uesato, Richard Ngo, Tom Everitt, Victoria Krakovna, and
  Shane Legg.
\newblock {REALab}: An embedded perspective on tampering.
\newblock \emph{arXiv preprint arXiv:2011.08820}, 2020.

\bibitem[Li et~al.(2017)Li, Vasile, and Belta]{li2017reinforcement}
Xiao Li, Cristian-Ioan Vasile, and Calin Belta.
\newblock Reinforcement learning with temporal logic rewards.
\newblock In \emph{Proceedings of the International Conference on Intelligent
  Robots and Systems}, 2017.

\bibitem[Littman(2017)]{littmanRH}
Michael~L. Littman.
\newblock The reward hypothesis, 2017.
\newblock URL
  \url{https://www.coursera.org/lecture/fundamentals-of-reinforcement-learning/michael-littman-the-reward-hypothesis-q6x0e}.

\bibitem[Littman et~al.(2017)Littman, Topcu, Fu, Isbell, Wen, and
  MacGlashan]{littman2017environment}
Michael~L. Littman, Ufuk Topcu, Jie Fu, Charles Isbell, Min Wen, and James
  MacGlashan.
\newblock Environment-independent task specifications via {GLTL}.
\newblock \emph{arXiv preprint arXiv:1704.04341}, 2017.

\bibitem[MacGlashan et~al.(2015)MacGlashan, Babes-Vroman, desJardins, Littman,
  Muresan, Squire, Tellex, Arumugam, and Yang]{macglashan15}
James MacGlashan, Monica Babes-Vroman, Marie desJardins, Michael~L. Littman,
  Smaranda Muresan, Shawn Squire, Stefanie Tellex, Dilip Arumugam, and Lei
  Yang.
\newblock Grounding {E}nglish commands to reward functions.
\newblock In \emph{Proceedings of Robotics: Science and Systems}, 2015.

\bibitem[MacGlashan et~al.(2016)MacGlashan, Littman, Roberts, Loftin, Peng, and
  Taylor]{macglashan2016convergent}
James MacGlashan, Michael~L. Littman, David~L. Roberts, Robert Loftin, Bei
  Peng, and Matthew~E. Taylor.
\newblock Convergent actor critic by humans.
\newblock In \emph{Proceedings of the International Conference on Intelligent
  Robots and Systems}, 2016.

\bibitem[MacGlashan et~al.(2017)MacGlashan, Ho, Loftin, Peng, Wang, Roberts,
  Taylor, and Littman]{macglashan2017interactive}
James MacGlashan, Mark~K. Ho, Robert Loftin, Bei Peng, Guan Wang, David~L.
  Roberts, Matthew~E. Taylor, and Michael~L. Littman.
\newblock Interactive learning from policy-dependent human feedback.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}. PMLR, 2017.

\bibitem[Mataric(1994)]{mataric1994reward}
Maja~J. Mataric.
\newblock Reward functions for accelerated learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 1994.

\bibitem[Mitten(1974)]{mitten1974preference}
L.~G. Mitten.
\newblock Preference order dynamic programming.
\newblock \emph{Management Science}, 21\penalty0 (1):\penalty0 43--46, 1974.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Andrew~Y. Ng, Daishi Harada, and Stuart Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 1999.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Andrew~Y. Ng, Stuart~J. Russell, et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2000.

\bibitem[Novoseller et~al.(2020)Novoseller, Wei, Sui, Yue, and
  Burdick]{novoseller2020dueling}
Ellen Novoseller, Yibing Wei, Yanan Sui, Yisong Yue, and Joel Burdick.
\newblock Dueling posterior sampling for preference-based reinforcement
  learning.
\newblock In \emph{Proceedings of the Conference on Uncertainty in Artificial
  Intelligence}, 2020.

\bibitem[Ortega et~al.(2018)Ortega, Maini, and the DeepMind
  Safety~Team]{ortega2018}
Pedro~A. Ortega, Vishal Maini, and the DeepMind Safety~Team.
\newblock Building safe artificial intelligence: specification, robustness, and
  assurance, 2018.
\newblock URL
  \url{https://medium.com/@deepmindsafetyresearch/building-safe-artificial-intelligence-52f5f75058f1}.

\bibitem[Pitis(2019)]{pitis2019rethinking}
Silviu Pitis.
\newblock Rethinking the discount factor in reinforcement learning: A decision
  theoretic approach.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2019.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L. Puterman.
\newblock \emph{Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Russell and Norvig(1994)]{russell94}
Stuart~J. Russell and Peter Norvig.
\newblock \emph{Artificial Intelligence: {A} Modern Approach}.
\newblock Prentice-Hall, Englewood Cliffs, NJ, 1994.
\newblock ISBN 0-13-103805-2.

\bibitem[Shah et~al.(2021)Shah, Freire, Alex, Freedman, Krasheninnikov, Chan,
  Dennis, Abbeel, Dragan, and Russell]{shah2021benefits}
Rohin Shah, Pedro Freire, Neel Alex, Rachel Freedman, Dmitrii Krasheninnikov,
  Lawrence Chan, Michael~D. Dennis, Pieter Abbeel, Anca Dragan, and Stuart
  Russell.
\newblock Benefits of assistance over reward learning, 2021.
\newblock URL \url{https://openreview.net/forum?id=DFIoGDZejIB}.

\bibitem[Silver et~al.(2021)Silver, Singh, Precup, and
  Sutton]{silver2021reward}
David Silver, Satinder Singh, Doina Precup, and Richard~S. Sutton.
\newblock Reward is enough.
\newblock \emph{Artificial Intelligence}, page 103535, 2021.

\bibitem[Singh et~al.(2005)Singh, Barto, and Chentanez]{singh2005intrinsically}
Satinder Singh, Andrew~G. Barto, and Nuttapong Chentanez.
\newblock Intrinsically motivated reinforcement learning.
\newblock Technical report, University of Massachusetts at Amherst Department
  of Computer Science, 2005.

\bibitem[Singh et~al.(2009)Singh, Lewis, and Barto]{singh2009rewards}
Satinder Singh, Richard~L Lewis, and Andrew~G Barto.
\newblock Where do rewards come from?
\newblock In \emph{Proceedings of the Annual Conference of the Cognitive
  Science Society}, 2009.

\bibitem[Singh et~al.(2010)Singh, Lewis, Sorg, Barto, and
  Helou]{singh2010separating}
Satinder Singh, Richard~L. Lewis, Jonathan Sorg, Andrew~G. Barto, and Akram
  Helou.
\newblock On separating agent designer goals from agent goals: Breaking the
  preferences--parameters confound, 2010.

\bibitem[Sobel(1975)]{sobel1975ordinal}
Matthew~J. Sobel.
\newblock Ordinal dynamic programming.
\newblock \emph{Management science}, 21\penalty0 (9):\penalty0 967--975, 1975.

\bibitem[Sobel(2013)]{sobel2013discounting}
Matthew~J. Sobel.
\newblock Discounting axioms imply risk neutrality.
\newblock \emph{Annals of Operations Research}, 208\penalty0 (1):\penalty0
  417--432, 2013.

\bibitem[Sorg(2011)]{sorg2011optimal}
Jonathan Sorg.
\newblock \emph{The Optimal Reward Problem: Designing Effective Reward for
  Bounded Agents}.
\newblock PhD thesis, University of Michigan, 2011.

\bibitem[Sorg et~al.(2010)Sorg, Lewis, and Singh]{sorg2010reward}
Jonathan Sorg, Richard~L. Lewis, and Satinder Singh.
\newblock Reward design via online gradient ascent.
\newblock \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Sunehag and Hutter(2011)]{sunehag2011axioms}
Peter Sunehag and Marcus Hutter.
\newblock Axioms for rational reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Algorithmic
  Learning Theory}, 2011.

\bibitem[Sutton(2004)]{suttonwebRLhypothesis}
Richard~S. Sutton.
\newblock The reward hypothesis, 2004.
\newblock URL
  \url{http://incompleteideas.net/rlai.cs.ualberta.ca/RLAI/rewardhypothesis.html}.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S. Sutton and Andrew~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 2018.

\bibitem[Syed et~al.(2008)Syed, Bowling, and Schapire]{syed2008apprenticeship}
Umar Syed, Michael Bowling, and Robert~E. Schapire.
\newblock Apprenticeship learning using linear programming.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2008.

\bibitem[Szepesvári(2020)]{csabaRLhypothesis}
Csaba Szepesvári.
\newblock Constrained {MDP}s and the reward hypothesis, 2020.
\newblock URL
  \url{http://readingsml.blogspot.com/2020/03/constrained-mdps-and-reward-hypothesis.html}.

\bibitem[Tasse et~al.(2020)Tasse, James, and Rosman]{tasse2020boolean}
Geraud~Nangue Tasse, Steven James, and Benjamin Rosman.
\newblock A {B}oolean task algebra for reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Toro~Icarte et~al.(2018)Toro~Icarte, Klassen, Valenzano, and
  McIlraith]{toro2018teaching}
Rodrigo Toro~Icarte, Toryn~Q. Klassen, Richard Valenzano, and Sheila~A.
  McIlraith.
\newblock Teaching multiple tasks to an {RL} agent using {LTL}.
\newblock In \emph{Proceedings of the International Conference on Autonomous
  Agents and Multiagent Systems}, 2018.

\bibitem[von Neumann and Morgenstern(1953)]{vonneumann1953theory}
John von Neumann and Oskar Morgenstern.
\newblock \emph{Theory of Games and Economic Behavior}.
\newblock Princeton University Press, 1953.

\bibitem[Weng(2011)]{weng2011markov}
Paul Weng.
\newblock Markov decision processes with ordinal rewards: Reference point-based
  preferences.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, 2011.

\bibitem[White(2017)]{white2017unifying}
Martha White.
\newblock Unifying task specification in reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2017.

\bibitem[Williams et~al.(2018)Williams, Gopalan, Rhee, and
  Tellex]{williams2018learning}
Edward~C. Williams, Nakul Gopalan, Mine Rhee, and Stefanie Tellex.
\newblock Learning to parse natural language to grounded reward functions with
  weak supervision.
\newblock In \emph{Proceedings of the International Conference on Robotics and
  Automation}, 2018.

\bibitem[Wilson et~al.(2012)Wilson, Fern, and Tadepalli]{wilson2012bayesian}
Aaron Wilson, Alan Fern, and Prasad Tadepalli.
\newblock A {B}ayesian approach for policy learning from trajectory preference
  queries.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2012.

\bibitem[Wirth et~al.(2017)Wirth, Akrour, Neumann, and
  F{\"u}rnkranz]{wirth2017survey}
Christian Wirth, Riad Akrour, Gerhard Neumann, and Johannes F{\"u}rnkranz.
\newblock A survey of preference-based reinforcement learning methods.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 4945--4990, 2017.

\bibitem[Xu et~al.(2020)Xu, Wang, Yang, Singh, and Dubrawski]{xu2020preference}
Yichong Xu, Ruosong Wang, Lin Yang, Aarti Singh, and Artur Dubrawski.
\newblock Preference-based reinforcement learning with finite-time guarantees.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zheng et~al.(2020)Zheng, Oh, Hessel, Xu, Kroiss, van Hasselt, Silver,
  and Singh]{zheng2020can}
Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado van
  Hasselt, David Silver, and Satinder Singh.
\newblock What can learned intrinsic rewards capture?
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\end{thebibliography}
