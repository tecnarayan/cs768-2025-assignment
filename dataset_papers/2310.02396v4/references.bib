
@inproceedings{lubana_how_2022,
	title = {How do quadratic regularizers prevent catastrophic forgetting: {The} role of interpolation},
	booktitle = {Conference on {Lifelong} {Learning} {Agents}},
	publisher = {PMLR},
	author = {Lubana, Ekdeep Singh and Trivedi, Puja and Koutra, Danai and Dick, Robert},
	year = {2022},
	pages = {819--837},
}

@inproceedings{ji_gradient_2020,
	title = {Gradient descent follows the regularization path for general losses},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Ji, Ziwei and Dudík, Miroslav and Schapire, Robert E and Telgarsky, Matus},
	year = {2020},
	pages = {2109--2136},
}

@article{parhi_near-minimax_2023,
	title = {Near-{Minimax} {Optimal} {Estimation} {With} {Shallow} {ReLU} {Neural} {Networks}},
	volume = {69},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/abstract/document/9899453},
	doi = {10.1109/TIT.2022.3208653},
	abstract = {We study the problem of estimating an unknown function from noisy data using shallow ReLU neural networks. The estimators we study minimize the sum of squared data-fitting errors plus a regularization term proportional to the squared Euclidean norm of the network weights. This minimization corresponds to the common approach of training a neural network with weight decay. We quantify the performance (mean-squared error) of these neural network estimators when the data-generating function belongs to the second-order Radon-domain bounded variation space. This space of functions was recently proposed as the natural function space associated with shallow ReLU neural networks. We derive a minimax lower bound for the estimation problem for this function space and show that the neural network estimators are minimax optimal up to logarithmic factors. This minimax rate is immune to the curse of dimensionality. We quantify an explicit gap between neural networks and linear methods (which include kernel methods) by deriving a linear minimax lower bound for the estimation problem, showing that linear methods necessarily suffer the curse of dimensionality in this function space. As a result, this paper sheds light on the phenomenon that neural networks seem to break the curse of dimensionality.},
	number = {2},
	urldate = {2024-10-30},
	journal = {IEEE Transactions on Information Theory},
	author = {Parhi, Rahul and Nowak, Robert D.},
	month = feb,
	year = {2023},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Biological neural networks, Estimation, Neural networks, Neurons, Noise measurement, Radon, TV, Training, function approximation, nonparametric function estimation, ridge functions, sparsity},
	pages = {1125--1140},
}

@inproceedings{collins_provable_2024,
	title = {Provable {Multi}-{Task} {Representation} {Learning} by {Two}-{Layer} {ReLU} {Neural} {Networks}},
	url = {https://openreview.net/forum?id=M8UbECx485},
	abstract = {An increasingly popular machine learning paradigm is to pretrain a neural network (NN) on many tasks offline, then adapt it to downstream tasks, often by re-training only the last linear layer of the network. This approach yields strong downstream performance in a variety of contexts, demonstrating that multitask pretraining leads to effective feature learning. Although several recent theoretical studies have shown that shallow NNs learn meaningful features when either (i) they are trained on a *single* task or (ii) they are *linear*, very little is known about the closer-to-practice case of *nonlinear* NNs trained on *multiple* tasks. In this work, we present the first results proving that feature learning occurs during training with a nonlinear model on multiple tasks. Our key insight is that multi-task pretraining induces a pseudo-contrastive loss that favors representations that align points that typically have the same label across tasks. Using this observation, we show that when the tasks are binary classification tasks with labels depending on the projection of the data onto an \$r\$-dimensional subspace within the \$d{\textbackslash}gg r\$-dimensional input space, a simple gradient-based multitask learning algorithm on a two-layer ReLU NN recovers this projection, allowing for generalization to downstream tasks with sample and neuron complexity independent of \$d\$. In contrast, we show that with high probability over the draw of a single task, training on this single task cannot guarantee to learn all \$r\$ ground-truth features.},
	language = {en},
	urldate = {2024-10-30},
	author = {Collins, Liam and Hassani, Hamed and Soltanolkotabi, Mahdi and Mokhtari, Aryan and Shakkottai, Sanjay},
	month = jun,
	year = {2024},
}

@inproceedings{yang_better_2022,
	title = {A {Better} {Way} to {Decay}: {Proximal} {Gradient} {Training} {Algorithms} for {Neural} {Nets}},
	shorttitle = {A {Better} {Way} to {Decay}},
	url = {https://openreview.net/forum?id=4y1xh8jClhC},
	abstract = {Weight decay is one of the most widely used forms of regularization in deep learning, and has been shown to improve generalization and robustness. The optimization objective driving weight decay is a sum of losses plus a term proportional to the sum of squared weights. This paper argues that stochastic gradient descent (SGD) may be an inefficient algorithm for this objective. For neural networks with ReLU activations, solutions to the weight decay objective are equivalent to those of a different objective in which the regularization term is instead a sum of products of \${\textbackslash}ell\_2\$ (not squared) norms of the input and output weights associated each ReLU. This alternative (and effectively equivalent) regularization suggests a novel proximal gradient algorithm for network training. Theory and experiments support the new training approach, showing that it can converge much faster to the sparse solutions it shares with standard weight decay training.},
	language = {en},
	urldate = {2024-10-30},
	author = {Yang, Liu and Zhang, Jifan and Shenouda, Joseph and Papailiopoulos, Dimitris and Lee, Kangwook and Nowak, Robert D.},
	month = nov,
	year = {2022},
}

@article{shenouda_variation_2024,
	title = {Variation {Spaces} for {Multi}-{Output} {Neural} {Networks}: {Insights} on {Multi}-{Task} {Learning} and {Network} {Compression}},
	volume = {25},
	issn = {1533-7928},
	shorttitle = {Variation {Spaces} for {Multi}-{Output} {Neural} {Networks}},
	url = {http://jmlr.org/papers/v25/23-0677.html},
	abstract = {This paper introduces a novel theoretical framework for the analysis of vector-valued neural networks through the development of vector-valued variation spaces, a new class of reproducing kernel Banach spaces. These spaces emerge from studying the regularization effect of weight decay in training networks with activation functions like the rectified linear unit (ReLU). This framework offers a deeper understanding of multi-output networks and their function-space characteristics. A key contribution of this work is the development of a representer theorem for the vector-valued variation spaces. This representer theorem establishes that shallow vector-valued neural networks are the solutions to data-fitting problems over these infinite-dimensional spaces, where the network widths are bounded by the square of the number of training data. This observation reveals that the norm associated with these vector-valued variation spaces encourages the learning of features that are useful for multiple tasks, shedding new light on multi-task learning with neural networks. Finally, this paper develops a connection between weight-decay regularization and the multi-task lasso problem. This connection leads to novel bounds for layer widths in deep networks that depend on the intrinsic dimensions of the training data representations. This insight not only deepens the understanding of the deep network architectural requirements, but also yields a simple convex optimization method for deep neural network compression. The performance of this compression procedure is evaluated on various architectures.},
	number = {231},
	urldate = {2024-10-30},
	journal = {Journal of Machine Learning Research},
	author = {Shenouda, Joseph and Parhi, Rahul and Lee, Kangwook and Nowak, Robert D.},
	year = {2024},
	pages = {1--40},
}

@inproceedings{lee_maslows_2022,
	title = {Maslow’s {Hammer} in {Catastrophic} {Forgetting}: {Node} {Re}-{Use} vs. {Node} {Activation}},
	shorttitle = {Maslow’s {Hammer} in {Catastrophic} {Forgetting}},
	url = {https://proceedings.mlr.press/v162/lee22g.html},
	abstract = {Continual learning—learning new tasks in sequence while maintaining performance on old tasks—remains particularly challenging for artificial neural networks. Surprisingly, the amount of forgetting does not increase with the dissimilarity between the learned tasks, but appears to be worst in an intermediate similarity regime. In this paper we theoretically analyse both a synthetic teacher-student framework and a real data setup to provide an explanation of this phenomenon that we name Maslow’s Hammer hypothesis. Our analysis reveals the presence of a trade-off between node activation and node re-use that results in worst forgetting in the intermediate regime. Using this understanding we reinterpret popular algorithmic interventions for catastrophic interference in terms of this trade-off, and identify the regimes in which they are most effective.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Sebastian and Mannelli, Stefano Sarao and Clopath, Claudia and Goldt, Sebastian and Saxe, Andrew},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {12455--12477},
}

@inproceedings{lee_continual_2021,
	title = {Continual {Learning} in the {Teacher}-{Student} {Setup}: {Impact} of {Task} {Similarity}},
	shorttitle = {Continual {Learning} in the {Teacher}-{Student} {Setup}},
	url = {https://proceedings.mlr.press/v139/lee21e.html},
	abstract = {Continual learning\{—\}the ability to learn many tasks in sequence\{—\}is critical for artificial learning systems. Yet standard training methods for deep networks often suffer from catastrophic forgetting, where learning new tasks erases knowledge of the earlier tasks. While catastrophic forgetting labels the problem, the theoretical reasons for interference between tasks remain unclear. Here, we attempt to narrow this gap between theory and practice by studying continual learning in the teacher-student setup. We extend previous analytical work on two-layer networks in the teacher-student setup to multiple teachers. Using each teacher to represent a different task, we investigate how the relationship between teachers affects the amount of forgetting and transfer exhibited by the student when the task switches. In line with recent work, we find that when tasks depend on similar features, intermediate task similarity leads to greatest forgetting. However, feature similarity is only one way in which tasks may be related. The teacher-student approach allows us to disentangle task similarity at the level of {\textbackslash}emph\{readouts\} (hidden-to-output weights) as well as {\textbackslash}emph\{features\} (input-to-hidden weights). We find a complex interplay between both types of similarity, initial transfer/forgetting rates, maximum transfer/forgetting, and the long-time (post-switch) amount of transfer/forgetting. Together, these results help illuminate the diverse factors contributing to catastrophic forgetting.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lee, Sebastian and Goldt, Sebastian and Saxe, Andrew},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {6109--6119},
}

@inproceedings{lin_theory_2023,
	title = {Theory on {Forgetting} and {Generalization} of {Continual} {Learning}},
	url = {https://proceedings.mlr.press/v202/lin23f.html},
	abstract = {Continual learning (CL), which aims to learn a sequence of tasks, has attracted significant recent attention. However, most work has focused on the experimental performance of CL, and theoretical studies of CL are still limited. In particular, there is a lack of understanding on what factors are important and how they affect "catastrophic forgetting" and generalization performance. To fill this gap, our theoretical analysis, under overparameterized linear models, provides the first-known explicit form of the expected forgetting and generalization error for a general CL setup with an arbitrary number of tasks. Further analysis of such a key result yields a number of theoretical explanations about how overparameterization, task similarity, and task ordering affect both forgetting and generalization error of CL. More interestingly, by conducting experiments on real datasets using deep neural networks (DNNs), we show that some of these insights even go beyond the linear models and can be carried over to practical setups. In particular, we use concrete examples to show that our results not only explain some interesting empirical observations in recent studies, but also motivate better practical algorithm designs of CL.},
	language = {en},
	urldate = {2024-10-29},
	booktitle = {Proceedings of the 40th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lin, Sen and Ju, Peizhong and Liang, Yingbin and Shroff, Ness},
	month = jul,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {21078--21100},
}

@inproceedings{goldfarb_joint_2024,
	title = {The {Joint} {Effect} of {Task} {Similarity} and {Overparameterization} on {Catastrophic} {Forgetting}—{An} {Analytical} {Model}},
	booktitle = {The {Twelfth} {International} {Conference} on {Learning} {Representations}},
	author = {Goldfarb, Daniel and Evron, Itay and Weinberger, Nir and Soudry, Daniel and HAnd, PAul},
	year = {2024},
}

@inproceedings{evron_how_2022,
	title = {How catastrophic can catastrophic forgetting be in linear regression?},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Evron, Itay and Moroshko, Edward and Ward, Rachel and Srebro, Nathan and Soudry, Daniel},
	year = {2022},
	pages = {4028--4079},
}

@inproceedings{evron_continual_2023,
	title = {Continual learning in linear classification on separable data},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Evron, Itay and Moroshko, Edward and Buzaglo, Gon and Khriesh, Maroun and Marjieh, Badea and Srebro, Nathan and Soudry, Daniel},
	year = {2023},
	pages = {9440--9484},
}

@misc{domine_lazy_2024,
	title = {From {Lazy} to {Rich}: {Exact} {Learning} {Dynamics} in {Deep} {Linear} {Networks}},
	shorttitle = {From {Lazy} to {Rich}},
	url = {http://arxiv.org/abs/2409.14623},
	doi = {10.48550/arXiv.2409.14623},
	abstract = {Biological and artificial neural networks develop internal representations that enable them to perform complex tasks. In artificial networks, the effectiveness of these models relies on their ability to build task specific representation, a process influenced by interactions among datasets, architectures, initialization strategies, and optimization algorithms. Prior studies highlight that different initializations can place networks in either a lazy regime, where representations remain static, or a rich/feature learning regime, where representations evolve dynamically. Here, we examine how initialization influences learning dynamics in deep linear neural networks, deriving exact solutions for lambda-balanced initializations-defined by the relative scale of weights across layers. These solutions capture the evolution of representations and the Neural Tangent Kernel across the spectrum from the rich to the lazy regimes. Our findings deepen the theoretical understanding of the impact of weight initialization on learning regimes, with implications for continual learning, reversal learning, and transfer learning, relevant to both neuroscience and practical applications.},
	urldate = {2024-10-29},
	publisher = {arXiv},
	author = {Dominé, Clémentine C. J. and Anguita, Nicolas and Proca, Alexandra M. and Braun, Lukas and Kunin, Daniel and Mediano, Pedro A. M. and Saxe, Andrew M.},
	month = sep,
	year = {2024},
	note = {arXiv:2409.14623},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	doi = {10.1109/CVPR.2009.5206848},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = {2009},
	keywords = {Explosions, Image databases, Image retrieval, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine},
	pages = {248--255},
}

@article{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex and Hinton, Geoffrey and {others}},
	year = {2009},
	note = {Publisher: Toronto, ON, Canada},
}

@article{dosovitskiy_image_2020,
	title = {An image is worth 16x16 words: {Transformers} for image recognition at scale},
	journal = {arXiv preprint arXiv:2010.11929},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and {others}},
	year = {2020},
}

@inproceedings{he_deep_2016,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
	urldate = {2024-05-16},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2016},
	pages = {770--778},
}

@article{paszke_pytorch_2019,
	title = {Pytorch: {An} imperative style, high-performance deep learning library},
	volume = {32},
	journal = {Advances in neural information processing systems},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and {others}},
	year = {2019},
}

@article{ongie_function_2019,
	title = {A function space view of bounded norm infinite width relu nets: {The} multivariate case},
	journal = {arXiv preprint arXiv:1910.01635},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
}

@misc{schug_discovering_2024,
	title = {Discovering modular solutions that generalize compositionally},
	url = {http://arxiv.org/abs/2312.15001},
	doi = {10.48550/arXiv.2312.15001},
	abstract = {Many complex tasks can be decomposed into simpler, independent parts. Discovering such underlying compositional structure has the potential to enable compositional generalization. Despite progress, our most powerful systems struggle to compose flexibly. It therefore seems natural to make models more modular to help capture the compositional nature of many tasks. However, it is unclear under which circumstances modular systems can discover hidden compositional structure. To shed light on this question, we study a teacher-student setting with a modular teacher where we have full control over the composition of ground truth modules. This allows us to relate the problem of compositional generalization to that of identification of the underlying modules. In particular we study modularity in hypernetworks representing a general class of multiplicative interactions. We show theoretically that identification up to linear transformation purely from demonstrations is possible without having to learn an exponential number of module combinations. We further demonstrate empirically that under the theoretically identified conditions, meta-learning from finite data can discover modular policies that generalize compositionally in a number of complex environments.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Schug, Simon and Kobayashi, Seijin and Akram, Yassir and Wołczyk, Maciej and Proca, Alexandra and von Oswald, Johannes and Pascanu, Razvan and Sacramento, João and Steger, Angelika},
	month = mar,
	year = {2024},
	note = {arXiv:2312.15001 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	url = {https://proceedings.mlr.press/v97/kornblith19a.html},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	language = {en},
	urldate = {2023-11-16},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = may,
	year = {2019},
	pages = {3519--3529},
}

@misc{gao_theory_2017,
	title = {A theory of multineuronal dimensionality, dynamics and measurement},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/214262v2},
	doi = {10.1101/214262},
	abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
	language = {en},
	urldate = {2023-11-16},
	publisher = {bioRxiv},
	author = {Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
	month = nov,
	year = {2017},
}

@misc{giaffar_effective_2023,
	title = {The effective number of shared dimensions: {A} simple method for revealing shared structure between datasets},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {The effective number of shared dimensions},
	url = {https://www.biorxiv.org/content/10.1101/2023.07.27.550815v1},
	doi = {10.1101/2023.07.27.550815},
	abstract = {A number of recent studies have sought to understand the behavior of artificial and biological neural networks by comparing representations across layers, networks and brain areas. Simultaneously, there has been growing interest in using dimensionality of a dataset as a proxy for computational complexity. At the intersection of these topics, studies exploring the dimensionality of shared computational and representational subspaces have relied on model-based methods, but a standard, model-free measure is lacking. Here we present a candidate measure for shared dimensionality that we call the effective number of shared dimensions (ENSD). The ENSD can be applied to data matrices sharing at least one dimension, reduces to the well-known participation ratio when both data sets are equivalent and has a number of other robust and intuitive mathematical properties. Notably, the ENSD can be written as a similarity metric that is a re-scaled version of centered kernel alignment (CKA) but additionally describes the dimensionality of the aligned subspaces. Unlike methods like canonical correlation analysis (CCA), the ENSD is robust to cases where data is sparse or low rank. We demonstrate its utility and computational efficiency by a direct comparison of CKA and ENSD on across-layer similarities in convolutional neural networks as well as by recovering results from recent studies in neuroscience on communication subspaces between brain regions. Finally, we demonstrate how the ENSD and its constituent statistics allow us to perform a variety of multi-modal analyses of multivariate datasets. Specifically, we use connectomic data to probe the alignment of parallel pathways in the fly olfactory system, revealing novel results in the interaction between innate and learned olfactory representations. Altogether, we show that the ENSD is an interpretable and computationally efficient model-free measure of shared dimensionality and that it can be used to probe shared structure in a wide variety of data types.},
	language = {en},
	urldate = {2023-11-16},
	publisher = {bioRxiv},
	author = {Giaffar, Hamza and Buxó, Camille Rullán and Aoi, Mikio},
	month = jul,
	year = {2023},
}

@inproceedings{dai_representation_2021,
	title = {Representation {Costs} of {Linear} {Neural} {Networks}: {Analysis} and {Design}},
	volume = {34},
	shorttitle = {Representation {Costs} of {Linear} {Neural} {Networks}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/e22cb9d6bbb4c290a94e4fff4d68a831-Abstract.html},
	urldate = {2023-11-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Dai, Zhen and Karzand, Mina and Srebro, Nathan},
	year = {2021},
	pages = {26884--26896},
}

@inproceedings{azulay_implicit_2021,
	title = {On the {Implicit} {Bias} of {Initialization} {Shape}: {Beyond} {Infinitesimal} {Mirror} {Descent}},
	shorttitle = {On the {Implicit} {Bias} of {Initialization} {Shape}},
	url = {https://proceedings.mlr.press/v139/azulay21a.html},
	abstract = {Recent work has highlighted the role of initialization scale in determining the structure of the solutions that gradient methods converge to. In particular, it was shown that large initialization leads to the neural tangent kernel regime solution, whereas small initialization leads to so called “rich regimes”. However, the initialization structure is richer than the overall scale alone and involves relative magnitudes of different weights and layers in the network. Here we show that these relative scales, which we refer to as initialization shape, play an important role in determining the learned model. We develop a novel technique for deriving the inductive bias of gradient-flow and use it to obtain closed-form implicit regularizers for multiple cases of interest.},
	language = {en},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake E. and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
	month = jul,
	year = {2021},
	pages = {468--477},
}

@inproceedings{kornblith_better_2019,
	title = {Do better {ImageNet} models transfer better?},
	url = {https://arxiv.org/pdf/1805.08974.pdf},
	urldate = {2023-09-28},
	author = {Kornblith, Simon and Shlens, Jon and Le, Quoc V.},
	year = {2019},
}

@inproceedings{boursier_gradient_2022,
	title = {Gradient flow dynamics of shallow {ReLU} networks for square loss and orthogonal inputs},
	url = {https://openreview.net/forum?id=L74c-iUxQ1I},
	abstract = {The training of neural networks by gradient descent methods is a cornerstone of the deep learning revolution. Yet, despite some recent progress, a complete theory explaining its success is still missing. This article presents, for orthogonal input vectors, a precise description of the gradient flow dynamics of training one-hidden layer ReLU neural networks for the mean squared error at small initialisation. In this setting, despite non-convexity, we show that the gradient flow converges to zero loss and characterise its implicit bias towards minimum variation norm. Furthermore, some interesting phenomena are highlighted: a quantitative description of the initial alignment phenomenon and a proof that the process follows a specific saddle to saddle dynamics.},
	language = {en},
	urldate = {2023-09-28},
	author = {Boursier, Etienne and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	month = may,
	year = {2022},
}

@misc{huh_low-rank_2023,
	title = {The {Low}-{Rank} {Simplicity} {Bias} in {Deep} {Networks}},
	url = {http://arxiv.org/abs/2103.10427},
	doi = {10.48550/arXiv.2103.10427},
	abstract = {Modern deep neural networks are highly over-parameterized compared to the data on which they are trained, yet they often generalize remarkably well. A flurry of recent work has asked: why do deep networks not overfit to their training data? In this work, we make a series of empirical observations that investigate and extend the hypothesis that deeper networks are inductively biased to find solutions with lower effective rank embeddings. We conjecture that this bias exists because the volume of functions that maps to low effective rank embedding increases with depth. We show empirically that our claim holds true on finite width linear and non-linear models on practical learning paradigms and show that on natural data, these are often the solutions that generalize well. We then show that the simplicity bias exists at both initialization and after training and is resilient to hyper-parameters and learning methods. We further demonstrate how linear over-parameterization of deep non-linear models can be used to induce low-rank bias, improving generalization performance on CIFAR and ImageNet without changing the modeling capacity.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Huh, Minyoung and Mobahi, Hossein and Zhang, Richard and Cheung, Brian and Agrawal, Pulkit and Isola, Phillip},
	month = mar,
	year = {2023},
	note = {arXiv:2103.10427 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{haochen_shape_2021,
	title = {Shape {Matters}: {Understanding} the {Implicit} {Bias} of the {Noise} {Covariance}},
	shorttitle = {Shape {Matters}},
	url = {https://proceedings.mlr.press/v134/haochen21a.html},
	abstract = {The noise in stochastic gradient descent (SGD) provides a crucial implicit regularization effect for training overparameterized models. Prior theoretical work largely focuses on spherical Gaussian noise, whereas empirical studies demonstrate the phenomenon that parameter-dependent noise — induced by mini-batches or label perturbation — is far more effective than Gaussian noise.  This paper theoretically characterizes this phenomenon on a quadratically-parameterized model introduced by Vaskevicius et al. and Woodworth et al.  We show that in an over-parameterized setting, SGD with label noise recovers the sparse ground-truth with an arbitrary initialization, whereas SGD with Gaussian noise or gradient descent overfits to dense solutions with large norms. Our analysis reveals that parameter-dependent noise introduces a bias towards local minima with smaller noise variance, whereas spherical Gaussian noise does not.},
	language = {en},
	urldate = {2023-09-28},
	booktitle = {Proceedings of {Thirty} {Fourth} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {HaoChen, Jeff Z. and Wei, Colin and Lee, Jason and Ma, Tengyu},
	month = jul,
	year = {2021},
	pages = {2315--2357},
}

@article{savarese_how_2019,
	title = {How do infinite width bounded norm networks look in function space?: 32nd {Conference} on {Learning} {Theory}, {COLT} 2019},
	volume = {99},
	shorttitle = {How do infinite width bounded norm networks look in function space?},
	url = {http://www.scopus.com/inward/record.url?scp=85132757852&partnerID=8YFLogxK},
	abstract = {We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions f : R → R and a single hidden layer, we show that the minimal network norm for representing f is max(´ {\textbar}f00(x){\textbar} dx, {\textbar}f0(-∞) + f0(+∞){\textbar}), and hence the minimal norm fit for a sample is given by a linear spline interpolation.},
	urldate = {2023-09-28},
	journal = {Proceedings of Machine Learning Research},
	author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	year = {2019},
	pages = {2667--2690},
}

@inproceedings{pesme_implicit_2021,
	title = {Implicit {Bias} of {SGD} for {Diagonal} {Linear} {Networks}: a {Provable} {Benefit} of {Stochasticity}},
	shorttitle = {Implicit {Bias} of {SGD} for {Diagonal} {Linear} {Networks}},
	url = {https://openreview.net/forum?id=vvi7KqHQiA},
	abstract = {Understanding the implicit bias of training algorithms is of crucial importance in order to explain the success of overparametrised neural networks. In this paper, we study the dynamics of stochastic gradient descent over diagonal linear networks through its continuous time version, namely stochastic gradient flow. We explicitly characterise the solution chosen by the stochastic flow and prove that it always enjoys better generalisation properties than that of gradient flow.Quite surprisingly, we show that the convergence speed of the training loss controls the magnitude of the biasing effect: the slower the convergence, the better the bias. To fully complete our analysis, we provide convergence guarantees for the dynamics. We also give experimental results which support our theoretical claims. Our findings highlight the fact that structured noise can induce better generalisation and they help explain the greater performances of stochastic gradient descent over gradient descent observed in practice.},
	language = {en},
	urldate = {2023-09-28},
	author = {Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	month = nov,
	year = {2021},
}

@inproceedings{lyle_comparative_2019,
	address = {Honolulu, Hawaii, USA},
	series = {{AAAI}'19/{IAAI}'19/{EAAI}'19},
	title = {A comparative analysis of expected and distributional reinforcement learning},
	isbn = {9781577358091},
	url = {https://dl.acm.org/doi/10.1609/aaai.v33i01.33014504},
	doi = {10.1609/aaai.v33i01.33014504},
	abstract = {Since their introduction a year ago, distributional approaches to reinforcement learning (distributional RL) have produced strong results relative to the standard approach which models expected values (expected RL). However, aside from convergence guarantees, there have been few theoretical results investigating the reasons behind the improvements distributional RL provides. In this paper we begin the investigation into this fundamental question by analyzing the differences in the tabular, linear approximation, and non-linear approximation settings. We prove that in many realizations of the tabular and linear approximation settings, distributional RL behaves exactly the same as expected RL. In cases where the two methods behave differently, distributional RL can in fact hurt performance when it does not induce identical behaviour. We then continue with an empirical analysis comparing distributional and expected RL methods in control settings with non-linear approximators to tease apart where the improvements from distributional RL methods are coming from.},
	urldate = {2023-09-27},
	booktitle = {Proceedings of the {Thirty}-{Third} {AAAI} {Conference} on {Artificial} {Intelligence} and {Thirty}-{First} {Innovative} {Applications} of {Artificial} {Intelligence} {Conference} and {Ninth} {AAAI} {Symposium} on {Educational} {Advances} in {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Lyle, Clare and Bellemare, Marc G. and Castro, Pablo Samuel},
	month = jan,
	year = {2019},
	pages = {4504--4511},
}

@misc{jaderberg_reinforcement_2016,
	title = {Reinforcement {Learning} with {Unsupervised} {Auxiliary} {Tasks}},
	url = {http://arxiv.org/abs/1611.05397},
	doi = {10.48550/arXiv.1611.05397},
	abstract = {Deep reinforcement learning agents have achieved state-of-the-art results by directly maximising cumulative reward. However, environments contain a much wider variety of possible training signals. In this paper, we introduce an agent that also maximises many other pseudo-reward functions simultaneously by reinforcement learning. All of these tasks share a common representation that, like unsupervised learning, continues to develop in the absence of extrinsic rewards. We also introduce a novel mechanism for focusing this representation upon extrinsic rewards, so that learning can rapidly adapt to the most relevant aspects of the actual task. Our agent significantly outperforms the previous state-of-the-art on Atari, averaging 880{\textbackslash}\% expert human performance, and a challenging suite of first-person, three-dimensional {\textbackslash}emph\{Labyrinth\} tasks leading to a mean speedup in learning of 10\${\textbackslash}times\$ and averaging 87{\textbackslash}\% expert human performance on Labyrinth.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z. and Silver, David and Kavukcuoglu, Koray},
	month = nov,
	year = {2016},
	note = {arXiv:1611.05397 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@inproceedings{lyle_effect_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On the {Effect} of {Auxiliary} {Tasks} on {Representation} {Dynamics}},
	volume = {130},
	url = {http://proceedings.mlr.press/v130/lyle21a.html},
	urldate = {2023-09-28},
	booktitle = {The 24th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}, {AISTATS} 2021, {April} 13-15, 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Lyle, Clare and Rowland, Mark and Ostrovski, Georg and Dabney, Will},
	editor = {Banerjee, Arindam and Fukumizu, Kenji},
	year = {2021},
	pages = {1--9},
}

@inproceedings{weller_when_2022,
	address = {Dublin, Ireland},
	title = {When to {Use} {Multi}-{Task} {Learning} vs {Intermediate} {Fine}-{Tuning} for {Pre}-{Trained} {Encoder} {Transfer} {Learning}},
	url = {https://aclanthology.org/2022.acl-short.30},
	doi = {10.18653/v1/2022.acl-short.30},
	abstract = {Transfer learning (TL) in natural language processing (NLP) has seen a surge of interest in recent years, as pre-trained models have shown an impressive ability to transfer to novel tasks. Three main strategies have emerged for making use of multiple supervised datasets during fine-tuning: training on an intermediate task before training on the target task (STILTs), using multi-task learning (MTL) to train jointly on a supplementary task and the target task (pairwise MTL), or simply using MTL to train jointly on all available datasets (MTL-ALL). In this work, we compare all three TL methods in a comprehensive analysis on the GLUE dataset suite. We find that there is a simple heuristic for when to use one of these techniques over the other: pairwise MTL is better than STILTs when the target task has fewer instances than the supporting task and vice versa. We show that this holds true in more than 92\% of applicable cases on the GLUE dataset and validate this hypothesis with experiments varying dataset size. The simplicity and effectiveness of this heuristic is surprising and warrants additional exploration by the TL community. Furthermore, we find that MTL-ALL is worse than the pairwise methods in almost every case. We hope this study will aid others as they choose between TL methods for NLP tasks.},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Weller, Orion and Seppi, Kevin and Gardner, Matt},
	month = may,
	year = {2022},
	pages = {272--282},
}

@inproceedings{dery_should_2021,
	title = {Should {We} {Be} {Pre}-training? {An} {Argument} for {End}-task {Aware} {Training} as an {Alternative}},
	shorttitle = {Should {We} {Be} {Pre}-training?},
	url = {https://openreview.net/forum?id=2bO2x8NAIMB},
	abstract = {In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pre-trained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020). We next introduce an online meta-learning algorithm that learns a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end-task performance and data efficiency.},
	language = {en},
	urldate = {2023-09-28},
	author = {Dery, Lucio M. and Michel, Paul and Talwalkar, Ameet and Neubig, Graham},
	month = oct,
	year = {2021},
}

@misc{wu_understanding_2020,
	title = {Understanding and {Improving} {Information} {Transfer} in {Multi}-{Task} {Learning}},
	url = {http://arxiv.org/abs/2005.00944},
	doi = {10.48550/arXiv.2005.00944},
	abstract = {We investigate multi-task learning approaches that use a shared feature representation for all tasks. To better understand the transfer of task information, we study an architecture with a shared module for all tasks and a separate output module for each task. We study the theory of this setting on linear and ReLU-activated models. Our key observation is that whether or not tasks' data are well-aligned can significantly affect the performance of multi-task learning. We show that misalignment between task data can cause negative transfer (or hurt performance) and provide sufficient conditions for positive transfer. Inspired by the theoretical insights, we show that aligning tasks' embedding layers leads to performance gains for multi-task training and transfer learning on the GLUE benchmark and sentiment analysis tasks; for example, we obtain a 2.35\% GLUE score average improvement on 5 GLUE tasks over BERT-LARGE using our alignment method. We also design an SVD-based task reweighting scheme and show that it improves the robustness of multi-task training on a multi-label image dataset.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Wu, Sen and Zhang, Hongyang R. and Ré, Christopher},
	month = may,
	year = {2020},
	note = {arXiv:2005.00944 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{ruder_overview_2017,
	title = {An {Overview} of {Multi}-{Task} {Learning} in {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.05098},
	doi = {10.48550/arXiv.1706.05098},
	abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Ruder, Sebastian},
	month = jun,
	year = {2017},
	note = {arXiv:1706.05098 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maurer_benefit_2016,
	title = {The {Benefit} of {Multitask} {Representation} {Learning}},
	volume = {17},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v17/15-242.html},
	abstract = {We discuss a general method to learn data representations from multiple tasks. We provide a justification for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage offered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneficial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.},
	number = {81},
	urldate = {2023-09-28},
	journal = {Journal of Machine Learning Research},
	author = {Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
	year = {2016},
	pages = {1--32},
}

@inproceedings{steed_upstream_2022,
	address = {Dublin, Ireland},
	title = {Upstream {Mitigation} {Is} \textit{ {N}}ot {All} {You} {Need}: {Testing} the {Bias} {Transfer} {Hypothesis} in {Pre}-{Trained} {Language} {Models}},
	shorttitle = {Upstream {Mitigation} {Is} \textit{ {N}}ot {All} {You} {Need}},
	url = {https://aclanthology.org/2022.acl-long.247},
	doi = {10.18653/v1/2022.acl-long.247},
	abstract = {A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier's discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Steed, Ryan and Panda, Swetasudha and Kobren, Ari and Wick, Michael},
	month = may,
	year = {2022},
	pages = {3524--3542},
}

@misc{wang_overwriting_2023,
	title = {Overwriting {Pretrained} {Bias} with {Finetuning} {Data}},
	url = {http://arxiv.org/abs/2303.06167},
	doi = {10.48550/arXiv.2303.06167},
	abstract = {Transfer learning is beneficial by allowing the expressive features of models pretrained on large-scale datasets to be finetuned for the target task of smaller, more domain-specific datasets. However, there is a concern that these pretrained models may come with their own biases which would propagate into the finetuned model. In this work, we investigate bias when conceptualized as both spurious correlations between the target task and a sensitive attribute as well as underrepresentation of a particular group in the dataset. Under both notions of bias, we find that (1) models finetuned on top of pretrained models can indeed inherit their biases, but (2) this bias can be corrected for through relatively minor interventions to the finetuning dataset, and often with a negligible impact to performance. Our findings imply that careful curation of the finetuning dataset is important for reducing biases on a downstream task, and doing so can even compensate for bias in the pretrained model.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Wang, Angelina and Russakovsky, Olga},
	month = aug,
	year = {2023},
	note = {arXiv:2303.06167 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{neyshabur_what_2020,
	title = {What is being transferred in transfer learning?},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0607f4c705595b911a4f3e7a127b44e0-Abstract.html},
	abstract = {One desired capability for machines is the ability to transfer their understanding of one domain to another domain where data is (usually) scarce. Despite ample adaptation of transfer learning in many deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analysis to address these fundamental questions. Through a series of analysis on transferring to block-shuffled images, we separate the effect of feature reuse from learning high-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
	urldate = {2023-09-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
	year = {2020},
	pages = {512--523},
}

@misc{bommasani_opportunities_2022,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = jul,
	year = {2022},
	note = {arXiv:2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@inproceedings{lovering_predicting_2020,
	title = {Predicting {Inductive} {Biases} of {Pre}-{Trained} {Models}},
	url = {https://openreview.net/forum?id=mNtmhaDkAr},
	abstract = {Most current NLP systems are based on a pre-train-then-fine-tune paradigm, in which a large neural network is first trained in a self-supervised way designed to encourage the network to extract broadly-useful linguistic features, and then fine-tuned for a specific task of interest. Recent work attempts to understand why this recipe works and explain when it fails. Currently, such analyses have produced two sets of apparently-contradictory results. Work that analyzes the representations that result from pre-training (via "probing classifiers") finds evidence that rich features of linguistic structure can be decoded with high accuracy, but work that analyzes model behavior after fine-tuning (via "challenge sets") indicates that decisions are often not based on such structure but rather on spurious heuristics specific to the training set. In this work, we test the hypothesis that the extent to which a feature influences a model's decisions can be predicted using a combination of two factors: The feature's "extractability" after pre-training (measured using information-theoretic probing techniques), and the "evidence" available during fine-tuning (defined as the feature's co-occurrence rate with the label). In experiments with both synthetic and natural language data, we find strong evidence (statistically significant correlations) supporting this hypothesis.},
	language = {en},
	urldate = {2023-09-28},
	author = {Lovering, Charles and Jha, Rohan and Linzen, Tal and Pavlick, Ellie},
	month = oct,
	year = {2020},
}

@inproceedings{shachaf_theoretical_2021,
	title = {A {Theoretical} {Analysis} of {Fine}-tuning with {Linear} {Teachers}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/82039d16dce0aab3913b6a7ac73deff7-Abstract.html},
	abstract = {Fine-tuning is a common practice in deep learning, achieving excellent generalization results on downstream tasks using relatively little training data. Although widely used in practice, it is not well understood theoretically. Here we analyze the sample complexity of this scheme for regression with linear teachers in several settings. Intuitively, the success of fine-tuning depends on the similarity between the source tasks and the target task. But what is the right way of measuring this similarity? We show that the relevant measure has to do with the relation between the source task, the target task and the covariance structure of the target data. In the setting of linear regression, we show that under realistic settings there can be substantial sample complexity reduction when the above measure is low. For deep linear regression, we propose a novel result regarding the inductive bias of gradient-based training when the network is initialized with pretrained weights. Using this result we show that the similarity measure for this setting is also affected by the depth of the network. We conclude with results on shallow ReLU models, and analyze the dependence of sample complexity there on source and target tasks. We empirically demonstrate our results for both synthetic and realistic data.},
	urldate = {2023-09-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Shachaf, Gal and Brutzkus, Alon and Globerson, Amir},
	year = {2021},
	pages = {15382--15394},
}

@inproceedings{moroshko_implicit_2020,
	title = {Implicit {Bias} in {Deep} {Linear} {Classification}: {Initialization} {Scale} vs {Training} {Accuracy}},
	volume = {33},
	shorttitle = {Implicit {Bias} in {Deep} {Linear} {Classification}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/fc2022c89b61c76bbef978f1370660bf-Abstract.html},
	abstract = {We provide a detailed asymptotic study of gradient flow trajectories and their implicit optimization bias when minimizing the exponential loss over "diagonal linear networks". This is the simplest model displaying a transition between "kernel" and non-kernel ("rich" or "active") regimes.  We show how the transition is controlled by the relationship between the initialization scale and how accurately we minimize the training loss.  Our results indicate that some limit behavior of gradient descent only kick in at ridiculous training accuracies (well beyond 10{\textasciicircum}-100). Moreover, the implicit bias at reasonable initialization scales and training accuracies is more complex and not captured by these limits.},
	urldate = {2023-09-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
	year = {2020},
	pages = {22182--22193},
}

@inproceedings{yang_tensor_2021,
	title = {Tensor {Programs} {IV}: {Feature} {Learning} in {Infinite}-{Width} {Neural} {Networks}},
	shorttitle = {Tensor {Programs} {IV}},
	url = {https://proceedings.mlr.press/v139/yang21c.html},
	abstract = {As its width tends to infinity, a deep neural network’s behavior under gradient descent can become simplified and predictable (e.g. given by the Neural Tangent Kernel (NTK)), if it is parametrized appropriately (e.g. the NTK parametrization). However, we show that the standard and NTK parametrizations of a neural network do not admit infinite-width limits that can *learn* features, which is crucial for pretraining and transfer learning such as with BERT. We propose simple modifications to the standard parametrization to allow for feature learning in the limit. Using the *Tensor Programs* technique, we derive explicit formulas for such limits. On Word2Vec and few-shot learning on Omniglot via MAML, two canonical tasks that rely crucially on feature learning, we compute these limits exactly. We find that they outperform both NTK baselines and finite-width networks, with the latter approaching the infinite-width feature learning performance as width increases.},
	language = {en},
	urldate = {2023-09-28},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Yang, Greg and Hu, Edward J.},
	month = jul,
	year = {2021},
	pages = {11727--11737},
}

@misc{jacot_neural_2020,
	title = {Neural {Tangent} {Kernel}: {Convergence} and {Generalization} in {Neural} {Networks}},
	shorttitle = {Neural {Tangent} {Kernel}},
	url = {http://arxiv.org/abs/1806.07572},
	doi = {10.48550/arXiv.1806.07572},
	abstract = {At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function \$f\_{\textbackslash}theta\$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function \$f\_{\textbackslash}theta\$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	month = feb,
	year = {2020},
	note = {arXiv:1806.07572 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Probability, Statistics - Machine Learning},
}

@misc{neyshabur_implicit_2017,
	title = {Implicit {Regularization} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1709.01953},
	doi = {10.48550/arXiv.1709.01953},
	abstract = {In an attempt to better understand generalization in deep learning, we study several possible explanations. We show that implicit regularization induced by the optimization method is playing a key role in generalization and success of deep learning models. Motivated by this view, we study how different complexity measures can ensure generalization and explain how optimization algorithms can implicitly regularize complexity measures. We empirically investigate the ability of these measures to explain different observed phenomena in deep learning. We further study the invariances in neural networks, suggest complexity measures and optimization algorithms that have similar invariances to those in neural networks and evaluate them on a number of learning tasks.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Neyshabur, Behnam},
	month = sep,
	year = {2017},
	note = {arXiv:1709.01953 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{mulayoff_implicit_2021,
	title = {The {Implicit} {Bias} of {Minima} {Stability}: {A} {View} from {Function} {Space}},
	volume = {34},
	shorttitle = {The {Implicit} {Bias} of {Minima} {Stability}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/944a5ae3483ed5c1e10bbccb7942a279-Abstract.html},
	urldate = {2023-09-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mulayoff, Rotem and Michaeli, Tomer and Soudry, Daniel},
	year = {2021},
	pages = {17749--17761},
}

@misc{refinetti_neural_2023,
	title = {Neural networks trained with {SGD} learn distributions of increasing complexity},
	url = {http://arxiv.org/abs/2211.11567},
	doi = {10.48550/arXiv.2211.11567},
	abstract = {The ability of deep neural networks to generalise well even when they interpolate their training data has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first learning simple functions, say a linear classifier, before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias (DSB) in a solvable model of a neural network trained on synthetic data. We empirically demonstrate DSB in a range of deep convolutional networks and visual transformers trained on CIFAR10, and show that it even holds in networks pre-trained on ImageNet. We discuss the relation of DSB to other simplicity biases and consider its implications for the principle of Gaussian universality in learning.},
	urldate = {2023-09-28},
	publisher = {arXiv},
	author = {Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian},
	month = may,
	year = {2023},
	note = {arXiv:2211.11567 [cond-mat, stat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
}

@article{rahaman_spectral_2018,
	title = {On the {Spectral} {Bias} of {Neural} {Networks}},
	url = {https://openreview.net/forum?id=r1gR2sC9FX},
	abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we show that deep ReLU networks are biased towards low frequency functions, meaning that they cannot have local fluctuations without affecting their global behavior. Intuitively, this property is in line with the observation that over-parameterized networks find simple patterns that generalize across data samples. We also investigate how the shape of the data manifold affects expressivity by showing evidence that learning high frequencies gets easier with increasing manifold complexity, and present a theoretical understanding of this behavior. Finally, we study the robustness of the frequency components with respect to parameter perturbation, to develop the intuition that the parameters must be finely tuned to express high frequency functions.},
	language = {en},
	urldate = {2023-09-28},
	author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
	month = sep,
	year = {2018},
}

@inproceedings{smith_origin_2020,
	title = {On the {Origin} of {Implicit} {Regularization} in {Stochastic} {Gradient} {Descent}},
	url = {https://openreview.net/forum?id=rq_Qr0c1Hyo},
	abstract = {For infinitesimal learning rates, stochastic gradient descent (SGD) follows the path of gradient flow on the full batch loss function. However moderately large learning rates can achieve higher test accuracies, and this generalization benefit is not explained by convergence bounds, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss. To interpret this phenomenon we prove that for SGD with random shuffling, the mean SGD iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss. This modified loss is composed of the original loss function and an implicit regularizer, which penalizes the norms of the minibatch gradients. Under mild assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size. We verify empirically that explicitly including the implicit regularizer in the loss can enhance the test accuracy when the learning rate is small.},
	language = {en},
	urldate = {2023-09-28},
	author = {Smith, Samuel L. and Dherin, Benoit and Barrett, David and De, Soham},
	month = oct,
	year = {2020},
}

@article{bach_breaking_2017,
	title = {Breaking the curse of dimensionality with convex neural networks},
	volume = {18},
	issn = {1532-4435},
	abstract = {We consider neural networks with a single hidden layer and non-decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity-inducing norms on the input weights, we show that high-dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the nonconvex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non-convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomialtime algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Bach, Francis},
	month = jan,
	year = {2017},
	keywords = {convex optimization, convex relaxation, neural networks, non-parametric estimation},
	pages = {629--681},
}

@misc{wang_harmless_2021,
	title = {Harmless {Overparametrization} in {Two}-layer {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.04795},
	doi = {10.48550/arXiv.2106.04795},
	abstract = {Overparametrized neural networks, where the number of active parameters is larger than the sample size, prove remarkably effective in modern deep learning practice. From the classical perspective, however, much fewer parameters are sufficient for optimal estimation and prediction, whereas overparametrization can be harmful even in the presence of explicit regularization. To reconcile this conflict, we present a generalization theory for overparametrized ReLU networks by incorporating an explicit regularizer based on the scaled variation norm. Interestingly, this regularizer is equivalent to the ridge from the angle of gradient-based optimization, but is similar to the group lasso in terms of controlling model complexity. By exploiting this ridge-lasso duality, we show that overparametrization is generally harmless to two-layer ReLU networks. In particular, the overparametrized estimators are minimax optimal up to a logarithmic factor. By contrast, we show that overparametrized random feature models suffer from the curse of dimensionality and thus are suboptimal.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Wang, Huiyuan and Lin, Wei},
	month = jun,
	year = {2021},
	note = {arXiv:2106.04795 [cs, math, stat]},
	keywords = {62G08(primary), 62J07(secondary), 68T07, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@misc{pilanci_neural_2020,
	title = {Neural {Networks} are {Convex} {Regularizers}: {Exact} {Polynomial}-time {Convex} {Optimization} {Formulations} for {Two}-layer {Networks}},
	shorttitle = {Neural {Networks} are {Convex} {Regularizers}},
	url = {http://arxiv.org/abs/2002.10553},
	doi = {10.48550/arXiv.2002.10553},
	abstract = {We develop exact representations of training two-layer neural networks with rectified linear units (ReLUs) in terms of a single convex program with number of variables polynomial in the number of training samples and the number of hidden neurons. Our theory utilizes semi-infinite duality and minimum norm regularization. We show that ReLU networks trained with standard weight decay are equivalent to block \${\textbackslash}ell\_1\$ penalized convex models. Moreover, we show that certain standard convolutional linear networks are equivalent semi-definite programs which can be simplified to \${\textbackslash}ell\_1\$ regularized linear models in a polynomial sized discrete Fourier feature space.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Pilanci, Mert and Ergen, Tolga},
	month = aug,
	year = {2020},
	note = {arXiv:2002.10553 [cs, stat]},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_hidden_2022,
	title = {The {Hidden} {Convex} {Optimization} {Landscape} of {Two}-{Layer} {ReLU} {Neural} {Networks}: an {Exact} {Characterization} of the {Optimal} {Solutions}},
	shorttitle = {The {Hidden} {Convex} {Optimization} {Landscape} of {Two}-{Layer} {ReLU} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2006.05900},
	doi = {10.48550/arXiv.2006.05900},
	abstract = {We prove that finding all globally optimal two-layer ReLU neural networks can be performed by solving a convex optimization program with cone constraints. Our analysis is novel, characterizes all optimal solutions, and does not leverage duality-based analysis which was recently used to lift neural network training into convex spaces. Given the set of solutions of our convex optimization program, we show how to construct exactly the entire set of optimal neural networks. We provide a detailed characterization of this optimal set and its invariant transformations. As additional consequences of our convex perspective, (i) we establish that Clarke stationary points found by stochastic gradient descent correspond to the global optimum of a subsampled convex problem (ii) we provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss (iii) we provide an explicit construction of a continuous path between any neural network and the global minimum of its sublevel set and (iv) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys. Overall, we provide a rich framework for studying the landscape of neural network training loss through convexity.},
	urldate = {2023-07-20},
	publisher = {arXiv},
	author = {Wang, Yifei and Lacotte, Jonathan and Pilanci, Mert},
	month = mar,
	year = {2022},
	note = {arXiv:2006.05900 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{noauthor_notitle_nodate,
}

@article{bach_breaking_2017-1,
	title = {Breaking the {Curse} of {Dimensionality} with {Convex} {Neural} {Networks}},
	volume = {18},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v18/14-546.html},
	abstract = {We consider neural networks with a single hidden layer and non- decreasing positively homogeneous activation functions like the rectified linear units. By letting the number of hidden units grow unbounded and using classical non-Euclidean regularization tools on the output weights, they lead to a convex optimization problem and we provide a detailed theoretical analysis of their generalization performance, with a study of both the approximation and the estimation errors. We show in particular that they are adaptive to unknown underlying linear structures, such as the dependence on the projection of the input variables onto a low-dimensional subspace. Moreover, when using sparsity- inducing norms on the input weights, we show that high- dimensional non-linear variable selection may be achieved, without any strong assumption regarding the data and with a total number of variables potentially exponential in the number of observations. However, solving this convex optimization problem in infinite dimensions is only possible if the non- convex subproblem of addition of a new unit can be solved efficiently. We provide a simple geometric interpretation for our choice of activation functions and describe simple conditions for convex relaxations of the finite-dimensional non- convex subproblem to achieve the same generalization error bounds, even when constant-factor approximations cannot be found. We were not able to find strong enough convex relaxations to obtain provably polynomial-time algorithms and leave open the existence or non-existence of such tractable algorithms with non-exponential sample complexities.},
	number = {19},
	urldate = {2023-07-13},
	journal = {Journal of Machine Learning Research},
	author = {Bach, Francis},
	year = {2017},
	pages = {1--53},
}

@article{braun_exact_2022,
	title = {Exact learning dynamics of deep linear networks with prior knowledge},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/2b3bb2c95195130977a51b3bb251c40a-Abstract-Conference.html},
	language = {en},
	urldate = {2023-05-30},
	journal = {Advances in Neural Information Processing Systems},
	author = {Braun, Lukas and Dominé, Clémentine and Fitzgerald, James and Saxe, Andrew},
	month = dec,
	year = {2022},
	pages = {6615--6629},
}

@inproceedings{razin_implicit_2020,
	title = {Implicit {Regularization} in {Deep} {Learning} {May} {Not} {Be} {Explainable} by {Norms}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f21e255f89e0f258accbe4e984eef486-Abstract.html},
	abstract = {Mathematically characterizing the implicit regularization induced by gradient-based optimization is a longstanding pursuit in the theory of deep learning. A widespread hope is that a characterization based on minimization of norms may apply, and a standard test-bed for studying this prospect is matrix factorization (matrix completion via linear neural networks). It is an open question whether norms can explain the implicit regularization in matrix factorization. The current paper resolves this open question in the negative, by proving that there exist natural matrix factorization problems on which the implicit regularization drives all norms (and quasi-norms) towards infinity. Our results suggest that, rather than perceiving the implicit regularization via norms, a potentially more useful interpretation is minimization of rank. We demonstrate empirically that this interpretation extends to a certain class of non-linear neural networks, and hypothesize that it may be key to explaining generalization in deep learning.},
	urldate = {2023-05-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Razin, Noam and Cohen, Nadav},
	year = {2020},
	pages = {21174--21187},
}

@inproceedings{nacson_lexicographic_2019,
	title = {Lexicographic and {Depth}-{Sensitive} {Margins} in {Homogeneous} and {Non}-{Homogeneous} {Deep} {Models}},
	url = {https://proceedings.mlr.press/v97/nacson19a.html},
	abstract = {With an eye toward understanding complexity control in deep learning, we study how infinitesimal regularization or gradient descent optimization lead to margin maximizing solutions in both homogeneous and non homogeneous models, extending previous work that focused on infinitesimal regularization only in homogeneous models. To this end we study the limit of loss minimization with a diverging norm constraint (the “constrained path”), relate it to the limit of a “margin path” and characterize the resulting solution. For non-homogeneous ensemble models, which output is a sum of homogeneous sub-models, we show that this solution discards the shallowest sub-models if they are unnecessary. For homogeneous models, we show convergence to a “lexicographic max-margin solution”, and provide conditions under which max-margin solutions are also attained as the limit of unconstrained gradient descent.},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Nacson, Mor Shpigel and Gunasekar, Suriya and Lee, Jason and Srebro, Nathan and Soudry, Daniel},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4683--4692},
}

@misc{kumar_fine-tuning_2022,
	title = {Fine-{Tuning} can {Distort} {Pretrained} {Features} and {Underperform} {Out}-of-{Distribution}},
	url = {http://arxiv.org/abs/2202.10054},
	doi = {10.48550/arXiv.2202.10054},
	abstract = {When transferring a pretrained model to a downstream task, two popular methods are full fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer -- the "head"). It is well known that fine-tuning leads to better accuracy in-distribution (ID). However, in this paper, we find that fine-tuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (Breeds-Living17, Breeds-Entity30, DomainNet, CIFAR \${\textbackslash}to\$ STL, CIFAR10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), fine-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: fine-tuning overparameterized two-layer linear networks. We prove that the OOD error of fine-tuning is high when we initialize with a fixed or random head -- this is because while fine-tuning learns the head, the lower layers of the neural network change simultaneously and distort the pretrained features. Our analysis suggests that the easy two-step strategy of linear probing then full fine-tuning (LP-FT), sometimes used as a fine-tuning heuristic, combines the benefits of both fine-tuning and linear probing. Empirically, LP-FT outperforms both fine-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full fine-tuning).},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
	month = feb,
	year = {2022},
	note = {arXiv:2202.10054 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{vafaeikia_brief_2020,
	title = {A {Brief} {Review} of {Deep} {Multi}-task {Learning} and {Auxiliary} {Task} {Learning}},
	url = {http://arxiv.org/abs/2007.01126},
	doi = {10.48550/arXiv.2007.01126},
	abstract = {Multi-task learning (MTL) optimizes several learning tasks simultaneously and leverages their shared information to improve generalization and the prediction of the model for each task. Auxiliary tasks can be added to the main task to ultimately boost the performance. In this paper, we provide a brief review on the recent deep multi-task learning (dMTL) approaches followed by methods on selecting useful auxiliary tasks that can be used in dMTL to improve the performance of the model for the main task.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Vafaeikia, Partoo and Namdar, Khashayar and Khalvati, Farzad},
	month = jul,
	year = {2020},
	note = {arXiv:2007.01126 [cs, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{du_survey_2022,
	title = {A {Survey} of {Vision}-{Language} {Pre}-{Trained} {Models}},
	url = {http://arxiv.org/abs/2202.10936},
	doi = {10.48550/arXiv.2202.10936},
	abstract = {As transformer evolves, pre-trained models have advanced at a breakneck pace in recent years. They have dominated the mainstream techniques in natural language processing (NLP) and computer vision (CV). How to adapt pre-training to the field of Vision-and-Language (V-L) learning and improve downstream task performance becomes a focus of multimodal learning. In this paper, we review the recent progress in Vision-Language Pre-Trained Models (VL-PTMs). As the core content, we first briefly introduce several ways to encode raw images and texts to single-modal embeddings before pre-training. Then, we dive into the mainstream architectures of VL-PTMs in modeling the interaction between text and image representations. We further present widely-used pre-training tasks, and then we introduce some common downstream tasks. We finally conclude this paper and present some promising research directions. Our survey aims to provide researchers with synthesis and pointer to related research.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Du, Yifan and Liu, Zikang and Li, Junyi and Zhao, Wayne Xin},
	month = jul,
	year = {2022},
	note = {arXiv:2202.10936 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zhang_survey_2022,
	title = {A {Survey} on {Multi}-{Task} {Learning}},
	volume = {34},
	issn = {1558-2191},
	doi = {10.1109/TKDE.2021.3070203},
	abstract = {Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.},
	number = {12},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Zhang, Yu and Yang, Qiang},
	month = dec,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Knowledge and Data Engineering},
	keywords = {Classification algorithms, Computational modeling, Data models, Multi-task learning, Supervised learning, Task analysis, Training, Transfer learning, artificial intelligence, machine learning},
	pages = {5586--5609},
}

@misc{zhou_comprehensive_2023,
	title = {A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}: {A} {History} from {BERT} to {ChatGPT}},
	shorttitle = {A {Comprehensive} {Survey} on {Pretrained} {Foundation} {Models}},
	url = {http://arxiv.org/abs/2302.09419},
	doi = {10.48550/arXiv.2302.09419},
	abstract = {Pretrained Foundation Models (PFMs) are regarded as the foundation for various downstream tasks with different data modalities. A PFM (e.g., BERT, ChatGPT, and GPT-4) is trained on large-scale data which provides a reasonable parameter initialization for a wide range of downstream applications. BERT learns bidirectional encoder representations from Transformers, which are trained on large datasets as contextual language models. Similarly, the generative pretrained transformer (GPT) method employs Transformers as the feature extractor and is trained using an autoregressive paradigm on large datasets. Recently, ChatGPT shows promising success on large language models, which applies an autoregressive language model with zero shot or few shot prompting. The remarkable achievements of PFM have brought significant breakthroughs to various fields of AI. Numerous studies have proposed different methods, raising the demand for an updated survey. This study provides a comprehensive review of recent research advancements, challenges, and opportunities for PFMs in text, image, graph, as well as other data modalities. The review covers the basic components and existing pretraining methods used in natural language processing, computer vision, and graph learning. Additionally, it explores advanced PFMs used for different data modalities and unified PFMs that consider data quality and quantity. The review also discusses research related to the fundamentals of PFMs, such as model efficiency and compression, security, and privacy. Finally, the study provides key implications, future research directions, challenges, and open problems in the field of PFMs. Overall, this survey aims to shed light on the research of the PFMs on scalability, security, logical reasoning ability, cross-domain learning ability, and the user-friendly interactive ability for artificial general intelligence.},
	urldate = {2023-05-29},
	publisher = {arXiv},
	author = {Zhou, Ce and Li, Qian and Li, Chen and Yu, Jun and Liu, Yixin and Wang, Guangjing and Zhang, Kai and Ji, Cheng and Yan, Qiben and He, Lifang and Peng, Hao and Li, Jianxin and Wu, Jia and Liu, Ziwei and Xie, Pengtao and Xiong, Caiming and Pei, Jian and Yu, Philip S. and Sun, Lichao},
	month = may,
	year = {2023},
	note = {arXiv:2302.09419 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {https://proceedings.mlr.press/v119/chen20j.html},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	language = {en},
	urldate = {2023-05-29},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1597--1607},
}

@inproceedings{fort_deep_2020,
	title = {Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the {Neural} {Tangent} {Kernel}},
	volume = {33},
	shorttitle = {Deep learning versus kernel learning},
	url = {https://proceedings.neurips.cc/paper/2020/hash/405075699f065e43581f27d67bb68478-Abstract.html},
	abstract = {In suitably initialized wide networks, small learning rates transform deep neural networks (DNNs) into neural tangent kernel (NTK) machines, whose training dynamics is well-approximated by a linear weight expansion of the network at initialization.  Standard training, however, diverges from its linearization in ways that are poorly understood. We study the relationship between the training dynamics of nonlinear deep networks, the geometry of the loss landscape, and the time evolution of a data-dependent NTK. We do so through a large-scale phenomenological analysis of training, synthesizing diverse measures characterizing loss landscape geometry and NTK dynamics. In multiple neural architectures and datasets, we find these diverse measures evolve in a highly correlated manner, revealing a universal picture of the deep learning process.  In this picture, deep network training exhibits a highly chaotic rapid initial transient that within 2 to 3 epochs determines the final linearly connected basin of low loss containing the end point of training. During this chaotic transient, the NTK changes rapidly, learning useful features from the training data that enables it to outperform the standard initial NTK by a factor of 3 in less than 3 to 4 epochs. After this rapid chaotic transient, the NTK changes at constant velocity, and its performance matches that of full network training in 15{\textbackslash}\% to 45{\textbackslash}\% of training time. Overall, our analysis reveals a striking correlation between a diverse set of metrics over training time, governed by a rapid chaotic to stable transition in the first few epochs, that together poses challenges and opportunities for the development of more accurate theories of deep learning.},
	urldate = {2023-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fort, Stanislav and Dziugaite, Gintare Karolina and Paul, Mansheej and Kharaghani, Sepideh and Roy, Daniel M and Ganguli, Surya},
	year = {2020},
	pages = {5850--5861},
}

@misc{vyas_limitations_2022,
	title = {Limitations of the {NTK} for {Understanding} {Generalization} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2206.10012},
	doi = {10.48550/arXiv.2206.10012},
	abstract = {The ``Neural Tangent Kernel'' (NTK) (Jacot et al 2018), and its empirical variants have been proposed as a proxy to capture certain behaviors of real neural networks. In this work, we study NTKs through the lens of scaling laws, and demonstrate that they fall short of explaining important aspects of neural network generalization. In particular, we demonstrate realistic settings where finite-width neural networks have significantly better data scaling exponents as compared to their corresponding empirical and infinite NTKs at initialization. This reveals a more fundamental difference between the real networks and NTKs, beyond just a few percentage points of test accuracy. Further, we show that even if the empirical NTK is allowed to be pre-trained on a constant number of samples, the kernel scaling does not catch up to the neural network scaling. Finally, we show that the empirical NTK continues to evolve throughout most of the training, in contrast with prior work which suggests that it stabilizes after a few epochs of training. Altogether, our work establishes concrete limitations of the NTK approach in understanding generalization of real networks on natural datasets.},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {Vyas, Nikhil and Bansal, Yamini and Nakkiran, Preetum},
	month = jun,
	year = {2022},
	note = {arXiv:2206.10012 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{yuan_model_2006,
	title = {Model selection and estimation in regression with grouped variables},
	volume = {68},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00532.x},
	doi = {10.1111/j.1467-9868.2005.00532.x},
	abstract = {Summary. We consider the problem of selecting grouped variables (factors) for accurate prediction in regression. Such a problem arises naturally in many practical situations with the multifactor analysis-of-variance problem as the most important and well-known example. Instead of selecting factors by stepwise backward elimination, we focus on the accuracy of estimation and consider extensions of the lasso, the LARS algorithm and the non-negative garrotte for factor selection. The lasso, the LARS algorithm and the non-negative garrotte are recently proposed regression methods that can be used to select individual variables. We study and propose efficient algorithms for the extensions of these methods for factor selection and show that these extensions give superior performance to the traditional stepwise backward elimination method in factor selection problems. We study the similarities and the differences between these methods. Simulations and real examples are used to illustrate the methods.},
	language = {en},
	number = {1},
	urldate = {2023-05-28},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Yuan, Ming and Lin, Yi},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2005.00532.x},
	keywords = {Analysis of variance, Lasso, Least angle regression, Non-negative garrotte, Piecewise linear solution path},
	pages = {49--67},
}

@inproceedings{moody_effective_1991,
	title = {The {Effective} {Number} of {Parameters}: {An} {Analysis} of {Generalization} and {Regularization} in {Nonlinear} {Learning} {Systems}},
	volume = {4},
	shorttitle = {The {Effective} {Number} of {Parameters}},
	url = {https://proceedings.neurips.cc/paper/1991/hash/d64a340bcb633f536d56e51874281454-Abstract.html},
	abstract = {We  present  an  analysis  of how  the  generalization  performance  (expected  test set error) relates to the expected training set  error for  nonlinear learn(cid:173) ing systems, such as multilayer perceptrons and radial basis functions.  The  principal  result  is  the  following  relationship  (computed  to  second  order)  between  the  expected  test  set  and tlaining set  errors:},
	urldate = {2023-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Moody, John},
	year = {1991},
}

@inproceedings{krogh_simple_1991,
	title = {A {Simple} {Weight} {Decay} {Can} {Improve} {Generalization}},
	volume = {4},
	url = {https://proceedings.neurips.cc/paper/1991/hash/8eefcfdf5990e441f0fb6f3fad709e21-Abstract.html},
	abstract = {It has  been observed  in  numerical simulations that a weight decay  can  im(cid:173) prove generalization in a feed-forward  neural network.  This paper explains  why.  It is  proven  that  a  weight  decay  has  two effects  in  a  linear  network.  First,  it  suppresses  any  irrelevant  components  of  the  weight  vector  by  choosing  the smallest  vector  that solves  the learning  problem.  Second,  if  the size  is chosen  right,  a weight  decay  can suppress some of the effects  of  static  noise  on  the  targets,  which  improves  generalization  quite  a  lot.  It  is  then  shown  how  to extend  these  results  to networks  with hidden  layers  and  non-linear  units.  Finally  the  theory  is  confirmed  by  some numerical  simulations using  the  data from  NetTalk.},
	urldate = {2023-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Krogh, Anders and Hertz, John},
	year = {1991},
}

@inproceedings{chizat_lazy_2019,
	title = {On {Lazy} {Training} in {Differentiable} {Programming}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/ae614c557843b1df326cb29c57225459-Abstract.html},
	abstract = {In a series of recent theoretical works, it was shown that strongly over-parameterized neural networks trained with gradient-based methods could converge exponentially fast to zero training loss, with their parameters hardly varying. In this work, we show that this lazy training'' phenomenon is not specific to over-parameterized neural networks, and is due to a choice of scaling, often implicit, that makes the model behave as its linearization around the initialization, thus yielding a model equivalent to learning with positive-definite kernels. Through a theoretical analysis, we exhibit various situations where this phenomenon arises in non-convex optimization and we provide bounds on the distance between the lazy and linearized optimization paths. Our numerical experiments bring a critical note, as we observe that the performance of commonly used non-linear deep convolutional neural networks in computer vision degrades when trained in the lazy regime. This makes it unlikely thatlazy training'' is behind the many successes of neural networks in difficult high dimensional tasks.},
	urldate = {2023-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chizat, Lénaïc and Oyallon, Edouard and Bach, Francis},
	year = {2019},
}

@article{soudry_implicit_2018,
	title = {The implicit bias of gradient descent on separable data},
	volume = {19},
	issn = {1532-4435},
	abstract = {We examine gradient descent on unregularized logistic regression problems, with homogeneous linear predictors on linearly separable datasets. We show the predictor converges to the direction of the max-margin (hard margin SVM) solution. The result also generalizes to other monotone decreasing loss functions with an infimum at infinity, to multi-class problems, and to training a weight layer in a deep network in a certain restricted setting. Furthermore, we show this convergence is very slow, and only logarithmic in the convergence of the loss itself. This can help explain the benefit of continuing to optimize the logistic or cross-entropy loss even after the training error is zero and the training loss is extremely small, and, as we show, even if the validation loss increases. Our methodology can also aid in understanding implicit regularization in more complex models and with other optimization methods.},
	number = {1},
	journal = {The Journal of Machine Learning Research},
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	month = jan,
	year = {2018},
	keywords = {generalization, gradient descent, implicit regularization, logistic regression, margin},
	pages = {2822--2878},
}

@inproceedings{saxe_neural_2022,
	title = {The {Neural} {Race} {Reduction}: {Dynamics} of {Abstraction} in {Gated} {Networks}},
	shorttitle = {The {Neural} {Race} {Reduction}},
	url = {https://proceedings.mlr.press/v162/saxe22a.html},
	abstract = {Our theoretical understanding of deep learning has not kept pace with its empirical success. While network architecture is known to be critical, we do not yet understand its effect on learned representations and network behavior, or how this architecture should reflect task structure.In this work, we begin to address this gap by introducing the Gated Deep Linear Network framework that schematizes how pathways of information flow impact learning dynamics within an architecture. Crucially, because of the gating, these networks can compute nonlinear functions of their input. We derive an exact reduction and, for certain cases, exact solutions to the dynamics of learning. Our analysis demonstrates that the learning dynamics in structured networks can be conceptualized as a neural race with an implicit bias towards shared representations, which then govern the model’s ability to systematically generalize, multi-task, and transfer. We validate our key insights on naturalistic datasets and with relaxed assumptions. Taken together, our work gives rise to general hypotheses relating neural architecture to learning and provides a mathematical approach towards understanding the design of more complex architectures and the role of modularity and compositionality in solving real-world problems. The code and results are available at https://www.saxelab.org/gated-dln.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Saxe, Andrew and Sodhani, Shagun and Lewallen, Sam Jay},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {19287--19309},
}

@inproceedings{chizat_implicit_2020,
	title = {Implicit {Bias} of {Gradient} {Descent} for {Wide} {Two}-layer {Neural} {Networks} {Trained} with the {Logistic} {Loss}},
	url = {https://proceedings.mlr.press/v125/chizat20a.html},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Chizat, Lénaïc and Bach, Francis},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1305--1338},
}

@misc{lyu_gradient_2020,
	title = {Gradient {Descent} {Maximizes} the {Margin} of {Homogeneous} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1906.05890},
	doi = {10.48550/arXiv.1906.05890},
	abstract = {In this paper, we study the implicit regularization of the gradient descent algorithm in homogeneous neural networks, including fully-connected and convolutional neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient descent or gradient flow (i.e., gradient descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain threshold, then we can define a smoothed version of the normalized margin which increases over time. We also formulate a natural constrained optimization problem related to margin maximization, and prove that both the normalized margin and its smoothed version converge to the objective value at a KKT point of the optimization problem. Our results generalize the previous results for logistic regression with one-layer or multi-layer linear networks, and provide more quantitative convergence results with weaker assumptions than previous results for homogeneous smooth neural networks. We conduct several experiments to justify our theoretical finding on MNIST and CIFAR-10 datasets. Finally, as margin is closely related to robustness, we discuss potential benefits of training longer for improving the robustness of the model.},
	urldate = {2023-05-28},
	publisher = {arXiv},
	author = {Lyu, Kaifeng and Li, Jian},
	month = dec,
	year = {2020},
	note = {arXiv:1906.05890 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{gunasekar_implicit_2018,
	title = {Implicit {Bias} of {Gradient} {Descent} on {Linear} {Convolutional} {Networks}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/0e98aeeb54acf612b9eb4e48a269814c-Abstract.html},
	urldate = {2023-05-28},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
	year = {2018},
}

@inproceedings{woodworth_kernel_2020,
	title = {Kernel and {Rich} {Regimes} in {Overparametrized} {Models}},
	url = {https://proceedings.mlr.press/v125/woodworth20a.html},
	abstract = {A recent line of work studies overparametrized neural networks in the “kernel regime,” i.e. when  during training the network behaves as a kernelized linear predictor, and thus, training with gradient descent has the effect of finding the corresponding minimum RKHS norm solution.  This stands in contrast to other studies which demonstrate how gradient descent on overparametrized  networks can induce rich implicit biases that are not RKHS norms.  Building on an observation by {\textbackslash}citet\{chizat2018note\}, we show how the {\textbackslash}textbf\{{\textbackslash}textit\{scale of the initialization\}\} controls the transition between the “kernel” (aka lazy) and “rich” (aka active) regimes and affects generalization properties in multilayer homogeneous models. We provide a complete and detailed analysis for a family of simple depth-DDD linear networks that exhibit an interesting and meaningful transition between the kernel and rich regimes, and highlight an interesting role for the {\textbackslash}emph\{width\}  of the models. We further demonstrate this transition empirically for matrix factorization and multilayer non-linear networks.},
	language = {en},
	urldate = {2023-05-28},
	booktitle = {Proceedings of {Thirty} {Third} {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	month = jul,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {3635--3673},
}
