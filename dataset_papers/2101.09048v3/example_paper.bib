@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}
@inproceedings{mikolov2010recurrent,
  title={Recurrent neural network based language model},
  author={Mikolov, Tom{\'a}{\v{s}} and Karafi{\'a}t, Martin and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  booktitle={Eleventh annual conference of the international speech communication association},
  year={2010}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}
@inproceedings{zilly2017recurrent,
  title={Recurrent highway networks},
  author={Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\'\i}k, Jan and Schmidhuber, J{\"u}rgen},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={4189--4198},
  year={2017},
  organization={JMLR. org}
}
@inproceedings{kim2016character,
  title={Character-aware neural language models},
  author={Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M},
  booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
  year={2016}
}
@inproceedings{grave2016improving,
  title={Improving neural language models with a continuous cache},
  author={Grave, Edouard and Joulin, Armand and Usunier, Nicolas},
  booktitle={International Conference on Learning Representations},
  year={2017},
  url={https://openreview.net/forum?id=B184E5qee},
}
@inproceedings{dai2018fast,
  title={Fast training and model compression of gated RNNs via singular value decomposition},
  author={Dai, Rui and Li, Lefei and Yu, Wenjian},
  booktitle={2018 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--7},
  year={2018},
  organization={IEEE}
}
@inproceedings{inan2016tying,
  title={Tying word vectors and word classifiers: A loss framework for language modeling},
  author={Inan, Hakan and Khosravi, Khashayar and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017},
}
@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}
@article{zoph2016neural,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}
@article{shen2017neural,
  title={Neural language modeling by jointly learning syntax and lexicon},
  author={Shen, Yikang and Lin, Zhouhan and Huang, Chin-Wei and Courville, Aaron},
  journal={arXiv preprint arXiv:1711.02013},
  year={2017}
}
@inproceedings{merity2017pointer,
  title={Pointer sentinel mixture models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  booktitle={International Conference on Learning Representations},
  year={2017},
}
@inproceedings{
melis2018on,
title={On the State of the Art of Evaluation in Neural Language Models},
author={Gábor Melis and Chris Dyer and Phil Blunsom},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ByJHuTgA-},
}
@inproceedings{
merity2018regularizing,
title={Regularizing and Optimizing {LSTM} Language Models},
author={Stephen Merity and Nitish Shirish Keskar and Richard Socher},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=SyyGPP0TZ},
}
@inproceedings{
shen2018ordered,
title={Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks},
author={Yikang Shen and Shawn Tan and Alessandro Sordoni and Aaron Courville},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1l6qiR5F7},
}
@article{zaremba2014recurrent,
  title={Recurrent neural network regularization},
  author={Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1409.2329},
  year={2014}
}
@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}
@inproceedings{
frankle2018the,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}
@inproceedings{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  booktitle={International Conference on Learning Representations},
  year={2017},
}
@article{zhu2017prune,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Zhu, Michael and Gupta, Suyog},
  journal={arXiv preprint arXiv:1710.01878},
  year={2017}
}
@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}
@inproceedings{phuong2019towards,
  title={Towards Understanding Knowledge Distillation},
  author={Phuong, Mary and Lampert, Christoph},
  booktitle={International Conference on Machine Learning},
  pages={5142--5151},
  year={2019}
}
@inproceedings{krause2018dynamic,
  title={Dynamic evaluation of neural sequence models},
  author={Krause, Ben and Kahembwe, Emmanuel and Murray, Iain and Renals, Steve},
  booktitle={International Conference on Machine Learning},
  pages={2766--2775},
  year={2018}
}
@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}
@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={arXiv preprint arXiv:1611.06440},
  year={2016}
}
@article{de2020progressive,
  title={Progressive skeletonization: Trimming more fat from a network at initialization},
  author={de Jorge, Pau and Sanyal, Amartya and Behl, Harkirat S and Torr, Philip HS and Rogez, Gregory and Dokania, Puneet K},
  journal={arXiv preprint arXiv:2006.09081},
  year={2020}
}
@inproceedings{louizos2017learning,
  title={Learning Sparse Neural Networks through $ L\_0 $ Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  booktitle={International Conference on Learning Representations},
  year={2017}
}
@inproceedings{srivastava2015training,
  title={Training very deep networks},
  author={Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  booktitle={Advances in neural information processing systems},
  pages={2377--2385},
  year={2015}
}
@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}
@article{marcus1993building,
  title={Building a large annotated corpus of English: The Penn Treebank},
  author={Marcus, Mitchell and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
  year={1993}
}
@inproceedings{graves2013hybrid,
  title={Hybrid speech recognition with deep bidirectional LSTM},
  author={Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  booktitle={Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on},
  pages={273--278},
  year={2013},
  organization={IEEE}
}
@inproceedings{
Lee2020A,
title={A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Stephen Gould and Philip H. S. Torr},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=HJeTo2VFwH}
}
@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={ICLR},
  year={2014}
}
@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}
@inproceedings{yang2016hierarchical,
  title={Hierarchical attention networks for document classification},
  author={Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  booktitle={Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={1480--1489},
  year={2016}
}
@inproceedings{
wen2018learning,
title={Learning Intrinsic Sparse Structures within Long Short-Term Memory},
author={Wei Wen and Yuxiong He and Samyam Rajbhandari and Minjia Zhang and Wenhan Wang and Fang Liu and Bin Hu and Yiran Chen and Hai Li},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=rk6cfpRjZ},
}
@article{lobacheva2017bayesian,
  title={Bayesian Sparsification of Recurrent Neural Networks},
  author={Lobacheva, Ekaterina and Chirkova, Nadezhda and Vetrov, Dmitry},
  journal={arXiv preprint arXiv:1708.00077},
  year={2017}
}
@article{tian2017deep,
  title={Deep LSTM for large vocabulary continuous speech recognition},
  author={Tian, Xu and Zhang, Jun and Ma, Zejun and He, Yi and Wei, Juan and Wu, Peihao and Situ, Wenchang and Li, Shuai and Zhang, Yang},
  journal={arXiv preprint arXiv:1703.07090},
  year={2017}
}
@article{lu2016learning,
  title={Learning compact recurrent neural networks},
  author={Lu, Zhiyun and Sindhwani, Vikas and Sainath, Tara N},
  journal={arXiv preprint arXiv:1604.02594},
  year={2016}
}
@inproceedings{han2017ese,
  title={Ese: Efficient speech recognition engine with sparse lstm on fpga},
  author={Han, Song and Kang, Junlong and Mao, Huizi and Hu, Yiming and Li, Xin and Li, Yubin and Xie, Dongliang and Luo, Hong and Yao, Song and Wang, Yu and others},
  booktitle={Proceedings of the 2017 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
  pages={75--84},
  year={2017},
  organization={ACM}
}
@inproceedings{
bellec2018deep,
title={Deep Rewiring: Training very sparse deep networks},
author={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=BJ_wN01C-},
}
@inproceedings{jouppi2017datacenter,
  title={In-datacenter performance analysis of a tensor processing unit},
  author={Jouppi, Norman P and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and others},
  booktitle={Computer Architecture (ISCA), 2017 ACM/IEEE 44th Annual International Symposium on},
  pages={1--12},
  year={2017},
  organization={IEEE}
}
@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature Communications},
  volume={9},
  number={1},
  pages={2383},
  year={2018},
  publisher={Nature Publishing Group}
}
@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}
@inproceedings{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  booktitle={Advances in neural information processing systems},
  pages={164--171},
  year={1993}
}
@article{giles1994pruning,
  title={Pruning recurrent neural networks for improved generalization performance},
  author={Giles, C Lee and Omlin, Christian W},
  journal={IEEE transactions on neural networks},
  volume={5},
  number={5},
  pages={848--851},
  year={1994},
  publisher={IEEE}
}
@inproceedings{lebedev2016fast,
  title={Fast convnets using group-wise brain damage},
  author={Lebedev, Vadim and Lempitsky, Victor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2554--2564},
  year={2016}
}
@article{anwar2017structured,
  title={Structured pruning of deep convolutional neural networks},
  author={Anwar, Sajid and Hwang, Kyuyeon and Sung, Wonyong},
  journal={ACM Journal on Emerging Technologies in Computing Systems (JETC)},
  volume={13},
  number={3},
  pages={32},
  year={2017},
  publisher={ACM}
}
@article{changpinyo2017power,
  title={The power of sparsity in convolutional neural networks},
  author={Changpinyo, Soravit and Sandler, Mark and Zhmoginov, Andrey},
  journal={arXiv preprint arXiv:1702.06257},
  year={2017}
}
@article{dai2017nest,
  title={NeST: a neural network synthesis tool based on a grow-and-prune paradigm},
  author={Dai, Xiaoliang and Yin, Hongxu and Jha, Niraj K},
  journal={arXiv preprint arXiv:1711.02017},
  year={2017}
}
}@article{kuchaiev2017factorization,
  title={Factorization tricks for LSTM networks},
  author={Kuchaiev, Oleksii and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1703.10722},
  year={2017}
}
@article{zen2016fast,
  title={Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices},
  author={Zen, Heiga and Agiomyrgiannakis, Yannis and Egberts, Niels and Henderson, Fergus and Szczepaniak, Przemys{\l}aw},
  journal={arXiv preprint arXiv:1606.06061},
  year={2016}
}
@inproceedings{li2015word,
  title={Word Embedding R@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1},
  pages={142--150},
  year={2011},
  organization={Association for Computational Linguistics}
}evisited: A New Representation Learning and Explicit Matrix Factorization Perspective.},
  author={Li, Yitan and Xu, Linli and Tian, Fei and Jiang, Liang and Zhong, Xiaowei and Chen, Enhong},
  booktitle={IJCAI},
  pages={3650--3656},
  year={2015}
}
@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}
@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew L and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1},
  pages={142--150},
  year={2011},
  organization={Association for Computational Linguistics}
}
@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}
@inproceedings{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in neural information processing systems},
  pages={649--657},
  year={2015}
}
@article{joulin2016bag,
  title={Bag of tricks for efficient text classification},
  author={Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal={arXiv preprint arXiv:1607.01759},
  year={2016}
}
@article{zhang2015text,
  title={Text understanding from scratch},
  author={Zhang, Xiang and LeCun, Yann},
  journal={arXiv preprint arXiv:1502.01710},
  year={2015}
}
@article{xiao2016efficient,
  title={Efficient character-level document classification by combining convolution and recurrent layers},
  author={Xiao, Yijun and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:1602.00367},
  year={2016}
}
@article{conneau2016very,
  title={Very deep convolutional networks for natural language processing},
  author={Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\i}c and Lecun, Yann},
  journal={arXiv preprint arXiv:1606.01781},
  volume={2},
  year={2016}
}
@inproceedings{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@Article{Mocanu2016xbm,
author="Mocanu, Decebal Constantin
and Mocanu, Elena
and Nguyen, Phuong H.
and Gibescu, Madeleine
and Liotta, Antonio",
title="A topological insight into restricted Boltzmann machines",
journal="Machine Learning",
year="2016",
month="Sep",
day="01",
volume="104",
number="2",
pages="243--270",
abstract="Restricted Boltzmann Machines (RBMs) and models derived from them have been successfully used as basic building blocks in deep artificial neural networks for automatic features extraction, unsupervised weights initialization, but also as density estimators. Thus, their generative and discriminative capabilities, but also their computational time are instrumental to a wide range of applications. Our main contribution is to look at RBMs from a topological perspective, bringing insights from network science. Firstly, here we show that RBMs and Gaussian RBMs (GRBMs) are bipartite graphs which naturally have a small-world topology. Secondly, we demonstrate both on synthetic and real-world datasets that by constraining RBMs and GRBMs to a scale-free topology (while still considering local neighborhoods and data distribution), we reduce the number of weights that need to be computed by a few orders of magnitude, at virtually no loss in generative performance. Thirdly, we show that, for a fixed number of weights, our proposed sparse models (which by design have a higher number of hidden neurons) achieve better generative capabilities than standard fully connected RBMs and GRBMs (which by design have a smaller number of hidden neurons), at no additional computational costs.",
issn="1573-0565",
doi="10.1007/s10994-016-5570-z",
url="https://doi.org/10.1007/s10994-016-5570-z"
}

@phdthesis{decphdthesis,
title = "Network computations in artificial intelligence - Chapter 5",
author = "Decebal Constantin Mocanu",
year = "2017",
month = "June",
day = "29",
language = "English",
isbn = "978-90-386-4305-2",
school = "Eindhoven University of Technology",
url= "https://pure.tue.nl/ws/portalfiles/portal/69949254",
}

@article{lowmemory2019,
  title={Low-Memory Neural Network Training: A Technical Report},
  author={Nimit Sharad Sohoni, Christopher Richard Aberger, Megan Leszczynski, Jian Zhang, Christopher R{\e}},
  journal={arXiv preprint arxiv.org/abs/1904.10631},
  year={2019}
}

@article{lstmbigo,
  author    = {Hasim Sak and
               Andrew W. Senior and
               Fran{\c{c}}oise Beaufays},
  title     = {Long Short-Term Memory Based Recurrent Neural Network Architectures
               for Large Vocabulary Speech Recognition},
  journal   = {CoRR},
  volume    = {abs/1402.1128},
  year      = {2014},
  archivePrefix = {arXiv},
  eprint    = {1402.1128},
}

@inproceedings{xingjian2015convolutional,
  title={Convolutional LSTM network: A machine learning approach for precipitation nowcasting},
  author={Xingjian, SHI and Chen, Zhourong and Wang, Hao and Yeung, Dit-Yan and Wong, Wai-Kin and Woo, Wang-chun},
  booktitle={Advances in neural information processing systems},
  pages={802--810},
  year={2015}
}
@article{dettmers2019sparse,
  title={Sparse networks from scratch: Faster training without losing performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}
@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}
@inproceedings{moon2015rnndrop,
  title={Rnndrop: A novel dropout for rnns in asr},
  author={Moon, Taesup and Choi, Heeyoul and Lee, Hoshik and Song, Inchul},
  booktitle={2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)},
  pages={65--70},
  year={2015},
  organization={IEEE}
}
@article{semeniuta2016recurrent,
  title={Recurrent dropout without memory loss},
  author={Semeniuta, Stanislau and Severyn, Aliaksei and Barth, Erhardt},
  journal={arXiv preprint arXiv:1603.05118},
  year={2016}
}
@inproceedings{gal2016theoretically,
  title={A theoretically grounded application of dropout in recurrent neural networks},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={Advances in neural information processing systems},
  pages={1019--1027},
  year={2016}
}
@article{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  journal={arXiv preprint arXiv:1502.03167},
  year={2015}
}
@article{cooijmans2016recurrent,
  title={Recurrent batch normalization},
  author={Cooijmans, Tim and Ballas, Nicolas and Laurent, C{\'e}sar and G{\"u}l{\c{c}}ehre, {\c{C}}a{\u{g}}lar and Courville, Aaron},
  journal={arXiv preprint arXiv:1603.09025},
  year={2016}
}
@misc{
h.2018to,
title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
author={Michael H. Zhu and Suyog Gupta},
year={2018},
url={https://openreview.net/forum?id=S1lN69AT-},
}
@inproceedings{zhou2019deconstructing,
  title={Deconstructing lottery tickets: Zeros, signs, and the supermask},
  author={Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3597--3607},
  year={2019}
}
@inproceedings{
yang2018breaking,
title={Breaking the Softmax Bottleneck: A High-Rank {RNN} Language Model},
author={Zhilin Yang and Zihang Dai and Ruslan Salakhutdinov and William W. Cohen},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=HkwZSG-CZ},
}
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM Journal on Control and Optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
@article{xu2011towards,
  title={Towards optimal one pass large scale learning with averaged stochastic gradient descent},
  author={Xu, Wei},
  journal={arXiv preprint arXiv:1107.2490},
  year={2011}
}
@inproceedings{zhang2004solving,
  title={Solving large scale linear prediction problems using stochastic gradient descent algorithms},
  author={Zhang, Tong},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={116},
  year={2004},
  organization={ACM}
}
@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}
@article{loshchilov2016sgdr,
  title={Sgdr: Stochastic gradient descent with warm restarts},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1608.03983},
  year={2016}
}
@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={arXiv preprint arXiv:1810.05270},
  year={2018}
}
@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}
@inproceedings{strom1997sparse,
  title={Sparse connection and pruning in large dynamic artificial neural networks},
  author={Str{\"o}m, Nikko},
  booktitle={Fifth European Conference on Speech Communication and Technology},
  year={1997}
}
@inproceedings{guo2016dynamic,
  title={Dynamic network surgery for efficient dnns},
  author={Guo, Yiwen and Yao, Anbang and Chen, Yurong},
  booktitle={Advances In Neural Information Processing Systems},
  pages={1379--1387},
  year={2016}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}
@inproceedings{evci2019rigging,
  title={Rigging the Lottery: Making All Tickets Winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  year={2020}
}
@article{press2016using,
  title={Using the output embedding to improve language models},
  author={Press, Ofir and Wolf, Lior},
  journal={arXiv preprint arXiv:1608.05859},
  year={2016}
}
@article{sanfeliu1983distance,
  title={A distance measure between attributed relational graphs for pattern recognition},
  author={Sanfeliu, Alberto and Fu, King-Sun},
  journal={IEEE transactions on systems, man, and cybernetics},
  number={3},
  pages={353--362},
  year={1983},
  publisher={IEEE}
}
@inproceedings{li2016convergent,
  title={Convergent Learning: Do different neural networks learn the same representations?},
  author={Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John E},
  booktitle={Iclr},
  year={2016}
}
@article{liu2019intrinsically,
  title={Intrinsically sparse long short-term memory networks},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
  journal={arXiv preprint arXiv:1901.09208},
  year={2019}
}
@inproceedings{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@inproceedings{bonawitz2017practical,
  title={Practical secure aggregation for privacy-preserving machine learning},
  author={Bonawitz, Keith and Ivanov, Vladimir and Kreuter, Ben and Marcedone, Antonio and McMahan, H Brendan and Patel, Sarvar and Ramage, Daniel and Segal, Aaron and Seth, Karn},
  booktitle={Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security},
  pages={1175--1191},
  year={2017}
}
@inproceedings{zinkevich2003online,
  title={Online convex programming and generalized infinitesimal gradient ascent},
  author={Zinkevich, Martin},
  booktitle={Proceedings of the 20th international conference on machine learning (icml-03)},
  pages={928--936},
  year={2003}
}
@article{affolter2019applying,
  title={Applying augmented reality during a forensic autopsy—Microsoft HoloLens as a DICOM viewer},
  author={Affolter, Raffael and Eggert, Sebastian and Sieberth, Till and Thali, Michael and Ebert, Lars Christian},
  journal={Journal of Forensic Radiology and Imaging},
  volume={16},
  pages={5--8},
  year={2019},
  publisher={Elsevier}
}
@article{wang2019usability,
  title={Usability evaluation of an instructional application based on Google Glass for mobile phone disassembly tasks},
  author={Wang, Chao-Hung and Tsai, Ni-Hsin and Lu, Jun-Ming and Wang, Mao-Jiun J},
  journal={Applied ergonomics},
  volume={77},
  pages={58--69},
  year={2019},
  publisher={Elsevier}
}
@article{NVIDIA2020,
  title={NVIDIA A100 Tensor Core GPU Architecture},
  author={NVIDIA},
  journal={Retrieved from: https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  year={2020},
}
@article{moradi2019sparsemaps,
  title={Sparsemaps: convolutional networks with sparse feature maps for tiny image classification},
  author={Moradi, Reza and Berangi, Reza and Minaei, Behrouz},
  journal={Expert Systems with Applications},
  volume={119},
  pages={142--154},
  year={2019},
  publisher={Elsevier}
}
@article{ma2019transformed,
  title={Transformed l1 regularization for learning sparse deep neural networks},
  author={Ma, Rongrong and Miao, Jianyu and Niu, Lingfeng and Zhang, Peng},
  journal={Neural Networks},
  volume={119},
  pages={286--298},
  year={2019},
  publisher={Elsevier}
}
@article{yang2019feed,
  title={Feed-forward neural network training using sparse representation},
  author={Yang, Jie and Ma, Jun},
  journal={Expert Systems with Applications},
  volume={116},
  pages={255--264},
  year={2019},
  publisher={Elsevier}
}
@article{liu2020sparse,
  title={Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Matavalam, Amarsagar Reddy Ramapuram and Pei, Yulong and Pechenizkiy, Mykola},
  journal={Neural Computing and Applications},
  pages={1--16},
  year={2020},
  publisher={Springer}
}
@inproceedings{wang2017machine,
  title={Machine comprehension using match-lstm and answer pointer},
  author={Wang, Shuohang and Jiang, Jing},
  booktitle={International Conference on Learning Representations},
  year={2017},
}
@inproceedings{louizos2017bayesian,
  title={Bayesian compression for deep learning},
  author={Louizos, Christos and Ullrich, Karen and Welling, Max},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3288--3298},
  year={2017}
}
@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2498--2507},
  year={2017},
  organization={JMLR. org}
}
@inproceedings{kalchbrenner2013recurrent,
  title={Recurrent continuous translation models},
  author={Kalchbrenner, Nal and Blunsom, Phil},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1700--1709},
  year={2013}
}
@article{erdos1959random,
  title={On random graphs},
  author={Erd{\"o}s, Paul and R{\'e}nyi, Alfr{\'e}d and others},
  journal={Publicationes mathematicae},
  volume={6},
  number={26},
  pages={290--297},
  year={1959}
}
@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  journal={arXiv preprint arXiv:2006.05467},
  year={2020}
}
@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={International Conference on Machine Learning},
  pages={5544--5555},
  year={2020},
  organization={PMLR}
}
@inproceedings{
lee2018snip,
title={{SNIP}: {SINGLE}-{SHOT} {NETWORK} {PRUNING} {BASED} {ON} {CONNECTION} {SENSITIVITY}},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip Torr},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1VZqjAcYX},
}
@inproceedings{
Wang2020Picking,
title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgsACVKPH}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@inproceedings{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@article{xiao2019autoprune,
  title={Autoprune: Automatic network pruning by regularizing auxiliary parameters},
  author={Xiao, Xia and Wang, Zigeng and Rajasekaran, Sanguthevar},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{kingma2015variational,
  title={Variational dropout and the local reparameterization trick},
  author={Kingma, Diederik P and Salimans, Tim and Welling, Max},
  journal={arXiv preprint arXiv:1506.02557},
  year={2015}
}
@article{melis2017state,
  title={On the state of the art of evaluation in neural language models},
  author={Melis, G{\'a}bor and Dyer, Chris and Blunsom, Phil},
  journal={arXiv preprint arXiv:1707.05589},
  year={2017}
}
@inproceedings{prabhu2018deep,
  title={Deep expander networks: Efficient deep networks from graph theory},
  author={Prabhu, Ameya and Varma, Girish and Namboodiri, Anoop},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={20--35},
  year={2018}
}
@inproceedings{kepner2019radix,
  title={Radix-net: Structured sparse matrices for deep neural networks},
  author={Kepner, Jeremy and Robinett, Ryan},
  booktitle={2019 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={268--274},
  year={2019},
  organization={IEEE}
}
@inproceedings{
LIU2020Dynamic,
title={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},
author={Junjie Liu and Zhe XU and Runbin SHI and Ray C. C. Cheung and Hayden K.H. So},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJlbGJrtDB}
}
@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis}
}
@inproceedings{liu2020topological,
  title={Topological Insights into Sparse Neural Networks},
  author={Liu, Shiwei and van der Lee, TT and Yaman, Anil and Atashgahi, Zahra and Ferrar, D and Sokar, Ghada and Pechenizkiy, Mykola and Mocanu, DC},
  booktitle={Joint European Conference on
Machine Learning and Knowledge Discovery in Databases},
  year={2020}
}
@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8789--8798},
  year={2018}
}
@article{goodfellow2014qualitatively,
  title={Qualitatively characterizing neural network optimization problems},
  author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1412.6544},
  year={2014}
}
@inproceedings{fort2019large,
  title={Large scale structure of neural network loss landscapes},
  author={Fort, Stanislav and Jastrzebski, Stanislaw},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6709--6717},
  year={2019}
}
@article{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A},
  journal={arXiv preprint arXiv:1803.00885},
  year={2018}
}
@article{gray2017gpu,
  title={Gpu kernels for block-sparse weights},
  author={Gray, Scott and Radford, Alec and Kingma, Diederik P},
  journal={arXiv preprint arXiv:1711.09224},
  volume={3},
  year={2017}
}
@inproceedings{kalchbrenner2018efficient,
  title={Efficient neural audio synthesis},
  author={Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and Oord, Aaron van den and Dieleman, Sander and Kavukcuoglu, Koray},
  booktitle={International Conference on Learning Representations},
  year={2018},
}
@inproceedings{hirschman1999deep,
  title={Deep read: A reading comprehension system},
  author={Hirschman, Lynette and Light, Marc and Breck, Eric and Burger, John D},
  booktitle={Proceedings of the 37th annual meeting of the Association for Computational Linguistics},
  pages={325--332},
  year={1999}
}
@article{jayakumar2020top,
  title={Top-KAST: Top-K Always Sparse Training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}
@article{raihan2020sparse,
  title={Sparse weight activation training},
  author={Raihan, Md Aamir and Aamodt, Tor M},
  journal={arXiv preprint arXiv:2001.01969},
  year={2020}
}