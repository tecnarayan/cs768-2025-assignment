\begin{thebibliography}{}

\bibitem[Allen-Zhu et~al., 2018]{allen2018convergence}
Allen-Zhu, Z., Li, Y., and Song, Z. (2018).
\newblock On the convergence rate of training recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1810.12065}.

\bibitem[Bahdanau et~al., 2014]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y. (2014).
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock {\em arXiv preprint arXiv:1409.0473}.

\bibitem[Bertschinger and Natschl{\"a}ger, 2004]{bertschinger2004real}
Bertschinger, N. and Natschl{\"a}ger, T. (2004).
\newblock Real-time computation at the edge of chaos in recurrent neural
  networks.
\newblock {\em Neural computation}, 16(7):1413--1436.

\bibitem[Bertsekas and Tsitsiklis, 2008]{bertsekas2008introduction}
Bertsekas, D.~P. and Tsitsiklis, J.~N. (2008).
\newblock {\em Introduction to probability}, volume~1.
\newblock Athena Scientific.

\bibitem[Chatziafratis et~al., 2020a]{chatziafratis2020better}
Chatziafratis, V., Nagarajan, S.~G., and Panageas, I. (2020a).
\newblock Better depth-width trade-offs for neural networks through the lens of
  dynamical systems.
\newblock In {\em International Conference on Machine Learning}, pages
  1469--1478. PMLR.

\bibitem[Chatziafratis et~al., 2020b]{iclr}
Chatziafratis, V., Nagarajan, S.~G., Panageas, I., and Wang, X. (2020b).
\newblock Depth-width trade-offs for relu networks via sharkovsky's theorem.
\newblock {\em International Conference on Learning Representations, Addis
  Ababa, Africa}.

\bibitem[Cho et~al., 2014]{cho2014learning}
Cho, K., Van~Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
  Schwenk, H., and Bengio, Y. (2014).
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}.

\bibitem[Chung et~al., 2014]{chung2014empirical}
Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014).
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock {\em arXiv preprint arXiv:1412.3555}.

\bibitem[Collet and Eckmann, 2009]{collet2009iterated}
Collet, P. and Eckmann, J.-P. (2009).
\newblock {\em Iterated maps on the interval as dynamical systems}.
\newblock Springer Science \& Business Media.

\bibitem[Eldan and Shamir, 2016]{eldan2016COLT}
Eldan, R. and Shamir, O. (2016).
\newblock The power of depth for feedforward neural networks.
\newblock In {\em Conference on learning theory}, pages 907--940.

\bibitem[Glorot and Bengio, 2010]{glorot2010understanding}
Glorot, X. and Bengio, Y. (2010).
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256. JMLR Workshop and
  Conference Proceedings.

\bibitem[Hanin, 2018]{hanin2018neural}
Hanin, B. (2018).
\newblock Which neural net architectures give rise to exploding and vanishing
  gradients?
\newblock {\em arXiv preprint arXiv:1801.03744}.

\bibitem[Hanin et~al., 2021]{hanin2021deep}
Hanin, B., Jeong, R., and Rolnick, D. (2021).
\newblock Deep relu networks preserve expected length.
\newblock {\em arXiv preprint arXiv:2102.10492}.

\bibitem[Hanin and Rolnick, 2019a]{hanin2019complexity}
Hanin, B. and Rolnick, D. (2019a).
\newblock Complexity of linear regions in deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2596--2604. PMLR.

\bibitem[Hanin and Rolnick, 2019b]{hanin2019deep}
Hanin, B. and Rolnick, D. (2019b).
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  359--368.

\bibitem[He et~al., 2015]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J. (2015).
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[Hochreiter and Schmidhuber, 1997]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J. (1997).
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780.

\bibitem[Huang and Ye, 2002]{huang2002devaney}
Huang, W. and Ye, X. (2002).
\newblock Devaney's chaos or 2-scattering implies li--yorke's chaos.
\newblock {\em Topology and its Applications}, 117(3):259--272.

\bibitem[Kadmon and Sompolinsky, 2015]{kadmon2015transition}
Kadmon, J. and Sompolinsky, H. (2015).
\newblock Transition to chaos in random neuronal networks.
\newblock {\em Physical Review X}, 5(4):041030.

\bibitem[Laurent and von Brecht, 2016]{laurent2016recurrent}
Laurent, T. and von Brecht, J. (2016).
\newblock A recurrent neural network without chaos.
\newblock {\em ICLR}.

\bibitem[Le et~al., 2015]{le2015simple}
Le, Q.~V., Jaitly, N., and Hinton, G.~E. (2015).
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock {\em arXiv preprint arXiv:1504.00941}.

\bibitem[Li and Yorke, 1975]{li1975period}
Li, T.-Y. and Yorke, J.~A. (1975).
\newblock Period three implies chaos.
\newblock {\em The American Mathematical Monthly}, 82(10):985--992.

\bibitem[Miller and Hardt, 2018]{miller2018stable}
Miller, J. and Hardt, M. (2018).
\newblock Stable recurrent models.
\newblock {\em arXiv preprint arXiv:1805.10369}.

\bibitem[Montufar et~al., 2014]{montufar2014number}
Montufar, G.~F., Pascanu, R., Cho, K., and Bengio, Y. (2014).
\newblock On the number of linear regions of deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  2924--2932.

\bibitem[Poole et~al., 2016]{poole2016NIPS}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S. (2016).
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In {\em Advances in neural information processing systems}, pages
  3360--3368.

\bibitem[Sanford and Chatziafratis, 2022]{sanford2022expressivity}
Sanford, C.~H. and Chatziafratis, V. (2022).
\newblock Expressivity of neural networks via chaotic itineraries beyond
  sharkovskyâ€™s theorem.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 9505--9549. PMLR.

\bibitem[Saxe et~al., 2013]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S. (2013).
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}.

\bibitem[Sharkovsky, 1964]{sharkovsky1964coexistence}
Sharkovsky, O. (1964).
\newblock Coexistence of the cycles of a continuous mapping of the line into
  itself.
\newblock {\em Ukrainskij matematicheskij zhurnal}, 16(01):61--71.

\bibitem[Sompolinsky et~al., 1988]{sompolinsky1988chaos}
Sompolinsky, H., Crisanti, A., and Sommers, H.-J. (1988).
\newblock Chaos in random neural networks.
\newblock {\em Physical review letters}, 61(3):259.

\bibitem[Telgarsky, 2015]{telgarsky15}
Telgarsky, M. (2015).
\newblock Representation benefits of deep feedforward networks.
\newblock {\em arXiv preprint arXiv:1509.08101}.

\bibitem[Telgarsky, 2016]{Telgarsky16}
Telgarsky, M. (2016).
\newblock Benefits of depth in neural networks.
\newblock In {\em Conference on Learning Theory}, pages 1517--1539.

\bibitem[Yang and Schoenholz, 2017]{yang2017mean}
Yang, G. and Schoenholz, S.~S. (2017).
\newblock Mean field residual networks: On the edge of chaos.
\newblock {\em arXiv preprint arXiv:1712.08969}.

\end{thebibliography}
