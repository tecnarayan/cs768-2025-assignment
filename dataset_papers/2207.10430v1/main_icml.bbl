\begin{thebibliography}{103}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmed et~al.(2016)Ahmed, Baig, and Torresani]{ahmed2016network}
Ahmed, K., Baig, M.~H., and Torresani, L.
\newblock Network of experts for large-scale image categorization.
\newblock In \emph{European Conference On Computer Vision}, pp.\  516--532.
  Springer, 2016.

\bibitem[Amodei et~al.(2016)Amodei, Ananthanarayanan, Anubhai, Bai, Battenberg,
  Case, Casper, Catanzaro, Chen, Chrzanowski, Coates, Diamos, Elsen, Engel,
  Fan, Fougner, Hannun, Jun, Han, LeGresley, Li, Lin, Narang, Ng, Ozair,
  Prenger, Qian, Raiman, Satheesh, Seetapun, Sengupta, Wang, Wang, Wang, Xiao,
  Xie, Yogatama, Zhan, and Zhu]{amodei2016deep}
Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case,
  C., Casper, J., Catanzaro, B., Chen, J., Chrzanowski, M., Coates, A., Diamos,
  G., Elsen, E., Engel, J.~H., Fan, L., Fougner, C., Hannun, A.~Y., Jun, B.,
  Han, T., LeGresley, P., Li, X., Lin, L., Narang, S., Ng, A.~Y., Ozair, S.,
  Prenger, R., Qian, S., Raiman, J., Satheesh, S., Seetapun, D., Sengupta, S.,
  Wang, C., Wang, Y., Wang, Z., Xiao, B., Xie, Y., Yogatama, D., Zhan, J., and
  Zhu, Z.
\newblock Deep speech 2 : End-to-end speech recognition in english and
  mandarin.
\newblock In Balcan, M. and Weinberger, K.~Q. (eds.), \emph{Proceedings of the
  33nd International Conference on Machine Learning, {ICML} 2016, New York
  City, NY, USA, June 19-24, 2016}, volume~48 of \emph{{JMLR} Workshop and
  Conference Proceedings}, pp.\  173--182. JMLR.org, 2016.
\newblock URL \url{http://proceedings.mlr.press/v48/amodei16.html}.

\bibitem[Andreas et~al.(2017)Andreas, Klein, and Levine]{andreas2017modular}
Andreas, J., Klein, D., and Levine, S.
\newblock Modular multitask reinforcement learning with policy sketches.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pp.\  166--175. {PMLR}, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/andreas17a.html}.

\bibitem[Arora et~al.(2020)Arora, Arora, Bruna, Cohen, Du, Ge, Gunasekar, Jin,
  Lee, Ma, and Others]{arora2020theory}
Arora, R., Arora, S., Bruna, J., Cohen, N., Du, S., Ge, R., Gunasekar, S., Jin,
  C., Lee, J., Ma, T., and Others.
\newblock Theory of deep learning, 2020.

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  244--253. {PMLR},
  2018.
\newblock URL \url{http://proceedings.mlr.press/v80/arora18a.html}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Cohen, Golowich, and
  Hu]{arora2018convergence}
Arora, S., Cohen, N., Golowich, N., and Hu, W.
\newblock A convergence analysis of gradient descent for deep linear neural
  networks.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net,
  2019{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=SkMQg3C5K7}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Cohen, Hu, and
  Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  7411--7422, 2019{\natexlab{b}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/c0c783b5fc0d7d808f1d14a6e9c8280d-Paper.pdf}.

\bibitem[Arora et~al.(2019{\natexlab{c}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora_exact_2019}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R., and Wang, R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  8139--8148, 2019{\natexlab{c}}.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/dbc4d84bfcfe2284ba11beffb853a8c4-Paper.pdf}.

\bibitem[Auda \& Kamel(1999)Auda and Kamel]{auda1999modular}
Auda, G. and Kamel, M.
\newblock Modular neural networks: A survey.
\newblock \emph{International Journal Of Neural Systems}, 9\penalty0
  (02):\penalty0 129--151, 1999.

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.
\newblock wav2vec 2.0: {A} framework for self-supervised learning of speech
  representations.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/92d1e1eb1cd6f9fba3227870bb6d7f07-Paper.pdf}.

\bibitem[Bahdanau et~al.(2019{\natexlab{a}})Bahdanau, de~Vries, O'Donnell,
  Murty, Beaudoin, Bengio, and Courville]{bahdanau2019closure}
Bahdanau, D., de~Vries, H., O'Donnell, T.~J., Murty, S., Beaudoin, P., Bengio,
  Y., and Courville, A.
\newblock Closure: Assessing systematic generalization of clevr models.
\newblock \emph{arXiv preprint arXiv:1912.05783}, 2019{\natexlab{a}}.

\bibitem[Bahdanau et~al.(2019{\natexlab{b}})Bahdanau, Murty, Noukhovitch,
  Nguyen, de~Vries, and Courville]{bahdanau2018systematic}
Bahdanau, D., Murty, S., Noukhovitch, M., Nguyen, T.~H., de~Vries, H., and
  Courville, A.~C.
\newblock Systematic generalization: What is required and can it be learned?
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net,
  2019{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=HkezXnA9YX}.

\bibitem[Bahri et~al.(2020)Bahri, Kadmon, Pennington, Schoenholz,
  Sohl-dickstein, and Ganguli]{bahri_statistical_2020}
Bahri, Y., Kadmon, J., Pennington, J., Schoenholz, S.~S., Sohl-dickstein, J.,
  and Ganguli, S.
\newblock Statistical {Mechanics} of {Deep} {Learning}.
\newblock \emph{Annual Review Of Condensed Matter Physics}, 11\penalty0
  (1):\penalty0 501--528, 2020.
\newblock \doi{10.1146/annurev-conmatphys-031119-050745}.
\newblock \_eprint: Https://doi.org/10.1146/annurev-conmatphys-031119-050745.

\bibitem[Baldi \& Hornik(1989)Baldi and Hornik]{Baldi1989}
Baldi, P. and Hornik, K.
\newblock Neural networks and principal component analysis: {Learning} from
  examples without local minima.
\newblock \emph{Neural Networks}, 2\penalty0 (1):\penalty0 53--58, 1989.
\newblock ISSN 08936080.
\newblock \doi{10.1016/0893-6080(89)90014-2}.
\newblock URL
  \url{http://linkinghub.elsevier.com/retrieve/pii/0893608089900142}.

\bibitem[Boccaletti et~al.(2006)Boccaletti, Latora, Moreno, Chavez, and
  Hwang]{boccaletti2006complex}
Boccaletti, S., Latora, V., Moreno, Y., Chavez, M., and Hwang, D.-u.
\newblock Complex networks: Structure and dynamics.
\newblock \emph{Physics Reports}, 424\penalty0 (4-5):\penalty0 175--308, 2006.

\bibitem[Budden et~al.(2020)Budden, Marblestone, Sezener, Lattimore, Wayne, and
  Veness]{Budden2020GaussianGL}
Budden, D., Marblestone, A.~H., Sezener, E., Lattimore, T., Wayne, G., and
  Veness, J.
\newblock Gaussian gated linear networks.
\newblock \emph{ArXiv}, abs/2006.05964, 2020.

\bibitem[Carleo et~al.(2019)Carleo, Cirac, Cranmer, Daudet, Schuld, Tishby,
  Vogt-maranto, and Zdeborov\'a]{Carleo2019}
Carleo, G., Cirac, I., Cranmer, K., Daudet, L., Schuld, M., Tishby, N.,
  Vogt-maranto, L., and Zdeborov\'a, L.
\newblock Machine learning and the physical sciences.
\newblock \emph{Rev. Mod. Phys.}, 91:\penalty0 045002, 2019.
\newblock \doi{10.1103/RevModPhys.91.045002}.

\bibitem[Caruana(1997)]{caruana1997multitask_learning}
Caruana, R.
\newblock Multitask learning.
\newblock \emph{Machine Learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Chen et~al.(1999)Chen, Xu, and Chi]{chen1999improved}
Chen, K., Xu, L., and Chi, H.
\newblock Improved learning algorithms for mixture of experts in multiclass
  classification.
\newblock \emph{Neural Networks}, 12\penalty0 (9):\penalty0 1229--1252, 1999.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{Chizat2018}
Chizat, L., Oyallon, E., and Bach, F.~R.
\newblock On lazy training in differentiable programming.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  2933--2943, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/ae614c557843b1df326cb29c57225459-Paper.pdf}.

\bibitem[Collobert et~al.(2011)Collobert, Weston, Bottou, Karlen, Kavukcuoglu,
  and Kuksa]{collobert2011natural}
Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa,
  P.
\newblock Natural language processing (almost) from scratch.
\newblock \emph{Journal Of Machine Learning Research}, 12\penalty0
  (Article):\penalty0 2493--2537, 2011.

\bibitem[Combes et~al.(2018)Combes, Pezeshki, Shabanian, Courville, and
  Bengio]{des2018learning}
Combes, R.~T., Pezeshki, M., Shabanian, S., Courville, A.~C., and Bengio, Y.
\newblock On the learning dynamics of deep neural networks.
\newblock \emph{ArXiv}, abs/1809.06848, 2018.

\bibitem[Deng(2012)]{deng2012mnist}
Deng, L.
\newblock The mnist database of handwritten digit images for machine learning
  research.
\newblock \emph{Ieee Signal Processing Magazine}, 29\penalty0 (6):\penalty0
  141--142, 2012.

\bibitem[Du \& Hu(2019)Du and Hu]{du2019width}
Du, S.~S. and Hu, W.
\newblock Width provably matters in optimization for deep linear neural
  networks.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June
  2019, Long Beach, California, {USA}}, volume~97 of \emph{Proceedings of
  Machine Learning Research}, pp.\  1655--1664. {PMLR}, 2019.
\newblock URL \url{http://proceedings.mlr.press/v97/du19a.html}.

\bibitem[Fedus et~al.(2021)Fedus, Zoph, and Shazeer]{fedus2021switch}
Fedus, W., Zoph, B., and Shazeer, N.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{ArXiv preprint}, abs/2101.03961, 2021.
\newblock URL \url{https://arxiv.org/abs/2101.03961}.

\bibitem[Flesch et~al.(2022)Flesch, Juechems, Dumbalska, Saxe, and
  Summerfield]{flesch_orthogonal_2022}
Flesch, T., Juechems, K., Dumbalska, T., Saxe, A., and Summerfield, C.
\newblock Orthogonal representations for robust context-dependent task
  performance in brains and neural networks.
\newblock \emph{Neuron}, 0\penalty0 (0), 2022.
\newblock ISSN 0896-6273.
\newblock \doi{10.1016/j.neuron.2022.01.005}.
\newblock URL \url{https://www.cell.com/neuron/abstract/S0896-6273(22)00005-8}.
\newblock Publisher: Elsevier.

\bibitem[Fort et~al.(2019)Fort, Hu, and Lakshminarayanan]{fort2019deep}
Fort, S., Hu, H., and Lakshminarayanan, B.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{ArXiv preprint}, abs/1912.02757, 2019.
\newblock URL \url{https://arxiv.org/abs/1912.02757}.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort2020deep}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/405075699f065e43581f27d67bb68478-Paper.pdf}.

\bibitem[Fukumizu(1998)]{Fukumizu1998}
Fukumizu, K.
\newblock Effect of {Batch} {Learning} {In} {Multilayer} {Neural} {Networks}.
\newblock In \emph{Proceedings of the 5th {International} {Conference} on
  {Neural} {Information} {Processing}}, pp.\  67--70, 1998.

\bibitem[Girdhar et~al.(2022)Girdhar, Singh, Ravi, Van Der~Maaten, Joulin, and
  Misra]{girdhar2022omnivore}
Girdhar, R., Singh, M., Ravi, N., Van Der~Maaten, L., Joulin, A., and Misra, I.
\newblock Omnivore: A single model for many visual modalities.
\newblock \emph{ArXiv preprint}, abs/2201.08377, 2022.
\newblock URL \url{https://arxiv.org/abs/2201.08377}.

\bibitem[Gontier et~al.(2020)Gontier, Sinha, Reddy, and
  Pal]{gontier2020measuring}
Gontier, N., Sinha, K., Reddy, S., and Pal, C.
\newblock Measuring systematic generalization in neural proof generation with
  transformers.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/fc84ad56f9f547eb89c72b9bac209312-Paper.pdf}.

\bibitem[Goyal et~al.(2020)Goyal, Sodhani, Binas, Peng, Levine, and
  Bengio]{goyal2019reinforcement}
Goyal, A., Sodhani, S., Binas, J., Peng, X.~B., Levine, S., and Bengio, Y.
\newblock Reinforcement learning with competitive ensembles of
  information-constrained primitives.
\newblock In \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=ryxgJTEYDr}.

\bibitem[Goyal et~al.(2021)Goyal, Lamb, Hoffmann, Sodhani, Levine, Bengio, and
  Sch{\"o}lkopf]{Goyal2021RecurrentIM}
Goyal, A., Lamb, A., Hoffmann, J., Sodhani, S., Levine, S., Bengio, Y., and
  Sch{\"o}lkopf, B.
\newblock Recurrent independent mechanisms.
\newblock \emph{ArXiv}, abs/1909.10893, 2021.

\bibitem[Gross et~al.(2017)Gross, Ranzato, and Szlam]{gross2017hard}
Gross, S., Ranzato, M., and Szlam, A.
\newblock Hard mixtures of experts for large scale weakly supervised vision.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pp.\
  5085--5093. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/CVPR.2017.540}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2017.540}.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pp.\  9482--9491, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/0e98aeeb54acf612b9eb4e48a269814c-Paper.pdf}.

\bibitem[Happel \& Murre(1994)Happel and Murre]{happel1994design}
Happel, B.~L. and Murre, J.~M.
\newblock Design and evolution of modular neural network architectures.
\newblock \emph{Neural Networks}, 7\penalty0 (6-7):\penalty0 985--1004, 1994.

\bibitem[Harris et~al.(2020)Harris, Millman, van~der Walt, Gommers, Virtanen,
  Cournapeau, Wieser, Taylor, Berg, Smith, Kern, Picus, Hoyer, van Kerkwijk,
  Brett, Haldane, del R{'{\i}}o, Wiebe, Peterson, G{'{e}}rard-Marchant,
  Sheppard, Reddy, Weckesser, Abbasi, Gohlke, and Oliphant]{harris2020array}
Harris, C.~R., Millman, K.~J., van~der Walt, S.~J., Gommers, R., Virtanen, P.,
  Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N.~J., Kern, R.,
  Picus, M., Hoyer, S., van Kerkwijk, M.~H., Brett, M., Haldane, A., del
  R{'{\i}}o, J.~F., Wiebe, M., Peterson, P., G{'{e}}rard-Marchant, P.,
  Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant,
  T.~E.
\newblock Array programming with {NumPy}.
\newblock \emph{Nature}, 585\penalty0 (7825):\penalty0 357--362, 2020.
\newblock \doi{10.1038/s41586-020-2649-2}.
\newblock URL \url{https://doi.org/10.1038/s41586-020-2649-2}.

\bibitem[He \& Boyd{-}Graber(2016)He and Boyd{-}Graber]{he2016opponent}
He, H. and Boyd{-}Graber, J.~L.
\newblock Opponent modeling in deep reinforcement learning.
\newblock In Balcan, M. and Weinberger, K.~Q. (eds.), \emph{Proceedings of the
  33nd International Conference on Machine Learning, {ICML} 2016, New York
  City, NY, USA, June 19-24, 2016}, volume~48 of \emph{{JMLR} Workshop and
  Conference Proceedings}, pp.\  1804--1813. JMLR.org, 2016.
\newblock URL \url{http://proceedings.mlr.press/v48/he16.html}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pp.\
  770--778. {IEEE} Computer Society, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2016.90}.

\bibitem[Huh(2020)]{pmlr-v119-huh20a}
Huh, D.
\newblock Curvature-corrected learning dynamics in deep neural networks.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  4552--4560. {PMLR},
  2020.
\newblock URL \url{http://proceedings.mlr.press/v119/huh20a.html}.

\bibitem[Idelbayev(2020)]{Idelbayev18a}
Idelbayev, Y.
\newblock Proper {ResNet} implementation for {CIFAR10/CIFAR100} in {PyTorch}.
\newblock \url{https://github.com/akamaster/pytorch\_resnet\_cifar10}, 2020.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and
  Hinton]{jacobs1991adaptive}
Jacobs, R.~A., Jordan, M.~I., Nowlan, S.~J., and Hinton, G.~E.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Jacot et~al.(2018)Jacot, Hongler, and Gabriel]{jacot2018neural}
Jacot, A., Hongler, C., and Gabriel, F.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pp.\  8580--8589, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/5a4be1fa34e62bb8a6ec6b91d2462f5a-Paper.pdf}.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2018gradient}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=HJflg30qKX}.

\bibitem[Johnson et~al.(2017)Johnson, Hariharan, van~der Maaten, Fei{-}Fei,
  Zitnick, and Girshick]{johnson2017clevr}
Johnson, J., Hariharan, B., van~der Maaten, L., Fei{-}Fei, L., Zitnick, C.~L.,
  and Girshick, R.~B.
\newblock {CLEVR:} {A} diagnostic dataset for compositional language and
  elementary visual reasoning.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pp.\
  1988--1997. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/CVPR.2017.215}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2017.215}.

\bibitem[Jordan \& Jacobs(1994)Jordan and Jacobs]{jordan1994hierarchical}
Jordan, M.~I. and Jacobs, R.~A.
\newblock Hierarchical mixtures of experts and the em algorithm.
\newblock \emph{Neural Computation}, 6\penalty0 (2):\penalty0 181--214, 1994.

\bibitem[Kokkinos(2017)]{kokkinos2017ubernet}
Kokkinos, I.
\newblock Ubernet: Training a universal convolutional neural network for low-,
  mid-, and high-level vision using diverse datasets and limited memory.
\newblock In \emph{2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017, Honolulu, HI, USA, July 21-26, 2017}, pp.\
  5454--5463. {IEEE} Computer Society, 2017.
\newblock \doi{10.1109/CVPR.2017.579}.
\newblock URL \url{https://doi.org/10.1109/CVPR.2017.579}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, and
  Others]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., and Others.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[La~Malfa et~al.(2021)La~Malfa, La~Malfa, Nicosia, and
  Latora]{la2021characterizing}
La~Malfa, E., La~Malfa, G., Nicosia, G., and Latora, V.
\newblock Characterizing learning dynamics of deep neural networks via complex
  networks.
\newblock In \emph{2021 Ieee 33rd International Conference On Tools With
  Artificial Intelligence (ictai)}, pp.\  344--351. Ieee, 2021.

\bibitem[Lake(2019)]{lake2019compositional}
Lake, B.~M.
\newblock Compositional generalization through meta sequence-to-sequence
  learning.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  9788--9798, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/f4d0e2e7fc057a58f7ca4a391f01940a-Paper.pdf}.

\bibitem[Lampinen \& Ganguli(2019)Lampinen and Ganguli]{lampinen2018analytic}
Lampinen, A.~K. and Ganguli, S.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock In \emph{7th International Conference on Learning Representations,
  {ICLR} 2019, New Orleans, LA, USA, May 6-9, 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=ryfMLoCqtQ}.

\bibitem[Laurent \& von Brecht(2018)Laurent and von Brecht]{laurent2018deep}
Laurent, T. and von Brecht, J.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In Dy, J.~G. and Krause, A. (eds.), \emph{Proceedings of the 35th
  International Conference on Machine Learning, {ICML} 2018,
  Stockholmsm{\"{a}}ssan, Stockholm, Sweden, July 10-15, 2018}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2908--2913. {PMLR},
  2018.
\newblock URL \url{http://proceedings.mlr.press/v80/laurent18a.html}.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl{-}Dickstein,
  and Pennington]{Lee2019}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl{-}Dickstein,
  J., and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  8570--8581, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/0d1a9651497a38d8b1c3871c84528bd4-Paper.pdf}.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{lepikhin2020gshard}
Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M.,
  Shazeer, N., and Chen, Z.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and
  Zettlemoyer]{lewis2021base}
Lewis, M., Bhosale, S., Dettmers, T., Goyal, N., and Zettlemoyer, L.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock \emph{ArXiv preprint}, abs/2103.16716, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.16716}.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Johns, and
  Davison]{end_to_end_multi_task_learning_with_attention}
Liu, S., Johns, E., and Davison, A.~J.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{{IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, pp.\
  1871--1880. Computer Vision Foundation / {IEEE}, 2019{\natexlab{a}}.
\newblock \doi{10.1109/CVPR.2019.00197}.
\newblock URL
  \url{http://openaccess.thecvf.com/content\_CVPR\_2019/html/Liu\_End-To-End\_Multi-Task\_Learning\_With\_Attention\_CVPR\_2019\_paper.html}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, He, Chen, and Gao]{liu2019multi}
Liu, X., He, P., Chen, W., and Gao, J.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4487--4496, Florence, Italy,
  2019{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1441}.
\newblock URL \url{https://aclanthology.org/P19-1441}.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei_mean_2018}
Mei, S., Montanari, A., and Nguyen, P.-m.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings Of The National Academy Of Sciences}, 115\penalty0
  (33):\penalty0 E7665--e7671, 2018.
\newblock ISSN 0027-8424, 1091-6490.
\newblock \doi{10.1073/pnas.1806579115}.
\newblock URL \url{https://www.pnas.org/content/115/33/E7665}.
\newblock Publisher: National Academy Of Sciences Section: Pnas Plus.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, and Others]{nature_dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., and
  Others.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mott et~al.(2019)Mott, Zoran, Chrzanowski, Wierstra, and
  Rezende]{mott2019towards}
Mott, A., Zoran, D., Chrzanowski, M., Wierstra, D., and Rezende, D.~J.
\newblock Towards interpretable reinforcement learning using attention
  augmented agents.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  12329--12338, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/e9510081ac30ffa83f10b68cde1cac07-Paper.pdf}.

\bibitem[Ngiam et~al.(2011)Ngiam, Khosla, Kim, Nam, Lee, and
  Ng]{ngiam2011multimodal}
Ngiam, J., Khosla, A., Kim, M., Nam, J., Lee, H., and Ng, A.~Y.
\newblock Multimodal deep learning.
\newblock In Getoor, L. and Scheffer, T. (eds.), \emph{Proceedings of the 28th
  International Conference on Machine Learning, {ICML} 2011, Bellevue,
  Washington, USA, June 28 - July 2, 2011}, pp.\  689--696. Omnipress, 2011.
\newblock URL \url{https://icml.cc/2011/papers/399\_icmlpaper.pdf}.

\bibitem[Nguyen et~al.(2020)Nguyen, Raghu, and Kornblith]{nguyen2020wide}
Nguyen, T., Raghu, M., and Kornblith, S.
\newblock Do wide and deep networks learn the same things? uncovering how
  neural network representations vary with width and depth.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito,
  Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K{\"{o}}pf, A., Yang,
  E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang,
  L., Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  8024--8035, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf}.

\bibitem[Patel et~al.(2015)Patel, Nguyen, and Baraniuk]{patel2015probabilistic}
Patel, A.~B., Nguyen, T., and Baraniuk, R.~G.
\newblock A probabilistic theory of deep learning.
\newblock \emph{ArXiv preprint}, abs/1504.00641, 2015.
\newblock URL \url{https://arxiv.org/abs/1504.00641}.

\bibitem[Poggio et~al.(2018)Poggio, Liao, Miranda, Burbanski, and
  Hidary]{poggio2018theory}
Poggio, T., Liao, Q., Miranda, B., Burbanski, A., and Hidary, J.
\newblock Theory iiib: Generalization in deep networks.
\newblock Technical report, Center for Brains, Minds and Machines (CBMM),
  arXiv. org, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language_models_are_unsupervised_multitask_learners}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{Openai Blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl{-}Dickstein]{raghu2017expressive}
Raghu, M., Poole, B., Kleinberg, J.~M., Ganguli, S., and Sohl{-}Dickstein, J.
\newblock On the expressive power of deep neural networks.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning, {ICML} 2017, Sydney, NSW,
  Australia, 6-11 August 2017}, volume~70 of \emph{Proceedings of Machine
  Learning Research}, pp.\  2847--2854. {PMLR}, 2017.
\newblock URL \url{http://proceedings.mlr.press/v70/raghu17a.html}.

\bibitem[Roberts et~al.(2021)Roberts, Yaida, and Hanin]{roberts2021principles}
Roberts, D.~A., Yaida, S., and Hanin, B.
\newblock The principles of deep learning theory.
\newblock \emph{ArXiv preprint}, abs/2106.10165, 2021.
\newblock URL \url{https://arxiv.org/abs/2106.10165}.

\bibitem[Rosenbaum et~al.(2019)Rosenbaum, Cases, Riemer, and
  Klinger]{rosenbaum2019routing}
Rosenbaum, C., Cases, I., Riemer, M., and Klinger, T.
\newblock Routing networks and the challenges of modular and compositional
  computation.
\newblock \emph{ArXiv preprint}, abs/1904.12774, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.12774}.

\bibitem[Rotskoff \& Vanden{-}Eijnden(2018)Rotskoff and
  Vanden{-}Eijnden]{rotskoff_parameters_2018}
Rotskoff, G.~M. and Vanden{-}Eijnden, E.
\newblock Parameters as interacting particles: long time convergence and
  asymptotic error scaling of neural networks.
\newblock In Bengio, S., Wallach, H.~M., Larochelle, H., Grauman, K.,
  Cesa{-}Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 31: Annual Conference on Neural Information
  Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr{\'{e}}al,
  Canada}, pp.\  7146--7155, 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/196f5641aa9dc87067da4ff90fd81e7b-Paper.pdf}.

\bibitem[Ruder(2017)]{ruder2017overview}
Ruder, S.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{ArXiv preprint}, abs/1706.05098, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.05098}.

\bibitem[Ruis et~al.(2020)Ruis, Andreas, Baroni, Bouchacourt, and
  Lake]{ruis2020benchmark}
Ruis, L., Andreas, J., Baroni, M., Bouchacourt, D., and Lake, B.~M.
\newblock A benchmark for systematic generalization in grounded language
  understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Salakhutdinov(2014{\natexlab{a}})]{goodfellow2016deep}
Salakhutdinov, R.
\newblock Deep learning.
\newblock In Macskassy, S.~A., Perlich, C., Leskovec, J., Wang, W., and Ghani,
  R. (eds.), \emph{The 20th {ACM} {SIGKDD} International Conference on
  Knowledge Discovery and Data Mining, {KDD} '14, New York, NY, {USA} - August
  24 - 27, 2014}, pp.\  1973. {ACM}, 2014{\natexlab{a}}.
\newblock \doi{10.1145/2623330.2630809}.
\newblock URL \url{https://doi.org/10.1145/2623330.2630809}.

\bibitem[Salakhutdinov(2014{\natexlab{b}})]{lecun2015deep}
Salakhutdinov, R.
\newblock Deep learning.
\newblock In Macskassy, S.~A., Perlich, C., Leskovec, J., Wang, W., and Ghani,
  R. (eds.), \emph{The 20th {ACM} {SIGKDD} International Conference on
  Knowledge Discovery and Data Mining, {KDD} '14, New York, NY, {USA} - August
  24 - 27, 2014}, pp.\  1973. {ACM}, 2014{\natexlab{b}}.
\newblock \doi{10.1145/2623330.2630809}.
\newblock URL \url{https://doi.org/10.1145/2623330.2630809}.

\bibitem[Santoro et~al.(2017)Santoro, Raposo, Barrett, Malinowski, Pascanu,
  Battaglia, and Lillicrap]{santoro2017simple}
Santoro, A., Raposo, D., Barrett, D. G.~T., Malinowski, M., Pascanu, R.,
  Battaglia, P.~W., and Lillicrap, T.
\newblock A simple neural network module for relational reasoning.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\
  4967--4976, 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/e6acf4b0f69f6f6e60e9a815938aa1ff-Paper.pdf}.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{Saxe2014}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock In Bengio, Y. and LeCun, Y. (eds.), \emph{2nd International
  Conference on Learning Representations, {ICLR} 2014, Banff, AB, Canada, April
  14-16, 2014, Conference Track Proceedings}, 2014.
\newblock URL \url{http://arxiv.org/abs/1312.6120}.

\bibitem[Saxe et~al.(2019)Saxe, Mcclelland, and
  Ganguli]{saxe_mathematical_2019}
Saxe, A.~M., Mcclelland, J.~L., and Ganguli, S.
\newblock A mathematical theory of semantic development in deep neural
  networks.
\newblock \emph{Proceedings Of The National Academy Of Sciences}, 116\penalty0
  (23):\penalty0 11537--11546, 2019.
\newblock ISSN 0027-8424, 1091-6490.
\newblock \doi{10.1073/pnas.1820226116}.

\bibitem[Scabini \& Bruno(2021)Scabini and Bruno]{scabini2021structure}
Scabini, L.~F. and Bruno, O.~M.
\newblock Structure and performance of fully connected neural networks:
  Emerging complex network properties.
\newblock \emph{ArXiv preprint}, abs/2107.14062, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.14062}.

\bibitem[Sharkey(1997)]{sharkey1997modularity}
Sharkey, A. J.~C.
\newblock Modularity, combining and artificial neural nets.
\newblock \emph{Connection Science}, 9\penalty0 (1):\penalty0 3--10, 1997.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q.~V., Hinton, G.~E.,
  and Dean, J.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=B1ckMDqlg}.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, and Others]{alpha_zero}
Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez,
  A., Hubert, T., Baker, L., Lai, M., Bolton, A., and Others.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Sinha et~al.(2019)Sinha, Sodhani, Dong, Pineau, and
  Hamilton]{Sinha2019CLUTRRAD}
Sinha, K., Sodhani, S., Dong, J., Pineau, J., and Hamilton, W.~L.
\newblock {CLUTRR}: A diagnostic benchmark for inductive reasoning from text.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4506--4515, Hong Kong,
  China, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1458}.
\newblock URL \url{https://aclanthology.org/D19-1458}.

\bibitem[Sinha et~al.(2020)Sinha, Sodhani, Pineau, and
  Hamilton]{Sinha2020EvaluatingLG}
Sinha, K., Sodhani, S., Pineau, J., and Hamilton, W.~L.
\newblock Evaluating logical generalization in graph neural networks.
\newblock \emph{ArXiv preprint}, abs/2003.06560, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.06560}.

\bibitem[Sirignano \& Spiliopoulos(2020)Sirignano and
  Spiliopoulos]{sirignano_mean_2020}
Sirignano, J. and Spiliopoulos, K.
\newblock Mean field analysis of neural networks: {A} central limit theorem.
\newblock \emph{Stochastic Processes And Their Applications}, 130\penalty0
  (3):\penalty0 1820--1852, 2020.
\newblock ISSN 0304-4149.
\newblock \doi{10.1016/j.spa.2019.06.003}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/S0304414918306197}.

\bibitem[Sodhani(2022)]{Sodhani_xplogger_Logging_utility_2022}
Sodhani, S.
\newblock {xplogger: Logging utility for ML experiments}, 2 2022.
\newblock URL \url{https://github.com/shagunsodhani/xplogger}.

\bibitem[Sodhani et~al.(2021)Sodhani, Zhang, and Pineau]{sodhani2021multi}
Sodhani, S., Zhang, A., and Pineau, J.
\newblock Multi-task reinforcement learning with context-based representations.
\newblock In \emph{International Conference On Machine Learning (icml)}, 2021.

\bibitem[Standley et~al.(2020)Standley, Zamir, Chen, Guibas, Malik, and
  Savarese]{standley2020tasks}
Standley, T., Zamir, A.~R., Chen, D., Guibas, L.~J., Malik, J., and Savarese,
  S.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9120--9132. {PMLR},
  2020.
\newblock URL \url{http://proceedings.mlr.press/v119/standley20a.html}.

\bibitem[Straat \& Biehl(2019)Straat and Biehl]{straat2019line}
Straat, M. and Biehl, M.
\newblock On-line learning dynamics of relu neural networks using statistical
  physics techniques.
\newblock \emph{ArXiv preprint}, abs/1903.07378, 2019.
\newblock URL \url{https://arxiv.org/abs/1903.07378}.

\bibitem[Team(2020)]{reback2020pandas}
Team, T. P.~D.
\newblock pandas-dev/pandas: Pandas, 2020.
\newblock URL \url{https://doi.org/10.5281/zenodo.3509134}.

\bibitem[Testolin et~al.(2020)Testolin, Piccolini, and
  Suweis]{testolin2020deep}
Testolin, A., Piccolini, M., and Suweis, S.
\newblock Deep learning systems as complex networks.
\newblock \emph{Journal Of Complex Networks}, 8\penalty0 (1):\penalty0 Cnz018,
  2020.

\bibitem[Tian et~al.(2019)Tian, Jiang, Gong, and Morcos]{tian2019luck}
Tian, Y., Jiang, T., Gong, Q., and Morcos, A.
\newblock Luck matters: Understanding training dynamics of deep relu networks.
\newblock \emph{ArXiv preprint}, abs/1905.13405, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.13405}.

\bibitem[Tsai et~al.(2016)Tsai, Saxe, Saxe, and Cox]{NIPS2016_b1563a78}
Tsai, C.-Y., Saxe, A.~M., Saxe, A.~M., and Cox, D.
\newblock Tensor switching networks.
\newblock In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/b1563a78ec59337587f6ab6397699afc-Paper.pdf}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H.~M., Fergus,
  R., Vishwanathan, S. V.~N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30: Annual Conference on Neural Information
  Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}}, pp.\
  5998--6008, 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Veness et~al.(2021)Veness, Lattimore, Bhoopchand, Budden, Mattern,
  Grabska-Barwinska, Toth, Schmitt, and Hutter]{Veness2021GatedLN}
Veness, J., Lattimore, T., Bhoopchand, A., Budden, D., Mattern, C.,
  Grabska-Barwinska, A., Toth, P., Schmitt, S., and Hutter, M.
\newblock Gated linear networks.
\newblock In \emph{AAAI}, 2021.

\bibitem[Vithayathil~Varghese \& Mahmoud(2020)Vithayathil~Varghese and
  Mahmoud]{electronics9091363}
Vithayathil~Varghese, N. and Mahmoud, Q.~H.
\newblock A survey of multi-task deep reinforcement learning.
\newblock \emph{Electronics}, 9\penalty0 (9), 2020.
\newblock ISSN 2079-9292.
\newblock \doi{10.3390/electronics9091363}.
\newblock URL \url{https://www.mdpi.com/2079-9292/9/9/1363}.

\bibitem[Wang et~al.(2019)Wang, Yu, Dunlap, Ma, Wang, Mirhoseini, Darrell, and
  Gonzalez]{wang2020deep}
Wang, X., Yu, F., Dunlap, L., Ma, Y., Wang, R., Mirhoseini, A., Darrell, T.,
  and Gonzalez, J.~E.
\newblock Deep mixture of experts via shallow embedding.
\newblock In Globerson, A. and Silva, R. (eds.), \emph{Proceedings of the
  Thirty-Fifth Conference on Uncertainty in Artificial Intelligence, {UAI}
  2019, Tel Aviv, Israel, July 22-25, 2019}, volume 115 of \emph{Proceedings of
  Machine Learning Research}, pp.\  552--562. {AUAI} Press, 2019.
\newblock URL \url{http://proceedings.mlr.press/v115/wang20d.html}.

\bibitem[Yadan(2019)]{Yadan2019Hydra}
Yadan, O.
\newblock Hydra - a framework for elegantly configuring complex applications.
\newblock Github, 2019.
\newblock URL \url{https://github.com/facebookresearch/hydra}.

\bibitem[Yang et~al.(2019)Yang, Bender, Le, and Ngiam]{yang2019condconv}
Yang, B., Bender, G., Le, Q.~V., and Ngiam, J.
\newblock Condconv: Conditionally parameterized convolutions for efficient
  inference.
\newblock In Wallach, H.~M., Larochelle, H., Beygelzimer, A.,
  d'Alch{\'{e}}{-}Buc, F., Fox, E.~B., and Garnett, R. (eds.), \emph{Advances
  in Neural Information Processing Systems 32: Annual Conference on Neural
  Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
  Vancouver, BC, Canada}, pp.\  1305--1316, 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/f2201f5191c4e92cc5af043eebfd0946-Paper.pdf}.

\bibitem[Yang et~al.(2020)Yang, Xu, WU, and
  Wang]{multi-task-rl-with-soft-modularization}
Yang, R., Xu, H., WU, Y., and Wang, X.
\newblock Multi-task reinforcement learning with soft modularization.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  4767--4777. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/32cfdce9631d8c7906e8e9d6e68b514b-Paper.pdf}.

\bibitem[Yuksel et~al.(2012)Yuksel, Wilson, and Gader]{yuksel2012twenty}
Yuksel, S.~E., Wilson, J.~N., and Gader, P.~D.
\newblock Twenty years of mixture of experts.
\newblock \emph{Ieee Transactions On Neural Networks And Learning Systems},
  23\penalty0 (8):\penalty0 1177--1193, 2012.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In Wilson, R.~C., Hancock, E.~R., and Smith, W. A.~P. (eds.),
  \emph{Proceedings of the British Machine Vision Conference 2016, {BMVC} 2016,
  York, UK, September 19-22, 2016}. {BMVA} Press, 2016.
\newblock URL \url{http://www.bmva.org/bmvc/2016/papers/paper087/index.html}.

\bibitem[Zambra et~al.(2020)Zambra, Maritan, and Testolin]{zambra2020emergence}
Zambra, M., Maritan, A., and Testolin, A.
\newblock Emergence of network motifs in deep neural networks.
\newblock \emph{Entropy}, 22\penalty0 (2):\penalty0 204, 2020.

\bibitem[Zhang et~al.(2014)Zhang, Luo, Loy, and
  Tang]{zhang2014facial_landmark_detection_by_deep_multitask_learning}
Zhang, Z., Luo, P., Loy, C.~C., and Tang, X.
\newblock Facial landmark detection by deep multi-task learning.
\newblock In \emph{European Conference On Computer Vision}, pp.\  94--108.
  Springer, 2014.

\end{thebibliography}
