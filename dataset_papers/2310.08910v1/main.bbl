\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An et~al.(2021)An, Ying, and Zhu]{An2020WhyRO}
Jing An, Lexing Ying, and Yuhua Zhu.
\newblock Why resampling outperforms reweighting for correcting sampling bias.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Ben-David and Schuller(2003)]{ben2003exploiting}
Shai Ben-David and Reba Schuller.
\newblock Exploiting task relatedness for multiple task learning.
\newblock In \emph{Learning Theory and Kernel Machines: 16th Annual Conference
  on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003, Washington, DC,
  USA, August 24-27, 2003. Proceedings}, pages 567--580. Springer, 2003.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{gb4}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando~C Pereira,
  and Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79:\penalty0 151--175, 2010.

\bibitem[Blitzer et~al.(2007)Blitzer, Crammer, Kulesza, Pereira, and
  Vaughan]{da2}
John Blitzer, Koby Crammer, Alex Kulesza, Fernando~C Pereira, and
  Jennifer~Wortman Vaughan.
\newblock Learning bounds for domain adaptation.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2007.

\bibitem[Boyd and Vandenberghe(2004)]{Boyd2004ConvexO}
Stephen~P. Boyd and Lieven Vandenberghe.
\newblock Convex optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 51:\penalty0
  1859--1859, 2004.

\bibitem[Bragman et~al.(2019)Bragman, Tanno, Ourselin, Alexander, and
  Cardoso]{bragman2019stochastic}
Felix~JS Bragman, Ryutaro Tanno, Sebastien Ourselin, Daniel~C Alexander, and
  Jorge Cardoso.
\newblock Stochastic filter groups for multi-task cnns: Learning specialist and
  generalist convolution kernels.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 1385--1394, 2019.

\bibitem[Chen et~al.(2017)Chen, Badrinarayanan, Lee, and
  Rabinovich]{Chen2017GradNormGN}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock {GradNorm}: {G}radient normalization for adaptive loss balancing in
  deep multitask networks.
\newblock \emph{ArXiv}, abs/1711.02257, 2017.

\bibitem[Chen et~al.(2020)Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and
  Anguelov]{Chen2020JustPA}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: {O}ptimizing deep multitask models with gradient
  sign dropout.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Coates et~al.(2011)Coates, Ng, and Lee]{stl}
Adam Coates, A.~Ng, and Honglak Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2011.

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
  Franke, Roth, and Schiele]{cordts2016cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 3213--3223, 2016.

\bibitem[Cubuk et~al.(2019{\natexlab{a}})Cubuk, Zoph, Man{\'e}, Vasudevan, and
  Le]{autoaugment}
Ekin~Dogus Cubuk, Barret Zoph, Dandelion Man{\'e}, Vijay Vasudevan, and Quoc~V.
  Le.
\newblock {AutoAugment}: {L}earning augmentation strategies from data.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages 113--123, 2019{\natexlab{a}}.

\bibitem[Cubuk et~al.(2019{\natexlab{b}})Cubuk, Zoph, Shlens, and
  Le]{randaugment}
Ekin~Dogus Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V. Le.
\newblock {RandAugment}: {P}ractical automated data augmentation with a reduced
  search space.
\newblock \emph{2020 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition Workshops (CVPRW)}, pages 3008--3017, 2019{\natexlab{b}}.

\bibitem[D{\'e}sid{\'e}ri(2012)]{Dsidri2012MultiplegradientDA}
Jean-Antoine D{\'e}sid{\'e}ri.
\newblock Multiple-gradient descent algorithm (mgda) for multiobjective
  optimization.
\newblock \emph{Comptes Rendus Mathematique}, 350:\penalty0 313--318, 2012.

\bibitem[Du et~al.(2018)Du, Czarnecki, Jayakumar, Farajtabar, Pascanu, and
  Lakshminarayanan]{du2018adapting}
Yunshu Du, Wojciech~M Czarnecki, Siddhant~M Jayakumar, Mehrdad Farajtabar,
  Razvan Pascanu, and Balaji Lakshminarayanan.
\newblock Adapting auxiliary losses using gradient similarity.
\newblock \emph{arXiv preprint arXiv:1812.02224}, 2018.

\bibitem[Dushatskiy et~al.(2023)Dushatskiy, Chebykin, Alderliesten, and
  Bosman]{dushatskiy2023multi}
Arkadiy Dushatskiy, Alexander Chebykin, Tanja Alderliesten, and Peter~A.N.
  Bosman.
\newblock Multi-objective population based training.
\newblock In \emph{International Conference on Machine Learning}, pages
  8969--8989. PMLR, 2023.

\bibitem[Fifty et~al.(2021)Fifty, Amid, Zhao, Yu, Anil, and
  Finn]{Fifty2021EfficientlyIT}
Christopher Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea
  Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Frazier(2018)]{Frazier2018ATO}
P.~Frazier.
\newblock A tutorial on {B}ayesian optimization.
\newblock \emph{ArXiv}, abs/1807.02811, 2018.

\bibitem[Gani et~al.(2022)Gani, Naseer, and Yaqub]{vits4}
Hanan Gani, Muzammal Naseer, and Mohammad Yaqub.
\newblock How to train vision transformer on small-scale datasets?
\newblock In \emph{British Machine Vision Conference (BMVC)}, 2022.

\bibitem[Ganin et~al.(2015)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{da1}
Yaroslav Ganin, E.~Ustinova, Hana Ajakan, Pascal Germain, H.~Larochelle,
  François Laviolette, Mario Marchand, and Victor~S. Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{ArXiv}, abs/1505.07818, 2015.

\bibitem[Gao et~al.(2019)Gao, Ma, Zhao, Liu, and Yuille]{gao2019nddr}
Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan~L Yuille.
\newblock {NDDR-CNN}: Layerwise feature fusing in multi-task cnns by neural
  discriminative dimensionality reduction.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 3205--3214, 2019.

\bibitem[Guo et~al.(2018)Guo, Haque, Huang, Yeung, and Fei-Fei]{guo2018dynamic}
Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li~Fei-Fei.
\newblock Dynamic task prioritization for multitask learning.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, pages
  270--287, 2018.

\bibitem[Han and Zhang(2015)]{Han2015LearningMT}
Lei Han and Yu~Zhang.
\newblock Learning multi-level task groups in multi-task learning.
\newblock In \emph{Conference on Artificial Intelligence (AAAI)}, 2015.

\bibitem[Ho et~al.(2019)Ho, Liang, Stoica, Abbeel, and
  Chen]{Ho2019PopulationBA}
Daniel Ho, Eric Liang, Ion Stoica, P.~Abbeel, and Xi~Chen.
\newblock Population based augmentation: Efficient learning of augmentation
  policy schedules.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2019.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,
  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,
  et~al.]{jaderberg2017population}
Max Jaderberg, Valentin Dalibard, Simon Osindero, Wojciech~M Czarnecki, Jeff
  Donahue, Ali Razavi, Oriol Vinyals, Tim Green, Iain Dunning, Karen Simonyan,
  et~al.
\newblock Population based training of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.09846}, 2017.

\bibitem[Javaloy and Valera(2021)]{Javaloy2021RotoGradGH}
Adri{\'a}n Javaloy and Isabel Valera.
\newblock {RotoGrad}: {G}radient homogenization in multitask learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Kendall et~al.(2017)Kendall, Gal, and Cipolla]{Kendall2017MultitaskLU}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 7482--7491, 2017.

\bibitem[Kokkinos(2016)]{Kokkinos2016UberNetTA}
Iasonas Kokkinos.
\newblock {UberNet}: Training a universal convolutional neural network for
  low-, mid-, and high-level vision using diverse datasets and limited memory.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016.

\bibitem[Krawczyk(2016)]{Krawczyk2016LearningFI}
B.~Krawczyk.
\newblock Learning from imbalanced data: open challenges and future directions.
\newblock \emph{Progress in Artificial Intelligence}, 5:\penalty0 221 -- 232,
  2016.

\bibitem[Krizhevsky(2009)]{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Kurin et~al.(2022)Kurin, Palma, Kostrikov, Whiteson, and
  Kumar]{Kurin2022InDO}
Vitaly Kurin, Alessandro~De Palma, Ilya Kostrikov, Shimon Whiteson, and
  M.~Pawan Kumar.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Li et~al.(2022)Li, Liu, and Bilen]{li2022Universal}
Wei-Hong Li, Xialei Liu, and Hakan Bilen.
\newblock Universal representations: {A} unified look at multiple task and
  domain learning.
\newblock \emph{arXiv preprint arXiv:2204.02744}, 2022.

\bibitem[Li et~al.(2021)Li, Yuan, Chen, Wang, and Vasconcelos]{li2021dynamic}
Yunsheng Li, Lu~Yuan, Yinpeng Chen, Pei Wang, and Nuno Vasconcelos.
\newblock Dynamic transfer for multi-source domain adaptation.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 10998--11007, 2021.

\bibitem[Liaw et~al.(2018)Liaw, Liang, Nishihara, Moritz, Gonzalez, and
  Stoica]{raytune}
Richard Liaw, Eric Liang, Robert Nishihara, Philipp Moritz, Joseph~E. Gonzalez,
  and Ion Stoica.
\newblock Tune: {A} research platform for distributed model selection and
  training.
\newblock \emph{ArXiv}, abs/1807.05118, 2018.

\bibitem[Lin et~al.(2021)Lin, Ye, and Zhang]{lin2021closer}
Baijiong Lin, Feiyang Ye, and Yu~Zhang.
\newblock A closer look at loss weighting in multi-task learning.
\newblock \emph{arXiv preprint arXiv:2111.10603}, 2021.

\bibitem[Lin et~al.(2022)Lin, Ye, Zhang, and Tsang]{Lin2022ReasonableEO}
Baijiong Lin, Feiyang Ye, Yu~Zhang, and Ivor Wai-Hung Tsang.
\newblock Reasonable eﬀectiveness of random weighting: A {L}itmus test for
  multi-task learning.
\newblock In \emph{IEEE Transactions on Machine Learning Research (T-MLR)},
  2022.

\bibitem[Lin et~al.(2019)Lin, Zhen, Li, Zhang, and Kwong]{Lin2019ParetoML}
Xi~Lin, Hui-Ling Zhen, Zhenhua Li, Qingfu Zhang, and Sam Tak~Wu Kwong.
\newblock Pareto multi-task learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Liu, Jin, Stone, and
  Liu]{Liu2021ConflictAverseGD}
Bo~Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  34:\penalty0 18878--18890, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Li, Kuang, Xue, Chen, Yang, Liao,
  and Zhang]{Liu2021TowardsIM}
Liyang Liu, Yi~Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang,
  Qingmin Liao, and Wayne Zhang.
\newblock Towards impartial multi-task learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2018)Liu, Johns, and Davison]{Liu2018EndToEndML}
Shikun Liu, Edward Johns, and Andrew~J. Davison.
\newblock End-to-end multi-task learning with attention.
\newblock \emph{Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages 1871--1880, 2018.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{celeba}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2015.

\bibitem[Lopes et~al.(2021)Lopes, Smullin, Cubuk, and
  Dyer]{Lopes2021TradeoffsID}
Raphael~Gontijo Lopes, Sylvia~J. Smullin, Ekin~Dogus Cubuk, and Ethan Dyer.
\newblock Tradeoffs in data augmentation: {A}n empirical study.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Mahapatra and Rajan(2020)]{Mahapatra2021ExactPO}
Debabrata Mahapatra and Vaibhav Rajan.
\newblock Multi-task learning with user preferences: Gradient descent with
  controlled ascent in {P}areto optimization.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2020.

\bibitem[Mathews and Hari(2019)]{Mathews2019LearningFI}
Lincy Mathews and Seetha Hari.
\newblock Learning from imbalanced data.
\newblock \emph{Advances in Computer and Electrical Engineering}, 2019.

\bibitem[Maurer et~al.(2015)Maurer, Pontil, and Romera-Paredes]{gb2}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, abs/1505.06279,
  2015.

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and
  Hebert]{misra2016cross}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 3994--4003, 2016.

\bibitem[Parker-Holder et~al.(2020)Parker-Holder, Nguyen, and
  Roberts]{ParkerHolder2020ProvablyEO}
Jack Parker-Holder, Vu~Nguyen, and Stephen~J. Roberts.
\newblock Provably efficient online hyperparameter optimization with
  population-based bandits.
\newblock \emph{arXiv: Learning}, 2020.

\bibitem[Peng et~al.(2018)Peng, Bai, Xia, Huang, Saenko, and Wang]{domainnet}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, pages
  1406--1415, 2018.

\bibitem[Pentina and Lampert(2016)]{gb3}
Anastasia Pentina and Christoph~H. Lampert.
\newblock Multi-task learning with labeled and unlabeled tasks.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2016.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  30, 2017.

\bibitem[Rebuffi et~al.(2018)Rebuffi, Bilen, and Vedaldi]{rebuffi2018efficient}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Efficient parametrization of multi-domain deep neural networks.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 8119--8127, 2018.

\bibitem[Rosenfeld and Tsotsos(2018)]{rosenfeld2018incremental}
Amir Rosenfeld and John~K Tsotsos.
\newblock Incremental learning through deep adaptation.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI)}, 42\penalty0 (3):\penalty0 651--663, 2018.

\bibitem[Ruchte and Grabocka(2021)]{Ruchte2021MultitaskPA}
Michael Ruchte and Josif Grabocka.
\newblock Multi-task problems are not multi-objective.
\newblock \emph{ArXiv}, abs/2110.07301, 2021.

\bibitem[Ruder et~al.(2019)Ruder, Bingel, Augenstein, and
  S{\o}gaard]{ruder2019latent}
Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S{\o}gaard.
\newblock Latent multi-task architecture learning.
\newblock In \emph{Conference on Artificial Intelligence (AAAI)}, 2019.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115:\penalty0
  211--252, 2015.

\bibitem[Sener and Koltun(2018)]{Sener2018MultiTaskLA}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Shui et~al.(2019)Shui, Abbasi, Robitaille, Wang, and Gagn{\'e}]{gb1}
Changjian Shui, Mahdieh Abbasi, Louis-{\'E}mile Robitaille, Boyu Wang, and
  Christian Gagn{\'e}.
\newblock A principled approach for learning task similarity in multitask
  learning.
\newblock In \emph{International Joint Conferences on Artificial Intelligence
  (IJCAI)}, volume abs/1903.09109, 2019.

\bibitem[Silberman et~al.(2012)Silberman, Hoiem, Kohli, and
  Fergus]{silberman2012indoor}
Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock \emph{European Conference on Computer Vision (ECCV)}, 7576:\penalty0
  746--760, 2012.

\bibitem[Standley et~al.(2019)Standley, Zamir, Chen, Guibas, Malik, and
  Savarese]{taskgrouping}
Trevor~Scott Standley, Amir~Roshan Zamir, Dawn Chen, Leonidas~J. Guibas,
  Jitendra Malik, and Silvio Savarese.
\newblock Which tasks should be learned together in multi-task learning?
\newblock \emph{ArXiv}, abs/1905.07553, 2019.

\bibitem[Strezoski et~al.(2019)Strezoski, Noord, and
  Worring]{strezoski2019many}
Gjorgji Strezoski, Nanne~van Noord, and Marcel Worring.
\newblock Many task learning with task routing.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, pages
  1375--1384, 2019.

\bibitem[Suteu and Guo(2019)]{suteu2019regularizing}
Mihai Suteu and Yike Guo.
\newblock Regularizing deep multi-task networks using orthogonal gradients.
\newblock \emph{arXiv preprint arXiv:1912.06844}, 2019.

\bibitem[Vandenhende et~al.(2021)Vandenhende, Georgoulis, Van~Gansbeke,
  Proesmans, Dai, and Van~Gool]{vandenhende2021multi}
Simon Vandenhende, Stamatios Georgoulis, Wouter Van~Gansbeke, Marc Proesmans,
  Dengxin Dai, and Luc Van~Gool.
\newblock Multi-task learning for dense prediction tasks: {A} survey.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 44\penalty0 (7):\penalty0 3614--3633, 2021.

\bibitem[Wang et~al.(2021)Wang, Tsvetkov, Firat, and Cao]{wang2020gradient}
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.
\newblock Gradient vaccine: {I}nvestigating and improving multi-task
  optimization in massively multilingual models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Xin et~al.(2022)Xin, Ghorbani, Garg, Firat, and Gilmer]{Xin2022DoCM}
Derrick Xin, B.~Ghorbani, Ankush Garg, Orhan Firat, and Justin Gilmer.
\newblock Do current multi-task optimization methods in deep learning even
  help?
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2022.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and
  Finn]{Yu2020GradientSF}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Zamir et~al.(2018)Zamir, Sax, Shen, Guibas, Malik, and
  Savarese]{taskonomy}
Amir~Roshan Zamir, Alexander Sax, Bokui~(William) Shen, Leonidas~J. Guibas,
  Jitendra Malik, and Silvio Savarese.
\newblock Taskonomy: {D}isentangling task transfer learning.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2018.

\bibitem[Zhang(2021)]{da3}
Youshan Zhang.
\newblock A survey of unsupervised domain adaptation for visual recognition.
\newblock \emph{ArXiv}, abs/2112.06745, 2021.

\end{thebibliography}
