\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in {M}arkov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR, 2020.

\bibitem[Agazzi \& Lu(2020)Agazzi and Lu]{agazzi2020global}
Agazzi, A. and Lu, J.
\newblock Global optimality of softmax policy gradient with single hidden layer
  neural networks in the mean-field regime.
\newblock \emph{arXiv preprint arXiv:2010.11858}, 2020.

\bibitem[Ambrosio et~al.(2008)Ambrosio, Gigli, and
  Savar{\'e}]{ambrosio2008gradient}
Ambrosio, L., Gigli, N., and Savar{\'e}, G.
\newblock \emph{Gradient flows: in metric spaces and in the space of
  probability measures}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Bertsekas \& Shreve(2004)Bertsekas and
  Shreve]{bertsekas2004stochastic}
Bertsekas, D.~P. and Shreve, S.
\newblock \emph{Stochastic optimal control: the discrete-time case}.
\newblock Athena Scientific, 2004.

\bibitem[Bogachev et~al.(2015)Bogachev, Krylov, R{\"o}ckner, and
  Shaposhnikov]{bogachev2015fokker}
Bogachev, V.~I., Krylov, N.~V., R{\"o}ckner, M., and Shaposhnikov, S.~V.
\newblock \emph{{F}okker--{P}lanck--{K}olmogorov Equations}, volume 207.
\newblock American Mathematical Soc., 2015.

\bibitem[Bogachev et~al.(2016)Bogachev, R{\"o}ckner, and
  Shaposhnikov]{bogachev2016distances}
Bogachev, V.~I., R{\"o}ckner, M., and Shaposhnikov, S.~V.
\newblock Distances between transition probabilities of diffusions and
  applications to nonlinear {F}okker--{P}lanck--{K}olmogorov equations.
\newblock \emph{Journal of Functional Analysis}, 271\penalty0 (5):\penalty0
  1262--1300, 2016.

\bibitem[Bogachev et~al.(2018)Bogachev, Kirillov, and
  Shaposhnikov]{bogachev2018distances}
Bogachev, V.~I., Kirillov, A.~I., and Shaposhnikov, S.~V.
\newblock Distances between stationary distributions of diffusions and
  solvability of nonlinear {F}okker--{P}lanck--{K}olmogorov equations.
\newblock \emph{Theory of Probability and its Applications}, 62\penalty0
  (1):\penalty0 12--34, 2018.

\bibitem[Bogachev et~al.(2019)Bogachev, R{\"o}ckner, and
  Shaposhnikov]{bogachev2019convergence}
Bogachev, V.~I., R{\"o}ckner, M., and Shaposhnikov, S.~V.
\newblock Convergence in variation of solutions of nonlinear
  {F}okker--{P}lanck--{K}olmogorov equations to stationary measures.
\newblock \emph{Journal of Functional Analysis}, 276\penalty0 (12):\penalty0
  3681--3713, 2019.

\bibitem[Butkovsky(2014)]{butkowsky2013ergodic}
Butkovsky, O.~A.
\newblock On ergodic properties of nonlinear {M}arkov chains and stochastic
  {M}c{K}ean-{V}lasov equations.
\newblock \emph{Theory of Probability and its Applications}, 58\penalty0
  (4):\penalty0 661--674, 2014.

\bibitem[Carmona \& Delarue(2018)Carmona and Delarue]{carmona2018probabilistic}
Carmona, R. and Delarue, F.
\newblock \emph{Probabilistic Theory of Mean Field Games with Applications
  I-II}.
\newblock Springer, 2018.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018global}
Chizat, L. and Bach, F.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3040--3050, 2018.

\bibitem[Delarue \& Tse(2021)Delarue and Tse]{delarue2021uniform}
Delarue, F. and Tse, A.
\newblock Uniform in time weak propagation of chaos on the {T}orus.
\newblock \emph{arXiv preprint arXiv:2104.14973}, 2021.

\bibitem[Dupuis \& Ellis(1997)Dupuis and Ellis]{dupuis1997weak}
Dupuis, P. and Ellis, R.~S.
\newblock \emph{A weak convergence approach to the theory of large deviations}.
\newblock John Wiley \& Sons, Inc., New York, 1997.

\bibitem[Fox et~al.(2015)Fox, Pakman, and Tishby]{fox2015taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock \emph{arXiv preprint arXiv:1512.08562}, 2015.

\bibitem[Funaki(1984)]{funaki1984certain}
Funaki, T.
\newblock A certain class of diffusion processes associated with nonlinear
  parabolic equations.
\newblock \emph{Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und Verwandte
  Gebiete}, 67\penalty0 (3):\penalty0 331--348, 1984.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2160--2169. PMLR, 2019.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1352--1361. PMLR, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1870. PMLR, 2018.

\bibitem[Hammersley et~al.(2021)Hammersley, {\v S}i\v{s}ka, and
  Szpruch]{hammersley2021mckean}
Hammersley, W. R.~P., {\v S}i\v{s}ka, D., and Szpruch, {\L}.
\newblock Mc{K}ean-{V}lasov {SDE}s under measure dependent {L}yapunov
  conditions.
\newblock \emph{Annales de l'Institut Henri Poincar\'{e} Probabilit\'{e}s et
  Statistiques}, 57\penalty0 (2):\penalty0 1032--1057, 2021.
\newblock ISSN 0246-0203.

\bibitem[Hern{\'a}ndez-Lerma \& Lasserre(2012)Hern{\'a}ndez-Lerma and
  Lasserre]{hernandez2012discrete}
Hern{\'a}ndez-Lerma, O. and Lasserre, J.~B.
\newblock \emph{Discrete-time Markov control processes: basic optimality
  criteria}, volume~30.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Hu et~al.(2019)Hu, Kazeykina, and Ren]{hu2019meanode}
Hu, K., Kazeykina, A., and Ren, Z.
\newblock Mean-field {L}angevin system, optimal control and deep neural
  networks.
\newblock \emph{arXiv:1909.07278}, 2019.

\bibitem[Hu et~al.(2021)Hu, Ren, {\v{S}}i{\v{s}}ka, and Szpruch]{hu2019mean}
Hu, K., Ren, Z., {\v{S}}i{\v{s}}ka, D., and Szpruch, {\L}.
\newblock Mean-field langevin dynamics and energy landscape of neural networks.
\newblock \emph{Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}s et
  Statistiques}, 57\penalty0 (4):\penalty0 2043--2065, 2021.

\bibitem[Jabir et~al.(2019)Jabir, {\v{S}}i{\v{s}}ka, and
  Szpruch]{jabir2019mean}
Jabir, J.-F., {\v{S}}i{\v{s}}ka, D., and Szpruch, {\L}.
\newblock Mean-field neural {ODE}s via relaxed optimal control.
\newblock \emph{arXiv preprint arXiv:1912.05475}, 2019.

\bibitem[Jordan et~al.(1998)Jordan, Kinderlehrer, and
  Otto]{jordan1998variational}
Jordan, R., Kinderlehrer, D., and Otto, F.
\newblock The variational formulation of the {F}okker--{P}lanck equation.
\newblock \emph{SIAM journal on mathematical analysis}, 29\penalty0
  (1):\penalty0 1--17, 1998.

\bibitem[Kinderlehrer \& Stampacchia(2000)Kinderlehrer and
  Stampacchia]{kinderlehrer2000introduction}
Kinderlehrer, D. and Stampacchia, G.
\newblock \emph{An introduction to variational inequalities and their
  applications}.
\newblock SIAM, 2000.

\bibitem[Komorowski \& Walczuk(2012)Komorowski and
  Walczuk]{komorowski2012central}
Komorowski, T. and Walczuk, A.
\newblock Central limit theorem for {M}arkov processes with spectral gap in the
  {W}asserstein metric.
\newblock \emph{Stochastic Processes and their Applications}, 122\penalty0
  (5):\penalty0 2155--2184, 2012.

\bibitem[Krylov(2008)]{krylov2008controlled}
Krylov, N.~V.
\newblock \emph{Controlled diffusion processes}, volume~14.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Kunze(2011)]{kunze2011pettis}
Kunze, M.
\newblock A {P}ettis-type integral and applications to transition semigroups.
\newblock \emph{Czechoslovak mathematical journal}, 61\penalty0 (2):\penalty0
  437--459, 2011.

\bibitem[Lasota \& Mackey(2013)Lasota and Mackey]{lasota2013chaos}
Lasota, A. and Mackey, M.~C.
\newblock \emph{Chaos, fractals, and noise: stochastic aspects of dynamics},
  volume~97.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Li et~al.(2021)Li, Wei, Chi, Gu, and Chen]{li2021softmax}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y.
\newblock Softmax policy gradient methods can take exponential time to
  converge.
\newblock \emph{arXiv preprint arXiv:2102.11270}, 2021.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Liu, B., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural trust region/proximal policy optimization attains globally
  optimal policy.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 10565--10576, 2019.

\bibitem[Manita \& Shaposhnikov(2014)Manita and
  Shaposhnikov]{manita2014nonlinear}
Manita, O. and Shaposhnikov, S.
\newblock Nonlinear parabolic equations for measures.
\newblock \emph{St. Petersburg Mathematical Journal}, 25\penalty0 (1):\penalty0
  43--62, 2014.

\bibitem[Manita et~al.(2015)Manita, Romanov, and
  Shaposhnikov]{manita2015uniqueness}
Manita, O.~A., Romanov, M.~S., and Shaposhnikov, S.~V.
\newblock On uniqueness of solutions to nonlinear
  {F}okker--{P}lanck--{K}olmogorov equations.
\newblock \emph{Nonlinear Analysis}, 128:\penalty0 199--226, 2015.

\bibitem[Mei et~al.(2020)Mei, Xiao, Szepesvari, and Schuurmans]{mei2020global}
Mei, J., Xiao, C., Szepesvari, C., and Schuurmans, D.
\newblock On the global convergence rates of softmax policy gradient methods.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6820--6829. PMLR, 2020.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pp.\  2388--2464. PMLR,
  2019.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{{N}ature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Otto \& Villani(2000)Otto and Villani]{otto2000generalization}
Otto, F. and Villani, C.
\newblock Generalization of an inequality by {T}alagrand and links with the
  logarithmic {S}obolev inequality.
\newblock \emph{Journal of Functional Analysis}, 173\penalty0 (2):\penalty0
  361--400, 2000.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rotskoff \& Vanden-Eijnden(2018)Rotskoff and
  Vanden-Eijnden]{rotskoff2018neural}
Rotskoff, G.~M. and Vanden-Eijnden, E.
\newblock Neural networks as interacting particle systems: Asymptotic convexity
  of the loss landscape and universal scaling of the approximation error.
\newblock \emph{stat}, 1050:\penalty0 22, 2018.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and {G}o through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Sirignano \& Spiliopoulos(2019)Sirignano and
  Spiliopoulos]{sirignano2019asymptotics}
Sirignano, J. and Spiliopoulos, K.
\newblock Asymptotics of reinforcement learning with neural networks.
\newblock \emph{arXiv preprint arXiv:1911.07304}, 2019.

\bibitem[Sirignano \& Spiliopoulos(2021)Sirignano and
  Spiliopoulos]{sirignano2021mean}
Sirignano, J. and Spiliopoulos, K.
\newblock Mean field analysis of deep neural networks.
\newblock \emph{Mathematics of Operations Research}, 2021.

\bibitem[{\v{S}}i{\v{s}}ka \& Szpruch(2020){\v{S}}i{\v{s}}ka and
  Szpruch]{vsivska2020gradient}
{\v{S}}i{\v{s}}ka, D. and Szpruch, {\L}.
\newblock Gradient flows for regularized stochastic control problems.
\newblock \emph{arXiv preprint arXiv:2006.05956}, 2020.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning}.
\newblock MIT Press, 2018.

\bibitem[Vieillard et~al.(2020)Vieillard, Kozuno, Scherrer, Pietquin, Munos,
  and Geist]{vieillard2020leverage}
Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M.
\newblock Leverage the average: an analysis of {KL} regularization in
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  12163--12174, 2020.

\bibitem[Villani(2009)]{villani2009optimal}
Villani, C.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer, 2009.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in {S}tarcraft {II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2020)Wang, Cai, Yang, and Wang]{wang2020neural}
Wang, L., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJgQfkSYDS}.

\bibitem[Weinan(2017)]{weinan2017proposal}
Weinan, E.
\newblock A proposal on machine learning via dynamical systems.
\newblock \emph{Communications in Mathematics and Statistics}, 5\penalty0
  (1):\penalty0 1--11, 2017.

\bibitem[Ziebart et~al.(2010)Ziebart, Bagnell, and Dey]{ziebart2010modeling}
Ziebart, B.~D., Bagnell, J.~A., and Dey, A.~K.
\newblock Modeling interaction via the principle of maximum causal entropy.
\newblock In \emph{ICML}, 2010.

\end{thebibliography}
