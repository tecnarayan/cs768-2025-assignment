\begin{thebibliography}{10}

\bibitem{campbell1968application}
Fergus~W Campbell and John~G Robson.
\newblock Application of fourier analysis to the visibility of gratings.
\newblock {\em The Journal of physiology}, 197(3):551, 1968.

\bibitem{solomon1994visual}
Joshua~A Solomon and Denis~G Pelli.
\newblock The visual filter mediating letter identification.
\newblock {\em Nature}, 369(6479):395--397, 1994.

\bibitem{majaj2002role}
Najib~J Majaj, Denis~G Pelli, Peri Kurshan, and Melanie Palomares.
\newblock The role of spatial frequency channels in letter identification.
\newblock {\em Vision research}, 42(9):1165--1184, 2002.

\bibitem{fletcher1940auditory}
Harvey Fletcher.
\newblock Auditory patterns.
\newblock {\em Reviews of modern physics}, 12(1):47, 1940.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115:211--252, 2015.

\bibitem{szegedy2014intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em International Conference on Learning Representations}, 2014.

\bibitem{geirhos2018imagenet}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A
  Wichmann, and Wieland Brendel.
\newblock Imagenet-trained cnns are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock {\em arXiv preprint arXiv:1811.12231}, 2018.

\bibitem{hermann2020origins}
Katherine Hermann, Ting Chen, and Simon Kornblith.
\newblock The origins and prevalence of texture bias in convolutional neural
  networks.
\newblock {\em Advances in Neural Information Processing Systems},
  33:19000--19015, 2020.

\bibitem{tuli2021convolutional}
Shikhar Tuli, Ishita Dasgupta, Erin Grant, and Thomas~L Griffiths.
\newblock Are convolutional neural networks or transformers more like human
  vision?
\newblock {\em arXiv preprint arXiv:2105.07197}, 2021.

\bibitem{landau1992syntactic}
Barbara Landau, Linda~B Smith, and Susan Jones.
\newblock Syntactic context and the shape bias in children's and adults'
  lexical learning.
\newblock {\em Journal of Memory and Language}, 31(6):807--825, 1992.

\bibitem{wang2020high}
Haohan Wang, Xindi Wu, Zeyi Huang, and Eric~P Xing.
\newblock High-frequency component helps explain the generalization of
  convolutional neural networks.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 8684--8694, 2020.

\bibitem{lieber2023sensitivity}
Justin~D Lieber, Gerick~M Lee, Najib~J Majaj, and J~Anthony Movshon.
\newblock Sensitivity to naturalistic texture relies primarily on high spatial
  frequencies.
\newblock {\em Journal of vision}, 23(2):4--4, 2023.

\bibitem{orucc2009scale}
Ipek Oru{\c{c}} and Michael~S Landy.
\newblock Scale dependence and channel switching in letter identification.
\newblock {\em Journal of Vision}, 9(9):4--4, 2009.

\bibitem{orucc2010critical}
{\.I}pek Oru{\c{c}} and Jason~JS Barton.
\newblock Critical frequencies in the perception of letters, faces, and novel
  shapes: Evidence for limited scale invariance for faces.
\newblock {\em Journal of Vision}, 10(12):20--20, 2010.

\bibitem{rahaman2019spectral}
Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred
  Hamprecht, Yoshua Bengio, and Aaron Courville.
\newblock On the spectral bias of neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  5301--5310. PMLR, 2019.

\bibitem{geirhos2020shortcut}
Robert Geirhos, J{\"o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock {\em Nature Machine Intelligence}, 2(11):665--673, 2020.

\bibitem{wang2022frequency}
Shunxin Wang, Raymond Veldhuis, Christoph Brune, and Nicola Strisciuglio.
\newblock Frequency shortcut learning in neural networks.
\newblock In {\em NeurIPS 2022 Workshop on Distribution Shifts: Connecting
  Methods and Applications}, 2022.

\bibitem{li2023robust}
Zhe Li, Josue Ortega~Caro, Evgenia Rusak, Wieland Brendel, Matthias Bethge,
  Fabio Anselmi, Ankit~B Patel, Andreas~S Tolias, and Xaq Pitkow.
\newblock Robust deep learning object recognition models rely on low frequency
  information in natural images.
\newblock {\em PLOS Computational Biology}, 19(3):e1010932, 2023.

\bibitem{yin2019fourier}
Dong Yin, Raphael Gontijo~Lopes, Jon Shlens, Ekin~Dogus Cubuk, and Justin
  Gilmer.
\newblock A fourier perspective on model robustness in computer vision.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{gavrikov2023extended}
Paul Gavrikov, Janis Keuper, and Margret Keuper.
\newblock An extended study of human-like behavior under adversarial training.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2360--2367, 2023.

\bibitem{abello2021dissecting}
Antonio~A Abello, Roberto Hirata, and Zhangyang Wang.
\newblock Dissecting the high-frequency bias in convolutional neural networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 863--871, 2021.

\bibitem{geirhos2018generalisation}
Robert Geirhos, Carlos~RM Temme, Jonas Rauber, Heiko~H Sch{\"u}tt, Matthias
  Bethge, and Felix~A Wichmann.
\newblock Generalisation in humans and deep neural networks.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{baker2018deep}
Nicholas Baker, Hongjing Lu, Gennady Erlikhman, and Philip~J Kellman.
\newblock Deep convolutional networks do not classify based on global object
  shape.
\newblock {\em PLoS computational biology}, 14(12):e1006613, 2018.

\bibitem{nguyen2015deep}
Anh Nguyen, Jason Yosinski, and Jeff Clune.
\newblock Deep neural networks are easily fooled: High confidence predictions
  for unrecognizable images.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 427--436, 2015.

\bibitem{elsayed2018adversarial}
Gamaleldin Elsayed, Shreya Shankar, Brian Cheung, Nicolas Papernot, Alexey
  Kurakin, Ian Goodfellow, and Jascha Sohl-Dickstein.
\newblock Adversarial examples that fool both computer vision and time-limited
  humans.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{veerabadran2023subtle}
Vijay Veerabadran, Josh Goldman, Shreya Shankar, Brian Cheung, Nicolas
  Papernot, Alexey Kurakin, Ian Goodfellow, Jonathon Shlens, Jascha
  Sohl-Dickstein, Michael~C Mozer, et~al.
\newblock Subtle adversarial image manipulations influence both human and
  machine perception.
\newblock {\em Nature Communications}, 14(1):4933, 2023.

\bibitem{zhou2019humans}
Zhenglong Zhou and Chaz Firestone.
\newblock Humans can decipher adversarial images.
\newblock {\em Nature communications}, 10(1):1334, 2019.

\bibitem{bernhard2021impact}
R{\'e}mi Bernhard, Pierre-Alain Mo{\"e}llic, Martial Mermillod, Yannick
  Bourrier, Romain Cohendet, Miguel Solinas, and Marina Reyboz.
\newblock Impact of spatial frequency based constraints on adversarial
  robustness.
\newblock In {\em 2021 International Joint Conference on Neural Networks
  (IJCNN)}, pages 1--8. IEEE, 2021.

\bibitem{maiya2021frequency}
Shishira~R Maiya, Max Ehrlich, Vatsal Agarwal, Ser-Nam Lim, Tom Goldstein, and
  Abhinav Shrivastava.
\newblock A frequency perspective of adversarial robustness.
\newblock {\em arXiv preprint arXiv:2111.00861}, 2021.

\bibitem{fukushima1982neocognitron}
Kunihiko Fukushima and Sei Miyake.
\newblock Neocognitron: A new algorithm for pattern recognition tolerant of
  deformations and shifts in position.
\newblock {\em Pattern recognition}, 15(6):455--469, 1982.

\bibitem{riesenhuber1999hierarchical}
Maximilian Riesenhuber and Tomaso Poggio.
\newblock Hierarchical models of object recognition in cortex.
\newblock {\em Nature neuroscience}, 2(11):1019--1025, 1999.

\bibitem{yamins2016using}
Daniel~LK Yamins and James~J DiCarlo.
\newblock Using goal-driven deep learning models to understand sensory cortex.
\newblock {\em Nature neuroscience}, 19(3):356--365, 2016.

\bibitem{khaligh2014deep}
Seyed-Mahdi Khaligh-Razavi and Nikolaus Kriegeskorte.
\newblock Deep supervised, but not unsupervised, models may explain it cortical
  representation.
\newblock {\em PLoS computational biology}, 10(11):e1003915, 2014.

\bibitem{schrimpf2018brain}
Martin Schrimpf, Jonas Kubilius, Ha~Hong, Najib~J Majaj, Rishi Rajalingham,
  Elias~B Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska
  Geiger, et~al.
\newblock Brain-score: Which artificial neural network for object recognition
  is most brain-like?
\newblock {\em BioRxiv}, page 407007, 2018.

\bibitem{dapello2020simulating}
Joel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, David Cox, and
  James~J DiCarlo.
\newblock Simulating a primary visual cortex at the front of cnns improves
  robustness to image perturbations.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13073--13087, 2020.

\bibitem{geirhos2021partial}
Robert Geirhos, Kantharaju Narayanappa, Benjamin Mitzkus, Tizian Thieringer,
  Matthias Bethge, Felix~A Wichmann, and Wieland Brendel.
\newblock Partial success in closing the gap between human and machine vision.
\newblock {\em Advances in Neural Information Processing Systems},
  34:23885--23899, 2021.

\bibitem{feather2022model}
Jenelle Feather, Guillaume Leclerc, Aleksander M{\k{a}}dry, and Josh~H
  McDermott.
\newblock Model metamers illuminate divergences between biological and
  artificial neural networks.
\newblock {\em bioRxiv}, pages 2022--05, 2022.

\bibitem{kriegeskorte2008representational}
Nikolaus Kriegeskorte, Marieke Mur, and Peter~A Bandettini.
\newblock Representational similarity analysis-connecting the branches of
  systems neuroscience.
\newblock {\em Frontiers in systems neuroscience}, page~4, 2008.

\bibitem{miller1995wordnet}
George~A Miller.
\newblock Wordnet: a lexical database for english.
\newblock {\em Communications of the ACM}, 38(11):39--41, 1995.

\bibitem{burt1987laplacian}
Peter~J Burt and Edward~H Adelson.
\newblock The laplacian pyramid as a compact image code.
\newblock In {\em Readings in computer vision}, pages 671--679. Elsevier, 1987.

\bibitem{henninger2021lab}
Felix Henninger, Yury Shevchenko, Ulf~K Mertens, Pascal~J Kieslich, and
  Benjamin~E Hilbig.
\newblock lab. js: A free, open, online study builder.
\newblock {\em Behavior Research Methods}, pages 1--18, 2021.

\bibitem{lange2015just}
Kristian Lange, Simone K{\"u}hn, and Elisa Filevich.
\newblock " just another tool for online studies”(jatos): An easy solution
  for setup and management of web servers supporting online studies.
\newblock {\em PloS one}, 10(6):e0130834, 2015.

\bibitem{marcel2010torchvision}
S{\'e}bastien Marcel and Yann Rodriguez.
\newblock Torchvision the machine-vision package of torch.
\newblock In {\em Proceedings of the 18th ACM international conference on
  Multimedia}, pages 1485--1488, 2010.

\bibitem{wu2018unsupervised}
Zhirong Wu, Yuanjun Xiong, Stella~X Yu, and Dahua Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3733--3742, 2018.

\bibitem{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 9729--9738, 2020.

\bibitem{chen2020improved}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{misra2020self}
Ishan Misra and Laurens van~der Maaten.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 6707--6717, 2020.

\bibitem{tian2020makes}
Yonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and
  Phillip Isola.
\newblock What makes for good views for contrastive learning?
\newblock {\em Advances in neural information processing systems},
  33:6827--6839, 2020.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In {\em Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part V 16}, pages 491--507.
  Springer, 2020.

\bibitem{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock {\em Advances in Neural Information Processing Systems},
  33:3533--3545, 2020.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{wightman2019pytorch}
Ross Wightman et~al.
\newblock Pytorch image models, 2019.

\bibitem{yalniz2019billion}
I~Zeki Yalniz, Herv{\'e} J{\'e}gou, Kan Chen, Manohar Paluri, and Dhruv
  Mahajan.
\newblock Billion-scale semi-supervised learning for image classification.
\newblock {\em arXiv preprint arXiv:1905.00546}, 2019.

\bibitem{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 10687--10698, 2020.

\bibitem{nicolae2018adversarial}
Maria-Irina Nicolae, Mathieu Sinn, Minh~Ngoc Tran, Beat Buesser, Ambrish Rawat,
  Martin Wistuba, Valentina Zantedeschi, Nathalie Baracaldo, Bryant Chen, Heiko
  Ludwig, et~al.
\newblock Adversarial robustness toolbox v1. 0.0.
\newblock {\em arXiv preprint arXiv:1807.01069}, 2018.

\bibitem{blakemore1969existence}
Colin Blakemore and Fergus~W Campbell.
\newblock On the existence of neurones in the human visual system selectively
  sensitive to the orientation and size of retinal images.
\newblock {\em The Journal of physiology}, 203(1):237--260, 1969.

\bibitem{dehghani2023scaling}
Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan
  Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim
  Alabdulmohsin, et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock {\em arXiv preprint arXiv:2302.05442}, 2023.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436--444, 2015.

\bibitem{parish1991object}
David~H Parish and George Sperling.
\newblock Object spatial frequencies, retinal spatial frequencies, noise, and
  the efficiency of letter discrimination.
\newblock {\em Vision research}, 31(7-8):1399--1415, 1991.

\end{thebibliography}
