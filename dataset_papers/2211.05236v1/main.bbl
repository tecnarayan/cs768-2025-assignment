\begin{thebibliography}{78}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2018)Agarwal, Beygelzimer, Dud{\'\i}k, Langford, and
  Wallach]{agarwal2018reductions}
A.~Agarwal, A.~Beygelzimer, M.~Dud{\'\i}k, J.~Langford, and H.~Wallach.
\newblock A reductions approach to fair classification.
\newblock In \emph{International Conference on Machine Learning}, pages 60--69.
  PMLR, 2018.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2204.14198}, 2022.

\bibitem[Andreassen et~al.(2021)Andreassen, Bahri, Neyshabur, and
  Roelofs]{andreassen2021evolution}
A.~Andreassen, Y.~Bahri, B.~Neyshabur, and R.~Roelofs.
\newblock The evolution of out-of-distribution robustness throughout
  fine-tuning.
\newblock \emph{arXiv preprint arXiv:2106.15831}, 2021.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
M.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Bachman et~al.(2014)Bachman, Alsharif, and
  Precup]{bachman2014learning}
P.~Bachman, O.~Alsharif, and D.~Precup.
\newblock Learning with pseudo-ensembles.
\newblock \emph{Advances in neural information processing systems}, 27, 2014.

\bibitem[Baevski et~al.(2022)Baevski, Hsu, Xu, Babu, Gu, and
  Auli]{baevski2022data2vec}
A.~Baevski, W.-N. Hsu, Q.~Xu, A.~Babu, J.~Gu, and M.~Auli.
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock \emph{arXiv preprint arXiv:2202.03555}, 2022.

\bibitem[Bao et~al.(2021)Bao, Dong, and Wei]{bao2021beit}
H.~Bao, L.~Dong, and F.~Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021.

\bibitem[Beery et~al.(2020)Beery, Cole, and Gjoka]{beery2020iwildcam}
S.~Beery, E.~Cole, and A.~Gjoka.
\newblock The iwildcam 2020 competition dataset.
\newblock \emph{arXiv preprint arXiv:2004.10340}, 2020.

\bibitem[Berthet et~al.(2020)Berthet, Blondel, Teboul, Cuturi, Vert, and
  Bach]{berthet2020learning}
Q.~Berthet, M.~Blondel, O.~Teboul, M.~Cuturi, J.-P. Vert, and F.~Bach.
\newblock Learning with differentiable pertubed optimizers.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 9508--9519, 2020.

\bibitem[Biglan et~al.(2000)Biglan, Ary, and Wagenaar]{biglan2000value}
A.~Biglan, D.~Ary, and A.~C. Wagenaar.
\newblock The value of interrupted time-series experiments for community
  intervention research.
\newblock \emph{Prevention Science}, 1\penalty0 (1):\penalty0 31--49, 2000.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
R.~Bommasani, D.~A. Hudson, E.~Adeli, R.~Altman, S.~Arora, S.~von Arx, M.~S.
  Bernstein, J.~Bohg, A.~Bosselut, E.~Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Borkan et~al.(2019)Borkan, Dixon, Sorensen, Thain, and
  Vasserman]{borkan2019nuanced}
D.~Borkan, L.~Dixon, J.~Sorensen, N.~Thain, and L.~Vasserman.
\newblock Nuanced metrics for measuring unintended bias with real data for text
  classification.
\newblock In \emph{Companion proceedings of the 2019 world wide web
  conference}, pages 491--500, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, J{\'e}gou, Mairal,
  Bojanowski, and Joulin]{caron2021emerging}
M.~Caron, H.~Touvron, I.~Misra, H.~J{\'e}gou, J.~Mairal, P.~Bojanowski, and
  A.~Joulin.
\newblock Emerging properties in self-supervised vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9650--9660, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.

\bibitem[Christian et~al.(2010)Christian, Murray-Kolb, Khatry, Katz, Schaefer,
  Cole, LeClerq, and Tielsch]{christian2010prenatal}
P.~Christian, L.~E. Murray-Kolb, S.~K. Khatry, J.~Katz, B.~A. Schaefer, P.~M.
  Cole, S.~C. LeClerq, and J.~M. Tielsch.
\newblock Prenatal micronutrient supplementation and intellectual and motor
  function in early school-aged children in nepal.
\newblock \emph{Jama}, 304\penalty0 (24):\penalty0 2716--2723, 2010.

\bibitem[Creager et~al.(2019)Creager, Madras, Jacobsen, Weis, Swersky, Pitassi,
  and Zemel]{creager2019flexibly}
E.~Creager, D.~Madras, J.-H. Jacobsen, M.~Weis, K.~Swersky, T.~Pitassi, and
  R.~Zemel.
\newblock Flexibly fair representation learning by disentanglement.
\newblock In \emph{International conference on machine learning}, pages
  1436--1445. PMLR, 2019.

\bibitem[Creager et~al.(2021)Creager, Jacobsen, and
  Zemel]{creager2021environment}
E.~Creager, J.-H. Jacobsen, and R.~Zemel.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2189--2200. PMLR, 2021.

\bibitem[Crump et~al.(2009)Crump, Hotz, Imbens, and Mitnik]{crump2009dealing}
R.~K. Crump, V.~J. Hotz, G.~W. Imbens, and O.~A. Mitnik.
\newblock Dealing with limited overlap in estimation of average treatment
  effects.
\newblock \emph{Biometrika}, 96\penalty0 (1):\penalty0 187--199, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Donini et~al.(2018)Donini, Oneto, Ben-David, Shawe-Taylor, and
  Pontil]{donini2018empirical}
M.~Donini, L.~Oneto, S.~Ben-David, J.~S. Shawe-Taylor, and M.~Pontil.
\newblock Empirical risk minimization under fairness constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Dunnmon et~al.(2019)Dunnmon, Yi, Langlotz, R\'e, Rubin, and
  Lungren]{DunYiLanReetal19}
J.~A. Dunnmon, D.~Yi, C.~P. Langlotz, C.~R\'e, D.~L. Rubin, and M.~P. Lungren.
\newblock Assessment of convolutional neural networks for automated
  classification of chest radiographs.
\newblock \emph{Radiology}, 290\penalty0 (2):\penalty0 537--544, 2019.

\bibitem[Dwibedi et~al.(2021)Dwibedi, Aytar, Tompson, Sermanet, and
  Zisserman]{dwibedi2021little}
D.~Dwibedi, Y.~Aytar, J.~Tompson, P.~Sermanet, and A.~Zisserman.
\newblock With a little help from my friends: Nearest-neighbor contrastive
  learning of visual representations.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9588--9597, 2021.

\bibitem[Dwork et~al.(2012)Dwork, Hardt, Pitassi, Reingold, and
  Zemel]{dwork2012fairness}
C.~Dwork, M.~Hardt, T.~Pitassi, O.~Reingold, and R.~Zemel.
\newblock Fairness through awareness.
\newblock In \emph{Proceedings of the 3rd innovations in theoretical computer
  science conference}, pages 214--226, 2012.

\bibitem[Feldman et~al.(2015)Feldman, Friedler, Moeller, Scheidegger, and
  Venkatasubramanian]{feldman2015certifying}
M.~Feldman, S.~A. Friedler, J.~Moeller, C.~Scheidegger, and
  S.~Venkatasubramanian.
\newblock Certifying and removing disparate impact.
\newblock In \emph{proceedings of the 21th ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 259--268, 2015.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Y.~Ganin, E.~Ustinova, H.~Ajakan, P.~Germain, H.~Larochelle, F.~Laviolette,
  M.~Marchand, and V.~Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The journal of machine learning research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Gong et~al.(2021)Gong, Wang, and Liu]{gong2021alphamatch}
C.~Gong, D.~Wang, and Q.~Liu.
\newblock Alphamatch: Improving consistency for semi-supervised learning with
  alpha-divergence.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13683--13692, 2021.

\bibitem[Goyal et~al.(2022)Goyal, Duval, Seessel, Caron, Singh, Misra, Sagun,
  Joulin, and Bojanowski]{goyal2022vision}
P.~Goyal, Q.~Duval, I.~Seessel, M.~Caron, M.~Singh, I.~Misra, L.~Sagun,
  A.~Joulin, and P.~Bojanowski.
\newblock Vision models are more robust and fair when pretrained on uncurated
  images without supervision.
\newblock \emph{arXiv preprint arXiv:2202.08360}, 2022.

\bibitem[Grill et~al.(2020)Grill, Strub, Altch{\'e}, Tallec, Richemond,
  Buchatskaya, Doersch, Avila~Pires, Guo, Gheshlaghi~Azar,
  et~al.]{grill2020bootstrap}
J.-B. Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Avila~Pires, Z.~Guo, M.~Gheshlaghi~Azar, et~al.
\newblock Bootstrap your own latent-a new approach to self-supervised learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 21271--21284, 2020.

\bibitem[Gulrajani and Lopez-Paz(2020)]{gulrajani2020search}
I.~Gulrajani and D.~Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Hardt et~al.(2016)Hardt, Price, and Srebro]{hardt2016equality}
M.~Hardt, E.~Price, and N.~Srebro.
\newblock Equality of opportunity in supervised learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 9729--9738, 2020.

\bibitem[He et~al.(2021)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{MaskedAutoencoders2021}
K.~He, X.~Chen, S.~Xie, Y.~Li, P.~Doll{\'a}r, and R.~Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock \emph{arXiv:2111.06377}, 2021.

\bibitem[Hurley and Adebayo(2017)]{HurAde17}
M.~Hurley and J.~Adebayo.
\newblock Credit scoring in the era of big data.
\newblock \emph{Yale Journal of Law and Technology}, 18:\penalty0 148--216,
  2017.

\bibitem[Idrissi et~al.(2022)Idrissi, Arjovsky, Pezeshki, and
  Lopez-Paz]{idrissi2022simple}
B.~Y. Idrissi, M.~Arjovsky, M.~Pezeshki, and D.~Lopez-Paz.
\newblock Simple data balancing achieves competitive worst-group-accuracy.
\newblock In \emph{Conference on Causal Learning and Reasoning}, pages
  336--351. PMLR, 2022.

\bibitem[Kamiran and Calders(2012)]{kamiran2012data}
F.~Kamiran and T.~Calders.
\newblock Data preprocessing techniques for classification without
  discrimination.
\newblock \emph{Knowledge and information systems}, 33\penalty0 (1):\penalty0
  1--33, 2012.

\bibitem[Kehrenberg et~al.(2020)Kehrenberg, Bartlett, Thomas, and
  Quadrianto]{kehrenberg2020null}
T.~Kehrenberg, M.~Bartlett, O.~Thomas, and N.~Quadrianto.
\newblock Null-sampling for interpretable and fair representations.
\newblock In \emph{European Conference on Computer Vision}, pages 565--580.
  Springer, 2020.

\bibitem[Kim et~al.(2022)Kim, Wang, Sclaroff, and Saenko]{kim2022broad}
D.~Kim, K.~Wang, S.~Sclaroff, and K.~Saenko.
\newblock A broad study of pre-training for domain generalization and
  adaptation.
\newblock \emph{arXiv preprint arXiv:2203.11819}, 2022.

\bibitem[King and Nielsen(2019)]{king2019propensity}
G.~King and R.~Nielsen.
\newblock Why propensity scores should not be used for matching.
\newblock \emph{Political Analysis}, 27\penalty0 (4):\penalty0 435--454, 2019.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, et~al.]{koh2021wilds}
P.~W. Koh, S.~Sagawa, H.~Marklund, S.~M. Xie, M.~Zhang, A.~Balsubramani, W.~Hu,
  M.~Yasunaga, R.~L. Phillips, I.~Gao, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Koohpayegani et~al.(2021)Koohpayegani, Tejankar, and
  Pirsiavash]{koohpayegani2021mean}
S.~A. Koohpayegani, A.~Tejankar, and H.~Pirsiavash.
\newblock Mean shift for self-supervised learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10326--10335, 2021.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Le~Priol, and Courville]{krueger2021out}
D.~Krueger, E.~Caballero, J.-H. Jacobsen, A.~Zhang, J.~Binas, D.~Zhang,
  R.~Le~Priol, and A.~Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock In \emph{International Conference on Machine Learning}, pages
  5815--5826. PMLR, 2021.

\bibitem[Lahoti et~al.(2019)Lahoti, Gummadi, and
  Weikum]{lahoti2019operationalizing}
P.~Lahoti, K.~Gummadi, and G.~Weikum.
\newblock Operationalizing individual fairness with pairwise fair
  representations.
\newblock \emph{Proceedings of the VLDB Endowment}, 13\penalty0 (4):\penalty0
  506--518, 2019.

\bibitem[Li et~al.(2017)Li, Yang, Song, and Hospedales]{li2017deeper}
D.~Li, Y.~Yang, Y.-Z. Song, and T.~M. Hospedales.
\newblock Deeper, broader and artier domain generalization.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 5542--5550, 2017.

\bibitem[Lienen and H\"{u}llermeier(2021)]{lienen2021credal}
J.~Lienen and E.~H\"{u}llermeier.
\newblock Credal self-supervised learning.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 14370--14382. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/7866c91c59f8bffc92a79a7cd09f9af9-Paper.pdf}.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{liu2022convnet}
Z.~Liu, H.~Mao, C.-Y. Wu, C.~Feichtenhofer, T.~Darrell, and S.~Xie.
\newblock A convnet for the 2020s.
\newblock \emph{arXiv preprint arXiv:2201.03545}, 2022.

\bibitem[Loshchilov and Hutter(2017)]{DBLP:conf/iclr/LoshchilovH17}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR:} stochastic gradient descent with warm restarts.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=Skq89Scxx}.

\bibitem[Loshchilov and Hutter(2018)]{loshchilov2018decoupled}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Madras et~al.(2018)Madras, Creager, Pitassi, and
  Zemel]{madras2018learning}
D.~Madras, E.~Creager, T.~Pitassi, and R.~Zemel.
\newblock Learning adversarially fair and transferable representations.
\newblock In \emph{International Conference on Machine Learning}, pages
  3384--3393. PMLR, 2018.

\bibitem[Mahajan et~al.(2021)Mahajan, Tople, and Sharma]{mahajan2021domain}
D.~Mahajan, S.~Tople, and A.~Sharma.
\newblock Domain generalization using causal matching.
\newblock In \emph{International Conference on Machine Learning}, pages
  7313--7324. PMLR, 2021.

\bibitem[Muandet et~al.(2013)Muandet, Balduzzi, and
  Sch{\"o}lkopf]{muandet2013domain}
K.~Muandet, D.~Balduzzi, and B.~Sch{\"o}lkopf.
\newblock Domain generalization via invariant feature representation.
\newblock In \emph{International Conference on Machine Learning}, pages 10--18.
  PMLR, 2013.

\bibitem[Oneto et~al.(2020)Oneto, Donini, Luise, Ciliberto, Maurer, and
  Pontil]{oneto2020exploiting}
L.~Oneto, M.~Donini, G.~Luise, C.~Ciliberto, A.~Maurer, and M.~Pontil.
\newblock Exploiting mmd and sinkhorn divergences for fair and transferable
  representation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15360--15370, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Quadrianto et~al.(2019)Quadrianto, Sharmanska, and
  Thomas]{quadrianto2019discovering}
N.~Quadrianto, V.~Sharmanska, and O.~Thomas.
\newblock Discovering fair representations in the data domain.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 8227--8236, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Romiti et~al.(2022)Romiti, Inskip, Sharmanska, and
  Quadrianto]{RomInsShaQua22}
S.~Romiti, C.~Inskip, V.~Sharmanska, and N.~Quadrianto.
\newblock Realpatch: {A} statistical matching framework for model patching with
  real samples.
\newblock \emph{CoRR}, abs/2208.02192, 2022.

\bibitem[Rosenbaum and Rubin(1983)]{rosenbaum1983central}
P.~R. Rosenbaum and D.~B. Rubin.
\newblock The central role of the propensity score in observational studies for
  causal effects.
\newblock \emph{Biometrika}, 70\penalty0 (1):\penalty0 41--55, 1983.

\bibitem[Rosenbaum and Rubin(1985)]{rosenbaum1985constructing}
P.~R. Rosenbaum and D.~B. Rubin.
\newblock Constructing a control group using multivariate matched sampling
  methods that incorporate the propensity score.
\newblock \emph{The American Statistician}, 39\penalty0 (1):\penalty0 33--38,
  1985.

\bibitem[Rubin(1973)]{rubin1973matching}
D.~B. Rubin.
\newblock Matching to remove bias in observational studies.
\newblock \emph{Biometrics}, pages 159--183, 1973.

\bibitem[Rubin(2001)]{rubin2001using}
D.~B. Rubin.
\newblock Using propensity scores to help design observational studies:
  application to the tobacco litigation.
\newblock \emph{Health Services and Outcomes Research Methodology}, 2\penalty0
  (3):\penalty0 169--188, 2001.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
S.~Sagawa, P.~W. Koh, T.~B. Hashimoto, and P.~Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Sagawa et~al.(2022)Sagawa, Koh, Lee, Gao, Xie, Shen, Kumar, Hu,
  Yasunaga, Marklund, Beery, David, Stavness, Guo, Leskovec, Saenko, Hashimoto,
  Levine, Finn, and Liang]{SagWeiLeeGaoetal22}
S.~Sagawa, P.~W. Koh, T.~Lee, I.~Gao, S.~M. Xie, K.~Shen, A.~Kumar, W.~Hu,
  M.~Yasunaga, H.~Marklund, S.~Beery, E.~David, I.~Stavness, W.~Guo,
  J.~Leskovec, K.~Saenko, T.~Hashimoto, S.~Levine, C.~Finn, and P.~Liang.
\newblock Extending the {WILDS} benchmark for unsupervised adaptation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=z7p2V6KROOV}.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
V.~Sanh, L.~Debut, J.~Chaumond, and T.~Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Scudder(1965)]{scudder1965probability}
H.~Scudder.
\newblock Probability of error of some adaptive pattern-recognition machines.
\newblock \emph{IEEE Transactions on Information Theory}, 11\penalty0
  (3):\penalty0 363--371, 1965.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
H.~Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, N.~Carlini, Z.~Zhang, H.~Zhang, C.~A. Raffel, E.~D.
  Cubuk, A.~Kurakin, and C.-L. Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 596--608, 2020.

\bibitem[Stuart(2010)]{stuart2010matching}
E.~A. Stuart.
\newblock Matching methods for causal inference: A review and a look forward.
\newblock \emph{Stat Sci}, 25\penalty0 (1):\penalty0 1--21, 2010.
\newblock ISSN 0883-4237.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
R.~Taori, A.~Dave, V.~Shankar, N.~Carlini, B.~Recht, and L.~Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18583--18599, 2020.

\bibitem[Tarvainen and Valpola(2017)]{tarvainen2017mean}
A.~Tarvainen and H.~Valpola.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Van~Gansbeke et~al.(2021)Van~Gansbeke, Vandenhende, Georgoulis, and
  Gool]{van2021revisiting}
W.~Van~Gansbeke, S.~Vandenhende, S.~Georgoulis, and L.~V. Gool.
\newblock Revisiting contrastive methods for unsupervised learning of visual
  representations.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Verma et~al.(2021)Verma, Luong, Kawaguchi, Pham, and
  Le]{verma2021towards}
V.~Verma, T.~Luong, K.~Kawaguchi, H.~Pham, and Q.~Le.
\newblock Towards domain-agnostic contrastive learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  10530--10541. PMLR, 2021.

\bibitem[Watson et~al.(2019)Watson, Krutzinna, Bruce, Griffiths, McInnes,
  Barnes, and Floridi]{watson2019clinical}
D.~S. Watson, J.~Krutzinna, I.~N. Bruce, C.~E. Griffiths, I.~B. McInnes, M.~R.
  Barnes, and L.~Floridi.
\newblock Clinical applications of machine learning algorithms: beyond the
  black box.
\newblock \emph{Bmj}, 364, 2019.

\bibitem[Wiles et~al.(2022)Wiles, Gowal, Stimberg, Rebuffi, Ktena, Dvijotham,
  and Cemgil]{wiles2022a}
O.~Wiles, S.~Gowal, F.~Stimberg, S.-A. Rebuffi, I.~Ktena, K.~D. Dvijotham, and
  A.~T. Cemgil.
\newblock A fine-grained analysis on distribution shift.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=Dl4LetuLdyK}.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Q.~Xie, M.-T. Luong, E.~Hovy, and Q.~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 10687--10698, 2020.

\bibitem[Yeh et~al.(2020)Yeh, Perez, Driscoll, Azzari, Tang, Lobell, Ermon, and
  Burke]{yeh2020using}
C.~Yeh, A.~Perez, A.~Driscoll, G.~Azzari, Z.~Tang, D.~Lobell, S.~Ermon, and
  M.~Burke.
\newblock Using publicly available satellite imagery and deep learning to
  understand economic well-being in africa.
\newblock \emph{Nature communications}, 11\penalty0 (1):\penalty0 1--11, 2020.

\bibitem[Yu et~al.(2020)Yu, Chen, Wang, Xian, Chen, Liu, Madhavan, and
  Darrell]{yu2020bdd100k}
F.~Yu, H.~Chen, X.~Wang, W.~Xian, Y.~Chen, F.~Liu, V.~Madhavan, and T.~Darrell.
\newblock Bdd100k: A diverse driving dataset for heterogeneous multitask
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 2636--2645, 2020.

\bibitem[Yu et~al.(2022)Yu, Wang, Vasudevan, Yeung, Seyedhosseini, and
  Wu]{yu2022coca}
J.~Yu, Z.~Wang, V.~Vasudevan, L.~Yeung, M.~Seyedhosseini, and Y.~Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock \emph{arXiv preprint arXiv:2205.01917}, 2022.

\end{thebibliography}
