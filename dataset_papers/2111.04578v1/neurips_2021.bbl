\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akiba et~al.(2019)Akiba, Sano, Yanase, Ohta, and Koyama]{optuna_2019}
Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru Ohta, and Masanori Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In \emph{International Conference on Knowledge Discovery and Data
  Mining}, 2019.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}. PMLR, 2019.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Peter Bartlett, Dylan~J Foster, and Matus Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock \emph{arXiv preprint arXiv:1706.08498}, 2017.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
Peter~L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2285--2301, 2019.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 2010.

\bibitem[Chen et~al.(2019)Chen, Wang, Fu, Long, and Wang]{chen2019catastrophic}
Xinyang Chen, Sinan Wang, Bo~Fu, Mingsheng Long, and Jianmin Wang.
\newblock Catastrophic forgetting meets negative transfer: Batch spectral
  shrinkage for safe transfer learning.
\newblock 2019.

\bibitem[Dhillon et~al.(2019)Dhillon, Chaudhari, Ravichandran, and
  Soatto]{dhillon2019baseline}
Guneet~S Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto.
\newblock A baseline for few-shot image classification.
\newblock \emph{arXiv preprint arXiv:1909.02729}, 2019.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{ICLR}, 2021.

\bibitem[Gouk et~al.(2021)Gouk, Hospedales, and Pontil]{gouk2020distance}
Henry Gouk, Timothy~M Hospedales, and Massimiliano Pontil.
\newblock Distance-based regularisation of deep networks for fine-tuning.
\newblock \emph{ICLR}, 2021.

\bibitem[Griffin et~al.(2007)Griffin, Holub, and Perona]{griffin2007caltech}
Gregory Griffin, Alex Holub, and Pietro Perona.
\newblock Caltech-256 object category dataset.
\newblock 2007.

\bibitem[Guo et~al.(2019)Guo, Shi, Kumar, Grauman, Rosing, and
  Feris]{guo2019spottune}
Yunhui Guo, Honghui Shi, Abhishek Kumar, Kristen Grauman, Tajana Rosing, and
  Rogerio Feris.
\newblock Spottune: transfer learning through adaptive fine-tuning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on computer vision and pattern recognition},
  pages 770--778, 2016.

\bibitem[He et~al.(2019)He, Girshick, and Doll{\'a}r]{he2019rethinking}
Kaiming He, Ross Girshick, and Piotr Doll{\'a}r.
\newblock Rethinking imagenet pre-training.
\newblock In \emph{International Conference on Computer Vision}, 2019.

\bibitem[Hu and Liu(2004)]{hu2004mining}
Minqing Hu and Bing Liu.
\newblock Mining and summarizing customer reviews.
\newblock In \emph{ACM SIGKDD international conference on Knowledge discovery
  and data mining}, 2004.

\bibitem[Huang et~al.(2020)Huang, Zhang, and Zhang]{huang2020self}
Lang Huang, Chao Zhang, and Hongyang Zhang.
\newblock Self-adaptive training: beyond empirical risk minimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Jiang et~al.(2020)Jiang, Neyshabur, Mobahi, Krishnan, and
  Bengio]{jiang2019fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock \emph{ICLR}, 2020.

\bibitem[Khosla et~al.(2011)Khosla, Jayadevaprakash, Yao, and
  Fei-Fei]{stanford_dogs2011}
Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li~Fei-Fei.
\newblock Novel dataset for fine-grained image categorization.
\newblock In \emph{Workshop on Fine-Grained Visual Categorization, IEEE
  Conference on Computer Vision and Pattern Recognition}, 2011.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krause et~al.(2013)Krause, Stark, Deng, and
  Fei-Fei]{stanford_cars2013}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In \emph{Workshop on 3D Representation and Recognition}, 2013.

\bibitem[Li et~al.(2020)Li, Chaudhari, Yang, Lam, Ravichandran, Bhotika, and
  Soatto]{li2020rethinking}
Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul
  Bhotika, and Stefano Soatto.
\newblock Rethinking the hyperparameters for fine-tuning.
\newblock \emph{arXiv preprint arXiv:2002.11770}, 2020.

\bibitem[Li and Roth(2002)]{li2002learning}
Xin Li and Dan Roth.
\newblock Learning question classifiers.
\newblock In \emph{International conference on Computational linguistics},
  2002.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Xiong, Wang, Rao, Liu, and
  Huan]{li2018delta}
Xingjian Li, Haoyi Xiong, Hanchao Wang, Yuxuan Rao, Liping Liu, and Jun Huan.
\newblock Delta: Deep learning transfer using feature map with attention for
  convolutional networks.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Grandvalet, and
  Davoine]{li2018explicit}
Xuhong Li, Yves Grandvalet, and Franck Davoine.
\newblock Explicit inductive bias for transfer learning with convolutional
  networks.
\newblock \emph{arXiv preprint arXiv:1802.01483}, 2018{\natexlab{b}}.

\bibitem[Li et~al.(2018{\natexlab{c}})Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pages 2--47. PMLR,
  2018{\natexlab{c}}.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{arXiv preprint arXiv:2007.00151}, 2020.

\bibitem[Long and Sedghi(2020)]{long2020generalization}
Philip~M Long and Hanie Sedghi.
\newblock Generalization bounds for deep convolutional neural networks.
\newblock \emph{ICLR}, 2020.

\bibitem[Ma et~al.(2020)Ma, Huang, Wang, Romano, Erfani, and
  Bailey]{ma2020normalized}
Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James
  Bailey.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Maji et~al.(2013)Maji, Rahtu, Kannala, Blaschko, and
  Vedaldi]{maji2013fine}
Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock \emph{arXiv preprint arXiv:1306.5151}, 2013.

\bibitem[McAllester(1999{\natexlab{a}})]{mcallester1999pac}
David~A McAllester.
\newblock Pac-bayesian model averaging.
\newblock In \emph{Proceedings of the twelfth annual conference on
  Computational learning theory}, pages 164--170, 1999{\natexlab{a}}.

\bibitem[McAllester(1999{\natexlab{b}})]{mcallester1999some}
David~A McAllester.
\newblock Some pac-bayesian theorems.
\newblock \emph{Machine Learning}, 37\penalty0 (3):\penalty0 355--363,
  1999{\natexlab{b}}.

\bibitem[M{\"u}ller et~al.(2019)M{\"u}ller, Kornblith, and
  Hinton]{muller2019does}
Rafael M{\"u}ller, Simon Kornblith, and Geoffrey Hinton.
\newblock When does label smoothing help?
\newblock \emph{arXiv preprint arXiv:1906.02629}, 2019.

\bibitem[Nagarajan and Kolter(2018)]{nagarajan2018deterministic}
Vaishnavh Nagarajan and Zico Kolter.
\newblock Deterministic pac-bayesian generalization bounds for deep networks
  via generalizing noise-resilience.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Nagarajan Natarajan, Inderjit~S Dhillon, Pradeep Ravikumar, and Ambuj Tewari.
\newblock Learning with noisy labels.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2013.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{ICLR}, 2018.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{Indian Conference on Computer Vision, Graphics \& Image
  Processing}, 2008.

\bibitem[Pang and Lee(2004)]{pang2004sentimental}
Bo~Pang and Lillian Lee.
\newblock A sentimental education: Sentiment analysis using subjectivity
  summarization based on minimum cuts.
\newblock In \emph{Annual meeting on Association for Computational
  Linguistics}, 2004.

\bibitem[Pang and Lee(2005)]{pang2005seeing}
Bo~Pang and Lillian Lee.
\newblock Seeing stars: Exploiting class relationships for sentiment
  categorization with respect to rating scales.
\newblock In \emph{Annual meeting on association for computational
  linguistics}, 2005.

\bibitem[Rajpurkar et~al.(2017)Rajpurkar, Irvin, Zhu, Yang, Mehta, Duan, Ding,
  Bagul, Langlotz, Shpanskaya, et~al.]{rajpurkar2017chexnet}
Pranav Rajpurkar, Jeremy Irvin, Kaylie Zhu, Brandon Yang, Hershel Mehta, Tony
  Duan, Daisy Ding, Aarti Bagul, Curtis Langlotz, Katie Shpanskaya, et~al.
\newblock Chexnet: Radiologist-level pneumonia detection on chest x-rays with
  deep learning.
\newblock \emph{arXiv preprint arXiv:1711.05225}, 2017.

\bibitem[Ratner et~al.(2016)Ratner, De~Sa, Wu, Selsam, and
  R{\'e}]{ratner2016data}
Alexander Ratner, Christopher De~Sa, Sen Wu, Daniel Selsam, and Christopher
  R{\'e}.
\newblock Data programming: Creating large training sets, quickly.
\newblock \emph{Advances in neural information processing systems}, 2016.

\bibitem[Ratner et~al.(2017)Ratner, Bach, Ehrenberg, Fries, Wu, and
  R{\'e}]{ratner2017snorkel}
Alexander Ratner, Stephen~H Bach, Henry Ehrenberg, Jason Fries, Sen Wu, and
  Christopher R{\'e}.
\newblock Snorkel: Rapid training data creation with weak supervision.
\newblock In \emph{International Conference on Very Large Data Bases}. NIH
  Public Access, 2017.

\bibitem[Ratner et~al.(2019)Ratner, Hancock, Dunnmon, Sala, Pandey, and
  R{\'e}]{ratner2019training}
Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash
  Pandey, and Christopher R{\'e}.
\newblock Training complex models with multi-task weak supervision.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 4763--4771, 2019.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 2015.

\bibitem[Saab et~al.(2021)Saab, Hooper, Sohoni, Parmar, Pogatchnik, Wu,
  Dunnmon, Zhang, Rubin, and R{\'e}]{saab2021observational}
Khaled Saab, Sarah~M Hooper, Nimit~S Sohoni, Jupinder Parmar, Brian Pogatchnik,
  Sen Wu, Jared~A Dunnmon, Hongyang~R Zhang, Daniel Rubin, and Christopher
  R{\'e}.
\newblock Observational supervision for medical image classification using gaze
  data.
\newblock In \emph{International Conference on Medical Image Computing and
  Computer-Assisted Intervention}, pages 603--614. Springer, 2021.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock \emph{arXiv preprint arXiv:2007.08489}, 2020.

\bibitem[Sharif~Razavian et~al.(2014)Sharif~Razavian, Azizpour, Sullivan, and
  Carlsson]{sharif2014cnn}
Ali Sharif~Razavian, Hossein Azizpour, Josephine Sullivan, and Stefan Carlsson.
\newblock Cnn features off-the-shelf: an astounding baseline for recognition.
\newblock In \emph{Conference on computer vision and pattern recognition
  workshops}, 2014.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Conference on empirical methods in natural language
  processing}, 2013.

\bibitem[Tai et~al.(2021)Tai, Bailis, and Valiant]{tai2021sinkhorn}
Kai~Sheng Tai, Peter Bailis, and Gregory Valiant.
\newblock Sinkhorn label allocation: Semi-supervised classification via
  annealed self-training.
\newblock \emph{arXiv preprint arXiv:2102.08622}, 2021.

\bibitem[Tan et~al.(2018)Tan, Sun, Kong, Zhang, Yang, and Liu]{tan2018survey}
Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang Liu.
\newblock A survey on deep transfer learning.
\newblock In \emph{International conference on artificial neural networks},
  2018.

\bibitem[Thulasidasan et~al.(2019)Thulasidasan, Bhattacharya, Bilmes,
  Chennupati, and Mohd-Yusof]{thulasidasan2019combating}
Sunil Thulasidasan, Tanmoy Bhattacharya, Jeff Bilmes, Gopinath Chennupati, and
  Jamal Mohd-Yusof.
\newblock Combating label noise in deep learning using abstention.
\newblock \emph{arXiv preprint arXiv:1905.10964}, 2019.

\bibitem[Tian et~al.(2020)Tian, Wang, Krishnan, Tenenbaum, and
  Isola]{tian2020rethinking}
Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua~B Tenenbaum, and Phillip Isola.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock \emph{arXiv preprint arXiv:2003.11539}, 2020.

\bibitem[Tropp(2015)]{tropp2015introduction}
Joel~A Tropp.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{arXiv preprint arXiv:1501.01571}, 2015.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Kavukcuoglu, and
  Wierstra]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Koray Kavukcuoglu, and Daan
  Wierstra.
\newblock Matching networks for one shot learning.
\newblock \emph{arXiv preprint arXiv:1606.04080}, 2016.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{WahCUB_200_2011}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~Belongie.
\newblock {The Caltech-UCSD Birds-200-2011 Dataset}.
\newblock Technical Report CNS-TR-2011-001, California Institute of Technology,
  2011.

\bibitem[Wang et~al.(2017)Wang, Peng, Lu, Lu, Bagheri, and
  Summers]{wang2017chestx}
Xiaosong Wang, Yifan Peng, Le~Lu, Zhiyong Lu, Mohammadhadi Bagheri, and
  Ronald~M Summers.
\newblock Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on
  weakly-supervised classification and localization of common thorax diseases.
\newblock In \emph{Conference on computer vision and pattern recognition},
  2017.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{wang2019symmetric}
Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{International Conference on Computer Vision}, pages
  322--330, 2019.

\bibitem[Wei and Ma(2019{\natexlab{a}})]{wei2019data}
Colin Wei and Tengyu Ma.
\newblock Data-dependent sample complexity of deep neural networks via
  lipschitz augmentation.
\newblock \emph{arXiv preprint arXiv:1905.03684}, 2019{\natexlab{a}}.

\bibitem[Wei and Ma(2019{\natexlab{b}})]{wei2019improved}
Colin Wei and Tengyu Ma.
\newblock Improved sample complexities for deep networks and robust
  classification via an all-layer margin.
\newblock \emph{arXiv preprint arXiv:1910.04284}, 2019{\natexlab{b}}.

\bibitem[Wiebe et~al.(2005)Wiebe, Wilson, and Cardie]{wiebe2005annotating}
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
\newblock Annotating expressions of opinions and emotions in language.
\newblock \emph{Language resources and evaluation}, 2005.

\bibitem[Wu et~al.(2020)Wu, Zhang, and R{\'e}]{WZR20}
Sen Wu, Hongyang~R Zhang, and Christopher R{\'e}.
\newblock Understanding and improving information transfer in multi-task
  learning.
\newblock \emph{ICLR}, 2020.

\bibitem[Xu et~al.(2021)Xu, Shang, Ye, Qian, Li, Sun, Li, and Jin]{xu2021dash}
Yi~Xu, Lei Shang, Jinxing Ye, Qi~Qian, Yu-Feng Li, Baigui Sun, Hao Li, and Rong
  Jin.
\newblock Dash: Semi-supervised learning with dynamic thresholding.
\newblock In \emph{International Conference on Machine Learning}, pages
  11525--11536. PMLR, 2021.

\bibitem[Yang et~al.(2021)Yang, Zhang, Wu, Su, and R{\'e}]{yang2020analysis}
Fan Yang, Hongyang~R Zhang, Sen Wu, Weijie~J Su, and Christopher R{\'e}.
\newblock Analysis of information transfer from heterogeneous sources via
  precise high-dimensional asymptotics.
\newblock \emph{arXiv preprint arXiv:2010.11750}, 2021.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang and Sabuncu(2018)]{zhang2018generalized}
Zhilu Zhang and Mert~R Sabuncu.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock \emph{arXiv preprint arXiv:1805.07836}, 2018.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Wenda Zhou, Victor Veitch, Morgane Austern, Ryan~P Adams, and Peter Orbanz.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock \emph{arXiv preprint arXiv:1804.05862}, 2018.

\bibitem[Zou et~al.(2019)Zou, Yu, Liu, Kumar, and Wang]{zou2019confidence}
Yang Zou, Zhiding Yu, Xiaofeng Liu, BVK Kumar, and Jinsong Wang.
\newblock Confidence regularized self-training.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 5982--5991, 2019.

\end{thebibliography}
