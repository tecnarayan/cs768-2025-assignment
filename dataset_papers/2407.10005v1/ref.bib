@inproceedings{muller2023pfns4bo,
  title={Pfns4bo: In-context learning for bayesian optimization},
  author={M{\"u}ller, Samuel and Feurer, Matthias and Hollmann, Noah and Hutter, Frank},
  booktitle={International Conference on Machine Learning},
  pages={25444--25470},
  year={2023},
  organization={PMLR}
}
@article{hollmann2022tabpfn,
  title={Tabpfn: A transformer that solves small tabular classification problems in a second},
  author={Hollmann, Noah and M{\"u}ller, Samuel and Eggensperger, Katharina and Hutter, Frank},
  journal={arXiv preprint arXiv:2207.01848},
  year={2022}
}
@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}
@article{lee2023exploring,
  title={Exploring the relationship between model architecture and in-context learning ability},
  author={Lee, Ivan and Jiang, Nan and Berg-Kirkpatrick, Taylor},
  journal={arXiv preprint arXiv:2310.08049},
  year={2023}
}
@article{zucchet2023gated,
  title={Gated recurrent neural networks discover attention},
  author={Zucchet, Nicolas and Kobayashi, Seijin and Akram, Yassir and Von Oswald, Johannes and Larcher, Maxime and Steger, Angelika and Sacramento, Joao},
  journal={arXiv preprint arXiv:2309.01775},
  year={2023}
}
@inproceedings{mahankali2024one,
title={One Step of Gradient Descent is Provably the Optimal In-Context Learner with One Layer of Linear Self-Attention},
author={Arvind V. Mahankali and Tatsunori Hashimoto and Tengyu Ma},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=8p3fu56lKc}
}
@article{wu2023many,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2310.08391},
  year={2023}
}

@article{yuelu,
  title={Asymptotic theory of in-context learning by linear attention},
  author={Lu, Yue and Letey, Mary I. and Zavatone-Veth, Jacob A. and Maiti, Anindita and Pehlevan, Cengiz},
  journal={arXiv preprint arXiv:2310.08391},
  year={2024}
}
@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{han2023context,
  title={In-context learning of large language models explained as kernel regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  journal={arXiv preprint arXiv:2305.12766},
  year={2023}
}

@inproceedings{hu2022lora,
title={Lo{RA}: Low-Rank Adaptation of Large Language Models},
author={Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=nZeVKeeFYf9}
}


@InProceedings{pmlr-v237-abernethy24a,
  title = 	 {A Mechanism for Sample-Efficient In-Context Learning for Sparse Retrieval Tasks},
  author =       {Abernethy, Jacob and Agarwal, Alekh and Marinov, Teodor Vanislavov and Warmuth, Manfred K.},
  booktitle = 	 {Proceedings of The 35th International Conference on Algorithmic Learning Theory},
  pages = 	 {3--46},
  year = 	 {2024},
  editor = 	 {Vernade, Claire and Hsu, Daniel},
  volume = 	 {237},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Feb},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v237/abernethy24a/abernethy24a.pdf},
  url = 	 {https://proceedings.mlr.press/v237/abernethy24a.html},
}

@inproceedings{liu2023transformers,
title={Transformers Learn Shortcuts to Automata},
author={Bingbin Liu and Jordan T. Ash and Surbhi Goel and Akshay Krishnamurthy and Cyril Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=De4FYqjFueZ}
}


@article{agarwal2024many,
  title={Many-Shot In-Context Learning},
  author={Agarwal, Rishabh and Singh, Avi and Zhang, Lei M and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and Co-Reyes, John D and Chu, Eric and others},
  journal={arXiv preprint arXiv:2404.11018},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@inproceedings{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{gu2021combining,
  title={Combining recurrent, convolutional, and continuous-time models with linear state space layers},
  author={Gu, Albert and Johnson, Isys and Goel, Karan and Saab, Khaled and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={572--585},
  year={2021}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{basu2023statistical,
  title={A statistical perspective on retrieval-based models},
  author={Basu, Soumya and Rawat, Ankit Singh and Zaheer, Manzil},
  booktitle={International Conference on Machine Learning},
  pages={1852--1886},
  year={2023},
  organization={PMLR}
}

@article{nakano2021webgpt,
  title={Webgpt: Browser-assisted question-answering with human feedback},
  author={Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
  journal={arXiv preprint arXiv:2112.09332},
  year={2021}
}

@article{wang2022training,
  title={Training data is more valuable than you think: A simple and effective method by retrieving from training data},
  author={Wang, Shuohang and Xu, Yichong and Fang, Yuwei and Liu, Yang and Sun, Siqi and Xu, Ruochen and Zhu, Chenguang and Zeng, Michael},
  journal={arXiv preprint arXiv:2203.08773},
  year={2022}
}

@article{izacard2023atlas,
  title={Atlas: Few-shot learning with retrieval augmented language models},
  author={Izacard, Gautier and Lewis, Patrick and Lomeli, Maria and Hosseini, Lucas and Petroni, Fabio and Schick, Timo and Dwivedi-Yu, Jane and Joulin, Armand and Riedel, Sebastian and Grave, Edouard},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={251},
  pages={1--43},
  year={2023}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{gpt4_techreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      journal={arXiv preprintarXiv:2303.08774},
      year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={GeminiTeam and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023_llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{dai2023gpt,
    title = "Why Can {GPT} Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers",
    author = "Dai, Damai  and
      Sun, Yutao  and
      Dong, Li  and
      Hao, Yaru  and
      Ma, Shuming  and
      Sui, Zhifang  and
      Wei, Furu",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.findings-acl.247",
    doi = "10.18653/v1/2023.findings-acl.247",
    pages = "4005--4019",
}
@article{duraisamy2024finite,
  title={Finite Sample Analysis and Bounds of Generalization Error of Gradient Descent in In-Context Linear Regression},
  author={Duraisamy, Karthik},
  journal={arXiv preprint arXiv:2405.02462},
  year={2024}
}
@inproceedings{xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=RdJVFCHjUMI}
}

@article{grazzi2024mamba,
  title={Is Mamba Capable of In-Context Learning?},
  author={Grazzi, Riccardo and Siems, Julien and Schrodi, Simon and Brox, Thomas and Hutter, Frank},
  journal={arXiv preprint arXiv:2402.03170},
  year={2024}
}
@article{ahn2024transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{fu2023hungry,
title={Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@inproceedings{katharopoulos2020transformers,
  title={Transformers are rnns: Fast autoregressive transformers with linear attention},
  author={Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle={International conference on machine learning},
  pages={5156--5165},
  year={2020},
  organization={PMLR}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}
@inproceedings{dauphin2017language,
  title={Language modeling with gated convolutional networks},
  author={Dauphin, Yann N and Fan, Angela and Auli, Michael and Grangier, David},
  booktitle={International conference on machine learning},
  pages={933--941},
  year={2017},
  organization={PMLR}
}
@inproceedings{akyrek2023what,
title={What learning algorithm is in-context learning? Investigations with linear models},
author={Ekin Aky{\"u}rek and Dale Schuurmans and Jacob Andreas and Tengyu Ma and Denny Zhou},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=0g0X4H8yN4I}
}
@article{collins2024context,
  title={In-Context Learning with Transformers: Softmax Attention Adapts to Function Lipschitzness},
  author={Collins, Liam and Parulekar, Advait and Mokhtari, Aryan and Sanghavi, Sujay and Shakkottai, Sanjay},
  journal={arXiv preprint arXiv:2402.11639},
  year={2024}
}
@inproceedings{li2020gradient,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  booktitle={International conference on artificial intelligence and statistics},
  pages={4313--4324},
  year={2020},
  organization={PMLR}
}
@article{cao2019towards,
  title={Towards understanding the spectral bias of deep learning},
  author={Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1912.01198},
  year={2019}
}
@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{canatar2021spectral,
  title={Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks},
  author={Canatar, Abdulkadir and Bordelon, Blake and Pehlevan, Cengiz},
  journal={Nature communications},
  volume={12},
  number={1},
  pages={2914},
  year={2021},
  publisher={Nature Publishing Group UK London}
}
@inproceedings{mahdavi2024revisiting,
  title={Revisiting the Equivalence of In-Context Learning and Gradient Descent: The Impact of Data Distribution},
  author={Mahdavi, Sadegh and Liao, Renjie and Thrampoulidis, Christos},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7410--7414},
  year={2024},
  organization={IEEE}
}
@article{du2023can,
  title={Can Transformers Learn Optimal Filtering for Unknown Systems?},
  author={Du, Zhe and Balim, Haldun and Oymak, Samet and Ozay, Necmiye},
  journal={IEEE Control Systems Letters},
  volume={7},
  pages={3525--3530},
  year={2023},
  publisher={IEEE}
}
@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}
@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18878--18891},
  year={2022}
}

@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@inproceedings{schlag2021linear,
  title={Linear transformers are secretly fast weight programmers},
  author={Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Machine Learning},
  pages={9355--9366},
  year={2021},
  organization={PMLR}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}
@article{li2024dissecting,
  title={Dissecting chain-of-thought: Compositionality through in-context filtering and learning},
  author={Li, Yingcong and Sreenivasan, Kartik and Giannou, Angeliki and Papailiopoulos, Dimitris and Oymak, Samet},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{gatmirycan,
  title={Can Looped Transformers Learn to Implement Multi-step Gradient Descent for In-context Learning?},
  author={Gatmiry, Khashayar and Saunshi, Nikunj and Reddi, Sashank J and Jegelka, Stefanie and Kumar, Sanjiv},
  booktitle={Forty-first International Conference on Machine Learning}
}
@article{lin2024dual,
  title={Dual Operating Modes of In-Context Learning},
  author={Lin, Ziqian and Lee, Kangwook},
  journal={arXiv preprint arXiv:2402.18819},
  year={2024}
}
@article{von2023uncovering,
  title={Uncovering mesa-optimization algorithms in transformers},
  author={von Oswald, Johannes and Niklasson, Eyvind and Schlegel, Maximilian and Kobayashi, Seijin and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and Pascanu, Razvan and others},
  journal={arXiv preprint arXiv:2309.05858},
  year={2023}
}
@article{zhang2024trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={49},
  pages={1--55},
  year={2024}
}

@article{park2024can,
  title={Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks},
  author={Park, Jongho and Park, Jaeseung and Xiong, Zheyang and Lee, Nayoung and Cho, Jaewoong and Oymak, Samet and Lee, Kangwook and Papailiopoulos, Dimitris},
  journal={International Conference on Machine Learning},
  year={2024}
}
