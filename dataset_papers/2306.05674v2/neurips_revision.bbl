\begin{thebibliography}{100}

\bibitem{abbasi2011improved}
Y.~Abbasi-Yadkori, D.~P{\'a}l, and C.~Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock {\em Advances in Neural Information Processing Systems},
  24:2312--2320, 2011.

\bibitem{abdar2021review}
M.~Abdar, F.~Pourpanah, S.~Hussain, D.~Rezazadegan, L.~Liu, M.~Ghavamzadeh,
  P.~Fieguth, X.~Cao, A.~Khosravi, U.~R. Acharya, et~al.
\newblock A review of uncertainty quantification in deep learning: Techniques,
  applications and challenges.
\newblock {\em Information Fusion}, 76:243--297, 2021.

\bibitem{alaa2020frequentist}
A.~Alaa and M.~Van Der~Schaar.
\newblock Frequentist uncertainty in recurrent neural networks via blockwise
  influence functions.
\newblock In {\em International Conference on Machine Learning}, pages
  175--190. PMLR, 2020.

\bibitem{allen2019learning}
Z.~Allen-Zhu, Y.~Li, and Y.~Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252. PMLR, 2019.

\bibitem{allen2019convergence2}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  32:6676--6688, 2019.

\bibitem{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~R. Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock {\em Advances in Neural Information Processing Systems},
  32:8141--8150, 2019.

\bibitem{ashukha2020pitfalls}
A.~Ashukha, A.~Lyzhov, D.~Molchanov, and D.~Vetrov.
\newblock Pitfalls of in-domain uncertainty estimation and ensembling in deep
  learning.
\newblock {\em arXiv preprint arXiv:2002.06470}, 2020.

\bibitem{auer2002finite}
P.~Auer, N.~Cesa-Bianchi, and P.~Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock {\em Machine Learning}, 47(2):235--256, 2002.

\bibitem{bai2021understanding}
Y.~Bai, S.~Mei, H.~Wang, and C.~Xiong.
\newblock Understanding the under-coverage bias in uncertainty estimation.
\newblock {\em arXiv preprint arXiv:2106.05515}, 2021.

\bibitem{barber2019limits}
R.~F. Barber, E.~J. Candes, A.~Ramdas, and R.~J. Tibshirani.
\newblock The limits of distribution-free conditional predictive inference.
\newblock {\em arXiv preprint arXiv:1903.04684}, 2019.

\bibitem{barber2019predictive}
R.~F. Barber, E.~J. Candes, A.~Ramdas, and R.~J. Tibshirani.
\newblock Predictive inference with the jackknife+.
\newblock {\em arXiv preprint arXiv:1905.02928}, 2019.

\bibitem{basu2020influence}
S.~Basu, P.~Pope, and S.~Feizi.
\newblock Influence functions in deep learning are fragile.
\newblock {\em arXiv preprint arXiv:2006.14651}, 2020.

\bibitem{berlinet2011reproducing}
A.~Berlinet and C.~Thomas-Agnan.
\newblock {\em Reproducing kernel Hilbert spaces in probability and
  statistics}.
\newblock Springer Science \& Business Media, 2011.

\bibitem{bietti2019inductive}
A.~Bietti and J.~Mairal.
\newblock On the inductive bias of neural tangent kernels.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{blundell2015weight}
C.~Blundell, J.~Cornebise, K.~Kavukcuoglu, and D.~Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In {\em International Conference on Machine Learning}, pages
  1613--1622. PMLR, 2015.

\bibitem{breiman1996bagging}
L.~Breiman.
\newblock Bagging predictors.
\newblock {\em Machine Learning}, 24:123--140, 1996.

\bibitem{breiman2001random}
L.~Breiman.
\newblock Random forests.
\newblock {\em Machine Learning}, 45:5--32, 2001.

\bibitem{cao2019generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock {\em Advances in Neural Information Processing Systems},
  32:10836--10846, 2019.

\bibitem{cao2020generalization}
Y.~Cao and Q.~Gu.
\newblock Generalization error bounds of gradient descent for learning
  over-parameterized deep relu networks.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  34(04):3349--3356, 2020.

\bibitem{carvalho2020scalable}
E.~D. Carvalho, R.~Clark, A.~Nicastro, and P.~H. Kelly.
\newblock Scalable uncertainty for computer vision with functional variational
  inference.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 12003--12013, 2020.

\bibitem{chen2021learning}
H.~Chen, Z.~Huang, H.~Lam, H.~Qian, and H.~Zhang.
\newblock Learning prediction intervals for regression: Generalization and
  calibration.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem{chizat2019lazy}
L.~Chizat, E.~Oyallon, and F.~Bach.
\newblock On lazy training in differentiable programming.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{christmann2013consistency}
A.~Christmann and R.~Hable.
\newblock On the consistency of the bootstrap approach for support vector
  machines and related kernel-based methods.
\newblock {\em Empirical inference: Festschrift in honor of vladimir n.
  vapnik}, pages 231--244, 2013.

\bibitem{christmann2007consistency}
A.~Christmann and I.~Steinwart.
\newblock Consistency and robustness of kernel-based regression in convex risk
  minimization.
\newblock {\em Bernoulli}, 13(3):799--819, 2007.

\bibitem{christmann2007svms}
A.~Christmann and I.~Steinwart.
\newblock How svms can estimate quantiles and the median.
\newblock {\em Advances in Neural Information Processing Systems}, 20:305--312,
  2007.

\bibitem{CS}
F.~Cucker and S.~Smale.
\newblock On the mathematical foundations of learning.
\newblock {\em Bulletin of the American Mathematical Society}, 39(1):1--49,
  2002.

\bibitem{cybenko1989approximation}
G.~Cybenko.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock {\em Mathematics of Control, Signals and Systems}, 2(4):303--314,
  1989.

\bibitem{dalmasso2020conditional}
N.~Dalmasso, T.~Pospisil, A.~B. Lee, R.~Izbicki, P.~E. Freeman, and A.~I. Malz.
\newblock Conditional density estimation tools in python and r with
  applications to photometric redshifts and likelihood-free cosmological
  inference.
\newblock {\em Astronomy and Computing}, 30:100362, 2020.

\bibitem{davison1997bootstrap}
A.~C. Davison and D.~V. Hinkley.
\newblock {\em Bootstrap methods and their application}.
\newblock Cambridge university press, 1997.

\bibitem{debruyne2008model}
M.~Debruyne, M.~Hubert, and J.~A. Suykens.
\newblock Model selection in kernel based regression using the influence
  function.
\newblock {\em Journal of Machine Learning Research}, 9(10), 2008.

\bibitem{depeweg2016learning}
S.~Depeweg, J.~M. Hern{\'a}ndez-Lobato, F.~Doshi-Velez, and S.~Udluft.
\newblock Learning and policy search in stochastic dynamical systems with
  bayesian neural networks.
\newblock {\em arXiv preprint arXiv:1605.07127}, 2016.

\bibitem{depeweg2018decomposition}
S.~Depeweg, J.-M. Hernandez-Lobato, F.~Doshi-Velez, and S.~Udluft.
\newblock Decomposition of uncertainty in bayesian deep learning for efficient
  and risk-sensitive learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1184--1193. PMLR, 2018.

\bibitem{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1675--1685. PMLR, 2019.

\bibitem{du2018gradient}
S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{dutordoir2018gaussian}
V.~Dutordoir, H.~Salimbeni, J.~Hensman, and M.~Deisenroth.
\newblock Gaussian process conditional density estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2385--2395, 2018.

\bibitem{efron1994introduction}
B.~Efron and R.~J. Tibshirani.
\newblock {\em An introduction to the bootstrap}.
\newblock CRC press, 1994.

\bibitem{fernholz2012mises}
L.~T. Fernholz.
\newblock {\em Von Mises calculus for statistical functionals}, volume~19.
\newblock Springer Science \& Business Media, 2012.

\bibitem{fisher1930inverse}
R.~A. Fisher.
\newblock Inverse probability.
\newblock {\em Mathematical Proceedings of the Cambridge Philosophical
  Society}, 26(4):528--535, 1930.

\bibitem{flegal2010batch}
J.~M. Flegal, G.~L. Jones, et~al.
\newblock Batch means and spectral variance estimators in {M}arkov chain
  {M}onte {C}arlo.
\newblock {\em The Annals of Statistics}, 38(2):1034--1070, 2010.

\bibitem{fort2019deep}
S.~Fort, H.~Hu, and B.~Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock {\em arXiv preprint arXiv:1912.02757}, 2019.

\bibitem{freeman2017unified}
P.~E. Freeman, R.~Izbicki, and A.~B. Lee.
\newblock A unified framework for constructing, tuning and assessing
  photometric redshift density estimates in a selection bias setting.
\newblock {\em Monthly Notices of the Royal Astronomical Society},
  468(4):4556--4565, 2017.

\bibitem{gal2016dropout}
Y.~Gal and Z.~Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1050--1059, 2016.

\bibitem{gelman2013bayesian}
A.~Gelman, J.~B. Carlin, H.~S. Stern, D.~B. Dunson, A.~Vehtari, and D.~B.
  Rubin.
\newblock {\em Bayesian data analysis}.
\newblock CRC Press, 2013.

\bibitem{geurts2006extremely}
P.~Geurts, D.~Ernst, and L.~Wehenkel.
\newblock Extremely randomized trees.
\newblock {\em Machine Learning}, 63:3--42, 2006.

\bibitem{geyer1992practical}
C.~J. Geyer.
\newblock Practical {M}arkov chain {M}onte {C}arlo.
\newblock {\em Statistical Science}, 7(4):473--483, 1992.

\bibitem{Glynn1990simulation}
P.~W. Glynn and D.~L. Iglehart.
\newblock Simulation output analysis using standardized time series.
\newblock {\em Mathematics of Operations Research}, 15(1):1--16, 1990.

\bibitem{glynn2018constructing}
P.~W. Glynn and H.~Lam.
\newblock Constructing simulation output intervals under input uncertainty via
  data sectioning.
\newblock In {\em 2018 Winter Simulation Conference (WSC)}, pages 1551--1562.
  IEEE, 2018.

\bibitem{graves2011practical}
A.~Graves.
\newblock Practical variational inference for neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem{hable2012asymptotic}
R.~Hable.
\newblock Asymptotic normality of support vector machine variants and other
  regularized kernel methods.
\newblock {\em Journal of Multivariate Analysis}, 106:92--117, 2012.

\bibitem{hampel1974influence}
F.~R. Hampel.
\newblock The influence curve and its role in robust estimation.
\newblock {\em Journal of the American Statistical Association},
  69(346):383--393, 1974.

\bibitem{hampel2011potential}
F.~R. Hampel.
\newblock Potential surprises.
\newblock {\em International Symposium on Imprecise Probability: Theories and
  Applications (ISIPTA)}, page 209, 2011.

\bibitem{hanin2017approximating}
B.~Hanin and M.~Sellke.
\newblock Approximating continuous functions by relu nets of minimal width.
\newblock {\em arXiv preprint arXiv:1710.11278}, 2017.

\bibitem{he2020bayesian}
B.~He, B.~Lakshminarayanan, and Y.~W. Teh.
\newblock Bayesian deep ensembles via the neural tangent kernel.
\newblock {\em arXiv preprint arXiv:2007.05864}, 2020.

\bibitem{he2015delving}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1026--1034, 2015.

\bibitem{hernandez2015probabilistic}
J.~M. Hern{\'a}ndez-Lobato and R.~Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1861--1869, 2015.

\bibitem{holmes2007fast}
M.~P. Holmes, A.~G. Gray, and C.~L. Isbell~Jr.
\newblock Fast nonparametric conditional density estimation.
\newblock In {\em Proceedings of the Twenty-Third Conference on Uncertainty in
  Artificial Intelligence}, pages 175--182, 2007.

\bibitem{hornik1991approximation}
K.~Hornik.
\newblock Approximation capabilities of multilayer feedforward networks.
\newblock {\em Neural Networks}, 4(2):251--257, 1991.

\bibitem{Hu2020Simple}
W.~Hu, Z.~Li, and D.~Yu.
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{huang2021quantifying}
Z.~Huang, H.~Lam, and H.~Zhang.
\newblock Quantifying epistemic uncertainty in deep learning.
\newblock {\em arXiv preprint arXiv:2110.12122}, 2021.

\bibitem{hullermeier2021aleatoric}
E.~H{\"u}llermeier and W.~Waegeman.
\newblock Aleatoric and epistemic uncertainty in machine learning: An
  introduction to concepts and methods.
\newblock {\em Machine Learning}, 110(3):457--506, 2021.

\bibitem{izbicki2016nonparametric}
R.~Izbicki and A.~B. Lee.
\newblock Nonparametric conditional density estimation in a high-dimensional
  regression setting.
\newblock {\em Journal of Computational and Graphical Statistics},
  25(4):1297--1316, 2016.

\bibitem{izbicki2017}
R.~Izbicki, A.~B. Lee, and P.~E. Freeman.
\newblock Photo-$z$ estimation: An example of nonparametric conditional density
  estimation under selection bias.
\newblock {\em Ann. Appl. Stat.}, 11(2):698--724, 06 2017.

\bibitem{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:1806.07572}, 2018.

\bibitem{jaeckel1972infinitesimal}
L.~Jaeckel.
\newblock The infinitesimal jackknife. memorandum.
\newblock Technical report, MM 72-1215-11, Bell Lab. Murray Hill, NJ, 1972.

\bibitem{jones2006fixed}
G.~L. Jones, M.~Haran, B.~S. Caffo, and R.~Neath.
\newblock Fixed-width output analysis for {M}arkov chain {M}onte {C}arlo.
\newblock {\em Journal of the American Statistical Association},
  101(476):1537--1547, 2006.

\bibitem{kendall2017uncertainties}
A.~Kendall and Y.~Gal.
\newblock What uncertainties do we need in bayesian deep learning for computer
  vision?
\newblock {\em arXiv preprint arXiv:1703.04977}, 2017.

\bibitem{khosravi2010lower}
A.~Khosravi, S.~Nahavandi, D.~Creighton, and A.~F. Atiya.
\newblock Lower upper bound estimation method for construction of neural
  network-based prediction intervals.
\newblock {\em IEEE Transactions on Neural Networks}, 22(3):337--346, 2010.

\bibitem{khosravi2011comprehensive}
A.~Khosravi, S.~Nahavandi, D.~Creighton, and A.~F. Atiya.
\newblock Comprehensive review of neural network-based prediction intervals and
  new advances.
\newblock {\em IEEE Transactions on Neural Networks}, 22(9):1341--1356, 2011.

\bibitem{koenker2001quantile}
R.~Koenker and K.~F. Hallock.
\newblock Quantile regression.
\newblock {\em Journal of Economic Perspectives}, 15(4):143--156, 2001.

\bibitem{koh2019accuracy}
P.~W. Koh, K.-S. Ang, H.~H. Teo, and P.~Liang.
\newblock On the accuracy of influence functions for measuring group effects.
\newblock {\em arXiv preprint arXiv:1905.13289}, 2019.

\bibitem{koh2017understanding}
P.~W. Koh and P.~Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em International Conference on Machine Learning}, pages
  1885--1894. PMLR, 2017.

\bibitem{lakshminarayanan2017simple}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6402--6413, 2017.

\bibitem{lam2022cheap1}
H.~Lam.
\newblock Cheap bootstrap for input uncertainty quantification.
\newblock In {\em 2022 Winter Simulation Conference (WSC)}, pages 2318--2329.
  IEEE, 2022.

\bibitem{lam2022cheap}
H.~Lam.
\newblock A cheap bootstrap method for fast inference.
\newblock {\em arXiv preprint arXiv:2202.00090}, 2022.

\bibitem{lamliu2023}
H.~Lam and Z.~Liu.
\newblock Bootstrap in high dimension with low computation.
\newblock {\em International Conference on Machine Learning (ICML)}, 2023.

\bibitem{lam2022subsampling}
H.~Lam and H.~Qian.
\newblock Subsampling to enhance efficiency in input uncertainty
  quantification.
\newblock {\em Operations Research}, 70(3):1891--1913, 2022.

\bibitem{lam2023doubly}
H.~Lam and H.~Zhang.
\newblock Doubly robust stein-kernelized monte carlo estimator: Simultaneous
  bias-variance reduction and supercanonical convergence.
\newblock {\em Journal of Machine Learning Research}, 24(85):1--58, 2023.

\bibitem{lee2017deep}
J.~Lee, Y.~Bahri, R.~Novak, S.~S. Schoenholz, J.~Pennington, and
  J.~Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock {\em arXiv preprint arXiv:1711.00165}, 2017.

\bibitem{lee2019wide}
J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock {\em Advances in Neural Information Processing Systems},
  32:8572--8583, 2019.

\bibitem{lee2015m}
S.~Lee, S.~Purushwalkam, M.~Cogswell, D.~Crandall, and D.~Batra.
\newblock Why m heads are better than one: Training a diverse ensemble of deep
  networks.
\newblock {\em arXiv preprint arXiv:1511.06314}, 2015.

\bibitem{lei2018distribution}
J.~Lei, M.~G’Sell, A.~Rinaldo, R.~J. Tibshirani, and L.~Wasserman.
\newblock Distribution-free predictive inference for regression.
\newblock {\em Journal of the American Statistical Association},
  113(523):1094--1111, 2018.

\bibitem{lei2015conformal}
J.~Lei, A.~Rinaldo, and L.~Wasserman.
\newblock A conformal prediction approach to explore functional data.
\newblock {\em Annals of Mathematics and Artificial Intelligence},
  74(1-2):29--43, 2015.

\bibitem{lei2014distribution}
J.~Lei and L.~Wasserman.
\newblock Distribution-free prediction bands for non-parametric regression.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 76(1):71--96, 2014.

\bibitem{li2018learning}
Y.~Li and Y.~Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock {\em Advances in Neural Information Processing Systems}, 2018.

\bibitem{li2019enhanced}
Z.~Li, R.~Wang, D.~Yu, S.~S. Du, W.~Hu, R.~Salakhutdinov, and S.~Arora.
\newblock Enhanced convolutional neural tangent kernels.
\newblock {\em arXiv preprint arXiv:1911.00809}, 2019.

\bibitem{littlestone1994weighted}
N.~Littlestone and M.~K. Warmuth.
\newblock The weighted majority algorithm.
\newblock {\em Information and computation}, 108(2):212--261, 1994.

\bibitem{lu2017expressive}
Z.~Lu, H.~Pu, F.~Wang, Z.~Hu, and L.~Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 6232--6240, 2017.

\bibitem{meinshausen2006quantile}
N.~Meinshausen.
\newblock Quantile regression forests.
\newblock {\em Journal of Machine Learning Research}, 7(Jun):983--999, 2006.

\bibitem{michelmore2018evaluating}
R.~Michelmore, M.~Kwiatkowska, and Y.~Gal.
\newblock Evaluating uncertainty quantification in end-to-end autonomous
  driving control.
\newblock {\em arXiv preprint arXiv:1811.06817}, 2018.

\bibitem{michelmore2020uncertainty}
R.~Michelmore, M.~Wicker, L.~Laurenti, L.~Cardelli, Y.~Gal, and M.~Kwiatkowska.
\newblock Uncertainty quantification with statistical guarantees in end-to-end
  autonomous driving control.
\newblock In {\em 2020 IEEE International Conference on Robotics and
  Automation}, pages 7344--7350. IEEE, 2020.

\bibitem{mirza2014conditional}
M.~Mirza and S.~Osindero.
\newblock Conditional generative adversarial nets.
\newblock {\em arXiv preprint arXiv:1411.1784}, 2014.

\bibitem{mohri2018foundations}
M.~Mohri, A.~Rostamizadeh, and A.~Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT Press, 2018.

\bibitem{pearce2018high}
T.~Pearce, M.~Zaki, A.~Brintrup, and A.~Neely.
\newblock High-quality prediction intervals for deep learning: A
  distribution-free, ensembled approach.
\newblock In {\em International Conference on Machine Learning, PMLR: Volume
  80}, 2018.

\bibitem{posch2020correlated}
K.~Posch and J.~Pilz.
\newblock Correlated parameters to accurately measure uncertainty in deep
  neural networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  32(3):1037--1051, 2020.

\bibitem{ren2016conditional}
Y.~Ren, J.~Zhu, J.~Li, and Y.~Luo.
\newblock Conditional generative moment-matching networks.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{romano2019conformalized}
Y.~Romano, E.~Patterson, and E.~Candes.
\newblock Conformalized quantile regression.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3543--3553, 2019.

\bibitem{rosenfeld2018discriminative}
N.~Rosenfeld, Y.~Mansour, and E.~Yom-Tov.
\newblock Discriminative learning of prediction intervals.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 347--355, 2018.

\bibitem{Schmeiser1982batch}
B.~Schmeiser.
\newblock Batch size effects in the analysis of simulation output.
\newblock {\em Operations Research}, 30(3):556--568, 1982.

\bibitem{schruben1983confidence}
L.~Schruben.
\newblock Confidence interval estimation using standardized time series.
\newblock {\em Operations Research}, 31(6):1090--1108, 1983.

\bibitem{senge2014reliable}
R.~Senge, S.~B{\"o}sner, K.~Dembczy{\'n}ski, J.~Haasenritter, O.~Hirsch,
  N.~Donner-Banzhoff, and E.~H{\"u}llermeier.
\newblock Reliable classification: Learning classifiers that distinguish
  aleatoric and epistemic uncertainty.
\newblock {\em Information Sciences}, 255:16--29, 2014.

\bibitem{shao2012jackknife}
J.~Shao and D.~Tu.
\newblock {\em The jackknife and bootstrap}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{siddhant2018deep}
A.~Siddhant and Z.~C. Lipton.
\newblock Deep bayesian active learning for natural language processing:
  Results of a large-scale empirical study.
\newblock {\em arXiv preprint arXiv:1808.05697}, 2018.

\bibitem{SZ}
S.~Smale and D.-X. Zhou.
\newblock Shannon sampling and function reconstruction from point values.
\newblock {\em Bulletin of the American Mathematical Society}, 41(3):279--306,
  2004.

\bibitem{SZ2}
S.~Smale and D.-X. Zhou.
\newblock Shannon sampling {II}: Connections to learning theory.
\newblock {\em Applied and Computational Harmonic Analysis}, 19(3):285--302,
  2005.

\bibitem{SZ3}
S.~Smale and D.-X. Zhou.
\newblock Learning theory estimates via integral operators and their
  approximations.
\newblock {\em Constructive Approximation}, 26(2):153--172, 2007.

\bibitem{sonoda2021ridge}
S.~Sonoda, I.~Ishikawa, and M.~Ikeda.
\newblock Ridge regression with over-parametrized two-layer networks converge
  to ridgelet spectrum.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2674--2682. PMLR, 2021.

\bibitem{steinwart2011estimating}
I.~Steinwart, A.~Christmann, et~al.
\newblock Estimating conditional quantiles with the help of the pinball loss.
\newblock {\em Bernoulli}, 17(1):211--225, 2011.

\bibitem{SW}
H.~Sun and Q.~Wu.
\newblock Application of integral operator for regularized least-square
  regression.
\newblock {\em Mathematical and Computer Modelling}, 49(1-2):276--285, 2009.

\bibitem{van2000asymptotic}
A.~W. Van~der Vaart.
\newblock {\em Asymptotic statistics}, volume~3.
\newblock Cambridge University Press, 2000.

\bibitem{vovk2005algorithmic}
V.~Vovk, A.~Gammerman, and G.~Shafer.
\newblock {\em Algorithmic learning in a random world}.
\newblock Springer Science \& Business Media, 2005.

\bibitem{wang2023pseudo}
K.~Wang.
\newblock Pseudo-labeling for kernel ridge regression under covariate shift.
\newblock {\em arXiv preprint arXiv:2302.10160}, 2023.

\bibitem{xiao2019quantifying}
Y.~Xiao and W.~Y. Wang.
\newblock Quantifying uncertainties in natural language processing tasks.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 7322--7329, 2019.

\bibitem{yang2019scaling}
G.~Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock {\em arXiv preprint arXiv:1902.04760}, 2019.

\bibitem{yang2020tensor}
G.~Yang.
\newblock Tensor programs ii: Neural tangent kernel for any architecture.
\newblock {\em arXiv preprint arXiv:2006.14548}, 2020.

\bibitem{zhang2019random}
H.~Zhang, J.~Zimmerman, D.~Nettleton, and D.~J. Nordman.
\newblock Random forest prediction intervals.
\newblock {\em The American Statistician}, pages 1--15, 2019.

\bibitem{zhang2021neural}
W.~Zhang, D.~Zhou, L.~Li, and Q.~Gu.
\newblock Neural thompson sampling.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{zhang2020type}
Y.~Zhang, Z.-Q.~J. Xu, T.~Luo, and Z.~Ma.
\newblock A type of generalization error induced by initialization in deep
  neural networks.
\newblock In {\em Mathematical and Scientific Machine Learning}, pages
  144--164. PMLR, 2020.

\bibitem{zhou2020neural}
D.~Zhou, L.~Li, and Q.~Gu.
\newblock Neural contextual bandits with ucb-based exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  11492--11502. PMLR, 2020.

\bibitem{zhou2021local}
M.~Zhou, R.~Ge, and C.~Jin.
\newblock A local convergence theory for mildly over-parameterized two-layer
  neural network.
\newblock In {\em Conference on Learning Theory}, pages 4577--4632. PMLR, 2021.

\bibitem{zhou2022deep}
X.~Zhou, Y.~Jiao, J.~Liu, and J.~Huang.
\newblock A deep generative approach to conditional sampling.
\newblock {\em Journal of the American Statistical Association}, pages 1--12,
  2022.

\bibitem{zou2020gradient}
D.~Zou, Y.~Cao, D.~Zhou, and Q.~Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109(3):467--492, 2020.

\end{thebibliography}
