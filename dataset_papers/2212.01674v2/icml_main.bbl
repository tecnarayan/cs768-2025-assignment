\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arazo et~al.(2019)Arazo, Ortego, Albert, O’Connor, and
  McGuinness]{arazo2019unsupervised}
Arazo, E., Ortego, D., Albert, P., O’Connor, N., and McGuinness, K.
\newblock Unsupervised label noise modeling and loss correction.
\newblock In \emph{ICML}, pp.\  312--321, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrz{{e}}bski, Ballas, Krueger, Bengio,
  Kanwal, Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrz{{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{ICML}, pp.\  233--242, 2017.

\bibitem[Baldock et~al.(2021)Baldock, Maennel, and Neyshabur]{baldock2021deep}
Baldock, R., Maennel, H., and Neyshabur, B.
\newblock Deep learning through the lens of example difficulty.
\newblock In \emph{NIPS}, volume~34, pp.\  10876--10889, 2021.

\bibitem[Blum \& Mitchell(1998)Blum and Mitchell]{blum1998combining}
Blum, A. and Mitchell, T.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In \emph{Proceedings of the eleventh annual conference on
  Computational learning theory}, pp.\  92--100, 1998.

\bibitem[Chen et~al.(2019)Chen, Liao, Chen, and Zhang]{chen2019understanding}
Chen, P., Liao, B.~B., Chen, G., and Zhang, S.
\newblock Understanding and utilizing deep neural networks trained with noisy
  labels.
\newblock In \emph{ICML}, pp.\  1062--1070, 2019.

\bibitem[D'souza et~al.(2021)D'souza, Nussbaum, Agarwal, and
  Hooker]{Hooker_tail}
D'souza, D., Nussbaum, Z., Agarwal, C., and Hooker, S.
\newblock A tale of two long tails.
\newblock \emph{CoRR}, abs/2107.13098, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.13098}.

\bibitem[Feldman(2020)]{Feldman2019}
Feldman, V.
\newblock Does learning require memorization? a short tale about a long tail.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, STOC 2020, pp.\  954–959, New York, NY, USA, 2020.
  Association for Computing Machinery.
\newblock ISBN 9781450369794.
\newblock \doi{10.1145/3357713.3384290}.
\newblock URL \url{https://doi.org/10.1145/3357713.3384290}.

\bibitem[Goldberger \& Ben-Reuven(2017)Goldberger and
  Ben-Reuven]{goldberger2017training}
Goldberger, J. and Ben-Reuven, E.
\newblock Training deep neural-networks using a noise adaptation layer.
\newblock In \emph{ICLR}, 2017.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock \emph{NIPS}, 31, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, pp.\  770--778, 2016.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2018mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, pp.\  2304--2313, 2018.

\bibitem[Karim et~al.(2022)Karim, Rizve, Rahnavard, Mian, and
  Shah]{karim2022unicon}
Karim, N., Rizve, M.~N., Rahnavard, N., Mian, A., and Shah, M.
\newblock Unicon: Combating label noise through uniform selection and
  contrastive learning.
\newblock In \emph{CVPR}, pp.\  9676--9686, 2022.

\bibitem[Kim et~al.(2021)Kim, Yun, Shon, and Kim]{kim2021joint}
Kim, Y., Yun, J., Shon, H., and Kim, J.
\newblock Joint negative and positive learning for noisy labels.
\newblock In \emph{CVPR}, pp.\  9442--9451, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Le \& Yang(2015)Le and Yang]{le2015tiny}
Le, Y. and Yang, X.
\newblock Tiny imagenet visual recognition challenge.
\newblock \emph{CS 231N}, 7\penalty0 (7):\penalty0 3, 2015.

\bibitem[Li et~al.(2019)Li, Wong, Zhao, and Kankanhalli]{li2019learning}
Li, J., Wong, Y., Zhao, Q., and Kankanhalli, M.~S.
\newblock Learning to learn from noisy labeled data.
\newblock In \emph{CVPR}, pp.\  5051--5059, 2019.

\bibitem[Li et~al.(2020)Li, Socher, and Hoi]{dividemix}
Li, J., Socher, R., and Hoi, S. C.~H.
\newblock Dividemix: Learning with noisy labels as semi-supervised learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Li et~al.(2022)Li, Xia, Ge, and Liu]{li2022selective}
Li, S., Xia, X., Ge, S., and Liu, T.
\newblock Selective-supervised contrastive learning with noisy labels.
\newblock In \emph{CVPR}, pp.\  316--325, 2022.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Wang, Li, Agustsson, and
  Gool]{li2017webvision}
Li, W., Wang, L., Li, W., Agustsson, E., and Gool, L.~V.
\newblock Webvision database: Visual learning and understanding from web data.
\newblock \emph{CoRR}, 2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Yang, Song, Cao, Luo, and
  Li]{Li2017LearningFN}
Li, Y., Yang, J., Song, Y., Cao, L., Luo, J., and Li, L.-J.
\newblock Learning from noisy labels with distillation.
\newblock \emph{2017 IEEE International Conference on Computer Vision (ICCV)},
  pp.\  1928--1936, 2017{\natexlab{b}}.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock In \emph{NIPS}, volume~33, pp.\  20331--20342, 2020.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In \emph{ICLR}, 2017.

\bibitem[Lu \& He(2022)Lu and He]{lu2022selc}
Lu, Y. and He, W.
\newblock Selc: Self-ensemble label correction improves learning with noisy
  labels.
\newblock In \emph{IJCAI}, 2022.

\bibitem[Ma et~al.(2017)Ma, Meng, Xie, Li, and Dong]{ma2017self}
Ma, F., Meng, D., Xie, Q., Li, Z., and Dong, X.
\newblock Self-paced co-training.
\newblock In \emph{ICML}, pp.\  2275--2284. PMLR, 2017.

\bibitem[Ma et~al.(2018)Ma, Wang, Houle, Zhou, Erfani, Xia, Wijewickrema, and
  Bailey]{ma2018dimensionality}
Ma, X., Wang, Y., Houle, M.~E., Zhou, S., Erfani, S., Xia, S., Wijewickrema,
  S., and Bailey, J.
\newblock Dimensionality-driven learning with noisy labels.
\newblock In \emph{ICML}, pp.\  3355--3364, 2018.

\bibitem[Ma et~al.(2020)Ma, Huang, Wang, Romano, Erfani, and
  Bailey]{ma2020normalized}
Ma, X., Huang, H., Wang, Y., Romano, S., Erfani, S., and Bailey, J.
\newblock Normalized loss functions for deep learning with noisy labels.
\newblock In \emph{ICML}, pp.\  6543--6553, 2020.

\bibitem[Malach \& Shalev-Shwartz(2017)Malach and Shalev-Shwartz]{decoupling}
Malach, E. and Shalev-Shwartz, S.
\newblock Decoupling "when to update" from "how to update".
\newblock In \emph{NIPS}, volume~30, 2017.

\bibitem[Ortego et~al.(2021)Ortego, Arazo, Albert, O'Connor, and
  McGuinness]{ortego2021multi}
Ortego, D., Arazo, E., Albert, P., O'Connor, N.~E., and McGuinness, K.
\newblock Multi-objective interpolation training for robustness to label noise.
\newblock In \emph{CVPR}, pp.\  6606--6615, 2021.

\bibitem[Pleiss et~al.(2020)Pleiss, Zhang, Elenberg, and
  Weinberger]{pleiss2020identifying}
Pleiss, G., Zhang, T., Elenberg, E., and Weinberger, K.~Q.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock In \emph{NIPS}, volume~33, pp.\  17044--17056, 2020.

\bibitem[Reed et~al.(2015)Reed, Lee, Anguelov, Szegedy, Erhan, and
  Rabinovich]{reed2015training}
Reed, S.~E., Lee, H., Anguelov, D., Szegedy, C., Erhan, D., and Rabinovich, A.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock In \emph{ICLR (Workshop)}, 2015.

\bibitem[Sarfraz et~al.(2021)Sarfraz, Arani, and Zonooz]{sarfraz2021noisy}
Sarfraz, F., Arani, E., and Zonooz, B.
\newblock Noisy concurrent training for efficient learning under label noise.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pp.\  3159--3168, 2021.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{sohn2020fixmatch}
Sohn, K., Berthelot, D., Carlini, N., Zhang, Z., Zhang, H., Raffel, C.~A.,
  Cubuk, E.~D., Kurakin, A., and Li, C.-L.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In \emph{NIPS}, volume~33, pp.\  596--608, 2020.

\bibitem[Song et~al.(2019)Song, Kim, and Lee]{song2019selfie}
Song, H., Kim, M., and Lee, J.-G.
\newblock Selfie: Refurbishing unclean samples for robust deep learning.
\newblock In \emph{ICML}, pp.\  5907--5915, 2019.

\bibitem[Tanaka et~al.(2018{\natexlab{a}})Tanaka, Ikami, Yamasaki, and
  Aizawa]{Tanaka2018JointOF}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K.
\newblock Joint optimization framework for learning with noisy labels.
\newblock \emph{2018 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  5552--5560, 2018{\natexlab{a}}.

\bibitem[Tanaka et~al.(2018{\natexlab{b}})Tanaka, Ikami, Yamasaki, and
  Aizawa]{tanaka2018joint}
Tanaka, D., Ikami, D., Yamasaki, T., and Aizawa, K.
\newblock Joint optimization framework for learning with noisy labels.
\newblock In \emph{CVPR}, pp.\  5552--5560, 2018{\natexlab{b}}.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{CVPR}, pp.\  2691--2699, 2015.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{ICML}, pp.\  7164--7173, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2018mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Zheng, Wu, Goswami, and
  Chen]{zhang2020learning}
Zhang, Y., Zheng, S., Wu, P., Goswami, M., and Chen, C.
\newblock Learning with feature-dependent label noise: A progressive approach.
\newblock In \emph{ICLR}, 2020.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{NIPS}, volume~31, 2018.

\bibitem[Zhu et~al.(2022)Zhu, Hedderich, Zhai, Adelani, and
  Klakow]{zhu2022bertlabel}
Zhu, D., Hedderich, M.~A., Zhai, F., Adelani, D.~I., and Klakow, D.
\newblock Is bert robust to label noise? a study on learning with noisy labels
  in text classification.
\newblock \emph{arXiv preprint 2204.09371}, 2022.

\end{thebibliography}
