@inproceedings{locatello2019challenging,
  title={Challenging common assumptions in the unsupervised learning of disentangled representations},
  author={Locatello, Francesco and Bauer, Stefan and Lucic, Mario and Raetsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  booktitle={international conference on machine learning},
  pages={4114--4124},
  year={2019},
  organization={PMLR}
}


@inproceedings{
federici2020learning,
title={Learning Robust Representations via Multi-View Information Bottleneck},
author={Marco Federici and Anjan Dutta and Patrick Forré and Nate Kushman and Zeynep Akata},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=B1xwcyHFDr}
}

@inproceedings{wang2022rethinking,
  title={Rethinking minimal sufficient representation in contrastive learning},
  author={Wang, Haoqing and Guo, Xun and Deng, Zhi-Hong and Lu, Yan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16041--16050},
  year={2022}
}

@inproceedings{bouchacourt2018multi,
  title={Multi-level variational autoencoder: Learning disentangled representations from grouped observations},
  author={Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{jha2018disentangling,
  title={Disentangling factors of variation with cycle-consistent variational auto-encoders},
  author={Jha, Ananya Harsh and Anand, Saket and Singh, Maneesh and Veeravasarapu, VS Rao},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={805--820},
  year={2018}
}

@inproceedings{vowels2020nestedvae,
  title={NestedVAE: Isolating common factors via weak supervision},
  author={Vowels, Matthew J and Camgoz, Necati Cihan and Bowden, Richard},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={9202--9212},
  year={2020}
}

@InProceedings{sanchez2020learning,
author="Sanchez, Eduardo Hugo
and Serrurier, Mathieu
and Ortner, Mathias",
editor="Vedaldi, Andrea
and Bischof, Horst
and Brox, Thomas
and Frahm, Jan-Michael",
title="Learning Disentangled Representations via Mutual Information Estimation",
booktitle="Computer Vision -- ECCV 2020",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="205--221",
abstract="In this paper, we investigate the problem of learning disentangled representations. Given a pair of images sharing some attributes, we aim to create a low-dimensional representation which is split into two parts: a shared representation that captures the common information between the images and an exclusive representation that contains the specific information of each image. To address this issue, we propose a model based on mutual information estimation without relying on image reconstruction or image generation. Mutual information maximization is performed to capture the attributes of data in the shared and exclusive representations while we minimize the mutual information between the shared and exclusive representation to enforce representation disentanglement. We show that these representations are useful to perform downstream tasks such as image classification and image retrieval based on the shared or exclusive component. Moreover, classification results show that our model outperforms the state-of-the-art models based on VAE/GAN approaches in representation disentanglement.",
isbn="978-3-030-58542-6"
}



@article{tian2020makes,
  title={What makes for good views for contrastive learning?},
  author={Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6827--6839},
  year={2020}
}

@article{smith2005development,
  title={The development of embodied cognition: Six lessons from babies},
  author={Smith, Linda and Gasser, Michael},
  journal={Artificial life},
  volume={11},
  number={1-2},
  pages={13--29},
  year={2005},
  publisher={MIT Press}
}


@inproceedings{tian2020contrastive,
  title={Contrastive multiview coding},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle={European conference on computer vision},
  pages={776--794},
  year={2020},
  organization={Springer}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}


@inproceedings{
Locatello2020Disentangling,
title={Disentangling Factors of Variations Using Few Labels},
author={Francesco Locatello and Michael Tschannen and Stefan Bauer and Gunnar Rätsch and Bernhard Schölkopf and Olivier Bachem},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SygagpEKwB}
}


@InProceedings{li2018disentangled,
  title = 	 {Disentangled Sequential Autoencoder},
  author =       {Yingzhen, Li and Mandt, Stephan},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {5670--5679},
  year = 	 {2018},
  editor = 	 {Dy, Jennifer and Krause, Andreas},
  volume = 	 {80},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {10--15 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v80/yingzhen18a/yingzhen18a.pdf},
  url = 	 {https://proceedings.mlr.press/v80/yingzhen18a.html},
  abstract = 	 {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.}
}


@inproceedings{wolf2004zero,
  title={Zero-error information and applications in cryptography},
  author={Wolf, Stefan and Wultschleger, J},
  booktitle={Information Theory Workshop},
  pages={1--6},
  year={2004},
  organization={IEEE}
}


@article{salamatian2016maximum,
  title={Maximum Entropy Functions: Approximate Gacs-Korner for Distributed Compression},
  author={Salamatian, Salman and Cohen, Asaf and M{\'e}dard, Muriel},
  journal={arXiv preprint arXiv:1604.03877},
  year={2016}
}

{
@article{objectron2021,
  title={Objectron: A Large Scale Dataset of Object-Centric Videos in the Wild with Pose Annotations},
  author={Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, Matthias Grundmann},
  journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year={2021}
}

@inproceedings{locatello2020weakly,
  title={Weakly-supervised disentanglement without compromises},
  author={Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6348--6359},
  year={2020},
  organization={PMLR}
}

@article{hyvarinen2000independent,
  title={Independent component analysis: algorithms and applications},
  author={Hyv{\"a}rinen, Aapo and Oja, Erkki},
  journal={Neural networks},
  volume={13},
  number={4-5},
  pages={411--430},
  year={2000},
  publisher={Elsevier}
}

@inproceedings{alemi2018fixing,
  title={Fixing a broken ELBO},
  author={Alemi, Alexander and Poole, Ben and Fischer, Ian and Dillon, Joshua and Saurous, Rif A and Murphy, Kevin},
  booktitle={International Conference on Machine Learning},
  pages={159--168},
  year={2018},
  organization={PMLR}
}


@article{kingma2016improved,
  title={Improved variational inference with inverse autoregressive flow},
  author={Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={4743--4751},
  year={2016}
}


@article{held1963movement,
  title={Movement-produced stimulation in the development of visually guided behavior.},
  author={Held, Richard and Hein, Alan},
  journal={Journal of comparative and physiological psychology},
  volume={56},
  number={5},
  pages={872},
  year={1963},
  publisher={American Psychological Association}
}


@article{watanabe1960information,
  title={Information theoretical analysis of multivariate correlation},
  author={Watanabe, Satosi},
  journal={IBM Journal of research and development},
  volume={4},
  number={1},
  pages={66--82},
  year={1960},
  publisher={IBM}
}


@article{chen2021boxhead,
  title={Boxhead: A Dataset for Learning Hierarchical Representations},
  author={Chen, Yukun and Tr{\"a}uble, Frederik and Dittadi, Andrea and Bauer, Stefan and Sch{\"o}lkopf, Bernhard},
  journal={arXiv preprint arXiv:2110.03628},
  year={2021}
}


@article{achille2018information,
  title={Information dropout: Learning optimal representations through noisy computation},
  author={Achille, Alessandro and Soatto, Stefano},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={40},
  number={12},
  pages={2897--2905},
  year={2018},
  publisher={IEEE}
}

@article{gabbay2021image,
  title={An image is worth more than a thousand words: Towards disentanglement in the wild},
  author={Gabbay, Aviv and Cohen, Niv and Hoshen, Yedid},
  journal={arXiv preprint arXiv:2106.15610},
  year={2021}
}


@inproceedings{kim2018disentangling,
  title={Disentangling by factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={2649--2658},
  year={2018},
  organization={PMLR}
}


@article{tokui2021disentanglement,
  title={Disentanglement Analysis with Partial Information Decomposition},
  author={Tokui, Seiya and Sato, Issei},
  journal={arXiv preprint arXiv:2108.13753},
  year={2021}
}

@article{chen2018isolating,
  title={Isolating Sources of Disentanglement in Variational Autoencoders},
  author={Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  journal={Advances in Neural Information Processing Systems},
  year={2018}
}


@inproceedings{
eastwood2018framework,
title={A framework for the quantitative evaluation of disentangled representations},
author={Cian Eastwood and Christopher K. I. Williams},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=By-7dz-AZ},
}

@misc{dsprites17,
author = {Loic Matthey and Irina Higgins and Demis Hassabis and Alexander Lerchner},
title = {dSprites: Disentanglement testing Sprites dataset},
howpublished= {https://github.com/deepmind/dsprites-dataset/},
year = "2017",
}

@misc{3dshapes18,
  title={3D Shapes Dataset},
  author={Burgess, Chris and Kim, Hyunjik},
  howpublished={https://github.com/deepmind/3dshapes-dataset/},
  year={2018}
}

@article{higgins2017beta,
  title={beta-vae: Learning basic visual concepts with a constrained variational framework},
  author={Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  journal={International Conference on Learning Representations},
  year={2017}
}


@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}


@article{santhanam2006high,
  title={A high-performance brain--computer interface},
  author={Santhanam, Gopal and Ryu, Stephen I and Byron, M Yu and Afshar, Afsheen and Shenoy, Krishna V},
  journal={nature},
  volume={442},
  number={7099},
  pages={195--198},
  year={2006},
  publisher={Nature Publishing Group}
}

@article{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}

@article{wang2016deep,
  title={Deep variational canonical correlation analysis},
  author={Wang, Weiran and Yan, Xinchen and Lee, Honglak and Livescu, Karen},
  journal={arXiv preprint arXiv:1610.03454},
  year={2016}
}


@article{yu2020tutorial,
  title={A Tutorial on VAEs: From Bayes' Rule to Lossless Compression},
  author={Yu, Ronald},
  journal={arXiv preprint arXiv:2006.10273},
  year={2020}
}

@article{maale2019biva,
    title={BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling},
    author={Lars Maaløe and Marco Fraccaro and Valentin Liévin and Ole Winther},
    year={2019},
    eprint={1902.02102},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{vahdat2020nvae,
  title={Nvae: A deep hierarchical variational autoencoder},
  author={Vahdat, Arash and Kautz, Jan},
  journal={arXiv preprint arXiv:2007.03898},
  year={2020}
}


@inproceedings{hinton1993keeping,
  title={Keeping the neural networks simple by minimizing the description length of the weights},
  author={Hinton, Geoffrey E and Van Camp, Drew},
  booktitle={Proceedings of the sixth annual conference on Computational learning theory},
  pages={5--13},
  year={1993}
}

@inproceedings{
townsend2019practical,
title={Practical lossless compression with latent variables using bits back coding},
author={James Townsend and Thomas Bird and David Barber},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=ryE98iR5tm},
}

@inproceedings{rybkin2021simple,
  title={Simple and effective VAE training with calibrated decoders},
  author={Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={9179--9189},
  year={2021},
  organization={PMLR}
}


@article{alemi2016deep,
  title={Deep variational information bottleneck},
  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},
  journal={arXiv preprint arXiv:1612.00410},
  year={2016}
}

@article{schneidman2003synergy,
  title={Synergy, redundancy, and independence in population codes},
  author={Schneidman, Elad and Bialek, William and Berry, Michael J},
  journal={Journal of Neuroscience},
  volume={23},
  number={37},
  pages={11539--11553},
  year={2003},
  publisher={Soc Neuroscience}
}

@article{vincent2010stacked,
  title={Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  author={Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  journal={Journal of machine learning research},
  volume={11},
  number={12},
  year={2010}
}


@phdthesis{williams2011information,
  title={Information dynamics: Its theory and application to embodied cognitive systems},
  author={Williams, Paul L},
  year={2011},
  school={Indiana University}
}


@article{valiant1984theory,
  title={A theory of the learnable},
  author={Valiant, Leslie G},
  journal={Communications of the ACM},
  volume={27},
  number={11},
  pages={1134--1142},
  year={1984},
  publisher={ACM New York, NY, USA}
}


@article{quax2017quantifying,
  title={Quantifying synergistic information using intermediate stochastic variables},
  author={Quax, Rick and Har-Shemesh, Omri and Sloot, Peter},
  journal={Entropy},
  volume={19},
  number={2},
  pages={85},
  year={2017},
  publisher={Multidisciplinary Digital Publishing Institute}
}

@article{timme2020method,
  title={A Method to Present and Analyze Ensembles of Information Sources},
  author={Timme, Nicholas M and Linsenbardt, David and Lapish, Christopher C},
  journal={Entropy},
  volume={22},
  number={5},
  pages={580},
  year={2020},
  publisher={Multidisciplinary Digital Publishing Institute}
}


@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}


@article{barrett2015exploration,
  title={Exploration of synergistic and redundant information sharing in static and dynamical Gaussian systems},
  author={Barrett, Adam B},
  journal={Physical Review E},
  volume={91},
  number={5},
  pages={052802},
  year={2015},
  publisher={APS}
}

@Article{faes2017multiscale,
AUTHOR = {Faes, Luca and Marinazzo, Daniele and Stramaglia, Sebastiano},
TITLE = {Multiscale Information Decomposition: Exact Computation for Multivariate Gaussian Processes},
JOURNAL = {Entropy},
VOLUME = {19},
YEAR = {2017},
NUMBER = {8},
ARTICLE-NUMBER = {408},
URL = {https://www.mdpi.com/1099-4300/19/8/408},
ISSN = {1099-4300},
ABSTRACT = {Exploiting the theory of state space models, we derive the exact expressions of the information transfer, as well as redundant and synergistic transfer, for coupled Gaussian processes observed at multiple temporal scales. All of the terms, constituting the frameworks known as interaction information decomposition and partial information decomposition, can thus be analytically obtained for different time scales from the parameters of the VAR model that fits the processes. We report the application of the proposed methodology firstly to benchmark Gaussian systems, showing that this class of systems may generate patterns of information decomposition characterized by prevalently redundant or synergistic information transfer persisting across multiple time scales or even by the alternating prevalence of redundant and synergistic source interaction depending on the time scale. Then, we apply our method to an important topic in neuroscience, i.e., the detection of causal interactions in human epilepsy networks, for which we show the relevance of partial information decomposition to the detection of multiscale information transfer spreading from the seizure onset zone.},
DOI = {10.3390/e19080408}
}

@inproceedings{belghazi2018mutual,
  title={Mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle={International Conference on Machine Learning},
  pages={531--540},
  year={2018},
  organization={PMLR}
}


@article{churchland2012neural,
  title={Neural population dynamics during reaching},
  author={Churchland, Mark M and Cunningham, John P and Kaufman, Matthew T and Foster, Justin D and Nuyujukian, Paul and Ryu, Stephen I and Shenoy, Krishna V},
  journal={Nature},
  volume={487},
  number={7405},
  pages={51--56},
  year={2012},
  publisher={Nature Publishing Group}
}

@article{nuyujukian2014performance,
  title={Performance sustaining intracortical neural prostheses},
  author={Nuyujukian, Paul and Kao, Jonathan C and Fan, Joline M and Stavisky, Sergey D and Ryu, Stephen I and Shenoy, Krishna V},
  journal={Journal of neural engineering},
  volume={11},
  number={6},
  pages={066003},
  year={2014},
  publisher={IOP Publishing}
}


@article{santhanam2009factor,
  title={Factor-analysis methods for higher-performance neural prostheses},
  author={Santhanam, Gopal and Yu, Byron M and Gilja, Vikash and Ryu, Stephen I and Afshar, Afsheen and Sahani, Maneesh and Shenoy, Krishna V},
  journal={Journal of neurophysiology},
  volume={102},
  number={2},
  pages={1315--1330},
  year={2009},
  publisher={American Physiological Society}
}

@article{gao2017theory,
  title={A theory of multineuronal dimensionality, dynamics and measurement},
  author={Gao, Peiran and Trautmann, Eric and Yu, Byron and Santhanam, Gopal and Ryu, Stephen and Shenoy, Krishna and Ganguli, Surya},
  journal={BioRxiv},
  pages={214262},
  year={2017},
  publisher={Cold Spring Harbor Laboratory}
}


@article{Harder2013bivariate,
  title = {Bivariate measure of redundant information},
  author = {Harder, Malte and Salge, Christoph and Polani, Daniel},
  journal = {Phys. Rev. E},
  volume = {87},
  issue = {1},
  pages = {012130},
  numpages = {14},
  year = {2013},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevE.87.012130},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.87.012130}
}

@inproceedings{Xu2020theory,
  author    = {Yilun Xu and
               Shengjia Zhao and
               Jiaming Song and
               Russell Stewart and
               Stefano Ermon},
  title     = {A Theory of Usable Information under Computational Constraints},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
  url       = {https://openreview.net/forum?id=r1eBeyHFDH},
  timestamp = {Thu, 07 May 2020 17:11:48 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/XuZSSE20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{wyner1975common,
  author={A. {Wyner}},
  journal={IEEE Transactions on Information Theory}, 
  title={The common information of two dependent random variables}, 
  year={1975},
  volume={21},
  number={2},
  pages={163-179},
  doi={10.1109/TIT.1975.1055346}}



@misc{allenzhu2020feature,
      title={Feature Purification: How Adversarial Training Performs Robust Deep Learning}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li},
      year={2020},
      eprint={2005.10190},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{kingma2014autoencoding,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}


@misc{higgins2020unsupervised,
      title={Unsupervised deep learning identifies semantic disentanglement in single inferotemporal neurons}, 
      author={Irina Higgins and Le Chang and Victoria Langston and Demis Hassabis and Christopher Summerfield and Doris Tsao and Matthew Botvinick},
      year={2020},
      eprint={2006.14304},
      archivePrefix={arXiv},
      primaryClass={q-bio.NC}
}

@article{gacs1973common,
  title={Common information is far less than mutual information},
  author={G{\'a}cs, Peter and K{\"o}rner, J{\'a}nos},
  journal={Problems of Control and Information Theory},
  volume={2},
  number={2},
  pages={149--162},
  year={1973}
}


@INPROCEEDINGS{salamatian2020approximate,  author={S. {Salamatian} and A. {Cohen} and M. {Médard}},  booktitle={2020 IEEE International Symposium on Information Theory (ISIT)},   title={Approximate Gács-Körner Common Information},   year={2020},  volume={},  number={},  pages={2234-2239},  doi={10.1109/ISIT44484.2020.9173956}}

@inproceedings{wang2015multi,
author = {Wang, Weiran and Arora, Raman and Livescu, Karen and Bilmes, Jeff},
title = {On Deep Multi-View Representation Learning},
year = {2015},
publisher = {JMLR.org},
abstract = {We consider learning representations (features) in the setting in which we have access to multiple unlabeled views of the data for representation learning while only one view is available at test time. Previous work on this problem has proposed several techniques based on deep neural networks, typically involving either autoencoder-like networks with a reconstruction objective or paired feedforward networks with a correlation-based objective. We analyze several techniques based on prior work, as well as new variants, and compare them experimentally on visual, speech, and language domains. To our knowledge this is the first head-to-head comparison of a variety of such techniques on multiple tasks. We find an advantage for correlation-based representation learning, while the best results on most tasks are obtained with our new variant, deep canonically correlated autoencoders (DCCAE).},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {1083–1092},
numpages = {10},
location = {Lille, France},
series = {ICML'15}
}

@article{quian2009extracting_review,
	Author = {Quian Quiroga, Rodrigo and Panzeri, Stefano},
	Journal = {Nature Reviews Neuroscience},
	Number = {3},
	Pages = {173--185},
	Title = {Extracting information from neuronal populations: information theory and decoding approaches},
	Volume = {10},
	Year = {2009}}


@inproceedings{dubois2020learning,
 author = {Dubois, Yann and Kiela, Douwe and Schwab, David J and Vedantam, Ramakrishna},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {18674--18690},
 publisher = {Curran Associates, Inc.},
 title = {Learning Optimal Representations with the Decodable Information Bottleneck},
 url = {https://proceedings.neurips.cc/paper/2020/file/d8ea5f53c1b1eb087ac2e356253395d8-Paper.pdf},
 volume = {33},
 year = {2020}
}


@article{Bertschinger_2014_quantifying, title={Quantifying Unique Information}, volume={16}, ISSN={1099-4300}, url={http://dx.doi.org/10.3390/e16042161}, DOI={10.3390/e16042161}, number={4}, journal={Entropy}, publisher={MDPI AG}, author={Bertschinger, Nils and Rauh, Johannes and Olbrich, Eckehard and Jost, Jürgen and Ay, Nihat}, year={2014}, month={Apr}, pages={2161–2183}}


@inproceedings{
kleinman2020usable,
title={Usable Information and Evolution of Optimal Representations During Training},
author={Michael Kleinman and Alessandro Achille and Daksh Idnani and Jonathan Kao},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=p8agn6bmTbr}
}

@book{cover2006book,
author = {Cover, Thomas M. and Thomas, Joy A.},
title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
year = {2006},
isbn = {0471241954},
publisher = {Wiley-Interscience},
address = {USA}
}

@article{kolchinsky2022novel,
  title={A novel approach to the partial information decomposition},
  author={Kolchinsky, Artemy},
  journal={Entropy},
  volume={24},
  number={3},
  pages={403},
  year={2022},
  publisher={MDPI}
}


@incollection{soatto2013actionable,
  title={Actionable information in vision},
  author={Soatto, Stefano},
  booktitle={Machine learning for computer vision},
  pages={17--48},
  year={2013},
  publisher={Springer}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{burgess2018understanding,
  title={Understanding disentangling in $\beta $-VAE},
  author={Burgess, Christopher P and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  journal={arXiv preprint arXiv:1804.03599},
  year={2018}
}


@article{Griffith_2015_quantifying, title={Quantifying Redundant Information in Predicting a Target Random Variable}, volume={17}, ISSN={1099-4300}, url={http://dx.doi.org/10.3390/e17074644}, DOI={10.3390/e17074644}, number={12}, journal={Entropy}, publisher={MDPI AG}, author={Griffith, Virgil and Ho, Tracey}, year={2015}, month={Jul}, pages={4644–4653}}

@article{banerjee2015synergy,
  title={Synergy, redundancy and common information},
  author={Banerjee, Pradeep Kr and Griffith, Virgil},
  journal={arXiv preprint arXiv:1509.03706},
  year={2015}
}

@article{shenoy2013cortical,
  title={Cortical control of arm movements: a dynamical systems perspective},
  author={Shenoy, Krishna V and Sahani, Maneesh and Churchland, Mark M},
  journal={Annual review of neuroscience},
  volume={36},
  pages={337--359},
  year={2013},
  publisher={Annual Reviews}
}


@article{griffith2014intersection, title={Intersection Information Based on Common Randomness}, volume={16}, ISSN={1099-4300}, url={http://dx.doi.org/10.3390/e16041985}, DOI={10.3390/e16041985}, number={4}, journal={Entropy}, publisher={MDPI AG}, author={Griffith, Virgil and Chong, Edwin and James, Ryan and Ellison, Christopher and Crutchfield, James}, year={2014}, month={Apr}, pages={1985–2000}}

@Inbook{Griffith2014quantifying,
author="Griffith, Virgil
and Koch, Christof",
editor="Prokopenko, Mikhail",
title="Quantifying Synergistic Mutual Information",
bookTitle="Guided Self-Organization: Inception",
year="2014",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="159--190",
abstract="Synergy is a fundamental concept in complex systems that has received much attention in computational biology (Narayanan et al. 2005; Balduzzi and Tononi 2008). Several papers (Schneidman et al. 2003a; Bell 2003; Nirenberg et al. 2001;Williams and Beer 2010) have proposed measures for quantifying synergy, but there remains no consensus which measure is most valid.",
isbn="978-3-642-53734-9",
doi="10.1007/978-3-642-53734-9_6",
url="https://doi.org/10.1007/978-3-642-53734-9_6"
}

@inproceedings{tishby99information,
  added-at = {2017-09-27T13:23:06.000+0200},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  biburl = {https://www.bibsonomy.org/bibtex/2c61af806ab3a8fe92154753e84736818/mo_xime},
  booktitle = {Proc. of the 37-th Annual Allerton Conference on Communication, Control and Computing},
  comment = {cte: information bottleneck},
  interhash = {15bd5efbf394791da00b09839b9a5757},
  intrahash = {c61af806ab3a8fe92154753e84736818},
  keywords = {information},
  pages = {368-377},
  timestamp = {2017-09-27T15:48:05.000+0200},
  title = {The information bottleneck method},
  url = {https://arxiv.org/abs/physics/0004057},
  year = 1999
}

@INPROCEEDINGS{he2015deep,
  author={K. {He} and X. {Zhang} and S. {Ren} and J. {Sun}},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  doi={10.1109/CVPR.2016.90}
}

@article{blackwell1953,
author = "Blackwell, David",
doi = "10.1214/aoms/1177729032",
fjournal = "Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "06",
number = "2",
pages = "265--272",
publisher = "The Institute of Mathematical Statistics",
title = "Equivalent Comparisons of Experiments",
url = "https://doi.org/10.1214/aoms/1177729032",
volume = "24",
year = "1953"
}



@INPROCEEDINGS{banerjee2018computing,
  author={P. K. {Banerjee} and J. {Rauh} and G. {Montúfar}},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)}, 
  title={Computing the Unique Information}, 
  year={2018},
  volume={},
  number={},
  pages={141-145},
  doi={10.1109/ISIT.2018.8437757}
}


@article{williams2010nonnegative,
  title={Nonnegative decomposition of multivariate information},
  author={Williams, Paul L and Beer, Randall D},
  journal={arXiv preprint arXiv:1004.2515},
  year={2010}
}


@inproceedings{sgld_welling_teh,
  author = {Welling, Max and Teh, Yee Whye},
  booktitle = {ICML},
  editor = {Getoor, Lise and Scheffer, Tobias},
  ee = {https://icml.cc/2011/papers/398_icmlpaper.pdf},
  keywords = {dblp},
  pages = {681-688},
  publisher = {Omnipress},
  timestamp = {2019-04-04T11:48:16.000+0200},
  title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics.},
  year = 2011
}

@inproceedings{barber2003variational
, author = {Barber, David and Agakov, Felix}, title = {The IM Algorithm: A Variational Approach to Information Maximization}, year = {2003}, publisher = {MIT Press}, address = {Cambridge, MA, USA}, booktitle = {Proceedings of the 16th International Conference on Neural Information Processing Systems}, pages = {201–208}, numpages = {8}, location = {Whistler, British Columbia, Canada}, series = {NIPS’03} }

@article {PoggioTheory3,
	title = {Theory III: Dynamics and Generalization in Deep Networks},
	year = {2018},
	month = {06/2018},
	author = {Andrzej Banburski and Qianli Liao and Brando Miranda and Tomaso Poggio and Lorenzo Rosasco and Jack Hidary and Fernanda De La Torre}
}

@InProceedings{poole19variational,
  title = 	 {On Variational Bounds of Mutual Information},
  author = 	 {Poole, Ben and Ozair, Sherjil and Van Den Oord, Aaron and Alemi, Alex and Tucker, George},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5171--5180},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Long Beach, California, USA},
  month = 	 {09--15 Jun},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/poole19a/poole19a.pdf},
  url = 	 {http://proceedings.mlr.press/v97/poole19a.html},
  abstract = 	 {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning, but bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks. However, the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of these new bounds for estimation and representation learning.}
}

@article{achille2018emergence,
  author  = {Alessandro Achille and Stefano Soatto},
  title   = {Emergence of Invariance and Disentanglement in Deep Representations },
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {19},
  number  = {50},
  pages   = {1-34},
  url     = {http://jmlr.org/papers/v19/17-646.html}
}

@inproceedings{
achille2018critical,
title={Critical Learning Periods in Deep Networks},
author={Alessandro Achille and Matteo Rovere and Stefano Soatto},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=BkeStsCcKQ},
}

@article{paninski2003samples,
author = {Paninski, Liam},
title = {Estimation of Entropy and Mutual Information},
year = {2003},
issue_date = {June 2003},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {15},
number = {6},
issn = {0899-7667},
url = {https://doi.org/10.1162/089976603321780272},
doi = {10.1162/089976603321780272},
journal = {Neural Comput.},
month = jun,
pages = {1191–1253},
numpages = {63}
}

@article{shwartz17opening,
  author    = {Ravid Shwartz{-}Ziv and
               Naftali Tishby},
  title     = {Opening the Black Box of Deep Neural Networks via Information},
  journal   = {CoRR},
  volume    = {abs/1703.00810},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.00810},
  archivePrefix = {arXiv},
  eprint    = {1703.00810},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Shwartz-ZivT17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{dubois2019dvae,
  title        = {Disentangling VAE},
  author       = {Dubois, Yann and Kastanos, Alexandros and Lines, Dave and Melman, Bart},
  month        = {march},
  year         = {2019},
  howpublished = {\url{http://github.com/YannDubs/disentangling-vae/}}
}

@inproceedings{saxe2018information,
title={On the Information Bottleneck Theory of Deep Learning},
author={Andrew Michael Saxe and Yamini Bansal and Joel Dapello and Madhu Advani and Artemy Kolchinsky and Brendan Daniel Tracey and David Daniel Cox},
booktitle={International Conference on Learning Representations},
year={2018},
url={https://openreview.net/forum?id=ry_WPG-A-},
}


@InProceedings{goldfeld2019estimating,
  title = 	 {Estimating Information Flow in Deep Neural Networks},
  author =       {Goldfeld, Ziv and Van Den Berg, Ewout and Greenewald, Kristjan and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {2299--2308},
  year = 	 {2019},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/goldfeld19a/goldfeld19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/goldfeld19a.html},
}


@book{goodfellow2016book,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@article{Bengio-2009, author = {Bengio, Yoshua}, title = {Learning Deep Architectures for AI}, year = {2009}, issue_date = {January 2009}, publisher = {Now Publishers Inc.}, address = {Hanover, MA, USA}, volume = {2}, number = {1}, issn = {1935-8237}, url = {https://doi.org/10.1561/2200000006}, doi = {10.1561/2200000006}, journal = {Found. Trends Mach. Learn.}, month = jan, pages = {1–127}, numpages = {127} }

@INPROCEEDINGS{Tishby-2015, 
author={N. {Tishby} and N. {Zaslavsky}}, 
booktitle={2015 IEEE Information Theory Workshop (ITW)}, 
title={Deep learning and the information bottleneck principle}, 
year={2015}, 
volume={}, 
number={}, 
pages={1-5}, 
keywords={bifurcation;data compression;learning (artificial intelligence);neural nets;information bottleneck principle;deep neural networks;DNN;IB principle;mutual information;output variables;optimal information theoretic limits;finite sample generalization bounds;optimal architecture;layer connections;bifurcation points;input layer compression;output layer;hierarchical layered network representations;structural phase transitions;information curve;optimality bounds;deep learning algorithms;layer features;Distortion;Complexity theory;Mutual information;Bifurcation;Computer architecture;Feature extraction;Training}, 
doi={10.1109/ITW.2015.7133169}, 
ISSN={null}, 
month={April},}


@article{bengio2009-representation,
  author    = {Yoshua Bengio and
               Aaron C. Courville and
               Pascal Vincent},
  title     = {Unsupervised Feature Learning and Deep Learning: {A} Review and New
               Perspectives},
  journal   = {CoRR},
  volume    = {abs/1206.5538},
  year      = {2012},
  url       = {http://arxiv.org/abs/1206.5538},
  archivePrefix = {arXiv},
  eprint    = {1206.5538},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1206-5538},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@Article{kleinman2021redundant,
AUTHOR = {Kleinman, Michael and Achille, Alessandro and Soatto, Stefano and Kao, Jonathan C.},
TITLE = {Redundant Information Neural Estimation},
JOURNAL = {Entropy},
VOLUME = {23},
YEAR = {2021},
NUMBER = {7},
ARTICLE-NUMBER = {922},
URL = {https://www.mdpi.com/1099-4300/23/7/922},
PubMedID = {34356463},
ISSN = {1099-4300},
ABSTRACT = {We introduce the Redundant Information Neural Estimator (RINE), a method that allows efficient estimation for the component of information about a target variable that is common to a set of sources, known as the âredundant informationâ. We show that existing definitions of the redundant information can be recast in terms of an optimization over a family of functions. In contrast to previous information decompositions, which can only be evaluated for discrete variables over small alphabets, we show that optimizing over functions enables the approximation of the redundant information for high-dimensional and continuous predictors. We demonstrate this on high-dimensional image classification and motor-neuroscience tasks.},
DOI = {10.3390/e23070922}
}





@article{chandrasekaran2017laminar,
	Author = {Chandrasekaran, Chandramouli and Peixoto, Diogo and Newsome, William T. and Shenoy, Krishna V.},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {614},
	Title = {Laminar differences in decision-related neural activity in dorsal premotor cortex},
	Volume = {8},
	Year = {2017}}


@article {kleinman2019multi,
	author = {Kleinman, Michael and Chandrasekaran, Chandramouli and Kao, Jonathan C.},
	title = {Recurrent neural network models of multi-area computation underlying decision-making},
	elocation-id = {798553},
	year = {2019},
	doi = {10.1101/798553},
	publisher = {Cold Spring Harbor Laboratory},
	URL = {https://www.biorxiv.org/content/early/2019/10/09/798553},
	eprint = {https://www.biorxiv.org/content/early/2019/10/09/798553.full.pdf},
	journal = {bioRxiv}
}


@incollection{golatkar2019regularization,
title = {Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence},
author = {Golatkar, Aditya Sharad and Achille, Alessandro and Soatto, Stefano},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {10677--10687},
year = {2019},
publisher = {Curran Associates, Inc.}
}

