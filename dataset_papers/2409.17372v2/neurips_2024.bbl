\begin{thebibliography}{10}

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv}, 2023.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv}, 2022.

\bibitem{scao2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, Matthias Gall{\'e}, et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock {\em arXiv}, 2022.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em NeurIPS}, 33:1877--1901, 2020.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock 2020.

\bibitem{frantar-sparsegpt}
Elias Frantar and Dan Alistarh.
\newblock {SparseGPT}: Massive language models can be accurately pruned in one-shot.
\newblock {\em arXiv preprint arXiv:2301.00774}, 2023.

\bibitem{ma2023llmpruner}
Xinyin Ma, Gongfan Fang, and Xinchao Wang.
\newblock Llm-pruner: On the structural pruning of large language models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2023.

\bibitem{ashkboos2024slicegpt}
Saleh Ashkboos, Maximilian~L. Croci, Marcelo~Gennari do~Nascimento, Torsten Hoefler, and James Hensman.
\newblock Slice{GPT}: Compress large language models by deleting rows and columns.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{an2023flap}
Yongqi An, Xu~Zhao, Tao Yu, Ming Tang, and Jinqiao Wang.
\newblock Fluctuation-based adaptive structured pruning for large language models, 2023.

\bibitem{zheng2024exploring}
Zheng Zhan, Zhenglun Kong, Yifan Gong, Yushu Wu, Zichong Meng, Hangyu Zheng, Xuan Shen, Stratis Ioannidis, Wei Niu, Pu~Zhao, and Yanzhi Wang.
\newblock Exploring token pruning in vision state space models.
\newblock {\em arXiv preprint arXiv:2409.18962}, 2024.

\bibitem{shen2024edgeqat}
Xuan Shen, Zhenglun Kong, Changdi Yang, Zhaoyang Han, Lei Lu, Peiyan Dong, et~al.
\newblock Edgeqat: Entropy and distribution guided quantization-aware training for the acceleration of lightweight llms on the edge.
\newblock {\em arXiv preprint arXiv:2402.10787}, 2024.

\bibitem{yang2023pruning}
Changdi Yang, Pu~Zhao, Yanyu Li, Wei Niu, Jiexiong Guan, Hao Tang, Minghai Qin, Bin Ren, Xue Lin, and Yanzhi Wang.
\newblock Pruning parameterization with bi-level optimization for efficient semantic segmentation on the edge.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 15402--15412, 2023.

\bibitem{zhang2022advancing}
Yihua Zhang, Yuguang Yao, Parikshit Ram, Pu~Zhao, Tianlong Chen, Mingyi Hong, Yanzhi Wang, and Sijia Liu.
\newblock Advancing model pruning via bi-level optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 35:18309--18326, 2022.

\bibitem{rtseg}
Yanyu Li, Changdi Yang, Pu~Zhao, Geng Yuan, Wei Niu, Jiexiong Guan, Hao Tang, Minghai Qin, Qing Jin, Bin Ren, Xue Lin, and Yanzhi Wang.
\newblock Towards real-time segmentation on the edge.
\newblock In {\em Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence}, AAAI'23/IAAI'23/EAAI'23. AAAI Press, 2023.

\bibitem{zhao2024pruningfoundationmodelshigh}
Pu~Zhao, Fei Sun, Xuan Shen, Pinrui Yu, Zhenglun Kong, Yanzhi Wang, and Xue Lin.
\newblock Pruning foundation models for high accuracy without retraining.
\newblock {\em arXiv preprint arXiv:2410.15567}, 2024.

\bibitem{frantar-gptq}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock {GPTQ}: Accurate post-training compression for generative pretrained transformers.
\newblock {\em arXiv preprint arXiv:2210.17323}, 2022.

\bibitem{xiao2023smoothquant}
Guangxuan Xiao, Ji~Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han.
\newblock {S}mooth{Q}uant: Accurate and efficient post-training quantization for large language models.
\newblock In {\em Proceedings of the 40th International Conference on Machine Learning}, 2023.

\bibitem{Shen_2024_Agile}
Xuan Shen, Peiyan Dong, Lei Lu, Zhenglun Kong, Zhengang Li, Ming Lin, Chao Wu, and Yanzhi Wang.
\newblock Agile-quant: Activation-guided quantization for faster inference of llms on the edge.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence}, 38(17):18944--18951, Mar. 2024.

\bibitem{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for bert model compression.
\newblock {\em arXiv preprint arXiv:1908.09355}, 2019.

\bibitem{sun-etal-2020-contrastive}
Siqi Sun, Zhe Gan, Yuwei Fang, Yu~Cheng, Shuohang Wang, and Jingjing Liu.
\newblock Contrastive distillation on intermediate representations for language model compression.
\newblock In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, {\em Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)}, pages 498--508, Online, November 2020. Association for Computational Linguistics.

\bibitem{pan-etal-2021-meta}
Haojie Pan, Chengyu Wang, Minghui Qiu, Yichang Zhang, Yaliang Li, and Jun Huang.
\newblock Meta-{KD}: A meta knowledge distillation framework for language model compression across domains.
\newblock In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, {\em Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3026--3036, Online, August 2021. Association for Computational Linguistics.

\bibitem{pmlr-v97-tan19a}
Mingxing Tan and Quoc Le.
\newblock {E}fficient{N}et: Rethinking model scaling for convolutional neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em Proceedings of the 36th International Conference on Machine Learning}, volume~97 of {\em Proceedings of Machine Learning Research}, pages 6105--6114. PMLR, 09--15 Jun 2019.

\bibitem{Shen_2023_CVPR}
Xuan Shen, Yaohua Wang, Ming Lin, Yilun Huang, Hao Tang, Xiuyu Sun, and Yanzhi Wang.
\newblock Deepmad: Mathematical architecture design for deep convolutional neural network.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pages 6163--6173, June 2023.

\bibitem{ming_zennas_iccv2021}
Ming Lin, Pichao Wang, Zhenhong Sun, Hesen Chen, Xiuyu Sun, Qi~Qian, Hao Li, and Rong Jin.
\newblock Zen-nas: A zero-shot nas for high-performance deep image recognition.
\newblock In {\em 2021 IEEE/CVF International Conference on Computer Vision, {ICCV} 2021}, 2021.

\bibitem{gong2022nasvit}
Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, qiang liu, and Vikas Chandra.
\newblock {NASV}it: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training.
\newblock In {\em International Conference on Learning Representations}, 2022.

\bibitem{su2021vision}
Xiu Su, Shan You, Jiyang Xie, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu.
\newblock Vision transformer architecture search.
\newblock {\em arXiv preprint arXiv:2106.13700}, 2021.

\bibitem{Ma_2023_CVPR}
Dongning Ma, Pengfei Zhao, and Xun Jiao.
\newblock Perfhd: Efficient vit architecture performance ranking using hyperdimensional computing.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops}, pages 2230--2237, June 2023.

\bibitem{autoFormer}
Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
\newblock Autoformer: Searching transformers for visual recognition.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 12270--12280, October 2021.

\bibitem{NEURIPS2023_081b0806}
Peiyan Dong, Zhenglun Kong, Xin Meng, Pinrui Yu, Yifan Gong, Geng Yuan, Hao Tang, and Yanzhi Wang.
\newblock Hotbev: Hardware-oriented transformer-based multi-view 3d detector for bev perception.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 2824--2836. Curran Associates, Inc., 2023.

\bibitem{zhan2021achieving}
Zheng Zhan, Yifan Gong, Pu~Zhao, Geng Yuan, Wei Niu, Yushu Wu, Tianyun Zhang, Malith Jayaweera, David Kaeli, Bin Ren, et~al.
\newblock Achieving on-mobile real-time super-resolution with neural architecture and pruning search.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 4821--4831, 2021.

\bibitem{wu2022compiler}
Yushu Wu, Yifan Gong, Pu~Zhao, Yanyu Li, Zheng Zhan, Wei Niu, Hao Tang, Minghai Qin, Bin Ren, and Yanzhi Wang.
\newblock Compiler-aware neural architecture search for on-mobile real-time super-resolution.
\newblock In {\em European Conference on Computer Vision}, pages 92--111. Springer, 2022.

\bibitem{yang2023fast}
Changdi Yang, Yi~Sheng, Peiyan Dong, Zhenglun Kong, Yanyu Li, Pinrui Yu, Lei Yang, Xue Lin, and Yanzhi Wang.
\newblock Fast and fair medical ai on the edge through neural architecture search for hybrid vision models.
\newblock In {\em 2023 IEEE/ACM International Conference on Computer Aided Design (ICCAD)}, pages 01--09. IEEE, 2023.

\bibitem{yang2023late}
Changdi Yang, Yi~Sheng, Peiyan Dong, Zhenglun Kong, Yanyu Li, Pinrui Yu, Lei Yang, and Xue Lin.
\newblock Late breaking results: Fast fair medical applications? hybrid vision models achieve the fairness on the edge.
\newblock In {\em 2023 60th ACM/IEEE Design Automation Conference (DAC)}, pages 1--2. IEEE, 2023.

\bibitem{dong2024hotbev}
Peiyan Dong, Zhenglun Kong, Xin Meng, Pinrui Yu, Yifan Gong, Geng Yuan, Hao Tang, and Yanzhi Wang.
\newblock Hotbev: Hardware-oriented transformer-based multi-view 3d detector for bev perception.
\newblock In A.~Oh, T.~Naumann, A.~Globerson, K.~Saenko, M.~Hardt, and S.~Levine, editors, {\em Advances in Neural Information Processing Systems}, volume~36, pages 2824--2836. Curran Associates, Inc., 2023.

\bibitem{li2022pruning}
Yanyu Li, Pu~Zhao, Geng Yuan, Xue Lin, Yanzhi Wang, and Xin Chen.
\newblock Pruning-as-search: Efficient neural architecture search via channel pruning and structural reparameterization.
\newblock {\em arXiv preprint arXiv:2206.01198}, 2022.

\bibitem{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock {\em Foundations and trends{\textregistered} in Optimization}, 1(3):127--239, 2014.

\bibitem{boyd2011distributed}
Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, Jonathan Eckstein, et~al.
\newblock Distributed optimization and statistical learning via the alternating direction method of multipliers.
\newblock {\em Foundations and Trends{\textregistered} in Machine learning}, 3(1):1--122, 2011.

\bibitem{10.1145/3240508.3240639}
Pu~Zhao, Sijia Liu, Yanzhi Wang, and Xue Lin.
\newblock An admm-based universal framework for adversarial attacks on deep neural networks.
\newblock In {\em Proceedings of the 26th ACM International Conference on Multimedia}, MM '18, page 1065–1073, New York, NY, USA, 2018. Association for Computing Machinery.

\bibitem{gong2020privacy}
Yifan Gong, Zheng Zhan, Zhengang Li, Wei Niu, Xiaolong Ma, Wenhao Wang, Bin Ren, Caiwen Ding, Xue Lin, Xiaolin Xu, et~al.
\newblock A privacy-preserving-oriented dnn pruning and mobile acceleration framework.
\newblock In {\em Proceedings of the 2020 on Great Lakes Symposium on VLSI}, pages 119--124, 2020.

\bibitem{sun2023wanda}
Mingjie Sun, Zhuang Liu, Anna Bair, and J.~Zico Kolter.
\newblock A simple and effective pruning approach for large language models.
\newblock {\em arXiv preprint arXiv:2306.11695}, 2023.

\bibitem{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural networks.
\newblock In {\em International conference on machine learning}, pages 6105--6114. PMLR, 2019.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In Jill Burstein, Christy Doran, and Thamar Solorio, editors, {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{dosovitskiy2020vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em ICLR}, 2021.

\bibitem{singh2020woodfisher}
Sidak~Pal Singh and Dan Alistarh.
\newblock Woodfisher: Efficient second-order approximation for neural network compression.
\newblock {\em Advances in Neural Information Processing Systems}, 33:18098--18109, 2020.

\bibitem{liao2021searching}
Yi-Lun Liao, Sertac Karaman, and Vivienne Sze.
\newblock Searching for efficient multi-stage vision transformers.
\newblock {\em arXiv preprint arXiv:2109.00642}, 2021.

\bibitem{goldstein2014fast}
Tom Goldstein, Brendan O'Donoghue, Simon Setzer, and Richard Baraniuk.
\newblock Fast alternating direction optimization methods.
\newblock {\em SIAM Journal on Imaging Sciences}, 7(3):1588--1623, 2014.

\bibitem{wikitextdataset}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em arXiv}, 2016.

\bibitem{ptbdataset}
Mitchell~P. Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of {E}nglish: The {P}enn {T}reebank.
\newblock {\em Computational Linguistics}, pages 313--330.

\bibitem{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no questions.
\newblock {\em arXiv preprint arXiv:1905.10044}, 2019.

\bibitem{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~34, pages 7432--7439, 2020.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock {\em arXiv preprint arXiv:1905.07830}, 2019.

\bibitem{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em Communications of the ACM}, 64(9):99--106, 2021.

\bibitem{clark2018arc}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock {\em arXiv preprint arXiv:1803.05457}, 2018.

\bibitem{mihaylov-etal-2018-suit-opqa}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock In Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun{'}ichi Tsujii, editors, {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pages 2381--2391, Brussels, Belgium, October-November 2018. Association for Computational Linguistics.

\bibitem{zheng2023vicuna}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\bibitem{silu_act}
Stefan Elfwing, Eiji Uchibe, and Kenji Doya.
\newblock Sigmoid-weighted linear units for neural network function approximation in reinforcement learning.
\newblock {\em Neural networks}, 107:3--11, 2018.

\end{thebibliography}
