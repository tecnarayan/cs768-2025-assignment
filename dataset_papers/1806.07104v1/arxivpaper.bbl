\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi et~al.(2013)Abbasi, Bartlett, Kanade, Seldin, and
  Szepesv{\'a}ri]{abbasi2013online}
Abbasi, Y., Bartlett, P.~L., Kanade, V., Seldin, Y., and Szepesv{\'a}ri, C.
\newblock Online learning in markov decision processes with adversarially
  chosen transition probability distributions.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2508--2516, 2013.

\bibitem[Abbasi-Yadkori \& Szepesv{\'a}ri(2011)Abbasi-Yadkori and
  Szepesv{\'a}ri]{abbasi2011regret}
Abbasi-Yadkori, Y. and Szepesv{\'a}ri, C.
\newblock Regret bounds for the adaptive control of linear quadratic systems.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pp.\  1--26, 2011.

\bibitem[Abbasi-Yadkori et~al.(2014)Abbasi-Yadkori, Bartlett, and
  Kanade]{abbasi2014tracking}
Abbasi-Yadkori, Y., Bartlett, P., and Kanade, V.
\newblock Tracking adversarial targets.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  369--377, 2014.

\bibitem[Abbeel et~al.(2007)Abbeel, Coates, Quigley, and
  Ng]{abbeel2007application}
Abbeel, P., Coates, A., Quigley, M., and Ng, A.~Y.
\newblock An application of reinforcement learning to aerobatic helicopter
  flight.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1--8, 2007.

\bibitem[Abeille \& Lazaric(2017)Abeille and Lazaric]{abeille2017}
Abeille, M. and Lazaric, A.
\newblock Thompson sampling for linear-quadratic control problems.
\newblock In \emph{AISTATS}, 2017.

\bibitem[Anderson et~al.(1972)Anderson, Moore, and
  Molinari]{anderson1972linear}
Anderson, B., Moore, J., and Molinari, B.
\newblock Linear optimal control.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics}, \penalty0
  (4):\penalty0 559--559, 1972.

\bibitem[Arora et~al.(2018)Arora, Hazan, Lee, Singh, Zhang, and
  Zhang]{arora2018towards}
Arora, S., Hazan, E., Lee, H., Singh, K., Zhang, C., and Zhang, Y.
\newblock Towards provable control for unknown linear dynamical systems.
\newblock \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=BygpQlbA-}.
\newblock workshop track.

\bibitem[{\AA}str{\"o}m \& Wittenmark(1973){\AA}str{\"o}m and
  Wittenmark]{astrom1973}
{\AA}str{\"o}m, K.~J. and Wittenmark, B.
\newblock On self tuning regulators.
\newblock \emph{Automatica}, 9\penalty0 (2):\penalty0 185--199, 1973.

\bibitem[Auer \& Ortner(2007)Auer and Ortner]{auer2007logarithmic}
Auer, P. and Ortner, R.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  49--56, 2007.

\bibitem[Balakrishnan \& Vandenberghe(2003)Balakrishnan and
  Vandenberghe]{balakrishnan2003semidefinite}
Balakrishnan, V. and Vandenberghe, L.
\newblock Semidefinite programming duality and linear time-invariant systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 48\penalty0
  (1):\penalty0 30--41, 2003.

\bibitem[Bertsekas(1995)]{bertsekas1995dynamic}
Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem[Bittanti \& Campi(2006)Bittanti and Campi]{bittanti2006}
Bittanti, S. and Campi, M.~C.
\newblock Adaptive control of linear time invariant systems: the bet on the
  best principle.
\newblock \emph{Communications in Information \& Systems}, 6\penalty0
  (4):\penalty0 299--320, 2006.

\bibitem[Bradtke(1993)]{bradtke1993reinforcement}
Bradtke, S.~J.
\newblock Reinforcement learning applied to linear quadratic regulation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  295--302, 1993.

\bibitem[Campi \& Kumar(1998)Campi and Kumar]{campi1998}
Campi, M.~C. and Kumar, P.
\newblock Adaptive linear quadratic gaussian control: the cost-biased approach
  revisited.
\newblock \emph{SIAM Journal on Control and Optimization}, 36\penalty0
  (6):\penalty0 1890--1907, 1998.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Dean et~al.(2017)Dean, Mania, Matni, Recht, and Tu]{dean2017sample}
Dean, S., Mania, H., Matni, N., Recht, B., and Tu, S.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock \emph{arXiv preprint arXiv:1710.01688}, 2017.

\bibitem[Dvijotham et~al.(2013)Dvijotham, Todorov, and
  Fazel]{dvijotham2013convex}
Dvijotham, K., Todorov, E., and Fazel, M.
\newblock Convex control design via covariance minimization.
\newblock In \emph{Communication, Control, and Computing (Allerton), 2013 51st
  Annual Allerton Conference on}, pp.\  93--99. IEEE, 2013.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Fazel, M., Ge, R., Kakade, S.~M., and Mesbahi, M.
\newblock Global convergence of policy gradient methods for linearized control
  problems.
\newblock \emph{arXiv preprint arXiv:1801.05039}, 2018.

\bibitem[Gao \& Jamidar(2014)Gao and Jamidar]{gao2014machine}
Gao, J. and Jamidar, R.
\newblock Machine learning applications for data center optimization.
\newblock \emph{Google White Paper}, 2014.

\bibitem[Hazan(2016)]{hazan2016introduction}
Hazan, E.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan et~al.(2017)Hazan, Singh, and Zhang]{hazan2017learning}
Hazan, E., Singh, K., and Zhang, C.
\newblock Learning linear dynamical systems via spectral filtering.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6705--6715, 2017.

\bibitem[Ibrahimi et~al.(2012)Ibrahimi, Javanmard, and Roy]{ibrahimi2012}
Ibrahimi, M., Javanmard, A., and Roy, B.~V.
\newblock Efficient reinforcement learning for high dimensional linear
  quadratic systems.
\newblock In \emph{Advances in Neural Information Processing Systems 25}, pp.\
  2636--2644. Curran Associates, Inc., 2012.

\bibitem[Kalai \& Vempala(2005)Kalai and Vempala]{kalai2005efficient}
Kalai, A. and Vempala, S.
\newblock Efficient algorithms for online decision problems.
\newblock \emph{Journal of Computer and System Sciences}, 71\penalty0
  (3):\penalty0 291--307, 2005.

\bibitem[Lee \& Hu(2016)Lee and Hu]{lee2016semidefinite}
Lee, D.-H. and Hu, J.
\newblock A semidefinite programming formulation of the lqr problem and its
  dual.
\newblock 2016.

\bibitem[Lee \& Khargonekar(2007)Lee and Khargonekar]{lee2007constrained}
Lee, J.-W. and Khargonekar, P.~P.
\newblock Constrained infinite-horizon linear quadratic regulation of
  discrete-time systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 52\penalty0
  (10):\penalty0 1951--1958, 2007.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lewis \& Vrabie(2009)Lewis and Vrabie]{lewis2009reinforcement}
Lewis, F.~L. and Vrabie, D.
\newblock Reinforcement learning and adaptive dynamic programming for feedback
  control.
\newblock \emph{IEEE circuits and systems magazine}, 9\penalty0 (3), 2009.

\bibitem[Neu \& G{\'o}mez(2017)Neu and G{\'o}mez]{neu2017fast}
Neu, G. and G{\'o}mez, V.
\newblock Fast rates for online learning in linearly solvable markov decision
  processes.
\newblock \emph{Proceedings of Machine Learning Research vol}, 65:\penalty0
  1--22, 2017.

\bibitem[Schildbach et~al.(2015)Schildbach, Goulart, and
  Morari]{schildbach2015linear}
Schildbach, G., Goulart, P., and Morari, M.
\newblock Linear controller design for chance constrained systems.
\newblock \emph{Automatica}, 51:\penalty0 278--284, 2015.

\bibitem[Shalev-Shwartz(2012)]{shalev2012online}
Shalev-Shwartz, S.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (2):\penalty0 107--194, 2012.

\bibitem[Sheckells et~al.(2017)Sheckells, Garimella, and
  Kobilarov]{sheckells2017robust}
Sheckells, M., Garimella, G., and Kobilarov, M.
\newblock Robust policy search with applications to safe vehicle navigation.
\newblock In \emph{Robotics and Automation (ICRA), 2017 IEEE International
  Conference on}, pp.\  2343--2349. IEEE, 2017.

\bibitem[Todorov(2009)]{todorov2009efficient}
Todorov, E.
\newblock Efficient computation of optimal actions.
\newblock \emph{Proceedings of the national academy of sciences}, 106\penalty0
  (28):\penalty0 11478--11483, 2009.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Vershynin, R.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Yu et~al.(2009)Yu, Mannor, and Shimkin]{yu2009markov}
Yu, J.~Y., Mannor, S., and Shimkin, N.
\newblock Markov decision processes with arbitrary reward processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  737--757, 2009.

\bibitem[Zhou et~al.(1996)Zhou, Doyle, Glover, et~al.]{zhou1996robust}
Zhou, K., Doyle, J.~C., Glover, K., et~al.
\newblock \emph{Robust and optimal control}, volume~40.
\newblock Prentice hall New Jersey, 1996.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pp.\  928--936, 2003.

\end{thebibliography}
