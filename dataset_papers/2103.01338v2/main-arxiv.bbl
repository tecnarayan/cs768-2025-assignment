\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{KMN{\etalchar{+}}16}

\bibitem[AAH{\etalchar{+}}20]{agarwal2020disentangling}
Naman Agarwal, Rohan Anil, Elad Hazan, Tomer Koren, and Cyril Zhang.
\newblock Disentangling adaptive gradient methods from learning rates.
\newblock {\em arXiv preprint arXiv:2002.11803}, 2020.

\bibitem[AAK{\etalchar{+}}20]{agarwal2020stochastic}
Naman Agarwal, Rohan Anil, Tomer Koren, Kunal Talwar, and Cyril Zhang.
\newblock Stochastic optimization with laggard data pipelines.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Ait27]{aitken1927xxv}
Alexander~Craig Aitken.
\newblock {XXV}.—{O}n {B}ernoulli's numerical solution of algebraic
  equations.
\newblock {\em Proceedings of the Royal Society of Edinburgh}, 46:289--305,
  1927.

\bibitem[AK89]{aarts1989simulated}
Emile Aarts and Jan Korst.
\newblock {\em Simulated annealing and Boltzmann machines: a stochastic
  approach to combinatorial optimization and neural computing}.
\newblock John Wiley \& Sons, Inc., 1989.

\bibitem[And65]{anderson1965iterative}
Donald~G Anderson.
\newblock Iterative procedures for nonlinear integral equations.
\newblock {\em Journal of the ACM (JACM)}, 12(4):547--560, 1965.

\bibitem[AZH16]{allen2016optimal}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Optimal black-box reductions between optimization objectives.
\newblock {\em arXiv preprint arXiv:1603.05642}, 2016.

\bibitem[AZO14]{allen2014linear}
Zeyuan Allen-Zhu and Lorenzo Orecchia.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock {\em arXiv preprint arXiv:1407.1537}, 2014.

\bibitem[Bac20]{BachBlog}
Francis Bach.
\newblock Machine learning research blog, 2020.

\bibitem[BB07]{bottou2007tradeoffs}
Leon Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Proceedings of the 20th International Conference on Neural
  Information Processing Systems}, pages 161--168, 2007.

\bibitem[BE02]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em The Journal of Machine Learning Research}, 2:499--526, 2002.

\bibitem[BJL{\etalchar{+}}19]{bubeck2019near}
S{\'e}bastien Bubeck, Qijia Jiang, Yin~Tat Lee, Yuanzhi Li, and Aaron Sidford.
\newblock Near-optimal method for highly smooth convex optimization.
\newblock In {\em Conference on Learning Theory}, pages 492--507. PMLR, 2019.

\bibitem[BLS15]{bubeck2015geometric}
S{\'e}bastien Bubeck, Yin~Tat Lee, and Mohit Singh.
\newblock A geometric alternative to nesterov's accelerated gradient descent.
\newblock {\em arXiv preprint arXiv:1506.08187}, 2015.

\bibitem[BMR{\etalchar{+}}20]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[BTd20]{barre2020complexity}
Mathieu Barr{\'e}, Adrien Taylor, and Alexandre d’Aspremont.
\newblock Complexity guarantees for polyak steps with momentum.
\newblock In {\em Conference on Learning Theory}, pages 452--478. PMLR, 2020.

\bibitem[Bub17]{bubeck2017convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends in Machine Learning}, 8, 2017.

\bibitem[Bub19]{BubeckBlogNemirovski}
S{\'e}bastien Bubeck.
\newblock Nemirovski's acceleration (blog post), 2019.

\bibitem[BV04]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock {\em Convex optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[BZVL17]{bello2017neural}
Irwan Bello, Barret Zoph, Vijay Vasudevan, and Quoc~V Le.
\newblock Neural optimizer search with reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  459--468. PMLR, 2017.

\bibitem[Che53]{chebyshev1853theorie}
Pafnuti~L'vovich Chebyshev.
\newblock {\em Th{\'e}orie des m{\'e}canismes connus sous le nom de
  parall{\'e}logrammes}.
\newblock Imprimerie de l'Acad{\'e}mie imp{\'e}riale des sciences, 1853.

\bibitem[CJY18]{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock {\em arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[CKL{\etalchar{+}}21]{cohen2020gd}
Jeremy Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem[CO18]{cutkosky2018black}
Ashok Cutkosky and Francesco Orabona.
\newblock Black-box reductions for parameter-free online learning in banach
  spaces.
\newblock In {\em Conference On Learning Theory}, pages 1493--1529. PMLR, 2018.

\bibitem[CO19]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock {\em arXiv preprint arXiv:1905.10018}, 2019.

\bibitem[CPSD19]{choi2019faster}
Dami Choi, Alexandre Passos, Christopher~J Shallue, and George~E Dahl.
\newblock Faster neural network training with data echoing.
\newblock {\em arXiv preprint arXiv:1907.05550}, 2019.

\bibitem[DGN14]{devolder2014first}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock {\em Mathematical Programming}, 146(1):37--75, 2014.

\bibitem[DHS11]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of machine learning research}, 12(7), 2011.

\bibitem[Doz16]{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating nesterov momentum into adam.
\newblock 2016.

\bibitem[dST21]{d2021acceleration}
Alexandre d'Aspremont, Damien Scieur, and Adrien Taylor.
\newblock Acceleration methods.
\newblock {\em arXiv preprint arXiv:2101.09545}, 2021.

\bibitem[FLL{\etalchar{+}}19]{fu2019cyclical}
Hao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao, Asli Celikyilmaz, and Lawrence
  Carin.
\newblock Cyclical annealing schedule: A simple approach to mitigating kl
  vanishing.
\newblock {\em arXiv preprint arXiv:1903.10145}, 2019.

\bibitem[FS50]{flanders1950numerical}
Donald~A Flanders and George Shortley.
\newblock Numerical determination of fundamental modes.
\newblock {\em Journal of Applied Physics}, 21(12):1326--1332, 1950.

\bibitem[Gav50]{gavurin1950use}
Mark~Konstantinovich Gavurin.
\newblock The use of polynomials of best approximation for improving the
  convergence of iterative processes.
\newblock {\em Uspekhi Matematicheskikh Nauk}, 5(3):156--160, 1950.

\bibitem[GHR20]{gorbunov2020unified}
Eduard Gorbunov, Filip Hanzely, and Peter Richt{\'a}rik.
\newblock A unified theory of sgd: Variance reduction, sampling, quantization
  and coordinate descent.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 680--690. PMLR, 2020.

\bibitem[GKKN19]{ge2019step}
Rong Ge, Sham~M Kakade, Rahul Kidambi, and Praneeth Netrapalli.
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock {\em Advances in Neural Information Processing Systems},
  32:14977--14988, 2019.

\bibitem[GNHS19]{giladi2019stability}
Niv Giladi, Mor~Shpigel Nacson, Elad Hoffer, and Daniel Soudry.
\newblock At stability's edge: How to adjust hyperparameters to preserve minima
  selection in asynchronous training of neural networks?
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem[GO77]{gottlieb1977numerical}
David Gottlieb and Steven~A Orszag.
\newblock {\em Numerical analysis of spectral methods: theory and
  applications}.
\newblock SIAM, 1977.

\bibitem[Hig02]{higham2002accuracy}
Nicholas~J Higham.
\newblock {\em Accuracy and stability of numerical algorithms}.
\newblock SIAM, 2002.

\bibitem[HK19]{hazan2019revisiting}
Elad Hazan and Sham Kakade.
\newblock Revisiting the polyak step size.
\newblock {\em arXiv preprint arXiv:1905.00313}, 2019.

\bibitem[HRS16]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em International Conference on Machine Learning}, pages
  1225--1234. PMLR, 2016.

\bibitem[HS52]{hestenes1952methods}
Magnus~R Hestenes and Eduard Stiefel.
\newblock Methods of conjugate gradients for solving linear systems.
\newblock {\em Journal cf Research of the National Bureau of Standards}, 49(6),
  1952.

\bibitem[HZRS16]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em European conference on computer vision}, pages 630--645.
  Springer, 2016.

\bibitem[JGN{\etalchar{+}}17]{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem[JZTM20]{jiang2020characterizing}
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael~C Mozer.
\newblock Characterizing structural regularities of labeled data in
  overparameterized models.
\newblock {\em arXiv e-prints}, pages arXiv--2002, 2020.

\bibitem[KB14]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kel99]{kelley1999iterative}
Carl~T Kelley.
\newblock {\em Iterative methods for optimization}.
\newblock SIAM, 1999.

\bibitem[KMN{\etalchar{+}}16]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[LA19]{li2019exponential}
Zhiyuan Li and Sanjeev Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock {\em arXiv preprint arXiv:1910.07454}, 2019.

\bibitem[Lan12]{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock {\em Mathematical Programming}, 133(1-2):365--397, 2012.

\bibitem[LBD{\etalchar{+}}89]{lecun1989backpropagation}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard,
  Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551, 1989.

\bibitem[LF71]{lebedev1971order}
Vyacheslav~Ivanovich Lebedev and S.A. Finogenov.
\newblock The order of choice of the iteration parameters in the cyclic
  {C}hebyshev iteration method.
\newblock {\em Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  11(2):425--438, 1971.

\bibitem[LF73]{lebedev1973solution}
VI~Lebedev and SA~Finogenov.
\newblock Solution of the parameter ordering problem in chebyshev iterative
  methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  13(1):21--41, 1973.

\bibitem[LF76]{lebedev1976utilization}
VI~Lebedev and SA~Finogenov.
\newblock Utilization of ordered chebyshev parameters in iterative methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  16(4):70--83, 1976.

\bibitem[LF02]{lebedev2002construction}
VI~Lebedev and SA~Finogenov.
\newblock On construction of the stable permutations of parameters for the
  chebyshev iterative methods. part i.
\newblock {\em Russian Journal of Numerical Analysis and Mathematical
  Modelling}, 17(5):437--456, 2002.

\bibitem[LF04]{lebedev2004construction}
VI~Lebedev and SA~Finogenov.
\newblock On construction of the stable permutations of parameters for the
  chebyshev iterative methods. part ii.
\newblock {\em Russian Journal of Numerical Analysis and Mathematical
  Modelling}, 19(3):251--263, 2004.

\bibitem[LH16]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock {\em arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[LL20]{li2020fast}
Zhize Li and Jian Li.
\newblock A fast anderson-chebyshev acceleration for nonlinear optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1047--1057. PMLR, 2020.

\bibitem[LLA20]{li2020reconciling}
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora.
\newblock Reconciling modern deep learning with traditional optimization
  analyses: The intrinsic learning rate.
\newblock {\em arXiv preprint arXiv:2010.02916}, 2020.

\bibitem[LMH18]{lin2018catalyst}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock Catalyst acceleration for first-order convex optimization: from
  theory to practice.
\newblock {\em Journal of Machine Learning Research}, 18(1):7854--7907, 2018.

\bibitem[LN89]{liu1989limited}
Dong~C Liu and Jorge Nocedal.
\newblock On the limited memory bfgs method for large scale optimization.
\newblock {\em Mathematical programming}, 45(1):503--528, 1989.

\bibitem[LRP16]{lessard2016analysis}
Laurent Lessard, Benjamin Recht, and Andrew Packard.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock {\em SIAM Journal on Optimization}, 26(1):57--95, 2016.

\bibitem[LSP92]{lecun1993automatic}
Yann LeCun, Patrice~Y Simard, and Barak Pearlmutter.
\newblock Automatic learning rate maximization by on-line estimation of the
  hessian's eigenvectors.
\newblock In {\em Proceedings of the 5th International Conference on Neural
  Information Processing Systems}, pages 156--163, 1992.

\bibitem[LTW17]{li2017stochastic}
Qianxiao Li, Cheng Tai, and E~Weinan.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  2101--2110. PMLR, 2017.

\bibitem[LWM19]{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock {\em arXiv preprint arXiv:1907.04595}, 2019.

\bibitem[MH02]{mason2002chebyshev}
John~C Mason and David~C Handscomb.
\newblock {\em Chebyshev polynomials}.
\newblock CRC press, 2002.

\bibitem[MS13]{monteiro2013accelerated}
Renato~DC Monteiro and Benar~Fux Svaiter.
\newblock An accelerated hybrid proximal extragradient method for convex
  optimization and its implications to second-order methods.
\newblock {\em SIAM Journal on Optimization}, 23(2):1092--1125, 2013.

\bibitem[Nes83]{nesterov1983method}
Yurii~Evgen'evich Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate o(k\^{}2).
\newblock In {\em Doklady Akademii Nauk}, volume 269, pages 543--547. Russian
  Academy of Sciences, 1983.

\bibitem[Nes08]{nesterov2008accelerating}
Yu~Nesterov.
\newblock Accelerating the cubic regularization of newton’s method on convex
  problems.
\newblock {\em Mathematical Programming}, 112(1):159--181, 2008.

\bibitem[OC15]{o2015adaptive}
Brendan O’{D}onoghue and Emmanuel Candes.
\newblock Adaptive restart for accelerated gradient schemes.
\newblock {\em Foundations of computational mathematics}, 15(3):715--732, 2015.

\bibitem[OT17]{orabona2017training}
Francesco Orabona and Tatiana Tommasi.
\newblock Training deep networks without learning rates through coin betting.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 2157--2167, 2017.

\bibitem[Oym21]{oymak2021super}
Samet Oymak.
\newblock Super-convergence with an unstable learning rate.
\newblock {\em arXiv preprint arXiv:2102.10734}, 2021.

\bibitem[Pol64a]{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em {USSR} Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem[Pol64b]{POLYAK19641}
B.T. Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem[Pol87]{polyak1987introduction}
Boris~T Polyak.
\newblock Introduction to optimization. optimization software.
\newblock {\em Inc., Publications Division, New York}, 1, 1987.

\bibitem[PS20]{pmlr-v119-pedregosa20a}
Fabian Pedregosa and Damien Scieur.
\newblock Acceleration through spectral density estimation.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 7553--7562. PMLR, 13--18 Jul
  2020.

\bibitem[Ric11]{richardson1911ix}
Lewis~Fry Richardson.
\newblock The approximate arithmetical solution by finite differences of
  physical problems involving differential equations, with an application to
  the stresses in a masonry dam.
\newblock {\em Philosophical Transactions of the Royal Society of London.
  Series A, Containing Papers of a Mathematical or Physical Character},
  210(459-470):307--357, 1911.

\bibitem[SBC14]{su2014differential}
Weijie Su, Stephen~P Boyd, and Emmanuel~J Candes.
\newblock A differential equation for modeling nesterov's accelerated gradient
  method: Theory and insights.
\newblock In {\em NIPS}, volume~14, pages 2510--2518, 2014.

\bibitem[SFS86]{sidi1986acceleration}
Avram Sidi, William~F Ford, and David~A Smith.
\newblock Acceleration of convergence of vector sequences.
\newblock {\em SIAM Journal on Numerical Analysis}, 23(1):178--196, 1986.

\bibitem[SKYL18]{smith2018don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem[SLA{\etalchar{+}}19]{shallue2019measuring}
Christopher~J Shallue, Jaehoon Lee, Joseph Antognini, Jascha Sohl-Dickstein,
  Roy Frostig, and George~E Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock {\em Journal of Machine Learning Research}, 20:1--49, 2019.

\bibitem[SMDH13]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147. PMLR, 2013.

\bibitem[Smi17]{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In {\em 2017 IEEE winter conference on applications of computer
  vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem[SP20]{pmlr-v119-scieur20a}
Damien Scieur and Fabian Pedregosa.
\newblock Universal asymptotic optimality of polyak momentum.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 8565--8572. PMLR, 13--18 Jul
  2020.

\bibitem[SRK{\etalchar{+}}19]{staib2019escaping}
Matthew Staib, Sashank Reddi, Satyen Kale, Sanjiv Kumar, and Suvrit Sra.
\newblock Escaping saddle points with adaptive gradient methods.
\newblock In {\em International Conference on Machine Learning}, pages
  5956--5965. PMLR, 2019.

\bibitem[Sti58]{stiefel1958kernel}
Eduard~L Stiefel.
\newblock Kernel polynomial in linear algebra and their numerical applications.
\newblock {\em NBS Applied Math. Ser.}, 49:1--22, 1958.

\bibitem[SV13]{sachdeva2013faster}
Sushant Sachdeva and Nisheeth~K Vishnoi.
\newblock Faster algorithms via approximation theory.
\newblock {\em Theoretical Computer Science}, 9(2):125--210, 2013.

\bibitem[SZL13]{schaul2013no}
Tom Schaul, Sixin Zhang, and Yann LeCun.
\newblock No more pesky learning rates.
\newblock In {\em International Conference on Machine Learning}, pages
  343--351. PMLR, 2013.

\bibitem[Vis12]{vishnoi2012laplacian}
Nisheeth~K Vishnoi.
\newblock Laplacian solvers and their algorithmic applications.
\newblock {\em Theoretical Computer Science}, 8(1-2):1--141, 2012.

\bibitem[WW15]{wibisono2015accelerated}
Andre Wibisono and Ashia~C Wilson.
\newblock On accelerated methods in optimization.
\newblock {\em arXiv preprint arXiv:1509.03616}, 2015.

\bibitem[WWB19]{ward2019adagrad}
Rachel Ward, Xiaoxia Wu, and Leon Bottou.
\newblock Adagrad stepsizes: Sharp convergence over nonconvex landscapes.
\newblock In {\em International Conference on Machine Learning}, pages
  6677--6686. PMLR, 2019.

\bibitem[Wyn56]{wynn1956device}
Peter Wynn.
\newblock On a device for computing the e m (s n) transformation.
\newblock {\em Mathematical Tables and Other Aids to Computation}, pages
  91--96, 1956.

\bibitem[YLR{\etalchar{+}}19]{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock {\em arXiv preprint arXiv:1904.00962}, 2019.

\bibitem[You53]{young1953richardson}
David Young.
\newblock On richardson's method for solving linear systems with positive
  definite matrices.
\newblock {\em Journal of Mathematics and Physics}, 32(1-4):243--255, 1953.

\bibitem[ZLN{\etalchar{+}}19]{zhang2019algorithmic}
Guodong Zhang, Lala Li, Zachary Nado, James Martens, Sushant Sachdeva, George~E
  Dahl, Christopher~J Shallue, and Roger Grosse.
\newblock Which algorithmic choices matter at which batch sizes? {I}nsights
  from a noisy quadratic model.
\newblock {\em arXiv preprint arXiv:1907.04164}, 2019.

\end{thebibliography}
