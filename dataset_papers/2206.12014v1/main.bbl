\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbaszadehpeivasti et~al.(2021)Abbaszadehpeivasti, de~Klerk, and
  Zamani]{abbaszadehpeivasti2021rate}
Hadi Abbaszadehpeivasti, Etienne de~Klerk, and Moslem Zamani.
\newblock On the rate of convergence of the difference-of-convex algorithm
  (dca).
\newblock \emph{arXiv preprint arXiv:2109.13566}, 2021.

\bibitem[Allen-Zhu and Hazan(2016)]{allen2016variance}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{International conference on machine learning}, pages
  699--707. PMLR, 2016.

\bibitem[Blei et~al.(2017)Blei, Kucukelbir, and McAuliffe]{blei2017variational}
David~M Blei, Alp Kucukelbir, and Jon~D McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock \emph{Journal of the American statistical Association}, 112\penalty0
  (518):\penalty0 859--877, 2017.

\bibitem[Canon and Cullum(1968)]{canon1968tight}
Michael~D Canon and Clifton~D Cullum.
\newblock A tight upper bound on the rate of convergence of frank-wolfe
  algorithm.
\newblock \emph{SIAM Journal on Control}, 6\penalty0 (4):\penalty0 509--516,
  1968.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
Arthur~P Dempster, Nan~M Laird, and Donald~B Rubin.
\newblock Maximum likelihood from incomplete data via the {EM} algorithm.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 39\penalty0 (1):\penalty0 1--22, 1977.

\bibitem[Drori and Teboulle(2014)]{drori2014performance}
Yoel Drori and Marc Teboulle.
\newblock Performance of first-order methods for smooth convex minimization: a
  novel approach.
\newblock \emph{Mathematical Programming}, 145\penalty0 (1):\penalty0 451--482,
  2014.

\bibitem[Frank and Wolfe(1956)]{fw_original}
M.~Frank and P.~Wolfe.
\newblock An algorithm for quadratic programming.
\newblock \emph{Naval Research Logistics Quarterly}, 3(95, 1956.

\bibitem[Hunter and Lange(2004)]{hunter2004tutorial}
David~R Hunter and Kenneth Lange.
\newblock A tutorial on {MM} algorithms.
\newblock \emph{The American Statistician}, 58\penalty0 (1):\penalty0 30--37,
  2004.

\bibitem[Jaggi(2013)]{jaggi2013revisiting}
Martin Jaggi.
\newblock Revisiting {F}rank-{W}olfe: {P}rojection-free sparse convex
  optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  427--435, 2013.

\bibitem[Kerdreux(2020)]{kerdreux2020accelerating}
Thomas Kerdreux.
\newblock \emph{Accelerating conditional gradient methods}.
\newblock PhD thesis, Universit{\'e} Paris sciences et lettres, 2020.

\bibitem[Khamaru and Wainwright(2018)]{khamaru2018convergence}
Koulik Khamaru and Martin Wainwright.
\newblock Convergence guarantees for a class of non-convex and non-smooth
  optimization problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  2601--2610. PMLR, 2018.

\bibitem[Lacoste{-}Julien(2016)]{lacoste16}
Simon Lacoste{-}Julien.
\newblock {Convergence Rate of Frank-Wolfe for Non-Convex Objectives}.
\newblock \emph{arXiv:1607.00345}, 2016.

\bibitem[Lanckriet and Sriperumbudur(2009)]{lanckriet2009convergence}
Gert Lanckriet and Bharath~K Sriperumbudur.
\newblock On the convergence of the concave-convex procedure.
\newblock \emph{Advances in Neural Information Processing Systems}, 22, 2009.

\bibitem[Le~Thi and Pham~Dinh(2018)]{le2018dc}
Hoai~An Le~Thi and Tao Pham~Dinh.
\newblock Dc programming and {DCA}: thirty years of developments.
\newblock \emph{Mathematical Programming}, 169\penalty0 (1):\penalty0 5--68,
  2018.

\bibitem[Levitin and Polyak(1966)]{levitin1966constrained}
Evgeny~S Levitin and Boris~T Polyak.
\newblock Constrained minimization methods.
\newblock \emph{USSR Computational mathematics and mathematical physics},
  6\penalty0 (5):\penalty0 1--50, 1966.

\bibitem[Lipp and Boyd(2016)]{lipp2016variations}
Thomas Lipp and Stephen Boyd.
\newblock Variations and extension of the convex-concave procedure.
\newblock \emph{Optimization and Engineering}, 17\penalty0 (2):\penalty0
  263--287, 2016.

\bibitem[Martinet(1970)]{martinet1970regularisation}
B~Martinet.
\newblock Regularisation, d'inequations variationelles par approximations
  succesives.
\newblock \emph{Revue Francaise d'Informatique et de Recherche Operationelle},
  1970.

\bibitem[Nesterov(2018)]{nesterov2018complexity}
Yu~Nesterov.
\newblock Complexity bounds for primal-dual methods minimizing the model of
  objective function.
\newblock \emph{Mathematical Programming}, 171\penalty0 (1):\penalty0 311--330,
  2018.

\bibitem[Reddi et~al.(2016{\natexlab{a}})Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016svrg}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{International conference on machine learning}, pages
  314--323. PMLR, 2016{\natexlab{a}}.

\bibitem[Reddi et~al.(2016{\natexlab{b}})Reddi, Sra, P{\'o}czos, and
  Smola]{reddi2016stochastic}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Stochastic {F}rank-{W}olfe methods for nonconvex optimization.
\newblock In \emph{Communication, Control, and Computing (Allerton), 2016 54th
  Annual Allerton Conference on}, pages 1244--1251. IEEE, 2016{\natexlab{b}}.

\bibitem[Rockafellar(1976)]{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock \emph{SIAM journal on control and optimization}, 14\penalty0
  (5):\penalty0 877--898, 1976.

\bibitem[Sinkhorn(1967)]{sinkhorn1967diagonal}
Richard Sinkhorn.
\newblock Diagonal equivalence to matrices with prescribed row and column sums.
\newblock \emph{The American Mathematical Monthly}, 74\penalty0 (4):\penalty0
  402--405, 1967.

\bibitem[Smola et~al.(2005)Smola, Vishwanathan, and Hofmann]{smola2005kernel}
Alex~J Smola, SVN Vishwanathan, and Thomas Hofmann.
\newblock Kernel methods for missing variables.
\newblock In \emph{International Workshop on Artificial Intelligence and
  Statistics}, pages 325--332. PMLR, 2005.

\bibitem[Tao(1997)]{tao1997convex}
Pham~Dinh Tao.
\newblock Convex analysis approach to {DC} programming: theory, algorithms and
  applications.
\newblock \emph{Acta mathematica vietnamica}, 22\penalty0 (1):\penalty0
  289--355, 1997.

\bibitem[Thekumparampil et~al.(2020)Thekumparampil, Jain, Netrapalli, and
  Oh]{thekumparampil2020projection}
Kiran~K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh.
\newblock Projection efficient subgradient method and optimal nonsmooth
  {F}rank-{W}olfe method.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 12211--12224, 2020.

\bibitem[Yuille and Rangarajan(2003)]{yuille2003concave}
Alan~L Yuille and Anand Rangarajan.
\newblock The concave-convex procedure.
\newblock \emph{Neural computation}, 15\penalty0 (4):\penalty0 915--936, 2003.

\bibitem[Yurtsever et~al.(2019)Yurtsever, Sra, and
  Cevher]{yurtsever2019conditional}
Alp Yurtsever, Suvrit Sra, and Volkan Cevher.
\newblock Conditional gradient methods via stochastic path-integrated
  differential estimator.
\newblock In \emph{International Conference on Machine Learning}, pages
  7282--7291. PMLR, 2019.

\end{thebibliography}
