\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baevski \& Auli(2019)Baevski and Auli]{baevski2018adaptive}
Baevski, A. and Auli, M.
\newblock Adaptive input representations for neural language modeling.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByxZX20qFQ}.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Berman et~al.(2019)Berman, J{\'e}gou, Vedaldi, Kokkinos, and
  Douze]{berman2019multigrain}
Berman, M., J{\'e}gou, H., Vedaldi, A., Kokkinos, I., and Douze, M.
\newblock Multigrain: a unified image embedding for classes and instances.
\newblock \emph{arXiv preprint arXiv:1902.05509}, 2019.

\bibitem[Bertasius et~al.(2021)Bertasius, Wang, and Torresani]{timesformer}
Bertasius, G., Wang, H., and Torresani, L.
\newblock Is space-time attention all you need for video understanding?
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, July 2021.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{detr}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
  S.
\newblock End-to-end object detection with transformers.
\newblock In \emph{European Conference on Computer Vision}, pp.\  213--229.
  Springer, 2020.

\bibitem[Chen et~al.(2021)Chen, Panda, and Fan]{chen2021regionvit}
Chen, C.-F., Panda, R., and Fan, Q.
\newblock Regionvit: Regional-to-local attention for vision transformers.
\newblock \emph{arXiv preprint arXiv:2106.02689}, 2021.

\bibitem[Chen et~al.(2020)Chen, Wang, Guo, Xu, Deng, Liu, Ma, Xu, Xu, and
  Gao]{IPT}
Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C., Xu, C.,
  and Gao, W.
\newblock Pre-trained image processing transformer.
\newblock \emph{arXiv preprint arXiv:2012.00364}, 2020.

\bibitem[Chen(2021)]{permuteformer}
Chen, P.
\newblock Permuteformer: Efficient relative position encoding for long
  sequences.
\newblock \emph{arXiv preprint arXiv:2109.02377}, 2021.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{sparse-transformer}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{performer}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
  Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Crow(1984)]{crow1984summed}
Crow, F.~C.
\newblock Summed-area tables for texture mapping.
\newblock In \emph{Proceedings of the 11th annual conference on Computer
  graphics and interactive techniques}, pp.\  207--212, 1984.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{random-augment}
Cubuk, E.~D., Zoph, B., Shlens, J., and Le, Q.~V.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pp.\  702--703, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai-etal-2019-transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q., and Salakhutdinov, R.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2978--2988, Florence, Italy, 2019.
  Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/P19-1285}.

\bibitem[d'Ascoli et~al.(2021)d'Ascoli, Touvron, Leavitt, Morcos, Biroli, and
  Sagun]{convit}
d'Ascoli, S., Touvron, H., Leavitt, M., Morcos, A., Biroli, G., and Sagun, L.
\newblock Convit: Improving vision transformers with soft convolutional
  inductive biases.
\newblock \emph{arXiv preprint arXiv:2103.10697}, 2021.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, L.
\newblock Universal transformers.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyzdRiR9Y7}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2021)Dong, Bao, Chen, Zhang, Yu, Yuan, Chen, and
  Guo]{cswin}
Dong, X., Bao, J., Chen, D., Zhang, W., Yu, N., Yuan, L., Chen, D., and Guo, B.
\newblock Cswin transformer: A general vision transformer backbone with
  cross-shaped windows.
\newblock \emph{arXiv preprint arXiv:2107.00652}, 2021.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Fukushima \& Miyake(1982)Fukushima and Miyake]{cnn1}
Fukushima, K. and Miyake, S.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of visual pattern recognition.
\newblock In \emph{Competition and cooperation in neural nets}, pp.\  267--285.
  Springer, 1982.

\bibitem[Gao et~al.(2021)Gao, Zheng, Wang, Dai, and Li]{smca}
Gao, P., Zheng, M., Wang, X., Dai, J., and Li, H.
\newblock Fast convergence of detr with spatially modulated co-attention.
\newblock \emph{arXiv preprint arXiv:2101.07448}, 2021.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{xavier}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Heo et~al.(2021)Heo, Yun, Han, Chun, Choe, and Oh]{pit}
Heo, B., Yun, S., Han, D., Chun, S., Choe, J., and Oh, S.~J.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock \emph{arXiv preprint arXiv:2103.16302}, 2021.

\bibitem[Ho et~al.(2019)Ho, Kalchbrenner, Weissenborn, and Salimans]{axial}
Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.
\newblock Axial attention in multidimensional transformers.
\newblock \emph{arXiv preprint arXiv:1912.12180}, 2019.

\bibitem[Hoffer et~al.(2020)Hoffer, Ben-Nun, Hubara, Giladi, Hoefler, and
  Soudry]{repeat-augment}
Hoffer, E., Ben-Nun, T., Hubara, I., Giladi, N., Hoefler, T., and Soudry, D.
\newblock Augment your batch: Improving generalization through instance
  repetition.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  8129--8138, 2020.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{stochastic-depth}
Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger, K.~Q.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European conference on computer vision}, pp.\  646--661.
  Springer, 2016.

\bibitem[Islam et~al.(2020)Islam, Jia, and Bruce]{ape1}
Islam, M.~A., Jia, S., and Bruce, N. D.~B.
\newblock How much position information do convolutional neural networks
  encode?
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rJeB36NKvB}.

\bibitem[Kasai et~al.(2021)Kasai, Peng, Zhang, Yogatama, Ilharco, Pappas, Mao,
  Chen, and Smith]{kasai2021finetuning}
Kasai, J., Peng, H., Zhang, Y., Yogatama, D., Ilharco, G., Pappas, N., Mao, Y.,
  Chen, W., and Smith, N.~A.
\newblock Finetuning pretrained transformers into rnns.
\newblock \emph{arXiv preprint arXiv:2103.13076}, 2021.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5156--5165. PMLR, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{cifar}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{cnn3}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Advances in neural information processing systems},
  25:\penalty0 1097--1105, 2012.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{cnn2}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.,
  and Jackel, L.~D.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Li et~al.(2021)Li, Zhang, Cao, Timofte, and Van~Gool]{localvit}
Li, Y., Zhang, K., Cao, J., Timofte, R., and Van~Gool, L.
\newblock Localvit: Bringing locality to vision transformers.
\newblock \emph{arXiv preprint arXiv:2104.05707}, 2021.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{coco}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{European conference on computer vision}, pp.\  740--755.
  Springer, 2014.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{swin-transformer}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., and Guo, B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock \emph{arXiv preprint arXiv:2103.14030}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Ning, Cao, Wei, Zhang, Lin, and
  Hu]{liu2021videoswin}
Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., and Hu, H.
\newblock Video swin transformer.
\newblock \emph{arXiv preprint arXiv:2106.13230}, 2021{\natexlab{b}}.

\bibitem[Liutkus et~al.(2021)Liutkus, C\'{í}fka, Wu, Simsekli, Yang, and
  Richard]{spe}
Liutkus, A., C\'{í}fka, O., Wu, S.-L., Simsekli, U., Yang, Y.-H., and
  Richard, G.
\newblock Relative positional encoding for transformers with linear complexity.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7067--7079. PMLR, 2021.

\bibitem[Loshchilov \& Hutter(2016)Loshchilov and Hutter]{cos-lr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2021)Luo, Li, Cai, He, Peng, Zheng, Ke, Wang, and
  Liu]{luo2021stable}
Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke, G., Wang, L., and
  Liu, T.-Y.
\newblock Stable, fast and accurate: Kernelized attention with relative
  positional encoding.
\newblock \emph{arXiv preprint arXiv:2106.12566}, 2021.

\bibitem[Meng et~al.(2021)Meng, Chen, Fan, Zeng, Li, Yuan, Sun, and
  Wang]{cond-detr}
Meng, D., Chen, X., Fan, Z., Zeng, G., Li, H., Yuan, Y., Sun, L., and Wang, J.
\newblock Conditional detr for fast training convergence.
\newblock \emph{arXiv preprint arXiv:2108.06152}, 2021.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{igpt}
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and
  Tran, D.
\newblock Image transformer.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4055--4064. PMLR, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 8026--8037, 2019.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{rfa}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.~A., and Kong, L.
\newblock Random feature attention.
\newblock \emph{arXiv preprint arXiv:2103.02143}, 2021.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Schlag, I., Irie, K., and Schmidhuber, J.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9355--9366. PMLR, 2021.

\bibitem[Shaw et~al.(2018)Shaw, Uszkoreit, and Vaswani]{shaw2018self}
Shaw, P., Uszkoreit, J., and Vaswani, A.
\newblock Self-attention with relative position representations.
\newblock In \emph{NAACL-HLT (2)}, 2018.

\bibitem[Simoncelli \& Olshausen(2001)Simoncelli and
  Olshausen]{cnn-inductive-bias}
Simoncelli, E.~P. and Olshausen, B.~A.
\newblock Natural image statistics and neural representation.
\newblock \emph{Annual review of neuroscience}, 24\penalty0 (1):\penalty0
  1193--1216, 2001.

\bibitem[Strudel et~al.(2021)Strudel, Garcia, Laptev, and Schmid]{segmenter}
Strudel, R., Garcia, R., Laptev, I., and Schmid, C.
\newblock Segmenter: Transformer for semantic segmentation.
\newblock \emph{arXiv preprint arXiv:2105.05633}, 2021.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{rotary}
Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock \emph{arXiv preprint arXiv:2104.09864}, 2021.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{efficient-attn-1}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Bahri, and
  Metzler]{efficient-attn-2}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020{\natexlab{b}}.

\bibitem[Touvron et~al.(2020)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{deit}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock \emph{arXiv preprint arXiv:2012.12877}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Viola et~al.(2001)Viola, Jones, et~al.]{viola2001robust}
Viola, P., Jones, M., et~al.
\newblock Robust real-time object detection.
\newblock \emph{International journal of computer vision}, 4\penalty0
  (34-47):\penalty0 4, 2001.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Li, Khabsa, Fang, and
  Ma]{linformer}
Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Xie, Li, Fan, Song, Liang, Lu,
  Luo, and Shao]{pvt}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock \emph{arXiv preprint arXiv:2102.12122}, 2021{\natexlab{a}}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Xie, Li, Fan, Song, Liang, Lu,
  Luo, and Shao]{pvtv2}
Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P.,
  and Shao, L.
\newblock Pvtv2: Improved baselines with pyramid vision transformer.
\newblock \emph{arXiv preprint arXiv:2106.13797}, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Xu, Wang, Shen, Cheng, Shen, and
  Xia]{instance-segmentation}
Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., and Xia, H.
\newblock End-to-end video instance segmentation with transformers.
\newblock \emph{arXiv preprint arXiv:2011.14503}, 2020{\natexlab{b}}.

\bibitem[Wasserman(2006)]{wasserman2006all}
Wasserman, L.
\newblock \emph{All of nonparametric statistics}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Wightman(2019)]{timm}
Wightman, R.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Wu et~al.(2021)Wu, Xiao, Codella, Liu, Dai, Yuan, and Zhang]{cvt}
Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., and Zhang, L.
\newblock Cvt: Introducing convolutions to vision transformers.
\newblock \emph{arXiv preprint arXiv:2103.15808}, 2021.

\bibitem[Xiao et~al.(2021)Xiao, Singh, Mintun, Darrell, Doll{\'a}r, and
  Girshick]{xiao2021early}
Xiao, T., Singh, M., Mintun, E., Darrell, T., Doll{\'a}r, P., and Girshick, R.
\newblock Early convolutions help transformers see better.
\newblock \emph{arXiv preprint arXiv:2106.14881}, 2021.

\bibitem[Xu et~al.(2021)Xu, Zhang, Zhang, and Tao]{vitae}
Xu, Y., Zhang, Q., Zhang, J., and Tao, D.
\newblock Vitae: Vision transformer advanced by exploring intrinsic inductive
  bias.
\newblock \emph{arXiv preprint arXiv:2106.03348}, 2021.

\bibitem[Yan et~al.(2021)Yan, Li, Li, Wang, Wu, and Zhang]{contnet}
Yan, H., Li, Z., Li, W., Wang, C., Wu, M., and Zhang, C.
\newblock Contnet: Why not use convolution and transformer at the same time?
\newblock \emph{arXiv preprint arXiv:2104.13497}, 2021.

\bibitem[Yang et~al.(2021)Yang, Li, Zhang, Dai, Xiao, Yuan, and Gao]{focal}
Yang, J., Li, C., Zhang, P., Dai, X., Xiao, B., Yuan, L., and Gao, J.
\newblock Focal attention for long-range interactions in vision transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Yuan et~al.(2021)Yuan, Guo, Liu, Zhou, Yu, and Wu]{ceit}
Yuan, K., Guo, S., Liu, Z., Zhou, A., Yu, F., and Wu, W.
\newblock Incorporating convolution designs into visual transformers.
\newblock \emph{arXiv preprint arXiv:2103.11816}, 2021.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{cutmix}
Yun, S., Han, D., Oh, S.~J., Chun, S., Choe, J., and Yoo, Y.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  6023--6032, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and Lopez-Paz]{mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{random-erasing}
Zhong, Z., Zheng, L., Kang, G., Li, S., and Yang, Y.
\newblock Random erasing data augmentation.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  13001--13008, 2020.

\end{thebibliography}
