\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong, Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Andrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong, R., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and Zaremba, W.
\newblock Hindsight experience replay.
\newblock \emph{CoRR}, abs/1707.01495, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.01495}.

\bibitem[Barto \& Mahadevan(2003)Barto and Mahadevan]{Barto03recentadvances}
Barto, A.~G. and Mahadevan, S.
\newblock Recent advances in hierarchical reinforcement learning.
\newblock \emph{Discrete Event Dynamic Systems}, 13:\penalty0 341--379, 2003.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley_terry}
Bradley, R.~A. and Terry, M.~E.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39:\penalty0 324, 1952.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:125209808}.

\bibitem[Cao et~al.(2020)Cao, Wong, and Lin]{cao2020human}
Cao, Z., Wong, K., and Lin, C.-T.
\newblock Human preference scaling with demonstrations for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.12904}, 2020.

\bibitem[Chane-Sane et~al.(2021)Chane-Sane, Schmid, and Laptev]{chane2021goal}
Chane-Sane, E., Schmid, C., and Laptev, I.
\newblock Goal-conditioned reinforcement learning with imagined subgoals.
\newblock In \emph{International Conference on Machine Learning}, pp.\  1430--1440. PMLR, 2021.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Dalal et~al.(2021)Dalal, Pathak, and Salakhutdinov]{dalal2021accelerating}
Dalal, M., Pathak, D., and Salakhutdinov, R.~R.
\newblock Accelerating robotic reinforcement learning via parameterized action primitives.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 21847--21859, 2021.

\bibitem[Daniel et~al.(2015)Daniel, Kroemer, Viering, Metz, and Peters]{daniel2015active}
Daniel, C., Kroemer, O., Viering, M., Metz, J., and Peters, J.
\newblock Active reward learning with a novel acquisition function.
\newblock \emph{Autonomous Robots}, 39:\penalty0 389--405, 2015.

\bibitem[Dayan \& Hinton(1992)Dayan and Hinton]{dayan1992feudal}
Dayan, P. and Hinton, G.~E.
\newblock Feudal reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 5, 1992.

\bibitem[Dietterich(1999)]{DBLP:journals/corr/cs-LG-9905014}
Dietterich, T.~G.
\newblock Hierarchical reinforcement learning with the {MAXQ} value function decomposition.
\newblock \emph{CoRR}, cs.LG/9905014, 1999.
\newblock URL \url{https://arxiv.org/abs/cs/9905014}.

\bibitem[Gu et~al.(2016)Gu, Holly, Lillicrap, and Levine]{DBLP:journals/corr/GuHLL16}
Gu, S., Holly, E., Lillicrap, T.~P., and Levine, S.
\newblock Deep reinforcement learning for robotic manipulation.
\newblock \emph{CoRR}, abs/1610.00633, 2016.
\newblock URL \url{http://arxiv.org/abs/1610.00633}.

\bibitem[Gupta et~al.(2019)Gupta, Kumar, Lynch, Levine, and Hausman]{gupta2019relay}
Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.11956}, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{DBLP:journals/corr/abs-1801-01290}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock \emph{CoRR}, abs/1801.01290, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.01290}.

\bibitem[Harb et~al.(2018)Harb, Bacon, Klissarov, and Precup]{harb2018waiting}
Harb, J., Bacon, P.-L., Klissarov, M., and Precup, D.
\newblock When waiting is not an option: Learning options with a deliberation cost.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~32, 2018.

\bibitem[Ibarz et~al.(2018)Ibarz, Leike, Pohlen, Irving, Legg, and Amodei]{DBLP:journals/corr/abs-1811-06521}
Ibarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D.
\newblock Reward learning from human preferences and demonstrations in atari, 2018.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog, Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, and Levine]{DBLP:journals/corr/abs-1806-10293}
Kalashnikov, D., Irpan, A., Pastor, P., Ibarz, J., Herzog, A., Jang, E., Quillen, D., Holly, E., Kalakrishnan, M., Vanhoucke, V., and Levine, S.
\newblock Qt-opt: Scalable deep reinforcement learning for vision-based robotic manipulation.
\newblock \emph{CoRR}, abs/1806.10293, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.10293}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Klissarov et~al.(2017)Klissarov, Bacon, Harb, and Precup]{klissarov2017learnings}
Klissarov, M., Bacon, P.-L., Harb, J., and Precup, D.
\newblock Learnings options end-to-end for continuous action tasks.
\newblock \emph{arXiv preprint arXiv:1712.00004}, 2017.

\bibitem[Knox \& Stone(2009)Knox and Stone]{knox2009interactively}
Knox, W.~B. and Stone, P.
\newblock Interactively shaping agents via human reinforcement: The tamer framework.
\newblock In \emph{Proceedings of the fifth international conference on Knowledge capture}, pp.\  9--16, 2009.

\bibitem[Kostrikov et~al.(2018)Kostrikov, Agrawal, Dwibedi, Levine, and Tompson]{kostrikov2018discriminator}
Kostrikov, I., Agrawal, K.~K., Dwibedi, D., Levine, S., and Tompson, J.
\newblock Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning.
\newblock \emph{arXiv preprint arXiv:1809.02925}, 2018.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{DBLP:journals/corr/abs-2106-05091}
Lee, K., Smith, L., and Abbeel, P.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training, 2021.

\bibitem[Levine(2018)]{levine2018reinforcement}
Levine, S.
\newblock Reinforcement learning and control as probabilistic inference: Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Levine et~al.(2015)Levine, Finn, Darrell, and Abbeel]{DBLP:journals/corr/LevineFDA15}
Levine, S., Finn, C., Darrell, T., and Abbeel, P.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{CoRR}, abs/1504.00702, 2015.
\newblock URL \url{http://arxiv.org/abs/1504.00702}.

\bibitem[Levy et~al.(2018)Levy, Konidaris, Platt, and Saenko]{levy2018learning}
Levy, A., Konidaris, G., Platt, R., and Saenko, K.
\newblock Learning multi-level hierarchies with hindsight.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou, Wierstra, and Riedmiller]{DBLP:journals/corr/MnihKSGAWR13}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., and Riedmiller, M.~A.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1312.5602, 2013.
\newblock URL \url{http://arxiv.org/abs/1312.5602}.

\bibitem[Nachum et~al.(2018{\natexlab{a}})Nachum, Gu, Lee, and Levine]{nachum2018near}
Nachum, O., Gu, S., Lee, H., and Levine, S.
\newblock Near-optimal representation learning for hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1810.01257}, 2018{\natexlab{a}}.

\bibitem[Nachum et~al.(2018{\natexlab{b}})Nachum, Gu, Lee, and Levine]{nachum2018data}
Nachum, O., Gu, S.~S., Lee, H., and Levine, S.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 31, 2018{\natexlab{b}}.

\bibitem[Nachum et~al.(2019)Nachum, Tang, Lu, Gu, Lee, and Levine]{nachum2019does}
Nachum, O., Tang, H., Lu, X., Gu, S., Lee, H., and Levine, S.
\newblock Why does hierarchy (sometimes) work so well in reinforcement learning?
\newblock \emph{arXiv preprint arXiv:1909.10618}, 2019.

\bibitem[Nair et~al.(2018)Nair, McGrew, Andrychowicz, Zaremba, and Abbeel]{nair2018overcoming}
Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., and Abbeel, P.
\newblock Overcoming exploration in reinforcement learning with demonstrations.
\newblock In \emph{2018 IEEE international conference on robotics and automation (ICRA)}, pp.\  6292--6299. IEEE, 2018.

\bibitem[Nasiriany et~al.(2021)Nasiriany, Liu, and Zhu]{DBLP:journals/corr/abs-2110-03655}
Nasiriany, S., Liu, H., and Zhu, Y.
\newblock Augmenting reinforcement learning with behavior primitives for diverse manipulation tasks.
\newblock \emph{CoRR}, abs/2110.03655, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.03655}.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock Policy invariance under reward transformations: Theory and application to reward shaping.
\newblock In \emph{Icml}, volume~99, pp.\  278--287. Citeseer, 1999.

\bibitem[Parr \& Russell(1998)Parr and Russell]{NIPS1997_5ca3e9b1}
Parr, R. and Russell, S.
\newblock Reinforcement learning with hierarchies of machines.
\newblock In Jordan, M., Kearns, M., and Solla, S. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~10. MIT Press, 1998.

\bibitem[Pilarski et~al.(2011)Pilarski, Dawson, Degris, Fahimi, Carey, and Sutton]{pilarski2011online}
Pilarski, P.~M., Dawson, M.~R., Degris, T., Fahimi, F., Carey, J.~P., and Sutton, R.~S.
\newblock Online human training of a myoelectric prosthesis controller via actor-critic reinforcement learning.
\newblock In \emph{2011 IEEE international conference on rehabilitation robotics}, pp.\  1--7. IEEE, 2011.

\bibitem[Rajeswaran et~al.(2017)Rajeswaran, Kumar, Gupta, Schulman, Todorov, and Levine]{DBLP:journals/corr/abs-1709-10087}
Rajeswaran, A., Kumar, V., Gupta, A., Schulman, J., Todorov, E., and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.
\newblock \emph{CoRR}, abs/1709.10087, 2017.
\newblock URL \url{http://arxiv.org/abs/1709.10087}.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sutton et~al.(1999)Sutton, Precup, and Singh]{sutton1999between}
Sutton, R.~S., Precup, D., and Singh, S.
\newblock Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning.
\newblock \emph{Artificial intelligence}, 112\penalty0 (1-2):\penalty0 181--211, 1999.

\bibitem[Vezhnevets et~al.(2017)Vezhnevets, Osindero, Schaul, Heess, Jaderberg, Silver, and Kavukcuoglu]{vezhnevets2017feudal}
Vezhnevets, A.~S., Osindero, S., Schaul, T., Heess, N., Jaderberg, M., Silver, D., and Kavukcuoglu, K.
\newblock Feudal networks for hierarchical reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3540--3549. PMLR, 2017.

\bibitem[Warnell et~al.(2018)Warnell, Waytowich, Lawhern, and Stone]{warnell2018deep}
Warnell, G., Waytowich, N., Lawhern, V., and Stone, P.
\newblock Deep tamer: Interactive agent shaping in high-dimensional state spaces.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Wilson et~al.(2012{\natexlab{a}})Wilson, Fern, and Tadepalli]{NIPS2012_16c222aa}
Wilson, A., Fern, A., and Tadepalli, P.
\newblock A bayesian approach for policy learning from trajectory preference queries.
\newblock In Pereira, F., Burges, C., Bottou, L., and Weinberger, K. (eds.), \emph{Advances in Neural Information Processing Systems}, volume~25. Curran Associates, Inc., 2012{\natexlab{a}}.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2012/file/16c222aa19898e5058938167c8ab6c57-Paper.pdf}.

\bibitem[Wilson et~al.(2012{\natexlab{b}})Wilson, Fern, and Tadepalli]{wilson2012bayesian}
Wilson, A., Fern, A., and Tadepalli, P.
\newblock A bayesian approach for policy learning from trajectory preference queries.
\newblock \emph{Advances in neural information processing systems}, 25, 2012{\natexlab{b}}.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, Dey, et~al.]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., Dey, A.~K., et~al.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{Aaai}, volume~8, pp.\  1433--1438. Chicago, IL, USA, 2008.

\end{thebibliography}
