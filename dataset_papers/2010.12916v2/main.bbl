\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amodei and Hernandez(2018)]{aicompute}
Dario Amodei and Danny Hernandez.
\newblock {AI} and compute, 2018.
\newblock URL \url{https://openai.com/blog/ai-and-compute}.

\bibitem[Baxter(2000)]{baxter00}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI Gym}.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chen et~al.(2020)Chen, Wang, Liu, Xu, and Darrell]{chen2020new}
Yinbo Chen, Xiaolong Wang, Zhuang Liu, Huijuan Xu, and Trevor Darrell.
\newblock A new meta-baseline for few-shot learning.
\newblock \emph{arXiv preprint arXiv:2003.04390}, 2020.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{rl2}
Yan Duan, John Schulman, Xi~Chen, Peter~L Bartlett, Ilya Sutskever, and Pieter
  Abbeel.
\newblock {RL}$^{2}$: Fast reinforcement learning via slow reinforcement
  learning.
\newblock \emph{arXiv:1611.02779}, 2016.

\bibitem[Fallah et~al.(2019)Fallah, Mokhtari, and Ozdaglar]{mamlconvergence}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1908.10400}, 2019.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and Ozdaglar]{mamlconvergencerl}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Provably convergent policy gradient methods for model-agnostic
  meta-reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.05135}, 2020.

\bibitem[Finn and Levine(2017)]{finnuniversality}
Chelsea Finn and Sergey Levine.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock \emph{arXiv preprint arXiv:1710.11622}, 2017.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{maml}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 1126--1135. JMLR, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and
  Levine]{onlinemetalearning}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock \emph{arXiv preprint arXiv:1902.08438}, 2019.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimilano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.04910}, 2018.

\bibitem[Ghadimi and Lan(2013)]{sgd_convergence}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first- and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{{SIAM} Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020iteration}
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo.
\newblock On the iteration complexity of hypergradient computation.
\newblock \emph{arXiv preprint arXiv:2006.16218}, 2020.

\bibitem[Hospedales et~al.(2020)Hospedales, Antoniou, Micaelli, and
  Storkey]{metalearningsurvey}
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{arXiv preprint arXiv:2004.05439}, 2020.

\bibitem[Marcotte and Savard(2005)]{nphard}
Patrice Marcotte and Gilles Savard.
\newblock Bilevel programming: A combinatorial perspective.
\newblock In \emph{Graph theory and combinatorial optimization}, pages
  191--217. Springer, 2005.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{reptile}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Packer et~al.(2018)Packer, Gao, Kos, Kr{\"a}henb{\"u}hl, Koltun, and
  Song]{benchmarkinggeneralization}
Charles Packer, Katelyn Gao, Jernej Kos, Philipp Kr{\"a}henb{\"u}hl, Vladlen
  Koltun, and Dawn Song.
\newblock Assessing generalization in deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1810.12282}, 2018.

\bibitem[Penrose(1956)]{penrose}
Roger Penrose.
\newblock On best approximate solutions of linear matrix equations.
\newblock \emph{Mathematical Proceedings of the Cambridge Philosophical
  Society}, 52\penalty0 (1):\penalty0 17â€“19, 1956.

\bibitem[Raghu et~al.(2019)Raghu, Raghu, Bengio, and Vinyals]{anil}
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock \emph{arXiv preprint arXiv:1909.09157}, 2019.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{implicitmaml}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  113--124, 2019.

\bibitem[Rothfuss et~al.(2018)Rothfuss, Lee, Clavera, Asfour, and
  Abbeel]{promp}
Jonas Rothfuss, Dennis Lee, Ignasi Clavera, Tamim Asfour, and Pieter Abbeel.
\newblock Promp: Proximal meta-policy search.
\newblock \emph{arXiv preprint arXiv:1810.06784}, 2018.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{trpo}
John Schulman, Sergey Levine, Pieter Abbeel, Michael~I. Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning (ICML)}, pages
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Thrun and Pratt(1998)]{thrun98}
Sebastian Thrun and Lorien Pratt.
\newblock Learning to learn: Introduction and overview.
\newblock In \emph{Learning to learn}, pages 3--17. Springer, 1998.

\bibitem[Tian et~al.(2020)Tian, Wang, Krishnan, Tenenbaum, and
  Isola]{tian2020rethinking}
Yonglong Tian, Yue Wang, Dilip Krishnan, Joshua~B Tenenbaum, and Phillip Isola.
\newblock Rethinking few-shot image classification: a good embedding is all you
  need?
\newblock \emph{arXiv preprint arXiv:2003.11539}, 2020.

\bibitem[Tobin et~al.(2017)Tobin, Fong, Ray, Schneider, Zaremba, and
  Abbeel]{dr}
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
  Pieter Abbeel.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In \emph{2017 IEEE/RSJ international conference on intelligent robots
  and systems (IROS)}, pages 23--30. IEEE, 2017.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock {MuJoCo}: {A} physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS)}, 2012.

\bibitem[Vershynin(2018)]{hdpbook}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge University Press, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Sun, and Li]{wang2020convergence}
Haoxiang Wang, Ruoyu Sun, and Bo~Li.
\newblock Global convergence and induced kernels of gradient-based
  meta-learning with neural nets.
\newblock \emph{arXiv preprint arXiv:2006.14606}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Cai, Yang, and
  Wang]{wang2020optimality}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock On the global optimality of model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:2006.13182}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{c}})Wang, Huang, Darrell, Gonzalez, and
  Yu]{wang2020frustratingly}
Xin Wang, Thomas~E Huang, Trevor Darrell, Joseph~E Gonzalez, and Fisher Yu.
\newblock Frustratingly simple few-shot object detection.
\newblock \emph{arXiv preprint arXiv:2003.06957}, 2020{\natexlab{c}}.

\bibitem[Wang et~al.(2019)Wang, Chao, Weinberger, and van~der
  Maaten]{simpleshot}
Yan Wang, Wei-Lun Chao, Kilian~Q Weinberger, and Laurens van~der Maaten.
\newblock Simpleshot: Revisiting nearest-neighbor classification for few-shot
  learning.
\newblock \emph{arXiv preprint arXiv:1911.04623}, 2019.

\bibitem[Yu et~al.(2019)Yu, Quillen, He, Julian, Hausman, Finn, and
  Levine]{metaworld}
Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea
  Finn, and Sergey Levine.
\newblock Meta-world: A benchmark and evaluation for multi-task and meta
  reinforcement learning.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2019.

\end{thebibliography}
