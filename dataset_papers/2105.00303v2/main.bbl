\begin{thebibliography}{76}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th USENIX Symposium on Operating Systems Design and
  Implementation}, 2016.

\bibitem[Abou-Moustafa \& Szepesv{\'a}ri(2019)Abou-Moustafa and
  Szepesv{\'a}ri]{abou2019exponential}
Abou-Moustafa, K. and Szepesv{\'a}ri, C.
\newblock An exponential efron-stein inequality for lq stable learning rules.
\newblock \emph{arXiv preprint arXiv:1903.05457}, 2019.

\bibitem[Achille et~al.(2017)Achille, Rovere, and Soatto]{achille2017critical}
Achille, A., Rovere, M., and Soatto, S.
\newblock Critical learning periods in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1711.08856}, 2017.

\bibitem[Ali et~al.(2018)Ali, Kolter, and Tibshirani]{ali2018continuous}
Ali, A., Kolter, J.~Z., and Tibshirani, R.~J.
\newblock A continuous-time view of early stopping for least squares.
\newblock \emph{arXiv preprint arXiv:1810.10082}, 2018.

\bibitem[Ali et~al.(2020)Ali, Dobriban, and Tibshirani]{ali2020implicit}
Ali, A., Dobriban, E., and Tibshirani, R.~J.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock \emph{arXiv preprint arXiv:2003.07802}, 2020.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2019learning}
Allen-Zhu, Z., Li, Y., and Liang, Y.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6158--6169, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  242--252. PMLR, 2019{\natexlab{b}}.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Arora, S., Ge, R., Neyshabur, B., and Zhang, Y.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.~S.,
  Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  233--242. JMLR. org, 2017.

\bibitem[Bansal et~al.(2020)Bansal, Kaplun, and Barak]{bansal2020self}
Bansal, Y., Kaplun, G., and Barak, B.
\newblock For self-supervised learning, rationality implies generalization,
  provably.
\newblock \emph{arXiv preprint arXiv:2010.08508}, 2020.

\bibitem[Bardenet et~al.(2015)Bardenet, Maillard,
  et~al.]{bardenet2015concentration}
Bardenet, R., Maillard, O.-A., et~al.
\newblock Concentration inequalities for sampling without replacement.
\newblock \emph{Bernoulli}, 21\penalty0 (3):\penalty0 1361--1385, 2015.

\bibitem[Bartlett \& Mendelson(2002)Bartlett and
  Mendelson]{bartlett2002rademacher}
Bartlett, P.~L. and Mendelson, S.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  6240--6249, 2017.

\bibitem[Blum \& Hardt(2015)Blum and Hardt]{blum2015ladder}
Blum, A. and Hardt, M.
\newblock The ladder: A reliable leaderboard for machine learning competitions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1006--1014. PMLR, 2015.

\bibitem[Blumer et~al.(1989)Blumer, Ehrenfeucht, Haussler, and
  Warmuth]{blumer1989learnability}
Blumer, A., Ehrenfeucht, A., Haussler, D., and Warmuth, M.~K.
\newblock Learnability and the vapnik-chervonenkis dimension.
\newblock \emph{Journal of the ACM (JACM)}, 36\penalty0 (4):\penalty0 929--965,
  1989.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Bousquet, O. and Elisseeff, A.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Chizat \& Bach(2020)Chizat and Bach]{chizat2020implicit}
Chizat, L. and Bach, F.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory}, pp.\  1305--1338. PMLR,
  2020.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2937--2947, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Du, S., Lee, J., Li, H., Wang, L., and Zhai, X.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Dwork et~al.(2015)Dwork, Feldman, Hardt, Pitassi, Reingold, and
  Roth]{dwork2015preserving}
Dwork, C., Feldman, V., Hardt, M., Pitassi, T., Reingold, O., and Roth, A.~L.
\newblock Preserving statistical validity in adaptive data analysis.
\newblock In \emph{Proceedings of the forty-seventh annual ACM symposium on
  Theory of computing}, pp.\  117--126, 2015.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and Roy]{dziugaite2017computing}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Elisseeff et~al.(2003)Elisseeff, Pontil, et~al.]{elisseeff2003leave}
Elisseeff, A., Pontil, M., et~al.
\newblock Leave-one-out error and stability of learning algorithms with
  applications.
\newblock \emph{NATO science series sub series iii computer and systems
  sciences}, 190:\penalty0 111--130, 2003.

\bibitem[Frankle et~al.(2020)Frankle, Schwab, and Morcos]{frankle2020early}
Frankle, J., Schwab, D.~J., and Morcos, A.~S.
\newblock The early phase of neural network training.
\newblock \emph{arXiv preprint arXiv:2002.10365}, 2020.

\bibitem[Friedman \& Popescu(2003)Friedman and Popescu]{friedman2003gradient}
Friedman, J. and Popescu, B.~E.
\newblock Gradient directed regularization for linear regression and
  classification.
\newblock Technical report, Technical Report, Statistics Department, Stanford
  University, 2003.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicitb}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock \emph{arXiv preprint arXiv:1806.00468}, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Woodworth,
  Bhojanapalli, Neyshabur, and Srebro]{gunasekar2018implicita}
Gunasekar, S., Woodworth, B., Bhojanapalli, S., Neyshabur, B., and Srebro, N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{2018 Information Theory and Applications Workshop (ITA)},
  pp.\  1--10. IEEE, 2018{\natexlab{b}}.

\bibitem[Gur-Ari et~al.(2018)Gur-Ari, Roberts, and Dyer]{gur2018gradient}
Gur-Ari, G., Roberts, D.~A., and Dyer, E.
\newblock Gradient descent happens in a tiny subspace.
\newblock \emph{arXiv preprint arXiv:1812.04754}, 2018.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1225--1234. PMLR, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2016.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hoeffding(1994)]{hoeffding1994probability}
Hoeffding, W.
\newblock Probability inequalities for sums of bounded random variables.
\newblock In \emph{The Collected Works of Wassily Hoeffding}, pp.\  409--426.
  Springer, 1994.

\bibitem[Hu et~al.(2019)Hu, Li, and Yu]{hu2019simple}
Hu, W., Li, Z., and Yu, D.
\newblock Simple and effective regularization methods for training on noisily
  labeled data with generalization guarantee.
\newblock \emph{arXiv preprint arXiv:1905.11368}, 2019.

\bibitem[Hu et~al.(2020)Hu, Xiao, Adlam, and Pennington]{hu2020surprising}
Hu, W., Xiao, L., Adlam, B., and Pennington, J.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock \emph{arXiv preprint arXiv:2006.14599}, 2020.

\bibitem[Hui \& Belkin(2020)Hui and Belkin]{hui2020evaluation}
Hui, L. and Belkin, M.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock \emph{arXiv preprint arXiv:2006.07322}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8571--8580, 2018.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2019implicit}
Ji, Z. and Telgarsky, M.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pp.\  1772--1798. PMLR,
  2019.

\bibitem[Kearns \& Ron(1999)Kearns and Ron]{kearns1999algorithmic}
Kearns, M. and Ron, D.
\newblock Algorithmic stability and sanity-check bounds for leave-one-out
  cross-validation.
\newblock \emph{Neural computation}, 11\penalty0 (6):\penalty0 1427--1453,
  1999.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock {Learning Multiple Layers of Features from Tiny Images}.
\newblock Technical report, Citeseer, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998mnist}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock {Gradient-Based Learning Applied to Document Recognition}.
\newblock \emph{Proceedings of the IEEE}, 86, 1998.

\bibitem[Li et~al.(2019)Li, Soltanolkotabi, and Oymak]{li2019gradient}
Li, M., Soltanolkotabi, M., and Oymak, S.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock \emph{arXiv preprint arXiv:1903.11680}, 2019.

\bibitem[Li et~al.(2020)Li, Soltanolkotabi, and Oymak]{li2020gradient}
Li, M., Soltanolkotabi, M., and Oymak, S.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4313--4324. PMLR, 2020.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Li, Y. and Liang, Y.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8157--8166, 2018.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{arXiv preprint arXiv:2007.00151}, 2020.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas2011learning}
Maas, A., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association
  for computational linguistics: Human language technologies}, pp.\  142--150,
  2011.

\bibitem[McDiarmid(1989)]{mcdiarmid1989}
McDiarmid, C.
\newblock \emph{On the method of bounded differences}, pp.\  148â€“188.
\newblock London Mathematical Society Lecture Note Series. Cambridge University
  Press, 1989.

\bibitem[Mukherjee et~al.(2006)Mukherjee, Niyogi, Poggio, and
  Rifkin]{mukherjee2006learning}
Mukherjee, S., Niyogi, P., Poggio, T., and Rifkin, R.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock \emph{Advances in Computational Mathematics}, 25\penalty0
  (1):\penalty0 161--193, 2006.

\bibitem[Murphy(2012)]{murphy2012machine}
Murphy, K.~P.
\newblock \emph{{Machine Learning: A Probabilistic Perspective}}.
\newblock MIT Press, 2012.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Narang, Subramanian, Belkin, Hsu,
  and Sahai]{muthukumar2020classification}
Muthukumar, V., Narang, A., Subramanian, V., Belkin, M., Hsu, D., and Sahai, A.
\newblock Classification vs regression in overparameterized regimes: Does the
  loss function matter?
\newblock \emph{arXiv preprint arXiv:2005.08054}, 2020.

\bibitem[Nagarajan \& Kolter(2019{\natexlab{a}})Nagarajan and
  Kolter]{nagarajan2019deterministic}
Nagarajan, V. and Kolter, J.~Z.
\newblock Deterministic pac-bayesian generalization bounds for deep networks
  via generalizing noise-resilience.
\newblock \emph{arXiv preprint arXiv:1905.13344}, 2019{\natexlab{a}}.

\bibitem[Nagarajan \& Kolter(2019{\natexlab{b}})Nagarajan and
  Kolter]{nagarajan2019uniform}
Nagarajan, V. and Kolter, J.~Z.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11615--11626, 2019{\natexlab{b}}.

\bibitem[Nakkiran et~al.(2019)Nakkiran, Kaplun, Kalimeris, Yang, Edelman,
  Zhang, and Barak]{nakkiran2019sgd}
Nakkiran, P., Kaplun, G., Kalimeris, D., Yang, T., Edelman, B.~L., Zhang, F.,
  and Barak, B.
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock \emph{arXiv preprint arXiv:1905.11604}, 2019.

\bibitem[Negrea et~al.(2020)Negrea, Dziugaite, and Roy]{negrea2020defense}
Negrea, J., Dziugaite, G.~K., and Roy, D.
\newblock In defense of uniform convergence: Generalization via derandomization
  with an application to interpolating predictors.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7263--7272. PMLR, 2020.

\bibitem[Neu \& Rosasco(2018)Neu and Rosasco]{neu2018iterate}
Neu, G. and Rosasco, L.
\newblock Iterate averaging as regularization for stochastic gradient descent.
\newblock In \emph{Conference On Learning Theory}, pp.\  3222--3242. PMLR,
  2018.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015norm}
Neyshabur, B., Tomioka, R., and Srebro, N.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Conference on Learning Theory}, pp.\  1376--1401, 2015.

\bibitem[Neyshabur et~al.(2017{\natexlab{a}})Neyshabur, Bhojanapalli,
  McAllester, and Srebro]{neyshabur2017exploring}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring generalization in deep learning.
\newblock \emph{arXiv preprint arXiv:1706.08947}, 2017{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2017{\natexlab{b}})Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Neyshabur, B., Bhojanapalli, S., and Srebro, N.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018role}
Neyshabur, B., Li, Z., Bhojanapalli, S., LeCun, Y., and Srebro, N.
\newblock The role of over-parametrization in generalization of neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{NEURIPS2019a9015}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Peters et~al.(2018)Peters, Neumann, Iyyer, Gardner, Clark, Lee, and
  Zettlemoyer]{Peters:2018}
Peters, M.~E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., and
  Zettlemoyer, L.
\newblock Deep contextualized word representations.
\newblock In \emph{Proc. of NAACL}, 2018.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Recht, B., Roelofs, R., Schmidt, L., and Shankar, V.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5389--5400. PMLR, 2019.

\bibitem[Rolnick et~al.(2017)Rolnick, Veit, Belongie, and
  Shavit]{rolnick2017deep}
Rolnick, D., Veit, A., Belongie, S., and Shavit, N.
\newblock Deep learning is robust to massive label noise.
\newblock \emph{arXiv preprint arXiv:1705.10694}, 2017.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
Shalev-Shwartz, S., Shamir, O., Srebro, N., and Sridharan, K.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2635--2670, 2010.

\bibitem[Sherman \& Morrison(1950)Sherman and Morrison]{sherman1950adjustment}
Sherman, J. and Morrison, W.~J.
\newblock Adjustment of an inverse matrix corresponding to a change in one
  element of a given matrix.
\newblock \emph{The Annals of Mathematical Statistics}, 21\penalty0
  (1):\penalty0 124--127, 1950.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Suggala, A., Prasad, A., and Ravikumar, P.~K.
\newblock Connecting optimization and regularization paths.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10608--10619, 2018.

\bibitem[Tsybakov et~al.(1997)]{tsybakov1997nonparametric}
Tsybakov, A.~B. et~al.
\newblock On nonparametric estimation of density level sets.
\newblock \emph{The Annals of Statistics}, 25\penalty0 (3):\penalty0 948--969,
  1997.

\bibitem[Vapnik(1999)]{vapnik1999overview}
Vapnik, V.~N.
\newblock An overview of statistical learning theory.
\newblock \emph{IEEE transactions on neural networks}, 10\penalty0
  (5):\penalty0 988--999, 1999.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45.
  Association for Computational Linguistics, 2020.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yao, Y., Rosasco, L., and Caponnetto, A.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhou et~al.(2020)Zhou, Sutherland, and Srebro]{zhou2020uniform}
Zhou, L., Sutherland, D.~J., and Srebro, N.
\newblock On uniform convergence and low-norm interpolation learning.
\newblock \emph{arXiv preprint arXiv:2006.05942}, 2020.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock Non-vacuous generalization bounds at the imagenet scale: a
  pac-bayesian compression approach.
\newblock \emph{arXiv preprint arXiv:1804.05862}, 2018.

\end{thebibliography}
