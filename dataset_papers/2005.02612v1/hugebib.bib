@inproceedings{mhealth1,
  title={mHealthDroid: a novel framework for agile development of mobile health applications},
  author={Banos, Oresti and Garcia, Rafael and Holgado-Terriza, Juan A and Damas, Miguel and Pomares, Hector and Rojas, Ignacio and Saez, Alejandro and Villalonga, Claudia},
  booktitle={International workshop on ambient assisted living},
  pages={91--98},
  year={2014},
  organization={Springer}
}

@article{mhealth2,
  title={Design, implementation and validation of a novel open framework for agile development of mobile health applications},
  author={Banos, Oresti and Villalonga, Claudia and Garcia, Rafael and Saez, Alejandro and Damas, Miguel and Holgado-Terriza, Juan A and Lee, Sungyong and Pomares, Hector and Rojas, Ignacio},
  journal={Biomedical engineering online},
  volume={14},
  number={2},
  pages={S6},
  year={2015},
  publisher={BioMed Central}
}

@article{wisdm,
  title={Smartphone and Smartwatch-Based Biometrics Using Activities of Daily Living},
  author={Weiss, Gary M and Yoneda, Kenichi and Hayajneh, Thaier},
  journal={IEEE Access},
  volume={7},
  pages={133190--133202},
  year={2019},
  publisher={IEEE}
}

@inproceedings{wharf,
  title={A public domain dataset for ADL recognition using wrist-placed accelerometers},
  author={Bruno, Barbara and Mastrogiovanni, Fulvio and Sgorbissa, Antonio},
  booktitle={the 23rd IEEE International Symposium on Robot and Human Interactive Communication},
  pages={738--743},
  year={2014},
  organization={IEEE}
}

@article{HermansBeyer2017Arxiv,
  title       = {{In Defense of the Triplet Loss for Person Re-Identification}},
  author      = {Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},
  journal     = {arXiv preprint arXiv:1703.07737},
  year        = {2017}
}

@article{denser,
	title={DENSER: Deep Evolutionary Network Structured Representation},
	author={Assun{\c{c}}ao, Filipe and Louren{\c{c}}o, Nuno and Machado, Penousal and Ribeiro, Bernardete},
	journal={arXiv preprint arXiv:1801.01563},
	year={2018}
}

@inproceedings{mmd_gan,
author="C. Li and W. Chang and Y. Cheng and Y. Yang and B. Poczos",
title="{MMD-GAN}: Towards Deeper Understanding of Moment Matching Network",
booktitle="Neural Information Processing Systems",
year="2017"
}

@inproceedings{angular_metric_learning,
author="J. Wang and F. Zhou and S. Wen and X. Liu and Y. Lin",
title="Deep Metric Learning with Angular Loss",
booktitle="International Conference on Computer Vision",
year="2017"
}

@inproceedings{deep_ranking,
author="F. Cakir and K. He and X. Xide and B. Kulis and S. Sclaroff",
title="Deep Metric Learning to Rank",
booktitle="Computer Visiona and Pattern Recognition",
year="2019"
}

@inproceedings{gmnn,
author="Y. Li and K. Swersky and R. Zemel",
title="Generative Moment Matching Networks",
booktitle="International Conference on Machine Learning",
year="2015"
}

@inproceedings{Lin10_NIPS,
	Author = {Dahua Lin and Eric Grimson and John Fisher},
	Title = {Construction of Dependent Dirichlet Processes based on Poisson Processes},
	Year = {2010},
	Booktitle = {Neural Information Processing Systems}}
@inproceedings{Kulis12_ICML,
	Author = {Brian Kulis and Michael I. Jordan},
	Title = {Revisiting k-means: New Algorithms via Bayesian Nonparametrics},
	Year = {2012},
	Booktitle = {Proceedings of the 29th International Conference on Machine Learning (ICML)},
	Address = {Edinburgh, Scotland}}

@article{Ferguson73_ANSTATS,
	Author = {Thomas S. Ferguson},
	Title = {A Bayesian Analysis of Some Nonparametric Problems},
	Year = {1973},
	Journal = {The Annals of Statistics},
	Volume = {1},
	Number = {2},
	Pages = {209--230}}

@article{Carvalho10_BA,
	Author = {Carlos M. Carvalho and Hedibert F. Lopes and Nicholas G. Polson and Matt A. Taddy},
	Title = {Particle Learning for General Mixtures},
	Year = {2010},
	Journal = {Bayesian Analysis},
	Volume = {5},
	Number = {4},
	Pages = {709--740}}

@inproceedings{Ishioka00_IDEAL,
	Author = {Tsunenori Ishioka},
	Title = {Extended K-means with an Efficient Estimation of the Number of Clusters},
	Year = {2000},
	Booktitle = {Proceedings of the 2nd International Conference on Intelligent Data Engineering and Automated Learning},
	Pages = {17--22}
}

@inproceedings{Pelleg00_ICML,
	Author = {Dan Pelleg and Andrew Moore},
	Title = {X-Means: Extending k-means with Efficient Estimation of the Number of Clusters},
	Year = {2000},
	Booktitle = {Proceedings of the 17th International Conference on Machine Learning}
}

@article{Tibshirani01_JRSTATS,
	Author = {Robert Tibshirani and Guenther Walther and Trevor Hastie},
	Title = {Estimating the number of clusters in a data set via the gap statistic},
	Journal = {Journal of the Royal Statistical Society B},
	Year = {2001},
	Volume = {63},
	Number = {2},
	Pages = {411--423}
}

@inproceedings{Kalnis05_SSTD,
	Author = {Panos Kalnis and Nikos Mamoulis and Spiridon Bakiras},
	Title = {On Discovering Moving Clusters in Spatio-temporal Data},
	Booktitle = {Proceedings of the 9th International Symposium on Spatial and Temporal Databases},
	Pages = {364--381},
	Year = {2005},
	Publisher = {Springer}}

@inproceedings{Spiliopoulou06_KDD,
	Author = {Myra Spiliopoulou and Irene Ntoutsi and Yannis Theodoridis and Rene Schult},
	Title = {MONIC - Modeling and Monitoring Cluster Transitions},
	Year = {2006},
	Booktitle = {Proceedings of the 12th International Conference on Knowledge Discovering and Data Mining},
	Pages = {706--711}}

@article{Carvalho10_SS,
	Author = {Carlos M. Carvalho and Michael S. Johannes and Hedibert F. Lopes and Nicholas G. Polson},
	Title = {Particle Learning and Smoothing},
	Year = {2010},
	Journal = {Statistical Science},
	Volume = {25},
	Number = {1},
	Pages = {88--106}}

@article{Hue02_TAES,
	Author = {Carine Hue and Jean-Pierre Le Cadre and Patrick P\'{e}rez},
	Title = {Tracking Multiple Objects with Particle Filtering},
	Year = {2002},
	Journal = {IEEE Transactions on Aerospace and Electronic Systems},
	Volume = {38},
	Number = {3},
	Pages = {791--812}}

@inproceedings{Vermaak03_ICCV,
	Author = {Jaco Vermaak and Arnaud Doucet and Partick P\'{e}rez},
	Title = {Maintaining Multi-Modality through Mixture Tracking},
	Year = {2003},
	Booktitle = {Proceedings of the 9th IEEE International Conference on Computer Vision}}

@article{Pitt99_JASA,
	Author = {Michael Pitt and Neil Shephard},
	Title = {Filtering via Simulation: Auxiliary Particle Filters},
	Journal = {Journal of the American Statistical Association},
	Volume = {94},
	Number = {446},
	Year = {1999},
	Pages = {590--599}}

@article{Sethuraman94_SS,
	Author = {Jayaram Sethuraman},
	Title = {A Constructive Definition of Dirichlet Priors},
	Journal = {Statistica Sinica},
	Volume = {4},
	Year = {1994},
	Pages = {639--650}}
@incollection{Teh10_EML,
	Author = {Yee Whye Teh},
	Title = {Dirichlet Processes},
	Booktitle = {Encyclopedia of Machine Learning},
	Publisher = {Springer},
	Address = {New York},
	Year = {2010}}
@inproceedings{MacEachern99_ASA,
	Author = {Steven N.~MacEachern},
	Title = {Dependent Nonparametric Processes},
	Year = {1999},
	Booktitle = {Proceedings of the Bayesian Statistical Science Section},
	Publisher = {American Statistical Association}}
		
@article{Neal00_JCG,
	Author = {Radford M. Neal},
	Title = {Markov Chain Sampling Methods for Dirichlet Process Mixture Models},
	Journal = {Journal of Computational and Graphical Statistics},
	Volume = {9},
	Number = {2},
	Year = {2000},
	Pages = {249--265}}

@article{Blei06_BA,
	Author = {David M.~Blei and Michael I.~Jordan},
	Title = {Variational Inference for Dirichlet Process Mixtures},
	Year = {2006},
	Volume = {1},
	Number = {1},
	Pages = {121--144},
	Journal = {Bayesian Analysis}}

@article{Lloyd82_IEEETIT,
	Author = {Stuart P.~Lloyd},
	Title = {Least Squares Quantization in PCM},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {129--137},
	Volume = {28},
	Number = {2},
	Year = {1982}}

@inproceedings{DoshiVelez09_ICML,
	Author = {Finale Doshi-Velez and Zoubin Ghahramani},
	Title = {Accelerated Sampling for the Indian Buffet Process},
	Booktitle = {Proceedings of the International Conference on Machine Learning},
	Year = {2009}}

@article{Hoffman12_AX,
	Author = {Matt Hoffman and David Blei and Chong Wang and John Paisley},
	Title = {Stochastic Variational Inference},
	Year = {2012},
	Journal = {arXiv ePrint 1206.7051}}

@inproceedings{Wang08_RSS,
	Author = {Zhikun Wang and Marc Deisenroth and Heni Ben Amor and David Vogt and Bernard Sch\"{o}lkopf and Jan Peters},
	Title = {Probabilistic Modeling of Human Movements for Intention Inference},
	Year = {2008},
	Booktitle = {Robotics Science and Systems}}

@inproceedings{Luber04_RSS,
	Author = {Matthias Luber and Kai Arras and Christian Plagemann and Wolfram Burgard},
	Title = {Classifying Dynamic Objects: An Unsupervised Learning Approach},
	Year = {2004},
	Booktitle = {Robotics Science and Systems}}

@inproceedings{Endres05_RSS,
	Author = {Felix Endres and Christian Plagemann and Cyrill Stachniss and Wolfram Burgard},
	Title = {Unsupervised Discovery of Object Classes from Range Data using Latent Dirichlet Allocation},
	Year = {2005},
	Booktitle = {Robotics Science and Systems}}
	
@article{CowlesCarlin96,
     author = {Cowles, Mary Kathryn and Carlin, Bradley P.},
     title = {Markov Chain Monte Carlo Convergence Diagnostics: A Comparative Review},
     journal = {Journal of the American Statistical Association},
     volume = {91},
     number = {434},
     pages = {pp. 883-904},
     year = {1996},
     publisher = {American Statistical Association},
    }

@inproceedings{ MacEachern,
  author = "S. N. MacEachern",
  title = "Dependent Nonparametric Processes",
  booktitle = "Proceedings of the section on Bayesian Statistical Science",
  pages = "",
  publisher = "",
  address = "",
  month = "1999",
  year = ""
}

@inproceedings{wang_ijcai19,
author="X. Wang and S. Wang and P. Chen and Y. Wang and B. Kulis and X. Lin and P. Chin",
title="Protecting Neural Networks with Hierarchical Random Switching: Towards Better Robustness-Accuracy Trade-off for Stochastic Defenses",
booktitle="Proc. 28th International Joint Conference on Artificial Intelligence (IJCAI)",
year="2019"
}
@inproceedings{lovell,
author="D. Lovell and R. P. Adams and V. K. Mansingka",
title="Parallel {M}arkov chain {M}onte {C}arlo for {D}irritate process mixtures",
booktitle="NIPS 2012 Workshop on Big Learning",
year="2012"
}

@article{fast_search,
author="B. Kulis and P. Jain and K. Grauman",
title="Fast Similarity Search for Learned Metrics",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="31",
number="12",
pages="2143--2157",
year="2009"
}

@inproceedings{power-law,
author="X. Zhou and J. Zhang and B. Kulis",
title="Power-law Graph Cuts",
booktitle="Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)",
year="2015"
}

@inproceedings{gal_pitfalls,
author="Y. Gal and Z. Ghahramani",
title="Pitfalls in the use of Parallel inference for the {D}irichlet process",
booktitle="Proc. International Conference on Machine Learning",
year="2014"
}

@inproceedings{jegelka_workshop,
author="B. Kulis and S. Jegelka and T. Darrell and M. I. Jordan",
title="{BNP}-asymptotics for modeling motion trajectories",
booktitle="NIPS 2012 Workshop on {B}ayesian nonparametric models for reliable planning and decision-making under uncertainty",
year="2012"
}

@inproceedings{girshick,
author="R. Girschick and J. Donahue and T. Darrell and J. Malik",
title="Rich Feature Hierarchices for Accurate Object Detection and Semantic Segmentation",
booktitle="IEEE Conference on Computer Vision and Pattern Recognition",
year="2014"
}

@inproceedings{alexnet,
author="A. Krizhevsky and I. Sutskever and G. E. Hinton",
title="Image{N}et Classification with Deep Convolutional Neural Networks",
booktitle="Advances in Neural Information Processing Systems",
year="2012"
}

@inproceedings{decaf,
author="J. Donahue and Y. Jia and O. Vinyals and J. Hoffman and N. Zhang and E. Tzeng and T. Darrell",
title-="{D}e{CAF}: A Deep Convolutional Activation Feature for Generic Visual Recognition",
booktitle="Proc. International Conference on Machine Learning",
year="2014"
}

@inproceedings{streaming_v,
author="T. Broderick and N. Boyd and A. Wibisono and A. Wilson and M. I. Jordan",
title="Streaming Variational {B}ayes",
booktitle="Advances in Neural Information Processing Systems",
year="2013"
}

@inproceedings{onlinehdp,
author="C. Wang and J. Paisley and D. Blei",
title="Online Variational Inference for the Hierarchical {D}irichlet Process",
booktitle="Proc. AISTATS Conference",
year="2011"
}

@inproceedings{onlinelda,
author="M. D. Hoffman and D. M. Blei and F. Bach",
title="Online Learning for Latent {D}irichlet Allocation",
booktitle="Advances in Neural Information Processing Systems",
year="2010"
}

@article{bsds,
author="P. Arbelaez and M. Maire and C. Fowlkes and J. Malik",
title="Contour Detection and Hierarchical Image Segmentation",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="33",
number="5",
pages="898--916",
year="2011"
}

@inproceedings{cdbn,
author="H. Lee and R. Grosse and R. Ranganath and A. Y. Ng",
title="Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations",
booktitle="Proc. International Conference on Machine Learning",
year="2009"
}

@ARTICLE{hjort_beta,
AUTHOR="N. L. Hjort",
TITLE=" Nonparametric {B}ayes Estimators Based on Beta Processes in Models for Life History Data",
JOURNAL="The Annals of Statistics",
VOLUME= "18",
ISSUE = "3",
PAGES= "1259 - 1294",
YEAR="1990"
}

@inproceedings{wang_carin,
author="Y. Wang and L. Carin",
title="L\'{e}vy Measure Decompositions for the Beta and Gamma Processes",
booktitle="Proc. International Conference on Machine Learning",
year="2012"
}

@inproceedings{ailon,
author="N. Ailon and R. Jaiswal and C. Montelioni",
title="Streaming k-means approximation",
booktitle="Advances in Neural Information Processing Systems",
year="2009"
}

@inproceedings{bahmani,
author="B. Bahmani and B. Moseley and A. Vattani and R. Kumar and S. Vassilvitskii",
title="Scalable k-means++",
booktitle="Proc. Very Large Data Bases (VLDB)",
year="2012"
}

@inproceedings{doshi,
author="F. Doshi-Velez and D. Knowles and S. Mohamed and Z. Ghahramani",
title="Large Scale Nonparametric {B}ayesian Inference: Data Parallelisation in the {I}ndian Buffet Process",
booktitle="Advances in Neural Information Processing Systems",
year="2009"
}

@inproceedings{williamson_icml,
author="S. Williamson and A. Dubey and E. P. Xing",
title="Parallel {M}arkov chain {M}onte {C}arlo for nonparametric mixture models",
booktitle="Proc. International Conference on Machine Learning",
year="2013"
}

@inproceedings{pan_nips,
author="X. Pan and J. E. Gonzalez and S. Jegelka and T. Broderick and M. I. Jordan",
title="Optimistic Concurrency Control for Distributed Unsupervised Learning",
booktitle="Advances in Neural Information Processing Systems",
year="2013"
}

@inproceedings{basu1,
author="S. Basu and M. Bilenko and R. J. Mooney",
title="A Probabilistic Framework for Semi-supervised Clustering",
booktitle="Proc. ACM SIGKDD Conference on Knowledge Discovery and Data Mining",
year="2004"
}

@inproceedings{basu2,
author="B. Kulis and S. Basu and I. S. Dhillon and R. J. Mooney",
title="Semi-supervised graph clustering: a kernel approach",
booktitle="Proc. International Conference on Machine Learning",
year="2005"
}

@inproceedings{local_search,
author="I. S. Dhillon and Y. Guan and J. Kogan",
title="Iterative Clustering of High Dimensional Data Augmented by Local Search",
booktitle="Proc. 2nd IEEE International Conference on Data Mining",
year="2002"
}

@article{consistency_kmeans,
author="D. Pollard",
title="Strong Consistency of k-means Clustering",
journal="Annals of Statistics",
volume="9",
number="1",
year="1981",
pages="135--140"
}

@inproceedings{roychowdhury_nips,
author="A. Roychowdhury and K. Jiang and B. Kulis",
title="Small-Variance Asymptotics for Hidden {M}arkov Models",
booktitle="Advances in Neural Information Processing Systems",
year="2013"
}

@inproceedings{campbell_nips,
author="T. Campbell and M. Liu and B. Kulis and J. How and L. Carin",
title="Dynamic Clustering via Asymptotics of the Dependent {D}irichlet Process Mixture",
booktitle="Advances in Neural Information Processing Systems",
year="2013"
}

@inproceedings{chakrabarti,
author="D. Chakrabarti and R. Kumar and A. Tomkins",
title="Evolutionary Clustering",
booktitle="Proc. SIGKDD International Conference on Knowledge Discovery and Data Mining",
year="2006"
}

@article{asur,
author="S. Asur and S. Parthasarathy and D. Ucar",
title="An Event-based Framework for Characterizing  the Evolutionary Behavior of Interaction Graphs",
journal="ACM Transactions on Knowledge Discovery from Data",
volume="3",
number="4",
year="2009"
}

@inproceedings{evolutionary_spectral,
author="Y. Chi and X. Song and D. Zhou and K. Hino and B. L. Tseng",
title="Evolutionary Spectral Clustering by Incorporating Temporal Smoothness",
booktitle="Proc. SIGKDD International Conference on Knowledge Discovery and Data Mining",
year="2007"
}

@inproceedings{sun06,
author="J. Sun and D. Tao and C. Faloutsos",
title="Beyond Streams and Graphs: Dynamic Tensor Analysis",
booktitle="Proc. SIGKDD International Conference on Knowledge Discovery and Data Mining",
year="2006"
}

@inproceedings{sun07,
author="J. Sun and C. Faloutsos and S. Papadimitriou and P. S. Yu",
title="Graph{S}cope: Parameter-free Mining of Large Time-Evolving Graphs",
booktitle="Proc. SIGKDD International Conference on Knowledge Discovery and Data Mining",
year="2007"
}

@article{foldit,
author="S. Cooper and F. Khatib and A. Treuille and K. Barbero and J. Lee and M. Beenen and A. Leaver-Fay and D. Baker and Z. Popovic",
title="Predicting protein structures with a multiplayer online game",
journal="Nature",
volume="466",
pages="756--760",
year="2010"
}

@inproceedings{leskovec,
author="J. Leskovec and J. Kleinberg and C. Faloutsos",
title="Graphs over time: Densification Laws, Shrinking Diameters and Possible Explanations",
booktitle="Proc. ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
year="2005"
}

@inproceedings{vbow,
author="L. Fei-Fei and P. Perona",
title="A {B}ayesian Hierarchical model for learning natural scene categories",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition",
year="2005"
}

@inproceedings{scene_understanding,
author="E. Sudderth and A. Torralba and W. Freeman and A. Willsky",
title="Learning Hierarchical Models of Scenes, Objects, and Parts",
booktitle="Proc. IEEE International Conference on Computer Vision",
year="2005"
}

@inproceedings{scene_understanding2,
author="E. Sudderth and A. Torralba and W. Freeman and A. Willsky",
title="Describing Visual Scenes using Transformed {D}irichlet Processes",
booktitle="Advances in Neural Information Processing Systems",
year="2005"
}

@article{fast_search,
author="B. Kulis and P. Jain and K. Grauman",
title="Fast Similarity Search for Learned Metrics",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="31",
number="12",
pages="2143--2157",
year="2009"
}

@inproceedings{domain_adaptation,
author="K. Saenko and B. Kulis and M. Fritz and T. Darrell",
title="Adapting Visual Category Models to New Domains",
booktitle="Proc. 11th European Conference on Computer Vision",
year="2010"
}

@inproceedings{domain_adaptation2,
author="B. Kulis and K. Saenko and T. Darrell",
title="What you saw is not what you get: Domain adaptation using asymmetric kernel transforms",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition",
year="2011"
}

@inproceedings{domain_adaptation3,
author="J. Hoffman and B. Kulis and K. Saenko and T. Darrell",
title="Discovering latent domains for multisource domain adaptation",
booktitle="Proc. 12th European Conference on Computer Vision",
year="2012"
}

@inproceedings{hsu,
author="D. Hsu and S. M. Kakade",
title="Learning Mixtures of spherical {G}aussians: moment methods and spectral decompositions",
booktitle="Fourth Innovations in Theoretical Computer Science",
year="2013"
}

@inproceedings{hsu2,
author="A. Anandkumar and D. P. Foster and D. Hsu and S. M. Kakade and Y.-K. Liu",
title="A Spectral Algorithm for latent {D}irichlet Allocation",
booktitle="Advances in Neural Information Processing Systems",
year="2012"
}

@article{hsu3,
author="D. Hsu and S. M. Kakade and T. Zhang",
title="A spectral learning algorithm for learning hidden {M}arkov models",
journal="Journal of Computer and System Sciences",
volume="78",
number="5",
pages="1460--1480",
year="2012"
}

@inproceedings{sudderth_py,
author="E. Sudderth and M. I. Jordan",
title="Shared Segmentation of Natural Scenes using Dependent {P}itman-{Y}or Processes",
booktitle="Advances in Neural Information Processing Systems",
year="2008"
}

@article{barabasi,
author="A-L. Barabasi and R. Albert",
title="Emergence of scaling in random networks",
journal="Science",
volume="286",
number="5439",
pages="509--512",
year="1999"
}

@inproceedings{fu2009,
author="W. Fu and L. Song and E. P. Xing",
title="Dynamic Mixed Membership Block Model for Evolving Networks",
booktitle="Proc. 26th International Conference on Machine Learning",
year="2009"
}

@article{aldous,
author="D. J. Aldous",
title="Representations for partially exchangeable arrays of random variables",
journal="Journal of Multivariate Analysis",
volume="11",
number="4",
pages="581--598",
year="1985"
}

@techreport{hoover,
author="D. N. Hoover",
title="Relations on probability spaces and arrays of random variables",
institution="Institute of Advanced Study, {P}rinceton",
year="1979"
}

@incollection{rbm,
author="P. Smolensky",
title="Information Processing in Dynamical Systems: Foundations of Harmony Theory",
editor="D. E. Rumelhart and J. L. McLelland",
booktitle="Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations",
publisher="{MIT} Press",
pages="194--281",
year="1986"
}

@article{contrastive_divergence,
author="G. Hinton",
title="Training products of experts by minimizing contrastive divergence",
journal="Neural Computation",
volume="14",
pages="1771--1800",
year="2002"
}

@inproceedings{cd_learning,
author="M. A. Carreira-Perpinan and G. Hinton",
title="On Contrastive Divergence Learning",
booktitle="Proc. AISTATS Conference",
year="2005"
}

@inproceedings{williamson_parallel,
author="S. Williamson and A. Dubey and E. P. Xing",
title="Parallel {M}arkov chain {M}onte {C}arlo for Nonparametric Mixture Models",
booktitle="Proc. 30th International Conference on Machine Learning",
year="2013"
}

@inproceedings{mondrian,
author="D. M. Roy and Y. W. Teh",
title="The {M}ondrian Process",
booktitle="Advances in Neural Information Processing Systems",
year="2009"
}

@inproceedings{nystrom,
author="C. K. I. Williams and M. Seeger",
title="Using the Nystrom Method to Speed up Kernel Machines",
booktitle="Advances in Neural Information Processing Systems",
year="2000"
}

@book{gaussian_processes,
author="C. E. Rasmussen and C. K. I. Williams",
title="Gaussian Processes for Machine Learning",
publisher="{MIT} Press",
year="2006"
}

@article{forgetron,
author="O. Dekel and S. Shalev-Shwartz and Y. Singer",
title="The {F}orgetron: A Kernel-based Perceptron on a Budget",
journal="{SIAM} Journal of Computing",
volume="37",
number="5",
pages="1342--1372",
year="2007"
}

@inproceedings{rahimi_recht,
author="A. Rahimi and B. Recht",
title="Random Features for Large-scale Kernel Machines",
booktitle="Advances in Neural Information Processing Systems",
year="2007"
}

@incollection{smo,
author="J. Platt",
title="Fast Training of Support Vector Machines using Sequential Minimal Optimization",
booktitle="Advances in Kernel Methods - Support Vector Learning",
editor="B. Schoelkopf and C. Burges and A. Smola",
pages="185--208",
publisher="{MIT} Press",
year="1999"
}

@article{kernelPCA,
author="B. Schoelkopf and A. Smola and K. R. Mueller",
title="Nonlinear component analysis as a kernel eigenvalue problem",
journal="Neural Computation",
volume="10",
number="5",
pages="1299--1319",
year="1998"
}

@article{kernel_metric,
author="P. Jain and B. Kulis and J. Davis and I. Dhillon",
title="Metric and Kernel Learning using a Linear Transformation",
journal="Journal of Machine Learning Research",
volume="13",
pages="519--547",
year="2012"
}

@article{klsh,
author="B. Kulis and K. Grauman",
title="Kernelized Locality-Sensitive Hashing",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="32",
number="6",
pages="1092--1104",
year="2012"
}

@misc{DDPmeans,
author="T. Campbell and M. Liu and B. Kulis and J. How",
title="Dynamic Clustering via Asymptotics of the Dependent {D}irichlet Process",
howpublished="{arXiv} Preprint 1305.6659",
year="2013"
}

@inproceedings{mad_bayes,
author="T. Broderick and B. Kulis and M. I. Jordan",
title="{MAD}-{B}ayes: {MAP}-based Asymptotic Derivations from {B}ayes",
booktitle="Proc. 29th International Conference on Machine Learning",
year="2012"
}

@article{wang_wong,
author="Y. G. Wang and G. Y. Wong",
title="Stochastic Blockmodels for Directed Graphs",
journal="Journal of the American Statistical Association",
volume="82",
pages="8--19",
year="1987"
}

@article{snijders,
author="T. A. B. Snijders and K. Nowicki",
title="Estimations and Prediction for Stochastic Blockmodels for Graphs with Latent Block Structure",
journal="Journal of Classification",
volume="14",
pages="75--100",
year="1997"
}

@book{sra_optimization,
author="S. Sra and S. Nowozin and S. J. Wright",
title="Optimization for Machine Learning",
publisher="{MIT} Press",
year="2011"
}

@article{mm_blockmodel,
author="E. Airoldi and D. Blei and S. Fienberg and E. Xing",
title="Mixed Membership Stochastic Blockmodels",
journal="Journal of Machine Learning Research",
volume="9",
pages="1981--2014",
year="2008"
}

@incollection{bottou,
author="L. Bottou",
title="Online Algorithms and Stochastic Approximations",
editor="D. Saad",
booktitle="Online Learning and Neural Networks",
publisher="Cambridge University Press",
year="1998"
}

@book{online_learning,
author="N. Cesa-Bianchi and G. Lugosi",
title="Prediction, Learning, and Games",
publisher="Cambridge University Press",
year="2006"
}

@book{kernels,
author="B. Schoelkopf and A. Smola",
title="Learning with Kernels",
publisher="MIT Press",
year="2002"
}

@article{blei_variational,
author="D. Blei and M. I. Jordan",
title="Variational Inference for {D}irichlet Process mixtures",
journal="Journal of Bayesian Analysis",
volume="1",
number="1",
pages="121--144",
year="2006"
}

@book{pgm,
author="D. Koller and N. Friedman",
title="Probabilistic Graphical Models: Principles and Techniques",
publisher="MIT Press",
year="2009"
}

@inproceedings{Lin10_NIPS,
	Author = {D. Lin and E. Grimson and J. Fisher},
	Title = {Construction of Dependent {D}irichlet Processes based on {P}oisson Processes},
	Year = {2010},
	Booktitle = {Advances in Neural Information Processing Systems}}
@inproceedings{Kulis12_ICML,
	Author = {B. Kulis and M. I. Jordan},
	Title = {Revisiting k-means: New Algorithms via Bayesian Nonparametrics},
	Year = {2012},
	Booktitle = {Proc. International Conference on Machine Learning},
	Address = {Edinburgh, Scotland}}

@article{Ferguson73_ANSTATS,
	Author = {T. S. Ferguson},
	Title = {A {B}ayesian Analysis of Some Nonparametric Problems},
	Year = {1973},
	Journal = {The Annals of Statistics},
	Volume = {1},
	Number = {2},
	Pages = {209--230}}

@article{Carvalho10_BA,
	Author = {Carlos M. Carvalho and Hedibert F. Lopes and Nicholas G. Polson and Matt A. Taddy},
	Title = {Particle Learning for General Mixtures},
	Year = {2010},
	Journal = {Bayesian Analysis},
	Volume = {5},
	Number = {4},
	Pages = {709--740}}

@inproceedings{Ishioka00_IDEAL,
	Author = {Tsunenori Ishioka},
	Title = {Extended K-means with an Efficient Estimation of the Number of Clusters},
	Year = {2000},
	Booktitle = {Proceedings of the 2nd International Conference on Intelligent Data Engineering and Automated Learning},
	Pages = {17--22}
}

@inproceedings{Pelleg00_ICML,
	Author = {Dan Pelleg and Andrew Moore},
	Title = {X-Means: Extending k-means with Efficient Estimation of the Number of Clusters},
	Year = {2000},
	Booktitle = {Proceedings of the 17th International Conference on Machine Learning}
}

@article{Tibshirani01_JRSTATS,
	Author = {Robert Tibshirani and Guenther Walther and Trevor Hastie},
	Title = {Estimating the number of clusters in a data set via the gap statistic},
	Journal = {Journal of the Royal Statistical Society B},
	Year = {2001},
	Volume = {63},
	Number = {2},
	Pages = {411--423}
}

@inproceedings{Kalnis05_SSTD,
	Author = {Panos Kalnis and Nikos Mamoulis and Spiridon Bakiras},
	Title = {On Discovering Moving Clusters in Spatio-temporal Data},
	Booktitle = {Proceedings of the 9th International Symposium on Spatial and Temporal Databases},
	Pages = {364--381},
	Year = {2005},
	Publisher = {Springer}}

@inproceedings{Spiliopoulou06_KDD,
	Author = {Myra Spiliopoulou and Irene Ntoutsi and Yannis Theodoridis and Rene Schult},
	Title = {MONIC - Modeling and Monitoring Cluster Transitions},
	Year = {2006},
	Booktitle = {Proceedings of the 12th International Conference on Knowledge Discovering and Data Mining},
	Pages = {706--711}}

@article{Carvalho10_SS,
	Author = {Carlos M. Carvalho and Michael S. Johannes and Hedibert F. Lopes and Nicholas G. Polson},
	Title = {Particle Learning and Smoothing},
	Year = {2010},
	Journal = {Statistical Science},
	Volume = {25},
	Number = {1},
	Pages = {88--106}}

@article{Hue02_TAES,
	Author = {Carine Hue and Jean-Pierre Le Cadre and Patrick P\'{e}rez},
	Title = {Tracking Multiple Objects with Particle Filtering},
	Year = {2002},
	Journal = {IEEE Transactions on Aerospace and Electronic Systems},
	Volume = {38},
	Number = {3},
	Pages = {791--812}}

@inproceedings{Vermaak03_ICCV,
	Author = {Jaco Vermaak and Arnaud Doucet and Partick P\'{e}rez},
	Title = {Maintaining Multi-Modality through Mixture Tracking},
	Year = {2003},
	Booktitle = {Proceedings of the 9th IEEE International Conference on Computer Vision}}

@article{Pitt99_JASA,
	Author = {Michael Pitt and Neil Shephard},
	Title = {Filtering via Simulation: Auxiliary Particle Filters},
	Journal = {Journal of the American Statistical Association},
	Volume = {94},
	Number = {446},
	Year = {1999},
	Pages = {590--599}}

@article{Sethuraman94_SS,
	Author = {Jayaram Sethuraman},
	Title = {A Constructive Definition of Dirichlet Priors},
	Journal = {Statistica Sinica},
	Volume = {4},
	Year = {1994},
	Pages = {639--650}}
@incollection{Teh10_EML,
	Author = {Y. W. Teh},
	Title = {Dirichlet Processes},
	Booktitle = {Encyclopedia of Machine Learning},
	Publisher = {Springer},
	Address = {New York},
	Year = {2010}}
@inproceedings{MacEachern99_ASA,
	Author = {S. N.~MacEachern},
	Title = {Dependent Nonparametric Processes},
	Year = {1999},
	Booktitle = {Proceedings of the Bayesian Statistical Science Section},
	Publisher = {American Statistical Association}}
		
@article{Neal00_JCG,
	Author = {Radford M. Neal},
	Title = {Markov Chain Sampling Methods for Dirichlet Process Mixture Models},
	Journal = {Journal of Computational and Graphical Statistics},
	Volume = {9},
	Number = {2},
	Year = {2000},
	Pages = {249--265}}

@article{Blei06_BA,
	Author = {David M.~Blei and Michael I.~Jordan},
	Title = {Variational Inference for Dirichlet Process Mixtures},
	Year = {2006},
	Volume = {1},
	Number = {1},
	Pages = {121--144},
	Journal = {Bayesian Analysis}}

@article{Lloyd82_IEEETIT,
	Author = {Stuart P.~Lloyd},
	Title = {Least Squares Quantization in PCM},
	Journal = {IEEE Transactions on Information Theory},
	Pages = {129--137},
	Volume = {28},
	Number = {2},
	Year = {1982}}

@inproceedings{DoshiVelez09_ICML,
	Author = {Finale Doshi-Velez and Zoubin Ghahramani},
	Title = {Accelerated Sampling for the Indian Buffet Process},
	Booktitle = {Proceedings of the International Conference on Machine Learning},
	Year = {2009}}

@article{Hoffman12_AX,
	Author = {Matt Hoffman and David Blei and Chong Wang and John Paisley},
	Title = {Stochastic Variational Inference},
	Year = {2012},
	Journal = {arXiv ePrint 1206.7051}}

@inproceedings{Wang08_RSS,
	Author = {Zhikun Wang and Marc Deisenroth and Heni Ben Amor and David Vogt and Bernard Sch\"{o}lkopf and Jan Peters},
	Title = {Probabilistic Modeling of Human Movements for Intention Inference},
	Year = {2008},
	Booktitle = {Robotics Science and Systems}}

@inproceedings{Luber04_RSS,
	Author = {Matthias Luber and Kai Arras and Christian Plagemann and Wolfram Burgard},
	Title = {Classifying Dynamic Objects: An Unsupervised Learning Approach},
	Year = {2004},
	Booktitle = {Robotics Science and Systems}}

@inproceedings{Endres05_RSS,
	Author = {Felix Endres and Christian Plagemann and Cyrill Stachniss and Wolfram Burgard},
	Title = {Unsupervised Discovery of Object Classes from Range Data using Latent Dirichlet Allocation},
	Year = {2005},
	Booktitle = {Robotics Science and Systems}}
	
@article{adams:2010:tree,
  title={Tree-structured stick breaking for hierarchical data},
  author={Adams, R.P. and Ghahramani, Z. and Jordan, M.~I.},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={19--27},
  year={2010}
}

@article{aldous:1985:exchangeability,
  title={Exchangeability and related topics},
  author={Aldous, D.},
  journal={{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIIIÃ‘1983},
  pages={1--198},
  year={1985},
  publisher={Springer}
}

@article{akaike:1974:new,
  title={A new look at the statistical model identification},
  author={Akaike, H.},
  journal={IEEE Transactions on Automatic Control},
  volume={19},
  number={6},
  pages={716--723},
  year={1974},
  publisher={Ieee}
}

@article{antoniak:1974:mixtures,
  title={Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems},
  author={Antoniak, C.~E.},
  journal={The Annals of Statistics},
  pages={1152--1174},
  year={1974},
  publisher={JSTOR}
}

@inproceedings{arthur:2007:k,
  title={{k-means++}: The advantages of careful seeding},
  author={Arthur, D. and Vassilvitskii, S.},
  booktitle={Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1027--1035},
  year={2007},
  organization={Society for Industrial and Applied Mathematics}
}

@article{berkhin:2006:survey,
  title={A survey of clustering data mining techniques},
  author={Berkhin, P.},
  journal={Grouping Multidimensional Data},
  pages={25--71},
  year={2006},
  publisher={Springer Berlin Heidelberg}
}

@book{bertoin:1998:levy,
  title={{L}{\'e}vy Processes},
  author={Bertoin, J.},
  volume={121},
  year={1998},
  publisher={Cambridge University Press}
}

@article{bertoin:2000:subordinators_LE,
  title={Subordinators, {L}{\'e}vy processes with no negative jumps, and branching processes},
  author={Bertoin, J.},
  year={2000},
  publisher={Centre for Mathematical Physics and Stochastics, University of Aarhus}
}

@article{bertoin:2004:subordinators_EX,
  title={Subordinators: examples and applications},
  author={Bertoin, J.},
  journal={Lectures on Probability Theory and Statistics},
  pages={1--91},
  year={2004},
  publisher={Springer}
}

@book{bishop:2006:pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, C.M.},
  year={2006},
  publisher={Springer New York}
}

@article{blackwell:1973:ferguson,
  title={{F}erguson distributions via {P}{\'o}lya Urn Schemes},
  author={Blackwell, D. and MacQueen, J.~B.},
  journal={The Annals of Statistics},
  volume={1},
  number={2},
  pages={353--355},
  year={1973},
  publisher={JSTOR}
}

@article{blei:2003:latent,
  title={Latent {D}irichlet allocation},
  author={Blei, D.~M. and Ng, A.~Y. and Jordan, M.~I.},
  journal={The Journal of Machine Learning Research},
  volume={3},
  pages={993--1022},
  year={2003}
}

@article{blei:2006:variational,
  title={Variational inference for Dirichlet process mixtures},
  author={Blei, D.M. and Jordan, M.~I.},
  journal={Bayesian Analysis},
  volume={1},
  number={1},
  pages={121--144},
  year={2006}
}

@inproceedings{blei:2010:distance,
  title={Distance dependent {C}hinese restaurant processes},
  author={Blei, D. and Frazier, P.},
  booktitle={International Conference on Machine Learning},
  year={2010},
}

@article{blei:2010:nested,
  title={The nested {C}hinese restaurant process and {B}ayesian nonparametric inference of topic hierarchies},
  author={Blei, D.~M. and Griffiths, T.~L. and Jordan, M.~I.},
  journal={Journal of the ACM (JACM)},
  volume={57},
  number={2},
  pages={7},
  year={2010},
  publisher={ACM}
}

@book{bochner:1955:harmonic,
  title={Harmonic analysis and the theory of probability},
  author={Bochner, S.},
  year={1955},
  publisher={University of California press}
}

@article{broderick:2011:combinatorial,
  title={Combinatorial clustering and the beta negative binomial process},
  author={Broderick, T. and Mackey, L. and Paisley, J. and Jordan, M.~I.},
  journal={Arxiv preprint arXiv:1111.1802},
  year={2011}
}

@article{broderick:2012:beta,
	title={Beta processes, stick-breaking, and power laws},
	author={T. Broderick and M.~I. Jordan and J. Pitman},
	journal={Bayesian Analysis},
	volume={7},
	pages={439--476},
	year={2012}
}

@article{broderick:2013:feature,
  title={Feature allocations, probability functions, and paintboxes},
  author={Broderick, T. and Pitman, J. and Jordan, M.~I.},
  journal={Bayesian Analysis, to appear},
  note={ArXiv preprint arXiv:1301.6647},
  year={2013}
}

@article{broderick:2013:clusters,
	title={Clusters and features from combinatorial stochastic processes},
	author={T. Broderick and M.~I. Jordan and J. Pitman},
	journal={Statistical Science, to appear},
	note={Arxiv preprint arXiv:1206.5862},
	year={2013}
}

@Article{de_finetti:1931:funzione,
	Author={{De Finetti}, B.},
	Title={Funzione caratteristica di un fenomeno aleatorio},
	Journal={Atti della R. Academia Nazionale dei Lincei, Serie 6.},
	volume={4},
	pages={251--299},
	year={1931},
	note={in {I}talian}
}

@article{dunson:2008:kernel,
  title={Kernel stick-breaking processes},
  author={Dunson, D.B. and Park, J.H.},
  journal={Biometrika},
  volume={95},
  number={2},
  pages={307--323},
  year={2008},
  publisher={Biometrika Trust}
}

@inproceedings{erosheva:2005:bayesian,
  title={Bayesian mixed membership models for soft clustering and classification},
  author={Erosheva, E.~A. and Fienberg, S.~E.},
  booktitle={Classification--The Ubiquitous Challenge},
  pages={11--26},
  publisher={Springer},
  address={New York},
  year={2005}
}

@article{escobar:1994:estimating,
  title={Estimating normal means with a {D}irichlet process prior},
  author={Escobar, M.~D.},
  journal={Journal of the American Statistical Association},
  pages={268--277},
  year={1994},
  publisher={JSTOR}
}

@article{escobar:1995:bayesian,
  title={Bayesian density estimation and inference using mixtures},
  author={Escobar, M.~D. and West, M.},
  journal={Journal of the American Statistical Association},
  pages={577--588},
  year={1995},
  publisher={JSTOR}
}

@incollection{ewens:1990:population,
    author = {Ewens, W.},
    booktitle = {Mathematical and Statistical Developments of Evolutionary Theory},
    editor = {Lessard, S.},
    pages = {177--227},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Population Genetics Theory - The Past and the Future},
    year = {1990}
}

@article{Ferguson73,
  author = {Ferguson, T.~S.},
  journal = {Annals of Statistics},
  number = 2,
  pages = {209--230},
  title = {A {B}ayesian analysis of some nonparametric problems},
  volume = 1,
  year = 1973,
}

@article{FraleyRaftery02,
  author = {Fraley, C. and Raftery, A.~E.},
  journal = {Journal of the American Statistical Association}, 
  pages = {611--631},
  title = {Model-based clustering, discriminant analysis and density estimation}, 
  volume = 97, 
  year = 2002,
}

@article{freedman:1965:bernard,
  title={Bernard {F}riedman's urn},
  author={Freedman, D.~A.},
  journal={The Annals of Mathematical Statistics},
  volume={36},
  number={3},
  pages={956--970},
  year={1965},
  publisher={JSTOR}
}

@article{geman:1984:stochastic,
  title={Stochastic relaxation, {G}ibbs distributions, and the {B}ayesian restoration of images},
  author={Geman, S. and Geman, D.},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}

@article{gnedin:2006:exchangeable,
  title={Exchangeable {G}ibbs partitions and {S}tirling triangles},
  author={Gnedin, A. and Pitman, J.},
  journal={Journal of Mathematical Sciences},
  volume={138},
  number={3},
  pages={5674--5685},
  year={2006},
  publisher={Springer}
}

@article{gordon:1977:algorithm,
  title={An algorithm for {E}uclidean sum of squares classification},
  author={Gordon, A.~D. and Henderson, J.~T.},
  journal={Biometrics},
  pages={355--362},
  year={1977},
  publisher={JSTOR}
}

@incollection{griffiths:2006:infinite,
 title = {Infinite latent feature models and the {I}ndian buffet process},
 author = {T. Griffiths and Z. Ghahramani},
 booktitle = {Advances in Neural Information Processing Systems 18},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 publisher = {MIT Press},
 address = {Cambridge, MA},
 pages = {475--482},
 year = {2006}
}

@article{griffiths:2011:indian,
  title={The {I}ndian buffet process: an introduction and review},
  author={Griffiths, T. and Ghahramani, Z.},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={April},
  pages={1185--1224},
  year={2011}
}

@techreport{hansen:1998:prediction,
	author={Hansen, B. and Pitman, J.},
	title={Prediction rules for exchangeable sequences related to species sampling},
	institution={University of California, Berkeley},
	number={520},
	year={1998},
	month={May}
}

@Article{hewitt:1955:symmetric,
	author = {Hewitt, E. and Savage, L.~J.},
	title = {Symmetric measures on {C}artesian products},
	journal = {Transactions of the {A}merican Mathematical Society},
	year = {1955},
	month = {November},
	volume = {80},
	number = {2},
	pages = {470--501}
}

@article{hjort:1990:nonparametric,
  title={Nonparametric {B}ayes estimators based on beta processes in models for life history data},
  author={Hjort, N.~L.},
  journal={Annals of Statistics},
  volume={18},
  number={3},
  pages={1259--1294},
  year={1990},
  publisher={Institute of Mathematical Statistics}
}

@article{hoppe:1984:polya,
  title={{P}{\`o}lya-like urns and the {E}wens' sampling formula},
  author={Hoppe, F.~M.},
  journal={Journal of Mathematical Biology},
  volume={20},
  number={1},
  pages={91--94},
  year={1984},
  publisher={Springer}
}

@article{ishwaran:2000:markov,
  title={Markov chain {M}onte {C}arlo in approximate {D}irichlet and beta two-parameter process hierarchical models},
  author={Ishwaran, H. and Zarepour, M.},
  journal={Biometrika},
  volume={87},
  number={2},
  pages={371--390},
  year={2000},
  publisher={Biometrika Trust}
}

@article{ishwaran:2001:gibbs,
  title={{G}ibbs sampling methods for stick-breaking priors},
  author={Ishwaran, H. and James, L.~F.},
  journal={Journal of the American Statistical Association},
  volume={96},
  number={453},
  pages={161--173},
  year={2001},
  publisher={ASA}
}

@article{jain:2010:data,
  title={Data clustering: 50 years beyond {K}-means},
  author={Jain, A.~K.},
  journal={Pattern Recognition Letters},
  volume={31},
  number={8},
  pages={651--666},
  year={2010},
  publisher={Elsevier}
}

@article{jordan:1999:introduction,
  title={An introduction to variational methods for graphical models},
  author={Jordan, M.~I. and Ghahramani, Z. and Jaakkola, T.~S. and Saul, L.~K.},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}

@article{Kim99,
     title = {Nonparametric {B}ayesian estimators for counting processes},
     author = {Kim, Y.},
     journal = {Annals of Statistics},
     volume = {27},
     number = {2},
     pages = {562--588},
     year = {1999}
 }

@article{kingman:1967:completely,
  title={Completely random measures.},
  author={Kingman, J.~F.~C.},
  journal={Pacific Journal of Mathematics},
  volume={21},
  number={1},
  pages={59--78},
  year={1967},
  publisher={Pacific Journal of Mathematics}
}

@article{kingman:1978:representation,
  title={The representation of partition structures},
  author={Kingman, J.~F.~C.},
  journal={Journal of the London Mathematical Society},
  volume={2},
  number={2},
  pages={374},
  year={1978},
  publisher={Oxford University Press}
}

@article{kingman:1982:coalescent,
  title={The coalescent},
  author={Kingman, JFC},
  journal={Stochastic processes and their applications},
  volume={13},
  number={3},
  pages={235--248},
  year={1982},
  publisher={Elsevier}
}

@book{kingman:1993:poisson,
  title={Poisson Processes},
  author={Kingman, J.~F.~C.},
  isbn={0198536933},
  year={1993},
  publisher={Oxford University Press}
}

@inproceedings{kulis:2012:revisiting,
  title={Revisiting k-means: {N}ew algorithms via {B}ayesian nonparametrics},
  author={Kulis, B. and Jordan, M.~I.},
  booktitle={Proceedings of the 23rd International Conference on Machine Learning},
  year={2012},
}

@inproceedings{li:2006:pachinko,
  title={Pachinko allocation: DAG-structured mixture models of topic correlations},
  author={Li, W. and McCallum, A.},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={577--584},
  year={2006},
  organization={ACM}
}

@article{maceachern:1994:estimating,
  title={Estimating normal means with a conjugate style {D}irichlet process prior},
  author={MacEachern, S.~N.},
  journal={Communications in Statistics-Simulation and Computation},
  volume={23},
  number={3},
  pages={727--741},
  year={1994},
  publisher={Taylor \& Francis}
}

@inproceedings{maceachern:1999:dependent,
  title={Dependent nonparametric processes},
  author={MacEachern, S.N.},
  booktitle={Proceedings of the Section on Bayesian Statistical Science},
  pages={50--55},
  year={1999}
}

@book{MclachlanBasford88,
  title={Mixture Models: Inference and Applications to Clustering},
  author={McLachlan, G. and Basford, K.},
  publisher={Dekker},
  address={New York},
  year={1988}
}

@phdthesis{mccloskey:1965:model,
	author={McCloskey, J.~W.},
	title={A model for the distribution of individuals by species 
in an environment},
	year={1965},
	school={Michigan State University}
}

@article{mccullagh:2008:gibbs,
  title={{G}ibbs fragmentation trees},
  author={McCullagh, P. and Pitman, J. and Winkel, M.},
  journal={Bernoulli},
  volume={14},
  number={4},
  pages={988--1002},
  year={2008},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@phdthesis{Miller11,
    Author = {Miller, K.~T.},
    Title = {Bayesian Nonparametric Latent Feature Models},
    School = {EECS Department, University of California, Berkeley},
    Year = {2011},
}

@article{mitzenmacher:2004:brief,
  title={A brief history of generative models for power law and lognormal distributions},
  author={Mitzenmacher, M.},
  journal={Internet mathematics},
  volume={1},
  number={2},
  pages={226--251},
  year={2004},
  publisher={AK Peters}
}

@TechReport{Neal93,
  author =	"R.~M. Neal",
  title =	"Probabilistic Inference using {M}arkov Chain {M}onte
		 {C}arlo Methods",
  institution = "University of Toronto",
  year = 	"1993",
}

@article{neal:2000:markov,
  title={Markov chain sampling methods for {D}irichlet process mixture models},
  author={Neal, R.~M.},
  journal={Journal of Computational and Graphical Statistics},
  volume={9},
  number={2},
  pages={249--265},
  year={2000}
}

@article{Neal03,
     title = {Slice sampling},
     author = {Neal, R.~M.},
     journal = {Annals of Statistics},
     volume = {31},
     number = {3},
     pages = {705-741},
     year = {2003}
}

@inproceedings{paisley:2010:stick,
  title={A stick-breaking construction of the beta process},
  author={Paisley, J. and Zaas, A. and Woods, C.~W. and Ginsburg, G.~S. and Carin, L.},
  booktitle={International Conference on Machine Learning},
  address={Haifa, Israel},
  year={2010}
}

@misc{paisley:2011:stick,
  title={The Stick-Breaking Construction of the Beta Process as a {P}oisson Process},
  author={Paisley, J. and Blei, D. and Jordan, M.~I.},
  year={2011},
  howpublished={Pre-print arXiv:1109.0343v1 [math.ST]}
}

@article{papaspiliopoulos:2008:retrospective,
  title={Retrospective {M}arkov chain {M}onte {C}arlo methods for {D}irichlet process hierarchical models},
  author={Papaspiliopoulos, O. and Roberts, G.O.},
  journal={Biometrika},
  volume={95},
  number={1},
  pages={169--186},
  year={2008},
  publisher={Biometrika Trust}
}

@techreport{Papaspiliopoulos08, 
title={A note on posterior sampling from {D}irichlet mixture models}, 
number={8}, 
institution={University of Warwick, Centre for Research in Statistical Methodology}, 
author={Papaspiliopoulos, O.}, 
year={2008}
}

@inproceedings{patil:1977:diversity,
  title={Diversity as a concept and its implications for random communities},
  author={Patil, G.~P. and Taillie, C.},
  year={1977},
  booktitle={Proceedings of the 41st Session of the International Statistical Institute},
  address={New Delhi},
  pages={497--515}
}

@article{pena:1999:empirical,
  title={An empirical comparison of four initialization methods for the {K}-{M}eans algorithm},
  author={Pe\~{n}a, J.~M. and Lozano, J.~A. and Larra\~{n}aga, P.},
  journal={Pattern Recognition Letters},
  volume={20},
  number={10},
  pages={1027--1040},
  year={1999},
  publisher={Elsevier}
}

@article{perman:1992:size,
  title={Size-biased sampling of {P}oisson point processes and excursions},
  author={Perman, M. and Pitman, J. and Yor, M.},
  journal={Probability Theory and Related Fields},
  volume={92},
  number={1},
  pages={21--39},
  year={1992},
  publisher={Springer}
}

@article{pitman:1995:exchangeable,
  title={Exchangeable and partially exchangeable random partitions},
  author={Pitman, J.},
  journal={Probability Theory and Related Fields},
  volume={102},
  number={2},
  pages={145--158},
  year={1995},
  publisher={Springer}
}

@article{pitman:1996:some,
  title={Some developments of the {B}lackwell-{MacQ}ueen urn scheme},
  author={Pitman, J.},
  journal={Lecture Notes-Monograph Series},
  pages={245--267},
  year={1996},
  publisher={JSTOR}
}

@article{pitman:1997:two,
  title={The two-parameter {P}oisson-{D}irichlet distribution derived from a stable subordinator},
  author={Pitman, J. and Yor, M.},
  journal={Annals of Probability},
  pages={855--900},
  volume={25},
  year={1997}
}

@article{pitman:2003:poisson,
  title={{P}oisson-{K}ingman partitions},
  author={Pitman, J.},
  journal={Lecture Notes-Monograph Series},
  volume={40},
  pages={1--34},
  year={2003},
  publisher={JSTOR}
}

@book{pitman:2006:combinatorial,
   AUTHOR = {Pitman, J.},
    TITLE = {Combinatorial stochastic processes},
   SERIES = {Lecture Notes in Mathematics},
   VOLUME = {1875},
 PUBLISHER = {Springer-Verlag},
  ADDRESS = {Berlin},
     YEAR = {2006},
    PAGES = {x+256},
     ISBN = {978-3-540-30990-1; 3-540-30990-X},
  MRCLASS = {60-02 (05C80 60C05 60G09 60J65 60J80)},
 MRNUMBER = {MR2245368},
       URL = {http://bibserver.berkeley.edu/csp/april05/bookcsp.pdf},
       DOI = {10.1007/b11601500},
}

@article{polya:1930:sur,
	title={Sur quelques points de la th\'{e}orie des probabilit\'{e}s},
	author={G. P\'{o}lya},
	year={1930},
	pages={117--161},
	journal={Annales de l'I.H.P.},
	volume={1},
	number={2}
}

@article{PritchardStDo00, 
author={Pritchard, J.~K. and Stephens, M. and Donnelly, P.}, 
title={Inference of population structure using multilocus genotype data}, 
volume={155}, 
number={2}, 
journal={Genetics}, 
year={2000}, 
pages={945--959}
}

@book {rogers:2000:diffusions,
   AUTHOR = {Rogers, L.~C.~G. and Williams, David},
    TITLE = {Diffusions, {M}arkov processes, and martingales. {V}ol. 1},
   SERIES = {Cambridge Mathematical Library},
     NOTE = {Foundations,
             Reprint of the second (1994) edition},
 PUBLISHER = {Cambridge University Press},
  ADDRESS = {Cambridge},
     YEAR = {2000},
    PAGES = {xx+386},
     ISBN = {0-521-77594-9},
  MRCLASS = {60J60 (60G07 60H05 60J25)},
 MRNUMBER = {1796539 (2001g:60188)},
}

@article{sethuraman:1994:constructive,
  title={A constructive definition of {D}irichlet priors},
  author={Sethuraman, J.},
  journal={Statistica Sinica},
  volume={4},
  number={2},
  pages={639--650},
  year={1994}
}

@article{steinley:2006:k,
  title={{K}-means clustering: {A} half-century synthesis},
  author={Steinley, D.},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={59},
  number={1},
  pages={1--34},
  year={2006},
  publisher={Wiley Online Library}
}

@article{sung:1998:example,
  title={Example-based learning for view-based human face detection},
  author={Sung, K. and Poggio, T.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={20},
  number={1},
  pages={39--51},
  year={1998},
  publisher={IEEE}
}

@inproceedings{teh:2007:stick,
  title={Stick-breaking construction for the Indian buffet process},
  author={Teh, Y.W. and G{\"o}r{\"u}r, D. and Ghahramani, Z.},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  volume={11},
  year={2007}
}

@inproceedings{ThibauxJo07,
author = {R. Thibaux and M.~I. Jordan},
title = {Hierarchical beta processes and the {I}ndian buffet process},
booktitle = {Proceedings of the International Conference on Artificial 
	Intelligence and Statistics},
volume=         {11},
year=           {2007}
}

@article{thomaz:2010:new,
	author={C.~E. Thomaz and G.~A. Giraldi},
	title={A new ranking method for principal components analysis and its application to face image analysis},
	journal={Image and Vision Computing},
	volume={28},
	number={6},
	pages={902--913},
	year={2010},
	month={June},
	note={We use files \url{http://fei.edu.br/~cet/frontalimages_spatiallynormalized_partX.zip} with \url{X}=\url{1,2}.}
}

@article{walker:2007:sampling,
  title={Sampling the Dirichlet mixture model with slices},
  author={Walker, S.G.},
  journal={Communications in Statistics---Simulation and Computation{\textregistered}},
  volume={36},
  number={1},
  pages={45--54},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{wolpert:2004:reflecting,
  title={Reflecting uncertainty in inverse problems: A Bayesian solution using L{\'e}vy processes},
  author={Wolpert, R.L. and Ickstadt, K.},
  journal={Inverse Problems},
  volume={20},
  pages={1759},
  year={2004},
  publisher={IOP Publishing}
}

@article{liu:1994:collapsed,
  title={The collapsed {G}ibbs sampler in {B}ayesian computations with
	applications to a gene regulation problem},
  author={Liu, J.},
  journal={Journal of the American Statistical Association},
  pages={958--966},
  volume={89},
  year={1994}
}

@inproceedings{online_lda,
author="M. D. Hoffman and D. M. Blei and F. Bach",
title="Online Learning for {L}atent {D}irichlet {A}llocation",
booktitle="Advances in Neural Information Processing Systems",
year="2010"
}

@inproceedings{online_hdp,
author="C. Wang and J. Paisley and D. M. Blei",
title="Online Variational Inference for the hierarchical {D}irichlet process",
booktitle="Proceedings of the 14th International Conference on Artificial Intelligence and Statistics",
year="2011"
}

@inproceedings{imagenet,
author="J. Deng and W. Dong and R. Socher and L.-J. Li and K. Li and L. Fei-Fei",
title="{I}mage{N}et: A Large-scale Hierarchical Image Database",
booktitle="IEEE Conference on Computer Vision and Patterns Recognition",
year="2009"
}

@article{agarwal_daume,
author="A. Agarwal and H. Daume",
title="A Geometric View of Conjugate Priors",
journal="Machine Learning",
volume="81",
number="1",
pages="99--113",
year="2010"
}

@inproceedings{forster,
author="J. Forster and M. K. Warmuth",
title="Relative expected instantaneous loss bounds",
booktitle="Proceedings of 13th Conference on Computational Learning Theory",
year="2000"
}

@book{barndorff,
author="O. Barndorff-Nielsen",
title="Information and Exponential Families in Statistical Theory",
publisher="Wiley Publishers",
year="1978"
}

@inproceedings{roweis_pca,
author="S. Roweis",
title="{EM} Algorithms for {PCA} and {SPCA}",
booktitle="Advances in Neural Information Processing Systems",
year="1998"
}

@article{tipping_bishop,
author="M. E. Tipping and C. M. Bishop",
title="Probabilistic principal component analysis",
journal="Journal of the Royal Statistical Society, Series B",
volume="21",
number="3",
pages="611--622",
year="1999"
}

@inproceedings{kulis_jordan,
author="B. Kulis and M. I. Jordan",
title="Revisiting k-means: New Algorithms via {B}ayesian nonparametrics",
booktitle="Proceedings of the 29th International Conference on Machine Learning",
year="2012"
}

@article{kurihara,
author="K. Kurihara and M. Welling",
title="Bayesian k-means as a ``{M}aximization-{E}xpectation'' Algorithm",
journal="Neural Computation",
volume="21",
number="4",
pages="1145--1172",
year="2008"
}

@inproceedings{photo_tourism,
author="N. Snavely and S. Seitz and R. Szeliski",
title="Photo Tourism: Exploring Photo Collections in {3D}",
booktitle="Proc. ACM SIGGRAPH",
year="2006"
}

@article{escobar,
author="M. D. Escobar and M. West",
title="Bayesian Density Estimation and Inference Using Mixtures",
journal="Journal of the American Statistical Association",
volume="90",
number="430",
pages="577--588",
year="1995"
}

@inproceedings{vbow,
author="L. Fei-Fei and P. Perona",
title="A {B}ayesian Hierarchical Model for Learning Natural Scene Categories",
booktitle="IEEE Conference on Computer Vision and Patterns Recognition",
year="2005"
}

@article{lda,
author="D. Blei and A. Ng and M. I. Jordan",
title="Latent {D}irichlet Allocation",
journal="Journal of Machine Learning Research",
volume="3",
pages="993--1022",
year="2003"
}

@book{hjort,
author="N. Hjort and C. Holmes and P. Mueller and S. Walker",
title="Bayesian Nonparametrics: Principles and Practice",
publisher="Cambridge University Press",
address="Cambridge, UK",
year="2010"
}

@article{bischof,
author="H. Bischof and A. Leonardis and A. Selb",
title="{MDL} Principle for Robust Vector Quantisation",
journal="Pattern Analysis and Applications",
volume="2",
number="1",
pages="59--72",
year="1999"
}

@inproceedings{hamerly,
title="Learning the k in k-means",
author="G. Hamerly and C. Elkan",
booktitle="Advances in Neural Information Processing Systems",
year="2003"
}

@article{jain_neal,
author="S. Jain and R. M. Neal",
title="A Split-Merge {M}arkov chain {M}onte {C}arlo Procedure for the {D}irichlet Process Mixture Model",
journal="Journal of Computational and Graphical Statistics",
volume="13",
number="1",
pages="158--182",
year="2004"
}

@article{blei_variational,
author="D. Blei and M. Jordan",
title="Variational Inference for {D}irichlet Process mixtures",
journal="Journal of Bayesian Analysis",
volume="1",
number="1",
pages="121--144",
year="2006"
}

@inproceedings{liang_permutation,
author="P. Liang and M. Jordan and B. Taskar",
title="A Permutation-Augmented Sampler for {DP} Mixture Models",
booktitle="Proceedings of the 24th International Conference on Machine Learning",
year="2007"
}

@article{teh,
author="Y. W. Teh and M. I. Jordan and M. J. Beal and D. M. Blei",
title="Hierarchical {D}irichlet Processes",
journal="Journal of the American Statistical Association",
volume="101",
number="476",
pages="1566--1581",
year="2006"
}

@article{sugar_james,
author="C. A. Sugar and G. M. James",
title="Finding the number of clusters in a data set: An information theoretic approach",
journal="Journal of the American Statistical Association",
volume="98",
pages="750--763",
year="2003"
}

@article{lleti,
author="R. Lleti and M. C. Ortiz and L. A. Sarabia and M. S. Sanchez",
title="Selecting Variables for k-means Cluster Analysis by using a genetic algorithm that optimizes the silhouettes",
journal="Analytica Chimica Acta",
volume="515",
pages="87--100",
year="2004"
}

@article{ketchen,
author="D. J. Ketchen and C . L. Shook",
title="The application of cluster analysis in strategic management research: an analysis and critique",
journal="Strategic Management Journal",
volume="17",
number="6",
pages="441--458",
year="1996"
}

@article{fraley,
author="C. Fraley and A. E. Raftery",
title="How many clusters? {W}hich clustering method? {A}nswers via model-based cluster analysis",
journal="Computer Journal",
volume="41",
number="8",
pages="578--588",
year="1998"
}

@article{tibshirani,
author="R. Tibshirani and G. Walther and T. Hastie",
title="Estimating the number of clusters in a data set via the gap statistic",
journal="Journal of the Royal Statistical Society Series B",
volume="63",
pages="411--423",
year="2001"
}

@book{manning,
author="C. D. Manning and P. Raghavan and H. Sch{\"{u}}tze",
title="Introduction to Information Retrieval",
publisher="Cambridge University Press",
year="2008"
}

@article{neal_dp,
author="R. M. Neal",
title="Markov Chain Sampling Methods for {D}irichlet process mixture models",
journal="Journal of Computational and Graphical Statistics",
volume="9",
pages="249--265",
year="2000"
}

@incollection{west,
author="M. West and P. M{\"{u}}ller and M. D. Escobar",
title="Hierarchical priors and mixture models, with application in regression and density estimation",
booktitle="Aspects of Uncertainty",
publisher="John Wiley",
editor="P. R. Freeman and A. F. M. Smith",
pages="363--386",
year="1994"
}

@inproceedings{localsearch,
author="I. S. Dhillon and Y. Guan and J. Kogan",
title="Iterative Clustering of High dimensional Text data augmented by local search",
booktitle="IEEE International Conference on Data Mining",
pages="131--138",
year="2002"
}

@article{dhillon,
author="I. S. Dhillon and Y. Guan and B. Kulis",
title="Weighted Graph cuts without Eigenvectors: A Multilevel Approach",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="29",
number="11",
pages="1944--1957",
year="2007"
}

@article{ratio_cut,
author="P. Chan and M. Schlag and J. Zien",
title="Spectral k-way ratio cut partitioning",
journal="IEEE Transactions on CAD-Integrated Circuits and Systems",
volume="13",
pages="1088--1096",
year="1994"
}

@article{norm_cut,
author="J. Shi and J. Malik",
title="Normalized cuts and image segmentation",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="22",
number="8",
pages="888--905",
year="2000"
}

@inproceedings{multiclass_ncut,
author="S. X. Yu and J. Shi",
title="Multiclass spectral clustering",
booktitle="Proceedings of the 9th International Conference on Computer Vision",
year="2003"
}

@inproceedings{rasmussen,
author="C. Rasmussen",
title="The Infinite {G}aussian mixture model",
booktitle="Advances in Neural Information Processing Systems",
year="2000"
}

@article{banerjee,
author="A. Banerjee and S. Merugu and I. S. Dhillon and J. Ghosh",
title="Clustering with {B}regman divergences",
journal="Journal of Machine Learning Research",
volume="6",
pages="1705--1749",
year="2005"
}

@article{ishwaran,
author="H. Ishwaran and L. F. James",
title="Gibbs sampling methods for stick-breaking priors",
journal="Journal of the American Statistical Association",
volume="96",
number="453",
pages="161--173",
year="2001"
}

@book{pitman,
author="J. Pitman",
title="Combinatorial Stochastic Processes",
publisher="Springer-Verlag",
note="Lectures from the Saint-Flour Summer School on Probability Theory",
year="2006"
}

@inproceedings{zha_spectral,
author="H. Zha and X. He and C. Ding and H. Simon and M. Gu",
title="Spectral Relaxation for k-means Clustering",
booktitle="Advances in Neural Information Processing Systems",
year="2001"
}

@misc{frank_asuncion,
author="A. Frank and A. Asuncion",
title="U{CI} {M}achine {L}earning {R}epository",
url="http://archive.ics.uci.edu/ml",
institution="University of California, Irvine, School of Information and Computer Sciences",
year="2010"
}

@inproceedings{tong_koller,
author="S. Tong and D. Koller",
title="Restricted {B}ayes Optimal Classifiers",
booktitle="Proc. 17th AAAI Conference",
year="2000"
}


@inproceedings{Erik12,
 author    = {E. Sudderth},
 title     = {Toward reliable {B}ayesian nonparametric learning},
 year      = {2012},
 booktitle     = {NIPS Workshop on Bayesian Noparametric Models for Reliable Planning
 		and Decision-Making Under Uncertainty}
}

@inproceedings{SVEP,
 author    = {K. Jiang and B. Kulis and M.~I. Jordan},
 title     = {Small-variance asymptotics for exponential family {D}irichlet process mixture models},
 year      = {2012},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{BeamSampler,
 author    = {J.~V. Gael and Y. Saatci and Y.~W. Teh and Z. Ghahramani},
 title     = {Beam Sampling for the Infinite Hidden {M}arkov Model},
 year      = {2008},
 booktitle     = {Proceedings of the 25th International Conference
              on Machine Learning}
}

@inproceedings{RevisitKmeans,
 author    = {B. Kulis and M.~I. Jordan},
 title     = {Revisiting k-means: New algorithms via {B}ayesian nonparametrics},
 year      = {2012},
 booktitle     = {Proceedings of the 29th International Conference
              on Machine Learning}
}

@inproceedings{SmallVariance,
 author    = {K. Jiang and B. Kulis and M.~I. Jordan},
 title     = {Small-variance asymptotics for exponential family {D}irichlet process mixture models},
 year      = {2012},
 booktitle     = {Advances in neural information processing systems}
}

@inproceedings{iHMM,
 author    = {M.~J. Beal and Z. Ghahramani and C.~E. Rasmussen},
 title     = {The infinite hidden {M}arkov model},
 year      = {2002},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@Article{HDP,
  author = 	 "Y.~W. Teh and M.~I. Jordan and M.~J. Beal and D.~M. Blei",
  title = 	 "Hierarchical {D}irichlet Processes",
  journal =	 "Journal of the American Statistical Association",
  year =	 "2006",
  volume =	 "101",
  number =	 "476",
  pages =	 "1566--1581"
}

@Article{Antoniak74,
  author = 	 "C.~E. Antoniak",
  title = 	 "Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems",
  journal =	 "The Annals of Statistics",
  year =	 "1974",
  volume =	 "2",
  number =	 "6",
  pages =	 "1152--1174"
}

@Article{Rabiner89,
  author = 	 "L.~R. Rabiner",
  title = 	 "A tutorial on hidden {M}arkov models and selected applications in speech recognition",
  journal =	 "Proceedings of the IEEE",
  year =	 "1989",
  volume =	 "77",
  number = "2",
  pages =	 "257--286"
}

@Article{Leroux92,
  author = 	 "B.~G. Leroux",
  title = 	 "Maximum-likelihood estimation for hidden {M}arkov models",
  journal =	 "Stochastic Processes and their Applications",
  year =	 "1992",
  volume =	 "40",
  number = "1",
  pages =	 "127--143"
}

@Article{Ryden95,
  author = 	 "T. Ryd\'en",
  title = 	 "Estimating the order of hidden {M}arkov models",
  journal =	 "Statistics",
  year =	 "1995",
  volume =	 "26",
  pages =	 "345--354"
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2011},
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@book{prml,
    author    = "C. M. Bishop",
    title     = "Pattern Recognition and Machine Learning",
    publisher = "Springer",
    year      = "2006",
}

@Article{stick,
  author = 	 "J. Sethuraman",
  title = 	 "A constructive definition of dirichlet priors",
  journal =	 "Statistica Sinica",
  year =	 "1994",
  volume =	 "4",
  pages =	 "639--650"
}

@Article{scott,
  author = 	 "S. L. Scott",
  title = 	 "Bayesian methods for hidden Markov models: Recursive computing in the 21st century",
  journal =	 "Journal of the American Statistical Association",
  year =	 "2002",
  volume =	 "97",
  pages =	 "337--351"
}

@book{wellog,
    author    = "J. Ruanaidh and W. J. Fitzgerald",
    title     = "Numerical {B}ayesian methods applied to signal processing",
    publisher = "Springer-Verlag New York Inc",
    year      = "1996",
}

@Article{Bregman,
  author = 	 "A. Banerjee and S. Merugu and I.~S. Dhillon and J. Ghosh",
  title = 	 "Clustering with {B}regman divergences",
  journal =	 "Journal of Machine Learning Research",
  year =	 "2005",
  volume =	 "6",
  pages =	 "1705--1749"
}

@Article{PPCA,
  author = 	 "M.~E. Tipping and C.~M. Bishop",
  title = 	 "Probabilistic principal component analysis",
  journal =	 "Journal of Royal Statistical Society, Series B",
  year =	 "1999",
  volume =	 "21",
  number = "3",
  pages =	 "611--622"
}

@inproceedings{Roweis98,
 author    = {S. Roweis},
 title     = {E{M} algorithms for {PCA} and {SPCA}},
 year      = {1998},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{tong_koller,
author="S. Tong and D. Koller",
title="Restricted {B}ayes Optimal Classifiers",
booktitle="Proc. 17th AAAI Conference",
year="2000"
}


@inproceedings{Erik12,
 author    = {E. Sudderth},
 title     = {Toward reliable {B}ayesian nonparametric learning},
 year      = {2012},
 booktitle     = {NIPS Workshop on Bayesian Noparametric Models for Reliable Planning
 		and Decision-Making Under Uncertainty}
}

@inproceedings{SVEP,
 author    = {K. Jiang and B. Kulis and M.~I. Jordan},
 title     = {Small-variance asymptotics for exponential family {D}irichlet process mixture models},
 year      = {2012},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{BeamSampler,
 author    = {J.~V. Gael and Y. Saatci and Y.~W. Teh and Z. Ghahramani},
 title     = {Beam Sampling for the Infinite Hidden {M}arkov Model},
 year      = {2008},
 booktitle     = {Proceedings of the 25th International Conference
              on Machine Learning}
}

@inproceedings{RevisitKmeans,
 author    = {B. Kulis and M.~I. Jordan},
 title     = {Revisiting k-means: New algorithms via {B}ayesian nonparametrics},
 year      = {2012},
 booktitle     = {Proceedings of the 29th International Conference
              on Machine Learning}
}

@inproceedings{svahmm,
 author    = {A. Roychowdhury and K. Jiang and B. Kulis},
 title     = "Small-{V}ariance {A}symptotics for {H}idden {M}arkov {M}odels",
 year      = {2013},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{svaddp,
 author    = {T. Campbell and M. Liu and B. Kulis and J. How and L. Carin},
 title     = "{D}ynamic {C}lustering via {A}symptotics of the {D}ependent {D}irichlet {P}rocess",
 year      = {2013},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{iHMM,
 author    = {M.~J. Beal and Z. Ghahramani and C.~E. Rasmussen},
 title     = {The infinite hidden {M}arkov model},
 year      = {2002},
 booktitle     = {Advances in neural information processing systems}
}

@Article{HDP,
  author = 	 "Y.~W. Teh and M.~I. Jordan and M.~J. Beal and D.~M. Blei",
  title = 	 "Hierarchical {D}irichlet Processes",
  journal =	 "Journal of the American Statistical Association",
  year =	 "2006",
  volume =	 "101",
  number =	 "476",
  pages =	 "1566--1581"
}

@Article{Antoniak74,
  author = 	 "C.~E. Antoniak",
  title = 	 "Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems",
  journal =	 "The Annals of Statistics",
  year =	 "1974",
  volume =	 "2",
  number =	 "6",
  pages =	 "1152--1174"
}

@Article{Rabiner89,
  author = 	 "L.~R. Rabiner",
  title = 	 "A tutorial on hidden {M}arkov models and selected applications in speech recognition",
  journal =	 "Proceedings of the IEEE",
  year =	 "1989",
  volume =	 "77",
  number = "2",
  pages =	 "257--286"
}

@Article{Leroux92,
  author = 	 "B.~G. Leroux",
  title = 	 "Maximum-likelihood estimation for hidden {M}arkov models",
  journal =	 "Stochastic Processes and their Applications",
  year =	 "1992",
  volume =	 "40",
  number = "1",
  pages =	 "127--143"
}

@Article{Ryden95,
  author = 	 "T. Ryd\'en",
  title = 	 "Estimating the order of hidden {M}arkov models",
  journal =	 "Statistics",
  year =	 "1995",
  volume =	 "26",
  pages =	 "345--354"
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2011},
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@book{prml,
    author    = "Christopher M. Bishop",
    title     = "Pattern Recognition and Machine Learning",
    publisher = "Springer",
    year      = "2006",
}

@Article{stick,
  author = 	 "J. Sethuraman",
  title = 	 "A constructive definition of dirichlet priors",
  journal =	 "Statistica Sinica",
  year =	 "1994",
  volume =	 "4",
  pages =	 "639--650"
}

@Article{scott,
  author = 	 "S. L. Scott",
  title = 	 "Bayesian methods for hidden Markov models: Recursive computing in the 21st century",
  journal =	 "Journal of the American Statistical Association",
  year =	 "2002",
  volume =	 "97",
  pages =	 "337--351"
}

@book{wellog,
    author    = "J. Ruanaidh and W. J. Fitzgerald",
    title     = "Numerical {B}ayesian methods applied to signal processing",
    publisher = "Springer-Verlag New York Inc",
    year      = "1996",
}

@Article{Bregman,
  author = 	 "A. Banerjee and S. Merugu and I.~S. Dhillon and J. Ghosh",
  title = 	 "Clustering with {B}regman divergences",
  journal =	 "Journal of Machine Learning Research",
  year =	 "2005",
  volume =	 "6",
  pages =	 "1705--1749"
}

@Article{PPCA,
  author = 	 "M.~E. Tipping and C.~M. Bishop",
  title = 	 "Probabilistic principal component analysis",
  journal =	 "Journal of Royal Statistical Society, Series B",
  year =	 "1999",
  volume =	 "21",
  number = "3",
  pages =	 "611--622"
}

@inproceedings{Roweis98,
 author    = {S. Roweis},
 title     = {E{M} algorithms for {PCA} and {SPCA}},
 year      = {1998},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{MADBayes,
  author = 	 "T. Broderick and B. Kulis and M.~I. Jordan",
  title = 	 "M{AD}-{Bayes}: {MAP}-based asymptotic derivations from {B}ayes",
  year = {2013},
  booktitle = {Proceedings of the 30th International Conference
              on Machine Learning},
}

@inproceedings{GamPois,
 author = {M. Titsias},
 title = "The {I}nfinite {G}amma-{P}oisson {M}odel",
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2008},
}

@InCollection{crmjord,
 author = {M. I. Jordan},
 title = "Hierarchical {M}odels, {N}ested {M}odels and {C}ompletely {R}andom {M}easures",
 booktitle = {Frontiers of Statistical Decision Making and Bayesian Analysis: In Honor of James O. Berger},
 publisher = {New York: Springer},
 year = {2010},
 editor = {M.-H. Chen and D. Dey and P. Mueller and D. Sun and K. Ye},
}

@Article{crmorig,
 author = {J.F.C. Kingman},
 title = "Completely {R}andom {M}easures",
 journal = {Pacific Journal of Mathematics},
 year = {1967},
 OPTkey = {},
 volume = {21},
 number = {1},
 pages = {59-78},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@InCollection{wolp,
 author = {R. Wolpert and K. Ickstadt},
 title = "Simulation of {L}\'{e}vy {R}andom {F}ields",
 booktitle = {Practical Nonparametric and Semiparametric Bayesian Statistics},
 year = {1998},
 publisher = {Springer-Verlag},
 OPTkey = {},
 OPTvolume = {},
 OPTnumber = {},
 OPTpages = {},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@PhdThesis{thithesis,
 author = {R.J. Thibaux},
 title = {Nonparametric Bayesian Models for Machine Learning},
 school = {University of California at Berkeley},
 year = {2008},
 OPTkey = {},
 OPTtype = {},
 OPTaddress = {},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@PhdThesis{millthesis,
 author = {K.T. Miller},
 title = {Bayesian Nonparametric Latent Feature Models},
 school = {University of California at Berkeley},
 year = {2011},
 OPTkey = {},
 OPTtype = {},
 OPTaddress = {},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@book{kingman_pp,
  address = {New York},
  author = {Kingman, J. F. C.},
  publisher = {Oxford University Press},
  series = {Oxford Studies in Probability},
  title = {Poisson Processes},
  volume = {3},
  year = {1993}
}

@INPROCEEDINGS{beta_st_pp,
    author = {J. Paisley and D. M. Blei and M. I. Jordan},
    title = "Stick-{B}reaking {B}eta {P}rocesses and the {P}oisson {P}rocess",
    booktitle = {Artificial Intelligence and Statistics},
    year = {2012},
}

@INPROCEEDINGS{beta_st,
    author = {J. Paisley and A. Zaas and C. W. Woods and G. S. Ginsburg and L. Carin},
    title = "A {S}tick-{B}reaking {C}onstruction of the {B}eta {P}rocess",
    booktitle = {International Conference on Machine Learning},
    year = {2010},
}

@INPROCEEDINGS{beta_st_vi,
    author = {J. Paisley and L. Carin and D. M. Blei},
    title = " Variational {I}nference for {S}tick-{B}reaking {B}eta {P}rocess {P}riors",
    booktitle = {International Conference on Machine Learning},
    year = {2011},
}

@inproceedings{ibp_vi_fdv,
  author    = {F. Doshi-Velez and
               K. Miller and
               J. Van Gael and
               Y. W. Teh},
  title     = "Variational {I}nference for the {I}ndian {B}uffet {P}rocess",
  booktitle = {AISTATS},
  year      = {2009},
  ee        = {http://www.jmlr.org/proceedings/papers/v5/doshi09a.html},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@ARTICLE{ish_james_2001,
    author = {H. Ishwaran and L. F. James},
    title = "Gibbs {S}ampling {M}ethods for {S}tick-{B}reaking {P}riors",
    journal = {Journal of the American Statistical Association},
    year = {2001},
    volume = {96},
    pages = {161--173}
}

@ARTICLE{ish_james_2000,
    author = {H. Ishwaran and L. F. James},
    title = "Approximate {D}irichlet {P}rocess {C}omputing in {F}inite {N}ormal {M}ixtures: {S}moothing and {P}rior {I}nformation",
    journal = {Journal of Computational and Graphical Statistics},
    year = {2000},
    volume = {11},
    pages = {508--532}
}

@ARTICLE{pit_yor_1997,
    author = {J. Pitman and M. Yor},
    title = "The {T}wo-{P}arameter {P}oissonÃ¢Â€Â“{D}irichlet {D}istribution derived from a {S}table {S}ubordinator",
    journal = {Annals of Probability},
    year = {1997},
    volume = {25},
   number= {2},
    pages = {855--900}
}

@inproceedings{ibp_stick,
author=         "Y. W. Teh and D. {G\"or\"ur} and Z. Ghahramani",
title=          "Stick-breaking Construction for the {I}ndian Buffet Process",
booktitle=      "Proceedings of the International Conference on Artificial
                 Intelligence and Statistics",
volume=         "11",
year=           "2007"
}

@inproceedings{thi_jord,
author=         " R. Thibaux and M. I. Jordan",
title=          "Hierarchical beta processes and the {I}ndian buffet process",
booktitle=      "Proceedings of the Tenth International Conference on Artificial
                 Intelligence and Statistics",
year=           "2007"
}

@inproceedings{zhou_1,
  author = {M. Zhou and L. Carin},
  booktitle = {NIPS},
  title = {Augment-and-Conquer Negative Binomial Processes},
  year = 2012,
  }

@inproceedings{zhou_2,
author = {M. Zhou and L. Hannah and D. Dunson and L. Carin},
title = {Beta-Negative Binomial Process and {P}oisson Factor Analysis
},
year = {2012},
booktitle= {AISTATS},
}

@article{tab_bnb,
Author = {T. Broderick and L. Mackey and J. Paisley and M. I. Jordan},
Title = {Combinatorial clustering and the beta negative binomial process},
Year = {2011},
Eprint = {arXiv:1111.1802},
journal={arXiv}
}

@article{nest_hdp,
Author = {John Paisley and Chong Wang and David M. Blei and Michael I. Jordan},
Title = {Nested Hierarchical Dirichlet Processes},
Year = {2012},
Eprint = {arXiv:1210.6738},
journal={arXiv}
}

@misc{orbanz_w,
Author={P. Orbanz and S.Williamson},
Title={Unit-rate {P}oisson representations of completely random measures}
}

@ARTICLE{fergs_dp,
    author = {T.S. Ferguson},
    title = "A {B}ayesian {A}nalysis of {S}ome {N}onparametric {P}roblems",
    journal = {The Annals of Statistics},
    year = {1973},
    volume = {1},
   number= {2},
    pages = {209-230}
}

@misc{pl_luce_gp,
Author = {F. Caron and Y. W. Teh and B. T. Murphy},
Note = {arXiv:1211.5037},
Title = "Bayesian {N}onparametric {P}lackett-{L}uce Models for the {A}nalysis of {C}lustered {R}anked {D}ata",
Year = {2013}}

@misc{graphs_gp,
Author = {F. Caron and E. B. Fox},
Note = {arXiv:1401.1137},
Title = "Bayesian {N}onparametric {M}odels of {S}parse and {E}xchangeable {R}andom {G}raphs",
Year = {2013}}

@ARTICLE{v_dp,
    author = {D. Blei and M. Jordan},
    title = "Variational methods for {D}irichlet process mixtures",
    journal = {Bayesian Analysis},
    volume = {1},
    pages = {121-144}
}

@inproceedings{cv_hdp,
  author = {Y.W. Teh and K. Kurihara and M. Welling},
  booktitle = {NIPS},
  title = {Collapsed variational inference for {HDP}},
  year = 2007,
  }

@inproceedings{ov_hdp,
author = {C. Wang and J. Paisley and D. Blei},
title = {Online variational inference for the hierarchical {D}irichlet process
},
year = {2011},
booktitle= {AISTATS},
}

@inproceedings{cdbn_icml09,
author = {H. Lee and R. Grosse and R. Ranganath and A. Y. Ng},
title = "{C}onvolutional {D}eep {B}elief {N}etworks for {S}calable {U}nsupervised {L}earning of {H}ierarchical {R}epresentations",
year = {2009},
booktitle= {ICML},
}

@ARTICLE{cd_hint,
    author = {G. E. Hinton},
    title = "Training {P}roducts of {E}xperts by {M}inimizing {C}ontrastive {D}ivergence",
    journal = {Neural Computation},
    volume = {14},
    year={2002},
    pages = {1771-1800}
}

@ARTICLE{hint_teh,
    author = {G. E. Hinton and S. Osindero and Y.-W. Teh},
    title = "A fast learning algorithm for deep belief nets",
    journal = {Neural Computation},
    volume = {18},
   year = {2006},
    pages = {1527-1554}
}

@inproceedings{leeaudionips09,
author = {H. Lee and Y. Largman and P. Pham and A. Y. Ng},
title = {Unsupervised feature learning for audio classification using convolutional deep belief networks},
year = {2009},
booktitle= {NIPS},
}

@inproceedings{sohndsift,
author = {K. Sohn and D. Y. Jung and H. Lee and A. O. Hero III},
title = "{E}fficient {L}earning of {S}parse, {D}istributed, {C}onvolutional {F}eature {R}epresentations for {O}bject {R}ecognition",
year = {2011},
booktitle= {ICCV},
}

@inproceedings{alexnet,
author = {A. Krizhevsky and I. Sutskever and G. E. Hinton},
title = "{I}mageNet {C}lassification with {D}eep {C}onvolutional {N}eural {N}etworks",
year = {2012},
booktitle= {NIPS},
}

@inproceedings{relu1,
author = {V. Nair and G. E. Hinton},
title = "{R}ectified {L}inear {U}nits {I}mprove {R}estricted {B}oltzmann {M}achines",
year = {2010},
booktitle= {ICML},
}
@article{ben_arxiv,
author="B. Usman and K. Saenko and B. Kulis",
title="Stable Distribution Alignment using the Dual of the Adversarial Distance",
journal = "arXiv 1707.04046",
year="2017"
}

@article{Onsiderations2017,
archivePrefix = {arXiv},
arxivId = {1611.02200},
author = {Onsiderations, C and Iagnostic, D and Hallenges, C and Ostbiopsy, P and Anagement, C Linical M},
eprint = {1611.02200},
file = {:scratch/download/mendeley/2017/Unsupervised Cross-Domain Image Generation - 2017 - Onsiderations et al.pdf:pdf},
pages = {1--14},
title = {{Unsupervised Cross-Domain Image Generation}},
year = {2017}
}
@article{Cuturi2013,
abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms in the probability simplex. Despite their appealing theoretical properties, excellent performance and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance on the MNIST benchmark problem over competing distances.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.0895v1},
author = {Cuturi, Marco},
eprint = {arXiv:1306.0895v1},
file = {:scratch/download/mendeley/2013/Sinkhorn Distances Lightspeed Computation of Optimal Transport - 2013 - Cuturi.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 26},
pages = {2292--2300},
title = {{Sinkhorn Distances: Lightspeed Computation of Optimal Transport}},
url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf},
year = {2013}
}
@article{Courty2015,
abstract = {Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.},
archivePrefix = {arXiv},
arxivId = {1507.00504},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis and Rakotomamonjy, Alain},
doi = {10.1109/TPAMI.2016.2615921},
eprint = {1507.00504},
file = {:scratch/download/mendeley/2015/Optimal Transport for Domain Adaptation - 2015 - Courty et al.pdf:pdf},
isbn = {9782875870148},
issn = {0162-8828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {X},
pages = {1--14},
title = {{Optimal Transport for Domain Adaptation}},
url = {http://arxiv.org/abs/1507.00504},
volume = {X},
year = {2015}
}
@article{Arjovsky2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.07725v2},
author = {Arjovsky, Martin and Bottou, Leon},
eprint = {arXiv:1605.07725v2},
file = {:scratch/download/mendeley/2017/Towards Principled Methods for Training Generative Adversarial Networks - 2017 - Arjovsky, Bottou.pdf:pdf},
isbn = {1584880309},
pages = {1--16},
title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
year = {2017}
}
@article{Courty2014,
abstract = {We present a new and original method to solve the domain adaptation problem using optimal transport. By searching for the best transportation plan between the probability distribution functions of a source and a target domain, a non-linear and invertible transformation of the learning samples can be estimated. Any standard machine learning method can then be applied on the transformed set, which makes our method very generic. We propose a new optimal transport algorithm that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term. By using the proposed optimal transport with label regularization, we obtain significant increase in performance compared to the original transport solution. The proposed algorithm is computationally efficient and effective, as illustrated by its evaluation on a toy example and a challenging real life vision dataset, against which it achieves competitive results with respect to state-of-the-art methods.},
author = {Courty, Nicolas and Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis},
doi = {10.1007/978-3-662-44848-9_18},
file = {:scratch/download/mendeley/2014/Domain adaptation with regularized optimal transport - 2014 - Courty et al.pdf:pdf},
isbn = {9783662448472},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {274--289},
title = {{Domain adaptation with regularized optimal transport}},
volume = {8724 LNAI},
year = {2014}
}
@article{Mirza,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
author = {Mirza, Mehdi and Osindero, Simon},
file = {:scratch/download/mendeley/Unknown/Conditional Generative Adversarial Nets - Unknown - Mirza, Osindero.pdf:pdf},
title = {{Conditional Generative Adversarial Nets}}
}
@article{Dziugaite2015,
abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD.},
archivePrefix = {arXiv},
arxivId = {1505.03906},
author = {Dziugaite, Gintare Karolina and Roy, Daniel M and Ghahramani, Zoubin},
eprint = {1505.03906},
file = {:scratch/download/mendeley/2015/Training generative neural networks via Maximum Mean Discrepancy optimization - 2015 - Dziugaite, Roy, Ghahramani.pdf:pdf},
isbn = {9780000000002},
journal = {Uai},
title = {{Training generative neural networks via Maximum Mean Discrepancy optimization}},
url = {http://arxiv.org/abs/1505.03906{\%}5Cnhttp://www.arxiv.org/pdf/1505.03906.pdf},
year = {2015}
}
@article{Sun2016,
abstract = {Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1607.01719},
author = {Sun, Baochen and Saenko, Kate},
doi = {10.1007/978-3-319-49409-8_35},
eprint = {1607.01719},
file = {:scratch/download/mendeley/2016/Deep CORAL Correlation alignment for deep domain adaptation - 2016 - Sun, Saenko.pdf:pdf},
isbn = {9783319494081},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {443--450},
pmid = {4520227},
title = {{Deep CORAL: Correlation alignment for deep domain adaptation}},
volume = {9915 LNCS},
year = {2016}
}
@article{Mohri,
author = {Mohri, Mehryar and {Cortes Yishay Mansour}, Corinna and Rostamizadeh, Afshin},
file = {:scratch/download/mendeley/Unknown/Domain Adaptation Theory and Algorithms slides - Unknown - Mohri.pdf:pdf},
title = {{Domain Adaptation Theory and Algorithms [slides]}}
}
@article{,
file = {:scratch/download/mendeley/Unknown/AitF FULL Collaborative Research PEARL PErceptual Adaptive Representation Learning in the Wild - Unknown - Unknown.pdf:pdf},
number = {d},
pages = {1--20},
title = {{AitF : FULL : Collaborative Research : PEARL : PErceptual Adaptive Representation Learning in the Wild}},
volume = {1}
}
@article{MeschederLARSMESCHEDER2016,
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the in-ference model used during training. We in-troduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary dis-criminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Net-works (GANs). We show that in the nonparamet-ric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact poste-rior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {{Mescheder LARSMESCHEDER}, Lars and {Nowozin SEBASTIANNOWOZIN}, Sebastian and {Geiger ANDREASGEIGER}, Andreas},
eprint = {1701.04722},
file = {:scratch/download/mendeley/2016/Adversarial Variational Bayes Unifying Variational Autoencoders and Generative Adversarial Networks - 2016 - Mescheder LARSMESCHEDER, No.pdf:pdf},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
year = {2016}
}
@article{Taylor2009,
abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
archivePrefix = {arXiv},
arxivId = {0803.0476},
author = {Taylor, Matthew E and Stone, Peter},
doi = {10.1007/978-3-642-27645-3},
eprint = {0803.0476},
file = {:scratch/download/mendeley/2009/Transfer Learning for Reinforcement Learning Domains A Survey - 2009 - Taylor, Stone.pdf:pdf},
isbn = {15324435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {1,1998,actions with goal,example,leaning agents take sequential,maximizing a reward,multi task learning,problems,reinforcement learning,rl,signal,sutton barto,transfer learning,transfer learning objectives,which may time delayed},
pages = {1633--1685},
pmid = {260529900010},
title = {{Transfer Learning for Reinforcement Learning Domains : A Survey}},
url = {http://portal.acm.org/citation.cfm?id=1755839},
volume = {10},
year = {2009}
}
@article{Luan2017,
abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
archivePrefix = {arXiv},
arxivId = {1703.07511},
author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
eprint = {1703.07511},
file = {:scratch/download/mendeley/2017/Deep Photo Style Transfer - 2017 - Cvpr, Id.pdf:pdf},
month = {mar},
title = {{Deep Photo Style Transfer}},
url = {http://arxiv.org/abs/1703.07511},
year = {2017}
}
@article{Ruderman2012,
author = {Ruderman, Avraham and Reid, Mark and Garc{\'{i}}a-Garc{\'{i}}a, Dar{\'{i}}o and Petterson, James},
file = {:scratch/download/mendeley/2012/Tighter Variational Representations of f-Divergences via Restriction to Probability Measures - 2012 - Ruderman et al.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
pages = {671--678},
title = {{Tighter Variational Representations of f-Divergences via Restriction to Probability Measures}},
year = {2012}
}
@article{Nowozin2016,
abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
archivePrefix = {arXiv},
arxivId = {1606.00709},
author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
eprint = {1606.00709},
file = {:scratch/download/mendeley/2016/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - 2016 - Nowozin, Cseke, Tomioka.pdf:pdf},
journal = {arXiv},
pages = {17},
title = {{f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}},
url = {http://arxiv.org/abs/1606.00709},
year = {2016}
}
@article{Ganin2015,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:scratch/download/mendeley/2015/Domain-Adversarial Training of Neural Networks - 2015 - Ganin et al.pdf:pdf},
issn = {1475-7516},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
url = {http://arxiv.org/abs/1505.07818},
volume = {17},
year = {2015}
}
@article{Galanti2017,
abstract = {When learning a mapping from an input space to an output space, the assumption that the sample distribution of the training data is the same as that of the test data is often violated. Unsupervised domain shift methods adapt the learned function in order to correct for this shift. Previous work has focused on utilizing unlabeled samples from the target distribution. We consider the complementary problem in which the unlabeled samples are given post mapping, i.e., we are given the outputs of the mapping of unknown samples from the shifted domain. Two other variants are also studied: the two sided version, in which unlabeled samples are give from both the input and the output spaces, and the Domain Transfer problem, which was recently formalized. In all cases, we derive generalization bounds that employ discrepancy terms.},
archivePrefix = {arXiv},
arxivId = {1703.01606},
author = {Galanti, Tomer and Wolf, Lior},
eprint = {1703.01606},
file = {:scratch/download/mendeley/2017/A Theory of Output-Side Unsupervised Domain Adaptation - 2017 - Galanti, Wolf.pdf:pdf},
pages = {1--12},
title = {{A Theory of Output-Side Unsupervised Domain Adaptation}},
url = {http://arxiv.org/abs/1703.01606},
year = {2017}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
doi = {10.1109/CVPR.2016.95},
eprint = {1606.04080},
file = {:scratch/download/mendeley/2016/Matching Networks for One Shot Learning - 2016 - Vinyals et al.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {arXiv},
pages = {1--12},
pmid = {24920543},
title = {{Matching Networks for One Shot Learning}},
url = {http://arxiv.org/abs/1606.04080},
year = {2016}
}
@article{Kim2017,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon and Kim, Jiwon},
eprint = {1703.05192},
file = {:scratch/download/mendeley/2017/Learning to Discover Cross-Domain Relations with Generative Adversarial Networks - 2017 - Kim et al.pdf:pdf},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1703.05192},
year = {2017}
}
@article{Garcia-Garcia2011,
abstract = {We derive a generalized notion of f-divergences, called (f,l)-divergences. We show that this generalization enjoys many of the nice properties of f-divergences, although it is a richer family. It also provides alternative definitions of standard divergences in terms of surrogate risks. As a first practical application of this theory, we derive a new estimator for the Kulback-Leibler divergence that we use for clustering sets of vectors.},
author = {Garcia-Garcia, Dario and Santos-Rodriguez, R and von Luxburg, Ulrike and Santos-Rodr$\backslash$'iiguez, Ra{\'{u}}l},
file = {:scratch/download/mendeley/2011/Risk-based generalizations of f-divergences - 2011 - Garcia-Garcia et al.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Icml},
pages = {417--424},
title = {{Risk-based generalizations of f-divergences}},
url = {http://eprints.pascal-network.org/archive/00008634/},
year = {2011}
}
@article{Ben-David2007,
abstract = {Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. Under what conditions can we adapt a classifier trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set.},
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
file = {:scratch/download/mendeley/2007/Analysis of representations for domain adaptation - 2007 - Ben-David et al.pdf:pdf},
isbn = {0262195682},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
pages = {137--144},
title = {{Analysis of representations for domain adaptation}},
url = {http://books.google.com/books?hl=en{\&}lr={\&}id=Tbn1l9P1220C{\&}oi=fnd{\&}pg=PA137{\&}dq=Analysis+of+Representations+for+Domain+Adaptation{\&}ots=V2r8Igir2Y{\&}sig=atDEzSKwIklN3ijbkeYhNAtw76k{\%}5Cnhttp://books.google.com/books?hl=en{\&}lr={\&}id=Tbn1l9P1220C{\&}oi=fnd{\&}pg=PA137{\&}dq=Analy},
volume = {19},
year = {2007}
}
@article{Rai2010,
abstract = {In this work, we show how active learning in some (target) domain can leverage information from a different but related (source) domain. We present an algorithm that harnesses the source domain data to learn the best possible initializer hypothesis for doing active learning in the target domain, resulting in improved label complexity. We also present a variant of this algorithm which additionally uses the domain divergence information to selectively query the most informative points in the target domain, leading to further reductions in label complexity. Experimental results on a variety of datasets establish the efficacy of the proposed methods.},
author = {Rai, Piyush and Saha, Avishek and Daum, Hal},
file = {:scratch/download/mendeley/2010/Domain Adaptation meets Active Learning - 2010 - Rai, Saha, Daum.pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {27--32},
title = {{Domain Adaptation meets Active Learning}},
year = {2010}
}
@article{Mohria,
author = {Mohri, Mehryar},
file = {:scratch/download/mendeley/Unknown/Domain Adaptation Theory and Algorithms slides - Unknown - Mohri.pdf:pdf},
title = {{Domain Adaptation Theory and Algorithms [slides]}}
}
@article{Liu2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.03253v2},
author = {Liu, Qiang and Science, Computer and Lee, Jason D and Jordan, Michael},
eprint = {arXiv:1602.03253v2},
file = {:scratch/download/mendeley/2015/A Kernelized Stein Discrepancy for Goodness-of-fit Tests - 2015 - Liu et al.pdf:pdf},
number = {1},
title = {{A Kernelized Stein Discrepancy for Goodness-of-fit Tests}},
year = {2015}
}
@article{Ganin2015a,
abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of Ã¢Â€ÂœdeepÃ¢Â€Â features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
archivePrefix = {arXiv},
arxivId = {1409.7495v2},
author = {Ganin, Yaroslav and Lempitsky, Victor},
eprint = {1409.7495v2},
file = {:scratch/download/mendeley/2015/Unsupervised Domain Adaptation by Backpropagation - 2015 - Ganin, Lempitsky.pdf:pdf},
isbn = {9781510810587},
journal = {Proceedings of the International Conference on Machine learning (ICML)},
number = {i},
pages = {1180--1189},
title = {{Unsupervised Domain Adaptation by Backpropagation}},
url = {http://arxiv.org/abs/1409.7495{\%}5Cnhttp://jmlr.org/proceedings/papers/v37/ganin15.html},
year = {2015}
}
@article{Denton2015,
abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40{\%} of the time, compared to 10{\%} for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
archivePrefix = {arXiv},
arxivId = {1506.05751},
author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
eprint = {1506.05751},
file = {:scratch/download/mendeley/2015/Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks - 2015 - Denton et al.pdf:pdf},
isbn = {1505.05770},
issn = {10495258},
journal = {Arxiv},
pages = {1--10},
title = {{Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks}},
url = {http://arxiv.org/abs/1506.05751},
year = {2015}
}
@article{Borgwardt2006,
abstract = {MOTIVATION: Many problems in data integration in bioinformatics can be posed as one common question: Are two sets of observations generated by the same distribution? We propose a kernel-based statistical test for this problem, based on the fact that two distributions are different if and only if there exists at least one function having different expectation on the two distributions. Consequently we use the maximum discrepancy between function means as the basis of a test statistic. The Maximum Mean Discrepancy (MMD) can take advantage of the kernel trick, which allows us to apply it not only to vectors, but strings, sequences, graphs, and other common structured data types arising in molecular biology. RESULTS: We study the practical feasibility of an MMD-based test on three central data integration tasks: Testing cross-platform comparability of microarray data, cancer diagnosis, and data-content based schema matching for two different protein function classification schemas. In all of these experiments, including high-dimensional ones, MMD is very accurate in finding samples that were generated from the same distribution, and outperforms its best competitors. Conclusions: We have defined a novel statistical test of whether two samples are from the same distribution, compatible with both multivariate and structured data, that is fast, easy to implement, and works well, as confirmed by our experiments. AVAILABILITY: http://www.dbs.ifi.lmu.de/{\~{}}borgward/MMD. http://www.ncbi.nlm.nih.gov/pubmed/16873512},
author = {Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J and Kriegel, Hans-Peter and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J},
doi = {10.1093/bioinformatics/btl242},
file = {:scratch/download/mendeley/2006/Maximum Mean Discrepancy - 2006 - Borgwardt et al.pdf:pdf},
issn = {13674811},
journal = {Bioinformatics},
number = {14},
pages = {49--57},
pmid = {16873512},
title = {{Maximum Mean Discrepancy}},
volume = {22},
year = {2006}
}
@article{Mathieu2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03383v1},
author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and Lecun, Yann},
eprint = {arXiv:1611.03383v1},
file = {:scratch/download/mendeley/2016/Disentangling factors of variation in deep representations using adversarial training - 2016 - Mathieu et al.pdf:pdf},
number = {Nips},
title = {{Disentangling factors of variation in deep representations using adversarial training}},
year = {2016}
}
@article{Li2015,
abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02761v1},
author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
eprint = {arXiv:1502.02761v1},
file = {:scratch/download/mendeley/2015/Generative Moment Matching Networks - 2015 - Li, Swersky, Zemel.pdf:pdf},
isbn = {9781510810587},
journal = {Proceedings of The 32nd International Conference on Machine Learning},
pages = {1718--1727},
title = {{Generative Moment Matching Networks}},
url = {http://jmlr.org/proceedings/papers/v37/li15.html{\%}5Cnhttps://github.com/yujiali/gmmn},
volume = {37},
year = {2015}
}
@article{Jaech2016,
abstract = {The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques.},
archivePrefix = {arXiv},
arxivId = {1604.00117},
author = {Jaech, Aaron and Heck, Larry and Ostendorf, Mari},
eprint = {1604.00117},
file = {:scratch/download/mendeley/2016/Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding - 2016 - Jaech, Heck, Ostendorf.pdf:pdf},
journal = {Acl},
keywords = {Domain Adaptation,RNN,Semantics},
pages = {2--6},
title = {{Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding}},
url = {http://arxiv.org/abs/1604.00117},
year = {2016}
}
@article{Patel2015,
abstract = {In pattern recognition and computer vision, one is often faced with scenarios where the training data used to learn a model has different distribution from the data on which the model is applied. Regardless of the cause, any distributional change that occurs after learning a classifier can degrade its performance at test time. Domain adaptation tries to mitigate this degradation. In this paper, we provide a survey of domain adaptation methods for visual recognition. We discuss the merits and drawbacks of existing domain adaptation approaches and identify promising avenues for research in this rapidly evolving field.},
author = {Patel, VM and Gopalan, Raghuraman},
doi = {10.1109/MSP.2014.2347059},
file = {:scratch/download/mendeley/2015/Visual Domain Adaptation A survey of recent advances - 2015 - Patel, Gopalan.pdf:pdf},
issn = {1053-5888},
journal = {Signal Processing {\ldots}},
pages = {1--36},
title = {{Visual Domain Adaptation: A survey of recent advances}},
url = {http://www.umiacs.umd.edu/{~}pvishalm/Journal{\_}pub/SPM{\_}DA{\_}v9.pdf{\%}5Cnhttp://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=7078994},
volume = {02138},
year = {2015}
}
@inproceedings{Courty2014a,
abstract = {We present a new and original method to solve the domain adaptation problem using optimal transport. By searching for the best transportation plan between the probability distribution functions of a source and a target domain, a non-linear and invertible transformation of the learning samples can be estimated. Any standard machine learning method can then be applied on the transformed set, which makes our method very generic. We propose a new optimal transport algorithm that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term. By using the proposed optimal transport with label regularization, we obtain significant increase in performance compared to the original transport solution. The proposed algorithm is computationally efficient and effective, as illustrated by its evaluation on a toy example and a challenging real life vision dataset, against which it achieves competitive results with respect to state-of-the-art methods.},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-662-44848-9_18},
isbn = {9783662448472},
issn = {16113349},
number = {PART 1},
pages = {274--289},
title = {{Domain adaptation with regularized optimal transport}},
volume = {8724 LNAI},
year = {2014}
}
@article{Dai2017,
abstract = {Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the "ground-truth" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.},
archivePrefix = {arXiv},
arxivId = {1703.06029},
author = {Dai, Bo and Lin, Dahua and Urtasun, Raquel and Fidler, Sanja},
eprint = {1703.06029},
file = {:scratch/download/mendeley/2017/Towards Diverse and Natural Image Descriptions via a Conditional GAN - 2017 - Dai et al.pdf:pdf},
title = {{Towards Diverse and Natural Image Descriptions via a Conditional GAN}},
url = {http://arxiv.org/abs/1703.06029},
year = {2017}
}
@article{Sutherland2016,
abstract = {We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.},
archivePrefix = {arXiv},
arxivId = {1611.04488},
author = {Sutherland, Dougal J. and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
eprint = {1611.04488},
file = {:scratch/download/mendeley/2016/Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy - 2016 - Sutherland et al.pdf:pdf},
isbn = {9783901882760},
number = {2008},
pages = {1--13},
title = {{Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy}},
url = {http://arxiv.org/abs/1611.04488},
year = {2016}
}
@article{Dong2017,
abstract = {It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the "image-to-image translation" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation},
archivePrefix = {arXiv},
arxivId = {1701.02676},
author = {Dong, Hao and Neekhara, Paarth and Wu, Chao and Guo, Yike},
eprint = {1701.02676},
file = {:scratch/download/mendeley/2017/Unsupervised Image-to-Image Translation with Generative Adversarial Networks - 2017 - Dong et al.pdf:pdf},
title = {{Unsupervised Image-to-Image Translation with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.02676},
volume = {2},
year = {2017}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
eprint = {1703.10593},
file = {:scratch/download/mendeley/2017/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks - 2017 - Zhu et al.pdf:pdf},
month = {mar},
title = {{Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks}},
url = {http://arxiv.org/abs/1703.10593},
year = {2017}
}
@article{Nguyen2017,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 ÃƒÂ— 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models Ã¢Â€ÂœPlug and Play Generative Networks.Ã¢Â€Â PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable Ã¢Â€ÂœconditionÃ¢Â€Â network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1738978},
author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
eprint = {1738978},
file = {:scratch/download/mendeley/2017/Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space - 2017 - Nguyen et al.pdf:pdf},
journal = {Iccv},
number = {3},
primaryClass = {arXiv:submit},
title = {{Plug {\&} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
year = {2017}
}
@article{Saha2011,
abstract = {In this paper, we harness the synergy between two important learning$\backslash$nparadigms, namely, active learning and domain adaptation. We show$\backslash$nhow active learning in a target domain can leverage information from$\backslash$na different but related source domain. Our proposed framework, Active$\backslash$nLearning Domain Adapted (Alda), uses source domain knowledge to transfer$\backslash$ninformation that facilitates active learning in the target domain.$\backslash$nWe propose two variants of Alda: a batch B-Alda and an online O-Alda.$\backslash$nEmpirical comparisons with numerous baselines on real-world datasets$\backslash$nestablish the efficacy of the proposed methods.},
author = {Saha, Avishek and Rai, Piyush and Daum{\'{e}}, Hal and Venkatasubramanian, Suresh and DuVall, Scott L.},
doi = {10.1007/978-3-642-23808-6_7},
file = {:scratch/download/mendeley/2011/Active supervised domain adaptation - 2011 - Saha et al.pdf:pdf},
isbn = {9783642238079},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {active learning,batch,domain adaptation,online},
number = {PART 3},
pages = {97--112},
title = {{Active supervised domain adaptation}},
volume = {6913 LNAI},
year = {2011}
}
@article{To,
author = {To, Thanks and Borgwardt, Karsten and Rasch, Malte and Sch{\"{o}}lkopf, Bernhard and Huang, Jiayuan and Gretton, Arthur and Smola, Alexander J},
title = {{Maximum Mean Discrepancy}}
}
@article{Sener2016,
abstract = {Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions. Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. We will make our learned models as well as the source code available immediately upon acceptance.},
author = {Sener, Ozan and Song, Hyun Oh and Saxena, Ashutosh and Savarese, Silvio},
file = {:scratch/download/mendeley/2016/Learning Transferrable Representations for Unsupervised Domain Adaptation - 2016 - Sener et al.pdf:pdf},
journal = {Nips},
number = {Nips},
pages = {2110--2118},
title = {{Learning Transferrable Representations for Unsupervised Domain Adaptation}},
year = {2016}
}
@article{Long2016,
abstract = {The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.04433},
author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
eprint = {1602.04433},
file = {:scratch/download/mendeley/2016/Unsupervised Domain Adaptation with Residual Transfer Networks - 2016 - Long et al.pdf:pdf},
journal = {Nips},
keywords = {Deep learning, domain adaptation, residual learnin},
pages = {136--144},
pmid = {199870},
title = {{Unsupervised Domain Adaptation with Residual Transfer Networks}},
url = {https://papers.nips.cc/paper/6110-unsupervised-domain-adaptation-with-residual-transfer-networks},
year = {2016}
}
@article{Hoffman2016,
abstract = {Fully convolutional models for dense prediction have proven successful for a wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset.},
archivePrefix = {arXiv},
arxivId = {1612.02649},
author = {Hoffman, Judy and Wang, Dequan and Yu, Fisher and Darrell, Trevor},
eprint = {1612.02649},
file = {:scratch/download/mendeley/2016/FCNs in the Wild Pixel-level Adversarial and Constraint-based Adaptation - 2016 - Hoffman et al.pdf:pdf},
journal = {Arxiv},
title = {{FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation}},
url = {http://arxiv.org/abs/1612.02649},
year = {2016}
}
@article{Jiang2008,
author = {Jiang, Jing},
file = {:scratch/download/mendeley/2008/A literature survey on domain adaptation of statistical classifiers - 2008 - Jiang.pdf:pdf},
journal = {://Sifaka. Cs. Uiuc. Edu/Jiang4/Domainadaptation/Survey},
number = {March},
pages = {1--12},
title = {{A literature survey on domain adaptation of statistical classifiers}},
url = {http://sifaka.cs.uiuc.edu/jiang4/domain{\_}adaptation/survey/da{\_}survey.pdf},
year = {2008}
}
@article{Goodfellow2016,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1701.00160},
file = {:scratch/download/mendeley/2016/NIPS 2016 Tutorial Generative Adversarial Networks - 2016 - Goodfellow.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
pmid = {15040217},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1701.00160},
year = {2016}
}
@article{Tzeng,
abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simulta-neously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adapta-tion method offers empirical performance which exceeds previously published results on two standard benchmark vi-sual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.},
archivePrefix = {arXiv},
arxivId = {1510.02192},
author = {Tzeng, Eric and Hoffman, Judy and Darrell, Trevor and Saenko, Kate and Lowell, Umass},
doi = {10.1109/ICCV.2015.463},
eprint = {1510.02192},
file = {:scratch/download/mendeley/Unknown/Simultaneous Deep Transfer Across Domains and Tasks - Unknown - Tzeng et al.pdf:pdf},
isbn = {978-1-4673-8391-2},
title = {{Simultaneous Deep Transfer Across Domains and Tasks}}
}
@article{Tzeng2016,
author = {Tzeng, E. and Hoffman, J. and Saenko, K. and Darrell, T.},
file = {:scratch/download/mendeley/2016/Adversarial discriminative domain adaptation - 2016 - Tzeng et al(2).pdf:pdf},
pages = {1--4},
title = {{Adversarial discriminative domain adaptation}},
year = {2016}
}
@article{Boesen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.09300v2},
author = {Boesen, Anders and Larsen, Lindbo and Larochelle, Hugo and Winther, Ole and Science, Computer and Centre, Bioinformatics},
eprint = {arXiv:1512.09300v2},
file = {:scratch/download/mendeley/2014/Autoencoding beyond pixels using a learned similarity metric - 2014 - Boesen et al.pdf:pdf},
title = {{Autoencoding beyond pixels using a learned similarity metric}},
year = {2014}
}
@article{Gupta2017,
abstract = {People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of "analogy making", or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.},
archivePrefix = {arXiv},
arxivId = {1703.02949},
author = {Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
eprint = {1703.02949},
file = {:scratch/download/mendeley/2017/Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning - 2017 - Gupta et al.pdf:pdf},
keywords = {binaries,pho-,planetary systems,radial velocities,spectroscopic,techniques},
month = {mar},
title = {{Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning}},
url = {http://arxiv.org/abs/1703.02949},
year = {2017}
}
@article{Csurka2017,
abstract = {The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.},
archivePrefix = {arXiv},
arxivId = {1702.05374},
author = {Csurka, Gabriela},
eprint = {1702.05374},
file = {:scratch/download/mendeley/2017/Domain Adaptation for Visual Applications A Comprehensive Survey - 2017 - Csurka.pdf:pdf},
pages = {1--46},
title = {{Domain Adaptation for Visual Applications: A Comprehensive Survey}},
url = {http://arxiv.org/abs/1702.05374},
year = {2017}
}
@article{Adel2014,
abstract = {The aim of domain adaptation algorithms is to establish a learner, trained on labeled data from a source domain, that can classify samples from a target domain, in which few or no labeled data are available for training. Covariate shift, a primary assumption in several works on domain adaptation, assumes that the labeling functions of source and target domains are identical. We present a domain adaptation algorithm that assumes a relaxed version of covariate shift where the assumption that the labeling functions of the source and target domains are identical holds with a certain probability. Assuming a source deterministic large margin binary classifier, the farther a target instance is from the source decision boundary, the higher the probability that covariate shift holds. In this context, given a target unlabeled sample and no target labeled data, we develop a domain adaptation algorithm that bases its labeling decisions both on the source learner and on the similarities between the target unlabeled instances. The source labeling function decisions associated with probabilistic covariate shift, along with the target similarities are concurrently expressed on a similarity graph.We evaluate our proposed algorithm on a benchmark sentiment analysis (and domain adaptation) dataset, where state-of-the-art adaptation results are achieved. We also derive a lower bound on the performance of the algorithm.},
author = {Adel, Tameem and Wong, Alexander},
file = {:scratch/download/mendeley/2014/A Probabilistic Covariate Shift Assumption for Domain Adaptation - 2014 - Adel, Wong.pdf:pdf},
isbn = {9781577357025},
journal = {Aaai},
keywords = {Dataset: Amazon sentiment review},
pages = {2476--2482},
title = {{A Probabilistic Covariate Shift Assumption for Domain Adaptation}},
year = {2014}
}
@article{Liu2016,
abstract = {We propose the coupled generative adversarial network (CoGAN) framework for generating pairs of corresponding images in two different domains. It consists of a pair of generative adversarial networks, each responsible for generating images in one domain. We show that by enforcing a simple weight-sharing constraint, the CoGAN learns to generate pairs of corresponding images without existence of any pairs of corresponding images in the two domains in the training set. In other words, the CoGAN learns a joint distribution of images in the two domains from images drawn separately from the marginal distributions of the individual domains. This is in contrast to the existing multi-modal generative models, which require corresponding images for training. We apply the CoGAN to several pair image generation tasks. For each task, the GoGAN learns to generate convincing pairs of corresponding images. We further demonstrate the applications of the CoGAN framework for the domain adaptation and cross-domain image generation tasks.},
archivePrefix = {arXiv},
arxivId = {1606.07536},
author = {Liu, Ming-Yu and Tuzel, Oncel},
doi = {arXiv:1606.07536},
eprint = {1606.07536},
file = {:scratch/download/mendeley/2016/Coupled Generative Adversarial Networks - 2016 - Liu, Tuzel.pdf:pdf},
journal = {Nips},
number = {Nips},
title = {{Coupled Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1606.07536},
year = {2016}
}
@article{DaumeIII2016,
abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough " target " data to do slightly better than just using only " source " data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
archivePrefix = {arXiv},
arxivId = {0907.1815},
author = {{Daum{\'{e}} III}, Hal},
doi = {10.1.1.110.2062},
eprint = {0907.1815},
file = {:scratch/download/mendeley/2016/Frustratingly Easy Neural Domain Adaptation - 2016 - Daum{\'{e}} III.pdf:pdf},
isbn = {0736587X},
issn = {0736587X},
title = {{Frustratingly Easy Neural Domain Adaptation}},
year = {2016}
}
@article{Isola,
abstract = {Labels to Facade BW to Color Aerial to Map Labels to Street Scene Edges to Photo input output input input input input output output output output input output Day to Night Figure 1: Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss func-tion to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demon-strate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a commu-nity, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
file = {:scratch/download/mendeley/Unknown/Image-to-Image Translation with Conditional Adversarial Networks - Unknown - Isola et al.pdf:pdf},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}}
}
@article{Bousmalis2016,
abstract = {Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.},
archivePrefix = {arXiv},
arxivId = {1612.05424},
author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
eprint = {1612.05424},
file = {:scratch/download/mendeley/2016/Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks - 2016 - Bousmalis et al.pdf:pdf},
title = {{Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks}},
url = {http://arxiv.org/abs/1612.05424},
year = {2016}
}
@article{Arjovsky2017a,
abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
archivePrefix = {arXiv},
arxivId = {1701.07875},
author = {Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'{e}}on},
eprint = {1701.07875},
file = {:scratch/download/mendeley/2017/Wasserstein GAN - 2017 - Arjovsky, Chintala, Bottou.pdf:pdf},
title = {{Wasserstein GAN}},
url = {http://arxiv.org/abs/1701.07875},
year = {2017}
}

@article{AE1,
  title={Reducing the dimensionality of data with neural networks},
  author={Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
  journal={science},
  volume={313},
  number={5786},
  pages={504--507},
  year={2006},
  publisher={American Association for the Advancement of Science}
}


@article{AE2,
  title={Unsupervised learning of invariant feature hierarchies with applications to object recognition},
  author={Huang, Fu Jie and Boureau, Y-Lan and LeCun, Yann and others},
  booktitle={Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on},
  pages={1--8},
  year={2007},
  organization={IEEE}
}

@inproceedings{unet,
  title={U-net: Convolutional networks for biomedical image segmentation},
  author={Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  pages={234--241},
  year={2015},
  organization={Springer}
}


@inproceedings{relu,
  title={Rectified linear units improve restricted {B}oltzmann machines},
  author={Nair, Vinod and Hinton, Geoffrey E},
  booktitle={Proceedings of the 27th international conference on machine learning (ICML-10)},
  pages={807--814},
  year={2010}
}

@inproceedings{bn,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International Conference on Machine Learning},
  pages={448--456},
  year={2015}
}

@article{xception,
  title={Xception: Deep Learning with Depthwise Separable Convolutions},
  author={Chollet, Fran{\c{c}}ois},
  journal={arXiv preprint arXiv:1610.02357},
  year={2016}
}


@article{ncut,
  title={Normalized cuts and image segmentation},
  author={Shi, Jianbo and Malik, Jitendra},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={22},
  number={8},
  pages={888--905},
  year={2000},
  publisher={Ieee}
}


@misc{voc2012,
	author = "Everingham, M. and Van~Gool, L. and Williams, C. K. I. and Winn, J. and Zisserman, A.",
	title = "The {PASCAL} {V}isual {O}bject {C}lasses {C}hallenge 2012 {(VOC2012)} {R}esults",
	howpublished = "http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html"}	
    
    

@article{gPb-owr-ucm,
  title={Contour detection and hierarchical image segmentation},
  author={Arbelaez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={5},
  pages={898--916},
  year={2011},
  publisher={IEEE}
}
    
    

@article{Felz-Hutt,
  title={Efficient graph-based image segmentation},
  author={Felzenszwalb, Pedro F and Huttenlocher, Daniel P},
  journal={International journal of computer vision},
  volume={59},
  number={2},
  pages={167--181},
  year={2004},
  publisher={Springer}
}


@article{MS,
  title={A robust approach toward feature space analysis},
  author={Comaneci, Dorin and Shift, Peter Meer Mean},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={24},
  number={5},
  year={2002}
}


@article{chanvese,
  title={A variational framework for multiregion pairwise-similarity-based image segmentation},
  author={Bertelli, Luca and Sumengen, Baris and Manjunath, BS and Gibou, Fr{\'e}d{\'e}ric},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={30},
  number={8},
  pages={1400--1414},
  year={2008},
  publisher={IEEE}
}


@inproceedings{mnc,
  title={Spectral segmentation with multiscale graph decomposition},
  author={Cour, Timothee and Benezit, Florence and Shi, Jianbo},
  booktitle={Computer Vision and Pattern Recognition, 2005. CVPR 2005. IEEE Computer Society Conference on},
  volume={2},
  pages={1124--1131},
  year={2005},
  organization={IEEE}
}



@article{SWA,
  title={Hierarchy and adaptivity in segmenting visual scenes},
  author={Sharon, Eitan and Galun, Meirav and Sharon, Dahlia and Basri, Ronen and Brandt, Achi},
  journal={Nature},
  volume={442},
  number={7104},
  pages={810--813},
  year={2006},
  publisher={Nature Publishing Group}
}





@inproceedings{mostajabi2015feedforward,
  title={Feedforward semantic segmentation with zoom-out features},
  author={Mostajabi, Mohammadreza and Yadollahpour, Payman and Shakhnarovich, Gregory},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3376--3385},
  year={2015}
}

@inproceedings{hariharan2014simultaneous,
  title={Simultaneous detection and segmentation},
  author={Hariharan, Bharath and Arbel{\'a}ez, Pablo and Girshick, Ross and Malik, Jitendra},
  booktitle={European Conference on Computer Vision},
  pages={297--312},
  year={2014},
  organization={Springer}
}


@inproceedings{hariharan2015hypercolumns,
  title={Hypercolumns for object segmentation and fine-grained localization},
  author={Hariharan, Bharath and Arbel{\'a}ez, Pablo and Girshick, Ross and Malik, Jitendra},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={447--456},
  year={2015}
}


@article{farabet2013learning,
  title={Learning hierarchical features for scene labeling},
  author={Farabet, Clement and Couprie, Camille and Najman, Laurent and LeCun, Yann},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1915--1929},
  year={2013},
  publisher={IEEE}
}

@inproceedings{dai2015convolutional,
  title={Convolutional feature masking for joint object and stuff segmentation},
  author={Dai, Jifeng and He, Kaiming and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3992--4000},
  year={2015}
}





@inproceedings{fcn,
  title={Fully convolutional networks for semantic segmentation},
  author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={3431--3440},
  year={2015}
}


@inproceedings{krahenbuhl2011efficient,
  title={Efficient inference in fully connected crfs with gaussian edge potentials},
  author={Kr{\"a}henb{\"u}hl, Philipp and Koltun, Vladlen},
  booktitle={Advances in neural information processing systems},
  pages={109--117},
  year={2011}
}


@inproceedings{zheng2015conditional,
  title={Conditional random fields as recurrent neural networks},
  author={Zheng, Shuai and Jayasumana, Sadeep and Romera-Paredes, Bernardino and Vineet, Vibhav and Su, Zhizhong and Du, Dalong and Huang, Chang and Torr, Philip HS},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1529--1537},
  year={2015}
}




@inproceedings{arbelaez2009contours,
  title={From contours to regions: An empirical evaluation},
  author={Arbelaez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
  booktitle={Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on},
  pages={2294--2301},
  year={2009},
  organization={IEEE}
}


@article{arbelaez2011contour,
  title={Contour detection and hierarchical image segmentation},
  author={Arbelaez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={33},
  number={5},
  pages={898--916},
  year={2011},
  publisher={IEEE}
}



@article{paszke2016enet,
  title={Enet: A deep neural network architecture for real-time semantic segmentation},
  author={Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  journal={arXiv preprint arXiv:1606.02147},
  year={2016}
}

@article{chaurasia2017linknet,
  title={LinkNet: Exploiting Encoder Representations for Efficient Semantic Segmentation},
  author={Chaurasia, Abhishek and Culurciello, Eugenio},
  journal={arXiv preprint arXiv:1707.03718},
  year={2017}
}

@article{badrinarayanan2015segnet,
  title={Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  author={Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  journal={arXiv preprint arXiv:1511.00561},
  year={2015}
}

@inproceedings{noh2015learning,
  title={Learning deconvolution network for semantic segmentation},
  author={Noh, Hyeonwoo and Hong, Seunghoon and Han, Bohyung},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1520--1528},
  year={2015}
}
    @inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2011},
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@inproceedings{my_gp,
Author = {A. Roychowdhury and B. Kulis},
Title = "Gamma {P}rocesses, {S}tick-{B}reaking, and {V}ariational {I}nference",
booktitle="Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)",
Year = {2014}}

@ARTICLE{nbp_pami,
    author = {M. Zhou and L. Carin},
    title = "Negative {B}inomial {P}rocess {C}ount and {M}ixture {M}odeling",
    journal = {{IEEE} Trans. Pattern Anal. Mach. Intell.},
    volume  = {37},
    number = {2},
    pages  = {307--320},
   year = {2015},
}

@inproceedings{sgld,
author = {M. Welling and Y. W. Teh},
title = {Bayesian {L}earning via {S}tochastic {G}radient {L}angevin {D}ynamics},
year = {2011},
booktitle= {ICML},
}

@inproceedings{rie_sgld,
author = {S. Patterson and Y. W. Teh},
title = {Stochastic {G}radient {R}iemannian {L}angevin {D}ynamics on the {P}robability {S}implex},
year = {2013},
booktitle= {NIPS},
}

@inproceedings{sg_hmc,
author = {T. Chen and E. Chen and C. Guestrin},
title = {Stochastic {G}radient {H}amiltonian {M}onte {C}arlo},
year = {2014},
booktitle= {ICML},
}

@inproceedings{sgnht,
author = {N. Ding and Y. Fang and R. Babbush and C. Chen and R. D. Skeel and H. Neven},
title = {Bayesian {S}ampling using {S}tochastic {G}radient {T}hermostats},
year = {2014},
booktitle= {NIPS},
}

@ARTICLE{girolami_calderhead,
    author = {M. Girolami and B. Calderhead},
    title = "Riemann manifold {L}angevin and {H}amiltonian {M}onte {C}arlo methods",
    journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
    volume = {73},
    number = {2}, 
    pages = {123--214},
   year = {2011},
}

@InCollection{neal_hmc,
 author = {R. M. Neal},
 title = "M{CMC} using {H}amiltonian dynamics",
 booktitle = {Handbook of Markov Chain Monte Carlo},
 publisher = {Chapman \&  Hall / CRC Press},
 year = {2011},
 editor = {S. Brooks and A. Gelman and G. L. Jones and X.-L. Meng},
 pages={113-162}
}

@ARTICLE{nose_poinc,
    author = {S. D. Bond and B. J. Lemkuhler and B. B. Laird},
    title = "The {N}os\'{e}-{P}oincar\'{e} {M}ethod for {C}onstant {T}emperature {M}olecular {D}ynamics",
    journal = {J. Comput. Phys},
    year = {1999},
    volume = {151},
    pages = {114--134},
}

@book{leim_reich,
author = "B. Leimkuhler and S. Reich",
title = "Simulating {H}amiltonian {D}ynamics", 
publisher = {Cambridge University Press}, 
year = {2004},
}

@ARTICLE{duaneetal,
    author = {S. Duane and A.D. Kennedy and B.J. Pendleton and D. Roweth},
    title = "Hybrid {M}onte {C}arlo",
    journal = {Physics Letters B},
    volume = {195},
    number = {2}, 
    pages = {216--222},
   year = {1987},
}

@ARTICLE{nose_hamlt,
    author = {S. Nos\'{e}},
    title = "A molecular dynamics method for simulations in the canonical ensemble",
    journal = {Molecular Physics},
    volume = {52},
    number = {2}, 
    pages = {255--268},
   year = {1984},
}

@ARTICLE{nose_hoover,
    author = {W.G. Hoover},
    title = "Canonical dynamics: {E}quilibrium phase-space distributions",
    journal = {Physical Review A (General Physics)},
    volume = {31},
    number = {3}, 
    pages = {1695--1697},
   year = {1985},
}

@ARTICLE{robbins_monro,
    author = {H. Robbins and S. Monro},
    title = "A {S}tochastic {A}pproximation {M}ethod",
    journal = {The Annals of Mathematical Statistics},
    volume = {22},
    number = {3}, 
    pages = {400--407},
   year = {1951},
}

@ARTICLE{yin_ao,
    author = {L. Yin and P. Ao},
    title = "Existence and {C}onstruction of {D}ynamical {P}otential in {N}onequilibrium {P}rocesses without {D}etailed {B}alance",
    journal = {Journal of Physics A: Mathematical and General},
    volume = {39},
    number = {27}, 
    pages = {8593},
   year = {2006},
}

@inproceedings{sriv2013,
author = {N. Srivastava and R. Salakhutdinov and G. E. Hinton},
title = "Modeling documents with deep {B}oltzmann machines",
year = {2013},
booktitle= {UAI},
}

@inproceedings{gan2015,
author = {Z. Gan and C. Chen and R. Henao and D. Carlson and L. Carin},
title = "Scalable {D}eep {P}oisson {F}actor {A}nalysis for {T}opic {M}odeling",
year = {2015},
booktitle= {ICML},
}@article{adams:2010:tree,
  title={Tree-structured stick breaking for hierarchical data},
  author={Adams, R.P. and Ghahramani, Z. and Jordan, M.~I.},
  journal={Advances in Neural Information Processing Systems},
  volume={23},
  pages={19--27},
  year={2010}
}

@article{aldous:1985:exchangeability,
  title={Exchangeability and related topics},
  author={Aldous, D.},
  journal={{\'E}cole d'{\'E}t{\'e} de Probabilit{\'e}s de Saint-Flour XIIIÃ‘1983},
  pages={1--198},
  year={1985},
  publisher={Springer}
}

@article{akaike:1974:new,
  title={A new look at the statistical model identification},
  author={Akaike, H.},
  journal={IEEE Transactions on Automatic Control},
  volume={19},
  number={6},
  pages={716--723},
  year={1974},
  publisher={Ieee}
}

@article{antoniak:1974:mixtures,
  title={Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems},
  author={Antoniak, C.~E.},
  journal={The Annals of Statistics},
  pages={1152--1174},
  year={1974},
  publisher={JSTOR}
}

@inproceedings{arthur:2007:k,
  title={{k-means++}: The advantages of careful seeding},
  author={Arthur, D. and Vassilvitskii, S.},
  booktitle={Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={1027--1035},
  year={2007},
  organization={Society for Industrial and Applied Mathematics}
}

@article{berkhin:2006:survey,
  title={A survey of clustering data mining techniques},
  author={Berkhin, P.},
  journal={Grouping Multidimensional Data},
  pages={25--71},
  year={2006},
  publisher={Springer Berlin Heidelberg}
}

@book{bertoin:1998:levy,
  title={{L}{\'e}vy Processes},
  author={Bertoin, J.},
  volume={121},
  year={1998},
  publisher={Cambridge University Press}
}

@article{bertoin:2000:subordinators_LE,
  title={Subordinators, {L}{\'e}vy processes with no negative jumps, and branching processes},
  author={Bertoin, J.},
  year={2000},
  publisher={Centre for Mathematical Physics and Stochastics, University of Aarhus}
}

@article{bertoin:2004:subordinators_EX,
  title={Subordinators: examples and applications},
  author={Bertoin, J.},
  journal={Lectures on Probability Theory and Statistics},
  pages={1--91},
  year={2004},
  publisher={Springer}
}

@book{bishop:2006:pattern,
  title={Pattern Recognition and Machine Learning},
  author={Bishop, C.M.},
  year={2006},
  publisher={Springer New York}
}

@article{blackwell:1973:ferguson,
  title={{F}erguson distributions via {P}{\'o}lya Urn Schemes},
  author={Blackwell, D. and MacQueen, J.~B.},
  journal={The Annals of Statistics},
  volume={1},
  number={2},
  pages={353--355},
  year={1973},
  publisher={JSTOR}
}

@article{blei:2003:latent,
  title={Latent {D}irichlet allocation},
  author={Blei, D.~M. and Ng, A.~Y. and Jordan, M.~I.},
  journal={The Journal of Machine Learning Research},
  volume={3},
  pages={993--1022},
  year={2003}
}

@article{blei:2006:variational,
  title={Variational inference for Dirichlet process mixtures},
  author={Blei, D.M. and Jordan, M.~I.},
  journal={Bayesian Analysis},
  volume={1},
  number={1},
  pages={121--144},
  year={2006}
}

@inproceedings{blei:2010:distance,
  title={Distance dependent {C}hinese restaurant processes},
  author={Blei, D. and Frazier, P.},
  booktitle={International Conference on Machine Learning},
  year={2010},
}

@article{blei:2010:nested,
  title={The nested {C}hinese restaurant process and {B}ayesian nonparametric inference of topic hierarchies},
  author={Blei, D.~M. and Griffiths, T.~L. and Jordan, M.~I.},
  journal={Journal of the ACM (JACM)},
  volume={57},
  number={2},
  pages={7},
  year={2010},
  publisher={ACM}
}

@book{bochner:1955:harmonic,
  title={Harmonic analysis and the theory of probability},
  author={Bochner, S.},
  year={1955},
  publisher={University of California press}
}

@article{broderick:2011:combinatorial,
  title={Combinatorial clustering and the beta negative binomial process},
  author={Broderick, T. and Mackey, L. and Paisley, J. and Jordan, M.~I.},
  journal={Arxiv preprint arXiv:1111.1802},
  year={2011}
}

@article{broderick:2012:beta,
	title={Beta processes, stick-breaking, and power laws},
	author={T. Broderick and M.~I. Jordan and J. Pitman},
	journal={Bayesian Analysis},
	volume={7},
	pages={439--476},
	year={2012}
}

@article{broderick:2013:feature,
  title={Feature allocations, probability functions, and paintboxes},
  author={Broderick, T. and Pitman, J. and Jordan, M.~I.},
  journal={Bayesian Analysis, to appear},
  note={ArXiv preprint arXiv:1301.6647},
  year={2013}
}

@article{broderick:2013:clusters,
	title={Clusters and features from combinatorial stochastic processes},
	author={T. Broderick and M.~I. Jordan and J. Pitman},
	journal={Statistical Science, to appear},
	note={Arxiv preprint arXiv:1206.5862},
	year={2013}
}

@Article{de_finetti:1931:funzione,
	Author={{De Finetti}, B.},
	Title={Funzione caratteristica di un fenomeno aleatorio},
	Journal={Atti della R. Academia Nazionale dei Lincei, Serie 6.},
	volume={4},
	pages={251--299},
	year={1931},
	note={in {I}talian}
}

@article{dunson:2008:kernel,
  title={Kernel stick-breaking processes},
  author={Dunson, D.B. and Park, J.H.},
  journal={Biometrika},
  volume={95},
  number={2},
  pages={307--323},
  year={2008},
  publisher={Biometrika Trust}
}

@inproceedings{erosheva:2005:bayesian,
  title={Bayesian mixed membership models for soft clustering and classification},
  author={Erosheva, E.~A. and Fienberg, S.~E.},
  booktitle={Classification--The Ubiquitous Challenge},
  pages={11--26},
  publisher={Springer},
  address={New York},
  year={2005}
}

@article{escobar:1994:estimating,
  title={Estimating normal means with a {D}irichlet process prior},
  author={Escobar, M.~D.},
  journal={Journal of the American Statistical Association},
  pages={268--277},
  year={1994},
  publisher={JSTOR}
}

@article{escobar:1995:bayesian,
  title={Bayesian density estimation and inference using mixtures},
  author={Escobar, M.~D. and West, M.},
  journal={Journal of the American Statistical Association},
  pages={577--588},
  year={1995},
  publisher={JSTOR}
}

@incollection{ewens:1990:population,
    author = {Ewens, W.},
    booktitle = {Mathematical and Statistical Developments of Evolutionary Theory},
    editor = {Lessard, S.},
    pages = {177--227},
    priority = {2},
    publisher = {Kluwer Academic Publishers},
    title = {Population Genetics Theory - The Past and the Future},
    year = {1990}
}

@article{Ferguson73,
  author = {Ferguson, T.~S.},
  journal = {Annals of Statistics},
  number = 2,
  pages = {209--230},
  title = {A {B}ayesian analysis of some nonparametric problems},
  volume = 1,
  year = 1973,
}

@article{FraleyRaftery02,
  author = {Fraley, C. and Raftery, A.~E.},
  journal = {Journal of the American Statistical Association}, 
  pages = {611--631},
  title = {Model-based clustering, discriminant analysis and density estimation}, 
  volume = 97, 
  year = 2002,
}

@article{freedman:1965:bernard,
  title={Bernard {F}riedman's urn},
  author={Freedman, D.~A.},
  journal={The Annals of Mathematical Statistics},
  volume={36},
  number={3},
  pages={956--970},
  year={1965},
  publisher={JSTOR}
}

@article{geman:1984:stochastic,
  title={Stochastic relaxation, {G}ibbs distributions, and the {B}ayesian restoration of images},
  author={Geman, S. and Geman, D.},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  number={6},
  pages={721--741},
  year={1984},
  publisher={IEEE}
}

@article{gnedin:2006:exchangeable,
  title={Exchangeable {G}ibbs partitions and {S}tirling triangles},
  author={Gnedin, A. and Pitman, J.},
  journal={Journal of Mathematical Sciences},
  volume={138},
  number={3},
  pages={5674--5685},
  year={2006},
  publisher={Springer}
}

@article{gordon:1977:algorithm,
  title={An algorithm for {E}uclidean sum of squares classification},
  author={Gordon, A.~D. and Henderson, J.~T.},
  journal={Biometrics},
  pages={355--362},
  year={1977},
  publisher={JSTOR}
}

@incollection{griffiths:2006:infinite,
 title = {Infinite latent feature models and the {I}ndian buffet process},
 author = {T. Griffiths and Z. Ghahramani},
 booktitle = {Advances in Neural Information Processing Systems 18},
 editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
 publisher = {MIT Press},
 address = {Cambridge, MA},
 pages = {475--482},
 year = {2006}
}

@article{griffiths:2011:indian,
  title={The Indian buffet process: an introduction and review},
  author={Griffiths, T. and Ghahramani, Z.},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={April},
  pages={1185--1224},
  year={2011}
}

@techreport{hansen:1998:prediction,
	author={Hansen, B. and Pitman, J.},
	title={Prediction rules for exchangeable sequences related to species sampling},
	institution={University of California, Berkeley},
	number={520},
	year={1998},
	month={May}
}

@Article{hewitt:1955:symmetric,
	author = {Hewitt, E. and Savage, L.~J.},
	title = {Symmetric measures on {C}artesian products},
	journal = {Transactions of the {A}merican Mathematical Society},
	year = {1955},
	month = {November},
	volume = {80},
	number = {2},
	pages = {470--501}
}

@article{hjort:1990:nonparametric,
  title={Nonparametric {B}ayes estimators based on beta processes in models for life history data},
  author={Hjort, N.~L.},
  journal={Annals of Statistics},
  volume={18},
  number={3},
  pages={1259--1294},
  year={1990},
  publisher={Institute of Mathematical Statistics}
}

@article{hoppe:1984:polya,
  title={{P}{\`o}lya-like urns and the {E}wens' sampling formula},
  author={Hoppe, F.~M.},
  journal={Journal of Mathematical Biology},
  volume={20},
  number={1},
  pages={91--94},
  year={1984},
  publisher={Springer}
}

@article{ishwaran:2000:markov,
  title={Markov chain {M}onte {C}arlo in approximate {D}irichlet and beta two-parameter process hierarchical models},
  author={Ishwaran, H. and Zarepour, M.},
  journal={Biometrika},
  volume={87},
  number={2},
  pages={371--390},
  year={2000},
  publisher={Biometrika Trust}
}

@article{ishwaran:2001:gibbs,
  title={{G}ibbs sampling methods for stick-breaking priors},
  author={Ishwaran, H. and James, L.~F.},
  journal={Journal of the American Statistical Association},
  volume={96},
  number={453},
  pages={161--173},
  year={2001},
  publisher={ASA}
}

@article{jain:2010:data,
  title={Data clustering: 50 years beyond {K}-means},
  author={Jain, A.~K.},
  journal={Pattern Recognition Letters},
  volume={31},
  number={8},
  pages={651--666},
  year={2010},
  publisher={Elsevier}
}

@article{jordan:1999:introduction,
  title={An introduction to variational methods for graphical models},
  author={Jordan, M.~I. and Ghahramani, Z. and Jaakkola, T.~S. and Saul, L.~K.},
  journal={Machine learning},
  volume={37},
  number={2},
  pages={183--233},
  year={1999},
  publisher={Springer}
}

@article{Kim99,
     title = {Nonparametric {B}ayesian estimators for counting processes},
     author = {Kim, Y.},
     journal = {Annals of Statistics},
     volume = {27},
     number = {2},
     pages = {562--588},
     year = {1999}
 }

@article{kingman:1967:completely,
  title={Completely random measures.},
  author={Kingman, J.~F.~C.},
  journal={Pacific Journal of Mathematics},
  volume={21},
  number={1},
  pages={59--78},
  year={1967},
  publisher={Pacific Journal of Mathematics}
}

@article{kingman:1978:representation,
  title={The representation of partition structures},
  author={Kingman, J.~F.~C.},
  journal={Journal of the London Mathematical Society},
  volume={2},
  number={2},
  pages={374},
  year={1978},
  publisher={Oxford University Press}
}

@article{kingman:1982:coalescent,
  title={The coalescent},
  author={Kingman, JFC},
  journal={Stochastic processes and their applications},
  volume={13},
  number={3},
  pages={235--248},
  year={1982},
  publisher={Elsevier}
}

@book{kingman:1993:poisson,
  title={Poisson Processes},
  author={Kingman, J.~F.~C.},
  isbn={0198536933},
  year={1993},
  publisher={Oxford University Press}
}

@inproceedings{kulis:2012:revisiting,
  title={Revisiting k-means: {N}ew algorithms via {B}ayesian nonparametrics},
  author={Kulis, B. and Jordan, M.~I.},
  booktitle={Proceedings of the 23rd International Conference on Machine Learning},
  year={2012},
}

@inproceedings{li:2006:pachinko,
  title={Pachinko allocation: DAG-structured mixture models of topic correlations},
  author={Li, W. and McCallum, A.},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={577--584},
  year={2006},
  organization={ACM}
}

@article{maceachern:1994:estimating,
  title={Estimating normal means with a conjugate style {D}irichlet process prior},
  author={MacEachern, S.~N.},
  journal={Communications in Statistics-Simulation and Computation},
  volume={23},
  number={3},
  pages={727--741},
  year={1994},
  publisher={Taylor \& Francis}
}

@inproceedings{maceachern:1999:dependent,
  title={Dependent nonparametric processes},
  author={MacEachern, S.N.},
  booktitle={Proceedings of the Section on Bayesian Statistical Science},
  pages={50--55},
  year={1999}
}

@book{MclachlanBasford88,
  title={Mixture Models: Inference and Applications to Clustering},
  author={McLachlan, G. and Basford, K.},
  publisher={Dekker},
  address={New York},
  year={1988}
}

@phdthesis{mccloskey:1965:model,
	author={McCloskey, J.~W.},
	title={A model for the distribution of individuals by species 
in an environment},
	year={1965},
	school={Michigan State University}
}

@article{mccullagh:2008:gibbs,
  title={{G}ibbs fragmentation trees},
  author={McCullagh, P. and Pitman, J. and Winkel, M.},
  journal={Bernoulli},
  volume={14},
  number={4},
  pages={988--1002},
  year={2008},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@phdthesis{Miller11,
    Author = {Miller, K.~T.},
    Title = {Bayesian Nonparametric Latent Feature Models},
    School = {EECS Department, University of California, Berkeley},
    Year = {2011},
}

@article{mitzenmacher:2004:brief,
  title={A brief history of generative models for power law and lognormal distributions},
  author={Mitzenmacher, M.},
  journal={Internet mathematics},
  volume={1},
  number={2},
  pages={226--251},
  year={2004},
  publisher={AK Peters}
}

@TechReport{Neal93,
  author =	"R.~M. Neal",
  title =	"Probabilistic Inference using {M}arkov Chain {M}onte
		 {C}arlo Methods",
  institution = "University of Toronto",
  year = 	"1993",
}

@article{neal:2000:markov,
  title={Markov chain sampling methods for {D}irichlet process mixture models},
  author={Neal, R.~M.},
  journal={Journal of Computational and Graphical Statistics},
  volume={9},
  number={2},
  pages={249--265},
  year={2000}
}

@article{Neal03,
     title = {Slice sampling},
     author = {Neal, R.~M.},
     journal = {Annals of Statistics},
     volume = {31},
     number = {3},
     pages = {705-741},
     year = {2003}
}

@inproceedings{paisley:2010:stick,
  title={A stick-breaking construction of the beta process},
  author={Paisley, J. and Zaas, A. and Woods, C.~W. and Ginsburg, G.~S. and Carin, L.},
  booktitle={International Conference on Machine Learning},
  address={Haifa, Israel},
  year={2010}
}

@misc{paisley:2011:stick,
  title={The Stick-Breaking Construction of the Beta Process as a {P}oisson Process},
  author={Paisley, J. and Blei, D. and Jordan, M.~I.},
  year={2011},
  howpublished={Pre-print arXiv:1109.0343v1 [math.ST]}
}

@article{papaspiliopoulos:2008:retrospective,
  title={Retrospective {M}arkov chain {M}onte {C}arlo methods for {D}irichlet process hierarchical models},
  author={Papaspiliopoulos, O. and Roberts, G.O.},
  journal={Biometrika},
  volume={95},
  number={1},
  pages={169--186},
  year={2008},
  publisher={Biometrika Trust}
}

@techreport{Papaspiliopoulos08, 
title={A note on posterior sampling from {D}irichlet mixture models}, 
number={8}, 
institution={University of Warwick, Centre for Research in Statistical Methodology}, 
author={Papaspiliopoulos, O.}, 
year={2008}
}

@inproceedings{patil:1977:diversity,
  title={Diversity as a concept and its implications for random communities},
  author={Patil, G.~P. and Taillie, C.},
  year={1977},
  booktitle={Proceedings of the 41st Session of the International Statistical Institute},
  address={New Delhi},
  pages={497--515}
}

@article{pena:1999:empirical,
  title={An empirical comparison of four initialization methods for the {K}-{M}eans algorithm},
  author={Pe\~{n}a, J.~M. and Lozano, J.~A. and Larra\~{n}aga, P.},
  journal={Pattern Recognition Letters},
  volume={20},
  number={10},
  pages={1027--1040},
  year={1999},
  publisher={Elsevier}
}

@article{perman:1992:size,
  title={Size-biased sampling of {P}oisson point processes and excursions},
  author={Perman, M. and Pitman, J. and Yor, M.},
  journal={Probability Theory and Related Fields},
  volume={92},
  number={1},
  pages={21--39},
  year={1992},
  publisher={Springer}
}

@article{pitman:1995:exchangeable,
  title={Exchangeable and partially exchangeable random partitions},
  author={Pitman, J.},
  journal={Probability Theory and Related Fields},
  volume={102},
  number={2},
  pages={145--158},
  year={1995},
  publisher={Springer}
}

@article{pitman:1996:some,
  title={Some developments of the {B}lackwell-{MacQ}ueen urn scheme},
  author={Pitman, J.},
  journal={Lecture Notes-Monograph Series},
  pages={245--267},
  year={1996},
  publisher={JSTOR}
}

@article{pitman:1997:two,
  title={The two-parameter {P}oisson-{D}irichlet distribution derived from a stable subordinator},
  author={Pitman, J. and Yor, M.},
  journal={Annals of Probability},
  pages={855--900},
  volume={25},
  year={1997}
}

@article{pitman:2003:poisson,
  title={{P}oisson-{K}ingman partitions},
  author={Pitman, J.},
  journal={Lecture Notes-Monograph Series},
  volume={40},
  pages={1--34},
  year={2003},
  publisher={JSTOR}
}

@book{pitman:2006:combinatorial,
   AUTHOR = {Pitman, J.},
    TITLE = {Combinatorial stochastic processes},
   SERIES = {Lecture Notes in Mathematics},
   VOLUME = {1875},
 PUBLISHER = {Springer-Verlag},
  ADDRESS = {Berlin},
     YEAR = {2006},
    PAGES = {x+256},
     ISBN = {978-3-540-30990-1; 3-540-30990-X},
  MRCLASS = {60-02 (05C80 60C05 60G09 60J65 60J80)},
 MRNUMBER = {MR2245368},
       URL = {http://bibserver.berkeley.edu/csp/april05/bookcsp.pdf},
       DOI = {10.1007/b11601500},
}

@article{polya:1930:sur,
	title={Sur quelques points de la th\'{e}orie des probabilit\'{e}s},
	author={G. P\'{o}lya},
	year={1930},
	pages={117--161},
	journal={Annales de l'I.H.P.},
	volume={1},
	number={2}
}

@article{PritchardStDo00, 
author={Pritchard, J.~K. and Stephens, M. and Donnelly, P.}, 
title={Inference of population structure using multilocus genotype data}, 
volume={155}, 
number={2}, 
journal={Genetics}, 
year={2000}, 
pages={945--959}
}

@book {rogers:2000:diffusions,
   AUTHOR = {Rogers, L.~C.~G. and Williams, David},
    TITLE = {Diffusions, {M}arkov processes, and martingales. {V}ol. 1},
   SERIES = {Cambridge Mathematical Library},
     NOTE = {Foundations,
             Reprint of the second (1994) edition},
 PUBLISHER = {Cambridge University Press},
  ADDRESS = {Cambridge},
     YEAR = {2000},
    PAGES = {xx+386},
     ISBN = {0-521-77594-9},
  MRCLASS = {60J60 (60G07 60H05 60J25)},
 MRNUMBER = {1796539 (2001g:60188)},
}

@article{sethuraman:1994:constructive,
  title={A constructive definition of {D}irichlet priors},
  author={Sethuraman, J.},
  journal={Statistica Sinica},
  volume={4},
  number={2},
  pages={639--650},
  year={1994}
}

@article{steinley:2006:k,
  title={{K}-means clustering: {A} half-century synthesis},
  author={Steinley, D.},
  journal={British Journal of Mathematical and Statistical Psychology},
  volume={59},
  number={1},
  pages={1--34},
  year={2006},
  publisher={Wiley Online Library}
}

@article{sung:1998:example,
  title={Example-based learning for view-based human face detection},
  author={Sung, K. and Poggio, T.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={20},
  number={1},
  pages={39--51},
  year={1998},
  publisher={IEEE}
}

@inproceedings{teh:2007:stick,
  title={Stick-breaking construction for the Indian buffet process},
  author={Teh, Y.W. and G{\"o}r{\"u}r, D. and Ghahramani, Z.},
  booktitle={Proceedings of the International Conference on Artificial Intelligence and Statistics},
  volume={11},
  year={2007}
}

@inproceedings{ThibauxJo07,
author = {R. Thibaux and M.~I. Jordan},
title = {Hierarchical beta processes and the {I}ndian buffet process},
booktitle = {Proceedings of the International Conference on Artificial 
	Intelligence and Statistics},
volume=         {11},
year=           {2007}
}

@article{thomaz:2010:new,
	author={C.~E. Thomaz and G.~A. Giraldi},
	title={A new ranking method for principal components analysis and its application to face image analysis},
	journal={Image and Vision Computing},
	volume={28},
	number={6},
	pages={902--913},
	year={2010},
	month={June},
	note={We use files \url{http://fei.edu.br/~cet/frontalimages_spatiallynormalized_partX.zip} with \url{X}=\url{1,2}.}
}

@article{walker:2007:sampling,
  title={Sampling the Dirichlet mixture model with slices},
  author={Walker, S.G.},
  journal={Communications in Statistics---Simulation and Computation{\textregistered}},
  volume={36},
  number={1},
  pages={45--54},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{wolpert:2004:reflecting,
  title={Reflecting uncertainty in inverse problems: A Bayesian solution using L{\'e}vy processes},
  author={Wolpert, R.L. and Ickstadt, K.},
  journal={Inverse Problems},
  volume={20},
  pages={1759},
  year={2004},
  publisher={IOP Publishing}
}

@article{liu:1994:collapsed,
  title={The collapsed {G}ibbs sampler in {B}ayesian computations with
	applications to a gene regulation problem},
  author={Liu, J.},
  journal={Journal of the American Statistical Association},
  pages={958--966},
  volume={89},
  year={1994}
}

@techreport{sra_breg,
author="S. Sra and S. Jegelka and A. Banerjee",
title="Approximation algorithms for {B}regman clustering, co-clustering, and tensor clustering",
institution="MPI for Biological Cybernetics",
number="177",
year="2008"
}



@inproceedings{verma_nips,
author="N. Verma and K. Branson",
title="Sample Complexity of Learning {M}ahalanobis distance metrics",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2015"
}

@article{bellet_robust,
author="A. Bellet and A. Habrard",
title="Robustness and Generalization for Metric Learning",
journal="Neurocomputing",
volume="151",
number="1",
pages="259--267",
year="2015"
}

@inproceedings{sohn_nips,
author="K. Sohn",
title="Improved deep metric learning with multi-class N-pair loss objective",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2016"
}

@inproceedings{song_cvpr16,
author="H. Song and Y. Xiang and S. Jegelka and S. Savarese",
title="Deep Metric learning via lifted feature embedding",
booktitle="Proc. Computer Vision and Pattern Recognition (CVPR)",
year="2016"
}


@inproceedings{wang_cvpr,
author="J. Wang and Y. Song and T. Leung and C. Rosenberg and J. Wang and J. Philbin and B. Chen and Y. Wu",
title="Learning fine-grained image similarity with deep ranking",
booktitle="Proc. Computer Vision and Pattern Recognition (CVPR)",
year="2014"
}

@inproceedings{shroff_cvpr,
author="F. Schroff and D. Kalenichenko and J. Philbin",
title="Face{N}et: A unified embedding for face recognition and clustering",
booktitle="Proc. Computer Vision and Pattern Recognition (CVPR)",
year="2015"
}

@inproceedings{amos_icml,
author="B. Amos and L. Xu and J. Z. Kolter",
title="Input Convex Neural Networks",
booktitle="Proc. 34th International Conference on Machine Learning",
year="2017"
}

@article{ml_survey,
author="B. Kulis",
title="Metric Learning: A Survey",
journal="Foundations and Trends in Machine Learning",
volume="5",
number="4",
pages="287--364",
year="2012"
}

@book{prml,
    author    = "Christopher M. Bishop",
    title     = "Pattern Recognition and Machine Learning",
    publisher = "Springer",
    year      = "2006",
}

@article{fletcher,
author="R. Fletcher",
title="A new variational result for quasi-{N}ewton formulae",
journal="{SIAM} Journal on Optimization",
volume="1",
number="1",
year="1991"
}

@article{james_stein,
author="W. James and C. Stein",
title="Estimation with Quadratic Loss",
journal="Proc. Fourth {B}erkeley Symposium on Mathematical Statistics and Probability",
volume="1",
pages="361--379",
year="1961"
}

@inproceedings{passive_aggressive,
author="K. Crammer and O. Dekel and S. Shalev-Shwartz and Y. Singer",
title="Online Passive-Aggressive Algorithms",
booktitle="Advances in Neural Information Processing Systems",
year="2004"
}

@article{lasso,
author="R. Tibshirani",
title="Regression Shrinkage and Selection via the Lasso",
journal="Journal of the Royal Statistical Society, Series B",
volume="58",
number="1",
pages="267--288",
year="1996"
}

@book{lda,
author="G. J. McLachlan",
title="Discriminant Analysis and Statistical Pattern Recognition",
publisher="Wiley Interscience",
year="2004"
}

@Article{PPCA,
  author = 	 "M.~E. Tipping and C.~M. Bishop",
  title = 	 "Probabilistic principal component analysis",
  journal =	 "Journal of Royal Statistical Society, Series B",
  year =	 "1999",
  volume =	 "21",
  number = "3",
  pages =	 "611--622"
}

@inproceedings{Roweis98,
 author    = {S. Roweis},
 title     = {E{M} algorithms for {PCA} and {SPCA}},
 year      = {1998},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{rosales06,
author="R. Rosales and G. Fung",
title="Learning Sparse Metric via Linear Programming",
booktitle="Proc. SIGKDD Conference",
year="2006"
}

@article{cao_arxiv,
author="Q. Cao and Z. C. Guo and Y. Ying",
title="Generalization Bounds for Metric and Similarity Learning",
archivePrefix="arXiv",
journal="arXiv:1207.5437",
eprint="1207.5437",
year="2012"
}

@inproceedings{hoi06,
author="S. C. H. Hoi and W. Liu and M. R. Lyu and W. Y. Ma",
title="Learning Distance Metrics with Contextual Constraints for Image Retrieval",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2006"
}

@inproceedings{jin09,
author="R. Jin and S. Wang and Y. Zhou",
title="Regularized Distance Metric Learning: Theory and Algorithm",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2009"
}

@inproceedings{ying09,
author="Y. Ying and K. Huang and C. Campbell",
title="Sparse Metric Learning via Smooth Optimization",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2009"
}

@article{ying12,
author="Y. Ying and P. Li",
title="Distance Metric Learning with Eigenvalue Optimization",
journal="Journal of Machine Learning Research",
volume="13",
pages="1--26",
year="2012"
}

@inproceedings{kaski03,
author="S. Kaski and J. Peltonen",
title="Informative Discriminant Analysis",
booktitle="Proc. 20th International Conference on Machine Learning (ICML)",
year="2003"
}

@inproceedings{sal_hinton07,
author="R. Salakhutdinov and G. Hinton",
title="Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure",
booktitle="Proc. 11th International Conference on Artificial Intelligence and Statistics (AISTATS)",
year="2007"
}

@inproceedings{chechik09,
author="G. Chechik and V. Sharma and U. Shalit and S. Bengio",
title="An Online Algorithm for Lrage Scale Image Similarity Learning",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2009"
}

@inproceedings{torresani07,
author="L. Torresani and K. Lee",
title="Large Margin Component Analysis",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2007"
}

@inproceedings{bilenko04,
author="M. Bilenko and S. Basu and R. Mooney",
title="Integrating constraints and Metric Learning in Semi-Supervised Clustering",
booktitle="Proc. 21st International Conference on Machine Learning (ICML)",
year="2004"
}

@article{mahalanobis,
author="P. C. Mahalanobis",
title="On the Generalized Distance in Statistics",
journal="Proceedings of the National Institute of Sciences of India",
volume="2",
number="1",
pages="49--55",
year="1936"
}

@inproceedings{icml,
author="J. Davis and B. Kulis and P. Jain and S. Sra and I. Dhillon",
title="Information-theoretic Metric Learning",
booktitle="Proc. 24th International Conference on Machine Learning (ICML)",
year="2007"
}

@article{jain_jmlr12,
author="P. Jain and B. Kulis and J. Davis and I. Dhillon",
title="Metric and Kernel Learning using a Linear Transformation",
journal="Journal of Machine Learning Research",
volume="13",
pages="519--547",
year="2012"
}

@inproceedings{kulis_cvpr11,
author="B. Kulis and K. Saenko and T. Darrell",
title="What You Saw is Not What You Get: Domain Adaptation Using Asymmetric Kernel Transforms",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2011"
}

@inproceedings{taylor_aamas11,
author="M. Taylor and B. Kulis and F. Sha",
title="Metric Learning for Reinforcement Learning Agents",
booktitle="Proc. 10th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)",
year="2011"
}

@inproceedings{saenko_eccv10,
author="K. Saenko and B. Kulis and M. Fritz and T. Darrell",
title="Adapting Visual Category Models to New Domains",
booktitle="Proc. 11th European Conference on Computer Vision (ECCV)",
year="2010"
}

@article{kulis_pami09,
author="B. Kulis and P. Jain and K. Grauman",
title="Fast Similarity Search for Learned Metrics",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="31",
number="12",
pages="2143--2157",
year="2009"
}

@inproceedings{jain_nips10,
author="P. Jain and B. Kulis and I. Dhillon",
title="Inductive Regularized Learning of Kernel Functions",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2010"
}

@article{kulis_jmlr09,
author="B. Kulis and M. Sustik and I. Dhillon",
title="Low-Rank Kernel Learning with {B}regman Matrix Divergences",
journal="Journal of Machine Learning Research",
volume="10",
pages="341--376",
year="2009"
}

@inproceedings{kulis_icml06,
author="B. Kulis and M. Sustik and I. Dhillon",
title="Learning Low-Rank Kernel Matrices",
booktitle="Proc. 23rd International Conference on Machine Learning (ICML)",
year="2006"
}

@inproceedings{jain_nips08,
author="P. Jain and B. Kulis and I. Dhillon and K. Grauman",
title="Online Metric Learning and Fast Similarity Search",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2008"
}

@inproceedings{jain_cvpr08,
author="P. Jain and B. Kulis and K. Grauman",
title="Fast Image Search for Learned Metrics",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2008"
}

@inproceedings{weinberger_nips05,
author="K. Q. Weinberger and J. Blitzer and L. K. Saul",
title="Distance Metric Learning for Large Margin Nearest Neighbor Classification",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2005"
}

@inproceedings{weinberger_aistats07,
author="K. Q. Weinberger and G. Tesauro",
title="Metric Learning for Kernel Regression",
booktitle="Proc. 11th International Conference on Artificial Intelligence and Statistics (AISTATS)",
year="2007"
}

@inproceedings{weinberger_icml08,
author="K. Q. Weinberger and L. K. Saul",
title="Fast Solvers and Efficient Implementations for Distance Metric Learning",
booktitle="Proc. 25th International Conference on Machine Learning (ICML)",
year="2008"
}


@inproceedings{slaney_ismir08,
author="M. Slaney and K. Q. Weinberger and W. White",
title="Learning a Metric for Music Similarity",
booktitle="International Symposium on Music Information Retrieval (ISMIR)",
year="2008"
}

@article{weinberger_jmlr09,
author="K. Q. Weinberger and L. K. Saul",
title="Distance Metric Learning for Large Margin Nearest Neighbor Classification",
journal="Journal of Machine Learning Research",
volume="10",
pages="207--244",
year="2009"
}

@inproceedings{kedem_nips12,
author="D. Kedem and S. Tyree and K. Q. Weinberger and F. Sha and G. Lanckriet",
title="Non-Linear Metric Learning",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2012"
}

@inproceedings{parameswaran_nips10,
author="S. Parameswaran and K. Q. Weinberger",
title="Large Margin Multi-Task Metric Learning",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2010"
}

@inproceedings{goldberger_nips04,
author="J. Goldberger and S. Roweis and G. Hinton and R. Salakhutdinov",
title="Neighbourhood Components Analysis",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2004"
}

@inproceedings{xing_nips02,
author="E. P. Xing and A. Y. Ng and M. I. Jordan and S. Russell",
title="Distance Metric Learning, with Application to Clustering with Side-Information",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2002"
}

@inproceedings{shalev_shwartz_icml04,
author="S. Shalev-Shwartz and Y. Singer and A. Y. Ng",
title="Online Learning of Pseudo-Metrics",
booktitle="Proc. 21st International Conference on Machine Learning (ICML)",
year="2004"
}

@inproceedings{schultz_nips03,
author=" M. Schultz and T. Joachims",
title="Learning a Distance Metric From Relative Comparisons",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2003"
}

@article{chatpatanasiri_neurocomputing10,
author="R. Chatpatanasiri and T. Korsrilabutr and P. Tangchanachaianan and B. Kijsirikul",
title="A New Kernelization Framework for {M}ahalanobis Distance Learning Algorithms",
journal="Neurocomputing",
volume="73",
number="10--12",
pages="1570--1579",
year="2010"
}

@inproceedings{wang_nips11,
author="J. Wang and H. Do and A. Woznica and A. Kalousis",
title="Metric Learning with Multiple Kernels",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2011"
}

@inproceedings{kwok_icml03,
author="J. Kwok and I. Tsang",
title="Learning with Idealized Kernels",
booktitle="Proc. 20th International Conference on Machine Learning (ICML)",
year="2003"
}

@inproceedings{globerson_nips05,
author="A. Globerson and S. Roweis",
title="Metric Learning by Collapsing Classes",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2005"
}

@book{boyd_convex,
author="S. Boyd and L. Vandenberghe",
title="Convex Optimization",
publisher="Cambridge University Press",
year="2004"
}

@book{bertsekas_nonlinear,
author="D. P. Bertsekas",
title="Nonlinear Programming",
publisher="Athena Scientific",
year="1999"
}

@article{goldstein,
author="A. A. Goldstein",
title="Convex programming in {H}ilbert space",
journal="Bulletin of the American Mathematical Society",
volume="70",
pages="709--710",
year="1964"
}

@article{levitin,
author="E. S. Levitin and B. T. Polyak",
title="Constrained Minimization Problems",
journal="USSR Computational Mathematics and Mathematical Physics",
volume="6",
pages="1--50",
year="1966"
}

@article{bregman_67,
author="L. M. Bregman",
title="The Relxation Method of Finding the Common Points of Convex Sets and its Application to the Solution of Problems in Convex Programming",
journal="USSR Computational Mathematics and Mathematical Physics",
volume="7",
number="3",
pages="200--217",
year="1967"
}

@book{golub_matrix,
author="G. H. Golub and C. F. {Van Loan}",
title="Matrix Computations",
publisher="Johns Hopkins University Press",
year="1996"
}

@article{boyle_lns86,
author="J. P. Boyle and R. L. Dykstra",
title="A Method for Finding Projections onto the Intersection of Convex Sets in {H}ilbert Spaces",
journal="Lecture Notes in Statistics",
volume="37",
pages="28--47",
year="1986"
}

@incollection{bottou_stochastic98,
author="L. Bottou",
title="Online Algorithms and Stochastic Approximations",
booktitle="Online Learning and Neural Networks",
editor="D. Saad",
publisher="Cambridge University Press",
year="1998"
}

@book{nemirovski_opt83,
author="A. Nemirovski and D. Yudin",
title="Problem Complexity and Method Efficiency in Optimization",
publisher="Wiley, New York",
year="1983"
}

@article{beck_orl03,
author="A. Beck and M. Teboulle",
title="Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization",
journal="Operations Research Letters",
volume="31",
pages="167--175",
year="2003"
}

@inproceedings{zinkevich_icml03,
author="M. Zinkevich",
title="Online Convex Programming and Generalized Infinitesimal Gradient Ascent",
booktitle="Proc. 20th International Conference on Machine Learning (ICML)",
year="2003"
}

@book{cesa_bianchi_ocp06,
author="N. Cesa-Bianchi and G. Lugosi",
title="Prediction, Learning, and Games",
publisher="Cambridge University Press",
year="2006"
}

@book{shawe_taylor_kernels04,
author="J. Shawe-Taylor and N. Cristianini",
title="Kernel Methods for Pattern Analysis",
publisher="Cambridge University Press",
year="2004"
}

@book{schoelkopf_kernels02,
author="B. Schoelkopf and A. Smola",
title="Learning with Kernels",
publisher="MIT Press",
year="2002"
}

@article{grauman_jmlr07,
author="K. Grauman and T. Darrell",
title="The Pyramid Match Kernel: Efficient Learning with Sets of Features",
journal="Journal of Machine Learning Research",
volume="8",
pages="725--760",
year="2007"
}

@article{schoelkopf_neural98,
author="B. Schoelkopf and A. Smola and K.-R. Mueller",
title="Nonlinear Component Analysis as a Kernel Eigenvalue Problem",
journal="Neural Computation",
volume="10",
number="5",
pages="1299--1319",
year="1998"
}

@article{gaertner,
author="T. Gaertner",
title="A Survey of Kernels for Structured Data",
journal="ACM SIGKDD Explorations Newsletter",
volume="5",
number="1",
year="2003"
}

@article{lodhi_jmlr02,
author="H. Lodhi and C. Saunders and J. Shawe-Taylor and N. Cristanini and C. Watkins",
title="Text Classification using String Kernels",
journal="Journal of Machine Learning Research",
volume="2",
pages="419--444",
year="2002"
}

@inproceedings{davis_kdd08,
author="J. Davis and I. S. Dhillon",
title="Structured Metric Learning for High-Dimensional Problems",
booktitle="Proc. 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
year="2008"
}

@inproceedings{frome_iccv07,
author="A. Frome and Y. Singer and F. Sha and J. Malik",
title="Learning Globally Consistent Local Distance Functions for Shape-Based Image Retrieval and Classification",
booktitle="Proc. 11th IEEE International Conference on Computer Vision (ICCV)",
year="2007"
}

@inproceedings{chopra_cvpr05,
author="S. Chopra and R. Hadsell and Y. Le{C}un",
title="Learning a Similarity Metric Discriminatively, with Application to Face Verification",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2005"
}

@inproceedings{mcfee_icml10,
author="B. McFee and G. Lanckriet",
title="Metric Learning to Rank",
booktitle="Proc. 27th International Conference on Machine Learning {ICML}",
year="2010"
}

@inproceedings{indyk_lsh,
author="P. Indyk and R. Motwani",
title="Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality",
booktitle="Proc. 30th Symposium on Theory of Computing (STOC)",
year="1998"
}

@article{friedman_kdtree,
author="J. Friedman and J. Bentley and A. Finkel",
title="An Algorithm for Finding Best Matches in Logarithmic Expected Time",
journal="ACM Trans. Math. Software",
volume="3",
number="3",
pages="209--226",
year="1977"
}

@article{uhlmann_ipl91,
author="J. Uhlmann",
title="Satisfying General Proximity/Similarity Queries with Metric Trees",
journal="Information Processing Letters",
volume="40",
pages="175--179",
year="1991"
}

@inproceedings{ciaccia_vldb97,
author="P. Ciaccia and M. Patella and P. Zezula",
title="M-{T}ree: An Efficient Access Method for Similarity Search in Metric Spaces",
booktitle="Proc. International Conference on Very Large Data Bases (VLDB)",
year="1997"
}

@inproceedings{daume_acl,
author="H Daume",
title="Frustratingly Easy Domain Adaptation",
booktitle="Conference of the Association for Computational Linguistics (ACL)",
year="2007"
}

@inproceedings{snavely_siggraph06,
author="N. Snavely and S. Seitz and R. Szeliski",
title="Photo Tourism: Exploring Photo Collections in {3D}",
booktitle="Proc. ACM SIGGRAPH",
year="2006"
}

@inproceedings{hua_iccv07,
author="G. Hua and M. Brown and S. Winder",
title="Discriminant Embedding for Local Image Descriptors",
booktitle="Proc. 11th IEEE International Conference on Computer Vision (ICCV)",
year="2007"
}

@inproceedings{guillaumin_iccv09,
author="M. Guillaumin and J. Verbeek and C. Schmid",
title="Is That You?  {M}etric Learning Approaches for Face Identification",
booktitle="Proc. 12th IEEE International Conference on Computer Vision (ICCV)",
year="2009"
}

@inproceedings{tran_eccv08,
author="D. Tran and A. Sorokin",
title="Human Activity Recognition with Metric Learning",
booktitle="Proc. European Conference on Computer Vision (ECCV)",
year="2008"
}

@inproceedings{shakhnarovich_iccv03,
author="G. Shakhnarovich and P. Viola and T. Darrell",
title="Fast Pose Estimation with Parameter-Sensitive Hashing",
booktitle="Proc. 9th IEEE International Conference on Computer Vision (ICCV)",
year="2003"
}

@manual{poser,
organization="Curious Labs, Inc.",
address="Santa Cruz, CA",
title="Poser 5 - Reference Manual",
year="2002"
}

@inproceedings{clarify,
author="J. Ha and C. Rossbach and J. Davis and I. Roy and D. Chen and H. Ramadan and E. Witchel",
title="Improved Error Reporting for Software that Uses Black Box Components",
booktitle="Proc. Programming Language Design and Implementation (PLDI)",
year="2007"
}

@article{lebanon_pami06,
author="G. Lebanon",
title="Metric Learning for Text Documents",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="28",
number="4",
pages="497--508",
year="2006"
}

@inproceedings{takeuchi_iwsmi09,
author="I. Takeuchi and M. Nakagawa and M. Seto",
title="Metric Learning for {DNA} Microarray Data Analysis",
booktitle="In Proc. International Workshop on Statistical-Mechanical Informatics (IW-SMI)",
year="2009"
}

@article{jiong_bio06,
author="H. Xiong and X. Chen",
title="Kernel-based Distance Metric Learning for Microarray Data Classification",
journal="BMC Bioinformatics",
volume="7",
pages="299",
year="2006"
}

@article{cula_ijcv04,
author="O. G. Cula and K. J. Dana",
title="{3D} Texture Recognition using Bidirectional Feature Histograms",
journal="International Journal of Computer Vision (IJCV)",
volume="59",
number="1",
pages="33--60",
year="2004"
}

@article{tuytelaars_ftcg08,
author="T. Tuytelaars and K. Mikolajczyk",
title="Local Invariant Feature Detectors: A Survey",
journal="Foundations and Trends in Computer Graphics and Vision",
volume="3",
number="3",
pages="177--280",
year="2008"
}

@article{varma_tpami09,
author="M. Varma and A. Zisserman",
title="A Statistical Approach to Material Classification Using Image Patch Exemplars",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="31",
number="11",
pages="2032--2047",
year="2009"
}@inproceedings{online_lda,
author="M. D. Hoffman and D. M. Blei and F. Bach",
title="Online Learning for {L}atent {D}irichlet {A}llocation",
booktitle="Advances in Neural Information Processing Systems",
year="2010"
}

@inproceedings{online_hdp,
author="C. Wang and J. Paisley and D. M. Blei",
title="Online Variational Inference for the hierarchical {D}irichlet process",
booktitle="Proceedings of the 14th International Conference on Artificial Intelligence and Statistics",
year="2011"
}

@inproceedings{imagenet,
author="J. Deng and W. Dong and R. Socher and L.-J. Li and K. Li and L. Fei-Fei",
title="{I}mage{N}et: A Large-scale Hierarchical Image Database",
booktitle="IEEE Conference on Computer Vision and Patterns Recognition",
year="2009"
}

@article{agarwal_daume,
author="A. Agarwal and H. Daume",
title="A Geometric View of Conjugate Priors",
journal="Machine Learning",
volume="81",
number="1",
pages="99--113",
year="2010"
}

@inproceedings{forster,
author="J. Forster and M. K. Warmuth",
title="Relative expected instantaneous loss bounds",
booktitle="Proceedings of 13th Conference on Computational Learning Theory",
year="2000"
}

@book{barndorff,
author="O. Barndorff-Nielsen",
title="Information and Exponential Families in Statistical Theory",
publisher="Wiley Publishers",
year="1978"
}

@inproceedings{roweis_pca,
author="S. Roweis",
title="{EM} Algorithms for {PCA} and {SPCA}",
booktitle="Advances in Neural Information Processing Systems",
year="1998"
}

@article{tipping_bishop,
author="M. E. Tipping and C. M. Bishop",
title="Probabilistic principal component analysis",
journal="Journal of the Royal Statistical Society, Series B",
volume="21",
number="3",
pages="611--622",
year="1999"
}

@inproceedings{kulis_jordan,
author="B. Kulis and M. I. Jordan",
title="Revisiting k-means: New Algorithms via {B}ayesian nonparametrics",
booktitle="Proceedings of the 29th International Conference on Machine Learning",
year="2012"
}

@article{kurihara,
author="K. Kurihara and M. Welling",
title="Bayesian k-means as a ``{M}aximization-{E}xpectation'' Algorithm",
journal="Neural Computation",
volume="21",
number="4",
pages="1145--1172",
year="2008"
}

@inproceedings{photo_tourism,
author="N. Snavely and S. Seitz and R. Szeliski",
title="Photo Tourism: Exploring Photo Collections in {3D}",
booktitle="Proc. ACM SIGGRAPH",
year="2006"
}

@article{escobar,
author="M. D. Escobar and M. West",
title="Bayesian Density Estimation and Inference Using Mixtures",
journal="Journal of the American Statistical Association",
volume="90",
number="430",
pages="577--588",
year="1995"
}

@inproceedings{vbow,
author="L. Fei-Fei and P. Perona",
title="A {B}ayesian Hierarchical Model for Learning Natural Scene Categories",
booktitle="IEEE Conference on Computer Vision and Patterns Recognition",
year="2005"
}

@article{lda,
author="D. Blei and A. Ng and M. I. Jordan",
title="Latent {D}irichlet Allocation",
journal="Journal of Machine Learning Research",
volume="3",
pages="993--1022",
year="2003"
}

@book{hjort,
author="N. Hjort and C. Holmes and P. Mueller and S. Walker",
title="Bayesian Nonparametrics: Principles and Practice",
publisher="Cambridge University Press",
address="Cambridge, UK",
year="2010"
}

@article{bischof,
author="H. Bischof and A. Leonardis and A. Selb",
title="{MDL} Principle for Robust Vector Quantisation",
journal="Pattern Analysis and Applications",
volume="2",
number="1",
pages="59--72",
year="1999"
}

@inproceedings{hamerly,
title="Learning the k in k-means",
author="G. Hamerly and C. Elkan",
booktitle="Advances in Neural Information Processing Systems",
year="2003"
}

@article{jain_neal,
author="S. Jain and R. M. Neal",
title="A Split-Merge {M}arkov chain {M}onte {C}arlo Procedure for the {D}irichlet Process Mixture Model",
journal="Journal of Computational and Graphical Statistics",
volume="13",
number="1",
pages="158--182",
year="2004"
}

@article{blei_variational,
author="D. Blei and M. Jordan",
title="Variational Inference for {D}irichlet Process mixtures",
journal="Journal of Bayesian Analysis",
volume="1",
number="1",
pages="121--144",
year="2006"
}

@inproceedings{liang_permutation,
author="P. Liang and M. Jordan and B. Taskar",
title="A Permutation-Augmented Sampler for {DP} Mixture Models",
booktitle="Proceedings of the 24th International Conference on Machine Learning",
year="2007"
}

@article{teh,
author="Y. W. Teh and M. I. Jordan and M. J. Beal and D. M. Blei",
title="Hierarchical {D}irichlet Processes",
journal="Journal of the American Statistical Association",
volume="101",
number="476",
pages="1566--1581",
year="2006"
}

@article{sugar_james,
author="C. A. Sugar and G. M. James",
title="Finding the number of clusters in a data set: An information theoretic approach",
journal="Journal of the American Statistical Association",
volume="98",
pages="750--763",
year="2003"
}

@article{lleti,
author="R. Lleti and M. C. Ortiz and L. A. Sarabia and M. S. Sanchez",
title="Selecting Variables for k-means Cluster Analysis by using a genetic algorithm that optimizes the silhouettes",
journal="Analytica Chimica Acta",
volume="515",
pages="87--100",
year="2004"
}

@article{ketchen,
author="D. J. Ketchen and C . L. Shook",
title="The application of cluster analysis in strategic management research: an analysis and critique",
journal="Strategic Management Journal",
volume="17",
number="6",
pages="441--458",
year="1996"
}

@article{fraley,
author="C. Fraley and A. E. Raftery",
title="How many clusters? {W}hich clustering method? {A}nswers via model-based cluster analysis",
journal="Computer Journal",
volume="41",
number="8",
pages="578--588",
year="1998"
}

@article{tibshirani,
author="R. Tibshirani and G. Walther and T. Hastie",
title="Estimating the number of clusters in a data set via the gap statistic",
journal="Journal of the Royal Statistical Society Series B",
volume="63",
pages="411--423",
year="2001"
}

@book{manning,
author="C. D. Manning and P. Raghavan and H. Sch{\"{u}}tze",
title="Introduction to Information Retrieval",
publisher="Cambridge University Press",
year="2008"
}

@article{neal_dp,
author="R. M. Neal",
title="Markov Chain Sampling Methods for {D}irichlet process mixture models",
journal="Journal of Computational and Graphical Statistics",
volume="9",
pages="249--265",
year="2000"
}

@incollection{west,
author="M. West and P. M{\"{u}}ller and M. D. Escobar",
title="Hierarchical priors and mixture models, with application in regression and density estimation",
booktitle="Aspects of Uncertainty",
publisher="John Wiley",
editor="P. R. Freeman and A. F. M. Smith",
pages="363--386",
year="1994"
}

@inproceedings{localsearch,
author="I. S. Dhillon and Y. Guan and J. Kogan",
title="Iterative Clustering of High dimensional Text data augmented by local search",
booktitle="IEEE International Conference on Data Mining",
pages="131--138",
year="2002"
}

@article{dhillon,
author="I. S. Dhillon and Y. Guan and B. Kulis",
title="Weighted Graph cuts without Eigenvectors: A Multilevel Approach",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="29",
number="11",
pages="1944--1957",
year="2007"
}

@article{ratio_cut,
author="P. Chan and M. Schlag and J. Zien",
title="Spectral k-way ratio cut partitioning",
journal="IEEE Transactions on CAD-Integrated Circuits and Systems",
volume="13",
pages="1088--1096",
year="1994"
}

@article{norm_cut,
author="J. Shi and J. Malik",
title="Normalized cuts and image segmentation",
journal="IEEE Transactions on Pattern Analysis and Machine Intelligence",
volume="22",
number="8",
pages="888--905",
year="2000"
}

@inproceedings{multiclass_ncut,
author="S. X. Yu and J. Shi",
title="Multiclass spectral clustering",
booktitle="Proceedings of the 9th International Conference on Computer Vision",
year="2003"
}

@inproceedings{rasmussen,
author="C. Rasmussen",
title="The Infinite {G}aussian mixture model",
booktitle="Advances in Neural Information Processing Systems",
year="2000"
}


@article{ishwaran,
author="H. Ishwaran and L. F. James",
title="Gibbs sampling methods for stick-breaking priors",
journal="Journal of the American Statistical Association",
volume="96",
number="453",
pages="161--173",
year="2001"
}

@book{pitman,
author="J. Pitman",
title="Combinatorial Stochastic Processes",
publisher="Springer-Verlag",
note="Lectures from the Saint-Flour Summer School on Probability Theory",
year="2006"
}

@inproceedings{zha_spectral,
author="H. Zha and X. He and C. Ding and H. Simon and M. Gu",
title="Spectral Relaxation for k-means Clustering",
booktitle="Advances in Neural Information Processing Systems",
year="2001"
}

@misc{frank_asuncion,
author="A. Frank and A. Asuncion",
title="U{CI} {M}achine {L}earning {R}epository",
url="http://archive.ics.uci.edu/ml",
institution="University of California, Irvine, School of Information and Computer Sciences",
year="2010"
}

@inproceedings{fcn_cvpr,
author="J. Long and E. Shelhamer and T. Darrell",
title="Fully Convolutional Networks for Semantic Segmentation",
booktitle="Proc. Computer Vision and Pattern Recognition (CVPR)",
year="2015"
}

@inproceedings{kulis2012revisiting,
  title={Revisiting k-means: New Algorithms via {B}ayesian Nonparametrics},
  author={Kulis, Brian and Jordan, Michael I},
  booktitle={Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
  pages={513--520},
  year={2012}
}

@article{greig1989exact,
  title={Exact maximum a posteriori estimation for binary images},
  author={Greig, DM and Porteous, BT and Seheult, Allan H},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={271--279},
  year={1989},
  publisher={JSTOR}
}


@article{shi2000normalized,
  title={Normalized cuts and image segmentation},
  author={Shi, Jianbo and Malik, Jitendra},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={22},
  number={8},
  pages={888--905},
  year={2000},
  publisher={IEEE}
}

@article{kernighan1970efficient,
  title={An efficient heuristic procedure for partitioning graphs},
  author={Kernighan, Brian W and Lin, Shen},
  journal={Bell system technical journal},
  volume={49},
  number={2},
  pages={291--307},
  year={1970},
  publisher={Wiley Online Library}
}

@inproceedings{dhillon2004kernel,
  title={Kernel k-means: spectral clustering and normalized cuts},
  author={I. Dhillon and Y. Guan and B. Kulis},
  booktitle={Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining},
  year={2004},
  organization={ACM}
}

@inproceedings{sudderth2008shared,
  title={Shared Segmentation of Natural Scenes Using Dependent {P}itman-{Y}or Processes.},
  author={E. B. Sudderth and M. I. Jordan},
  booktitle={NIPS},
  pages={1585--1592},
  year={2008}
}

@article{clauset2009power,
  title={Power-law distributions in empirical data},
  author={A. Clauset and C. R. Shalizi and M. E. J. Newman},
  journal={SIAM review},
  volume={51},
  number={4},
  pages={661--703},
  year={2009},
  publisher={SIAM}
}

@article{pitman1997two,
  title={The two-parameter {P}oisson-{D}irichlet distribution derived from a stable subordinator},
  author={Pitman, Jim and Yor, Marc},
  journal={The Annals of Probability},
  pages={855--900},
  year={1997},
  publisher={JSTOR}
}

@article{von2007tutorial,
  title={A tutorial on spectral clustering},
  author={Von Luxburg, Ulrike},
  journal={Statistics and computing},
  volume={17},
  number={4},
  pages={395--416},
  year={2007},
  publisher={Springer}
}

@article{DBLP:journals/corr/FanZC13,
  author    = {Xuhui Fan and
               Yiling Zeng and
               Longbing Cao},
  title     = {Non-parametric Power-law Data Clustering},
  journal   = {CoRR},
  volume    = {abs/1306.3003},
  year      = {2013},
  ee        = {http://arxiv.org/abs/1306.3003},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@article{chan1994spectral,
  title={Spectral k-way ratio-cut partitioning and clustering},
  author={Chan, Pak K and Schlag, Martine DF and Zien, Jason Y},
  journal={Computer-Aided Design of Integrated Circuits and Systems, IEEE Transactions on},
  volume={13},
  number={9},
  pages={1088--1096},
  year={1994},
  publisher={IEEE}
}

@article{kernighan1970efficient,
  title={An efficient heuristic procedure for partitioning graphs},
  author={Kernighan, Brian W and Lin, Shen},
  journal={Bell system technical journal},
  volume={49},
  number={2},
  pages={291--307},
  year={1970},
  publisher={Wiley Online Library}
}

@article{,
  journal={arXiv preprint arXiv:1212.2126},
  year={2012}
}

@inproceedings{broderick2012mad,
  title={{MAD}-{b}ayes: {MAP}-based asymptotic derivations from {B}ayes},
  author={Broderick, Tamara and Kulis, Brian and Jordan, Michael I},
  booktitle={Proceedings of the 30th International Conference on Machine Learning (ICML-13)},
  year={2013}
}

@inproceedings{yu2003multiclass,
  title={Multiclass spectral clustering},
  author={Yu, Stella X and Shi, Jianbo},
  booktitle={Computer Vision, 2003. Proceedings. Ninth IEEE International Conference on},
  pages={313--319},
  year={2003},
  organization={IEEE}
}

@article{arbelaez2011contour,
  title={Contour detection and hierarchical image segmentation},
  author={Arbelaez, Pablo and Maire, Michael and Fowlkes, Charless and Malik, Jitendra},
  journal={Pattern Analysis and Machine Intelligence, IEEE Transactions on},
  volume={33},
  number={5},
  pages={898--916},
  year={2011},
  publisher={IEEE}
}

@book{pitman,
author="Jim Pitman",
title="Combinatorial Stochastic Processes",
publisher="Springer-Verlag",
note="Lectures from the Saint-Flour Summer School on Probability Theory",
year="2006"
}

@book{hjort,
author="Nils Hjort and Chris Holmes and Peter Mueller and Stephen Walker",
title="Bayesian Nonparametrics: Principles and Practice",
publisher="Cambridge University Press",
address="Cambridge, UK",
year="2010"
}

@book{boyd_vandenberghe_2004, place={Cambridge}, title={Convex Optimization}, DOI={10.1017/CBO9780511804441}, publisher={Cambridge University Press}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}}

@article{JMLR:v17:15-189,
  author  = {Nihar B. Shah and Sivaraman Balakrishnan and Joseph Bradley and Abhay Parekh and Kannan Ramch and ran and Martin J. Wainwright},
  title   = {Estimation from Pairwise Comparisons: Sharp Minimax Bounds with Topology Dependence},
  journal = {Journal of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {58},
  pages   = {1-47},
  url     = {http://jmlr.org/papers/v17/15-189.html}
}

@phdthesis{balazs2016convex,
  title={Convex Regression: Theory, Practice, and Applications},
  author={Bal{\'a}zs, G{\'a}bor},
  year={2016},
  school={University of Alberta}
}

@article{minty1964monotonicity,
  title={On the monotonicity of the gradient of a convex function.},
  author={Minty, George J and others},
  journal={Pacific Journal of Mathematics},
  volume={14},
  number={1},
  pages={243--247},
  year={1964},
  publisher={Pacific Journal of Mathematics}
}

@inproceedings{pollard1990empirical,
  title={Empirical processes: theory and applications},
  author={Pollard, David},
  booktitle={NSF-CBMS regional conference series in probability and statistics},
  pages={i--86},
  year={1990},
  organization={JSTOR}
}

@inproceedings{balazs2015near,
  title={Near-optimal max-affine estimators for convex regression.},
  author={Bal{\'a}zs, G{\'a}bor and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={AISTATS},
  year={2015}
}

@article{banerjee2005clustering,
  title={Clustering with Bregman divergences},
  author={Banerjee, Arindam and Merugu, Srujana and Dhillon, Inderjit S and Ghosh, Joydeep},
  journal={Journal of machine learning research},
  volume={6},
  number={Oct},
  pages={1705--1749},
  year={2005}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@article{weinberger2009distance,
  title={Distance metric learning for large margin nearest neighbor classification},
  author={Weinberger, Kilian Q and Saul, Lawrence K},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={Feb},
  pages={207--244},
  year={2009}
}


@inproceedings{davis2007information,
  title={Information-theoretic metric learning},
  author={Davis, Jason V and Kulis, Brian and Jain, Prateek and Sra, Suvrit and Dhillon, Inderjit S},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={209--216},
  year={2007},
  organization={ACM}
}

@inproceedings{schultz2004learning,
  title={Learning a distance metric from relative comparisons},
  author={Schultz, Matthew and Joachims, Thorsten},
  booktitle={Advances in neural information processing systems},
  pages={41--48},
  year={2004}
}

@article{liu2009learning,
  title={Learning to rank for information retrieval},
  author={Liu, Tie-Yan and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={3},
  pages={225--331},
  year={2009},
  publisher={Now Publishers, Inc.}
}

@inproceedings{goldberger2005neighbourhood,
  title={Neighbourhood components analysis},
  author={Goldberger, Jacob and Hinton, Geoffrey E and Roweis, Sam T and Salakhutdinov, Ruslan R},
  booktitle={Advances in neural information processing systems},
  pages={513--520},
  year={2005}
}

@article{bellet2015robustness,
  title={Robustness and generalization for metric learning},
  author={Bellet, Aur{\'e}lien and Habrard, Amaury},
  journal={Neurocomputing},
  volume={151},
  pages={259--267},
  year={2015},
  publisher={Elsevier}
}

@article{cao2016generalization,
  title={Generalization bounds for metric and similarity learning},
  author={Cao, Qiong and Guo, Zheng-Chu and Ying, Yiming},
  journal={Machine Learning},
  volume={102},
  number={1},
  pages={115--132},
  year={2016},
  publisher={Springer}
}

@article{kulis2013metric,
  title={Metric learning: A survey},
  author={Kulis, Brian},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={5},
  number={4},
  pages={287--364},
  year={2013},
  publisher={Now Publishers, Inc.}
}

@inproceedings{mcfee2010metric,
  title={Metric learning to rank},
  author={McFee, Brian and Lanckriet, Gert R},
  booktitle={Proceedings of the 27th International Conference on Machine Learning (ICML-10)},
  pages={775--782},
  year={2010}
}

@inproceedings{yi2014deep,
  title={Deep metric learning for person re-identification},
  author={Yi, Dong and Lei, Zhen and Liao, Shengcai and Li, Stan Z},
  booktitle={2014 22nd International Conference on Pattern Recognition},
  pages={34--39},
  year={2014},
  organization={IEEE}
}

@inproceedings{hoffer2015deep,
  title={Deep metric learning using triplet network},
  author={Hoffer, Elad and Ailon, Nir},
  booktitle={International Workshop on Similarity-Based Pattern Recognition},
  pages={84--92},
  year={2015},
  organization={Springer}
}


@inproceedings{wagstaff2001constrained,
  title={Constrained k-means clustering with background knowledge},
  author={Wagstaff, Kiri and Cardie, Claire and Rogers, Seth and Schr{\"o}dl, Stefan and others},
  booktitle={Icml},
  volume={1},
  pages={577--584},
  year={2001}
}

@article{magnani2009convex,
  title={Convex piecewise-linear fitting},
  author={Magnani, Alessandro and Boyd, Stephen P},
  journal={Optimization and Engineering},
  volume={10},
  number={1},
  pages={1--17},
  year={2009},
  publisher={Springer}
}


@article{hannah2013multivariate,
  title={Multivariate convex regression with adaptive partitioning},
  author={Hannah, Lauren A and Dunson, David B},
  journal={The Journal of Machine Learning Research},
  volume={14},
  number={1},
  pages={3261--3294},
  year={2013},
  publisher={JMLR. org}
}

@inproceedings{feder1988optimal,
  title={Optimal algorithms for approximate clustering},
  author={Feder, Tom{\'a}s and Greene, Daniel},
  booktitle={Proceedings of the twentieth annual ACM symposium on Theory of computing},
  pages={434--444},
  year={1988},
  organization={ACM}
}


@article{gonzalez1985clustering,
  title={Clustering to minimize the maximum intercluster distance},
  author={Gonzalez, Teofilo F},
  journal={Theoretical Computer Science},
  volume={38},
  pages={293--306},
  year={1985},
  publisher={Elsevier}
}

@book{anthony2009neural,
  title={Neural network learning: Theoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@inproceedings{jain2009online,
  title={Online metric learning and fast similarity search},
  author={Jain, Prateek and Kulis, Brian and Dhillon, Inderjit S and Grauman, Kristen},
  booktitle={Advances in neural information processing systems},
  pages={761--768},
  year={2009}
}


@inproceedings{shalev2004online,
  title={Online and batch learning of pseudo-metrics},
  author={Shalev-Shwartz, Shai and Singer, Yoram and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={94},
  year={2004},
  organization={ACM}
}

@inproceedings{xing2003distance,
  title={Distance metric learning with application to clustering with side-information},
  author={Xing, Eric P and Jordan, Michael I and Russell, Stuart J and Ng, Andrew Y},
  booktitle={Advances in neural information processing systems},
  pages={521--528},
  year={2003}
}


@inproceedings{wu2009learning,
  title={Learning {B}regman distance functions and its application for semi-supervised clustering},
  author={Wu, Lei and Jin, Rong and Hoi, Steven C and Zhu, Jianke and Yu, Nenghai},
  booktitle={Advances in neural information processing systems},
  pages={2089--2097},
  year={2009}
}

@article{julian1998canonical,
  title={Canonical piecewise-linear approximation of smooth functions},
  author={Juli{\'a}n, Pedro and Jord{\'a}n, Mario and Desages, Alfredo},
  journal={IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume={45},
  number={5},
  pages={567--571},
  year={1998},
  publisher={IEEE}
}

@article{mangasarian2005global,
  title={Global minimization via piecewise-linear underestimation},
  author={Mangasarian, Olvi L and Rosen, J Ben and Thompson, ME},
  journal={Journal of Global Optimization},
  volume={32},
  number={1},
  pages={1--9},
  year={2005},
  publisher={Springer}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@inproceedings{gothoskar2002piecewise,
  title={Piecewise-linear modeling of analog circuits based on model extraction from trained neural networks},
  author={Gothoskar, Gaurav and Doboli, Alex and Doboli, Simona},
  booktitle={Proceedings of the 2002 IEEE International Workshop on Behavioral Modeling and Simulation, 2002. BMAS 2002.},
  pages={41--46},
  year={2002},
  organization={IEEE}
}

@inproceedings{ferrari2002new,
  title={A new learning method for piecewise linear regression},
  author={Ferrari-Trecate, Giancarlo and Muselli, Marco},
  booktitle={International conference on artificial neural networks},
  pages={444--449},
  year={2002},
  organization={Springer}
}

@article{bellet2015metric,
  title={Metric learning},
  author={Bellet, Aur{\'e}lien and Habrard, Amaury and Sebban, Marc},
  journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
  volume={9},
  number={1},
  pages={1--151},
  year={2015},
  publisher={Morgan \& Claypool Publishers}
}

@misc{gurobi,
  author = "Gurobi Optimization, LLC",
  title = "Gurobi Optimizer Reference Manual",
  year = 2018,
  url = "http://www.gurobi.com"
}


@article{holder1889ueber,
  title={Ueber einen mittelwerthabsatz},
  author={H{\"o}lder, Otto},
  journal={Nachrichten von der K{\"o}nigl. Gesellschaft der Wissenschaften und der Georg-Augusts-Universit{\"a}t zu G{\"o}ttingen},
  volume={1889},
  pages={38--47},
  year={1889}
}
@string{eccv = {European Conference on Computer Vision ({ECCV})}}
@string{iccv = {International Conference on Computer Vision ({ICCV})}}
@string{cvpr = {Computer Vision and Pattern Recognition ({CVPR})}}
@string{cvprw = {Computer Vision and Pattern Recognition ({CVPR}) Workshops}}
@string{iclr = {International Conference on Learning Representations ({ICLR})}}
@string{iclrw = {International Conference on Learning Representations ({ICLR}) Workshops}}
@string{icml = {International  Conference on Machine Learning ({ICML})}}
@string{ijcv = {International  Journal of Computer Vision}}
@string{tpami = {Transactions on Pattern Analysis and Machine Intelligence {(T-PAMI)}}}
@string{nips = {Neural Information Processing Systems ({NIPS})}}

@article{zhu_arxiv17,
    title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
    author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},
    journal={arXiv preprint arXiv:1703.10593},
    year={2017}
}

@InProceedings{richter_eccv16,
    author = {Stephan R. Richter and Vibhav Vineet and Stefan Roth and Vladlen Koltun},
    title = {Playing for Data: {G}round Truth from Computer Games},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2016},
    editor = {Bastian Leibe and Jiri Matas and Nicu Sebe and Max Welling},
    series = {LNCS},
    volume = {9906},
    publisher = {Springer International Publishing},
    pages = {102--118}
}

@inproceedings{cordts_cvpr16,
    title={The {Cityscapes} Dataset for Semantic Urban Scene Understanding},
    author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
    booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016}
}

@inproceedings{ros_cvpr16,
    author={German Ros and Laura Sellart and Joanna Materzynska and David Vazquez and Antonio Lopez},
    title={ {The SYNTHIA Dataset}: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes},
    booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016}
}

@article{hoffman_arxiv16,
  author    = {Judy Hoffman and
               Dequan Wang and
               Fisher Yu and
               Trevor Darrell},
  title     = {{FCN}s in the Wild: Pixel-level Adversarial and Constraint-based Adaptation},
  journal   = {CoRR},
  volume    = {abs/1612.02649},
  year      = {2016},
  timestamp = {Mon, 02 Jan 2017 11:09:15 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HoffmanWYD16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{tzeng_arxiv17,
  author    = {Eric Tzeng and
               Judy Hoffman and
               Kate Saenko and
               Trevor Darrell},
  title     = {Adversarial Discriminative Domain Adaptation},
  journal   = {ICLR},
  year      = {2017},
  timestamp = {Wed, 01 Mar 2017 14:26:00 +0100},
}

@inproceedings{yu_cvpr17,
    author    = {Fisher Yu and Vladlen Koltun and Thomas Funkhouser},
    title     = {Dilated Residual Networks},
    booktitle = {CVPR},
    year      = {2017},
}

@inproceedings{ghifary_eccv16,
  title={Deep Reconstruction-Classification Networks for Unsupervised Domain Adaptation},
  author={Ghifary, Muhammad and Kleijn, W Bastiaan and Zhang, Mengjie and Balduzzi, David and Li, Wen},
  booktitle={ECCV},
  year={2016},
}

@InProceedings{richter_eccv16,
    author = {Stephan R. Richter and Vibhav Vineet and Stefan Roth and Vladlen Koltun},
    title = {Playing for Data: {G}round Truth from Computer Games},
    booktitle = {European Conference on Computer Vision (ECCV)},
    year = {2016},
    editor = {Bastian Leibe and Jiri Matas and Nicu Sebe and Max Welling},
    series = {LNCS},
    volume = {9906},
    publisher = {Springer International Publishing},
    pages = {102--118}
}

@inproceedings{cordts_cvpr16,
    title={The {Cityscapes} Dataset for Semantic Urban Scene Understanding},
    author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
    booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016}
}

@inproceedings{ros_cvpr16,
    author={German Ros and Laura Sellart and Joanna Materzynska and David Vazquez and Antonio Lopez},
    title={ {The SYNTHIA Dataset}: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes},
    booktitle={Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2016}
}

@article{hoffman_arxiv16,
  author    = {Judy Hoffman and
               Dequan Wang and
               Fisher Yu and
               Trevor Darrell},
  title     = {{FCN}s in the Wild: Pixel-level Adversarial and Constraint-based Adaptation},
  journal   = {CoRR},
  volume    = {abs/1612.02649},
  year      = {2016},
  timestamp = {Mon, 02 Jan 2017 11:09:15 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/HoffmanWYD16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{tzeng_cvpr17,
  author    = {Eric Tzeng and
               Judy Hoffman and
               Kate Saenko and
               Trevor Darrell},
  title     = {Adversarial Discriminative Domain Adaptation},
  booktitle = {CVPR},
  year      = {2017},
}

@inproceedings{tzeng_iccv15,
author={Eric Tzeng and Judy Hoffman and Trevor Darrell and Kate Saenko},
title={Simultaneous Deep Transfer Across Domains and Tasks},
booktitle={ICCV},
year={2015},
}

@inproceedings{yu_cvpr17,
    author    = {Fisher Yu and Vladlen Koltun and Thomas Funkhouser},
    title     = {Dilated Residual Networks},
    booktitle = {CVPR},
    year      = {2017},
}

@article{mao_arxiv16,
  author    = {Xudong Mao and
               Qing Li and
               Haoran Xie and
               Raymond Y. K. Lau and
               Zhen Wang},
  title     = {Multi-class Generative Adversarial Networks with the {L2} Loss Function},
  journal   = {CoRR},
  year      = {2016},
  timestamp = {Thu, 01 Dec 2016 19:32:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MaoLXLW16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@misc{arjovsky_arxiv17,
  author    = {Martin Arjovsky and Soumith Chintala and LÃƒÂ©on Bottou},
  title     = {Wasserstein {GAN}},
  note = "arXiv:1701.07875",
  year      = {2017},
}

@inproceedings{arjovsky2017towards,
  title={Towards principled methods for training generative adversarial networks},
  author={Arjovsky, Martin and Bottou, L{\'e}on},
  year={2017}
}

@inproceedings{denton2015deep,
  title={Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks},
  author={Denton, Emily L and Chintala, Soumith and Fergus, Rob and others},
  booktitle={NIPS},
  year={2015}
}

@article{donahue2016adversarial,
  title={Adversarial feature learning},
  author={Donahue, Jeff and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
  journal={arXiv preprint arXiv:1605.09782},
  year={2016}
}

@article{zhao2016energy,
  title={Energy-based Generative Adversarial Network},
  author={Zhao, Junbo and Mathieu, Michael and LeCun, Yann},
  year={2016}
}
@article{karacan2016learning,
  title={Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts},
  author={Karacan, Levent and Akata, Zeynep and Erdem, Aykut and Erdem, Erkut},
  journal={arXiv preprint arXiv:1612.00215},
  year={2016}
}

@article{sangkloy2016scribbler,
  title={Scribbler: Controlling Deep Image Synthesis with Sketch and Color},
  author={Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
  journal={arXiv preprint arXiv:1612.00835},
  year={2016}
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={arXiv preprint arXiv:1511.06434},
  year={2015}
}

@article{salimans2016improved,
  title={Improved techniques for training {GANs}},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  year={2016}
}


@article{isola2016image,
  title={Image-to-image translation with conditional adversarial networks},
  author={Isola, Phillip and Zhu, Jun and Zhou, Tinghui and Efros, Alexei},
  year={2016}
}


@inproceedings{zhu2016generative,
  title={Generative Visual Manipulation on the Natural Image Manifold},
  author={Zhu, Jun-Yan and Kr{\"a}henb{\"u}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
  booktitle=eccv,
  year={2016}
}

@article{salimans_arxiv16,
  author    = {Tim Salimans and
               Ian J. Goodfellow and
               Wojciech Zaremba and
               Vicki Cheung and
               Alec Radford and
               Xi Chen},
  title     = {Improved Techniques for Training GANs},
  journal   = {CoRR},
  volume    = {abs/1606.03498},
  year      = {2016},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/SalimansGZCRC16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{mirza_arxiv14,
  author    = {Mehdi Mirza and
               Simon Osindero},
  title     = {Conditional Generative Adversarial Nets},
  journal   = {CoRR},
  volume    = {abs/1411.1784},
  year      = {2014},
  timestamp = {Mon, 01 Dec 2014 14:32:13 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MirzaO14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{liu_arxiv16,
  author    = {Ming{-}Yu Liu and
               Oncel Tuzel},
  title     = {Coupled Generative Adversarial Networks},
  journal   = {CoRR},
  volume    = {abs/1606.07536},
  year      = {2016},
  timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/0001T16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@incollection{goodfellow_nips14,
title = {Generative Adversarial Nets},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {NIPS},
year = {2014},
}

@inproceedings{ganin_icml15,
   title="Unsupervised Domain Adaptation by Backpropagation",
   Author={Ganin, Yaroslav and Lempitsky, Victor},
   year="2015",
   Booktitle="ICML",
   }
   
 @inproceedings{long_icml15,
 	title={Learning transferable features with deep adaptation nets},
 	author={Long, Mingsheng and Wang, Jianmin},
 	booktitle={ICML},
 	year={2015}
 }
 
   
@inproceedings{bousmalis_nips16,
  title={Domain separation networks},
  author={Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  booktitle={Advances in Neural Information Processing Systems},
  pages={343--351},
  year={2016}
}

@inproceedings{taigman_iclr17,
  title={Unsupervised Cross-Domain Image Generation},
  author={Taigman, Yaniv and Polyak, Adam and Wolf, Lior},
  booktitle=iclr,
  year={2017}
}

@inproceedings{song_nips16,
  title={Learning Transferrable Representations for Unsupervised Domain Adaptation},
  author={Ozan Sener and Hyun Oh Song and Ashutosh Saxena and Silvio Savarese},
  booktitle={NIPS},
  year={2016}
}

@article{ros_arxiv16,
  author    = {Germ{\'{a}}n Ros and
               Simon Stent and
               Pablo F. Alcantarilla and
               Tomoki Watanabe},
  title     = {Training Constrained Deconvolutional Networks for Road Scene Semantic
               Segmentation},
  journal   = {CoRR},
  volume    = {abs/1604.01545},
  year      = {2016},
  timestamp = {Mon, 02 May 2016 18:22:52 +0200},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{yoo_eccv16,
  author    = {Donggeun Yoo and
               Namil Kim and
               Sunggyun Park and
               Anthony S. Paek and
               In{-}So Kweon},
  title     = {Pixel-Level Domain Transfer},
  booktitle = eccv,
  year      = {2016},
}


@article {lecun_ieee98,
original =    "orig/lecun-98.ps.gz",
author = 	"LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.",
title = 	"Gradient-Based Learning Applied to Document Recognition",
journal =	"Proceedings of the IEEE",
month =         "November",
volume =        "86",
number =        "11",
pages =         "2278-2324",
year =		1998
}

@inproceedings{sun_taskcv16,
  author    = {Baochen Sun and
               Kate Saenko},
  title     = {Deep {CORAL:} Correlation Alignment for Deep Domain Adaptation},
  booktitle = {ICCV workshop on Transferring and Adapting Source Knowledge in Computer Vision (TASK-CV)},
  year      = {2016}
}

@article{tzeng_arxiv15,
  author    = {Eric Tzeng and
               Judy Hoffman and
               Ning Zhang and
               Kate Saenko and
               Trevor Darrell},
  title     = {Deep Domain Confusion: Maximizing for Domain Invariance},
  journal   = {CoRR},
  volume    = {abs/1412.3474},
  year      = {2014},
  timestamp = {Thu, 01 Jan 2015 19:51:08 +0100},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/TzengHZSD14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


@inproceedings{saenko_eccv10,
  title={Adapting visual category models to new domains},
  author={Saenko, Kate and Kulis, Brian and Fritz, Mario and Darrell, Trevor},
  booktitle={European conference on computer vision},
  pages={213--226},
  year={2010},
  organization={Springer}
}


@inproceedings{efros_cvpr11,
   author = "Antonio Torralba and Alexei A. Efros",
   title = "Unbiased Look at Dataset Bias",
   booktitle = "CVPR'11",
   month = "June",
   year = "2011",
} 

@inproceedings{levinkov_iccv13,
title = {Sequential Bayesian Model Update under Structured Scene Prior for Semantic Road Scenes Labeling},
author = {Evgeny Levinkov and Mario Fritz},
year = {2013},
date = {2013-12-03},
booktitle = {IEEE International Conference on Computer Vision (ICCV)},
keywords = {2013, bayesian, iccv},
pubstate = {published},
tppubtype = {inproceedings}
}

@article{LedigTHCATTWS16,
  author    = {Christian Ledig and
               Lucas Theis and
               Ferenc Huszar and
               Jose Caballero},
  title     = {Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial
               Network},
  year      = {2016},
  timestamp = {Tue, 04 Oct 2016 08:10:36 +0200},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}


% BEN BELOW
@article{Onsiderations2017,
archivePrefix = {arXiv},
arxivId = {1611.02200},
author = {Onsiderations, C and Iagnostic, D and Hallenges, C and Ostbiopsy, P and Anagement, C Linical M},
eprint = {1611.02200},
file = {:scratch/download/mendeley/2017/Unsupervised Cross-Domain Image Generation - 2017 - Onsiderations et al.pdf:pdf},
pages = {1--14},
title = {{Unsupervised Cross-Domain Image Generation}},
year = {2017}
}
@article{Cuturi2013,
abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms in the probability simplex. Despite their appealing theoretical properties, excellent performance and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance on the MNIST benchmark problem over competing distances.},
archivePrefix = {arXiv},
arxivId = {arXiv:1306.0895v1},
author = {Cuturi, Marco},
eprint = {arXiv:1306.0895v1},
file = {:scratch/download/mendeley/2013/Sinkhorn Distances Lightspeed Computation of Optimal Transport - 2013 - Cuturi.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 26},
pages = {2292--2300},
title = {{Sinkhorn Distances: Lightspeed Computation of Optimal Transport}},
url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf},
year = {2013}
}
@article{Courty2015,
abstract = {Domain adaptation from one data space (or domain) to another is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a specific data space become more robust when confronted to data depicting the same semantic concepts (the classes), but observed by another observation system with its own specificities. Among the many strategies proposed to adapt a domain to another, finding a common representation has shown excellent properties: by finding a common representation for both domains, a single classifier can be effective in both and use labelled samples from the source domain to predict the unlabelled samples of the target domain. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labelled samples in the source domain to remain close during transport. This way, we exploit at the same time the few labeled information in the source and the unlabelled distributions observed in both domains. Experiments in toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches.},
archivePrefix = {arXiv},
arxivId = {1507.00504},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis and Rakotomamonjy, Alain},
doi = {10.1109/TPAMI.2016.2615921},
eprint = {1507.00504},
file = {:scratch/download/mendeley/2015/Optimal Transport for Domain Adaptation - 2015 - Courty et al.pdf:pdf},
isbn = {9782875870148},
issn = {0162-8828},
journal = {Pattern Analysis and Machine Intelligence, IEEE Transactions on},
number = {X},
pages = {1--14},
title = {{Optimal Transport for Domain Adaptation}},
url = {http://arxiv.org/abs/1507.00504},
volume = {X},
year = {2015}
}
@article{Arjovsky2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1605.07725v2},
author = {Arjovsky, Martin and Bottou, Leon},
eprint = {arXiv:1605.07725v2},
file = {:scratch/download/mendeley/2017/Towards Principled Methods for Training Generative Adversarial Networks - 2017 - Arjovsky, Bottou.pdf:pdf},
title = {{Towards Principled Methods for Training Generative Adversarial Networks}},
year = {2017}
}
@article{Courty2014,
abstract = {We present a new and original method to solve the domain adaptation problem using optimal transport. By searching for the best transportation plan between the probability distribution functions of a source and a target domain, a non-linear and invertible transformation of the learning samples can be estimated. Any standard machine learning method can then be applied on the transformed set, which makes our method very generic. We propose a new optimal transport algorithm that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term. By using the proposed optimal transport with label regularization, we obtain significant increase in performance compared to the original transport solution. The proposed algorithm is computationally efficient and effective, as illustrated by its evaluation on a toy example and a challenging real life vision dataset, against which it achieves competitive results with respect to state-of-the-art methods.},
author = {Courty, Nicolas and Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis},
doi = {10.1007/978-3-662-44848-9_18},
file = {:scratch/download/mendeley/2014/Domain adaptation with regularized optimal transport - 2014 - Courty et al.pdf:pdf},
isbn = {9783662448472},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {274--289},
title = {{Domain adaptation with regularized optimal transport}},
volume = {8724 LNAI},
year = {2014}
}
@article{Mirza,
abstract = {Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels.},
author = {Mirza, Mehdi and Osindero, Simon},
file = {:scratch/download/mendeley/Unknown/Conditional Generative Adversarial Nets - Unknown - Mirza, Osindero.pdf:pdf},
title = {{Conditional Generative Adversarial Nets}}
}
@article{Dziugaite2015,
abstract = {We consider training a deep neural network to generate samples from an unknown distribution given i.i.d. data. We frame learning as an optimization minimizing a two-sample test statistic---informally speaking, a good generator network produces samples that cause a two-sample test to fail to reject the null hypothesis. As our two-sample test statistic, we use an unbiased estimate of the maximum mean discrepancy, which is the centerpiece of the nonparametric kernel two-sample test proposed by Gretton et al. (2012). We compare to the adversarial nets framework introduced by Goodfellow et al. (2014), in which learning is a two-player game between a generator network and an adversarial discriminator network, both trained to outwit the other. From this perspective, the MMD statistic plays the role of the discriminator. In addition to empirical comparisons, we prove bounds on the generalization error incurred by optimizing the empirical MMD.},
archivePrefix = {arXiv},
arxivId = {1505.03906},
author = {Dziugaite, Gintare Karolina and Roy, Daniel M and Ghahramani, Zoubin},
eprint = {1505.03906},
file = {:scratch/download/mendeley/2015/Training generative neural networks via Maximum Mean Discrepancy optimization - 2015 - Dziugaite, Roy, Ghahramani.pdf:pdf},
isbn = {9780000000002},
journal = {Uai},
title = {{Training generative neural networks via Maximum Mean Discrepancy optimization}},
year = {2015}
}
@article{Sun2016,
abstract = {Deep neural networks are able to learn powerful representations from large quantities of labeled input data, however they cannot always generalize well across changes in input distributions. Domain adaptation algorithms have been proposed to compensate for the degradation in performance due to domain shift. In this paper, we address the case when the target domain is unlabeled, requiring unsupervised adaptation. CORAL is a "frustratingly easy" unsupervised domain adaptation method that aligns the second-order statistics of the source and target distributions with a linear transformation. Here, we extend CORAL to learn a nonlinear transformation that aligns correlations of layer activations in deep neural networks (Deep CORAL). Experiments on standard benchmark datasets show state-of-the-art performance.},
archivePrefix = {arXiv},
arxivId = {1607.01719},
author = {Sun, Baochen and Saenko, Kate},
title = {{Deep CORAL: Correlation alignment for deep domain adaptation}},
year = {2016},
journal={ECCV}
}
@article{Mohri,
author = {Mohri, Mehryar and {Cortes Yishay Mansour}, Corinna and Rostamizadeh, Afshin},
file = {:scratch/download/mendeley/Unknown/Domain Adaptation Theory and Algorithms slides - Unknown - Mohri.pdf:pdf},
title = {{Domain Adaptation Theory and Algorithms [slides]}}
}
@article{,
file = {:scratch/download/mendeley/Unknown/AitF FULL Collaborative Research PEARL PErceptual Adaptive Representation Learning in the Wild - Unknown - Unknown.pdf:pdf},
number = {d},
pages = {1--20},
title = {{AitF : FULL : Collaborative Research : PEARL : PErceptual Adaptive Representation Learning in the Wild}},
volume = {1}
}
@article{MeschederLARSMESCHEDER2016,
abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the in-ference model used during training. We in-troduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary dis-criminative network that allows to rephrase the maximum-likelihood-problem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Net-works (GANs). We show that in the nonparamet-ric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact poste-rior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
archivePrefix = {arXiv},
arxivId = {1701.04722},
author = {{Mescheder LARSMESCHEDER}, Lars and {Nowozin SEBASTIANNOWOZIN}, Sebastian and {Geiger ANDREASGEIGER}, Andreas},
eprint = {1701.04722},
file = {:scratch/download/mendeley/2016/Adversarial Variational Bayes Unifying Variational Autoencoders and Generative Adversarial Networks - 2016 - Mescheder LARSMESCHEDER, No.pdf:pdf},
title = {{Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks}},
year = {2016}
}
@article{Gretton2012,
abstract = {We propose a framework for analyzing and comparing distributions, which we use to construct statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS), and is called the maximum mean discrepancy (MMD). We present two distribution-free tests based on large deviation bounds for the MMD, and a third test based on the asymptotic distribution of this statistic. The MMD can be computed in quadratic time, although efficient linear time approximations are available. Our statistic is an instance of an integral probability metric, and various classical metrics on distributions are obtained when alternative function classes are used in place of an RKHS. We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
author = {Gretton, Arthur},
file = {:scratch/download/mendeley/2012/A Kernel Two-Sample Test - 2012 - Gretton.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {723--773},
title = {{A Kernel Two-Sample Test}},
volume = {13},
year = {2012}
}
@article{Taylor2009,
abstract = {The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.},
archivePrefix = {arXiv},
arxivId = {0803.0476},
author = {Taylor, Matthew E and Stone, Peter},
doi = {10.1007/978-3-642-27645-3},
eprint = {0803.0476},
file = {:scratch/download/mendeley/2009/Transfer Learning for Reinforcement Learning Domains A Survey - 2009 - Taylor, Stone.pdf:pdf},
isbn = {15324435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {1,1998,actions with goal,example,leaning agents take sequential,maximizing a reward,multi task learning,problems,reinforcement learning,rl,signal,sutton barto,transfer learning,transfer learning objectives,which may time delayed},
pages = {1633--1685},
pmid = {260529900010},
title = {{Transfer Learning for Reinforcement Learning Domains : A Survey}},
volume = {10},
year = {2009}
}
@article{Luan2017,
abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom CNN layer through which we can backpropagate. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
archivePrefix = {arXiv},
arxivId = {1703.07511},
author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
eprint = {1703.07511},
file = {:scratch/download/mendeley/2017/Deep Photo Style Transfer - 2017 - Cvpr, Id.pdf:pdf},
month = {mar},
title = {{Deep Photo Style Transfer}},
year = {2017}
}
@article{Ruderman2012,
author = {Ruderman, Avraham and Reid, Mark and Garc{\'{i}}a-Garc{\'{i}}a, Dar{\'{i}}o and Petterson, James},
file = {:scratch/download/mendeley/2012/Tighter Variational Representations of f-Divergences via Restriction to Probability Measures - 2012 - Ruderman et al.pdf:pdf},
isbn = {978-1-4503-1285-1},
journal = {Proceedings of the 29th International Conference on Machine Learning (ICML-12)},
pages = {671--678},
title = {{Tighter Variational Representations of f-Divergences via Restriction to Probability Measures}},
year = {2012}
}
@article{Nowozin2016,
abstract = {Generative neural samplers are probabilistic models that implement sampling using feedforward neural networks: they take a random input vector and produce a sample from a probability distribution defined by the network weights. These models are expressive and allow efficient computation of samples and derivatives, but cannot be used for computing likelihoods or for marginalization. The generative-adversarial training method allows to train such models through the use of an auxiliary discriminative neural network. We show that the generative-adversarial approach is a special case of an existing more general variational divergence estimation approach. We show that any f-divergence can be used for training generative neural samplers. We discuss the benefits of various choices of divergence functions on training complexity and the quality of the obtained generative models.},
archivePrefix = {arXiv},
arxivId = {1606.00709},
author = {Nowozin, Sebastian and Cseke, Botond and Tomioka, Ryota},
eprint = {1606.00709},
file = {:scratch/download/mendeley/2016/f-GAN Training Generative Neural Samplers using Variational Divergence Minimization - 2016 - Nowozin, Cseke, Tomioka.pdf:pdf},
journal = {arXiv},
pages = {17},
title = {{f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization}},
year = {2016}
}
@article{Ganin2015,
abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but different distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for effective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains. The approach implements this idea in the context of neural network architectures that are trained on labeled data from the source domain and unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of features that are (i) discriminative for the main learning task on the source domain and (ii) indiscriminate with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation and stochastic gradient descent, and can thus be implemented with little effort using any of the deep learning packages. We demonstrate the success of our approach for two distinct classification problems (document sentiment analysis and image classification), where state-of-the-art domain adaptation performance on standard benchmarks is achieved. We also validate the approach for descriptor learning task in the context of person re-identification application.},
archivePrefix = {arXiv},
arxivId = {1505.07818},
author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
doi = {10.1088/1475-7516/2015/08/013},
eprint = {1505.07818},
file = {:scratch/download/mendeley/2015/Domain-Adversarial Training of Neural Networks - 2015 - Ganin et al.pdf:pdf},
issn = {1475-7516},
pages = {1--35},
title = {{Domain-Adversarial Training of Neural Networks}},
volume = {17},
year = {2015}
}
@article{Galanti2017,
abstract = {When learning a mapping from an input space to an output space, the assumption that the sample distribution of the training data is the same as that of the test data is often violated. Unsupervised domain shift methods adapt the learned function in order to correct for this shift. Previous work has focused on utilizing unlabeled samples from the target distribution. We consider the complementary problem in which the unlabeled samples are given post mapping, i.e., we are given the outputs of the mapping of unknown samples from the shifted domain. Two other variants are also studied: the two sided version, in which unlabeled samples are give from both the input and the output spaces, and the Domain Transfer problem, which was recently formalized. In all cases, we derive generalization bounds that employ discrepancy terms.},
archivePrefix = {arXiv},
arxivId = {1703.01606},
author = {Galanti, Tomer and Wolf, Lior},
eprint = {1703.01606},
file = {:scratch/download/mendeley/2017/A Theory of Output-Side Unsupervised Domain Adaptation - 2017 - Galanti, Wolf.pdf:pdf},
pages = {1--12},
title = {{A Theory of Output-Side Unsupervised Domain Adaptation}},
year = {2017}
}
@article{Vinyals2016,
abstract = {Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6{\%} to 93.2{\%} and from 88.0{\%} to 93.8{\%} on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.},
archivePrefix = {arXiv},
arxivId = {1606.04080},
author = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Kavukcuoglu, Koray and Wierstra, Daan},
doi = {10.1109/CVPR.2016.95},
eprint = {1606.04080},
file = {:scratch/download/mendeley/2016/Matching Networks for One Shot Learning - 2016 - Vinyals et al.pdf:pdf},
isbn = {9781467369640},
issn = {10636919},
journal = {arXiv},
pages = {1--12},
pmid = {24920543},
title = {{Matching Networks for One Shot Learning}},
year = {2016}
}
@article{Kim2017,
abstract = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity.},
archivePrefix = {arXiv},
arxivId = {1703.05192},
author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jungkwon and Kim, Jiwon},
eprint = {1703.05192},
file = {:scratch/download/mendeley/2017/Learning to Discover Cross-Domain Relations with Generative Adversarial Networks - 2017 - Kim et al.pdf:pdf},
title = {{Learning to Discover Cross-Domain Relations with Generative Adversarial Networks}},
year = {2017}
}
@article{Garcia-Garcia2011,
abstract = {We derive a generalized notion of f-divergences, called (f,l)-divergences. We show that this generalization enjoys many of the nice properties of f-divergences, although it is a richer family. It also provides alternative definitions of standard divergences in terms of surrogate risks. As a first practical application of this theory, we derive a new estimator for the Kulback-Leibler divergence that we use for clustering sets of vectors.},
author = {Garcia-Garcia, Dario and Santos-Rodriguez, R and von Luxburg, Ulrike and Santos-Rodr$\backslash$'iiguez, Ra{\'{u}}l},
file = {:scratch/download/mendeley/2011/Risk-based generalizations of f-divergences - 2011 - Garcia-Garcia et al.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Icml},
pages = {417--424},
title = {{Risk-based generalizations of f-divergences}},
year = {2011}
}
@article{Ben-David2007,
abstract = {Discriminative learning methods for classification perform well when training and test data are drawn from the same distribution. In many situations, though, we have labeled training data for a source domain, and we wish to learn a classifier which performs well on a target domain with a different distribution. Under what conditions can we adapt a classifier trained on the source domain for use in the target domain? Intuitively, a good feature representation is a crucial factor in the success of domain adaptation. We formalize this intuition theoretically with a generalization bound for domain adaption. Our theory illustrates the tradeoffs inherent in designing a representation for domain adaptation and gives a new justification for a recently proposed model. It also points toward a promising new model for domain adaptation: one which explicitly minimizes the difference between the source and target domains, while at the same time maximizing the margin of the training set.},
author = {Ben-David, S. and Blitzer, J. and Crammer, K. and Pereira, F.},
file = {:scratch/download/mendeley/2007/Analysis of representations for domain adaptation - 2007 - Ben-David et al.pdf:pdf},
journal = {NIPS},
title = {{Analysis of representations for domain adaptation}},
year = {2007}
}
@article{Rai2010,
abstract = {In this work, we show how active learning in some (target) domain can leverage information from a different but related (source) domain. We present an algorithm that harnesses the source domain data to learn the best possible initializer hypothesis for doing active learning in the target domain, resulting in improved label complexity. We also present a variant of this algorithm which additionally uses the domain divergence information to selectively query the most informative points in the target domain, leading to further reductions in label complexity. Experimental results on a variety of datasets establish the efficacy of the proposed methods.},
author = {Rai, Piyush and Saha, Avishek and Daum, Hal},
file = {:scratch/download/mendeley/2010/Domain Adaptation meets Active Learning - 2010 - Rai, Saha, Daum.pdf:pdf},
journal = {Computational Linguistics},
number = {June},
pages = {27--32},
title = {{Domain Adaptation meets Active Learning}},
year = {2010}
}
@article{Mohria,
author = {Mohri, Mehryar},
file = {:scratch/download/mendeley/Unknown/Domain Adaptation Theory and Algorithms slides - Unknown - Mohri.pdf:pdf},
title = {{Domain Adaptation Theory and Algorithms [slides]}}
}
@article{Liu2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1602.03253v2},
author = {Liu, Qiang and Science, Computer and Lee, Jason D and Jordan, Michael},
eprint = {arXiv:1602.03253v2},
file = {:scratch/download/mendeley/2015/A Kernelized Stein Discrepancy for Goodness-of-fit Tests - 2015 - Liu et al.pdf:pdf},
number = {1},
title = {{A Kernelized Stein Discrepancy for Goodness-of-fit Tests}},
year = {2015}
}
@article{Ganin2015a,
abstract = {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of Ã¢Â€ÂœdeepÃ¢Â€Â features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.},
archivePrefix = {arXiv},
arxivId = {1409.7495v2},
author = {Ganin, Yaroslav and Lempitsky, Victor},
eprint = {1409.7495v2},
file = {:scratch/download/mendeley/2015/Unsupervised Domain Adaptation by Backpropagation - 2015 - Ganin, Lempitsky.pdf:pdf},
isbn = {9781510810587},
journal = {ICML},
title = {{Unsupervised Domain Adaptation by Backpropagation}},
year = {2015}
}
@article{Denton2015,
abstract = {In this paper we introduce a generative parametric model capable of producing high quality samples of natural images. Our approach uses a cascade of convolutional networks within a Laplacian pyramid framework to generate images in a coarse-to-fine fashion. At each level of the pyramid, a separate generative convnet model is trained using the Generative Adversarial Nets (GAN) approach (Goodfellow et al.). Samples drawn from our model are of significantly higher quality than alternate approaches. In a quantitative assessment by human evaluators, our CIFAR10 samples were mistaken for real images around 40{\%} of the time, compared to 10{\%} for samples drawn from a GAN baseline model. We also show samples from models trained on the higher resolution images of the LSUN scene dataset.},
archivePrefix = {arXiv},
arxivId = {1506.05751},
author = {Denton, Emily and Chintala, Soumith and Szlam, Arthur and Fergus, Rob},
eprint = {1506.05751},
file = {:scratch/download/mendeley/2015/Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks - 2015 - Denton et al.pdf:pdf},
isbn = {1505.05770},
issn = {10495258},
journal = {Arxiv},
pages = {1--10},
title = {{Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks}},
year = {2015}
}
@article{Borgwardt2006,
abstract = {MOTIVATION: Many problems in data integration in bioinformatics can be posed as one common question: Are two sets of observations generated by the same distribution? We propose a kernel-based statistical test for this problem, based on the fact that two distributions are different if and only if there exists at least one function having different expectation on the two distributions. Consequently we use the maximum discrepancy between function means as the basis of a test statistic. The Maximum Mean Discrepancy (MMD) can take advantage of the kernel trick, which allows us to apply it not only to vectors, but strings, sequences, graphs, and other common structured data types arising in molecular biology. RESULTS: We study the practical feasibility of an MMD-based test on three central data integration tasks: Testing cross-platform comparability of microarray data, cancer diagnosis, and data-content based schema matching for two different protein function classification schemas. In all of these experiments, including high-dimensional ones, MMD is very accurate in finding samples that were generated from the same distribution, and outperforms its best competitors. Conclusions: We have defined a novel statistical test of whether two samples are from the same distribution, compatible with both multivariate and structured data, that is fast, easy to implement, and works well, as confirmed by our experiments. AVAILABILITY: http://www.dbs.ifi.lmu.de/{\~{}}borgward/MMD. http://www.ncbi.nlm.nih.gov/pubmed/16873512},
author = {Borgwardt, Karsten M and Gretton, Arthur and Rasch, Malte J and Kriegel, Hans-Peter and Sch{\"{o}}lkopf, Bernhard and Smola, Alexander J},
doi = {10.1093/bioinformatics/btl242},
file = {:scratch/download/mendeley/2006/Maximum Mean Discrepancy - 2006 - Borgwardt et al.pdf:pdf},
issn = {13674811},
journal = {Bioinformatics},
number = {14},
pages = {49--57},
pmid = {16873512},
title = {{Maximum Mean Discrepancy}},
volume = {22},
year = {2006}
}
@article{Mathieu2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1611.03383v1},
author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and Lecun, Yann},
eprint = {arXiv:1611.03383v1},
file = {:scratch/download/mendeley/2016/Disentangling factors of variation in deep representations using adversarial training - 2016 - Mathieu et al.pdf:pdf},
number = {NIPS},
title = {{Disentangling factors of variation in deep representations using adversarial training}},
year = {2016}
}
@article{Li2015,
abstract = {We consider the problem of learning deep generative models from data. We formulate a method that generates an independent sample via a single feedforward pass through a multilayer preceptron, as in the recently proposed generative adversarial networks (Goodfellow et al., 2014). Training a generative adversarial network, however, requires careful optimization of a difficult minimax program. Instead, we utilize a technique from statistical hypothesis testing known as maximum mean discrepancy (MMD), which leads to a simple objective that can be interpreted as matching all orders of statistics between a dataset and samples from the model, and can be trained by backpropagation. We further boost the performance of this approach by combining our generative network with an auto-encoder network, using MMD to learn to generate codes that can then be decoded to produce samples. We show that the combination of these techniques yields excellent generative models compared to baseline approaches as measured on MNIST and the Toronto Face Database.},
archivePrefix = {arXiv},
arxivId = {arXiv:1502.02761v1},
author = {Li, Yujia and Swersky, Kevin and Zemel, Rich},
file = {:scratch/download/mendeley/2015/Generative Moment Matching Networks - 2015 - Li, Swersky, Zemel.pdf:pdf},
isbn = {9781510810587},
journal = {ICML},
title = {{Generative Moment Matching Networks}},
year = {2015}
}
@article{Jaech2016,
abstract = {The goal of this paper is to use multi-task learning to efficiently scale slot filling models for natural language understanding to handle multiple target tasks or domains. The key to scalability is reducing the amount of training data needed to learn a model for a new task. The proposed multi-task model delivers better performance with less data by leveraging patterns that it learns from the other tasks. The approach supports an open vocabulary, which allows the models to generalize to unseen words, which is particularly important when very little training data is used. A newly collected crowd-sourced data set, covering four different domains, is used to demonstrate the effectiveness of the domain adaptation and open vocabulary techniques.},
archivePrefix = {arXiv},
arxivId = {1604.00117},
author = {Jaech, Aaron and Heck, Larry and Ostendorf, Mari},
eprint = {1604.00117},
file = {:scratch/download/mendeley/2016/Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding - 2016 - Jaech, Heck, Ostendorf.pdf:pdf},
journal = {Acl},
keywords = {Domain Adaptation,RNN,Semantics},
pages = {2--6},
title = {{Domain Adaptation of Recurrent Neural Networks for Natural Language Understanding}},
year = {2016}
}
@article{Patel2015,
abstract = {In pattern recognition and computer vision, one is often faced with scenarios where the training data used to learn a model has different distribution from the data on which the model is applied. Regardless of the cause, any distributional change that occurs after learning a classifier can degrade its performance at test time. Domain adaptation tries to mitigate this degradation. In this paper, we provide a survey of domain adaptation methods for visual recognition. We discuss the merits and drawbacks of existing domain adaptation approaches and identify promising avenues for research in this rapidly evolving field.},
author = {Patel, VM and Gopalan, Raghuraman},
doi = {10.1109/MSP.2014.2347059},
file = {:scratch/download/mendeley/2015/Visual Domain Adaptation A survey of recent advances - 2015 - Patel, Gopalan.pdf:pdf},
issn = {1053-5888},
journal = {Signal Processing {\ldots}},
pages = {1--36},
title = {{Visual Domain Adaptation: A survey of recent advances}},
volume = {02138},
year = {2015}
}
@inproceedings{Courty2014a,
abstract = {We present a new and original method to solve the domain adaptation problem using optimal transport. By searching for the best transportation plan between the probability distribution functions of a source and a target domain, a non-linear and invertible transformation of the learning samples can be estimated. Any standard machine learning method can then be applied on the transformed set, which makes our method very generic. We propose a new optimal transport algorithm that incorporates label information in the optimization: this is achieved by combining an efficient matrix scaling technique together with a majoration of a non-convex regularization term. By using the proposed optimal transport with label regularization, we obtain significant increase in performance compared to the original transport solution. The proposed algorithm is computationally efficient and effective, as illustrated by its evaluation on a toy example and a challenging real life vision dataset, against which it achieves competitive results with respect to state-of-the-art methods.},
author = {Courty, Nicolas and Flamary, R{\'{e}}mi and Tuia, Devis},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-662-44848-9_18},
isbn = {9783662448472},
issn = {16113349},
number = {PART 1},
pages = {274--289},
title = {{Domain adaptation with regularized optimal transport}},
volume = {8724 LNAI},
year = {2014}
}
@article{Dai2017,
abstract = {Despite the substantial progress in recent years, the image captioning techniques are still far from being perfect.Sentences produced by existing methods, e.g. those based on RNNs, are often overly rigid and lacking in variability. This issue is related to a learning principle widely used in practice, that is, to maximize the likelihood of training samples. This principle encourages high resemblance to the "ground-truth" captions while suppressing other reasonable descriptions. Conventional evaluation metrics, e.g. BLEU and METEOR, also favor such restrictive methods. In this paper, we explore an alternative approach, with the aim to improve the naturalness and diversity -- two essential properties of human expression. Specifically, we propose a new framework based on Conditional Generative Adversarial Networks (CGAN), which jointly learns a generator to produce descriptions conditioned on images and an evaluator to assess how well a description fits the visual content. It is noteworthy that training a sequence generator is nontrivial. We overcome the difficulty by Policy Gradient, a strategy stemming from Reinforcement Learning, which allows the generator to receive early feedback along the way. We tested our method on two large datasets, where it performed competitively against real people in our user study and outperformed other methods on various tasks.},
archivePrefix = {arXiv},
arxivId = {1703.06029},
author = {Dai, Bo and Lin, Dahua and Urtasun, Raquel and Fidler, Sanja},
eprint = {1703.06029},
file = {:scratch/download/mendeley/2017/Towards Diverse and Natural Image Descriptions via a Conditional GAN - 2017 - Dai et al.pdf:pdf},
title = {{Towards Diverse and Natural Image Descriptions via a Conditional GAN}},
year = {2017}
}
@article{Sutherland2016,
abstract = {We propose a method to optimize the representation and distinguishability of samples from two probability distributions, by maximizing the estimated power of a statistical test based on the maximum mean discrepancy (MMD). This optimized MMD is applied to the setting of unsupervised learning by generative adversarial networks (GAN), in which a model attempts to generate realistic samples, and a discriminator attempts to tell these apart from data samples. In this context, the MMD may be used in two roles: first, as a discriminator, either directly on the samples, or on features of the samples. Second, the MMD can be used to evaluate the performance of a generative model, by testing the model's samples against a reference data set. In the latter role, the optimized MMD is particularly helpful, as it gives an interpretable indication of how the model and data distributions differ, even in cases where individual model samples are not easily distinguished either by eye or by classifier.},
archivePrefix = {arXiv},
arxivId = {1611.04488},
author = {Sutherland, Dougal J. and Tung, Hsiao-Yu and Strathmann, Heiko and De, Soumyajit and Ramdas, Aaditya and Smola, Alex and Gretton, Arthur},
eprint = {1611.04488},
file = {:scratch/download/mendeley/2016/Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy - 2016 - Sutherland et al.pdf:pdf},
isbn = {9783901882760},
number = {2008},
pages = {1--13},
title = {{Generative Models and Model Criticism via Optimized Maximum Mean Discrepancy}},
year = {2016}
}
@article{Dong2017,
abstract = {It's useful to automatically transform an image from its original form to some synthetic form (style, partial contents, etc.), while keeping the original structure or semantics. We define this requirement as the "image-to-image translation" problem, and propose a general approach to achieve it, based on deep convolutional and conditional generative adversarial networks (GANs), which has gained a phenomenal success to learn mapping images from noise input since 2014. In this work, we develop a two step (unsupervised) learning method to translate images between different domains by using unlabeled images without specifying any correspondence between them, so that to avoid the cost of acquiring labeled data. Compared with prior works, we demonstrated the capacity of generality in our model, by which variance of translations can be conduct by a single type of model. Such capability is desirable in applications like bidirectional translation},
archivePrefix = {arXiv},
arxivId = {1701.02676},
author = {Dong, Hao and Neekhara, Paarth and Wu, Chao and Guo, Yike},
eprint = {1701.02676},
title = {Unsupervised Image-to-Image Translation with Generative Adversarial Networks},
year = {2017}
}
@article{Zhu2017,
abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain {\$}X{\$} to a target domain {\$}Y{\$} in the absence of paired examples. Our goal is to learn a mapping {\$}G: X \backslashrightarrow Y{\$} such that the distribution of images from {\$}G(X){\$} is indistinguishable from the distribution {\$}Y{\$} using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping {\$}F: Y \backslashrightarrow X{\$} and introduce a cycle consistency loss to push {\$}F(G(X)) \backslashapprox X{\$} (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
archivePrefix = {arXiv},
arxivId = {1703.10593},
author = {Zhu, Jun and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
eprint = {1703.10593},
file = {:scratch/download/mendeley/2017/Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks - 2017 - Zhu et al.pdf:pdf},
title = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
journal={arXiv},
year = {2017}
}
@article{Nguyen2017,
abstract = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. [36] showed one interesting way to synthesize novel images by performing gradient ascent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227 ÃƒÂ— 227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models Ã¢Â€ÂœPlug and Play Generative Networks.Ã¢Â€Â PPGNs are composed of 1) a generator network G that is capable of drawing a wide range of image types and 2) a replaceable Ã¢Â€ÂœconditionÃ¢Â€Â network C that tells the generator what to draw. We demonstrate the generation of images conditioned on a class (when C is an ImageNet or MIT Places classification network) and also conditioned on a caption (when C is an image captioning network). Our method also improves the state of the art of Multifaceted Feature Visualization [39], which generates the set of synthetic inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While image models are used in this paper, the approach is modality-agnostic and can be applied to many types of data.},
archivePrefix = {arXiv},
arxivId = {arXiv:submit/1738978},
author = {Nguyen, Anh and Yosinski, Jason and Bengio, Yoshua and Dosovitskiy, Alexey and Clune, Jeff},
eprint = {1738978},
file = {:scratch/download/mendeley/2017/Plug {\&} Play Generative Networks Conditional Iterative Generation of Images in Latent Space - 2017 - Nguyen et al.pdf:pdf},
journal = {Iccv},
number = {3},
primaryClass = {arXiv:submit},
title = {{Plug {\&} Play Generative Networks: Conditional Iterative Generation of Images in Latent Space}},
year = {2017}
}
@article{Saha2011,
abstract = {In this paper, we harness the synergy between two important learning$\backslash$nparadigms, namely, active learning and domain adaptation. We show$\backslash$nhow active learning in a target domain can leverage information from$\backslash$na different but related source domain. Our proposed framework, Active$\backslash$nLearning Domain Adapted (Alda), uses source domain knowledge to transfer$\backslash$ninformation that facilitates active learning in the target domain.$\backslash$nWe propose two variants of Alda: a batch B-Alda and an online O-Alda.$\backslash$nEmpirical comparisons with numerous baselines on real-world datasets$\backslash$nestablish the efficacy of the proposed methods.},
author = {Saha, Avishek and Rai, Piyush and Daum{\'{e}}, Hal and Venkatasubramanian, Suresh and DuVall, Scott L.},
doi = {10.1007/978-3-642-23808-6_7},
file = {:scratch/download/mendeley/2011/Active supervised domain adaptation - 2011 - Saha et al.pdf:pdf},
isbn = {9783642238079},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {active learning,batch,domain adaptation,online},
number = {PART 3},
pages = {97--112},
title = {{Active supervised domain adaptation}},
volume = {6913 LNAI},
year = {2011}
}
@article{To,
author = {To, Thanks and Borgwardt, Karsten and Rasch, Malte and Sch{\"{o}}lkopf, Bernhard and Huang, Jiayuan and Gretton, Arthur and Smola, Alexander J},
title = {{Maximum Mean Discrepancy}}
}
@article{Sener2016,
abstract = {Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions. Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin. We will make our learned models as well as the source code available immediately upon acceptance.},
author = {Sener, Ozan and Song, Hyun Oh and Saxena, Ashutosh and Savarese, Silvio},
file = {:scratch/download/mendeley/2016/Learning Transferrable Representations for Unsupervised Domain Adaptation - 2016 - Sener et al.pdf:pdf},
journal = {Nips},
number = {Nips},
pages = {2110--2118},
title = {{Learning Transferrable Representations for Unsupervised Domain Adaptation}},
year = {2016}
}
@article{Long2016,
abstract = {The recent success of deep neural networks relies on massive amounts of labeled data. For a target task where labeled data is unavailable, domain adaptation can transfer a learner from a different source domain. In this paper, we propose a new approach to domain adaptation in deep networks that can jointly learn adaptive classifiers and transferable features from labeled data in the source domain and unlabeled data in the target domain. We relax a shared-classifier assumption made by previous methods and assume that the source classifier and target classifier differ by a residual function. We enable classifier adaptation by plugging several layers into deep network to explicitly learn the residual function with reference to the target classifier. We fuse features of multiple layers with tensor product and embed them into reproducing kernel Hilbert spaces to match distributions for feature adaptation. The adaptation can be achieved in most feed-forward models by extending them with new residual layers and loss functions, which can be trained efficiently via back-propagation. Empirical evidence shows that the new approach outperforms state of the art methods on standard domain adaptation benchmarks.},
archivePrefix = {arXiv},
arxivId = {1602.04433},
author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
eprint = {1602.04433},
file = {:scratch/download/mendeley/2016/Unsupervised Domain Adaptation with Residual Transfer Networks - 2016 - Long et al.pdf:pdf},
journal = {Nips},
keywords = {Deep learning, domain adaptation, residual learnin},
pages = {136--144},
pmid = {199870},
title = {{Unsupervised Domain Adaptation with Residual Transfer Networks}},
year = {2016}
}
@article{Hoffman2016,
abstract = {Fully convolutional models for dense prediction have proven successful for a wide range of visual tasks. Such models perform well in a supervised setting, but performance can be surprisingly poor under domain shifts that appear mild to a human observer. For example, training on one city and testing on another in a different geographic region and/or weather condition may result in significantly degraded performance due to pixel-level distribution shift. In this paper, we introduce the first domain adaptive semantic segmentation method, proposing an unsupervised adversarial approach to pixel prediction problems. Our method consists of both global and category specific adaptation techniques. Global domain alignment is performed using a novel semantic segmentation network with fully convolutional domain adversarial learning. This initially adapted space then enables category specific adaptation through a generalization of constrained weak learning, with explicit transfer of the spatial layout from the source to the target domains. Our approach outperforms baselines across different settings on multiple large-scale datasets, including adapting across various real city environments, different synthetic sub-domains, from simulated to real environments, and on a novel large-scale dash-cam dataset.},
archivePrefix = {arXiv},
arxivId = {1612.02649},
author = {Hoffman, Judy and Wang, Dequan and Yu, Fisher and Darrell, Trevor},
eprint = {1612.02649},
file = {:scratch/download/mendeley/2016/FCNs in the Wild Pixel-level Adversarial and Constraint-based Adaptation - 2016 - Hoffman et al.pdf:pdf},
journal = {Arxiv},
title = {{FCNs in the Wild: Pixel-level Adversarial and Constraint-based Adaptation}},
year = {2016}
}
@article{Jiang2008,
author = {Jiang, Jing},
file = {:scratch/download/mendeley/2008/A literature survey on domain adaptation of statistical classifiers - 2008 - Jiang.pdf:pdf},
journal = {://Sifaka. Cs. Uiuc. Edu/Jiang4/Domainadaptation/Survey},
number = {March},
pages = {1--12},
title = {{A literature survey on domain adaptation of statistical classifiers}},
year = {2008}
}
@article{Goodfellow2016,
abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
archivePrefix = {arXiv},
arxivId = {1701.00160},
author = {Goodfellow, Ian},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {1701.00160},
file = {:scratch/download/mendeley/2016/NIPS 2016 Tutorial Generative Adversarial Networks - 2016 - Goodfellow.pdf:pdf},
isbn = {1581138285},
issn = {0253-0465},
pmid = {15040217},
title = {{NIPS 2016 Tutorial: Generative Adversarial Networks}},
year = {2016}
}
@article{Tzeng,
abstract = {Recent reports suggest that a generic supervised deep CNN model trained on a large-scale dataset reduces, but does not remove, dataset bias. Fine-tuning deep models in a new domain can require a significant amount of labeled data, which for many applications is simply not available. We propose a new CNN architecture to exploit unlabeled and sparsely labeled target domain data. Our approach simulta-neously optimizes for domain invariance to facilitate domain transfer and uses a soft label distribution matching loss to transfer information between tasks. Our proposed adapta-tion method offers empirical performance which exceeds previously published results on two standard benchmark vi-sual domain adaptation tasks, evaluated across supervised and semi-supervised adaptation settings.},
archivePrefix = {arXiv},
arxivId = {1510.02192},
author = {Tzeng, Eric and Hoffman, Judy and Darrell, Trevor and Saenko, Kate and Lowell, Umass},
doi = {10.1109/ICCV.2015.463},
eprint = {1510.02192},
file = {:scratch/download/mendeley/Unknown/Simultaneous Deep Transfer Across Domains and Tasks - Unknown - Tzeng et al.pdf:pdf},
isbn = {978-1-4673-8391-2},
title = {{Simultaneous Deep Transfer Across Domains and Tasks}}
}
@article{Tzeng2016,
author = {Tzeng, Eric and Hoffman, Judy and Saenko, Kate and Darrell, Trevor},
file = {:scratch/download/mendeley/2016/Adversarial discriminative domain adaptation - 2016 - Tzeng et al(2).pdf:pdf},
pages = {1--4},
title = {{Adversarial discriminative domain adaptation}},
year = {2016}
}
@article{Boesen2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1512.09300v2},
author = {Boesen, Anders and Larsen, Lindbo and Larochelle, Hugo and Winther, Ole and Science, Computer and Centre, Bioinformatics},
eprint = {arXiv:1512.09300v2},
file = {:scratch/download/mendeley/2014/Autoencoding beyond pixels using a learned similarity metric - 2014 - Boesen et al.pdf:pdf},
title = {{Autoencoding beyond pixels using a learned similarity metric}},
year = {2014}
}
@article{Gupta2017,
abstract = {People can learn a wide range of tasks from their own experience, but can also learn from observing other creatures. This can accelerate acquisition of new skills even when the observed agent differs substantially from the learning agent in terms of morphology. In this paper, we examine how reinforcement learning algorithms can transfer knowledge between morphologically different agents (e.g., different robots). We introduce a problem formulation where two agents are tasked with learning multiple skills by sharing information. Our method uses the skills that were learned by both agents to train invariant feature spaces that can then be used to transfer other skills from one agent to another. The process of learning these invariant feature spaces can be viewed as a kind of "analogy making", or implicit learning of partial correspondences between two distinct domains. We evaluate our transfer learning algorithm in two simulated robotic manipulation skills, and illustrate that we can transfer knowledge between simulated robotic arms with different numbers of links, as well as simulated arms with different actuation mechanisms, where one robot is torque-driven while the other is tendon-driven.},
archivePrefix = {arXiv},
arxivId = {1703.02949},
author = {Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
eprint = {1703.02949},
file = {:scratch/download/mendeley/2017/Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning - 2017 - Gupta et al.pdf:pdf},
keywords = {binaries,pho-,planetary systems,radial velocities,spectroscopic,techniques},
month = {mar},
title = {{Learning Invariant Feature Spaces to Transfer Skills with Reinforcement Learning}},
year = {2017}
}
@article{Csurka2017,
abstract = {The aim of this paper is to give an overview of domain adaptation and transfer learning with a specific view on visual applications. After a general motivation, we first position domain adaptation in the larger transfer learning problem. Second, we try to address and analyze briefly the state-of-the-art methods for different types of scenarios, first describing the historical shallow methods, addressing both the homogeneous and the heterogeneous domain adaptation methods. Third, we discuss the effect of the success of deep convolutional architectures which led to new type of domain adaptation methods that integrate the adaptation within the deep architecture. Fourth, we overview the methods that go beyond image categorization, such as object detection or image segmentation, video analyses or learning visual attributes. Finally, we conclude the paper with a section where we relate domain adaptation to other machine learning solutions.},
archivePrefix = {arXiv},
arxivId = {1702.05374},
author = {Csurka, Gabriela},
eprint = {1702.05374},
file = {:scratch/download/mendeley/2017/Domain Adaptation for Visual Applications A Comprehensive Survey - 2017 - Csurka.pdf:pdf},
pages = {1--46},
title = {{Domain Adaptation for Visual Applications: A Comprehensive Survey}},
year = {2017}
}
@article{Adel2014,
abstract = {The aim of domain adaptation algorithms is to establish a learner, trained on labeled data from a source domain, that can classify samples from a target domain, in which few or no labeled data are available for training. Covariate shift, a primary assumption in several works on domain adaptation, assumes that the labeling functions of source and target domains are identical. We present a domain adaptation algorithm that assumes a relaxed version of covariate shift where the assumption that the labeling functions of the source and target domains are identical holds with a certain probability. Assuming a source deterministic large margin binary classifier, the farther a target instance is from the source decision boundary, the higher the probability that covariate shift holds. In this context, given a target unlabeled sample and no target labeled data, we develop a domain adaptation algorithm that bases its labeling decisions both on the source learner and on the similarities between the target unlabeled instances. The source labeling function decisions associated with probabilistic covariate shift, along with the target similarities are concurrently expressed on a similarity graph.We evaluate our proposed algorithm on a benchmark sentiment analysis (and domain adaptation) dataset, where state-of-the-art adaptation results are achieved. We also derive a lower bound on the performance of the algorithm.},
author = {Adel, Tameem and Wong, Alexander},
file = {:scratch/download/mendeley/2014/A Probabilistic Covariate Shift Assumption for Domain Adaptation - 2014 - Adel, Wong.pdf:pdf},
isbn = {9781577357025},
journal = {Aaai},
keywords = {Dataset: Amazon sentiment review},
pages = {2476--2482},
title = {{A Probabilistic Covariate Shift Assumption for Domain Adaptation}},
year = {2014}
}
@article{Liu2016,
abstract = {We propose the coupled generative adversarial network (CoGAN) framework for generating pairs of corresponding images in two different domains. It consists of a pair of generative adversarial networks, each responsible for generating images in one domain. We show that by enforcing a simple weight-sharing constraint, the CoGAN learns to generate pairs of corresponding images without existence of any pairs of corresponding images in the two domains in the training set. In other words, the CoGAN learns a joint distribution of images in the two domains from images drawn separately from the marginal distributions of the individual domains. This is in contrast to the existing multi-modal generative models, which require corresponding images for training. We apply the CoGAN to several pair image generation tasks. For each task, the GoGAN learns to generate convincing pairs of corresponding images. We further demonstrate the applications of the CoGAN framework for the domain adaptation and cross-domain image generation tasks.},
archivePrefix = {arXiv},
author = {Liu, Ming-Yu and Tuzel, Oncel},
eprint = {1606.07536},
file = {:scratch/download/mendeley/2016/Coupled Generative Adversarial Networks - 2016 - Liu, Tuzel.pdf:pdf},
journal = {NIPS},
title = {{Coupled Generative Adversarial Networks}},
year = {2016}
}
@article{DaumeIII2016,
abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough " target " data to do slightly better than just using only " source " data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
archivePrefix = {arXiv},
arxivId = {0907.1815},
author = {{Daum{\'{e}} III}, Hal},
doi = {10.1.1.110.2062},
eprint = {0907.1815},
file = {:scratch/download/mendeley/2016/Frustratingly Easy Neural Domain Adaptation - 2016 - Daum{\'{e}} III.pdf:pdf},
isbn = {0736587X},
issn = {0736587X},
title = {{Frustratingly Easy Neural Domain Adaptation}},
year = {2016}
}
@article{Isola,
abstract = {Labels to Facade BW to Color Aerial to Map Labels to Street Scene Edges to Photo input output input input input input output output output output input output Day to Night Figure 1: Many problems in image processing, graphics, and vision involve translating an input image into a corresponding output image. These problems are often treated with application-specific algorithms, even though the setting is always the same: map pixels to pixels. Conditional adversarial nets are a general-purpose solution that appears to work well on a wide variety of these problems. Here we show results of the method on several. In each case we use the same architecture and objective, and simply train on different data. Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss func-tion to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demon-strate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a commu-nity, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
file = {:scratch/download/mendeley/Unknown/Image-to-Image Translation with Conditional Adversarial Networks - Unknown - Isola et al.pdf:pdf},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}}
}
@article{Bousmalis2016,
abstract = {Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.},
archivePrefix = {arXiv},
arxivId = {1612.05424},
author = {Bousmalis, Konstantinos and Silberman, Nathan and Dohan, David and Erhan, Dumitru and Krishnan, Dilip},
eprint = {1612.05424},
file = {:scratch/download/mendeley/2016/Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks - 2016 - Bousmalis et al.pdf:pdf},
title = {{Unsupervised pixel-level domain adaptation with generative adversarial networks}},
year = {2017}
}


%% MANUAL

@article{minka2003comparison,
  title={A comparison of numerical optimizers for logistic regression},
  author={Minka, Thomas P},
  year={2003}
}

@article{green1984iteratively,
  title={Iteratively reweighted least squares for maximum likelihood estimation, and some robust and resistant alternatives},
  author={Green, Peter J},
  journal={Journal of the Royal Statistical Society. Series B (Methodological)},
  pages={149--192},
  year={1984},
  publisher={JSTOR}
}

@article{goodfellow2016nips,
  title={NIPS 2016 Tutorial: Generative Adversarial Networks},
  author={Goodfellow, Ian},
  journal={arXiv preprint arXiv:1701.00160},
  year={2016}
}

@inproceedings{takac2013mini,
  title={Mini-Batch Primal and Dual Methods for SVMs.},
  author={Tak{\'a}c, Martin and Bijral, Avleen Singh and Richt{\'a}rik, Peter and Srebro, Nati}
}

@article{bauschke_borwein,
author="H. H. Bauschke and J. M. Borwein",
title="Joint and Separate Convexity of the {B}regman Distance",
journal="Studies in Computational Mathematics",
volume="8",
pages="23--36",
year="2001"
}

@article{dat,
author="Y. Ganin and E. Ustinova and H. Ajakan and P. Germain and H. Larochelle and F. Laviolette an M. Marchand and V. Lempitsky",
title="Domain-Adversarial Training of Neural Networks",
journal="Journal of Machine Learning Research",
volume="17",
pages="1--35",
year="2016"
}

@inproceedings{davis_clustering,
author="J. Davis and I. S. Dhillon",
title="Differential Entropic Clustering of Multivariate {G}aussians",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2006"
}

@article{functional_bd,
author="B. A. Frigyik and S. Srivastava and M. R. Gupta",
title="Functional {B}regman Divergences and {B}ayesian Estimation of Distributions",
journal="IEEE Transactions on Information Theory",
volume="54",
number="11",
year="2008",
pages="5130--5139"
}

@misc{siahkamari_arxiv,
author="A. Siahkamari and V. Saligrama and D. Castanon and B. Kulis",
title="Learning {B}regman Divergences",
year="2019",
note="arXiv:1905.11545"
}


@inproceedings{tong_koller,
author="S. Tong and D. Koller",
title="Restricted {B}ayes Optimal Classifiers",
booktitle="Proc. 17th AAAI Conference",
year="2000"
}


@inproceedings{Erik12,
 author    = {E. Sudderth},
 title     = {Toward reliable {B}ayesian nonparametric learning},
 year      = {2012},
 booktitle     = {NIPS Workshop on Bayesian Noparametric Models for Reliable Planning
 		and Decision-Making Under Uncertainty}
}

@inproceedings{SVEP,
 author    = {K. Jiang and B. Kulis and M.~I. Jordan},
 title     = {Small-variance asymptotics for exponential family {D}irichlet process mixture models},
 year      = {2012},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{BeamSampler,
 author    = {J.~V. Gael and Y. Saatci and Y.~W. Teh and Z. Ghahramani},
 title     = {Beam Sampling for the Infinite Hidden {M}arkov Model},
 year      = {2008},
 booktitle     = {Proceedings of the 25th International Conference
              on Machine Learning}
}

@inproceedings{RevisitKmeans,
 author    = {B. Kulis and M.~I. Jordan},
 title     = {Revisiting k-means: New algorithms via {B}ayesian nonparametrics},
 year      = {2012},
 booktitle     = {Proceedings of the 29th International Conference
              on Machine Learning}
}

@inproceedings{SmallVariance,
 author    = {K. Jiang and B. Kulis and M.~I. Jordan},
 title     = {Small-variance asymptotics for exponential family {D}irichlet process mixture models},
 year      = {2012},
 booktitle     = {Advances in neural information processing systems}
}

@inproceedings{iHMM,
 author    = {M.~J. Beal and Z. Ghahramani and C.~E. Rasmussen},
 title     = {The infinite hidden {M}arkov model},
 year      = {2002},
 booktitle     = {Advances in neural information processing systems}
}

@Article{HDP,
  author = 	 "Y.~W. Teh and M.~I. Jordan and M.~J. Beal and D.~M. Blei",
  title = 	 "Hierarchical {D}irichlet Processes",
  journal =	 "Journal of the American Statistical Association",
  year =	 "2006",
  volume =	 "101",
  number =	 "476",
  pages =	 "1566--1581"
}

@Article{Antoniak74,
  author = 	 "C.~E. Antoniak",
  title = 	 "Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems",
  journal =	 "The Annals of Statistics",
  year =	 "1974",
  volume =	 "2",
  number =	 "6",
  pages =	 "1152--1174"
}

@Article{Rabiner89,
  author = 	 "L.~R. Rabiner",
  title = 	 "A tutorial on hidden {M}arkov models and selected applications in speech recognition",
  journal =	 "Proceedings of the IEEE",
  year =	 "1989",
  volume =	 "77",
  number = "2",
  pages =	 "257--286"
}

@Article{Leroux92,
  author = 	 "B.~G. Leroux",
  title = 	 "Maximum-likelihood estimation for hidden {M}arkov models",
  journal =	 "Stochastic Processes and their Applications",
  year =	 "1992",
  volume =	 "40",
  number = "1",
  pages =	 "127--143"
}

@Article{Ryden95,
  author = 	 "T. Ryd\'en",
  title = 	 "Estimating the order of hidden {M}arkov models",
  journal =	 "Statistics",
  year =	 "1995",
  volume =	 "26",
  pages =	 "345--354"
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2011},
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@book{prml,
    author    = "Christopher M. Bishop",
    title     = "Pattern Recognition and Machine Learning",
    publisher = "Springer",
    year      = "2006",
}

@Article{stick,
  author = 	 "J. Sethuraman",
  title = 	 "A constructive definition of dirichlet priors",
  journal =	 "Statistica Sinica",
  year =	 "1994",
  volume =	 "4",
  pages =	 "639--650"
}

@Article{scott,
  author = 	 "S. L. Scott",
  title = 	 "Bayesian methods for hidden Markov models: Recursive computing in the 21st century",
  journal =	 "Journal of the American Statistical Association",
  year =	 "2002",
  volume =	 "97",
  pages =	 "337--351"
}

@book{wellog,
    author    = "J. Ruanaidh and W. J. Fitzgerald",
    title     = "Numerical {B}ayesian methods applied to signal processing",
    publisher = "Springer-Verlag New York Inc",
    year      = "1996",
}

@Article{Bregman,
  author = 	 "A. Banerjee and S. Merugu and I.~S. Dhillon and J. Ghosh",
  title = 	 "Clustering with {B}regman divergences",
  journal =	 "Journal of Machine Learning Research",
  year =	 "2005",
  volume =	 "6",
  pages =	 "1705--1749"
}

@Article{PPCA,
  author = 	 "M.~E. Tipping and C.~M. Bishop",
  title = 	 "Probabilistic principal component analysis",
  journal =	 "Journal of Royal Statistical Society, Series B",
  year =	 "1999",
  volume =	 "21",
  number = "3",
  pages =	 "611--622"
}

@inproceedings{Roweis98,
 author    = {S. Roweis},
 title     = {E{M} algorithms for {PCA} and {SPCA}},
 year      = {1998},
 booktitle     = {Advances in Neural Information Processing Systems}
}

@inproceedings{MADBayes,
  author = 	 "T. Broderick and B. Kulis and M.~I. Jordan",
  title = 	 "M{AD}-{Bayes}: {MAP}-based asymptotic derivations from {B}ayes",
  year = {2013},
  booktitle = {Proceedings of the 30th International Conference
              on Machine Learning},
}

@inproceedings{GamPois,
 author = {M. Titsias},
 title = "The {I}nfinite {G}amma-{P}oisson {M}odel",
 booktitle = {Advances in Neural Information Processing Systems},
 year = {2008},
}

@InCollection{crmjord,
 author = {M. I. Jordan},
 title = "Hierarchical {M}odels, {N}ested {M}odels and {C}ompletely {R}andom {M}easures",
 booktitle = {Frontiers of Statistical Decision Making and Bayesian Analysis: In Honor of James O. Berger},
 publisher = {New York: Springer},
 year = {2010},
 editor = {M.-H. Chen and D. Dey and P. Mueller and D. Sun and K. Ye},
}

@Article{crmorig,
 author = {J.F.C. Kingman},
 title = "Completely {R}andom {M}easures",
 journal = {Pacific Journal of Mathematics},
 year = {1967},
 OPTkey = {},
 volume = {21},
 number = {1},
 pages = {59-78},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@InCollection{wolp,
 author = {R. Wolpert and K. Ickstadt},
 title = "Simulation of {L}\'{e}vy {R}andom {F}ields",
 booktitle = {Practical Nonparametric and Semiparametric Bayesian Statistics},
 year = {1998},
 publisher = {Springer-Verlag},
 OPTkey = {},
 OPTvolume = {},
 OPTnumber = {},
 OPTpages = {},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@PhdThesis{thithesis,
 author = {R.J. Thibaux},
 title = {Nonparametric Bayesian Models for Machine Learning},
 school = {University of California at Berkeley},
 year = {2008},
 OPTkey = {},
 OPTtype = {},
 OPTaddress = {},
 OPTmonth = {},
 OPTnote = {},
 OPTannote = {},
 OPTurl = {},
 OPTurldate = {},
 OPTlastchecked = {},
 OPTdoi = {},
 OPTisbn = {},
 OPTissn = {},
 OPTlocalfile = {},
 OPTabstract = {},
 OPTkeywords = {},
}

@book{kingman_pp,
  address = {New York},
  author = {Kingman, J. F. C.},
  publisher = {Oxford University Press},
  series = {Oxford Studies in Probability},
  title = {Poisson Processes},
  volume = {3},
  year = {1993}
}

@INPROCEEDINGS{beta_st_pp,
    author = {J. Paisley and D. M. Blei and M. I. Jordan},
    title = "Stick-{B}reaking {B}eta {P}rocesses and the {P}oisson {P}rocess",
    booktitle = {Artificial Intelligence and Statistics},
    year = {2012},
}

@INPROCEEDINGS{beta_st,
    author = {J. Paisley and A. Zaas and C. W. Woods and G. S. Ginsburg and L. Carin},
    title = "A {S}tick-{B}reaking {C}onstruction of the {B}eta {P}rocess",
    booktitle = {International Conference on Machine Learning},
    year = {2010},
}

@INPROCEEDINGS{beta_st_vi,
    author = {J. Paisley and L. Carin and D. M. Blei},
    title = " Variational {I}nference for {S}tick-{B}reaking {B}eta {P}rocess {P}riors",
    booktitle = {International Conference on Machine Learning},
    year = {2011},
}

@inproceedings{ibp_vi_fdv,
  author    = {F. Doshi-Velez and
               K. Miller and
               J. Van Gael and
               Y. W. Teh},
  title     = "Variational {I}nference for the {I}ndian {B}uffet {P}rocess",
  booktitle = {AISTATS},
  year      = {2009},
  ee        = {http://www.jmlr.org/proceedings/papers/v5/doshi09a.html},
  bibsource = {DBLP, http://dblp.uni-trier.de}
}

@ARTICLE{ish_james_2001,
    author = {H. Ishwaran and L. F. James},
    title = "Gibbs {S}ampling {M}ethods for {S}tick-{B}reaking {P}riors",
    journal = {Journal of the American Statistical Association},
    year = {2001},
    volume = {96},
    pages = {161--173}
}

@ARTICLE{ish_james_2000,
    author = {H. Ishwaran and L. F. James},
    title = "Approximate {D}irichlet {P}rocess {C}omputing in {F}inite {N}ormal {M}ixtures: {S}moothing and {P}rior {I}nformation",
    journal = {Journal of Computational and Graphical Statistics},
    year = {2000},
    volume = {11},
    pages = {508--532}
}

@ARTICLE{pit_yor_1997,
    author = {J. Pitman and M. Yor},
    title = "The {T}wo-{P}arameter {P}oissonÃ¢Â€Â“{D}irichlet {D}istribution derived from a {S}table {S}ubordinator",
    journal = {Annals of Probability},
    year = {1997},
    volume = {25},
   number= {2},
    pages = {855--900}
}

@inproceedings{ibp_stick,
author=         "Y. W. Teh and D. {G\"or\"ur} and Z. Ghahramani",
title=          "Stick-breaking Construction for the {I}ndian Buffet Process",
booktitle=      "Proceedings of the International Conference on Artificial
                 Intelligence and Statistics",
volume=         "11",
year=           "2007"
}

@inproceedings{thi_jord,
author=         " R. Thibaux and M. I. Jordan",
title=          "Hierarchical beta processes and the {I}ndian buffet process",
booktitle=      "Proceedings of the Tenth International Conference on Artificial
                 Intelligence and Statistics",
year=           "2007"
}

@inproceedings{zhou_1,
  author = {M. Zhou and L. Carin},
  booktitle = {NIPS},
  title = {Augment-and-Conquer Negative Binomial Processes},
  year = 2012,
  }

@inproceedings{zhou_2,
author = {M. Zhou and L. Hannah and D. Dunson and L. Carin},
title = {Beta-Negative Binomial Process and {P}oisson Factor Analysis
},
year = {2012},
booktitle= {AISTATS},
}

@article{tab_bnb,
Author = {T. Broderick and L. Mackey and J. Paisley and M. I. Jordan},
Title = {Combinatorial clustering and the beta negative binomial process},
Year = {2011},
Eprint = {arXiv:1111.1802},
journal={arXiv}
}

@article{nest_hdp,
Author = {John Paisley and Chong Wang and David M. Blei and Michael I. Jordan},
Title = {Nested Hierarchical Dirichlet Processes},
Year = {2012},
Eprint = {arXiv:1210.6738},
journal={arXiv}
}

@misc{orbanz_w,
Author={P. Orbanz and S.Williamson},
Title={Unit-rate {P}oisson representations of completely random measures}
}

@ARTICLE{fergs_dp,
    author = {T.S. Ferguson},
    title = "A {B}ayesian {A}nalysis of {S}ome {N}onparametric {P}roblems",
    journal = {The Annals of Statistics},
    year = {1973},
    volume = {1},
   number= {2},
    pages = {209-230}
}

@misc{pl_luce_gp,
Author = {F. Caron and Y. W. Teh and B. T. Murphy},
Note = {arXiv:1211.5037},
Title = "Bayesian {N}onparametric {P}lackett-{L}uce Models for the {A}nalysis of {C}lustered {R}anked {D}ata",
Year = {2013}}

@misc{graphs_gp,
Author = {F. Caron and E. B. Fox},
Note = {arXiv:1401.1137},
Title = "Bayesian {N}onparametric {M}odels of {S}parse and {E}xchangeable {R}andom {G}raphs",
Year = {2013}}

@ARTICLE{v_dp,
    author = {D. Blei and M. Jordan},
    title = "Variational methods for {D}irichlet process mixtures",
    journal = {Bayesian Analysis},
    volume = {1},
    pages = {121-144}
}

@inproceedings{cv_hdp,
  author = {Y.W. Teh and K. Kurihara and M. Welling},
  booktitle = {NIPS},
  title = {Collapsed variational inference for {HDP}},
  year = 2007,
  }

@inproceedings{ov_hdp,
author = {C. Wang and J. Paisley and D. Blei},
title = {Online variational inference for the hierarchical {D}irichlet process
},
year = {2011},
booktitle= {AISTATS},
}@book{boyd_vandenberghe_2004, place={Cambridge}, title={Convex Optimization}, DOI={10.1017/CBO9780511804441}, publisher={Cambridge University Press}, author={Boyd, Stephen and Vandenberghe, Lieven}, year={2004}}

@phdthesis{balazs2016convex,
  title={Convex Regression: Theory, Practice, and Applications},
  author={Bal{\'a}zs, G{\'a}bor},
  year={2016},
  school={University of Alberta}
}
@article{minty1964monotonicity,
  title={On the monotonicity of the gradient of a convex function.},
  author={Minty, George J and others},
  journal={Pacific Journal of Mathematics},
  volume={14},
  number={1},
  pages={243--247},
  year={1964},
  publisher={Pacific Journal of Mathematics}
}
@inproceedings{pollard1990empirical,
  title={Empirical processes: theory and applications},
  author={Pollard, David},
  booktitle={NSF-CBMS regional conference series in probability and statistics},
  pages={i--86},
  year={1990},
  organization={JSTOR}
}

@inproceedings{alexnet,
author="A. Krizhevsky and I. Sutskever and G. E. Hinton",
title="Image{N}et Classification with Deep Convolutional Neural Networks",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2012"
}

@inproceedings{girshick_cvpr2014,
author="R. Girshick and J. Donahue and T. Darrell and J. Malik",
title="Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2014"
}

@inproceedings{szegedy,
author="C. Szegedy and W. Liu and Y. Jia and P. Sermanet and S. Reed and D. Anguelov and D. Ehran and V. Vanhoucke and A. Rabinovich",
title="Going Deeper with Convolutions",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2015"
}

@article{russokovsky,
author="O. Russakovsky and J. Deng and H. Su and J. Krause and S. Satheesh and S. Ma and Z. Huang and A. Karpathy and A. Khosla and M. Bernstein and A. C. Berg and L. Fei-Fei",
title="Image{N}et Large Scale Visual Recognition Challenge",
journal="International Journal of Computer Vision",
volume="115",
number="3",
pages="211--252",
year="2015"
}

@article{dropout,
author="N. Srivastava and G. Hinton and A. Krizhevsky and I. Sutskever and R. Salakhutdinov",
title="Dropout: A Simple Way to Prevent Neural Networks from Overfitting",
journal="Journal of Machine Learning Research",
volume="15",
pages="1929--1958",
year="2014"
}

@inproceedings{socher_sentiment,
author="R. Socher and A. Perelygin and J. Wu and J. Chuang and C. Manning and A. Ng and C. Potts",
title="Recursive Deep Models for Semantic Compositionality over a Sentiment Treebank",
booktitle="Conference on Empirical Methods in Natural Language Processing (EMNLP)",
year="2013"
}

@inproceedings{socher_parsing,
author="R. Socher and C. Lin and C. Manning and A. Ng",
title="Parsing Natural Scenes and Natural Language with Recursive Neural Networks",
booktitle="Proc. International Conference on Machine Learning (ICML)",
year="2011"
}

@inproceedings{socher_parsing2,
author="R. Socher and J. Bauer and C. Manning and A. Ng",
title="Parsing with Compositional Vector Grammars",
booktitle="Association for Computational Linguistics (ACL)",
year="2013"
}

@inproceedings{cho_translation,
author="K. Cho and B. van Meerienboer and C. Gulcehre and D. Bahdanau and F. Bougares and H. Schwenk and Y. Bengio",
title="Learning Phrase Representations using {RNN} Decoder-Encoder for Statistical Machine Translation",
booktitle="Empirical Methods in Natural Language Processing (EMNLP)",
year="2014"
}

@inproceedings{bahdanau,
author="D. Bahdanau and K. Cho and Y. Bengio",
title="Neural Machine Translation by Jointly Learning to Align and Translate",
booktitle="International Conference on Learning Representations (ICLR)",
year="2015"
}

@article{mnih_drl,
author="V. Mnih and K. Kavukcuoglu and D. Silver and A. A. Rusu and J. Veness and M. G. Bellemare and A. Graves and M. Riedmiller and A. K. Fidjeland and G. Ostrovski and S. Petersen and C. Beattie and A. Sadik and I. Antonoglou and H. King and D. Kumaran and D. Wierstra and S. Legg and D. Hassabis",
title="Human-Level Control Through Deep Reinforcement Learning",
journal="Nature",
volume="518",
pages="529--533",
year="2015"
}

@article{alphago,
author="D. Silver and A. Huang and C. J. Maddison and A. Guez and L. Sifre and G. van den Driessche and H. Schrittwieser and I. Antonoglou and V. Panneershelvam and M. Lanctot and S. Dieleman and D. Grewe and J. Nham and N. Kalchbrenner and I. Sutskever and T. Lillicrap and M. Leach and K. Kavukcuoglu and T. Graepel and D. Hassabis",
title="Mastering the game of {G}o with Deep Neural Networks and Tree Search",
journal="Nature",
volume="529",
pages="484--489",
year="2016"
}

@book{deep_learning_book,
    title={Deep Learning},
    author="I. Goodfellow and Y. Bengio and A. Courville",
    publisher="MIT Press",
    year={2016}
}

@article{hinton_speech_recognition,
author="G. Hinton and L. Deng and D. Yu and G. Dahl and A. Mohamed and N. Jatily and A. Senior and V. Vanhoucke and P. Nguyen and B. Kingsbury and T. Sainath",
title = "Deep Neural Networks for Acoustic Modeling in Speech Recognition",
year = "2012",
journal = "IEEE Signal Processing Magazine",
volume = "29",
pages="82--97"
}

@inproceedings{gans,
author="I. Goodfellow and J. Pouget-Abadie and M. Mirza and B. Xu and D. Warde-Farley and S. Ozair and A. Courville and Y. Bengio",
title="Generative Adversarial Networks",
booktitle="Advances in Neural Information Processing Systems (NIPS)",
year="2014"
}

@incollection{hmc,
author="R. M. Neal",
title="MCMC using Hamiltonian Dynamics",
booktitle="Handbook of Markov Chain Monte Carlo",
editor="S. Brooks and A. Gelman and G. Jones and X. Meng",
publisher="CRC Press",
year="2010"
}

@inproceedings{skip_layer,
author="K. Ke and X. Zhang and S. Ren and J. Sun",
title="Deep Residual Learning for Image Recognition",
booktitle="Proc. Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2016"
}

@article{relu,
author="R. Hahnloser and R. Sarpeshkar and M. A. Mahowald and R. J. Douglas and H. S. Seung",
title="Digital Selection and Analogue Amplification Coexist in a Cortex-Inspired Silicon Circuit",
journal="Nature",
volume="405",
pages="947--951",
year="2000"
}

@inproceedings{relu2,
author="X. Glorot and A. Bordes and Y. Bengio",
title="Deep sparse rectifier neural networks",
booktitle="Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)",
year="2011"
}

@inproceedings{firefly,
author="D. Maclaurin and R. P. Adams",
title="Firefly {M}onte {C}arlo: Exact {MCMC} with Subsets of Data",
booktitle="Uncertainty in Artificial Intelligence (UAI)",
year="2014"
}

@inproceedings{sparse_autoencoder,
author="A. Makhzani and B. Frey",
title="k-Sparse Autoencoders",
booktitle="Proc. International Conference on Learning Representations (ICLR)",
year="2014"
}

@inproceedings{deepBM,
author="R. Salakhutdinov and G. Hinton",
title="Deep {B}oltzmann Machines",
booktitle="Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)",
year="2009"
}

@article{dbn,
author="G. E. Hinton and S. Osindero and Y. W. Teh",
title="A Fast Learning Algorithm for Deep Belief Nets",
journal="Neural Computation",
volume="18",
number="7",
pages="1527--1554",
year="2006"
}

@inproceedings{sde,
author="K. Q. Weinberger and L. K. Saul",
title="Unsupervised Learning of Image Manifolds by Semidefinite Programming",
booktitle="Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2004"
}

@article{lle,
author="S. Roweis and L. Saul",
title="Nonlinear Dimensionality Reduction by Locally Linear Embedding",
journal="Science",
volume="290",
number="5500",
pages="2323--2326",
year="2000"
}

@inproceedings{semantic,
author="J. Long and E. Shelhamer and T. Darrell",
title="Fully Convolutional Models for Semantic Segmentation",
booktitle="Proc. International Conference on Computer Vision and Pattern Recognition (CVPR)",
year="2015"
}

@inproceedings{gated_rbn,
author="K. Sohn and G. Zhou and C. Lee and H. Lee",
title="Learning and Selecting Features Jointly with Point-wise Gated {B}oltzmann Machines",
booktitle="Proc. International Conference on Machine Learning (ICML)",
year="2013"
}

@inproceedings{roychowdhury_hmc,
author="A. Roychowdhury and B. Kulis and S. Parthasarathy",
title="Robust {M}onte {C}arlo Sampling using {R}iemannian {N}os\'{e}-{P}oincare {H}amiltonian Dynamics",
booktitle="Proc. International Conference on Machine Learning (ICML)",
year="2016"
}

@inproceedings{xie_clustering,
author="J. Xie and R. Gershick and A. Farhadi",
title="Unsupervised Deep Embedding for Clustering Analysis",
booktitle="Proc International Conference on Machine Learning (ICML)",
year="2016"
}

@article{russ_science,
author="G. E. Hinton and R. Salakhutdinov",
title="Reducing the dimensionality of data with neural networks",
journal="Science",
volume="313",
number="5786",
pages="504--507",
year="2006"
}

@inproceedings{russ_aistats,
author="R. Salakhutdinov and G. E. Hinton",
title="Learning a nonlinear embedding by preserving class neighbourhood structure",
booktitle="Proc. International Conference on Artificial Intelligence and Statistics (AISTATS)",
year="2007"
}


@book{kreyszig1978introductory,
  title={Introductory functional analysis with applications},
  author={Kreyszig, Erwin},
  volume={1},
  year={1978},
  publisher={wiley New York}
}

@book{conway2019course,
  title={A course in functional analysis},
  author={Conway, John B},
  volume={96},
  year={2019},
  publisher={Springer}
}

@article{gierz1987integral,
  title={Integral representations of linear functionals on function modules},
  author={Gierz, Gerhard},
  journal={The Rocky Mountain Journal of Mathematics},
  pages={545--554},
  year={1987},
  publisher={JSTOR}
}

@book{gelfand2000calculus,
  title={Calculus of variations},
  author={Gelfand, Izrail Moiseevitch and Silverman, Richard A and others},
  year={2000},
  publisher={Courier Corporation}
}

@article{frechet1907ensembles,
  title={Sur les ensembles de fonctions et les op{\'e}rations lin{\'e}aires},
  author={Fr{\'e}chet, Maurice},
  journal={CR Acad. Sci. Paris},
  volume={144},
  pages={1414--1416},
  year={1907}
}

@article{kaya2019deep,
  title={Deep metric learning: a survey},
  author={Kaya, Mahmut and Bilge, Hasan {\c{S}}akir},
  journal={Symmetry},
  volume={11},
  number={9},
  pages={1066},
  year={2019},
  publisher={Multidisciplinary Digital Publishing Institute}
}