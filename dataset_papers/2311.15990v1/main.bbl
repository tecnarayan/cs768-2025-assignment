\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asuncion and Newman(2007)]{asuncion2007uci}
Arthur Asuncion and David Newman.
\newblock Uci machine learning repository, 2007.

\bibitem[Bishop(2006)]{Bishop2006PatternRA}
Christopher~M. Bishop.
\newblock Pattern recognition and machine learning (information science and
  statistics).
\newblock 2006.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromańska, Soatto, LeCun,
  Baldassi, Borgs, Chayes, Sagun, and Zecchina]{Chaudhari2016EntropySGDBG}
Pratik Chaudhari, Anna Choromańska, Stefano Soatto, Yann LeCun, Carlo
  Baldassi, Christian Borgs, Jennifer~T. Chayes, Levent Sagun, and Riccardo
  Zecchina.
\newblock Entropy-sgd: biasing gradient descent into wide valleys.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment}, 2019,
  2016.

\bibitem[Clanuwat et~al.(2018)Clanuwat, Bober-Irizar, Kitamoto, Lamb, Yamamoto,
  and Ha]{clanuwat2018deep}
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
  Yamamoto, and David Ha.
\newblock Deep learning for classical japanese literature, 2018.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1019--1028, 2017.

\bibitem[Dodson and Poston(2013)]{dodson2013tensor}
Christopher Terence~John Dodson and Timothy Poston.
\newblock \emph{Tensor geometry: the geometric viewpoint and its uses}, volume
  130.
\newblock Springer Science \& Business Media, 2013.

\bibitem[El-Yaniv and Wiener(2010)]{elyaniv2010foundations}
Ran El-Yaniv and Yair Wiener.
\newblock On the foundations of noise-free selective classification.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (53):\penalty0 1605--1641, 2010.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016}, pages
  770--778. {IEEE} Computer Society, 2016.
\newblock \doi{10.1109/CVPR.2016.90}.

\bibitem[Hochreiter and Schmidhuber(1997)]{Hochreiter1997FlatM}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9:\penalty0 1--42, 1997.

\bibitem[Izmailov et~al.(2018)Izmailov, Wilson, Podoprikhin, Vetrov, and
  Garipov]{izmailov2018averaging}
P~Izmailov, AG~Wilson, D~Podoprikhin, D~Vetrov, and T~Garipov.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{34th Conference on Uncertainty in Artificial Intelligence
  2018, UAI 2018}, pages 876--885, 2018.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Klarner et~al.(2023)Klarner, Rudner, Reutlinger, Schindler, Morris,
  Deane, and Teh]{klarner2023qsavi}
Leo Klarner, Tim G.~J. Rudner, Michael Reutlinger, Torsten Schindler,
  Garrett~M. Morris, Charlotte Deane, and Yee~Whye Teh.
\newblock {D}rug {D}iscovery {u}nder {C}ovariate {S}hift {w}ith
  {D}omain-{I}nformed {P}rior {D}istributions {o}ver {F}unctions.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research. PMLR, 2023.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, and Goldstein]{Li2017VisualizingTL}
Hao Li, Zheng Xu, Gavin Taylor, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Maddox et~al.(2020)Maddox, Benton, and
  Wilson]{journals/corr/abs-2003-02139}
Wesley~J. Maddox, Gregory~W. Benton, and Andrew~Gordon Wilson.
\newblock Rethinking parameter counting in deep models: Effective
  dimensionality revisited.
\newblock \emph{CoRR}, abs/2003.02139, 2020.

\bibitem[Murphy(2013)]{murphy2013probabilistic}
Kevin~P. Murphy.
\newblock \emph{Machine learning : a probabilistic perspective}.
\newblock MIT Press, Cambridge, Mass. [u.a.], 2013.
\newblock ISBN 9780262018029 0262018020.

\bibitem[Naeini et~al.(2015)Naeini, Cooper, and
  Hauskrecht]{naeini2015obtaining}
Mahdi~Pakdaman Naeini, Gregory Cooper, and Milos Hauskrecht.
\newblock Obtaining well calibrated probabilities using bayesian binning.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~29, 2015.

\bibitem[Peskin(2018)]{peskin2018introduction}
Michael Peskin.
\newblock \emph{An introduction to quantum field theory}.
\newblock CRC press, 2018.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar10.1}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock 2018.

\bibitem[Rudner et~al.(2022{\natexlab{a}})Rudner, Chen, Teh, and
  Gal]{rudner2022fsvi}
Tim G.~J. Rudner, Zonghao Chen, Yee~Whye Teh, and Yarin Gal.
\newblock {T}ractable {F}unction-{S}pace {V}ariational {I}nference in
  {B}ayesian {N}eural {N}etworks.
\newblock In \emph{Advances in Neural Information Processing Systems 35},
  2022{\natexlab{a}}.

\bibitem[Rudner et~al.(2022{\natexlab{b}})Rudner, Smith, Feng, Teh, and
  Gal]{rudner2022sfsvi}
Tim G.~J. Rudner, Freddie~Bickford Smith, Qixuan Feng, Yee~Whye Teh, and Yarin
  Gal.
\newblock {C}ontinual {L}earning via {S}equential {F}unction-{S}pace
  {V}ariational {I}nference.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research. PMLR,
  2022{\natexlab{b}}.

\bibitem[Rudner et~al.(2023)Rudner, Kapoor, Qiu, and Wilson]{rudner2023fseb}
Tim G.~J. Rudner, Sanyam Kapoor, Shikai Qiu, and Andrew~Gordon Wilson.
\newblock {F}unction-{S}pace {R}egularization in {N}eural {N}etworks: {A}
  {P}robabilistic {P}erspective.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research. PMLR, 2023.

\bibitem[Tibshirani(1996)]{Tibshirani1996RegressionSA}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the royal statistical society series
  b-methodological}, 58:\penalty0 267--288, 1996.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Weinberg(1972)]{weinberg1972gravitation}
Steven Weinberg.
\newblock Gravitation and cosmology: principles and applications of the general
  theory of relativity.
\newblock 1972.

\bibitem[Wilson and Izmailov(2020)]{wilson2020bayesian}
Andrew~Gordon Wilson and Pavel Izmailov.
\newblock {B}ayesian deep learning and a probabilistic perspective of
  generalization.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, \emph{Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\bibitem[Wolpert(1993)]{wolpert1993fsmap}
David~H. Wolpert.
\newblock Bayesian backpropagation over i-o functions rather than weights.
\newblock In J.~Cowan, G.~Tesauro, and J.~Alspector, editors, \emph{Advances in
  Neural Information Processing Systems}, volume~6. Morgan-Kaufmann, 1993.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017/online}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\end{thebibliography}
