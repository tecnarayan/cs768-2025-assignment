@article{mhaskar1997neural,
  title={Neural networks for functional approximation and system identification},
  author={Mhaskar, Hrushikesh Narhar and Hahm, Nahmwoo},
  journal={Neural Computation},
  volume={9},
  number={1},
  pages={143--159},
  year={1997},
  publisher={MIT Press}
}

@article{stinchcombe1999neural,
  title={Neural network approximation of continuous functionals and continuous functions on compactifications},
  author={Stinchcombe, Maxwell B},
  journal={Neural Networks},
  volume={12},
  number={3},
  pages={467--477},
  year={1999},
  publisher={Elsevier}
}

@article{sandberg1996notes,
  title={Notes on weighted norms and network approximation of functionals},
  author={Sandberg, Irwin W},
  journal={IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications},
  volume={43},
  number={7},
  pages={600--601},
  year={1996},
  publisher={IEEE}
}

@article{rossi2005functional,
  title={Functional multi-layer perceptron: a non-linear tool for functional data analysis},
  author={Rossi, Fabrice and Conan-Guez, Brieuc},
  journal={Neural networks},
  volume={18},
  number={1},
  pages={45--60},
  year={2005},
  publisher={Elsevier}
}

@article{sandberg1996network,
  title={Network approximation of input-output maps and functionals},
  author={Sandberg, Irwin W and Xu, Lilian},
  journal={Circuits, Systems and Signal Processing},
  volume={15},
  number={6},
  pages={711--725},
  year={1996},
  publisher={Springer}
}

@article{savarese_how_2019,
	title = {How do infinite width bounded norm networks look in function space?},
	url = {http://arxiv.org/abs/1902.05040},
	abstract = {We consider the question of what functions can be captured by ReLU networks with an unbounded number of units (infinite width), but where the overall network Euclidean norm (sum of squares of all weights in the system, except for an unregularized bias term for each unit) is bounded; or equivalently what is the minimal norm required to approximate a given function. For functions \$f : {\textbackslash}mathbb R {\textbackslash}rightarrow {\textbackslash}mathbb R\$ and a single hidden layer, we show that the minimal network norm for representing \$f\$ is \${\textbackslash}max({\textbackslash}int {\textbar}f''(x){\textbar} dx, {\textbar}f'(-{\textbackslash}infty) + f'(+{\textbackslash}infty){\textbar})\$, and hence the minimal norm fit for a sample is given by a linear spline interpolation.},
	urldate = {2019-12-18},
	journal = {arXiv:1902.05040 [cs, stat]},
	author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.05040},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\Y3L2N5TL\\Savarese et al. - 2019 - How do infinite width bounded norm networks look i.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\W7AGWUBA\\1902.html:text/html}
}

@article{niles2019estimation,
  title={Estimation of wasserstein distances in the spiked transport model},
  author={Niles-Weed, Jonathan and Rigollet, Philippe},
  journal={arXiv preprint arXiv:1909.07513},
  year={2019}
}

@inproceedings{rosset2007,
  title={$\ell_1$ regularization in infinite dimensional feature spaces},
  author={Rosset, Saharon and Swirszcz, Grzegorz and Srebro, Nathan and Zhu, Ji},
  booktitle={International Conference on Computational Learning Theory},
  pages={544--558},
  year={2007},
  organization={Springer}
}

@inproceedings{bengio2006convex,
  title={Convex neural networks},
  author={Bengio, Yoshua and Roux, Nicolas L and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
  booktitle={Advances in neural information processing systems},
  pages={123--130},
  year={2006}
}

@article{wojtowytsch2020banach,
  title={On the Banach spaces associated with multi-layer ReLU networks: Function representation, approximation theory and gradient descent dynamics},
  author={Wojtowytsch, Stephan and others},
  journal={arXiv preprint arXiv:2007.15623},
  year={2020}
}

@article{de2020sparsity,
  title={On Sparsity in Overparametrised Shallow ReLU Networks},
  author={de Dios, Jaume and Bruna, Joan},
  journal={arXiv preprint arXiv:2006.10225},
  year={2020}
}

@article{ma2020quenching,
  title={The Quenching-Activation Behavior of the Gradient Descent Dynamics for Two-layer Neural Network Models},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:2006.14450},
  year={2020}
}


@article{raginsky2017non,
  title={Non-convex learning via stochastic gradient Langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1702.03849},
  year={2017}
}


@inproceedings{de2019stochastic,
  title={Stochastic deep networks},
  author={De Bie, Gwendoline and Peyr{\'e}, Gabriel and Cuturi, Marco},
  booktitle={International Conference on Machine Learning},
  pages={1556--1565},
  year={2019}
}

@article{blanc_implicit_2019,
	title = {Implicit regularization for deep neural networks driven by an {Ornstein}-{Uhlenbeck} like process},
	url = {http://arxiv.org/abs/1904.09080},
	abstract = {We consider deep networks, trained via stochastic gradient descent to minimize L2 loss, with the training labels perturbed by independent noise at each iteration. We characterize the behavior of the training dynamics near any parameter vector that achieves zero training error, in terms of an implicit regularization term corresponding to the sum over the data points, of the squared L2 norm of the gradient of the model with respect to the parameter vector, evaluated at each data point. We then leverage this general characterization, which holds for networks of any connectivity, width, depth, and choice of activation function, to show that for 2-layer ReLU networks of arbitrary width and L2 loss, when trained on one-dimensional labeled data \$(x\_1,y\_1),{\textbackslash}ldots,(x\_n,y\_n),\$ the only stable solutions with zero training error correspond to functions that: 1) are linear over any set of three or more co-linear training points (i.e. the function has no extra "kinks"); and 2) change convexity the minimum number of times that is necessary to fit the training data. Additionally, for 2-layer networks of arbitrary width, with tanh or logistic activations, we show that when trained on a single \$d\$-dimensional point \$(x,y)\$ the only stable solutions correspond to networks where the activations of all hidden units at the datapoint, and all weights from the hidden units to the output, take at most two distinct values, or are zero. In this sense, we show that when trained on "simple" data, models corresponding to stable parameters are also "simple"; in short, despite fitting in an over-parameterized regime where the vast majority of expressible functions are complicated and badly behaved, stable parameters reached by training with noise express nearly the "simplest possible" hypothesis consistent with the data. These results shed light on the mystery of why deep networks generalize so well in practice.},
	urldate = {2019-12-18},
	journal = {arXiv:1904.09080 [cs, stat]},
	author = {Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.09080},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\RH66VYZ4\\Blanc et al. - 2019 - Implicit regularization for deep neural networks d.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\6YMUI78K\\1904.html:text/html}
}

@article{maennel_gradient_2018,
	title = {Gradient {Descent} {Quantizes} {ReLU} {Network} {Features}},
	url = {http://arxiv.org/abs/1803.08367},
	abstract = {Deep neural networks are often trained in the over-parametrized regime (i.e. with far more parameters than training examples), and understanding why the training converges to solutions that generalize remains an open problem. Several studies have highlighted the fact that the training procedure, i.e. mini-batch Stochastic Gradient Descent (SGD) leads to solutions that have specific properties in the loss landscape. However, even with plain Gradient Descent (GD) the solutions found in the over-parametrized regime are pretty good and this phenomenon is poorly understood. We propose an analysis of this behavior for feedforward networks with a ReLU activation function under the assumption of small initialization and learning rate and uncover a quantization effect: The weight vectors tend to concentrate at a small number of directions determined by the input data. As a consequence, we show that for given input data there are only finitely many, "simple" functions that can be obtained, independent of the network size. This puts these functions in analogy to linear interpolations (for given input data there are finitely many triangulations, which each determine a function by linear interpolation). We ask whether this analogy extends to the generalization properties - while the usual distribution-independent generalization property does not hold, it could be that for e.g. smooth functions with bounded second derivative an approximation property holds which could "explain" generalization of networks (of unbounded size) to unseen inputs.},
	urldate = {2019-12-18},
	journal = {arXiv:1803.08367 [cs, stat]},
	author = {Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.08367},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\AXATKXRG\\Maennel et al. - 2018 - Gradient Descent Quantizes ReLU Network Features.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\PMV24QVX\\1803.html:text/html}
}

@misc{noauthor_[1910.01635]_nodate,
	title = {[1910.01635] {A} {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	url = {https://arxiv.org/abs/1910.01635},
	urldate = {2019-12-18},
	file = {[1910.01635] A Function Space View of Bounded Norm Infinite Width ReLU Nets\: The Multivariate Case:C\:\\Users\\Isabel\\Zotero\\storage\\JDU9E23J\\1910.html:text/html;Texto completo:C\:\\Users\\Isabel\\Zotero\\storage\\AMN4M6DH\\[1910.01635] A Function Space View of Bounded Norm.pdf:application/pdf}
}

@article{ongie_function_2019,
	title = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}: {The} {Multivariate} {Case}},
	shorttitle = {A {Function} {Space} {View} of {Bounded} {Norm} {Infinite} {Width} {ReLU} {Nets}},
	url = {http://arxiv.org/abs/1910.01635},
	abstract = {A key element of understanding the efficacy of overparameterized neural networks is characterizing how they represent functions as the number of weights in the network approaches infinity. In this paper, we characterize the norm required to realize a function \$f:{\textbackslash}mathbb\{R\}{\textasciicircum}d{\textbackslash}rightarrow{\textbackslash}mathbb\{R\}\$ as a single hidden-layer ReLU network with an unbounded number of units (infinite width), but where the Euclidean norm of the weights is bounded, including precisely characterizing which functions can be realized with finite norm. This was settled for univariate univariate functions in Savarese et al. (2019), where it was shown that the required norm is determined by the L1-norm of the second derivative of the function. We extend the characterization to multivariate functions (i.e., networks with d input units), relating the required norm to the L1-norm of the Radon transform of a (d+1)/2-power Laplacian of the function. This characterization allows us to show that all functions in Sobolev spaces \$W{\textasciicircum}\{s,1\}({\textbackslash}mathbb\{R\})\$, \$s{\textbackslash}geq d+1\$, can be represented with bounded norm, to calculate the required norm for several specific functions, and to obtain a depth separation result. These results have important implications for understanding generalization performance and the distinction between neural networks and more traditional kernel learning.},
	urldate = {2019-12-18},
	journal = {arXiv:1910.01635 [cs, stat]},
	author = {Ongie, Greg and Willett, Rebecca and Soudry, Daniel and Srebro, Nathan},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.01635},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\B27XXESI\\Ongie et al. - 2019 - A Function Space View of Bounded Norm Infinite Wid.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\FIRGMA2H\\1910.html:text/html}
}

@incollection{bartlett_for_1997,
	title = {For {Valid} {Generalization} the {Size} of the {Weights} is {More} {Important} than the {Size} of the {Network}},
	url = {http://papers.nips.cc/paper/1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-network.pdf},
	urldate = {2019-12-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 9},
	publisher = {MIT Press},
	author = {Bartlett, Peter L.},
	editor = {Mozer, M. C. and Jordan, M. I. and Petsche, T.},
	year = {1997},
	pages = {134--140},
	file = {NIPS Full Text PDF:C\:\\Users\\Isabel\\Zotero\\storage\\QULCGYAN\\Bartlett - 1997 - For Valid Generalization the Size of the Weights i.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\FYKC5EI3\\1204-for-valid-generalization-the-size-of-the-weights-is-more-important-than-the-size-of-the-ne.html:text/html}
}

@article{neyshabur_norm-based_2015,
	title = {Norm-{Based} {Capacity} {Control} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/1503.00036},
	abstract = {We investigate the capacity, convexity and characterization of a general family of norm-constrained feed-forward networks.},
	urldate = {2019-12-18},
	journal = {arXiv:1503.00036 [cs, stat]},
	author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
	month = apr,
	year = {2015},
	note = {arXiv: 1503.00036},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: 29 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\YID4ZVB3\\Neyshabur et al. - 2015 - Norm-Based Capacity Control in Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\W7JZ2NX3\\1503.html:text/html}
}

@article{carroll_construction_1989,
	title = {Construction of neural nets using the radon transform},
	doi = {10.1109/IJCNN.1989.118639},
	abstract = {The authors present a method for constructing a feedforward neural net implementing an arbitrarily good approximation to any L/sub 2/ function over (-1, 1)/sup n/. The net uses n input nodes, a single hidden layer whose width is determined by the function to be implemented and the allowable mean square error, and a linear output neuron. Error bounds and an example are given for the method.{\textless}{\textgreater}},
	journal = {International 1989 Joint Conference on Neural Networks},
	author = {Carroll, Spencer and Dickinson, Bradley W.},
	year = {1989},
	keywords = {Approximation, Artificial neural network, Ephrin Type-B Receptor 1, human, Feedforward neural network, Mean squared error, Multilayer perceptron, Neuron, Radon, width},
	pages = {607--611 vol.1}
}

@article{ito_representation_1991,
	title = {Representation of {Functions} by {Superpositions} of a {Step} or {Sigmoid} {Function} and {Their} {Applications} to {Neural} {Network} {Theory}},
	volume = {4},
	issn = {0893-6080},
	url = {http://dx.doi.org/10.1016/0893-6080(91)90075-G},
	doi = {10.1016/0893-6080(91)90075-G},
	number = {3},
	urldate = {2019-12-18},
	journal = {Neural Netw.},
	author = {Ito, Yoshifusa},
	month = jun,
	year = {1991},
	pages = {385--394}
}

@article{silverman_ridgelets:_1999,
	title = {Ridgelets: a key to higher-dimensional intermittency?},
	volume = {357},
	shorttitle = {Ridgelets},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.1999.0444},
	doi = {10.1098/rsta.1999.0444},
	abstract = {In dimensions two and higher, wavelets can efficiently represent only a small range of the full diversity of interesting behaviour. In effect, wavelets are well adapted for point–like phenomena, whereas in dimensions greater than one, interesting phenomena can be organized along lines, hyperplanes and other non–point–like structures, for which wavelets are poorly adapted.We discuss in this paper a new subject, ridgelet analysis, which can effectively deal with line–like phenomena in dimension 2, plane–like phenomena in dimension 3 and so on. It encompasses a collection of tools which all begin from the idea of analysis by ridge functions ψ(u1x1 +  ⃛ + unxn whose ridge profiles ψ are wavelets, or alternatively from performing a wavelet analysis in the Radon domain.The paper reviews recent work on the continuous ridgelet transform (CRT), ridgelet frames, ridgelet orthonormal bases, ridgelets and edges and describes a new notion of smoothness naturally attached to this new representation.},
	number = {1760},
	urldate = {2019-12-18},
	journal = {Philosophical Transactions of the Royal Society of London. Series A: Mathematical, Physical and Engineering Sciences},
	author = {Silverman, B. W. and Vassilicos, J. C. and Candès, Emmanuel J. and Donoho, David L.},
	month = sep,
	year = {1999},
	pages = {2495--2509},
	file = {Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\NZER7ALM\\rsta.1999.html:text/html}
}

@article{sonoda_neural_2017,
	title = {Neural {Network} with {Unbounded} {Activation} {Functions} is {Universal} {Approximator}},
	volume = {43},
	issn = {10635203},
	url = {http://arxiv.org/abs/1505.03654},
	doi = {10.1016/j.acha.2015.12.005},
	abstract = {This paper presents an investigation of the approximation property of neural networks with unbounded activation functions, such as the rectified linear unit (ReLU), which is the new de-facto standard of deep learning. The ReLU network can be analyzed by the ridgelet transform with respect to Lizorkin distributions. By showing three reconstruction formulas by using the Fourier slice theorem, the Radon transform, and Parseval's relation, it is shown that a neural network with unbounded activation functions still satisfies the universal approximation property. As an additional consequence, the ridgelet transform, or the backprojection filter in the Radon domain, is what the network learns after backpropagation. Subject to a constructive admissibility condition, the trained network can be obtained by simply discretizing the ridgelet transform, without backpropagation. Numerical examples not only support the consistency of the admissibility condition but also imply that some non-admissible cases result in low-pass filtering.},
	number = {2},
	urldate = {2019-12-18},
	journal = {Applied and Computational Harmonic Analysis},
	author = {Sonoda, Sho and Murata, Noboru},
	month = sep,
	year = {2017},
	note = {arXiv: 1505.03654},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Functional Analysis},
	pages = {233--268},
	annote = {Comment: under review; first revised version},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\MJYGF34J\\Sonoda y Murata - 2017 - Neural Network with Unbounded Activation Functions.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\JCSMYTTR\\1505.html:text/html}
}

@article{murray_enhanced_1994,
	title = {Enhanced {MLP} performance and fault tolerance resulting from synaptic weight noise during training},
	volume = {5},
	issn = {1941-0093},
	doi = {10.1109/72.317730},
	abstract = {We analyze the effects of analog noise on the synaptic arithmetic during multilayer perceptron training, by expanding the cost function to include noise-mediated terms. Predictions are made in the light of these calculations that suggest that fault tolerance, training quality and training trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The results appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implications for all applications, particularly those involving "inaccurate" analog neural VLSI.{\textless}{\textgreater}},
	number = {5},
	journal = {IEEE Transactions on Neural Networks},
	author = {Murray, A.F. and Edwards, P.J.},
	month = sep,
	year = {1994},
	keywords = {Arithmetic, cost function, Cost function, Degradation, fault tolerance, Fault tolerance, fault tolerant computing, feedforward neural nets, learning (artificial intelligence), Multi-layer neural network, multilayer perceptron, Multilayer perceptrons, Neural networks, noise, noise-injection, pattern recognition, Performance analysis, Senior members, synaptic weight noise, training quality, training trajectory, Very large scale integration},
	pages = {792--802},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\Isabel\\Zotero\\storage\\WFNQCWUX\\317730.html:text/html}
}

@article{rifai_adding_2011,
	title = {Adding noise to the input of a model trained with a regularized objective},
	url = {http://arxiv.org/abs/1104.3250},
	abstract = {Regularization is a well studied problem in the context of neural networks. It is usually used to improve the generalization performance when the number of input samples is relatively small or heavily contaminated with noise. The regularization of a parametric model can be achieved in different manners some of which are early stopping (Morgan and Bourlard, 1990), weight decay, output smoothing that are used to avoid overfitting during the training of the considered model. From a Bayesian point of view, many regularization techniques correspond to imposing certain prior distributions on model parameters (Krogh and Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective function when a restricted type of noise is added to the input of a parametric function, we derive the higher order terms of the Taylor expansion and analyze the coefficients of the regularization terms induced by the noisy input. In particular we study the effect of penalizing the Hessian of the mapping function with respect to the input in terms of generalization performance. We also show how we can control independently this coefficient by explicitly penalizing the Jacobian of the mapping function on corrupted inputs.},
	urldate = {2019-12-18},
	journal = {arXiv:1104.3250 [cs]},
	author = {Rifai, Salah and Glorot, Xavier and Bengio, Yoshua and Vincent, Pascal},
	month = apr,
	year = {2011},
	note = {arXiv: 1104.3250},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\BRH9HE62\\Rifai et al. - 2011 - Adding noise to the input of a model trained with .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\EJT8PWIU\\1104.html:text/html}
}

@inproceedings{bengio_convex_2005,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'05},
	title = {Convex {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2976248.2976264},
	abstract = {Convexity has recently received a lot of attention in the machine learning community, and the lack of convexity has been seen as a major disadvantage of many learning algorithms, such as multi-layer artificial neural networks. We show that training multi-layer neural networks in which the number of hidden units is learned can be viewed as a convex optimization problem. This problem involves an infinite number of variables, but can be solved by incrementally inserting a hidden unit at a time, each time finding a linear classifier that minimizes a weighted sum of errors.},
	urldate = {2019-12-18},
	booktitle = {Proceedings of the 18th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Roux, Nicolas Le and Vincent, Pascal and Delalleau, Olivier and Marcotte, Patrice},
	year = {2005},
	note = {event-place: Vancouver, British Columbia, Canada},
	pages = {123--130}
}

@article{boyer2019representer,
  title={On representer theorems and convex regularization},
  author={Boyer, Claire and Chambolle, Antonin and Castro, Yohann De and Duval, Vincent and De Gournay, Fr{\'e}d{\'e}ric and Weiss, Pierre},
  journal={SIAM Journal on Optimization},
  volume={29},
  number={2},
  pages={1260--1281},
  year={2019},
  publisher={SIAM}
}

@article{fisher1975spline,
  title={Spline solutions to L1 extremal problems in one and several variables},
  author={Fisher, SD and Jerome, Joseph W},
  journal={Journal of Approximation Theory},
  volume={13},
  number={1},
  pages={73--83},
  year={1975},
  publisher={Academic Press}
}

@inproceedings{diakonikolas2017being,
  title={Being robust (in high dimensions) can be practical},
  author={Diakonikolas, Ilias and Kamath, Gautam and Kane, Daniel M and Li, Jerry and Moitra, Ankur and Stewart, Alistair},
  booktitle={International Conference on Machine Learning},
  pages={999--1008},
  year={2017},
  organization={PMLR}
}

@article{neyshabur2014search,
  title={In search of the real inductive bias: On the role of implicit regularization in deep learning},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  journal={arXiv preprint arXiv:1412.6614},
  year={2014}
}

@article{zuho1948,
    title={Remarks on problems in approximation theory},
    author={Zuhovickii, S.},
    journal={Mat. Zbirnik KDU},
    year={1948}
}

article{ma2019barron,
  title={Barron spaces and the compositional function spaces for neural network models},
  author={Ma, Chao and Wu, Lei and others},
  journal={arXiv preprint arXiv:1906.08039},
  year={2019}
}

@article{belkin_two_2019,
	title = {Two models of double descent for weak features},
	url = {http://arxiv.org/abs/1903.07571},
	abstract = {The "double descent" risk curve was recently proposed to qualitatively describe the out-of-sample prediction accuracy of variably-parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features \$p\$ is close to the sample size \$n\$, but also that the risk decreases towards its minimum as \$p\$ increases beyond \$n\$. This behavior is contrasted with that of "prescient" models that select features in an a priori optimal order.},
	urldate = {2019-12-18},
	journal = {arXiv:1903.07571 [cs, stat]},
	author = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.07571},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\GZESTF7Y\\Belkin et al. - 2019 - Two models of double descent for weak features.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\3VLDK5GT\\1903.html:text/html}
}

@article{chizat2019sparse,
  title={Sparse optimization on measures with over-parameterized gradient descent},
  author={Chizat, Lenaic},
  journal={arXiv preprint arXiv:1907.10300},
  year={2019}
}

@article{belkin_reconciling_2019,
	title = {Reconciling modern machine learning practice and the bias-variance trade-off},
	url = {http://arxiv.org/abs/1812.11118},
	abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
	urldate = {2019-12-18},
	journal = {arXiv:1812.11118 [cs, stat]},
	author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
	month = sep,
	year = {2019},
	note = {arXiv: 1812.11118},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Isabel\\Zotero\\storage\\5VDGIPMA\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Isabel\\Zotero\\storage\\6NFAZWC8\\1812.html:text/html}
}



@string{uaii = "Uncertainty in Artificial Intelligence (UAI)"}
@string{icml = "Int. Conference on Machine Learning (ICML)"}
@string{nips = "Advances in Neural Information Processing Systems (NIPS)"}
@string{neurips = "Advances in Neural Information Processing Systems (NeurIPS)"}
@string{eccv = "Europ. Conference on Computer Vision (ECCV)"}
@string{iccv = "Int. Conference on Computer Vision (ICCV)"}
@string{cvpr = "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"}
@string{mathprog = "Mathematical Programming"}
@string{aistats = "Proc. Int. Conference on Artificial Intelligence and Statistics (AISTATS)"}
@string{aaai = "Proc. AAAI Conference on Artificial Intelligence"}
@string{icra = "Proc. IEEE Int. Conference on Robotics and Automation (ICRA)"}
@string{iclr = "Int. Conf. on Learning Representations (ICLR)"}

@article{arjovsky2019invariant,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{abbe2018provable,
  title={Provable limitations of deep learning},
  author={Abbe, Emmanuel and Sandon, Colin},
  journal={arXiv preprint arXiv:1812.06369},
  year={2018}
}

@misc{donohocourse,
  title={Theories of Deep Learning},
  author={Dave Donoho},
  journal={Stanford Graduate Course},
  year={2019},
  url={https://stats385.github.io.},
}

@inproceedings{shalev2017failures,
  title={Failures of gradient-based deep learning},
  author={Shalev-Shwartz, Shai and Shamir, Ohad and Shammah, Shaked},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={3067--3075},
  year={2017},
  organization={JMLR. org}
}

@article{shamir2018distribution,
  title={Distribution-specific hardness of learning neural networks},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={1135--1163},
  year={2018},
  publisher={JMLR. org}
}

@article{mardani2019degrees,
  title={Degrees of Freedom Analysis of Unrolled Neural Networks},
  author={Mardani, Morteza and Sun, Qingyun and Papyan, Vardan and Vasanawala, Shreyas and Pauly, John and Donoho, David},
  journal={arXiv preprint arXiv:1906.03742},
  year={2019}
}

@article{mou2019improved,
  title={Improved Bounds for Discretization of Langevin Diffusions: Near-Optimal Rates without Convexity},
  author={Mou, Wenlong and Flammarion, Nicolas and Wainwright, Martin J and Bartlett, Peter L},
  journal={arXiv preprint arXiv:1907.11331},
  year={2019}
}

@misc{yann_selfsup, 
title={Self-Supervised Learning},
author={LeCun, Yann},
year={2020},
howpublished = "\url{https://drive.google.com/file/d/1r-mDL4IX_hzZLDBKp8_e8VZqD7fOzBkF/view}"
}

@article{arjovsky2017wasserstein,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@article{darve2002calculating,
	Author = {Darve, Eric and Wilson, Michael A and Pohorille, Andrew},
	Date-Added = {2020-04-23 20:00:49 -0700},
	Date-Modified = {2020-04-23 20:00:49 -0700},
	Journal = {Molecular Simulation},
	Number = {1-2},
	Pages = {113--144},
	Publisher = {Taylor \& Francis},
	Title = {Calculating free energies using a scaled-force molecular dynamics algorithm},
	Volume = {28},
	Year = {2002}}

@article{rodriguez2004assessing,
	Author = {Rodriguez-Gomez, David and Darve, Eric and Pohorille, Andrew},
	Date-Added = {2020-04-23 20:00:44 -0700},
	Date-Modified = {2020-04-23 20:00:44 -0700},
	Journal = {The Journal of chemical physics},
	Number = {8},
	Pages = {3563--3578},
	Publisher = {American Institute of Physics},
	Title = {Assessing the efficiency of free energy calculation methods},
	Volume = {120},
	Year = {2004}}

@article{darve2008adaptive,
	Author = {Darve, Eric and Rodr{\'\i}guez-G{\'o}mez, David and Pohorille, Andrew},
	Date-Added = {2020-04-23 20:00:39 -0700},
	Date-Modified = {2020-04-23 20:00:39 -0700},
	Journal = {The Journal of chemical physics},
	Number = {14},
	Pages = {144120},
	Publisher = {American Institute of Physics},
	Title = {Adaptive biasing force method for scalar and vector free energy calculations},
	Volume = {128},
	Year = {2008}}

@article{darve2001calculating,
	Author = {Darve, Eric and Pohorille, Andrew},
	Date-Added = {2020-04-23 20:00:33 -0700},
	Date-Modified = {2020-04-23 20:00:33 -0700},
	Journal = {The Journal of chemical physics},
	Number = {20},
	Pages = {9169--9183},
	Publisher = {American Institute of Physics},
	Title = {Calculating free energies using average force},
	Volume = {115},
	Year = {2001}}

@article{li_neural_2019,
	Author = {Li, Lingge and Holbrook, Andrew and Shahbaba, Babak and Baldi, Pierre},
	Doi = {10/gf7jbk},
	Issn = {0943-4062, 1613-9658},
	Journal = {Computational Statistics},
	Language = {en},
	Month = mar,
	Number = {1},
	Pages = {281--299},
	Title = {Neural Network Gradient {{Hamiltonian Monte Carlo}}},
	Volume = {34},
	Year = {2019},
	Bdsk-Url-1 = {https://doi.org/10/gf7jbk}}

@article{xu2020learning,
	Author = {Xu, Kailai and Huang, Daniel Z and Darve, Eric},
	Journal = {arXiv preprint arXiv:2004.00265},
	Title = {Learning Constitutive Relations using Symmetric Positive Definite Neural Networks},
	Year = {2020}}

@article{arous2020classification,
	Author = {Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
	Journal = {arXiv preprint arXiv:2003.10409},
	Title = {A classification for the performance of online SGD for high-dimensional inference},
	Year = {2020}}

@article{huang2020matrix,
	Author = {Huang, De and Niles-Weed, Jonathan and Tropp, Joel A and Ward, Rachel},
	Journal = {arXiv preprint arXiv:2003.05437},
	Title = {Matrix Concentration for Products},
	Year = {2020}}

@article{wilson2020bayesian,
	Author = {Wilson, Andrew Gordon and Izmailov, Pavel},
	Journal = {arXiv preprint arXiv:2002.08791},
	Title = {Bayesian Deep Learning and a Probabilistic Perspective of Generalization},
	Year = {2020}}

@article{khoo2019switchnet,
	Author = {Khoo, Yuehaw and Ying, Lexing},
	Journal = {SIAM Journal on Scientific Computing},
	Number = {5},
	Pages = {A3182--A3201},
	Publisher = {SIAM},
	Title = {SwitchNet: a neural network model for forward and inverse scattering problems},
	Volume = {41},
	Year = {2019}}

@inproceedings{wilson2016deep,
	Author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P},
	Booktitle = {Artificial Intelligence and Statistics},
	Pages = {370--378},
	Title = {Deep kernel learning},
	Year = {2016}}

@article{ma2019sampling,
	Author = {Ma, Yi-An and Chen, Yuansi and Jin, Chi and Flammarion, Nicolas and Jordan, Michael I},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {42},
	Pages = {20881--20885},
	Publisher = {National Acad Sciences},
	Title = {Sampling can be faster than optimization},
	Volume = {116},
	Year = {2019}}

@inproceedings{mena2019statistical,
	Author = {Mena, Gonzalo and Niles-Weed, Jonathan},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {4543--4553},
	Title = {Statistical bounds for entropic optimal transport: sample complexity and the central limit theorem},
	Year = {2019}}


@article{zhang2020complexity,
	Author = {Zhang, Jingzhao and Lin, Hongzhou and Sra, Suvrit and Jadbabaie, Ali},
	Journal = {arXiv preprint arXiv:2002.04130},
	Title = {On Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions},
	Year = {2020}}

@article{donoho2018optimal,
	Author = {Donoho, David L and Ghorbani, Behrooz},
	Journal = {arXiv preprint arXiv:1810.07403},
	Title = {Optimal covariance estimation for condition number loss in the spiked model},
	Year = {2018}}

@article{mnih_variational_2016,
	Archiveprefix = {arXiv},
	Author = {Mnih, Andriy and Rezende, Danilo J.},
	Eprint = {1602.06725},
	Eprinttype = {arxiv},
	Journal = {arXiv:1602.06725 [cs, stat]},
	Language = {en},
	Month = feb,
	Primaryclass = {cs, stat},
	Title = {Variational Inference for {{Monte Carlo}} Objectives},
	Year = {2016}}

@phdthesis{bruna2013scattering,
	Author = {Bruna, Joan},
	Title = {Scattering representations for recognition},
	Year = {2013}}

@inproceedings{du2019graph,
	Author = {Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {5724--5734},
	Title = {Graph neural tangent kernel: Fusing graph neural networks with graph kernels},
	Year = {2019}}

@article{song_-nice-mc_2018,
	Archiveprefix = {arXiv},
	Author = {Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
	Eprint = {1706.07561},
	Eprinttype = {arxiv},
	Journal = {arXiv:1706.07561 [cs, stat]},
	Language = {en},
	Month = mar,
	Primaryclass = {cs, stat},
	Shorttitle = {A-{{NICE}}-{{MC}}},
	Title = {A-{{NICE}}-{{MC}}: {{Adversarial Training}} for {{MCMC}}},
	Year = {2018}}

@article{levy_generalizing_2018,
	Archiveprefix = {arXiv},
	Author = {Levy, Daniel and Hoffman, Matthew D. and {Sohl-Dickstein}, Jascha},
	Eprint = {1711.09268},
	Eprinttype = {arxiv},
	Journal = {arXiv:1711.09268 [cs, stat]},
	Language = {en},
	Primaryclass = {cs, stat},
	Title = {Generalizing {{Hamiltonian Monte Carlo}} with {{Neural Networks}}},
	Year = {2018}}

@article{scarselli2008graph,
	Author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
	Journal = {IEEE Transactions on Neural Networks},
	Number = {1},
	Pages = {61--80},
	Publisher = {IEEE},
	Title = {The graph neural network model},
	Volume = {20},
	Year = {2008}}

@article{barbier2019optimal,
	Author = {Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Miolane, L{\'e}o and Zdeborov{\'a}, Lenka},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {12},
	Pages = {5451--5460},
	Publisher = {National Acad Sciences},
	Title = {Optimal errors and phase transitions in high-dimensional generalized linear models},
	Volume = {116},
	Year = {2019}}

@article{maillard2019landscape,
  title={Landscape Complexity for the Empirical Risk of Generalized Linear Models},
  author={Maillard, Antoine and Arous, G{\'e}rard Ben and Biroli, Giulio},
  journal={arXiv preprint arXiv:1912.02143},
  year={2019}
}

@article{gerace2020generalisation,
	Author = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
	Journal = {arXiv preprint arXiv:2002.09339},
	Title = {Generalisation error in learning with random features and the hidden manifold model},
	Year = {2020}}

@article{chen2020can,
	Author = {Chen, Zhengdao and Chen, Lei and Villar, Soledad and Bruna, Joan},
	Journal = {arXiv preprint arXiv:2002.04025},
	Title = {Can graph neural networks count substructures?},
	Year = {2020}}

@article{grohs2018proof,
	Author = {Grohs, Philipp and Hornung, Fabian and Jentzen, Arnulf and Von Wurstemberger, Philippe},
	Date-Added = {2020-04-16 17:56:08 -0700},
	Date-Modified = {2020-04-16 17:56:08 -0700},
	Journal = {arXiv preprint arXiv:1809.02362},
	Title = {A proof that artificial neural networks overcome the curse of dimensionality in the numerical approximation of Black-Scholes partial differential equations},
	Year = {2018}}

@inproceedings{chen2019equivalence,
	Author = {Chen, Zhengdao and Villar, Soledad and Chen, Lei and Bruna, Joan},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {15868--15876},
	Title = {On the equivalence between graph isomorphism testing and function approximation with gnns},
	Year = {2019}}

@article{xu2018powerful,
	Author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
	Journal = {arXiv preprint arXiv:1810.00826},
	Title = {How powerful are graph neural networks?},
	Year = {2018}}

@article{bronstein2017geometric,
	Author = {Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
	Journal = {IEEE Signal Processing Magazine},
	Number = {4},
	Pages = {18--42},
	Publisher = {IEEE},
	Title = {Geometric deep learning: going beyond euclidean data},
	Volume = {34},
	Year = {2017}}

@article{bruna2013invariant,
	Author = {Bruna, Joan and Mallat, St{\'e}phane},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Number = {8},
	Pages = {1872--1886},
	Publisher = {IEEE},
	Title = {Invariant scattering convolution networks},
	Volume = {35},
	Year = {2013}}

@article{mallat2012group,
	Author = {Mallat, St{\'e}phane},
	Journal = {Communications on Pure and Applied Mathematics},
	Number = {10},
	Pages = {1331--1398},
	Publisher = {Wiley Online Library},
	Title = {Group invariant scattering},
	Volume = {65},
	Year = {2012}}

@article{mairal2011task,
	Author = {Mairal, Julien and Bach, Francis and Ponce, Jean},
	Journal = {IEEE transactions on pattern analysis and machine intelligence},
	Number = {4},
	Pages = {791--804},
	Publisher = {IEEE},
	Title = {Task-driven dictionary learning},
	Volume = {34},
	Year = {2011}}

@inproceedings{chen2018neural,
	Author = {Chen, Tian Qi and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
	Booktitle = {Advances in neural information processing systems},
	Pages = {6571--6583},
	Title = {Neural ordinary differential equations},
	Year = {2018}}

@inproceedings{he2016deep,
	Author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	Booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	Pages = {770--778},
	Title = {Deep residual learning for image recognition},
	Year = {2016}}

@article{papyan2017convolutional,
	Author = {Papyan, Vardan and Romano, Yaniv and Elad, Michael},
	Journal = {The Journal of Machine Learning Research},
	Number = {1},
	Pages = {2887--2938},
	Publisher = {JMLR. org},
	Title = {Convolutional neural networks analyzed via convolutional sparse coding},
	Volume = {18},
	Year = {2017}}

@article{bruna2020depthsep,
	Author = {Bruna, Joan and Jelassi, Samy and Venturi, Luca},
	Journal = {preprint},
	Title = {Depth Separation for Residual Networks in the high-dimensional regime},
	Year = {2020}}

@article{vardi2020barriers,
	Author = {Vardi, Gal and Shamir, Ohad},
	Journal = {preprint},
	Title = {Barriers to Depth Separation Results in Neural Networks},
	Year = {2020}}

@inproceedings{gregor2010learning,
	Author = {Gregor, Karol and LeCun, Yann},
	Booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
	Pages = {399--406},
	Title = {Learning fast approximations of sparse coding},
	Year = {2010}}

@article{donoho2006compressed,
	Author = {Donoho, David L},
	Journal = {IEEE Transactions on information theory},
	Number = {4},
	Pages = {1289--1306},
	Publisher = {IEEE},
	Title = {Compressed sensing},
	Volume = {52},
	Year = {2006}}

@article{zarka2019deep,
	Author = {Zarka, John and Thiry, Louis and Angles, Tom{\'a}s and Mallat, St{\'e}phane},
	Journal = {arXiv preprint arXiv:1910.03561},
	Title = {Deep Network classification by Scattering and Homotopy dictionary learning},
	Year = {2019}}

@article{olshausen1996emergence,
	Author = {Olshausen, Bruno A and Field, David J},
	Journal = {Nature},
	Number = {6583},
	Pages = {607--609},
	Publisher = {Nature Publishing Group},
	Title = {Emergence of simple-cell receptive field properties by learning a sparse code for natural images},
	Volume = {381},
	Year = {1996}}

@article{tibshirani1996regression,
	Author = {Tibshirani, Robert},
	Journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	Number = {1},
	Pages = {267--288},
	Publisher = {Wiley Online Library},
	Title = {Regression shrinkage and selection via the lasso},
	Volume = {58},
	Year = {1996}}

@article{candes2006robust,
	Author = {Cand{\`e}s, Emmanuel J and Romberg, Justin and Tao, Terence},
	Journal = {IEEE Transactions on information theory},
	Number = {2},
	Pages = {489--509},
	Publisher = {IEEE},
	Title = {Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information},
	Volume = {52},
	Year = {2006}}

@article{raissi2018forward,
	Author = {Raissi, Maziar},
	Date-Added = {2020-04-16 17:55:43 -0700},
	Date-Modified = {2020-04-16 17:55:43 -0700},
	Journal = {arXiv preprint arXiv:1804.07010},
	Title = {Forward-backward stochastic neural networks: Deep learning of high-dimensional partial differential equations},
	Year = {2018}}

@article{sirignano2018dgm,
	Author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	Date-Added = {2020-04-16 17:55:18 -0700},
	Date-Modified = {2020-04-16 17:55:18 -0700},
	Journal = {Journal of Computational Physics},
	Pages = {1339--1364},
	Publisher = {Elsevier},
	Title = {DGM: A deep learning algorithm for solving partial differential equations},
	Volume = {375},
	Year = {2018}}

@article{han2018solving,
	Author = {Han, Jiequn and Jentzen, Arnulf and E, Weinan},
	Date-Added = {2020-04-16 17:54:57 -0700},
	Date-Modified = {2020-04-16 17:54:57 -0700},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {34},
	Pages = {8505--8510},
	Publisher = {National Acad Sciences},
	Title = {Solving high-dimensional partial differential equations using deep learning},
	Volume = {115},
	Year = {2018}}

@article{haastad1987computational,
	Author = {H{\aa}stad, Johan},
	Title = {Computational limitations of small-depth circuits},
	Year = {1987}}

@article{maiorov2000near,
	Author = {Maiorov, VE and Meir, Ron},
	Journal = {Advances in Computational Mathematics},
	Number = {1},
	Pages = {79--103},
	Publisher = {Springer},
	Title = {On the near optimality of the stochastic approximation of smooth functions by neural networks},
	Volume = {13},
	Year = {2000}}

@inproceedings{eldan2016power,
	Author = {Eldan, Ronen and Shamir, Ohad},
	Booktitle = {Conference on learning theory},
	Pages = {907--940},
	Title = {The power of depth for feedforward neural networks},
	Year = {2016}}

@article{safran2019depth,
	Author = {Safran, Itay and Eldan, Ronen and Shamir, Ohad},
	Journal = {arXiv preprint arXiv:1904.06984},
	Title = {Depth Separations in Neural Networks: What is Actually Being Separated?},
	Year = {2019}}

@article{weinan2017deep,
	Author = {E, Weinan and Han, Jiequn and Jentzen, Arnulf},
	Date-Added = {2020-04-16 17:54:30 -0700},
	Date-Modified = {2020-04-16 17:54:30 -0700},
	Journal = {Communications in Mathematics and Statistics},
	Number = {4},
	Pages = {349--380},
	Publisher = {Springer},
	Title = {Deep learning-based numerical methods for high-dimensional parabolic partial differential equations and backward stochastic differential equations},
	Volume = {5},
	Year = {2017}}

@article{safran2019depth_sep,
	Author = {Safran, Itay and Eldan, Ronen and Shamir, Ohad},
	Journal = {arXiv preprint arXiv:1904.06984},
	Title = {Depth Separations in Neural Networks: What is Actually Being Separated?},
	Year = {2019}}

@article{xu2018calibrating,
	Author = {Xu, Kailai and Darve, Eric},
	Date-Added = {2020-04-15 15:23:28 -0700},
	Date-Modified = {2020-04-15 15:23:28 -0700},
	Journal = {arXiv preprint arXiv:1812.08883},
	Title = {Calibrating {L\'evy} Processes from Observations Based on Neural Networks and Automatic Differentiation},
	Year = {2018}}

@article{zhu2020general,
	Author = {Zhu, Weiqiang and Xu, Kailai and Darve, Eric and Beroza, Gregory C},
	Date-Added = {2020-04-15 15:22:51 -0700},
	Date-Modified = {2020-04-15 15:22:51 -0700},
	Journal = {arXiv preprint arXiv:2003.06027},
	Title = {A General Approach to Seismic Inversion with Automatic Differentiation},
	Year = {2020}}

@article{xu2019adversarial,
	Author = {Xu, Kailai and Darve, Eric},
	Date-Added = {2020-04-15 15:22:33 -0700},
	Date-Modified = {2020-04-15 15:22:33 -0700},
	Journal = {arXiv preprint arXiv:1910.06936},
	Title = {Adversarial Numerical Analysis for Inverse Problems},
	Year = {2019}}

@article{xu2020physics,
	Author = {Xu, Kailai and Darve, Eric},
	Date-Added = {2020-04-15 15:22:13 -0700},
	Date-Modified = {2020-04-15 15:22:13 -0700},
	Journal = {arXiv preprint arXiv:2002.10521},
	Title = {Physics Constrained Learning for Data-driven Inverse Modeling from Sparse Observations},
	Year = {2020}}

@article{huang2019predictive,
	Author = {Huang, Daniel Z and Xu, Kailai and Farhat, Charbel and Darve, Eric},
	Date-Added = {2020-04-15 15:22:00 -0700},
	Date-Modified = {2020-04-15 15:22:00 -0700},
	Journal = {arXiv preprint arXiv:1905.12530},
	Title = {Predictive modeling with learned constitutive laws from indirect observations},
	Year = {2019}}

@article{xu2019neural,
	Author = {Xu, Kailai and Darve, Eric},
	Date-Added = {2020-04-15 15:21:50 -0700},
	Date-Modified = {2020-04-15 15:21:50 -0700},
	Journal = {arXiv preprint arXiv:1901.07758},
	Title = {The neural network approach to inverse problems in differential equations},
	Year = {2019}}

@article{cybenko1989approximation,
	Author = {Cybenko, George},
	Journal = {Mathematics of control, signals and systems},
	Number = {4},
	Pages = {303--314},
	Publisher = {Springer},
	Title = {Approximation by superpositions of a sigmoidal function},
	Volume = {2},
	Year = {1989}}

@article{daubechies2019nonlinear,
	Author = {Daubechies, Ingrid and DeVore, Ronald and Foucart, Simon and Hanin, Boris and Petrova, Guergana},
	Journal = {arXiv preprint arXiv:1905.02199},
	Title = {Nonlinear approximation and (deep) relu networks},
	Year = {2019}}

@article{allen2020backward,
	Author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	Journal = {arXiv preprint arXiv:2001.04413},
	Title = {Backward Feature Correction: How Deep Learning Performs Deep Learning},
	Year = {2020}}

@article{chatziafratis2020better,
	Author = {Chatziafratis, Vaggos and Nagarajan, Sai Ganesh and Panageas, Ioannis},
	Journal = {arXiv preprint arXiv:2003.00777},
	Title = {Better Depth-Width Trade-offs for Neural Networks through the lens of Dynamical Systems},
	Year = {2020}}

@article{arora2018optimization,
	Author = {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
	Journal = {arXiv preprint arXiv:1802.06509},
	Title = {On the optimization of deep networks: Implicit acceleration by overparameterization},
	Year = {2018}}

@article{ma2019barron,
	Author = {Ma, Chao and Wu, Lei and E, Weinan},
	Journal = {arXiv preprint arXiv:1906.08039},
	Title = {Barron spaces and the compositional function spaces for neural network models},
	Year = {2019}}

@article{telgarsky2016benefits,
	Author = {Telgarsky, Matus},
	Journal = {arXiv preprint arXiv:1602.04485},
	Title = {Benefits of depth in neural networks},
	Year = {2016}}

@article{telgarsky2015representation,
	Author = {Telgarsky, Matus},
	Journal = {arXiv preprint arXiv:1509.08101},
	Title = {Representation benefits of deep feedforward networks},
	Year = {2015}}

@inproceedings{cohen2016expressive,
	Author = {Cohen, Nadav and Sharir, Or and Shashua, Amnon},
	Booktitle = {Conference on Learning Theory},
	Pages = {698--728},
	Title = {On the expressive power of deep learning: A tensor analysis},
	Year = {2016}}

@article{poggio2017and,
	Author = {Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
	Journal = {International Journal of Automation and Computing},
	Number = {5},
	Pages = {503--519},
	Publisher = {Springer},
	Title = {Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
	Volume = {14},
	Year = {2017}}

@inproceedings{safran2017depth,
	Author = {Safran, Itay and Shamir, Ohad},
	Booktitle = {Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	Organization = {JMLR. org},
	Pages = {2979--2987},
	Title = {Depth-width tradeoffs in approximating natural functions with neural networks},
	Year = {2017}}

@inproceedings{delalleau2011shallow,
	Author = {Delalleau, Olivier and Bengio, Yoshua},
	Booktitle = {Advances in neural information processing systems},
	Pages = {666--674},
	Title = {Shallow vs. deep sum-product networks},
	Year = {2011}}

@inproceedings{martens2013representational,
	Author = {Martens, James and Chattopadhya, Arkadev and Pitassi, Toni and Zemel, Richard},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {2877--2885},
	Title = {On the representational efficiency of restricted boltzmann machines},
	Year = {2013}}

@inproceedings{montufar2014number,
	Author = {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Advances in neural information processing systems},
	Pages = {2924--2932},
	Title = {On the number of linear regions of deep neural networks},
	Year = {2014}}

@article{liang2016deep,
	Author = {Liang, Shiyu and Srikant, Rayadurgam},
	Journal = {arXiv preprint arXiv:1610.04161},
	Title = {Why deep neural networks for function approximation?},
	Year = {2016}}

@article{shaham2018provable,
	Author = {Shaham, Uri and Cloninger, Alexander and Coifman, Ronald R},
	Journal = {Applied and Computational Harmonic Analysis},
	Number = {3},
	Pages = {537--557},
	Publisher = {Elsevier},
	Title = {Provable approximation properties for deep neural networks},
	Volume = {44},
	Year = {2018}}

@article{yarotsky2017error,
	Author = {Yarotsky, Dmitry},
	Journal = {Neural Networks},
	Pages = {103--114},
	Publisher = {Elsevier},
	Title = {Error bounds for approximations with deep ReLU networks},
	Volume = {94},
	Year = {2017}}

@article{daniely2017depth,
	Author = {Daniely, Amit},
	Journal = {arXiv preprint arXiv:1702.08489},
	Title = {Depth separation for neural networks},
	Year = {2017}}

@inproceedings{zhang2019cyclical,
	Author = {Zhang, Ruqi and Li, Chunyuan and Zhang, Jianyi and Chen, Changyou and Wilson, Andrew Gordon},
	Booktitle = {International Conference on Learning Representations (ICLR)},
	Title = {Cyclical stochastic gradient mcmc for {B}ayesian deep learning},
	Year = {2020}}

@article{yedidia2003understanding,
	Author = {Yedidia, Jonathan S and Freeman, William T and Weiss, Yair},
	Journal = {Exploring artificial intelligence in the new millennium},
	Pages = {236--239},
	Title = {Understanding belief propagation and its generalizations},
	Volume = {8},
	Year = {2003}}

@inproceedings{recht2019imagenet,
	Author = {Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
	Booktitle = {International Conference on Machine Learning},
	Pages = {5389--5400},
	Title = {Do ImageNet Classifiers Generalize to ImageNet?},
	Year = {2019}}

@article{maddox2020rethinking,
	Author = {Maddox, Wesley J and Benton, Gregory and Wilson, Andrew Gordon},
	Journal = {arXiv preprint arXiv:2003.02139},
	Title = {Rethinking Parameter Counting in Deep Models: Effective Dimensionality Revisited},
	Year = {2020}}

@article{defossez2020convergence,
	Author = {D{\'e}fossez, Alexandre and Bottou, L{\'e}on and Bach, Francis and Usunier, Nicolas},
	Journal = {arXiv preprint arXiv:2003.02395},
	Title = {On the Convergence of Adam and Adagrad},
	Year = {2020}}

@article{finzi2020generalizing,
	Author = {Finzi, Marc and Stanton, Samuel and Izmailov, Pavel and Wilson, Andrew Gordon},
	Journal = {arXiv preprint arXiv:2002.12880},
	Title = {Generalizing Convolutional Neural Networks for Equivariance to Lie Groups on Arbitrary Continuous Data},
	Year = {2020}}

@inproceedings{cohen2016group,
	Author = {Cohen, Taco and Welling, Max},
	Booktitle = {International conference on machine learning},
	Pages = {2990--2999},
	Title = {Group equivariant convolutional networks},
	Year = {2016}}

@inproceedings{aubin2018committee,
	Author = {Aubin, Benjamin and Maillard, Antoine and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'a}, Lenka and others},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {3223--3234},
	Title = {The committee machine: Computational to statistical gaps in learning a two-layers neural network},
	Year = {2018}}

@inproceedings{mannelli2019passed,
	Author = {Mannelli, Stefano Sarao and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborova, Lenka},
	Booktitle = {international conference on machine learning},
	Pages = {4333--4342},
	Title = {Passed \& spurious: Descent algorithms and local minima in spiked matrix-tensor models},
	Year = {2019}}

@article{ward2018adagrad,
	Author = {Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
	Journal = {arXiv preprint arXiv:1806.01811},
	Title = {Adagrad stepsizes: Sharp convergence over nonconvex landscapes, from any initialization},
	Year = {2018}}

@article{zdeborova2016statistical,
	Author = {Zdeborov{\'a}, Lenka and Krzakala, Florent},
	Journal = {Advances in Physics},
	Number = {5},
	Pages = {453--552},
	Publisher = {Taylor \& Francis},
	Title = {Statistical physics of inference: Thresholds and algorithms},
	Volume = {65},
	Year = {2016}}

@article{mannelli2020marvels,
	Author = {Mannelli, Stefano Sarao and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
	Journal = {Physical Review X},
	Number = {1},
	Pages = {011057},
	Publisher = {APS},
	Title = {Marvels and pitfalls of the langevin algorithm in noisy high-dimensional inference},
	Volume = {10},
	Year = {2020}}

@article{bach2017breaking,
	Author = {Bach, Francis},
	Journal = {The Journal of Machine Learning Research},
	Number = {1},
	Pages = {629--681},
	Publisher = {JMLR. org},
	Title = {Breaking the curse of dimensionality with convex neural networks},
	Volume = {18},
	Year = {2017}}

@article{goldt2019modelling,
	Author = {Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Journal = {arXiv preprint arXiv:1909.11500},
	Title = {Modelling the influence of data structure on learning in neural networks},
	Year = {2019}}

@article{chen2018convergence,
	Author = {Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
	Journal = {arXiv preprint arXiv:1808.02941},
	Title = {On the convergence of a class of adam-type algorithms for non-convex optimization},
	Year = {2018}}

@article{blum1992training,
	Author = {Blum, Avrim L and Rivest, Ronald L},
	Journal = {Neural Networks},
	Number = {1},
	Pages = {117--127},
	Publisher = {Elsevier},
	Title = {Training a 3-node neural network is NP-complete},
	Volume = {5},
	Year = {1992}}

@inproceedings{rahimi2008random,
	Author = {Rahimi, Ali and Recht, Benjamin},
	Booktitle = {Advances in neural information processing systems},
	Pages = {1177--1184},
	Title = {Random features for large-scale kernel machines},
	Year = {2008}}

@article{soudry2018implicit,
	Author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
	Journal = {The Journal of Machine Learning Research},
	Number = {1},
	Pages = {2822--2878},
	Publisher = {JMLR. org},
	Title = {The implicit bias of gradient descent on separable data},
	Volume = {19},
	Year = {2018}}

@article{chizat2018note,
	Author = {Chizat, Lenaic and Bach, Francis},
	Journal = {arXiv preprint arXiv:1812.07956},
	Title = {A note on lazy training in supervised differentiable programming},
	Year = {2018}}

@inproceedings{jacot2018neural,
	Author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	Booktitle = {Advances in neural information processing systems},
	Pages = {8571--8580},
	Title = {Neural tangent kernel: Convergence and generalization in neural networks},
	Year = {2018}}

@article{woodworth2019kernel,
	Author = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro Nathan},
	Journal = {arXiv preprint arXiv:1906.05827},
	Title = {Kernel and Deep Regimes in Overparametrized Models},
	Year = {2019}}

@article{kingma2014adam,
	Author = {Kingma, Diederik P and Ba, Jimmy},
	Journal = {arXiv preprint arXiv:1412.6980},
	Title = {Adam: A method for stochastic optimization},
	Year = {2014}}

@inproceedings{ge2015escaping,
	Author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
	Booktitle = {Conference on Learning Theory},
	Pages = {797--842},
	Title = {Escaping from saddle points---online stochastic gradient for tensor decomposition},
	Year = {2015}}

@article{freeman2016topology,
	Author = {Freeman, C Daniel and Bruna, Joan},
	Journal = {arXiv preprint arXiv:1611.01540},
	Title = {Topology and geometry of half-rectified network optimization},
	Year = {2016}}

@inproceedings{xujk18,
	Author = {Keyulu Xu and Chengtao Li and Yonglong Tian and Tomohiro Sonobe and {Ken-ichi} Kawarabayashi and Stefanie Jegelka},
	Booktitle = icml,
	Title = {Representation Learning on Graphs with Jumping Knowledge Networks},
	Year = 2018}

@inproceedings{xuhlj19,
	Author = {Keyulu Xu and Weihua Hu and Jure Leskovec and Stefanie Jegelka},
	Booktitle = iclr,
	Title = {How Powerful are Graph Neural Networks?},
	Year = 2019}

@inproceedings{xulz20,
	Author = {K. Xu and J. Li and M. Zhang and S. Du and K. Kawarabayashi and S. Jegelka},
	Booktitle = iclr,
	Title = {What Can Neural Networks Reason About?},
	Year = 2020}

@article{garg20,
	Archiveprefix = {arXiv},
	Author = {{Garg}, Vikas K. and {Jegelka}, Stefanie and {Jaakkola}, Tommi},
	Eid = {arXiv:2002.06157},
	Eprint = {2002.06157},
	Journal = {arXiv e-prints},
	Month = feb,
	Pages = {arXiv:2002.06157},
	Title = {{Generalization and Representational Limits of Graph Neural Networks}},
	Year = 2020}

@article{kohler2018exponential,
	Author = {Kohler, Jonas and Daneshmand, Hadi and Lucchi, Aurelien and Zhou, Ming and Neymeyr, Klaus and Hofmann, Thomas},
	Journal = {arXiv preprint arXiv:1805.10694},
	Title = {Exponential convergence rates for Batch Normalization: The power of length-direction decoupling in non-convex optimization},
	Year = {2018}}

@article{duchi2011adaptive,
	Author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	Journal = {Journal of machine learning research},
	Number = {Jul},
	Pages = {2121--2159},
	Title = {Adaptive subgradient methods for online learning and stochastic optimization},
	Volume = {12},
	Year = {2011}}

@article{arora2018theoretical,
	Author = {Arora, Sanjeev and Li, Zhiyuan and Lyu, Kaifeng},
	Journal = {arXiv preprint arXiv:1812.03981},
	Title = {Theoretical analysis of auto rate-tuning by batch normalization},
	Year = {2018}}

@article{wu2018wngrad,
	Author = {Wu, Xiaoxia and Ward, Rachel and Bottou, L{\'e}on},
	Journal = {arXiv preprint arXiv:1803.02865},
	Title = {Wngrad: Learn the learning rate in gradient descent},
	Year = {2018}}

@article{santurkar2018does,
	Author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {2483--2493},
	Title = {How does batch normalization help optimization?},
	Year = {2018}}

@article{ioffe2015batch,
	Author = {Ioffe, Sergey and Szegedy, Christian},
	Journal = {arXiv preprint arXiv:1502.03167},
	Title = {Batch normalization: Accelerating deep network training by reducing internal covariate shift},
	Year = {2015}}

@article{battaglia2018relational,
	Author = {Battaglia, Peter W and Hamrick, Jessica B and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
	Journal = {arXiv preprint arXiv:1806.01261},
	Title = {Relational inductive biases, deep learning, and graph networks},
	Year = {2018}}

@article{barron1993universal,
	Author = {Barron, Andrew R.},
	Journal = {IEEE Transactions on Information theory},
	Number = {3},
	Pages = {930--945},
	Publisher = {IEEE},
	Title = {Universal approximation bounds for superpositions of a sigmoidal function},
	Volume = {39},
	Year = {1993}}
	
@book{efthimiou2014spherical,
  title={Spherical harmonics in p dimensions},
  author={Efthimiou, Costas and Frye, Christopher},
  year={2014},
  publisher={World Scientific}
}

@article{kurkova2001bounds,
	Author = {Kurkov{\'a}, Vera and Sanguineti, Marcello},
	Journal = {IEEE Transactions on Information Theory},
	Number = {6},
	Pages = {2659--2665},
	Publisher = {IEEE},
	Title = {Bounds on rates of variable-basis and neural-network approximation},
	Volume = {47},
	Year = {2001}}

@inproceedings{wei2019regularization,
	Author = {Wei, Colin and Lee, Jason D. and Liu, Qiang and Ma, Tengyu},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {9709--9721},
	Title = {Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
	Year = {2019}}

@article{chizat2020implicit,
	Author = {Chizat, Lenaic and Bach, Francis},
	Journal = {arXiv preprint arXiv:2002.04486},
	Title = {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
	Year = {2020}}

@inproceedings{chizat2018global,
	Author = {Chizat, Lenaic and Bach, Francis},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {3036--3046},
	Title = {On the global convergence of gradient descent for over-parameterized models using optimal transport},
	Year = {2018}}

@article{mei2018mean,
	Author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {33},
	Pages = {E7665--E7671},
	Publisher = {National Acad. Sciences},
	Title = {A mean field view of the landscape of two-layer neural networks},
	Volume = {115},
	Year = {2018}}

@inproceedings{rotskoff2018parameters,
	Author = {Rotskoff, Grant and Vanden-Eijnden, Eric},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {7146--7155},
	Title = {Parameters as interacting particles: long time convergence and asymptotic error scaling of neural networks},
	Year = {2018}}

@article{sirignano2020mean,
	Author = {Sirignano, Justin and Spiliopoulos, Konstantinos},
	Journal = {Stochastic Processes and their Applications},
	Number = {3},
	Pages = {1820--1852},
	Publisher = {Elsevier},
	Title = {Mean field analysis of neural networks: A central limit theorem},
	Volume = {130},
	Year = {2020}}

@article{nitanda2017stochastic,
	Author = {Nitanda, Atsushi and Suzuki, Taiji},
	Journal = {arXiv preprint arXiv:1712.05438},
	Title = {Stochastic particle gradient descent for infinite ensembles},
	Year = {2017}}

@inproceedings{savarese2019infinite,
	Author = {Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
	Booktitle = {Conference on Learning Theory},
	Pages = {2667--2690},
	Title = {How do infinite width bounded norm networks look in function space?},
	Year = {2019}}

@article{CheDwiWaiYu18,
	Author = {Y. Chen and R. Dwivedi and M. J. Wainwright and B. Yu},
	Journal = {Journal of Machine Learning Research},
	Pages = {1--86},
	Title = {Fast {MCMC} Sampling Algorithms on Polytopes},
	Volume = 19,
	Year = 2018}

@techreport{MouFlaWaiBar19b,
	Author = {W. Mou and N. Flammarion and M. J. Wainwright and P. L. Bartlett},
	Institution = {UC Berkeley},
	Month = {October},
	Title = {An efficient sampling algorithm for non-smooth composite potentials},
	Year = 2019}

@article{donoho2009message,
	Author = {Donoho, David L and Maleki, Arian and Montanari, Andrea},
	Journal = {Proceedings of the National Academy of Sciences},
	Number = {45},
	Pages = {18914--18919},
	Publisher = {National Acad Sciences},
	Title = {Message-passing algorithms for compressed sensing},
	Volume = {106},
	Year = {2009}}

@article{wainwright2008graphical,
	Author = {Wainwright, Martin J and Jordan, Michael I and others},
	Journal = {Foundations and Trends{\textregistered} in Machine Learning},
	Number = {1--2},
	Pages = {1--305},
	Publisher = {Now Publishers, Inc.},
	Title = {Graphical models, exponential families, and variational inference},
	Volume = {1},
	Year = {2008}}

@book{mezard1987spin,
	Author = {M{\'e}zard, Marc and Parisi, Giorgio and Virasoro, Miguel},
	Publisher = {World Scientific Publishing Company},
	Title = {Spin glass theory and beyond: An Introduction to the Replica Method and Its Applications},
	Volume = {9},
	Year = {1987}}

@techreport{MouMaWaiBarJor19,
	Author = {W. Mou and Y. Ma and M. J. Wainwright and P. L. Bartlett and M. I. Jordan},
	Institution = {UC Berkeley},
	Month = {August},
	Title = {High-Order {L}angevin Diffusion Yields an Accelerated {M}{C}{M}{C} Algorithm},
	Year = 2019}

@article{DwiCheWaiYu19,
	Author = {R. Dwivedi and Y. Chen and M. J. Wainwright and B. Yu},
	Journal = {Journal of {M}achine {L}earning {R}esearch},
	Number = 183,
	Pages = {1--42},
	Title = {Log-concave sampling: {M}etropolis-{H}astings algorithms are fast.},
	Volume = 20,
	Year = 2019}

@inproceedings{safran2018spurious,
	Author = {Safran, Itay and Shamir, Ohad},
	Booktitle = {International Conference on Machine Learning},
	Pages = {4433--4441},
	Title = {Spurious Local Minima are Common in Two-Layer {ReLU} Neural Networks},
	Year = {2018}}

@inproceedings{livni2014computational,
	Author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {855--863},
	Title = {On the computational efficiency of training neural networks},
	Year = {2014}}

@article{manurangsi2018computational,
	Author = {Manurangsi, Pasin and Reichman, Daniel},
	Journal = {arXiv preprint arXiv:1810.04207},
	Title = {The computational complexity of training relu (s)},
	Year = {2018}}

@book{gautschi1997numerical,
	Author = {Gautschi, Walter},
	Publisher = {Springer Science \& Business Media},
	Title = {Numerical Analysis},
	Year = {1997}}

@inproceedings{scieur2017integration,
	Author = {Scieur, Damien and Roulet, Vincent and Bach, Francis and d'Aspremont, Alexandre},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {1109--1118},
	Title = {Integration methods and optimization algorithms},
	Year = {2017}}

@article{su2016differential,
	Author = {Su, Weijie and Boyd, Stephen and Cand{\`e}s, Emmanuel J.},
	Journal = {The Journal of Machine Learning Research},
	Number = {1},
	Pages = {5312--5354},
	Publisher = {JMLR. org},
	Title = {A differential equation for modeling Nesterov's accelerated gradient method: theory and insights},
	Volume = {17},
	Year = {2016}}

@article{alvarez2002second,
	Author = {Alvarez, Felipe and Attouch, Hedy and Bolte, J{\'e}r{\^o}me and Redont, Patrick},
	Journal = {Journal de math{\'e}matiques pures et appliqu{\'e}es},
	Number = {8},
	Pages = {747--779},
	Publisher = {Elsevier},
	Title = {A second-order gradient-like dissipative dynamical system with hessian-driven damping: Application to optimization and mechanics},
	Volume = {81},
	Year = {2002}}

@article{fan2019bcr,
	Author = {Fan, Yuwei and Bohorquez, Cindy Orozco and Ying, Lexing},
	Journal = {Journal of Computational Physics},
	Pages = {1--15},
	Publisher = {Elsevier},
	Title = {BCR-Net: A neural network based on the nonstandard wavelet form},
	Volume = {384},
	Year = {2019}}

@article{fan2020solvingEIT,
	Author = {Fan, Yuwei and Ying, Lexing},
	Journal = {Journal of Computational Physics},
	Pages = {109119},
	Publisher = {Elsevier},
	Title = {Solving electrical impedance tomography with deep learning},
	Volume = {404},
	Year = {2020}}

@article{weed2019sharp,
  title={Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance},
  author={Weed, Jonathan and Bach, Francis and others},
  journal={Bernoulli},
  volume={25},
  number={4A},
  pages={2620--2648},
  year={2019},
  publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@article{fan2019solvingOT,
	Author = {Fan, Yuwei and Ying, Lexing},
	Journal = {arXiv preprint arXiv:1910.04756},
	Title = {Solving optical tomography with deep learning},
	Year = {2019}}

@unpublished{mad,
title = {Ma{D}+ seminar},
note =  {\url{http://mad.cds.nyu.edu/madplus/}}
}

@unpublished{msml,
title = {Mathematical and Scientific Machine Learning},
note = {\url{http://msml-conf.org/}}
}

@article{fan2019solvingTT,
	Author = {Fan, Yuwei and Ying, Lexing},
	Journal = {arXiv preprint arXiv:1911.11636},
	Title = {Solving Traveltime Tomography with Deep Learning},
	Year = {2019}}

@article{fan2019solvingIWS,
	Author = {Fan, Yuwei and Ying, Lexing},
	Journal = {arXiv preprint arXiv:1911.13202},
	Title = {Solving Inverse Wave Scattering with Deep Learning},
	Year = {2019}}
	
	@inproceedings{mairal2014convolutional,
  title={Convolutional kernel networks},
  author={Mairal, Julien and Koniusz, Piotr and Harchaoui, Zaid and Schmid, Cordelia},
  booktitle={Advances in neural information processing systems},
  pages={2627--2635},
  year={2014}
}

@inproceedings{goldt2019dynamics,
	Author = {Goldt, Sebastian and Advani, Madhu and Saxe, Andrew M and Krzakala, Florent and Zdeborov{\'a}, Lenka},
	Booktitle = {Advances in Neural Information Processing Systems},
	Pages = {6979--6989},
	Title = {Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup},
	Year = {2019}}

@article{goldt2019generalisation,
	Author = {Goldt, Sebastian and Advani, Madhu S and Saxe, Andrew M and Krzakala, Florent and Zdeborova, Lenka},
	Journal = {arXiv preprint arXiv:1901.09085},
	Title = {Generalisation dynamics of online learning in over-parameterised neural networks},
	Year = {2019}}

@book{berlinet2011reproducing,
  title={Reproducing kernel Hilbert spaces in probability and statistics},
  author={Berlinet, Alain and Thomas-Agnan, Christine},
  year={2011},
  publisher={Springer Science \& Business Media}
}

@article{sriperumbudur2010hilbert,
  title={Hilbert space embeddings and metrics on probability measures},
  author={Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Sch{\"o}lkopf, Bernhard and Lanckriet, Gert RG},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={Apr},
  pages={1517--1561},
  year={2010}
}

@misc{lu2020universal,
	Author = {Lu, Jianfeng and Lu, Yulong},
	Notes = {submitted},
	Title = {A Universal Approximation Theorem of Deep Neural Networks for Expressing Distributions},
	Year = {2020}}

@article{khoo2019solving,
	Author = {Khoo, Yuehaw and Lu, Jianfeng and Ying, Lexing},
	Journal = {Research in the Mathematical Sciences},
	Number = {1},
	Pages = {1},
	Publisher = {Springer},
	Title = {Solving for high-dimensional committor functions using artificial neural networks},
	Volume = {6},
	Year = {2019}}

@article{bach2017equivalence,
  title={On the equivalence between kernel quadrature rules and random feature expansions},
  author={Bach, Francis},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={714--751},
  year={2017},
  publisher={JMLR. org}
}

@article{han2020solving,
	Author = {Han, Jiequn and Lu, Jianfeng and Zhou, Mo},
	Journal = {arXiv preprint arXiv:2002.02600},
	Title = {Solving high-dimensional eigenvalue problems using deep neural networks: A diffusion Monte Carlo like approach},
	Year = {2020}}

@article{auffinger2013complexity,
	Author = {Auffinger, Antonio and Arous, Gerard Ben and others},
	Journal = {The Annals of Probability},
	Number = {6},
	Pages = {4214--4247},
	Publisher = {Institute of Mathematical Statistics},
	Title = {Complexity of random smooth functions on the high-dimensional sphere},
	Volume = {41},
	Year = {2013}}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}



@article{subag2018following,
	Author = {Subag, Eliran},
	Journal = {arXiv preprint arXiv:1812.04588},
	Title = {Following the ground-states of full-RSB spherical spin glasses},
	Year = {2018}}

@article{lu2020mean,
	Author = {Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
	Journal = {arXiv preprint arXiv:2003.05508},
	Title = {A Mean-field Analysis of Deep ResNet and Beyond: Towards Provable Optimization Via Overparameterization From Depth},
    Year = {2020}}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}

@article{nguyen2020rigorous,
	Author = {Nguyen, Phan-Minh and Pham, Huy Tuan},
	Journal = {arXiv preprint arXiv:2001.11443},
	Title = {A rigorous framework for the mean field limit of multilayer neural networks},
	Year = {2020}}

@article{araujo2019mean,
	Author = {Ara{\'u}jo, Dyego and Oliveira, Roberto I and Yukimura, Daniel},
	Journal = {arXiv preprint arXiv:1906.00193},
	Title = {A mean-field limit for certain deep neural networks},
	Year = {2019}}

@article{celentano2020estimation,
  title={The estimation error of general first order methods},
  author={Celentano, Michael and Montanari, Andrea and Wu, Yuchen},
  journal={arXiv preprint arXiv:2002.12903},
  year={2020}
}

@article{alaoui2020optimization,
  title={Optimization of Mean-field Spin Glasses},
  author={Alaoui, Ahmed El and Montanari, Andrea and Sellke, Mark},
  journal={arXiv preprint arXiv:2001.00904},
  year={2020}
}

@inproceedings{rangan2011generalized,
  title={Generalized approximate message passing for estimation with random linear mixing},
  author={Rangan, Sundeep},
  booktitle={2011 IEEE International Symposium on Information Theory Proceedings},
  pages={2168--2172},
  year={2011},
  organization={IEEE}
}

@article{dalalyan2017theoretical,
	Author = {Dalalyan, Arnak S},
	Journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	Number = {3},
	Pages = {651--676},
	Publisher = {Wiley Online Library},
	Title = {Theoretical guarantees for approximate sampling from smooth and log-concave densities},
	Volume = {79},
	Year = {2017}}

@InProceedings{lin18res,
  author = 	 {Hongzhou Lin and Stefanie Jegelka},
  title = 	 {{ResNet} with one-neuron hidden layers is a Universal Approximator},
  booktitle = neurips,
  year = 	 2018}

@InProceedings{staib19,
  author = 	 {M. Staib and S. Jegelka},
  title = 	 {Distributionally Robust Optimization and Generalization in Kernel Methods},
  booktitle = neurips,
  year = 	 2019}



@article{yarotsky2018universal,
  title={Universal approximations of invariant maps by neural networks},
  author={Yarotsky, Dmitry},
  journal={arXiv preprint arXiv:1804.10306},
  year={2018}
}

@inproceedings{kondor2018generalization,
  title={On the Generalization of Equivariance and Convolution in Neural Networks to the Action of Compact Groups},
  author={Kondor, Risi and Trivedi, Shubhendu},
  booktitle={International Conference on Machine Learning},
  pages={2747--2755},
  year={2018}
}

@article{sannai2019improved,
  title={Improved generalization bound of permutation invariant deep neural networks},
  author={Sannai, Akiyoshi and Imaizumi, Masaaki},
  journal={arXiv preprint arXiv:1910.06552},
  year={2019}
}

@inproceedings{wagstaff2019limitations,
  title={On the Limitations of Representing Functions on Sets},
  author={Wagstaff, Edward and Fuchs, Fabian and Engelcke, Martin and Posner, Ingmar and Osborne, Michael A},
  booktitle={International Conference on Machine Learning},
  pages={6487--6494},
  year={2019}
}

@inproceedings{maron2019universality,
  title={On the Universality of Invariant Networks},
  author={Maron, Haggai and Fetaya, Ethan and Segol, Nimrod and Lipman, Yaron},
  booktitle={International Conference on Machine Learning},
  pages={4363--4371},
  year={2019}
}

@inproceedings{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  booktitle={Advances in neural information processing systems},
  pages={3391--3401},
  year={2017}
}

@article{sannai2019universal,
  title={Universal approximations of permutation invariant/equivariant functions by deep neural networks},
  author={Sannai, Akiyoshi and Takai, Yuuki and Cordonnier, Matthieu},
  journal={arXiv preprint arXiv:1903.01939},
  year={2019}
}

@inproceedings{hartford2018deep,
  title={Deep Models of Interactions Across Sets},
  author={Hartford, Jason and Graham, Devon and Leyton-Brown, Kevin and Ravanbakhsh, Siamak},
  booktitle={International Conference on Machine Learning},
  pages={1909--1918},
  year={2018}
}

@inproceedings{ravanbakhsh2017equivariance,
  title={Equivariance through parameter-sharing},
  author={Ravanbakhsh, Siamak and Schneider, Jeff and P{\'o}czos, Barnab{\'a}s},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2892--2901},
  year={2017}
}

@inproceedings{cohen2019general,
  title={A general theory of equivariant cnns on homogeneous spaces},
  author={Cohen, Taco S and Geiger, Mario and Weiler, Maurice},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9142--9153},
  year={2019}
}

@article{han2019universal,
  title={Universal approximation of symmetric and anti-symmetric functions},
  author={Han, Jiequn and Li, Yingzhou and Lin, Lin and Lu, Jianfeng and Zhang, Jiefu and Zhang, Linfeng},
  journal={arXiv preprint arXiv:1912.01765},
  year={2019}
}

@article{barvinok2005low,
  title={Low rank approximations of symmetric polynomials and asymptotic counting of contingency tables},
  author={Barvinok, Alexander},
  journal={arXiv preprint math/0503170},
  year={2005}
}

@article{domokos2006multisymmetric,
  title={Multisymmetric syzygies},
  author={Domokos, M},
  journal={arXiv preprint math.RT/0602303},
  year={2006}
}

@inproceedings{prohorov1961method,
  title={The method of characteristic functionals},
  author={Prohorov, Yu V},
  booktitle={Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability},
  volume={2},
  pages={403--419},
  year={1961},
  organization={University of California Berkeley, California}
}

@inproceedings{qi2017pointnet,
  title={Pointnet: Deep learning on point sets for 3d classification and segmentation},
  author={Qi, Charles R and Su, Hao and Mo, Kaichun and Guibas, Leonidas J},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={652--660},
  year={2017}
}

@article{pevny2019approximation,
  title={Approximation capability of neural networks on spaces of probability measures and tree-structured domains},
  author={Pevny, Tomas and Kovarik, Vojtech},
  journal={arXiv preprint arXiv:1906.00764},
  year={2019}
}

@article{vinyals2015order,
  title={Order matters: Sequence to sequence for sets},
  author={Vinyals, Oriol and Bengio, Samy and Kudlur, Manjunath},
  journal={arXiv preprint arXiv:1511.06391},
  year={2015}
}

@inproceedings{kakade2009complexity,
  title={On the complexity of linear prediction: Risk bounds, margin bounds, and regularization},
  author={Kakade, Sham M and Sridharan, Karthik and Tewari, Ambuj},
  booktitle={Advances in neural information processing systems},
  pages={793--800},
  year={2009}
}

@article{bolley2007quantitative,
  title={Quantitative concentration inequalities for empirical measures on non-compact spaces},
  author={Bolley, Fran{\c{c}}ois and Guillin, Arnaud and Villani, C{\'e}dric},
  journal={Probability Theory and Related Fields},
  volume={137},
  number={3-4},
  pages={541--593},
  year={2007},
  publisher={Springer}
}

@article{fournier2015rate,
  title={On the rate of convergence in Wasserstein distance of the empirical measure},
  author={Fournier, Nicolas and Guillin, Arnaud},
  journal={Probability Theory and Related Fields},
  volume={162},
  number={3-4},
  pages={707--738},
  year={2015},
  publisher={Springer}
}

@article{conca2013algebraic,
  title={An algebraic characterization of injectivity in phase retrieval},
  author={Conca, Aldo and Edidin, Dan and Hering, Milena and Vinzant, Cynthia},
  journal={arXiv preprint arXiv:1312.0158},
  year={2013}
}

@article{luxburg2004distance,
  title={Distance-based classification with Lipschitz functions},
  author={Luxburg, Ulrike von and Bousquet, Olivier},
  journal={Journal of Machine Learning Research},
  volume={5},
  number={Jun},
  pages={669--695},
  year={2004}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@book{macdonald1998symmetric,
  title={Symmetric functions and Hall polynomials},
  author={Macdonald, Ian Grant},
  year={1998},
  publisher={Oxford university press}
}

@inproceedings{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2933--2943},
  year={2019}
}

@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric},
  volume={338},
  year={2008},
  publisher={Springer Science \& Business Media}
}

@book{vershynin2018high,
  title={High-dimensional probability: An introduction with applications in data science},
  author={Vershynin, Roman},
  volume={47},
  year={2018},
  publisher={Cambridge university press}
}


@inproceedings{frogner2015learning,
  title={Learning with a Wasserstein loss},
  author={Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya, Mauricio and Poggio, Tomaso A},
  booktitle={Advances in neural information processing systems},
  pages={2053--2061},
  year={2015}
}

@inproceedings{hashimoto2016learning,
  title={Learning population-level diffusions with generative RNNs},
  author={Hashimoto, Tatsunori and Gifford, David and Jaakkola, Tommi},
  booktitle={International Conference on Machine Learning},
  pages={2417--2426},
  year={2016}
}

@article{srivastava2014dropout,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The journal of machine learning research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}