\begin{thebibliography}{63}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[An et~al.(2023)An, Gong, Zhong, Li, Zhang, Kong, and Qiu]{leval}
An, C., Gong, S., Zhong, M., Li, M., Zhang, J., Kong, L., and Qiu, X.
\newblock L-eval: Instituting standardized evaluation for long context language models.
\newblock \emph{arXiv preprint arXiv:2307.11088}, 2023.

\bibitem[{Anthropic}(2023)]{claude}
{Anthropic}.
\newblock Introducing {{100K Context Windows}}, 2023.
\newblock URL \url{https://www.anthropic.com/index/100k-context-windows}.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, Hui, Ji, Li, Lin, Lin, Liu, Liu, Lu, Lu, Ma, Men, Ren, Ren, Tan, Tan, Tu, Wang, Wang, Wang, Wu, Xu, Xu, Yang, Yang, Yang, Yang, Yao, Yu, Yuan, Yuan, Zhang, Zhang, Zhang, Zhang, Zhou, Zhou, Zhou, and Zhu]{bai2023qwen}
Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., Ma, J., Men, R., Ren, X., Ren, X., Tan, C., Tan, S., Tu, J., Wang, P., Wang, S., Wang, W., Wu, S., Xu, B., Xu, J., Yang, A., Yang, H., Yang, J., Yang, S., Yao, Y., Yu, B., Yuan, H., Yuan, Z., Zhang, J., Zhang, X., Zhang, Y., Zhang, Z., Zhou, C., Zhou, J., Zhou, X., and Zhu, T.
\newblock Qwen technical report, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Li, Meng, Liang, and Bing]{chen2023clex}
Chen, G., Li, X., Meng, Z., Liang, S., and Bing, L.
\newblock Clex: Continuous length extrapolation for large language models, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wong, Chen, and Tian]{pi}
Chen, S., Wong, S., Chen, L., and Tian, Y.
\newblock Extending context window of large language models via positional interpolation, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Qian, Tang, Lai, Liu, Han, and Jia]{longlora}
Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J.
\newblock Longlora: Efficient fine-tuning of long-context large language models.
\newblock \emph{arXiv:2309.12307}, 2023{\natexlab{c}}.

\bibitem[Chi et~al.(2023)Chi, Fan, Rudnicky, and Ramadge]{chi2023dissecting}
Chi, T.-C., Fan, T.-H., Rudnicky, A.~I., and Ramadge, P.~J.
\newblock Dissecting transformer length extrapolation via the lens of receptive field analysis, 2023.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{blocksparse}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Chowdhury \& Caragea(2023)Chowdhury and Caragea]{chowdhury2023monotonic}
Chowdhury, J.~R. and Caragea, C.
\newblock Monotonic location attention for length generalization, 2023.

\bibitem[Computer(2023)]{together2023redpajama}
Computer, T.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dao(2023)]{dao2023flashattention2}
Dao, T.
\newblock Flashattention-2: Faster attention with better parallelism and work partitioning, 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'{e}}]{flashattention}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'{e}}, C.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Dasigi et~al.(2021)Dasigi, Lo, Beltagy, Cohan, Smith, and Gardner]{qasper}
Dasigi, P., Lo, K., Beltagy, I., Cohan, A., Smith, N.~A., and Gardner, M.
\newblock A dataset of information-seeking questions and answers anchored in research papers.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  4599--4610, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.365}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.365}.

\bibitem[Han et~al.(2023)Han, Wang, Xiong, Chen, Ji, and Wang]{han2023lminfinite}
Han, C., Wang, Q., Xiong, W., Chen, Y., Ji, H., and Wang, S.
\newblock Lm-infinite: Simple on-the-fly length generalization for large language models, 2023.

\bibitem[He et~al.(2024)He, Feng, Luo, Yang, He, Xu, Zhang, Yang, and Wang]{he2024stones}
He, Z., Feng, G., Luo, S., Yang, K., He, D., Xu, J., Zhang, Z., Yang, H., and Wang, L.
\newblock Two stones hit one bird: Bilevel positional encoding for better length extrapolation, 2024.

\bibitem[Hu et~al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2021lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lora: Low-rank adaptation of large language models, 2021.

\bibitem[Jin et~al.(2024)Jin, Han, Yang, Jiang, Liu, Chang, Chen, and Hu]{selfextend}
Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C.-Y., Chen, H., and Hu, X.
\newblock Llm maybe longlm: Self-extend llm context window without tuning, 2024.

\bibitem[Kazemnejad et~al.(2023)Kazemnejad, Padhi, Ramamurthy, Das, and Reddy]{kazemnejad2023impact}
Kazemnejad, A., Padhi, I., Ramamurthy, K.~N., Das, P., and Reddy, S.
\newblock The impact of positional encoding on length generalization in transformers, 2023.

\bibitem[Ko{\v{c}}isk{\'y} et~al.(2018)Ko{\v{c}}isk{\'y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{narrativeqa}
Ko{\v{c}}isk{\'y}, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.~M., Melis, G., and Grefenstette, E.
\newblock The {N}arrative{QA} reading comprehension challenge.
\newblock \emph{Transactions of the Association for Computational Linguistics}, 6:\penalty0 317--328, 2018.
\newblock \doi{10.1162/tacl_a_00023}.
\newblock URL \url{https://aclanthology.org/Q18-1023}.

\bibitem[Lee et~al.(2023)Lee, Hartmann, Park, Papailiopoulos, and Lee]{Lee_2023}
Lee, G., Hartmann, V., Park, J., Papailiopoulos, D., and Lee, K.
\newblock Prompted llms as chatbot modules for long open-domain conversation.
\newblock In \emph{Findings of the Association for Computational Linguistics: ACL 2023}. Association for Computational Linguistics, 2023.
\newblock \doi{10.18653/v1/2023.findings-acl.277}.
\newblock URL \url{http://dx.doi.org/10.18653/v1/2023.findings-acl.277}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Shao, Xie, Sheng, Zheng, Gonzalez, Stoica, Ma, and Zhang]{longchat}
Li, D., Shao, R., Xie, A., Sheng, Y., Zheng, L., Gonzalez, J.~E., Stoica, I., Ma, X., and Zhang, H.
\newblock How long can open-source llms truly promise on context length.
\newblock 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, You, Guruganesh, Ainslie, Ontanon, Zaheer, Sanghai, Yang, Kumar, and Bhojanapalli]{li2023functional}
Li, S., You, C., Guruganesh, G., Ainslie, J., Ontanon, S., Zaheer, M., Sanghai, S., Yang, Y., Kumar, S., and Bhojanapalli, S.
\newblock Functional interpolation for relative positions improves long context transformers, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Lin, Hewitt, Paranjape, Bevilacqua, Petroni, and Liang]{lost_in_the_middle}
Liu, N.~F., Lin, K., Hewitt, J., Paranjape, A., Bevilacqua, M., Petroni, F., and Liang, P.
\newblock Lost in the middle: How language models use long contexts, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Yan, Zhang, An, Qiu, and Lin]{liu2023scaling}
Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., and Lin, D.
\newblock Scaling laws of rope-based extrapolation, 2023{\natexlab{b}}.

\bibitem[LMSYS(2023)]{vicuna}
LMSYS.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[LocalLLaMA(2023{\natexlab{a}})]{dynamicNTK}
LocalLLaMA.
\newblock Dynamically scaled rope further increases performance of long context llama with zero fine-tuning, July 2023{\natexlab{a}}.
\newblock URL \url{https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}.

\bibitem[LocalLLaMA(2023{\natexlab{b}})]{fixedNTK}
LocalLLaMA.
\newblock Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation., June 2023{\natexlab{b}}.
\newblock URL \url{https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}.

\bibitem[Lv et~al.(2024)Lv, Liu, Guo, Yan, He, Qiu, and Lin]{lv2024longwanjuan}
Lv, K., Liu, X., Guo, Q., Yan, H., He, C., Qiu, X., and Lin, D.
\newblock Longwanjuan: Towards systematic measurement for long text quality, 2024.

\bibitem[Mohtashami \& Jaggi(2023)Mohtashami and Jaggi]{passkey}
Mohtashami, A. and Jaggi, M.
\newblock Landmark attention: Random-access infinite context length for transformers.
\newblock \emph{arXiv preprint arXiv:2305.16300}, 2023.

\bibitem[MosaicML(2023{\natexlab{a}})]{MPT30b}
MosaicML.
\newblock Introducing mpt-30b: Raising the bar for open-source foundation models, 2023{\natexlab{a}}.
\newblock URL \url{www.mosaicml.com/blog/mpt-30b}.
\newblock Accessed: 2023-06-22.

\bibitem[MosaicML(2023{\natexlab{b}})]{MPT7b}
MosaicML.
\newblock Introducing mpt-7b: A new standard for open-source, ly usable llms, 2023{\natexlab{b}}.
\newblock URL \url{www.mosaicml.com/blog/mpt-7b}.

\bibitem[OpenAI(2023)]{gpt-4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pang et~al.(2022)Pang, Parrish, Joshi, Nangia, Phang, Chen, Padmakumar, Ma, Thompson, He, and Bowman]{quality}
Pang, R.~Y., Parrish, A., Joshi, N., Nangia, N., Phang, J., Chen, A., Padmakumar, V., Ma, J., Thompson, J., He, H., and Bowman, S.
\newblock {Q}u{ALITY}: Question answering with long input texts, yes!
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5336--5358, Seattle, United States, July 2022. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2022.naacl-main.391}.
\newblock URL \url{https://aclanthology.org/2022.naacl-main.391}.

\bibitem[Peng et~al.(2023)Peng, Quesnelle, Fan, and Shippole]{yarn}
Peng, B., Quesnelle, J., Fan, H., and Shippole, E.
\newblock Yarn: Efficient context window extension of large language models, 2023.

\bibitem[Press et~al.(2022)Press, Smith, and Lewis]{alibi}
Press, O., Smith, N.~A., and Lewis, M.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation, 2022.

\bibitem[Qin et~al.(2024)Qin, Sun, Li, Shen, Sun, and Zhong]{Qin2024LightningAA}
Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., and Zhong, Y.
\newblock Lightning attention-2: A free lunch for handling unlimited sequence lengths in large language models.
\newblock \emph{ArXiv}, abs/2401.04658, 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:266900042}.

\bibitem[Rae et~al.(2020)Rae, Potapenko, Jayakumar, Hillier, and Lillicrap]{pg19}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., Hillier, C., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock In \emph{8th International Conference on Learning Representations, {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.
\newblock URL \url{https://openreview.net/forum?id=SylKikSYDH}.

\bibitem[Ratner et~al.(2023)Ratner, Levine, Belinkov, Ram, Magar, Abend, Karpas, Shashua, Leyton-Brown, and Shoham]{ratner2023parallel}
Ratner, N., Levine, Y., Belinkov, Y., Ram, O., Magar, I., Abend, O., Karpas, E., Shashua, A., Leyton-Brown, K., and Shoham, Y.
\newblock Parallel context windows for large language models, 2023.

\bibitem[Robertson et~al.(2009)Robertson, Zaragoza, et~al.]{robertson2009probabilistic}
Robertson, S., Zaragoza, H., et~al.
\newblock The probabilistic relevance framework: Bm25 and beyond.
\newblock \emph{Foundations and Trends{\textregistered} in Information Retrieval}, 3\penalty0 (4):\penalty0 333--389, 2009.

\bibitem[Rozière et~al.(2023)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{codellama}
Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C.~C., Grattafiori, A., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G.
\newblock Code llama: Open foundation models for code, 2023.

\bibitem[Rula \& D'Souza(2023)Rula and D'Souza]{rula2023procedural}
Rula, A. and D'Souza, J.
\newblock Procedural text mining with large language models, 2023.

\bibitem[Ruoss et~al.(2023)Ruoss, Delétang, Genewein, Grau-Moya, Csordás, Bennani, Legg, and Veness]{ruoss2023randomized}
Ruoss, A., Delétang, G., Genewein, T., Grau-Moya, J., Csordás, R., Bennani, M., Legg, S., and Veness, J.
\newblock Randomized positional encodings boost length generalization of transformers, 2023.

\bibitem[Saad-Falcon et~al.(2023)Saad-Falcon, Barrow, Siu, Nenkova, Yoon, Rossi, and Dernoncourt]{saadfalcon2023pdftriage}
Saad-Falcon, J., Barrow, J., Siu, A., Nenkova, A., Yoon, D.~S., Rossi, R.~A., and Dernoncourt, F.
\newblock Pdftriage: Question answering over long, structured documents, 2023.

\bibitem[Song et~al.(2023)Song, Wang, Cho, Pan, and Yu]{song2023zebra}
Song, K., Wang, X., Cho, S., Pan, X., and Yu, D.
\newblock Zebra: Extending context window with layerwise grouped local-global attention, 2023.

\bibitem[Su(2023)]{rerope2023}
Su, J.
\newblock Rectified rotary position embeddings.
\newblock \url{https://github.com/bojone/rerope}, 2023.

\bibitem[Su et~al.(2022)Su, Lu, Pan, Murtadha, Wen, and Liu]{rope}
Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2022.

\bibitem[Sun et~al.(2022)Sun, Dong, Patra, Ma, Huang, Benhaim, Chaudhary, Song, and Wei]{xpos}
Sun, Y., Dong, L., Patra, B., Ma, S., Huang, S., Benhaim, A., Chaudhary, V., Song, X., and Wei, F.
\newblock A length-extrapolatable transformer, 2022.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Together(2023)]{together}
Together.
\newblock Llama-2-7b-32k-instruct — and fine-tuning for llama-2 models with together api, 2023.
\newblock URL \url{https://together.ai/blog/llama-2-7b-32k-instruct}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozière, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Tworkowski et~al.(2023)Tworkowski, Staniszewski, Pacek, Wu, Michalewski, and Miłoś]{focused}
Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Miłoś, P.
\newblock Focused transformer: Contrastive training for context scaling, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformers}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need, 2017.

\bibitem[Wang et~al.(2024)Wang, Yang, and Wei]{wang2024learning}
Wang, L., Yang, N., and Wei, F.
\newblock Learning to retrieve in-context examples for large language models, 2024.

\bibitem[Wei et~al.(2023)Wei, Kim, Jung, and Kim]{wei2023leveraging}
Wei, J., Kim, S., Jung, H., and Kim, Y.-H.
\newblock Leveraging large language models to power chatbots for collecting user self-reported data, 2023.

\bibitem[Xiao et~al.(2024)Xiao, Zhang, Han, Xiao, Lin, Zhang, Liu, Han, and Sun]{xiao2024infllm}
Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M.
\newblock Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory, 2024.

\bibitem[Xiao et~al.(2023)Xiao, Tian, Chen, Han, and Lewis]{xiao2023efficient}
Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M.
\newblock Efficient streaming language models with attention sinks, 2023.

\bibitem[Xiong et~al.(2023)Xiong, Liu, Molybog, Zhang, Bhargava, Hou, Martin, Rungta, Sankararaman, Oguz, Khabsa, Fang, Mehdad, Narang, Malik, Fan, Bhosale, Edunov, Lewis, Wang, and Ma]{llama2long}
Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K.~A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H.
\newblock Effective long-context scaling of foundation models.
\newblock \emph{CoRR}, abs/2309.16039, 2023.
\newblock \doi{10.48550/ARXIV.2309.16039}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2309.16039}.

\bibitem[Ye et~al.(2023)Ye, Wu, Feng, Yu, and Kong]{ye2023compositional}
Ye, J., Wu, Z., Feng, J., Yu, T., and Kong, L.
\newblock Compositional exemplars for in-context learning.
\newblock \emph{arXiv preprint arXiv:2302.05698}, 2023.

\bibitem[Zhang et~al.(2023)Zhang, Jiang, Feng, Zheng, and Kong]{Zhang2023LinearAV}
Zhang, J., Jiang, S., Feng, J., Zheng, L., and Kong, L.
\newblock Linear attention via orthogonal memory.
\newblock \emph{ArXiv}, abs/2312.11135, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:266359128}.

\bibitem[Zhang et~al.(2024)Zhang, Liu, Xiao, Shao, Ye, and Dou]{Zhang2024SoaringF4}
Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z.
\newblock Soaring from 4k to 400k: Extending llm's context with activation beacon.
\newblock \emph{ArXiv}, abs/2401.03462, 2024.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:266844488}.

\bibitem[Zhong et~al.(2021)Zhong, Yin, Yu, Zaidi, Mutuma, Jha, Awadallah, Celikyilmaz, Liu, Qiu, and Radev]{qmsum}
Zhong, M., Yin, D., Yu, T., Zaidi, A., Mutuma, M., Jha, R., Awadallah, A.~H., Celikyilmaz, A., Liu, Y., Qiu, X., and Radev, D.
\newblock {QMS}um: A new benchmark for query-based multi-domain meeting summarization.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pp.\  5905--5921, Online, June 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.naacl-main.472}.
\newblock URL \url{https://aclanthology.org/2021.naacl-main.472}.

\bibitem[Zhu et~al.(2023)Zhu, Yang, Wang, Song, Wu, Wei, and Li]{zhu2023pose}
Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S.
\newblock Pose: Efficient context window extension of llms via positional skip-wise training, 2023.

\end{thebibliography}
