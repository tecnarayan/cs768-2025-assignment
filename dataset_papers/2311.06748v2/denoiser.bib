@article{hanin2021ridgeless,
  title={Ridgeless Interpolation with Shallow ReLU Networks in $1 D $ is Nearest Neighbor Curvature Extrapolation and Provably Generalizes on Lipschitz Functions},
  author={Hanin, Boris},
  journal={arXiv preprint arXiv:2109.12960},
  year={2021}
}
@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{raya2023spontaneous,
  title={Spontaneous symmetry breaking in generative diffusion models},
  author={Raya, Gabriel and Ambrogioni, Luca},
  journal={arXiv preprint arXiv:2305.19693},
  year={2023}
}

@article{bansal2022cold,
  title={Cold diffusion: Inverting arbitrary image transforms without noise},
  author={Bansal, Arpit and Borgnia, Eitan and Chu, Hong-Min and Li, Jie S and Kazemi, Hamid and Huang, Furong and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
  journal={arXiv preprint arXiv:2208.09392},
  year={2022}
}
@article{horrace2015moments,
  title={Moments of the truncated normal distribution},
  author={Horrace, William C},
  journal={Journal of Productivity Analysis},
  volume={43},
  pages={133--138},
  year={2015},
  publisher={Springer}
}
@article{Manjunath_Wilhelm_2021, title={Moments Calculation for the Doubly Truncated Multivariate Normal Density}, volume={1}, url={https://jbds.isdsa.org/index.php/jbds/article/view/9}, DOI={10.35566/jbds/v1n1/p2}, abstractNote={&amp;lt;p&amp;gt;In the present article, we derive an explicit expression for the truncated mean and variance for the multivariate normal distribution with arbitrary rectangular double truncation. We use the moment generating approach of Tallis (1961) and extend it to general&amp;amp;nbsp;&amp;lt;span id=&amp;quot;MathJax-Element-1-Frame&amp;quot; class=&amp;quot;mjx-chtml MathJax_CHTML&amp;quot; style=&amp;quot;display: inline-block; line-height: 0; text-indent: 0px; text-align: left; text-transform: none; font-style: normal; font-weight: 400; font-size: 16.38px; letter-spacing: normal; overflow-wrap: normal; word-spacing: 0px; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; margin: 0px; padding: 1px 0px; color: #333333; font-family: Arial, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: #ffffff; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; position: relative;&amp;quot; tabindex=&amp;quot;0&amp;quot; role=&amp;quot;presentation&amp;quot; data-mathml=&amp;quot;&amp;lt;math xmlns=&amp;amp;quot;http://www.w3.org/1998/Math/MathML&amp;amp;quot;&amp;gt;&amp;lt;mrow class=&amp;amp;quot;MJX-TeXAtom-ORD&amp;amp;quot;&amp;gt;&amp;lt;mi&amp;gt;&amp;amp;amp;#x03BC;&amp;lt;/mi&amp;gt;&amp;lt;/mrow&amp;gt;&amp;lt;/math&amp;gt;&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;MJX_Assistive_MathML&amp;quot; role=&amp;quot;presentation&amp;quot;&amp;gt;μ&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;,&amp;amp;nbsp;&amp;lt;span id=&amp;quot;MathJax-Element-2-Frame&amp;quot; class=&amp;quot;mjx-chtml MathJax_CHTML&amp;quot; style=&amp;quot;display: inline-block; line-height: 0; text-indent: 0px; text-align: left; text-transform: none; font-style: normal; font-weight: 400; font-size: 16.38px; letter-spacing: normal; overflow-wrap: normal; word-spacing: 0px; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; margin: 0px; padding: 1px 0px; color: #333333; font-family: Arial, sans-serif; font-variant-ligatures: normal; font-variant-caps: normal; orphans: 2; widows: 2; -webkit-text-stroke-width: 0px; background-color: #ffffff; text-decoration-thickness: initial; text-decoration-style: initial; text-decoration-color: initial; position: relative;&amp;quot; tabindex=&amp;quot;0&amp;quot; role=&amp;quot;presentation&amp;quot; data-mathml=&amp;quot;&amp;lt;math xmlns=&amp;amp;quot;http://www.w3.org/1998/Math/MathML&amp;amp;quot;&amp;gt;&amp;lt;mrow class=&amp;amp;quot;MJX-TeXAtom-ORD&amp;amp;quot;&amp;gt;&amp;lt;mi mathvariant=&amp;amp;quot;bold&amp;amp;quot;&amp;gt;&amp;amp;amp;#x03A3;&amp;lt;/mi&amp;gt;&amp;lt;/mrow&amp;gt;&amp;lt;/math&amp;gt;&amp;quot;&amp;gt;&amp;lt;span id=&amp;quot;MJXc-Node-6&amp;quot; class=&amp;quot;mjx-math&amp;quot; aria-hidden=&amp;quot;true&amp;quot;&amp;gt;&amp;lt;span id=&amp;quot;MJXc-Node-7&amp;quot; class=&amp;quot;mjx-mrow&amp;quot;&amp;gt;&amp;lt;span id=&amp;quot;MJXc-Node-8&amp;quot; class=&amp;quot;mjx-texatom&amp;quot;&amp;gt;&amp;lt;span id=&amp;quot;MJXc-Node-9&amp;quot; class=&amp;quot;mjx-mrow&amp;quot;&amp;gt;&amp;lt;span id=&amp;quot;MJXc-Node-10&amp;quot; class=&amp;quot;mjx-mi&amp;quot;&amp;gt;&amp;lt;span class=&amp;quot;mjx-char MJXc-TeX-main-B&amp;quot;&amp;gt;Σ&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;lt;/span&amp;gt;&amp;amp;nbsp;and all combinations of truncation. As part of the solution, we also give a formula for the bivariate marginal density of truncated multinormal variates. We also prove an invariance property of some elements of the inverse covariance after truncation. Computer algorithms for computing the truncated mean, variance and the bivariate marginal probabilities for doubly truncated multivariate normal variates have been written in R and are presented along with three examples.&amp;lt;/p&amp;gt;}, number={1}, journal={Journal of Behavioral Data Science}, author={Manjunath, B. G. and Wilhelm, Stefan}, year={2021}, month={May}, pages={17–33} }
@inproceedings{
rissanen2023generative,
title={Generative Modelling with Inverse Heat Dissipation},
author={Severi Rissanen and Markus Heinonen and Arno Solin},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=4PJUBT9f2Ol}
}
@article{radhakrishnan2018memorization,
  title={Memorization in overparameterized autoencoders},
  author={Radhakrishnan, Adityanarayanan and Yang, Karren and Belkin, Mikhail and Uhler, Caroline},
  journal={arXiv preprint arXiv:1810.10333},
  year={2018}
}
@InProceedings{pmlr-v32-cheng14,
  title = 	 {Marginalized Denoising Auto-encoders for Nonlinear Representations},
  author = 	 {Chen, Minmin and Weinberger, Kilian and Sha, Fei and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
  pages = 	 {1476--1484},
  year = 	 {2014},
  editor = 	 {Xing, Eric P. and Jebara, Tony},
  volume = 	 {32},
  number =       {2},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Bejing, China},
  month = 	 {22--24 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v32/cheng14.pdf},
  url = 	 {https://proceedings.mlr.press/v32/cheng14.html},
  abstract = 	 {Denoising auto-encoders (DAEs) have been successfully  used to learn new representations for a  wide range of machine learning tasks. During  training, DAEs make many passes over the training  dataset and reconstruct it from partial corruption  generated from a pre-specified corrupting  distribution. This process learns robust representation,  though at the expense of requiring many  training epochs, in which the data is explicitly  corrupted. In this paper we present the marginalized  Denoising Auto-encoder (mDAE), which  (approximately) marginalizes out the corruption  during training. Effectively, the mDAE takes  into account infinitely many corrupted copies of  the training data in every epoch, and therefore is  able to match or outperform the DAE with much  fewer training epochs. We analyze our proposed  algorithm and show that it can be understood as  a classic auto-encoder with a special form of regularization.  In empirical evaluations we show  that it attains 1-2 order-of-magnitude speedup in  training time over other competing approaches.}
}
@inproceedings{
Ongie2020A,
title={A Function Space View of Bounded Norm Infinite Width ReLU Nets: The Multivariate Case},
author={Greg Ongie and Rebecca Willett and Daniel Soudry and Nathan Srebro},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=H1lNPxHKDH}
}
@article{gribonval2011should,
  title={Should penalized least squares regression be interpreted as maximum a posteriori estimation?},
  author={Gribonval, R{\'e}mi},
  journal={IEEE Transactions on Signal Processing},
  volume={59},
  number={5},
  pages={2405--2410},
  year={2011},
  publisher={IEEE}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@article{alain2014regularized,
  title={What regularized auto-encoders learn from the data-generating distribution},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={3563--3593},
  year={2014},
  publisher={JMLR. org}
}
@article{doi:10.1137/23M1545859,
author = {Elad, Michael and Kawar, Bahjat and Vaksman, Gregory},
title = {Image Denoising: The Deep Learning Revolution and Beyond—A Survey Paper},
journal = {SIAM Journal on Imaging Sciences},
volume = {16},
number = {3},
pages = {1594-1654},
year = {2023},
doi = {10.1137/23M1545859},

URL = { 
    
        https://doi.org/10.1137/23M1545859
    
    

},
eprint = { 
    
        https://doi.org/10.1137/23M1545859
    
    

}
,
    abstract = { Abstract. Image denoising—removal of additive white Gaussian noise from an image—is one of the oldest and most studied problems in image processing. Extensive work over several decades has led to thousands of papers on this subject, and to many well-performing algorithms for this task. Indeed, 10 years ago, these achievements led some researchers to suspect that “Denoising is Dead,” in the sense that all that can be achieved in this domain has already been obtained. However, this turned out to be far from the truth, with the penetration of deep learning (DL) into the realm of image processing. The era of DL brought a revolution to image denoising, both by taking the lead in today’s ability for noise suppression in images, and by broadening the scope of denoising problems being treated. Our paper starts by describing this evolution, highlighting in particular the tension and synergy that exist between classical approaches and modern artificial intelligence (AI) alternatives in design of image denoisers. The recent transitions in the field of image denoising go far beyond the ability to design better denoisers. In the second part of this paper we focus on recently discovered abilities and prospects of image denoisers. We expose the possibility of using image denoisers for service of other problems, such as regularizing general inverse problems and serving as the prime engine in diffusion-based image synthesis. We also unveil the (strange?) idea that denoising and other inverse problems might not have a unique solution, as common algorithms would have us believe. Instead, we describe constructive ways to produce randomized and diverse high perceptual quality results for inverse problems, all fueled by the progress that DL brought to image denoising. This is a survey paper, and its prime goal is to provide a broad view of the history of the field of image denoising and closely related topics in image processing. Our aim is to give a better context to recent discoveries, and to the influence of the AI revolution in our domain. }
}
@article{zhang2021plug,
  title={Plug-and-play image restoration with deep denoiser prior},
  author={Zhang, Kai and Li, Yawei and Zuo, Wangmeng and Zhang, Lei and Van Gool, Luc and Timofte, Radu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={10},
  pages={6360--6376},
  year={2021},
  publisher={IEEE}
}
@article{zhang2017beyond,
  title={Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising},
  author={Zhang, Kai and Zuo, Wangmeng and Chen, Yunjin and Meng, Deyu and Zhang, Lei},
  journal={IEEE transactions on image processing},
  volume={26},
  number={7},
  pages={3142--3155},
  year={2017},
  publisher={IEEE}
}
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}
@inproceedings{savarese2019infinite,
  title={How do infinite width bounded norm networks look in function space?},
  author={Savarese, Pedro and Evron, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={2667--2690},
  year={2019},
  organization={PMLR}
}

@article{cite-key,
	abstract = {We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach provides a practical method for learning high-order Markov random field (MRF) models with potential functions that extend over large pixel neighborhoods. These clique potentials are modeled using the Product-of-Experts framework that uses non-linear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field-of-Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with specialized techniques.},
	author = {Roth, Stefan and Black, Michael J.},
	date = {2009/04/01},
	date-added = {2023-05-14 16:22:58 +0300},
	date-modified = {2023-05-14 16:22:58 +0300},
	doi = {10.1007/s11263-008-0197-6},
	id = {Roth2009},
	isbn = {1573-1405},
	journal = {International Journal of Computer Vision},
	number = {2},
	pages = {205--229},
	title = {Fields of Experts},
	url = {https://doi.org/10.1007/s11263-008-0197-6},
	volume = {82},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1007/s11263-008-0197-6}}
@inproceedings{
sahiner2021convex,
title={Convex Regularization behind Neural Reconstruction},
author={Arda Sahiner and Morteza Mardani and Batu Ozturkler and Mert Pilanci and John M. Pauly},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=VErQxgyrbfn}
}
@inproceedings{hasinoff2010noise,
  title={Noise-optimal capture for high dynamic range photography},
  author={Hasinoff, Samuel W and Durand, Fr{\'e}do and Freeman, William T},
  booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
  pages={553--560},
  year={2010},
  organization={IEEE}
}

@article{ergen2021convex,
  title={Convex geometry and duality of over-parameterized neural networks},
  author={Ergen, Tolga and Pilanci, Mert},
  journal={The Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={9646--9708},
  year={2021},
  publisher={JMLRORG}
}

@inproceedings{neyshabur2015norm,
  title={Norm-based capacity control in neural networks},
  author={Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  booktitle={Conference on Learning Theory (COLT)},
  pages={1376--1401},
  year={2015},
  organization={PMLR}
}

@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={30},
  year={2017}
}

@article{parhi2021banach,
  title={Banach space representer theorems for neural networks and ridge splines},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={1},
  pages={1960--1999},
  year={2021}
}

@article{parhi2022kinds,
  title={What kinds of functions do deep neural networks learn? Insights from variational spline theory},
  author={Parhi, Rahul and Nowak, Robert D},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={4},
  number={2},
  pages={464--489},
  year={2022},
  publisher={SIAM}
}

@inproceedings{nacsonimplicit,
  title={The Implicit Bias of Minima Stability in Multivariate Shallow ReLU Networks},
  author={Nacson, Mor Shpigel and Mulayoff, Rotem and Ongie, Greg and Michaeli, Tomer and Soudry, Daniel},
  booktitle={International Conference on Learning Representations (ICLR)},
    year={2023}
}

@article{mulayoff2021implicit,
  title={The implicit bias of minima stability: A view from function space},
  author={Mulayoff, Rotem and Michaeli, Tomer and Soudry, Daniel},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={34},
  pages={17749--17761},
  year={2021}
}
@article{udell2019big,
  title={Why are big data matrices approximately low rank?},
  author={Udell, Madeleine and Townsend, Alex},
  journal={SIAM Journal on Mathematics of Data Science},
  volume={1},
  number={1},
  pages={144--160},
  year={2019},
  publisher={SIAM}
}
@article{
sonthalia2023training,
title={Training Data Size Induced Double Descent For Denoising Feedforward Neural Networks and the Role of Training Noise},
author={Rishi Sonthalia and Raj Rao Nadakuditi},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2023},
url={https://openreview.net/forum?id=FdMWtpVT1I},
note={}
}


@inproceedings{jacot2022implicit,
  title={Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions},
  author={Jacot, Arthur},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@inproceedings{jacotNeurips,
 author = {Jacot, Arthur and Golikov, Eugene and Hongler, Clement and Gabriel, Franck},
 booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
 pages = {6763--6774},
 title = {Feature Learning in L\_2-regularized DNNs: Attraction/Repulsion and Sparsity},
 volume = {35},
 year = {2022}
}

@article{boursier2023penalising,
  title={Penalising the biases in norm regularisation enforces sparsity},
  author={Boursier, Etienne and Flammarion, Nicolas},
  journal={arXiv preprint arXiv:2303.01353},
  year={2023}
}

@article{shenouda2023vector,
  title={Vector-Valued Variation Spaces and Width Bounds for {DNN}s: Insights on Weight Decay Regularization},
  author={Shenouda, Joseph and Parhi, Rahul and Lee, Kangwook and Nowak, Robert D},
  journal={arXiv preprint arXiv:2305.16534},
  year={2023}
}