\begin{thebibliography}{10}

\bibitem{an2021uncertainty}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock {\em Proc. of NeurIPS}, 2021.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em ArXiv preprint}, 2016.

\bibitem{bai2021pessimistic}
Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi-Hong Deng, Animesh Garg, Peng
  Liu, and Zhaoran Wang.
\newblock Pessimistic bootstrapping for uncertainty-driven offline
  reinforcement learning.
\newblock In {\em Proc. of ICLR}, 2021.

\bibitem{brandfonbrener2022does}
David Brandfonbrener, Alberto Bietti, Jacob Buckman, Romain Laroche, and Joan
  Bruna.
\newblock When does return-conditioned supervised learning work for offline
  reinforcement learning?
\newblock In {\em Proc. of NeurIPS}, 2022.

\bibitem{brandfonbrener2021offline}
David Brandfonbrener, William~F Whitney, Rajesh Ranganath, and Joan Bruna.
\newblock Offline rl without off-policy evaluation.
\newblock {\em Proc. of NeurIPS}, 2021.

\bibitem{brandfonbrener2021quantile}
David Brandfonbrener, William~F Whitney, Rajesh Ranganath, and Joan Bruna.
\newblock Quantile filtered imitation learning.
\newblock {\em ArXiv preprint}, 2021.

\bibitem{chen2021decision}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock {\em Proc. of NeurIPS}, 2021.

\bibitem{chen2020bail}
Xinyue Chen, Zijian Zhou, Zheng Wang, Che Wang, Yanqiu Wu, and Keith~W. Ross.
\newblock {BAIL:} best-action imitation learning for batch deep reinforcement
  learning.
\newblock In {\em Proc. of NeurIPS}, 2020.

\bibitem{christiano2016transfer}
Paul Christiano, Zain Shah, Igor Mordatch, Jonas Schneider, Trevor Blackwell,
  Joshua Tobin, Pieter Abbeel, and Wojciech Zaremba.
\newblock Transfer from simulation to real world through learning deep inverse
  dynamics model.
\newblock {\em ArXiv preprint}, 2016.

\bibitem{emmons2021rvs}
Scott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine.
\newblock Rvs: What is essential for offline rl via supervised learning?
\newblock {\em ArXiv preprint}, 2021.

\bibitem{eysenbach2022imitating}
Benjamin Eysenbach, Soumith Udatha, Sergey Levine, and Ruslan Salakhutdinov.
\newblock Imitating past successes can be very suboptimal.
\newblock In {\em Proc. of NeurIPS}, 2022.

\bibitem{florensa2018automatic}
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In {\em Proc. of ICML}, 2018.

\bibitem{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock {\em ArXiv preprint}, 2020.

\bibitem{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock {\em ArXiv preprint}, 2021.

\bibitem{fujimoto2018addressing}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em Proc. of ICML}, pages 1582--1591, 2018.

\bibitem{ghosh2020learning}
Dibya Ghosh, Abhishek Gupta, Ashwin Reddy, Justin Fu, Coline~Manon Devin,
  Benjamin Eysenbach, and Sergey Levine.
\newblock Learning to reach goals via iterated supervised learning.
\newblock In {\em Proc. of ICLR}, 2021.

\bibitem{gulcehre2021regularized}
Caglar Gulcehre, Sergio~G{\'o}mez Colmenarejo, Ziyu Wang, Jakub Sygnowski,
  Thomas Paine, Konrad Zolna, Yutian Chen, Matthew Hoffman, Razvan Pascanu, and
  Nando de~Freitas.
\newblock Regularized behavior value estimation.
\newblock {\em ArXiv preprint}, 2021.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em Proc. of ICML}, pages 1856--1865, 2018.

\bibitem{hinton2002stochastic}
Geoffrey~E. Hinton and Sam~T. Roweis.
\newblock Stochastic neighbor embedding.
\newblock In {\em Proc. of NeurIPS}, pages 833--840, 2002.

\bibitem{janner2021offline}
Michael Janner, Qiyang Li, and Sergey Levine.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock {\em Proc. of NeurIPS}, 2021.

\bibitem{kakade2001natural}
Sham~M. Kakade.
\newblock A natural policy gradient.
\newblock In {\em Proc. of NeurIPS}, pages 1531--1538, 2001.

\bibitem{kalashnikov2021mt}
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico
  Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock {\em ArXiv preprint}, 2021.

\bibitem{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock In {\em Proc. of NeurIPS}, 2020.

\bibitem{adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In {\em Proc. of ICLR}, 2015.

\bibitem{kostrikov2021offline}
Ilya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum.
\newblock Offline reinforcement learning with fisher divergence critic
  regularization.
\newblock In {\em Proc. of ICML}, pages 5774--5783, 2021.

\bibitem{kostrikov2021iql}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock {\em ArXiv preprint}, 2021.

\bibitem{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In {\em Proc. of NeurIPS}, pages 11761--11771, 2019.

\bibitem{kumar2019reward}
Aviral Kumar, Xue~Bin Peng, and Sergey Levine.
\newblock Reward-conditioned policies.
\newblock {\em ArXiv preprint}, 2019.

\bibitem{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock In {\em Proc. of NeurIPS}, 2020.

\bibitem{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In {\em Reinforcement learning}. 2012.

\bibitem{laskin_srinivas2020curl}
Michael Laskin, Aravind Srinivas, and Pieter Abbeel.
\newblock {CURL:} contrastive unsupervised representations for reinforcement
  learning.
\newblock In {\em Proc. of ICML}, pages 5639--5650, 2020.

\bibitem{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock {\em ArXiv preprint}, 2020.

\bibitem{liu2022learning}
Hsueh-Ti~Derek Liu, Francis Williams, Alec Jacobson, Sanja Fidler, and
  Or~Litany.
\newblock Learning smooth neural functions via lipschitz regularization.
\newblock {\em ArXiv preprint}, 2022.

\bibitem{liu2022plan}
Minghuan Liu, Zhengbang Zhu, Yuzheng Zhuang, Weinan Zhang, Jianye Hao, Yong Yu,
  and Jun Wang.
\newblock Plan your target and learn your skills: Transferable state-only
  imitation learning via decoupled policy optimization.
\newblock {\em ArXiv preprint}, 2022.

\bibitem{lynch2020learning}
Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey
  Levine, and Pierre Sermanet.
\newblock Learning latent plans from play.
\newblock In {\em Conference on robot learning}, 2020.

\bibitem{ma2022offline}
Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan
  Zhao, and Bin Liang.
\newblock Offline reinforcement learning with value-based episodic memory.
\newblock In {\em Proc. of ICLR}, 2022.

\bibitem{miyato2018spectral}
Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In {\em Proc. of ICLR}, 2018.

\bibitem{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock {\em ArXiv preprint}, 2019.

\bibitem{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock {\em ArXiv preprint}, 2020.

\bibitem{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A. Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In {\em Proc. of ICML}, pages 2778--2787, 2017.

\bibitem{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock {\em ArXiv preprint}, 2019.

\bibitem{peters2007reinforcement}
Jan Peters and Stefan Schaal.
\newblock Reinforcement learning by reward-weighted regression for operational
  space control.
\newblock In {\em Proc. of ICML}, 2007.

\bibitem{pomerleau1989alvinn}
Dean~A Pomerleau.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock In {\em Proc. of NeurIPS}, 1989.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael~I. Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em Proc. of ICML}, pages 1889--1897, 2015.

\bibitem{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin~A. Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em Proc. of ICML}, pages 387--395, 2014.

\bibitem{srivastava2019training}
Rupesh~Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja{\'s}kowski, and
  J{\"u}rgen Schmidhuber.
\newblock Training agents using upside-down reinforcement learning.
\newblock {\em ArXiv preprint}, 2019.

\bibitem{sutton1998introduction}
Richard~S Sutton, Andrew~G Barto, et~al.
\newblock {\em Introduction to reinforcement learning}.
\newblock MIT press Cambridge, 1998.

\bibitem{tang2021model}
Shengpu Tang and Jenna Wiens.
\newblock Model selection for offline reinforcement learning: Practical
  considerations for healthcare settings.
\newblock In {\em Proc. of ML4H}, 2021.

\bibitem{torabi2018behavioral}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Behavioral cloning from observation.
\newblock In {\em Proc. of IJCAI}, pages 4950--4957, 2018.

\bibitem{van2018deep}
Hado Van~Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat,
  and Joseph Modayil.
\newblock Deep reinforcement learning and the deadly triad.
\newblock {\em ArXiv preprint}, 2018.

\bibitem{virmaux2018lipschitz}
Aladin Virmaux and Kevin Scaman.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock In {\em Proc. of NeurIPS}, pages 3839--3848, 2018.

\bibitem{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad Zolna, Josh Merel, Jost~Tobias
  Springenberg, Scott~E. Reed, Bobak Shahriari, Noah~Y. Siegel, {\c{C}}aglar
  G{\"{u}}l{\c{c}}ehre, Nicolas Heess, and Nando de~Freitas.
\newblock Critic regularized regression.
\newblock In {\em Proc. of NeurIPS}, 2020.

\bibitem{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 1992.

\bibitem{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em ArXiv preprint}, 2019.

\bibitem{wu2021uncertainty}
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua~M. Susskind, Jian Zhang,
  Ruslan Salakhutdinov, and Hanlin Goh.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock In {\em Proc. of ICML}, pages 11319--11328, 2021.

\bibitem{xu2023offline}
Haoran Xu, Li~Jiang, Jianxiong Li, Zhuoran Yang, Zhaoran Wang, Victor Wai~Kin
  Chan, and Xianyuan Zhan.
\newblock Offline rl with no ood actions: In-sample learning via implicit value
  regularization.
\newblock In {\em Proc. of ICLR}, 2023.

\bibitem{xu2021offline}
Haoran Xu, Xianyuan Zhan, Jianxiong Li, and Honglei Yin.
\newblock Offline reinforcement learning with soft behavior regularization.
\newblock {\em ArXiv preprint}, 2021.

\bibitem{xu2022discriminator}
Haoran Xu, Xianyuan Zhan, Honglei Yin, and Huiling Qin.
\newblock Discriminator-weighted offline imitation learning from suboptimal
  demonstrations.
\newblock In {\em Proc. of ICML}, pages 24725--24742, 2022.

\bibitem{xu2022constraints}
Haoran Xu, Xianyuan Zhan, and Xiangyu Zhu.
\newblock Constraints penalized q-learning for safe offline reinforcement
  learning.
\newblock In {\em Proc. of AAAI}, pages 8753--8760, 2022.

\bibitem{yang2022rethinking}
Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han,
  and Chongjie Zhang.
\newblock Rethinking goal-conditioned supervised learning and its connection to
  offline rl.
\newblock {\em ArXiv preprint}, 2022.

\bibitem{yu2022leverage}
Tianhe Yu, Aviral Kumar, Yevgen Chebotar, Karol Hausman, Chelsea Finn, and
  Sergey Levine.
\newblock How to leverage unlabeled data in offline reinforcement learning.
\newblock {\em ArXiv preprint}, 2022.

\bibitem{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y. Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock {MOPO:} model-based offline policy optimization.
\newblock In {\em Proc. of NeurIPS}, 2020.

\bibitem{zhan2022deepthermal}
Xianyuan Zhan, Haoran Xu, Yue Zhang, Xiangyu Zhu, Honglei Yin, and Yu~Zheng.
\newblock Deepthermal: Combustion optimization for thermal power generating
  units using offline reinforcement learning.
\newblock In {\em Proc. of AAAI}, pages 4680--4688, 2022.

\bibitem{zhan2022model}
Xianyuan Zhan, Xiangyu Zhu, and Haoran Xu.
\newblock Model-based offline planning with trajectory pruning.
\newblock In {\em Proc. of IJCAI}, 2022.

\bibitem{zhang2020learning}
Amy Zhang, Rowan~Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey
  Levine.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock In {\em Proc. of ICLR}, 2021.

\bibitem{zhou2020latent}
Wenxuan Zhou, Sujay Bajracharya, and David Held.
\newblock Latent action space for offline reinforcement learning.
\newblock In {\em Conference on Robot Learning}, 2020.

\end{thebibliography}
