\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alquier et~al.(2016)Alquier, Mai, and Pontil]{alquier2016regret}
Pierre Alquier, The~Tien Mai, and Massimiliano Pontil.
\newblock Regret bounds for lifelong learning.
\newblock \emph{arXiv preprint arXiv:1610.08628}, 2016.

\bibitem[Anderson et~al.(2010)Anderson, Guionnet, and
  Zeitouni]{anderson2010introduction}
Greg~W Anderson, Alice Guionnet, and Ofer Zeitouni.
\newblock \emph{An introduction to random matrices}, volume 118.
\newblock Cambridge university press, 2010.

\bibitem[Argyriou et~al.(2007)Argyriou, Evgeniou, and
  Pontil]{argyriou2007multi}
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil.
\newblock Multi-task feature learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  41--48, 2007.

\bibitem[Arnold et~al.(2019)Arnold, Iqbal, and Sha]{arnold2019maml}
S{\'e}bastien~MR Arnold, Shariq Iqbal, and Fei Sha.
\newblock When maml can adapt fast and how to assist when it cannot.
\newblock \emph{arXiv preprint arXiv:1910.13603}, 2019.

\bibitem[Bai and Lee(2019)]{bai2019beyond}
Yu~Bai and Jason~D Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock \emph{arXiv preprint arXiv:1910.01619}, 2019.

\bibitem[Bai and Silverstein(2010)]{bai2010spectral}
Zhidong Bai and Jack~W Silverstein.
\newblock \emph{Spectral analysis of large dimensional random matrices},
  volume~20.
\newblock Springer, 2010.

\bibitem[Baxter(2000)]{baxter2000model}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{J. Artif. Int. Res.}, 2000.

\bibitem[Caruana(1997)]{caruana1997}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine Learning}, 28\penalty0 (1):\penalty0 41--75, Jul 1997.
\newblock ISSN 1573-0565.
\newblock \doi{10.1023/A:1007379606734}.
\newblock URL \url{https://doi.org/10.1023/A:1007379606734}.

\bibitem[Collins et~al.(2020)Collins, Mokhtari, and
  Shakkottai]{collins2020does}
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock Why does maml outperform erm? an optimization perspective.
\newblock \emph{arXiv preprint arXiv:2010.14672}, 2020.

\bibitem[Denevi et~al.(2018{\natexlab{a}})Denevi, Ciliberto, Stamos, and
  Pontil]{denevi2018incremental}
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil.
\newblock Incremental learning-to-learn with statistical guarantees.
\newblock \emph{arXiv preprint arXiv:1803.08089}, 2018{\natexlab{a}}.

\bibitem[Denevi et~al.(2018{\natexlab{b}})Denevi, Ciliberto, Stamos, and
  Pontil]{denevi2018learning}
Giulia Denevi, Carlo Ciliberto, Dimitris Stamos, and Massimiliano Pontil.
\newblock Learning to learn around a common mean.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10169--10179, 2018{\natexlab{b}}.

\bibitem[Dobriban et~al.(2018)Dobriban, Wager, et~al.]{dobriban2018high}
Edgar Dobriban, Stefan Wager, et~al.
\newblock High-dimensional asymptotics of prediction: Ridge regression and
  classification.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (1):\penalty0 247--279,
  2018.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Evgeniou et~al.(2005)Evgeniou, Micchelli, and
  Pontil]{evgeniou2005learning}
Theodoros Evgeniou, Charles~A Micchelli, and Massimiliano Pontil.
\newblock Learning multiple tasks with kernel methods.
\newblock \emph{Journal of machine learning research}, 6\penalty0
  (Apr):\penalty0 615--637, 2005.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020convergence}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1082--1092, 2020.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{finn2019online}
Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine.
\newblock Online meta-learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, 2019.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimilano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.04910}, 2018.

\bibitem[Galanti et~al.(2016)Galanti, Wolf, and Hazan]{galanti2016theoretical}
Tomer Galanti, Lior Wolf, and Tamir Hazan.
\newblock A theoretical framework for deep transfer learning.
\newblock \emph{Information and Inference: A Journal of the IMA}, 5\penalty0
  (2):\penalty0 159--209, 2016.

\bibitem[Gao and Sener(2020)]{gao2020modeling}
Katelyn Gao and Ozan Sener.
\newblock Modeling and optimization trade-off in meta-learning.
\newblock \emph{arXiv preprint arXiv:2010.12916}, 2020.

\bibitem[Goldblum et~al.(2020)Goldblum, Reich, Fowl, Ni, Cherepanova, and
  Goldstein]{goldblum2020unraveling}
Micah Goldblum, Steven Reich, Liam Fowl, Renkun Ni, Valeriia Cherepanova, and
  Tom Goldstein.
\newblock Unraveling meta-learning: Understanding feature representations for
  few-shot tasks.
\newblock \emph{arXiv preprint arXiv:2002.06753}, 2020.

\bibitem[Gu et~al.(2018)Gu, Wang, Chen, Cho, and Li]{gu2018meta}
Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor~OK Li.
\newblock Meta-learning for low-resource neural machine translation.
\newblock \emph{arXiv preprint arXiv:1808.08437}, 2018.

\bibitem[Ji et~al.(2020)Ji, Lee, Liang, and Poor]{ji2020convergence}
Kaiyi Ji, Jason~D Lee, Yingbin Liang, and H~Vincent Poor.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock \emph{arXiv preprint arXiv:2006.09486}, 2020.

\bibitem[Khodak et~al.(2019)Khodak, Balcan, and Talwalkar]{khodak2019adaptive}
Mikhail Khodak, Maria-Florina Balcan, and Ameet Talwalkar.
\newblock Adaptive gradient-based meta-learning methods.
\newblock \emph{arXiv preprint arXiv:1906.02717}, 2019.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock pages 1097--1105, 2012.

\bibitem[Lee et~al.(2019)Lee, Maji, Ravichandran, and Soatto]{lee2019meta}
Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto.
\newblock Meta-learning with differentiable convex optimization.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 10657--10665, 2019.

\bibitem[Liu et~al.(2009)Liu, Palatucci, and Zhang]{liu2009blockwise}
Han Liu, Mark Palatucci, and Jian Zhang.
\newblock Blockwise coordinate descent procedures for the multi-task lasso,
  with applications to neural semantic basis discovery.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 649--656, 2009.

\bibitem[Liu et~al.(2015)Liu, Wang, and Zhao]{liu2015calibrated}
Han Liu, Lie Wang, and Tuo Zhao.
\newblock Calibrated multivariate regression with application to neural
  semantic basis discovery.
\newblock \emph{Journal of machine learning research: JMLR}, 16:\penalty0 1579,
  2015.

\bibitem[Maurer et~al.(2016)Maurer, Pontil, and
  Romera-Paredes]{maurer2016benefit}
Andreas Maurer, Massimiliano Pontil, and Bernardino Romera-Paredes.
\newblock The benefit of multitask representation learning.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2853--2884, 2016.

\bibitem[McNamara and Balcan(2017)]{mcnamara2017risk}
Daniel McNamara and Maria-Florina Balcan.
\newblock Risk bounds for transferring representations with and without
  fine-tuning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2373--2381. JMLR. org, 2017.

\bibitem[Nichol and Schulman(2018)]{nichol2018reptile}
A.~Nichol and J.~Schulman.
\newblock Reptile: a scalable metalearning algorithm.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2, 2018.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Raghu et~al.(2020)Raghu, Raghu, Bengio, and Vinyals]{raghu2020rapid}
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgMkCEtPB}.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  113--124, 2019.

\bibitem[Ravi and Larochelle(2017)]{ravi2016optimization}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock 2017.

\bibitem[Ren et~al.(2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{ren2018meta}
M.~Ren, E.~Triantafillou, S.~Ravi, J.~Snell, K.~Swersky, J.~Tenenbaum,
  H.~Larochelle, and R.~Zemel.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock \emph{arXiv preprint arXiv:1803.00676}, 2018.

\bibitem[Ruder(2017)]{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, and Bernstein]{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, and M.~Bernstein.
\newblock Imagenet large scale visual recognition challenge.
\newblock 115\penalty0 (3):\penalty0 211--252, 2015.

\bibitem[Saunshi et~al.(2020)Saunshi, Zhang, Khodak, and
  Arora]{saunshi2020sample}
Nikunj Saunshi, Yi~Zhang, Mikhail Khodak, and Sanjeev Arora.
\newblock A sample complexity separation between non-convex and convex
  meta-learning.
\newblock \emph{arXiv preprint arXiv:2002.11172}, 2020.

\bibitem[Schmidhuber(1987)]{schmidhuber1987evolutionary}
J{\"u}rgen Schmidhuber.
\newblock \emph{Evolutionary principles in self-referential learning, or on
  learning how to learn: the meta-meta-... hook}.
\newblock PhD thesis, Technische Universit{\"a}t M{\"u}nchen, 1987.

\bibitem[Setlur et~al.(2020)Setlur, Li, and Smith]{setlur2020support}
Amrith Setlur, Oscar Li, and Virginia Smith.
\newblock Is support set diversity necessary for meta-learning?, 2020.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
J.~Snell, K.~Swersky, and R.~Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock pages 4077--4087, 2017.

\bibitem[Thrun and Pratt(1998)]{thrun1998}
Sebastian Thrun and Lorien Pratt.
\newblock \emph{Learning to Learn: Introduction and Overview}, pages 3--17.
\newblock Springer US, Boston, MA, 1998.
\newblock ISBN 978-1-4615-5529-2.
\newblock \doi{10.1007/978-1-4615-5529-2_1}.
\newblock URL \url{https://doi.org/10.1007/978-1-4615-5529-2_1}.

\bibitem[Tripuraneni et~al.(2020{\natexlab{a}})Tripuraneni, Jin, and
  Jordan]{tripuraneni2020provable}
Nilesh Tripuraneni, Chi Jin, and Michael~I Jordan.
\newblock Provable meta-learning of linear representations.
\newblock \emph{arXiv preprint arXiv:2002.11684}, 2020{\natexlab{a}}.

\bibitem[Tripuraneni et~al.(2020{\natexlab{b}})Tripuraneni, Jordan, and
  Jin]{tripuraneni2020theory}
Nilesh Tripuraneni, Michael~I Jordan, and Chi Jin.
\newblock On the theory of transfer learning: The importance of task diversity.
\newblock \emph{arXiv preprint arXiv:2006.11650}, 2020{\natexlab{b}}.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Aad~W Van~der Vaart.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Sun, and Li]{wang2020global}
Haoxiang Wang, Ruoyu Sun, and Bo~Li.
\newblock Global convergence and induced kernels of gradient-based
  meta-learning with neural nets.
\newblock \emph{arXiv preprint arXiv:2006.14606}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Cai, Yang, and
  Wang]{wang2020globalmaml}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock On the global optimality of model-agnostic meta-learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  9837--9846. PMLR, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2020{\natexlab{c}})Wang, Yuan, Wu, and
  Ge]{wang2020guarantees}
Xiang Wang, Shuai Yuan, Chenwei Wu, and Rong Ge.
\newblock Guarantees for tuning the step size using a learning-to-learn
  approach.
\newblock \emph{arXiv preprint arXiv:2006.16495}, 2020{\natexlab{c}}.

\bibitem[Xie et~al.(2019)Xie, Jiang, Liu, Zhao, and Zha]{xie2019meta}
Yujia Xie, Haoming Jiang, Feng Liu, Tuo Zhao, and Hongyuan Zha.
\newblock Meta learning with relational information for short sequences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9904--9915, 2019.

\bibitem[Yao et~al.(2020)Yao, Huang, Wei, Tian, Huang, and Li]{yao2020don}
Huaxiu Yao, Longkai Huang, Ying Wei, Li~Tian, Junzhou Huang, and Zhenhui Li.
\newblock Don't overlook the support set: Towards improving generalization in
  meta-learning.
\newblock \emph{arXiv preprint arXiv:2007.13040}, 2020.

\bibitem[Zhou et~al.(2019)Zhou, Yuan, Xu, Yan, and Feng]{zhou2019efficient}
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng.
\newblock Efficient meta learning via minibatch proximal update.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1534--1544, 2019.

\end{thebibliography}
