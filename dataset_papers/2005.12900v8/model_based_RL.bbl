\begin{thebibliography}{}

\bibitem[Agarwal et~al., 2020]{agarwal2019optimality}
Agarwal, A., Kakade, S., and Yang, L.~F. (2020).
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock {\em Conference on Learning Theory (COLT)}.

\bibitem[Azar et~al., 2012]{azar2012sample}
Azar, M.~G., Munos, R., and Kappen, B. (2012).
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock {\em arXiv preprint arXiv:1206.6461}.

\bibitem[Azar et~al., 2013]{azar2013minimax}
Azar, M.~G., Munos, R., and Kappen, H.~J. (2013).
\newblock Minimax {PAC} bounds on the sample complexity of reinforcement
  learning with a generative model.
\newblock {\em Machine learning}, 91(3):325--349.

\bibitem[Azar et~al., 2017]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R. (2017).
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org.

\bibitem[Bai et~al., 2019]{bai2019provably}
Bai, Y., Xie, T., Jiang, N., and Wang, Y.-X. (2019).
\newblock Provably efficient {Q}-learning with low switching cost.
\newblock {\em Advances in Neural Information Processing Systems}, 32.

\bibitem[Beck and Srikant, 2012]{beck2012error}
Beck, C.~L. and Srikant, R. (2012).
\newblock Error bounds for constant step-size {Q}-learning.
\newblock {\em Systems \& control letters}, 61(12):1203--1208.

\bibitem[Bellman, 1952]{bellman1952theory}
Bellman, R. (1952).
\newblock On the theory of dynamic programming.
\newblock {\em Proceedings of the National Academy of Sciences of the United
  States of America}, 38(8):716.

\bibitem[Bertsekas, 2017]{bertsekas2017dynamic}
Bertsekas, D.~P. (2017).
\newblock {\em Dynamic programming and optimal control (4th edition)}.
\newblock Athena Scientific.

\bibitem[Bhandari et~al., 2018]{bhandari2018finite}
Bhandari, J., Russo, D., and Singal, R. (2018).
\newblock A finite time analysis of temporal difference learning with linear
  function approximation.
\newblock In {\em Conference On Learning Theory}, pages 1691--1692.

\bibitem[Bradtke and Barto, 1996]{bradtke1996linear}
Bradtke, S.~J. and Barto, A.~G. (1996).
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock {\em Machine learning}, 22(1-3):33--57.

\bibitem[Cai et~al., 2019]{cai2019neural}
Cai, Q., Yang, Z., Lee, J.~D., and Wang, Z. (2019).
\newblock Neural temporal-difference learning converges to global optima.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11312--11322.

\bibitem[Chen et~al., 2021]{chen2021spectral}
Chen, Y., Chi, Y., Fan, J., and Ma, C. (2021).
\newblock Spectral methods for data science: A statistical perspective.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  14(5):566--806.

\bibitem[Chen et~al., 2019a]{chen2019spectral}
Chen, Y., Fan, J., Ma, C., and Wang, K. (2019a).
\newblock Spectral method and regularized {MLE} are both optimal for top-{K}
  ranking.
\newblock {\em Annals of statistics}, 47(4):2204.

\bibitem[Chen et~al., 2019b]{Chen22931}
Chen, Y., Fan, J., Ma, C., and Yan, Y. (2019b).
\newblock Inference and uncertainty quantification for noisy matrix completion.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(46):22931--22937.

\bibitem[Chen et~al., 2020]{chen2020finite}
Chen, Z., Maguluri, S.~T., Shakkottai, S., and Shanmugam, K. (2020).
\newblock Finite-sample analysis of stochastic approximation using smooth
  convex envelopes.
\newblock {\em arXiv preprint arXiv:2002.00874}.

\bibitem[Cui and Yang, 2021]{cui2021minimax}
Cui, Q. and Yang, L.~F. (2021).
\newblock Minimax sample complexity for turn-based stochastic game.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 1496--1504.
  PMLR.

\bibitem[Dalal et~al., 2018]{dalal2018finite}
Dalal, G., Sz{\"o}r{\'e}nyi, B., Thoppe, G., and Mannor, S. (2018).
\newblock Finite sample analyses for {TD}(0) with function approximation.
\newblock In {\em Thirty-Second AAAI Conference on Artificial Intelligence}.

\bibitem[Domingues et~al., 2021]{domingues2021episodic}
Domingues, O.~D., M{\'e}nard, P., Kaufmann, E., and Valko, M. (2021).
\newblock Episodic reinforcement learning in finite {MDP}s: Minimax lower
  bounds revisited.
\newblock In {\em Algorithmic Learning Theory}, pages 578--598.

\bibitem[El~Karoui, 2015]{el2015impact}
El~Karoui, N. (2015).
\newblock On the impact of predictor geometry on the performance on
  high-dimensional ridge-regularized generalized robust regression estimators.
\newblock {\em Probability Theory and Related Fields}, pages 1--81.

\bibitem[Even-Dar and Mansour, 2003]{even2003learning}
Even-Dar, E. and Mansour, Y. (2003).
\newblock Learning rates for {Q}-learning.
\newblock {\em Journal of machine learning Research}, 5(Dec):1--25.

\bibitem[Fan et~al., 2019]{yang2019theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z. (2019).
\newblock A theoretical analysis of deep {Q}-learning.
\newblock {\em arXiv preprint arXiv:1901.00137}.

\bibitem[Gupta et~al., 2019]{gupta2019finite}
Gupta, H., Srikant, R., and Ying, L. (2019).
\newblock Finite-time performance bounds and adaptive learning rate selection
  for two time-scale reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4706--4715.

\bibitem[Jaakkola et~al., 1994]{jaakkola1994convergence}
Jaakkola, T., Jordan, M.~I., and Singh, S.~P. (1994).
\newblock Convergence of stochastic iterative dynamic programming algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  703--710.

\bibitem[Jin et~al., 2018]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I. (2018).
\newblock Is {Q}-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873.

\bibitem[Jin et~al., 2020]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I. (2020).
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In {\em Conference on Learning Theory}, pages 2137--2143. PMLR.

\bibitem[Kakade, 2003]{kakade2003sample}
Kakade, S. (2003).
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London.

\bibitem[Kaledin et~al., 2020]{kaledin2020finite}
Kaledin, M., Moulines, E., Naumov, A., Tadic, V., and Wai, H.-T. (2020).
\newblock Finite time analysis of linear two-timescale stochastic approximation
  with {M}arkovian noise.
\newblock {\em arXiv preprint arXiv:2002.01268}.

\bibitem[Kearns et~al., 2002]{kearns2002sparse}
Kearns, M., Mansour, Y., and Ng, A.~Y. (2002).
\newblock A sparse sampling algorithm for near-optimal planning in large
  {M}arkov decision processes.
\newblock {\em Machine learning}, 49(2-3):193--208.

\bibitem[Kearns and Singh, 1999]{kearns1999finite}
Kearns, M.~J. and Singh, S.~P. (1999).
\newblock Finite-sample convergence rates for {Q}-learning and indirect
  algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  996--1002.

\bibitem[Khamaru et~al., 2020]{khamaru2020temporal}
Khamaru, K., Pananjady, A., Ruan, F., Wainwright, M.~J., and Jordan, M.~I.
  (2020).
\newblock Is temporal difference learning optimal? an instance-dependent
  analysis.
\newblock {\em arXiv preprint arXiv:2003.07337}.

\bibitem[Lakshminarayanan and Szepesvari, 2018]{lakshminarayanan2018linear}
Lakshminarayanan, C. and Szepesvari, C. (2018).
\newblock Linear stochastic approximation: How far does constant step-size and
  iterate averaging go?
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1347--1355.

\bibitem[Lattimore and Hutter, 2012]{lattimore2012pac}
Lattimore, T. and Hutter, M. (2012).
\newblock {PAC bounds for discounted MDPs}.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 320--334. Springer.

\bibitem[Li et~al., 2021a]{li2021tightening}
Li, G., Cai, C., Chen, Y., Gu, Y., Wei, Y., and Chi, Y. (2021a).
\newblock Tightening the dependence on horizon in the sample complexity of
  {Q}-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  6296--6306.

\bibitem[Li et~al., 2023]{li2021q}
Li, G., Cai, C., Chen, Y., Wei, Y., and Chi, Y. (2023).
\newblock Is {Q}-learning minimax optimal? a tight sample complexity analysis.
\newblock {\em accepted to Operations Research}.

\bibitem[Li et~al., 2021b]{li2021sample}
Li, G., Chen, Y., Chi, Y., Gu, Y., and Wei, Y. (2021b).
\newblock Sample-efficient reinforcement learning is feasible for linearly
  realizable {MDP}s with limited revisiting.
\newblock {\em Advances in Neural Information Processing Systems},
  34:16671--16685.

\bibitem[Li et~al., 2022a]{li2022minimax}
Li, G., Chi, Y., Wei, Y., and Chen, Y. (2022a).
\newblock Minimax-optimal multi-agent {RL} in zero-sum {M}arkov games with a
  generative model.
\newblock {\em arXiv preprint arXiv:2208.10458}.

\bibitem[Li et~al., 2022b]{li2022settling}
Li, G., Shi, L., Chen, Y., Chi, Y., and Wei, Y. (2022b).
\newblock Settling the sample complexity of model-based offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2204.05275}.

\bibitem[Li et~al., 2021c]{li2021breaking}
Li, G., Shi, L., Chen, Y., Gu, Y., and Chi, Y. (2021c).
\newblock Breaking the sample complexity barrier to regret-optimal model-free
  reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Li et~al., 2022c]{li2020sample}
Li, G., Wei, Y., Chi, Y., Gu, Y., and Chen, Y. (2022c).
\newblock Sample complexity of asynchronous {Q}-learning: Sharper analysis and
  variance reduction.
\newblock {\em IEEE Transactions on Information Theory}, 68(1):448--473.

\bibitem[Ma et~al., 2020]{ma2017implicit}
Ma, C., Wang, K., Chi, Y., and Chen, Y. (2020).
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval, matrix completion and blind
  deconvolution.
\newblock {\em Foundations of Computational Mathematics}, 20(3):451--632.

\bibitem[Mou et~al., 2020]{mou2020linear}
Mou, W., Li, C.~J., Wainwright, M.~J., Bartlett, P.~L., and Jordan, M.~I.
  (2020).
\newblock On linear stochastic approximation: Fine-grained {P}olyak-{R}uppert
  and non-asymptotic concentration.
\newblock {\em arXiv preprint arXiv:2004.04719}.

\bibitem[Pananjady and Wainwright, 2019]{pananjady2019value}
Pananjady, A. and Wainwright, M.~J. (2019).
\newblock Value function estimation in {M}arkov reward processes:
  Instance-dependent $\ell_{\infty}$-bounds for policy evaluation.
\newblock {\em arXiv preprint arXiv:1909.08749}.

\bibitem[Qu and Wierman, 2020]{qu2020finite}
Qu, G. and Wierman, A. (2020).
\newblock Finite-time analysis of asynchronous stochastic approximation and
  {Q}-learning.
\newblock In {\em Conference on Learning Theory}, pages 3185--3205.

\bibitem[Shah and Xie, 2018]{shah2018q}
Shah, D. and Xie, Q. (2018).
\newblock Q-learning with nearest neighbors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3111--3121.

\bibitem[Shi et~al., 2022]{shi2022pessimistic}
Shi, L., Li, G., Wei, Y., Chen, Y., and Chi, Y. (2022).
\newblock Pessimistic {Q}-learning for offline reinforcement learning: Towards
  optimal sample complexity.
\newblock {\em International Conference on Machine Learning}.

\bibitem[Sidford et~al., 2018a]{sidford2018near}
Sidford, A., Wang, M., Wu, X., Yang, L., and Ye, Y. (2018a).
\newblock Near-optimal time and sample complexities for solving {M}arkov
  decision processes with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5186--5196.

\bibitem[Sidford et~al., 2018b]{sidford2018variance}
Sidford, A., Wang, M., Wu, X., and Ye, Y. (2018b).
\newblock Variance reduced value iteration and faster algorithms for solving
  {M}arkov decision processes.
\newblock In {\em Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 770--787.

\bibitem[Srikant and Ying, 2019]{srikant2019finite}
Srikant, R. and Ying, L. (2019).
\newblock Finite-time error bounds for linear stochastic approximation and {TD}
  learning.
\newblock In {\em Conference on Learning Theory}, pages 2803--2830.

\bibitem[Strehl et~al., 2006]{strehl2006pac}
Strehl, A.~L., Li, L., Wiewiora, E., Langford, J., and Littman, M.~L. (2006).
\newblock {PAC} model-free reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  881--888.

\bibitem[Sutton and Barto, 2018]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G. (2018).
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press.

\bibitem[Szepesv{\'a}ri, 1998]{szepesvari1998asymptotic}
Szepesv{\'a}ri, C. (1998).
\newblock The asymptotic convergence-rate of {Q}-learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1064--1070.

\bibitem[Szepesv{\'a}ri, 2010]{szepesvari2010algorithms}
Szepesv{\'a}ri, C. (2010).
\newblock Algorithms for reinforcement learning.
\newblock {\em Synthesis lectures on artificial intelligence and machine
  learning}, 4(1):1--103.

\bibitem[Tsitsiklis and Van~Roy, 1997]{tsitsiklis1997analysis}
Tsitsiklis, J. and Van~Roy, B. (1997).
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock {\em IEEE Transactions on Automatic Control}, 42(5):674--690.

\bibitem[Tsitsiklis, 1994]{tsitsiklis1994asynchronous}
Tsitsiklis, J.~N. (1994).
\newblock Asynchronous stochastic approximation and {Q}-learning.
\newblock {\em Machine learning}, 16(3):185--202.

\bibitem[Tu and Recht, 2019]{tu2018gap}
Tu, S. and Recht, B. (2019).
\newblock The gap between model-based and model-free methods on the linear
  quadratic regulator: An asymptotic viewpoint.
\newblock In {\em Conference on Learning Theory}, pages 3036--3083.

\bibitem[Vershynin, 2018]{vershynin2018high}
Vershynin, R. (2018).
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press.

\bibitem[Wainwright, 2019a]{wainwright2019stochastic}
Wainwright, M.~J. (2019a).
\newblock Stochastic approximation with cone-contractive operators: Sharp
  $\ell_{\infty}$-bounds for {Q}-learning.
\newblock {\em arXiv preprint arXiv:1905.06265}.

\bibitem[Wainwright, 2019b]{wainwright2019variance}
Wainwright, M.~J. (2019b).
\newblock Variance-reduced {Q}-learning is minimax optimal.
\newblock {\em arXiv preprint arXiv:1906.04697}.

\bibitem[Wang et~al., 2021]{wang2021sample}
Wang, B., Yan, Y., and Fan, J. (2021).
\newblock Sample-efficient reinforcement learning for linearly-parameterized
  {MDP}s with a generative model.
\newblock {\em Neural Information Processing Systems}.

\bibitem[Wang, 2019]{wang2017randomized}
Wang, M. (2019).
\newblock Randomized linear programming solves the {M}arkov decision problem in
  nearly linear (sometimes sublinear) time.
\newblock {\em Mathematics of Operations Research}.

\bibitem[Xu and Gu, 2020]{xu2019finite}
Xu, P. and Gu, Q. (2020).
\newblock A finite-time analysis of {Q}-learning with neural network function
  approximation.
\newblock In {\em International Conference on Machine Learning}, pages
  10555--10565.

\bibitem[Xu et~al., 2019]{xu2019two}
Xu, T., Zou, S., and Liang, Y. (2019).
\newblock Two time-scale off-policy {TD} learning: Non-asymptotic analysis over
  {M}arkovian samples.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10633--10643.

\bibitem[Yan et~al., 2021]{yan2021inference}
Yan, Y., Chen, Y., and Fan, J. (2021).
\newblock Inference for heteroskedastic {PCA} with missing data.
\newblock {\em arXiv preprint arXiv:2107.12365}.

\bibitem[Yan et~al., 2022a]{yan2022efficacy}
Yan, Y., Li, G., Chen, Y., and Fan, J. (2022a).
\newblock The efficacy of pessimism in asynchronous {Q}-learning.
\newblock {\em arXiv preprint arXiv:2203.07368}.

\bibitem[Yan et~al., 2022b]{yan2022model}
Yan, Y., Li, G., Chen, Y., and Fan, J. (2022b).
\newblock Model-based reinforcement learning is minimax-optimal for offline
  zero-sum {M}arkov games.
\newblock {\em arXiv preprint arXiv:2206.04044}.

\bibitem[Yang and Wang, 2019]{yang2019sample}
Yang, L. and Wang, M. (2019).
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In {\em International Conference on Machine Learning}, pages
  6995--7004.

\bibitem[Yin et~al., 2021]{yin2021near}
Yin, M., Bai, Y., and Wang, Y.-X. (2021).
\newblock Near-optimal provable uniform convergence in offline policy
  evaluation for reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1567--1575.

\end{thebibliography}
