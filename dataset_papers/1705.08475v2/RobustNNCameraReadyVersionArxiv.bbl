\begin{thebibliography}{10}

\bibitem{AbaAgaBar2016}
M.~Abadi, A.~Agarwal, P.~Barham, E.~Brevdo, Z.~Chen, C.~Citro, G.~S. Corrado,
  A.~Davis, J.~Dean, M.~Devin, S.~Ghemawat, I.~J. Goodfellow, A.~Harp,
  G.~Irving, M.~Isard, Y.~Jia, R.~J{\'{o}}zefowicz, L.~Kaiser, M.~Kudlur,
  J.~Levenberg, D.~Man{\'{e}}, R.~Monga, S.~Moore, D.~G. Murray, C.~Olah,
  M.~Schuster, J.~Shlens, B.~Steiner, I.~Sutskever, K.~Talwar, P.~A. Tucker,
  V.~Vanhoucke, V.~Vasudevan, F.~B. Vi{\'{e}}gas, O.~Vinyals, P.~Warden,
  M.~Wattenberg, M.~Wicke, Y.~Yu, and X.~Zheng.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems, 2016.

\bibitem{BasEtAl2016}
O.~Bastani, Y.~Ioannou, L.~Lampropoulos, D.~Vytiniotis, A.~Nori, and
  A.~Criminisi.
\newblock Measuring neural net robustness with constraints.
\newblock In {\em NIPS}, 2016.

\bibitem{CarWag2017}
N.~Carlini and D.~Wagner.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock In {\em ACM Workshop on Artificial Intelligence and Security}, 2017.

\bibitem{CisEtAl2017}
M.~Cisse, P.~Bojanowksi, E.~Grave, Y.~Dauphin, and N.~Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In {\em ICML}, 2017.

\bibitem{DalEtAl2004}
N.~Dalvi, P.~Domingos, Mausam, S.~Sanghai, and D.~Verma.
\newblock Adversarial classification.
\newblock In {\em KDD}, 2004.

\bibitem{DruCun1992}
H.~Drucker and Y.~Le Cun.
\newblock Double backpropagation increasing generalization performance.
\newblock In {\em IJCNN}, 1992.

\bibitem{GooShlSze2015}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In {\em ICLR}, 2015.

\bibitem{GuRig2015}
S.~Gu and L.~Rigazio.
\newblock Towards deep neural network architectures robust to adversarial
  examples.
\newblock In {\em ICLR Workshop}, 2015.

\bibitem{HeZhaRen2015}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{HelLon2015}
D.~P. Helmbold and P.~Long.
\newblock On the inductive bias of dropout.
\newblock {\em Journal of Machine Learning Research}, 16:3403--3454, 2015.

\bibitem{SchHoc1995}
S.~Hochreiter and J.~Schmidhuber.
\newblock Simplifying neural nets by discovering flat minima.
\newblock In {\em NIPS}, 1995.

\bibitem{HuaEtAl2016}
R.~Huang, B.~Xu, D.~Schuurmans, and C.~Szepesvari.
\newblock Learning with a strong adversary.
\newblock In {\em ICLR}, 2016.

\bibitem{KosFisSon2017}
J.~Kos, I.~Fischer, and D.~Song.
\newblock Adversarial examples for generative models.
\newblock In {\em ICLR Workshop}, 2017.

\bibitem{KurGooBen2016a}
A.~Kurakin, I.~J. Goodfellow, and S.~Bengio.
\newblock Adversarial examples in the physical world.
\newblock In {\em ICLR Workshop}, 2017.

\bibitem{LiuEtAl2016}
Y.~Liu, X.~Chen, C.~Liu, and D.~Song.
\newblock Delving into transferable adversarial examples and black-box attacks.
\newblock In {\em ICLR}, 2017.

\bibitem{LowMee2005}
D.~Lowd and C.~Meek.
\newblock Adversarial learning.
\newblock In {\em KDD}, 2005.

\bibitem{MooEtAl2016}
S.M. Moosavi-Dezfooli, A.~Fawzi, O.~Fawzi, and P.~Frossard.
\newblock Universal adversarial perturbations.
\newblock In {\em CVPR}, 2017.

\bibitem{PapEtAl2016a}
N.~Papernot, P.~McDonald, X.~Wu, S.~Jha, and A.~Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  networks.
\newblock In {\em IEEE Symposium on Security \& Privacy}, 2016.

\bibitem{MooFawFro2016}
P.~Frossard S.-M. Moosavi-Dezfooli, A.~Fawzi.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In {\em CVPR}, pages 2574--2582, 2016.

\bibitem{SchSmo2002}
B.~Sch{\"o}lkopf and A.~J. Smola.
\newblock {\em Learning with Kernels}.
\newblock MIT Press, Cambridge, MA, 2002.

\bibitem{ShaYamNeg2016}
U.~Shaham, Y.~Yamada, and S.~Negahban.
\newblock Understanding adversarial training: Increasing local stability of
  neural nets through robust optimization.
\newblock In {\em NIPS}, 2016.

\bibitem{SriEtAl2014}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15:1929--1958, 2014.

\bibitem{GTSB2012}
J.~Stallkamp, M.~Schlipsing, J.~Salmen, and C.~Igel.
\newblock Man vs. computer: Benchmarking machine learning algorithms for
  traffic sign recognition.
\newblock {\em Neural Networks}, 32:323--332, 2012.

\bibitem{SzeEtal2014}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock In {\em ICLR}, pages 2503--2511, 2014.

\bibitem{ZagKom2016}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock In {\em BMVC}, pages 87.1--87.12.

\bibitem{ZheEtAl2016}
S.~Zheng, Y.~Song, T.~Leung, and I.~J. Goodfellow.
\newblock Improving the robustness of deep neural networks via stability
  training.
\newblock In {\em CVPR}, 2016.

\end{thebibliography}
