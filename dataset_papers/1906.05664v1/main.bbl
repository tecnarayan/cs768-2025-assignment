\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Antoniol et~al.(1995)Antoniol, Brugnara, Cettolo, and
  Federico]{antoniol1995language}
Giuliano Antoniol, Fabio Brugnara, Mauro Cettolo, and Marcello Federico.
\newblock Language model representations for beam-search decoding.
\newblock In \emph{1995 International Conference on Acoustics, Speech, and
  Signal Processing}, volume~1, pages 588--591. IEEE, 1995.

\bibitem[Brown(1986)]{Brown:1986:FSE:41464}
L.~D. Brown.
\newblock \emph{Fundamentals of Statistical Exponential Families: With
  Applications in Statistical Decision Theory}.
\newblock Institute of Mathematical Statistics, Hayworth, CA, USA, 1986.
\newblock ISBN 0-940-60010-2.

\bibitem[Clarkson and Robinson(1999)]{clarkson1999towards}
Philip Clarkson and Tony Robinson.
\newblock Towards improved language model evaluation measures.
\newblock In \emph{Sixth European Conference on Speech Communication and
  Technology}, 1999.

\bibitem[Cover and Thomas(2006)]{Cover:2006:EIT:1146355}
Thomas~M. Cover and Joy~A. Thomas.
\newblock \emph{Elements of Information Theory (Wiley Series in
  Telecommunications and Signal Processing)}.
\newblock Wiley-Interscience, New York, NY, USA, 2006.
\newblock ISBN 0471241954.

\bibitem[Csiszar and K{\"o}rner(2011)]{pinsker}
Imre Csiszar and J{\'a}nos K{\"o}rner.
\newblock \emph{Information theory: coding theorems for discrete memoryless
  systems}.
\newblock Cambridge University Press, 2011.

\bibitem[Dai et~al.(2018)Dai, Yang, Yang, Cohen, Carbonell, Le, and
  Salakhutdinov]{dai2018transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, William~W Cohen, Jaime Carbonell, Quoc~V
  Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Language modeling with longer-term dependency.
\newblock 2018.

\bibitem[Dawid(1982)]{dawid82}
A.~P. Dawid.
\newblock The well-calibrated bayesian.
\newblock \emph{Journal of the Am. Stat. Assoc}, 77, 1982.

\bibitem[Dawid(1985)]{dawid85}
A.~P. Dawid.
\newblock The impossibility of inductive inference.
\newblock \emph{Journal of the Am. Stat. Assoc}, 80, 1985.

\bibitem[Foster(1991)]{F91}
D.~P. Foster.
\newblock Prediction in the worst case.
\newblock \emph{Annals of Statistics}, 19, 1991.

\bibitem[Gong et~al.(2018)Gong, He, Tan, Qin, Wang, and Liu]{gong2018frage}
Chengyue Gong, Di~He, Xu~Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu.
\newblock Frage: frequency-agnostic word representation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1341--1352, 2018.

\bibitem[Grave et~al.(2016)Grave, Joulin, and Usunier]{grave2016improving}
Edouard Grave, Armand Joulin, and Nicolas Usunier.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{arXiv preprint arXiv:1612.04426}, 2016.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1321--1330. JMLR. org, 2017.

\bibitem[Ji et~al.(2015)Ji, Cohn, Kong, Dyer, and Eisenstein]{ji2015document}
Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein.
\newblock Document context language models.
\newblock \emph{arXiv preprint arXiv:1511.03962}, 2015.

\bibitem[Jost and Atwell(1994)]{jost1994proposal}
Uwe Jost and ES~Atwell.
\newblock Proposal for a mutual-information based language model.
\newblock In \emph{Proceedings of the 1994 AISB Workshop on Computational
  Linguistics for Speech and Handwriting Recognition}. AISB, 1994.

\bibitem[Jozefowicz et~al.(2016)Jozefowicz, Vinyals, Schuster, Shazeer, and
  Wu]{jozefowicz2016exploring}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
\newblock Exploring the limits of language modeling.
\newblock \emph{arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[Kalai et~al.(1999)Kalai, Lehrer, and Smorodinsky]{kls:merging}
E.~Kalai, E.~Lehrer, and R.~Smorodinsky.
\newblock Calibrated forecasting and merging.
\newblock \emph{Games and Economic Behavior}, 29, 1999.

\bibitem[Ke et~al.(2018)Ke, Goyal, Bilaniuk, Binas, Mozer, Pal, and
  Bengio]{ke2018sparse}
Nan~Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael~C
  Mozer, Chris Pal, and Yoshua Bengio.
\newblock Sparse attentive backtracking: Temporal credit assignment through
  reminding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7651--7662, 2018.

\bibitem[Khandelwal et~al.(2018)Khandelwal, He, Qi, and
  Jurafsky]{khandelwal2018sharp}
Urvashi Khandelwal, He~He, Peng Qi, and Dan Jurafsky.
\newblock Sharp nearby, fuzzy far away: How neural language models use context.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages
  284--294, 2018.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{le2015simple}
Quoc~V Le, Navdeep Jaitly, and Geoffrey~E Hinton.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[Lin and Tegmark(2017)]{lin2017critical}
Henry Lin and Max Tegmark.
\newblock Critical behavior in physics and probabilistic formal languages.
\newblock \emph{Entropy}, 19\penalty0 (7):\penalty0 299, 2017.

\bibitem[Marcus et~al.(1993)Marcus, Santorini, and
  Marcinkiewicz]{marcus1993building}
Mitchell Marcus, Beatrice Santorini, and Mary~Ann Marcinkiewicz.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock 1993.

\bibitem[McAllester(2018)]{mcallester2018information}
David McAllester.
\newblock Information theoretic co-training.
\newblock \emph{arXiv preprint arXiv:1802.07572}, 2018.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017state}
G{\'a}bor Melis, Chris Dyer, and Phil Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2017)Merity, Keskar, and Socher]{merityRegOpt}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock {Regularizing and Optimizing LSTM Language Models}.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017.

\bibitem[Merity et~al.(2018)Merity, Keskar, and Socher]{merityAnalysis}
Stephen Merity, Nitish~Shirish Keskar, and Richard Socher.
\newblock {An Analysis of Neural Language Modeling at Multiple Scales}.
\newblock \emph{arXiv preprint arXiv:1803.08240}, 2018.

\bibitem[Mikolov and Zweig(2012)]{mikolov2012context}
Tomas Mikolov and Geoffrey Zweig.
\newblock Context dependent recurrent neural network language model.
\newblock In \emph{2012 IEEE Spoken Language Technology Workshop (SLT)}, pages
  234--239. IEEE, 2012.

\bibitem[Mroueh and Sercu(2017)]{NIPS2017_6845}
Youssef Mroueh and Tom Sercu.
\newblock Fisher gan.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 2513--2523. Curran Associates,
  Inc., 2017.
\newblock URL \url{http://papers.nips.cc/paper/6845-fisher-gan.pdf}.

\bibitem[Müller(1997)]{muller}
Alfred Müller.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock \emph{Advances in Applied Probability}, 29:\penalty0 429--443, 06
  1997.
\newblock \doi{10.2307/1428011}.

\bibitem[Niculescu-Mizil and Caruana(2005)]{niculescu2005predicting}
Alexandru Niculescu-Mizil and Rich Caruana.
\newblock Predicting good probabilities with supervised learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 625--632. ACM, 2005.

\bibitem[Ortmanns and Ney(2000)]{ortmanns2000look}
Stefan Ortmanns and Hermann Ney.
\newblock Look-ahead techniques for fast beam search.
\newblock \emph{Computer Speech \& Language}, 14\penalty0 (1):\penalty0 15--32,
  2000.

\bibitem[Platt(1999)]{Platt99probabilisticoutputs}
John~C. Platt.
\newblock Probabilistic outputs for support vector machines and comparisons to
  regularized likelihood methods.
\newblock In \emph{Advances in Large Margin Classifiers}, pages 61--74. MIT
  Press, 1999.

\bibitem[Radford et~al.(2018)Radford, Narasimhan, Salimans, and
  Sutskever]{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock \emph{URL https://s3-us-west-2. amazonaws.
  com/openai-assets/research-covers/languageunsupervised/language understanding
  paper. pdf}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Shannon(1951)]{shannon1951prediction}
Claude~E Shannon.
\newblock Prediction and entropy of printed english.
\newblock \emph{Bell system technical journal}, 30\penalty0 (1):\penalty0
  50--64, 1951.

\bibitem[Sriperumbudur et~al.(2009)Sriperumbudur, Fukumizu, Gretton,
  Schölkopf, and Lanckriet]{sriperumbudur}
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Bernhard Schölkopf, and
  Gert Lanckriet.
\newblock On integral probability metrics, phi-divergences and binary
  classification.
\newblock 01 2009.

\bibitem[Steinbiss et~al.(1994)Steinbiss, Tran, and
  Ney]{steinbiss1994improvements}
Volker Steinbiss, Bach-Hiep Tran, and Hermann Ney.
\newblock Improvements in beam search.
\newblock In \emph{Third International Conference on Spoken Language
  Processing}, 1994.

\bibitem[Takahashi and Tanaka-Ishii(2018)]{takahashi2018cross}
Shuntaro Takahashi and Kumiko Tanaka-Ishii.
\newblock Cross entropy of neural language models at infinity—a new bound of
  the entropy rate.
\newblock \emph{Entropy}, 20\penalty0 (11):\penalty0 839, 2018.

\bibitem[Takase et~al.(2018)Takase, Suzuki, and Nagata]{takase2018direct}
Sho Takase, Jun Suzuki, and Masaaki Nagata.
\newblock Direct output connection for a high-rank language model.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pages 4599--4609, 2018.

\bibitem[Trinh et~al.(2018)Trinh, Dai, Luong, and Le]{trinh2018learning}
Trieu~H Trinh, Andrew~M Dai, Thang Luong, and Quoc~V Le.
\newblock Learning longer-term dependencies in rnns with auxiliary losses.
\newblock \emph{arXiv preprint arXiv:1803.00144}, 2018.

\bibitem[Vaswani et~al.(2017{\natexlab{a}})Vaswani, Shazeer, Parmar, Uszkoreit,
  Jones, Gomez, Kaiser, and Polosukhin]{vaswani}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017{\natexlab{a}}.

\bibitem[Vaswani et~al.(2017{\natexlab{b}})Vaswani, Shazeer, Parmar, Uszkoreit,
  Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017{\natexlab{b}}.

\bibitem[Vovk(2001)]{Vovk01}
V.~Vovk.
\newblock Competitive on-line statistics.
\newblock \emph{International Statistical Review}, 69, 2001.

\bibitem[Wang and Cho(2016)]{wang2016larger}
Tian Wang and Kyunghyun Cho.
\newblock Larger-context language modelling with recurrent neural network.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, volume~1, pages
  1319--1329, 2016.

\bibitem[Wang et~al.(2017)Wang, Gan, Wang, Shen, Huang, Ping, Satheesh, and
  Carin]{wang2017topic}
Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev
  Satheesh, and Lawrence Carin.
\newblock Topic compositional neural language model.
\newblock \emph{arXiv preprint arXiv:1712.09783}, 2017.

\bibitem[Zadrozny and Elkan(2002)]{Zadrozny02transformingclassifier}
Bianca Zadrozny and Charles Elkan.
\newblock Transforming classifier scores into accurate multiclass probability
  estimates, 2002.

\end{thebibliography}
