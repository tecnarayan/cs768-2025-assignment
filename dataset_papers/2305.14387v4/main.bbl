 \newcommand*{\annalstat}{Annals of Statistics} \newcommand*{\jasa}{Journal of
  the American Statistical Association (JASA)} \newcommand*{\tacl}{Transactions
  of the Association for Computational Linguistics (TACL)}
  \newcommand*{\coling}{International Conference on Computational Linguistics
  (COLING)} \newcommand*{\acl}{Association for Computational Linguistics (ACL)}
  \newcommand*{\naacl}{North American Association for Computational Linguistics
  (NAACL)} \newcommand*{\aclijcnlp}{Association for Computational Linguistics
  and International Joint Conference on Natural Language Processing
  (ACL-IJCNLP)} \newcommand*{\emnlpconll}{Empirical Methods in Natural Language
  Processing and Computational Natural Language Learning (EMNLP/CoNLL)}
  \newcommand*{\emnlpijnlp}{Empirical Methods in Natural Language Processing
  and International Joint Conference on Natural Language Processing
  (EMNLP-IJCNLP)} \newcommand*{\emnlp}{Empirical Methods in Natural Language
  Processing} \newcommand*{\hltnaacl}{Human Language Technology and North
  American Association for Computational Linguistics (HLT/NAACL)}
  \newcommand*{\eacl}{European Association for Computational Linguistics
  (EACL)}  \newcommand*{\icml}{International Conference on Machine Learning
  (ICML)} \newcommand*{\neurips}{Advances in Neural Information Processing
  Systems (NeurIPS)} \newcommand*{\nips}{Advances in Neural Information
  Processing Systems} \newcommand*{\iclr}{International Conference on Learning
  Representations (ICLR)} \newcommand*{\iclrworkshop}{International Conference
  on Learning Representations Workshop (ICLR)} \newcommand*{\jmlr}{Journal of
  Machine Learning Research (JMLR)} \newcommand*{\fatml}{Conference on
  Fairness, Accountability, and Transparency} \newcommand*{\aistats}{Artificial
  Intelligence and Statistics (AISTATS)} \newcommand*{\cvpr}{Conference on
  Computer Vision and Pattern Recognition (CVPR)}
  \newcommand*{\iccv}{International Conference on Computer Vision (ICCV)}
  \newcommand*{\icpr}{International Conference on Pattern Recognition (ICPR)}
  \newcommand*{\eccv}{European Conference on Computer Vision (ECCV)}
  \newcommand*{\uai}{Conference on Uncertainty in Artificial Intelligence
  (UAI)}  \newcommand*{\ecai}{European Conference on Artificial Intelligence}
  \newcommand*{\aaai}{AAAI Conference on Artificial Intelligence}
  \newcommand*{\packdd}{Pacific-Asia Conference on Knowledge Discovery and Data
  Mining} \newcommand*{\kdd}{International Conference on Knowledge Discovery
  and Data Mining (KDD)} \newcommand*{\neurcom}{Neural Computation}
  \newcommand*{\msml}{Mathematical and Scientific Machine Learning Conference
  (MSML)} \newcommand*{\ijcnn}{International Joint Conference on Neural
  Networks (IJCNN)} \newcommand*{\ieeesigproc}{IEEE Transactions on Signal
  Processing} \newcommand*{\ieeeec}{IEEE Transactions on Electronic Computers}
  \newcommand*{\procieee}{Proceedings of the IEEE}
  \newcommand*{\pnas}{Proceedings of the National Academy of Sciences}
  \newcommand*{\chiconf}{Conference on Human Factors in Computing Systems
  (CHI)} \newcommand*{\ieeecp}{IEEE Symposium on Security and Privacy (SP)}
  \newcommand*{\stoc}{Symposium on Theory of Computing (STOC)}
  \newcommand*{\pods}{Symposium on Principles of Database Systems (PODS)}
  \newcommand*{\colt}{Conference on Learning Theory (COLT)}
  \newcommand*{\www}{The World Wide Web Conference (WWW)}
  \newcommand*{\soda}{Symposium on Discrete Algorithms (SODA)}
  \newcommand*{\focs}{Symposium on Foundations of Computer Science (FOCS)}
  \newcommand*{\acm}{Communications of the Association for Computing Machinery
  (ACM)} \newcommand*{\ieeeaccess}{IEEE Access}
  \newcommand*{\ijcv}{International Journal of Computer Vision (IJCV)}
  \newcommand*{\ieeetpami}{IEEE Transactions on Pattern Analysis and Machine
  Intelligence} \newcommand*{\ieeetit}{IEEE Transactions on Information Theory}
  \newcommand*{\alt}{Conference on Algorithmic Learning Theory (ALT)}
  \newcommand*{\cocoon}{International Computing and Combinatorics Conference
  (COCOON)} \newcommand*{\arxiv}[1]{arXiv preprint arXiv:#1}
\begin{thebibliography}{10}

\bibitem{Aher22}
Gati Aher, Rosa~I Arriaga, and Adam~Tauman Kalai.
\newblock Using large language models to simulate multiple humans.
\newblock {\em arXiv preprint arXiv:2208.10264}, 2022.

\bibitem{Anthony17}
Thomas Anthony, Zheng Tian, and David Barber.
\newblock Thinking fast and slow with deep learning and tree search.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{Argyle22}
Lisa~P Argyle, Ethan~C Busby, Nancy Fulda, Joshua Gubler, Christopher Rytting,
  and David Wingate.
\newblock Out of one, many: Using language models to simulate human samples.
\newblock {\em arXiv preprint arXiv:2209.06899}, 2022.

\bibitem{Aribandi21}
Vamsi Aribandi, Yi~Tay, Tal Schuster, Jinfeng Rao, Huaixiu~Steven Zheng,
  Sanket~Vaibhav Mehta, Honglei Zhuang, Vinh~Q Tran, Dara Bahri, Jianmo Ni,
  et~al.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning.
\newblock {\em arXiv preprint arXiv:2111.10952}, 2021.

\bibitem{Askell21}
Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan,
  Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac
  Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine
  Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and
  Jared Kaplan.
\newblock A general language assistant as a laboratory for alignment, 2021.

\bibitem{Bai22*a}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,
  Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph,
  Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage,
  Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna
  Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown,
  Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning
  from human feedback, 2022.

\bibitem{Bai22}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
  Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,
  et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock {\em arXiv preprint arXiv:2212.08073}, 2022.

\bibitem{Bakker22}
Michiel Bakker, Martin Chadwick, Hannah Sheahan, Michael Tessler, Lucy
  Campbell-Gillingham, Jan Balaguer, Nat McAleese, Amelia Glaese, John
  Aslanides, Matt Botvinick, et~al.
\newblock Fine-tuning language models to find agreement among humans with
  diverse preferences.
\newblock {\em Advances in Neural Information Processing Systems},
  35:38176--38189, 2022.

\bibitem{Bommasani21}
R.~Bommasani et~al.
\newblock On the opportunities and risks of foundation models.
\newblock {\em \arxiv{2108.07258}}, 2021.

\bibitem{Bradley52}
Ralph~Allan Bradley and Milton~E Terry.
\newblock Rank analysis of incomplete block designs: I. the method of paired
  comparisons.
\newblock {\em Biometrika}, 39(3/4):324--345, 1952.

\bibitem{Brockman16}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{Brown20}
T.~B. Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, S.~Agarwal, A.~Herbert-Voss,
  G.~Krueger, T.~Henighan, R.~Child, A.~Ramesh, D.~M. Ziegler, J.~Wu,
  C.~Winter, C.~Hesse, M.~Chen, E.~Sigler, M.~Litwin, S.~Gray, B.~Chess,
  J.~Clark, C.~Berner, S.~McCandlish, A.~Radford, I.~Sutskever, and D.~Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em \neurips}, 2020.

\bibitem{Chen23}
Angelica Chen, J{\'e}r{\'e}my Scheurer, Tomasz Korbak, Jon~Ander Campos,
  Jun~Shern Chan, Samuel~R Bowman, Kyunghyun Cho, and Ethan Perez.
\newblock Improving code generation by training with natural language feedback.
\newblock {\em arXiv preprint arXiv:2303.16749}, 2023.

\bibitem{Chiang23}
Cheng-Han Chiang and Hung-yi Lee.
\newblock Can large language models be an alternative to human evaluations?
\newblock {\em arXiv preprint arXiv:2305.01937}, 2023.

\bibitem{Chiang23*a}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
  Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and
  Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90{\%} chatgpt
  quality, March 2023.

\bibitem{Christiano17*b}
Paul~F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
\newblock Deep reinforcement learning from human preferences.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{Fan18}
Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta, Joan
  Creus-Costa, Silvio Savarese, and Li~Fei-Fei.
\newblock Surreal: Open-source reinforcement learning framework and robot
  manipulation benchmark.
\newblock In {\em Conference on Robot Learning}, pages 767--782. PMLR, 2018.

\bibitem{Freeman21}
C~Daniel Freeman, Erik Frey, Anton Raichuk, Sertan Girgin, Igor Mordatch, and
  Olivier Bachem.
\newblock Brax--a differentiable physics engine for large scale rigid body
  simulation.
\newblock {\em arXiv preprint arXiv:2106.13281}, 2021.

\bibitem{Gao22}
Leo Gao, John Schulman, and Jacob Hilton.
\newblock Scaling laws for reward model overoptimization.
\newblock {\em arXiv preprint arXiv:2210.10760}, 2022.

\bibitem{Geng23}
Xinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey
  Levine, and Dawn Song.
\newblock Koala: A dialogue model for academic research, March 2023.

\bibitem{Glaese22}
Amelia Glaese, Nat McAleese, Maja Tr{\k{e}}bacz, John Aslanides, Vlad Firoiu,
  Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, Phoebe Thacker,
  et~al.
\newblock Improving alignment of dialogue agents via targeted human judgements.
\newblock {\em arXiv preprint arXiv:2209.14375}, 2022.

\bibitem{Hancock19}
Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston.
\newblock Learning from dialogue after deployment: Feed yourself, chatbot!
\newblock {\em arXiv preprint arXiv:1901.05415}, 2019.

\bibitem{Juliani18}
Arthur Juliani, Vincent-Pierre Berges, Ervin Teng, Andrew Cohen, Jonathan
  Harper, Chris Elion, Chris Goy, Yuan Gao, Hunter Henry, Marwan Mattar, et~al.
\newblock Unity: A general platform for intelligent agents.
\newblock {\em arXiv preprint arXiv:1809.02627}, 2018.

\bibitem{Kakade02}
S.~Kakade, Y.~W. Teh, and S.~Roweis.
\newblock An alternate objective function for {M}arkovian fields.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2002.

\bibitem{Karra22}
Saketh~Reddy Karra, Son Nguyen, and Theja Tulabandhula.
\newblock Ai personification: Estimating the personality of language models.
\newblock {\em arXiv preprint arXiv:2204.12000}, 2022.

\bibitem{Keskar19}
N.~S. Keskar, B.~McCann, L.~R. Varshney, C.~Xiong, and R.~Socher.
\newblock {CTRL}: {A} {Conditional} {Transformer} {Language} {Model} for
  {Controllable} {Generation}.
\newblock {\em arXiv preprint arXiv:1909.05858}, 2019.

\bibitem{Kiegeland21}
Samuel Kiegeland and Julia Kreutzer.
\newblock Revisiting the weaknesses of reinforcement learning for neural
  machine translation.
\newblock {\em arXiv preprint arXiv:2106.08942}, 2021.

\bibitem{Korbak23}
Tomasz Korbak, Kejian Shi, Angelica Chen, Rasika Bhalerao, Christopher~L
  Buckley, Jason Phang, Samuel~R Bowman, and Ethan Perez.
\newblock Pretraining language models with human preferences.
\newblock {\em arXiv preprint arXiv:2302.08582}, 2023.

\bibitem{Kreutzer18}
Julia Kreutzer, Shahram Khadivi, Evgeny Matusov, and Stefan Riezler.
\newblock Can neural machine translation be improved with user feedback?
\newblock {\em arXiv preprint arXiv:1804.05958}, 2018.

\bibitem{Lam18}
Tsz~Kin Lam, Julia Kreutzer, and Stefan Riezler.
\newblock A reinforcement learning approach to interactive-predictive neural
  machine translation.
\newblock {\em arXiv preprint arXiv:1805.01553}, 2018.

\bibitem{Lee23}
Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier,
  Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang~Shane Gu.
\newblock Aligning text-to-image models using human feedback.
\newblock {\em arXiv preprint arXiv:2302.12192}, 2023.

\bibitem{Li16*f}
Jiwei Li, Alexander~H Miller, Sumit Chopra, Marc'Aurelio Ranzato, and Jason
  Weston.
\newblock Dialogue learning with human-in-the-loop.
\newblock {\em arXiv preprint arXiv:1611.09823}, 2016.

\bibitem{alpaca_eval}
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023.

\bibitem{Liu23}
H~Liu, C~Sferrazza, and P~Abbeel.
\newblock Chain of hindsight aligns language models with feedback.
\newblock {\em arXiv preprint arXiv:2302.02676}, 2023.

\bibitem{Liu23*b}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em arXiv preprint arXiv:2304.08485}, 2023.

\bibitem{Liu23*a}
Yang Liu, Dan Iter, Xu~Yichong, Wang Shuohang, Xu~Ruochen, and Chenguang Zhu.
\newblock G-eval: Nlg evaluation using gpt-4 with better human alignmentg.
\newblock {\em arXiv preprint arXiv:2303.16634}, 2023.

\bibitem{Longpre23}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny
  Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective
  instruction tuning.
\newblock {\em arXiv preprint arXiv:2301.13688}, 2023.

\bibitem{Lu22}
X.~Lu, S.~Welleck, J.~Hessel, L.~Jiang, L.~Qin, P.~West, P.~Ammanabrolu, and
  Y.~Choi.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{Lu22*b}
Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West,
  Prithviraj Ammanabrolu, and Yejin Choi.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock {\em Advances in neural information processing systems},
  35:27591--27609, 2022.

\bibitem{Madaan23}
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah
  Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et~al.
\newblock Self-refine: Iterative refinement with self-feedback.
\newblock {\em arXiv preprint arXiv:2303.17651}, 2023.

\bibitem{Mishra21}
Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.
\newblock Cross-task generalization via natural language crowdsourcing
  instructions.
\newblock {\em arXiv preprint arXiv:2104.08773}, 2021.

\bibitem{Nguyen17}
Khanh Nguyen, Hal Daum{\'e}~III, and Jordan Boyd-Graber.
\newblock Reinforcement learning for bandit neural machine translation with
  simulated human feedback.
\newblock {\em arXiv preprint arXiv:1707.07402}, 2017.

\bibitem{OpenAI}
OpenAI.
\newblock Introducing chatgpt.

\bibitem{OpenAI*a}
OpenAI.
\newblock Model index for researchers.

\bibitem{OpenAI23}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem{Ouyang22}
Long Ouyang, Jeff Wu, Xu~Jiang, Diogo Almeida, Carroll~L. Wainwright, Pamela
  Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John
  Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda
  Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.
\newblock Training language models to follow instructions with human feedback,
  2022.

\bibitem{Park23}
Joon~Sung Park, Joseph~C O'Brien, Carrie~J Cai, Meredith~Ringel Morris, Percy
  Liang, and Michael~S Bernstein.
\newblock Generative agents: Interactive simulacra of human behavior.
\newblock {\em arXiv preprint arXiv:2304.03442}, 2023.

\bibitem{Park22}
Joon~Sung Park, Lindsay Popowski, Carrie Cai, Meredith~Ringel Morris, Percy
  Liang, and Michael~S Bernstein.
\newblock Social simulacra: Creating populated prototypes for social computing
  systems.
\newblock In {\em Proceedings of the 35th Annual ACM Symposium on User
  Interface Software and Technology}, pages 1--18, 2022.

\bibitem{Paulus17}
Romain Paulus, Caiming Xiong, and Richard Socher.
\newblock A deep reinforced model for abstractive summarization.
\newblock {\em arXiv preprint arXiv:1705.04304}, 2017.

\bibitem{Peng23}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock {\em arXiv preprint arXiv:2304.03277}, 2023.

\bibitem{Perez22}
Ethan Perez, Sam Ringer, Kamil{\.e} Luko{\v{s}}i{\=u}t{\.e}, Karina Nguyen,
  Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu,
  Saurav Kadavath, et~al.
\newblock Discovering language model behaviors with model-written evaluations.
\newblock {\em arXiv preprint arXiv:2212.09251}, 2022.

\bibitem{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D
  Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model.
\newblock {\em arXiv preprint arXiv:2305.18290}, 2023.

\bibitem{Ramamurthy22}
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, Kiant{\'e} Brantley, Jack Hessel,
  Rafet Sifa, Christian Bauckhage, Hannaneh Hajishirzi, and Yejin Choi.
\newblock Is reinforcement learning (not) for natural language processing?:
  Benchmarks, baselines, and building blocks for natural language policy
  optimization.
\newblock {\em arXiv preprint arXiv:2210.01241}, 2022.

\bibitem{Sanh21}
Victor Sanh, Albert Webson, Colin Raffel, Stephen~H Bach, Lintang Sutawika,
  Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven~Le Scao, Arun Raja,
  et~al.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock {\em arXiv preprint arXiv:2110.08207}, 2021.

\bibitem{Saunders22}
William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan
  Ward, and Jan Leike.
\newblock Self-critiquing models for assisting human evaluators.
\newblock {\em arXiv preprint arXiv:2206.05802}, 2022.

\bibitem{Scheurer23}
J{\'e}r{\'e}my Scheurer, Jon~Ander Campos, Tomasz Korbak, Jun~Shern Chan,
  Angelica Chen, Kyunghyun Cho, and Ethan Perez.
\newblock Training language models with language feedback at scale.
\newblock {\em arXiv preprint arXiv:2303.16755}, 2023.

\bibitem{Schulman15}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em arXiv preprint arXiv:1506.02438}, 2015.

\bibitem{Schulman17}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms, 2017.

\bibitem{Shi22}
Weiyan Shi, Emily Dinan, Kurt Shuster, Jason Weston, and Jing Xu.
\newblock When life gives you lemons, make cherryade: Converting feedback from
  bad responses into good labels.
\newblock {\em arXiv preprint arXiv:2210.15893}, 2022.

\bibitem{Silver17}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L., M.~Lai, A.~Bolton, et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354--359, 2017.

\bibitem{Snell22}
Charlie Snell, Ilya Kostrikov, Yi~Su, Mengjiao Yang, and Sergey Levine.
\newblock Offline rl for natural language generation with implicit language q
  learning.
\newblock {\em arXiv preprint arXiv:2206.11871}, 2022.

\bibitem{Sokolov16}
Artem Sokolov, Stefan Riezler, and Tanguy Urvoy.
\newblock Bandit structured prediction for learning from partial feedback in
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1601.04468}, 2016.

\bibitem{Stiennon20}
Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel~M. Ziegler, Ryan Lowe, Chelsea
  Voss, Alec Radford, Dario Amodei, and Paul Christiano.
\newblock Learning to summarize from human feedback, 2020.

\bibitem{Summers20}
Colin Summers, Kendall Lowrey, Aravind Rajeswaran, Siddhartha Srinivasa, and
  Emanuel Todorov.
\newblock Lyceum: An efficient and scalable ecosystem for robot learning.
\newblock In {\em Learning for Dynamics and Control}, pages 793--803. PMLR,
  2020.

\bibitem{Taori23}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
  Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Alpaca: A strong, replicable instruction-following modely, March
  2023.

\bibitem{Tassa18}
Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego de~Las
  Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq, et~al.
\newblock Deepmind control suite.
\newblock {\em arXiv preprint arXiv:1801.00690}, 2018.

\bibitem{Todorov12}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ international conference on intelligent robots
  and systems}, pages 5026--5033. IEEE, 2012.

\bibitem{Touvron23}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
  Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric
  Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{Uchendu21}
Adaku Uchendu, Zeyu Ma, Thai Le, Rui Zhang, and Dongwon Lee.
\newblock Turingbench: A benchmark environment for turing test in the age of
  neural text generation.
\newblock {\em arXiv preprint arXiv:2109.13296}, 2021.

\bibitem{Uesato22}
Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa
  Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins.
\newblock Solving math word problems with process- and outcome-based feedback,
  2022.

\bibitem{Wang22*c}
Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah~A. Smith, Daniel
  Khashabi, and Hannaneh Hajishirzi.
\newblock Self-instruct: Aligning language model with self generated
  instructions, 2022.

\bibitem{Wang22*b}
Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza
  Mirzaei, Atharva Naik, Arjun Ashok, Arut~Selvan Dhanasekaran, Anjana
  Arunkumar, David Stap, et~al.
\newblock Super-naturalinstructions: Generalization via declarative
  instructions on 1600+ nlp tasks.
\newblock In {\em Proceedings of the 2022 Conference on Empirical Methods in
  Natural Language Processing}, pages 5085--5109, 2022.

\bibitem{Wei21}
Jason Wei, Maarten Bosma, Vincent~Y Zhao, Kelvin Guu, Adams~Wei Yu, Brian
  Lester, Nan Du, Andrew~M Dai, and Quoc~V Le.
\newblock Finetuned language models are zero-shot learners.
\newblock {\em arXiv preprint arXiv:2109.01652}, 2021.

\bibitem{Weston16}
J.~E. Weston.
\newblock Dialog-based language learning.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  pages 829--837, 2016.

\bibitem{Wu16}
Y.~Wu, M.~Schuster, Z.~Chen, Q.~V. Le, M.~Norouzi, W.~Macherey, M.~Krikun,
  Y.~Cao, Q.~Gao, K.~Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock {\em arXiv preprint arXiv:1609.08144}, 2016.

\bibitem{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em arXiv preprint arXiv:2306.05685}, 2023.

\end{thebibliography}
