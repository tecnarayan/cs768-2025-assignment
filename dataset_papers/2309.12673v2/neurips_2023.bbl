\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anonymous(2023)]{anonymous2023stanhop}
Anonymous.
\newblock {ST}anhop: Sparse tandem hopfield model for memory-enhanced time
  series prediction.
\newblock In \emph{Submitted to The Twelfth International Conference on
  Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=6iwg437CZs}.
\newblock under review.

\bibitem[Bai et~al.(2019)Bai, Kolter, and Koltun]{bai2019deep}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock Deep equilibrium models.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.01377}.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.
\newblock URL \url{https://arxiv.org/abs/2004.05150}.

\bibitem[Bojar et~al.(2017)Bojar, Chatterjee, Federmann, Graham, Haddow, Huang,
  Huck, Koehn, Liu, Logacheva, et~al.]{wmt17}
Ond{\v{r}}ej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry
  Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara
  Logacheva, et~al.
\newblock Findings of the 2017 conference on machine translation (wmt17).
\newblock Association for Computational Linguistics, 2017.

\bibitem[Brandstetter(2021)]{hopfeildblog2021}
Johannes Brandstetter.
\newblock Blog post: Hopfield networks is all you need, 2021.
\newblock URL \url{https://ml-jku.github.io/hopfield-layers/}.
\newblock Accessed: April 4, 2023.

\bibitem[Brauchart et~al.(2018)Brauchart, Reznikov, Saff, Sloan, Wang, and
  Womersley]{brauchart2018random}
Johann~S Brauchart, Alexander~B Reznikov, Edward~B Saff, Ian~H Sloan, Yu~Guang
  Wang, and Robert~S Womersley.
\newblock Random point sets on the sphereâ€”hole radii, covering, and
  separation.
\newblock \emph{Experimental Mathematics}, 27\penalty0 (1):\penalty0 62--81,
  2018.
\newblock URL \url{https://arxiv.org/abs/1512.07470}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.
\newblock URL \url{https://arxiv.org/abs/1512.07470}.

\bibitem[Cai and Jiang(2012)]{cai2012phase}
T~Tony Cai and Tiefeng Jiang.
\newblock Phase transition in limiting distributions of coherence of
  high-dimensional random matrices.
\newblock \emph{Journal of Multivariate Analysis}, 107:\penalty0 24--39, 2012.
\newblock URL \url{https://arxiv.org/abs/1102.2926}.

\bibitem[Carbonneau et~al.(2018)Carbonneau, Cheplygina, Granger, and
  Gagnon]{carbonneau2018multiple}
Marc-Andr{\'e} Carbonneau, Veronika Cheplygina, Eric Granger, and Ghyslain
  Gagnon.
\newblock Multiple instance learning: A survey of problem characteristics and
  applications.
\newblock \emph{Pattern Recognition}, 77:\penalty0 329--353, 2018.
\newblock URL \url{https://arxiv.org/abs/1612.03365}.

\bibitem[Chen et~al.(2021)Chen, Peng, Fu, and Ling]{chen2021autoformer}
Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.
\newblock Autoformer: Searching transformers for visual recognition.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 12270--12280, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.00651}.

\bibitem[Chen et~al.(2006)Chen, Bi, and Wang]{chen2006miles}
Yixin Chen, Jinbo Bi, and James~Ze Wang.
\newblock Miles: Multiple-instance learning via embedded instance selection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 28\penalty0 (12):\penalty0 1931--1947, 2006.

\bibitem[Cheplygina et~al.(2015)Cheplygina, Tax, and
  Loog]{cheplygina2015dissimilarity}
Veronika Cheplygina, David~MJ Tax, and Marco Loog.
\newblock Dissimilarity-based ensembles for multiple instance learning.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  27\penalty0 (6):\penalty0 1379--1391, 2015.
\newblock URL \url{https://arxiv.org/abs/1402.1349}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.10509}.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{arXiv preprint arXiv:2204.02311}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.02311}.

\bibitem[Corless et~al.(1996)Corless, Gonnet, Hare, Jeffrey, and
  Knuth]{corless1996lambert}
Robert~M Corless, Gaston~H Gonnet, David~EG Hare, David~J Jeffrey, and Donald~E
  Knuth.
\newblock On the lambert w function.
\newblock \emph{Advances in Computational mathematics}, 5:\penalty0 329--359,
  1996.

\bibitem[Correia et~al.(2019)Correia, Niculae, and
  Martins]{correia2019adaptively}
Gon{\c{c}}alo~M Correia, Vlad Niculae, and Andr{\'e}~FT Martins.
\newblock Adaptively sparse transformers.
\newblock \emph{arXiv preprint arXiv:1909.00015}, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.00015}.

\bibitem[David and Nagaraja(2004)]{david2004order}
Herbert~A David and Haikady~N Nagaraja.
\newblock \emph{Order statistics}.
\newblock John Wiley \& Sons, 2004.

\bibitem[Demircigil et~al.(2017)Demircigil, Heusel, L{\"o}we, Upgang, and
  Vermet]{demircigil2017model}
Mete Demircigil, Judith Heusel, Matthias L{\"o}we, Sven Upgang, and Franck
  Vermet.
\newblock On a model of associative memory with huge storage capacity.
\newblock \emph{Journal of Statistical Physics}, 168:\penalty0 288--299, 2017.
\newblock URL \url{https://arxiv.org/abs/1702.01929}.

\bibitem[Dietterich et~al.(1997)Dietterich, Lathrop, and
  Lozano-P{\'e}rez]{dietterich1997solving}
Thomas~G Dietterich, Richard~H Lathrop, and Tom{\'a}s Lozano-P{\'e}rez.
\newblock Solving the multiple instance problem with axis-parallel rectangles.
\newblock \emph{Artificial intelligence}, 89\penalty0 (1-2):\penalty0 31--71,
  1997.

\bibitem[Elad(2010)]{elad2010sparse}
Michael Elad.
\newblock \emph{Sparse and redundant representations: from theory to
  applications in signal and image processing}, volume~2.
\newblock Springer, 2010.

\bibitem[F{\"o}ldiak(1990)]{foldiak1990forming}
Peter F{\"o}ldiak.
\newblock Forming sparse representations by local anti-hebbian learning.
\newblock \emph{Biological cybernetics}, 64\penalty0 (2):\penalty0 165--170,
  1990.

\bibitem[F{\"u}rst et~al.(2022)F{\"u}rst, Rumetshofer, Lehner, Tran, Tang,
  Ramsauer, Kreil, Kopp, Klambauer, Bitto, et~al.]{furst2022cloob}
Andreas F{\"u}rst, Elisabeth Rumetshofer, Johannes Lehner, Viet~T Tran, Fei
  Tang, Hubert Ramsauer, David Kreil, Michael Kopp, G{\"u}nter Klambauer,
  Angela Bitto, et~al.
\newblock Cloob: Modern hopfield networks with infoloob outperform clip.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 20450--20468, 2022.
\newblock URL \url{https://arxiv.org/abs/2110.11316}.

\bibitem[Gao and Pavel(2017)]{gao2017properties}
Bolin Gao and Lacra Pavel.
\newblock On the properties of the softmax function with application in game
  theory and reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1704.00805}, 2017.
\newblock URL \url{https://arxiv.org/abs/1704.00805}.

\bibitem[Gunawardana et~al.(2005)Gunawardana, Byrne, and
  Jordan]{gunawardana2005convergence}
Asela Gunawardana, William Byrne, and Michael~I Jordan.
\newblock Convergence theorems for generalized alternating minimization
  procedures.
\newblock \emph{Journal of machine learning research}, 6\penalty0 (12), 2005.
\newblock URL
  \url{https://www.jmlr.org/papers/volume6/gunawardana05a/gunawardana05a.pdf}.

\bibitem[Hoover et~al.(2023)Hoover, Liang, Pham, Panda, Strobelt, Chau, Zaki,
  and Krotov]{hoover2023energy}
Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt,
  Duen~Horng Chau, Mohammed~J Zaki, and Dmitry Krotov.
\newblock Energy transformer.
\newblock \emph{arXiv preprint arXiv:2302.07253}, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.07253}.

\bibitem[Hopfield(1982)]{hopfield1982neural}
John~J Hopfield.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock \emph{Proceedings of the national academy of sciences}, 79\penalty0
  (8):\penalty0 2554--2558, 1982.

\bibitem[Hopfield(1984)]{hopfield1984neurons}
John~J Hopfield.
\newblock Neurons with graded response have collective computational properties
  like those of two-state neurons.
\newblock \emph{Proceedings of the national academy of sciences}, 81\penalty0
  (10):\penalty0 3088--3092, 1984.

\bibitem[Ilse et~al.(2018)Ilse, Tomczak, and Welling]{ilse2018attention}
Maximilian Ilse, Jakub Tomczak, and Max Welling.
\newblock Attention-based deep multiple instance learning.
\newblock In \emph{International conference on machine learning}, pages
  2127--2136. PMLR, 2018.
\newblock URL \url{https://arxiv.org/abs/1802.04712}.

\bibitem[Ji et~al.(2021)Ji, Zhou, Liu, and Davuluri]{ji2021dnabert}
Yanrong Ji, Zhihan Zhou, Han Liu, and Ramana~V Davuluri.
\newblock Dnabert: pre-trained bidirectional encoder representations from
  transformers model for dna-language in genome.
\newblock \emph{Bioinformatics}, 37\penalty0 (15):\penalty0 2112--2120, 2021.
\newblock URL
  \url{https://www.biorxiv.org/content/10.1101/2020.09.17.301879v1}.

\bibitem[Kandemir et~al.(2014)Kandemir, Zhang, and
  Hamprecht]{kandemir2014empowering}
Melih Kandemir, Chong Zhang, and Fred~A Hamprecht.
\newblock Empowering multiple instance histopathology cancer diagnosis by cell
  graphs.
\newblock In \emph{Medical Image Computing and Computer-Assisted
  Intervention--MICCAI 2014: 17th International Conference, Boston, MA, USA,
  September 14-18, 2014, Proceedings, Part II 17}, pages 228--235. Springer,
  2014.

\bibitem[Kozachkov et~al.(2023)Kozachkov, Kastanenka, and
  Krotov]{kozachkov2022building}
Leo Kozachkov, Ksenia~V Kastanenka, and Dmitry Krotov.
\newblock Building transformers from neurons and astrocytes.
\newblock \emph{Proceedings of the National Academy of Sciences}, 120\penalty0
  (34):\penalty0 e2219150120, 2023.
\newblock URL
  \url{https://www.biorxiv.org/content/10.1101/2022.10.12.511910v1}.

\bibitem[Krotov and Hopfield(2016)]{krotov2016dense}
Dmitry Krotov and John~J Hopfield.
\newblock Dense associative memory for pattern recognition.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.01164}.

\bibitem[Krotov and Hopfield(2021)]{krotov2020large}
Dmitry Krotov and John~J. Hopfield.
\newblock Large associative memory problem in neurobiology and machine
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://arxiv.org/abs/2008.06996}.

\bibitem[K{\"u}{\c{c}}{\"u}ka{\c{s}}c{\i} and
  Baydo{\u{g}}an(2018)]{kuccukacsci2018bag}
Emel~{\c{S}}eyma K{\"u}{\c{c}}{\"u}ka{\c{s}}c{\i} and Mustafa~G{\"o}k{\c{c}}e
  Baydo{\u{g}}an.
\newblock Bag encoding strategies in multiple instance learning problems.
\newblock \emph{Information Sciences}, 467:\penalty0 559--578, 2018.

\bibitem[Lee et~al.(1986)Lee, Doolen, Chen, Sun, Maxwell, and
  Lee]{lee1986machine}
YC~Lee, Gary Doolen, HH~Chen, GZ~Sun, Tom Maxwell, and HY~Lee.
\newblock Machine learning using a higher order correlation network.
\newblock Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
  (United States); Univ. of Maryland, College Park, MD (United States), 1986.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 10012--10022, 2021.
\newblock URL \url{https://arxiv.org/abs/2103.14030}.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.
\newblock URL \url{https://arxiv.org/abs/1711.05101}.

\bibitem[Mairal et~al.(2010)Mairal, Bach, Ponce, and Sapiro]{mairal2010online}
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (1), 2010.
\newblock URL \url{https://arxiv.org/abs/0908.0050}.

\bibitem[Makhzani and Frey(2015)]{makhzani2015winner}
Alireza Makhzani and Brendan~J Frey.
\newblock Winner-take-all autoencoders.
\newblock \emph{Advances in neural information processing systems}, 28, 2015.
\newblock URL \url{https://arxiv.org/abs/1409.2752}.

\bibitem[Maron and Lozano-P{\'e}rez(1997)]{maron1997framework}
Oded Maron and Tom{\'a}s Lozano-P{\'e}rez.
\newblock A framework for multiple-instance learning.
\newblock \emph{Advances in neural information processing systems}, 10, 1997.

\bibitem[Martins and Astudillo(2016)]{martins2016softmax}
Andre Martins and Ramon Astudillo.
\newblock From softmax to sparsemax: A sparse model of attention and
  multi-label classification.
\newblock In \emph{International conference on machine learning}, pages
  1614--1623. PMLR, 2016.
\newblock URL \url{https://arxiv.org/abs/1602.02068}.

\bibitem[Martins et~al.(2023)Martins, Niculae, and McNamee]{martins2023sparse}
Andre F.~T. Martins, Vlad Niculae, and Daniel McNamee.
\newblock Sparse modern hopfield networks.
\newblock \emph{Associative Memory \& Hopfield Networks in 2023. NeurIPS 2023
  workshop.}, 2023.
\newblock URL \url{https://openreview.net/pdf?id=zwqlV7HoaT}.

\bibitem[Millidge et~al.(2022)Millidge, Salvatori, Song, Lukasiewicz, and
  Bogacz]{millidge2022universal}
Beren Millidge, Tommaso Salvatori, Yuhang Song, Thomas Lukasiewicz, and Rafal
  Bogacz.
\newblock Universal hopfield networks: A general framework for single-shot
  associative memory models.
\newblock In \emph{International Conference on Machine Learning}, pages
  15561--15583. PMLR, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.04557}.

\bibitem[Newman(1988)]{newman1988memory}
Charles~M Newman.
\newblock Memory capacity in neural network models: Rigorous lower bounds.
\newblock \emph{Neural Networks}, 1\penalty0 (3):\penalty0 223--238, 1988.

\bibitem[Olshausen and Field(1997)]{olshausen1997sparse}
Bruno~A Olshausen and David~J Field.
\newblock Sparse coding with an overcomplete basis set: A strategy employed by
  v1?
\newblock \emph{Vision research}, 37\penalty0 (23):\penalty0 3311--3325, 1997.

\bibitem[Olver et~al.(2010)Olver, Lozier, Boisvert, and Clark]{olver2010nist}
Frank~WJ Olver, Daniel~W Lozier, Ronald~F Boisvert, and Charles~W Clark.
\newblock \emph{NIST handbook of mathematical functions hardback and CD-ROM}.
\newblock Cambridge university press, 2010.

\bibitem[Paischer et~al.(2022)Paischer, Adler, Patil, Bitto-Nemling,
  Holzleitner, Lehner, Eghbal-Zadeh, and Hochreiter]{paischer2022history}
Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus
  Holzleitner, Sebastian Lehner, Hamid Eghbal-Zadeh, and Sepp Hochreiter.
\newblock History compression via language models in reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  17156--17185. PMLR, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.12258}.

\bibitem[Palm(2013)]{palm2013neural}
G{\"u}nther Palm.
\newblock Neural associative memories and sparse coding.
\newblock \emph{Neural Networks}, 37:\penalty0 165--171, 2013.

\bibitem[Peretto and Niez(1986)]{peretto1986long}
Pierre Peretto and Jean-Jacques Niez.
\newblock Long term memory storage capacity of multiconnected neural networks.
\newblock \emph{Biological Cybernetics}, 54\penalty0 (1):\penalty0 53--63,
  1986.

\bibitem[Peters et~al.(2019)Peters, Niculae, and Martins]{peters2019sparse}
Ben Peters, Vlad Niculae, and Andr{\'e}~FT Martins.
\newblock Sparse sequence-to-sequence models.
\newblock \emph{arXiv preprint arXiv:1905.05702}, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.05702}.

\bibitem[Qiu et~al.(2019)Qiu, Ma, Levy, Yih, Wang, and Tang]{qiu2019blockwise}
Jiezhong Qiu, Hao Ma, Omer Levy, Scott Wen-tau Yih, Sinong Wang, and Jie Tang.
\newblock Blockwise self-attention for long document understanding.
\newblock \emph{arXiv preprint arXiv:1911.02972}, 2019.
\newblock URL \url{https://arxiv.org/abs/1911.02972}.

\bibitem[Ramsauer et~al.(2021)Ramsauer, Sch{\"a}fl, Lehner, Seidl, Widrich,
  Gruber, Holzleitner, Adler, Kreil, Kopp, Klambauer, Brandstetter, and
  Hochreiter]{ramsauer2020hopfield}
Hubert Ramsauer, Bernhard Sch{\"a}fl, Johannes Lehner, Philipp Seidl, Michael
  Widrich, Lukas Gruber, Markus Holzleitner, Thomas Adler, David Kreil,
  Michael~K Kopp, G{\"u}nter Klambauer, Johannes Brandstetter, and Sepp
  Hochreiter.
\newblock Hopfield networks is all you need.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://arxiv.org/abs/2008.02217}.

\bibitem[Rubinstein et~al.(2010)Rubinstein, Bruckstein, and
  Elad]{rubinstein2010dictionaries}
Ron Rubinstein, Alfred~M Bruckstein, and Michael Elad.
\newblock Dictionaries for sparse representation modeling.
\newblock \emph{Proceedings of the IEEE}, 98\penalty0 (6):\penalty0 1045--1057,
  2010.

\bibitem[Seidl et~al.(2022)Seidl, Renz, Dyubankova, Neves, Verhoeven, Wegner,
  Segler, Hochreiter, and Klambauer]{seidl2022improving}
Philipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas Verhoeven,
  Jorg~K Wegner, Marwin Segler, Sepp Hochreiter, and Gunter Klambauer.
\newblock Improving few-and zero-shot reaction template prediction using modern
  hopfield networks.
\newblock \emph{Journal of chemical information and modeling}, 62\penalty0
  (9):\penalty0 2111--2120, 2022.

\bibitem[Sriperumbudur and Lanckriet(2009)]{sriperumbudur2009convergence}
Bharath~K Sriperumbudur and Gert~RG Lanckriet.
\newblock On the convergence of the concave-convex procedure.
\newblock In \emph{Advances in neural information processing systems},
  volume~9, pages 1759--1767, 2009.
\newblock URL
  \url{https://papers.nips.cc/paper_files/paper/2009/file/8b5040a8a5baf3e0e67386c2e3a9b903-Paper.pdf}.

\bibitem[Tay et~al.(2020)Tay, Bahri, Yang, Metzler, and Juan]{tay2020sparse}
Yi~Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan.
\newblock Sparse sinkhorn attention.
\newblock In \emph{International Conference on Machine Learning}, pages
  9438--9447. PMLR, 2020.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{tay2022efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (6):\penalty0 1--28, 2022.
\newblock URL \url{https://arxiv.org/abs/2009.06732}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.
\newblock URL \url{https://arxiv.org/abs/1706.03762}.

\bibitem[Wainwright et~al.(2008)Wainwright, Jordan,
  et~al.]{wainwright2008graphical}
Martin~J Wainwright, Michael~I Jordan, et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  1\penalty0 (1--2):\penalty0 1--305, 2008.

\bibitem[Wang and Zucker(2000)]{wang2000solving}
Jun Wang and Jean-Daniel Zucker.
\newblock Solving multiple-instance problem: A lazy learning approach.
\newblock 2000.

\bibitem[Widrich et~al.(2020)Widrich, Sch{\"a}fl, Pavlovi{\'c}, Ramsauer,
  Gruber, Holzleitner, Brandstetter, Sandve, Greiff, Hochreiter,
  et~al.]{widrich2020modern}
Michael Widrich, Bernhard Sch{\"a}fl, Milena Pavlovi{\'c}, Hubert Ramsauer,
  Lukas Gruber, Markus Holzleitner, Johannes Brandstetter, Geir~Kjetil Sandve,
  Victor Greiff, Sepp Hochreiter, et~al.
\newblock Modern hopfield networks and attention for immune repertoire
  classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18832--18845, 2020.
\newblock URL \url{https://arxiv.org/abs/2007.13505}.

\bibitem[Yang et~al.(2022)Yang, Huang, and Wipf]{yang2022transformers}
Yongyi Yang, Zengfeng Huang, and David Wipf.
\newblock Transformers from an optimization perspective.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=VT0Y4PlV2m0}.

\bibitem[Yuille and Rangarajan(2001)]{yuille2001concave}
Alan~L Yuille and Anand Rangarajan.
\newblock The concave-convex procedure (cccp).
\newblock \emph{Advances in neural information processing systems}, 14, 2001.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/2001/file/a012869311d64a44b5a0d567cd20de04-Paper.pdf}.

\bibitem[Yuille and Rangarajan(2003)]{yuille2003concave}
Alan~L Yuille and Anand Rangarajan.
\newblock The concave-convex procedure.
\newblock \emph{Neural computation}, 15\penalty0 (4):\penalty0 915--936, 2003.

\bibitem[Zangwill(1969)]{zangwill1969nonlinear}
Willard~I Zangwill.
\newblock \emph{Nonlinear programming: a unified approach}, volume~52.
\newblock Prentice-Hall Englewood Cliffs, NJ, 1969.

\bibitem[Zhang and Yan(2022)]{zhang2022crossformer}
Yunhao Zhang and Junchi Yan.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for
  multivariate time series forecasting.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=vSVLM2j9eie}.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and
  Zhang]{zhou2021informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,
  and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~35, pages 11106--11115, 2021.
\newblock URL \url{https://arxiv.org/abs/2012.07436}.

\bibitem[Zhou et~al.(2022)Zhou, Ma, Wen, Sun, Yao, Yin, Jin,
  et~al.]{zhou2022film}
Tian Zhou, Ziqing Ma, Qingsong Wen, Liang Sun, Tao Yao, Wotao Yin, Rong Jin,
  et~al.
\newblock Film: Frequency improved legendre memory model for long-term time
  series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 12677--12690, 2022.
\newblock URL \url{https://arxiv.org/abs/2205.08897}.

\end{thebibliography}
