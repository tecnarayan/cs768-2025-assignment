\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2010)Agarwal, Dekel, and Xiao]{agarwal2010optimal}
Agarwal, A., Dekel, O., and Xiao, L.
\newblock Optimal algorithms for online convex optimization with multi-point
  bandit feedback.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  28--40, 2010.

\bibitem[Allen-Zhu \& Hazan(2016)Allen-Zhu and Hazan]{Allen_Zhu2016}
Allen-Zhu, Z. and Hazan, E.
\newblock Variance reduction for faster non-convex optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  699--707, 2016.

\bibitem[Balasubramanian \& Ghadimi(2018)Balasubramanian and
  Ghadimi]{balasubramanian2018zeroth}
Balasubramanian, K. and Ghadimi, S.
\newblock Zeroth-order (non)-convex stochastic optimization via conditional
  gradient and gradient updates.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3459--3468, 2018.

\bibitem[Chang \& Lin(2011)Chang and Lin]{chang2011libsvm}
Chang, C.-C. and Lin, C.-J.
\newblock Libsvm: a library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2\penalty0 (3):\penalty0 27, 2011.

\bibitem[Chen et~al.(2017)Chen, Zhang, Sharma, Yi, and Hsieh]{chen2017zoo}
Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J.
\newblock Zoo: Zeroth order optimization based black-box attacks to deep neural
  networks without training substitute models.
\newblock In \emph{Proceedings of the 10th ACM Workshop on Artificial
  Intelligence and Security}, pp.\  15--26, 2017.

\bibitem[Choromanski et~al.(2018)Choromanski, Rowland, Sindhwani, Turner, and
  Weller]{choromanski2018structured}
Choromanski, K., Rowland, M., Sindhwani, V., Turner, R.~E., and Weller, A.
\newblock Structured evolution with compact architectures for scalable policy
  optimization.
\newblock \emph{arXiv preprint arXiv:1804.02395}, 2018.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{Defazio2014}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock {SAGA: A fast incremental gradient method with support for
  non-strongly convex composite objectives}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1646--1654. 2014.

\bibitem[Duchi et~al.(2015)Duchi, Jordan, Wainwright, and
  Wibisono]{duchi2015optimal}
Duchi, J.~C., Jordan, M.~I., Wainwright, M.~J., and Wibisono, A.
\newblock Optimal rates for zero-order convex optimization: The power of two
  function evaluations.
\newblock \emph{IEEE Transactions on Information Theory}, 61\penalty0
  (5):\penalty0 2788--2806, 2015.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator.
\newblock \emph{arXiv preprint arXiv:1807.01695}, 2018.

\bibitem[Flaxman et~al.(2005)Flaxman, Kalai, and McMahan]{flaxman2005online}
Flaxman, A.~D., Kalai, A.~T., and McMahan, H.~B.
\newblock Online convex optimization in the bandit setting: gradient descent
  without a gradient.
\newblock In \emph{Proceedings of the Sixteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pp.\  385--394, 2005.

\bibitem[Gao et~al.(2014)Gao, Jiang, and Zhang]{gao2014information}
Gao, X., Jiang, B., and Zhang, S.
\newblock On the information-adaptive variants of the admm: an iteration
  complexity perspective.
\newblock \emph{Journal of Scientific Computing}, pp.\  1--37, 2014.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghadimi et~al.(2016)Ghadimi, Lan, and Zhang]{ghadimi2016mini}
Ghadimi, S., Lan, G., and Zhang, H.
\newblock Mini-batch stochastic approximation methods for nonconvex stochastic
  composite optimization.
\newblock \emph{Mathematical Programming}, 155\penalty0 (1-2):\penalty0
  267--305, 2016.

\bibitem[Gu et~al.(2018)Gu, Huo, Deng, and Huang]{gu2018faster}
Gu, B., Huo, Z., Deng, C., and Huang, H.
\newblock Faster derivative-free stochastic algorithm for shared memory
  machines.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1807--1816, 2018.

\bibitem[Ji et~al.(2019)Ji, Wang, Zhou, and Liang]{ji2019faster}
Ji, K., Wang, Z., Zhou, Y., and Liang, Y.
\newblock Faster stochastic algorithms via history-gradient aided batch size
  adaptation.
\newblock \emph{arXiv preprint arXiv:1910.09670}, 2019.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  315--323, 2013.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial machine learning at scale.
\newblock \emph{arXiv preprint arXiv:1611.01236}, 2016.

\bibitem[Li \& Li(2018)Li and Li]{li2018simple}
Li, Z. and Li, J.
\newblock A simple proximal stochastic gradient method for nonsmooth nonconvex
  optimization.
\newblock \emph{arXiv preprint arXiv:1802.04477}, 2018.

\bibitem[Lian et~al.(2016)Lian, Zhang, Hsieh, Huang, and
  Liu]{lian2016comprehensive}
Lian, X., Zhang, H., Hsieh, C.-J., Huang, Y., and Liu, J.
\newblock A comprehensive linear speedup analysis for asynchronous stochastic
  parallel optimization from zeroth-order to first-order.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  3054--3062, 2016.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Cheng, Hsieh, and
  Tao]{liu2018stochastic}
Liu, L., Cheng, M., Hsieh, C.-J., and Tao, D.
\newblock Stochastic zeroth-order optimization via variance reduction method.
\newblock \emph{arXiv preprint arXiv:1805.11811}, 2018{\natexlab{a}}.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Kailkhura, Chen, Ting, Chang, and
  Amini]{liu2018zeroth}
Liu, S., Kailkhura, B., Chen, P.-Y., Ting, P., Chang, S., and Amini, L.
\newblock Zeroth-order stochastic variance reduction for nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3731--3741, 2018{\natexlab{b}}.

\bibitem[Nemirovsky \& Yudin(1983)Nemirovsky and Yudin]{nemirovsky1983problem}
Nemirovsky, A.~S. and Yudin, D.~B.
\newblock Problem complexity and method efficiency in optimization.
\newblock 1983.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Nesterov \& Spokoiny(2011)Nesterov and Spokoiny]{nesterov2011random}
Nesterov, Y. and Spokoiny, V.
\newblock Random gradient-free minimization of convex functions.
\newblock Technical report, Universit{\'e} catholique de Louvain, Center for
  Operations Research and Econometrics (CORE), 2011.

\bibitem[Nguyen et~al.(2017{\natexlab{a}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Sarah: A novel method for machine learning problems using stochastic
  recursive gradient.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2613--2621, 2017{\natexlab{a}}.

\bibitem[Nguyen et~al.(2017{\natexlab{b}})Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017stochastic}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock Stochastic recursive gradient algorithm for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1705.07261}, 2017{\natexlab{b}}.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{papernot2017practical}
Papernot, N., McDaniel, P., Goodfellow, I., Jha, S., Celik, Z.~B., and Swami,
  A.
\newblock Practical black-box attacks against machine learning.
\newblock In \emph{Proceedings of the 2017 ACM on Asia Conference on Computer
  and Communications Security}, pp.\  506--519. ACM, 2017.

\bibitem[Polyak(1963)]{polyak1963gradient}
Polyak, B.~T.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, 1963.

\bibitem[Reddi et~al.(2016{\natexlab{a}})Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2016stochastic}
Reddi, S.~J., Hefny, A., Sra, S., Poczos, B., and Smola, A.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  314--323, 2016{\natexlab{a}}.

\bibitem[Reddi et~al.(2016{\natexlab{b}})Reddi, Sra, Poczos, and
  Smola]{Reddi2016}
Reddi, S.~J., Sra, S., Poczos, B., and Smola, A.
\newblock Proximal stochastic methods for nonsmooth nonconvex finite-sum
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1145--1153. 2016{\natexlab{b}}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{robbins1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 09 1951.

\bibitem[Roux et~al.(2012)Roux, Schmidt, and Bach]{Nicolas2012}
Roux, N.~L., Schmidt, M., and Bach, F.~R.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  2663--2671. 2012.

\bibitem[Shamir(2013)]{shamir2013complexity}
Shamir, O.
\newblock On the complexity of bandit and derivative-free stochastic convex
  optimization.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  3--24, 2013.

\bibitem[Taskar et~al.(2005)Taskar, Chatalbashev, Koller, and
  Guestrin]{taskar2005learning}
Taskar, B., Chatalbashev, V., Koller, D., and Guestrin, C.
\newblock Learning structured prediction models: A large margin approach.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  896--903, 2005.

\bibitem[Wainwright et~al.(2008)Wainwright, Jordan,
  et~al.]{wainwright2008graphical}
Wainwright, M.~J., Jordan, M.~I., et~al.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  1\penalty0 (1--2):\penalty0 1--305, 2008.

\bibitem[Wang et~al.(2017)Wang, Du, Balakrishnan, and
  Singh]{wang2017stochastic}
Wang, Y., Du, S., Balakrishnan, S., and Singh, A.
\newblock Stochastic zeroth-order optimization in high dimensions.
\newblock \emph{arXiv preprint arXiv:1710.10551}, 2017.

\bibitem[Wang et~al.(2018)Wang, Ji, Zhou, Liang, and
  Tarokh]{wang2018spiderboost}
Wang, Z., Ji, K., Zhou, Y., Liang, Y., and Tarokh, V.
\newblock Spiderboost: A class of faster variance-reduced algorithms for
  nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:1810.10690}, 2018.

\bibitem[Zhong et~al.(2017)Zhong, Song, Jain, Bartlett, and
  Dhillon]{zhong2017recovery}
Zhong, K., Song, Z., Jain, P., Bartlett, P.~L., and Dhillon, I.~S.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4140--4149, 2017.

\bibitem[Zhou et~al.(2018)Zhou, Xu, and Gu]{zhou2018stochastic}
Zhou, D., Xu, P., and Gu, Q.
\newblock Stochastic nested variance reduced gradient descent for nonconvex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3921--3932, 2018.

\bibitem[Zhou et~al.(2016)Zhou, Zhang, and Liang]{zhou2016geometrical}
Zhou, Y., Zhang, H., and Liang, Y.
\newblock Geometrical properties and accelerated gradient solvers of non-convex
  phase retrieval.
\newblock In \emph{54th Annual Allerton Conference on Communication, Control,
  and Computing (Allerton)}, pp.\  331--335, 2016.

\end{thebibliography}
