\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahri et~al.(2020)Bahri, Kadmon, Pennington, Schoenholz,
  Sohl-Dickstein, and Ganguli]{BKPx20}
Bahri, Y., Kadmon, J., Pennington, J., Schoenholz, S., Sohl-Dickstein, J., and
  Ganguli, S.
\newblock Statistical mechanics of deep learning.
\newblock \emph{Annual Review of Condensed Matter Physics}, 11:\penalty0
  501--528, 2020.

\bibitem[Baity-Jesi et~al.(2018)Baity-Jesi, Sagun, Geiger, Spigler, Arous,
  Cammarota, LeCun, Wyart, and Biroli]{baity2018comparing}
Baity-Jesi, M., Sagun, L., Geiger, M., Spigler, S., Arous, G.~B., Cammarota,
  C., LeCun, Y., Wyart, M., and Biroli, G.
\newblock Comparing dynamics: Deep neural networks versus glassy systems.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Ballard et~al.(2017)Ballard, Das, Martiniani, Mehta, Sagun, Stevenson,
  and Wales]{ballard2017energy}
Ballard, A.~J., Das, R., Martiniani, S., Mehta, D., Sagun, L., Stevenson,
  J.~D., and Wales, D.~J.
\newblock Energy landscapes for machine learning.
\newblock \emph{Physical Chemistry Chemical Physics}, 19\penalty0
  (20):\penalty0 12585--12603, 2017.

\bibitem[Barbier et~al.(2019)Barbier, Krzakala, Macris, Miolane, and
  Zdeborov{\'a}]{barbier2019optimal}
Barbier, J., Krzakala, F., Macris, N., Miolane, L., and Zdeborov{\'a}, L.
\newblock Optimal errors and phase transitions in high-dimensional generalized
  linear models.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (12):\penalty0 5451--5460, 2019.

\bibitem[Barra \& Guerra(2008)Barra and Guerra]{barra2008ergodic}
Barra, A. and Guerra, F.
\newblock About the ergodic regime in the analogical {H}opfield neural
  networks: moments of the partition function.
\newblock \emph{Journal of mathematical physics}, 49\penalty0 (12):\penalty0
  125217, 2008.

\bibitem[Barra et~al.(2012)Barra, Bernacchia, Santucci, and
  Contucci]{barra2012equivalence}
Barra, A., Bernacchia, A., Santucci, E., and Contucci, P.
\newblock On the equivalence of {H}opfield networks and {B}oltzmann machines.
\newblock \emph{Neural Networks}, 34:\penalty0 1--9, 2012.

\bibitem[Barsbey et~al.(2021)Barsbey, Sefidgaran, Erdogdu, Richard, and
  Simsekli]{barsbey2021heavy}
Barsbey, M., Sefidgaran, M., Erdogdu, M.~A., Richard, G., and Simsekli, U.
\newblock Heavy tails in sgd and compressibility of overparametrized neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Blalock et~al.(2020)Blalock, Gonzalez~Ortiz, Frankle, and
  Guttag]{blalock2020state}
Blalock, D., Gonzalez~Ortiz, J.~J., Frankle, J., and Guttag, J.
\newblock What is the state of neural network pruning?
\newblock In \emph{Proceedings of Machine Learning and Systems}, 2020.

\bibitem[Bojar et~al.(2014)Bojar, Buck, Federmann, Haddow, Koehn, Leveling,
  Monz, Pecina, Post, Saint-Amand, Soricut, Specia, and
  Tamchyna]{bojar-etal-2014-findings}
Bojar, O., Buck, C., Federmann, C., Haddow, B., Koehn, P., Leveling, J., Monz,
  C., Pecina, P., Post, M., Saint-Amand, H., Soricut, R., Specia, L., and
  Tamchyna, A.
\newblock Findings of the 2014 workshop on statistical machine translation.
\newblock In \emph{Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, 2014.

\bibitem[Brooks et~al.(2001)Brooks, Onuchic, and Wales]{brooks2001}
Brooks, C., Onuchic, J., and Wales, D.
\newblock Statistical thermodynamics - taking a walk on a landscape.
\newblock \emph{Science (New York, N.Y.)}, 293:\penalty0 612--3, 08 2001.

\bibitem[Brush(1967)]{Brush1967HistoryOT}
Brush, S.~G.
\newblock History of the {L}enz-{I}sing model.
\newblock \emph{Reviews of Modern Physics}, 39:\penalty0 883--893, 1967.

\bibitem[Draxler et~al.(2018)Draxler, Veschgini, Salmhofer, and
  Hamprecht]{draxler2018essentially}
Draxler, F., Veschgini, K., Salmhofer, M., and Hamprecht, F.
\newblock Essentially no barriers in neural network energy landscape.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Engel \& den Broeck(2001)Engel and den Broeck]{EB01_BOOK}
Engel, A. and den Broeck, C. P. L.~V.
\newblock \emph{Statistical mechanics of learning}.
\newblock Cambridge University Press, New York, NY, USA, 2001.

\bibitem[Evci et~al.(2019)Evci, Pedregosa, Gomez, and Elsen]{evci2019the}
Evci, U., Pedregosa, F., Gomez, A., and Elsen, E.
\newblock The difficulty of training sparse neural networks.
\newblock In \emph{International Conference on Machine Learning 2019 Workshop
  on Identifying and Understanding Deep Learning Phenomena}, 2019.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Frankle, J., Dziugaite, G.~K., Roy, D., and Carbin, M.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Garipov et~al.(2018)Garipov, Izmailov, Podoprikhin, Vetrov, and
  Wilson]{garipov2018loss}
Garipov, T., Izmailov, P., Podoprikhin, D., Vetrov, D.~P., and Wilson, A.~G.
\newblock {Loss surfaces, mode connectivity, and fast ensembling of DNNs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Han, S., Pool, J., Tran, J., and Dally, W.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Haussler et~al.(1996)Haussler, Kearns, Seung, and Tishby]{DKST96}
Haussler, D., Kearns, M., Seung, H.~S., and Tishby, N.
\newblock Rigorous learning curve bounds from statistical mechanics.
\newblock \emph{Machine Learning}, 25\penalty0 (2):\penalty0 195--236, 1996.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016identity}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, 2016.

\bibitem[Hopfield(1982)]{article}
Hopfield, J.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock \emph{Proceedings of the National Academy of Sciences of the United
  States of America}, 79:\penalty0 2554--8, 05 1982.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2017.

\bibitem[Kornblith et~al.(2019)Kornblith, Norouzi, Lee, and
  Hinton]{kornblith2019similarity}
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G.
\newblock Similarity of neural network representations revisited.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Nair, and
  Hinton]{krizhevsky2009cifar}
Krizhevsky, A., Nair, V., and Hinton, G.
\newblock Cifar-10 and cifar-100 datasets.
\newblock 2009.

\bibitem[Le \& Hua(2021)Le and Hua]{le2021network}
Le, D.~H. and Hua, B.-S.
\newblock Network pruning that matters: A case study on retraining variants.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Lecun et~al.(1989)Lecun, Denker, and Solla]{obd1989}
Lecun, Y., Denker, J., and Solla, S.
\newblock Optimal brain damage.
\newblock In \emph{Advances in Neural Information Processing Systems}, 1989.

\bibitem[Lee et~al.(2021)Lee, Park, Mo, Ahn, and Shin]{lee2021layeradaptive}
Lee, J., Park, S., Mo, S., Ahn, S., and Shin, J.
\newblock Layer-adaptive sparsity for the magnitude-based pruning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Li et~al.(2020)Li, Wallace, Shen, Lin, Keutzer, Klein, and
  Gonzalez]{li2020train}
Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., and Gonzalez,
  J.
\newblock Train big, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Liu et~al.(2021)Liu, Chen, Chen, Atashgahi, Yin, Kou, Shen,
  Pechenizkiy, Wang, and Mocanu]{liu2021sparse}
Liu, S., Chen, T., Chen, X., Atashgahi, Z., Yin, L., Kou, H., Shen, L.,
  Pechenizkiy, M., Wang, Z., and Mocanu, D.~C.
\newblock Sparse training via boosting pruning plasticity with
  neuroregeneration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Liu et~al.(2019)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Liu, Z., Sun, M., Zhou, T., Huang, G., and Darrell, T.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Martin \& Mahoney(2017)Martin and Mahoney]{martin2017rethinking}
Martin, C.~H. and Mahoney, M.~W.
\newblock Rethinking generalization requires revisiting old ideas: statistical
  mechanics approaches and complex learning behavior.
\newblock \emph{arXiv preprint arXiv:1710.09553}, 2017.

\bibitem[Martin \& Mahoney(2021{\natexlab{a}})Martin and
  Mahoney]{MM21a_simpsons_TR}
Martin, C.~H. and Mahoney, M.~W.
\newblock Post-mortem on a deep learning contest: a {S}impson's paradox and the
  complementary roles of scale metrics versus shape metrics.
\newblock Technical Report Preprint: arXiv:2106.00734, 2021{\natexlab{a}}.

\bibitem[Martin \& Mahoney(2021{\natexlab{b}})Martin and
  Mahoney]{martin2018implicit_JMLRversion}
Martin, C.~H. and Mahoney, M.~W.
\newblock Implicit self-regularization in deep neural networks: Evidence from
  random matrix theory and implications for learning.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (165):\penalty0 1--73, 2021{\natexlab{b}}.

\bibitem[Martin et~al.(2021)Martin, Peng, and
  Mahoney]{martin2020predicting_NatComm}
Martin, C.~H., Peng, T.~S., and Mahoney, M.~W.
\newblock Predicting trends in the quality of state-of-the-art neural networks
  without access to training or testing data.
\newblock \emph{Nature Communications}, 2021.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
McCandlish, S., Kaplan, J., Amodei, D., and Team, O.~D.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem[Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila, and
  Kautz]{molchanov2019pruning}
Molchanov, P., Tyree, S., Karras, T., Aila, T., and Kautz, J.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Na et~al.(2022)Na, Mehta, and Strubell]{na-etal-2022-train}
Na, C., Mehta, S.~V., and Strubell, E.
\newblock {Train Flat, Then Compress: Sharpness-Aware Minimization Learns More
  Compressible Models}.
\newblock In \emph{Findings of the Conference on Empirical Methods in Natural
  Language Processing}, 2022.

\bibitem[Renda et~al.(2020)Renda, Frankle, and Carbin]{Renda2020Comparing}
Renda, A., Frankle, J., and Carbin, M.
\newblock Comparing rewinding and fine-tuning in neural network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Rosenfeld et~al.(2021)Rosenfeld, Frankle, Carbin, and
  Shavit]{rosenfeld2021predictability}
Rosenfeld, J.~S., Frankle, J., Carbin, M., and Shavit, N.
\newblock On the predictability of pruning across scales.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Sermanet et~al.(2011)Sermanet, Kavukcuoglu, and
  LeCun]{sermanet2011traffic}
Sermanet, P., Kavukcuoglu, K., and LeCun, Y.
\newblock Traffic signs and pedestrians vision with multi-scale convolutional
  networks.
\newblock In \emph{Snowbird Machine Learning Workshop}, 2011.

\bibitem[Seung et~al.(1992)Seung, Sompolinsky, and Tishby]{SST92}
Seung, H.~S., Sompolinsky, H., and Tishby, N.
\newblock Statistical mechanics of learning from examples.
\newblock \emph{Physical Review A}, 45\penalty0 (8):\penalty0 6056--6091, 1992.

\bibitem[Shen et~al.(2022)Shen, Molchanov, Yin, and Alvarez]{Shen_2022_CVPR}
Shen, M., Molchanov, P., Yin, H., and Alvarez, J.~M.
\newblock When to prune? a policy towards early structural pruning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2022.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Sorscher et~al.(2022)Sorscher, Geirhos, Shekhar, Ganguli, and
  Morcos]{sorscher2022beyond}
Sorscher, B., Geirhos, R., Shekhar, S., Ganguli, S., and Morcos, A.
\newblock Beyond neural scaling laws: beating power law scaling via data
  pruning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Stillinger(2015)]{stillinger2015energy}
Stillinger, F.
\newblock \emph{Energy Landscapes, Inherent Structures, and Condensed-Matter
  Phenomena}.
\newblock Princeton University Press, 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Wales(2003)]{wales2003energy}
Wales, D.
\newblock \emph{Energy Landscapes: Applications to Clusters, Biomolecules and
  Glasses}.
\newblock Cambridge Molecular Science. Cambridge University Press, 2003.

\bibitem[Watkin et~al.(1993)Watkin, Rau, and Biehl]{WRB93}
Watkin, T. L.~H., Rau, A., and Biehl, M.
\newblock The statistical mechanics of learning a rule.
\newblock \emph{Rev. Mod. Phys.}, 65\penalty0 (2):\penalty0 499--556, 1993.

\bibitem[Yang et~al.(2021)Yang, Hodgkinson, Theisen, Zou, Gonzalez,
  Ramchandran, and Mahoney]{yang2021taxonomizing}
Yang, Y., Hodgkinson, L., Theisen, R., Zou, J., Gonzalez, J.~E., Ramchandran,
  K., and Mahoney, M.~W.
\newblock Taxonomizing local versus global structure in neural network loss
  landscapes.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Yang et~al.(2022)Yang, Theisen, Hodgkinson, Gonzalez, Ramchandran,
  Martin, and Mahoney]{yang2022evaluating}
Yang, Y., Theisen, R., Hodgkinson, L., Gonzalez, J.~E., Ramchandran, K.,
  Martin, C.~H., and Mahoney, M.~W.
\newblock Evaluating natural language processing models with generalization
  metrics that do not need access to any training or testing data.
\newblock \emph{arXiv preprint arXiv:2202.02842}, 2022.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M.~W.
\newblock Py{H}essian: Neural networks through the lens of the {H}essian.
\newblock In \emph{IEEE International Conference on Big Data}, 2020.

\end{thebibliography}
