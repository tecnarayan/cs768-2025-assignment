\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Atanov et~al.(2019)Atanov, Volokhova, Ashukha, Sosnovik, and
  Vetrov]{atanov2019semi}
Atanov, A., Volokhova, A., Ashukha, A., Sosnovik, I., and Vetrov, D.
\newblock Semi-conditional normalizing flows for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.00505}, 2019.

\bibitem[Athiwaratkun et~al.(2019)Athiwaratkun, Finzi, Izmailov, and
  Wilson]{athiwaratkun2018there}
Athiwaratkun, B., Finzi, M., Izmailov, P., and Wilson, A.~G.
\newblock There are many consistent explanations of unlabeled data: Why you
  should average.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=rkgKBhA5Y7}.

\bibitem[Behrmann et~al.(2018)Behrmann, Duvenaud, and
  Jacobsen]{behrmann2018invertible}
Behrmann, J., Duvenaud, D., and Jacobsen, J.-H.
\newblock Invertible residual networks.
\newblock \emph{arXiv preprint arXiv:1811.00995}, 2018.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
Berthelot, D., Carlini, N., Goodfellow, I., Papernot, N., Oliver, A., and
  Raffel, C.
\newblock {MixMatch}: A holistic approach to semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1905.02249}, 2019.

\bibitem[Berthelot et~al.(2020)Berthelot, Carlini, Cubuk, Kurakin, Sohn, Zhang,
  and Raffel]{berthelot2020remixmatch}
Berthelot, D., Carlini, N., Cubuk, E.~D., Kurakin, A., Sohn, K., Zhang, H., and
  Raffel, C.
\newblock Remixmatch: Semi-supervised learning with distribution matching and
  augmentation anchoring.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=HklkeR4KPB}.

\bibitem[Chen et~al.(2019)Chen, Behrmann, Duvenaud, and
  Jacobsen]{chen2019residual}
Chen, R.~T., Behrmann, J., Duvenaud, D., and Jacobsen, J.-H.
\newblock Residual flows for invertible generative modeling.
\newblock \emph{arXiv preprint arXiv:1906.02735}, 2019.

\bibitem[Dai et~al.(2017)Dai, Yang, Yang, Cohen, and Salakhutdinov]{dai2017}
Dai, Z., Yang, Z., Yang, F., Cohen, W.~W., and Salakhutdinov, R.~R.
\newblock Good semi-supervised learning that requires a bad {GAN}.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pp.\
  6510--6520, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dinh et~al.(2014)Dinh, Krueger, and Bengio]{dinh2014nice}
Dinh, L., Krueger, D., and Bengio, Y.
\newblock {NICE}: Non-linear independent components estimation.
\newblock \emph{arXiv preprint arXiv:1410.8516}, 2014.

\bibitem[Dinh et~al.(2016)Dinh, Sohl-Dickstein, and Bengio]{dinh2016density}
Dinh, L., Sohl-Dickstein, J., and Bengio, S.
\newblock Density estimation using {Real NVP}.
\newblock \emph{arXiv preprint arXiv:1605.08803}, 2016.

\bibitem[Finzi et~al.(2019)Finzi, Izmailov, Maddox, Kirichenko, and
  Wilson]{finzi2019invertible}
Finzi, M., Izmailov, P., Maddox, W., Kirichenko, P., and Wilson, A.~G.
\newblock Invertible convolutional networks.
\newblock In \emph{Workshop on Invertible Neural Nets and Normalizing Flows,
  International Conference on Machine Learning}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and
  Weinberger]{weinberger2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{CoRR}, abs/1706.04599, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.04599}.

\bibitem[Izmailov et~al.(2019)Izmailov, Kirichenko, Finzi, and
  Wilson]{izmailov2019semi}
Izmailov, P., Kirichenko, P., Finzi, M., and Wilson, A.~G.
\newblock Semi-supervised learning with normalizing flows.
\newblock In \emph{Workshop on Invertible Neural Nets and Normalizing Flows,
  International Conference on Machine Learning}, 2019.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1$\times$1 convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  10215--10224, 2018.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {Bayes}.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kingma et~al.(2014)Kingma, Mohamed, Rezende, and
  Welling]{kingma2014semi}
Kingma, D.~P., Mohamed, S., Rezende, D.~J., and Welling, M.
\newblock Semi-supervised learning with deep generative models.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3581--3589, 2014.

\bibitem[Laine \& Aila(2016)Laine and Aila]{laine2016temporal}
Laine, S. and Aila, T.
\newblock Temporal ensembling for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Mahendran \& Vedaldi(2015)Mahendran and
  Vedaldi]{mahendran2015understanding}
Mahendran, A. and Vedaldi, A.
\newblock Understanding deep image representations by inverting them.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  5188--5196, 2015.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Ishii, and
  Koyama]{miyato2018virtual}
Miyato, T., Maeda, S.-i., Ishii, S., and Koyama, M.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2018.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[Nalisnick et~al.(2018)Nalisnick, Matsukawa, Teh, Gorur, and
  Lakshminarayanan]{nalisnick2018deep}
Nalisnick, E., Matsukawa, A., Teh, Y.~W., Gorur, D., and Lakshminarayanan, B.
\newblock Do deep generative models know what they don't know?
\newblock \emph{arXiv preprint arXiv:1810.09136}, 2018.

\bibitem[Nalisnick et~al.(2019)Nalisnick, Matsukawa, Teh, Gorur, and
  Lakshminarayanan]{nalisnick2019hybrid}
Nalisnick, E., Matsukawa, A., Teh, Y.~W., Gorur, D., and Lakshminarayanan, B.
\newblock Hybrid models with deep and invertible features.
\newblock \emph{arXiv preprint arXiv:1902.02767}, 2019.

\bibitem[Olah et~al.(2017)Olah, Mordvintsev, and Schubert]{olah2017feature}
Olah, C., Mordvintsev, A., and Schubert, L.
\newblock Feature visualization.
\newblock \emph{Distill}, 2017.
\newblock \doi{10.23915/distill.00007}.
\newblock https://distill.pub/2017/feature-visualization.

\bibitem[Oliver et~al.(2018)Oliver, Odena, Raffel, Cubuk, and
  Goodfellow]{oliver2018realistic}
Oliver, A., Odena, A., Raffel, C.~A., Cubuk, E.~D., and Goodfellow, I.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3235--3246, 2018.

\bibitem[Oord et~al.(2016)Oord, Kalchbrenner, and Kavukcuoglu]{oord2016pixel}
Oord, A. v.~d., Kalchbrenner, N., and Kavukcuoglu, K.
\newblock Pixel recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1601.06759}, 2016.

\bibitem[Papamakarios et~al.(2017)Papamakarios, Pavlakou, and
  Murray]{papamakarios2017masked}
Papamakarios, G., Pavlakou, T., and Murray, I.
\newblock Masked autoregressive flow for density estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2338--2347, 2017.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen,
  X.
\newblock Improved techniques for training {GAN}s.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2234--2242, 2016.

\bibitem[Song et~al.(2019)Song, Meng, and Ermon]{song2019mintnet}
Song, Y., Meng, C., and Ermon, S.
\newblock Mintnet: Building invertible neural networks with masked
  convolutions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  11002--11012, 2019.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Tarvainen \& Valpola(2017)Tarvainen and Valpola]{tarvainen2017mean}
Tarvainen, A. and Valpola, H.
\newblock Mean teachers are better role models: Weight-averaged consistency
  targets improve semi-supervised deep learning results.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1195--1204, 2017.

\bibitem[Verma et~al.(2019)Verma, Lamb, Kannala, Bengio, and
  Lopez-Paz]{verma2019interpolation}
Verma, V., Lamb, A., Kannala, J., Bengio, Y., and Lopez-Paz, D.
\newblock Interpolation consistency training for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1903.03825}, 2019.

\bibitem[Xie et~al.(2020)Xie, Dai, Hovy, Luong, and Le]{xie2020unsupervised}
Xie, Q., Dai, Z., Hovy, E., Luong, M.-T., and Le, Q.~V.
\newblock Unsupervised data augmentation for consistency training, 2020.
\newblock URL \url{https://openreview.net/forum?id=ByeL1R4FvS}.

\bibitem[Xu et~al.(2017)Xu, Sun, Deng, and Tan]{xu2017variational}
Xu, W., Sun, H., Deng, C., and Tan, Y.
\newblock Variational autoencoder for semi-supervised text classification.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem[Zhou et~al.(2004)Zhou, Bousquet, Lal, Weston, and
  Sch{\"o}lkopf]{zhou2004learning}
Zhou, D., Bousquet, O., Lal, T.~N., Weston, J., and Sch{\"o}lkopf, B.
\newblock Learning with local and global consistency.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  321--328, 2004.

\end{thebibliography}
