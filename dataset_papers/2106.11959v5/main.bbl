\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Akiba et~al.(2019)Akiba, Sano, Yanase, Ohta, and Koyama]{optuna}
T.~Akiba, S.~Sano, T.~Yanase, T.~Ohta, and M.~Koyama.
\newblock Optuna: A next-generation hyperparameter optimization framework.
\newblock In \emph{KDD}, 2019.

\bibitem[Arik and Pfister(2020)]{tabnet}
S.~O. Arik and T.~Pfister.
\newblock Tabnet: Attentive interpretable tabular learning.
\newblock \emph{arXiv}, 1908.07442v5, 2020.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{layernorm}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv}, 1607.06450v1, 2016.

\bibitem[Badirli et~al.(2020)Badirli, Liu, Xing, Bhowmik, Doan, and Keerthi]{grownet}
S.~Badirli, X.~Liu, Z.~Xing, A.~Bhowmik, K.~Doan, and S.~S. Keerthi.
\newblock Gradient boosting neural networks: Grownet.
\newblock \emph{arXiv}, 2002.07971v2, 2020.

\bibitem[Baldi et~al.(2014)Baldi, Sadowski, and Whiteson]{higgs}
P.~Baldi, P.~Sadowski, and D.~Whiteson.
\newblock Searching for exotic particles in high-energy physics with deep learning.
\newblock \emph{Nature Communications}, 5, 2014.

\bibitem[Bertin-Mahieux et~al.(2011)Bertin-Mahieux, Ellis, Whitman, and Lamere]{year}
T.~Bertin-Mahieux, D.~P. Ellis, B.~Whitman, and P.~Lamere.
\newblock The million song dataset.
\newblock In \emph{{Proceedings of the 12th International Conference on Music Information Retrieval ({ISMIR} 2011)}}, 2011.

\bibitem[Beutel et~al.(2018)Beutel, Covington, Jain, Xu, Li, Gatto, and Chi]{latent-cross}
A.~Beutel, P.~Covington, S.~Jain, C.~Xu, J.~Li, V.~Gatto, and E.~H. Chi.
\newblock Latent cross: Making use of context in recurrent recommender systems.
\newblock In \emph{WSDM 2018: The Eleventh ACM International Conference on Web Search and Data Mining}, 2018.

\bibitem[Blackard and Dean.(2000)]{covertype}
J.~A. Blackard and D.~J. Dean.
\newblock Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables.
\newblock \emph{Computers and Electronics in Agriculture}, 24\penalty0 (3):\penalty0 131--151, 2000.

\bibitem[Breiman(2001)]{random-forest}
L.~Breiman.
\newblock Random forests.
\newblock \emph{Machine Learning}, 45\penalty0 (1):\penalty0 5--32, 2001.

\bibitem[Chapelle and Chang(2011)]{yahoo}
O.~Chapelle and Y.~Chang.
\newblock Yahoo! learning to rank challenge overview.
\newblock In \emph{Proceedings of the Learning to Rank Challenge}, volume~14, 2011.

\bibitem[Chen and Guestrin(2016)]{xgboost}
T.~Chen and C.~Guestrin.
\newblock Xgboost: A scalable tree boosting system.
\newblock In \emph{SIGKDD}, 2016.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and Fei-Fei]{imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv}, 1810.04805v2, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai, T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Fort et~al.(2020)Fort, Hu, and Lakshminarayanan]{ensembles-loss}
S.~Fort, H.~Hu, and B.~Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock \emph{arXiv}, 1912.02757v2, 2020.

\bibitem[Friedman(2001)]{greedy-func-approx}
J.~H. Friedman.
\newblock Greedy function approximation: A gradient boosting machine.
\newblock \emph{The Annals of Statistics}, 29\penalty0 (5):\penalty0 1189--1232, 2001.

\bibitem[Geusebroek et~al.(2005)Geusebroek, Burghouts, , and Smeulders]{aloi}
J.~M. Geusebroek, G.~J. Burghouts, , and A.~W.~M. Smeulders.
\newblock The amsterdam library of object images.
\newblock \emph{Int. J. Comput. Vision}, 61\penalty0 (1):\penalty0 103--112, 2005.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and Courville]{goodfellow-book}
I.~Goodfellow, Y.~Bengio, and A.~Courville.
\newblock \emph{Deep Learning}.
\newblock MIT Press, 2016.
\newblock \url{http://www.deeplearningbook.org}.

\bibitem[Guyon et~al.(2019)Guyon, Sun-Hosoya, Boull\'e, Escalante, Escalera, Liu, Jajetic, Ray, Saeed, Sebag, Statnikov, Tu, and Viegas]{helena-jannis}
I.~Guyon, L.~Sun-Hosoya, M.~Boull\'e, H.~J. Escalante, S.~Escalera, Z.~Liu, D.~Jajetic, B.~Ray, M.~Saeed, M.~Sebag, A.~Statnikov, W.~Tu, and E.~Viegas.
\newblock Analysis of the automl challenge series 2015-2018.
\newblock In \emph{AutoML}, Springer series on Challenges in Machine Learning, 2019.

\bibitem[Hazimeh et~al.(2020)Hazimeh, Ponomareva, Mol, Tan, and Mazumder]{tel}
H.~Hazimeh, N.~Ponomareva, P.~Mol, Z.~Tan, and R.~Mazumder.
\newblock The tree ensemble layer: Differentiability meets conditional computation.
\newblock In \emph{ICML}, 2020.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and Sun]{kaiming-init}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.
\newblock In \emph{ICCV}, 2015{\natexlab{a}}.

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{resnet}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{arXiv}, 1512.03385v1, 2015{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{preactivation}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{ECCV}, 2016.

\bibitem[Huang et~al.(2020{\natexlab{a}})Huang, Khetan, Cvitkovic, and Karnin]{tabtransformer}
X.~Huang, A.~Khetan, M.~Cvitkovic, and Z.~Karnin.
\newblock Tabtransformer: Tabular data modeling using contextual embeddings.
\newblock \emph{arXiv}, 2012.06678v1, 2020{\natexlab{a}}.

\bibitem[Huang et~al.(2020{\natexlab{b}})Huang, Perez, Ba, and Volkovs]{tfixup}
X.~S. Huang, F.~Perez, J.~Ba, and M.~Volkovs.
\newblock Improving transformer optimization through better initialization.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Ke et~al.(2017)Ke, Meng, Finley, Wang, Chen, Ma, Ye, and Liu]{lightgbm}
G.~Ke, Q.~Meng, T.~Finley, T.~Wang, W.~Chen, W.~Ma, Q.~Ye, and T.-Y. Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock \emph{Advances in neural information processing systems}, 30:\penalty0 3146--3154, 2017.

\bibitem[{Kelley Pace} and Barry(1997)]{california}
R.~{Kelley Pace} and R.~Barry.
\newblock Sparse spatial autoregressions.
\newblock \emph{Statistics \& Probability Letters}, 33\penalty0 (3):\penalty0 291--297, 1997.

\bibitem[Kingma and Ba(2017)]{adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv}, 1412.6980v9, 2017.

\bibitem[Klambauer et~al.(2017)Klambauer, Unterthiner, Mayr, and Hochreiter]{snn}
G.~Klambauer, T.~Unterthiner, A.~Mayr, and S.~Hochreiter.
\newblock Self-normalizing neural networks.
\newblock In \emph{NIPS}, 2017.

\bibitem[Kohavi(1996)]{adult}
R.~Kohavi.
\newblock Scaling up the accuracy of naive-bayes classifiers: a decision-tree hybrid.
\newblock In \emph{KDD}, 1996.

\bibitem[Kontschieder et~al.(2015)Kontschieder, Fiterau, Criminisi, and Rota~Bulo]{dndf}
P.~Kontschieder, M.~Fiterau, A.~Criminisi, and S.~Rota~Bulo.
\newblock Deep neural decision forests.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, 2015.

\bibitem[Liu et~al.(2020)Liu, Liu, Gao, Chen, and Han]{admin}
L.~Liu, X.~Liu, J.~Gao, W.~Chen, and J.~Han.
\newblock Understanding the difficulty of training transformers.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Loshchilov and Hutter(2019)]{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{ICLR}, 2019.

\bibitem[Lou and Obukhov(2017)]{odt}
Y.~Lou and M.~Obukhov.
\newblock Bdt: Gradient boosted decision tables for high accuracy and scoring efficiency.
\newblock In \emph{Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining}, 2017.

\bibitem[Moro et~al.(2014)Moro, Cortez, and Rita]{dataset-bank}
S.~Moro, P.~Cortez, and P.~Rita.
\newblock A data-driven approach to predict the success of bank telemarketing.
\newblock \emph{Decis. Support Syst.}, 62:\penalty0 22--31, 2014.

\bibitem[Narang et~al.(2021)Narang, Chung, Tay, Fedus, Fevry, Matena, Malkan, Fiedel, Shazeer, Lan, Zhou, Li, Ding, Marcus, Roberts, and Raffel]{transformer-modifications}
S.~Narang, H.~W. Chung, Y.~Tay, W.~Fedus, T.~Fevry, M.~Matena, K.~Malkan, N.~Fiedel, N.~Shazeer, Z.~Lan, Y.~Zhou, W.~Li, N.~Ding, J.~Marcus, A.~Roberts, and C.~Raffel.
\newblock Do transformer modifications transfer across implementations and applications?
\newblock \emph{arXiv}, 2102.11972v1, 2021.

\bibitem[Nguyen and Salazar(2019)]{tears}
T.~Q. Nguyen and J.~Salazar.
\newblock Transformers without tears: Improving the normalization of self-attention.
\newblock In \emph{IWSLT}, 2019.

\bibitem[Pedregosa et~al.(2011)Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, and Duchesnay]{scikit-learn}
F.~Pedregosa, G.~Varoquaux, A.~Gramfort, V.~Michel, B.~Thirion, O.~Grisel, M.~Blondel, P.~Prettenhofer, R.~Weiss, V.~Dubourg, J.~Vanderplas, A.~Passos, D.~Cournapeau, M.~Brucher, M.~Perrot, and E.~Duchesnay.
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2825--2830, 2011.

\bibitem[Popov et~al.(2020)Popov, Morozov, and Babenko]{node}
S.~Popov, S.~Morozov, and A.~Babenko.
\newblock Neural oblivious decision ensembles for deep learning on tabular data.
\newblock In \emph{ICLR}, 2020.

\bibitem[Prokhorenkova et~al.(2018)Prokhorenkova, Gusev, Vorobev, Dorogush, and Gulin]{catboost}
L.~Prokhorenkova, G.~Gusev, A.~Vorobev, A.~V. Dorogush, and A.~Gulin.
\newblock Catboost: unbiased boosting with categorical features.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Qin and Liu(2013)]{microsoft}
T.~Qin and T.~Liu.
\newblock Introducing {LETOR} 4.0 datasets.
\newblock \emph{arXiv}, 1306.2597v1, 2013.

\bibitem[Qin et~al.(2021)Qin, Yan, Zhuang, Tay, Pasumarthi, Wang, Bendersky, and Najork]{dasalc}
Z.~Qin, L.~Yan, H.~Zhuang, Y.~Tay, R.~K. Pasumarthi, X.~Wang, M.~Bendersky, and M.~Najork.
\newblock Are neural rankers still outperformed by gradient boosted decision trees?
\newblock In \emph{ICLR}, 2021.

\bibitem[Shazeer(2020)]{glu-variants}
N.~Shazeer.
\newblock Glu variants improve transformer.
\newblock \emph{arXiv}, 2002.05202v1, 2020.

\bibitem[Song et~al.(2019)Song, Shi, Xiao, Duan, Xu, Zhang, and Tang]{autoint}
W.~Song, C.~Shi, Z.~Xiao, Z.~Duan, Y.~Xu, M.~Zhang, and J.~Tang.
\newblock Autoint: Automatic feature interaction learning via self-attentive neural networks.
\newblock In \emph{CIKM}, 2019.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov]{dropout}
N.~Srivastava, G.~E. Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0 (1):\penalty0 1929--1958, 2014.

\bibitem[Sun and Iyyer(2021)]{resnet-nlp}
S.~Sun and M.~Iyyer.
\newblock Revisiting simple neural probabilistic language models.
\newblock In \emph{NAACL}, 2021.

\bibitem[Sundararajan et~al.(2017)Sundararajan, Taly, and Yan]{integrated-gradients}
M.~Sundararajan, A.~Taly, and Q.~Yan.
\newblock Axiomatic attribution for deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Tay et~al.(2020)Tay, Dehghani, Bahri, and Metzler]{efficient-transformers}
Y.~Tay, M.~Dehghani, D.~Bahri, and D.~Metzler.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv}, 2009.06732v1, 2020.

\bibitem[Turner et~al.(2021)Turner, Eriksson, McCourt, Kiili, Laaksonen, Xu, and Guyon]{hp-tuning}
R.~Turner, D.~Eriksson, M.~McCourt, J.~Kiili, E.~Laaksonen, Z.~Xu, and I.~Guyon.
\newblock Bayesian optimization is superior to random search for machine learning hyperparameter tuning: Analysis of the black-box optimization challenge 2020.
\newblock \emph{arXiv}, https://arxiv.org/abs/2104.10201v1, 2021.

\bibitem[Vanschoren et~al.(2014)Vanschoren, van Rijn, Bischl, and Torgo]{openml}
J.~Vanschoren, J.~N. van Rijn, B.~Bischl, and L.~Torgo.
\newblock Openml: networked science in machine learning.
\newblock \emph{arXiv}, 1407.7722v1, 2014.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{transformer}
A.~Vaswani, N.~Shazeer, N.~Parmar, J.~Uszkoreit, L.~Jones, A.~N. Gomez, L.~Kaiser, and I.~Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Wang et~al.(2019{\natexlab{a}})Wang, Singh, Michael, Hill, Levy, and Bowman]{glue}
A.~Wang, A.~Singh, J.~Michael, F.~Hill, O.~Levy, and S.~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural language understanding.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Wang et~al.(2019{\natexlab{b}})Wang, Li, Xiao, Zhu, Li, Wong, and Chao]{prenorm}
Q.~Wang, B.~Li, T.~Xiao, J.~Zhu, C.~Li, D.~F. Wong, and L.~S. Chao.
\newblock Learning deep transformer models for machine translation.
\newblock In \emph{ACL}, 2019{\natexlab{b}}.

\bibitem[Wang et~al.(2017)Wang, Fu, Fu, and Wang]{dcn}
R.~Wang, B.~Fu, G.~Fu, and M.~Wang.
\newblock Deep \& cross network for ad click predictions.
\newblock In \emph{ADKDD}, 2017.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Shivanna, Cheng, Jain, Lin, Hong, and Chi]{dcn2}
R.~Wang, R.~Shivanna, D.~Z. Cheng, S.~Jain, D.~Lin, L.~Hong, and E.~H. Chi.
\newblock Dcn v2: Improved deep \& cross network and practical lessons for web-scale learning to rank systems.
\newblock \emph{arXiv}, 2008.13535v2, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Li, Khabsa, Fang, and Ma]{linformer}
S.~Wang, B.~Z. Li, M.~Khabsa, H.~Fang, and H.~Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv}, 2006.04768v3, 2020{\natexlab{b}}.

\bibitem[Wies et~al.(2021)Wies, Levine, Jannai, and Shashua]{which-transformer-fits-my-data}
N.~Wies, Y.~Levine, D.~Jannai, and A.~Shashua.
\newblock Which transformer architecture fits my data? a vocabulary bottleneck in self-attention.
\newblock In \emph{ICLM}, 2021.

\bibitem[Wilcoxon(1945)]{wilcoxon}
F.~Wilcoxon.
\newblock Individual comparisons by ranking methods.
\newblock \emph{Biometrics Bulletin}, 1\penalty0 (6):\penalty0 80, 1945.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{hf-transformers}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac, T.~Rault, R.~Louf, M.~Funtowicz, J.~Davison, S.~Shleifer, P.~von Platen, C.~Ma, Y.~Jernite, J.~Plu, C.~Xu, T.~L. Scao, S.~Gugger, M.~Drame, Q.~Lhoest, and A.~M. Rush.
\newblock Huggingface's transformers: State-of-the-art natural language processing.
\newblock \emph{arXiv}, 1910.03771v5, 2020.

\bibitem[Yang et~al.(2018)Yang, Morillo, and Hospedales]{dndt}
Y.~Yang, I.~G. Morillo, and T.~M. Hospedales.
\newblock Deep neural decision trees.
\newblock \emph{arXiv}, 1806.06988v1, 2018.

\end{thebibliography}
