\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu(2017)]{allen2017katyusha}
Allen-Zhu, Z.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  1200--1205. ACM, 2017.

\bibitem[Bengio(2012)]{bengio2012practical}
Bengio, Y.
\newblock Practical recommendations for gradient-based training of deep
  architectures.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  437--478.
  Springer, 2012.

\bibitem[Csiba \& Richt{\'a}rik(2015)Csiba and Richt{\'a}rik]{csiba2015primal}
Csiba, D. and Richt{\'a}rik, P.
\newblock Primal method for erm with flexible mini-batching schemes and
  non-convex losses.
\newblock \emph{arXiv preprint arXiv:1506.02227}, 2015.

\bibitem[De et~al.(2016)De, Yadav, Jacobs, and Goldstein]{de2016big}
De, S., Yadav, A., Jacobs, D., and Goldstein, T.
\newblock Big batch sgd: Automated inference using adaptive batch sizes.
\newblock \emph{arXiv preprint arXiv:1610.05792}, 2016.

\bibitem[Defazio(2014)]{defazio2014new}
Defazio, A.
\newblock \emph{New Optimization Methods for Machine Learning}.
\newblock PhD thesis, PhD thesis, Australian National University, 2014.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1646--1654, 2014.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Dekel, O., Gilad-Bachrach, R., Shamir, O., and Xiao, L.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jan):\penalty0 165--202, 2012.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Fan et~al.(2008)Fan, Chang, Hsieh, Wang, and Lin]{fan2008liblinear}
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J.
\newblock Liblinear: A library for large linear classification.
\newblock \emph{Journal of Machine Learning Research}, 9:\penalty0 1871--1874,
  2008.

\bibitem[Harikandeh et~al.(2015)Harikandeh, Ahmed, Virani, Schmidt,
  Kone{\v{c}}n{\`y}, and Sallinen]{harikandeh2015stopwasting}
Harikandeh, R., Ahmed, M.~O., Virani, A., Schmidt, M., Kone{\v{c}}n{\`y}, J.,
  and Sallinen, S.
\newblock Stopwasting my gradients: Practical svrg.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2251--2259, 2015.

\bibitem[Hofmann et~al.(2015)Hofmann, Lucchi, Lacoste-Julien, and
  McWilliams]{hofmann2015variance}
Hofmann, T., Lucchi, A., Lacoste-Julien, S., and McWilliams, B.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2305--2313, 2015.

\bibitem[Hsieh et~al.(2015)Hsieh, Yu, and Dhillon]{hsieh2015passcode}
Hsieh, C.-J., Yu, H.-F., and Dhillon, I.
\newblock Passcode: Parallel asynchronous stochastic dual co-ordinate descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2370--2379, 2015.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013accelerating}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  315--323, 2013.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Konecn{\`y} \& Richt{\'a}rik(2013)Konecn{\`y} and
  Richt{\'a}rik]{konecny2013semi}
Konecn{\`y}, J. and Richt{\'a}rik, P.
\newblock Semi-stochastic gradient descent methods.
\newblock \emph{arXiv preprint arXiv:1312.1666}, 2\penalty0 (2.1):\penalty0 3,
  2013.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, Liu, Richt{\'a}rik,
  and Tak{\'a}{\v{c}}]{konevcny2016mini}
Kone{\v{c}}n{\`y}, J., Liu, J., Richt{\'a}rik, P., and Tak{\'a}{\v{c}}, M.
\newblock Mini-batch semi-stochastic gradient descent in the proximal setting.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  10\penalty0 (2):\penalty0 242--255, 2016.

\bibitem[Leblond et~al.(2016)Leblond, Pedregosa, and
  Lacoste-Julien]{leblond2016asaga}
Leblond, R., Pedregosa, F., and Lacoste-Julien, S.
\newblock Asaga: asynchronous parallel saga.
\newblock \emph{arXiv preprint arXiv:1606.04809}, 2016.

\bibitem[Li et~al.(2014)Li, Zhang, Chen, and Smola]{li2014efficient}
Li, M., Zhang, T., Chen, Y., and Smola, A.~J.
\newblock Efficient mini-batch training for stochastic optimization.
\newblock In \emph{Proceedings of the 20th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pp.\  661--670. ACM, 2014.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Lin, H., Mairal, J., and Harchaoui, Z.
\newblock A universal catalyst for first-order optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3384--3392, 2015.

\bibitem[Nesterov(2012)]{nesterov2012efficiency}
Nesterov, Y.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0
  341--362, 2012.

\bibitem[Qu et~al.(2015)Qu, Richt{\'a}rik, and Zhang]{qu2015quartz}
Qu, Z., Richt{\'a}rik, P., and Zhang, T.
\newblock Quartz: Randomized dual coordinate ascent with arbitrary sampling.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  865--873, 2015.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Recht, B., Re, C., Wright, S., and Niu, F.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  693--701, 2011.

\bibitem[Reddi et~al.(2015)Reddi, Hefny, Sra, Poczos, and
  Smola]{reddi2015variance}
Reddi, S.~J., Hefny, A., Sra, S., Poczos, B., and Smola, A.~J.
\newblock On variance reduction in stochastic gradient descent and its
  asynchronous variants.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2647--2655, 2015.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2016)Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{richtarik2016parallel}
Richt{\'a}rik, P. and Tak{\'a}{\v{c}}, M.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock \emph{Mathematical Programming}, 156\penalty0 (1-2):\penalty0
  433--484, 2016.

\bibitem[Schmidt et~al.()Schmidt, Le~Roux, and Bach]{schmidt2013minimizing}
Schmidt, M., Le~Roux, N., and Bach, F.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, pp.\  1--30.

\bibitem[Shalev-Shwartz \& Zhang(2013)Shalev-Shwartz and
  Zhang]{shalev2013stochastic}
Shalev-Shwartz, S. and Zhang, T.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Feb):\penalty0 567--599, 2013.

\bibitem[Tak{\'a}c et~al.(2013)Tak{\'a}c, Bijral, Richt{\'a}rik, and
  Srebro]{takac2013mini}
Tak{\'a}c, M., Bijral, A.~S., Richt{\'a}rik, P., and Srebro, N.
\newblock Mini-batch primal and dual methods for svms.
\newblock In \emph{ICML (3)}, pp.\  1022--1030, 2013.

\bibitem[Xiao \& Zhang(2014)Xiao and Zhang]{xiao2014proximal}
Xiao, L. and Zhang, T.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\end{thebibliography}
