\begin{thebibliography}{77}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ai \& Chen(2003)Ai and Chen]{ai2003efficient}
Ai, C. and Chen, X.
\newblock Efficient estimation of models with conditional moment restrictions
  containing unknown functions.
\newblock \emph{Econometrica}, 71\penalty0 (6):\penalty0 1795--1843, 2003.

\bibitem[Atkinson(1967)]{atkinson1967numerical}
Atkinson, K.~E.
\newblock The numerical solution of {F}redholm integral equations of the second
  kind.
\newblock \emph{SIAM Journal on Numerical Analysis}, 4\penalty0 (3):\penalty0
  337--348, 1967.

\bibitem[Bellour et~al.(2016)Bellour, Sbibih, and Zidna]{bellour2016two}
Bellour, A., Sbibih, D., and Zidna, A.
\newblock Two cubic spline methods for solving {F}redholm integral equations.
\newblock \emph{Applied Mathematics and Computation}, 276:\penalty0 1--11,
  2016.

\bibitem[Bickel et~al.(1998)Bickel, Klaassen, Ritov, and
  Wellner]{bickel1998efficient}
Bickel, P.~J., Klaassen, C. A.~J., Ritov, Y., and Wellner, J.~A.
\newblock \emph{Efficient and Adaptive Estimation for Semiparametric Models}.
\newblock Johns Hopkins Series in the Mathematical Sciences. Springer New York,
  1998.
\newblock ISBN 9780387984735.

\bibitem[Carone et~al.(2019)Carone, Luedtke, and {van der
  Laan}]{carone2019toward}
Carone, M., Luedtke, A.~R., and {van der Laan}, M.~J.
\newblock Toward computerized efficient estimation in infinite-dimensional
  models.
\newblock \emph{Journal of the American Statistical Association}, 114\penalty0
  (527):\penalty0 1174--1190, 2019.

\bibitem[Carrillo et~al.(2021)Carrillo, Jin, Li, and
  Zhu]{carrillo2021consensus}
Carrillo, J.~A., Jin, S., Li, L., and Zhu, Y.
\newblock A consensus-based global optimization method for high dimensional
  machine learning problems.
\newblock \emph{ESAIM: Control, Optimisation and Calculus of Variations},
  27:\penalty0 S5, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Chen, and
  Tamer]{chen2023efficient}
Chen, J., Chen, X., and Tamer, E.
\newblock Efficient estimation of average derivatives in {NPIV} models:
  Simulation comparisons of neural network estimators.
\newblock \emph{Journal of Econometrics}, 235\penalty0 (2):\penalty0
  1848--1875, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Li, Li, and
  Zhang]{chen2023learninggan}
Chen, S., Li, J., Li, Y., and Zhang, A.~R.
\newblock Learning polynomial transformations via generalized tensor
  decompositions.
\newblock In \emph{Proceedings of the 55th Annual ACM Symposium on Theory of
  Computing}, pp.\  1671--1684, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021closing}
Chen, T., Sun, Y., and Yin, W.
\newblock Closing the gap: Tighter analysis of alternating stochastic gradient
  methods for bilevel problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 25294--25307, 2021.

\bibitem[Chen \& Santos(2018)Chen and Santos]{chen2018overidentification}
Chen, X. and Santos, A.
\newblock Overidentification in regular models.
\newblock \emph{Econometrica}, 86\penalty0 (5):\penalty0 1771--1817, 2018.

\bibitem[Chen \& White(1999)Chen and White]{chen1999improved}
Chen, X. and White, H.
\newblock Improved rates and asymptotic normality for nonparametric neural
  network estimators.
\newblock \emph{IEEE Transactions on Information Theory}, 45\penalty0
  (2):\penalty0 682--691, 1999.

\bibitem[Chen et~al.(2024)Chen, Liu, Ma, and Zhang]{chen2024causal}
Chen, X., Liu, Y., Ma, S., and Zhang, Z.
\newblock Causal inference of general treatment effects using neural networks
  with a diverging number of confounders.
\newblock \emph{Journal of Econometrics}, 238\penalty0 (1):\penalty0 105555,
  2024.

\bibitem[Chernozhukov et~al.(2018)Chernozhukov, Chetverikov, Demirer, Duflo,
  Hansen, Newey, and Robins]{chernozhukov2018double}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey,
  W., and Robins, J.
\newblock Double/debiased machine learning for treatment and structural
  parameters.
\newblock \emph{The Econometrics Journal}, 21\penalty0 (1):\penalty0 C1--C68,
  2018.

\bibitem[Chernozhukov et~al.(2022)Chernozhukov, Newey, Quintas-Mart{\i}nez, and
  Syrgkanis]{chernozhukov2022riesznet}
Chernozhukov, V., Newey, W., Quintas-Mart{\i}nez, V.~M., and Syrgkanis, V.
\newblock {R}iesz{N}et and {F}orest{R}iesz: Automatic debiased machine learning
  with neural nets and random forests.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3901--3914. PMLR, 2022.

\bibitem[Cichocki et~al.(2016)Cichocki, Lee, Oseledets, Phan, Zhao, and
  Mandic]{cichocki2016tensor}
Cichocki, A., Lee, N., Oseledets, I., Phan, A.-H., Zhao, Q., and Mandic, D.~P.
\newblock Tensor networks for dimensionality reduction and large-scale
  optimization: Part 1 low-rank tensor decompositions.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  9\penalty0 (4-5):\penalty0 249--429, 2016.

\bibitem[Cichocki et~al.(2017)Cichocki, Phan, Zhao, Lee, Oseledets, Sugiyama,
  and Mandic]{cichocki2017tensor}
Cichocki, A., Phan, A.-H., Zhao, Q., Lee, N., Oseledets, I., Sugiyama, M., and
  Mandic, D.~P.
\newblock Tensor networks for dimensionality reduction and large-scale
  optimization: Part 2 applications and future perspectives.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  9\penalty0 (6):\penalty0 431--673, 2017.

\bibitem[Corona et~al.(2017)Corona, Rahimian, and Zorin]{corona2017tensor}
Corona, E., Rahimian, A., and Zorin, D.
\newblock A tensor-train accelerated solver for integral equations in complex
  geometries.
\newblock \emph{Journal of Computational Physics}, 334:\penalty0 145--169,
  2017.

\bibitem[Crucinio et~al.(2022)Crucinio, De~Bortoli, Doucet, and
  Johansen]{crucinio2022solving}
Crucinio, F.~R., De~Bortoli, V., Doucet, A., and Johansen, A.~M.
\newblock Solving {F}redholm integral equations of the first kind via
  {W}asserstein gradient flows.
\newblock \emph{arXiv preprint arXiv:2209.09936}, 2022.

\bibitem[Cui et~al.(2023)Cui, Pu, Shi, Miao, and
  Tchetgen~Tchetgen]{cui2023semiparametric}
Cui, Y., Pu, H., Shi, X., Miao, W., and Tchetgen~Tchetgen, E.
\newblock Semiparametric proximal causal inference.
\newblock \emph{Journal of the American Statistical Association}, 2023.

\bibitem[E \& Yu(2018)E and Yu]{e2018deep}
E, W. and Yu, B.
\newblock The deep {R}itz method: a deep learning-based numerical algorithm for
  solving variational problems.
\newblock \emph{Communications in Mathematics and Statistics}, 6\penalty0
  (1):\penalty0 1--12, 2018.

\bibitem[Farrell et~al.(2021)Farrell, Liang, and Misra]{farrell2021deep}
Farrell, M.~H., Liang, T., and Misra, S.
\newblock Deep neural networks for estimation and inference.
\newblock \emph{Econometrica}, 89\penalty0 (1):\penalty0 181--213, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1126--1135. PMLR, 2017.

\bibitem[Frangakis et~al.(2015)Frangakis, Qian, Wu, and
  Diaz]{frangakis2015deductive}
Frangakis, C.~E., Qian, T., Wu, Z., and Diaz, I.
\newblock Deductive derivation and {Turing}-computerization of semiparametric
  efficient estimation.
\newblock \emph{Biometrics}, 71\penalty0 (4):\penalty0 867--874, 2015.

\bibitem[Garg et~al.(2020)Garg, Kayal, and Saha]{garg2020learning}
Garg, A., Kayal, N., and Saha, C.
\newblock Learning sums of powers of low-degree polynomials in the
  non-degenerate case.
\newblock In \emph{2020 IEEE 61st Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  889--899. IEEE, 2020.

\bibitem[Gin{\'e} \& Nickl(2016)Gin{\'e} and Nickl]{gine2016mathematical}
Gin{\'e}, E. and Nickl, R.
\newblock \emph{Mathematical foundations of infinite-dimensional statistical
  models}, volume~40.
\newblock Cambridge University Press, 2016.

\bibitem[Goel et~al.(2020)Goel, Gollakota, Jin, Karmalkar, and
  Klivans]{goel2020superpolynomial}
Goel, S., Gollakota, A., Jin, Z., Karmalkar, S., and Klivans, A.
\newblock Superpolynomial lower bounds for learning one-layer neural networks
  using gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3587--3596. PMLR, 2020.

\bibitem[Gong et~al.(2016)Gong, Zhang, Liu, Tao, Glymour, and
  Sch{\"o}lkopf]{gong2016domain}
Gong, M., Zhang, K., Liu, T., Tao, D., Glymour, C., and Sch{\"o}lkopf, B.
\newblock Domain adaptation with conditional transferable components.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2839--2848. PMLR, 2016.

\bibitem[Guan et~al.(2022)Guan, Fang, Zhang, and Jin]{guan2022solving}
Guan, Y., Fang, T., Zhang, D., and Jin, C.
\newblock Solving {F}redholm integral equations using deep learning.
\newblock \emph{International Journal of Applied and Computational
  Mathematics}, 8\penalty0 (2):\penalty0 87, 2022.

\bibitem[Hines et~al.(2022)Hines, Dukes, Diaz-Ordaz, and
  Vansteelandt]{hines2022demystifying}
Hines, O., Dukes, O., Diaz-Ordaz, K., and Vansteelandt, S.
\newblock Demystifying statistical learning based on efficient influence
  functions.
\newblock \emph{The American Statistician}, 76\penalty0 (3):\penalty0 292--304,
  2022.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D.~P. and Ba, J.~L.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kompa et~al.(2022)Kompa, Bellamy, Kolokotrones, Robins, and
  Beam]{kompa2022deep}
Kompa, B., Bellamy, D., Kolokotrones, T., Robins, J.~M., and Beam, A.
\newblock Deep learning methods for proximal inference via maximum moment
  restriction.
\newblock In \emph{Proceedings of the 36th International Conference on Neural
  Information Processing Systems}, pp.\  11189--11201, 2022.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Image{N}et classification with deep convolutional neural networks.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems}, pp.\  1097--1105, 2012.

\bibitem[Lee et~al.(2023)Lee, Bhattacharya, Nabi, and Shpitser]{lee2023ananke}
Lee, J.~J., Bhattacharya, R., Nabi, R., and Shpitser, I.
\newblock \texttt{Ananke}: A python package for causal inference using
  graphical models.
\newblock \emph{arXiv preprint arXiv:2301.11477}, 2023.

\bibitem[Li et~al.(2020)Li, Xu, and Zhang]{li2020multi}
Li, X.-A., Xu, Z.-Q.~J., and Zhang, L.
\newblock A multi-scale {DNN} algorithm for nonlinear elliptic equations with
  multiple scales.
\newblock \emph{Communications in Computational Physics}, 28\penalty0
  (5):\penalty0 1886--1906, 2020.

\bibitem[Li et~al.(2023)Li, Cai, Chen, Sun, Hao, and Zhang]{li2023subspace}
Li, Z., Cai, R., Chen, G., Sun, B., Hao, Z., and Zhang, K.
\newblock Subspace identification for multi-source domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~36, 2023.

\bibitem[Liang(2021)]{liang2021well}
Liang, T.
\newblock How well generative adversarial networks learn distributions.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (228):\penalty0 1--41, 2021.

\bibitem[Liu(2001)]{liu2001monte}
Liu, J.~S.
\newblock \emph{Monte {C}arlo strategies in scientific computing}, volume~75.
\newblock Springer, 2001.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Shahn, Robins, and
  Rotnitzky]{liu2021efficient}
Liu, L., Shahn, Z., Robins, J.~M., and Rotnitzky, A.
\newblock Efficient estimation of optimal regimes under a no direct effect
  assumption.
\newblock \emph{Journal of the American Statistical Association}, 116\penalty0
  (533):\penalty0 224--239, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Xu, Jiang, and
  Wong]{liu2021density}
Liu, Q., Xu, J., Jiang, R., and Wong, W.~H.
\newblock Density estimation using deep generative neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0
  (15):\penalty0 e2101344118, 2021{\natexlab{b}}.

\bibitem[Lu et~al.(2022)Lu, Chen, Lu, Ying, and Blanchet]{lu2022machine}
Lu, Y., Chen, H., Lu, J., Ying, L., and Blanchet, J.
\newblock Machine learning for elliptic {PDE}s: Fast rate generalization bound,
  neural scaling law and minimax optimality.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Luedtke(2024)]{luedtke2024simplifying}
Luedtke, A.
\newblock Simplifying debiased inference via automatic differentiation and
  probabilistic programming.
\newblock \emph{arXiv preprint arXiv:2405.08675}, 2024.

\bibitem[Miao et~al.(2024)Miao, Liu, Li, Tchetgen~Tchetgen, and
  Geng]{miao2024identification}
Miao, W., Liu, L., Li, Y., Tchetgen~Tchetgen, E.~J., and Geng, Z.
\newblock Identification and semiparametric efficiency theory of nonignorable
  missing data with a shadow variable.
\newblock \emph{ACM/JMS Journal of Data Science}, 1\penalty0 (2):\penalty0
  1--23, 2024.

\bibitem[Min et~al.(2023)Min, Shi, Lewis, Chen, Yih, Hajishirzi, and
  Zettlemoyer]{min2023nonparametric}
Min, S., Shi, W., Lewis, M., Chen, X., Yih, W.-t., Hajishirzi, H., and
  Zettlemoyer, L.
\newblock Nonparametric masked language modeling.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}, pp.\  2097--2118, 2023.

\bibitem[Newey(1990)]{newey1990semiparametric}
Newey, W.~K.
\newblock Semiparametric efficiency bounds.
\newblock \emph{Journal of Applied Econometrics}, 5\penalty0 (2):\penalty0
  99--135, 1990.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Neyshabur, B.
\newblock \emph{Implicit regularization in deep learning}.
\newblock PhD thesis, Toyota Technological Institute at Chicago (TTIC), 2017.

\bibitem[Padilla et~al.(2022)Padilla, Tansey, and Chen]{padilla2022quantile}
Padilla, O. H.~M., Tansey, W., and Chen, Y.
\newblock Quantile regression with {R}e{LU} networks: Estimators and minimax
  rates.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (247):\penalty0 1--42, 2022.

\bibitem[Pan et~al.(2022)Pan, Yao, Zhang, Yu, Yu, and Chen]{pan2022knowledge}
Pan, X., Yao, W., Zhang, H., Yu, D., Yu, D., and Chen, J.
\newblock Knowledge-in-context: Towards knowledgeable semi-parametric language
  models.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Polu \& Sutskever(2020)Polu and Sutskever]{polu2020generative}
Polu, S. and Sutskever, I.
\newblock Generative language modeling for automated theorem proving.
\newblock \emph{arXiv preprint arXiv:2009.03393}, 2020.

\bibitem[Qiu et~al.(2023)Qiu, Tchetgen, and Dobriban]{qiu2023efficient}
Qiu, H., Tchetgen, E.~T., and Dobriban, E.
\newblock Efficient and multiply robust risk estimation under general forms of
  dataset shift.
\newblock \emph{arXiv preprint arXiv:2306.16406}, 2023.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and
  Karniadakis]{raissi2019physics}
Raissi, M., Perdikaris, P., and Karniadakis, G.~E.
\newblock Physics-informed neural networks: A deep learning framework for
  solving forward and inverse problems involving nonlinear partial differential
  equations.
\newblock \emph{Journal of Computational Physics}, 378:\penalty0 686--707,
  2019.

\bibitem[Ren et~al.(1999)Ren, Zhang, and Qiao]{ren1999simple}
Ren, Y., Zhang, B., and Qiao, H.
\newblock A simple {T}aylor-series expansion method for a class of second kind
  integral equations.
\newblock \emph{Journal of Computational and Applied Mathematics}, 110\penalty0
  (1):\penalty0 15--24, 1999.

\bibitem[Robins et~al.(1994)Robins, Rotnitzky, and Zhao]{robins1994estimation}
Robins, J.~M., Rotnitzky, A., and Zhao, L.~P.
\newblock Estimation of regression coefficients when some regressors are not
  always observed.
\newblock \emph{Journal of the American Statistical Association}, 89\penalty0
  (427):\penalty0 846--866, 1994.

\bibitem[Robins et~al.(2000)Robins, Rotnitzky, and
  Scharfstein]{robins2000sensitivity}
Robins, J.~M., Rotnitzky, A., and Scharfstein, D.~O.
\newblock Sensitivity analysis for selection bias and unmeasured confounding in
  missing data and causal inference models.
\newblock In \emph{Statistical Models in Epidemiology, the Environment, and
  Clinical Trials}, pp.\  1--94. Springer, 2000.

\bibitem[Schmidt-Hieber(2020)]{schmidt2020nonparametric}
Schmidt-Hieber, J.
\newblock Nonparametric regression using deep neural networks with {R}e{LU}
  activation function.
\newblock \emph{Annals of Statistics}, 48\penalty0 (4):\penalty0 1875--1897,
  2020.

\bibitem[Scott(2019)]{scott2019generalized}
Scott, C.
\newblock A generalized {N}eyman-{P}earson criterion for optimal domain
  adaptation.
\newblock In \emph{Algorithmic Learning Theory}, pp.\  738--761. PMLR, 2019.

\bibitem[Shi et~al.(2021)Shi, Xu, Bergsma, and Li]{shi2021double}
Shi, C., Xu, T., Bergsma, W., and Li, L.
\newblock Double generative adversarial networks for conditional independence
  testing.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (285):\penalty0 1--32, 2021.

\bibitem[Siegel \& Xu(2020)Siegel and Xu]{siegel2020approximation}
Siegel, J.~W. and Xu, J.
\newblock Approximation rates for neural networks with general activation
  functions.
\newblock \emph{Neural Networks}, 128:\penalty0 313--321, 2020.

\bibitem[Suzuki(2019)]{suzuki2019adaptivity}
Suzuki, T.
\newblock Adaptivity of deep {R}e{LU} network for learning in {B}esov and mixed
  smooth {B}esov spaces: optimal rate and curse of dimensionality.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Tian et~al.(2023)Tian, Zhang, and Zhao]{tian2023elsa}
Tian, Q., Zhang, X., and Zhao, J.
\newblock {ELSA}: Efficient label shift adaptation through the lens of
  semiparametric models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  34120--34142. PMLR, 2023.

\bibitem[Trinh et~al.(2024)Trinh, Wu, Le, He, and Luong]{trinh2024solving}
Trinh, T.~H., Wu, Y., Le, Q.~V., He, H., and Luong, T.
\newblock Solving olympiad geometry without human demonstrations.
\newblock \emph{Nature}, 625\penalty0 (7995):\penalty0 476--482, 2024.

\bibitem[Tsiatis(2007)]{tsiatis2007semiparametric}
Tsiatis, A.
\newblock \emph{Semiparametric theory and missing data}.
\newblock Springer Science \& Business Media, 2007.

\bibitem[{van der Laan} \& Robins(2003){van der Laan} and
  Robins]{van2003unified}
{van der Laan}, M.~J. and Robins, J.~M.
\newblock \emph{Unified methods for censored longitudinal data and causality}.
\newblock Springer Science \& Business Media, 2003.

\bibitem[{van der Vaart}(2002)]{van2002part}
{van der Vaart}, A.
\newblock Part {III}: Semiparameric statistics.
\newblock \emph{Lectures on Probability Theory and Statistics}, pp.\  331--457,
  2002.

\bibitem[van~der Vaart \& Wellner(2023)van~der Vaart and Wellner]{van2023weak}
van~der Vaart, A.~W. and Wellner, J.
\newblock \emph{Weak Convergence and Empirical Processes: with Applications to
  Statistics}.
\newblock Springer Science \& Business Media, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  6000--6010, 2017.

\bibitem[Vempala \& Wilmes(2019)Vempala and Wilmes]{vempala2019gradient}
Vempala, S. and Wilmes, J.
\newblock Gradient descent for one-hidden-layer neural networks: Polynomial
  convergence and {SQ} lower bounds.
\newblock In \emph{Conference on Learning Theory}, pp.\  3115--3117. PMLR,
  2019.

\bibitem[Wang et~al.(2023)Wang, Fu, Du, Gao, Huang, Liu, Chandak, Liu,
  Van~Katwyk, Deac, et~al.]{wang2023scientific}
Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S.,
  Van~Katwyk, P., Deac, A., et~al.
\newblock Scientific discovery in the age of artificial intelligence.
\newblock \emph{Nature}, 620\penalty0 (7972):\penalty0 47--60, 2023.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, and
  Recht]{wilson2017marginal}
Wilson, A.~C., Roelofs, R., Stern, M., Srebro, N., and Recht, B.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  4151--4161, 2017.

\bibitem[Xu et~al.(2022)Xu, Liu, and Liu]{xu2022deepmed}
Xu, S., Liu, L., and Liu, Z.
\newblock Deep{M}ed: Semiparametric causal mediation analysis with debiased
  deep learning.
\newblock In \emph{Proceedings of the 36th International Conference on Neural
  Information Processing Systems}, pp.\  28238--28251, 2022.

\bibitem[Yang(2022)]{yang2022semiparametric}
Yang, S.
\newblock Semiparametric estimation of structural nested mean models with
  irregularly spaced longitudinal observations.
\newblock \emph{Biometrics}, 78\penalty0 (3):\penalty0 937--949, 2022.

\bibitem[Zahner \& Daskalakis(1997)Zahner and Daskalakis]{zahner1997factors}
Zahner, G.~E. and Daskalakis, C.
\newblock Factors associated with mental health, general health, and
  school-based service use for child psychopathology.
\newblock \emph{American Journal of Public Health}, 87\penalty0 (9):\penalty0
  1440--1448, 1997.

\bibitem[Zappala et~al.(2023)Zappala, Fonseca, Moberly, Higley, Abdallah,
  Cardin, and van Dijk]{zappala2023neural}
Zappala, E., Fonseca, A. H. d.~O., Moberly, A.~H., Higley, M.~J., Abdallah, C.,
  Cardin, J.~A., and van Dijk, D.
\newblock Neural integro-differential equations.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume 37 (9), pp.\  11104--11112, 2023.

\bibitem[Zhang \& Tchetgen~Tchetgen(2022)Zhang and
  Tchetgen~Tchetgen]{zhang2022semi}
Zhang, B. and Tchetgen~Tchetgen, E.~J.
\newblock A semi-parametric approach to model-based sensitivity analysis in
  observational studies.
\newblock \emph{Journal of the Royal Statistical Society Series A: Statistics
  in Society}, 185\penalty0 (Supplement\_2):\penalty0 S668--S691, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Gong, Stojanov, Huang, Liu, and
  Glymour]{zhang2020domain}
Zhang, K., Gong, M., Stojanov, P., Huang, B., Liu, Q., and Glymour, C.
\newblock Domain adaptation as a problem of inference on graphical models.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, pp.\  4965--4976, 2020.

\bibitem[Zhao \& Ma(2022)Zhao and Ma]{zhao2022versatile}
Zhao, J. and Ma, Y.
\newblock A versatile estimation procedure without estimating the nonignorable
  missingness mechanism.
\newblock \emph{Journal of the American Statistical Association}, 117\penalty0
  (540):\penalty0 1916--1930, 2022.

\bibitem[Zhong et~al.(2022)Zhong, Mueller, and Wang]{zhong2022deep}
Zhong, Q., Mueller, J., and Wang, J.-L.
\newblock Deep learning for the partially linear {C}ox model.
\newblock \emph{The Annals of Statistics}, 50\penalty0 (3):\penalty0
  1348--1375, 2022.

\bibitem[Zhou et~al.(2020)Zhou, Feng, Ma, Xiong, Hoi, and E]{zhou2020towards}
Zhou, P., Feng, J., Ma, C., Xiong, C., Hoi, S., and E, W.
\newblock Towards theoretically understanding why {SGD} generalizes better than
  {ADAM} in deep learning.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, pp.\  21285--21296, 2020.

\end{thebibliography}
