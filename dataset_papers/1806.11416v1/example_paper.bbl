\begin{thebibliography}{14}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bianchini \& Scarselli(2014)Bianchini and
  Scarselli]{bianchini2014complexity}
Bianchini, Monica and Scarselli, Franco.
\newblock On the complexity of neural network classifiers: A comparison between
  shallow and deep architectures.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  25\penalty0 (8):\penalty0 1553--1565, 2014.

\bibitem[DasGupta \& Schnitger(1993)DasGupta and Schnitger]{16}
DasGupta, Bhaskar and Schnitger, Georg.
\newblock The power of approximating: A comparison of activation functions.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  615--622, 1993.

\bibitem[Delalleau \& Bengio(2011)Delalleau and Bengio]{delalleau2011shallow}
Delalleau, Olivier and Bengio, Yoshua.
\newblock Shallow vs. deep sum-product networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  666--674, 2011.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{7}
Hornik, Kurt, Stinchcombe, Maxwell, and White, Halbert.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and Hinton]{14}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1097--1105, 2012.

\bibitem[Liang \& Srikant(2017)Liang and Srikant]{1}
Liang, Shiyu and Srikant, R.
\newblock Why deep neural networks for function approximation?
\newblock In \emph{5th International Conference on Learning Representations
  (ICLR)}, pp.\  1--13, 2017.

\bibitem[Lu et~al.(2017)Lu, Pu, Wang, Hu, and Wang]{DBLP:conf/nips/LuPWH017}
Lu, Zhou, Pu, Hongming, Wang, Feicheng, Hu, Zhiqiang, and Wang, Liwei.
\newblock The expressive power of neural networks: {A} view from the width.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6232--6240, 2017.

\bibitem[Mhaskar et~al.(2016)Mhaskar, Liao, and Poggio]{8}
Mhaskar, Hrushikesh, Liao, Qianli, and Poggio, Tomaso.
\newblock Learning functions: When is deep better than shallow.
\newblock \emph{arXiv preprint, arXiv:1603.00988}, 2016.

\bibitem[Montufar et~al.(2014)Montufar, Pascanu, Cho, and Bengio]{18}
Montufar, Guido~F, Pascanu, Razvan, Cho, Kyunghyun, and Bengio, Yoshua.
\newblock On the number of linear regions of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2924--2932, 2014.

\bibitem[Pascanu et~al.(2014)Pascanu, Montufar, and
  Bengio]{Pascanu+et+al-ICLR2014b}
Pascanu, Razvan, Montufar, Guido, and Bengio, Yoshua.
\newblock On the number of inference regions of deep feed forward networks with
  piece-wise linear activations.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Raghu et~al.(2017)Raghu, Poole, Kleinberg, Ganguli, and
  Sohl-Dickstein]{6}
Raghu, Maithra, Poole, Ben, Kleinberg, Jon, Ganguli, Surya, and Sohl-Dickstein,
  Jascha.
\newblock On the expressive power of deep neural networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2847--2854, 2017.

\bibitem[Telgarsky(2015)]{2}
Telgarsky, Matus.
\newblock Representation benefits of deep feedforward networks.
\newblock \emph{arXiv preprint, arXiv:1509.08101}, 2015.

\bibitem[Telgarsky(2016)]{3}
Telgarsky, Matus.
\newblock Benefits of depth in neural networks.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 49:\penalty0
  1--23, 2016.

\bibitem[Yarotsky(2017)]{5}
Yarotsky, Dmitry.
\newblock Error bounds for approximations with deep relu networks.
\newblock \emph{Neural Networks}, 94:\penalty0 103--114, 2017.

\end{thebibliography}
