@article{mnih2013,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@incollection{thrun1998learning,
  title={Learning to learn: Introduction and overview},
  author={Thrun, Sebastian and Pratt, Lorien},
  booktitle={Learning to learn},
  pages={3--17},
  year={1998},
  publisher={Springer}
}

@book{bengio1990learning,
  title={Learning a synaptic learning rule},
  author={Bengio, Yoshua and Bengio, Samy and Cloutier, Jocelyn},
  year={1990},
  publisher={Citeseer}
}

@article{schmidhuber1996simple,
  title={Simple principles of metalearning},
  author={Schmidhuber, Juergen and Zhao, Jieyu and Wiering, MA},
  journal={Technical report IDSIA},
  volume={69},
  pages={1--23},
  year={1996},
  publisher={IDSIA}
}

@article{popoviciu1965certaines,
  title={Sur certaines in{\'e}galit{\'e}s qui caract{\'e}risent les fonctions convexes},
  author={Popoviciu, Tiberiu},
  journal={Analele Stiintifice Univ.“Al. I. Cuza”, Iasi, Sectia Mat},
  volume={11},
  pages={155--164},
  year={1965}
}

@misc{ross6277simulation,
  title={Simulation, 2002},
  author={Ross, SM},
  year={2002},
  publisher={Academic Press}
}

@article{ajalloeian2020analysis,
  title={Analysis of SGD with biased gradient estimators},
  author={Ajalloeian, Ahmad and Stich, Sebastian U},
  journal={arXiv preprint arXiv:2008.00051},
  year={2020}
}

@article{nichol2018first,
  title={On first-order meta-learning algorithms},
  author={Nichol, Alex and Achiam, Joshua and Schulman, John},
  journal={arXiv preprint arXiv:1803.02999},
  year={2018}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016},
  organization={PMLR}
}

@inproceedings{huang2020importance,
  title={From importance sampling to doubly robust policy gradient},
  author={Huang, Jiawei and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={4434--4443},
  year={2020},
  organization={PMLR}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}
@inproceedings{munos2016safe,
  title={Safe and efficient off-policy reinforcement learning},
  author={Munos, R{\'e}mi and Stepleton, Tom and Harutyunyan, Anna and Bellemare, Marc},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1054--1062},
  year={2016}
}

@article{kapturowski2018recurrent,
  title={Recurrent experience replay in distributed reinforcement learning},
  author={Kapturowski, Steven and Ostrovski, Georg and Quan, John and Munos, Remi and Dabney, Will},
  year={2018}
}

@article{barth2018distributed,
  title={Distributed distributional deterministic policy gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Tb, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}

@article{oh2018self,
  title={Self-imitation learning},
  author={Oh, Junhyuk and Guo, Yijie and Singh, Satinder and Lee, Honglak},
  journal={arXiv preprint arXiv:1806.05635},
  year={2018}
}

@article{rowland2019adaptive,
  title={Adaptive Trade-Offs in Off-Policy Learning},
  author={Rowland, Mark and Dabney, Will and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:1910.07478},
  year={2019}
}

@article{silver2016,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Research}
}

@article{salimans2017evolution,
  title={Evolution strategies as a scalable alternative to reinforcement learning},
  author={Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.03864},
  year={2017}
}

@inproceedings{khadka2018evolution,
  title={Evolution-guided policy gradient in reinforcement learning},
  author={Khadka, Shauharda and Tumer, Kagan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1188--1200},
  year={2018}
}

@inproceedings{nachum2017bridging,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2775--2785},
  year={2017}
}

@article{tassa2018deepmind,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}

@misc{klimov2017roboschool,
  title={Roboschool},
  author={Klimov, Oleg and Schulman, John},
  year={2017}
}

@inproceedings{hasselt2010double,
  title={Double Q-learning},
  author={Hasselt, Hado V},
  booktitle={Advances in neural information processing systems},
  pages={2613--2621},
  year={2010}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}

@article{coumans2010bullet,
  title={Bullet physics engine},
  author={Coumans, Erwin},
  journal={Open Source Software: http://bulletphysics. org},
  volume={1},
  number={3},
  pages={84},
  year={2010}
}

@article{bellman1957markovian,
  title={A Markovian decision process},
  author={Bellman, Richard},
  journal={Journal of mathematics and mechanics},
  pages={679--684},
  year={1957},
  publisher={JSTOR}
}

@article{pourchot2018cem,
  title={CEM-RL: Combining evolutionary and gradient-based methods for policy search},
  author={Pourchot, Alo{\"\i}s and Sigaud, Olivier},
  journal={arXiv preprint arXiv:1810.01222},
  year={2018}
}
@inproceedings{xu2018meta,
  title={Meta-gradient reinforcement learning},
  author={Xu, Zhongwen and van Hasselt, Hado P and Silver, David},
  booktitle={Advances in neural information processing systems},
  pages={2396--2407},
  year={2018}
}

@article{paul2019fast,
  title={Fast efficient hyperparameter tuning for policy gradients},
  author={Paul, Supratik and Kurin, Vitaly and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1902.06583},
  year={2019}
}

@article{fujimoto2018addressing,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@article{van2018deep,
  title={Deep reinforcement learning and the deadly triad},
  author={Van Hasselt, Hado and Doron, Yotam and Strub, Florian and Hessel, Matteo and Sonnerat, Nicolas and Modayil, Joseph},
  journal={arXiv preprint arXiv:1812.02648},
  year={2018}
}

@article{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{lillicrap2015continuous,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}


@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

@article{achiam2018openai,
  title={Openai Spinning Up},
  author={Achiam, Joshua},
  journal={GitHub, GitHub repository},
  year={2018}
}


@article{horgan2018distributed,
  title={Distributed prioritized experience replay},
  author={Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}



@article{reddy2019sqil,
  title={SQIL: imitation learning via regularized behavioral cloning},
  author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey},
  journal={arXiv preprint arXiv:1905.11108},
  year={2019}
}

@inproceedings{precup2001off,
  title={Off-policy temporal-difference learning with function approximation},
  author={Precup, Doina and Sutton, Richard S and Dasgupta, Sanjoy},
  booktitle={ICML},
  pages={417--424},
  year={2001}
}

@article{glynn1989importance,
  title={Importance sampling for stochastic simulations},
  author={Glynn, Peter W and Iglehart, Donald L},
  journal={Management science},
  volume={35},
  number={11},
  pages={1367--1392},
  year={1989},
  publisher={INFORMS}
}

@article{tang2020taylor,
  title={Taylor expansion policy optimization},
  author={Tang, Yunhao and Valko, Michal and Munos, R{\'e}mi},
  journal={arXiv preprint arXiv:2003.06259},
  year={2020}
}

@article{guo2019efficient,
  title={Efficient Exploration with Self-Imitation Learning via Trajectory-Conditioned Policy},
  author={Guo, Yijie and Choi, Jongwook and Moczulski, Marcin and Bengio, Samy and Norouzi, Mohammad and Lee, Honglak},
  journal={arXiv preprint arXiv:1907.10247},
  year={2019}
}

@article{he2016learning,
  title={Learning to play in a day: Faster deep reinforcement learning by optimality tightening},
  author={He, Frank S and Liu, Yang and Schwing, Alexander G and Peng, Jian},
  journal={arXiv preprint arXiv:1611.01606},
  year={2016}
}

@inproceedings{asadi2017alternative,
  title={An alternative softmax operator for reinforcement learning},
  author={Asadi, Kavosh and Littman, Michael L},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={243--252},
  year={2017},
  organization={JMLR. org}
}

@article{fox2015taming,
  title={Taming the noise in reinforcement learning via soft updates},
  author={Fox, Roy and Pakman, Ari and Tishby, Naftali},
  journal={arXiv preprint arXiv:1512.08562},
  year={2015}
}

@article{ziebart2010modeling,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010}
}

@incollection{baird1995residual,
  title={Residual algorithms: Reinforcement learning with function approximation},
  author={Baird, Leemon},
  booktitle={Machine Learning Proceedings 1995},
  pages={30--37},
  year={1995},
  publisher={Elsevier}
}

@article{gangwani2018learning,
  title={Learning self-imitating diverse policies},
  author={Gangwani, Tanmay and Liu, Qiang and Peng, Jian},
  journal={arXiv preprint arXiv:1805.10309},
  year={2018}
}

@article{peng2019advantage,
  title={Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}

@inproceedings{zinkevich2008regret,
  title={Regret minimization in games with incomplete information},
  author={Zinkevich, Martin and Johanson, Michael and Bowling, Michael and Piccione, Carmelo},
  booktitle={Advances in neural information processing systems},
  pages={1729--1736},
  year={2008}
}

@inproceedings{heinrich2015fictitious,
  title={Fictitious self-play in extensive-form games},
  author={Heinrich, Johannes and Lanctot, Marc and Silver, David},
  booktitle={International Conference on Machine Learning},
  pages={805--813},
  year={2015}
}

@inproceedings{pinto2017robust,
  title={Robust adversarial reinforcement learning},
  author={Pinto, Lerrel and Davidson, James and Sukthankar, Rahul and Gupta, Abhinav},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2817--2826},
  year={2017},
  organization={JMLR. org}
}
 
 @article{yu2017preparing,
  title={Preparing for the unknown: Learning a universal policy with online system identification},
  author={Yu, Wenhao and Tan, Jie and Liu, C Karen and Turk, Greg},
  journal={arXiv preprint arXiv:1702.02453},
  year={2017}
}

 @article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
%rl for pomdp
@article{stone2017,
  title={Deep recurrent Q-learning for partially observable MDPs},
  author={Hausknecht, Matthew and Stone, Peter},
  journal={arXiv preprint arXiv:1507.06527v4},
  year={2017}
}

@article{jin2017,
  title={Regret minimization for partially observable deep reinforcement learning},
  author={Jin, Peter H and Levine, Sergey and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1710.11424},
  year={2017}
}

@inproceedings{jordan1995pomdp,
  title={Reinforcement learning algorithm for partially observable Markov decision problems},
  author={Jaakkola, Tommi and Singh, Satinder and Jordan, Michael},
  booktitle={Advances in neural information processing systems},
  pages={345--352},
  year={1995}
}

@inproceedings{variationalRL2018,
  title={Deep variational reinforcement learning for POMDPs},
  author={Igl, Maximilian and Zintgraf, Luisa and Le, Tuan Anh and Wood, Frank and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={2117--2126},
  year={2018}
}

@inproceedings{bartlett2000,
  title={Reinforcement learning in POMDP's via direct gradient ascent},
  author={Baxter, Jonathan and Bartlett, Peter},
  booktitle={International Conference on Machine Learning},
  pages={41--48},
  year={2000}
}
%%%%%%%%%%%%%%%%%%   
% infer 2 control
@inproceedings{Marc2011,
  title={PILCO: A model-based and data-efficient approach to policy search},
  author={Deisenroth, Marc and Rasmussen, Carl},
  booktitle={International Conference on Machine Learning},
  pages={456--472},
  year={2011}
}

@inproceedings{Marc2009,
  title={Robot trajectory optimization using approximate inference},
  author={Toussaint, Marc},
  booktitle={International Conference on Machine Learning},
  pages={1049--1056},
  year={2009}
}


%%%%%%%%%%%%%%%%%

@inproceedings{duanxi2016,
  title={Benchmarking deep reinforcement learning for continuous control},
  author={Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={1329--1338},
  year={2016}
}
    
@inproceedings{osband2016,
  title={Deep exploration via bootstrapped DQN},
  author={Osband, Ian and Blundell, Charles and Pritzel, Alexander and Van Roy, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={4026--4034},
  year={2016}
}

@article{osband2015,
  title={Bootstrapped thompson sampling and deep exploration},
  author={Osband, Ian and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1507.00300},
  year={2015}
}
    
@article{fortunato2017,
  title={Noisy networks for exploration},
  author={Fortunato, Meire and Azar, Mohammad Gheshlaghi and Piot, Bilal and Menick, Jacob and Osband, Ian and Graves, Alex and Mnih, Vlad and Munos, Remi and Hassabis, Demis and Pietquin, Olivier and others},
  journal={arXiv preprint arXiv:1706.10295},
  year={2017}
}
    
@article{timothy2016,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
    
@inproceedings{schulman2015,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International Conference on Machine Learning},
  pages={1889--1897},
  year={2015}
}
       
@article{levine2016,
  title={End-to-end training of deep visuomotor policies},
  author={Levine, Sergey and Finn, Chelsea and Darrell, Trevor and Abbeel, Pieter},
  journal={The Journal of Machine Learning Research},
  volume={17},
  number={1},
  pages={1334--1373},
  year={2016},
  publisher={JMLR. org}
}
    
@article{osband2017,
  title={Deep exploration via randomized value functions},
  author={Osband, Ian and Russo, Daniel and Wen, Zheng and Van Roy, Benjamin},
  journal={arXiv preprint arXiv:1703.07608},
  year={2017}
}
    
@inproceedings{furmston2010,
  title={Variational methods for reinforcement learning},
  author={Furmston, Thomas and Barber, David},
  booktitle={Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages={241--248},
  year={2010}
}
    
@article{schaul2016,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}
 
@inproceedings{silver2014,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  booktitle={ICML},
  year={2014}
}

@inproceedings{hasselt2016,
  title={Deep Reinforcement Learning with Double Q-Learning.},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={AAAI},
  volume={16},
  pages={2094--2100},
  year={2016}
}
    
@article{wang2016,
  title={Dueling network architectures for deep reinforcement learning},
  author={Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Van Hasselt, Hado and Lanctot, Marc and De Freitas, Nando},
  journal={arXiv preprint arXiv:1511.06581},
  year={2015}
}
    
@inproceedings{todorov2008,
  title={General duality between optimal control and estimation},
  author={Todorov, Emanuel},
  booktitle={Decision and Control, 2008. CDC 2008. 47th IEEE Conference on},
  pages={4286--4292},
  year={2008},
  organization={IEEE}
}      
    
@book{sutton1998,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  volume={1},
  number={1},
  year={1998},
  publisher={MIT press Cambridge}
}
        
@incollection{williams1992,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  booktitle={Reinforcement Learning},
  pages={5--32},
  year={1992},
  publisher={Springer}
}   
       
@article{blei2017,
  title={Variational inference: A review for statisticians},
  author={Blei, David M and Kucukelbir, Alp and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={112},
  number={518},
  pages={859--877},
  year={2017},
  publisher={Taylor \& Francis}
}
         
@inproceedings{blei2015,
  title={Black box variational inference},
  author={Ranganath, Rajesh and Gerrish, Sean and Blei, David},
  booktitle={Artificial Intelligence and Statistics},
  pages={814--822},
  year={2014}
}

@article{tran2016,
  title={Deep probabilistic programming},
  author={Tran, Dustin and Hoffman, Matthew D and Saurous, Rif A and Brevdo, Eugene and Murphy, Kevin and Blei, David M},
  journal={arXiv preprint arXiv:1701.03757},
  year={2017}
}  

@article{brockman2016,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}
     
@article{lipton2016,
  title={Efficient dialogue policy learning with bbq-networks},
  author={Lipton, Zachary C and Li, Xiujun and Gao, Jianfeng and Li, Lihong and Ahmed, Faisal and Deng, Li and Birnbaum, Tobias and Eldar, Yonina C and Needell, Deanna and Memon, Adnan and others},
  journal={arXiv preprint ArXiv:1608.05081},
  year={2016}
}

@inproceedings{hessel2018rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@inproceedings{harutyunyan2016q,
  title={Q (lambda) with Off-Policy Corrections},
  author={Harutyunyan, Anna and Bellemare, Marc G and Stepleton, Tom and Munos, R{\'e}mi},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={305--320},
  year={2016},
  organization={Springer}
}
    
@article{william1933,
  title={On the likelihood that one unknown probability exceeds another in view of the evidence of two samples},
  author={Thompson, William R},
  journal={Biometrika},
  volume={25},
  number={3/4},
  pages={285--294},
  year={1933},
  publisher={JSTOR}
}

@article{kucukelbir2016automatic,
    Author = {Alp Kucukelbir, and Dustin Tran, and Rajesh Ranganath, and Andrew Gelman, and David M. Blei},
    Journal = {Journal of Machine Learning Research, 18(14):1-45},
    Title = {Automatic differentiation variational inference},
    Year = {2017}}  
        
@inproceedings{goodfellow2015,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}    
    
@article{li2018,
  title={Gradient Estimators for Implicit Models},
  author={Li, Yingzhen and Turner, Richard E},
  journal={arXiv preprint arXiv:1705.07107},
  year={2017}
}
    
@article{dustin2017,
  title={Hierarchical Implicit Models and Likelihood-Free Variational Inference},
  author={Tran, Dustin and Ranganath, Rajesh and Blei, David M},
  journal={arXiv preprint arXiv:1702.08896},
  year={2017}
}

@article{srivastava2014,
  title={Dropout: A simple way to prevent neural networks from overfitting},
  author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1929--1958},
  year={2014},
  publisher={JMLR. org}
}

@inproceedings{gal2016,
  title={Dropout as a Bayesian approximation: Representing model uncertainty in deep learning},
  author={Gal, Yarin and Ghahramani, Zoubin},
  booktitle={international conference on machine learning},
  pages={1050--1059},
  year={2016}
}   

@article{tuomas2017,
  title={Reinforcement learning with deep energy-based policies},
  author={Haarnoja, Tuomas and Tang, Haoran and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1702.08165},
  year={2017}
}

@article{plappert2018,
  title={Parameter space noise for exploration},
  author={Plappert, Matthias and Houthooft, Rein and Dhariwal, Prafulla and Sidor, Szymon and Chen, Richard Y and Chen, Xi and Asfour, Tamim and Abbeel, Pieter and Andrychowicz, Marcin},
  journal={arXiv preprint arXiv:1706.01905},
  year={2017}
}  
    
@article{tang2017,
  title={Variational Deep Q Network},
  author={Tang, Yunhao and Kucukelbir, Alp},
  journal={arXiv preprint arXiv:1711.11225},
  year={2017}
}  

@inproceedings{marian2017,
  title={Sobolev training for neural networks},
  author={Czarnecki, Wojciech M and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4281--4290},
  year={2017}
}

@article{schulman2017chen,
  title={Equivalence between policy gradients and soft q-learning},
  author={Schulman, John and Chen, Xi and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.06440},
  year={2017}
}
    
@article{degris2012,
  title={Off-policy actor-critic},
  author={Degris, Thomas and White, Martha and Sutton, Richard S},
  journal={arXiv preprint arXiv:1205.4839},
  year={2012}
}

@article{donoghue2017,
  title={Pgq: Combining policy gradient and q-learning},
  author={O'Donoghue, Brendan and Munos, Remi and Kavukcuoglu, Koray and Mnih, Volodymyr},
  journal={arXiv preprint arXiv:1611.01626},
  year={2016}
}
      
@inproceedings{mnih2016,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year={2016}
}
    
@inproceedings{sutton1999,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}
 
@article{schulman2017,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
    
@article{rezende2015,
  title={Variational inference with normalizing flows},
  author={Rezende, Danilo Jimenez and Mohamed, Shakir},
  journal={arXiv preprint arXiv:1505.05770},
  year={2015}
} 
    
@article{dinh2017,
  title={Density estimation using Real NVP},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}
    
@article{dinh2015,
  title={NICE: Non-linear independent components estimation},
  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1410.8516},
  year={2014}
}

@inproceedings{nachum2017,
  title={Bridging the gap between value and policy based reinforcement learning},
  author={Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2772--2782},
  year={2017}
}
    
@inproceedings{asadi2017,
  title={An Alternative Softmax Operator for Reinforcement Learning},
  author={Asadi, Kavosh and Littman, Michael L},
  booktitle={International Conference on Machine Learning},
  pages={243--252},
  year={2017}
}      
 
@inproceedings{ross2011,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}
 

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@inproceedings{ho2016,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4565--4573},
  year={2016}
}
 
@article{finn2016,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@book{ziebart2010,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010},
  publisher={Carnegie Mellon University}
}

@inproceedings{abbeel2010,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
} 
 
@inproceedings{schulman2015gradient,
  title={Gradient estimation using stochastic computation graphs},
  author={Schulman, John and Heess, Nicolas and Weber, Theophane and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3528--3536},
  year={2015}
}

@inproceedings{todorov2012,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}
    
@article{tuomas2018,
  title={Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@inproceedings{rajeswaran2017,
  title={Towards generalization and simplicity in continuous control},
  author={Rajeswaran, Aravind and Lowrey, Kendall and Todorov, Emanuel V and Kakade, Sham M},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6553--6564},
  year={2017}
}

@article{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Volodymir and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  journal={arXiv preprint arXiv:1802.01561},
  year={2018}
}

@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@article{kingma2013,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{liu2016,
  title={Stein variational gradient descent: A general purpose bayesian inference algorithm},
  author={Liu, Qiang and Wang, Dilin},
  booktitle={Advances In Neural Information Processing Systems},
  pages={2378--2386},
  year={2016}
}         

@article{tuomas2018b,
  title={Latent Space Policies for Hierarchical Reinforcement Learning},
  author={Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1804.02808},
  year={2018}
}

@article{levine2018,
  title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}
      
@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}  

@article{mania2018simple,
  title={Simple random search provides a competitive approach to reinforcement learning},
  author={Mania, Horia and Guy, Aurelia and Recht, Benjamin},
  journal={arXiv preprint arXiv:1803.07055},
  year={2018}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}   

@article{haarnoja2018composable,
  title={Composable Deep Reinforcement Learning for Robotic Manipulation},
  author={Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1803.06773},
  year={2018}
}

@article{haarnoja2018latent,
  title={Latent Space Policies for Hierarchical Reinforcement Learning},
  author={Haarnoja, Tuomas and Hartikainen, Kristian and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1804.02808},
  year={2018}
}
%@article{bellemare2016},
  %  Author = {Marc G. Bellemare, and Sriram Srinivasan, and Georg Ostrovski, and Tom Schaul, and David Saxton, and Remi Munos},
%    Journal = {Advances in Neural Information Processing Systems},
   % Title = {Unifying Count-Based Exploration and Intrinsic Motivation},
    %Year = {2016}}        
 
%@article{tang2016},
   % Author = {Haoran Tang, and Rein Houthooft, and Davis Foote, and Adam Stooke, and Xi Chen, and Yan Duan, and John Schulman, and Filip De Turck, and Pieter Abbeel},
    %Journal = {arXiv: 1611.04717},
    %Title = {$#$Exploration: A Study of Count-Based Exploration for Deep Reinforcement Learning},
    %Year = {2017}}       

@article{wainwright2008graphical,
  title={Graphical models, exponential families, and variational inference},
  author={Wainwright, Martin J and Jordan, Michael I and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={1},
  number={1--2},
  pages={1--305},
  year={2008},
  publisher={Now Publishers, Inc.}
}

@inproceedings{levine2013variational,
  title={Variational policy search via trajectory optimization},
  author={Levine, Sergey and Koltun, Vladlen},
  booktitle={Advances in Neural Information Processing Systems},
  pages={207--215},
  year={2013}
}

@inproceedings{neumann2011variational,
  title={Variational inference for policy search in changing situations},
  author={Neumann, Gerhard and others},
  booktitle={Proceedings of the 28th International Conference on Machine Learning, ICML 2011},
  pages={817--824},
  year={2011}
}

@article{tang2017variational,
  title={Variational Deep Q Network},
  author={Tang, Yunhao and Kucukelbir, Alp},
  journal={arXiv preprint arXiv:1711.11225},
  year={2017}
}

@article{lipton2016efficient,
  title={Efficient exploration for dialogue policy learning with BBQ networks \& replay buffer spiking},
  author={Lipton, Zachary C and Gao, Jianfeng and Li, Lihong and Li, Xiujun and Ahmed, Faisal and Deng, Li},
  journal={arXiv preprint arXiv:1608.05081},
  year={2016}
}

@article{ji2020multi,
  title={Multi-step model-agnostic meta-learning: Convergence and improved algorithms},
  author={Ji, Kaiyi and Yang, Junjie and Liang, Yingbin},
  journal={arXiv preprint arXiv:2002.07836},
  year={2020}
}

@inproceedings{fallah2020convergence,
  title={On the convergence theory of gradient-based model-agnostic meta-learning algorithms},
  author={Fallah, Alireza and Mokhtari, Aryan and Ozdaglar, Asuman},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1082--1092},
  year={2020},
  organization={PMLR}
}

@inproceedings{houthooft2016vime,
  title={Vime: Variational information maximizing exploration},
  author={Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1109--1117},
  year={2016}
}

 
@inproceedings{todorov2008general,
  title={General duality between optimal control and estimation},
  author={Todorov, Emanuel},
  booktitle={Decision and Control, 2008. CDC 2008. 47th IEEE Conference on},
  pages={4286--4292},
  year={2008},
  organization={IEEE}
}

@article{kalman1959general,
  title={On the general theory of control systems},
  author={Kalman, Rudolf},
  journal={IRE Transactions on Automatic Control},
  volume={4},
  number={3},
  pages={110--110},
  year={1959}
}

@inproceedings{ziebart2008maximum,
  title={Maximum Entropy Inverse Reinforcement Learning.},
  author={Ziebart, Brian D and Maas, Andrew L and Bagnell, J Andrew and Dey, Anind K},
  booktitle={AAAI},
  volume={8},
  pages={1433--1438},
  year={2008},
  organization={Chicago, IL, USA}
}

@article{levine2018reinforcement,
  title={Reinforcement Learning and Control as Probabilistic Inference: Tutorial and Review},
  author={Levine, Sergey},
  journal={arXiv preprint arXiv:1805.00909},
  year={2018}
}

@article{abdolmaleki2018maximum,
  title={Maximum a Posteriori Policy Optimisation},
  author={Abdolmaleki, Abbas and Springenberg, Jost Tobias and Tassa, Yuval and Munos, Remi and Heess, Nicolas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1806.06920},
  year={2018}
}
 
@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}

@inproceedings{bertsekas1995neuro,
  title={Neuro-dynamic programming: an overview},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  booktitle={Proceedings of 1995 34th IEEE Conference on Decision and Control},
  volume={1},
  pages={560--564},
  year={1995},
  organization={IEEE}
}

@article{puterman1990markov,
  title={Markov decision processes},
  author={Puterman, Martin L},
  journal={Handbooks in operations research and management science},
  volume={2},
  pages={331--434},
  year={1990},
  publisher={Elsevier}
}


@inproceedings{lee2013bias,
  title={Bias-corrected Q-learning to control max-operator bias in Q-learning},
  author={Lee, Donghun and Defourny, Boris and Powell, Warren B},
  booktitle={2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)},
  pages={93--99},
  year={2013},
  organization={IEEE}
}

@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@article{pritzel2017neural,
  title={Neural episodic control},
  author={Pritzel, Alexander and Uria, Benigno and Srinivasan, Sriram and Puigdomenech, Adria and Vinyals, Oriol and Hassabis, Demis and Wierstra, Daan and Blundell, Charles},
  journal={arXiv preprint arXiv:1703.01988},
  year={2017}
}

@article{blundell2016model,
  title={Model-free episodic control},
  author={Blundell, Charles and Uria, Benigno and Pritzel, Alexander and Li, Yazhe and Ruderman, Avraham and Leibo, Joel Z and Rae, Jack and Wierstra, Daan and Hassabis, Demis},
  journal={arXiv preprint arXiv:1606.04460},
  year={2016}
}

@inproceedings{abbeel2004apprenticeship,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}


@article{dudik2014doubly,
  title={Doubly robust policy evaluation and optimization},
  author={Dud{\'\i}k, Miroslav and Erhan, Dumitru and Langford, John and Li, Lihong and others},
  journal={Statistical Science},
  volume={29},
  number={4},
  pages={485--511},
  year={2014},
  publisher={Institute of Mathematical Statistics}
}

@article{farajtabar2018more,
  title={More robust doubly robust off-policy evaluation},
  author={Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
  journal={arXiv preprint arXiv:1802.03493},
  year={2018}
}

@article{mahmood2017multi,
  title={Multi-step off-policy learning without importance sampling ratios},
  author={Mahmood, Ashique Rupam and Yu, Huizhen and Sutton, Richard S},
  journal={arXiv preprint arXiv:1702.03006},
  year={2017}
}

@article{baxter2001infinite,
  title={Infinite-horizon policy-gradient estimation},
  author={Baxter, Jonathan and Bartlett, Peter L},
  journal={Journal of Artificial Intelligence Research},
  volume={15},
  pages={319--350},
  year={2001}
}

@article{al2017continuous,
  title={Continuous adaptation via meta-learning in nonstationary and competitive environments},
  author={Al-Shedivat, Maruan and Bansal, Trapit and Burda, Yuri and Sutskever, Ilya and Mordatch, Igor and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1710.03641},
  year={2017}
}

@article{wu2018variance,
  title={Variance reduction for policy gradient with action-dependent factorized baselines},
  author={Wu, Cathy and Rajeswaran, Aravind and Duan, Yan and Kumar, Vikash and Bayen, Alexandre M and Kakade, Sham and Mordatch, Igor and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1803.07246},
  year={2018}
}

@article{liu2017action,
  title={Action-depedent Control Variates for Policy Optimization via Stein's Identity},
  author={Liu, Hao and Feng, Yihao and Mao, Yi and Zhou, Dengyong and Peng, Jian and Liu, Qiang},
  journal={arXiv preprint arXiv:1710.11198},
  year={2017}
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th $\{$USENIX$\}$ symposium on operating systems design and implementation ($\{$OSDI$\}$ 16)},
  pages={265--283},
  year={2016}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{fakoor2019meta,
  title={Meta-q-learning},
  author={Fakoor, Rasool and Chaudhari, Pratik and Soatto, Stefano and Smola, Alexander J},
  journal={arXiv preprint arXiv:1910.00125},
  year={2019}
}

@article{houthooft2018evolved,
  title={Evolved policy gradients},
  author={Houthooft, Rein and Chen, Richard Y and Isola, Phillip and Stadie, Bradly C and Wolski, Filip and Ho, Jonathan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1802.04821},
  year={2018}
}

@article{oh2020discovering,
  title={Discovering reinforcement learning algorithms},
  author={Oh, Junhyuk and Hessel, Matteo and Czarnecki, Wojciech M and Xu, Zhongwen and van Hasselt, Hado and Singh, Satinder and Silver, David},
  journal={arXiv preprint arXiv:2007.08794},
  year={2020}
}

@article{xu2020meta,
  title={Meta-gradient reinforcement learning with an objective discovered online},
  author={Xu, Zhongwen and van Hasselt, Hado and Hessel, Matteo and Oh, Junhyuk and Singh, Satinder and Silver, David},
  journal={arXiv preprint arXiv:2007.08433},
  year={2020}
}

@article{zahavy2020self,
  title={A Self-Tuning Actor-Critic Algorithm},
  author={Zahavy, Tom and Xu, Zhongwen and Veeriah, Vivek and Hessel, Matteo and Oh, Junhyuk and van Hasselt, Hado and Silver, David and Singh, Satinder},
  journal={arXiv preprint arXiv:2002.12928},
  year={2020}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@article{ortega2019meta,
  title={Meta-learning of sequential strategies},
  author={Ortega, Pedro A and Wang, Jane X and Rowland, Mark and Genewein, Tim and Kurth-Nelson, Zeb and Pascanu, Razvan and Heess, Nicolas and Veness, Joel and Pritzel, Alex and Sprechmann, Pablo and others},
  journal={arXiv preprint arXiv:1905.03030},
  year={2019}
}

@article{wang2016learning,
  title={Learning to reinforcement learn},
  author={Wang, Jane X and Kurth-Nelson, Zeb and Tirumala, Dhruva and Soyer, Hubert and Leibo, Joel Z and Munos, Remi and Blundell, Charles and Kumaran, Dharshan and Botvinick, Matt},
  journal={arXiv preprint arXiv:1611.05763},
  year={2016}
}

@inproceedings{rakelly2019efficient,
  title={Efficient off-policy meta-reinforcement learning via probabilistic context variables},
  author={Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle={International conference on machine learning},
  pages={5331--5340},
  year={2019},
  organization={PMLR}
}

@article{zintgraf2019varibad,
  title={Varibad: A very good method for bayes-adaptive deep rl via meta-learning},
  author={Zintgraf, Luisa and Shiarlis, Kyriacos and Igl, Maximilian and Schulze, Sebastian and Gal, Yarin and Hofmann, Katja and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1910.08348},
  year={2019}
}

@article{duan2016rl,
  title={Rl2: Fast reinforcement learning via slow reinforcement learning},
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779},
  year={2016}
}

@inproceedings{liang2018rllib,
  title={RLlib: Abstractions for distributed reinforcement learning},
  author={Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle={International Conference on Machine Learning},
  pages={3053--3062},
  year={2018},
  organization={PMLR}
}

@book{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@inproceedings{weber2019credit,
  title={Credit assignment techniques in stochastic computation graphs},
  author={Weber, Th{\'e}ophane and Heess, Nicolas and Buesing, Lars and Silver, David},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={2650--2660},
  year={2019},
  organization={PMLR}
}

@inproceedings{tucker2018mirage,
  title={The mirage of action-dependent baselines in reinforcement learning},
  author={Tucker, George and Bhupatiraju, Surya and Gu, Shixiang and Turner, Richard and Ghahramani, Zoubin and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={5015--5024},
  year={2018},
  organization={PMLR}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in markov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@inproceedings{kaelbling1993learning,
  title={Learning to achieve goals},
  author={Kaelbling, Leslie Pack},
  booktitle={IJCAI},
  pages={1094--1099},
  year={1993},
  organization={Citeseer}
}

@inproceedings{konda2000actor,
  title={Actor-critic algorithms},
  author={Konda, Vijay R and Tsitsiklis, John N},
  booktitle={Advances in neural information processing systems},
  pages={1008--1014},
  year={2000},
  organization={Citeseer}
}

@incollection{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010},
  pages={177--186},
  year={2010},
  publisher={Springer}
}

@article{andrychowicz2017hindsight,
  title={Hindsight experience replay},
  author={Andrychowicz, Marcin and Wolski, Filip and Ray, Alex and Schneider, Jonas and Fong, Rachel and Welinder, Peter and McGrew, Bob and Tobin, Josh and Abbeel, Pieter and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1707.01495},
  year={2017}
}

@article{stadie2018some,
  title={Some considerations on learning to explore via meta-reinforcement learning},
  author={Stadie, Bradly C and Yang, Ge and Houthooft, Rein and Chen, Xi and Duan, Yan and Wu, Yuhuai and Abbeel, Pieter and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1803.01118},
  year={2018}
}


@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}


@inproceedings{foerster2018dice,
  title={Dice: The infinitely differentiable monte carlo estimator},
  author={Foerster, Jakob and Farquhar, Gregory and Al-Shedivat, Maruan and Rockt{\"a}schel, Tim and Xing, Eric and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={1529--1538},
  year={2018},
  organization={PMLR}
}

@inproceedings{mao2019baseline,
  title={A baseline for any order gradient estimation in stochastic computation graphs},
  author={Mao, Jingkai and Foerster, Jakob and Rockt{\"a}schel, Tim and Al-Shedivat, Maruan and Farquhar, Gregory and Whiteson, Shimon},
  booktitle={International Conference on Machine Learning},
  pages={4343--4351},
  year={2019},
  organization={PMLR}
}

@article{farquhar2019loaded,
  title={Loaded dice: Trading off bias and variance in any-order score function gradient estimators for reinforcement learning},
  author={Farquhar, Gregory and Whiteson, Shimon and Foerster, Jakob},
  year={2019}
}

@inproceedings{liu2019taming,
  title={Taming maml: Efficient unbiased meta-reinforcement learning},
  author={Liu, Hao and Socher, Richard and Xiong, Caiming},
  booktitle={International Conference on Machine Learning},
  pages={4061--4071},
  year={2019},
  organization={PMLR}
}

@article{rothfuss2018promp,
  title={Promp: Proximal meta-policy search},
  author={Rothfuss, Jonas and Lee, Dennis and Clavera, Ignasi and Asfour, Tamim and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1810.06784},
  year={2018}
}



@inproceedings{zhang2017weighted,
  title={Weighted Double Q-learning.},
  author={Zhang, Zongzhang and Pan, Zhiyuan and Kochenderfer, Mykel J},
  booktitle={IJCAI},
  pages={3455--3461},
  year={2017}
}

@article{heess2015learning,
  title={Learning continuous control policies by stochastic value gradients},
  author={Heess, Nicolas and Wayne, Greg and Silver, David and Lillicrap, Timothy and Tassa, Yuval and Erez, Tom},
  journal={arXiv preprint arXiv:1510.09142},
  year={2015}
}

@article{tang2021unifying,
  title={Unifying Gradient Estimators for Meta-Reinforcement Learning via Off-Policy Evaluation},
  author={Tang, Yunhao and Kozuno, Tadashi and Rowland, Mark and Munos, R{\'e}mi and Valko, Michal},
  journal={arXiv preprint arXiv:2106.13125},
  year={2021}
}

@article{fallah2020provably,
  title={Provably convergent policy gradient methods for model-agnostic meta-reinforcement learning},
  author={Fallah, Alireza and Georgiev, Kristian and Mokhtari, Aryan and Ozdaglar, Asuman},
  journal={arXiv preprint arXiv:2002.05135},
  year={2020}
}

@article{mohamed2020monte,
  title={Monte Carlo Gradient Estimation in Machine Learning.},
  author={Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  journal={J. Mach. Learn. Res.},
  volume={21},
  number={132},
  pages={1--62},
  year={2020}
}

@article{lan2020maxmin,
  title={Maxmin Q-learning: Controlling the Estimation Bias of Q-learning},
  author={Lan, Qingfeng and Pan, Yangchen and Fyshe, Alona and White, Martha},
  journal={arXiv preprint arXiv:2002.06487},
  year={2020}
}

@inproceedings{anschel2017averaged,
  title={Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning},
  author={Anschel, Oron and Baram, Nir and Shimkin, Nahum},
  booktitle={International Conference on Machine Learning},
  pages={176--185},
  year={2017},
  organization={PMLR}
}
    
    
       