\begin{thebibliography}{67}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Geirhos et~al.(2018)Geirhos, Temme, Rauber, Sch\"{u}tt, Bethge, and
  Wichmann]{geirhos2018noise}
Robert Geirhos, Carlos R.~M. Temme, Jonas Rauber, Heiko~H. Sch\"{u}tt, Matthias
  Bethge, and Felix~A. Wichmann.
\newblock Generalisation in humans and deep neural networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems 31}, pages 7538--7550. Curran Associates, Inc., 2018.
\newblock URL
  \url{http://papers.nips.cc/paper/7982-generalisation-in-humans-and-deep-neural-networks.pdf}.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2018benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICLR)}, 2015.

\bibitem[Schneider et~al.(2018)Schneider, Ecker, Macke, and
  Bethge]{schneider2018multi}
Steffen Schneider, Alexander~S Ecker, Jakob~H Macke, and Matthias Bethge.
\newblock Multi-task generalization and adaptation between noisy digit
  datasets: An empirical study.
\newblock In \emph{Neural Information Processing Systems (NeurIPS), Workshop on
  Continual Learning}, 2018.

\bibitem[Cariucci et~al.(2017)Cariucci, Porzi, Caputo, Ricci, and
  Bulo]{cariucci2017autodial}
Fabio~Maria Cariucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and
  Samuel~Rota Bulo.
\newblock Autodial: Automatic domain alignment layers.
\newblock In \emph{2017 IEEE International Conference on Computer Vision
  (ICCV)}, 2017.

\bibitem[Li et~al.(2017)Li, Wang, Shi, Liu, and
  Hou]{DBLP:journals/corr/LiWSLH16}
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou.
\newblock Revisiting batch normalization for practical domain adaptation.
\newblock In \emph{International Conference on Machine Learning (ICLR)}, 2017.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision (IJCV)}, 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Conference on computer vision and pattern recognition
  (CVPR)}, 2009.

\bibitem[Krizhevsky et~al.(2012{\natexlab{a}})Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012{\natexlab{a}}.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in {PyTorch}.
\newblock In \emph{NIPS Autodiff Workshop}, 2017.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{santurkar2018does}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Sugiyama and Kawanabe(2012)]{sugiyama2012machine}
Masashi Sugiyama and Motoaki Kawanabe.
\newblock \emph{Machine learning in non-stationary environments: Introduction
  to covariate shift adaptation}.
\newblock MIT press, 2012.

\bibitem[Sch\"{o}lkopf et~al.(2012)Sch\"{o}lkopf, Janzing, Peters, Sgouritsa,
  Zhang, and Mooij]{schoelkopf2012causal}
Bernhard Sch\"{o}lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun
  Zhang, and Joris Mooij.
\newblock On causal and anticausal learning.
\newblock In \emph{Proceedings of the 29th International Coference on
  International Conference on Machine Learning}, ICML’12, page 459–466,
  Madison, WI, USA, 2012. Omnipress.
\newblock ISBN 9781450312851.

\bibitem[Bishop(2006)]{bishop}
Christopher~M. Bishop.
\newblock \emph{Pattern Recognition and Machine Learning (Information Science
  and Statistics)}.
\newblock Springer-Verlag, Berlin, Heidelberg, 2006.
\newblock ISBN 0387310738.

\bibitem[Huang et~al.(2017)Huang, Liu, and Weinberger]{densenet}
Gao Huang, Zhuang Liu, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2017.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{googlenet}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott~E. Reed,
  Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{inception}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Conference on computer vision and pattern recognition
  (CVPR)}, 2016.

\bibitem[Tan et~al.(2019)Tan, Chen, Pang, Vasudevan, Sandler, Howard, and
  Le]{mnasnet}
Mingxing Tan, Bo~Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew
  Howard, and Quoc~V Le.
\newblock Mnasnet: Platform-aware neural architecture search for mobile.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2019.

\bibitem[Sandler et~al.(2018)Sandler, Howard, Zhu, Zhmoginov, and
  Chen]{mobilenet}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In \emph{Conference on computer vision and pattern recognition
  (CVPR)}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Conference on computer vision and pattern recognition
  (CVPR)}, 2016.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Conference on computer vision and pattern recognition
  (CVPR)}, 2017.

\bibitem[Ma et~al.(2018)Ma, Zhang, Zheng, and Sun]{shufflenet}
Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.
\newblock Shufflenet v2: Practical guidelines for efficient cnn architecture
  design.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, 2018.

\bibitem[Simonyan and Zisserman(2015)]{vgg}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2015.

\bibitem[Zagoruyko and Komodakis(2016)]{wideresnet}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{CoRR}, abs/1605.07146, 2016.

\bibitem[Marcel and Rodriguez(2010)]{10.1145/1873951.1874254}
S\'{e}bastien Marcel and Yann Rodriguez.
\newblock Torchvision the machine-vision package of torch.
\newblock In \emph{ACM International Conference on Multimedia}, 2010.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and van~der Maaten]{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens van~der Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, 2018.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Swersky, Norouzi, and
  Hinton]{chen2020big}
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey
  Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock \emph{CoRR}, abs/2006.10029, 2020.

\bibitem[Geirhos et~al.(2019)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{geirhos2018imagenettrained}
Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix~A.
  Wichmann, and Wieland Brendel.
\newblock Imagenet-trained {CNN}s are biased towards texture; increasing shape
  bias improves accuracy and robustness.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Rusak et~al.(2020)Rusak, Schott, Zimmermann, Bitterwolf, Bringmann,
  Bethge, and Brendel]{Rusak2020IncreasingTR}
Evgenia Rusak, Lukas Schott, Roland Zimmermann, Julian Bitterwolf, Oliver
  Bringmann, Matthias Bethge, and Wieland Brendel.
\newblock Increasing the robustness of dnns against image corruptions by
  playing the game of noise.
\newblock \emph{CoRR}, abs/2001.06057, 2020.

\bibitem[Hendrycks et~al.(2020{\natexlab{a}})Hendrycks, Mu, Cubuk, Zoph,
  Gilmer, and Lakshminarayanan]{hendrycks2019augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020{\natexlab{a}}.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Man{\'{e}}, Vasudevan, and
  Le]{autoaugment2019}
Ekin~Dogus Cubuk, Barret Zoph, Dandelion Man{\'{e}}, Vijay Vasudevan, and
  Quoc~V. Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2019.

\bibitem[Lee et~al.(2020)Lee, Won, and Hong]{lee2020compounding}
Jungkyu Lee, Taeryun Won, and Kiho Hong.
\newblock Compounding the performance improvements of assembled techniques in a
  convolutional neural network.
\newblock \emph{CoRR}, abs/2001.06268, 2020.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Zhao, Basart, Steinhardt, and
  Song]{DBLP:journals/corr/abs-1907-07174}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock \emph{CoRR}, abs/1907.07174, 2019.

\bibitem[Recht et~al.(2020)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2020.

\bibitem[Barbu et~al.(2019)Barbu, Mayo, Alverio, Luo, Wang, Gutfreund,
  Tenenbaum, and Katz]{barbu2019objectnet}
Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan
  Gutfreund, Josh Tenenbaum, and Boris Katz.
\newblock Objectnet: A large-scale bias-controlled dataset for pushing the
  limits of object recognition models.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, 2019.

\bibitem[Hendrycks et~al.(2020{\natexlab{b}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2020many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock \emph{CoRR}, abs/2006.16241, 2020{\natexlab{b}}.

\bibitem[Orhan(2019)]{orhan2019robustness}
A~Emin Orhan.
\newblock Robustness properties of facebook's resnext wsl models.
\newblock \emph{CoRR}, abs/1907.07640, 2019.

\bibitem[Galloway et~al.(2019)Galloway, Golubeva, Tanay, Moussa, and
  Taylor]{galloway2019batch}
Angus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham~W
  Taylor.
\newblock Batch normalization is a cause of adversarial vulnerability.
\newblock \emph{CoRR}, abs/1905.02161, 2019.

\bibitem[Zhang et~al.(2019)Zhang, Dauphin, and Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{CoRR}, abs/1901.09321, 2019.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, 2018.

\bibitem[Mu and Gilmer(2019)]{mu2019mnist}
Norman Mu and Justin Gilmer.
\newblock \uppercase{MNIST-C}: A robustness benchmark for computer vision.
\newblock \emph{CoRR}, abs/1906.02337, 2019.

\bibitem[Michaelis et~al.(2019)Michaelis, Mitzkus, Geirhos, Rusak, Bringmann,
  Ecker, Bethge, and Brendel]{michaelis2019benchmarking}
Claudio Michaelis, Benjamin Mitzkus, Robert Geirhos, Evgenia Rusak, Oliver
  Bringmann, Alexander~S Ecker, Matthias Bethge, and Wieland Brendel.
\newblock Benchmarking robustness in object detection: Autonomous driving when
  winter is coming.
\newblock \emph{CoRR}, abs/1907.07484, 2019.

\bibitem[Kamann and Rother(2019)]{Kamann2019BenchmarkingTR}
Christoph Kamann and Carsten Rother.
\newblock Benchmarking the robustness of semantic segmentation models.
\newblock \emph{CoRR}, abs/1908.05005, 2019.

\bibitem[Ford et~al.(2019)Ford, Gilmer, Carlini, and
  Cubuk]{ford2019adversarial}
Nic Ford, Justin Gilmer, Nicolas Carlini, and Dogus Cubuk.
\newblock Adversarial examples are a natural consequence of test error in
  noise.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Mikołajczyk and Grochowski(2018)]{Mikoajczyk2018DataAF}
Agnieszka Mikołajczyk and Michał Grochowski.
\newblock Data augmentation for improving deep learning in image classification
  problem.
\newblock In \emph{International Interdisciplinary PhD Workshop (IIPhDW)},
  2018.

\bibitem[Zhang(2019)]{zhang2019making}
Richard Zhang.
\newblock Making convolutional networks shift-invariant again.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Sun et~al.(2017)Sun, Feng, and Saenko]{sun2017correlation}
Baochen Sun, Jiashi Feng, and Kate Saenko.
\newblock Correlation alignment for unsupervised domain adaptation.
\newblock In \emph{Domain Adaptation in Computer Vision Applications}, pages
  153--171. Springer, 2017.

\bibitem[Bug et~al.(2017)Bug, Schneider, Grote, Oswald, Feuerhake, Sch{\"u}ler,
  and Merhof]{bug2017context}
Daniel Bug, Steffen Schneider, Anne Grote, Eva Oswald, Friedrich Feuerhake,
  Julia Sch{\"u}ler, and Dorit Merhof.
\newblock Context-based normalization of histological stains using deep
  convolutional features.
\newblock In \emph{Deep Learning in Medical Image Analysis and Multimodal
  Learning for Clinical Decision Support}. Springer, 2017.

\bibitem[Sun et~al.(2019)Sun, Wang, Liu, Miller, Efros, and Hardt]{sun2019test}
Yu~Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei~A Efros, and Moritz
  Hardt.
\newblock Test-time training for out-of-distribution generalization.
\newblock \emph{CoRR}, abs/1909.13231, 2019.

\bibitem[French et~al.(2017)French, Mackiewicz, and Fisher]{french2017self}
Geoffrey French, Michal Mackiewicz, and Mark~H. Fisher.
\newblock Self-ensembling for domain adaptation.
\newblock \emph{CoRR}, abs/1706.05208, 2017.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10687--10698, 2020.

\bibitem[Wang et~al.(2020)Wang, Shelhamer, Liu, Olshausen, and
  Darrell]{wang2020fully}
Dequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell.
\newblock Fully test-time adaptation by entropy minimization.
\newblock \emph{CoRR}, abs/2006.10726, 2020.

\bibitem[Xie and Yuille(2020)]{xie2020intriguing}
Cihang Xie and Alan~L. Yuille.
\newblock Intriguing properties of adversarial training.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{rebuffi2017learning}
Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Frankle et~al.(2020)Frankle, Schwab, and Morcos]{frankle2020training}
Jonathan Frankle, David~J Schwab, and Ari~S Morcos.
\newblock Training batchnorm and only batchnorm: On the expressive power of
  random features in cnns.
\newblock \emph{CoRR}, abs/2003.00152, 2020.

\bibitem[Nado et~al.(2020)Nado, Padhy, Sculley, D'Amour, Lakshminarayanan, and
  Snoek]{nado2020evaluating}
Zachary Nado, Shreyas Padhy, D~Sculley, Alexander D'Amour, Balaji
  Lakshminarayanan, and Jasper Snoek.
\newblock Evaluating prediction-time batch normalization for robustness under
  covariate shift.
\newblock \emph{CoRR}, abs/2006.10963, 2020.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos2020shortcut}
Robert Geirhos, J{\"o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{CoRR}, abs/2004.07780, 2020.

\bibitem[Villani(2008)]{villani2008optimal}
C{\'e}dric Villani.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Engstrom et~al.(2020)Engstrom, Ilyas, Santurkar, Tsipras, Steinhardt,
  and Madry]{engstrom2020identifying}
Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Jacob
  Steinhardt, and Aleksander Madry.
\newblock Identifying statistical bias in dataset replication.
\newblock \emph{CoRR}, abs/2005.09619, 2020.

\bibitem[Merkel(2014)]{10.5555/2600239.2600241}
Dirk Merkel.
\newblock Docker: Lightweight linux containers for consistent development and
  deployment.
\newblock \emph{Linux J.}, 2014\penalty0 (239), March 2014.
\newblock ISSN 1075-3583.

\bibitem[{Virtanen} et~al.(2020){Virtanen}, {Gommers}, {Oliphant}, {Haberland},
  {Reddy}, {Cournapeau}, {Burovski}, {Peterson}, {Weckesser}, {Bright}, {van
  der Walt}, {Brett}, {Wilson}, {Jarrod Millman}, {Mayorov}, {Nelson}, {Jones},
  {Kern}, {Larson}, {Carey}, {Polat}, {Feng}, {Moore}, {Vand erPlas},
  {Laxalde}, {Perktold}, {Cimrman}, {Henriksen}, {Quintero}, {Harris},
  {Archibald}, {Ribeiro}, {Pedregosa}, {van Mulbregt}, and
  {Contributors}]{2020SciPy-NMeth}
Pauli {Virtanen}, Ralf {Gommers}, Travis~E. {Oliphant}, Matt {Haberland}, Tyler
  {Reddy}, David {Cournapeau}, Evgeni {Burovski}, Pearu {Peterson}, Warren
  {Weckesser}, Jonathan {Bright}, St{\'e}fan~J. {van der Walt}, Matthew
  {Brett}, Joshua {Wilson}, K.~{Jarrod Millman}, Nikolay {Mayorov}, Andrew
  R.~J. {Nelson}, Eric {Jones}, Robert {Kern}, Eric {Larson}, CJ~{Carey},
  {\.I}lhan {Polat}, Yu~{Feng}, Eric~W. {Moore}, Jake {Vand erPlas}, Denis
  {Laxalde}, Josef {Perktold}, Robert {Cimrman}, Ian {Henriksen}, E.~A.
  {Quintero}, Charles~R {Harris}, Anne~M. {Archibald}, Ant{\^o}nio~H.
  {Ribeiro}, Fabian {Pedregosa}, Paul {van Mulbregt}, and SciPy 1.~0
  {Contributors}.
\newblock {SciPy 1.0: Fundamental Algorithms for Scientific Computing in
  Python}.
\newblock \emph{Nature Methods}, 17:\penalty0 261--272, 2020.
\newblock \doi{https://doi.org/10.1038/s41592-019-0686-2}.

\bibitem[Tange(2011)]{Tange2011a}
O.~Tange.
\newblock Gnu parallel - the command-line power tool.
\newblock \emph{;login: The USENIX Magazine}, 36\penalty0 (1):\penalty0 42--47,
  Feb 2011.
\newblock URL \url{http://www.gnu.org/s/parallel}.

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 16)}, pages 265--283, 2016.

\bibitem[Lin(2020 (accessed October 21, 2020))]{convert-pytorch-simclr}
Ji~Lin.
\newblock \emph{A PyTorch Converter for SimCLR Checkpoints}, 2020 (accessed
  October 21, 2020).
\newblock URL \url{https://github.com/tonylins/simclr-converter}.
\newblock Commit ID: 139d3cb0bd0c64b5ad32aab810e0bd0a0dddaae0.

\bibitem[Weisstein(2020)]{weisstein}
Eric Weisstein.
\newblock Standard deviation distribution, 2020.
\newblock URL
  \url{https://mathworld.wolfram.com/StandardDeviationDistribution.html}.

\bibitem[Becker(2012)]{becker2012variance}
Robert~A. Becker.
\newblock The variance drain and jensen's inequality.
\newblock 2012-004, 2012.

\bibitem[Krizhevsky et~al.(2012{\natexlab{b}})Krizhevsky, Sutskever, and
  Hinton]{alexnet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)}.
  2012{\natexlab{b}}.

\end{thebibliography}
