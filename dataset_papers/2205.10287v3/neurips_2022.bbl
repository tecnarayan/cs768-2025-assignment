\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chen et~al.(2021)Chen, Zhou, Tang, Yang, Cao, and Gu]{padam}
Jinghui Chen, Dongruo Zhou, Yiqi Tang, Ziyan Yang, Yuan Cao, and Quanquan Gu.
\newblock Closing the generalization gap of adaptive gradient methods in
  training deep neural networks.
\newblock In \emph{Proceedings of the Twenty-Ninth International Joint
  Conference on Artificial Intelligence}, IJCAI'20, 2021.
\newblock ISBN 9780999241165.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255, 2009.
\newblock \doi{10.1109/CVPR.2009.5206848}.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Timothy Dozat.
\newblock Incorporating {N}esterov momentum into {A}dam.
\newblock In \emph{Workshop}, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Granziol et~al.(2022)Granziol, Zohren, and
  Roberts]{granziol2021learning}
Diego Granziol, Stefan Zohren, and Stephen Roberts.
\newblock Learning rates as a function of batch size: A random matrix theory
  approach to neural network training.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (173):\penalty0 1--65, 2022.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and Sun]{He_2015_ICCV}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {ImageNet} classification.
\newblock In \emph{The IEEE International Conference on Computer Vision
  (ICCV)}, December 2015{\natexlab{a}}.

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  {ImageNet} classification.
\newblock In \emph{The IEEE International Conference on Computer Vision
  (ICCV)}, December 2015{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hoffer et~al.(2017)Hoffer, Hubara, and Soudry]{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 1731--1741. Curran Associates,
  Inc., 2017.

\bibitem[Izsak et~al.(2021)Izsak, Berchansky, and Levy]{izsak2021how}
Peter Izsak, Moshe Berchansky, and Omer Levy.
\newblock How to train {BERT} with an academic budget.
\newblock \emph{arXiv preprint arXiv:2104.07705}, 2021.

\bibitem[Jastrz\k{e}bski et~al.(2017)Jastrz\k{e}bski, Kenton, Arpit, Ballas,
  Fischer, Bengio, and Storkey]{jastrzkebski2017three}
Stanis{\l}aw Jastrz\k{e}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in {SGD}.
\newblock \emph{arXiv preprint arXiv:1711.04623}, 2017.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Krizhevsky(2014)]{krizhevsky2014one}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock \emph{arXiv preprint arXiv:1404.5997}, 2014.

\bibitem[Krizhevsky et~al.()Krizhevsky, Nair, and Hinton]{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock {CIFAR}-10 ({C}anadian {I}nstitute for {A}dvanced {R}esearch).
\newblock URL \url{http://www.cs.toronto.edu/~kriz/cifar.html}.

\bibitem[Kunin et~al.(2021)Kunin, Sagastuy-Brena, Ganguli, Yamins, and
  Tanaka]{kunin2021neural}
Daniel Kunin, Javier Sagastuy-Brena, Surya Ganguli, Daniel~LK Yamins, and
  Hidenori Tanaka.
\newblock Neural mechanics: Symmetry and broken conservation laws in deep
  learning dynamics.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Li et~al.(2019)Li, Tai, and E]{li2019stochastic}
Qianxiao Li, Cheng Tai, and Weinan E.
\newblock Stochastic modified equations and dynamics of stochastic gradient
  algorithms {I}: Mathematical foundations.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (40):\penalty0 1--47, 2019.

\bibitem[Li and Arora(2020)]{li2020exp}
Zhiyuan Li and Sanjeev Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Li et~al.(2020)Li, Lyu, and Arora]{li2020reconciling}
Zhiyuan Li, Kaifeng Lyu, and Sanjeev Arora.
\newblock Reconciling modern deep learning with traditional optimization
  analyses: The intrinsic learning rate.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 14544--14555. Curran Associates, Inc., 2020.

\bibitem[Li et~al.(2021)Li, Malladi, and Arora]{li2021validity}
Zhiyuan Li, Sadhika Malladi, and Sanjeev Arora.
\newblock On the validity of modeling sgd with stochastic differential
  equations (sdes).
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 12712--12725. Curran Associates, Inc., 2021.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {R}o{BERT}a: A robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2019)Luo, Xiong, and Liu]{luo2018adaptive}
Liangchen Luo, Yuanhao Xiong, and Yan Liu.
\newblock Adaptive gradient methods with dynamic bound of learning rate.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ma et~al.(2022)Ma, Wu, and E]{ma2020qualitative}
Chao Ma, Lei Wu, and Weinan E.
\newblock A qualitative study of the dynamic behavior for adaptive gradient
  algorithms.
\newblock In Joan Bruna, Jan Hesthaven, and Lenka Zdeborova, editors,
  \emph{Proceedings of the 2nd Mathematical and Scientific Machine Learning
  Conference}, volume 145 of \emph{Proceedings of Machine Learning Research},
  pages 671--692. PMLR, 16--19 Aug 2022.

\bibitem[Ma and Yarats(2019)]{ma2018quasihyperbolic}
Jerry Ma and Denis Yarats.
\newblock Quasi-hyperbolic momentum and {A}dam for deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2017pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le]{ramachandran2017searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{amsgrad}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of {A}dam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Simsekli et~al.(2019)Simsekli, Sagun, and
  Gurbuzbalaban]{simsekli2019tailindex}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  5827--5837. PMLR, 09--15 Jun 2019.

\bibitem[Strubell et~al.(2020)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for modern deep learning research.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  34\penalty0 (09):\penalty0 13693--13696, Apr 2020.

\bibitem[Tieleman and Hinton(2012)]{hinton2012rmsprop}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude., 2012.
\newblock URL
  \url{https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE}: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wettig et~al.(2022)Wettig, Gao, Zhong, and Chen]{wettig2022should}
Alexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen.
\newblock Should you mask 15\% in masked language modeling?
\newblock \emph{arXiv preprint arXiv:2202.08005}, 2022.

\bibitem[Xie et~al.(2021)Xie, Sato, and Sugiyama]{xie2021diffusion}
Zeke Xie, Issei Sato, and Masashi Sugiyama.
\newblock A diffusion theory for deep learning dynamics: Stochastic gradient
  descent exponentially favors flat minima.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xie et~al.(2022)Xie, Wang, Zhang, Sato, and Sugiyama]{xie2022adai}
Zeke Xie, Xinrui Wang, Huishuai Zhang, Issei Sato, and Masashi Sugiyama.
\newblock Adaptive inertia: Disentangling the effects of adaptive learning rate
  and momentum.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 24430--24459. PMLR,
  17--23 Jul 2022.

\bibitem[You et~al.(2020)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song,
  Demmel, Keutzer, and Hsieh]{you2020lamb}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training {BERT} in 76
  minutes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zaheer et~al.(2018)Zaheer, Reddi, Sachan, Kale, and Kumar]{yogi}
Manzil Zaheer, Sashank Reddi, Devendra Sachan, Satyen Kale, and Sanjiv Kumar.
\newblock Adaptive methods for nonconvex optimization.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Zhang et~al.(2020)Zhang, Karimireddy, Veit, Kim, Reddi, Kumar, and
  Sra]{zhang2020adaptive}
Jingzhao Zhang, Sai~Praneeth Karimireddy, Andreas Veit, Seungyeon Kim, Sashank
  Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why are adaptive methods good for attention models?
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 15383--15393. Curran Associates, Inc., 2020.

\bibitem[Zhou et~al.(2020)Zhou, Feng, Ma, Xiong, Hoi, and
  E]{zhou2020theoretically}
Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu~Hong Hoi, and Weinan
  E.
\newblock Towards theoretically understanding why {SGD} generalizes better than
  {A}dam in deep learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 21285--21296. Curran Associates, Inc., 2020.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,
  Antonio Torralba, and Sanja Fidler.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{2015 IEEE International Conference on Computer Vision
  (ICCV)}, pages 19--27, 2015.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{zhuang2020adabelief}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar~C Tatikonda, Nicha Dvornek,
  Xenophon Papademetris, and James Duncan.
\newblock Ada{B}elief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18795--18806. Curran Associates, Inc., 2020.

\end{thebibliography}
