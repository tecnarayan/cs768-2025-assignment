\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman,
  Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
J.~Achiam, S.~Adler, S.~Agarwal, L.~Ahmad, I.~Akkaya, F.~L. Aleman, D.~Almeida,
  J.~Altenschmidt, S.~Altman, S.~Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Ahrabian et~al.(2024)Ahrabian, Sourati, Sun, Zhang, Jiang, Morstatter,
  and Pujara]{ahrabian2024curious}
K.~Ahrabian, Z.~Sourati, K.~Sun, J.~Zhang, Y.~Jiang, F.~Morstatter, and
  J.~Pujara.
\newblock The curious case of nonverbal abstract reasoning with multi-modal
  large language models.
\newblock \emph{arXiv preprint arXiv:2401.12117}, 2024.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc,
  A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 23716--23736, 2022.

\bibitem[Albert and Barab{\'a}si(2002)]{barabasi}
R.~Albert and A.-L. Barab{\'a}si.
\newblock Statistical mechanics of complex networks.
\newblock \emph{Reviews of modern physics}, 74\penalty0 (1):\penalty0 47, 2002.

\bibitem[Anthropic(2024)]{anthropic2024claude}
A.~Anthropic.
\newblock The claude 3 model family: Opus, sonnet, haiku.
\newblock \emph{Claude-3 Model Card}, 2024.

\bibitem[Antol et~al.(2015)Antol, Agrawal, Lu, Mitchell, Batra, Zitnick, and
  Parikh]{antol2015vqa}
S.~Antol, A.~Agrawal, J.~Lu, M.~Mitchell, D.~Batra, C.~L. Zitnick, and
  D.~Parikh.
\newblock Vqa: Visual question answering.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2425--2433, 2015.

\bibitem[Awadalla et~al.(2023)Awadalla, Gao, Gardner, Hessel, Hanafy, Zhu,
  Marathe, Bitton, Gadre, Sagawa, et~al.]{awadalla2023openflamingo}
A.~Awadalla, I.~Gao, J.~Gardner, J.~Hessel, Y.~Hanafy, W.~Zhu, K.~Marathe,
  Y.~Bitton, S.~Gadre, S.~Sagawa, et~al.
\newblock Openflamingo: An open-source framework for training large
  autoregressive vision-language models.
\newblock \emph{arXiv preprint arXiv:2308.01390}, 2023.

\bibitem[Barab{\'a}si and Albert(1999)]{scalefree}
A.-L. Barab{\'a}si and R.~Albert.
\newblock Emergence of scaling in random networks.
\newblock \emph{science}, 286\penalty0 (5439):\penalty0 509--512, 1999.

\bibitem[Beyer* et~al.(2024)Beyer*, Steiner*, Susano~Pinto*, Kolesnikov*,
  Wang*, Salz, Neumann, Alabdulmohsin, Tschannen, Bugliarello, Unterthiner,
  Keysers, Gritsenko, Chen, Koppula, Grycner, Bauer, Bošnjak, Liu, Houlsby,
  Kumar, Rong, Eisenschlos, Minderer, Voigtlaender, Bica, Balazevic,
  Puigcerver, Papalampidi, Henaff, Xiong, Soricut, Harmsen, and
  Zhai*]{paligemma}
L.~Beyer*, A.~Steiner*, A.~Susano~Pinto*, A.~Kolesnikov*, X.~Wang*, D.~Salz,
  M.~Neumann, I.~Alabdulmohsin, M.~Tschannen, E.~Bugliarello, T.~Unterthiner,
  D.~Keysers, A.~Gritsenko, X.~Chen, S.~Koppula, A.~Grycner, M.~Bauer,
  M.~Bošnjak, F.~Liu, N.~Houlsby, M.~Kumar, K.~Rong, J.~Eisenschlos,
  M.~Minderer, P.~Voigtlaender, I.~Bica, I.~Balazevic, J.~Puigcerver,
  P.~Papalampidi, O.~Henaff, X.~Xiong, R.~Soricut, J.~Harmsen, and X.~Zhai*.
\newblock {PaliGemma: A versatile 3B VLM for transfer}, 2024.
\newblock To appear.

\bibitem[Chen et~al.(2022)Chen, Wang, Changpinyo, Piergiovanni, Padlewski,
  Salz, Goodman, Grycner, Mustafa, Beyer, et~al.]{chen2022pali}
X.~Chen, X.~Wang, S.~Changpinyo, A.~Piergiovanni, P.~Padlewski, D.~Salz,
  S.~Goodman, A.~Grycner, B.~Mustafa, L.~Beyer, et~al.
\newblock Pali: A jointly-scaled multilingual language-image model.
\newblock \emph{arXiv preprint arXiv:2209.06794}, 2022.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick,
  and Tafjord]{clark2018think}
P.~Clark, I.~Cowhey, O.~Etzioni, T.~Khot, A.~Sabharwal, C.~Schoenick, and
  O.~Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Erd\H{o}s and R\'enyi(1959)]{erdds1959random}
P.~Erd\H{o}s and A.~R\'enyi.
\newblock On random graphs.
\newblock \emph{Publicationes Mathematicae Debrecen}, 6:\penalty0 290--297,
  1959.

\bibitem[Fan et~al.(2024)Fan, Hua, Li, Zhu, Jin, Li, Ling, Chi, Wang, Ma,
  et~al.]{fan2024nphardeval4v}
L.~Fan, W.~Hua, X.~Li, K.~Zhu, M.~Jin, L.~Li, H.~Ling, J.~Chi, J.~Wang, X.~Ma,
  et~al.
\newblock Nphardeval4v: A dynamic reasoning benchmark of multimodal large
  language models.
\newblock \emph{arXiv preprint arXiv:2403.01777}, 2024.

\bibitem[Fatemi et~al.(2024)Fatemi, Halcrow, and Perozzi]{fatemi2024talk}
B.~Fatemi, J.~Halcrow, and B.~Perozzi.
\newblock Talk like a graph: Encoding graphs for large language models.
\newblock In \emph{ICLR}, 2024.

\bibitem[Fu et~al.(2023)Fu, Chen, Shen, Qin, Zhang, Lin, Qiu, Lin, Yang, Zheng,
  Li, Sun, and Ji]{Fu2023MMEAC}
C.~Fu, P.~Chen, Y.~Shen, Y.~Qin, M.~Zhang, X.~Lin, Z.~Qiu, W.~Lin, J.~Yang,
  X.~Zheng, K.~Li, X.~Sun, and R.~Ji.
\newblock Mme: A comprehensive evaluation benchmark for multimodal large
  language models.
\newblock \emph{ArXiv}, abs/2306.13394, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:259243928}.

\bibitem[Fu et~al.(2024)Fu, Hu, Li, Feng, Wang, Lin, Roth, Smith, Ma, and
  Krishna]{fu2024blink}
X.~Fu, Y.~Hu, B.~Li, Y.~Feng, H.~Wang, X.~Lin, D.~Roth, N.~A. Smith, W.-C. Ma,
  and R.~Krishna.
\newblock Blink: Multimodal large language models can see but not perceive.
\newblock \emph{arXiv preprint arXiv:2404.12390}, 2024.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and
  Parikh]{goyal2017making}
Y.~Goyal, T.~Khot, D.~Summers-Stay, D.~Batra, and D.~Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding
  in visual question answering.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 6904--6913, 2017.

\bibitem[Hagberg et~al.(2008)Hagberg, Swart, and S~Chult]{networkx}
A.~Hagberg, P.~Swart, and D.~S~Chult.
\newblock Exploring network structure, dynamics, and function using networkx.
\newblock Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
  (United States), 2008.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song,
  and Steinhardt]{hendrycks2020measuring}
D.~Hendrycks, C.~Burns, S.~Basart, A.~Zou, M.~Mazeika, D.~Song, and
  J.~Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Kadavath, Arora, Basart, Tang,
  Song, and Steinhardt]{hendrycks2021measuring}
D.~Hendrycks, C.~Burns, S.~Kadavath, A.~Arora, S.~Basart, E.~Tang, D.~Song, and
  J.~Steinhardt.
\newblock Measuring mathematical problem solving with the math dataset.
\newblock In J.~Vanschoren and S.~Yeung, editors, \emph{Proceedings of the
  Neural Information Processing Systems Track on Datasets and Benchmarks},
  volume~1. Curran, 2021.

\bibitem[Holland et~al.(1983)Holland, Laskey, and
  Leinhardt]{holland1983stochastic}
P.~W. Holland, K.~B. Laskey, and S.~Leinhardt.
\newblock Stochastic blockmodels: First steps.
\newblock \emph{Social networks}, 5\penalty0 (2):\penalty0 109--137, 1983.

\bibitem[Huang et~al.(2024)Huang, Dong, Wang, Hao, Singhal, Ma, Lv, Cui,
  Mohammed, Patra, et~al.]{huang2024language}
S.~Huang, L.~Dong, W.~Wang, Y.~Hao, S.~Singhal, S.~Ma, T.~Lv, L.~Cui, O.~K.
  Mohammed, B.~Patra, et~al.
\newblock Language is not all you need: Aligning perception with language
  models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Kazemi et~al.(2023{\natexlab{a}})Kazemi, Alvari, Anand, Wu, Chen, and
  Soricut]{kazemi2023geomverse}
M.~Kazemi, H.~Alvari, A.~Anand, J.~Wu, X.~Chen, and R.~Soricut.
\newblock Geomverse: A systematic evaluation of large models for geometric
  reasoning.
\newblock \emph{arXiv preprint arXiv:2312.12241}, 2023{\natexlab{a}}.

\bibitem[Kazemi et~al.(2023{\natexlab{b}})Kazemi, Yuan, Bhatia, Kim, Xu,
  Imbrasaite, and Ramachandran]{kazemi2023boardgameqa}
M.~Kazemi, Q.~Yuan, D.~Bhatia, N.~Kim, X.~Xu, V.~Imbrasaite, and
  D.~Ramachandran.
\newblock Boardgameqa: A dataset for natural language reasoning with
  contradictory information.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Kazemzadeh et~al.(2014)Kazemzadeh, Ordonez, Matten, and
  Berg]{kazemzadeh2014referitgame}
S.~Kazemzadeh, V.~Ordonez, M.~Matten, and T.~Berg.
\newblock Referitgame: Referring to objects in photographs of natural scenes.
\newblock In \emph{Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, pages 787--798, 2014.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski,
  Ramasesh, Slone, Anil, Schlag, Gutman-Solo, et~al.]{lewkowycz2022solving}
A.~Lewkowycz, A.~Andreassen, D.~Dohan, E.~Dyer, H.~Michalewski, V.~Ramasesh,
  A.~Slone, C.~Anil, I.~Schlag, T.~Gutman-Solo, et~al.
\newblock Solving quantitative reasoning problems with language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 3843--3857, 2022.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Ge, Ge, Wang, Wang, Zhang, and
  Shan]{li2023seed}
B.~Li, Y.~Ge, Y.~Ge, G.~Wang, R.~Wang, R.~Zhang, and Y.~Shan.
\newblock Seed-bench-2: Benchmarking multimodal large language models.
\newblock \emph{arXiv preprint arXiv:2311.17092}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Savarese, and Hoi]{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image
  encoders and large language models.
\newblock \emph{arXiv preprint arXiv:2301.12597}, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
T.-Y. Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan,
  P.~Doll{\'a}r, and C.~L. Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Computer Vision--ECCV 2014: 13th European Conference,
  Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages
  740--755. Springer, 2014.

\bibitem[Lindstr{\"o}m and Abraham(2022)]{lindstrom2022clevr}
A.~D. Lindstr{\"o}m and S.~S. Abraham.
\newblock Clevr-math: A dataset for compositional language, visual and
  mathematical reasoning.
\newblock \emph{arXiv preprint arXiv:2208.05358}, 2022.

\bibitem[Liu et~al.(2021)Liu, Bugliarello, Ponti, Reddy, Collier, and
  Elliott]{liu-etal-2021-visually}
F.~Liu, E.~Bugliarello, E.~M. Ponti, S.~Reddy, N.~Collier, and D.~Elliott.
\newblock Visually grounded reasoning across languages and cultures.
\newblock In M.-F. Moens, X.~Huang, L.~Specia, and S.~W.-t. Yih, editors,
  \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural
  Language Processing}, pages 10467--10485, Online and Punta Cana, Dominican
  Republic, Nov. 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.818}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.818}.

\bibitem[Liu et~al.(2024)Liu, Li, Wu, and Lee]{liu2024visual}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2024.

\bibitem[Liu et~al.(2023)Liu, Duan, Zhang, Li, Zhang, Zhao, Yuan, Wang, He,
  Liu, et~al.]{liu2023mmbench}
Y.~Liu, H.~Duan, Y.~Zhang, B.~Li, S.~Zhang, W.~Zhao, Y.~Yuan, J.~Wang, C.~He,
  Z.~Liu, et~al.
\newblock Mmbench: Is your multi-modal model an all-around player?
\newblock \emph{arXiv preprint arXiv:2307.06281}, 2023.

\bibitem[Lu et~al.(2021)Lu, Gong, Jiang, Qiu, Huang, Liang, and
  Zhu]{lu2021inter}
P.~Lu, R.~Gong, S.~Jiang, L.~Qiu, S.~Huang, X.~Liang, and S.-C. Zhu.
\newblock Inter-gps: Interpretable geometry problem solving with formal
  language and symbolic reasoning.
\newblock \emph{arXiv preprint arXiv:2105.04165}, 2021.

\bibitem[Lu et~al.(2022)Lu, Mishra, Xia, Qiu, Chang, Zhu, Tafjord, Clark, and
  Kalyan]{lu2022learn}
P.~Lu, S.~Mishra, T.~Xia, L.~Qiu, K.-W. Chang, S.-C. Zhu, O.~Tafjord, P.~Clark,
  and A.~Kalyan.
\newblock Learn to explain: Multimodal reasoning via thought chains for science
  question answering.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 2507--2521, 2022.

\bibitem[Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang,
  Galley, and Gao]{lu2023mathvista}
P.~Lu, H.~Bansal, T.~Xia, J.~Liu, C.~Li, H.~Hajishirzi, H.~Cheng, K.-W. Chang,
  M.~Galley, and J.~Gao.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in
  visual contexts.
\newblock \emph{arXiv preprint arXiv:2310.02255}, 2023.

\bibitem[Marino et~al.(2019)Marino, Rastegari, Farhadi, and
  Mottaghi]{marino2019ok}
K.~Marino, M.~Rastegari, A.~Farhadi, and R.~Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external
  knowledge.
\newblock In \emph{Proceedings of the IEEE/cvf conference on computer vision
  and pattern recognition}, pages 3195--3204, 2019.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Paperno et~al.(2016)Paperno, Kruszewski, Lazaridou, Pham, Bernardi,
  Pezzelle, Baroni, Boleda, and Fern{\'a}ndez]{paperno2016lambada}
D.~Paperno, G.~Kruszewski, A.~Lazaridou, Q.~N. Pham, R.~Bernardi, S.~Pezzelle,
  M.~Baroni, G.~Boleda, and R.~Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock \emph{arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[Rajani et~al.(2019)Rajani, McCann, Xiong, and
  Socher]{rajani2019explain}
N.~F. Rajani, B.~McCann, C.~Xiong, and R.~Socher.
\newblock Explain yourself! leveraging language models for commonsense
  reasoning.
\newblock \emph{arXiv preprint arXiv:1906.02361}, 2019.

\bibitem[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap,
  Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{reid2024gemini}
M.~Reid, N.~Savinov, D.~Teplyashin, D.~Lepikhin, T.~Lillicrap, J.-b. Alayrac,
  R.~Soricut, A.~Lazaridou, O.~Firat, J.~Schrittwieser, et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of
  tokens of context.
\newblock \emph{arXiv preprint arXiv:2403.05530}, 2024.

\bibitem[Saparov et~al.(2024)Saparov, Pang, Padmakumar, Joshi, Kazemi, Kim, and
  He]{saparov2024testing}
A.~Saparov, R.~Y. Pang, V.~Padmakumar, N.~Joshi, M.~Kazemi, N.~Kim, and H.~He.
\newblock Testing the general deductive reasoning capacity of large language
  models using ood examples.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Shi et~al.(2022)Shi, Suzgun, Freitag, Wang, Srivats, Vosoughi, Chung,
  Tay, Ruder, Zhou, et~al.]{shi2022language}
F.~Shi, M.~Suzgun, M.~Freitag, X.~Wang, S.~Srivats, S.~Vosoughi, H.~W. Chung,
  Y.~Tay, S.~Ruder, D.~Zhou, et~al.
\newblock Language models are multilingual chain-of-thought reasoners.
\newblock \emph{arXiv preprint arXiv:2210.03057}, 2022.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch,
  Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
A.~Srivastava, A.~Rastogi, A.~Rao, A.~A.~M. Shoeb, A.~Abid, A.~Fisch, A.~R.
  Brown, A.~Santoro, A.~Gupta, A.~Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the
  capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Suhr et~al.(2017)Suhr, Lewis, Yeh, and Artzi]{suhr-etal-2017-corpus}
A.~Suhr, M.~Lewis, J.~Yeh, and Y.~Artzi.
\newblock A corpus of natural language for visual reasoning.
\newblock In R.~Barzilay and M.-Y. Kan, editors, \emph{Proceedings of the 55th
  Annual Meeting of the Association for Computational Linguistics (Volume 2:
  Short Papers)}, pages 217--223, Vancouver, Canada, July 2017. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/P17-2034}.
\newblock URL \url{https://aclanthology.org/P17-2034}.

\bibitem[Suhr et~al.(2019)Suhr, Zhou, Zhang, Zhang, Bai, and
  Artzi]{suhr-etal-2019-corpus}
A.~Suhr, S.~Zhou, A.~Zhang, I.~Zhang, H.~Bai, and Y.~Artzi.
\newblock A corpus for reasoning about natural language grounded in
  photographs.
\newblock In A.~Korhonen, D.~Traum, and L.~M{\`a}rquez, editors,
  \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 6418--6428, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1644}.
\newblock URL \url{https://aclanthology.org/P19-1644}.

\bibitem[Tafjord et~al.(2021)Tafjord, Dalvi, and Clark]{tafjord2021proofwriter}
O.~Tafjord, B.~Dalvi, and P.~Clark.
\newblock {P}roof{W}riter: Generating implications, proofs, and abductive
  statements over natural language.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL-IJCNLP 2021}, pages 3621--3634, Online, Aug. 2021. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/2021.findings-acl.317}.
\newblock URL \url{https://aclanthology.org/2021.findings-acl.317}.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,
  Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
G.~Team, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut,
  J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Tsimpoukelli et~al.(2021)Tsimpoukelli, Menick, Cabi, Eslami, Vinyals,
  and Hill]{tsimpoukelli2021multimodal}
M.~Tsimpoukelli, J.~L. Menick, S.~Cabi, S.~Eslami, O.~Vinyals, and F.~Hill.
\newblock Multimodal few-shot learning with frozen language models.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 200--212, 2021.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman]{wang2019superglue}
A.~Wang, Y.~Pruksachatkun, N.~Nangia, A.~Singh, J.~Michael, F.~Hill, O.~Levy,
  and S.~Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, F.~Xia, E.~Chi, Q.~V. Le, D.~Zhou,
  et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Yue et~al.(2023)Yue, Ni, Zhang, Zheng, Liu, Zhang, Stevens, Jiang,
  Ren, Sun, et~al.]{yue2023mmmu}
X.~Yue, Y.~Ni, K.~Zhang, T.~Zheng, R.~Liu, G.~Zhang, S.~Stevens, D.~Jiang,
  W.~Ren, Y.~Sun, et~al.
\newblock Mmmu: A massive multi-discipline multimodal understanding and
  reasoning benchmark for expert agi.
\newblock \emph{arXiv preprint arXiv:2311.16502}, 2023.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and
  Choi]{zellers2019hellaswag}
R.~Zellers, A.~Holtzman, Y.~Bisk, A.~Farhadi, and Y.~Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock \emph{arXiv preprint arXiv:1905.07830}, 2019.

\end{thebibliography}
