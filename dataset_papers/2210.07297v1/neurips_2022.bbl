\begin{thebibliography}{10}

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
  Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds,
  et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em arXiv preprint arXiv:2204.14198}, 2022.

\bibitem{aydin2016distributed}
Kevin Aydin, MohammadHossein Bateni, and Vahab Mirrokni.
\newblock Distributed balanced partitioning via linear embedding.
\newblock In {\em Proceedings of the Ninth ACM International Conference on Web
  Search and Data Mining}, pages 387--396, 2016.

\bibitem{bertsimas1993simulated}
Dimitris Bertsimas and John Tsitsiklis.
\newblock Simulated annealing.
\newblock {\em Statistical science}, 8(1):10--15, 1993.

\bibitem{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{cui2016geeps}
Henggang Cui, Hao Zhang, Gregory~R Ganger, Phillip~B Gibbons, and Eric~P Xing.
\newblock Geeps: Scalable deep learning on distributed gpus with a
  gpu-specialized parameter server.
\newblock In {\em Proceedings of the eleventh european conference on computer
  systems}, pages 1--16, 2016.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{fan2021dapple}
Shiqing Fan, Yi~Rong, Chen Meng, Zongyan Cao, Siyu Wang, Zhen Zheng, Chuan Wu,
  Guoping Long, Jun Yang, Lixue Xia, et~al.
\newblock Dapple: A pipelined data parallel approach for training large models.
\newblock In {\em Proceedings of the 26th ACM SIGPLAN Symposium on Principles
  and Practice of Parallel Programming}, pages 431--445, 2021.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{ho2013more}
Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin~Kyu Kim, Phillip~B
  Gibbons, Garth~A Gibson, Greg Ganger, and Eric~P Xing.
\newblock More effective distributed ml via a stale synchronous parallel
  parameter server.
\newblock In {\em Advances in neural information processing systems}, pages
  1223--1231, 2013.

\bibitem{huang2019gpipe}
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen,
  HyoukJoong Lee, Jiquan Ngiam, Quoc~V Le, Yonghui Wu, et~al.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock {\em Advances in neural information processing systems}, 32:103--112,
  2019.

\bibitem{jia2019beyond}
Zhihao Jia, Matei Zaharia, and Alex Aiken.
\newblock Beyond data and model parallelism for deep neural networks.
\newblock {\em SysML 2019}, 2019.

\bibitem{jiang2021transgan}
Yifan Jiang, Shiyu Chang, and Zhangyang Wang.
\newblock Transgan: Two pure transformers can make one strong gan, and that can
  scale up.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{jiang2020unified}
Yimin Jiang, Yibo Zhu, Chang Lan, Bairen Yi, Yong Cui, and Chuanxiong Guo.
\newblock A unified architecture for accelerating distributed $\{$DNN$\}$
  training in heterogeneous gpu/cpu clusters.
\newblock In {\em 14th $\{$USENIX$\}$ Symposium on Operating Systems Design and
  Implementation ($\{$OSDI$\}$ 20)}, pages 463--479, 2020.

\bibitem{lepikhin2020gshard}
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping
  Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen.
\newblock Gshard: Scaling giant models with conditional computation and
  automatic sharding.
\newblock {\em arXiv preprint arXiv:2006.16668}, 2020.

\bibitem{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em 11th $\{$USENIX$\}$ Symposium on Operating Systems Design and
  Implementation ($\{$OSDI$\}$ 14)}, pages 583--598, 2014.

\bibitem{li2021terapipe}
Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, Dawn Song, and
  Ion Stoica.
\newblock Terapipe: Token-level pipeline parallelism for training large-scale
  language models.
\newblock {\em arXiv preprint arXiv:2102.07988}, 2021.

\bibitem{mirhoseini2017device}
Azalia Mirhoseini, Hieu Pham, Quoc~V Le, Benoit Steiner, Rasmus Larsen, Yuefeng
  Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean.
\newblock Device placement optimization with reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2430--2439. PMLR, 2017.

\bibitem{narayanan2019pipedream}
Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil~R
  Devanur, Gregory~R Ganger, Phillip~B Gibbons, and Matei Zaharia.
\newblock Pipedream: generalized pipeline parallelism for dnn training.
\newblock In {\em Proceedings of the 27th ACM Symposium on Operating Systems
  Principles}, pages 1--15, 2019.

\bibitem{narayanan2021efficient}
Deepak Narayanan, Mohammad Shoeybi, Jared Casper, Patrick LeGresley, Mostofa
  Patwary, Vijay Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie
  Bernauer, Bryan Catanzaro, et~al.
\newblock Efficient large-scale language model training on gpu clusters using
  megatron-lm.
\newblock In {\em Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--15, 2021.

\bibitem{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
  Sutskever, et~al.
\newblock Language models are unsupervised multitask learners.
\newblock {\em OpenAI blog}, 1(8):9, 2019.

\bibitem{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models
  with over 100 billion parameters.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem{sergeev2018horovod}
Alexander Sergeev and Mike Del~Balso.
\newblock Horovod: fast and easy distributed deep learning in tensorflow.
\newblock {\em arXiv preprint arXiv:1802.05799}, 2018.

\bibitem{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock {\em arXiv preprint arXiv:1909.08053}, 2019.

\bibitem{tarnawski2021piper}
Jakub~M Tarnawski, Deepak Narayanan, and Amar Phanishayee.
\newblock Piper: Multidimensional planner for dnn parallelization.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{van1987simulated}
Peter~JM Van~Laarhoven and Emile~HL Aarts.
\newblock Simulated annealing.
\newblock In {\em Simulated annealing: Theory and applications}, pages 7--15.
  Springer, 1987.

\bibitem{williams2009roofline}
Samuel Williams, Andrew Waterman, and David Patterson.
\newblock Roofline: an insightful visual performance model for multicore
  architectures.
\newblock {\em Communications of the ACM}, 52(4):65--76, 2009.

\bibitem{zhangautodist}
Hao Zhang, Peng Wu, Zhijie Deng, Christy Li, Qirong Ho, Aurick Qiao, Zeya Wang,
  and Eric~P Xing.
\newblock Autodist: Acomposable and automated synchronization system for
  distributed deep learning.

\bibitem{zhang2017poseidon}
Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang, Zhiting
  Hu, Jinliang Wei, Pengtao Xie, and Eric~P Xing.
\newblock Poseidon: An efficient communication architecture for distributed
  deep learning on $\{$GPU$\}$ clusters.
\newblock In {\em 2017 USENIX Annual Technical Conference (USENIX ATC 17)},
  pages 181--193, 2017.

\bibitem{zheng2022alpa}
Lianmin Zheng, Zhuohan Li, Hao Zhang, Yonghao Zhuang, Zhifeng Chen, Yanping
  Huang, Yida Wang, Yuanzhong Xu, Danyang Zhuo, Joseph~E Gonzalez, et~al.
\newblock Alpa: Automating inter-and intra-operator parallelism for distributed
  deep learning.
\newblock {\em arXiv preprint arXiv:2201.12023}, 2022.

\end{thebibliography}
