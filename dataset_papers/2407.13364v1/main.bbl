\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2004.

\bibitem[Angelotti et~al.(2021)Angelotti, Drougard, and Chanel]{angelotti2021expert}
Angelotti, G., Drougard, N., and Chanel, C.~P.
\newblock Expert-guided symmetry detection in {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:2111.10297}, 2021.

\bibitem[Biza \& Platt(2018)Biza and Platt]{biza2018online}
Biza, O. and Platt, R.
\newblock Online abstraction with mdp homomorphisms for deep learning.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent Systems}, 2018.

\bibitem[Chaloner \& Verdinelli(1995)Chaloner and Verdinelli]{chaloner1995bayesian}
Chaloner, K. and Verdinelli, I.
\newblock Bayesian experimental design: A review.
\newblock \emph{Statistical Science}, pp.\  273--304, 1995.

\bibitem[Cheng et~al.(2021)Cheng, Gong, Liu, Song, and Zou]{cheng2021molecular}
Cheng, Y., Gong, Y., Liu, Y., Song, B., and Zou, Q.
\newblock Molecular design in drug discovery: a comprehensive review of deep generative models.
\newblock \emph{Briefings in Bioinformatics}, 22\penalty0 (6), 2021.

\bibitem[Dong et~al.(2022)Dong, Zhao, Liu, Su, and Zeng]{dong2022deep}
Dong, J., Zhao, M., Liu, Y., Su, Y., and Zeng, X.
\newblock Deep learning in retrosynthesis planning: datasets, models and tools.
\newblock \emph{Briefings in Bioinformatics}, 23\penalty0 (1), 2022.

\bibitem[Elton et~al.(2019)Elton, Boukouvalas, Fuge, and Chung]{elton2019deep}
Elton, D.~C., Boukouvalas, Z., Fuge, M.~D., and Chung, P.~W.
\newblock Deep learning for molecular designâ€”a review of the state of the art.
\newblock \emph{Molecular Systems Design \& Engineering}, 4\penalty0 (4):\penalty0 828--849, 2019.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and Fern{\'a}ndez]{garcia2015comprehensive}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0 (1):\penalty0 1437--1480, 2015.

\bibitem[Geist et~al.(2022)Geist, P{\'e}rolat, Lauri{\`e}re, Elie, Perrin, Bachem, Munos, and Pietquin]{geist2021concave}
Geist, M., P{\'e}rolat, J., Lauri{\`e}re, M., Elie, R., Perrin, S., Bachem, O., Munos, R., and Pietquin, O.
\newblock Concave utility reinforcement learning: The mean-field game viewpoint.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent Systems}, 2022.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and Van~Soest]{hazan2019maxent}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Jaggi(2013)]{pmlr-v28-jaggi13}
Jaggi, M.
\newblock Revisiting {Frank-Wolfe}: Projection-free sparse convex optimization.
\newblock In \emph{International Conference on Machine Learning}, 2013.

\bibitem[Jiang et~al.(2014)Jiang, Singh, and Lewis]{jiang2014improving}
Jiang, N., Singh, S., and Lewis, R.
\newblock Improving {UCT} planning via approximate homomorphisms.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent Systems}, 2014.

\bibitem[Krause(2008)]{krause2008optimizing}
Krause, A.
\newblock \emph{Optimizing sensing: Theory and applications}.
\newblock PhD thesis, Carnegie Mellon University, 2008.

\bibitem[Lafferty et~al.(2008)Lafferty, Liu, and Wasserman]{lafferty2008concentration}
Lafferty, J., Liu, H., and Wasserman, L.
\newblock Concentration of measure.
\newblock \emph{Online at \url{https://www.stat.cmu.edu/~larry/=sml/Concentration.pdf}}, 2008.

\bibitem[Lindner et~al.(2022)Lindner, Krause, and Ramponi]{lindner2022active}
Lindner, D., Krause, A., and Ramponi, G.
\newblock Active exploration for inverse reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Mahajan \& Tulabandhula(2017)Mahajan and Tulabandhula]{mahajan2017symmetry}
Mahajan, A. and Tulabandhula, T.
\newblock Symmetry learning for function approximation in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1706.02999}, 2017.

\bibitem[Mavor-Parker et~al.(2022)Mavor-Parker, Sargent, Banino, Griffin, and Barry]{mavor2022simple}
Mavor-Parker, A.~N., Sargent, M.~J., Banino, A., Griffin, L.~D., and Barry, C.
\newblock A simple approach for state-action abstraction using a learned mdp homomorphism.
\newblock \emph{arXiv preprint arXiv:2209.06356}, 2022.

\bibitem[Mondal et~al.(2022)Mondal, Jain, Siddiqi, and Ravanbakhsh]{pmlr-v162-mondal22a}
Mondal, A.~K., Jain, V., Siddiqi, K., and Ravanbakhsh, S.
\newblock {E}q{R}: Equivariant representations for data-efficient reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\bibitem[Mutny et~al.(2023)Mutny, Janik, and Krause]{mutny2023active}
Mutny, M., Janik, T., and Krause, A.
\newblock Active exploration via experiment design in {M}arkov chains.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2023.

\bibitem[Mutti et~al.(2022{\natexlab{a}})Mutti, De~Santi, De~Bartolomeis, and Restelli]{mutti2022challenging}
Mutti, M., De~Santi, R., De~Bartolomeis, P., and Restelli, M.
\newblock Challenging common assumptions in convex reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Mutti et~al.(2022{\natexlab{b}})Mutti, De~Santi, and Restelli]{mutti2022importance}
Mutti, M., De~Santi, R., and Restelli, M.
\newblock The importance of non-{M}arkovianity in maximum state entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, 2022{\natexlab{b}}.

\bibitem[Mutti et~al.(2023)Mutti, De~Santi, De~Bartolomeis, and Restelli]{mutti2023convex}
Mutti, M., De~Santi, R., De~Bartolomeis, P., and Restelli, M.
\newblock Convex reinforcement learning in finite trials.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (250):\penalty0 1--42, 2023.

\bibitem[Narayanamurthy \& Ravindran(2008)Narayanamurthy and Ravindran]{narayanamurthy2008hardness}
Narayanamurthy, S.~M. and Ravindran, B.
\newblock On the hardness of finding symmetries in {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, 2008.

\bibitem[Nesterov(2014)]{10.5555/2670022}
Nesterov, Y.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Springer Publishing Company, 2014.

\bibitem[Panaganti \& Kalathil(2022)Panaganti and Kalathil]{panaganti2022sample}
Panaganti, K. and Kalathil, D.
\newblock Sample complexity of robust reinforcement learning with a generative model.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2022.

\bibitem[Pukelsheim(2006)]{pukelsheim2006optimal}
Pukelsheim, F.
\newblock \emph{Optimal design of experiments}.
\newblock SIAM, 2006.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Ravindran(2003)]{ravindran2003smdp}
Ravindran, B.
\newblock Smdp homomorphisms: An algebraic approach to abstraction in semi {M}arkov decision processes.
\newblock In \emph{International Joint Conference on Artificial Intelligence}, 2003.

\bibitem[Ravindran \& Barto(2001)Ravindran and Barto]{ravindran2001symmetries}
Ravindran, B. and Barto, A.~G.
\newblock Symmetries and model minimization in {M}arkov decision processes.
\newblock \emph{Technical report}, 2001.

\bibitem[Ravindran \& Barto(2002)Ravindran and Barto]{ravindran2002model}
Ravindran, B. and Barto, A.~G.
\newblock Model minimization in hierarchical reinforcement learning.
\newblock In \emph{Abstraction, Reformulation, and Approximation: 5th International Symposium}, 2002.

\bibitem[Ravindran \& Barto(2004)Ravindran and Barto]{ravindran2004approximate}
Ravindran, B. and Barto, A.~G.
\newblock Approximate homomorphisms: A framework for non-exact minimization in {M}arkov decision processes.
\newblock \emph{Technical report}, 2004.

\bibitem[Rezaei-Shoshtari et~al.(2022)Rezaei-Shoshtari, Zhao, Panangaden, Meger, and Precup]{rezaei2022continuous}
Rezaei-Shoshtari, S., Zhao, R., Panangaden, P., Meger, D., and Precup, D.
\newblock Continuous mdp homomorphisms and homomorphic policy gradient.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Rotman(2010)]{rotman2010advanced}
Rotman, J.~J.
\newblock \emph{Advanced Modern Algebra}, volume 114.
\newblock American Mathematical Society, 2010.

\bibitem[Schreck et~al.(2019)Schreck, Coley, and Bishop]{schreck2019learning}
Schreck, J.~S., Coley, C.~W., and Bishop, K.~J.
\newblock Learning retrosynthetic planning through simulated experience.
\newblock \emph{ACS Central Science}, 5\penalty0 (6):\penalty0 970--981, 2019.

\bibitem[Soni \& Singh(2006)Soni and Singh]{soni2006using}
Soni, V. and Singh, S.
\newblock Using homomorphisms to transfer options across continuous reinforcement learning domains.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~6, pp.\  494--499, 2006.

\bibitem[Szepesv{\'a}ri(2009)]{szepesvari2009reinforcement}
Szepesv{\'a}ri, C.
\newblock Reinforcement learning algorithms for mdps.
\newblock 2009.

\bibitem[Tarbouriech \& Lazaric(2019)Tarbouriech and Lazaric]{tarbouriech2019active}
Tarbouriech, J. and Lazaric, A.
\newblock Active exploration in {M}arkov decision processes.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2019.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Shekhar, Pirotta, Ghavamzadeh, and Lazaric]{tarbouriech2020active}
Tarbouriech, J., Shekhar, S., Pirotta, M., Ghavamzadeh, M., and Lazaric, A.
\newblock Active model estimation in markov decision processes.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2020.

\bibitem[Thiede et~al.(2022)Thiede, Krenn, Nigam, and Aspuru-Guzik]{Thiede_2022}
Thiede, L.~A., Krenn, M., Nigam, A., and Aspuru-Guzik, A.
\newblock Curiosity in exploring chemical spaces: intrinsic rewards for molecular reinforcement learning.
\newblock \emph{Machine Learning: Science and Technology}, 3\penalty0 (3), 2022.

\bibitem[van~der Pol et~al.(2020{\natexlab{a}})van~der Pol, Kipf, Oliehoek, and Welling]{van2020plannable}
van~der Pol, E., Kipf, T., Oliehoek, F.~A., and Welling, M.
\newblock Plannable approximations to mdp homomorphisms: Equivariance under actions.
\newblock In \emph{International Conference on Autonomous Agents and Multiagent Systems}, 2020{\natexlab{a}}.

\bibitem[van~der Pol et~al.(2020{\natexlab{b}})van~der Pol, Worrall, van Hoof, Oliehoek, and Welling]{derPol2020homomorphic}
van~der Pol, E., Worrall, D., van Hoof, H., Oliehoek, F., and Welling, M.
\newblock Mdp homomorphic networks: Group symmetries in reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020{\natexlab{b}}.

\bibitem[van~der Pol et~al.(2022)van~der Pol, van Hoof, Oliehoek, and Welling]{van2021multi}
van~der Pol, E., van Hoof, H., Oliehoek, F.~A., and Welling, M.
\newblock Multi-agent mdp homomorphic networks.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wang et~al.(2023)Wang, Fu, Du, Gao, Huang, Liu, Chandak, Liu, Van~Katwyk, Deac, et~al.]{wang2023scientific}
Wang, H., Fu, T., Du, Y., Gao, W., Huang, K., Liu, Z., Chandak, P., Liu, S., Van~Katwyk, P., Deac, A., et~al.
\newblock Scientific discovery in the age of artificial intelligence.
\newblock \emph{Nature}, 620\penalty0 (7972):\penalty0 47--60, 2023.

\bibitem[Welch(1982)]{welch1982algorithmic}
Welch, W.~J.
\newblock Algorithmic complexity: three np-hard problems in computational statistics.
\newblock \emph{Journal of Statistical Computation and Simulation}, 15\penalty0 (1):\penalty0 17--25, 1982.

\bibitem[Wolfe \& Barto(2006)Wolfe and Barto]{wolfe2006decision}
Wolfe, A.~P. and Barto, A.~G.
\newblock Decision tree methods for finding reusable mdp homomorphisms.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2006.

\bibitem[Zahavy et~al.(2021)Zahavy, O'Donoghue, Desjardins, and Singh]{zahavy2021reward}
Zahavy, T., O'Donoghue, B., Desjardins, G., and Singh, S.
\newblock Reward is enough for convex mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Koppel, Bedi, Szepesvari, and Wang]{zhang2020variational}
Zhang, J., Koppel, A., Bedi, A.~S., Szepesvari, C., and Wang, M.
\newblock Variational policy gradient method for reinforcement learning with general utilities.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhao(2022)]{zhao2022continuous}
Zhao, R.~Y.
\newblock \emph{Continuous Homomorphisms and Leveraging Symmetries in Policy Gradient Algorithms for Markov Decision Processes}.
\newblock PhD thesis, 2022.

\bibitem[Zhu et~al.(2022)Zhu, Jiang, Liu, Yu, and Zhang]{zhu2022invariant}
Zhu, Z.-M., Jiang, S., Liu, Y.-R., Yu, Y., and Zhang, K.
\newblock Invariant action effect model for reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2022.

\end{thebibliography}
