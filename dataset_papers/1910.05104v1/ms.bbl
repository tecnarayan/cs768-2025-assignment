\begin{thebibliography}{10}

\bibitem{gpipe}
Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam,
  Quoc~V. Le, and Zhifeng Chen.
\newblock Gpipe: Efficient training of giant neural networks using pipeline
  parallelism.
\newblock {\em CoRR}, abs/1811.06965, 2018.

\bibitem{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em 11th USENIX Symposium on Operating Systems Design and
  Implementation (OSDI 14)}, pages 583--598, 2014.

\bibitem{petrowski1993performance}
Alain Petrowski, Gerard Dreyfus, and Claude Girault.
\newblock Performance analysis of a pipelined backpropagation parallel
  algorithm.
\newblock {\em IEEE Transactions on Neural Networks}, 4(6):970--981, 1993.

\bibitem{wu2016google}
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc~V Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et~al.
\newblock Google's neural machine translation system: Bridging the gap between
  human and machine translation.
\newblock {\em arXiv preprint arXiv:1609.08144}, 2016.

\bibitem{chen2018efficient}
Chi-Chung Chen, Chia-Lin Yang, and Hsiang-Yun Cheng.
\newblock Efficient and robust parallel dnn training through model parallelism
  on multi-gpu platform.
\newblock {\em arXiv preprint arXiv:1809.02839}, 2018.

\bibitem{doi:10.1137/110831659}
John~C. Duchi, Peter~L. Bartlett, and Martin~J. Wainwright.
\newblock Randomized smoothing for stochastic optimization.
\newblock {\em SIAM Journal on Optimization}, 22(2):674--701, 2012.

\bibitem{NIPS2018_7539}
Kevin Scaman, Francis Bach, Sebastien Bubeck, Laurent Massouli\'{e}, and
  Yin~Tat Lee.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  2740--2749. 2018.

\bibitem{valiant1990bridging}
Leslie~G Valiant.
\newblock A bridging model for parallel computation.
\newblock {\em Communications of the ACM}, 33(8):103--111, 1990.

\bibitem{krizhevsky2014one}
Alex Krizhevsky.
\newblock One weird trick for parallelizing convolutional neural networks.
\newblock {\em arXiv preprint arXiv:1404.5997}, 2014.

\bibitem{lee2014model}
Seunghak Lee, Jin~Kyu Kim, Xun Zheng, Qirong Ho, Garth~A Gibson, and Eric~P
  Xing.
\newblock On model parallelization and scheduling strategies for distributed
  machine learning.
\newblock In {\em Advances in neural information processing systems}, pages
  2834--2842, 2014.

\bibitem{NIPS2018_7875}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  6389--6399. 2018.

\bibitem{chen2018optimal}
Yize Chen, Yuanyuan Shi, and Baosen Zhang.
\newblock Optimal control via neural networks: A convex approach.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{pmlr-v70-amos17b}
Brandon Amos, Lei Xu, and J.~Zico Kolter.
\newblock Input convex neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70, pages 146--155, 2017.

\bibitem{NIPS2005_2800}
Yoshua Bengio, Nicolas~L. Roux, Pascal Vincent, Olivier Delalleau, and Patrice
  Marcotte.
\newblock Convex neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 18}, pages
  123--130. 2006.

\bibitem{burke2005robust}
James~V Burke, Adrian~S Lewis, and Michael~L Overton.
\newblock A robust gradient sampling algorithm for nonsmooth, nonconvex
  optimization.
\newblock {\em SIAM Journal on Optimization}, 15(3):751--779, 2005.

\bibitem{que2016randomized}
Xiaocun Que.
\newblock Randomized algorithms for nonconvex nonsmooth optimization.
\newblock 2016.

\bibitem{2018arXiv180209941B}
Tal {Ben-Nun} and Torsten {Hoefler}.
\newblock {Demystifying Parallel and Distributed Deep Learning: An In-Depth
  Concurrency Analysis}.
\newblock {\em arXiv e-prints}, 2018.

\bibitem{bubeck2015convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends in Machine Learning}, 8(3-4):231--357,
  2015.

\bibitem{nesterov2004introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization : a basic course}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem{2017arXiv171011606C}
Yair {Carmon}, John~C. {Duchi}, Oliver {Hinder}, and Aaron {Sidford}.
\newblock {Lower Bounds for Finding Stationary Points I}.
\newblock {\em arXiv e-prints}, 2017.

\bibitem{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{Krizhevsky:2012:ICD:2999134.2999257}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1097--1105, 2012.

\end{thebibliography}
