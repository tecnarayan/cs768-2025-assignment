\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1120--1128, 2016.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{caianiello1982systemic}
Eduardo~R Caianiello, Gaetano Scarpetta, and Giovanna Simoncelli.
\newblock A systemic study of monetary systems.
\newblock {\em International Journal Of General System}, 8(2):81--92, 1982.

\bibitem{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1609.01704}, 2016.

\bibitem{chung2014empirical}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock {\em arXiv preprint arXiv:1412.3555}, 2014.

\bibitem{cooijmans2016recurrent}
Tim Cooijmans, Nicolas Ballas, C{\'e}sar Laurent, {\c{C}}a{\u{g}}lar
  G{\"u}l{\c{c}}ehre, and Aaron Courville.
\newblock Recurrent batch normalization.
\newblock {\em arXiv preprint arXiv:1603.09025}, 2016.

\bibitem{el1995hierarchical}
Salah El~Hihi and Yoshua Bengio.
\newblock Hierarchical recurrent neural networks for long-term dependencies.
\newblock In {\em Nips}, volume 409, 1995.

\bibitem{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock {\em arXiv preprint arXiv:1609.09106}, 2016.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{jaeger2001short}
Herbert Jaeger.
\newblock {\em Short term memory in echo state networks}, volume~5.
\newblock GMD-Forschungszentrum Informationstechnik, 2001.

\bibitem{koutnik2014clockwork}
Jan Koutnik, Klaus Greff, Faustino Gomez, and Juergen Schmidhuber.
\newblock A clockwork rnn.
\newblock {\em arXiv preprint arXiv:1402.3511}, 2014.

\bibitem{krueger2016zoneout}
David Krueger, Tegan Maharaj, J{\'a}nos Kram{\'a}r, Mohammad Pezeshki, Nicolas
  Ballas, Nan~Rosemary Ke, Anirudh Goyal, Yoshua Bengio, Hugo Larochelle, Aaron
  Courville, et~al.
\newblock Zoneout: Regularizing rnns by randomly preserving hidden activations.
\newblock {\em arXiv preprint arXiv:1606.01305}, 2016.

\bibitem{le2015simple}
Quoc~V Le, Navdeep Jaitly, and Geoffrey~E Hinton.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock {\em arXiv preprint arXiv:1504.00941}, 2015.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{marcus1993building}
Mitchell~P Marcus, Mary~Ann Marcinkiewicz, and Beatrice Santorini.
\newblock Building a large annotated corpus of english: The penn treebank.
\newblock {\em Computational linguistics}, 19(2):313--330, 1993.

\bibitem{Neil2016phased}
Daniel Neil, Michael Pfeiffer, and Shih{-}Chii Liu.
\newblock Phased {LSTM:} accelerating recurrent network training for long or
  event-based sequences.
\newblock {\em arXiv preprint arXiv:1610.09513}, 2016.

\bibitem{pascanu2013difficulty}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock {\em ICML (3)}, 28:1310--1318, 2013.

\bibitem{sainath2015learning}
Tara~N Sainath, Ron~J Weiss, Andrew Senior, Kevin~W Wilson, and Oriol Vinyals.
\newblock Learning the speech front-end with raw waveform cldnns.
\newblock In {\em Sixteenth Annual Conference of the International Speech
  Communication Association}, 2015.

\bibitem{semeniuta2016recurrent}
Stanislau Semeniuta, Aliaksei Severyn, and Erhardt Barth.
\newblock Recurrent dropout without memory loss.
\newblock {\em arXiv preprint arXiv:1603.05118}, 2016.

\bibitem{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 4(2), 2012.

\bibitem{van2016wavenet}
A{\"a}ron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol
  Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock {\em CoRR abs/1609.03499}, 2016.

\bibitem{vezhnevets2017feudal}
Alexander~Sasha Vezhnevets, Simon Osindero, Tom Schaul, Nicolas Heess, Max
  Jaderberg, David Silver, and Koray Kavukcuoglu.
\newblock Feudal networks for hierarchical reinforcement learning.
\newblock {\em arXiv preprint arXiv:1703.01161}, 2017.

\bibitem{wisdom2016full}
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le~Roux, and Les Atlas.
\newblock Full-capacity unitary recurrent neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4880--4888, 2016.

\bibitem{xing2010brief}
Zhengzheng Xing, Jian Pei, and Eamonn Keogh.
\newblock A brief survey on sequence classification.
\newblock {\em ACM Sigkdd Explorations Newsletter}, 12(1):40--48, 2010.

\bibitem{yamagishi2012vctk}
Junichi Yamagishi.
\newblock English multi-speaker corpus for cstr voice cloning toolkit.
\newblock
  \url{http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html}, 2012.

\bibitem{yu2017learning}
Adams~W Yu, Hongrae Lee, and Quoc~V Le.
\newblock Learning to skim text.
\newblock {\em arXiv preprint arXiv:1704.06877}, 2017.

\bibitem{yu2015multi}
Fisher Yu and Vladlen Koltun.
\newblock Multi-scale context aggregation by dilated convolutions.
\newblock {\em arXiv preprint arXiv:1511.07122}, 2015.

\bibitem{yu2017dilated}
Fisher Yu, Vladlen Koltun, and Thomas Funkhouser.
\newblock Dilated residual networks.
\newblock {\em arXiv preprint arXiv:1705.09914}, 2017.

\bibitem{zhang2016architectural}
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan~R
  Salakhutdinov, and Yoshua Bengio.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1822--1830, 2016.

\end{thebibliography}
