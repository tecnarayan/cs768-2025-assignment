\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pp.\  308--318, 2016.

\bibitem[Aghakhani et~al.(2021)Aghakhani, Meng, Wang, Kruegel, and
  Vigna]{aghakhani2021bullseye}
Aghakhani, H., Meng, D., Wang, Y.-X., Kruegel, C., and Vigna, G.
\newblock Bullseye polytope: A scalable clean-label poisoning attack with
  improved transferability.
\newblock In \emph{2021 IEEE European Symposium on Security and Privacy
  (EuroS\&P)}, pp.\  159--178. IEEE, 2021.

\bibitem[Borgnia et~al.(2021)Borgnia, Cherepanova, Fowl, Ghiasi, Geiping,
  Goldblum, Goldstein, and Gupta]{borgnia2021strong}
Borgnia, E., Cherepanova, V., Fowl, L., Ghiasi, A., Geiping, J., Goldblum, M.,
  Goldstein, T., and Gupta, A.
\newblock Strong data augmentation sanitizes poisoning and backdoor attacks
  without an accuracy tradeoff.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  3855--3859. IEEE, 2021.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Chen et~al.(2019)Chen, Carvalho, Baracaldo, Ludwig, Edwards, Lee,
  Molloy, and Srivastava]{chen2019detecting}
Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T.,
  Molloy, I., and Srivastava, B.
\newblock Detecting backdoor attacks on deep neural networks by activation
  clustering.
\newblock In \emph{SafeAI@ AAAI}, 2019.

\bibitem[Chen et~al.(2017)Chen, Liu, Li, Lu, and Song]{chen2017targeted}
Chen, X., Liu, C., Li, B., Lu, K., and Song, D.
\newblock Targeted backdoor attacks on deep learning systems using data
  poisoning.
\newblock \emph{arXiv preprint arXiv:1712.05526}, 2017.

\bibitem[Cretu et~al.(2008)Cretu, Stavrou, Locasto, Stolfo, and
  Keromytis]{cretu2008casting}
Cretu, G.~F., Stavrou, A., Locasto, M.~E., Stolfo, S.~J., and Keromytis, A.~D.
\newblock Casting out demons: Sanitizing training data for anomaly sensors.
\newblock In \emph{2008 IEEE Symposium on Security and Privacy (sp 2008)}, pp.\
   81--95. IEEE, 2008.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pp.\  248--255. Ieee, 2009.

\bibitem[Fort et~al.(2020)Fort, Dziugaite, Paul, Kharaghani, Roy, and
  Ganguli]{fort2020deep}
Fort, S., Dziugaite, G.~K., Paul, M., Kharaghani, S., Roy, D.~M., and Ganguli,
  S.
\newblock Deep learning versus kernel learning: an empirical study of loss
  landscape geometry and the time evolution of the neural tangent kernel.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5850--5861, 2020.

\bibitem[Geiping et~al.(2021{\natexlab{a}})Geiping, Fowl, Somepalli, Goldblum,
  Moeller, and Goldstein]{geiping2021doesn}
Geiping, J., Fowl, L., Somepalli, G., Goldblum, M., Moeller, M., and Goldstein,
  T.
\newblock What doesn't kill you makes you robust (er): Adversarial training
  against poisons and backdoors.
\newblock \emph{arXiv preprint arXiv:2102.13624}, 2021{\natexlab{a}}.

\bibitem[Geiping et~al.(2021{\natexlab{b}})Geiping, Fowl, Huang, Czaja, Taylor,
  Moeller, and Goldstein]{geiping2021witches}
Geiping, J., Fowl, L.~H., Huang, W.~R., Czaja, W., Taylor, G., Moeller, M., and
  Goldstein, T.
\newblock Witches' brew: Industrial scale data poisoning via gradient matching.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=01olnfLIbD}.

\bibitem[Gu et~al.(2017)Gu, Dolan-Gavitt, and Garg]{gu2017badnets}
Gu, T., Dolan-Gavitt, B., and Garg, S.
\newblock Badnets: Identifying vulnerabilities in the machine learning model
  supply chain.
\newblock \emph{arXiv preprint arXiv:1708.06733}, 2017.

\bibitem[Hong et~al.(2020)Hong, Chandrasekaran, Kaya, Dumitra{\c{s}}, and
  Papernot]{hong2020effectiveness}
Hong, S., Chandrasekaran, V., Kaya, Y., Dumitra{\c{s}}, T., and Papernot, N.
\newblock On the effectiveness of mitigating data poisoning attacks with
  gradient shaping.
\newblock \emph{arXiv preprint arXiv:2002.11497}, 2020.

\bibitem[Huang et~al.(2020)Huang, Geiping, Fowl, Taylor, and
  Goldstein]{huang2020metapoison}
Huang, W.~R., Geiping, J., Fowl, L., Taylor, G., and Goldstein, T.
\newblock Metapoison: Practical general-purpose clean-label data poisoning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Jayaraman \& Evans(2019)Jayaraman and Evans]{jayaraman2019evaluating}
Jayaraman, B. and Evans, D.
\newblock Evaluating differentially private machine learning in practice.
\newblock In \emph{28th $\{$USENIX$\}$ Security Symposium ($\{$USENIX$\}$
  Security 19)}, pp.\  1895--1912, 2019.

\bibitem[Katharopoulos \& Fleuret(2018)Katharopoulos and
  Fleuret]{katharopoulos2018not}
Katharopoulos, A. and Fleuret, F.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2525--2534, 2018.

\bibitem[Koh et~al.(2018)Koh, Steinhardt, and Liang]{koh2018stronger}
Koh, P.~W., Steinhardt, J., and Liang, P.
\newblock Stronger data poisoning attacks break data sanitization defenses.
\newblock \emph{arXiv preprint arXiv:1811.00741}, 2018.

\bibitem[Levine \& Feizi(2020)Levine and Feizi]{levine2020deep}
Levine, A. and Feizi, S.
\newblock Deep partition aggregation: Provable defenses against general
  poisoning attacks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Li et~al.(2021)Li, Lyu, Koren, Lyu, Li, and Ma]{li2021anti}
Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., and Ma, X.
\newblock Anti-backdoor learning: Training clean models on poisoned data.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Liu et~al.(2020)Liu, Zhu, and Belkin]{liu2020toward}
Liu, C., Zhu, L., and Belkin, M.
\newblock Toward a theory of optimization for over-parameterized systems of
  non-linear equations: the lessons of deep learning.
\newblock \emph{arXiv preprint arXiv:2003.00307}, 2020.

\bibitem[Liu et~al.(2017)Liu, Ma, Aafer, Lee, Zhai, Wang, and
  Zhang]{liu2017trojaning}
Liu, Y., Ma, S., Aafer, Y., Lee, W.-C., Zhai, J., Wang, W., and Zhang, X.
\newblock Trojaning attack on neural networks.
\newblock 2017.

\bibitem[Ma et~al.(2019)Ma, Zhu, and Hsu]{ma2019data}
Ma, Y., Zhu, X.~Z., and Hsu, J.
\newblock Data poisoning against differentially-private learners: Attacks and
  defenses.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2019.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Minoux(1978)]{minoux1978accelerated}
Minoux, M.
\newblock Accelerated greedy algorithms for maximizing submodular set
  functions.
\newblock In \emph{Optimization techniques}, pp.\  234--243. Springer, 1978.

\bibitem[Mirzasoleiman et~al.(2013)Mirzasoleiman, Karbasi, Sarkar, and
  Krause]{mirzasoleiman2013distributed}
Mirzasoleiman, B., Karbasi, A., Sarkar, R., and Krause, A.
\newblock Distributed submodular maximization: Identifying representative
  elements in massive data.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2049--2057, 2013.

\bibitem[Mirzasoleiman et~al.(2015)Mirzasoleiman, Badanidiyuru, Karbasi,
  Vondr{\'a}k, and Krause]{mirzasoleiman2015lazier}
Mirzasoleiman, B., Badanidiyuru, A., Karbasi, A., Vondr{\'a}k, J., and Krause,
  A.
\newblock Lazier than lazy greedy.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Peri et~al.(2020)Peri, Gupta, Huang, Fowl, Zhu, Feizi, Goldstein, and
  Dickerson]{peri2020deep}
Peri, N., Gupta, N., Huang, W.~R., Fowl, L., Zhu, C., Feizi, S., Goldstein, T.,
  and Dickerson, J.~P.
\newblock Deep k-nn defense against clean-label data poisoning attacks.
\newblock In \emph{European Conference on Computer Vision}, pp.\  55--70.
  Springer, 2020.

\bibitem[Saha et~al.(2019)Saha, Subramanya, and Pirsiavash]{Saha2019htbd}
Saha, A., Subramanya, A., and Pirsiavash, H.
\newblock Hidden trigger backdoor attacks, 2019.

\bibitem[Schwarzschild et~al.(2020)Schwarzschild, Goldblum, Gupta, Dickerson,
  and Goldstein]{schwarzschild2020just}
Schwarzschild, A., Goldblum, M., Gupta, A., Dickerson, J.~P., and Goldstein, T.
\newblock Just how toxic is data poisoning? a unified benchmark for backdoor
  and data poisoning attacks.
\newblock \emph{arXiv preprint arXiv:2006.12557}, 2020.

\bibitem[Shafahi et~al.(2018)Shafahi, Huang, Najibi, Suciu, Studer, Dumitras,
  and Goldstein]{Shafahi2018poisonfrogs}
Shafahi, A., Huang, W.~R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and
  Goldstein, T.
\newblock Poison frogs! targeted clean-label poisoning attacks on neural
  networks, 2018.

\bibitem[Simonyan \& Zisserman(2014)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Souri et~al.(2021)Souri, Goldblum, Fowl, Chellappa, and
  Goldstein]{souri2021sleeper}
Souri, H., Goldblum, M., Fowl, L., Chellappa, R., and Goldstein, T.
\newblock Sleeper agent: Scalable hidden trigger backdoors for neural networks
  trained from scratch.
\newblock \emph{arXiv preprint arXiv:2106.08970}, 2021.

\bibitem[Steinhardt et~al.(2017)Steinhardt, Koh, and
  Liang]{Steinhardt17certified}
Steinhardt, J., Koh, P.~W., and Liang, P.
\newblock Certified defenses for data poisoning attacks, 2017.

\bibitem[Tao et~al.(2021)Tao, Feng, Yi, Huang, and Chen]{tao2021better}
Tao, L., Feng, L., Yi, J., Huang, S.-J., and Chen, S.
\newblock Better safe than sorry: Preventing delusive adversaries with
  adversarial training.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Tran et~al.(2018)Tran, Li, and Madry]{tran2018spectral}
Tran, B., Li, J., and Madry, A.
\newblock Spectral signatures in backdoor attacks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8000--8010, 2018.

\bibitem[Turner et~al.(2018)Turner, Tsipras, and Madry]{turner2018clean}
Turner, A., Tsipras, D., and Madry, A.
\newblock Clean-label backdoor attacks.
\newblock 2018.

\bibitem[Veldanda \& Garg(2020)Veldanda and Garg]{veldanda2020evaluating}
Veldanda, A. and Garg, S.
\newblock On evaluating neural network backdoor defenses.
\newblock \emph{arXiv preprint arXiv:2010.12186}, 2020.

\bibitem[Weber et~al.(2020)Weber, Xu, Karla{\v{s}}, Zhang, and
  Li]{weber2020rab}
Weber, M., Xu, X., Karla{\v{s}}, B., Zhang, C., and Li, B.
\newblock Rab: Provable robustness against backdoor attacks.
\newblock \emph{arXiv preprint arXiv:2003.08904}, 2020.

\bibitem[Wolsey(1982)]{wolsey1982analysis}
Wolsey, L.~A.
\newblock An analysis of the greedy algorithm for the submodular set covering
  problem.
\newblock \emph{Combinatorica}, 2\penalty0 (4):\penalty0 385--393, 1982.

\bibitem[Zhu et~al.(2019)Zhu, Huang, Li, Taylor, Studer, and
  Goldstein]{zhu2019transferable}
Zhu, C., Huang, W.~R., Li, H., Taylor, G., Studer, C., and Goldstein, T.
\newblock Transferable clean-label poisoning attacks on deep neural nets.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7614--7623, 2019.

\end{thebibliography}
