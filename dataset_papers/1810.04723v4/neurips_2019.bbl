\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2009)Agarwal, Wainwright, Bartlett, and
  Ravikumar]{AgarwalBartlettRavikumarEtAl}
Alekh Agarwal, Martin~J Wainwright, Peter~L Bartlett, and Pradeep~K Ravikumar.
\newblock Information-theoretic lower bounds on the oracle complexity of convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1--9, 2009.

\bibitem[Bertsekas(1999)]{Bertsekas1999}
D.P. Bertsekas.
\newblock \emph{{Nonlinear Programming}}.
\newblock Athena Scientific, 1999.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{bottou2016optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv:1606.04838}, 2016.

\bibitem[Boyd and Vandenberghe(2004)]{BoydVandenberghe2004}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{{Convex Optimization}}.
\newblock Cambridge University Press, 2004.

\bibitem[Cover and Thomas(1991)]{CoverThomas1991}
Thomas~M. Cover and Joy~A. Thomas.
\newblock \emph{{Elements of Information Theory}}.
\newblock Wiley-Interscience, New York, NY, USA, 1991.
\newblock ISBN 0-471-06259-6.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{NIPS}, pages 1646--1654, 2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock {Adaptive subgradient methods for online learning and stochastic
  optimization}.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Gower et~al.(2019)Gower, Loizou, Qian, Sailanbayev, Shulgin, and
  Richtarik]{gower2019sgd}
Robert~Mansel Gower, Nicolas Loizou, Xun Qian, Alibek Sailanbayev, Egor
  Shulgin, and Peter Richtarik.
\newblock Sgd: General analysis and improved rates.
\newblock \emph{arXiv preprint arXiv:1901.09401}, 2019.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, and Friedman]{ESL}
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
\newblock \emph{The Elements of Statistical Learning: Data Mining, Inference,
  and Prediction}.
\newblock Springer Series in Statistics, 2nd edition, 2009.

\bibitem[Johnson and Zhang(2013)]{SVRG}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, pages 315--323, 2013.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock {Adam: A method for stochastic optimization}.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{SAG}
Nicolas Le~Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{NIPS}, pages 2663--2671, 2012.

\bibitem[Leblond et~al.(2018)Leblond, Pederegosa, and
  Lacoste-Julien]{Leblond2018}
R{\'e}mi Leblond, Fabian Pederegosa, and Simon Lacoste-Julien.
\newblock Improved asynchronous parallel optimization analysis for stochastic
  incremental methods.
\newblock \emph{arXiv preprint arXiv:1801.03749}, 2018.

\bibitem[LeCam et~al.(1973)]{LeCamothers1973}
Lucien LeCam et~al.
\newblock {Convergence of estimates under dimensionality restrictions}.
\newblock \emph{The Annals of Statistics}, 1\penalty0 (1):\penalty0 38--53,
  1973.

\bibitem[Moulines and Bach(2011)]{Moulines2011}
Eric Moulines and Francis~R Bach.
\newblock {Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  451--459, 2011.

\bibitem[Nemirovsky~A.S. and IUdin(c1983.)]{NemirovskyYudin1983}
Arkadii~Semenovich. Nemirovsky~A.S. and D.~B. IUdin.
\newblock \emph{Problem complexity and method efficiency in optimization /}.
\newblock Wiley,, Chichester ;, c1983.
\newblock "A Wiley-Interscience publication.".

\bibitem[Nesterov(2004)]{nesterov2004}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization : a basic course}.
\newblock Applied optimization. Kluwer Academic Publ., Boston, Dordrecht,
  London, 2004.
\newblock ISBN 1-4020-7553-7.

\bibitem[Nguyen et~al.(2018{\natexlab{a}})Nguyen, Nguyen, Richtarik,
  Scheinberg, Takac, and van Dijk]{Nguyen2018_NewAspectsSGD}
Lam Nguyen, Phuong~Ha Nguyen, Peter Richtarik, Katya Scheinberg, Martin Takac,
  and Marten van Dijk.
\newblock New convergence aspects of stochastic gradient algorithms.
\newblock \emph{arXiv preprint arXiv:1811.12403}, 2018{\natexlab{a}}.

\bibitem[Nguyen et~al.(2018{\natexlab{b}})Nguyen, Nguyen, van Dijk, Richtarik,
  Scheinberg, and Takac]{Nguyen2018}
Lam Nguyen, Phuong~Ha Nguyen, Marten van Dijk, Peter Richtarik, Katya
  Scheinberg, and Martin Takac.
\newblock {SGD} and hogwild! {C}onvergence without the bounded gradients
  assumption.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{Nguyen2017_sarah}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{ICML}, 2017.

\bibitem[Nocedal and Wright(2006)]{Nocedal2006NO}
Jorge Nocedal and Stephen~J. Wright.
\newblock \emph{Numerical Optimization}.
\newblock Springer, New York, 2nd edition, 2006.

\bibitem[Raginsky and Rakhlin(2011)]{RaginskyRakhlin2011}
Maxim Raginsky and Alexander Rakhlin.
\newblock {Information-Based Complexity, Feedback and Dynamics in Convex
  Programming}.
\newblock \emph{{IEEE} Trans. Information Theory}, 57\penalty0 (10):\penalty0
  7036--7056, 2011.

\bibitem[Robbins and Monro(1951)]{RM1951}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 1951.

\bibitem[Schmidt and Roux(2013)]{schmidt2013fast}
Mark Schmidt and Nicolas~Le Roux.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition.
\newblock \emph{arXiv preprint arXiv:1308.6370}, 2013.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock {On the importance of initialization and momentum in deep learning}.
\newblock In \emph{International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem[Yu(1997)]{Yu1997}
Bin Yu.
\newblock {Assouad, Fano, and Le Cam}.
\newblock In \emph{Festschrift for Lucien Le Cam}, pages 423--435. Springer,
  1997.

\bibitem[Zou et~al.(2018)Zou, Shen, Jie, Zhang, and Liu]{zou2018sufficient}
Fangyu Zou, Li~Shen, Zequn Jie, Weizhong Zhang, and Wei Liu.
\newblock {A Sufficient Condition for Convergences of Adam and RMSProp}.
\newblock \emph{arXiv preprint arXiv:1811.09358}, 2018.

\end{thebibliography}
