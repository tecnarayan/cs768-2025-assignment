\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bickel \& Freedman(1981)Bickel and Freedman]{bickel1981some}
Bickel, P.~J. and Freedman, D.~A.
\newblock Some asymptotic theory for the bootstrap.
\newblock \emph{The annals of statistics}, pp.\  1196--1217, 1981.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesv{\'a}ri, and
  Schuurmans]{dai2020coindice}
Dai, B., Nachum, O., Chow, Y., Li, L., Szepesv{\'a}ri, C., and Schuurmans, D.
\newblock Coindice: Off-policy confidence interval estimation.
\newblock \emph{arXiv preprint arXiv:2010.11652}, 2020.

\bibitem[Duan \& Wang(2020)Duan and Wang]{duan2020minimax}
Duan, Y. and Wang, M.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock \emph{Internation Conference on Machine Learning}, 2020.

\bibitem[Eck(2018)]{eck2018bootstrapping}
Eck, D.~J.
\newblock Bootstrapping for multivariate linear regression models.
\newblock \emph{Statistics \& Probability Letters}, 134:\penalty0 141--149,
  2018.

\bibitem[Efron(1982)]{efron1982jackknife}
Efron, B.
\newblock \emph{The jackknife, the bootstrap and other resampling plans}.
\newblock SIAM, 1982.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 503--556, 2005.

\bibitem[Feng et~al.(2020)Feng, Ren, Tang, and Liu]{feng2020accountable}
Feng, Y., Ren, T., Tang, Z., and Liu, Q.
\newblock Accountable off-policy evaluation with kernel bellman statistics.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Feng et~al.(2021)Feng, Tang, Zhang, and Liu]{feng2021nonasymptotic}
Feng, Y., Tang, Z., Zhang, N., and Liu, Q.
\newblock Non-asymptotic confidence intervals of off-policy evaluation: Primal
  and dual bounds.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=dKg5D1Z1Lm}.

\bibitem[Fonteneau et~al.(2013)Fonteneau, Murphy, Wehenkel, and
  Ernst]{fonteneau2013batch}
Fonteneau, R., Murphy, S.~A., Wehenkel, L., and Ernst, D.
\newblock Batch mode reinforcement learning based on the synthesis of
  artificial trajectories.
\newblock \emph{Annals of operations research}, 208\penalty0 (1):\penalty0
  383--416, 2013.

\bibitem[Freedman et~al.(1981)]{freedman1981bootstrapping}
Freedman, D.~A. et~al.
\newblock Bootstrapping regression models.
\newblock \emph{The Annals of Statistics}, 9\penalty0 (6):\penalty0 1218--1228,
  1981.

\bibitem[Hallak \& Mannor(2017)Hallak and Mannor]{hallak2017consistent}
Hallak, A. and Mannor, S.
\newblock Consistent on-line off-policy evaluation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  1372--1383. JMLR. org, 2017.

\bibitem[Hanna et~al.(2017)Hanna, Stone, and Niekum]{hanna2017bootstrapping}
Hanna, J.~P., Stone, P., and Niekum, S.
\newblock Bootstrapping with models: Confidence intervals for off-policy
  evaluation.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem[Hao et~al.(2020{\natexlab{a}})Hao, Abbasi-Yadkori, Wen, and
  Cheng]{hao2019bootstrapping}
Hao, B., Abbasi-Yadkori, Y., Wen, Z., and Cheng, G.
\newblock Bootstrapping upper confidence bound.
\newblock \emph{Thirty-fourth Annual Conference on Neural Information
  Processing Systems}, 2020{\natexlab{a}}.

\bibitem[Hao et~al.(2020{\natexlab{b}})Hao, Duan, Lattimore, Szepesv{\'a}ri,
  and Wang]{hao2020sparse}
Hao, B., Duan, Y., Lattimore, T., Szepesv{\'a}ri, C., and Wang, M.
\newblock Sparse feature selection makes batch reinforcement learning more
  sample efficient.
\newblock \emph{arXiv preprint arXiv:2011.04019}, 2020{\natexlab{b}}.

\bibitem[Jiang \& Huang(2020)Jiang and Huang]{jiang2020minimax}
Jiang, N. and Huang, J.
\newblock Minimax value interval for off-policy evaluation and policy
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  652--661, 2016.

\bibitem[Kallus \& Uehara(2020)Kallus and Uehara]{kallus2020double}
Kallus, N. and Uehara, M.
\newblock Double reinforcement learning for efficient off-policy evaluation in
  markov decision processes.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (167):\penalty0 1--63, 2020.

\bibitem[Kato(2011)]{kato2011note}
Kato, K.
\newblock A note on moment convergence of bootstrap m-estimators.
\newblock \emph{Statistics \& Risk Modeling}, 28\penalty0 (1):\penalty0 51--61,
  2011.

\bibitem[Kleiner et~al.(2014)Kleiner, Talwalkar, Sarkar, and
  Jordan]{kleiner2014scalable}
Kleiner, A., Talwalkar, A., Sarkar, P., and Jordan, M.~I.
\newblock A scalable bootstrap for massive data.
\newblock \emph{Journal of the Royal Statistical Society: Series B: Statistical
  Methodology}, pp.\  795--816, 2014.

\bibitem[Kostrikov \& Nachum(2020)Kostrikov and
  Nachum]{kostrikov2020statistical}
Kostrikov, I. and Nachum, O.
\newblock Statistical bootstrapping for uncertainty estimation in off-policy
  evaluation.
\newblock \emph{arXiv preprint arXiv:2007.13609}, 2020.

\bibitem[Kuzborskij et~al.(2020)Kuzborskij, Vernade, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{kuzborskij2020confident}
Kuzborskij, I., Vernade, C., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C.
\newblock Confident off-policy evaluation and selection through self-normalized
  importance weighting.
\newblock \emph{arXiv preprint arXiv:2006.10460}, 2020.

\bibitem[Lagoudakis \& Parr(2003)Lagoudakis and Parr]{lagoudakis2003least}
Lagoudakis, M.~G. and Parr, R.
\newblock Least-squares policy iteration.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Dec):\penalty0 1107--1149, 2003.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{le2019batch}
Le, H.~M., Voloshin, C., and Yue, Y.
\newblock Batch policy learning under constraints.
\newblock \emph{Proceedings of Machine Learning Research}, 97:\penalty0
  3703--3712, 2019.

\bibitem[Liao et~al.(2019)Liao, Klasnja, and Murphy]{liao2019off}
Liao, P., Klasnja, P., and Murphy, S.
\newblock Off-policy estimation of long-term average outcomes with applications
  to mobile health.
\newblock \emph{arXiv preprint arXiv:1912.13088}, 2019.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5356--5366, 2018.

\bibitem[McLeish et~al.(1974)]{mcleish1974dependent}
McLeish, D.~L. et~al.
\newblock Dependent central limit theorems and invariance principles.
\newblock \emph{the Annals of Probability}, 2\penalty0 (4):\penalty0 620--628,
  1974.

\bibitem[Moore(1990)]{moore1990efficient}
Moore, A.~W.
\newblock Efficient memory-based learning for robot control.
\newblock 1990.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock {DualDICE}: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2315--2325, 2019.

\bibitem[Oberst \& Sontag(2019)Oberst and Sontag]{oberst2019counterfactual}
Oberst, M. and Sontag, D.
\newblock Counterfactual off-policy evaluation with gumbel-max structural
  causal models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4881--4890, 2019.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{paine2020hyperparameter}
Paine, T.~L., Paduraru, C., Michi, A., Gulcehre, C., Zolna, K., Novikov, A.,
  Wang, Z., and de~Freitas, N.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Petersen \& Pedersen(2008)Petersen and Pedersen]{petersen2008matrix}
Petersen, K. and Pedersen, M.
\newblock The matrix cookbook. technical university of denmark.
\newblock \emph{Technical Manual}, 2008.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Precup, D., Sutton, R.~S., and Singh, S.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{ICML'00 Proceedings of the Seventeenth International
  Conference on Machine Learning}, 2000.

\bibitem[Sengupta et~al.(2016)Sengupta, Volgushev, and
  Shao]{sengupta2016subsampled}
Sengupta, S., Volgushev, S., and Shao, X.
\newblock A subsampled double bootstrap for massive data.
\newblock \emph{Journal of the American Statistical Association}, 111\penalty0
  (515):\penalty0 1222--1232, 2016.

\bibitem[Shi et~al.(2020)Shi, Zhang, Lu, and Song]{shi2020statistical}
Shi, C., Zhang, S., Lu, W., and Song, R.
\newblock Statistical inference of the value function for reinforcement
  learning in infinite horizon settings.
\newblock \emph{arXiv preprint arXiv:2001.04515}, 2020.

\bibitem[Singh(1981)]{singh1981asymptotic}
Singh, K.
\newblock On the asymptotic accuracy of efron's bootstrap.
\newblock \emph{The Annals of Statistics}, pp.\  1187--1195, 1981.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2139--2148, 2016.

\bibitem[Thomas et~al.(2015)Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015high}
Thomas, P., Theocharous, G., and Ghavamzadeh, M.
\newblock High confidence policy improvement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2380--2388. PMLR, 2015.

\bibitem[Uehara \& Jiang(2019)Uehara and Jiang]{uehara2019minimax}
Uehara, M. and Jiang, N.
\newblock Minimax weight and {Q}-function learning for off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:1910.12809}, 2019.

\bibitem[Van~der Vaart(2000)]{van2000asymptotic}
Van~der Vaart, A.~W.
\newblock \emph{Asymptotic statistics}, volume~3.
\newblock Cambridge university press, 2000.

\bibitem[Voloshin et~al.(2019)Voloshin, Le, Jiang, and
  Yue]{voloshin2019empirical}
Voloshin, C., Le, H.~M., Jiang, N., and Yue, Y.
\newblock Empirical study of off-policy policy evaluation for reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1911.06854}, 2019.

\bibitem[Wang et~al.(2020)Wang, Foster, and Kakade]{wang2020statistical}
Wang, R., Foster, D.~P., and Kakade, S.~M.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Xie, T., Ma, Y., and Wang, Y.-X.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  9665--9675, 2019.

\bibitem[Yin \& Wang(2020)Yin and Wang]{yin2020asymptotically}
Yin, M. and Wang, Y.-X.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Dai, Li, and
  Schuurmans]{zhang2020gendice}
Zhang, R., Dai, B., Li, L., and Schuurmans, D.
\newblock {GenDICE}: Generalized offline estimation of stationary values.
\newblock \emph{arXiv preprint arXiv:2002.09072}, 2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Liu, and
  Whiteson]{zhang2020gradientdice}
Zhang, S., Liu, B., and Whiteson, S.
\newblock {GradientDICE}: Rethinking generalized offline estimation of
  stationary values.
\newblock \emph{arXiv preprint arXiv:2001.11113}, 2020{\natexlab{b}}.

\end{thebibliography}
