@inproceedings{glue,
     title={{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
     author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
     booktitle={ICLR},
     year={2019}
 }

@article{cola,
    title={Neural Network Acceptability Judgments},
    author={Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R},
    journal={arXiv preprint arXiv:1805.12471},
    year={2018}
}

@InProceedings{mnli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "NAACL", 
  year = "2018",
}

@inproceedings{sst2,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={EMNLP},
  year={2013}
}

@inproceedings{mrpc,
  title={Automatically constructing a corpus of sentential paraphrases},
  author={Dolan, William B and Brockett, Chris},
  booktitle={Proceedings of the Third International Workshop on Paraphrasing},
  year={2005}
}

@article{qnli,
  title={{SQuAD}: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{rte,
  title={The {PASCAL} recognising textual entailment challenge},
  author={Dagan, Ido and Glickman, Oren and Magnini, Bernardo},
  booktitle={Machine Learning Challenges Workshop},
  year={2005},
}

@misc{qqp,
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},
  title = {First quora dataset release: Question pairs},
  url = {https://www.quora.com/q/quoradata/First-Quora-Dataset-Release-Question-Pairs},
  month = {January},
  year = {2017}
}

@inproceedings{bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={NAACL},
  year={2019}
}

@article{albert,
  title={{ALBERT}: A lite {BERT} for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{bert-pkd,
  title={Patient knowledge distillation for {BERT} model compression},
  author={Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
  journal={arXiv preprint arXiv:1908.09355},
  year={2019}
}

@article{distil-bert,
  title={{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{summarization,
  title={Text summarization with pretrained encoders},
  author={Liu, Yang and Lapata, Mirella},
  journal={arXiv preprint arXiv:1908.08345},
  year={2019}
}

@article{biomine,
  title={{BioBERT}: {P}re-trained biomedical language representation model for biomedical text mining},
  author={Lee, Jinhyuk and Yoon, Wonjin and Kim, Sungdong and Kim, Donghyeon and Kim, Sunkyu and So, Chan Ho and Kang, Jaewoo},
  journal={Bioinformatics},
  year={2019}
}

@inproceedings{prune,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={NIPS},
  year={2015}
}

@article{quantize,
  title={Compressing deep convolutional networks using vector quantization},
  author={Gong, Yunchao and Liu, Liu and Yang, Ming and Bourdev, Lubomir},
  journal={arXiv preprint arXiv:1412.6115},
  year={2014}
}

@inproceedings{svd,
  title={Predicting parameters in deep learning},
  author={Denil, Misha and Shakibi, Babak and Dinh, Laurent and Ranzato, Marc'Aurelio and De Freitas, Nando},
  booktitle={NIPS},
  year={2013}
}

@article{head-prune,
  title={Are Sixteen Heads Really Better than One?},
  author={Michel, Paul and Levy, Omer and Neubig, Graham},
  journal={arXiv preprint arXiv:1905.10650},
  year={2019}
}

@article{wang2019structured,
    title={Structured Pruning of Large Language Models},
    author={Ziheng Wang and Jeremy Wohlwend and Tao Lei},
    year={2019},
    journal={arxiv preprint: 1910.04732},
}

@article{shen2019qbert,
    title={{Q-BERT}: Hessian Based Ultra Low Precision Quantization of {BERT}},
    author={Sheng Shen and Zhen Dong and Jiayu Ye and Linjian Ma and Zhewei Yao and Amir Gholami and Michael W. Mahoney and Kurt Keutzer},
    year={2019},
    journal = {arxiv preprint:1909.05840},
}

@article{zafrir2019q8bert,
    title={{Q8BERT}: Quantized 8Bit {BERT}},
    author={Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
    year={2019},
    journal={arxiv preprint:1910.06188},
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={ICCV},
  year={2017}
}

@article{molchanov2017variational,
    title={Variational Dropout Sparsifies Deep Neural Networks},
    author={Dmitry Molchanov and Arsenii Ashukha and Dmitry Vetrov},
    year={2017},
    journal={arxiv preprint: 1701.05369},
}

@article{molchanov2016pruning,
    title={Pruning Convolutional Neural Networks for Resource Efficient Inference},
    author={Pavlo Molchanov and Stephen Tyree and Tero Karras and Timo Aila and Jan Kautz},
    year={2016},
    journal={arxiv preprint: 1611.06440},
}

@article{kim2015compression,
    title={Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications},
    author={Yong-Deok Kim and Eunhyeok Park and Sungjoo Yoo and Taelim Choi and Lu Yang and Dongjun Shin},
    year={2015},
    journal={arxiv preprint: 1511.06530},
}

@article{liu2019improving,
    title={Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding},
    author={Xiaodong Liu and Pengcheng He and Weizhu Chen and Jianfeng Gao},
    year={2019},
    journal={arxiv preprint: 1904.09482},
}

@article{mccarley2019pruning,
    title={Pruning a {BERT}-based Question Answering Model},
    author={J. S. McCarley},
    year={2019},
    journal={arxiv preprint: 1910.06360},
}

@article{hinton2015distilling,
    title={Distilling the Knowledge in a Neural Network},
    author={Geoffrey Hinton and Oriol Vinyals and Jeff Dean},
    year={2015},
    journal={arxiv preprint: 1503.02531},
}

@article{sau2016deep,
    title={Deep Model Compression: Distilling Knowledge from Noisy Teachers},
    author={Bharat Bhusan Sau and Vineeth N. Balasubramanian},
    year={2016},
    journal={arxiv preprint: 1610.09650},
}

@inproceedings{lai-etal-2017-race,
    title = "{RACE}: Large-scale {R}e{A}ding {C}omprehension Dataset From {E}xaminations",
    author = "Lai, Guokun  and
      Xie, Qizhe  and
      Liu, Hanxiao  and
      Yang, Yiming  and
      Hovy, Eduard",
    booktitle = "EMNLP",
    year = {2017},
}    

@InProceedings{maas-EtAlACL-HLT2011,
  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},
  title     = {Learning Word Vectors for Sentiment Analysis},
  booktitle = {ACL},
  year      = {2011},
}

@article{sparse-transformer,
    title={Explicit Sparse Transformer: Concentrated Attention Through Explicit Selection},
    author={Guangxiang Zhao and Junyang Lin and Zhiyuan Zhang and Xuancheng Ren and Qi Su and Xu Sun},
    year={2019},
    eprint={1912.11637},
    journal={arXiv,1912.11637},
    primaryClass={cs.CL}
}

@article{Correia_2019,
   title={Adaptively Sparse Transformers},
   url={http://dx.doi.org/10.18653/v1/d19-1223},
   DOI={10.18653/v1/d19-1223},
   journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
   publisher={Association for Computational Linguistics},
   author={Correia, Gonçalo M. and Niculae, Vlad and Martins, André F. T.},
   year={2019}
}

@article{Peters_2019,
   title={Sparse Sequence-to-Sequence Models},
   url={http://dx.doi.org/10.18653/v1/p19-1146},
   DOI={10.18653/v1/p19-1146},
   journal={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
   publisher={Association for Computational Linguistics},
   author={Peters, Ben and Niculae, Vlad and Martins, André F. T.},
   year={2019}
}

@article{martins2016softmax,
    title={From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification},
    author={André F. T. Martins and Ramón Fernandez Astudillo},
    year={2016},
    eprint={1602.02068},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC^2 Workshop},
  year={2019}
}

@misc{BERT-PKD-Code,
    author = {Sun, Siqi and Cheng, Yu and Gan, Zhe and Liu, Jingjing},
    url = {"https://github.com/intersun/PKD-for-BERT-Model-Compression"},
    year = {2019},
}


@misc{DistilBERT-Code,
    author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
    url = {"https://github.com/huggingface/transformers/tree/master/examples/distillation"},
    year= {2019},
}

@misc{Head-Prune-Code,
    author={Michel, Paul and  Levy, Omer and Neubig, Graham},
    url       = {"https://github.com/pmichel31415/are-16-heads-really-better-than-1"},
    year= {2019}
}


@inproceedings{cer-etal-2017-semeval,
    title = "{S}em{E}val-2017 Task 1: Semantic Textual Similarity Multilingual and Crosslingual Focused Evaluation",
    author = "Cer, Daniel  and
      Diab, Mona  and
      Agirre, Eneko  and
      Lopez-Gazpio, I{\~n}igo  and
      Specia, Lucia",
    booktitle = "Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017)",
    month = aug,
    year = {2017},
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/S17-2001",
    doi = "10.18653/v1/S17-2001",
    pages = "1--14",
}

@article{lai2017large,
    title={RACE: Large-scale ReAding Comprehension Dataset From Examinations},
    author={Lai, Guokun and Xie, Qizhe and Liu, Hanxiao and Yang, Yiming and Hovy, Eduard},
    journal={arXiv preprint arXiv:1704.04683},  
    year={2017}
}

