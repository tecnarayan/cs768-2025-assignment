@misc{franceschi2017forward,
      title={Forward and Reverse Gradient-Based Hyperparameter Optimization}, 
      author={Luca Franceschi and Michele Donini and Paolo Frasconi and Massimiliano Pontil},
      year={2017},
      eprint={1703.01785},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{lorraine2020optimizing,
  title={Optimizing millions of hyperparameters by implicit differentiation},
  author={Lorraine, Jonathan and Vicol, Paul and Duvenaud, David},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1540--1552},
  year={2020},
  organization={PMLR}
}

@misc{grazzi2021convergence,
      title={Convergence Properties of Stochastic Hypergradients}, 
      author={Riccardo Grazzi and Massimiliano Pontil and Saverio Salzo},
      year={2021},
      eprint={2011.07122},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{shaban2019truncated,
  title={Truncated back-propagation for bilevel optimization},
  author={Shaban, Amirreza and Cheng, Ching-An and Hatch, Nathan and Boots, Byron},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={1723--1732},
  year={2019},
  organization={PMLR}
}

@misc{grazzi2020iteration,
      title={On the Iteration Complexity of Hypergradient Computation}, 
      author={Riccardo Grazzi and Luca Franceschi and Massimiliano Pontil and Saverio Salzo},
      year={2020},
      eprint={2006.16218},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{zhang2022revisiting,
  title={Revisiting and advancing fast adversarial training through the lens of bi-level optimization},
  author={Zhang, Yihua and Zhang, Guanhua and Khanduri, Prashant and Hong, Mingyi and Chang, Shiyu and Liu, Sijia},
  booktitle={International Conference on Machine Learning},
  pages={26693--26712},
  year={2022},
  organization={PMLR}
}

@article{borsos2020coresets,
  title={Coresets via bilevel optimization for continual learning and streaming},
  author={Borsos, Zal{\'a}n and Mutny, Mojmir and Krause, Andreas},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14879--14890},
  year={2020}
}

@article{zhou2022dataset,
  title={Dataset distillation using neural feature regression},
  author={Zhou, Yongchao and Nezhadarya, Ehsan and Ba, Jimmy},
  journal={arXiv preprint arXiv:2206.00719},
  year={2022}
}

@article{ren2021comprehensive,
  title={A comprehensive survey of neural architecture search: Challenges and solutions},
  author={Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Chen, Xiaojiang and Wang, Xin},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={4},
  pages={1--34},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@misc{liu2019darts,
      title={DARTS: Differentiable Architecture Search}, 
      author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
      year={2019},
      eprint={1806.09055},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{zhang2023advancing,
      title={Advancing Model Pruning via Bi-level Optimization}, 
      author={Yihua Zhang and Yuguang Yao and Parikshit Ram and Pu Zhao and Tianlong Chen and Mingyi Hong and Yanzhi Wang and Sijia Liu},
      year={2023},
      eprint={2210.04092},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{finn2017modelagnostic,
      title={Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks}, 
      author={Chelsea Finn and Pieter Abbeel and Sergey Levine},
      year={2017},
      eprint={1703.03400},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{rajeswaran2019metalearning,
      title={Meta-Learning with Implicit Gradients}, 
      author={Aravind Rajeswaran and Chelsea Finn and Sham Kakade and Sergey Levine},
      year={2019},
      eprint={1909.04630},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{fan2021signmaml,
      title={Sign-MAML: Efficient Model-Agnostic Meta-Learning by SignSGD}, 
      author={Chen Fan and Parikshit Ram and Sijia Liu},
      year={2021},
      eprint={2109.07497},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{falk1995bilevel,
  title={On bilevel programming, Part I: general nonlinear cases},
  author={Falk, James E and Liu, Jiming},
  journal={Mathematical Programming},
  volume={70},
  pages={47--72},
  year={1995},
  publisher={Springer}
}

@article{vicente1994descent,
  title={Descent approaches for quadratic bilevel programming},
  author={Vicente, Luis and Savard, Gilles and J{\'u}dice, Joaquim},
  journal={Journal of optimization theory and applications},
  volume={81},
  number={2},
  pages={379--399},
  year={1994},
  publisher={Springer}
}

@article{ishizuka1992double,
  title={Double penalty method for bilevel optimization problems},
  author={Ishizuka, Yo and Aiyoshi, Eitaro},
  journal={Annals of Operations Research},
  volume={34},
  number={1},
  pages={73--88},
  year={1992},
  publisher={Springer}
}

@misc{ghadimi2018approximation,
      title={Approximation Methods for Bilevel Programming}, 
      author={Saeed Ghadimi and Mengdi Wang},
      year={2018},
      eprint={1802.02246},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{hong2022twotimescale,
      title={A Two-Timescale Framework for Bilevel Optimization: Complexity Analysis and Application to Actor-Critic}, 
      author={Mingyi Hong and Hoi-To Wai and Zhaoran Wang and Zhuoran Yang},
      year={2022},
      eprint={2007.05170},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{chen2021tighter,
      title={Tighter Analysis of Alternating Stochastic Gradient Method for Stochastic Nested Problems}, 
      author={Tianyi Chen and Yuejiao Sun and Wotao Yin},
      year={2021},
      eprint={2106.13781},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{ji2022bilevel,
      title={Will Bilevel Optimizers Benefit from Loops}, 
      author={Kaiyi Ji and Mingrui Liu and Yingbin Liang and Lei Ying},
      year={2022},
      eprint={2205.14224},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ji2021bilevel,
      title={Bilevel Optimization: Convergence Analysis and Enhanced Design}, 
      author={Kaiyi Ji and Junjie Yang and Yingbin Liang},
      year={2021},
      eprint={2010.07962},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{chen2022singletimescale,
      title={A Single-Timescale Method for Stochastic Bilevel Optimization}, 
      author={Tianyi Chen and Yuejiao Sun and Quan Xiao and Wotao Yin},
      year={2022},
      eprint={2102.04671},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{khanduri2021nearoptimal,
      title={A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum}, 
      author={Prashant Khanduri and Siliang Zeng and Mingyi Hong and Hoi-To Wai and Zhaoran Wang and Zhuoran Yang},
      year={2021},
      eprint={2102.07367},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{yang2021provably,
      title={Provably Faster Algorithms for Bilevel Optimization}, 
      author={Junjie Yang and Kaiyi Ji and Yingbin Liang},
      year={2021},
      eprint={2106.04692},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{cutkosky2020momentumbased,
      title={Momentum-Based Variance Reduction in Non-Convex SGD}, 
      author={Ashok Cutkosky and Francesco Orabona},
      year={2020},
      eprint={1905.10018},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{dagréou2022framework,
      title={A framework for bilevel optimization that enables stochastic and global variance reduction algorithms}, 
      author={Mathieu Dagréou and Pierre Ablin and Samuel Vaiter and Thomas Moreau},
      year={2022},
      eprint={2201.13409},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{dagréou2023lower,
      title={A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization}, 
      author={Mathieu Dagréou and Thomas Moreau and Samuel Vaiter and Pierre Ablin},
      year={2023},
      eprint={2302.08766},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{fang2018spider,
      title={SPIDER: Near-Optimal Non-Convex Optimization via Stochastic Path Integrated Differential Estimator}, 
      author={Cong Fang and Chris Junchi Li and Zhouchen Lin and Tong Zhang},
      year={2018},
      eprint={1807.01695},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{defazio2014saga,
      title={SAGA: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives}, 
      author={Aaron Defazio and Francis Bach and Simon Lacoste-Julien},
      year={2014},
      eprint={1407.0202},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{nguyen2017sarah,
  title={SARAH: A novel method for machine learning problems using stochastic recursive gradient},
  author={Nguyen, Lam M and Liu, Jie and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
  booktitle={International Conference on Machine Learning},
  pages={2613--2621},
  year={2017},
  organization={PMLR}
}

@misc{vaswani2021painless,
      title={Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates}, 
      author={Sharan Vaswani and Aaron Mishkin and Issam Laradji and Mark Schmidt and Gauthier Gidel and Simon Lacoste-Julien},
      year={2021},
      eprint={1905.09997},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{loizou2021stochastic,
      title={Stochastic Polyak Step-size for SGD: An Adaptive Learning Rate for Fast Convergence}, 
      author={Nicolas Loizou and Sharan Vaswani and Issam Laradji and Simon Lacoste-Julien},
      year={2021},
      eprint={2002.10542},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{orvieto2022dynamics,
      title={Dynamics of SGD with Stochastic Polyak Stepsizes: Truly Adaptive Variants and Convergence to Exact Solution}, 
      author={Antonio Orvieto and Simon Lacoste-Julien and Nicolas Loizou},
      year={2022},
      eprint={2205.04583},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}

@misc{yu2023dataset,
      title={Dataset Distillation: A Comprehensive Review}, 
      author={Ruonan Yu and Songhua Liu and Xinchao Wang},
      year={2023},
      eprint={2301.07014},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wang2020dataset,
      title={Dataset Distillation}, 
      author={Tongzhou Wang and Jun-Yan Zhu and Antonio Torralba and Alexei A. Efros},
      year={2020},
      eprint={1811.10959},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2019convergence,
      title={On the Convergence of Stochastic Gradient Descent with Adaptive Stepsizes}, 
      author={Xiaoyu Li and Francesco Orabona},
      year={2019},
      eprint={1805.08114},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}

@misc{ward2021adagrad,
      title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes}, 
      author={Rachel Ward and Xiaoxia Wu and Leon Bottou},
      year={2021},
      eprint={1806.01811},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{vaswani2020adaptive,
  title={Adaptive gradient methods converge faster with over-parameterization (but you should do a line-search)},
  author={Vaswani, Sharan and Laradji, Issam and Kunstner, Frederik and Meng, Si Yi and Schmidt, Mark and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:2006.06835},
  year={2020}
}

@misc{sow2022convergence,
      title={On the Convergence Theory for Hessian-Free Bilevel Algorithms}, 
      author={Daouda Sow and Kaiyi Ji and Yingbin Liang},
      year={2022},
      eprint={2110.07004},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@inproceedings{shalev2007pegasos,
  title={Pegasos: Primal estimated sub-gradient solver for svm},
  author={Shalev-Shwartz, Shai and Singer, Yoram and Srebro, Nathan},
  booktitle={Proceedings of the 24th international conference on Machine learning},
  pages={807--814},
  year={2007}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@misc{huang2023biadam,
      title={BiAdam: Fast Adaptive Bilevel Optimization Methods}, 
      author={Feihu Huang and Junyi Li and Shangqian Gao},
      year={2023},
      eprint={2106.11396},
      archivePrefix={arXiv},
      primaryClass={math.OC}
}


@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@misc{loshchilov2019decoupled,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{luo2019adaptive,
      title={Adaptive Gradient Methods with Dynamic Bound of Learning Rate}, 
      author={Liangchen Luo and Yuanhao Xiong and Yan Liu and Xu Sun},
      year={2019},
      eprint={1902.09843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{wright2006numerical,
  title={Numerical optimization},
  author={Wright, Jorge Nocedal Stephen J},
  year={2006},
  publisher={springer publication}
}

@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}

@book{beck2017first,
  title={First-order methods in optimization},
  author={Beck, Amir},
  year={2017},
  publisher={SIAM}
}

@inproceedings{gower2019sgd,
  title={SGD: General analysis and improved rates},
  author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
  booktitle={International conference on machine learning},
  pages={5200--5209},
  year={2019},
  organization={PMLR}
}
