%% BioMed_Central_Bib_Style_v1.01

\begin{thebibliography}{10}
\providecommand{\url}[1]{[#1]}
\providecommand{\urlprefix}{}

\bibitem{williams:01}
Williams C, Seeger M: \textbf{Using the {N}ystr{\"o}m method to speed up kernel
  machines}. In \emph{NIPS} 2001.

\bibitem{rahimi:08}
Rahimi A, Recht B: \textbf{Weighted sums of random kitchen sinks: {R}eplacing
  minimization with randomization in learning}. In \emph{Adv in Neural
  Information Processing Systems (NIPS)} 2008.

\bibitem{yang:12}
Yang T, Li YF, Mahdavi M, Jin R, Zhou ZH: \textbf{Nystr{\"o}m {M}ethod vs
  {R}andom {F}ourier {F}eatures: {A} {T}heoretical and {E}mpirical
  {C}omparison}. In \emph{NIPS} 2012.

\bibitem{gittens:13}
Gittens A, Mahoney MW: \textbf{Revisiting the {N}ystr{\"o}m method for improved
  large-scale machine learning}. In \emph{ICML} 2013.

\bibitem{bach:13}
Bach F: \textbf{Sharp analysis of low-rank kernel approximations}. In
  \emph{COLT} 2013.

\bibitem{rahimi:07}
Rahimi A, Recht B: \textbf{Random {F}eatures for {L}arge-{S}cale {K}ernel
  {M}achines}. In \emph{Adv in Neural Information Processing Systems} 2007.

\bibitem{kakade:07}
Kakade S, Foster DP: \textbf{Multi-view {R}egression {V}ia {C}anonical
  {C}orrelation {A}nalysis}. In \emph{Computational Learning Theory (COLT)}
  2007.

\bibitem{hotelling:36}
Hotelling H: \textbf{Relations between two sets of variates}. \emph{Biometrika}
  1936, \textbf{28}:312--377.

\bibitem{hardoon:04}
Hardoon DR, Szedmak S, Shawe-Taylor J: \textbf{Canonical {C}orrelation
  {A}nalysis: {A}n {O}verview with {A}pplication to {L}earning {M}ethods}.
  \emph{Neural Comp} 2004, \textbf{16}(12):2639--2664.

\bibitem{ji:12}
Ji M, Yang T, Lin B, Jin R, Han J: \textbf{A {S}imple {A}lgorithm for
  {S}emi-supervised {L}earning with {I}mproved {G}eneralization {E}rror
  {B}ound}. In \emph{ICML} 2012.

\bibitem{belkin:06}
Belkin M, Niyogi P, Sindhwani V: \textbf{Manifold regularization: {A} geometric
  framework for learning from labeled and unlabeled examples}. \emph{JMLR}
  2006, \textbf{7}:2399--2434.

\bibitem{blum:98}
Blum A, Mitchell T: \textbf{Combining labeled and unlabeled data with
  co-training}. In \emph{COLT} 1998.

\bibitem{chaudhuri:09}
Chaudhuri K, Kakade SM, Livescu K, Sridharan K: \textbf{Multiview clustering
  via {C}anonical {C}orrelation {A}nalysis}. In \emph{ICML} 2009.

\bibitem{mcwilliams:12}
McWilliams B, Montana G: \textbf{Multi-view predictive partitioning in high
  dimensions}. \emph{Statistical Analysis and Data Mining} 2012,
  \textbf{5}:304--321.

\bibitem{drineas:05}
Drineas P, Mahoney MW: \textbf{On the {N}ystr{\"o}m {M}ethod for
  {A}pproximating a {G}ram {M}atrix for {I}mproved {K}ernel-{B}ased
  {L}earning}. \emph{JMLR} 2005, \textbf{6}:2153--2175.

\bibitem{avron:13}
Avron H, Boutsidis C, Toledo S, Zouzias A: \textbf{Efficient {D}imensionality
  {R}eduction for {C}anonical {C}orrelation {A}nalysis}. In \emph{ICML} 2013.

\bibitem{hsu:12}
Hsu D, Kakade S, Zhang T: \textbf{An {A}nalysis of {R}andom {D}esign {L}inear
  {R}egression}. In \emph{COLT} 2012.

\bibitem{dhillon:11}
Dhillon PS, Foster DP, Kakade SM, Ungar LH: \textbf{A Risk Comparison of
  Ordinary Least Squares vs Ridge Regression}. \emph{Journal of Machine
  Learning Research} 2013, \textbf{14}:1505--1511.

\bibitem{andrew:13}
Andrew G, Arora R, Bilmes J, Livescu K: \textbf{Deep {C}anonical {C}orrelation
  {A}nalysis}. In \emph{ICML} 2013.

\bibitem{kumar:12}
Kumar S, Mohri M, Talwalkar A: \textbf{Sampling methods for the {N}ystr{\"o}m
  method}. \emph{JMLR} 2012, \textbf{13}:981--1006.

\end{thebibliography}

\newcommand{\BMCxmlcomment}[1]{}

\BMCxmlcomment{

<refgrp>

<bibl id="B1">
  <title><p>Using the {N}ystr{\"o}m method to speed up kernel
  machines</p></title>
  <aug>
    <au><snm>Williams</snm><fnm>C</fnm></au>
    <au><snm>Seeger</snm><fnm>M</fnm></au>
  </aug>
  <source>NIPS</source>
  <pubdate>2001</pubdate>
</bibl>

<bibl id="B2">
  <title><p>Weighted sums of random kitchen sinks: {R}eplacing minimization
  with randomization in learning</p></title>
  <aug>
    <au><snm>Rahimi</snm><fnm>A</fnm></au>
    <au><snm>Recht</snm><fnm>B</fnm></au>
  </aug>
  <source>Adv in Neural Information Processing Systems (NIPS)</source>
  <pubdate>2008</pubdate>
</bibl>

<bibl id="B3">
  <title><p>Nystr{\"o}m {M}ethod vs {R}andom {F}ourier {F}eatures: {A}
  {T}heoretical and {E}mpirical {C}omparison</p></title>
  <aug>
    <au><snm>Yang</snm><fnm>T</fnm></au>
    <au><snm>Li</snm><fnm>YF</fnm></au>
    <au><snm>Mahdavi</snm><fnm>M</fnm></au>
    <au><snm>Jin</snm><fnm>R</fnm></au>
    <au><snm>Zhou</snm><fnm>ZH</fnm></au>
  </aug>
  <source>NIPS</source>
  <pubdate>2012</pubdate>
</bibl>

<bibl id="B4">
  <title><p>Revisiting the {N}ystr{\"o}m method for improved large-scale
  machine learning</p></title>
  <aug>
    <au><snm>Gittens</snm><fnm>A</fnm></au>
    <au><snm>Mahoney</snm><fnm>MW</fnm></au>
  </aug>
  <source>ICML</source>
  <pubdate>2013</pubdate>
</bibl>

<bibl id="B5">
  <title><p>Sharp analysis of low-rank kernel approximations</p></title>
  <aug>
    <au><snm>Bach</snm><fnm>F</fnm></au>
  </aug>
  <source>COLT</source>
  <pubdate>2013</pubdate>
</bibl>

<bibl id="B6">
  <title><p>Random {F}eatures for {L}arge-{S}cale {K}ernel
  {M}achines</p></title>
  <aug>
    <au><snm>Rahimi</snm><fnm>A</fnm></au>
    <au><snm>Recht</snm><fnm>B</fnm></au>
  </aug>
  <source>Adv in Neural Information Processing Systems</source>
  <pubdate>2007</pubdate>
</bibl>

<bibl id="B7">
  <title><p>Multi-view {R}egression {V}ia {C}anonical {C}orrelation
  {A}nalysis</p></title>
  <aug>
    <au><snm>Kakade</snm><fnm>S</fnm></au>
    <au><snm>Foster</snm><fnm>DP</fnm></au>
  </aug>
  <source>Computational Learning Theory (COLT)</source>
  <pubdate>2007</pubdate>
</bibl>

<bibl id="B8">
  <title><p>Relations between two sets of variates</p></title>
  <aug>
    <au><snm>Hotelling</snm><fnm>H</fnm></au>
  </aug>
  <source>Biometrika</source>
  <pubdate>1936</pubdate>
  <volume>28</volume>
  <fpage>312</fpage>
  <lpage>377</lpage>
</bibl>

<bibl id="B9">
  <title><p>Canonical {C}orrelation {A}nalysis: {A}n {O}verview with
  {A}pplication to {L}earning {M}ethods</p></title>
  <aug>
    <au><snm>Hardoon</snm><fnm>D R</fnm></au>
    <au><snm>Szedmak</snm><fnm>S</fnm></au>
    <au><snm>Shawe Taylor</snm><fnm>J</fnm></au>
  </aug>
  <source>Neural Comp</source>
  <pubdate>2004</pubdate>
  <volume>16</volume>
  <issue>12</issue>
  <fpage>2639</fpage>
  <lpage>2664</lpage>
</bibl>

<bibl id="B10">
  <title><p>A {S}imple {A}lgorithm for {S}emi-supervised {L}earning with
  {I}mproved {G}eneralization {E}rror {B}ound</p></title>
  <aug>
    <au><snm>Ji</snm><fnm>M</fnm></au>
    <au><snm>Yang</snm><fnm>T</fnm></au>
    <au><snm>Lin</snm><fnm>B</fnm></au>
    <au><snm>Jin</snm><fnm>R</fnm></au>
    <au><snm>Han</snm><fnm>J</fnm></au>
  </aug>
  <source>ICML</source>
  <pubdate>2012</pubdate>
</bibl>

<bibl id="B11">
  <title><p>Manifold regularization: {A} geometric framework for learning from
  labeled and unlabeled examples</p></title>
  <aug>
    <au><snm>Belkin</snm><fnm>M</fnm></au>
    <au><snm>Niyogi</snm><fnm>P</fnm></au>
    <au><snm>Sindhwani</snm><fnm>V</fnm></au>
  </aug>
  <source>JMLR</source>
  <pubdate>2006</pubdate>
  <volume>7</volume>
  <fpage>2399</fpage>
  <lpage>2434</lpage>
</bibl>

<bibl id="B12">
  <title><p>Combining labeled and unlabeled data with co-training</p></title>
  <aug>
    <au><snm>Blum</snm><fnm>A</fnm></au>
    <au><snm>Mitchell</snm><fnm>T</fnm></au>
  </aug>
  <source>COLT</source>
  <pubdate>1998</pubdate>
</bibl>

<bibl id="B13">
  <title><p>Multiview clustering via {C}anonical {C}orrelation
  {A}nalysis</p></title>
  <aug>
    <au><snm>Chaudhuri</snm><fnm>K</fnm></au>
    <au><snm>Kakade</snm><fnm>SM</fnm></au>
    <au><snm>Livescu</snm><fnm>K</fnm></au>
    <au><snm>Sridharan</snm><fnm>K</fnm></au>
  </aug>
  <source>ICML</source>
  <pubdate>2009</pubdate>
</bibl>

<bibl id="B14">
  <title><p>Multi-view predictive partitioning in high dimensions</p></title>
  <aug>
    <au><snm>McWilliams</snm><fnm>B</fnm></au>
    <au><snm>Montana</snm><fnm>G</fnm></au>
  </aug>
  <source>Statistical Analysis and Data Mining</source>
  <pubdate>2012</pubdate>
  <volume>5</volume>
  <fpage>304</fpage>
  <lpage>321</lpage>
</bibl>

<bibl id="B15">
  <title><p>On the {N}ystr{\"o}m {M}ethod for {A}pproximating a {G}ram {M}atrix
  for {I}mproved {K}ernel-{B}ased {L}earning</p></title>
  <aug>
    <au><snm>Drineas</snm><fnm>P</fnm></au>
    <au><snm>Mahoney</snm><fnm>MW</fnm></au>
  </aug>
  <source>JMLR</source>
  <pubdate>2005</pubdate>
  <volume>6</volume>
  <fpage>2153</fpage>
  <lpage>2175</lpage>
</bibl>

<bibl id="B16">
  <title><p>Efficient {D}imensionality {R}eduction for {C}anonical
  {C}orrelation {A}nalysis</p></title>
  <aug>
    <au><snm>Avron</snm><fnm>H</fnm></au>
    <au><snm>Boutsidis</snm><fnm>C</fnm></au>
    <au><snm>Toledo</snm><fnm>S</fnm></au>
    <au><snm>Zouzias</snm><fnm>A</fnm></au>
  </aug>
  <source>ICML</source>
  <pubdate>2013</pubdate>
</bibl>

<bibl id="B17">
  <title><p>An {A}nalysis of {R}andom {D}esign {L}inear
  {R}egression</p></title>
  <aug>
    <au><snm>Hsu</snm><fnm>D</fnm></au>
    <au><snm>Kakade</snm><fnm>S</fnm></au>
    <au><snm>Zhang</snm><fnm>T</fnm></au>
  </aug>
  <source>COLT</source>
  <pubdate>2012</pubdate>
</bibl>

<bibl id="B18">
  <title><p>A Risk Comparison of Ordinary Least Squares vs Ridge
  Regression</p></title>
  <aug>
    <au><snm>Dhillon</snm><fnm>PS</fnm></au>
    <au><snm>Foster</snm><fnm>DP</fnm></au>
    <au><snm>Kakade</snm><fnm>SM</fnm></au>
    <au><snm>Ungar</snm><fnm>LH</fnm></au>
  </aug>
  <source>Journal of Machine Learning Research</source>
  <pubdate>2013</pubdate>
  <volume>14</volume>
  <fpage>1505</fpage>
  <lpage>1511</lpage>
</bibl>

<bibl id="B19">
  <title><p>Deep {C}anonical {C}orrelation {A}nalysis</p></title>
  <aug>
    <au><snm>Andrew</snm><fnm>G</fnm></au>
    <au><snm>Arora</snm><fnm>R</fnm></au>
    <au><snm>Bilmes</snm><fnm>J</fnm></au>
    <au><snm>Livescu</snm><fnm>K</fnm></au>
  </aug>
  <source>ICML</source>
  <pubdate>2013</pubdate>
</bibl>

<bibl id="B20">
  <title><p>Sampling methods for the {N}ystr{\"o}m method</p></title>
  <aug>
    <au><snm>Kumar</snm><fnm>S</fnm></au>
    <au><snm>Mohri</snm><fnm>M</fnm></au>
    <au><snm>Talwalkar</snm><fnm>A</fnm></au>
  </aug>
  <source>JMLR</source>
  <pubdate>2012</pubdate>
  <volume>13</volume>
  <fpage>981</fpage>
  <lpage>1006</lpage>
</bibl>

</refgrp>
} % end of \BMCxmlcomment
