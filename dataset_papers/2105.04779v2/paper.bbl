\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Bhojanapalli et~al.(2020)Bhojanapalli, Yun, Rawat, Reddi, and
  Kumar]{pmlr-v119-bhojanapalli20a}
Bhojanapalli, S., Yun, C., Rawat, A.~S., Reddi, S., and Kumar, S.
\newblock Low-rank bottleneck in multi-head attention models.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  864--873. PMLR, 13--18
  Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/bhojanapalli20a.html}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Cho et~al.(2020)Cho, Lu, Schwenk, Hajishirzi, and
  Kembhavi]{cho-etal-2020-x}
Cho, J., Lu, J., Schwenk, D., Hajishirzi, H., and Kembhavi, A.
\newblock {X-LXMERT}: {P}aint, {C}aption and {A}nswer {Q}uestions with
  {M}ulti-{M}odal {T}ransformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  8785--8805, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.707}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-main.707}.

\bibitem[Fan et~al.(2019)Fan, Grave, and Joulin]{fan2019reducing}
Fan, A., Grave, E., and Joulin, A.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Goyal et~al.(2020)Goyal, Choudhury, Raje, Chakaravarthy, Sabharwal,
  and Verma]{goyal2020power}
Goyal, S., Choudhury, A.~R., Raje, S., Chakaravarthy, V., Sabharwal, Y., and
  Verma, A.
\newblock Power-bert: Accelerating bert inference via progressive word-vector
  elimination.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3690--3699. PMLR, 2020.

\bibitem[Gu et~al.(2018)Gu, Bradbury, Xiong, Li, and
  Socher]{gu2018nonautoregressive}
Gu, J., Bradbury, J., Xiong, C., Li, V.~O., and Socher, R.
\newblock Non-autoregressive neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1l8BtlCb}.

\bibitem[Hermann et~al.(2015)Hermann, Kocisky, Grefenstette, Espeholt, Kay,
  Suleyman, and Blunsom]{NIPS2015_afdec700}
Hermann, K.~M., Kocisky, T., Grefenstette, E., Espeholt, L., Kay, W., Suleyman,
  M., and Blunsom, P.
\newblock Teaching machines to read and comprehend.
\newblock In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett, R.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/afdec7005cc9f14302cd0474fd0f3c96-Paper.pdf}.

\bibitem[Huang et~al.(2018)Huang, Vaswani, Uszkoreit, Shazeer, Simon,
  Hawthorne, Dai, Hoffman, Dinculescu, and Eck]{huang2018music}
Huang, C.-Z.~A., Vaswani, A., Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne,
  C., Dai, A.~M., Hoffman, M.~D., Dinculescu, M., and Eck, D.
\newblock Music transformer.
\newblock \emph{arXiv preprint arXiv:1809.04281}, 2018.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{pmlr-v119-katharopoulos20a}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5156--5165. PMLR,
  13--18 Jul 2020.
\newblock URL \url{http://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{Kitaev2020Reformer:}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Lee et~al.(2018)Lee, Mansimov, and Cho]{lee-etal-2018-deterministic}
Lee, J., Mansimov, E., and Cho, K.
\newblock Deterministic non-autoregressive neural sequence modeling by
  iterative refinement.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1173--1182, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1149}.
\newblock URL \url{https://www.aclweb.org/anthology/D18-1149}.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis-etal-2020-bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L.
\newblock {BART}: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  7871--7880, Online, July 2020.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.703}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.acl-main.703}.

\bibitem[Liu et~al.(2020)Liu, Yan, Gong, Qi, Zhang, Jiao, Chen, Fu, Shou, Gong,
  et~al.]{liu2020glge}
Liu, D., Yan, Y., Gong, Y., Qi, W., Zhang, H., Jiao, J., Chen, W., Fu, J.,
  Shou, L., Gong, M., et~al.
\newblock Glge: A new general language generation evaluation benchmark.
\newblock \emph{arXiv preprint arXiv:2011.11928}, 2020.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and
  Lapata]{narayan-etal-2018-dont}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Don{'}t give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  1797--1807, Brussels, Belgium,
  October-November 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D18-1206}.
\newblock URL \url{https://www.aclweb.org/anthology/D18-1206}.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott-etal-2019-fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  pp.\  48--53, Minneapolis, Minnesota, June 2019. Association for
  Computational Linguistics.
\newblock \doi{10.18653/v1/N19-4009}.
\newblock URL \url{https://www.aclweb.org/anthology/N19-4009}.

\bibitem[Parmar et~al.(2018)Parmar, Vaswani, Uszkoreit, Kaiser, Shazeer, Ku,
  and Tran]{parmar2018image}
Parmar, N., Vaswani, A., Uszkoreit, J., Kaiser, L., Shazeer, N., Ku, A., and
  Tran, D.
\newblock Image transformer.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4055--4064. PMLR, 2018.

\bibitem[Qi et~al.(2020)Qi, Gong, Jiao, Yan, Liu, Chen, Tang, Li, Chen, Zhang,
  et~al.]{qi2020bang}
Qi, W., Gong, Y., Jiao, J., Yan, Y., Liu, D., Chen, W., Tang, K., Li, H., Chen,
  J., Zhang, R., et~al.
\newblock Bang: Bridging autoregressive and non-autoregressive generation with
  large scale pretraining.
\newblock \emph{arXiv preprint arXiv:2012.15525}, 2020.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar-etal-2016-squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock {SQ}u{AD}: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  2383--2392, Austin, Texas, November 2016.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D16-1264}.
\newblock URL \url{https://www.aclweb.org/anthology/D16-1264}.

\bibitem[Shazeer(2019)]{shazeer2019fast}
Shazeer, N.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{arXiv preprint arXiv:1911.02150}, 2019.

\bibitem[Shleifer \& Rush(2020)Shleifer and Rush]{shleifer2020pre}
Shleifer, S. and Rush, A.~M.
\newblock Pre-trained summarization distillation.
\newblock \emph{arXiv preprint arXiv:2010.13002}, 2020.

\bibitem[Tay et~al.(2020)Tay, Bahri, Yang, Metzler, and Juan]{tay2020sparse}
Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
\newblock Sparse sinkhorn attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9438--9447. PMLR, 2020.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2021long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=qVyeW-grC2k}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{NIPS2017_3f5ee243}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Vaswani et~al.(2018)Vaswani, Bengio, Brevdo, Chollet, Gomez, Gouws,
  Jones, Kaiser, Kalchbrenner, Parmar, Sepassi, Shazeer, and
  Uszkoreit]{vaswani-etal-2018-tensor2tensor}
Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A., Gouws, S., Jones,
  L., Kaiser, {\L}., Kalchbrenner, N., Parmar, N., Sepassi, R., Shazeer, N.,
  and Uszkoreit, J.
\newblock {T}ensor2{T}ensor for neural machine translation.
\newblock In \emph{Proceedings of the 13th Conference of the Association for
  Machine Translation in the {A}mericas (Volume 1: Research Track)}, pp.\
  193--199, Boston, MA, March 2018. Association for Machine Translation in the
  Americas.
\newblock URL \url{https://www.aclweb.org/anthology/W18-1819}.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Li, Khabsa, Fang, and
  Ma]{wang2020linformer}
Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Yang, Farrell, Kurth, and
  Williams]{wang2020hierarchical}
Wang, Y., Yang, C., Farrell, S., Kurth, T., and Williams, S.
\newblock Hierarchical roofline performance analysis for deep learning
  applications.
\newblock \emph{arXiv preprint arXiv:2009.05257}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Wei, and Brooks]{wang2019benchmarking}
Wang, Y.~E., Wei, G.-Y., and Brooks, D.
\newblock Benchmarking tpu, gpu, and cpu platforms for deep learning.
\newblock \emph{arXiv preprint arXiv:1907.10701}, 2019.

\bibitem[Williams et~al.(2009)Williams, Waterman, and
  Patterson]{williams2009roofline}
Williams, S., Waterman, A., and Patterson, D.
\newblock Roofline: an insightful visual performance model for multicore
  architectures.
\newblock \emph{Communications of the ACM}, 52\penalty0 (4):\penalty0 65--76,
  2009.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Le~Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le~Scao, T., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pp.\  38--45, Online,
  October 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.6}.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Yang et~al.(2020)Yang, Kurth, and Williams]{yang2020hierarchical}
Yang, C., Kurth, T., and Williams, S.
\newblock Hierarchical roofline analysis for gpus: Accelerating performance
  optimization for the nersc-9 perlmutter system.
\newblock \emph{Concurrency and Computation: Practice and Experience},
  32\penalty0 (20):\penalty0 e5547, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, and Ahmed]{NEURIPS2020_c8512d14}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon,
  S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A.
\newblock Big bird: Transformers for longer sequences.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  17283--17297. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/c8512d142a2d849725f31a9a7a361ab9-Paper.pdf}.

\end{thebibliography}
