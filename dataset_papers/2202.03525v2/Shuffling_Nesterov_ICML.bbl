\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow}: Large-scale machine learning on heterogeneous systems,
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ahn et~al.(2020)Ahn, Yun, and Sra]{ahn2020sgd}
Ahn, K., Yun, C., and Sra, S.
\newblock Sgd with shuffling: optimal rates without component convexity and
  large epoch requirements.
\newblock \emph{arXiv preprint arXiv:2006.06946}, 2020.

\bibitem[Bottou(2009)]{bottou2009curiously}
Bottou, L.
\newblock Curiously fast convergence of some stochastic gradient descent
  algorithms.
\newblock In \emph{Proceedings of the symposium on learning and data science,
  Paris}, volume~8, pp.\  2624--2633, 2009.

\bibitem[Bottou(2012)]{bottou2012stochastic}
Bottou, L.
\newblock Stochastic gradient descent tricks.
\newblock In \emph{Neural networks: Tricks of the trade}, pp.\  421--436.
  Springer, 2012.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and Nocedal]{Bottou2018}
Bottou, L., Curtis, F.~E., and Nocedal, J.
\newblock {O}ptimization {M}ethods for {L}arge-{S}cale {M}achine {L}earning.
\newblock \emph{SIAM Rev.}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Chang \& Lin(2011)Chang and Lin]{LIBSVM}
Chang, C.-C. and Lin, C.-J.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Chollet et~al.(2015)]{chollet2015keras}
Chollet, F. et~al.
\newblock Keras.
\newblock \emph{GitHub}, 2015.
\newblock URL \url{https://github.com/fchollet/keras}.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and Lacoste-Julien]{SAGA}
Defazio, A., Bach, F., and Lacoste-Julien, S.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1646--1654, 2014.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and
  Nesterov]{springer_devolder2014}
Devolder, O., Glineur, F., and Nesterov, Y.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Springer-Verlag}, 146\penalty0 (1–2), 2014.
\newblock ISSN 0025-5610.
\newblock \doi{10.1007/s10107-013-0677-5}.
\newblock URL \url{https://doi.org/10.1007/s10107-013-0677-5}.

\bibitem[Dozat(2016)]{dozat2016incorporating}
Dozat, T.
\newblock Incorporating nesterov momentum into {ADAM}.
\newblock \emph{ICLR Workshop}, 1:\penalty0 2013–--2016, 2016.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{AdaGrad}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12:\penalty0 2121--2159,
  2011.

\bibitem[Ghadimi \& Lan(2013)Ghadimi and Lan]{ghadimi2013stochastic}
Ghadimi, S. and Lan, G.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM J. Optim.}, 23\penalty0 (4):\penalty0 2341--2368, 2013.

\bibitem[Gürbüzbalaban et~al.(2019)Gürbüzbalaban, Ozdaglar, and
  Parrilo]{Gurbuzbalaban2019}
Gürbüzbalaban, M., Ozdaglar, A., and Parrilo, P.~A.
\newblock Why random reshuffling beats stochastic gradient descent.
\newblock \emph{Mathematical Programming}, Oct 2019.
\newblock ISSN 1436-4646.
\newblock \doi{10.1007/s10107-019-01440-w}.

\bibitem[Haochen \& Sra(2019)Haochen and Sra]{haochen2019random}
Haochen, J. and Sra, S.
\newblock Random shuffling beats sgd after finite epochs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2624--2633. PMLR, 2019.

\bibitem[Hu et~al.(2009)Hu, Pan, and Kwok]{NIPS_hu2009}
Hu, C., Pan, W., and Kwok, J.
\newblock Accelerated gradient methods for stochastic optimization and online
  learning.
\newblock In Bengio, Y., Schuurmans, D., Lafferty, J., Williams, C., and
  Culotta, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~22. Curran Associates, Inc., 2009.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2009/file/ec5aa0b7846082a2415f0902f0da88f2-Paper.pdf}.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{SVRG}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, pp.\  315--323, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Kingma2014}
Kingma, D.~P. and Ba, J.
\newblock {ADAM}: {A} {M}ethod for {S}tochastic {O}ptimization.
\newblock \emph{Proceedings of the 3rd International Conference on Learning
  Representations (ICLR)}, abs/1412.6980, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and Hinton]{CIFAR10}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Lan(2012)]{Lan2012}
Lan, G.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133:\penalty0 365--397, 2012.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{SAG}
Le~Roux, N., Schmidt, M., and Bach, F.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{NIPS}, pp.\  2663--2671, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{MNIST}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lessard et~al.(2016)Lessard, Recht, and Packard]{siam_lessard2016}
Lessard, L., Recht, B., and Packard, A.
\newblock Analysis and design of optimization algorithms via integral quadratic
  constraints.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  57--95, 2016.
\newblock \doi{10.1137/15M1009597}.
\newblock URL \url{https://doi.org/10.1137/15M1009597}.

\bibitem[Liu \& Belkin(2018)Liu and Belkin]{corr_liu2018}
Liu, C. and Belkin, M.
\newblock Mass: an accelerated stochastic method for over-parametrized
  learning.
\newblock \emph{CoRR}, abs/1810.13395, 2018.
\newblock URL \url{http://arxiv.org/abs/1810.13395}.

\bibitem[Mishchenko et~al.(2020)Mishchenko, Khaled Ragab~Bayoumi, and
  Richt{\'a}rik]{mishchenko2020random}
Mishchenko, K., Khaled Ragab~Bayoumi, A., and Richt{\'a}rik, P.
\newblock Random reshuffling: Simple analysis with vast improvements.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Mishchenko et~al.(2021)Mishchenko, Khaled, and
  Richtárik]{mishchenko2021proximal}
Mishchenko, K., Khaled, A., and Richtárik, P.
\newblock Proximal and federated random reshuffling, 2021.

\bibitem[Nagaraj et~al.(2019)Nagaraj, Jain, and Netrapalli]{nagaraj2019sgd}
Nagaraj, D., Jain, P., and Netrapalli, P.
\newblock Sgd without replacement: Sharper rates for general smooth convex
  functions.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4703--4711, 2019.

\bibitem[Nedic \& Bertsekas(2001{\natexlab{a}})Nedic and
  Bertsekas]{nedic2001convergence}
Nedic, A. and Bertsekas, D.
\newblock Convergence rate of incremental subgradient algorithms.
\newblock In \emph{Stochastic optimization: algorithms and applications}, pp.\
  223--264. Springer, 2001{\natexlab{a}}.

\bibitem[Nedic \& Bertsekas(2001{\natexlab{b}})Nedic and
  Bertsekas]{nedic2001incremental}
Nedic, A. and Bertsekas, D.~P.
\newblock Incremental subgradient methods for nondifferentiable optimization.
\newblock \emph{SIAM J. on Optim.}, 12\penalty0 (1):\penalty0 109--138,
  2001{\natexlab{b}}.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{Nemirovski2009}
Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM J. on Optimization}, 19\penalty0 (4):\penalty0 1574--1609,
  2009.

\bibitem[Nesterov(1983)]{Nesterov1983}
Nesterov, Y.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence $\mathcal{O}(1/k^2)$.
\newblock \emph{Doklady AN SSSR}, 269:\penalty0 543--547, 1983.
\newblock Translated as Soviet Math. Dokl.

\bibitem[Nesterov(2004)]{Nesterov2004}
Nesterov, Y.
\newblock \emph{{I}ntroductory lectures on convex optimization: {A} basic
  course}, volume~87 of \emph{Applied Optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nguyen et~al.(2018)Nguyen, Nguyen, van Dijk, Richtarik, Scheinberg,
  and Takac]{Nguyen2018_sgdhogwild}
Nguyen, L., Nguyen, P.~H., van Dijk, M., Richtarik, P., Scheinberg, K., and
  Takac, M.
\newblock {SGD} and {H}ogwild! convergence without the bounded gradients
  assumption.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning-Volume 80}, pp.\  3747--3755, 2018.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{Nguyen2017sarah}
Nguyen, L.~M., Liu, J., Scheinberg, K., and Tak{\'a}{\v{c}}, M.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  2613--2621. JMLR. org, 2017.

\bibitem[Nguyen et~al.(2021)Nguyen, Tran-Dinh, Phan, Nguyen, and van
  Dijk]{nguyen2020unified}
Nguyen, L.~M., Tran-Dinh, Q., Phan, D.~T., Nguyen, P.~H., and van Dijk, M.
\newblock A unified convergence analysis for shuffling-type gradient methods.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (207):\penalty0 1--44, 2021.

\bibitem[Nguyen et~al.(2022)Nguyen, Tran, and van Dijk]{Nguyen2022_OptDL}
Nguyen, L.~M., Tran, T.~H., and van Dijk, M.
\newblock Finite-sum optimization: A new perspective for convergence to a
  global solution.
\newblock \emph{arXiv preprint arXiv:2202.03524}, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{Polyak1992}
Polyak, B. and Juditsky, A.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM J. Control Optim.}, 30\penalty0 (4):\penalty0 838--855,
  1992.

\bibitem[Polyak(1964)]{Polyak1964}
Polyak, B.~T.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock \emph{{USSR} Computational Mathematics and Mathematical Physics},
  4\penalty0 (5):\penalty0 1--17, 1964.

\bibitem[Rajput et~al.(2020)Rajput, Gupta, and
  Papailiopoulos]{rajput2020closing}
Rajput, S., Gupta, A., and Papailiopoulos, D.
\newblock Closing the convergence gap of sgd without replacement.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7964--7973. PMLR, 2020.

\bibitem[Recht \& Ré(2011)Recht and Ré]{Recht2011}
Recht, B. and Ré, C.
\newblock Parallel stochastic gradient algorithms for large-scale matrix
  completion.
\newblock \emph{Mathematical Programming Computation}, 5, 04 2011.
\newblock \doi{10.1007/s12532-013-0053-8}.

\bibitem[Robbins \& Monro(1951)Robbins and Monro]{RM1951}
Robbins, H. and Monro, S.
\newblock A stochastic approximation method.
\newblock \emph{The Annals of Mathematical Statistics}, 22\penalty0
  (3):\penalty0 400--407, 1951.

\bibitem[Safran \& Shamir(2020)Safran and Shamir]{safran2020good}
Safran, I. and Shamir, O.
\newblock How good is sgd with random shuffling?
\newblock In \emph{Conference on Learning Theory}, pp.\  3250--3284. PMLR,
  2020.

\bibitem[Schmidt \& Roux(2013)Schmidt and Roux]{schmidt2013fast}
Schmidt, M. and Roux, N.~L.
\newblock Fast convergence of stochastic gradient descent under a strong growth
  condition, 2013.

\bibitem[Shamir(2016)]{shamir2016without}
Shamir, O.
\newblock Without-replacement sampling for stochastic gradient methods.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  46--54, 2016.

\bibitem[Shamir \& Zhang(2013)Shamir and Zhang]{pmlr-v28-shamir13}
Shamir, O. and Zhang, T.
\newblock Stochastic gradient descent for non-smooth optimization: Convergence
  results and optimal averaging schemes.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  71--79, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/shamir13.html}.

\bibitem[Sra et~al.(2012)Sra, Nowozin, and Wright]{sra2012optimization}
Sra, S., Nowozin, S., and Wright, S.~J.
\newblock \emph{{O}ptimization for {M}achine {L}earning}.
\newblock MIT Press, 2012.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{pmlr-v28-sutskever13}
Sutskever, I., Martens, J., Dahl, G., and Hinton, G.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In Dasgupta, S. and McAllester, D. (eds.), \emph{Proceedings of the
  30th International Conference on Machine Learning}, volume~28 of
  \emph{Proceedings of Machine Learning Research}, pp.\  1139--1147, Atlanta,
  Georgia, USA, 17--19 Jun 2013. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v28/sutskever13.html}.

\bibitem[Tran et~al.(2021)Tran, Nguyen, and Tran-Dinh]{pmlr-v139-tran21b}
Tran, T.~H., Nguyen, L.~M., and Tran-Dinh, Q.
\newblock {SMG}: A shuffling gradient-based method with momentum.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  10379--10389. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/tran21b.html}.

\bibitem[Vaswani et~al.(2019)Vaswani, Bach, and Schmidt]{pmlr-v89-vaswani19a}
Vaswani, S., Bach, F., and Schmidt, M.
\newblock Fast and faster convergence of sgd for over-parameterized models and
  an accelerated perceptron.
\newblock In Chaudhuri, K. and Sugiyama, M. (eds.), \emph{Proceedings of the
  Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of \emph{Proceedings of Machine Learning Research},
  pp.\  1195--1204. PMLR, 16--18 Apr 2019.
\newblock URL \url{https://proceedings.mlr.press/v89/vaswani19a.html}.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yuan et~al.(2016)Yuan, Ying, and Sayed]{JMLR_yuan2016}
Yuan, K., Ying, B., and Sayed, A.~H.
\newblock On the influence of momentum acceleration on online learning.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (192):\penalty0 1--66, 2016.
\newblock URL \url{http://jmlr.org/papers/v17/16-157.html}.

\bibitem[Zhong \& Kwok(2014)Zhong and Kwok]{pmlr-v33-zhong14}
Zhong, W. and Kwok, J.
\newblock {Accelerated Stochastic Gradient Method for Composite
  Regularization}.
\newblock In Kaski, S. and Corander, J. (eds.), \emph{Proceedings of the
  Seventeenth International Conference on Artificial Intelligence and
  Statistics}, volume~33 of \emph{Proceedings of Machine Learning Research},
  pp.\  1086--1094, Reykjavik, Iceland, 22--25 Apr 2014. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v33/zhong14.html}.

\end{thebibliography}
