\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alquier et~al.(2016)Alquier, Ridgway, and
  Chopin]{alquier2016properties}
Pierre Alquier, James Ridgway, and Nicolas Chopin.
\newblock On the properties of variational approximations of gibbs posteriors.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 8374--8414, 2016.

\bibitem[Bain(2021)]{bain2021visualizing}
Robert Bain.
\newblock Visualizing the loss landscape of winning lottery tickets.
\newblock \emph{arXiv preprint arXiv:2112.08538}, 2021.

\bibitem[Bartoldson et~al.(2020)Bartoldson, Morcos, Barbu, and
  Erlebacher]{bartoldson2020generalization}
Brian Bartoldson, Ari Morcos, Adrian Barbu, and Gordon Erlebacher.
\newblock The generalization-stability tradeoff in neural network pruning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 20852--20864, 2020.

\bibitem[Dinh et~al.(2017)Dinh, Pascanu, Bengio, and Bengio]{dinh2017sharp}
Laurent Dinh, Razvan Pascanu, Samy Bengio, and Yoshua Bengio.
\newblock Sharp minima can generalize for deep nets.
\newblock In \emph{International Conference on Machine Learning}, pages
  1019--1028. PMLR, 2017.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{In Proceedings of the Thirty-Third Conference on Uncertainty in
  Artificial Intelligence}, 2017.

\bibitem[Dziugaite et~al.(2021{\natexlab{a}})Dziugaite, Hsu, Gharbieh, Arpino,
  and Roy]{dziugaite2020role}
Gintare~Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and
  Daniel Roy.
\newblock On the role of data in pac-bayes.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 604--612. PMLR, 2021{\natexlab{a}}.

\bibitem[Dziugaite et~al.(2021{\natexlab{b}})Dziugaite, Hsu, Gharbieh, Arpino,
  and Roy]{dziugaite2021role}
Gintare~Karolina Dziugaite, Kyle Hsu, Waseem Gharbieh, Gabriel Arpino, and
  Daniel~M Roy.
\newblock On the role of data in pac-bayes bounds.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2021{\natexlab{b}}.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Frankle(2020)]{open_lth}
Jonathan Frankle.
\newblock Openlth: A framework for lottery tickets and beyond.
\newblock \url{https://github.com/facebookresearch/open_lth}, 2020.

\bibitem[Frankle and Carbin(2018)]{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{frankle2020linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{International Conference on Machine Learning}, pages
  3259--3269. PMLR, 2020.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{NIPS}, 2015.

\bibitem[Han et~al.(2017)Han, Pool, Narang, Mao, Gong, Tang, Elsen, Vajda,
  Paluri, Tran, et~al.]{han2016dsd}
Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian Tang, Erich
  Elsen, Peter Vajda, Manohar Paluri, John Tran, et~al.
\newblock Dsd: Dense-sparse-dense training for deep neural networks.
\newblock \emph{In 5th International Conference on Learning Representations,
  ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings.
  OpenReview.net}, 2017.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{hassibi1993optimal}
Babak Hassibi, David~G Stork, and Gregory~J Wolff.
\newblock Optimal brain surgeon and general network pruning.
\newblock In \emph{IEEE international conference on neural networks}, pages
  293--299. IEEE, 1993.

\bibitem[Hayou et~al.(2021)Hayou, He, and Dziugaite]{hayou2021probabilistic}
Soufiane Hayou, Bobby He, and Gintare~Karolina Dziugaite.
\newblock Probabilistic fine-tuning of pruning masks and pac-bayes self-bounded
  learning.
\newblock \emph{arXiv preprint arXiv:2110.11804}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2022)He, Zhu, and Qin]{he2022can}
Zheng He, Quanzhi Zhu, and Zengchang Qin.
\newblock Can network pruning benefit deep learning under label noise?
\newblock 2022.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997flat}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Flat minima.
\newblock \emph{Neural computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho*, and Geras*]{Jastrzebski2020The}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho*, and Krzysztof Geras*.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Jiang* et~al.(2020)Jiang*, Neyshabur*, Mobahi, Krishnan, and
  Bengio]{Jiang*2020Fantastic}
Yiding Jiang*, Behnam Neyshabur*, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{ICLR}, 2017.

\bibitem[Lawrence et~al.(1998)Lawrence, Giles, and Tsoi]{lawrence1998size}
Steve Lawrence, C~Lee Giles, and Ah~Chung Tsoi.
\newblock What size neural network gives optimal generalization? convergence
  properties of backpropagation.
\newblock Technical report, 1998.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun1989optimal}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock \emph{Advances in neural information processing systems}, 2, 1989.

\bibitem[Lewkowycz et~al.(2020)Lewkowycz, Bahri, Dyer, Sohl-Dickstein, and
  Gur-Ari]{lewkowycz2020large}
Aitor Lewkowycz, Yasaman Bahri, Ethan Dyer, Jascha Sohl-Dickstein, and Guy
  Gur-Ari.
\newblock The large learning rate phase of deep learning: the catapult
  mechanism.
\newblock 2020.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Yuanzhi Li, Colin Wei, and Tengyu Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Liu et~al.(2021)Liu, Yuan, Che, Shen, Ma, Jin, Ren, Tang, Liu, and
  Wang]{liu2021lottery}
Ning Liu, Geng Yuan, Zhengping Che, Xuan Shen, Xiaolong Ma, Qing Jin, Jian Ren,
  Jian Tang, Sijia Liu, and Yanzhi Wang.
\newblock Lottery ticket preserves weight correlation: Is it desirable or not?
\newblock In \emph{International Conference on Machine Learning}, pages
  7011--7020. PMLR, 2021.

\bibitem[Liu et~al.(2018)Liu, Sun, Zhou, Huang, and Darrell]{liu2018rethinking}
Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and Trevor Darrell.
\newblock Rethinking the value of network pruning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[MacKay et~al.(2003)MacKay, Mac~Kay, et~al.]{mackay2003information}
David~JC MacKay, David~JC Mac~Kay, et~al.
\newblock \emph{Information theory, inference and learning algorithms}.
\newblock Cambridge university press, 2003.

\bibitem[Malach et~al.(2020)Malach, Yehudai, Shalev-Schwartz, and
  Shamir]{malach2020proving}
Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In \emph{International Conference on Machine Learning}, pages
  6682--6691. PMLR, 2020.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{DBLP:journals/corr/NeyshaburTS14}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{ICLR (Workshop)}, 2015.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2018a}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and
  Rastegari]{Ramanujan_2020_CVPR}
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and
  Mohammad Rastegari.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Savarese et~al.(2020)Savarese, Silva, and Maire]{savarese2020winning}
Pedro Savarese, Hugo Silva, and Michael Maire.
\newblock Winning the lottery with continuous sparsification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 11380--11390, 2020.

\bibitem[Simonyan and Zisserman(2015)]{simonyan2015very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{ICLR}, 2015.

\bibitem[Tonolini et~al.(2020)Tonolini, Jensen, and
  Murray-Smith]{tonolini2020variational}
Francesco Tonolini, Bj{\o}rn~Sand Jensen, and Roderick Murray-Smith.
\newblock Variational sparse coding.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 690--700.
  PMLR, 2020.

\bibitem[Xie et~al.(2021{\natexlab{a}})Xie, He, Fu, Sato, Tao, and
  Sugiyama]{xie2021artificial}
Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng Tao, and Masashi
  Sugiyama.
\newblock Artificial neural variability for deep learning: on overfitting,
  noise memorization, and catastrophic forgetting.
\newblock \emph{Neural computation}, 33\penalty0 (8):\penalty0 2163--2192,
  2021{\natexlab{a}}.

\bibitem[Xie et~al.(2021{\natexlab{b}})Xie, Sato, and Sugiyama]{xie2021a}
Zeke Xie, Issei Sato, and Masashi Sugiyama.
\newblock A diffusion theory for deep learning dynamics: Stochastic gradient
  descent exponentially favors flat minima.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.

\bibitem[Yao et~al.(2018)Yao, Gholami, Lei, Keutzer, and
  Mahoney]{yao2018hessian}
Zhewei Yao, Amir Gholami, Qi~Lei, Kurt Keutzer, and Michael~W Mahoney.
\newblock Hessian-based analysis of large batch training and robustness to
  adversaries.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Yao et~al.(2020)Yao, Gholami, Keutzer, and Mahoney]{yao2020pyhessian}
Zhewei Yao, Amir Gholami, Kurt Keutzer, and Michael~W Mahoney.
\newblock Pyhessian: Neural networks through the lens of the hessian.
\newblock In \emph{2020 IEEE International Conference on Big Data (Big Data)},
  pages 581--590. IEEE, 2020.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{ICLR}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Wang, Liu, Chen, and Xiong]{zhang2021lottery}
Shuai Zhang, Meng Wang, Sijia Liu, Pin-Yu Chen, and Jinjun Xiong.
\newblock Why lottery ticket wins? a theoretical perspective of sample
  complexity on sparse neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
