\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Barham, Chen, Chen, Davis, Dean, Devin,
  Ghemawat, Irving, Isard, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In \emph{12th $\{$USENIX$\}$ symposium on operating systems design
  and implementation ($\{$OSDI$\}$ 16)}, pages 265--283, 2016.

\bibitem[Achille et~al.(2019{\natexlab{a}})Achille, Lam, Tewari, Ravichandran,
  Maji, Fowlkes, Soatto, and Perona]{achille2019task2vec}
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu
  Maji, Charless~C Fowlkes, Stefano Soatto, and Pietro Perona.
\newblock Task2vec: Task embedding for meta-learning.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 6430--6439, 2019{\natexlab{a}}.

\bibitem[Achille et~al.(2019{\natexlab{b}})Achille, Paolini, Mbeng, and
  Soatto]{achille2019information}
Alessandro Achille, Giovanni Paolini, Glen Mbeng, and Stefano Soatto.
\newblock The information complexity of learning tasks, their structure and
  their distance.
\newblock \emph{arXiv preprint arXiv:1904.03292}, 2019{\natexlab{b}}.

\bibitem[Argyriou et~al.(2008)Argyriou, Evgeniou, and
  Pontil]{argyriou2008convex}
Andreas Argyriou, Theodoros Evgeniou, and Massimiliano Pontil.
\newblock Convex multi-task feature learning.
\newblock \emph{Machine learning}, 73\penalty0 (3):\penalty0 243--272, 2008.

\bibitem[Baxter(2000)]{baxter2000}
Jonathan Baxter.
\newblock A model of inductive bias learning.
\newblock \emph{Journal of artificial intelligence research}, 12:\penalty0
  149--198, 2000.

\bibitem[Ben-David and Schuller(2003)]{ben2003exploiting}
Shai Ben-David and Reba Schuller.
\newblock Exploiting task relatedness for multiple task learning.
\newblock In \emph{Learning Theory and Kernel Machines}, pages 567--580.
  Springer, 2003.

\bibitem[Brinkmeyer et~al.(2019)Brinkmeyer, Drumond, Scholz, Grabocka, and
  Schmidt-Thieme]{chameleon}
Lukas Brinkmeyer, Rafael~Rego Drumond, Randolf Scholz, Josif Grabocka, and Lars
  Schmidt-Thieme.
\newblock Chameleon: Learning model initializations across tasks with different
  schemas.
\newblock \emph{arXiv preprint arXiv:1909.13576}, 2019.

\bibitem[Caruana(1997)]{caruana1998multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine Learning, 28}, pages 41--75, 1997.

\bibitem[Caruana(1993)]{caruana1993multitask}
Richard Caruana.
\newblock Multitask learning: A knowledge-based source of inductive bias.
\newblock In \emph{Proceedings of the Tenth International Conference on Machine
  Learning}, pages 41--48. Morgan Kaufmann, 1993.

\bibitem[Chen et~al.(2017)Chen, Badrinarayanan, Lee, and Rabinovich]{gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock \emph{arXiv preprint arXiv:1711.02257}, 2017.

\bibitem[Chen et~al.(2020)Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and
  Anguelov]{graddrop}
Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning
  Chai, and Dragomir Anguelov.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign
  dropout.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Chollet et~al.(2018)]{chollet2018keras}
Fran{\c{c}}ois Chollet et~al.
\newblock Keras: The python deep learning library.
\newblock \emph{ascl}, pages ascl--1806, 2018.

\bibitem[Covington et~al.(2016)Covington, Adams, and Sargin]{youtube}
Paul Covington, Jay Adams, and Emre Sargin.
\newblock Deep neural networks for youtube recommendations.
\newblock In \emph{Proceedings of the 10th ACM conference on recommender
  systems}, pages 191--198, 2016.

\bibitem[Duong et~al.(2015)Duong, Cohn, Bird, and Cook]{duong2015low}
Long Duong, Trevor Cohn, Steven Bird, and Paul Cook.
\newblock Low resource dependency parsing: Cross-lingual parameter sharing in a
  neural network parser.
\newblock In \emph{Proceedings of the 53rd Annual Meeting of the Association
  for Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing (Volume 2: Short Papers)}, pages 845--850, 2015.

\bibitem[Dwivedi and Roig(2019)]{dwivedi2019representation}
Kshitij Dwivedi and Gemma Roig.
\newblock Representation similarity analysis for efficient task taxonomy \&
  transfer learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 12387--12396, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{maml}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem[Grant et~al.(2018)Grant, Finn, Levine, Darrell, and
  Griffiths]{grant2018recasting}
Erin Grant, Chelsea Finn, Sergey Levine, Trevor Darrell, and Thomas Griffiths.
\newblock Recasting gradient-based meta-learning as hierarchical bayes, 2018.

\bibitem[Guo et~al.(2018)Guo, Haque, Huang, Yeung, and Fei-Fei]{guo2018dynamic}
Michelle Guo, Albert Haque, De-An Huang, Serena Yeung, and Li~Fei-Fei.
\newblock Dynamic task prioritization for multitask learning.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 270--287, 2018.

\bibitem[Guo et~al.(2020)Guo, Lee, and Ulbricht]{guo2020learning}
Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht.
\newblock Learning to branch for multi-task learning.
\newblock \emph{arXiv preprint arXiv:2006.01895}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hinton and Plaut(1987)]{hinton1987using}
Geoffrey~E Hinton and David~C Plaut.
\newblock Using fast weights to deblur old memories.
\newblock In \emph{Proceedings of the ninth annual conference of the Cognitive
  Science Society}, pages 177--186, 1987.

\bibitem[Huang et~al.(2018)Huang, Li, Cheng, Zhang, and Hauptmann]{Huang_2018}
Siyu Huang, Xi~Li, Zhi-Qi Cheng, Zhongfei Zhang, and Alexander Hauptmann.
\newblock {GNAS}: A greedy neural architecture search method for
  multi-attribute learning.
\newblock \emph{2018 ACM Multimedia Conference on Multimedia Conference - MM
  â€™18}, 2018.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{swa}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Advances in neural information processing systems}, pages
  315--323, 2013.

\bibitem[Kalashnikov et~al.(2021)Kalashnikov, Varley, Chebotar, Swanson,
  Jonschkowski, Finn, Levine, and Hausman]{mt-opt}
Dmitry Kalashnikov, Jacob Varley, Yevgen Chebotar, Benjamin Swanson, Rico
  Jonschkowski, Chelsea Finn, Sergey Levine, and Karol Hausman.
\newblock Mt-opt: Continuous multi-task robotic reinforcement learning at
  scale.
\newblock \emph{arXiv preprint arXiv:2104.08212}, 2021.

\bibitem[Kang et~al.(2011)Kang, Grauman, and Sha]{kang2011learning}
Zhuoliang Kang, Kristen Grauman, and Fei Sha.
\newblock Learning with whom to share in multi-task feature learning.
\newblock In \emph{ICML}, 2011.

\bibitem[Karpathy(2019)]{karpathy_mtl}
Andrej Karpathy.
\newblock Multi-task learning in the wilderness.
\newblock ICML, 2019.
\newblock URL
  \url{https://slideslive.com/38917690/multitask-learning-in-the-wilderness}.

\bibitem[Kendall et~al.(2018)Kendall, Gal, and Cipolla]{uncertainty}
Alex Kendall, Yarin Gal, and Roberto Cipolla.
\newblock Multi-task learning using uncertainty to weigh losses for scene
  geometry and semantics.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7482--7491, 2018.

\bibitem[Kim et~al.(2018)Kim, Yoon, Dia, Kim, Bengio, and Ahn]{kim2018bayesian}
Taesup Kim, Jaesik Yoon, Ousmane Dia, Sungwoong Kim, Yoshua Bengio, and Sungjin
  Ahn.
\newblock Bayesian model-agnostic meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.03836}, 2018.

\bibitem[Kumar and Daume~III(2012)]{kumar2012learning}
Abhishek Kumar and Hal Daume~III.
\newblock Learning task grouping and overlap in multi-task learning.
\newblock \emph{arXiv preprint arXiv:1206.6417}, 2012.

\bibitem[Lee et~al.(2016)Lee, Yang, and Hwang]{lee2016asymmetric}
Giwoong Lee, Eunho Yang, and Sung Hwang.
\newblock Asymmetric multi-task learning based on task relatedness and loss.
\newblock In \emph{International conference on machine learning}, pages
  230--238. PMLR, 2016.

\bibitem[Lee et~al.(2018)Lee, Yang, and Hwang]{lee2018asymmetric}
Hae~Beom Lee, Eunho Yang, and Sung~Ju Hwang.
\newblock Deep asymmetric multi-task feature learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2956--2964. PMLR, 2018.

\bibitem[Lin et~al.(2019{\natexlab{a}})Lin, Zhen, Li, Zhang, and
  Kwong]{pareto_moo}
Xi~Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong.
\newblock Pareto multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  12037--12047, 2019{\natexlab{a}}.

\bibitem[Lin et~al.(2019{\natexlab{b}})Lin, Baweja, Kantor, and Held]{adaptive}
Xingyu Lin, Harjatin~Singh Baweja, George Kantor, and David Held.
\newblock Adaptive auxiliary task weighting for reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Liu et~al.(2018)Liu, Zoph, Neumann, Shlens, Hua, Li, Fei-Fei, Yuille,
  Huang, and Murphy]{liu2018progressive}
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
  Li~Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
\newblock Progressive neural architecture search.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 19--34, 2018.

\bibitem[Liu et~al.(2021)Liu, Li, Kuang, Xue, Chen, Yang, Liao, and
  Zhang]{imtl}
Liyang Liu, Yi~Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang,
  Qingmin Liao, and Wayne Zhang.
\newblock Towards impartial multi-task learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=IMPnRXEWpvr}.

\bibitem[Liu et~al.(2019)Liu, Johns, and Davison]{liu2019end}
Shikun Liu, Edward Johns, and Andrew~J Davison.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1871--1880, 2019.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{celeba}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock \emph{2015 IEEE International Conference on Computer Vision (ICCV)},
  Dec 2015.

\bibitem[Lu et~al.(2017)Lu, Kumar, Zhai, Cheng, Javidi, and Feris]{Lu_2017}
Yongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu~Cheng, Tara Javidi, and Rogerio
  Feris.
\newblock Fully-adaptive feature sharing in multi-task networks with
  applications in person attribute classification.
\newblock \emph{2017 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, Jul 2017.

\bibitem[Misra et~al.(2016)Misra, Shrivastava, Gupta, and Hebert]{cross_stitch}
Ishan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3994--4003, 2016.

\bibitem[Nesterov(1983)]{nesterov27method}
Yu~Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate $\mathcal{O}(1/k^{2})$.
\newblock In \emph{Sov. Math. Dokl}, volume~27, pages 372--376, 1983.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{reptile}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1803.02999}, 2018.

\bibitem[Ruder(2017)]{ruder_overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1706.05098}, 2017.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock \emph{arXiv preprint arXiv:1606.04671}, 2016.

\bibitem[Sener and Koltun(2018)]{moo}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  527--538, 2018.

\bibitem[Standley et~al.(2019)Standley, Zamir, Chen, Guibas, Malik, and
  Savarese]{standley2019tasks}
Trevor Standley, Amir~R Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and
  Silvio Savarese.
\newblock Which tasks should be learned together in multi-task learning?
\newblock \emph{arXiv preprint arXiv:1905.07553}, 2019.

\bibitem[Sun et~al.(2019)Sun, Panda, and Feris]{sun2019adashare}
Ximeng Sun, Rameswar Panda, and Rogerio Feris.
\newblock Adashare: Learning what to share for efficient deep multi-task
  learning.
\newblock \emph{arXiv preprint arXiv:1911.12423}, 2019.

\bibitem[Suteu and Guo(2019)]{suteu2019regularizing}
Mihai Suteu and Yike Guo.
\newblock Regularizing deep multi-task networks using orthogonal gradients.
\newblock \emph{arXiv preprint arXiv:1912.06844}, 2019.

\bibitem[Vandenhende et~al.(2019)Vandenhende, Georgoulis, De~Brabandere, and
  Van~Gool]{branched}
Simon Vandenhende, Stamatios Georgoulis, Bert De~Brabandere, and Luc Van~Gool.
\newblock Branched multi-task networks: deciding what layers to share.
\newblock \emph{arXiv preprint arXiv:1904.02920}, 2019.

\bibitem[Wang et~al.(2020)Wang, Tsvetkov, Firat, and Cao]{gradvaccine}
Zirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao.
\newblock Gradient vaccine: Investigating and improving multi-task optimization
  in massively multilingual models.
\newblock \emph{arXiv preprint arXiv:2010.05874}, 2020.

\bibitem[Wu et~al.(2020)Wu, Zhang, and R{\'e}]{wu2020understanding}
Sen Wu, Hongyang~R Zhang, and Christopher R{\'e}.
\newblock Understanding and improving information transfer in multi-task
  learning.
\newblock \emph{arXiv preprint arXiv:2005.00944}, 2020.

\bibitem[Yang and Hospedales(2016)]{yang2016trace}
Yongxin Yang and Timothy~M. Hospedales.
\newblock Trace norm regularised deep multi-task learning.
\newblock \emph{arXiv preprint arXiv:1606.04038}, 2016.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and Finn]{pcgrad}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{arXiv preprint arXiv:2001.06782}, 2020.

\bibitem[Zamir et~al.(2018)Zamir, Sax, Shen, Guibas, Malik, and
  Savarese]{taskonomy}
Amir~R. Zamir, Alexander Sax, William Shen, Leonidas Guibas, Jitendra Malik,
  and Silvio Savarese.
\newblock Taskonomy: Disentangling task transfer learning.
\newblock \emph{2018 IEEE/CVF Conference}, Jun 2018.

\bibitem[Zhang et~al.(2019)Zhang, Lucas, Ba, and Hinton]{zhang2019lookahead}
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey~E Hinton.
\newblock Lookahead optimizer: k steps forward, 1 step back.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9597--9608, 2019.

\bibitem[Zhang and Yang(2017)]{zhang2017survey}
Yu~Zhang and Qiang Yang.
\newblock A survey on multi-task learning.
\newblock \emph{arXiv preprint arXiv:1707.08114}, 2017.

\bibitem[Zhao et~al.(2018)Zhao, Li, Shen, Liang, and Wu]{modulation}
Xiangyun Zhao, Haoxiang Li, Xiaohui Shen, Xiaodan Liang, and Ying Wu.
\newblock A modulation module for multi-task learning with applications in
  image retrieval.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 401--416, 2018.

\bibitem[Zhuang et~al.(2020)Zhuang, Qi, Duan, Xi, Zhu, Zhu, Xiong, and
  He]{zhuang2020comprehensive}
Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui
  Xiong, and Qing He.
\newblock A comprehensive survey on transfer learning.
\newblock \emph{Proceedings of the IEEE}, 2020.

\end{thebibliography}
