\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Chandra and Kapoor(2020)]{chandra2020bayesian}
Rohitash Chandra and Arpit Kapoor.
\newblock Bayesian neural multi-source transfer learning.
\newblock \emph{Neurocomputing}, 378:\penalty0 54--64, 2020.

\bibitem[Chen et~al.(2017)Chen, Papandreou, Schroff, and
  Adam]{chen2017rethinking}
Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam.
\newblock Rethinking atrous convolution for semantic image segmentation.
\newblock \emph{arXiv preprint arXiv:1706.05587}, 2017.

\bibitem[Chen et~al.(2018)Chen, Zhu, Papandreou, Schroff, and
  Adam]{chen2018encoder}
Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig
  Adam.
\newblock Encoder-decoder with atrous separable convolution for semantic image
  segmentation.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 801--818, 2018.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014stochastic}
Tianqi Chen, Emily Fox, and Carlos Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In \emph{International conference on machine learning}, pages
  1683--1691. PMLR, 2014.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Cordts et~al.(2016)Cordts, Omran, Ramos, Rehfeld, Enzweiler, Benenson,
  Franke, Roth, and Schiele]{cordts2016cityscapes}
Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler,
  Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele.
\newblock The cityscapes dataset for semantic urban scene understanding.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3213--3223, 2016.

\bibitem[Dai et~al.(2021)Dai, Liu, Le, and Tan]{dai2021coatnet}
Zihang Dai, Hanxiao Liu, Quoc~V Le, and Mingxing Tan.
\newblock Coatnet: Marrying convolution and attention for all data sizes.
\newblock \emph{arXiv preprint arXiv:2106.04803}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Do et~al.(2020)Do, Tran, and Venkatesh]{do2020semi}
Kien Do, Truyen Tran, and Svetha Venkatesh.
\newblock Semi-supervised learning with variational bayesian inference and
  maximum uncertainty regularization.
\newblock \emph{arXiv preprint arXiv:2012.01793}, 2020.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Ebrahimi et~al.(2019)Ebrahimi, Elhoseiny, Darrell, and
  Rohrbach]{ebrahimi2019uncertainty}
Sayna Ebrahimi, Mohamed Elhoseiny, Trevor Darrell, and Marcus Rohrbach.
\newblock Uncertainty-guided continual learning with bayesian neural networks.
\newblock \emph{arXiv preprint arXiv:1906.02425}, 2019.

\bibitem[Everingham et~al.(2010)Everingham, Van~Gool, Williams, Winn, and
  Zisserman]{Everingham10}
M.~Everingham, L.~Van~Gool, C.~K.~I. Williams, J.~Winn, and A.~Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock \emph{International Journal of Computer Vision}, 88\penalty0
  (2):\penalty0 303--338, June 2010.

\bibitem[Fortuin et~al.(2021)Fortuin, Garriga-Alonso, Wenzel, R{\"a}tsch,
  Turner, van~der Wilk, and Aitchison]{fortuin2021bayesian}
Vincent Fortuin, Adri{\`a} Garriga-Alonso, Florian Wenzel, Gunnar R{\"a}tsch,
  Richard Turner, Mark van~der Wilk, and Laurence Aitchison.
\newblock Bayesian neural network priors revisited.
\newblock \emph{arXiv preprint arXiv:2102.06571}, 2021.

\bibitem[Hafner et~al.(2020)Hafner, Tran, Lillicrap, Irpan, and
  Davidson]{hafner2020noise}
Danijar Hafner, Dustin Tran, Timothy Lillicrap, Alex Irpan, and James Davidson.
\newblock Noise contrastive priors for functional uncertainty.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 905--914.
  PMLR, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2020momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9729--9738, 2020.

\bibitem[Howard and Ruder(2018)]{howard2018universal}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock \emph{arXiv preprint arXiv:1801.06146}, 2018.

\bibitem[Huang et~al.(2020)Huang, Emam, Goldblum, Fowl, Terry, Huang, and
  Goldstein]{huang2020understanding}
W~Ronny Huang, Zeyad Emam, Micah Goldblum, Liam Fowl, Justin~K Terry, Furong
  Huang, and Tom Goldstein.
\newblock Understanding generalization through visualizations.
\newblock \emph{``I Can't Believe It's Not Better!'' NeurIPS 2020 workshop},
  2020.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018averaging}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Izmailov et~al.(2021{\natexlab{a}})Izmailov, Nicholson, Lotfi, and
  Wilson]{izmailov2021dangers}
Pavel Izmailov, Patrick Nicholson, Sanae Lotfi, and Andrew~Gordon Wilson.
\newblock Dangers of bayesian model averaging under covariate shift.
\newblock \emph{arXiv preprint arXiv:2106.11905}, 2021{\natexlab{a}}.

\bibitem[Izmailov et~al.(2021{\natexlab{b}})Izmailov, Vikram, Hoffman, and
  Wilson]{izmailov2021bayesian}
Pavel Izmailov, Sharad Vikram, Matthew~D Hoffman, and Andrew~Gordon Wilson.
\newblock What are bayesian neural network posteriors really like?
\newblock \emph{arXiv preprint arXiv:2104.14421}, 2021{\natexlab{b}}.

\bibitem[Jean et~al.(2018)Jean, Xie, and Ermon]{jean2018semi}
Neal Jean, Sang~Michael Xie, and Stefano Ermon.
\newblock Semi-supervised deep kernel learning: Regression with unlabeled data
  by minimizing predictive variance.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Kapoor et~al.(2021)Kapoor, Karaletsos, and Bui]{kapoor2021variational}
Sanyam Kapoor, Theofanis Karaletsos, and Thang~D Bui.
\newblock Variational auto-regressive gaussian processes for continual
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5290--5300. PMLR, 2021.

\bibitem[Karbalayghareh et~al.(2018)Karbalayghareh, Qian, and
  Dougherty]{karbalayghareh2018optimal}
Alireza Karbalayghareh, Xiaoning Qian, and Edward~R Dougherty.
\newblock Optimal bayesian transfer learning.
\newblock \emph{IEEE Transactions on Signal Processing}, 66\penalty0
  (14):\penalty0 3724--3739, 2018.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Li et~al.(2017)Li, Xu, Taylor, Studer, and
  Goldstein]{li2017visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock \emph{arXiv preprint arXiv:1712.09913}, 2017.

\bibitem[Maddox et~al.(2021)Maddox, Tang, Moreno, Wilson, and
  Damianou]{maddox2021fast}
Wesley Maddox, Shuai Tang, Pablo Moreno, Andrew~Gordon Wilson, and Andreas
  Damianou.
\newblock Fast adaptation with linearized neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2737--2745. PMLR, 2021.

\bibitem[Maddox et~al.(2019)Maddox, Izmailov, Garipov, Vetrov, and
  Wilson]{maddox2019simple}
Wesley~J Maddox, Pavel Izmailov, Timur Garipov, Dmitry~P Vetrov, and
  Andrew~Gordon Wilson.
\newblock A simple baseline for bayesian uncertainty in deep learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 13153--13164, 2019.

\bibitem[Nguyen et~al.(2017)Nguyen, Li, Bui, and Turner]{nguyen2017variational}
Cuong~V Nguyen, Yingzhen Li, Thang~D Bui, and Richard~E Turner.
\newblock Variational continual learning.
\newblock \emph{arXiv preprint arXiv:1710.10628}, 2017.

\bibitem[Nilsback and Zisserman(2008)]{nilsback2008automated}
Maria-Elena Nilsback and Andrew Zisserman.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pages 722--729. IEEE, 2008.

\bibitem[Pan et~al.(2020)Pan, Swaroop, Immer, Eschenhagen, Turner, and
  Khan]{pan2020continual}
Pingbo Pan, Siddharth Swaroop, Alexander Immer, Runa Eschenhagen, Richard
  Turner, and Mohammad Emtiyaz~E Khan.
\newblock Continual deep learning by functional regularisation of memorable
  past.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4453--4464, 2020.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012cats}
Omkar~M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV~Jawahar.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE conference on computer vision and pattern
  recognition}, pages 3498--3505. IEEE, 2012.

\bibitem[Patacchiola et~al.(2020)Patacchiola, Turner, Crowley, O'Boyle, and
  Storkey]{patacchiola2020bayesian}
Massimiliano Patacchiola, Jack Turner, Elliot~J Crowley, Michael O'Boyle, and
  Amos~J Storkey.
\newblock Bayesian meta-learning for the few-shot setting via deep kernels.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16108--16118, 2020.

\bibitem[Ravichandran et~al.(2021)Ravichandran, Lansner, and
  Herman]{ravichandran2021semi}
Naresh~Balaji Ravichandran, Anders Lansner, and Pawel Herman.
\newblock Semi-supervised learning with bayesian confidence propagation neural
  network.
\newblock \emph{arXiv preprint arXiv:2106.15546}, 2021.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock \emph{Advances in neural information processing systems},
  28:\penalty0 91--99, 2015.

\bibitem[Rudner et~al.(2021)Rudner, Smith, Feng, Teh, and
  Gal]{rudner2021continual}
Tim~GJ Rudner, Freddie~Bickford Smith, Qixuan Feng, Yee~Whye Teh, and Yarin
  Gal.
\newblock Continual learning via function-space variational inference.
\newblock In \emph{ICML Workshop on Theory and Foundations of Continual
  Learning}, 2021.

\bibitem[Tseran et~al.(2018)Tseran, Khan, Harada, and Bui]{tseran2018natural}
Hanna Tseran, Mohammad~Emtiyaz Khan, Tatsuya Harada, and Thang~D Bui.
\newblock Natural variational continual learning.
\newblock In \emph{Continual Learning Workshop@ NeurIPS}, volume~2, 2018.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688. Citeseer, 2011.

\bibitem[Wilson and Izmailov(2020)]{wilson2020bayesian}
Andrew~Gordon Wilson and Pavel Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock \emph{arXiv preprint arXiv:2002.08791}, 2020.

\bibitem[Xuan et~al.(2021)Xuan, Lu, and Zhang]{xuan2021bayesian}
Junyu Xuan, Jie Lu, and Guangquan Zhang.
\newblock Bayesian transfer learning: An overview of probabilistic graphical
  models for transfer learning.
\newblock \emph{arXiv preprint arXiv:2109.13233}, 2021.

\bibitem[Zhang et~al.(2019)Zhang, Li, Zhang, Chen, and
  Wilson]{zhang2019cyclical}
Ruqi Zhang, Chunyuan Li, Jianyi Zhang, Changyou Chen, and Andrew~Gordon Wilson.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock \emph{arXiv preprint arXiv:1902.03932}, 2019.

\end{thebibliography}
