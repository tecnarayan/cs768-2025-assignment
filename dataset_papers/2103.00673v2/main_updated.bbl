\begin{thebibliography}{10}

\bibitem{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in {\em Advances in neural information
  processing systems}, pp.~1097--1105, 2012.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{szegedy2015going}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in {\em
  Proceedings of the IEEE conference on computer vision and pattern
  recognition}, pp.~1--9, 2015.

\bibitem{ioffe2015batch}
S.~Ioffe and C.~Szegedy, ``Batch normalization: Accelerating deep network
  training by reducing internal covariate shift,'' in {\em ICML}, pp.~448--456,
  2015.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in {\em Proceedings of the IEEE conference on computer vision
  and pattern recognition}, pp.~770--778, 2016.

\bibitem{xie2017aggregated}
S.~Xie, R.~Girshick, P.~Doll{\'a}r, Z.~Tu, and K.~He, ``Aggregated residual
  transformations for deep neural networks,'' in {\em Proceedings of the IEEE
  conference on computer vision and pattern recognition}, pp.~1492--1500, 2017.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger, ``Densely connected
  convolutional networks,'' in {\em Proceedings of the IEEE conference on
  computer vision and pattern recognition}, pp.~4700--4708, 2017.

\bibitem{huang2020normalization}
L.~Huang, J.~Qin, Y.~Zhou, F.~Zhu, L.~Liu, and L.~Shao, ``Normalization
  techniques in training dnns: Methodology, analysis and application,'' {\em
  arXiv preprint arXiv:2009.12836}, 2020.

\bibitem{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton, ``Layer normalization,'' {\em arXiv
  preprint arXiv:1607.06450}, 2016.

\bibitem{ulyanov2016instance}
D.~Ulyanov, A.~Vedaldi, and V.~Lempitsky, ``Instance normalization: The missing
  ingredient for fast stylization,'' {\em arXiv preprint arXiv:1607.08022},
  2016.

\bibitem{wu2018group}
Y.~Wu and K.~He, ``Group normalization,'' in {\em Proceedings of the European
  conference on computer vision (ECCV)}, pp.~3--19, 2018.

\bibitem{he2017mask}
K.~He, G.~Gkioxari, P.~Doll{\'a}r, and R.~Girshick, ``Mask r-cnn,'' in {\em
  Proceedings of the IEEE international conference on computer vision},
  pp.~2961--2969, 2017.

\bibitem{sun2020test}
Y.~Sun, X.~Wang, Z.~Liu, J.~Miller, A.~Efros, and M.~Hardt, ``Test-time
  training with self-supervision for generalization under distribution
  shifts,'' in {\em International Conference on Machine Learning},
  pp.~9229--9248, PMLR, 2020.

\bibitem{salimans2016weight}
T.~Salimans and D.~P. Kingma, ``Weight normalization: A simple
  reparameterization to accelerate training of deep neural networks,'' {\em
  arXiv preprint arXiv:1602.07868}, 2016.

\bibitem{miyato2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida, ``Spectral normalization for
  generative adversarial networks,'' {\em arXiv preprint arXiv:1802.05957},
  2018.

\bibitem{araujo2021lipschitz}
A.~Araujo, B.~Negrevergne, Y.~Chevaleyre, and J.~Atif, ``On lipschitz
  regularization of convolutional layers using toeplitz matrix theory,'' 2021.

\bibitem{qian2018l2}
H.~Qian and M.~N. Wegman, ``L2-nonexpansive neural networks,'' in {\em
  International Conference on Learning Representations}, 2018.

\bibitem{saxe2013exact}
A.~M. Saxe, J.~L. McClelland, and S.~Ganguli, ``Exact solutions to the
  nonlinear dynamics of learning in deep linear neural networks,'' {\em arXiv
  preprint arXiv:1312.6120}, 2013.

\bibitem{pennington2018emergence}
J.~Pennington, S.~Schoenholz, and S.~Ganguli, ``The emergence of spectral
  universality in deep networks,'' in {\em International Conference on
  Artificial Intelligence and Statistics}, pp.~1924--1932, 2018.

\bibitem{xiao2018dynamical}
L.~Xiao, Y.~Bahri, J.~Sohl-Dickstein, S.~Schoenholz, and J.~Pennington,
  ``Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks,'' in {\em International
  Conference on Machine Learning}, pp.~5393--5402, 2018.

\bibitem{hu2020provable}
W.~Hu, L.~Xiao, and J.~Pennington, ``Provable benefit of orthogonal
  initialization in optimizing deep linear networks,'' in {\em International
  Conference on Learning Representations}, 2020.

\bibitem{qi2020deep}
H.~Qi, C.~You, X.~Wang, Y.~Ma, and J.~Malik, ``Deep isometric learning for
  visual recognition,'' in {\em International Conference on Machine Learning},
  pp.~7824--7835, PMLR, 2020.

\bibitem{arjovsky2016unitary}
M.~Arjovsky, A.~Shah, and Y.~Bengio, ``Unitary evolution recurrent neural
  networks,'' in {\em International Conference on Machine Learning},
  pp.~1120--1128, 2016.

\bibitem{vorontsov2017orthogonality}
E.~Vorontsov, C.~Trabelsi, S.~Kadoury, and C.~Pal, ``On orthogonality and
  learning recurrent networks with long term dependencies,'' in {\em
  International Conference on Machine Learning}, pp.~3570--3578, 2017.

\bibitem{helfrich2018orthogonal}
K.~Helfrich, D.~Willmott, and Q.~Ye, ``Orthogonal recurrent neural networks
  with scaled cayley transform,'' in {\em International Conference on Machine
  Learning}, pp.~1969--1978, PMLR, 2018.

\bibitem{lezcano2019cheap}
M.~Lezcano-Casado and D.~Mart{\i}nez-Rubio, ``Cheap orthogonal constraints in
  neural networks: A simple parametrization of the orthogonal and unitary
  group,'' in {\em International Conference on Machine Learning},
  pp.~3794--3803, 2019.

\bibitem{jia2019orthogonal}
K.~Jia, S.~Li, Y.~Wen, T.~Liu, and D.~Tao, ``Orthogonal deep neural networks,''
  {\em IEEE transactions on pattern analysis and machine intelligence}, 2019.

\bibitem{cisse2017parseval}
M.~Cisse, P.~Bojanowski, E.~Grave, Y.~Dauphin, and N.~Usunier, ``Parseval
  networks: Improving robustness to adversarial examples,'' {\em arXiv preprint
  arXiv:1704.08847}, 2017.

\bibitem{trockman2021orthogonalizing}
A.~Trockman and J.~Z. Kolter, ``Orthogonalizing convolutional layers with the
  cayley transform,'' in {\em International Conference on Learning
  Representations}, 2021.

\bibitem{liu2020oogan}
B.~Liu, Y.~Zhu, Z.~Fu, G.~de~Melo, and A.~Elgammal, ``Oogan: Disentangling gan
  with one-hot sampling and orthogonal regularization.,'' in {\em AAAI},
  pp.~4836--4843, 2020.

\bibitem{ye2020network}
C.~Ye, M.~Evanusa, H.~He, A.~Mitrokhin, T.~Goldstein, J.~A. Yorke,
  C.~Fermuller, and Y.~Aloimonos, ``Network deconvolution,'' in {\em
  International Conference on Learning Representations}, 2020.

\bibitem{brock2018large}
A.~Brock, J.~Donahue, and K.~Simonyan, ``Large scale gan training for high
  fidelity natural image synthesis,'' in {\em International Conference on
  Learning Representations}, 2018.

\bibitem{odena2018generator}
A.~Odena, J.~Buckman, C.~Olsson, T.~B. Brown, C.~Olah, C.~Raffel, and
  I.~Goodfellow, ``Is generator conditioning causally related to gan
  performance?,'' {\em arXiv preprint arXiv:1802.08768}, 2018.

\bibitem{atzmon2020isometric}
M.~Atzmon, A.~Gropp, and Y.~Lipman, ``Isometric autoencoders,'' {\em arXiv
  preprint arXiv:2006.09289}, 2020.

\bibitem{harandi2016generalized}
M.~Harandi and B.~Fernando, ``Generalized backpropagation, \'{E}tude de cas:
  Orthogonality,'' {\em arXiv}, 2016.

\bibitem{bansal2018can}
N.~Bansal, X.~Chen, and Z.~Wang, ``Can we gain more from orthogonality
  regularizations in training deep cnns?,'' in {\em Proceedings of the 32nd
  International Conference on Neural Information Processing Systems},
  pp.~4266--4276, Curran Associates Inc., 2018.

\bibitem{zhang2019approximated}
G.~Zhang, K.~Niwa, and W.~B. Kleijn, ``Approximated orthonormal normalisation
  in training neural networks,'' 2019.

\bibitem{Li2020Efficient}
J.~Li, F.~Li, and S.~Todorovic, ``Efficient riemannian optimization on the
  stiefel manifold via the cayley transform,'' in {\em International Conference
  on Learning Representations}, 2020.

\bibitem{huang2020controllable}
L.~Huang, L.~Liu, F.~Zhu, D.~Wan, Z.~Yuan, B.~Li, and L.~Shao, ``Controllable
  orthogonalization in training dnns,'' 2020.

\bibitem{qu2020geometric}
Q.~Qu, Y.~Zhai, X.~Li, Y.~Zhang, and Z.~Zhu, ``Geometric analysis of nonconvex
  optimization landscapes for overcomplete learning,'' in {\em International
  Conference on Learning Representations}, 2020.

\bibitem{huang2017orthogonal}
L.~Huang, X.~Liu, B.~Lang, A.~W. Yu, and B.~Li, ``Orthogonal weight
  normalization: Solution to optimization over multiple dependent stiefel
  manifolds in deep neural networks,'' {\em CoRR}, vol.~abs/1709.06079, 2017.

\bibitem{li2019preventing}
Q.~Li, S.~Haque, C.~Anil, J.~Lucas, R.~Grosse, and J.-H. Jacobsen, ``Preventing
  gradient attenuation in lipschitz constrained convolutional networks,'' {\em
  Conference on Neural Information Processing Systems}, 2019.

\bibitem{wang2019orthogonal}
J.~Wang, Y.~Chen, R.~Chakraborty, and S.~X. Yu, ``Orthogonal convolutional
  neural networks,'' 2019.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky {\em et~al.}, ``Learning multiple layers of features from tiny
  images,'' 2009.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, {\em et~al.}, ``Imagenet large scale
  visual recognition challenge,'' {\em International journal of computer
  vision}, vol.~115, no.~3, pp.~211--252, 2015.

\bibitem{qu2019nonconvex}
Q.~Qu, X.~Li, and Z.~Zhu, ``A nonconvex approach for exact and efficient
  multichannel sparse blind deconvolution,'' in {\em Advances in Neural
  Information Processing Systems}, pp.~4017--4028, 2019.

\bibitem{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov,
  ``Dropout: a simple way to prevent neural networks from overfitting,'' {\em
  The journal of machine learning research}, vol.~15, no.~1, pp.~1929--1958,
  2014.

\bibitem{janocha2017loss}
K.~Janocha and W.~M. Czarnecki, ``On loss functions for deep neural networks in
  classification,'' {\em arXiv preprint arXiv:1702.05659}, 2017.

\bibitem{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner, ``Gradient-based learning
  applied to document recognition,'' {\em Proceedings of the IEEE}, vol.~86,
  no.~11, pp.~2278--2324, 1998.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton, ``Deep learning,'' {\em nature}, vol.~521,
  no.~7553, pp.~436--444, 2015.

\bibitem{sedghi2018singular}
H.~Sedghi, V.~Gupta, and P.~M. Long, ``The singular values of convolutional
  layers,'' {\em arXiv preprint arXiv:1805.10408}, 2018.

\bibitem{gregor2010learning}
K.~Gregor and Y.~LeCun, ``Learning fast approximations of sparse coding.,'' in
  {\em ICML}, pp.~399--406, 2010.

\bibitem{monga2019algorithm}
V.~Monga, Y.~Li, and Y.~C. Eldar, ``Algorithm unrolling: Interpretable,
  efficient deep learning for signal and image processing,'' {\em arXiv
  preprint arXiv:1912.10557}, 2019.

\bibitem{nocedal2006numerical}
J.~Nocedal and S.~J. Wright, {\em Numerical Optimization}.
\newblock New York, NY, USA: Springer, second~ed., 2006.

\bibitem{chen2020exploring}
X.~Chen and K.~He, ``Exploring simple siamese representation learning,'' {\em
  arXiv preprint arXiv:2011.10566}, 2020.

\bibitem{guo2019simple}
C.~Guo, J.~Gardner, Y.~You, A.~G. Wilson, and K.~Weinberger, ``Simple black-box
  adversarial attacks,'' in {\em Proceedings of the 36th International
  Conference on Machine Learning}, pp.~2484--2493, 2019.

\bibitem{goodfellow2015explaining}
I.~Goodfellow, J.~Shlens, and C.~Szegedy, ``Explaining and harnessing
  adversarial examples,'' in {\em International Conference on Learning
  Representations}, 2015.

\bibitem{madry2018towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu, ``Towards deep
  learning models resistant to adversarial attacks,'' in {\em International
  Conference on Learning Representations}, 2018.

\bibitem{Goodfellow2014GenerativeAN}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~C. Courville, and Y.~Bengio, ``Generative adversarial nets,'' in {\em
  NIPS}, 2014.

\bibitem{Simonyan2015VeryDC}
K.~Simonyan and A.~Zisserman, ``Very deep convolutional networks for
  large-scale image recognition,'' {\em CoRR}, vol.~abs/1409.1556, 2015.

\bibitem{Glorot2010UnderstandingTD}
X.~Glorot and Y.~Bengio, ``Understanding the difficulty of training deep
  feedforward neural networks,'' in {\em AISTATS}, 2010.

\bibitem{shafahi2019adversarial}
A.~Shafahi, M.~Najibi, M.~A. Ghiasi, Z.~Xu, J.~Dickerson, C.~Studer, L.~S.
  Davis, G.~Taylor, and T.~Goldstein, ``Adversarial training for free!,'' in
  {\em Advances in neural information processing systems}, vol.~32, 2019.

\bibitem{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna, ``Rethinking the
  inception architecture for computer vision,'' in {\em Proceedings of the IEEE
  conference on computer vision and pattern recognition}, pp.~2818--2826, 2016.

\bibitem{arpit2017closer}
D.~Arpit, S.~Jastrz{\k{e}}bski, N.~Ballas, D.~Krueger, E.~Bengio, M.~S. Kanwal,
  T.~Maharaj, A.~Fischer, A.~Courville, Y.~Bengio, {\em et~al.}, ``A closer
  look at memorization in deep networks,'' in {\em International Conference on
  Machine Learning}, pp.~233--242, PMLR, 2017.

\bibitem{li2020gradient}
M.~Li, M.~Soltanolkotabi, and S.~Oymak, ``Gradient descent with early stopping
  is provably robust to label noise for overparameterized neural networks,'' in
  {\em International Conference on Artificial Intelligence and Statistics},
  pp.~4313--4324, PMLR, 2020.

\bibitem{liu2020early}
S.~Liu, J.~Niles-Weed, N.~Razavian, and C.~Fernandez-Granda, ``Early-learning
  regularization prevents memorization of noisy labels,'' {\em Advances in
  Neural Information Processing Systems}, vol.~33, 2020.

\bibitem{hu2019simple}
W.~Hu, Z.~Li, and D.~Yu, ``Simple and effective regularization methods for
  training on noisily labeled data with generalization guarantee,'' {\em arXiv
  preprint arXiv:1905.11368}, 2019.

\bibitem{patrini2017making}
G.~Patrini, A.~Rozza, A.~Krishna~Menon, R.~Nock, and L.~Qu, ``Making deep
  neural networks robust to label noise: A loss correction approach,'' in {\em
  Proceedings of the IEEE Conference on Computer Vision and Pattern
  Recognition}, pp.~1944--1952, 2017.

\bibitem{Zhou2019LipschitzGA}
Z.~Zhou, J.~Liang, Y.~Song, L.~Yu, H.~Wang, W.~Zhang, Y.~Yu, and Z.~Zhang,
  ``Lipschitz generative adversarial nets,'' in {\em ICML}, 2019.

\bibitem{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~C. Courville,
  ``Improved training of wasserstein {GAN}s,'' in {\em Advances in neural
  information processing systems}, pp.~5767--5777, 2017.

\bibitem{kodali2017convergence}
N.~Kodali, J.~Abernethy, J.~Hays, and Z.~Kira, ``On convergence and stability
  of {GAN}s,'' {\em arXiv preprint arXiv:1705.07215}, 2017.

\bibitem{petzka2018on}
H.~Petzka, A.~Fischer, and D.~Lukovnikov, ``On the regularization of
  wasserstein {GAN}s,'' in {\em International Conference on Learning
  Representations}, 2018.

\bibitem{salimans2016improved}
T.~Salimans, I.~Goodfellow, W.~Zaremba, V.~Cheung, A.~Radford, and X.~Chen,
  ``Improved techniques for training gans,'' {\em arXiv preprint
  arXiv:1606.03498}, 2016.

\bibitem{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter, ``{GAN}s
  trained by a two time-scale update rule converge to a local nash
  equilibrium,'' in {\em Advances in neural information processing systems},
  pp.~6626--6637, 2017.

\bibitem{lucic2018gans}
M.~Lucic, K.~Kurach, M.~Michalski, S.~Gelly, and O.~Bousquet, ``Are {GAN}s
  created equal? a large-scale study,'' in {\em Advances in neural information
  processing systems}, pp.~700--709, 2018.

\bibitem{kong2017stride}
C.~Kong and S.~Lucey, ``Take it in your stride: Do we need striding in cnns?,''
  {\em arXiv preprint arXiv:1712.02502}, 2017.

\end{thebibliography}
