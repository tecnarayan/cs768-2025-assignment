@article{wu2020lite,
  title={Lite transformer with long-short range attention},
  author={Wu, Zhanghao and Liu, Zhijian and Lin, Ji and Lin, Yujun and Han, Song},
  journal={arXiv preprint arXiv:2004.11886},
  year={2020}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={OpenAI}
}

@article{gonen2022demystifying,
  title={Demystifying prompts in language models via perplexity estimation},
  author={Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2212.04037},
  year={2022}
}

@article{wang2023unleashing,
  title={Unleashing cognitive synergy in large language models: A task-solving agent through multi-persona self-collaboration},
  author={Wang, Zhenhailong and Mao, Shaoguang and Wu, Wenshan and Ge, Tao and Wei, Furu and Ji, Heng},
  journal={arXiv preprint arXiv:2307.05300},
  year={2023}
}

@article{zaporojets2022tempel,
  title={TempEL: Linking dynamically evolving and newly emerging entities},
  author={Zaporojets, Klim and Kaffee, Lucie-Aim{\'e}e and Deleu, Johannes and Demeester, Thomas and Develder, Chris and Augenstein, Isabelle},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={1850--1866},
  year={2022}
}

@article{welbl2021challenges,
  title={Challenges in detoxifying language models},
  author={Welbl, Johannes and Glaese, Amelia and Uesato, Jonathan and Dathathri, Sumanth and Mellor, John and Hendricks, Lisa Anne and Anderson, Kirsty and Kohli, Pushmeet and Coppin, Ben and Huang, Po-Sen},
  journal={arXiv preprint arXiv:2109.07445},
  year={2021}
}

@inproceedings{ge2018eventwiki,
  title={Eventwiki: a knowledge base of major events},
  author={Ge, Tao and Cui, Lei and Chang, Baobao and Sui, Zhifang and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)},
  year={2018}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{yao2022react,
  title={React: Synergizing reasoning and acting in language models},
  author={Yao, Shunyu and Zhao, Jeffrey and Yu, Dian and Du, Nan and Shafran, Izhak and Narasimhan, Karthik and Cao, Yuan},
  journal={arXiv preprint arXiv:2210.03629},
  year={2022}
}

@inproceedings{qin2021learning,
  title={Learning How to Ask: Querying LMs with Mixtures of Soft Prompts},
  author={Qin, Guanghui and Eisner, Jason},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5203--5212},
  year={2021}
}

@article{ge2023incontext,
      title={In-context Autoencoder for Context Compression in a Large Language Model}, 
      author={Tao Ge and Jing Hu and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
      year={2023},
      eprint={2307.06945},
      archivePrefix={arXiv},
      journal={arXiv preprint arXiv:2307.06945},
      primaryClass={cs.CL}
}

@article{su2021transferability,
  title={On transferability of prompt tuning for natural language processing},
  author={Su, Yusheng and Wang, Xiaozhi and Qin, Yujia and Chan, Chi-Min and Lin, Yankai and Wang, Huadong and Wen, Kaiyue and Liu, Zhiyuan and Li, Peng and Li, Juanzi and others},
  journal={arXiv preprint arXiv:2111.06719},
  year={2021}
}

@inproceedings{vu2022spot,
  title={SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer},
  author={Vu, Tu and Lester, Brian and Constant, Noah and Al-Rfou, Rami and Cer, Daniel},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={5039--5059},
  year={2022}
}

@article{lester2022reducing,
  title={Reducing Retraining by Recycling Parameter-Efficient Prompts},
  author={Lester, Brian and Yurtsever, Joshua and Shakeri, Siamak and Constant, Noah},
  journal={arXiv preprint arXiv:2208.05577},
  year={2022}
}

@inproceedings{gal2022image,
  title={An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion},
  author={Gal, Rinon and Alaluf, Yuval and Atzmon, Yuval and Patashnik, Or and Bermano, Amit Haim and Chechik, Gal and Cohen-or, Daniel},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{rao2018dear,
  title={Dear Sir or Madam, May I Introduce the GYAFC Dataset: Corpus, Benchmarks and Metrics for Formality Style Transfer},
  author={Rao, Sudha and Tetreault, Joel},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={129--140},
  year={2018}
}

@article{xu2019formality,
  title={Formality style transfer with hybrid textual annotations},
  author={Xu, Ruochen and Ge, Tao and Wei, Furu},
  journal={arXiv preprint arXiv:1903.06353},
  year={2019}
}

@article{holtzman2019curious,
  title={The curious case of neural text degeneration},
  author={Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09751},
  year={2019}
}

@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{zhang2020parallel,
  title={Parallel Data Augmentation for Formality Style Transfer},
  author={Zhang, Yi and Ge, Tao and Sun, Xu},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={3221--3228},
  year={2020}
}

@inproceedings{li2022text,
  title={Text revision by on-the-fly representation optimization},
  author={Li, Jingjing and Li, Zichao and Ge, Tao and King, Irwin and Lyu, Michael R},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={10},
  pages={10956--10964},
  year={2022}
}

@article{liu2022sample,
  title={Are Sample-Efficient NLP Models More Robust?},
  author={Liu, Nelson F and Kumar, Ananya and Liang, Percy and Jia, Robin},
  journal={arXiv preprint arXiv:2210.06456},
  year={2022}
}

@article{rao2018dear,
  title={Dear sir or madam, may I introduce the GYAFC dataset: Corpus, benchmarks and metrics for formality style transfer},
  author={Rao, Sudha and Tetreault, Joel},
  journal={arXiv preprint arXiv:1803.06535},
  year={2018}
}

@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

@article{wang2022pay,
  title={Pay Attention to Your Tone: Introducing a New Dataset for Polite Language Rewrite},
  author={Wang, Xun and Ge, Tao and Mao, Allen and Li, Yuki and Wei, Furu and Chen, Si-Qing},
  journal={arXiv preprint arXiv:2212.10190},
  year={2022}
}

@inproceedings{devlin-etal-2019-bert,
    title = "{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding",
    author = "Devlin, Jacob  and
      Chang, Ming-Wei  and
      Lee, Kenton  and
      Toutanova, Kristina",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1423",
    doi = "10.18653/v1/N19-1423",
    pages = "4171--4186",
    abstract = "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{dong2019unified,
  title={Unified language model pre-training for natural language understanding and generation},
  author={Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  journal={arXiv preprint arXiv:1905.03197},
  year={2019}
}

@article{ma2021deltalm,
  title={Deltalm: Encoder-decoder pre-training for language generation and translation by augmenting pretrained multilingual encoders},
  author={Ma, Shuming and Dong, Li and Huang, Shaohan and Zhang, Dongdong and Muzio, Alexandre and Singhal, Saksham and Awadalla, Hany Hassan and Song, Xia and Wei, Furu},
  journal={arXiv preprint arXiv:2106.13736},
  year={2021}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{dhar2019device,
  title={On-device machine learning: An algorithms and learning theory perspective},
  author={Dhar, Sauptik and Guo, Junyao and Liu, Jiayi and Tripathi, Samarth and Kurup, Unmesh and Shah, Mohak},
  journal={arXiv preprint arXiv:1911.00623},
  year={2019}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{panahi2021shapeshifter,
  title={Shapeshifter: a Parameter-efficient Transformer using Factorized Reshaped Matrices},
  author={Panahi, Aliakbar and Saeedi, Seyran and Arodz, Tom},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{kasai2020deep,
  title={Deep encoder, shallow decoder: Reevaluating non-autoregressive machine translation},
  author={Kasai, Jungo and Pappas, Nikolaos and Peng, Hao and Cross, James and Smith, Noah A},
  journal={arXiv preprint arXiv:2006.10369},
  year={2020}
}

@inproceedings{sun2021instantaneous,
  title={Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding},
  author={Sun, Xin and Ge, Tao and Wei, Furu and Wang, Houfeng},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5937--5947},
  year={2021}
}

@inproceedings{bryant2019bea,
  title={The BEA-2019 shared task on grammatical error correction},
  author={Bryant, Christopher and Felice, Mariano and Andersen, {\O}istein E and Briscoe, Ted},
  booktitle={Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={52--75},
  year={2019}
}

@article{kudo2018sentencepiece,
  title={Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing},
  author={Kudo, Taku and Richardson, John},
  journal={arXiv preprint arXiv:1808.06226},
  year={2018}
}

@article{ott2018scaling,
  title={Scaling neural machine translation},
  author={Ott, Myle and Edunov, Sergey and Grangier, David and Auli, Michael},
  journal={arXiv preprint arXiv:1806.00187},
  year={2018}
}

@inproceedings{yannakoudakis2011new,
  title={A new dataset and method for automatically grading ESOL texts},
  author={Yannakoudakis, Helen and Briscoe, Ted and Medlock, Ben},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
  pages={180--189},
  year={2011}
}

@inproceedings{mizumoto2011mining,
  title={Mining revision log of language learning SNS for automated Japanese error correction of second language learners},
  author={Mizumoto, Tomoya and Komachi, Mamoru and Nagata, Masaaki and Matsumoto, Yuji},
  booktitle={Proceedings of 5th International Joint Conference on Natural Language Processing},
  pages={147--155},
  year={2011}
}

@inproceedings{ng-etal-2013-conll,
    title = "The {C}o{NLL}-2013 Shared Task on Grammatical Error Correction",
    author = "Ng, Hwee Tou  and
      Wu, Siew Mei  and
      Wu, Yuanbin  and
      Hadiwinoto, Christian  and
      Tetreault, Joel",
    booktitle = "Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task",
    month = aug,
    year = "2013",
    address = "Sofia, Bulgaria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-3601",
    pages = "1--12",
}

@inproceedings{sun2021instantaneous,
  title={Instantaneous Grammatical Error Correction with Shallow Aggressive Decoding},
  author={Sun, Xin and Ge, Tao and Wei, Furu and Wang, Houfeng},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  pages={5937--5947},
  year={2021}
}

@inproceedings{fan2019reducing,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Fan, Angela and Grave, Edouard and Joulin, Armand},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{xu2020bert,
  title={BERT-of-Theseus: Compressing BERT by Progressive Module Replacing},
  author={Xu, Canwen and Zhou, Wangchunshu and Ge, Tao and Wei, Furu and Zhou, Ming},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={7859--7869},
  year={2020}
}

@inproceedings{dahlmeier2012better,
  title={Better evaluation for grammatical error correction},
  author={Dahlmeier, Daniel and Ng, Hwee Tou},
  booktitle={Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={568--572},
  year={2012}
}

@article{post2018call,
  title={A call for clarity in reporting BLEU scores},
  author={Post, Matt},
  journal={arXiv preprint arXiv:1804.08771},
  year={2018}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@inproceedings{ng2014conll,
  title={The CoNLL-2014 shared task on grammatical error correction},
  author={Ng, Hwee Tou and Wu, Siew Mei and Briscoe, Ted and Hadiwinoto, Christian and Susanto, Raymond Hendy and Bryant, Christopher},
  booktitle={Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task},
  pages={1--14},
  year={2014}
}

@article{kim2016sequence,
  title={Sequence-level knowledge distillation},
  author={Kim, Yoon and Rush, Alexander M},
  journal={arXiv preprint arXiv:1606.07947},
  year={2016}
}

@article{lester2021power,
  title={The power of scale for parameter-efficient prompt tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  journal={arXiv preprint arXiv:2104.08691},
  year={2021}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{ben2021bitfit,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Ben Zaken, Elad and Ravfogel, Shauli and Goldberg, Yoav},
  journal={arXiv e-prints},
  pages={arXiv--2106},
  year={2021}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@inproceedings{wu2018pay,
  title = {Pay Less Attention with Lightweight and Dynamic Convolutions},
  author = {Felix Wu and Angela Fan and Alexei Baevski and Yann Dauphin and Michael Auli},
  booktitle = {International Conference on Learning Representations},
  year = {2019},
  url = {https://arxiv.org/abs/1901.10430},
}

@inproceedings{chen-etal-2020-improving-efficiency,
    title = "Improving the Efficiency of Grammatical Error Correction with Erroneous Span Detection and Correction",
    author = "Chen, Mengyun  and
      Ge, Tao  and
      Zhang, Xingxing  and
      Wei, Furu  and
      Zhou, Ming",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "7162--7169",
    abstract = "We propose a novel language-independent approach to improve the efficiency for Grammatical Error Correction (GEC) by dividing the task into two subtasks: Erroneous Span Detection (ESD) and Erroneous Span Correction (ESC). ESD identifies grammatically incorrect text spans with an efficient sequence tagging model. Then, ESC leverages a seq2seq model to take the sentence with annotated erroneous spans as input and only outputs the corrected text for these spans. Experiments show our approach performs comparably to conventional seq2seq approaches in both English and Chinese GEC benchmarks with less than 50{\%} time cost for inference.",
}

@inproceedings{zhou2021improving,
  title={Improving Sequence-to-Sequence Pre-training via Sequence Span Rewriting},
  author={Zhou, Wangchunshu and Ge, Tao and Xu, Canwen and Xu, Ke and Wei, Furu},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={571--582},
  year={2021}
}

@misc{fedus2021switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  year={2021}
}

@inproceedings{tambe2021edgebert,
  title={Edgebert: Sentence-level energy optimizations for latency-aware multi-task nlp inference},
  author={Tambe, Thierry and Hooper, Coleman and Pentecost, Lillian and Jia, Tianyu and Yang, En-Yu and Donato, Marco and Sanh, Victor and Whatmough, Paul and Rush, Alexander M and Brooks, David and others},
  booktitle={MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture},
  pages={830--844},
  year={2021}
}

@article{li2022dq,
  title={DQ-BART: Efficient Sequence-to-Sequence Model via Joint Distillation and Quantization},
  author={Li, Zheng and Wang, Zijian and Tan, Ming and Nallapati, Ramesh and Bhatia, Parminder and Arnold, Andrew and Xiang, Bing and Roth, Dan},
  journal={arXiv preprint arXiv:2203.11239},
  year={2022}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{wang2020hat,
  title={Hat: Hardware-aware transformers for efficient natural language processing},
  author={Wang, Hanrui and Wu, Zhanghao and Liu, Zhijian and Cai, Han and Zhu, Ligeng and Gan, Chuang and Han, Song},
  journal={arXiv preprint arXiv:2005.14187},
  year={2020}
}

@article{narayan2018don,
  title={Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={arXiv preprint arXiv:1808.08745},
  year={2018}
}

@inproceedings{du2017learning,
    title={Learning to Ask: Neural Question Generation for Reading Comprehension},
    author={Du, Xinya and Shao, Junru and Cardie, Claire},
    booktitle={Association for Computational Linguistics (ACL)},
    year={2017}
}

@article{chen2019reinforcement,
  title={Reinforcement learning based graph-to-sequence model for natural question generation},
  author={Chen, Yu and Wu, Lingfei and Zaki, Mohammed J},
  journal={arXiv preprint arXiv:1908.04942},
  year={2019}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{reid2021subformer,
  title={Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers},
  author={Reid, Machel and Marrese-Taylor, Edison and Matsuo, Yutaka},
  journal={arXiv preprint arXiv:2101.00234},
  year={2021}
}

@article{boroumand2021mitigating,
  title={Mitigating Edge Machine Learning Inference Bottlenecks: An Empirical Study on Accelerating Google Edge Models},
  author={Boroumand, Amirali and Ghose, Saugata and Akin, Berkin and Narayanaswami, Ravi and Oliveira, Geraldo F and Ma, Xiaoyu and Shiu, Eric and Mutlu, Onur},
  journal={arXiv preprint arXiv:2103.00768},
  year={2021}
}

@article{takase2021lessons,
  title={Lessons on parameter sharing across layers in transformers},
  author={Takase, Sho and Kiyono, Shun},
  journal={arXiv preprint arXiv:2104.06022},
  year={2021}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}

@article{zhang2021beyond,
  title={Beyond Fully-Connected Layers with Quaternions: Parameterization of Hypercomplex Multiplications with $1/n $ Parameters},
  author={Zhang, Aston and Tay, Yi and Zhang, Shuai and Chan, Alvin and Luu, Anh Tuan and Hui, Siu Cheung and Fu, Jie},
  journal={arXiv preprint arXiv:2102.08597},
  year={2021}
}

@inproceedings{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  booktitle={International Conference on Machine Learning},
  pages={1243--1252},
  year={2017},
  organization={PMLR}
}

@article{mehta2020delight,
  title={DeLighT: Deep and Light-weight Transformer},
  author={Mehta, Sachin and Ghazvininejad, Marjan and Iyer, Srinivasan and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2008.00623},
  year={2020}
}

@article{naber2003rule,
  title={A rule-based style and grammar checker},
  author={Naber, Daniel},
  year={2003},
  publisher={Bielefeld University Bielefeld, Germany}
}

@inproceedings{park1997english,
  title={An English Grammar Checker as a Writing Aid for Students of English as a Second Language.},
  author={Park, Jong C and Palmer, Martha Stone and Washburn, Clay},
  booktitle={ANLP},
  year={1997}
}

@inproceedings{yannakoudakis2017neural,
  title={Neural Sequence-Labelling Models for Grammatical Error Correction},
  author={Yannakoudakis, Helen and Rei, Marek and Andersen, {\O}istein E and Yuan, Zheng},
  booktitle={EMNLP},
  year={2017}
}

@inproceedings{ji2017nested,
  title={A Nested Attention Neural Hybrid Model for Grammatical Error Correction},
  author={Ji, Jianshu and Wang, Qinlong and Toutanova, Kristina and Gong, Yongen and Truong, Steven and Gao, Jianfeng},
  booktitle={ACL},
  year={2017}
}

@article{xie2016neural,
  title={Neural language correction with character-based attention},
  author={Xie, Ziang and Avati, Anand and Arivazhagan, Naveen and Jurafsky, Dan and Ng, Andrew Y},
  journal={arXiv preprint arXiv:1603.09727},
  year={2016}
}

@inproceedings{ng2014conll,
  title={The CoNLL-2014 shared task on grammatical error correction},
  author={Ng, Hwee Tou and Wu, Siew Mei and Briscoe, Ted and Hadiwinoto, Christian and Susanto, Raymond Hendy and Bryant, Christopher},
  booktitle={CoNLL (Shared Task)},
  year={2014}
}

@article{chollampatt2016neural,
  title={Neural network translation models for grammatical error correction},
  author={Chollampatt, Shamil and Taghipour, Kaveh and Ng, Hwee Tou},
  journal={arXiv preprint arXiv:1606.00189},
  year={2016}
}

@inproceedings{rozovskaya2016grammatical,
  title={Grammatical error correction: Machine translation and classifiers},
  author={Rozovskaya, Alla and Roth, Dan},
  booktitle={ACL},
  year={2016}
}

@inproceedings{junczys2014amu,
  title={The AMU system in the CoNLL-2014 shared task: Grammatical error correction by data-intensive and feature-rich statistical machine translation},
  author={Junczys-Dowmunt, Marcin and Grundkiewicz, Roman},
  booktitle={CoNLL (Shared Task)},
  year={2014}
}

@inproceedings{yuan2013constrained,
  title={Constrained grammatical error correction using statistical machine translation},
  author={Yuan, Zheng and Felice, Mariano},
  booktitle={CoNLL (Shared Task)},
  year={2013}
}

@article{junczys2016phrase,
  title={Phrase-based machine translation is state-of-the-art for automatic grammatical error correction},
  author={Junczys-Dowmunt, Marcin and Grundkiewicz, Roman},
  journal={arXiv preprint arXiv:1605.06353},
  year={2016}
}

@inproceedings{rozovskaya2014illinois,
  title={The Illinois-Columbia system in the CoNLL-2014 shared task},
  author={Rozovskaya, Alla and Chang, Kai-Wei and Sammons, Mark and Roth, Dan and Habash, Nizar},
  booktitle={CoNLL (Shared Task)},
  year={2014}
}

@inproceedings{felice2014grammatical,
  title={Grammatical error correction using hybrid systems and type filtering},
  author={Felice, Mariano and Yuan, Zheng and Andersen, {\O}istein E and Yannakoudakis, Helen and Kochmar, Ekaterina},
  booktitle={CoNLL (Shared Task)},
  year={2014}
}

@inproceedings{chodorow2007detection,
  title={Detection of grammatical errors involving prepositions},
  author={Chodorow, Martin and Tetreault, Joel R and Han, Na-Rae},
  booktitle={ACL-SIGSEM workshop on prepositions},
  year={2007},
}

@inproceedings{de2008classifier,
  title={A classifier-based approach to preposition and determiner error correction in L2 English},
  author={De Felice, Rachele and Pulman, Stephen G},
  booktitle={COLING},
  year={2008},
}

@inproceedings{han2010using,
  title={Using an Error-Annotated Learner Corpus to Develop an ESL/EFL Error Correction System.},
  author={Han, Na-Rae and Tetreault, Joel R and Lee, Soo-Hwa and Ha, Jin-Young},
  booktitle={LREC},
  year={2010}
}

@inproceedings{tetreault2010using,
  title={Using parse features for preposition selection and error detection},
  author={Tetreault, Joel and Foster, Jennifer and Chodorow, Martin},
  booktitle={ACL},
  year={2010},
}

@inproceedings{dale2011helping,
  title={Helping our own: The HOO 2011 pilot shared task},
  author={Dale, Robert and Kilgarriff, Adam},
  booktitle={European Workshop on Natural Language Generation},
  year={2011},
}

@article{leacock2010automated,
  title={Automated grammatical error detection for language learners},
  author={Leacock, Claudia and Chodorow, Martin and Gamon, Michael and Tetreault, Joel},
  journal={Synthesis lectures on human language technologies},
  volume={3},
  number={1},
  pages={1--134},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@inproceedings{brockett2006correcting,
  title={Correcting ESL errors using phrasal SMT techniques},
  author={Brockett, Chris and Dolan, William B and Gamon, Michael},
  booktitle={COLING/ACL},
  year={2006},
}

@inproceedings{dahlmeier2011correcting,
  title={Correcting semantic collocation errors with L1-induced paraphrases},
  author={Dahlmeier, Daniel and Ng, Hwee Tou},
  booktitle={EMNLP},
  year={2011}
}

@inproceedings{dahlmeier2012beam,
  title={A beam-search decoder for grammatical error correction},
  author={Dahlmeier, Daniel and Ng, Hwee Tou},
  booktitle={EMNLP/CoNLL},
  year={2012}
}


@inproceedings{yoshimoto2013naist,
  title={NAIST at 2013 CoNLL grammatical error correction shared task},
  author={Yoshimoto, Ippei and Kose, Tomoya and Mitsuzawa, Kensuke and Sakaguchi, Keisuke and Mizumoto, Tomoya and Hayashibe, Yuta and Komachi, Mamoru and Matsumoto, Yuji},
  booktitle={CoNLL (Shared Task)},
  year={2013}
}

@inproceedings{yuan2016grammatical,
  title={Grammatical error correction using neural machine translation},
  author={Yuan, Zheng and Briscoe, Ted},
  booktitle={NAACL},
  year={2016}
}

@inproceedings{sakaguchi2017grammatical,
  title={Grammatical Error Correction with Neural Reinforcement Learning},
  author={Sakaguchi, Keisuke and Post, Matt and Van Durme, Benjamin},
  booktitle={IJCNLP},
  year={2017}
}

@inproceedings{sennrich2016improving,
  title={Improving Neural Machine Translation Models with Monolingual Data},
  author={Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  booktitle={ACL},
  year={2016}
}

@inproceedings{he2016dual,
  title={Dual learning for machine translation},
  author={He, Di and Xia, Yingce and Qin, Tao and Wang, Liwei and Yu, Nenghai and Liu, Tieyan and Ma, Wei-Ying},
  booktitle={NIPS},
  year={2016}
}

@inproceedings{bryant2015far,
  title={How Far are We from Fully Automatic High Quality Grammatical Error Correction?},
  author={Bryant, Christopher and Ng, Hwee Tou},
  booktitle={ACL/IJCNLP},
  year={2015}
}

@inproceedings{mizumoto2011mining,
  title={Mining revision log of language learning SNS for automated Japanese error correction of second language learners},
  author={Mizumoto, Tomoya and Komachi, Mamoru and Nagata, Masaaki and Matsumoto, Yuji},
  booktitle={IJCNLP},
  year={2011}
}

@inproceedings{tajiri2012tense,
  title={Tense and aspect error correction for ESL learners using global context},
  author={Tajiri, Toshikazu and Komachi, Mamoru and Matsumoto, Yuji},
  booktitle={ACL},
  year={2012}
}

@inproceedings{nicholls2003cambridge,
  title={The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT},
  author={Nicholls, Diane},
  booktitle={Corpus Linguistics 2003 conference},
  year={2003}
}

@inproceedings{dahlmeier2013building,
  title={Building a large annotated corpus of learner english: The nus corpus of learner english},
  author={Dahlmeier, Daniel and Ng, Hwee Tou and Wu, Siew Mei},
  booktitle={Workshop on innovative use of NLP for building educational applications},
  year={2013}
}

@article{napoles2017jfleg,
  title={JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction},
  author={Napoles, Courtney and Sakaguchi, Keisuke and Tetreault, Joel},
  journal={arXiv preprint arXiv:1702.04066},
  year={2017}
}

@inproceedings{bryant2017automatic,
  title={Automatic annotation and evaluation of error types for grammatical error correction},
  author={Bryant, Christopher and Felice, Mariano and Briscoe, E},
  booktitle={ACL},
  year={2017}
}

@inproceedings{kaneko2017grammatical,
  title={Grammatical Error Detection Using Error-and Grammaticality-Specific Word Embeddings},
  author={Kaneko, Masahiro and Sakaizawa, Yuya and Komachi, Mamoru},
  booktitle={IJCNLP},
  year={2017}
}

@inproceedings{asano2017reference,
  title={Reference-based Metrics can be Replaced with Reference-less Metrics in Evaluating Grammatical Error Correction Systems},
  author={Asano, Hiroki and Mizumoto, Tomoya and Inui, Kentaro},
  booktitle={IJCNLP},
  year={2017}
}

@inproceedings{napoles2015ground,
  title={Ground truth for grammatical error correction metrics},
  author={Napoles, Courtney and Sakaguchi, Keisuke and Post, Matt and Tetreault, Joel},
  booktitle={ACL/IJCNLP},
  year={2015}
}

@inproceedings{behera2013automated,
  title={Automated grammar correction using hierarchical phrase-based statistical machine translation},
  author={Behera, Bibek and Bhattacharyya, Pushpak},
  booktitle={IJCNLP},
  year={2013}
}

@inproceedings{dahlmeier2012better,
  title={Better evaluation for grammatical error correction},
  author={Dahlmeier, Daniel and Ng, Hwee Tou},
  booktitle={NAACL},
  year={2012}
}

@inproceedings{tetreault2010rethinking,
  title={Rethinking grammatical error annotation and evaluation with the Amazon Mechanical Turk},
  author={Tetreault, Joel R and Filatova, Elena and Chodorow, Martin},
  booktitle={Workshop on Innovative Use of NLP for Building Educational Applications},
  year={2010}
}

@inproceedings{madnani2011they,
  title={They can help: Using crowdsourcing to improve the evaluation of grammatical error detection systems},
  author={Madnani, Nitin and Tetreault, Joel and Chodorow, Martin and Rozovskaya, Alla},
  booktitle={ACL},
  year={2011}
}

@inproceedings{napoles2016there,
  title={There's No Comparison: Reference-less Evaluation Metrics in Grammatical Error Correction},
  author={Napoles, Courtney and Sakaguchi, Keisuke and Tetreault, Joel},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{mizumoto2016discriminative,
  title={Discriminative reranking for grammatical error correction with statistical machine translation},
  author={Mizumoto, Tomoya and Matsumoto, Yuji},
  booktitle={NAACL},
  year={2016}
}

@inproceedings{chollampatt2016adapting,
  title={Adapting grammatical error correction based on the native language of writers with neural network joint models},
  author={Chollampatt, Shamil and Hoang, Duc Tam and Ng, Hwee Tou},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{Susanto2014System,
  title={System Combination for Grammatical Error Correction},
  author={Susanto, Raymond Hendy and Phandi, Peter and Ng, Hwee Tou},
  booktitle={EMNLP},
  year={2014},
}

@inproceedings{Yuan2016Candidate,
  title={Candidate re-ranking for SMT-based grammatical error correction},
  author={Yuan, Zheng and Briscoe, Ted and Felice, Mariano and Yuan, Zheng and Briscoe, Ted and Felice, Mariano},
  booktitle={Workshop on Innovative Use of NLP for Building Educational Applications},
  year={2016},
}

@InProceedings{rei-yannakoudakis:2016:P16-1,
  author    = {Rei, Marek  and  Yannakoudakis, Helen},
  title     = {Compositional Sequence Labeling Models for Error Detection in Learner Writing},
  booktitle = {ACL},
  year      = {2016}
}

@inproceedings{Hoang2016Exploiting,
  title={Exploiting N-Best Hypotheses to Improve an SMT Approach to Grammatical Error Correction},
  author={Hoang, Duc Tam and Chollampatt, Shamil and Ng, Hwee Tou},
  booktitle={IJCAI},
  year={2016},
}

@InProceedings{papineni-EtAl:2002:ACL,
  author    = {Kishore Papineni  and  Salim Roukos  and  Todd Ward  and  Wei-Jing Zhu},
  title     = {Bleu: a Method for Automatic Evaluation of Machine Translation},
  booktitle = {ACL},
  year      = {2002}
}

@inproceedings{Rozovskaya2012The,
  title={The UI system in the HOO 2012 shared task on error correction},
  author={Rozovskaya, Alla and Sammons, Mark and Dan, Roth},
  booktitle={Workshop on Building Educational Applications Using NLP},
  year={2012},
}

@InProceedings{felice-yuan:2014:SRW,
  author    = {Felice, Mariano  and  Yuan, Zheng},
  title     = {Generating artificial errors for grammatical error correction},
  booktitle = {Student Research Workshop at EACL},
  month     = {April},
  year      = {2014}
}

@inproceedings{Mikolov2010Recurrent,
  title={Recurrent neural network based language model},
  author={Mikolov, Tomas and Karafiát, Martin and Burget, Lukas and Cernocký, Jan and Khudanpur, Sanjeev},
  booktitle={INTERSPEECH},
  year={2010},
}

@article{DBLP:journals/corr/GulcehreFXCBLBS15,
  author    = {{\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Orhan Firat and
               Kelvin Xu and
               Kyunghyun Cho and
               Lo{\"{\i}}c Barrault and
               Huei{-}Chi Lin and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {On Using Monolingual Corpora in Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1503.03535},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.03535},
  archivePrefix = {arXiv},
  eprint    = {1503.03535},
  timestamp = {Wed, 07 Jun 2017 14:40:33 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/GulcehreFXCBLBS15},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@InProceedings{dahlmeier-ng:2012:NAACL-HLT,
  author    = {Dahlmeier, Daniel  and  Ng, Hwee Tou},
  title     = {Better Evaluation for Grammatical Error Correction},
  booktitle = {NAACL},
  year      = {2012}
}

@InProceedings{luong-pham-manning:2015:EMNLP,
  author    = {Luong, Thang  and  Pham, Hieu  and  Manning, Christopher D.},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  booktitle = {EMNLP},
  year      = {2015}
}

@article{DBLP:journals/corr/SutskeverVL14,
  author    = {Ilya Sutskever and
               Oriol Vinyals and
               Quoc V. Le},
  title     = {Sequence to Sequence Learning with Neural Networks},
  journal   = {CoRR},
  volume    = {abs/1409.3215},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.3215},
  archivePrefix = {arXiv},
  eprint    = {1409.3215},
  timestamp = {Wed, 07 Jun 2017 14:40:10 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/SutskeverVL14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@InProceedings{cho-EtAl:2014:EMNLP2014,
  author    = {Cho, Kyunghyun  and  van Merrienboer, Bart  and  Gulcehre, Caglar  and  Bahdanau, Dzmitry  and  Bougares, Fethi  and  Schwenk, Holger  and  Bengio, Yoshua},
  title     = {Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  booktitle = {EMNLP},
  month     = {October},
  year      = {2014}
}

@article{DBLP:journals/corr/BahdanauCB14,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  journal   = {CoRR},
  volume    = {abs/1409.0473},
  year      = {2014},
  url       = {http://arxiv.org/abs/1409.0473},
  archivePrefix = {arXiv},
  eprint    = {1409.0473},
  timestamp = {Wed, 07 Jun 2017 14:40:19 +0200},
  biburl    = {http://dblp.org/rec/bib/journals/corr/BahdanauCB14},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@InProceedings{chollampatt-ng:2017:BEA,
  author    = {Chollampatt, Shamil  and  Ng, Hwee Tou},
  title     = {Connecting the Dots: Towards Human-Level Grammatical Error Correction},
  booktitle = {Workshop on Innovative Use of NLP for Building Educational Applications},
  year      = {2017}
}

@InProceedings{schmaltz-EtAl:2017:EMNLP2017,
  author    = {Schmaltz, Allen  and  Kim, Yoon  and  Rush, Alexander  and  Shieber, Stuart},
  title     = {Adapting Sequence Models for Sentence Correction},
  booktitle = {EMNLP},
  year      = {2017}
}

@inproceedings{DXiaTWLQYL17,
  author    = {Yingce Xia and
               Fei Tian and
               Lijun Wu and
               Jianxin Lin and
               Tao Qin and
               Nenghai Yu and
               Tie{-}Yan Liu},
  title     = {Deliberation Networks: Sequence Generation Beyond One-Pass Decoding},
  booktitle = {NIPS},
  year      = {2017}
}

@article{chollampatt2018,
  title={A Multilayer Convolutional Encoder-Decoder Neural Network for Grammatical Error Correction},
  author={Chollampatt, Shamil and Ng, Hwee Tou},
  journal={arXiv preprint arXiv:1801.08831},
  year={2018}
}

@article{grundkiewicz2018near,
  title={Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation},
  author={Grundkiewicz, Roman and Junczys-Dowmunt, Marcin},
  journal={arXiv preprint arXiv:1804.05945},
  year={2018}
}

@article{junczys2018approaching,
  title={Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task},
  author={Junczys-Dowmunt, Marcin and Grundkiewicz, Roman and Guha, Shubha and Heafield, Kenneth},
  journal={arXiv preprint arXiv:1804.05940},
  year={2018}
}

@inproceedings{foster2009generrate,
  title={GenERRate: generating errors for use in grammatical error detection},
  author={Foster, Jennifer and Andersen, {\O}istein E},
  booktitle={Workshop on innovative use of nlp for building educational applications},
  year={2009}
}

@inproceedings{rozovskaya2010training,
  title={Training paradigms for correcting errors in grammar and usage},
  author={Rozovskaya, Alla and Roth, Dan},
  booktitle={NAACL},
  year={2010}
}

@inproceedings{rozovskaya2011algorithm,
  title={Algorithm selection and model adaptation for ESL correction tasks},
  author={Rozovskaya, Alla and Roth, Dan},
  booktitle={ACL},
  year={2011}
}

@article{rei2017artificial,
  title={Artificial Error Generation with Machine Translation and Syntactic Patterns},
  author={Rei, Marek and Felice, Mariano and Yuan, Zheng and Briscoe, Ted},
  journal={arXiv preprint arXiv:1707.05236},
  year={2017}
}

@article{zhang2018joint,
  title={Joint Training for Neural Machine Translation Models with Monolingual Data},
  author={Zhang, Zhirui and Liu, Shujie and Li, Mu and Zhou, Ming and Chen, Enhong},
  journal={arXiv preprint arXiv:1803.00353},
  year={2018}
}

@article{sakaguchi2016reassessing,
  title={Reassessing the goals of grammatical error correction: Fluency instead of grammaticality},
  author={Sakaguchi, Keisuke and Napoles, Courtney and Post, Matt and Tetreault, Joel},
  journal={Transactions of the Association of Computational Linguistics},
  volume={4},
  number={1},
  pages={169--182},
  year={2016}
}

@inproceedings{xie2018noising,
  title={Noising and Denoising Natural Language: Diverse Backtranslation for Grammar Correction},
  author={Xie, Ziang and Genthial, Guillaume and Xie, Stanley and Ng, Andrew and Jurafsky, Dan},
  booktitle={NAACL},
  year={2018}
}

@inproceedings{sutskever2013importance,
  title={On the importance of initialization and momentum in deep learning},
  author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  booktitle={ICML},
  year={2013}
}

@article{gehring2017convolutional,
  title={Convolutional sequence to sequence learning},
  author={Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N},
  journal={arXiv preprint arXiv:1705.03122},
  year={2017}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={ICML},
  year={2013}
}

@inproceedings{ge2018fluency,
  title={Fluency boost learning and inference for neural grammatical error correction},
  author={Ge, Tao and Wei, Furu and Zhou, Ming},
  booktitle={ACL},
  year={2018}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={NIPS},
  year={2017}
}

@article{choshen2018inherent,
  title={Inherent Biases in Reference-based Evaluation for Grammatical Error Correction and Text Simplification},
  author={Choshen, Leshem and Abend, Omeri},
  journal={arXiv preprint arXiv:1804.11254},
  year={2018}
}

@article{schick2020exploiting,
  title={Exploiting cloze questions for few shot text classification and natural language inference},
  author={Schick, Timo and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2001.07676},
  year={2020}
}

@article{Jiang2019HowCW,
  title={How Can We Know What Language Models Know?},
  author={Zhengbao Jiang and Frank F. Xu and J. Araki and Graham Neubig},
  journal={Transactions of the Association for Computational Linguistics},
  year={2019},
  volume={8},
  pages={423-438}
}

@article{shin2020autoprompt,
  title={Autoprompt: Eliciting knowledge from language models with automatically generated prompts},
  author={Shin, Taylor and Razeghi, Yasaman and Logan IV, Robert L and Wallace, Eric and Singh, Sameer},
  journal={arXiv preprint arXiv:2010.15980},
  year={2020}
}

@inproceedings{gao-etal-2021-making,
    title = "Making Pre-trained Language Models Better Few-shot Learners",
    author = "Gao, Tianyu  and
      Fisch, Adam  and
      Chen, Danqi",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.295",
    doi = "10.18653/v1/2021.acl-long.295",
    pages = "3816--3830",
    abstract = "The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot performance solely by leveraging a natural-language prompt and a few task demonstrations as input context. Inspired by their findings, we study few-shot learning in a more practical scenario, where we use smaller language models for which fine-tuning is computationally efficient. We present LM-BFF{---}better few-shot fine-tuning of language models{---}a suite of simple and complementary techniques for fine-tuning language models on a small number of annotated examples. Our approach includes (1) prompt-based fine-tuning together with a novel pipeline for automating prompt generation; and (2) a refined strategy for dynamically and selectively incorporating demonstrations into each context. Finally, we present a systematic evaluation for analyzing few-shot performance on a range of NLP tasks, including classification and regression. Our experiments demonstrate that our methods combine to dramatically outperform standard fine-tuning procedures in this low resource setting, achieving up to 30{\%} absolute improvement, and 11{\%} on average across all tasks. Our approach makes minimal assumptions on task resources and domain expertise, and hence constitutes a strong task-agnostic method for few-shot learning.",
}

@article{BenDavid2021PADAEP,
  title={PADA: Example-based Prompt Learning for on-the-fly Adaptation to Unseen Domains},
  author={Eyal Ben-David and Nadav Oved and Roi Reichart},
  journal={Transactions of the Association for Computational Linguistics},
  year={2021},
  volume={10},
  pages={414-433}
}

@inproceedings{davison-etal-2019-commonsense,
    title = "Commonsense Knowledge Mining from Pretrained Models",
    author = "Davison, Joe  and
      Feldman, Joshua  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1109",
    doi = "10.18653/v1/D19-1109",
    pages = "1173--1178",
    abstract = "Inferring commonsense knowledge is a key challenge in machine learning. Due to the sparsity of training data, previous work has shown that supervised methods for commonsense knowledge mining underperform when evaluated on novel data. In this work, we develop a method for generating commonsense knowledge using a large, pre-trained bidirectional language model. By transforming relational triples into masked sentences, we can use this model to rank a triple{'}s validity by the estimated pointwise mutual information between the two entities. Since we do not update the weights of the bidirectional model, our approach is not biased by the coverage of any one commonsense knowledge base. Though we do worse on a held-out test set than models explicitly trained on a corresponding training set, our approach outperforms these methods when mining commonsense knowledge from new sources, suggesting that our unsupervised technique generalizes better than current supervised approaches.",
}

@inproceedings{li-liang-2021-prefix,
    title = "Prefix-Tuning: Optimizing Continuous Prompts for Generation",
    author = "Li, Xiang Lisa  and
      Liang, Percy",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.353",
    doi = "10.18653/v1/2021.acl-long.353",
    pages = "4582--4597",
    abstract = "Fine-tuning is the de facto way of leveraging large pretrained language models for downstream tasks. However, fine-tuning modifies all the language model parameters and therefore necessitates storing a full copy for each task. In this paper, we propose prefix-tuning, a lightweight alternative to fine-tuning for natural language generation tasks, which keeps language model parameters frozen and instead optimizes a sequence of continuous task-specific vectors, which we call the prefix. Prefix-tuning draws inspiration from prompting for language models, allowing subsequent tokens to attend to this prefix as if it were {``}virtual tokens{''}. We apply prefix-tuning to GPT-2 for table-to-text generation and to BART for summarization. We show that by learning only 0.1{\%} of the parameters, prefix-tuning obtains comparable performance in the full data setting, outperforms fine-tuning in low-data settings, and extrapolates better to examples with topics that are unseen during training.",
}

@article{liu2021gpt,
  title={GPT understands, too},
  author={Liu, Xiao and Zheng, Yanan and Du, Zhengxiao and Ding, Ming and Qian, Yujie and Yang, Zhilin and Tang, Jie},
  journal={arXiv preprint arXiv:2103.10385},
  year={2021}
}

@article{han2022ptr,
  title={Ptr: Prompt tuning with rules for text classification},
  author={Han, Xu and Zhao, Weilin and Ding, Ning and Liu, Zhiyuan and Sun, Maosong},
  journal={AI Open},
  volume={3},
  pages={182--192},
  year={2022},
  publisher={Elsevier}
}

@article{hu2021knowledgeable,
  title={Knowledgeable prompt-tuning: Incorporating knowledge into prompt verbalizer for text classification},
  author={Hu, Shengding and Ding, Ning and Wang, Huadong and Liu, Zhiyuan and Li, Juanzi and Sun, Maosong},
  journal={arXiv preprint arXiv:2108.02035},
  year={2021}
}

@article{reid2021lewis,
  title={LEWIS: Levenshtein editing for unsupervised text style transfer},
  author={Reid, Machel and Zhong, Victor},
  journal={arXiv preprint arXiv:2105.08206},
  year={2021}
}

@inproceedings{li-etal-2018-delete,
    title = "Delete, Retrieve, Generate: a Simple Approach to Sentiment and Style Transfer",
    author = "Li, Juncen  and
      Jia, Robin  and
      He, He  and
      Liang, Percy",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-1169",
    doi = "10.18653/v1/N18-1169",
    pages = "1865--1874",
    abstract = "We consider the task of text attribute transfer: transforming a sentence to alter a specific attribute (e.g., sentiment) while preserving its attribute-independent content (e.g., {``}screen is just the right size{''} to {``}screen is too small{''}). Our training data includes only sentences labeled with their attribute (e.g., positive and negative), but not pairs of sentences that only differ in the attributes, so we must learn to disentangle attributes from attribute-independent content in an unsupervised way. Previous work using adversarial methods has struggled to produce high-quality outputs. In this paper, we propose simpler methods motivated by the observation that text attributes are often marked by distinctive phrases (e.g., {``}too small{''}). Our strongest method extracts content words by deleting phrases associated with the sentence{'}s original attribute value, retrieves new phrases associated with the target attribute, and uses a neural model to fluently combine these into a final output. Based on human evaluation, our best method generates grammatical and appropriate responses on 22{\%} more inputs than the best previous system, averaged over three attribute transfer datasets: altering sentiment of reviews on Yelp, altering sentiment of reviews on Amazon, and altering image captions to be more romantic or humorous.",
}

@article{shen2017style,
  title={Style transfer from non-parallel text by cross-alignment},
  author={Shen, Tianxiao and Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{hu2017toward,
  title={Toward controlled generation of text},
  author={Hu, Zhiting and Yang, Zichao and Liang, Xiaodan and Salakhutdinov, Ruslan and Xing, Eric P},
  booktitle={International conference on machine learning},
  pages={1587--1596},
  year={2017},
  organization={PMLR}
}

@inproceedings{fu2018style,
  title={Style transfer in text: Exploration and evaluation},
  author={Fu, Zhenxin and Tan, Xiaoye and Peng, Nanyun and Zhao, Dongyan and Yan, Rui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@article{reif2021recipe,
  title={A recipe for arbitrary text style transfer with large language models},
  author={Reif, Emily and Ippolito, Daphne and Yuan, Ann and Coenen, Andy and Callison-Burch, Chris and Wei, Jason},
  journal={arXiv preprint arXiv:2109.03910},
  year={2021}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{kumari2022multi,
  title={Multi-Concept Customization of Text-to-Image Diffusion},
  author={Kumari, Nupur and Zhang, Bingliang and Zhang, Richard and Shechtman, Eli and Zhu, Jun-Yan},
  journal={arXiv preprint arXiv:2212.04488},
  year={2022}
}