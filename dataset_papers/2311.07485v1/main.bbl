\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Yu, Richt{\'a}rik, Suresh, and Bacon]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik, Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication efficiency.
\newblock \emph{arXiv preprint arXiv:1610.05492}, 2016.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson, and Arcas]{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Ag{\"u}era~y Arcas.
\newblock Communication-efficient learning of deep networks from decentralized data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Wang et~al.(2020)Wang, Yu, and Wornell]{wang2020federated}
Jianyu Wang, Ananda~Theertha Yu, and Gregory Wornell.
\newblock Federated learning with matched averaging.
\newblock \emph{arXiv preprint arXiv:2002.06440}, 2020.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and Vojnovic]{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1709--1720, 2017.

\bibitem[Mao et~al.(2022)Mao, Zhao, Yan, Liu, Lan, Song, and Ding]{mao2022communication}
Yuzhu Mao, Zihao Zhao, Guangfeng Yan, Yang Liu, Tian Lan, Linqi Song, and Wenbo Ding.
\newblock Communication-efficient federated learning with adaptive quantization.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology (TIST)}, 13\penalty0 (4):\penalty0 1--26, 2022.

\bibitem[Stich(2018)]{stich2018local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock \emph{arXiv preprint arXiv:1805.09767}, 2018.

\bibitem[Smith et~al.(2017)Smith, Chiang, Sanjabi, and Talwalkar]{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet~S Talwalkar.
\newblock Federated multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 4424--4434, 2017.

\bibitem[Sattler et~al.(2019{\natexlab{a}})Sattler, Wiedemann, M{\"u}ller, and Samek]{sattler2019robust}
Felix Sattler, Simon Wiedemann, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Robust and communication-efficient federated learning from non-iid data.
\newblock In \emph{IEEE transactions on neural networks and learning systems}, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2020)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and Smith]{li2020fair}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith.
\newblock Fair resource allocation in federated learning.
\newblock \emph{arXiv preprint arXiv:1905.10497}, 2020.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Reinforcement learning}, pages 5--32, 1992.

\bibitem[Sehnke et~al.(2010)Sehnke, Osendorfer, R{\"u}ckstie{\ss}, Graves, Peters, and Schmidhuber]{sehnke2010parameter}
Frank Sehnke, Christian Osendorfer, Thomas R{\"u}ckstie{\ss}, Alex Graves, Jan Peters, and J{\"u}rgen Schmidhuber.
\newblock Parameter-exploring policy gradients.
\newblock \emph{Neural Networks}, 23\penalty0 (4):\penalty0 551--559, 2010.

\bibitem[Wierstra et~al.(2014)Wierstra, Schaul, Glasmachers, Sun, Peters, and Schmidhuber]{wierstra2014natural}
Daan Wierstra, Tom Schaul, Tobias Glasmachers, Yi~Sun, Jan Peters, and J{\"u}rgen Schmidhuber.
\newblock Natural evolution strategies.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0 (1):\penalty0 949--980, 2014.

\bibitem[Yao(1999)]{yao1999evolving}
Xin Yao.
\newblock Evolving artificial neural networks.
\newblock \emph{Proceedings of the IEEE}, 87\penalty0 (9):\penalty0 1423--1447, 1999.

\bibitem[Lehman and Miikkulainen(2013)]{lehman2013neuroevolution}
Joel Lehman and Risto Miikkulainen.
\newblock Neuroevolution.
\newblock \emph{Scholarpedia}, 8\penalty0 (6):\penalty0 30977, 2013.

\bibitem[Risi and Togelius(2014)]{DBLP:journals/corr/RisiT14}
Sebastian Risi and Julian Togelius.
\newblock Neuroevolution in games: State of the art and open challenges.
\newblock \emph{CoRR}, abs/1410.7326, 2014.

\bibitem[Risi and Togelius(2015)]{risi2015neuroevolution}
Sebastian Risi and Julian Togelius.
\newblock Neuroevolution in games: State of the art and open challenges.
\newblock \emph{IEEE Transactions on Computational Intelligence and AI in Games}, 9\penalty0 (1):\penalty0 25--41, 2015.

\bibitem[Golovin et~al.(2019)Golovin, Karro, Kochanski, Lee, Song, and Zhang]{golovin2019gradientless}
Daniel Golovin, John Karro, Greg Kochanski, Chansoo Lee, Xingyou Song, and Qiuyi Zhang.
\newblock Gradientless descent: High-dimensional zeroth-order optimization.
\newblock \emph{arXiv preprint arXiv:1911.06317}, 2019.

\bibitem[Lange et~al.(2023)Lange, Schaul, Chen, Lu, Zahavy, Dalibard, and Flennerhag]{lange2023discovering}
Robert~Tjarko Lange, Tom Schaul, Yutian Chen, Chris Lu, Tom Zahavy, Valentin Dalibard, and Sebastian Flennerhag.
\newblock Discovering attention-based genetic algorithms via meta-black-box optimization.
\newblock \emph{arXiv preprint arXiv:2304.03995}, 2023.

\bibitem[Sattler et~al.(2019{\natexlab{b}})Sattler, Wiedemann, M{\"u}ller, and Samek]{sattler2019sparse}
Felix Sattler, Simon Wiedemann, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Sparse binary compression: Towards distributed deep learning with minimal communication.
\newblock In \emph{2019 International Joint Conference on Neural Networks (IJCNN)}, pages 1--8. IEEE, 2019{\natexlab{b}}.

\bibitem[Xu et~al.(2020)Xu, Du, Jin, He, and Cheng]{xu2020ternary}
Jinjin Xu, Wenli Du, Yaochu Jin, Wangli He, and Ran Cheng.
\newblock Ternary compression for communication-efficient federated learning.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2020.

\bibitem[Jeong et~al.(2018)Jeong, Oh, Kim, Park, Bennis, and Kim]{jeong2018communication}
Eunjeong Jeong, Seungeun Oh, Hyesung Kim, Jihong Park, Mehdi Bennis, and Seong-Lyun Kim.
\newblock Communication-efficient on-device machine learning: Federated distillation and augmentation under non-iid private data.
\newblock \emph{arXiv preprint arXiv:1811.11479}, 2018.

\bibitem[Itahara et~al.(2020)Itahara, Nishio, Koda, Morikura, and Yamamoto]{itahara2020distillation}
Sohei Itahara, Takayuki Nishio, Yusuke Koda, Masahiro Morikura, and Koji Yamamoto.
\newblock Distillation-based semi-supervised federated learning for communication-efficient collaborative training with non-iid private data.
\newblock \emph{arXiv preprint arXiv:2008.06180}, 2020.

\bibitem[Sattler et~al.(2020)Sattler, Marban, Rischke, and Samek]{sattler2020communication}
Felix Sattler, Arturo Marban, Roman Rischke, and Wojciech Samek.
\newblock Communication-efficient federated distillation.
\newblock \emph{arXiv preprint arXiv:2012.00632}, 2020.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{lin2020ensemble}
Tao Lin, Lingjing Kong, Sebastian~U Stich, and Martin Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2351--2363, 2020.

\bibitem[Wu et~al.(2022)Wu, Wu, Lyu, Huang, and Xie]{wu2022communication}
Chuhan Wu, Fangzhao Wu, Lingjuan Lyu, Yongfeng Huang, and Xing Xie.
\newblock Communication-efficient federated learning via knowledge distillation.
\newblock \emph{Nature communications}, 13\penalty0 (1):\penalty0 1--8, 2022.

\bibitem[Chen et~al.(2021)Chen, Shlezinger, Poor, Eldar, and Cui]{chen2021communication}
Mingzhe Chen, Nir Shlezinger, H~Vincent Poor, Yonina~C Eldar, and Shuguang Cui.
\newblock Communication-efficient federated learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0 (17):\penalty0 e2024789118, 2021.

\bibitem[Nishio and Yonetani(2019)]{nishio2019client}
Takayuki Nishio and Ryo Yonetani.
\newblock Client selection for federated learning with heterogeneous resources in mobile edge.
\newblock In \emph{ICC 2019-2019 IEEE international conference on communications (ICC)}, pages 1--7. IEEE, 2019.

\bibitem[Dinh et~al.(2020)Dinh, Tran, Nguyen, Hong, Bao, Zomaya, and Gramoli]{dinh2020federated}
Canh~T Dinh, Nguyen~H Tran, Minh~NH Nguyen, Choong~Seon Hong, Wei Bao, Albert~Y Zomaya, and Vincent Gramoli.
\newblock Federated learning over wireless networks: Convergence analysis and resource allocation.
\newblock \emph{IEEE/ACM Transactions on Networking}, 29\penalty0 (1):\penalty0 398--409, 2020.

\bibitem[Huning(1976)]{huning1976evolutionsstrategie}
Alois Huning.
\newblock Evolutionsstrategie. optimierung technischer systeme nach prinzipien der biologischen evolution, 1976.

\bibitem[Wierstra et~al.(2008)Wierstra, Schaul, Peters, and Schmidhuber]{wierstra2008fitness}
Daan Wierstra, Tom Schaul, Jan Peters, and J{\"u}rgen Schmidhuber.
\newblock Fitness expectation maximization.
\newblock In \emph{Parallel Problem Solving from Nature--PPSN X: 10th International Conference, Dortmund, Germany, September 13-17, 2008. Proceedings 10}, pages 337--346. Springer, 2008.

\bibitem[Yi et~al.(2009)Yi, Wierstra, Schaul, and Schmidhuber]{yi2009stochastic}
Sun Yi, Daan Wierstra, Tom Schaul, and J{\"u}rgen Schmidhuber.
\newblock Stochastic search using the natural gradient.
\newblock In \emph{Proceedings of the 26th Annual International Conference on Machine Learning}, pages 1161--1168, 2009.

\bibitem[Sun et~al.(2009)Sun, Wierstra, Schaul, and Schmidhuber]{sun2009efficient}
Yi~Sun, Daan Wierstra, Tom Schaul, and J{\"u}rgen Schmidhuber.
\newblock Efficient natural evolution strategies.
\newblock In \emph{Proceedings of the 11th Annual conference on Genetic and evolutionary computation}, pages 539--546, 2009.

\bibitem[Glasmachers et~al.(2010{\natexlab{a}})Glasmachers, Schaul, and Schmidhuber]{glasmachers2010natural}
Tobias Glasmachers, Tom Schaul, and J{\"u}rgen Schmidhuber.
\newblock A natural evolution strategy for multi-objective optimization.
\newblock In \emph{Parallel Problem Solving from Nature, PPSN XI: 11th International Conference, Krak{\'o}w, Poland, September 11-15, 2010, Proceedings, Part I 11}, pages 627--636. Springer, 2010{\natexlab{a}}.

\bibitem[Glasmachers et~al.(2010{\natexlab{b}})Glasmachers, Schaul, Yi, Wierstra, and Schmidhuber]{glasmachers2010exponential}
Tobias Glasmachers, Tom Schaul, Sun Yi, Daan Wierstra, and J{\"u}rgen Schmidhuber.
\newblock Exponential natural evolution strategies.
\newblock In \emph{Proceedings of the 12th annual conference on Genetic and evolutionary computation}, pages 393--400, 2010{\natexlab{b}}.

\bibitem[Schaul et~al.(2011)Schaul, Glasmachers, and Schmidhuber]{schaul2011high}
Tom Schaul, Tobias Glasmachers, and J{\"u}rgen Schmidhuber.
\newblock High dimensions and heavy tails for natural evolution strategies.
\newblock In \emph{Proceedings of the 13th annual conference on Genetic and evolutionary computation}, pages 845--852, 2011.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and Sutskever]{salimans2017evolution}
Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.03864}, 2017.

\bibitem[Spall(1992)]{spall1992multivariate}
James~C Spall.
\newblock Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.
\newblock \emph{IEEE transactions on automatic control}, 37\penalty0 (3):\penalty0 332--341, 1992.

\bibitem[Nesterov and Spokoiny(2017)]{nesterov2017random}
Yurii Nesterov and Vladimir Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17:\penalty0 527--566, 2017.

\bibitem[Diffie and Hellman(1976)]{diffie-hellman}
W.~Diffie and M.~Hellman.
\newblock New directions in cryptography.
\newblock \emph{IEEE Transactions on Information Theory}, 22\penalty0 (6):\penalty0 644--654, 1976.
\newblock \doi{10.1109/TIT.1976.1055638}.

\bibitem[Zhu and Jin(2021)]{zhu2021real}
Hangyu Zhu and Yaochu Jin.
\newblock Real-time federated evolutionary neural architecture search.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 26\penalty0 (2):\penalty0 364--378, 2021.

\bibitem[Mocanu et~al.(2018)Mocanu, Mocanu, Stone, Nguyen, Gibescu, and Liotta]{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen, Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science.
\newblock \emph{Nature communications}, 9\penalty0 (1):\penalty0 1--12, 2018.

\bibitem[Zhu and Jin(2019)]{zhu2019multi}
Hangyu Zhu and Yaochu Jin.
\newblock Multi-objective evolutionary federated learning.
\newblock \emph{IEEE transactions on neural networks and learning systems}, 31\penalty0 (4):\penalty0 1310--1322, 2019.

\bibitem[Chai et~al.(2022)Chai, Yang, and Li]{chai2022communication}
Zheng-yi Chai, Chuan-dong Yang, and Ya-lun Li.
\newblock Communication efficiency optimization in federated learning based on multi-objective evolutionary algorithm.
\newblock \emph{Evolutionary Intelligence}, pages 1--12, 2022.

\bibitem[Zhang and Li(2007)]{zhang2007moea}
Qingfu Zhang and Hui Li.
\newblock Moea/d: A multiobjective evolutionary algorithm based on decomposition.
\newblock \emph{IEEE Transactions on evolutionary computation}, 11\penalty0 (6):\penalty0 712--731, 2007.

\bibitem[De~Falco et~al.(2023)De~Falco, Della~Cioppa, Koutny, Ubl, Krcma, Scafuri, and Tarantino]{de2023federated}
Ivanoe De~Falco, Antonio Della~Cioppa, Tomas Koutny, Martin Ubl, Michal Krcma, Umberto Scafuri, and Ernesto Tarantino.
\newblock A federated learning-inspired evolutionary algorithm: Application to glucose prediction.
\newblock \emph{Sensors}, 23\penalty0 (6):\penalty0 2957, 2023.

\bibitem[Geweke(1988)]{geweke1988antithetic}
John Geweke.
\newblock Antithetic acceleration of monte carlo integration in bayesian inference.
\newblock \emph{Journal of Econometrics}, 38\penalty0 (1-2):\penalty0 73--89, 1988.

\bibitem[Brockhoff et~al.(2010)Brockhoff, Auger, Hansen, Arnold, and Hohm]{brockhoff2010mirrored}
Dimo Brockhoff, Anne Auger, Nikolaus Hansen, Dirk~V Arnold, and Tim Hohm.
\newblock Mirrored sampling and sequential selection for evolution strategies.
\newblock In \emph{Parallel Problem Solving from Nature, PPSN XI: 11th International Conference, Krak{\'o}w, Poland, September 11-15, 2010, Proceedings, Part I 11}, pages 11--21. Springer, 2010.

\bibitem[Yagisawa(2015)]{yagisawa2015fully}
Masahiro Yagisawa.
\newblock Fully homomorphic encryption without bootstrapping.
\newblock \emph{Cryptology ePrint Archive}, 2015.

\bibitem[Chen et~al.(2017)Chen, Laine, and Player]{chen2017simple}
Hao Chen, Kim Laine, and Rachel Player.
\newblock Simple encrypted arithmetic library-seal v2. 1.
\newblock In \emph{Financial Cryptography and Data Security: FC 2017 International Workshops, WAHC, BITCOIN, VOTING, WTSC, and TA, Sliema, Malta, April 7, 2017, Revised Selected Papers 21}, pages 3--18. Springer, 2017.

\bibitem[Fan and Vercauteren(2012)]{fan2012somewhat}
Junfeng Fan and Frederik Vercauteren.
\newblock Somewhat practical fully homomorphic encryption.
\newblock \emph{Cryptology ePrint Archive}, 2012.

\bibitem[Anati et~al.(2013)Anati, Gueron, Johnson, and Scarlata]{anati2013innovative}
Ittai Anati, Shay Gueron, Simon Johnson, and Vincent Scarlata.
\newblock Innovative technology for cpu based attestation and sealing.
\newblock In \emph{Proceedings of the 2nd international workshop on hardware and architectural support for security and privacy}, volume~13. ACM New York, NY, USA, 2013.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0 2278--2324, 1998.

\bibitem[Krizhevsky and Hinton(2010)]{krizhevsky2010convolutional}
Alex Krizhevsky and Geoff Hinton.
\newblock Convolutional deep belief networks on cifar-10.
\newblock \emph{Unpublished manuscript}, 40\penalty0 (7):\penalty0 1--9, 2010.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary, Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang]{jax2018github}
James Bradbury, Roy Frostig, Peter Hawkins, Matthew~James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake Vander{P}las, Skye Wanderman-{M}ilne, and Qiao Zhang.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs.
\newblock \emph{Unpublished manuscript}, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Lange(2022)]{evosax2022github}
Robert~Tjarko Lange.
\newblock evosax: Jax-based evolution strategies.
\newblock \emph{arXiv preprint arXiv:2212.04180}, 2022.

\end{thebibliography}
