@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epje	= {Eur.\ Phys.\ J E} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{epl	= {Europhys.\ Lett.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{pnas	= {Proc.\ Natl.\ Acad.\ Sci.\ U.S.A.} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{prb	= {Phys.\ Rev.\ B} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{pre	= {Phys.\ Rev.\ E} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{prl	= {Phys.\ Rev.\ Lett.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }
@STRING{rmp	= {Rev.\ Mod.\ Phys.} }

@Article{	  1742-5468-2012-07-p07009,
  abstract	= {In this paper we continue our investigation on the high
		  storage regime of a neural network with Gaussian patterns.
		  Through an exact mapping between its partition function and
		  one of a bipartite spin glass (whose parties consist of
		  Ising and Gaussian spins respectively), we give a complete
		  control of the whole annealed region. The strategy explored
		  is based on an interpolation between the bipartite system
		  and two independent spin glasses built respectively by
		  dichotomic and Gaussian spins: critical line, behavior of
		  the principal thermodynamic observables and their
		  fluctuations as well as overlap fluctuations are obtained
		  and discussed. Then, we move further, extending such an
		  equivalence beyond the critical line, to explore the broken
		  ergodicity phase under the assumption of replica symmetry
		  and show that the quenched free energy of this (analogical)
		  Hopfield model can be described as a linear combination of
		  the two quenched spin glass free energies even in the
		  replica symmetric framework.},
  author	= {Barra, Adriano and Genovese, Giuseppe and Guerra,
		  Francesco and Tantari, Daniele},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {07},
  pages		= {P07009},
  title		= {{How glassy are neural networks?}},
  url		= {http://stacks.iop.org/1742-5468/2012/i=07/a=P07009},
  volume	= {2012},
  year		= {2012}
}

###Article{	  1742-5468-2012-07-p07009,
  abstract	= {In this paper we continue our investigation on the high
		  storage regime of a neural network with Gaussian patterns.
		  Through an exact mapping between its partition function and
		  one of a bipartite spin glass (whose parties consist of
		  Ising and Gaussian spins respectively), we give a complete
		  control of the whole annealed region. The strategy explored
		  is based on an interpolation between the bipartite system
		  and two independent spin glasses built respectively by
		  dichotomic and Gaussian spins: critical line, behavior of
		  the principal thermodynamic observables and their
		  fluctuations as well as overlap fluctuations are obtained
		  and discussed. Then, we move further, extending such an
		  equivalence beyond the critical line, to explore the broken
		  ergodicity phase under the assumption of replica symmetry
		  and show that the quenched free energy of this (analogical)
		  Hopfield model can be described as a linear combination of
		  the two quenched spin glass free energies even in the
		  replica symmetric framework.},
  author	= {Barra, Adriano and Genovese, Giuseppe and Guerra,
		  Francesco and Tantari, Daniele},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {07},
  pages		= {P07009},
  title		= {{How glassy are neural networks?}},
  url		= {http://stacks.iop.org/1742-5468/2012/i=07/a=P07009},
  volume	= {2012},
  year		= {2012},
  bdsk-url-1	= {http://stacks.iop.org/1742-5468/2012/i=07/a=P07009}
}

@article{zec1,
  title={Shaping the learning landscape in neural networks around wide flat minima},
  author={Baldassi, Carlo and Pittorino, Fabrizio and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1905.07833},
  year={2019}
}

@article{zec2,
  title={Unreasonable effectiveness of learning neural networks: From accessible states and robust ensembles to basic algorithmic schemes},
  author={Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T and Ingrosso, Alessandro and Lucibello, Carlo and Saglietti, Luca and Zecchina, Riccardo},
  journal={Proceedings of the National Academy of Sciences},
  volume={113},
  number={48},
  pages={E7655--E7662},
  year={2016},
  publisher={National Acad Sciences}
}

@article{anandkumar2016homotopy,
  title={Homotopy analysis for tensor pca},
  author={Anandkumar, Anima and Deng, Yuan and Ge, Rong and Mobahi, Hossein},
  journal={arXiv preprint arXiv:1610.09322},
  year={2016}
}

@article{lee2018learning,
  title={Learning finite-dimensional coding schemes with nonlinear reconstruction maps},
  author={Lee, Jaeho and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1812.09658},
  year={2018}
}

@Article{	  achille17,
  title		= {Emergence of invariance and disentangling in deep
		  representations},
  author	= {Achille, Alessandro and Soatto, Stefano},
  journal	= {arXiv preprint arXiv:1706.01350},
  year		= {2017}
}

###Article{	  achille17,
  author	= {Achille, Alessandro and Soatto, Stefano},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1706.01350},
  title		= {Emergence of invariance and disentangling in deep
		  representations},
  year		= {2017}
}

###Article{	  achille17,
  title		= {Emergence of invariance and disentangling in deep
		  representations},
  author	= {Achille, Alessandro and Soatto, Stefano},
  journal	= {arXiv preprint arXiv:1706.01350},
  year		= {2017}
}

@InProceedings{	  achlioptas2008algorithmic,
  title		= {Algorithmic barriers from phase transitions},
  author	= {Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle	= {Foundations of Computer Science, 2008. FOCS'08. IEEE 49th
		  Annual IEEE Symposium on},
  pages		= {793--802},
  year		= {2008},
  organization	= {IEEE}
}

###InProceedings{ achlioptas2008algorithmic,
  author	= {Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle	= {Foundations of Computer Science, 2008. FOCS'08. IEEE 49th
		  Annual IEEE Symposium on},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  organization	= {IEEE},
  pages		= {793--802},
  title		= {Algorithmic barriers from phase transitions},
  year		= {2008}
}

###InProceedings{ achlioptas2008algorithmic,
  title		= {Algorithmic barriers from phase transitions},
  author	= {Achlioptas, Dimitris and Coja-Oghlan, Amin},
  booktitle	= {Foundations of Computer Science, 2008. FOCS'08. IEEE 49th
		  Annual IEEE Symposium on},
  pages		= {793--802},
  year		= {2008},
  organization	= {IEEE}
}

@Book{		  adler2009random,
  title		= {Random fields and geometry},
  author	= {Adler, Robert J and Taylor, Jonathan E},
  year		= {2009},
  publisher	= {Springer Science \& Business Media}
}

###Book{	  adler2009random,
  author	= {Adler, Robert J and Taylor, Jonathan E},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {Springer Science \& Business Media},
  title		= {Random fields and geometry},
  year		= {2009}
}

###Book{	  adler2009random,
  title		= {Random fields and geometry},
  author	= {Adler, Robert J and Taylor, Jonathan E},
  year		= {2009},
  publisher	= {Springer Science \& Business Media}
}

@Article{	  adler2014,
  abstract	= {The structure of Gaussian random fields over high levels
		  is a well researched and well understood area, particularly
		  if the field is smooth. However, the question as to whether
		  or not two or more points which lie in an excursion set
		  belong to the same connected component has constantly
		  eluded analysis. We study this problem from the point of
		  view of large deviations, finding the asymptotic
		  probabilities that two such points are connected by a path
		  laying within the excursion set, and so belong to the same
		  component. In addition, we obtain a characterization and
		  descriptions of the most likely paths, given that one
		  exists.},
  archiveprefix	= {arXiv},
  arxivid	= {1204.0206},
  author	= {Adler, Robert J. and Moldavskaya, Elina and Samorodnitsky,
		  Gennady},
  doi		= {10.1214/12-AOP794},
  eprint	= {1204.0206},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1204.0206v1.pdf:pdf},
  issn		= {0091-1798},
  journal	= {The Annals of Probability},
  month		= may,
  number	= {3},
  pages		= {1020--1053},
  title		= {{On the existence of paths between points in high level
		  excursion sets of Gaussian random fields}},
  url		= {http://arxiv.org/abs/1204.0206
		  http://projecteuclid.org/euclid.aop/1395838123},
  volume	= {42},
  year		= {2014}
}

###Article{	  adler2014,
  abstract	= {The structure of Gaussian random fields over high levels
		  is a well researched and well understood area, particularly
		  if the field is smooth. However, the question as to whether
		  or not two or more points which lie in an excursion set
		  belong to the same connected component has constantly
		  eluded analysis. We study this problem from the point of
		  view of large deviations, finding the asymptotic
		  probabilities that two such points are connected by a path
		  laying within the excursion set, and so belong to the same
		  component. In addition, we obtain a characterization and
		  descriptions of the most likely paths, given that one
		  exists.},
  archiveprefix	= {arXiv},
  arxivid	= {1204.0206},
  author	= {Adler, Robert J. and Moldavskaya, Elina and Samorodnitsky,
		  Gennady},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1214/12-AOP794},
  eprint	= {1204.0206},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1204.0206v1.pdf:pdf},
  issn		= {0091-1798},
  journal	= {The Annals of Probability},
  month		= may,
  number	= {3},
  pages		= {1020--1053},
  title		= {{On the existence of paths between points in high level
		  excursion sets of Gaussian random fields}},
  url		= {http://arxiv.org/abs/1204.0206
		  http://projecteuclid.org/euclid.aop/1395838123},
  volume	= {42},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1204.0206%20http://projecteuclid.org/euclid.aop/1395838123},
  bdsk-url-2	= {https://doi.org/10.1214/12-AOP794}
}

@Article{	  advani2017high,
  title		= {High-dimensional dynamics of generalization error in
		  neural networks},
  author	= {Advani, Madhu S and Saxe, Andrew M},
  journal	= {arXiv preprint arXiv:1710.03667},
  year		= {2017}
}

###Article{	  advani2017high,
  title		= {High-dimensional dynamics of generalization error in
		  neural networks},
  author	= {Advani, Madhu S and Saxe, Andrew M},
  journal	= {arXiv preprint arXiv:1710.03667},
  year		= {2017}
}

@article{chen2015net2net,
  title={Net2net: Accelerating learning via knowledge transfer},
  author={Chen, Tianqi and Goodfellow, Ian and Shlens, Jonathon},
  journal={arXiv preprint arXiv:1511.05641},
  year={2015}
}

@article{frankle2018lottery,
  title={The lottery ticket hypothesis: Finding sparse, trainable neural networks},
  author={Frankle, Jonathan and Carbin, Michael},
  journal={arXiv preprint arXiv:1803.03635},
  year={2018}
}

@article{wu2017towards,
  title={Towards understanding generalization of deep learning: Perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@inproceedings{liang2018adding,
  title={Adding one neuron can eliminate all bad local minima},
  author={Liang, Shiyu and Sun, Ruoyu and Lee, Jason D and Srikant, R},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4350--4360},
  year={2018}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@article{chaudhari2016entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={arXiv preprint arXiv:1611.01838},
  year={2016}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{bucilu2006model,
  title={Model compression},
  author={Buciluǎ, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={535--541},
  year={2006},
  organization={ACM}
}

@inproceedings{coates2011selecting,
  title={Selecting receptive fields in deep networks},
  author={Coates, Adam and Ng, Andrew Y},
  booktitle={Advances in neural information processing systems},
  pages={2528--2536},
  year={2011}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@inproceedings{saxena2016convolutional,
  title={Convolutional neural fabrics},
  author={Saxena, Shreyas and Verbeek, Jakob},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4053--4061},
  year={2016}
}

@article{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  journal={arXiv preprint arXiv:1802.05296},
  year={2018}
}

@inproceedings{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nati},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5947--5956},
  year={2017}
}

@article{du2017gradient,
  title={Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima},
  author={Du, Simon S and Lee, Jason D and Tian, Yuandong and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1712.00779},
  year={2017}
}

@inproceedings{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry P and Wilson, Andrew G},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8789--8798},
  year={2018}
}

@Article{	  agliari2014,
  abstract	= {Neural networks are nowadays both powerful operational
		  tools (e.g., for pattern recognition, data mining, error
		  correction codes) and complex theoretical models on the
		  focus of scientific investigation. As for the research
		  branch, neural networks are handled and studied by
		  psychologists, neurobiologists, engineers, mathematicians
		  and theoretical physicists. In particular, in theoretical
		  physics, the key instrument for the quantitative analysis
		  of neural networks is statistical mechanics. From this
		  perspective, here, we first review attractor networks:
		  starting from ferromagnets and spin-glass models, we
		  discuss the underlying philosophy and we recover the strand
		  paved by Hopfield, Amit-Gutfreund-Sompolinky. One step
		  forward, we highlight the structural equivalence between
		  Hopfield networks (modeling retrieval) and Boltzmann
		  machines (modeling learning), hence realizing a deep bridge
		  linking two inseparable aspects of biological and robotic
		  spontaneous cognition. As a sideline, in this walk we
		  derive two alternative (with respect to the original Hebb
		  proposal) ways to recover the Hebbian paradigm, stemming
		  from ferromagnets and from spin-glasses, respectively.
		  Further, as these notes are thought of for an Engineering
		  audience, we highlight also the mappings between
		  ferromagnets and operational amplifiers and between
		  antiferromagnets and flip-flops (as neural networks -built
		  by op-amp and flip-flops- are particular spin-glasses and
		  the latter are indeed combinations of ferromagnets and
		  antiferromagnets), hoping that such a bridge plays as a
		  concrete prescription to capture the beauty of robotics
		  from the statistical mechanical perspective.},
  archiveprefix	= {arXiv},
  arxivid	= {1407.5300},
  author	= {Agliari, Elena and Barra, Adriano and Galluzzi, Andrea and
		  Tantari, Daniele and Tavani, Flavia},
  eprint	= {1407.5300},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1407.5300v1.pdf:pdf},
  month		= jul,
  pages		= {12},
  title		= {{A walk in the statistical mechanical formulation of
		  neural networks}},
  url		= {http://arxiv.org/abs/1407.5300},
  volume	= {1948},
  year		= {2014}
}

###Article{	  agliari2014,
  abstract	= {Neural networks are nowadays both powerful operational
		  tools (e.g., for pattern recognition, data mining, error
		  correction codes) and complex theoretical models on the
		  focus of scientific investigation. As for the research
		  branch, neural networks are handled and studied by
		  psychologists, neurobiologists, engineers, mathematicians
		  and theoretical physicists. In particular, in theoretical
		  physics, the key instrument for the quantitative analysis
		  of neural networks is statistical mechanics. From this
		  perspective, here, we first review attractor networks:
		  starting from ferromagnets and spin-glass models, we
		  discuss the underlying philosophy and we recover the strand
		  paved by Hopfield, Amit-Gutfreund-Sompolinky. One step
		  forward, we highlight the structural equivalence between
		  Hopfield networks (modeling retrieval) and Boltzmann
		  machines (modeling learning), hence realizing a deep bridge
		  linking two inseparable aspects of biological and robotic
		  spontaneous cognition. As a sideline, in this walk we
		  derive two alternative (with respect to the original Hebb
		  proposal) ways to recover the Hebbian paradigm, stemming
		  from ferromagnets and from spin-glasses, respectively.
		  Further, as these notes are thought of for an Engineering
		  audience, we highlight also the mappings between
		  ferromagnets and operational amplifiers and between
		  antiferromagnets and flip-flops (as neural networks -built
		  by op-amp and flip-flops- are particular spin-glasses and
		  the latter are indeed combinations of ferromagnets and
		  antiferromagnets), hoping that such a bridge plays as a
		  concrete prescription to capture the beauty of robotics
		  from the statistical mechanical perspective.},
  archiveprefix	= {arXiv},
  arxivid	= {1407.5300},
  author	= {Agliari, Elena and Barra, Adriano and Galluzzi, Andrea and
		  Tantari, Daniele and Tavani, Flavia},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {1407.5300},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1407.5300v1.pdf:pdf},
  month		= jul,
  pages		= {12},
  title		= {{A walk in the statistical mechanical formulation of
		  neural networks}},
  url		= {http://arxiv.org/abs/1407.5300},
  volume	= {1948},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1407.5300}
}

@InProceedings{	  ahn15,
  author	= {S. Ahn and A. Korattikara and N. Liu and S. Rajan and M.
		  Welling },
  title		= {Large-Scale Distributed {B}ayesian Matrix Factorization
		  using Stochastic Gradient {MCMC}},
  booktitle	= {ACM SIGKDD Conference on Knowledge Discovery and Data
		  Mining},
  year		= {2015},
  month		= aug
}

###InProceedings{ ahn15,
  author	= {S. Ahn and A. Korattikara and N. Liu and S. Rajan and M.
		  Welling},
  booktitle	= {ACM SIGKDD Conference on Knowledge Discovery and Data
		  Mining},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= aug,
  title		= {Large-Scale Distributed {B}ayesian Matrix Factorization
		  using Stochastic Gradient {MCMC}},
  year		= {2015}
}

@InProceedings{	  ahnkorwel2012,
  author	= {S. Ahn and A. Korattikara and M. Welling},
  title		= {Bayesian Posterior Sampling via Stochastic Gradient
		  {F}isher Scoring},
  booktitle	= {International Conference on Machine Learning},
  year		= {2012},
  month		= jun
}

###InProceedings{ ahnkorwel2012,
  author	= {S. Ahn and A. Korattikara and M. Welling},
  booktitle	= {International Conference on Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= jun,
  title		= {Bayesian Posterior Sampling via Stochastic Gradient
		  {F}isher Scoring},
  year		= {2012}
}

@InProceedings{	  ahnshawel2014,
  author	= {S. Ahn and B. Shahbaba and M. Welling},
  title		= {Distributed Stochastic Gradient {MCMC}},
  booktitle	= {International Conference on Machine Learning},
  year		= {2014},
  month		= jun
}

###InProceedings{ ahnshawel2014,
  author	= {S. Ahn and B. Shahbaba and M. Welling},
  booktitle	= {International Conference on Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= jun,
  title		= {Distributed Stochastic Gradient {MCMC}},
  year		= {2014}
}

@InCollection{	  alexnet,
  title		= {ImageNet Classification with Deep Convolutional Neural
		  Networks},
  author	= {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in Neural Information Processing Systems 25},
  editor	= {F. Pereira and C. J. C. Burges and L. Bottou and K. Q.
		  Weinberger},
  pages		= {1097--1105},
  year		= {2012},
  publisher	= {Curran Associates, Inc.},
  url		= {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

###InCollection{  alexnet,
  author	= {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in Neural Information Processing Systems 25},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {F. Pereira and C. J. C. Burges and L. Bottou and K. Q.
		  Weinberger},
  pages		= {1097--1105},
  publisher	= {Curran Associates, Inc.},
  title		= {ImageNet Classification with Deep Convolutional Neural
		  Networks},
  url		= {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
  year		= {2012},
  bdsk-url-1	= {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

###InCollection{  alexnet,
  title		= {ImageNet Classification with Deep Convolutional Neural
		  Networks},
  author	= {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in Neural Information Processing Systems 25},
  editor	= {F. Pereira and C. J. C. Burges and L. Bottou and K. Q.
		  Weinberger},
  pages		= {1097--1105},
  year		= {2012},
  publisher	= {Curran Associates, Inc.},
  url		= {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@Article{	  alstott2014,
  abstract	= {Power laws are theoretically interesting probability
		  distributions that are also frequently used to describe
		  empirical data. In recent years, effective statistical
		  methods for fitting power laws have been developed, but
		  appropriate use of these techniques requires significant
		  programming and statistical insight. In order to greatly
		  decrease the barriers to using good statistical methods for
		  fitting power law distributions, we developed the powerlaw
		  Python package. This software package provides easy
		  commands for basic fitting and statistical analysis of
		  distributions. Notably, it also seeks to support a variety
		  of user needs by being exhaustive in the options available
		  to the user. The source code is publicly available and
		  easily extensible.},
  archiveprefix	= {arXiv},
  arxivid	= {1305.0215},
  author	= {Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  doi		= {10.1371/journal.pone.0085777},
  eprint	= {1305.0215},
  file		= {:Users/ugur/Dropbox/.dropbox.cache/2014-12-19/1305.0215
		  (deleted f32e5ac253e200eb391ae096609e5fc4).pdf:pdf},
  issn		= {1932-6203},
  journal	= {PloS one},
  keywords	= {Research Design,Software,Statistical Distributions},
  month		= jan,
  number	= {1},
  pages		= {e85777},
  pmid		= {24489671},
  title		= {{Powerlaw: a Python package for analysis of heavy-tailed
		  distributions.}},
  url		= {http://arxiv.org/abs/1305.0215
		  http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3906378\&tool=pmcentrez\&rendertype=abstract},
  volume	= {9},
  year		= {2014}
}

###Article{	  alstott2014,
  abstract	= {Power laws are theoretically interesting probability
		  distributions that are also frequently used to describe
		  empirical data. In recent years, effective statistical
		  methods for fitting power laws have been developed, but
		  appropriate use of these techniques requires significant
		  programming and statistical insight. In order to greatly
		  decrease the barriers to using good statistical methods for
		  fitting power law distributions, we developed the powerlaw
		  Python package. This software package provides easy
		  commands for basic fitting and statistical analysis of
		  distributions. Notably, it also seeks to support a variety
		  of user needs by being exhaustive in the options available
		  to the user. The source code is publicly available and
		  easily extensible.},
  archiveprefix	= {arXiv},
  arxivid	= {1305.0215},
  author	= {Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1371/journal.pone.0085777},
  eprint	= {1305.0215},
  file		= {:Users/ugur/Dropbox/.dropbox.cache/2014-12-19/1305.0215
		  (deleted f32e5ac253e200eb391ae096609e5fc4).pdf:pdf},
  issn		= {1932-6203},
  journal	= {PloS one},
  keywords	= {Research Design,Software,Statistical Distributions},
  month		= jan,
  number	= {1},
  pages		= {e85777},
  pmid		= {24489671},
  title		= {{Powerlaw: a Python package for analysis of heavy-tailed
		  distributions.}},
  url		= {http://arxiv.org/abs/1305.0215
		  http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3906378\&tool=pmcentrez\&rendertype=abstract},
  volume	= {9},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1305.0215%20http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3906378%5C&tool=pmcentrez%5C&rendertype=abstract},
  bdsk-url-2	= {https://doi.org/10.1371/journal.pone.0085777}
}

@Article{	  ambady1993half,
  title		= {Half a minute: Predicting teacher evaluations from thin
		  slices of nonverbal behavior and physical attractiveness.},
  author	= {Ambady, Nalini and Rosenthal, Robert},
  journal	= {Journal of personality and social psychology},
  volume	= {64},
  number	= {3},
  pages		= {431},
  year		= {1993},
  publisher	= {American Psychological Association}
}

###Article{	  ambady1993half,
  author	= {Ambady, Nalini and Rosenthal, Robert},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of personality and social psychology},
  number	= {3},
  pages		= {431},
  publisher	= {American Psychological Association},
  title		= {Half a minute: Predicting teacher evaluations from thin
		  slices of nonverbal behavior and physical attractiveness.},
  volume	= {64},
  year		= {1993}
}

@Article{	  arora2018optimization,
  title		= {On the optimization of deep networks: Implicit
		  acceleration by overparameterization},
  author	= {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  journal	= {arXiv preprint arXiv:1802.06509},
  year		= {2018}
}

@Article{	  arous2018algorithmic,
  title		= {Algorithmic thresholds for tensor PCA},
  author	= {Arous, G\'erard Ben and Gheissari, Reza and Jagannath,
		  Aukosh},
  journal	= {arXiv preprint arXiv:1808.00921},
  year		= {2018}
}

@Article{	  arpit2017closer,
  title		= {A closer look at memorization in deep networks},
  author	= {Arpit, Devansh and {Jastrz{\k{e}}bski}, {Stanis{\l}aw} and
		  Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and
		  Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and
		  Courville, Aaron and Bengio, Yoshua and others},
  journal	= {arXiv preprint arXiv:1706.05394},
  year		= {2017}
}

###Article{	  arpit2017closer,
  title		= {A closer look at memorization in deep networks},
  author	= {Arpit, Devansh and {Jastrz{\k{e}}bski}, {Stanis{\l}aw} and
		  Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and
		  Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and
		  Courville, Aaron and Bengio, Yoshua and others},
  journal	= {arXiv preprint arXiv:1706.05394},
  year		= {2017}
}

@Article{	  asylum_preprint,
  title		= {Early Predictability of Asylum Court Decisions},
  author	= {Sagun, Levent and Dunn, Matt and Sirin, Hale and Chen,
		  Daniel},
  journal	= {Preprint},
  year		= {2016}
}

@Book{		  atkinson,
  title		= {{An Introduction to Numerical Analysis}},
  publisher	= {Wiley India Pvt. Limited},
  year		= {2008},
  edition	= 2,
  author	= {Atkinson, K.E.},
  address	= {New Delhi}
}

###Book{	  atkinson,
  address	= {New Delhi},
  author	= {Atkinson, K.E.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  edition	= 2,
  publisher	= {Wiley India Pvt. Limited},
  title		= {{An Introduction to Numerical Analysis}},
  year		= {2008}
}

@Article{	  auffinger2010,
  abstract	= {We give an asymptotic evaluation of the complexity of
		  spherical p-spin spin-glass models via random matrix
		  theory. This study enables us to obtain detailed
		  information about the bottom of the energy landscape,
		  including the absolute minimum (the ground state), the
		  other local minima, and describe an interesting layered
		  structure of the low critical values for the Hamiltonians
		  of these models. We also show that our approach allows us
		  to compute the related TAP-complexity and extend the
		  results known in the physics literature. As an independent
		  tool, we prove a LDP for the k-th largest eigenvalue of the
		  GOE, extending the results of Ben Arous, Dembo and
		  Guionnett (2001).},
  archiveprefix	= {arXiv},
  arxivid	= {1003.1129},
  author	= {Auffinger, Antonio and {Ben Arous}, G\'{e}rard and
		  \v{C}ern\'{y}, Ji\u{r}\'{\i}},
  doi		= {10.1002/cpa.21422},
  eprint	= {1003.1129},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1003.1129v2.pdf:pdf},
  issn		= {00103640},
  journal	= {Communications on Pure and Applied Mathematics},
  month		= feb,
  number	= {2},
  pages		= {165--201},
  title		= {{Random Matrices and Complexity of Spin Glasses}},
  url		= {http://doi.wiley.com/10.1002/cpa.21422},
  volume	= {66},
  year		= {2013}
}

###Article{	  auffinger2010,
  abstract	= {We give an asymptotic evaluation of the complexity of
		  spherical p-spin spin-glass models via random matrix
		  theory. This study enables us to obtain detailed
		  information about the bottom of the energy landscape,
		  including the absolute minimum (the ground state), the
		  other local minima, and describe an interesting layered
		  structure of the low critical values for the Hamiltonians
		  of these models. We also show that our approach allows us
		  to compute the related TAP-complexity and extend the
		  results known in the physics literature. As an independent
		  tool, we prove a LDP for the k-th largest eigenvalue of the
		  GOE, extending the results of Ben Arous, Dembo and
		  Guionnett (2001).},
  archiveprefix	= {arXiv},
  arxivid	= {1003.1129},
  author	= {Auffinger, Antonio and {Ben Arous}, G\'{e}rard and
		  \v{C}ern\'{y}, Ji\u{r}\'{\i}},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1002/cpa.21422},
  eprint	= {1003.1129},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1003.1129v2.pdf:pdf},
  issn		= {00103640},
  journal	= {Communications on Pure and Applied Mathematics},
  month		= feb,
  number	= {2},
  pages		= {165--201},
  title		= {{Random Matrices and Complexity of Spin Glasses}},
  url		= {http://doi.wiley.com/10.1002/cpa.21422},
  volume	= {66},
  year		= {2013},
  bdsk-url-1	= {http://doi.wiley.com/10.1002/cpa.21422},
  bdsk-url-2	= {https://doi.org/10.1002/cpa.21422}
}

@Article{	  auffinger2013,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:1110.5872v3},
  author	= {Auffinger, Antonio and {Ben Arous}, G\'{e}rard},
  doi		= {10.1214/13-AOP862},
  eprint	= {arXiv:1110.5872v3},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1110.5872v3.pdf:pdf},
  issn		= {0091-1798},
  journal	= {The Annals of Probability},
  keywords	= {15A52,60G60,82D30,Parisi formula,Sample,and
		  phrases,critical points,parisi,random matrices,sample,spin
		  glasses},
  month		= nov,
  number	= {6},
  pages		= {4214--4247},
  title		= {{Complexity of random smooth functions on the
		  high-dimensional sphere}},
  url		= {http://projecteuclid.org/euclid.aop/1384957786},
  volume	= {41},
  year		= {2013}
}

###Article{	  auffinger2013,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:1110.5872v3},
  author	= {Auffinger, Antonio and {Ben Arous}, G\'{e}rard},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1214/13-AOP862},
  eprint	= {arXiv:1110.5872v3},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1110.5872v3.pdf:pdf},
  issn		= {0091-1798},
  journal	= {The Annals of Probability},
  keywords	= {15A52,60G60,82D30,Parisi formula,Sample,and
		  phrases,critical points,parisi,random matrices,sample,spin
		  glasses},
  month		= nov,
  number	= {6},
  pages		= {4214--4247},
  title		= {{Complexity of random smooth functions on the
		  high-dimensional sphere}},
  url		= {http://projecteuclid.org/euclid.aop/1384957786},
  volume	= {41},
  year		= {2013},
  bdsk-url-1	= {http://projecteuclid.org/euclid.aop/1384957786},
  bdsk-url-2	= {https://doi.org/10.1214/13-AOP862}
}

@Article{	  auffinger2013random,
  title		= {Random matrices and complexity of spin glasses},
  author	= {Auffinger, Antonio and Ben Arous, G{\'e}rard and
		  {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  journal	= {Communications on Pure and Applied Mathematics},
  volume	= {66},
  number	= {2},
  pages		= {165--201},
  year		= {2013},
  publisher	= {Wiley Online Library}
}

###Article{	  auffinger2013random,
  author	= {Auffinger, Antonio and Ben Arous, G{\'e}rard and
		  {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Communications on Pure and Applied Mathematics},
  number	= {2},
  pages		= {165--201},
  publisher	= {Wiley Online Library},
  title		= {Random matrices and complexity of spin glasses},
  volume	= {66},
  year		= {2013}
}

###Article{	  auffinger2013random,
  title		= {Random matrices and complexity of spin glasses},
  author	= {Auffinger, Antonio and Ben Arous, G{\'e}rard and
		  {\v{C}}ern{\'y}, Ji{\v{r}}{\'\i}},
  journal	= {Communications on Pure and Applied Mathematics},
  volume	= {66},
  number	= {2},
  pages		= {165--201},
  year		= {2013},
  publisher	= {Wiley Online Library}
}

###Article{	  auffinger2013random,
  title		= {Random matrices and complexity of spin glasses},
  author	= {Auffinger, Antonio and Ben Arous, {G{\'e}rard} and
		  {\v{C}}ern{\`y}, Ji{\v{r}}{\'\i}},
  journal	= {Communications on Pure and Applied Mathematics},
  volume	= {66},
  number	= {2},
  pages		= {165--201},
  year		= {2013},
  publisher	= {Wiley Online Library}
}

@Article{	  auffinger2014,
  abstract	= {We investigate both free energy and complexity of the
		  spherical bipartite spin glass model. We first prove a
		  variational formula in high temperature for the limiting
		  free energy based on the well-known Crisanti-Sommers
		  representation of the mixed p-spin spherical model. Next,
		  we show that the mean number of local minima at low levels
		  of energy is exponentially large in the size of the system
		  and we derive a bound on the location of the ground state
		  energy.},
  archiveprefix	= {arXiv},
  arxivid	= {1405.2321},
  author	= {Auffinger, Antonio and Chen, Wei-kuo},
  doi		= {10.1007/s10955-014-1073-0},
  eprint	= {1405.2321},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1405.2321v1.pdf:pdf},
  issn		= {0022-4715},
  journal	= {Journal of Statistical Physics},
  month		= jul,
  number	= {1},
  pages		= {40--59},
  title		= {{Free Energy and Complexity of Spherical Bipartite
		  Models}},
  url		= {http://link.springer.com/10.1007/s10955-014-1073-0},
  volume	= {157},
  year		= {2014}
}

###Article{	  auffinger2014,
  abstract	= {We investigate both free energy and complexity of the
		  spherical bipartite spin glass model. We first prove a
		  variational formula in high temperature for the limiting
		  free energy based on the well-known Crisanti-Sommers
		  representation of the mixed p-spin spherical model. Next,
		  we show that the mean number of local minima at low levels
		  of energy is exponentially large in the size of the system
		  and we derive a bound on the location of the ground state
		  energy.},
  archiveprefix	= {arXiv},
  arxivid	= {1405.2321},
  author	= {Auffinger, Antonio and Chen, Wei-kuo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1007/s10955-014-1073-0},
  eprint	= {1405.2321},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1405.2321v1.pdf:pdf},
  issn		= {0022-4715},
  journal	= {Journal of Statistical Physics},
  month		= jul,
  number	= {1},
  pages		= {40--59},
  title		= {{Free Energy and Complexity of Spherical Bipartite
		  Models}},
  url		= {http://link.springer.com/10.1007/s10955-014-1073-0},
  volume	= {157},
  year		= {2014},
  bdsk-url-1	= {http://link.springer.com/10.1007/s10955-014-1073-0},
  bdsk-url-2	= {https://doi.org/10.1007/s10955-014-1073-0}
}

@Article{	  baik2005phase,
  title		= {Phase transition of the largest eigenvalue for nonnull
		  complex sample covariance matrices},
  author	= {Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e},
		  Sandrine and others},
  journal	= {The Annals of Probability},
  volume	= {33},
  number	= {5},
  pages		= {1643--1697},
  year		= {2005},
  publisher	= {Institute of Mathematical Statistics}
}

###Article{	  baik2005phase,
  author	= {Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e},
		  Sandrine and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {The Annals of Probability},
  number	= {5},
  pages		= {1643--1697},
  publisher	= {Institute of Mathematical Statistics},
  title		= {Phase transition of the largest eigenvalue for nonnull
		  complex sample covariance matrices},
  volume	= {33},
  year		= {2005}
}

###Article{	  baik2005phase,
  title		= {Phase transition of the largest eigenvalue for nonnull
		  complex sample covariance matrices},
  author	= {Baik, Jinho and Ben Arous, {G{\'e}rard} and
		  {P{\'e}ch{\'e}}, Sandrine and others},
  journal	= {The Annals of Probability},
  volume	= {33},
  number	= {5},
  pages		= {1643--1697},
  year		= {2005},
  publisher	= {Institute of Mathematical Statistics}
}

@InProceedings{	  baity18,
  title		= {Comparing Dynamics: Deep Neural Networks versus Glassy
		  Systems},
  author	= {Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and
		  Spigler, Stefano and Arous, Gerard Ben and Cammarota,
		  Chiara and LeCun, Yann and Wyart, Matthieu and Biroli,
		  Giulio},
  booktitle	= {Proceedings of the 35th International Conference on
		  Machine Learning},
  pages		= {314--323},
  year		= {2018},
  editor	= {Dy, Jennifer and Krause, Andreas},
  volume	= {80},
  series	= {Proceedings of Machine Learning Research},
  address	= {Stockholmsmässan, Stockholm Sweden},
  month		= {10--15 Jul},
  publisher	= {PMLR},
  pdf		= {http://proceedings.mlr.press/v80/baity-jesi18a/baity-jesi18a.pdf},
  url		= {http://proceedings.mlr.press/v80/baity-jesi18a.html},
  abstract	= {We analyze numerically the training dynamics of deep
		  neural networks (DNN) by using methods developed in
		  statistical physics of glassy systems. The two main issues
		  we address are the complexity of the loss-landscape and of
		  the dynamics within it, and to what extent DNNs share
		  similarities with glassy systems. Our findings, obtained
		  for different architectures and data-sets, suggest that
		  during the training process the dynamics slows down because
		  of an increasingly large number of flat directions. At
		  large times, when the loss is approaching zero, the system
		  diffuses at the bottom of the landscape. Despite some
		  similarities with the dynamics of mean-field glassy
		  systems, in particular, the absence of barrier crossing, we
		  find distinctive dynamical behaviors in the two cases, thus
		  showing that the statistical properties of the
		  corresponding loss and energy landscapes are different. In
		  contrast, when the network is under-parametrized we observe
		  a typical glassy behavior, thus suggesting the existence of
		  different phases depending on whether the network is
		  under-parametrized or over-parametrized.}
}

###InProceedings{ baity18,
  title		= {Comparing Dynamics: Deep Neural Networks versus Glassy
		  Systems},
  author	= {Baity-Jesi, Marco and Sagun, Levent and Geiger, Mario and
		  Spigler, Stefano and Arous, G\'erard Ben and Cammarota,
		  Chiara and LeCun, Yann and Wyart, Matthieu and Biroli,
		  Giulio},
  booktitle	= {Proceedings of the 35th International Conference on
		  Machine Learning},
  pages		= {314--323},
  year		= {2018},
  editor	= {Dy, Jennifer and Krause, Andreas},
  volume	= {80},
  series	= {Proceedings of Machine Learning Research},
  address	= {Stockholmsmässan, Stockholm Sweden},
  month		= {10--15 Jul},
  publisher	= {PMLR},
  pdf		= {http://proceedings.mlr.press/v80/baity-jesi18a/baity-jesi18a.pdf},
  url		= {http://proceedings.mlr.press/v80/baity-jesi18a.html},
  abstract	= {We analyze numerically the training dynamics of deep
		  neural networks (DNN) by using methods developed in
		  statistical physics of glassy systems. The two main issues
		  we address are the complexity of the loss-landscape and of
		  the dynamics within it, and to what extent DNNs share
		  similarities with glassy systems. Our findings, obtained
		  for different architectures and data-sets, suggest that
		  during the training process the dynamics slows down because
		  of an increasingly large number of flat directions. At
		  large times, when the loss is approaching zero, the system
		  diffuses at the bottom of the landscape. Despite some
		  similarities with the dynamics of mean-field glassy
		  systems, in particular, the absence of barrier crossing, we
		  find distinctive dynamical behaviors in the two cases, thus
		  showing that the statistical properties of the
		  corresponding loss and energy landscapes are different. In
		  contrast, when the network is under-parametrized we observe
		  a typical glassy behavior, thus suggesting the existence of
		  different phases depending on whether the network is
		  under-parametrized or over-parametrized.}
}

@Article{	  bak,
  author	= {Murray, Adam},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/SI
		  Lecture 10.pdf:pdf},
  title		= {{How Nature Works}}
}

###Article{	  bak,
  author	= {Murray, Adam},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/SI
		  Lecture 10.pdf:pdf},
  title		= {{How Nature Works}}
}

@Article{	  bakhtin2012neural,
  title		= {A neural computation model for decision-making times},
  author	= {Bakhtin, Yuri and Correll, Joshua},
  journal	= {Journal of Mathematical Psychology},
  volume	= {56},
  number	= {5},
  pages		= {333--340},
  year		= {2012},
  publisher	= {Elsevier}
}

###Article{	  bakhtin2012neural,
  author	= {Bakhtin, Yuri and Correll, Joshua},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of Mathematical Psychology},
  number	= {5},
  pages		= {333--340},
  publisher	= {Elsevier},
  title		= {A neural computation model for decision-making times},
  volume	= {56},
  year		= {2012}
}

###Article{	  bakhtin2012neural,
  title		= {A neural computation model for decision-making times},
  author	= {Bakhtin, Yuri and Correll, Joshua},
  journal	= {Journal of Mathematical Psychology},
  volume	= {56},
  number	= {5},
  pages		= {333--340},
  year		= {2012},
  publisher	= {Elsevier}
}

@Book{		  bakry2013analysis,
  title		= {Analysis and geometry of {M}arkov diffusion operators},
  author	= {Bakry, Dominique and Gentil, Ivan and Ledoux, Michel},
  volume	= {348},
  year		= {2013},
  publisher	= {Springer Science \& Business Media}
}

###Book{	  bakry2013analysis,
  author	= {Bakry, Dominique and Gentil, Ivan and Ledoux, Michel},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {Springer Science \& Business Media},
  title		= {Analysis and geometry of {M}arkov diffusion operators},
  volume	= {348},
  year		= {2013}
}

@Article{	  baldassi2016unreasonable,
  title		= {Unreasonable effectiveness of learning neural networks:
		  {From} accessible states and robust ensembles to basic
		  algorithmic schemes},
  volume	= {113},
  issn		= {0027-8424, 1091-6490},
  shorttitle	= {Unreasonable effectiveness of learning neural networks},
  doi		= {10.1073/pnas.1608103113},
  language	= {en},
  number	= {48},
  urldate	= {2016-11-30},
  journal	= {Proceedings of the National Academy of Sciences},
  author	= {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer
		  T. and Ingrosso, Alessandro and Lucibello, Carlo and
		  Saglietti, Luca and Zecchina, Riccardo},
  month		= nov,
  year		= {2016},
  pages		= {E7655--E7662}
}

###Article{	  baldassi2016unreasonable,
  author	= {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer
		  T. and Ingrosso, Alessandro and Lucibello, Carlo and
		  Saglietti, Luca and Zecchina, Riccardo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1073/pnas.1608103113},
  issn		= {0027-8424, 1091-6490},
  journal	= {Proceedings of the National Academy of Sciences},
  language	= {en},
  month		= nov,
  number	= {48},
  pages		= {E7655--E7662},
  shorttitle	= {Unreasonable effectiveness of learning neural networks},
  title		= {Unreasonable effectiveness of learning neural networks:
		  {From} accessible states and robust ensembles to basic
		  algorithmic schemes},
  urldate	= {2016-11-30},
  volume	= {113},
  year		= {2016},
  bdsk-url-1	= {https://doi.org/10.1073/pnas.1608103113}
}

###Article{	  baldassi2016unreasonable,
  title		= {Unreasonable effectiveness of learning neural networks:
		  {From} accessible states and robust ensembles to basic
		  algorithmic schemes},
  volume	= {113},
  issn		= {0027-8424, 1091-6490},
  shorttitle	= {Unreasonable effectiveness of learning neural networks},
  doi		= {10.1073/pnas.1608103113},
  language	= {en},
  number	= {48},
  urldate	= {2016-11-30},
  journal	= {Proceedings of the National Academy of Sciences},
  author	= {Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer
		  T. and Ingrosso, Alessandro and Lucibello, Carlo and
		  Saglietti, Luca and Zecchina, Riccardo},
  month		= nov,
  year		= {2016},
  pages		= {E7655--E7662}
}

@Article{	  balduzzi16,
  title		= {Deep online convex optimization with gated games},
  author	= {Balduzzi, David},
  journal	= {arXiv preprint arXiv:1604.01952},
  year		= {2016}
}

###Article{	  balduzzi16,
  author	= {Balduzzi, David},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1604.01952},
  title		= {Deep online convex optimization with gated games},
  year		= {2016}
}

###Article{	  balduzzi16,
  title		= {Deep online convex optimization with gated games},
  author	= {Balduzzi, David},
  journal	= {arXiv preprint arXiv:1604.01952},
  year		= {2016}
}

@Article{	  balduzzi17,
  title		= {The Shattered Gradients Problem: If resnets are the
		  answer, then what is the question?},
  author	= {Balduzzi, David and Frean, Marcus and Leary, Lennox and
		  Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  journal	= {arXiv preprint arXiv:1702.08591},
  year		= {2017}
}

###Article{	  balduzzi17,
  author	= {Balduzzi, David and Frean, Marcus and Leary, Lennox and
		  Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1702.08591},
  title		= {The Shattered Gradients Problem: If resnets are the
		  answer, then what is the question?},
  year		= {2017}
}

###Article{	  balduzzi17,
  title		= {The Shattered Gradients Problem: If resnets are the
		  answer, then what is the question?},
  author	= {Balduzzi, David and Frean, Marcus and Leary, Lennox and
		  Lewis, JP and Ma, Kurt Wan-Duo and McWilliams, Brian},
  journal	= {arXiv preprint arXiv:1702.08591},
  year		= {2017}
}

@Article{	  ballard17,
  title		= {Energy landscapes for machine learning},
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  journal	= {Physical Chemistry Chemical Physics},
  year		= {2017},
  publisher	= {Royal Society of Chemistry}
}

###Article{	  ballard17,
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Chemistry Chemical Physics},
  publisher	= {Royal Society of Chemistry},
  title		= {Energy landscapes for machine learning},
  year		= {2017}
}

###Article{	  ballard17,
  title		= {Energy landscapes for machine learning},
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  journal	= {Physical Chemistry Chemical Physics},
  year		= {2017},
  publisher	= {Royal Society of Chemistry}
}

@Article{	  ballard2017energy,
  title		= {Energy landscapes for machine learning},
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  journal	= {Physical Chemistry Chemical Physics},
  year		= {2017},
  publisher	= {Royal Society of Chemistry}
}

###Article{	  ballard2017energy,
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Chemistry Chemical Physics},
  publisher	= {Royal Society of Chemistry},
  title		= {Energy landscapes for machine learning},
  year		= {2017}
}

@Article{	  ballard2017perspective,
  title		= {Perspective: Energy Landscapes for Machine Learning},
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  journal	= {arXiv preprint arXiv:1703.07915},
  year		= {2017}
}

###Article{	  ballard2017perspective,
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1703.07915},
  title		= {Perspective: Energy Landscapes for Machine Learning},
  year		= {2017}
}

###Article{	  ballard2017perspective,
  title		= {Perspective: Energy Landscapes for Machine Learning},
  author	= {Ballard, Andrew J and Das, Ritankar and Martiniani,
		  Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson,
		  Jacob D and Wales, David J},
  journal	= {arXiv preprint arXiv:1703.07915},
  year		= {2017}
}

@Article{	  bansal2018minnorm,
  title		= {Minnorm training: an algorithm for training
		  over-parameterized deep neural networks.},
  author	= {Bansal, Yamini and Advani, Madhu and Cox, David D and
		  Saxe, Andrew M},
  journal	= {CoRR},
  year		= {2018}
}

###Article{	  bansal2018minnorm,
  title		= {Minnorm training: an algorithm for training
		  over-parameterized deep neural networks.},
  author	= {Bansal, Yamini and Advani, Madhu and Cox, David D and
		  Saxe, Andrew M},
  journal	= {CoRR},
  year		= {2018}
}

@Misc{		  bastien-theano-2012,
  author	= {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and
		  Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J.
		  and Bergeron, Arnaud and Bouchard, Nicolas and Bengio,
		  Yoshua},
  title		= {Theano: new features and speed improvements},
  year		= {2012},
  howpublished	= {Deep Learning and Unsupervised Feature Learning NIPS 2012
		  Workshop},
  abstract	= {Theano is a linear algebra compiler that optimizes a
		  userâ€™s symbolically-speciï¬ed mathematical
		  computations to produce efï¬cient low-level
		  implementations. In this paper, we present new features and
		  efï¬ciency improvements to Theano, and benchmarks
		  demonstrating Theanoâ€™s performance relative to
		  Torch7, a recently introduced machine learning library, and
		  to RNNLM, a C++ library targeted at recurrent neural
		  networks.}
}

###Misc{	  bastien-theano-2012,
  abstract	= {Theano is a linear algebra compiler that optimizes a
		  user{\^a}€{\texttrademark}s
		  symbolically-speci{\"\i}¬ed mathematical computations to
		  produce ef{\"\i}¬cient low-level implementations. In
		  this paper, we present new features and ef{\"\i}¬ciency
		  improvements to Theano, and benchmarks demonstrating
		  Theano{\^a}€{\texttrademark}s performance relative to
		  Torch7, a recently introduced machine learning library, and
		  to RNNLM, a C++ library targeted at recurrent neural networks.},
  author	= {Bastien, Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and
		  Pascanu, Razvan and Bergstra, James and Goodfellow, Ian J.
		  and Bergeron, Arnaud and Bouchard, Nicolas and Bengio,
		  Yoshua},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  howpublished	= {Deep Learning and Unsupervised Feature Learning NIPS 2012
		  Workshop},
  title		= {Theano: new features and speed improvements},
  year		= {2012}
}

@Article{	  baum88,
  title		= {On the capabilities of multilayer perceptrons},
  author	= {Baum, Eric B},
  journal	= {Journal of complexity},
  volume	= {4},
  number	= {3},
  pages		= {193--215},
  year		= {1988},
  publisher	= {Academic Press}
}

###Article{	  baum88,
  author	= {Baum, Eric B},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of complexity},
  number	= {3},
  pages		= {193--215},
  publisher	= {Academic Press},
  title		= {On the capabilities of multilayer perceptrons},
  volume	= {4},
  year		= {1988}
}

###Article{	  baum88,
  title		= {On the capabilities of multilayer perceptrons},
  author	= {Baum, Eric B},
  journal	= {Journal of complexity},
  volume	= {4},
  number	= {3},
  pages		= {193--215},
  year		= {1988},
  publisher	= {Academic Press}
}

@InCollection{	  beliakov2002,
  author	= {Beliakov, Gleb and Abraham, Ajith},
  booktitle	= {Hybrid Information Systems},
  doi		= {10.1007/978-3-7908-1782-9\_8},
  editor	= {Abraham, Ajith and K\"{o}ppen, Mario},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/BelA02.pdf:pdf},
  isbn		= {978-3-7908-1480-4},
  pages		= {79--92},
  publisher	= {Physica-Verlag HD},
  title		= {{Global Optimisation of Neural Networks Using a
		  Deterministic Hybrid Approach}},
  url		= {http://link.springer.com/chapter/10.1007\%2F978-3-7908-1782-9\_8},
  year		= {2002}
}

###InCollection{  beliakov2002,
  author	= {Beliakov, Gleb and Abraham, Ajith},
  booktitle	= {Hybrid Information Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1007/978-3-7908-1782-9\_8},
  editor	= {Abraham, Ajith and K\"{o}ppen, Mario},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/BelA02.pdf:pdf},
  isbn		= {978-3-7908-1480-4},
  pages		= {79--92},
  publisher	= {Physica-Verlag HD},
  title		= {{Global Optimisation of Neural Networks Using a
		  Deterministic Hybrid Approach}},
  url		= {http://link.springer.com/chapter/10.1007\%2F978-3-7908-1782-9\_8},
  year		= {2002},
  bdsk-url-1	= {http://link.springer.com/chapter/10.1007%5C%2F978-3-7908-1782-9%5C_8},
  bdsk-url-2	= {https://doi.org/10.1007/978-3-7908-1782-9%5C_8}
}

@Article{	  benarousj,
  title		= {Spectral gap estimates in mean field spin glasses},
  author	= {Ben Arous, G{\'e}rard and Jagannath, Aukosh},
  journal	= {arXiv preprint arXiv:1705.04243},
  year		= {2017}
}

###Article{	  benarousj,
  author	= {Ben Arous, G{\'e}rard and Jagannath, Aukosh},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1705.04243},
  title		= {Spectral gap estimates in mean field spin glasses},
  year		= {2017}
}

###Article{	  benarousj,
  title		= {Spectral gap estimates in mean field spin glasses},
  author	= {Ben Arous, {G{\'e}rard} and Jagannath, Aukosh},
  journal	= {arXiv preprint arXiv:1705.04243},
  year		= {2017}
}

@Article{	  bengio2009learning,
  title		= {Learning deep architectures for AI},
  author	= {Bengio, Yoshua and others},
  journal	= {Foundations and trends{\textregistered} in Machine
		  Learning},
  volume	= {2},
  number	= {1},
  pages		= {1--127},
  year		= {2009},
  publisher	= {Now Publishers, Inc.}
}

###Article{	  bengio2009learning,
  author	= {Bengio, Yoshua and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Foundations and trends{\textregistered} in Machine
		  Learning},
  number	= {1},
  pages		= {1--127},
  publisher	= {Now Publishers, Inc.},
  title		= {Learning deep architectures for AI},
  volume	= {2},
  year		= {2009}
}

###Article{	  bengio2009learning,
  title		= {Learning deep architectures for AI},
  author	= {Bengio, Yoshua and others},
  journal	= {Foundations and trends{\textregistered} in Machine
		  Learning},
  volume	= {2},
  number	= {1},
  pages		= {1--127},
  year		= {2009},
  publisher	= {Now Publishers, Inc.}
}

@InProceedings{	  bergstra+al:2010-scipy,
  author	= {Bergstra, James and Breuleux, Olivier and Bastien,
		  Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu,
		  Razvan and Desjardins, Guillaume and Turian, Joseph and
		  Warde-Farley, David and Bengio, Yoshua},
  month		= jun,
  title		= {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle	= {Proceedings of the Python for Scientific Computing
		  Conference ({SciPy})},
  year		= {2010},
  location	= {Austin, TX},
  note		= {Oral Presentation},
  abstract	= {Theano is a compiler for mathematical expressions in
		  Python that combines the convenience of NumPyâ€™s
		  syntax with the speed of optimized native machine language.
		  The user composes mathematical expressions in a high-level
		  description that mimics NumPyâ€™s syntax and
		  semantics, while being statically typed and functional (as
		  opposed to imperative). These expressions allow Theano to
		  provide symbolic differentiation. Before performing
		  computation, Theano optimizes the choice of expressions,
		  translates them into C++ (or CUDA for GPU), compiles them
		  into dynamically loaded Python modules, all automatically.
		  Common machine learning algorithms implemented with Theano
		  are from 1.6Ã— to 7.5Ã— faster than competitive
		  alternatives (including those implemented with C/C++,
		  NumPy/SciPy and MATLAB) when compiled for the CPU and
		  between 6.5Ã— and 44Ã— faster when compiled for the
		  GPU. This paper illustrates how to use Theano, outlines the
		  scope of the compiler, provides benchmarks on both CPU and
		  GPU processors, and explains its overall design.}
}

###InProceedings{ bergstra+al:2010-scipy,
  abstract	= {Theano is a compiler for mathematical expressions in
		  Python that combines the convenience of
		  NumPy{\^a}€{\texttrademark}s syntax with the speed of
		  optimized native machine language. The user composes
		  mathematical expressions in a high-level description that
		  mimics NumPy{\^a}€{\texttrademark}s syntax and semantics,
		  while being statically typed and functional (as opposed to
		  imperative). These expressions allow Theano to provide
		  symbolic differentiation. Before performing computation,
		  Theano optimizes the choice of expressions, translates them
		  into C++ (or CUDA for GPU), compiles them into dynamically
		  loaded Python modules, all automatically. Common machine
		  learning algorithms implemented with Theano are from
		  1.6{\~A}--- to 7.5{\~A}--- faster than competitive
		  alternatives (including those implemented with C/C++,
		  NumPy/SciPy and MATLAB) when compiled for the CPU and
		  between 6.5{\~A}--- and 44{\~A}--- faster when compiled for
		  the GPU. This paper illustrates how to use Theano, outlines
		  the scope of the compiler, provides benchmarks on both CPU
		  and GPU processors, and explains its overall design.},
  author	= {Bergstra, James and Breuleux, Olivier and Bastien,
		  Fr{\'{e}}d{\'{e}}ric and Lamblin, Pascal and Pascanu,
		  Razvan and Desjardins, Guillaume and Turian, Joseph and
		  Warde-Farley, David and Bengio, Yoshua},
  booktitle	= {Proceedings of the Python for Scientific Computing
		  Conference ({SciPy})},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  location	= {Austin, TX},
  month		= jun,
  note		= {Oral Presentation},
  title		= {Theano: a {CPU} and {GPU} Math Expression Compiler},
  year		= {2010}
}

@InProceedings{	  beutel2014flexifact,
  title		= {Flexifact: Scalable flexible factorization of coupled
		  tensors on {H}adoop},
  author	= {Beutel, A. and Talukdar, P. P. and Kumar, A. and
		  Faloutsos, C. and Papalexakis, E. E. and Xing, E. P.},
  booktitle	= {ICDM},
  pages		= {109--117},
  year		= {2014},
  organization	= {SIAM}
}

###InProceedings{ beutel2014flexifact,
  author	= {Beutel, A. and Talukdar, P. P. and Kumar, A. and
		  Faloutsos, C. and Papalexakis, E. E. and Xing, E. P.},
  booktitle	= {ICDM},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {SIAM},
  pages		= {109--117},
  title		= {Flexifact: Scalable flexible factorization of coupled
		  tensors on {H}adoop},
  year		= {2014}
}

@Article{	  bianchini14,
  title		= {On the complexity of neural network classifiers: A
		  comparison between shallow and deep architectures},
  author	= {Bianchini, Monica and Scarselli, Franco},
  journal	= {IEEE transactions on neural networks and learning
		  systems},
  volume	= {25},
  number	= {8},
  pages		= {1553--1565},
  year		= {2014},
  publisher	= {IEEE}
}

###Article{	  bianchini14,
  author	= {Bianchini, Monica and Scarselli, Franco},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {IEEE transactions on neural networks and learning
		  systems},
  number	= {8},
  pages		= {1553--1565},
  publisher	= {IEEE},
  title		= {On the complexity of neural network classifiers: A
		  comparison between shallow and deep architectures},
  volume	= {25},
  year		= {2014}
}

###Article{	  bianchini14,
  title		= {On the complexity of neural network classifiers: A
		  comparison between shallow and deep architectures},
  author	= {Bianchini, Monica and Scarselli, Franco},
  journal	= {IEEE transactions on neural networks and learning
		  systems},
  volume	= {25},
  number	= {8},
  pages		= {1553--1565},
  year		= {2014},
  publisher	= {IEEE}
}

@Article{	  bieroza2012new,
  title		= {New data mining and calibration approaches to the
		  assessment of water treatment efficiency},
  author	= {Bieroza, Magdalena and Baker, Andy and Bridgeman, John},
  journal	= {Advances in Engineering Software},
  volume	= {44},
  number	= {1},
  pages		= {126--135},
  year		= {2012},
  publisher	= {Elsevier}
}

###Article{	  bieroza2012new,
  author	= {Bieroza, Magdalena and Baker, Andy and Bridgeman, John},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Advances in Engineering Software},
  number	= {1},
  pages		= {126--135},
  publisher	= {Elsevier},
  title		= {New data mining and calibration approaches to the
		  assessment of water treatment efficiency},
  volume	= {44},
  year		= {2012}
}

@InCollection{	  birolileshouches,
  title		= {Slow relaxations and non-equilibrium dynamics in classical
		  and quantum systems},
  author	= {Biroli, Giulio},
  editor	= "Thierry Giamarchi, Andrew J. Millis, Olivier Parcollet",
  booktitle	= "Strongly Interacting Quantum Systems Out of Equilibrium",
  publisher	= "Oxford University Press",
  address	= "Oxford",
  year		= 2016,
  pages		= "207-261"
}

###InCollection{  birolileshouches,
  address	= {Oxford},
  author	= {Biroli, Giulio},
  booktitle	= {Strongly Interacting Quantum Systems Out of Equilibrium},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {Thierry Giamarchi, Andrew J. Millis, Olivier Parcollet},
  pages		= {207-261},
  publisher	= {Oxford University Press},
  title		= {Slow relaxations and non-equilibrium dynamics in classical
		  and quantum systems},
  year		= 2016
}

###InCollection{  birolileshouches,
  title		= {Slow relaxations and non-equilibrium dynamics in classical
		  and quantum systems},
  author	= {Biroli, Giulio},
  editor	= "Thierry Giamarchi, Andrew J. Millis, Olivier Parcollet",
  booktitle	= "Strongly Interacting Quantum Systems Out of Equilibrium",
  publisher	= "Oxford University Press",
  address	= "Oxford",
  year		= 2016,
  pages		= "207-261"
}

@Article{	  bjrem,
  title		= {Activated aging dynamics and effective trap model
		  description in the random energy model},
  author	= {Baity-Jesi, Marco and Biroli, Giulio and Cammarota,
		  Chiara},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  volume	= {2018},
  number	= {1},
  pages		= {013301},
  year		= {2018},
  publisher	= {IOP Publishing}
}

###Article{	  bjrem,
  author	= {Baity-Jesi, Marco and Biroli, Giulio and Cammarota,
		  Chiara},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {1},
  pages		= {013301},
  publisher	= {IOP Publishing},
  title		= {Activated aging dynamics and effective trap model
		  description in the random energy model},
  volume	= {2018},
  year		= {2018}
}

###Article{	  bjrem,
  title		= {Activated aging dynamics and effective trap model
		  description in the random energy model},
  author	= {Baity-Jesi, Marco and Biroli, Giulio and Cammarota,
		  Chiara},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  volume	= {2018},
  number	= {1},
  pages		= {013301},
  year		= {2018},
  publisher	= {IOP Publishing}
}

@Article{	  bless1996mood,
  title		= {Mood and stereotyping: Affective states and the use of
		  general knowledge structures},
  author	= {Bless, Herbert and Schwarz, Norbert and Kemmelmeier,
		  Markus},
  journal	= {European review of social psychology},
  volume	= {7},
  number	= {1},
  pages		= {63--93},
  year		= {1996},
  publisher	= {Taylor \& Francis}
}

###Article{	  bless1996mood,
  author	= {Bless, Herbert and Schwarz, Norbert and Kemmelmeier,
		  Markus},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {European review of social psychology},
  number	= {1},
  pages		= {63--93},
  publisher	= {Taylor \& Francis},
  title		= {Mood and stereotyping: Affective states and the use of
		  general knowledge structures},
  volume	= {7},
  year		= {1996}
}

@Article{	  bloemendal2016principal,
  title		= {On the principal components of sample covariance
		  matrices},
  author	= {Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer
		  and Yin, Jun},
  journal	= {Probability Theory and Related Fields},
  volume	= {164},
  number	= {1-2},
  pages		= {459--552},
  year		= {2016},
  publisher	= {Springer}
}

###Article{	  bloemendal2016principal,
  author	= {Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer
		  and Yin, Jun},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Probability Theory and Related Fields},
  number	= {1-2},
  pages		= {459--552},
  publisher	= {Springer},
  title		= {On the principal components of sample covariance
		  matrices},
  volume	= {164},
  year		= {2016}
}

###Article{	  bloemendal2016principal,
  title		= {On the principal components of sample covariance
		  matrices},
  author	= {Bloemendal, Alex and Knowles, Antti and Yau, Horng-Tzer
		  and Yin, Jun},
  journal	= {Probability Theory and Related Fields},
  volume	= {164},
  number	= {1-2},
  pages		= {459--552},
  year		= {2016},
  publisher	= {Springer}
}

@InProceedings{	  bolley2005weighted,
  title		= {Weighted {Csisz{\'a}r-Kullback-Pinsker} inequalities and
		  applications to transportation inequalities},
  author	= {Bolley, Fran{\c{c}}ois and Villani, C{\'e}dric},
  booktitle	= {ANNALES-FACULTE DES SCIENCES TOULOUSE MATHEMATIQUES},
  volume	= {14},
  number	= {3},
  pages		= {331},
  year		= {2005},
  organization	= {Universit{\'e} Paul Sabatier}
}

###InProceedings{ bolley2005weighted,
  author	= {Bolley, Fran{\c{c}}ois and Villani, C{\'e}dric},
  booktitle	= {ANNALES-FACULTE DES SCIENCES TOULOUSE MATHEMATIQUES},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  number	= {3},
  organization	= {Universit{\'e} Paul Sabatier},
  pages		= {331},
  title		= {Weighted {Csisz{\'a}r-Kullback-Pinsker} inequalities and
		  applications to transportation inequalities},
  volume	= {14},
  year		= {2005}
}

@InProceedings{	  bos1997dynamics,
  title		= {Dynamics of training},
  author	= {{B{\"o}s}, Siegfried and Opper, Manfred},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {141--147},
  year		= {1997}
}

###InProceedings{ bos1997dynamics,
  title		= {Dynamics of training},
  author	= {{B{\"o}s}, Siegfried and Opper, Manfred},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {141--147},
  year		= {1997}
}

@Article{	  bottou1991stochastic,
  title		= {Stochastic gradient learning in neural networks},
  author	= {Bottou, L{\'e}on},
  journal	= {Proceedings of Neuro-N{\i}mes},
  volume	= {91},
  number	= {8},
  year		= {1991}
}

###Article{	  bottou1991stochastic,
  author	= {Bottou, L{\'e}on},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Proceedings of Neuro-N{\i}mes},
  number	= {8},
  title		= {Stochastic gradient learning in neural networks},
  volume	= {91},
  year		= {1991}
}

###Article{	  bottou1991stochastic,
  title		= {Stochastic gradient learning in neural networks},
  author	= {Bottou, {L{\'e}on}},
  journal	= {Proceedings of Neuro-{N{\i}mes}},
  volume	= {91},
  number	= {8},
  year		= {1991}
}

@InProceedings{	  bottou2008tradeoffs,
  title		= {The tradeoffs of large scale learning},
  author	= {Bottou, L{\'e}on and Bousquet, Olivier},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {161--168},
  year		= {2008}
}

@InCollection{	  bottou2010,
  author	= {Bottou, L},
  booktitle	= {Proc. Int. Conf. Comput. Stat.},
  doi		= {10.1007/978-3-7908-2604-3_16},
  isbn		= {0269-2155},
  issn		= {0269-2155},
  pages		= {177--186},
  pmid		= {20876631},
  title		= {{Large-scale machine learning with stochastic gradient
		  descent}},
  year		= {2010}
}

###InCollection{  bottou2010,
  author	= {Bottou, L},
  booktitle	= {Proc. Int. Conf. Comput. Stat.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  doi		= {10.1007/978-3-7908-2604-3_16},
  isbn		= {0269-2155},
  issn		= {0269-2155},
  pages		= {177--186},
  pmid		= {20876631},
  title		= {{Large-scale machine learning with stochastic gradient
		  descent}},
  year		= {2010},
  bdsk-url-1	= {https://doi.org/10.1007/978-3-7908-2604-3_16}
}

@InCollection{	  bottou2010large,
  title		= {Large-scale machine learning with stochastic gradient
		  descent},
  author	= {Bottou, L{\'e}on},
  booktitle	= {Proceedings of COMPSTAT'2010},
  pages		= {177--186},
  year		= {2010},
  publisher	= {Physica-Verlag HD}
}

###InCollection{  bottou2010large,
  author	= {Bottou, L{\'e}on},
  booktitle	= {Proceedings of COMPSTAT'2010},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {177--186},
  publisher	= {Physica-Verlag HD},
  title		= {Large-scale machine learning with stochastic gradient
		  descent},
  year		= {2010}
}

###InCollection{  bottou2010large,
  title		= {Large-scale machine learning with stochastic gradient
		  descent},
  author	= {Bottou, L{\'e}on},
  booktitle	= {Proceedings of COMPSTAT'2010},
  pages		= {177--186},
  year		= {2010},
  publisher	= {Physica-Verlag HD}
}

###InCollection{  bottou2010large,
  title		= {Large-scale machine learning with stochastic gradient
		  descent},
  author	= {Bottou, {L{\'e}on}},
  booktitle	= {Proceedings of COMPSTAT'2010},
  pages		= {177--186},
  year		= {2010},
  publisher	= {Physica-Verlag HD}
}

@Article{	  bourrely1988neural,
  title		= {Parallelization of a neural learning algorithm on a
		  Hypercube},
  author	= {Jean Bourrely},
  journal	= {Hypercube and Distributed Computers},
  year		= {1988}
}

###Article{	  bourrely1988neural,
  author	= {Jean Bourrely},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Hypercube and Distributed Computers},
  title		= {Parallelization of a neural learning algorithm on a
		  Hypercube},
  year		= {1988}
}

@Article{	  bourrely1989parallelization,
  title		= {Parallelization of a neural network learning algorithm on
		  a hypercube},
  author	= {Bourrely, J},
  journal	= {Hypercube and distributed computers. Elsiever Science
		  Publishing},
  year		= {1989}
}

###Article{	  bourrely1989parallelization,
  author	= {Bourrely, J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Hypercube and distributed computers. Elsiever Science
		  Publishing},
  title		= {Parallelization of a neural network learning algorithm on
		  a hypercube},
  year		= {1989}
}

###Article{	  bourrely1989parallelization,
  title		= {Parallelization of a neural network learning algorithm on
		  a hypercube},
  author	= {Bourrely, J},
  journal	= {Hypercube and distributed computers. Elsiever Science
		  Publishing},
  year		= {1989}
}

@Article{	  brito18a,
  title		= {Universality of jamming of non-spherical particles},
  author	= {Brito, Carolina and Ikeda, Harukuni and Urbani,
		  Pierfrancesco and Wyart, Matthieu and Zamponi, Francesco},
  journal	= {arXiv preprint arXiv:1807.01975},
  year		= {2018}
}

###Article{	  brito18a,
  author	= {Brito, Carolina and Ikeda, Harukuni and Urbani,
		  Pierfrancesco and Wyart, Matthieu and Zamponi, Francesco},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1807.01975},
  title		= {Universality of jamming of non-spherical particles},
  year		= {2018}
}

###Article{	  brito18a,
  title		= {Universality of jamming of non-spherical particles},
  author	= {Brito, Carolina and Ikeda, Harukuni and Urbani,
		  Pierfrancesco and Wyart, Matthieu and Zamponi, Francesco},
  journal	= {arXiv preprint arXiv:1807.01975},
  year		= {2018}
}

@Article{	  brito2018theory,
  title		= {Theory for Swap Acceleration near the Glass and Jamming
		  Transitions},
  author	= {Brito, Carolina and Lerner, Edan and Wyart, Matthieu},
  journal	= {arXiv preprint arXiv:1801.03796},
  year		= {2018}
}

###Article{	  brito2018theory,
  title		= {Theory for Swap Acceleration near the Glass and Jamming
		  Transitions},
  author	= {Brito, Carolina and Lerner, Edan and Wyart, Matthieu},
  journal	= {arXiv preprint arXiv:1801.03796},
  year		= {2018}
}

@Article{	  bro1997parafac,
  title		= {{PARAFAC}. {Tutorial} and applications},
  author	= {Bro, Rasmus},
  journal	= {Chemometrics and intelligent laboratory systems},
  volume	= {38},
  number	= {2},
  pages		= {149--171},
  year		= {1997},
  publisher	= {Elsevier}
}

###Article{	  bro1997parafac,
  author	= {Bro, Rasmus},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Chemometrics and intelligent laboratory systems},
  number	= {2},
  pages		= {149--171},
  publisher	= {Elsevier},
  title		= {{PARAFAC}. {Tutorial} and applications},
  volume	= {38},
  year		= {1997}
}

@Article{	  bro2003new,
  title		= {A new efficient method for determining the number of
		  components in {PARAFAC} models},
  author	= {Bro, Rasmus and Kiers, Henk AL},
  journal	= {Journal of chemometrics},
  volume	= {17},
  number	= {5},
  pages		= {274--286},
  year		= {2003},
  publisher	= {Wiley Online Library}
}

###Article{	  bro2003new,
  author	= {Bro, Rasmus and Kiers, Henk AL},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of chemometrics},
  number	= {5},
  pages		= {274--286},
  publisher	= {Wiley Online Library},
  title		= {A new efficient method for determining the number of
		  components in {PARAFAC} models},
  volume	= {17},
  year		= {2003}
}

@Article{	  bruna2018multiscale,
  title		= {Multiscale Sparse Microcanonical Models},
  author	= {Bruna, Joan and Mallat, Stephane},
  journal	= {arXiv preprint arXiv:1801.02013},
  year		= {2018}
}

@Article{	  buzsaki2014,
  abstract	= {We often assume that the variables of functional and
		  structural brain parameters - such as synaptic weights, the
		  firing rates of individual neurons, the synchronous
		  discharge of neural populations, the number of synaptic
		  contacts between neurons and the size of dendritic boutons
		  - have a bell-shaped distribution. However, at many
		  physiological and anatomical levels in the brain, the
		  distribution of numerous parameters is in fact strongly
		  skewed with a heavy tail, suggesting that skewed (typically
		  lognormal) distributions are fundamental to structural and
		  functional brain organization. This insight not only has
		  implications for how we should collect and analyse data, it
		  may also help us to understand how the different levels of
		  skewed distributions - from synapses to cognition - are
		  related to each other.},
  author	= {Buzs\'{a}ki, Gy\"{o}rgy and Mizuseki, Kenji},
  doi		= {10.1038/nrn3687},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/nrn3687.pdf:pdf},
  issn		= {1471-0048},
  journal	= {Nature reviews. Neuroscience},
  keywords	= {Animals,Brain,Brain: physiology,Humans,Models,
		  Neurological,Nerve Net,Nerve Net: physiology},
  month		= apr,
  number	= {4},
  pages		= {264--78},
  pmid		= {24569488},
  publisher	= {Nature Publishing Group},
  title		= {{The log-dynamic brain: how skewed distributions affect
		  network operations.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/24569488},
  volume	= {15},
  year		= {2014}
}

###Article{	  buzsaki2014,
  abstract	= {We often assume that the variables of functional and
		  structural brain parameters - such as synaptic weights, the
		  firing rates of individual neurons, the synchronous
		  discharge of neural populations, the number of synaptic
		  contacts between neurons and the size of dendritic boutons
		  - have a bell-shaped distribution. However, at many
		  physiological and anatomical levels in the brain, the
		  distribution of numerous parameters is in fact strongly
		  skewed with a heavy tail, suggesting that skewed (typically
		  lognormal) distributions are fundamental to structural and
		  functional brain organization. This insight not only has
		  implications for how we should collect and analyse data, it
		  may also help us to understand how the different levels of
		  skewed distributions - from synapses to cognition - are
		  related to each other.},
  author	= {Buzs\'{a}ki, Gy\"{o}rgy and Mizuseki, Kenji},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1038/nrn3687},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/nrn3687.pdf:pdf},
  issn		= {1471-0048},
  journal	= {Nature reviews. Neuroscience},
  keywords	= {Animals,Brain,Brain: physiology,Humans,Models,
		  Neurological,Nerve Net,Nerve Net: physiology},
  month		= apr,
  number	= {4},
  pages		= {264--78},
  pmid		= {24569488},
  publisher	= {Nature Publishing Group},
  title		= {{The log-dynamic brain: how skewed distributions affect
		  network operations.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/24569488},
  volume	= {15},
  year		= {2014},
  bdsk-url-1	= {http://www.ncbi.nlm.nih.gov/pubmed/24569488},
  bdsk-url-2	= {https://doi.org/10.1038/nrn3687}
}

@InProceedings{	  caruana2001overfitting,
  title		= {Overfitting in neural nets: Backpropagation, conjugate
		  gradient, and early stopping},
  author	= {Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle	= {Advances in neural information processing systems},
  pages		= {402--408},
  year		= {2001}
}

###InProceedings{ caruana2001overfitting,
  title		= {Overfitting in neural nets: Backpropagation, conjugate
		  gradient, and early stopping},
  author	= {Caruana, Rich and Lawrence, Steve and Giles, C Lee},
  booktitle	= {Advances in neural information processing systems},
  pages		= {402--408},
  year		= {2001}
}

@Article{	  cavagnasgpedestrians,
  title		= {Spin-glass theory for pedestrians},
  author	= {Castellani, Tommaso and Cavagna, Andrea},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  volume	= {2005},
  number	= {05},
  pages		= {P05012},
  year		= {2005},
  publisher	= {IOP Publishing}
}

###Article{	  cavagnasgpedestrians,
  author	= {Castellani, Tommaso and Cavagna, Andrea},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {05},
  pages		= {P05012},
  publisher	= {IOP Publishing},
  title		= {Spin-glass theory for pedestrians},
  volume	= {2005},
  year		= {2005}
}

###Article{	  cavagnasgpedestrians,
  title		= {Spin-glass theory for pedestrians},
  author	= {Castellani, Tommaso and Cavagna, Andrea},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  volume	= {2005},
  number	= {05},
  pages		= {P05012},
  year		= {2005},
  publisher	= {IOP Publishing}
}

@Article{	  cemgil:cin,
  author	= {Cemgil, A. T.},
  title		= {Bayesian Inference in Non-negative Matrix Factorisation
		  Models},
  journal	= {Computational Intelligence and Neuroscience},
  year		= {2009},
  owner		= {umutsimsekli},
  timestamp	= {2012.05.10}
}

###Article{	  cemgil:cin,
  author	= {Cemgil, A. T.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Computational Intelligence and Neuroscience},
  owner		= {umutsimsekli},
  timestamp	= {2012.05.10},
  title		= {Bayesian Inference in Non-negative Matrix Factorisation
		  Models},
  year		= {2009}
}

@Article{	  charbonneau12,
  author	= {Charbonneau, Patrick and Corwin, Eric I. and Parisi,
		  Giorgio and Zamponi, Francesco},
  date		= {2012/11/13/},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  day		= {13},
  id		= {10.1103/PhysRevLett.109.205501},
  j1		= {PRL},
  journal	= {Physical Review Letters},
  journal1	= {Phys. Rev. Lett.},
  month		= {11},
  number	= {20},
  pages		= {205501--},
  publisher	= {American Physical Society},
  title		= {Universal Microstructure and Mechanical Stability of
		  Jammed Packings},
  ty		= {JOUR},
  url		= {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501}
}

###Article{	  charbonneau12,
  author	= {Charbonneau, Patrick and Corwin, Eric I. and Parisi,
		  Giorgio and Zamponi, Francesco},
  date		= {2012/11/13/},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  day		= {13},
  id		= {10.1103/PhysRevLett.109.205501},
  j1		= {PRL},
  journal	= {Physical Review Letters},
  journal1	= {Phys. Rev. Lett.},
  month		= {11},
  number	= {20},
  pages		= {205501--},
  publisher	= {American Physical Society},
  title		= {Universal Microstructure and Mechanical Stability of
		  Jammed Packings},
  ty		= {JOUR},
  url		= {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501}
}

###Article{	  charbonneau12,
  author	= {Charbonneau, Patrick and Corwin, Eric I. and Parisi,
		  Giorgio and Zamponi, Francesco},
  date		= {2012/11/13/},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  day		= {13},
  id		= {10.1103/PhysRevLett.109.205501},
  j1		= {PRL},
  journal	= {Physical Review Letters},
  journal1	= {Phys. Rev. Lett.},
  month		= {11},
  number	= {20},
  pages		= {205501--},
  publisher	= {American Physical Society},
  title		= {Universal Microstructure and Mechanical Stability of
		  Jammed Packings},
  ty		= {JOUR},
  url		= {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.205501}
}

@Article{	  charbonneau14,
  author	= {Charbonneau, Patrick and Kurchan, Jorge and Parisi,
		  Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  date		= {2014},
  date-added	= {2014-05-09 02:04:58 +0000},
  date-modified	= {2014-06-13 16:56:16 +0000},
  journal	= {Nature Communications},
  number	= {3725},
  publisher	= {Nature Publishing Group},
  title		= {Fractal free energy landscapes in structural glasses},
  volume	= {5},
  year		= {2014}
}

###Article{	  charbonneau14,
  author	= {Charbonneau, Patrick and Kurchan, Jorge and Parisi,
		  Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  date		= {2014},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Nature Communications},
  number	= {3725},
  publisher	= {Nature Publishing Group},
  title		= {Fractal free energy landscapes in structural glasses},
  volume	= {5},
  year		= {2014}
}

###Article{	  charbonneau14,
  author	= {Charbonneau, Patrick and Kurchan, Jorge and Parisi,
		  Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  date		= {2014},
  date-added	= {2014-05-09 02:04:58 +0000},
  date-modified	= {2014-06-13 16:56:16 +0000},
  journal	= {Nature Communications},
  number	= {3725},
  publisher	= {Nature Publishing Group},
  title		= {Fractal free energy landscapes in structural glasses},
  volume	= {5},
  year		= {2014}
}

@Article{	  charbonneau14a,
  author	= {Charbonneau, Patrick and Kurchan, Jorge and Parisi,
		  Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  date		= {2014},
  date-added	= {2014-10-20 02:26:59 +0000},
  date-modified	= {2014-10-20 02:27:25 +0000},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {10},
  pages		= {10009},
  publisher	= {IOP Publishing},
  title		= {Exact theory of dense amorphous hard spheres in high
		  dimension. III. The full replica symmetry breaking
		  solution},
  volume	= {2014},
  year		= {2014}
}

###Article{	  charbonneau14a,
  author	= {Charbonneau, Patrick and Kurchan, Jorge and Parisi,
		  Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  date		= {2014},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {10},
  pages		= {10009},
  publisher	= {IOP Publishing},
  title		= {Exact theory of dense amorphous hard spheres in high
		  dimension. III. The full replica symmetry breaking
		  solution},
  volume	= {2014},
  year		= {2014}
}

###Article{	  charbonneau14a,
  author	= {Charbonneau, Patrick and Kurchan, Jorge and Parisi,
		  Giorgio and Urbani, Pierfrancesco and Zamponi, Francesco},
  date		= {2014},
  date-added	= {2014-10-20 02:26:59 +0000},
  date-modified	= {2014-10-20 02:27:25 +0000},
  journal	= {Journal of Statistical Mechanics: Theory and Experiment},
  number	= {10},
  pages		= {10009},
  publisher	= {IOP Publishing},
  title		= {Exact theory of dense amorphous hard spheres in high
		  dimension. III. The full replica symmetry breaking
		  solution},
  volume	= {2014},
  year		= {2014}
}

@Article{	  charbonneau15,
  author	= {Charbonneau, Patrick and Corwin, Eric I and Parisi,
		  Giorgio and Zamponi, Francesco},
  date		= {2015},
  date-added	= {2015-04-16 02:52:40 +0000},
  date-modified	= {2015-04-16 02:52:40 +0000},
  journal	= {Physical Review Letters},
  number	= {12},
  pages		= {125504},
  publisher	= {APS},
  title		= {Jamming Criticality Revealed by Removing Localized
		  Buckling Excitations},
  volume	= {114},
  year		= {2015}
}

###Article{	  charbonneau15,
  author	= {Charbonneau, Patrick and Corwin, Eric I and Parisi,
		  Giorgio and Zamponi, Francesco},
  date		= {2015},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review Letters},
  number	= {12},
  pages		= {125504},
  publisher	= {APS},
  title		= {Jamming Criticality Revealed by Removing Localized
		  Buckling Excitations},
  volume	= {114},
  year		= {2015}
}

###Article{	  charbonneau15,
  author	= {Charbonneau, Patrick and Corwin, Eric I and Parisi,
		  Giorgio and Zamponi, Francesco},
  date		= {2015},
  date-added	= {2015-04-16 02:52:40 +0000},
  date-modified	= {2015-04-16 02:52:40 +0000},
  journal	= {Physical Review Letters},
  number	= {12},
  pages		= {125504},
  publisher	= {APS},
  title		= {Jamming Criticality Revealed by Removing Localized
		  Buckling Excitations},
  volume	= {114},
  year		= {2015}
}

@Article{	  chaudhari16,
  title		= {Entropy-sgd: Biasing gradient descent into wide valleys},
  author	= {Chaudhari, Pratik and Choromanska, Anna and Soatto,
		  Stefano and LeCun, Yann and Baldassi, Carlo and Borgs,
		  Christian and Chayes, Jennifer and Sagun, Levent and
		  Zecchina, Riccardo},
  journal	= {arXiv preprint arXiv:1611.01838},
  year		= {2016}
}

###Article{	  chaudhari16,
  author	= {Chaudhari, Pratik and Choromanska, Anna and Soatto,
		  Stefano and LeCun, Yann and Baldassi, Carlo and Borgs,
		  Christian and Chayes, Jennifer and Sagun, Levent and
		  Zecchina, Riccardo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1611.01838},
  title		= {Entropy-sgd: Biasing gradient descent into wide valleys},
  year		= {2016}
}

###Article{	  chaudhari16,
  title		= {Entropy-sgd: Biasing gradient descent into wide valleys},
  author	= {Chaudhari, Pratik and Choromanska, Anna and Soatto,
		  Stefano and LeCun, Yann and Baldassi, Carlo and Borgs,
		  Christian and Chayes, Jennifer and Sagun, Levent and
		  Zecchina, Riccardo},
  journal	= {arXiv preprint arXiv:1611.01838},
  year		= {2016}
}

@Article{	  chaudhari2015trivializing,
  title		= {Trivializing the energy landscape of deep networks},
  author	= {Chaudhari, Pratik and Soatto, Stefano},
  journal	= {arXiv preprint arXiv:1511.06485},
  year		= {2015}
}

###Article{	  chaudhari2015trivializing,
  author	= {Chaudhari, Pratik and Soatto, Stefano},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1511.06485},
  title		= {Trivializing the energy landscape of deep networks},
  year		= {2015}
}

###Article{	  chaudhari2015trivializing,
  title		= {Trivializing the energy landscape of deep networks},
  author	= {Chaudhari, Pratik and Soatto, Stefano},
  journal	= {arXiv preprint arXiv:1511.06485},
  year		= {2015}
}


@InProceedings{	  chaudhari2018stochastic,
  title		= {Stochastic gradient descent performs variational
		  inference, converges to limit cycles for deep networks},
  author	= {Chaudhari, Pratik and Soatto, Stefano},
  booktitle	= {2018 Information Theory and Applications Workshop (ITA)},
  pages		= {1--10},
  year		= {2018},
  organization	= {IEEE}
}

@InProceedings{	  chen2015convergence,
  title		= {On the convergence of stochastic gradient {MCMC}
		  algorithms with high-order integrators},
  author	= {Chen, C. and Ding, N. and Carin, L.},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {2269--2277},
  year		= {2015}
}

###InProceedings{ chen2015convergence,
  author	= {Chen, C. and Ding, N. and Carin, L.},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {2269--2277},
  title		= {On the convergence of stochastic gradient {MCMC}
		  algorithms with high-order integrators},
  year		= {2015}
}

@TechReport{	  chen2016decision,
  title		= {Decision-making under the gambler's fallacy: Evidence from
		  asylum judges, loan officers, and baseball umpires},
  author	= {Chen, Daniel and Moskowitz, Tobias J and Shue, Kelly},
  year		= {2016},
  institution	= {National Bureau of Economic Research}
}

###TechReport{	  chen2016decision,
  author	= {Chen, Daniel and Moskowitz, Tobias J and Shue, Kelly},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  institution	= {National Bureau of Economic Research},
  title		= {Decision-making under the gambler's fallacy: Evidence from
		  asylum judges, loan officers, and baseball umpires},
  year		= {2016}
}

###TechReport{	  chen2016decision,
  title		= {Decision-making under the gambler's fallacy: Evidence from
		  asylum judges, loan officers, and baseball umpires},
  author	= {Chen, Daniel and Moskowitz, Tobias J and Shue, Kelly},
  year		= {2016},
  institution	= {National Bureau of Economic Research}
}

@TechReport{	  chen2017asylum,
  title		= {Can Machine Learning Help Predict the Outcome of Asylum
		  Adjudications?},
  author	= {Chen, Daniel and Eagel, Jess},
  institution	= {National Bureau of Economic Research},
  year		= {2017}
}

###TechReport{	  chen2017asylum,
  author	= {Chen, Daniel and Eagel, Jess},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  institution	= {National Bureau of Economic Research},
  title		= {Can Machine Learning Help Predict the Outcome of Asylum
		  Adjudications?},
  year		= {2017}
}

@TechReport{	  chen2017mood,
  title		= {Mood and the Malleability of Moral Reasoning},
  author	= {Chen, Daniel},
  institution	= {National Bureau of Economic Research},
  year		= {2017}
}

###TechReport{	  chen2017mood,
  author	= {Chen, Daniel},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  institution	= {National Bureau of Economic Research},
  title		= {Mood and the Malleability of Moral Reasoning},
  year		= {2017}
}

@Article{	  cheng2012,
  abstract	= {Let \$X = \{X(t), t\backslash in \backslash R\^{}\{N\}\}\$
		  be a centered Gaussian random field with stationary
		  increments and let \$T \backslash subset \backslash
		  R\^{}N\$ be a compact rectangle. Under \$X(\backslash cdot)
		  \backslash in C\^{}2(\backslash R\^{}N)\$ and certain
		  additional regularity conditions, the mean Euler
		  characteristic of the excursion set \$A\_u = \{t\backslash
		  in T: X(t)\backslash geq u\}\$, denoted by \$\backslash
		  E\{\backslash varphi(A\_u)\}\$, is derived. By applying the
		  Rice method, it is shown that, as \$u \backslash to
		  \backslash infty\$, the excursion probability \$\backslash
		  P\{\backslash sup\_\{t\backslash in T\} X(t) \backslash geq
		  u\}\$ can be approximated by \$\backslash E\{\backslash
		  varphi(A\_u)\}\$ such that the error is exponentially
		  smaller than \$\backslash E\{\backslash varphi(A\_u)\}\$.
		  This verifies the expected Euler characteristic heuristic
		  for a large class of Gaussian random fields with stationary increments.},
  archiveprefix	= {arXiv},
  arxivid	= {1211.6693},
  author	= {Cheng, Dan and Xiao, Yimin},
  eprint	= {1211.6693},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1211.6693v2.pdf:pdf},
  keywords	= {euler characteristic,excursion probability,excursion
		  set,gaussian random fields,stationary
		  increments,super-exponentially small},
  month		= nov,
  number	= {2002},
  title		= {{The Mean Euler Characteristic and Excursion Probability
		  of Gaussian Random Fields with Stationary Increments}},
  url		= {http://www.amstat.org/meetings/jsm/2014/onlineprogram/AbstractDetails.cfm?abstractid=312451},
  year		= {2012}
}

###Article{	  cheng2012,
  abstract	= {Let \$X = \{X(t), t\backslash in \backslash R\^{}\{N\}\}\$
		  be a centered Gaussian random field with stationary
		  increments and let \$T \backslash subset \backslash
		  R\^{}N\$ be a compact rectangle. Under \$X(\backslash cdot)
		  \backslash in C\^{}2(\backslash R\^{}N)\$ and certain
		  additional regularity conditions, the mean Euler
		  characteristic of the excursion set \$A\_u = \{t\backslash
		  in T: X(t)\backslash geq u\}\$, denoted by \$\backslash
		  E\{\backslash varphi(A\_u)\}\$, is derived. By applying the
		  Rice method, it is shown that, as \$u \backslash to
		  \backslash infty\$, the excursion probability \$\backslash
		  P\{\backslash sup\_\{t\backslash in T\} X(t) \backslash geq
		  u\}\$ can be approximated by \$\backslash E\{\backslash
		  varphi(A\_u)\}\$ such that the error is exponentially
		  smaller than \$\backslash E\{\backslash varphi(A\_u)\}\$.
		  This verifies the expected Euler characteristic heuristic
		  for a large class of Gaussian random fields with stationary increments.},
  archiveprefix	= {arXiv},
  arxivid	= {1211.6693},
  author	= {Cheng, Dan and Xiao, Yimin},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {1211.6693},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1211.6693v2.pdf:pdf},
  keywords	= {euler characteristic,excursion probability,excursion
		  set,gaussian random fields,stationary
		  increments,super-exponentially small},
  month		= nov,
  number	= {2002},
  title		= {{The Mean Euler Characteristic and Excursion Probability
		  of Gaussian Random Fields with Stationary Increments}},
  url		= {http://www.amstat.org/meetings/jsm/2014/onlineprogram/AbstractDetails.cfm?abstractid=312451},
  year		= {2012},
  bdsk-url-1	= {http://www.amstat.org/meetings/jsm/2014/onlineprogram/AbstractDetails.cfm?abstractid=312451}
}

@Article{	  cheng2014,
  abstract	= {Let \$X= \backslash\{X(x): x\backslash in \backslash
		  mathbb\{S\}\^{}N\backslash\}\$ be a real-valued, centered
		  Gaussian random field indexed on the \$N\$-dimensional unit
		  sphere \$\backslash mathbb\{S\}\^{}N\$. Approximations to
		  the excursion probability \$\{\backslash mathbb
		  P\}\backslash big\backslash\{\backslash sup\_\{x\backslash
		  in \backslash mathbb\{S\}\^{}N\} X(x) \backslash ge u
		  \backslash big\backslash\}\$, as \$u\backslash to
		  \backslash infty\$, are obtained for two cases: (i) \$X\$
		  is locally isotropic and its sample path is non-smooth and;
		  (ii) \$X\$ is isotropic and its sample path is twice
		  differentiable. For case (i), it is shown that the
		  asymptotics is similar to Pickands' approximation on the
		  Euclidean space which involves Pickands' constant; while
		  for case (ii), we use the expected Euler characteristic
		  method to obtain a more precise approximation such that the error is super-exponentially small.},
  archiveprefix	= {arXiv},
  arxivid	= {1401.5498},
  author	= {Cheng, Dan and Xiao, Yimin},
  eprint	= {1401.5498},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1401.5498v1.pdf:pdf},
  keywords	= {constant,euler,excursion probability,gaussian random
		  fields on,pickands,sphere},
  month		= jan,
  number	= {2009},
  pages		= {1--19},
  title		= {{Excursion Probability of Gaussian Random Fields on
		  Sphere}},
  url		= {http://arxiv.org/abs/1401.5498},
  year		= {2014}
}

###Article{	  cheng2014,
  abstract	= {Let \$X= \backslash\{X(x): x\backslash in \backslash
		  mathbb\{S\}\^{}N\backslash\}\$ be a real-valued, centered
		  Gaussian random field indexed on the \$N\$-dimensional unit
		  sphere \$\backslash mathbb\{S\}\^{}N\$. Approximations to
		  the excursion probability \$\{\backslash mathbb
		  P\}\backslash big\backslash\{\backslash sup\_\{x\backslash
		  in \backslash mathbb\{S\}\^{}N\} X(x) \backslash ge u
		  \backslash big\backslash\}\$, as \$u\backslash to
		  \backslash infty\$, are obtained for two cases: (i) \$X\$
		  is locally isotropic and its sample path is non-smooth and;
		  (ii) \$X\$ is isotropic and its sample path is twice
		  differentiable. For case (i), it is shown that the
		  asymptotics is similar to Pickands' approximation on the
		  Euclidean space which involves Pickands' constant; while
		  for case (ii), we use the expected Euler characteristic
		  method to obtain a more precise approximation such that the error is super-exponentially small.},
  archiveprefix	= {arXiv},
  arxivid	= {1401.5498},
  author	= {Cheng, Dan and Xiao, Yimin},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {1401.5498},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1401.5498v1.pdf:pdf},
  keywords	= {constant,euler,excursion probability,gaussian random
		  fields on,pickands,sphere},
  month		= jan,
  number	= {2009},
  pages		= {1--19},
  title		= {{Excursion Probability of Gaussian Random Fields on
		  Sphere}},
  url		= {http://arxiv.org/abs/1401.5498},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1401.5498}
}

@InProceedings{	  chenicml2014,
  author	= {Chen, T. and Fox, E. B. and Guestrin, C.},
  title		= {Stochastic Gradient {H}amiltonian {M}onte {C}arlo},
  booktitle	= {Proc. International Conference on Machine Learning},
  year		= {2014},
  month		= jun
}

###InProceedings{ chenicml2014,
  author	= {Chen, T. and Fox, E. B. and Guestrin, C.},
  booktitle	= {Proc. International Conference on Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= jun,
  title		= {Stochastic Gradient {H}amiltonian {M}onte {C}arlo},
  year		= {2014}
}

@Article{	  chib1995,
  author	= { S. Chib },
  title		= {Marginal Likelihood from the {G}ibbs Output},
  journal	= {Journal of the American Statistical Association},
  year		= {1995},
  volume	= {90},
  pages		= {1313-1321},
  number	= {432}
}

###Article{	  chib1995,
  author	= {S. Chib},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of the American Statistical Association},
  number	= {432},
  pages		= {1313-1321},
  title		= {Marginal Likelihood from the {G}ibbs Output},
  volume	= {90},
  year		= {1995}
}

@Article{	  chib1995marginal,
  title		= {Marginal likelihood from the {Gibbs} output},
  author	= {Chib, Siddhartha},
  journal	= {Journal of the American Statistical Association},
  volume	= {90},
  number	= {432},
  pages		= {1313--1321},
  year		= {1995},
  publisher	= {Taylor \& Francis Group}
}

###Article{	  chib1995marginal,
  author	= {Chib, Siddhartha},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of the American Statistical Association},
  number	= {432},
  pages		= {1313--1321},
  publisher	= {Taylor \& Francis Group},
  title		= {Marginal likelihood from the {Gibbs} output},
  volume	= {90},
  year		= {1995}
}

@Article{	  chizat18,
  title		= {A Note on Lazy Training in Supervised Differentiable
		  Programming},
  author	= {Chizat, Lenaic and Bach, Francis},
  journal	= {arXiv preprint arXiv:1812.07956},
  year		= {2018}
}

@Article{	  chizat2018global,
  title		= {On the Global Convergence of Gradient Descent for
		  Over-parameterized Models using Optimal Transport},
  author	= {Chizat, Lenaic and Bach, Francis},
  journal	= {arXiv preprint arXiv:1805.09545},
  year		= {2018}
}

@InCollection{	  cho2009,
  title		= {Kernel Methods for Deep Learning},
  author	= {Youngmin Cho and Lawrence K. Saul},
  booktitle	= {Advances in Neural Information Processing Systems 22},
  pages		= {342--350},
  year		= {2009},
  publisher	= {Curran Associates, Inc.},
  url		= {http://papers.nips.cc/paper/3628-kernel-methods-for-deep-learning.pdf}
}

@Article{	  choromanska,
  archiveprefix	= {arXiv},
  arxivid	= {1412.0233},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and {Ben Arous}, G\'{e}rard and LeCun, Yann},
  eprint	= {1412.0233},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1412.0233v2.pdf:pdf},
  month		= nov,
  title		= {{The Loss Surface of Multilayer Networks}},
  url		= {http://arxiv.org/abs/1412.0233v2},
  year		= {2014}
}

###Article{	  choromanska,
  archiveprefix	= {arXiv},
  arxivid	= {1412.0233},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and {Ben Arous}, G\'{e}rard and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {1412.0233},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1412.0233v2.pdf:pdf},
  month		= nov,
  title		= {{The Loss Surface of Multilayer Networks}},
  url		= {http://arxiv.org/abs/1412.0233v2},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1412.0233v2}
}

@InProceedings{	  choromanska15,
  title		= {The loss surfaces of multilayer networks},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle	= {Artificial Intelligence and Statistics},
  pages		= {192--204},
  year		= {2015}
}

###InProceedings{ choromanska15,
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle	= {Artificial Intelligence and Statistics},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {192--204},
  title		= {The loss surfaces of multilayer networks},
  year		= {2015}
}

###InProceedings{ choromanska15,
  title		= {The loss surfaces of multilayer networks},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle	= {Artificial Intelligence and Statistics},
  pages		= {192--204},
  year		= {2015}
}

@Article{	  choromanska2014loss,
  title		= {The loss surface of multilayer networks},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  journal	= {arXiv preprint arXiv:1412.0233},
  year		= {2014}
}

###Article{	  choromanska2014loss,
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1412.0233},
  title		= {The loss surface of multilayer networks},
  year		= {2014}
}

###Article{	  choromanska2014loss,
  title		= {The loss surface of multilayer networks},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  journal	= {arXiv preprint arXiv:1412.0233},
  year		= {2014}
}

@InProceedings{	  choromanska2015loss,
  title		= {The loss surfaces of multilayer networks},
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle	= {Artificial Intelligence and Statistics},
  pages		= {192--204},
  year		= {2015}
}

###InProceedings{ choromanska2015loss,
  author	= {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael
		  and Ben Arous, G{\'e}rard and LeCun, Yann},
  booktitle	= {Artificial Intelligence and Statistics},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {192--204},
  title		= {The loss surfaces of multilayer networks},
  year		= {2015}
}

@Book{		  cichocki09,
  title		= {Nonnegative Matrix and Tensor Factorization},
  publisher	= {Wiley},
  year		= {2009},
  author	= {A. Cichoki and R. Zdunek and A. H. Phan and S. Amari},
  owner		= {umutsimsekli},
  timestamp	= {2011.05.20}
}

###Book{	  cichocki09,
  author	= {A. Cichoki and R. Zdunek and A. H. Phan and S. Amari},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  owner		= {umutsimsekli},
  publisher	= {Wiley},
  timestamp	= {2011.05.20},
  title		= {Nonnegative Matrix and Tensor Factorization},
  year		= {2009}
}

@Article{	  cichocki2015tensor,
  title		= {Tensor decompositions for signal processing applications:
		  From two-way to multiway component analysis},
  author	= {Cichocki, Andrzej and Mandic, Danilo and De Lathauwer,
		  Lieven and Zhou, Guoxu and Zhao, Qibin and Caiafa, Cesar
		  and Phan, Huy Anh},
  journal	= {IEEE Signal Processing Magazine},
  volume	= {32},
  number	= {2},
  pages		= {145--163},
  year		= {2015},
  publisher	= {IEEE}
}

###Article{	  cichocki2015tensor,
  author	= {Cichocki, Andrzej and Mandic, Danilo and De Lathauwer,
		  Lieven and Zhou, Guoxu and Zhao, Qibin and Caiafa, Cesar
		  and Phan, Huy Anh},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {IEEE Signal Processing Magazine},
  number	= {2},
  pages		= {145--163},
  publisher	= {IEEE},
  title		= {Tensor decompositions for signal processing applications:
		  From two-way to multiway component analysis},
  volume	= {32},
  year		= {2015}
}

@InProceedings{	  collobert_nipsworkshop_2011,
  author	= {Collobert, Ronan and Kavukcuoglu, Koray and Farabet,
		  Cl{\'{e}}ment},
  projects	= {Idiap},
  title		= {Torch7: A Matlab-like Environment for Machine Learning},
  booktitle	= {BigLearn, NIPS Workshop},
  year		= {2011},
  pdf		= {http://publications.idiap.ch/downloads/papers/2011/Collobert_NIPSWORKSHOP_2011.pdf}
}

###InProceedings{ collobert_nipsworkshop_2011,
  author	= {Collobert, Ronan and Kavukcuoglu, Koray and Farabet,
		  Cl{\'{e}}ment},
  booktitle	= {BigLearn, NIPS Workshop},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pdf		= {http://publications.idiap.ch/downloads/papers/2011/Collobert_NIPSWORKSHOP_2011.pdf},
  projects	= {Idiap},
  title		= {Torch7: A Matlab-like Environment for Machine Learning},
  year		= {2011}
}

@Misc{		  conflict,
  title		= {Global Conflict Risk Index.},
  author	= {Global Conflict Risk Index.},
  year		= {2016},
  howpublished	= "\url{http://conflictrisk.jrc.ec.europa.eu/}"
}

###Misc{	  conflict,
  author	= {Global Conflict Risk Index.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  howpublished	= {\url{http://conflictrisk.jrc.ec.europa.eu/}},
  title		= {Global Conflict Risk Index.},
  year		= {2016}
}

@Article{	  cong2015tensor,
  title		= {Tensor decomposition of {EEG} signals: a brief review},
  author	= {Cong, Fengyu and Lin, Qiu-Hua and Kuang, Li-Dan and Gong,
		  Xiao-Feng and Astikainen, Piia and Ristaniemi, Tapani},
  journal	= {Journal of neuroscience methods},
  volume	= {248},
  pages		= {59--69},
  year		= {2015},
  publisher	= {Elsevier}
}

###Article{	  cong2015tensor,
  author	= {Cong, Fengyu and Lin, Qiu-Hua and Kuang, Li-Dan and Gong,
		  Xiao-Feng and Astikainen, Piia and Ristaniemi, Tapani},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of neuroscience methods},
  pages		= {59--69},
  publisher	= {Elsevier},
  title		= {Tensor decomposition of {EEG} signals: a brief review},
  volume	= {248},
  year		= {2015}
}

@Article{	  cooper18,
  title		= {The loss landscape of overparameterized neural networks},
  author	= {Cooper, Yaim},
  journal	= {arXiv preprint arXiv:1804.10200},
  year		= {2018}
}

###Article{	  cooper18,
  author	= {Cooper, Yaim},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1804.10200},
  title		= {The loss landscape of overparameterized neural networks},
  year		= {2018}
}

###Article{	  cooper18,
  title		= {The loss landscape of overparameterized neural networks},
  author	= {Cooper, Yaim},
  journal	= {arXiv preprint arXiv:1804.10200},
  year		= {2018}
}

@Book{		  crank1979mathematics,
  title		= {The mathematics of diffusion},
  author	= {Crank, John},
  year		= {1979},
  publisher	= {Oxford university press}
}

###Book{	  crank1979mathematics,
  author	= {Crank, John},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  publisher	= {Oxford university press},
  title		= {The mathematics of diffusion},
  year		= {1979}
}

###Book{	  crank1979mathematics,
  title		= {The mathematics of diffusion},
  author	= {Crank, John},
  year		= {1979},
  publisher	= {Oxford university press}
}

@InProceedings{	  csimccekli2016stochastic,
  title		= {Stochastic thermodynamic integration: efficient {B}ayesian
		  model selection via stochastic gradient {MCMC}},
  author	= {{\c{S}}im{\c{s}}ekli, U. and Badeau, R. and Richard, G.
		  and Cemgil, A. T.},
  booktitle	= {ICASSP},
  pages		= {2574--2578},
  year		= {2016},
  organization	= {IEEE}
}

###InProceedings{ csimccekli2016stochastic,
  author	= {{\c{S}}im{\c{s}}ekli, U. and Badeau, R. and Richard, G.
		  and Cemgil, A. T.},
  booktitle	= {ICASSP},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {IEEE},
  pages		= {2574--2578},
  title		= {Stochastic thermodynamic integration: efficient {B}ayesian
		  model selection via stochastic gradient {MCMC}},
  year		= {2016}
}

@InProceedings{	  csimcsekli2012markov,
  author	= {{\c{S}}im{\c{s}}ekli, U. and Cemgil, A. T.},
  title		= {Markov chain {M}onte {C}arlo inference for probabilistic
		  latent tensor factorization},
  booktitle	= {2012 IEEE International Workshop on Machine Learning for
		  Signal Processing (MLSP)},
  year		= {2012},
  pages		= {1--6},
  month		= sep,
  organization	= {IEEE}
}

###InProceedings{ csimcsekli2012markov,
  author	= {{\c{S}}im{\c{s}}ekli, U. and Cemgil, A. T.},
  booktitle	= {2012 IEEE International Workshop on Machine Learning for
		  Signal Processing (MLSP)},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= sep,
  organization	= {IEEE},
  pages		= {1--6},
  title		= {Markov chain {M}onte {C}arlo inference for probabilistic
		  latent tensor factorization},
  year		= {2012}
}

@InProceedings{	  csimcsekli2015learning,
  title		= {Learning mixed divergences in coupled matrix and tensor
		  factorization models},
  author	= {{\c{S}}im{\c{s}}ekli, U. and Cemgil, A. T. and
		  Ermi{\c{s}}, B.},
  booktitle	= {ICASSP},
  pages		= {2120--2124},
  year		= {2015}
}

###InProceedings{ csimcsekli2015learning,
  author	= {{\c{S}}im{\c{s}}ekli, U. and Cemgil, A. T. and
		  Ermi{\c{s}}, B.},
  booktitle	= {ICASSP},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {2120--2124},
  title		= {Learning mixed divergences in coupled matrix and tensor
		  factorization models},
  year		= {2015}
}

@Article{	  csimcsekli2015parallel,
  title		= {Parallel stochastic gradient {Markov chain Monte Carlo}
		  for matrix factorisation models},
  author	= {{\c{S}}im{\c{s}}ekli, Umut and Koptagel, Hazal and
		  G{\"u}lda{\c{s}}, Hakan and Cemgil, A Taylan and
		  {\"O}ztoprak, Figen and Birbil, {\c{S}} {\.I}lker},
  journal	= {arXiv preprint arXiv:1506.01418},
  year		= {2015}
}

###Article{	  csimcsekli2015parallel,
  author	= {{\c{S}}im{\c{s}}ekli, Umut and Koptagel, Hazal and
		  G{\"u}lda{\c{s}}, Hakan and Cemgil, A Taylan and
		  {\"O}ztoprak, Figen and Birbil, {\c{S}} {\.I}lker},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1506.01418},
  title		= {Parallel stochastic gradient {Markov chain Monte Carlo}
		  for matrix factorisation models},
  year		= {2015}
}

@Article{	  csimcsekli2017fractional,
  title		= {Fractional {Langevin Monte Carlo: Exploring
		  L$\backslash$'$\{$e$\}$ vy Driven Stochastic Differential
		  Equations for Markov Chain Monte Carlo}},
  author	= {{\c{S}}im{\c{s}}ekli, Umut},
  journal	= {arXiv preprint arXiv:1706.03649},
  year		= {2017}
}

###Article{	  csimcsekli2017fractional,
  author	= {{\c{S}}im{\c{s}}ekli, Umut},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1706.03649},
  title		= {Fractional {Langevin Monte Carlo: Exploring
		  L$\backslash$'$\{$e$\}$ vy Driven Stochastic Differential
		  Equations for Markov Chain Monte Carlo}},
  year		= {2017}
}

@InCollection{	  cugliandololeshouches,
  title		= {Course 7: Dynamics of glassy systems},
  author	= {Cugliandolo, Leticia F},
  booktitle	= {Slow Relaxations and nonequilibrium dynamics in condensed
		  matter},
  pages		= {367--521},
  year		= {2003},
  publisher	= {Springer}
}

###InCollection{  cugliandololeshouches,
  author	= {Cugliandolo, Leticia F},
  booktitle	= {Slow Relaxations and nonequilibrium dynamics in condensed
		  matter},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {367--521},
  publisher	= {Springer},
  title		= {Course 7: Dynamics of glassy systems},
  year		= {2003}
}

###InCollection{  cugliandololeshouches,
  title		= {Course 7: Dynamics of glassy systems},
  author	= {Cugliandolo, Leticia F},
  booktitle	= {Slow Relaxations and nonequilibrium dynamics in condensed
		  matter},
  pages		= {367--521},
  year		= {2003},
  publisher	= {Springer}
}

@Article{	  cuku,
  title		= {Analytical solution of the off-equilibrium dynamics of a
		  long-range spin-glass model},
  author	= {Cugliandolo, Leticia F and Kurchan, Jorge},
  journal	= {Physical Review Letters},
  volume	= {71},
  number	= {1},
  pages		= {173},
  year		= {1993},
  publisher	= {APS}
}

###Article{	  cuku,
  author	= {Cugliandolo, Leticia F and Kurchan, Jorge},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review Letters},
  number	= {1},
  pages		= {173},
  publisher	= {APS},
  title		= {Analytical solution of the off-equilibrium dynamics of a
		  long-range spin-glass model},
  volume	= {71},
  year		= {1993}
}

###Article{	  cuku,
  title		= {Analytical solution of the off-equilibrium dynamics of a
		  long-range spin-glass model},
  author	= {Cugliandolo, Leticia F and Kurchan, Jorge},
  journal	= {Physical Review Letters},
  volume	= {71},
  number	= {1},
  pages		= {173},
  year		= {1993},
  publisher	= {APS}
}

@InProceedings{	  da2008robust,
  title		= {Robust methods based on the {HOSVD} for estimating the
		  model order in {PARAFAC} models},
  author	= {da Costa, Joao Paulo CL and Haardt, Martin and Romer,
		  Florian},
  booktitle	= {Sensor Array and Multichannel Signal Processing Workshop,
		  2008. SAM 2008. 5th IEEE},
  pages		= {510--514},
  year		= {2008},
  organization	= {IEEE}
}

###InProceedings{ da2008robust,
  author	= {da Costa, Joao Paulo CL and Haardt, Martin and Romer,
		  Florian},
  booktitle	= {Sensor Array and Multichannel Signal Processing Workshop,
		  2008. SAM 2008. 5th IEEE},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {IEEE},
  pages		= {510--514},
  title		= {Robust methods based on the {HOSVD} for estimating the
		  model order in {PARAFAC} models},
  year		= {2008}
}

@Article{	  da2011multi,
  title		= {Multi-dimensional model order selection},
  author	= {da Costa, Jo{\~a}o Paulo Carvalho Lustosa and Roemer,
		  Florian and Haardt, Martin and de Sousa, Rafael Tim{\'o}teo},
  journal	= {EURASIP Journal on Advances in Signal Processing},
  volume	= {2011},
  number	= {1, article 26},
  year		= {2011},
  publisher	= {Springer}
}

###Article{	  da2011multi,
  author	= {da Costa, Jo{\~a}o Paulo Carvalho Lustosa and Roemer,
		  Florian and Haardt, Martin and de Sousa, Rafael Tim{\'o}teo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {EURASIP Journal on Advances in Signal Processing},
  number	= {1, article 26},
  publisher	= {Springer},
  title		= {Multi-dimensional model order selection},
  volume	= {2011},
  year		= {2011}
}

@InProceedings{	  dauphin2014identifying,
  title		= {Identifying and attacking the saddle point problem in
		  high-dimensional non-convex optimization},
  author	= {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar
		  and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {2933--2941},
  year		= {2014}
}

###InProceedings{ dauphin2014identifying,
  author	= {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar
		  and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {2933--2941},
  title		= {Identifying and attacking the saddle point problem in
		  high-dimensional non-convex optimization},
  year		= {2014}
}

###InProceedings{ dauphin2014identifying,
  title		= {Identifying and attacking the saddle point problem in
		  high-dimensional non-convex optimization},
  author	= {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar
		  and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {2933--2941},
  year		= {2014}
}

###InProceedings{ dauphin2014identifying,
  title		= {Identifying and attacking the saddle point problem in
		  high-dimensional non-convex optimization},
  author	= {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar
		  and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {2933--2941},
  year		= {2014}
}

@Article{	  dean2008,
  abstract	= {We compute exact asymptotic results for the probability of
		  the occurrence of large deviations of the largest
		  (smallest) eigenvalue of random matrices belonging to the
		  Gaussian orthogonal, unitary and symplectic ensembles. In
		  particular, we show that the probability that all the
		  eigenvalues of an (NxN) random matrix are positive
		  (negative) decreases for large N as
		  \~{}$\backslash$exp[-$\backslash$beta $\backslash$theta(0)
		  N\^{}2] where the Dyson index $\backslash$beta
		  characterizes the ensemble and the exponent
		  $\backslash$theta(0)=($\backslash$ln 3)/4=0.274653... is
		  universal. We compute the probability that the eigenvalues
		  lie in the interval
		  [$\backslash$zeta\_1,$\backslash$zeta\_2] which allows us
		  to calculate the joint probability distribution of the
		  minimum and the maximum eigenvalue. As a byproduct, we also
		  obtain exactly the average density of states in Gaussian
		  ensembles whose eigenvalues are restricted to lie in the
		  interval [$\backslash$zeta\_1,$\backslash$zeta\_2], thus
		  generalizing the celebrated Wigner semi-circle law to these
		  restricted ensembles. It is found that the density of
		  states generically exhibits an inverse square-root
		  singularity at the location of the barriers. These results
		  are confirmed by numerical simulations.},
  archiveprefix	= {arXiv},
  arxivid	= {0801.1730},
  author	= {Dean, David S and Majumdar, Satya N},
  doi		= {10.1103/PhysRevE.77.041108},
  eprint	= {0801.1730},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/0801.1730.pdf:pdf},
  issn		= {1539-3755},
  journal	= {Physical Review E},
  month		= apr,
  number	= {4},
  pages		= {041108},
  title		= {{Extreme value statistics of eigenvalues of Gaussian
		  random matrices}},
  url		= {http://link.aps.org/doi/10.1103/PhysRevE.77.041108},
  volume	= {77},
  year		= {2008}
}

###Article{	  dean2008,
  abstract	= {We compute exact asymptotic results for the probability of
		  the occurrence of large deviations of the largest
		  (smallest) eigenvalue of random matrices belonging to the
		  Gaussian orthogonal, unitary and symplectic ensembles. In
		  particular, we show that the probability that all the
		  eigenvalues of an (NxN) random matrix are positive
		  (negative) decreases for large N as
		  \~{}$\backslash$exp[-$\backslash$beta $\backslash$theta(0)
		  N\^{}2] where the Dyson index $\backslash$beta
		  characterizes the ensemble and the exponent
		  $\backslash$theta(0)=($\backslash$ln 3)/4=0.274653... is
		  universal. We compute the probability that the eigenvalues
		  lie in the interval
		  [$\backslash$zeta\_1,$\backslash$zeta\_2] which allows us
		  to calculate the joint probability distribution of the
		  minimum and the maximum eigenvalue. As a byproduct, we also
		  obtain exactly the average density of states in Gaussian
		  ensembles whose eigenvalues are restricted to lie in the
		  interval [$\backslash$zeta\_1,$\backslash$zeta\_2], thus
		  generalizing the celebrated Wigner semi-circle law to these
		  restricted ensembles. It is found that the density of
		  states generically exhibits an inverse square-root
		  singularity at the location of the barriers. These results
		  are confirmed by numerical simulations.},
  archiveprefix	= {arXiv},
  arxivid	= {0801.1730},
  author	= {Dean, David S and Majumdar, Satya N},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevE.77.041108},
  eprint	= {0801.1730},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/0801.1730.pdf:pdf},
  issn		= {1539-3755},
  journal	= {Physical Review E},
  month		= apr,
  number	= {4},
  pages		= {041108},
  title		= {{Extreme value statistics of eigenvalues of Gaussian
		  random matrices}},
  url		= {http://link.aps.org/doi/10.1103/PhysRevE.77.041108},
  volume	= {77},
  year		= {2008},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevE.77.041108},
  bdsk-url-2	= {https://doi.org/10.1103/PhysRevE.77.041108}
}

@Article{	  degiuli14,
  author	= {DeGiuli, Eric and Laversanne-Finot, Adrien and D\"uring,
		  Gustavo Alberto and Lerner, Edan and Wyart, Matthieu},
  date		= {2014},
  date-added	= {2014-07-08 08:27:13 +0000},
  date-modified	= {2014-07-10 08:54:49 +0000},
  journal	= {Soft Matter},
  number	= {30},
  pages		= {5628-5644},
  publisher	= {Royal Society of Chemistry},
  title		= {Effects of coordination and pressure on sound attenuation,
		  boson peak and elasticity in amorphous solids},
  volume	= {10},
  year		= {2014}
}

###Article{	  degiuli14,
  author	= {DeGiuli, Eric and Laversanne-Finot, Adrien and D\"uring,
		  Gustavo Alberto and Lerner, Edan and Wyart, Matthieu},
  date		= {2014},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Soft Matter},
  number	= {30},
  pages		= {5628-5644},
  publisher	= {Royal Society of Chemistry},
  title		= {Effects of coordination and pressure on sound attenuation,
		  boson peak and elasticity in amorphous solids},
  volume	= {10},
  year		= {2014}
}

###Article{	  degiuli14,
  author	= {DeGiuli, Eric and Laversanne-Finot, Adrien and {D\"uring},
		  Gustavo Alberto and Lerner, Edan and Wyart, Matthieu},
  date		= {2014},
  date-added	= {2014-07-08 08:27:13 +0000},
  date-modified	= {2014-07-10 08:54:49 +0000},
  journal	= {Soft Matter},
  number	= {30},
  pages		= {5628-5644},
  publisher	= {Royal Society of Chemistry},
  title		= {Effects of coordination and pressure on sound attenuation,
		  boson peak and elasticity in amorphous solids},
  volume	= {10},
  year		= {2014}
}

@Book{		  deift2000orthogonal,
  title		= {Orthogonal polynomials and random matrices: a
		  Riemann-Hilbert approach},
  author	= {Deift, Percy},
  volume	= {3},
  year		= {2000},
  publisher	= {American Mathematical Soc.}
}

###Book{	  deift2000orthogonal,
  author	= {Deift, Percy},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {American Mathematical Soc.},
  title		= {Orthogonal polynomials and random matrices: a
		  Riemann-Hilbert approach},
  volume	= {3},
  year		= {2000}
}

###Book{	  deift2000orthogonal,
  title		= {Orthogonal polynomials and random matrices: a
		  Riemann-Hilbert approach},
  author	= {Deift, Percy},
  volume	= {3},
  year		= {2000},
  publisher	= {American Mathematical Soc.}
}

@Article{	  deift2014universality,
  title		= {Universality in numerical computations with random data},
  author	= {Deift, Percy and Menon, Govind and Olver, Sheehan and
		  Trogdon, Thomas},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {111},
  number	= {42},
  pages		= {14973--14978},
  year		= {2014},
  publisher	= {National Acad Sciences}
}

###Article{	  deift2014universality,
  author	= {Deift, Percy and Menon, Govind and Olver, Sheehan and
		  Trogdon, Thomas},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Proceedings of the National Academy of Sciences},
  number	= {42},
  pages		= {14973--14978},
  publisher	= {National Acad Sciences},
  title		= {Universality in numerical computations with random data},
  volume	= {111},
  year		= {2014}
}

###Article{	  deift2014universality,
  title		= {Universality in numerical computations with random data},
  author	= {Deift, Percy and Menon, Govind and Olver, Sheehan and
		  Trogdon, Thomas},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {111},
  number	= {42},
  pages		= {14973--14978},
  year		= {2014},
  publisher	= {National Acad Sciences}
}

@Article{	  deift2015condition,
  title		= {On the condition number of the critically-scaled Laguerre
		  Unitary Ensemble},
  author	= {Deift, Percy and Menon, Govind and Trogdon, Thomas},
  journal	= {arXiv preprint arXiv:1507.00750},
  year		= {2015}
}

###Article{	  deift2015condition,
  author	= {Deift, Percy and Menon, Govind and Trogdon, Thomas},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1507.00750},
  title		= {On the condition number of the critically-scaled Laguerre
		  Unitary Ensemble},
  year		= {2015}
}

###Article{	  deift2015condition,
  title		= {On the condition number of the critically-scaled Laguerre
		  Unitary Ensemble},
  author	= {Deift, Percy and Menon, Govind and Trogdon, Thomas},
  journal	= {arXiv preprint arXiv:1507.00750},
  year		= {2015}
}

@Article{	  deift2016,
  archiveprefix	= {arXiv},
  arxivid	= {1604.07384},
  author	= {Deift, Percy and Trogdon, Thomas},
  eprint	= {1604.07384},
  file		= {:Users/trogdon/Library/Application Support/Mendeley
		  Desktop/Downloaded/Deift, Trogdon - 2016 - Universality for
		  the Toda algorithm to compute the eigenvalues of a random
		  matrix.pdf:pdf},
  journal	= {arXiv Prepr. arXiv1604.07384},
  month		= {apr},
  title		= {{Universality for the Toda algorithm to compute the
		  eigenvalues of a random matrix}},
  url		= {http://arxiv.org/abs/1604.07384},
  year		= {2016}
}

###Article{	  deift2016,
  archiveprefix	= {arXiv},
  arxivid	= {1604.07384},
  author	= {Deift, Percy and Trogdon, Thomas},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  eprint	= {1604.07384},
  file		= {:Users/trogdon/Library/Application Support/Mendeley
		  Desktop/Downloaded/Deift, Trogdon - 2016 - Universality for
		  the Toda algorithm to compute the eigenvalues of a random
		  matrix.pdf:pdf},
  journal	= {arXiv Prepr. arXiv1604.07384},
  month		= {apr},
  title		= {{Universality for the Toda algorithm to compute the
		  eigenvalues of a random matrix}},
  url		= {http://arxiv.org/abs/1604.07384},
  year		= {2016},
  bdsk-url-1	= {http://arxiv.org/abs/1604.07384}
}

@Article{	  deift2016universality,
  title		= {Universality for the Toda algorithm to compute the
		  eigenvalues of a random matrix},
  author	= {Deift, Percy and Trogdon, Thomas},
  journal	= {arXiv preprint arXiv:1604.07384},
  year		= {2016}
}

###Article{	  deift2016universality,
  author	= {Deift, Percy and Trogdon, Thomas},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1604.07384},
  title		= {Universality for the Toda algorithm to compute the
		  eigenvalues of a random matrix},
  year		= {2016}
}

###Article{	  deift2016universality,
  title		= {Universality for the Toda algorithm to compute the
		  eigenvalues of a random matrix},
  author	= {Deift, Percy and Trogdon, Thomas},
  journal	= {arXiv preprint arXiv:1604.07384},
  year		= {2016}
}

@Article{	  deift2017universality,
  title		= {Universality for eigenvalue algorithms on sample
		  covariance matrices},
  author	= {Deift, Percy and Trogdon, Thomas},
  journal	= {arXiv Preprint arXiv:1701.01896},
  volume	= {},
  number	= {},
  pages		= {1--31},
  year		= {2017}
}

###Article{	  deift2017universality,
  author	= {Deift, Percy and Trogdon, Thomas},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv Preprint arXiv:1701.01896},
  pages		= {1--31},
  title		= {Universality for eigenvalue algorithms on sample
		  covariance matrices},
  year		= {2017}
}

@Article{	  dembo2014,
  abstract	= {We consider the quadratic optimization problem
		  \$\$F\_n\^{}\{W,h\}:= $\backslash$sup\_\{x $\backslash$in
		  S\^{}\{n-1\}\} ( x\^{}T W x/2 + h\^{}T x )$\backslash$,,
		  \$\$ with \$W\$ a (random) matrix and \$h\$ a random
		  external field. We study the probabilities of large
		  deviation of \$F\_n\^{}\{W,h\}\$ for \$h\$ a centered
		  Gaussian vector with i.i.d. entries, both conditioned on
		  \$W\$ (a general Wigner matrix), and unconditioned when
		  \$W\$ is a GOE matrix. Our results validate (in a certain
		  region) and correct (in another region), the prediction
		  obtained by the mathematically non-rigorous replica method
		  in Y. V. Fyodorov, P. Le Doussal, J. Stat. phys. 154 (2014).},
  archiveprefix	= {arXiv},
  arxivid	= {1409.4606},
  author	= {Dembo, Amir and Zeitouni, Ofer},
  eprint	= {1409.4606},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1409.4606v1.pdf:pdf},
  keywords	= {a grant from the,and courant institute,and phrases,israel
		  science foundation,large deviations,nsf grant
		  dms-1106627,random matrices,replica method,research
		  partially supported by,spin glass,stanford
		  university,weizmann institute of science},
  month		= sep,
  pages		= {1--21},
  title		= {{Matrix optimization under random external fields}},
  url		= {http://arxiv.org/abs/1409.4606},
  year		= {2014}
}

###Article{	  dembo2014,
  abstract	= {We consider the quadratic optimization problem
		  \$\$F\_n\^{}\{W,h\}:= $\backslash$sup\_\{x $\backslash$in
		  S\^{}\{n-1\}\} ( x\^{}T W x/2 + h\^{}T x )$\backslash$,,
		  \$\$ with \$W\$ a (random) matrix and \$h\$ a random
		  external field. We study the probabilities of large
		  deviation of \$F\_n\^{}\{W,h\}\$ for \$h\$ a centered
		  Gaussian vector with i.i.d. entries, both conditioned on
		  \$W\$ (a general Wigner matrix), and unconditioned when
		  \$W\$ is a GOE matrix. Our results validate (in a certain
		  region) and correct (in another region), the prediction
		  obtained by the mathematically non-rigorous replica method
		  in Y. V. Fyodorov, P. Le Doussal, J. Stat. phys. 154 (2014).},
  archiveprefix	= {arXiv},
  arxivid	= {1409.4606},
  author	= {Dembo, Amir and Zeitouni, Ofer},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {1409.4606},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1409.4606v1.pdf:pdf},
  keywords	= {a grant from the,and courant institute,and phrases,israel
		  science foundation,large deviations,nsf grant
		  dms-1106627,random matrices,replica method,research
		  partially supported by,spin glass,stanford
		  university,weizmann institute of science},
  month		= sep,
  pages		= {1--21},
  title		= {{Matrix optimization under random external fields}},
  url		= {http://arxiv.org/abs/1409.4606},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1409.4606}
}

@InProceedings{	  denton14,
  author	= {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and
		  LeCun, Yann and Fergus, Rob},
  title		= {Exploiting Linear Structure Within Convolutional Networks
		  for Efficient Evaluation},
  booktitle	= {Proceedings of the 27th International Conference on Neural
		  Information Processing Systems},
  series	= {NIPS'14},
  year		= {2014},
  location	= {Montreal, Canada},
  pages		= {1269--1277},
  numpages	= {9},
  url		= {http://dl.acm.org/citation.cfm?id=2968826.2968968},
  acmid		= {2968968},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA}
}

###InProceedings{ denton14,
  acmid		= {2968968},
  address	= {Cambridge, MA, USA},
  author	= {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and
		  LeCun, Yann and Fergus, Rob},
  booktitle	= {Proceedings of the 27th International Conference on Neural
		  Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  location	= {Montreal, Canada},
  numpages	= {9},
  pages		= {1269--1277},
  publisher	= {MIT Press},
  series	= {NIPS'14},
  title		= {Exploiting Linear Structure Within Convolutional Networks
		  for Efficient Evaluation},
  url		= {http://dl.acm.org/citation.cfm?id=2968826.2968968},
  year		= {2014},
  bdsk-url-1	= {http://dl.acm.org/citation.cfm?id=2968826.2968968}
}

###InProceedings{ denton14,
  author	= {Denton, Emily and Zaremba, Wojciech and Bruna, Joan and
		  LeCun, Yann and Fergus, Rob},
  title		= {Exploiting Linear Structure Within Convolutional Networks
		  for Efficient Evaluation},
  booktitle	= {Proceedings of the 27th International Conference on Neural
		  Information Processing Systems},
  series	= {NIPS'14},
  year		= {2014},
  location	= {Montreal, Canada},
  pages		= {1269--1277},
  numpages	= {9},
  url		= {http://dl.acm.org/citation.cfm?id=2968826.2968968},
  acmid		= {2968968},
  publisher	= {MIT Press},
  address	= {Cambridge, MA, USA}
}

@Article{	  devarajanbio,
  author	= {Devarajan, K.},
  title		= {Nonnegative Matrix Factorization: An Analytical and
		  Interpretive Tool in Computational Biology},
  journal	= {PLoS Computational Biology},
  year		= {2008},
  volume	= {4}
}

###Article{	  devarajanbio,
  author	= {Devarajan, K.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {PLoS Computational Biology},
  title		= {Nonnegative Matrix Factorization: An Analytical and
		  Interpretive Tool in Computational Biology},
  volume	= {4},
  year		= {2008}
}

@Article{	  diagonalrmt,
  author	= {Pfrang, Christian W and Deift, Percy and Menon, Govind},
  journal	= {Random matrix theory, Interact. Part. Syst. Integr. Syst.
		  MSRI Publ.},
  pages		= {411--442},
  title		= {{How long does it take to compute the eigenvalues of a
		  random symmetric matrix?}},
  volume	= {65},
  year		= {2014}
}

###Article{	  diagonalrmt,
  author	= {Pfrang, Christian W and Deift, Percy and Menon, Govind},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Random matrix theory, Interact. Part. Syst. Integr. Syst.
		  MSRI Publ.},
  pages		= {411--442},
  title		= {{How long does it take to compute the eigenvalues of a
		  random symmetric matrix?}},
  volume	= {65},
  year		= {2014}
}

###Article{	  diagonalrmt,
  author	= {Pfrang, Christian W and Deift, Percy and Menon, Govind},
  journal	= {Random matrix theory, Interact. Part. Syst. Integr. Syst.
		  MSRI Publ.},
  pages		= {411--442},
  title		= {{How long does it take to compute the eigenvalues of a
		  random symmetric matrix?}},
  volume	= {65},
  year		= {2014}
}

@InProceedings{	  dingfbcsn14,
  author	= {N. Ding and Y. Fang and R. Babbush and C. Chen and R. D.
		  Skeel and H. Neven},
  title		= {Bayesian Sampling Using Stochastic Gradient Thermostats},
  booktitle	= {Advances in Neural Information Processing Systems},
  year		= {2014},
  pages		= {3203--3211},
  month		= dec
}

###InProceedings{ dingfbcsn14,
  author	= {N. Ding and Y. Fang and R. Babbush and C. Chen and R. D.
		  Skeel and H. Neven},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= dec,
  pages		= {3203--3211},
  title		= {Bayesian Sampling Using Stochastic Gradient Thermostats},
  year		= {2014}
}

@Article{	  dinh2017sharp,
  title		= {Sharp Minima Can Generalize For Deep Nets},
  author	= {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and
		  Bengio, Yoshua},
  journal	= {arXiv preprint arXiv:1703.04933},
  year		= {2017}
}

###Article{	  dinh2017sharp,
  author	= {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and
		  Bengio, Yoshua},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1703.04933},
  title		= {Sharp Minima Can Generalize For Deep Nets},
  year		= {2017}
}

###Article{	  dinh2017sharp,
  title		= {Sharp Minima Can Generalize For Deep Nets},
  author	= {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and
		  Bengio, Yoshua},
  journal	= {arXiv preprint arXiv:1703.04933},
  year		= {2017}
}

@Article{	  doi:10.1093/qje/qjw017,
  title		= { Decision Making Under the Gambler’s Fallacy: Evidence
		  from Asylum Judges, Loan Officers, and Baseball Umpires},
  author	= {Chen, Daniel L. and Moskowitz, Tobias J. and Shue, Kelly},
  journal	= {The Quarterly Journal of Economics},
  volume	= {131},
  number	= {3},
  pages		= {1181},
  year		= {2016},
  doi		= {10.1093/qje/qjw017}
}

###Article{	  doi:10.1093/qje/qjw017,
  author	= {Chen, Daniel L. and Moskowitz, Tobias J. and Shue, Kelly},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1093/qje/qjw017},
  journal	= {The Quarterly Journal of Economics},
  number	= {3},
  pages		= {1181},
  title		= {Decision Making Under the Gambler's Fallacy: Evidence from
		  Asylum Judges, Loan Officers, and Baseball Umpires},
  volume	= {131},
  year		= {2016},
  bdsk-url-1	= {https://doi.org/10.1093/qje/qjw017}
}

@InProceedings{	  domingos00,
  title		= {A unified bias-variance decomposition},
  author	= {Domingos, Pedro},
  booktitle	= {Proceedings of 17th International Conference on Machine
		  Learning},
  pages		= {231--238},
  year		= {2000}
}

@Article{	  donev04a,
  abstract	= {Packing problems, such as how densely objects can fill a
		  volume, are among the most ancient and persistent problems
		  in mathematics and science. For equal spheres, it has only
		  recently been proved that the face-centered cubic lattice
		  has the highest possible packing fraction . It is also well
		  known that certain random (amorphous) jammed packings have
		  {\oe}{\"U} ‚{\^a}{\`a} 0.64. Here, we show experimentally
		  and with a new simulation algorithm that ellipsoids can
		  randomly pack more densely‚{\"A}{\^\i}up to {\oe}{\"U}=
		  0.68 to 0.71for spheroids with an aspect ratio close to
		  that of M&M's Candies‚{\"A}{\^\i}and even approach
		  {\oe}{\"U} ‚{\^a}{\`a} 0.74 for ellipsoids with other
		  aspect ratios. We suggest that the higher density is
		  directly related to the higher number of degrees of freedom
		  per particle and thus the larger number of particle
		  contacts required to mechanically stabilize the packing. We
		  measured the number of contacts per particle Z
		  ‚{\^a}{\`a} 10 for our spheroids, as compared to Z
		  ‚{\^a}{\`a} 6 for spheres. Our results have implications
		  for a broad range of scientific disciplines, including the
		  properties of granular media and ceramics, glass formation,
		  and discrete geometry.},
  author	= {Donev, Aleksandar and Cisse, Ibrahim and Sachs, David and
		  Variano, Evan A. and Stillinger, Frank H. and Connelly,
		  Robert and Torquato, Salvatore and Chaikin, P. M.},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  doi		= {10.1126/science.1093010},
  journal	= {Science},
  number	= {5660},
  pages		= {990-993},
  title		= {Improving the Density of Jammed Disordered Packings Using
		  Ellipsoids},
  volume	= {303},
  year		= {2004},
  bdsk-url-1	= {http://dx.doi.org/10.1126/science.1093010}
}

###Article{	  donev04a,
  abstract	= {Packing problems, such as how densely objects can fill a
		  volume, are among the most ancient and persistent problems
		  in mathematics and science. For equal spheres, it has only
		  recently been proved that the face-centered cubic lattice
		  has the highest possible packing fraction . It is also well
		  known that certain random (amorphous) jammed packings have
		  {\oe}{\"U} ‚{\^a}{\`a} 0.64. Here, we show experimentally
		  and with a new simulation algorithm that ellipsoids can
		  randomly pack more densely‚{\"A}{\^\i}up to {\oe}{\"U}=
		  0.68 to 0.71for spheroids with an aspect ratio close to
		  that of M&M's Candies‚{\"A}{\^\i}and even approach
		  {\oe}{\"U} ‚{\^a}{\`a} 0.74 for ellipsoids with other
		  aspect ratios. We suggest that the higher density is
		  directly related to the higher number of degrees of freedom
		  per particle and thus the larger number of particle
		  contacts required to mechanically stabilize the packing. We
		  measured the number of contacts per particle Z
		  ‚{\^a}{\`a} 10 for our spheroids, as compared to Z
		  ‚{\^a}{\`a} 6 for spheres. Our results have implications
		  for a broad range of scientific disciplines, including the
		  properties of granular media and ceramics, glass formation,
		  and discrete geometry.},
  author	= {Donev, Aleksandar and Cisse, Ibrahim and Sachs, David and
		  Variano, Evan A. and Stillinger, Frank H. and Connelly,
		  Robert and Torquato, Salvatore and Chaikin, P. M.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1126/science.1093010},
  journal	= {Science},
  number	= {5660},
  pages		= {990-993},
  title		= {Improving the Density of Jammed Disordered Packings Using
		  Ellipsoids},
  volume	= {303},
  year		= {2004},
  bdsk-url-1	= {http://dx.doi.org/10.1126/science.1093010}
}

###Article{	  donev04a,
  abstract	= {Packing problems, such as how densely objects can fill a
		  volume, are among the most ancient and persistent problems
		  in mathematics and science. For equal spheres, it has only
		  recently been proved that the face-centered cubic lattice
		  has the highest possible packing fraction . It is also well
		  known that certain random (amorphous) jammed packings have
		  {{\oe}{\"U}} ‚{{\^a}{\`a}} 0.64. Here, we show
		  experimentally and with a new simulation algorithm that
		  ellipsoids can randomly pack more densely‚{{\"A}{\^\i}}up
		  to {{\oe}{\"U}}= 0.68 to 0.71for spheroids with an aspect
		  ratio close to that of M&M's Candies‚{{\"A}{\^\i}}and
		  even approach {{\oe}{\"U}} ‚{{\^a}{\`a}} 0.74 for
		  ellipsoids with other aspect ratios. We suggest that the
		  higher density is directly related to the higher number of
		  degrees of freedom per particle and thus the larger number
		  of particle contacts required to mechanically stabilize the
		  packing. We measured the number of contacts per particle Z
		  ‚{{\^a}{\`a}} 10 for our spheroids, as compared to Z
		  ‚{{\^a}{\`a}} 6 for spheres. Our results have
		  implications for a broad range of scientific disciplines,
		  including the properties of granular media and ceramics,
		  glass formation, and discrete geometry.},
  author	= {Donev, Aleksandar and Cisse, Ibrahim and Sachs, David and
		  Variano, Evan A. and Stillinger, Frank H. and Connelly,
		  Robert and Torquato, Salvatore and Chaikin, P. M.},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  doi		= {10.1126/science.1093010},
  journal	= {Science},
  number	= {5660},
  pages		= {990-993},
  title		= {Improving the Density of Jammed Disordered Packings Using
		  Ellipsoids},
  volume	= {303},
  year		= {2004},
  bdsk-url-1	= {http://dx.doi.org/10.1126/science.1093010}
}

@Article{	  draxler2018essentially,
  title		= {Essentially No Barriers in Neural Network Energy
		  Landscape},
  author	= {Draxler, Felix and Veschgini, Kambis and Salmhofer,
		  Manfred and Hamprecht, Fred A},
  journal	= {arXiv preprint arXiv:1803.00885},
  year		= {2018}
}

@Article{	  ducation2007,
  author	= {Ducation, E},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Myers2007\_computational\_methods.pdf:pdf},
  pages		= {75--79},
  title		= {{Ython for}},
  year		= {2007}
}

###Article{	  ducation2007,
  author	= {Ducation, E},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Myers2007\_computational\_methods.pdf:pdf},
  pages		= {75--79},
  title		= {{Ython for}},
  year		= {2007}
}

@InProceedings{	  dunn2017early,
  title		= {Early predictability of asylum court decisions},
  author	= {Dunn, Matt and Sagun, Levent and {\c{S}}irin, Hale and
		  Chen, Daniel},
  booktitle	= {Proceedings of the 16th edition of the International
		  Conference on Articial Intelligence and Law},
  pages		= {233--236},
  year		= {2017},
  organization	= {ACM}
}

@Article{	  dunn2017searchqa,
  title		= {SearchQA: A new Q\&A dataset augmented with context from a
		  search engine},
  author	= {Dunn, Matthew and Sagun, Levent and Higgins, Mike and
		  Guney, V Ugur and Cirik, Volkan and Cho, Kyunghyun},
  journal	= {arXiv preprint arXiv:1704.05179},
  year		= {2017}
}

@Article{	  during13,
  author	= {D{\"u}ring, Gustavo and Lerner, Edan and Wyart, Matthieu},
  date		= {2013},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  journal	= {Soft Matter},
  number	= {1},
  pages		= {146-154},
  publisher	= {Royal Society of Chemistry},
  title		= {Phonon gap and localization lengths in floppy materials},
  volume	= {9},
  year		= {2013}
}

###Article{	  during13,
  author	= {D{\"u}ring, Gustavo and Lerner, Edan and Wyart, Matthieu},
  date		= {2013},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Soft Matter},
  number	= {1},
  pages		= {146-154},
  publisher	= {Royal Society of Chemistry},
  title		= {Phonon gap and localization lengths in floppy materials},
  volume	= {9},
  year		= {2013}
}

###Article{	  during13,
  author	= {{D{\"u}ring}, Gustavo and Lerner, Edan and Wyart,
		  Matthieu},
  date		= {2013},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  journal	= {Soft Matter},
  number	= {1},
  pages		= {146-154},
  publisher	= {Royal Society of Chemistry},
  title		= {Phonon gap and localization lengths in floppy materials},
  volume	= {9},
  year		= {2013}
}

@InProceedings{	  eldan2016power,
  title		= {The power of depth for feedforward neural networks},
  author	= {Eldan, Ronen and Shamir, Ohad},
  booktitle	= {Conference on Learning Theory},
  pages		= {907--940},
  year		= {2016}
}

###InProceedings{ eldan2016power,
  author	= {Eldan, Ronen and Shamir, Ohad},
  booktitle	= {Conference on Learning Theory},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {907--940},
  title		= {The power of depth for feedforward neural networks},
  year		= {2016}
}

###InProceedings{ eldan2016power,
  title		= {The power of depth for feedforward neural networks},
  author	= {Eldan, Ronen and Shamir, Ohad},
  booktitle	= {Conference on Learning Theory},
  pages		= {907--940},
  year		= {2016}
}

@Book{		  engel2001statistical,
  title		= {Statistical mechanics of learning},
  author	= {Engel, Andreas and Van den Broeck, Christian},
  year		= {2001},
  publisher	= {Cambridge University Press}
}

###Book{	  engel2001statistical,
  title		= {Statistical mechanics of learning},
  author	= {Engel, Andreas and Van den Broeck, Christian},
  year		= {2001},
  publisher	= {Cambridge University Press}
}

@Article{	  environment,
  author	= {Environment, Matlab-like and Learning, Machine},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/torch7.pdf:pdf},
  title		= {{Torch7 Scientific computing for Lua ( JIT ) Torch7}}
}

###Article{	  environment,
  author	= {Environment, Matlab-like and Learning, Machine},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/torch7.pdf:pdf},
  title		= {{Torch7 Scientific computing for Lua ( JIT ) Torch7}}
}

@Article{	  eqbaetal,
  title		= {Cugliandolo-Kurchan equations for dynamics of
		  Spin-Glasses},
  author	= {Ben Arous, G\'erard and Dembo, Amir and Guionnet, Alice},
  journal	= {Probability theory and related fields},
  volume	= {136},
  number	= {4},
  pages		= {619--660},
  year		= {2006},
  publisher	= {Springer}
}

###Article{	  eqbaetal,
  author	= {Ben Arous, G\'erard and Dembo, Amir and Guionnet, Alice},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Probability theory and related fields},
  number	= {4},
  pages		= {619--660},
  publisher	= {Springer},
  title		= {Cugliandolo-Kurchan equations for dynamics of
		  Spin-Glasses},
  volume	= {136},
  year		= {2006}
}

###Article{	  eqbaetal,
  title		= {Cugliandolo-Kurchan equations for dynamics of
		  Spin-Glasses},
  author	= {Ben Arous, {G\'erard} and Dembo, Amir and Guionnet, Alice},
  journal	= {Probability theory and related fields},
  volume	= {136},
  number	= {4},
  pages		= {619--660},
  year		= {2006},
  publisher	= {Springer}
}

@InProceedings{	  eusipco09,
  author	= {C. F{\'e}votte and A. T. Cemgil},
  title		= {Nonnegative matrix factorisations as probabilistic
		  inference in composite models},
  booktitle	= {European Signal Processing Conference},
  year		= {2009},
  pages		= {1913-1917},
  month		= aug,
  owner		= {umutsimsekli},
  timestamp	= {2014.01.14}
}

###InProceedings{ eusipco09,
  author	= {C. F{\'e}votte and A. T. Cemgil},
  booktitle	= {European Signal Processing Conference},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= aug,
  owner		= {umutsimsekli},
  pages		= {1913-1917},
  timestamp	= {2014.01.14},
  title		= {Nonnegative matrix factorisations as probabilistic
		  inference in composite models},
  year		= {2009}
}

@Misc{		  facebook,
  title		= {Facebook dataset},
  year		= {2009},
  howpublished	= {\url{http://socialnetworks.mpi-sws.org/data-wosn2009.html}}
}

###Misc{	  facebook,
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  howpublished	= {\url{http://socialnetworks.mpi-sws.org/data-wosn2009.html}},
  title		= {Facebook dataset},
  year		= {2009}
}

@Article{	  faury2018neural,
  title		= {Neural Generative Models for Global Optimization with
		  Gradients},
  author	= {Faury, Louis and Vasile, Flavian and Calauz{\`e}nes,
		  Cl{\'e}ment and Fercoq, Oliver},
  journal	= {arXiv preprint arXiv:1805.08594},
  year		= {2018}
}

@Article{	  fazio2012,
  author	= {Fazio, Giorgio and Modica, Marco},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/media\_238147\_en.pdf:pdf},
  pages		= {1--21},
  title		= {{Pareto or log-normal ? A recursive-truncation approach to
		  the distribution of ( all ) cities}},
  year		= {2012}
}

###Article{	  fazio2012,
  author	= {Fazio, Giorgio and Modica, Marco},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/media\_238147\_en.pdf:pdf},
  pages		= {1--21},
  title		= {{Pareto or log-normal ? A recursive-truncation approach to
		  the distribution of ( all ) cities}},
  year		= {2012}
}

@Article{	  forrester2012,
  author	= {Forrester, Peter J},
  doi		= {10.1088/1751-8113/45/7/075206},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1751-8121\_45\_7\_075206.pdf:pdf},
  issn		= {1751-8113},
  journal	= {Journal of Physics A: Mathematical and Theoretical},
  month		= feb,
  number	= {7},
  pages		= {075206},
  title		= {{Spectral density asymptotics for Gaussian and Laguerre
		  $\beta$-ensembles in the exponentially small region}},
  url		= {http://stacks.iop.org/1751-8121/45/i=7/a=075206?key=crossref.daa119d143f1d7a967b11b607fafb04f},
  volume	= {45},
  year		= {2012}
}

###Article{	  forrester2012,
  author	= {Forrester, Peter J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1088/1751-8113/45/7/075206},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1751-8121\_45\_7\_075206.pdf:pdf},
  issn		= {1751-8113},
  journal	= {Journal of Physics A: Mathematical and Theoretical},
  month		= feb,
  number	= {7},
  pages		= {075206},
  title		= {{Spectral density asymptotics for Gaussian and Laguerre
		  $\beta$-ensembles in the exponentially small region}},
  url		= {http://stacks.iop.org/1751-8121/45/i=7/a=075206?key=crossref.daa119d143f1d7a967b11b607fafb04f},
  volume	= {45},
  year		= {2012},
  bdsk-url-1	= {http://stacks.iop.org/1751-8121/45/i=7/a=075206?key=crossref.daa119d143f1d7a967b11b607fafb04f},
  bdsk-url-2	= {https://doi.org/10.1088/1751-8113/45/7/075206}
}

@Article{	  franz15,
  title		= {Universal spectrum of normal modes in low-temperature
		  glasses},
  author	= {Franz, Silvio and Parisi, Giorgio and Urbani,
		  Pierfrancesco and Zamponi, Francesco},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {112},
  number	= {47},
  pages		= {14539--14544},
  year		= {2015},
  publisher	= {National Acad Sciences}
}

###Article{	  franz15,
  author	= {Franz, Silvio and Parisi, Giorgio and Urbani,
		  Pierfrancesco and Zamponi, Francesco},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Proceedings of the National Academy of Sciences},
  number	= {47},
  pages		= {14539--14544},
  publisher	= {National Acad Sciences},
  title		= {Universal spectrum of normal modes in low-temperature
		  glasses},
  volume	= {112},
  year		= {2015}
}

###Article{	  franz15,
  title		= {Universal spectrum of normal modes in low-temperature
		  glasses},
  author	= {Franz, Silvio and Parisi, Giorgio and Urbani,
		  Pierfrancesco and Zamponi, Francesco},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {112},
  number	= {47},
  pages		= {14539--14544},
  year		= {2015},
  publisher	= {National Acad Sciences}
}

@Article{	  franz16,
  title		= {The simplest model of jamming},
  author	= {Franz, Silvio and Parisi, Giorgio},
  journal	= {Journal of Physics A: Mathematical and Theoretical},
  volume	= {49},
  number	= {14},
  pages		= {145001},
  year		= {2016},
  publisher	= {IOP Publishing}
}

###Article{	  franz16,
  author	= {Franz, Silvio and Parisi, Giorgio},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Physics A: Mathematical and Theoretical},
  number	= {14},
  pages		= {145001},
  publisher	= {IOP Publishing},
  title		= {The simplest model of jamming},
  volume	= {49},
  year		= {2016}
}

###Article{	  franz16,
  title		= {The simplest model of jamming},
  author	= {Franz, Silvio and Parisi, Giorgio},
  journal	= {Journal of Physics A: Mathematical and Theoretical},
  volume	= {49},
  number	= {14},
  pages		= {145001},
  year		= {2016},
  publisher	= {IOP Publishing}
}

@Article{	  franz17,
  title		= {Universality of the SAT-UNSAT (jamming) threshold in
		  non-convex continuous constraint satisfaction problems},
  author	= {Franz, Silvio and Parisi, Giorgio and Sevelev, Maxime and
		  Urbani, Pierfrancesco and Zamponi, Francesco},
  journal	= {SciPost Physics},
  volume	= {2},
  number	= {3},
  pages		= {019},
  year		= {2017}
}

###Article{	  franz17,
  author	= {Franz, Silvio and Parisi, Giorgio and Sevelev, Maxime and
		  Urbani, Pierfrancesco and Zamponi, Francesco},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {SciPost Physics},
  number	= {3},
  pages		= {019},
  title		= {Universality of the SAT-UNSAT (jamming) threshold in
		  non-convex continuous constraint satisfaction problems},
  volume	= {2},
  year		= {2017}
}

###Article{	  franz17,
  title		= {Universality of the SAT-UNSAT (jamming) threshold in
		  non-convex continuous constraint satisfaction problems},
  author	= {Franz, Silvio and Parisi, Giorgio and Sevelev, Maxime and
		  Urbani, Pierfrancesco and Zamponi, Francesco},
  journal	= {SciPost Physics},
  volume	= {2},
  number	= {3},
  pages		= {019},
  year		= {2017}
}

@Article{	  franz17b,
  title		= {Mean-field avalanches in jammed spheres},
  author	= {Franz, Silvio and Spigler, Stefano},
  journal	= {Physical Review E},
  volume	= {95},
  number	= {2},
  pages		= {022139},
  year		= {2017},
  publisher	= {APS}
}

###Article{	  franz17b,
  author	= {Franz, Silvio and Spigler, Stefano},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review E},
  number	= {2},
  pages		= {022139},
  publisher	= {APS},
  title		= {Mean-field avalanches in jammed spheres},
  volume	= {95},
  year		= {2017}
}

###Article{	  franz17b,
  title		= {Mean-field avalanches in jammed spheres},
  author	= {Franz, Silvio and Spigler, Stefano},
  journal	= {Physical Review E},
  volume	= {95},
  number	= {2},
  pages		= {022139},
  year		= {2017},
  publisher	= {APS}
}

@Article{	  franz2018jamming,
  title		= {Jamming in multilayer supervised learning models},
  author	= {Franz, Silvio and Hwang, Sungmin and Urbani,
		  Pierfrancesco},
  journal	= {arXiv preprint arXiv:1809.09945},
  year		= {2018}
}

###Article{	  franz2018jamming,
  title		= {Jamming in multilayer supervised learning models},
  author	= {Franz, Silvio and Hwang, Sungmin and Urbani,
		  Pierfrancesco},
  journal	= {arXiv preprint arXiv:1809.09945},
  year		= {2018}
}

@Article{	  franzpre,
  title		= {Jamming in multilayer supervised learning models},
  author	= {S. Franz, S. Hwang, P. Urbani},
  journal	= {arXiv preprint arXiv:1809.09945},
  year		= {2018}
}

###Article{	  franzpre,
  author	= {S. Franz, S. Hwang, P. Urbani},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1809.09945},
  title		= {Jamming in multilayer supervised learning models},
  year		= {2018}
}

###Article{	  franzpre,
  title		= {Jamming in multilayer supervised learning models},
  author	= {S. Franz, S. Hwang, P. Urbani},
  journal	= {arXiv preprint arXiv:1809.09945},
  year		= {2018}
}

@Article{	  freeman16,
  title		= {Topology and Geometry of Deep Rectified Network
		  Optimization Landscapes},
  author	= {Freeman, C Daniel and Bruna, Joan},
  journal	= {International Conference on Learning Representations},
  year		= {2017}
}

###Article{	  freeman16,
  author	= {Freeman, C Daniel and Bruna, Joan},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Topology and Geometry of Deep Rectified Network
		  Optimization Landscapes},
  year		= {2017}
}

@Article{	  freeman2016topology,
  title		= {Topology and Geometry of Deep Rectified Network
		  Optimization Landscapes},
  author	= {Freeman, C Daniel and Bruna, Joan},
  journal	= {arXiv preprint arXiv:1611.01540},
  year		= {2016}
}

###Article{	  freeman2016topology,
  author	= {Freeman, C Daniel and Bruna, Joan},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1611.01540},
  title		= {Topology and Geometry of Deep Rectified Network
		  Optimization Landscapes},
  year		= {2016}
}

@Book{		  friedman2001elements,
  title		= {The elements of statistical learning},
  author	= {Friedman, Jerome and Hastie, Trevor and Tibshirani,
		  Robert},
  year		= {2001},
  publisher	= {Springer series in statistics New York, NY, USA:}
}

@InProceedings{	  frieze_et_al:lipics:2008:1752,
  address	= {Dagstuhl, Germany},
  annote	= { From Duplicate 1 (
		  
		  A new approach to the planted clique problem
		  
		  - Frieze, Alan; Kannan, Ravi )
		  
		  Keywords: Planted Clique, Tensor, Random Graph
		  
		  },
  author	= {Frieze, Alan and Kannan, Ravi},
  booktitle	= {IARCS Annual Conference on Foundations of Software
		  Technology and Theoretical Computer Science},
  doi		= {http://dx.doi.org/10.4230/LIPIcs.FSTTCS.2008.1752},
  editor	= {Hariharan, Ramesh and Mukund, Madhavan and Vinay, V},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/planted.pdf:pdf},
  isbn		= {978-3-939897-08-8},
  issn		= {1868-8969},
  pages		= {187--198},
  publisher	= {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  series	= {Leibniz International Proceedings in Informatics
		  (LIPIcs)},
  title		= {{A new approach to the planted clique problem}},
  url		= {http://drops.dagstuhl.de/opus/volltexte/2008/1752},
  volume	= {2},
  year		= {2008}
}

###InProceedings{ frieze_et_al:lipics:2008:1752,
  address	= {Dagstuhl, Germany},
  annote	= { From Duplicate 1 (
		  
		  A new approach to the planted clique problem
		  
		  - Frieze, Alan; Kannan, Ravi )
		  
		  Keywords: Planted Clique, Tensor, Random Graph
		  
		  },
  author	= {Frieze, Alan and Kannan, Ravi},
  booktitle	= {IARCS Annual Conference on Foundations of Software
		  Technology and Theoretical Computer Science},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {http://dx.doi.org/10.4230/LIPIcs.FSTTCS.2008.1752},
  editor	= {Hariharan, Ramesh and Mukund, Madhavan and Vinay, V},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/planted.pdf:pdf},
  isbn		= {978-3-939897-08-8},
  issn		= {1868-8969},
  pages		= {187--198},
  publisher	= {Schloss Dagstuhl--Leibniz-Zentrum fuer Informatik},
  series	= {Leibniz International Proceedings in Informatics
		  (LIPIcs)},
  title		= {{A new approach to the planted clique problem}},
  url		= {http://drops.dagstuhl.de/opus/volltexte/2008/1752},
  volume	= {2},
  year		= {2008},
  bdsk-url-1	= {http://drops.dagstuhl.de/opus/volltexte/2008/1752},
  bdsk-url-2	= {http://dx.doi.org/10.4230/LIPIcs.FSTTCS.2008.1752}
}

@Article{	  frp08,
  author	= {Friel, N. and Pettitt, A. N.},
  title		= {{Marginal likelihood estimation via power posteriors}},
  journal	= {Journal of the Royal Statistical Society: Series B
		  (Statistical Methodology)},
  year		= {2008},
  volume	= {70},
  pages		= {589--607},
  number	= {3},
  month		= jul,
  day		= {1},
  publisher	= {Blackwell Publishing Ltd}
}

###Article{	  frp08,
  author	= {Friel, N. and Pettitt, A. N.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  day		= {1},
  journal	= {Journal of the Royal Statistical Society: Series B
		  (Statistical Methodology)},
  month		= jul,
  number	= {3},
  pages		= {589--607},
  publisher	= {Blackwell Publishing Ltd},
  title		= {{Marginal likelihood estimation via power posteriors}},
  volume	= {70},
  year		= {2008}
}

@Article{	  fyodorov2013,
  abstract	= {Our goal is to discuss in detail the calculation of the
		  mean number of stationary points and minima for random
		  isotropic Gaussian fields on a sphere as well as for
		  stationary Gaussian random fields in a background parabolic
		  confinement. After developing the general formalism based
		  on the high-dimensional Kac-Rice formulae we combine it
		  with the Random Matrix Theory (RMT) techniques to perform
		  analysis of the random energy landscape of \$p-\$spin
		  spherical spinglasses and a related glass model, both
		  displaying a zero-temperature one-step replica symmetry
		  breaking glass transition as a function of control
		  parameters (e.g. a magnetic field or curvature of the
		  confining potential). A particular emphasis of the
		  presented analysis is on understanding in detail the
		  picture of "topology trivialization" (in the sense of
		  drastic reduction of the number of stationary points) of
		  the landscape which takes place in the vicinity of the
		  zero-temperature glass transition in both models. We will
		  reveal the important role of the GOE "edge scaling"
		  spectral region and the Tracy-Widom distribution of the
		  maximal eigenvalue of GOE matrices for providing an
		  accurate quantitative description of the universal features
		  of the topology trivialization scenario.},
  archiveprefix	= {arXiv},
  arxivid	= {1307.2379},
  author	= {Fyodorov, Yan V},
  eprint	= {1307.2379},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1307.2379v2.pdf:pdf},
  month		= jul,
  pages		= {40},
  title		= {{High-Dimensional Random Fields and Random Matrix
		  Theory}},
  url		= {http://arxiv.org/abs/1307.2379},
  year		= {2013}
}

###Article{	  fyodorov2013,
  abstract	= {Our goal is to discuss in detail the calculation of the
		  mean number of stationary points and minima for random
		  isotropic Gaussian fields on a sphere as well as for
		  stationary Gaussian random fields in a background parabolic
		  confinement. After developing the general formalism based
		  on the high-dimensional Kac-Rice formulae we combine it
		  with the Random Matrix Theory (RMT) techniques to perform
		  analysis of the random energy landscape of \$p-\$spin
		  spherical spinglasses and a related glass model, both
		  displaying a zero-temperature one-step replica symmetry
		  breaking glass transition as a function of control
		  parameters (e.g. a magnetic field or curvature of the
		  confining potential). A particular emphasis of the
		  presented analysis is on understanding in detail the
		  picture of "topology trivialization" (in the sense of
		  drastic reduction of the number of stationary points) of
		  the landscape which takes place in the vicinity of the
		  zero-temperature glass transition in both models. We will
		  reveal the important role of the GOE "edge scaling"
		  spectral region and the Tracy-Widom distribution of the
		  maximal eigenvalue of GOE matrices for providing an
		  accurate quantitative description of the universal features
		  of the topology trivialization scenario.},
  archiveprefix	= {arXiv},
  arxivid	= {1307.2379},
  author	= {Fyodorov, Yan V},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {1307.2379},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1307.2379v2.pdf:pdf},
  month		= jul,
  pages		= {40},
  title		= {{High-Dimensional Random Fields and Random Matrix
		  Theory}},
  url		= {http://arxiv.org/abs/1307.2379},
  year		= {2013},
  bdsk-url-1	= {http://arxiv.org/abs/1307.2379}
}

@Article{	  fyodorov2013a,
  abstract	= {Finding the global minimum of a cost function given by the
		  sum of a quadratic and a linear form in N real variables
		  over (N-1)- dimensional sphere is one of the simplest, yet
		  paradigmatic problems in Optimization Theory known as the
		  "trust region subproblem" or "constraint least square
		  problem". When both terms in the cost function are random
		  this amounts to studying the ground state energy of the
		  simplest spherical spin glass in a random magnetic field.
		  We first identify and study two distinct large-N scaling
		  regimes in which the linear term (magnetic field) leads to
		  a gradual topology trivialization, i.e. reduction in the
		  total number N\_\{tot\} of critical (stationary) points in
		  the cost function landscape. In the first regime N\_\{tot\}
		  remains of the order \$N\$ and the cost function (energy)
		  has generically two almost degenerate minima with the
		  Tracy-Widom (TW) statistics. In the second regime the
		  number of critical points is of the order of unity with a
		  finite probability for a single minimum. In that case the
		  mean total number of extrema (minima and maxima) of the
		  cost function is given by the Laplace transform of the TW
		  density, and the distribution of the global minimum energy
		  is expected to take a universal scaling form generalizing
		  the TW law. Though the full form of that distribution is
		  not yet known to us, one of its far tails can be inferred
		  from the large deviation theory for the global minimum. In
		  the rest of the paper we show how to use the replica method
		  to obtain the probability density of the minimum energy in
		  the large-deviation approximation by finding both the rate
		  function and the leading pre-exponential factor.},
  archiveprefix	= {arXiv},
  arxivid	= {1304.0024},
  author	= {Fyodorov, Yan V and {Le Doussal}, Pierre},
  doi		= {10.1007/s10955-013-0838-1},
  eprint	= {1304.0024},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1304.0024v2.pdf:pdf},
  issn		= {0022-4715},
  journal	= {Journal of Statistical Physics},
  month		= sep,
  number	= {1-2},
  pages		= {466--490},
  title		= {{Topology Trivialization and Large Deviations for the
		  Minimum in the Simplest Random Optimization}},
  url		= {http://link.springer.com/10.1007/s10955-013-0838-1},
  volume	= {154},
  year		= {2013}
}

###Article{	  fyodorov2013a,
  abstract	= {Finding the global minimum of a cost function given by the
		  sum of a quadratic and a linear form in N real variables
		  over (N-1)- dimensional sphere is one of the simplest, yet
		  paradigmatic problems in Optimization Theory known as the
		  "trust region subproblem" or "constraint least square
		  problem". When both terms in the cost function are random
		  this amounts to studying the ground state energy of the
		  simplest spherical spin glass in a random magnetic field.
		  We first identify and study two distinct large-N scaling
		  regimes in which the linear term (magnetic field) leads to
		  a gradual topology trivialization, i.e. reduction in the
		  total number N\_\{tot\} of critical (stationary) points in
		  the cost function landscape. In the first regime N\_\{tot\}
		  remains of the order \$N\$ and the cost function (energy)
		  has generically two almost degenerate minima with the
		  Tracy-Widom (TW) statistics. In the second regime the
		  number of critical points is of the order of unity with a
		  finite probability for a single minimum. In that case the
		  mean total number of extrema (minima and maxima) of the
		  cost function is given by the Laplace transform of the TW
		  density, and the distribution of the global minimum energy
		  is expected to take a universal scaling form generalizing
		  the TW law. Though the full form of that distribution is
		  not yet known to us, one of its far tails can be inferred
		  from the large deviation theory for the global minimum. In
		  the rest of the paper we show how to use the replica method
		  to obtain the probability density of the minimum energy in
		  the large-deviation approximation by finding both the rate
		  function and the leading pre-exponential factor.},
  archiveprefix	= {arXiv},
  arxivid	= {1304.0024},
  author	= {Fyodorov, Yan V and {Le Doussal}, Pierre},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1007/s10955-013-0838-1},
  eprint	= {1304.0024},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1304.0024v2.pdf:pdf},
  issn		= {0022-4715},
  journal	= {Journal of Statistical Physics},
  month		= sep,
  number	= {1-2},
  pages		= {466--490},
  title		= {{Topology Trivialization and Large Deviations for the
		  Minimum in the Simplest Random Optimization}},
  url		= {http://link.springer.com/10.1007/s10955-013-0838-1},
  volume	= {154},
  year		= {2013},
  bdsk-url-1	= {http://link.springer.com/10.1007/s10955-013-0838-1},
  bdsk-url-2	= {https://doi.org/10.1007/s10955-013-0838-1}
}

@Article{	  gardner88,
  title		= {The space of interactions in neural network models},
  author	= {Gardner, Elizabeth},
  journal	= {Journal of physics A: Mathematical and general},
  volume	= {21},
  number	= {1},
  pages		= {257},
  year		= {1988},
  publisher	= {IOP Publishing}
}

###Article{	  gardner88,
  author	= {Gardner, Elizabeth},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of physics A: Mathematical and general},
  number	= {1},
  pages		= {257},
  publisher	= {IOP Publishing},
  title		= {The space of interactions in neural network models},
  volume	= {21},
  year		= {1988}
}

###Article{	  gardner88,
  title		= {The space of interactions in neural network models},
  author	= {Gardner, Elizabeth},
  journal	= {Journal of physics A: Mathematical and general},
  volume	= {21},
  number	= {1},
  pages		= {257},
  year		= {1988},
  publisher	= {IOP Publishing}
}

@Article{	  gastaldi17,
  title		= {Shake-Shake regularization of 3-branch residual networks},
  author	= {Gastaldi, Xavier},
  journal	= {International Conference on Learning Representations},
  year		= {2017}
}

###Article{	  gastaldi17,
  author	= {Gastaldi, Xavier},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Shake-Shake regularization of 3-branch residual networks},
  year		= {2017}
}

###Article{	  gastaldi17,
  title		= {Shake-Shake regularization of 3-branch residual networks},
  author	= {Gastaldi, Xavier},
  journal	= {International Conference on Learning Representations},
  year		= {2017}
}

@Article{	  geiger18,
  title		= {The jamming transition as a paradigm to understand the
		  loss landscape of deep neural networks},
  author	= {Geiger, Mario and Spigler, Stefano and d'Ascoli,
		  {St{\'e}phane} and Sagun, Levent and Baity-Jesi, Marco and
		  Biroli, Giulio and Wyart, Matthieu},
  journal	= {arXiv preprint arXiv:1809.09349},
  year		= {2018}
}

###Article{	  geiger18,
  title		= {The jamming transition as a paradigm to understand the
		  loss landscape of deep neural networks},
  author	= {Geiger, Mario and Spigler, Stefano and d'Ascoli,
		  {St{\'e}phane} and Sagun, Levent and Baity-Jesi, Marco and
		  Biroli, Giulio and Wyart, Matthieu},
  journal	= {arXiv preprint arXiv:1809.09349},
  year		= {2018}
}


@Article{	  gelman1998,
  author	= {Gelman, A. and Meng, X. L.},
  title		= {Simulating normalizing constants: from importance sampling
		  to bridge sampling to path sampling},
  journal	= {Statist. Sci.},
  year		= {1998},
  volume	= {13},
  pages		= {163--185},
  number	= {2},
  month		= {05},
  fjournal	= {Statistical Science},
  publisher	= {The Institute of Mathematical Statistics}
}

###Article{	  gelman1998,
  author	= {Gelman, A. and Meng, X. L.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  fjournal	= {Statistical Science},
  journal	= {Statist. Sci.},
  month		= {05},
  number	= {2},
  pages		= {163--185},
  publisher	= {The Institute of Mathematical Statistics},
  title		= {Simulating normalizing constants: from importance sampling
		  to bridge sampling to path sampling},
  volume	= {13},
  year		= {1998}
}

@Article{	  geman1992neural,
  title		= {Neural networks and the bias/variance dilemma},
  author	= {Geman, Stuart and Bienenstock, Elie and Doursat, Ren{\'e}},
  journal	= {Neural computation},
  volume	= {4},
  number	= {1},
  pages		= {1--58},
  year		= {1992},
  publisher	= {MIT Press}
}

@InProceedings{	  goroshin2014,
  author	= {Goroshin, Rostislav and Szlam, Arthur and Bruna, Joan and
		  Eigen, David and Tompson, Jonathan and Lecun, Yann},
  booktitle	= {Deep Learning and Representation Learning Workshop: NIPS
		  2014},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/3.pdf:pdf},
  pages		= {1--9},
  title		= {{Unsupervised Feature Learning from Temporal Data}},
  url		= {http://www.dlworkshop.org/49.pdf?attredirects=0},
  year		= {2014}
}

###InProceedings{ goroshin2014,
  author	= {Goroshin, Rostislav and Szlam, Arthur and Bruna, Joan and
		  Eigen, David and Tompson, Jonathan and Lecun, Yann},
  booktitle	= {Deep Learning and Representation Learning Workshop: NIPS
		  2014},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/3.pdf:pdf},
  pages		= {1--9},
  title		= {{Unsupervised Feature Learning from Temporal Data}},
  url		= {http://www.dlworkshop.org/49.pdf?attredirects=0},
  year		= {2014},
  bdsk-url-1	= {http://www.dlworkshop.org/49.pdf?attredirects=0}
}

@Article{	  gotmare2018using,
  title		= {Using Mode Connectivity for Loss Landscape Analysis},
  author	= {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong,
		  Caiming and Socher, Richard},
  journal	= {arXiv preprint arXiv:1806.06977},
  year		= {2018}
}

@Article{	  goyal2017accurate,
  title		= {Accurate, Large Minibatch SGD: Training ImageNet in 1
		  Hour},
  author	= {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and
		  Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo
		  and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal	= {arXiv preprint arXiv:1706.02677},
  year		= {2017}
}

###Article{	  goyal2017accurate,
  author	= {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and
		  Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo
		  and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1706.02677},
  title		= {Accurate, Large Minibatch SGD: Training ImageNet in 1
		  Hour},
  year		= {2017}
}

###Article{	  goyal2017accurate,
  title		= {Accurate, Large Minibatch SGD: Training ImageNet in 1
		  Hour},
  author	= {Goyal, Priya and {Doll{\'a}r}, Piotr and Girshick, Ross
		  and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola,
		  Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal	= {arXiv preprint arXiv:1706.02677},
  year		= {2017}
}

@Article{	  graziano2011,
  abstract	= {We proposed a theory of consciousness in which the
		  machinery for social perception constructs awareness, and
		  awareness is a perceptual model of the process of
		  attention. One can attribute awareness to others or to
		  oneself. Awareness of X is the brain's perceptual metaphor
		  for the deep attentive processing of X. A set of ten
		  comments on our hypothesis are included in this issue. Each
		  comment raises specific points some of which directly
		  challenge the hypothesis. Here we respond to these specific
		  points and challenges.},
  author	= {Graziano, Michael S A and Kastner, Sabine},
  doi		= {10.1080/17588928.2011.585237},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Cog\_Neurosci2011\_125.pdf:pdf},
  issn		= {1758-8928},
  journal	= {Cognitive neuroscience},
  month		= jun,
  number	= {2},
  pages		= {125--7},
  pmid		= {24168488},
  title		= {{Awareness as a perceptual model of attention.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/24168488},
  volume	= {2},
  year		= {2011}
}

###Article{	  graziano2011,
  abstract	= {We proposed a theory of consciousness in which the
		  machinery for social perception constructs awareness, and
		  awareness is a perceptual model of the process of
		  attention. One can attribute awareness to others or to
		  oneself. Awareness of X is the brain's perceptual metaphor
		  for the deep attentive processing of X. A set of ten
		  comments on our hypothesis are included in this issue. Each
		  comment raises specific points some of which directly
		  challenge the hypothesis. Here we respond to these specific
		  points and challenges.},
  author	= {Graziano, Michael S A and Kastner, Sabine},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1080/17588928.2011.585237},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Cog\_Neurosci2011\_125.pdf:pdf},
  issn		= {1758-8928},
  journal	= {Cognitive neuroscience},
  month		= jun,
  number	= {2},
  pages		= {125--7},
  pmid		= {24168488},
  title		= {{Awareness as a perceptual model of attention.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/24168488},
  volume	= {2},
  year		= {2011},
  bdsk-url-1	= {http://www.ncbi.nlm.nih.gov/pubmed/24168488},
  bdsk-url-2	= {https://doi.org/10.1080/17588928.2011.585237}
}

@Article{	  greenbaum1989behavior,
  title		= {Behavior of slightly perturbed Lanczos and
		  conjugate-gradient recurrences},
  author	= {Greenbaum, Anne},
  journal	= {Linear Algebra and its Applications},
  volume	= {113},
  pages		= {7--63},
  year		= {1989},
  publisher	= {Elsevier}
}

###Article{	  greenbaum1989behavior,
  author	= {Greenbaum, Anne},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Linear Algebra and its Applications},
  pages		= {7--63},
  publisher	= {Elsevier},
  title		= {Behavior of slightly perturbed Lanczos and
		  conjugate-gradient recurrences},
  volume	= {113},
  year		= {1989}
}

###Article{	  greenbaum1989behavior,
  title		= {Behavior of slightly perturbed Lanczos and
		  conjugate-gradient recurrences},
  author	= {Greenbaum, Anne},
  journal	= {Linear Algebra and its Applications},
  volume	= {113},
  pages		= {7--63},
  year		= {1989},
  publisher	= {Elsevier}
}

@Article{	  greenbaum1992predicting,
  title		= {Predicting the behavior of finite precision Lanczos and
		  conjugate gradient computations},
  author	= {Greenbaum, Anne and Strakos, Zdenek},
  journal	= {SIAM Journal on Matrix Analysis and Applications},
  volume	= {13},
  number	= {1},
  pages		= {121--137},
  year		= {1992},
  publisher	= {SIAM}
}

###Article{	  greenbaum1992predicting,
  author	= {Greenbaum, Anne and Strakos, Zdenek},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {SIAM Journal on Matrix Analysis and Applications},
  number	= {1},
  pages		= {121--137},
  publisher	= {SIAM},
  title		= {Predicting the behavior of finite precision Lanczos and
		  conjugate gradient computations},
  volume	= {13},
  year		= {1992}
}

###Article{	  greenbaum1992predicting,
  title		= {Predicting the behavior of finite precision Lanczos and
		  conjugate gradient computations},
  author	= {Greenbaum, Anne and Strakos, Zdenek},
  journal	= {SIAM Journal on Matrix Analysis and Applications},
  volume	= {13},
  number	= {1},
  pages		= {121--137},
  year		= {1992},
  publisher	= {SIAM}
}

@Article{	  gulcehre2016mollifying,
  title		= {Mollifying Networks},
  author	= {Gulcehre, Caglar and Moczulski, Marcin and Visin,
		  Francesco and Bengio, Yoshua},
  journal	= {arXiv preprint arXiv:1608.04980},
  year		= {2016}
}

###Article{	  gulcehre2016mollifying,
  author	= {Gulcehre, Caglar and Moczulski, Marcin and Visin,
		  Francesco and Bengio, Yoshua},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1608.04980},
  title		= {Mollifying Networks},
  year		= {2016}
}

###Article{	  gulcehre2016mollifying,
  title		= {Mollifying Networks},
  author	= {Gulcehre, Caglar and Moczulski, Marcin and Visin,
		  Francesco and Bengio, Yoshua},
  journal	= {arXiv preprint arXiv:1608.04980},
  year		= {2016}
}

@Misc{		  guney_2015,
  title		= {GitHub repo},
  author	= {V U\u{g}ur G\"uney},
  howpublished	= {\url{github.com/vug/decision-time-universality}},
  year		= {2015}
}

###Misc{	  guney_2015,
  author	= {V U\u{g}ur G\"uney},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  howpublished	= {\url{github.com/vug/decision-time-universality}},
  title		= {GitHub repo},
  year		= {2015}
}

###Misc{	  guney_2015,
  title		= {GitHub repo},
  author	= {V U\u{g}ur G\"uney},
  howpublished	= {\url{github.com/vug/decision-time-universality}},
  year		= {2015}
}

@InProceedings{	  haeffele2017global,
  title		= {Global optimality in neural network training},
  author	= {Haeffele, Benjamin D and Vidal, {Ren{\'e}}},
  booktitle	= {Proceedings of the IEEE Conference on Computer Vision and
		  Pattern Recognition},
  pages		= {7331--7339},
  year		= {2017}
}

###InProceedings{ haeffele2017global,
  title		= {Global optimality in neural network training},
  author	= {Haeffele, Benjamin D and Vidal, {Ren{\'e}}},
  booktitle	= {Proceedings of the IEEE Conference on Computer Vision and
		  Pattern Recognition},
  pages		= {7331--7339},
  year		= {2017}
}

@InProceedings{	  han15a,
  title		= {Learning both Weights and Connections for Efficient Neural
		  Network},
  author	= {Han, Song and Pool, Jeff and Tran, John and Dally,
		  William},
  booktitle	= {Advances in Neural Information Processing Systems (NIPS)},
  pages		= {1135--1143},
  year		= {2015}
}

###InProceedings{ han15a,
  author	= {Han, Song and Pool, Jeff and Tran, John and Dally,
		  William},
  booktitle	= {Advances in Neural Information Processing Systems (NIPS)},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1135--1143},
  title		= {Learning both Weights and Connections for Efficient Neural
		  Network},
  year		= {2015}
}

###InProceedings{ han15a,
  title		= {Learning both Weights and Connections for Efficient Neural
		  Network},
  author	= {Han, Song and Pool, Jeff and Tran, John and Dally,
		  William},
  booktitle	= {Advances in Neural Information Processing Systems (NIPS)},
  pages		= {1135--1143},
  year		= {2015}
}

@Article{	  han15b,
  title		= {Deep Compression: Compressing Deep Neural Networks with
		  Pruning, Trained Quantization and Huffman Coding},
  author	= {Han, Song and Mao, Huizi and Dally, William J},
  journal	= {International Conference on Learning Representations
		  (ICLR)},
  year		= {2016}
}

###Article{	  han15b,
  author	= {Han, Song and Mao, Huizi and Dally, William J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations
		  (ICLR)},
  title		= {Deep Compression: Compressing Deep Neural Networks with
		  Pruning, Trained Quantization and Huffman Coding},
  year		= {2016}
}

###Article{	  han15b,
  title		= {Deep Compression: Compressing Deep Neural Networks with
		  Pruning, Trained Quantization and Huffman Coding},
  author	= {Han, Song and Mao, Huizi and Dally, William J},
  journal	= {International Conference on Learning Representations
		  (ICLR)},
  year		= {2016}
}

@Article{	  hardt2015train,
  title		= {Train faster, generalize better: Stability of stochastic
		  gradient descent},
  author	= {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal	= {arXiv preprint arXiv:1509.01240},
  year		= {2015}
}

###Article{	  hardt2015train,
  author	= {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1509.01240},
  title		= {Train faster, generalize better: Stability of stochastic
		  gradient descent},
  year		= {2015}
}

###Article{	  hardt2015train,
  title		= {Train faster, generalize better: Stability of stochastic
		  gradient descent},
  author	= {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal	= {arXiv preprint arXiv:1509.01240},
  year		= {2015}
}

###Article{	  hardt2015train,
  title		= {Train faster, generalize better: Stability of stochastic
		  gradient descent},
  author	= {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  journal	= {arXiv preprint arXiv:1509.01240},
  year		= {2015}
}

@Article{	  hassibi1993second,
  title		= {Second order derivatives for network pruning: Optimal
		  brain surgeon},
  author	= {Hassibi, Babak and Stork, David G and others},
  journal	= {Advances in neural information processing systems},
  pages		= {164--164},
  year		= {1993},
  publisher	= {Morgan Kaufmann Publishers}
}

###Article{	  hassibi1993second,
  author	= {Hassibi, Babak and Stork, David G and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Advances in neural information processing systems},
  pages		= {164--164},
  publisher	= {Morgan Kaufmann Publishers},
  title		= {Second order derivatives for network pruning: Optimal
		  brain surgeon},
  year		= {1993}
}

###Article{	  hassibi1993second,
  title		= {Second order derivatives for network pruning: Optimal
		  brain surgeon},
  author	= {Hassibi, Babak and Stork, David G and others},
  journal	= {Advances in neural information processing systems},
  pages		= {164--164},
  year		= {1993},
  publisher	= {Morgan Kaufmann Publishers}
}

@InProceedings{	  hazan2016graduated,
  title		= {On graduated optimization for stochastic non-convex
		  problems},
  author	= {Hazan, Elad and Levy, Kfir Yehuda and Shalev-Shwartz,
		  Shai},
  booktitle	= {International Conference on Machine Learning},
  pages		= {1833--1841},
  year		= {2016}
}

###InProceedings{ hazan2016graduated,
  author	= {Hazan, Elad and Levy, Kfir Yehuda and Shalev-Shwartz,
		  Shai},
  booktitle	= {International Conference on Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1833--1841},
  title		= {On graduated optimization for stochastic non-convex
		  problems},
  year		= {2016}
}

###InProceedings{ hazan2016graduated,
  title		= {On graduated optimization for stochastic non-convex
		  problems},
  author	= {Hazan, Elad and Levy, Kfir Yehuda and Shalev-Shwartz,
		  Shai},
  booktitle	= {International Conference on Machine Learning},
  pages		= {1833--1841},
  year		= {2016}
}

@InProceedings{	  he16,
  title		= {Deep residual learning for image recognition},
  author	= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
		  Jian},
  booktitle	= {Proceedings of the IEEE conference on computer vision and
		  pattern recognition},
  pages		= {770--778},
  year		= {2016}
}

###InProceedings{ he16,
  author	= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
		  Jian},
  booktitle	= {Proceedings of the IEEE conference on computer vision and
		  pattern recognition},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {770--778},
  title		= {Deep residual learning for image recognition},
  year		= {2016}
}

###InProceedings{ he16,
  title		= {Deep residual learning for image recognition},
  author	= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
		  Jian},
  booktitle	= {Proceedings of the IEEE conference on computer vision and
		  pattern recognition},
  pages		= {770--778},
  year		= {2016}
}

@Article{	  he2009efficient,
  title		= {Efficient method for {T}ucker3 model selection},
  author	= {He, Z. and Cichocki, A. and Xie, S.},
  journal	= {Electronics letters},
  volume	= {45},
  number	= {15},
  pages		= {805--806},
  year		= {2009},
  publisher	= {IET}
}

###Article{	  he2009efficient,
  author	= {He, Z. and Cichocki, A. and Xie, S.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Electronics letters},
  number	= {15},
  pages		= {805--806},
  publisher	= {IET},
  title		= {Efficient method for {T}ucker3 model selection},
  volume	= {45},
  year		= {2009}
}

@InProceedings{	  he2016deep,
  title		= {Deep residual learning for image recognition},
  author	= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
		  Jian},
  booktitle	= {Proceedings of the IEEE conference on computer vision and
		  pattern recognition},
  pages		= {770--778},
  year		= {2016}
}

###InProceedings{ he2016deep,
  author	= {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun,
		  Jian},
  booktitle	= {Proceedings of the IEEE conference on computer vision and
		  pattern recognition},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {770--778},
  title		= {Deep residual learning for image recognition},
  year		= {2016}
}

@Article{	  hestenes1952methods,
  title		= {Methods of conjugate gradients for solving linear
		  systems},
  author	= {Hestenes, Magnus Rudolph and Stiefel, Eduard},
  year		= {1952},
  publisher	= {NBS}
}

###Article{	  hestenes1952methods,
  author	= {Hestenes, Magnus Rudolph and Stiefel, Eduard},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {NBS},
  title		= {Methods of conjugate gradients for solving linear
		  systems},
  year		= {1952}
}

###Article{	  hestenes1952methods,
  title		= {Methods of conjugate gradients for solving linear
		  systems},
  author	= {Hestenes, Magnus Rudolph and Stiefel, Eduard},
  year		= {1952},
  publisher	= {NBS}
}

@Article{	  hinton,
  author	= {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/geoff\_hinton\_dark14.pdf:pdf},
  title		= {{Dark Knowledge}}
}

###Article{	  hinton,
  author	= {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/geoff\_hinton\_dark14.pdf:pdf},
  title		= {{Dark Knowledge}}
}

@Article{	  hinton-nature,
  author	= {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date		= {2015/05/27/online},
  date-added	= {2019-01-16 19:13:35 +0000},
  date-modified	= {2019-01-16 19:13:35 +0000},
  day		= {27},
  journal	= {Nature},
  l3		= {10.1038/nature14539; },
  month		= {05},
  pages		= {436 EP -},
  publisher	= {Nature Publishing Group, a division of Macmillan
		  Publishers Limited. All Rights Reserved. SN -},
  title		= {Deep learning},
  ty		= {JOUR},
  url		= {https://doi.org/10.1038/nature14539},
  volume	= {521},
  year		= {2015},
  bdsk-url-1	= {https://doi.org/10.1038/nature14539}
}

@Article{	  hinton12,
  title		= {Deep neural networks for acoustic modeling in speech
		  recognition: The shared views of four research groups},
  author	= {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl,
		  George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and
		  Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick
		  and Sainath, Tara N and others},
  journal	= {IEEE Signal processing magazine},
  volume	= {29},
  number	= {6},
  pages		= {82--97},
  year		= {2012},
  publisher	= {IEEE}
}

###Article{	  hinton12,
  author	= {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl,
		  George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and
		  Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick
		  and Sainath, Tara N and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {IEEE Signal processing magazine},
  number	= {6},
  pages		= {82--97},
  publisher	= {IEEE},
  title		= {Deep neural networks for acoustic modeling in speech
		  recognition: The shared views of four research groups},
  volume	= {29},
  year		= {2012}
}

###Article{	  hinton12,
  title		= {Deep neural networks for acoustic modeling in speech
		  recognition: The shared views of four research groups},
  author	= {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl,
		  George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and
		  Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick
		  and Sainath, Tara N and others},
  journal	= {IEEE Signal processing magazine},
  volume	= {29},
  number	= {6},
  pages		= {82--97},
  year		= {2012},
  publisher	= {IEEE}
}

@InProceedings{	  hinton2014,
  author	= {Hinton, Geoffrey and Dean, Jeff},
  booktitle	= {Deep Learning and Representation Learning Workshop: NIPS
		  2014},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/65.pdf:pdf},
  pages		= {1--9},
  title		= {{Distilling the Knowledge in a Neural Network}},
  year		= {2014}
}

###InProceedings{ hinton2014,
  author	= {Hinton, Geoffrey and Dean, Jeff},
  booktitle	= {Deep Learning and Representation Learning Workshop: NIPS
		  2014},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/65.pdf:pdf},
  pages		= {1--9},
  title		= {{Distilling the Knowledge in a Neural Network}},
  year		= {2014}
}

@Article{	  hochreiter1997flat,
  title		= {Flat minima},
  author	= {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal	= {Neural Computation},
  volume	= {9},
  number	= {1},
  pages		= {1--42},
  year		= {1997},
  publisher	= {MIT Press}
}

###Article{	  hochreiter1997flat,
  author	= {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Neural Computation},
  number	= {1},
  pages		= {1--42},
  publisher	= {MIT Press},
  title		= {Flat minima},
  volume	= {9},
  year		= {1997}
}

###Article{	  hochreiter1997flat,
  title		= {Flat minima},
  author	= {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal	= {Neural Computation},
  volume	= {9},
  number	= {1},
  pages		= {1--42},
  year		= {1997},
  publisher	= {MIT Press}
}

###Article{	  hochreiter1997flat,
  title		= {Flat minima},
  author	= {Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal	= {Neural Computation},
  volume	= {9},
  number	= {1},
  pages		= {1--42},
  year		= {1997},
  publisher	= {MIT Press}
}

@Article{	  hochreiter97,
  title		= {Flat minima},
  author	= {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal	= {Neural Computation},
  volume	= {9},
  number	= {1},
  pages		= {1--42},
  year		= {1997},
  publisher	= {MIT Press}
}

###Article{	  hochreiter97,
  author	= {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Neural Computation},
  number	= {1},
  pages		= {1--42},
  publisher	= {MIT Press},
  title		= {Flat minima},
  volume	= {9},
  year		= {1997}
}

###Article{	  hochreiter97,
  title		= {Flat minima},
  author	= {Hochreiter, Sepp and Schmidhuber, {J{\"u}rgen}},
  journal	= {Neural Computation},
  volume	= {9},
  number	= {1},
  pages		= {1--42},
  year		= {1997},
  publisher	= {MIT Press}
}

@InProceedings{	  hoffer17,
  title		= {Train longer, generalize better: closing the
		  generalization gap in large batch training of neural
		  networks},
  author	= {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {1729--1739},
  year		= {2017}
}

###InProceedings{ hoffer17,
  author	= {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1729--1739},
  title		= {Train longer, generalize better: closing the
		  generalization gap in large batch training of neural
		  networks},
  year		= {2017}
}

###InProceedings{ hoffer17,
  title		= {Train longer, generalize better: closing the
		  generalization gap in large batch training of neural
		  networks},
  author	= {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {1729--1739},
  year		= {2017}
}

@InProceedings{	  hoffer2017train,
  title		= {Train longer, generalize better: closing the
		  generalization gap in large batch training of neural
		  networks},
  author	= {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {1729--1739},
  year		= {2017}
}

###InProceedings{ hoffer2017train,
  author	= {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1729--1739},
  title		= {Train longer, generalize better: closing the
		  generalization gap in large batch training of neural
		  networks},
  year		= {2017}
}

@Article{	  horn1962eigenvalues,
  title		= {Eigenvalues of sums of Hermitian matrices},
  author	= {Horn, Alfred},
  journal	= {Pacific Journal of Mathematics},
  volume	= {12},
  number	= {1},
  pages		= {225--241},
  year		= {1962},
  publisher	= {Mathematical Sciences Publishers}
}

###Article{	  horn1962eigenvalues,
  author	= {Horn, Alfred},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Pacific Journal of Mathematics},
  number	= {1},
  pages		= {225--241},
  publisher	= {Mathematical Sciences Publishers},
  title		= {Eigenvalues of sums of Hermitian matrices},
  volume	= {12},
  year		= {1962}
}

###Article{	  horn1962eigenvalues,
  title		= {Eigenvalues of sums of Hermitian matrices},
  author	= {Horn, Alfred},
  journal	= {Pacific Journal of Mathematics},
  volume	= {12},
  number	= {1},
  pages		= {225--241},
  year		= {1962},
  publisher	= {Mathematical Sciences Publishers}
}

@Article{	  hu2007subjective,
  author	= {Hu, Y. and Loizou, P. C.},
  title		= {Subjective comparison and evaluation of speech enhancement
		  algorithms},
  journal	= {Speech communication},
  year		= {2007},
  volume	= {49},
  pages		= {588--601},
  number	= {7}
}

###Article{	  hu2007subjective,
  author	= {Hu, Y. and Loizou, P. C.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Speech communication},
  number	= {7},
  pages		= {588--601},
  title		= {Subjective comparison and evaluation of speech enhancement
		  algorithms},
  volume	= {49},
  year		= {2007}
}

@Article{	  huang2013,
  abstract	= {The history of renormalization is reviewed with a critical
		  eye, starting with Lorentz's theory of radiation damping,
		  through perturbative QED with Dyson, Gell-Mann \& Low, and
		  others, to Wilson's formulation and Polchinski's functional
		  equation, and applications to "triviality", and dark energy
		  in cosmology.},
  archiveprefix	= {arXiv},
  arxivid	= {1310.5533},
  author	= {Huang, Kerson},
  doi		= {10.1142/S021775X13300500},
  eprint	= {1310.5533},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1310.5533.pdf:pdf},
  journal	= {International Journal of Modern Physics A},
  month		= oct,
  number	= {29},
  pages		= {24},
  title		= {{A Critical History of Renormalization}},
  url		= {http://www.worldscientific.com/doi/pdf/10.1142/S0217751X13300500},
  volume	= {28},
  year		= {2013}
}

###Article{	  huang2013,
  abstract	= {The history of renormalization is reviewed with a critical
		  eye, starting with Lorentz's theory of radiation damping,
		  through perturbative QED with Dyson, Gell-Mann \& Low, and
		  others, to Wilson's formulation and Polchinski's functional
		  equation, and applications to "triviality", and dark energy
		  in cosmology.},
  archiveprefix	= {arXiv},
  arxivid	= {1310.5533},
  author	= {Huang, Kerson},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1142/S021775X13300500},
  eprint	= {1310.5533},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1310.5533.pdf:pdf},
  journal	= {International Journal of Modern Physics A},
  month		= oct,
  number	= {29},
  pages		= {24},
  title		= {{A Critical History of Renormalization}},
  url		= {http://www.worldscientific.com/doi/pdf/10.1142/S0217751X13300500},
  volume	= {28},
  year		= {2013},
  bdsk-url-1	= {http://www.worldscientific.com/doi/pdf/10.1142/S0217751X13300500},
  bdsk-url-2	= {https://doi.org/10.1142/S021775X13300500}
}

@InProceedings{	  icml2014c2_satoa14,
  author	= {I. Sato and H. Nakagawa},
  title		= {Approximation Analysis of Stochastic Gradient {L}angevin
		  Dynamics by using {F}okker-{P}lanck Equation and {I}to
		  Process },
  booktitle	= {Proceedings of the 31st International Conference on
		  Machine Learning},
  year		= {2014},
  pages		= {982-990},
  month		= jun,
  publisher	= {JMLR Workshop and Conference Proceedings}
}

###InProceedings{ icml2014c2_satoa14,
  author	= {I. Sato and H. Nakagawa},
  booktitle	= {Proceedings of the 31st International Conference on
		  Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= jun,
  pages		= {982-990},
  publisher	= {JMLR Workshop and Conference Proceedings},
  title		= {Approximation Analysis of Stochastic Gradient {L}angevin
		  Dynamics by using {F}okker-{P}lanck Equation and {I}to
		  Process},
  year		= {2014}
}

@Book{		  ikeda2014stochastic,
  title		= {Stochastic differential equations and diffusion
		  processes},
  author	= {Ikeda, Nobuyuki and Watanabe, Shinzo},
  volume	= {24},
  year		= {2014},
  publisher	= {Elsevier}
}

###Book{	  ikeda2014stochastic,
  author	= {Ikeda, Nobuyuki and Watanabe, Shinzo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {Elsevier},
  title		= {Stochastic differential equations and diffusion
		  processes},
  volume	= {24},
  year		= {2014}
}

@InProceedings{	  ioffe15,
  title		= {Batch normalization: Accelerating deep network training by
		  reducing internal covariate shift},
  author	= {Ioffe, Sergey and Szegedy, Christian},
  booktitle	= {International conference on machine learning},
  pages		= {448--456},
  year		= {2015}
}

###InProceedings{ ioffe15,
  author	= {Ioffe, Sergey and Szegedy, Christian},
  booktitle	= {International conference on machine learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {448--456},
  title		= {Batch normalization: Accelerating deep network training by
		  reducing internal covariate shift},
  year		= {2015}
}

###InProceedings{ ioffe15,
  title		= {Batch normalization: Accelerating deep network training by
		  reducing internal covariate shift},
  author	= {Ioffe, Sergey and Szegedy, Christian},
  booktitle	= {International conference on machine learning},
  pages		= {448--456},
  year		= {2015}
}

@InProceedings{	  ioffe2015batch,
  title		= {Batch normalization: Accelerating deep network training by
		  reducing internal covariate shift},
  author	= {Ioffe, Sergey and Szegedy, Christian},
  booktitle	= {International conference on machine learning},
  pages		= {448--456},
  year		= {2015}
}

###InProceedings{ ioffe2015batch,
  author	= {Ioffe, Sergey and Szegedy, Christian},
  booktitle	= {International conference on machine learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {448--456},
  title		= {Batch normalization: Accelerating deep network training by
		  reducing internal covariate shift},
  year		= {2015}
}

@Article{	  jacot2018neural,
  title		= {Neural Tangent Kernel: Convergence and Generalization in
		  Neural Networks},
  author	= {Jacot, Arthur and Gabriel, Franck and Hongler,
		  {Cl{\'e}ment}},
  journal	= {arXiv preprint arXiv:1806.07572},
  year		= {2018}
}

###Article{	  jacot2018neural,
  title		= {Neural Tangent Kernel: Convergence and Generalization in
		  Neural Networks},
  author	= {Jacot, Arthur and Gabriel, Franck and Hongler,
		  {Cl{\'e}ment}},
  journal	= {arXiv preprint arXiv:1806.07572},
  year		= {2018}
}

@Article{	  jacot2019hessian,
  title		= {The Neural Tangent Kernel describes the Hessian of
		  Overparametrized DNNs},
  author	= {Jacot-Guillarmod, Arthur and Gabriel, Franck and Hongler,
		  Clement},
  year		= {2019}
}

@Article{	  jacot2019hessian2,
  title		= {The Neural Tangent Kernel describes the Hessian of
		  Overparametrized DNNs},
  author	= {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
  year		= {2019}
}

@Article{	  jastrzkebski2017three,
  title		= {Three Factors Influencing Minima in SGD},
  author	= {Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit,
		  Devansh and Ballas, Nicolas and Fischer, Asja and Bengio,
		  Yoshua and Storkey, Amos},
  journal	= {arXiv preprint arXiv:1711.04623},
  year		= {2017}
}

###Article{	  jastrzkebski2017three,
  author	= {Jastrzebski, Stanis{\l}aw and Kenton, Zachary and Arpit,
		  Devansh and Ballas, Nicolas and Fischer, Asja and Bengio,
		  Yoshua and Storkey, Amos},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1711.04623},
  title		= {Three Factors Influencing Minima in SGD},
  year		= {2017}
}

@Article{	  jastrzkebski2018dnn,
  title		= {DNN's Sharpest Directions Along the SGD Trajectory},
  author	= {Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and
		  Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and
		  Storkey, Amos},
  journal	= {arXiv preprint arXiv:1807.05031},
  year		= {2018}
}

@InProceedings{	  jaureguiberry2014multiple,
  author	= {Jaureguiberry, X. and Vincent, E. and Richard, G.},
  title		= {Multiple-order non-negative matrix factorization for
		  speech enhancement},
  booktitle	= {Interspeech},
  year		= {2014},
  pages		= {4},
  month		= may
}

###InProceedings{ jaureguiberry2014multiple,
  author	= {Jaureguiberry, X. and Vincent, E. and Richard, G.},
  booktitle	= {Interspeech},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= may,
  pages		= {4},
  title		= {Multiple-order non-negative matrix factorization for
		  speech enhancement},
  year		= {2014}
}

@InProceedings{	  kawaguchi2016deep,
  title		= {Deep learning without poor local minima},
  author	= {Kawaguchi, Kenji},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {586--594},
  year		= {2016}
}

###InProceedings{ kawaguchi2016deep,
  author	= {Kawaguchi, Kenji},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {586--594},
  title		= {Deep learning without poor local minima},
  year		= {2016}
}

###Article{	  kawaguchi2016deep,
  title		= {Deep Learning without Poor Local Minima},
  author	= {Kawaguchi, Kenji},
  journal	= {arXiv preprint arXiv:1605.07110},
  year		= {2016}
}

###InProceedings{ kawaguchi2016deep,
  title		= {Deep learning without poor local minima},
  author	= {Kawaguchi, Kenji},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {586--594},
  year		= {2016}
}

@Article{	  kelly2014,
  abstract	= {This study tested the possible relationship between
		  reported visual awareness ("I see a visual stimulus in
		  front of me") and the social attribution of awareness to
		  someone else ("That person is aware of an object next to
		  him"). Subjects were tested in two steps. First, in an fMRI
		  experiment, subjects were asked to attribute states of
		  awareness to a cartoon face. Activity associated with this
		  task was found bilaterally within the temporoparietal
		  junction (TPJ) among other areas. Second, the TPJ was
		  transiently disrupted using single-pulse transcranial
		  magnetic stimulation (TMS). When the TMS was targeted to
		  the same cortical sites that had become active during the
		  social attribution task, the subjects showed symptoms of
		  visual neglect in that their detection of visual stimuli
		  was significantly affected. In control trials, when TMS was
		  targeted to nearby cortical sites that had not become
		  active during the social attribution task, no significant
		  effect on visual detection was found. These results suggest
		  that there may be at least some partial overlap in brain
		  mechanisms that participate in the social attribution of
		  sensory awareness to other people and in attributing
		  sensory awareness to oneself.},
  author	= {Kelly, Yin T and Webb, Taylor W and Meier, Jeffrey D and
		  Arcaro, Michael J and Graziano, Michael S a},
  doi		= {10.1073/pnas.1401201111},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Kelly\_Graziano\_2014.pdf:pdf},
  issn		= {1091-6490},
  journal	= {Proceedings of the National Academy of Sciences of the
		  United States of America},
  keywords	= {Adolescent,Adult,Awareness,Awareness: physiology,Brain
		  Mapping,Female,Humans,Magnetic Resonance
		  Imaging,Male,Middle Aged,Social Behavior,Task Performance
		  and Analysis,Time Factors,Transcranial Magnetic
		  Stimulation,Visual Perception,Visual Perception:
		  physiology,Young Adult},
  month		= apr,
  number	= {13},
  pages		= {5012--7},
  pmid		= {24639542},
  title		= {{Attributing awareness to oneself and to others.}},
  url		= {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3977229\&tool=pmcentrez\&rendertype=abstract},
  volume	= {111},
  year		= {2014}
}

###Article{	  kelly2014,
  abstract	= {This study tested the possible relationship between
		  reported visual awareness ("I see a visual stimulus in
		  front of me") and the social attribution of awareness to
		  someone else ("That person is aware of an object next to
		  him"). Subjects were tested in two steps. First, in an fMRI
		  experiment, subjects were asked to attribute states of
		  awareness to a cartoon face. Activity associated with this
		  task was found bilaterally within the temporoparietal
		  junction (TPJ) among other areas. Second, the TPJ was
		  transiently disrupted using single-pulse transcranial
		  magnetic stimulation (TMS). When the TMS was targeted to
		  the same cortical sites that had become active during the
		  social attribution task, the subjects showed symptoms of
		  visual neglect in that their detection of visual stimuli
		  was significantly affected. In control trials, when TMS was
		  targeted to nearby cortical sites that had not become
		  active during the social attribution task, no significant
		  effect on visual detection was found. These results suggest
		  that there may be at least some partial overlap in brain
		  mechanisms that participate in the social attribution of
		  sensory awareness to other people and in attributing
		  sensory awareness to oneself.},
  author	= {Kelly, Yin T and Webb, Taylor W and Meier, Jeffrey D and
		  Arcaro, Michael J and Graziano, Michael S a},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1073/pnas.1401201111},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Kelly\_Graziano\_2014.pdf:pdf},
  issn		= {1091-6490},
  journal	= {Proceedings of the National Academy of Sciences of the
		  United States of America},
  keywords	= {Adolescent,Adult,Awareness,Awareness: physiology,Brain
		  Mapping,Female,Humans,Magnetic Resonance
		  Imaging,Male,Middle Aged,Social Behavior,Task Performance
		  and Analysis,Time Factors,Transcranial Magnetic
		  Stimulation,Visual Perception,Visual Perception:
		  physiology,Young Adult},
  month		= apr,
  number	= {13},
  pages		= {5012--7},
  pmid		= {24639542},
  title		= {{Attributing awareness to oneself and to others.}},
  url		= {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3977229\&tool=pmcentrez\&rendertype=abstract},
  volume	= {111},
  year		= {2014},
  bdsk-url-1	= {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3977229%5C&tool=pmcentrez%5C&rendertype=abstract},
  bdsk-url-2	= {https://doi.org/10.1073/pnas.1401201111}
}

###Article{	  keskar2016large,
  author	= {Keskar, Nitish Shirish and Mudigere, Dheevatsa and
		  Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak
		  Peter},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1609.04836},
  title		= {On Large-Batch Training for Deep Learning: Generalization
		  Gap and Sharp Minima},
  year		= {2016}
}

@Article{	  khot,
  author	= {Khot, Subhash and Naor, Assaf},
  doi		= {10.1002/cpa.21398},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Grothendieck-CPAM.pdf:pdf},
  issn		= {00103640},
  journal	= {Communications on Pure and Applied Mathematics},
  month		= jul,
  number	= {7},
  pages		= {992--1035},
  title		= {{Grothendieck-Type Inequalities in Combinatorial
		  Optimization}},
  url		= {http://doi.wiley.com/10.1002/cpa.21398},
  volume	= {65},
  year		= {2012}
}

###Article{	  khot,
  author	= {Khot, Subhash and Naor, Assaf},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1002/cpa.21398},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Grothendieck-CPAM.pdf:pdf},
  issn		= {00103640},
  journal	= {Communications on Pure and Applied Mathematics},
  month		= jul,
  number	= {7},
  pages		= {992--1035},
  title		= {{Grothendieck-Type Inequalities in Combinatorial
		  Optimization}},
  url		= {http://doi.wiley.com/10.1002/cpa.21398},
  volume	= {65},
  year		= {2012},
  bdsk-url-1	= {http://doi.wiley.com/10.1002/cpa.21398},
  bdsk-url-2	= {https://doi.org/10.1002/cpa.21398}
}

@Article{	  kiers2003fast,
  title		= {A fast method for choosing the numbers of components in
		  {T}ucker3 analysis},
  author	= {Kiers, H. A. and Kinderen, A.},
  journal	= {British Journal of Mathematical and Statistical
		  Psychology},
  volume	= {56},
  number	= {1},
  pages		= {119--125},
  year		= {2003},
  publisher	= {Wiley Online Library}
}

###Article{	  kiers2003fast,
  author	= {Kiers, H. A. and Kinderen, A.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {British Journal of Mathematical and Statistical
		  Psychology},
  number	= {1},
  pages		= {119--125},
  publisher	= {Wiley Online Library},
  title		= {A fast method for choosing the numbers of components in
		  {T}ucker3 analysis},
  volume	= {56},
  year		= {2003}
}

@Article{	  kingma14,
  title		= {Adam: A method for stochastic optimization},
  author	= {Kingma, Diederik P and Ba, Jimmy},
  journal	= {International Conference on Learning Representations},
  year		= {2015}
}

###Article{	  kingma14,
  author	= {Kingma, Diederik P and Ba, Jimmy},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Adam: A method for stochastic optimization},
  year		= {2015}
}

###Article{	  kingma14,
  title		= {Adam: A method for stochastic optimization},
  author	= {Kingma, Diederik P and Ba, Jimmy},
  journal	= {International Conference on Learning Representations},
  year		= {2015}
}

@Article{	  knutson2001honeycombs,
  title		= {Honeycombs and sums of Hermitian matrices},
  author	= {Knutson, Allen and Tao, Terence},
  journal	= {Notices Amer. Math. Soc},
  volume	= {48},
  number	= {2},
  year		= {2001}
}

###Article{	  knutson2001honeycombs,
  author	= {Knutson, Allen and Tao, Terence},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Notices Amer. Math. Soc},
  number	= {2},
  title		= {Honeycombs and sums of Hermitian matrices},
  volume	= {48},
  year		= {2001}
}

###Article{	  knutson2001honeycombs,
  title		= {Honeycombs and sums of Hermitian matrices},
  author	= {Knutson, Allen and Tao, Terence},
  journal	= {Notices Amer. Math. Soc},
  volume	= {48},
  number	= {2},
  year		= {2001}
}

@Article{	  kostlan1988complexity,
  title		= {Complexity theory of numerical linear algebra},
  author	= {Kostlan, Eric},
  journal	= {Journal of Computational and Applied Mathematics},
  volume	= {22},
  number	= {2},
  pages		= {219--230},
  year		= {1988},
  publisher	= {Elsevier}
}

###Article{	  kostlan1988complexity,
  author	= {Kostlan, Eric},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of Computational and Applied Mathematics},
  number	= {2},
  pages		= {219--230},
  publisher	= {Elsevier},
  title		= {Complexity theory of numerical linear algebra},
  volume	= {22},
  year		= {1988}
}

###Article{	  kostlan1988complexity,
  title		= {Complexity theory of numerical linear algebra},
  author	= {Kostlan, Eric},
  journal	= {Journal of Computational and Applied Mathematics},
  volume	= {22},
  number	= {2},
  pages		= {219--230},
  year		= {1988},
  publisher	= {Elsevier}
}

@Article{	  krishnan2017neumann,
  title		= {Neumann Optimizer: A Practical Optimization Algorithm for
		  Deep Neural Networks},
  author	= {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  journal	= {arXiv preprint arXiv:1712.03298},
  year		= {2017}
}

###Article{	  krishnan2017neumann,
  author	= {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1712.03298},
  title		= {Neumann Optimizer: A Practical Optimization Algorithm for
		  Deep Neural Networks},
  year		= {2017}
}

###Article{	  krishnan2017neumann,
  title		= {Neumann Optimizer: A Practical Optimization Algorithm for
		  Deep Neural Networks},
  author	= {Krishnan, Shankar and Xiao, Ying and Saurous, Rif A},
  journal	= {arXiv preprint arXiv:1712.03298},
  year		= {2017}
}

@InProceedings{	  krizhevsky12,
  title		= {Imagenet classification with deep convolutional neural
		  networks},
  author	= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in neural information processing systems},
  pages		= {1097--1105},
  year		= {2012}
}

###InProceedings{ krizhevsky12,
  author	= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in neural information processing systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1097--1105},
  title		= {Imagenet classification with deep convolutional neural
		  networks},
  year		= {2012}
}

###InProceedings{ krizhevsky12,
  title		= {Imagenet classification with deep convolutional neural
		  networks},
  author	= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in neural information processing systems},
  pages		= {1097--1105},
  year		= {2012}
}

@InProceedings{	  krizhevsky2012imagenet,
  title		= {Imagenet classification with deep convolutional neural
		  networks},
  author	= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in neural information processing systems},
  pages		= {1097--1105},
  year		= {2012}
}

###InProceedings{ krizhevsky2012imagenet,
  author	= {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey
		  E},
  booktitle	= {Advances in neural information processing systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1097--1105},
  title		= {Imagenet classification with deep convolutional neural
		  networks},
  year		= {2012}
}

@InProceedings{	  krogh1992simple,
  title		= {A simple weight decay can improve generalization},
  author	= {Krogh, Anders and Hertz, John A},
  booktitle	= {Advances in neural information processing systems},
  pages		= {950--957},
  year		= {1992}
}

###InProceedings{ krogh1992simple,
  title		= {A simple weight decay can improve generalization},
  author	= {Krogh, Anders and Hertz, John A},
  booktitle	= {Advances in neural information processing systems},
  pages		= {950--957},
  year		= {1992}
}

@Article{	  krzakala07,
  title		= {Landscape analysis of constraint satisfaction problems},
  author	= {Krzakala, Florent and Kurchan, Jorge},
  journal	= {Physical Review E},
  volume	= {76},
  number	= {2},
  pages		= {021122},
  year		= {2007},
  publisher	= {APS}
}

###Article{	  krzakala07,
  author	= {Krzakala, Florent and Kurchan, Jorge},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review E},
  number	= {2},
  pages		= {021122},
  publisher	= {APS},
  title		= {Landscape analysis of constraint satisfaction problems},
  volume	= {76},
  year		= {2007}
}

###Article{	  krzakala07,
  title		= {Landscape analysis of constraint satisfaction problems},
  author	= {Krzakala, Florent and Kurchan, Jorge},
  journal	= {Physical Review E},
  volume	= {76},
  number	= {2},
  pages		= {021122},
  year		= {2007},
  publisher	= {APS}
}

@Article{	  kurchanlaloux,
  title		= {Phase space geometry and slow dynamics},
  author	= {Kurchan, Jorge and Laloux, Laurent},
  journal	= {Journal of Physics A: Mathematical and General},
  volume	= {29},
  number	= {9},
  pages		= {1929},
  year		= {1996},
  publisher	= {IOP Publishing}
}

###Article{	  kurchanlaloux,
  author	= {Kurchan, Jorge and Laloux, Laurent},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Physics A: Mathematical and General},
  number	= {9},
  pages		= {1929},
  publisher	= {IOP Publishing},
  title		= {Phase space geometry and slow dynamics},
  volume	= {29},
  year		= {1996}
}

###Article{	  kurchanlaloux,
  title		= {Phase space geometry and slow dynamics},
  author	= {Kurchan, Jorge and Laloux, Laurent},
  journal	= {Journal of Physics A: Mathematical and General},
  volume	= {29},
  number	= {9},
  pages		= {1929},
  year		= {1996},
  publisher	= {IOP Publishing}
}

@Book{		  kushner,
  title		= {{Stochastic Approximation and Recursive Algorithms and
		  Applications}},
  publisher	= {Springer},
  year		= {2003},
  author	= {Kushner, H. and Yin, G.},
  address	= {New York}
}

###Book{	  kushner,
  address	= {New York},
  author	= {Kushner, H. and Yin, G.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {Springer},
  title		= {{Stochastic Approximation and Recursive Algorithms and
		  Applications}},
  year		= {2003}
}

@Article{	  laurent2017multilinear,
  title		= {The Multilinear Structure of ReLU Networks},
  author	= {Laurent, Thomas and von Brecht, James},
  journal	= {arXiv preprint arXiv:1712.10132},
  year		= {2017}
}

###Article{	  laurent2017multilinear,
  title		= {The Multilinear Structure of ReLU Networks},
  author	= {Laurent, Thomas and von Brecht, James},
  journal	= {arXiv preprint arXiv:1712.10132},
  year		= {2017}
}

@Article{	  le1991eigenvalues,
  title		= {Eigenvalues of covariance matrices: Application to
		  neural-network learning},
  author	= {Le Cun, Yann and Kanter, Ido and Solla, Sara A},
  journal	= {Physical Review Letters},
  volume	= {66},
  number	= {18},
  pages		= {2396},
  year		= {1991},
  publisher	= {APS}
}

###Article{	  le1991eigenvalues,
  title		= {Eigenvalues of covariance matrices: Application to
		  neural-network learning},
  author	= {Le Cun, Yann and Kanter, Ido and Solla, Sara A},
  journal	= {Physical Review Letters},
  volume	= {66},
  number	= {18},
  pages		= {2396},
  year		= {1991},
  publisher	= {APS}
}

@Article{	  lecun15,
  title		= {Deep learning},
  author	= {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal	= {Nature},
  volume	= {521},
  number	= {7553},
  pages		= {436},
  year		= {2015},
  publisher	= {Nature Publishing Group}
}

###Article{	  lecun15,
  author	= {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Nature},
  number	= {7553},
  pages		= {436},
  publisher	= {Nature Publishing Group},
  title		= {Deep learning},
  volume	= {521},
  year		= {2015}
}

@InProceedings{	  lecun1990optimal,
  title		= {Optimal brain damage},
  author	= {LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle	= {Advances in neural information processing systems},
  pages		= {598--605},
  year		= {1990}
}

###InProceedings{ lecun1990optimal,
  title		= {Optimal brain damage},
  author	= {LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle	= {Advances in neural information processing systems},
  pages		= {598--605},
  year		= {1990}
}

@Article{	  lecun1998efficient,
  title		= {Efficient backprop},
  author	= {LeCun, Yann and Bottou, L\'eon and Orr, GB and M{\"u}ller,
		  K-R},
  journal	= {Lecture notes in computer science},
  pages		= {9--50},
  year		= {1998},
  publisher	= {Springer}
}

###Article{	  lecun1998efficient,
  author	= {LeCun, Yann and Bottou, L\'eon and Orr, GB and M{\"u}ller,
		  K-R},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Lecture notes in computer science},
  pages		= {9--50},
  publisher	= {Springer},
  title		= {Efficient backprop},
  year		= {1998}
}

###Article{	  lecun1998efficient,
  title		= {Efficient backprop},
  author	= {LeCun, Yann and Bottou, {L\'eon} and Orr, GB and
		  {M{\"u}ller}, K-R},
  journal	= {Lecture notes in computer science},
  pages		= {9--50},
  year		= {1998},
  publisher	= {Springer}
}

@Article{	  lecun1998gradient,
  title		= {Gradient-based learning applied to document recognition},
  author	= {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and
		  Haffner, Patrick},
  journal	= {Proceedings of the IEEE},
  volume	= {86},
  number	= {11},
  pages		= {2278--2324},
  year		= {1998},
  publisher	= {IEEE}
}

###Article{	  lecun1998gradient,
  author	= {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and
		  Haffner, Patrick},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Proceedings of the IEEE},
  number	= {11},
  pages		= {2278--2324},
  publisher	= {IEEE},
  title		= {Gradient-based learning applied to document recognition},
  volume	= {86},
  year		= {1998}
}

###Article{	  lecun1998gradient,
  title		= {Gradient-based learning applied to document recognition},
  author	= {LeCun, Yann and Bottou, {L{\'e}on} and Bengio, Yoshua and
		  Haffner, Patrick},
  journal	= {Proceedings of the IEEE},
  volume	= {86},
  number	= {11},
  pages		= {2278--2324},
  year		= {1998},
  publisher	= {IEEE}
}

@InCollection{	  lecun2012efficient,
  title		= {Efficient backprop},
  author	= {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B
		  and M{\"u}ller, Klaus-Robert},
  booktitle	= {Neural networks: Tricks of the trade},
  pages		= {9--48},
  year		= {2012},
  publisher	= {Springer}
}

###InCollection{  lecun2012efficient,
  author	= {LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B
		  and M{\"u}ller, Klaus-Robert},
  booktitle	= {Neural networks: Tricks of the trade},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {9--48},
  publisher	= {Springer},
  title		= {Efficient backprop},
  year		= {2012}
}

@Article{	  lecun2015deep,
  title		= {Deep learning},
  author	= {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal	= {nature},
  volume	= {521},
  number	= {7553},
  pages		= {436},
  year		= {2015},
  publisher	= {Nature Publishing Group}
}

@InCollection{	  lecun90,
  title		= {Optimal Brain Damage},
  author	= {LeCun, Yann and John S. Denker and Sara A. Solla},
  booktitle	= {Advances in Neural Information Processing Systems 2},
  editor	= {D. S. Touretzky},
  pages		= {598--605},
  year		= {1990},
  publisher	= {Morgan-Kaufmann},
  url		= {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

###InCollection{  lecun90,
  author	= {LeCun, Yann and John S. Denker and Sara A. Solla},
  booktitle	= {Advances in Neural Information Processing Systems 2},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {D. S. Touretzky},
  pages		= {598--605},
  publisher	= {Morgan-Kaufmann},
  title		= {Optimal Brain Damage},
  url		= {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf},
  year		= {1990},
  bdsk-url-1	= {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

###InCollection{  lecun90,
  title		= {Optimal Brain Damage},
  author	= {LeCun, Yann and John S. Denker and Sara A. Solla},
  booktitle	= {Advances in Neural Information Processing Systems 2},
  editor	= {D. S. Touretzky},
  pages		= {598--605},
  year		= {1990},
  publisher	= {Morgan-Kaufmann},
  url		= {http://papers.nips.cc/paper/250-optimal-brain-damage.pdf}
}

@Article{	  lecun95,
  title		= {Convolutional networks for images, speech, and time
		  series},
  author	= {LeCun, Yann and Bengio, Yoshua and others},
  journal	= {The handbook of brain theory and neural networks},
  volume	= {3361},
  number	= {10},
  pages		= {1995},
  year		= {1995}
}

###Article{	  lecun95,
  author	= {LeCun, Yann and Bengio, Yoshua and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {The handbook of brain theory and neural networks},
  number	= {10},
  pages		= {1995},
  title		= {Convolutional networks for images, speech, and time
		  series},
  volume	= {3361},
  year		= {1995}
}

###Article{	  lecun95,
  title		= {Convolutional networks for images, speech, and time
		  series},
  author	= {LeCun, Yann and Bengio, Yoshua and others},
  journal	= {The handbook of brain theory and neural networks},
  volume	= {3361},
  number	= {10},
  pages		= {1995},
  year		= {1995}
}

@Article{	  lee2016gradient,
  title		= {Gradient descent converges to minimizers},
  author	= {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and
		  Recht, Benjamin},
  journal	= {University of California, Berkeley},
  volume	= {1050},
  pages		= {16},
  year		= {2016}
}

###Article{	  lee2016gradient,
  author	= {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and
		  Recht, Benjamin},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {University of California, Berkeley},
  pages		= {16},
  title		= {Gradient descent converges to minimizers},
  volume	= {1050},
  year		= {2016}
}

###Article{	  lee2016gradient,
  title		= {Gradient descent converges to minimizers},
  author	= {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and
		  Recht, Benjamin},
  journal	= {University of California, Berkeley},
  volume	= {1050},
  pages		= {16},
  year		= {2016}
}

###Article{	  lee2016gradient,
  title		= {Gradient descent converges to minimizers},
  author	= {Lee, Jason D and Simchowitz, Max and Jordan, Michael I and
		  Recht, Benjamin},
  journal	= {University of California, Berkeley},
  volume	= {1050},
  pages		= {16},
  year		= {2016}
}

@Article{	  lee2017,
  title		= {Deep Neural Networks as Gaussian Processes},
  author	= {Jae Hoon Lee and Yasaman Bahri and Roman Novak and Samuel
		  S. Schoenholz and Jeffrey Pennington and Jascha
		  Sohl-Dickstein},
  journal	= {ICLR},
  year		= {2018}
}

@InProceedings{	  lee2017ability,
  title		= {On the Ability of Neural Nets to Express Distributions},
  author	= {Holden Lee and Rong Ge and Tengyu Ma and Andrej Risteski
		  and Sanjeev Arora},
  booktitle	= {Proceedings of the 2017 Conference on Learning Theory},
  pages		= {1271--1296},
  year		= {2017},
  editor	= {Satyen Kale and Ohad Shamir},
  volume	= {65},
  series	= {Proceedings of Machine Learning Research},
  address	= {Amsterdam, Netherlands},
  month		= {07--10 Jul},
  publisher	= {PMLR},
  pdf		= {http://proceedings.mlr.press/v65/lee17a/lee17a.pdf},
  url		= {http://proceedings.mlr.press/v65/lee17a.html},
  abstract	= {Deep neural nets have caused a revolution in many
		  classification tasks. A related ongoing revolution—also
		  theoretically not understood—concerns their ability to
		  serve as generative models for complicated types of data
		  such as images and texts. These models are trained using
		  ideas like variational autoencoders and Generative
		  Adversarial Networks. We take a first cut at explaining the
		  expressivity of multilayer nets by giving a sufficient
		  criterion for a function to be approximable by a neural
		  network with $n$ hidden layers. A key ingredient is
		  Barron’s Theorem (Barron, 1993), which gives a Fourier
		  criterion for approximability of a function by a neural
		  network with 1 hidden layer. We show that a composition of
		  $n$ functions which satisfy certain Fourier conditions
		  (“Barron functions”) can be approximated by a
		  $n+1$-layer neural network. For probability distributions,
		  this translates into a criterion for a probability
		  distribution to be approximable in Wasserstein distance—a
		  natural metric on probability distributions—by a neural
		  network applied to a fixed base distribution (e.g.,
		  multivariate gaussian). Building up recent lower bound
		  work, we also give an example function that shows that
		  composition of Barron functions is more expressive than
		  Barron functions alone.}
}

###InProceedings{ lee2017ability,
  abstract	= {Deep neural nets have caused a revolution in many
		  classification tasks. A related ongoing revolution---also
		  theoretically not understood---concerns their ability to
		  serve as generative models for complicated types of data
		  such as images and texts. These models are trained using
		  ideas like variational autoencoders and Generative
		  Adversarial Networks. We take a first cut at explaining the
		  expressivity of multilayer nets by giving a sufficient
		  criterion for a function to be approximable by a neural
		  network with $n$ hidden layers. A key ingredient is
		  Barron's Theorem (Barron, 1993), which gives a Fourier
		  criterion for approximability of a function by a neural
		  network with 1 hidden layer. We show that a composition of
		  $n$ functions which satisfy certain Fourier conditions
		  (``Barron functions'') can be approximated by a $n+1$-layer
		  neural network. For probability distributions, this
		  translates into a criterion for a probability distribution
		  to be approximable in Wasserstein distance---a natural
		  metric on probability distributions---by a neural network
		  applied to a fixed base distribution (e.g., multivariate
		  gaussian). Building up recent lower bound work, we also
		  give an example function that shows that composition of
		  Barron functions is more expressive than Barron functions
		  alone.},
  address	= {Amsterdam, Netherlands},
  author	= {Holden Lee and Rong Ge and Tengyu Ma and Andrej Risteski
		  and Sanjeev Arora},
  booktitle	= {Proceedings of the 2017 Conference on Learning Theory},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  editor	= {Satyen Kale and Ohad Shamir},
  month		= {07--10 Jul},
  pages		= {1271--1296},
  pdf		= {http://proceedings.mlr.press/v65/lee17a/lee17a.pdf},
  publisher	= {PMLR},
  series	= {Proceedings of Machine Learning Research},
  title		= {On the Ability of Neural Nets to Express Distributions},
  url		= {http://proceedings.mlr.press/v65/lee17a.html},
  volume	= {65},
  year		= {2017},
  bdsk-url-1	= {http://proceedings.mlr.press/v65/lee17a.html}
}

###InProceedings{ lee2017ability,
  title		= {On the Ability of Neural Nets to Express Distributions},
  author	= {Holden Lee and Rong Ge and Tengyu Ma and Andrej Risteski
		  and Sanjeev Arora},
  booktitle	= {Proceedings of the 2017 Conference on Learning Theory},
  pages		= {1271--1296},
  year		= {2017},
  editor	= {Satyen Kale and Ohad Shamir},
  volume	= {65},
  series	= {Proceedings of Machine Learning Research},
  address	= {Amsterdam, Netherlands},
  month		= {07--10 Jul},
  publisher	= {PMLR},
  pdf		= {http://proceedings.mlr.press/v65/lee17a/lee17a.pdf},
  url		= {http://proceedings.mlr.press/v65/lee17a.html},
  abstract	= {Deep neural nets have caused a revolution in many
		  classification tasks. A related ongoing revolution—also
		  theoretically not understood—concerns their ability to
		  serve as generative models for complicated types of data
		  such as images and texts. These models are trained using
		  ideas like variational autoencoders and Generative
		  Adversarial Networks. We take a first cut at explaining the
		  expressivity of multilayer nets by giving a sufficient
		  criterion for a function to be approximable by a neural
		  network with $n$ hidden layers. A key ingredient is
		  Barron’s Theorem (Barron, 1993), which gives a Fourier
		  criterion for approximability of a function by a neural
		  network with 1 hidden layer. We show that a composition of
		  $n$ functions which satisfy certain Fourier conditions
		  (“Barron functions”) can be approximated by a
		  $n+1$-layer neural network. For probability distributions,
		  this translates into a criterion for a probability
		  distribution to be approximable in Wasserstein distance—a
		  natural metric on probability distributions—by a neural
		  network applied to a fixed base distribution (e.g.,
		  multivariate gaussian). Building up recent lower bound
		  work, we also give an example function that shows that
		  composition of Barron functions is more expressive than
		  Barron functions alone.}
}

@Article{	  lee_seung_1999,
  author	= {Lee, D. D. and Seung, H. S.},
  title		= {Learning the parts of objects by non-negative matrix
		  factorization.},
  journal	= {Nature},
  year		= {1999},
  volume	= {401},
  pages		= {788--791},
  owner		= {umutsimsekli},
  publisher	= {Nature Publishing Group},
  timestamp	= {2011.05.09}
}

###Article{	  lee_seung_1999,
  author	= {Lee, D. D. and Seung, H. S.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Nature},
  owner		= {umutsimsekli},
  pages		= {788--791},
  publisher	= {Nature Publishing Group},
  timestamp	= {2011.05.09},
  title		= {Learning the parts of objects by non-negative matrix
		  factorization.},
  volume	= {401},
  year		= {1999}
}

@Article{	  legomsky2010restructuring,
  title		= {Restructuring Immigration Adjudication},
  author	= {Legomsky, Stephen H},
  journal	= {Duke Law Journal},
  pages		= {1635--1721},
  year		= {2010},
  publisher	= {JSTOR}
}

###Article{	  legomsky2010restructuring,
  author	= {Legomsky, Stephen H},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Duke Law Journal},
  pages		= {1635--1721},
  publisher	= {JSTOR},
  title		= {Restructuring Immigration Adjudication},
  year		= {2010}
}

###Article{	  legomsky2010restructuring,
  title		= {Restructuring Immigration Adjudication},
  author	= {Legomsky, Stephen H},
  journal	= {Duke Law Journal},
  pages		= {1635--1721},
  year		= {2010},
  publisher	= {JSTOR}
}

@Article{	  lerner12,
  author	= {E. Lerner and G. D\"uring and M. Wyart},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-09-05 15:59:57 +0000},
  journal	= {EPL (Europhysics Letters)},
  number	= {5},
  pages		= {58003},
  title		= {Toward a microscopic description of flow near the jamming
		  threshold},
  volume	= {99},
  year		= {2012},
  bdsk-url-1	= {http://stacks.iop.org/0295-5075/99/i=5/a=58003}
}

###Article{	  lerner12,
  author	= {E. Lerner and G. D\"uring and M. Wyart},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {EPL (Europhysics Letters)},
  number	= {5},
  pages		= {58003},
  title		= {Toward a microscopic description of flow near the jamming
		  threshold},
  volume	= {99},
  year		= {2012},
  bdsk-url-1	= {http://stacks.iop.org/0295-5075/99/i=5/a=58003}
}

###Article{	  lerner12,
  author	= {E. Lerner and G. {D\"uring} and M. Wyart},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-09-05 15:59:57 +0000},
  journal	= {EPL (Europhysics Letters)},
  number	= {5},
  pages		= {58003},
  title		= {Toward a microscopic description of flow near the jamming
		  threshold},
  volume	= {99},
  year		= {2012},
  bdsk-url-1	= {http://stacks.iop.org/0295-5075/99/i=5/a=58003}
}

@Article{	  lerner13a,
  abstract	= {We study theoretically and numerically how hard
		  frictionless particles in random packings can rearrange. We
		  demonstrate the existence of two distinct unstable
		  non-linear modes of rearrangement{,} both associated with
		  the opening and the closing of contacts. The first mode{,}
		  whose density is characterized by some exponent [small
		  theta][prime or minute]{,} corresponds to motions of
		  particles extending throughout the entire system. The
		  second mode{,} whose density is characterized by an
		  exponent [small theta] [not equal] [small theta][prime or
		  minute]{,} corresponds to the local buckling of a few
		  particles. Extended modes are shown to yield at a much
		  higher rate than local modes when a stress is applied. We
		  show that the distribution of contact forces follows P(f)
		  [similar] fmin([small theta][prime or minute]{,}[small
		  theta]){,} and that imposing the restriction that the
		  packing cannot be densified further leads to the bounds and
		  {,} where [gamma] characterizes the singularity of the pair
		  distribution function g(r) at contact. These results extend
		  the theoretical analysis of [Wyart{,} Phys. Rev. Lett.{,}
		  2012{,} 109{,} 125502] where the existence of local modes
		  was not considered. We perform numerics that support that
		  these bounds are saturated with [gamma] [approximate]
		  0.38{,} [small theta] [approximate] 0.17 and [small
		  theta][prime or minute] [approximate] 0.44. We measure
		  systematically the stability of all such modes in
		  packings{,} and confirm their marginal stability. The
		  principle of marginal stability thus allows us to make
		  clearcut predictions on the ensemble of configurations
		  visited in these out-of-equilibrium systems{,} and on the
		  contact forces and pair distribution functions. It also
		  reveals the excitations that need to be included in a
		  description of plasticity or flow near jamming{,} and
		  suggests a new path to study two-level systems and soft
		  spots in simple amorphous solids of repulsive particles.},
  author	= {Lerner, Edan and During, Gustavo and Wyart, Matthieu},
  date-added	= {2014-05-22 20:15:23 +0000},
  date-modified	= {2014-09-05 16:00:04 +0000},
  doi		= {10.1039/C3SM50515D},
  issue		= {34},
  journal	= {Soft Matter},
  pages		= {8252-8263},
  publisher	= {The Royal Society of Chemistry},
  title		= {Low-energy non-linear excitations in sphere packings},
  volume	= {9},
  year		= {2013},
  bdsk-url-1	= {http://dx.doi.org/10.1039/C3SM50515D}
}

###Article{	  lerner13a,
  abstract	= {We study theoretically and numerically how hard
		  frictionless particles in random packings can rearrange. We
		  demonstrate the existence of two distinct unstable
		  non-linear modes of rearrangement{,} both associated with
		  the opening and the closing of contacts. The first mode{,}
		  whose density is characterized by some exponent [small
		  theta][prime or minute]{,} corresponds to motions of
		  particles extending throughout the entire system. The
		  second mode{,} whose density is characterized by an
		  exponent [small theta] [not equal] [small theta][prime or
		  minute]{,} corresponds to the local buckling of a few
		  particles. Extended modes are shown to yield at a much
		  higher rate than local modes when a stress is applied. We
		  show that the distribution of contact forces follows P(f)
		  [similar] fmin([small theta][prime or minute]{,}[small
		  theta]){,} and that imposing the restriction that the
		  packing cannot be densified further leads to the bounds and
		  {,} where [gamma] characterizes the singularity of the pair
		  distribution function g(r) at contact. These results extend
		  the theoretical analysis of [Wyart{,} Phys. Rev. Lett.{,}
		  2012{,} 109{,} 125502] where the existence of local modes
		  was not considered. We perform numerics that support that
		  these bounds are saturated with [gamma] [approximate]
		  0.38{,} [small theta] [approximate] 0.17 and [small
		  theta][prime or minute] [approximate] 0.44. We measure
		  systematically the stability of all such modes in
		  packings{,} and confirm their marginal stability. The
		  principle of marginal stability thus allows us to make
		  clearcut predictions on the ensemble of configurations
		  visited in these out-of-equilibrium systems{,} and on the
		  contact forces and pair distribution functions. It also
		  reveals the excitations that need to be included in a
		  description of plasticity or flow near jamming{,} and
		  suggests a new path to study two-level systems and soft
		  spots in simple amorphous solids of repulsive particles.},
  author	= {Lerner, Edan and During, Gustavo and Wyart, Matthieu},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1039/C3SM50515D},
  issue		= {34},
  journal	= {Soft Matter},
  pages		= {8252-8263},
  publisher	= {The Royal Society of Chemistry},
  title		= {Low-energy non-linear excitations in sphere packings},
  volume	= {9},
  year		= {2013},
  bdsk-url-1	= {http://dx.doi.org/10.1039/C3SM50515D}
}

###Article{	  lerner13a,
  abstract	= {We study theoretically and numerically how hard
		  frictionless particles in random packings can rearrange. We
		  demonstrate the existence of two distinct unstable
		  non-linear modes of rearrangement{,} both associated with
		  the opening and the closing of contacts. The first mode{,}
		  whose density is characterized by some exponent [small
		  theta][prime or minute]{,} corresponds to motions of
		  particles extending throughout the entire system. The
		  second mode{,} whose density is characterized by an
		  exponent [small theta] [not equal] [small theta][prime or
		  minute]{,} corresponds to the local buckling of a few
		  particles. Extended modes are shown to yield at a much
		  higher rate than local modes when a stress is applied. We
		  show that the distribution of contact forces follows P(f)
		  [similar] fmin([small theta][prime or minute]{,}[small
		  theta]){,} and that imposing the restriction that the
		  packing cannot be densified further leads to the bounds and
		  {,} where [gamma] characterizes the singularity of the pair
		  distribution function g(r) at contact. These results extend
		  the theoretical analysis of [Wyart{,} Phys. Rev. Lett.{,}
		  2012{,} 109{,} 125502] where the existence of local modes
		  was not considered. We perform numerics that support that
		  these bounds are saturated with [gamma] [approximate]
		  0.38{,} [small theta] [approximate] 0.17 and [small
		  theta][prime or minute] [approximate] 0.44. We measure
		  systematically the stability of all such modes in
		  packings{,} and confirm their marginal stability. The
		  principle of marginal stability thus allows us to make
		  clearcut predictions on the ensemble of configurations
		  visited in these out-of-equilibrium systems{,} and on the
		  contact forces and pair distribution functions. It also
		  reveals the excitations that need to be included in a
		  description of plasticity or flow near jamming{,} and
		  suggests a new path to study two-level systems and soft
		  spots in simple amorphous solids of repulsive particles.},
  author	= {Lerner, Edan and During, Gustavo and Wyart, Matthieu},
  date-added	= {2014-05-22 20:15:23 +0000},
  date-modified	= {2014-09-05 16:00:04 +0000},
  doi		= {10.1039/C3SM50515D},
  issue		= {34},
  journal	= {Soft Matter},
  pages		= {8252-8263},
  publisher	= {The Royal Society of Chemistry},
  title		= {Low-energy non-linear excitations in sphere packings},
  volume	= {9},
  year		= {2013},
  bdsk-url-1	= {http://dx.doi.org/10.1039/C3SM50515D}
}

@Article{	  lesieur2017statistical,
  title		= {Statistical and computational phase transitions in spiked
		  tensor estimation},
  author	= {Lesieur, Thibault and Miolane, L{\'e}o and Lelarge, Marc
		  and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  journal	= {arXiv preprint arXiv:1701.08010},
  year		= {2017}
}

@Article{	  li18,
  title		= {Measuring the intrinsic dimension of objective
		  landscapes},
  author	= {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and
		  Yosinski, Jason},
  journal	= {arXiv preprint arXiv:1804.08838},
  year		= {2018}
}

###Article{	  li18,
  author	= {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and
		  Yosinski, Jason},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1804.08838},
  title		= {Measuring the intrinsic dimension of objective
		  landscapes},
  year		= {2018}
}

###Article{	  li18,
  title		= {Measuring the intrinsic dimension of objective
		  landscapes},
  author	= {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and
		  Yosinski, Jason},
  journal	= {arXiv preprint arXiv:1804.08838},
  year		= {2018}
}

@Article{	  li2015dynamics,
  title		= {Dynamics of stochastic gradient algorithms},
  author	= {Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal	= {arXiv preprint arXiv:1511.06251},
  year		= {2015}
}

###Article{	  li2015dynamics,
  author	= {Li, Qianxiao and Tai, Cheng and Weinan, E},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1511.06251},
  title		= {Dynamics of stochastic gradient algorithms},
  year		= {2015}
}

@Article{	  li2015stochastic,
  title		= {Stochastic modified equations and adaptive stochastic
		  gradient algorithms},
  author	= {Li, Qianxiao and Tai, Cheng and others},
  journal	= {arXiv preprint arXiv:1511.06251},
  year		= {2015}
}

@InProceedings{	  li2016preconditioned,
  title		= {Preconditioned stochastic {gradient Langevin dynamics for
		  deep neural networks}.},
  author	= {Li, Chunyuan and Chen, Changyou and Carlson, David E and
		  Carin, Lawrence},
  booktitle	= {AAAI},
  volume	= {2},
  number	= {3},
  pages		= {4},
  year		= {2016}
}

###InProceedings{ li2016preconditioned,
  author	= {Li, Chunyuan and Chen, Changyou and Carlson, David E and
		  Carin, Lawrence},
  booktitle	= {AAAI},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  number	= {3},
  pages		= {4},
  title		= {Preconditioned stochastic {gradient Langevin dynamics for
		  deep neural networks}.},
  volume	= {2},
  year		= {2016}
}

@Article{	  li2017batch,
  title		= {Batch size matters: A diffusion approximation framework on
		  nonconvex stochastic gradient descent},
  author	= {Li, Chris Junchi and Li, Lei and Qian, Junyang and Liu,
		  Jian-Guo},
  journal	= {stat},
  volume	= {1050},
  pages		= {22},
  year		= {2017}
}

@Article{	  li2018measuring,
  title		= {Measuring the intrinsic dimension of objective
		  landscapes},
  author	= {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and
		  Yosinski, Jason},
  journal	= {arXiv preprint arXiv:1804.08838},
  year		= {2018}
}

@Article{	  liang2018just,
  title		= {Just Interpolate: Kernel" Ridgeless" Regression Can
		  Generalize},
  author	= {Liang, Tengyuan and Rakhlin, Alexander},
  journal	= {arXiv preprint arXiv:1808.00387},
  year		= {2018}
}

@Article{	  liao2018dynamics,
  title		= {The Dynamics of Learning: A Random Matrix Approach},
  author	= {Liao, Zhenyu and Couillet, Romain},
  journal	= {arXiv preprint arXiv:1805.11917},
  year		= {2018}
}

###Article{	  liao2018dynamics,
  title		= {The Dynamics of Learning: A Random Matrix Approach},
  author	= {Liao, Zhenyu and Couillet, Romain},
  journal	= {arXiv preprint arXiv:1805.11917},
  year		= {2018}
}

@Article{	  liers2008,
  author	= {Liers, Frauke},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Liers\_1.pdf:pdf},
  title		= {{Exact Optimization in Spin-Glass Physics}},
  year		= {2008}
}

###Article{	  liers2008,
  author	= {Liers, Frauke},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Liers\_1.pdf:pdf},
  title		= {{Exact Optimization in Spin-Glass Physics}},
  year		= {2008}
}

@Article{	  limpert2001,
  author	= {Limpert, Eckhard and Stahel, Werner A and Abbt, Markus},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/bioscience.pdf:pdf},
  journal	= {BioScience},
  number	= {5},
  pages		= {341--352},
  title		= {{Log-normal Distributions across the Sciences : Keys and
		  Clues}},
  volume	= {51},
  year		= {2001}
}

###Article{	  limpert2001,
  author	= {Limpert, Eckhard and Stahel, Werner A and Abbt, Markus},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/bioscience.pdf:pdf},
  journal	= {BioScience},
  number	= {5},
  pages		= {341--352},
  title		= {{Log-normal Distributions across the Sciences : Keys and
		  Clues}},
  volume	= {51},
  year		= {2001}
}

@Article{	  lipton16,
  title		= {Stuck in a what? adventures in weight space},
  author	= {Lipton, Zachary C},
  journal	= {International Conference on Learning Representations},
  year		= {2016}
}

###Article{	  lipton16,
  author	= {Lipton, Zachary C},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Stuck in a what? adventures in weight space},
  year		= {2016}
}

###Article{	  lipton16,
  title		= {Stuck in a what? adventures in weight space},
  author	= {Lipton, Zachary C},
  journal	= {International Conference on Learning Representations},
  year		= {2016}
}

@Article{	  lipton2016stuck,
  title		= {Stuck in a what? adventures in weight space},
  author	= {Lipton, Zachary C},
  journal	= {arXiv preprint arXiv:1602.07320},
  year		= {2016}
}

###Article{	  lipton2016stuck,
  author	= {Lipton, Zachary C},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1602.07320},
  title		= {Stuck in a what? adventures in weight space},
  year		= {2016}
}

@InBook{	  liu10,
  address	= {Oxford},
  author	= {Andrea J. Liu and Sidney R. Nagel and Wim van Saarloos and
		  Matthieu Wyart},
  booktitle	= {Dynamical heterogeneities in glasses, colloids, and
		  granular media},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2015-01-24 09:41:43 +0000},
  editor	= {L.Berthier and G. Biroli and J.P Bouchaud and L. Cipeletti
		  and W. van Saarloos},
  publisher	= {Oxford University Press},
  title		= {The jamming scenario: an introduction and outlook},
  year		= {2010}
}

###InBook{	  liu10,
  address	= {Oxford},
  author	= {Andrea J. Liu and Sidney R. Nagel and Wim van Saarloos and
		  Matthieu Wyart},
  booktitle	= {Dynamical heterogeneities in glasses, colloids, and
		  granular media},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {L.Berthier and G. Biroli and J.P Bouchaud and L. Cipeletti
		  and W. van Saarloos},
  publisher	= {Oxford University Press},
  title		= {The jamming scenario: an introduction and outlook},
  year		= {2010}
}

###Book{	  liu10,
  author	= {J Liu, Andrea and R Nagel, Sidney and Saarloos, W and
		  Wyart, Matthieu},
  year		= {2010},
  month		= {06},
  pages		= {},
  title		= {The jamming scenario - an introduction and outlook},
  booktitle	= {Dynamical Heterogeneities in Glasses, Colloids, and
		  Granular Media},
  publisher	= {OUP Oxford}
}

@InProceedings{	  liu2015,
  author	= {Baoyuan Liu and Min Wang and H. Foroosh and M. Tappen and
		  M. Penksy},
  booktitle	= {2015 IEEE Conference on Computer Vision and Pattern
		  Recognition (CVPR)},
  title		= {Sparse Convolutional Neural Networks},
  year		= {2015},
  pages		= {806-814},
  keywords	= {matrix decomposition;matrix multiplication;neural
		  nets;object detection;SCNN model;cascade model;object
		  detection problem;sparse convolutional neural
		  networks;sparse decomposition;sparse fully connected
		  layers;sparse matrix multiplication
		  algorithm;Accuracy;Convolutional codes;Kernel;Matrix
		  decomposition;Neural networks;Redundancy;Sparse matrices},
  doi		= {10.1109/CVPR.2015.7298681},
  issn		= {1063-6919},
  month		= {June}
}

###InProceedings{ liu2015,
  author	= {Baoyuan Liu and Min Wang and H. Foroosh and M. Tappen and
		  M. Penksy},
  booktitle	= {2015 IEEE Conference on Computer Vision and Pattern
		  Recognition (CVPR)},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1109/CVPR.2015.7298681},
  issn		= {1063-6919},
  keywords	= {matrix decomposition;matrix multiplication;neural
		  nets;object detection;SCNN model;cascade model;object
		  detection problem;sparse convolutional neural
		  networks;sparse decomposition;sparse fully connected
		  layers;sparse matrix multiplication
		  algorithm;Accuracy;Convolutional codes;Kernel;Matrix
		  decomposition;Neural networks;Redundancy;Sparse matrices},
  month		= {June},
  pages		= {806-814},
  title		= {Sparse Convolutional Neural Networks},
  year		= {2015},
  bdsk-url-1	= {https://doi.org/10.1109/CVPR.2015.7298681}
}

###InProceedings{ liu2015,
  author	= {Baoyuan Liu and Min Wang and H. Foroosh and M. Tappen and
		  M. Penksy},
  booktitle	= {2015 IEEE Conference on Computer Vision and Pattern
		  Recognition (CVPR)},
  title		= {Sparse Convolutional Neural Networks},
  year		= {2015},
  pages		= {806-814},
  keywords	= {matrix decomposition;matrix multiplication;neural
		  nets;object detection;SCNN model;cascade model;object
		  detection problem;sparse convolutional neural
		  networks;sparse decomposition;sparse fully connected
		  layers;sparse matrix multiplication
		  algorithm;Accuracy;Convolutional codes;Kernel;Matrix
		  decomposition;Neural networks;Redundancy;Sparse matrices},
  doi		= {10.1109/CVPR.2015.7298681},
  issn		= {1063-6919},
  month		= {June}
}

@Article{	  liu2016detection,
  title		= {Detection of number of components in {CANDECOMP/PARAFAC}
		  models via minimum description length},
  author	= {Liu, Kefei and da Costa, Jo{\~a}o Paulo CL and So, Hing
		  Cheung and Huang, Lei and Ye, Jieping},
  journal	= {Digital Signal Processing},
  volume	= {51},
  pages		= {110--123},
  year		= {2016},
  publisher	= {Elsevier}
}

###Article{	  liu2016detection,
  author	= {Liu, Kefei and da Costa, Jo{\~a}o Paulo CL and So, Hing
		  Cheung and Huang, Lei and Ye, Jieping},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Digital Signal Processing},
  pages		= {110--123},
  publisher	= {Elsevier},
  title		= {Detection of number of components in {CANDECOMP/PARAFAC}
		  models via minimum description length},
  volume	= {51},
  year		= {2016}
}

@Article{	  lopez2018easing,
  title		= {Easing non-convex optimization with neural networks},
  author	= {Lopez-Paz, David and Sagun, Levent},
  journal	= {ICLR 2018 Workshop Track},
  year		= {2018}
}

@Article{	  lu2014,
  abstract	= {A family of collective variables is proposed to perform
		  exact dynamical coarse-graining even in systems without
		  time scale separation. More precisely, it is shown that
		  these variables are not slow in general, yet satisfy an
		  overdamped Langevin equation that statistically preserves
		  the sequence in which any regions in collective variable
		  space are visited and permits to calculate exactly the mean
		  first passage times from any such region to another. The
		  role of the free energy and diffusion coefficient in this
		  overdamped Langevin equation is discussed, along with the
		  way they transform under any change of variable in
		  collective variable space. These results apply both to
		  systems with and without inertia, and they can be
		  generalized to using several collective variables
		  simultaneously. The view they offer on what makes
		  collective variables and reaction coordinates optimal
		  breaks from the standard notion that good collective
		  variable must be slow variable, and it suggests new ways to
		  interpret data from molecular dynamics simulations and
		  experiments.},
  author	= {Lu, Jianfeng and Vanden-Eijnden, Eric},
  doi		= {10.1063/1.4890367},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/J+Chem+Phys+2014+Lu.pdf:pdf},
  issn		= {1089-7690},
  journal	= {The Journal of chemical physics},
  month		= jul,
  number	= {4},
  pages		= {044109},
  pmid		= {25084883},
  title		= {{Exact dynamical coarse-graining without time-scale
		  separation.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/25084883},
  volume	= {141},
  year		= {2014}
}

###Article{	  lu2014,
  abstract	= {A family of collective variables is proposed to perform
		  exact dynamical coarse-graining even in systems without
		  time scale separation. More precisely, it is shown that
		  these variables are not slow in general, yet satisfy an
		  overdamped Langevin equation that statistically preserves
		  the sequence in which any regions in collective variable
		  space are visited and permits to calculate exactly the mean
		  first passage times from any such region to another. The
		  role of the free energy and diffusion coefficient in this
		  overdamped Langevin equation is discussed, along with the
		  way they transform under any change of variable in
		  collective variable space. These results apply both to
		  systems with and without inertia, and they can be
		  generalized to using several collective variables
		  simultaneously. The view they offer on what makes
		  collective variables and reaction coordinates optimal
		  breaks from the standard notion that good collective
		  variable must be slow variable, and it suggests new ways to
		  interpret data from molecular dynamics simulations and
		  experiments.},
  author	= {Lu, Jianfeng and Vanden-Eijnden, Eric},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1063/1.4890367},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/J+Chem+Phys+2014+Lu.pdf:pdf},
  issn		= {1089-7690},
  journal	= {The Journal of chemical physics},
  month		= jul,
  number	= {4},
  pages		= {044109},
  pmid		= {25084883},
  title		= {{Exact dynamical coarse-graining without time-scale
		  separation.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/25084883},
  volume	= {141},
  year		= {2014},
  bdsk-url-1	= {http://www.ncbi.nlm.nih.gov/pubmed/25084883},
  bdsk-url-2	= {https://doi.org/10.1063/1.4890367}
}

@Article{	  lustig2008inside,
  title		= {Inside the judges' chambers: Narrative responses from the
		  National Association of Immigration Judges stress and
		  burnout survey},
  author	= {Lustig, Stuart L and Karnik, Niranijan and Delucchi, Kevin
		  and Tennakoon, Lakshika},
  journal	= {Geo. Immigr. LJ},
  volume	= {23},
  pages		= {57},
  year		= {2008},
  publisher	= {HeinOnline}
}

###Article{	  lustig2008inside,
  author	= {Lustig, Stuart L and Karnik, Niranijan and Delucchi, Kevin
		  and Tennakoon, Lakshika},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Geo. Immigr. LJ},
  pages		= {57},
  publisher	= {HeinOnline},
  title		= {Inside the judges' chambers: Narrative responses from the
		  National Association of Immigration Judges stress and
		  burnout survey},
  volume	= {23},
  year		= {2008}
}

###Article{	  lustig2008inside,
  title		= {Inside the judges' chambers: Narrative responses from the
		  National Association of Immigration Judges stress and
		  burnout survey},
  author	= {Lustig, Stuart L and Karnik, Niranijan and Delucchi, Kevin
		  and Tennakoon, Lakshika},
  journal	= {Geo. Immigr. LJ},
  volume	= {23},
  pages		= {57},
  year		= {2008},
  publisher	= {HeinOnline}
}

@InProceedings{	  ma2015complete,
  title		= {A complete recipe for stochastic gradient {MCMC}},
  author	= {Ma, Y. A. and Chen, T. and Fox, E.},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {2899--2907},
  year		= {2015}
}

###InProceedings{ ma2015complete,
  author	= {Ma, Y. A. and Chen, T. and Fox, E.},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {2899--2907},
  title		= {A complete recipe for stochastic gradient {MCMC}},
  year		= {2015}
}

@Article{	  maaten2008,
  author	= {Maaten, Laurens Van Der and Hinton, Geoffrey},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/vandermaaten08a.pdf:pdf},
  keywords	= {dimensionality reduction,embedding algorithms,manifold
		  learning,multidimensional scaling,visualization},
  pages		= {2579--2605},
  title		= {{Visualizing Data using t-SNE}},
  volume	= {9},
  year		= {2008}
}

###Article{	  maaten2008,
  author	= {Maaten, Laurens Van Der and Hinton, Geoffrey},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/vandermaaten08a.pdf:pdf},
  keywords	= {dimensionality reduction,embedding algorithms,manifold
		  learning,multidimensional scaling,visualization},
  pages		= {2579--2605},
  title		= {{Visualizing Data using t-SNE}},
  volume	= {9},
  year		= {2008}
}

@Article{	  mailman09,
  author	= {Mailman, Mitch and Schreck, Carl F. and O'Hern, Corey S.
		  and Chakraborty, Bulbul},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  doi		= {10.1103/PhysRevLett.102.255501},
  journal	= {Phys. Rev. Lett.},
  month		= {Jun},
  number	= {25},
  numpages	= {4},
  pages		= {255501},
  publisher	= {American Physical Society},
  title		= {Jamming in Systems Composed of Frictionless Ellipse-Shaped
		  Particles},
  volume	= {102},
  year		= {2009},
  bdsk-url-1	= {http://dx.doi.org/10.1103/PhysRevLett.102.255501}
}

###Article{	  mailman09,
  author	= {Mailman, Mitch and Schreck, Carl F. and O'Hern, Corey S.
		  and Chakraborty, Bulbul},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevLett.102.255501},
  journal	= {Phys. Rev. Lett.},
  month		= {Jun},
  number	= {25},
  numpages	= {4},
  pages		= {255501},
  publisher	= {American Physical Society},
  title		= {Jamming in Systems Composed of Frictionless Ellipse-Shaped
		  Particles},
  volume	= {102},
  year		= {2009},
  bdsk-url-1	= {http://dx.doi.org/10.1103/PhysRevLett.102.255501}
}

###Article{	  mailman09,
  author	= {Mailman, Mitch and Schreck, Carl F. and O'Hern, Corey S.
		  and Chakraborty, Bulbul},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:33 +0000},
  doi		= {10.1103/PhysRevLett.102.255501},
  journal	= {Phys. Rev. Lett.},
  month		= {Jun},
  number	= {25},
  numpages	= {4},
  pages		= {255501},
  publisher	= {American Physical Society},
  title		= {Jamming in Systems Composed of Frictionless Ellipse-Shaped
		  Particles},
  volume	= {102},
  year		= {2009},
  bdsk-url-1	= {http://dx.doi.org/10.1103/PhysRevLett.102.255501}
}

@Article{	  majumdar2011,
  author	= {Majumdar, Satya N. and Nadal, C\'{e}line and Scardicchio,
		  Antonello and Vivo, Pierpaolo},
  doi		= {10.1103/PhysRevE.83.041105},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/majnadalsca11.pdf:pdf},
  issn		= {1539-3755},
  journal	= {Physical Review E},
  month		= apr,
  number	= {4},
  pages		= {041105},
  title		= {{How many eigenvalues of a Gaussian random matrix are
		  positive?}},
  url		= {http://link.aps.org/doi/10.1103/PhysRevE.83.041105},
  volume	= {83},
  year		= {2011}
}

###Article{	  majumdar2011,
  author	= {Majumdar, Satya N. and Nadal, C\'{e}line and Scardicchio,
		  Antonello and Vivo, Pierpaolo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevE.83.041105},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/majnadalsca11.pdf:pdf},
  issn		= {1539-3755},
  journal	= {Physical Review E},
  month		= apr,
  number	= {4},
  pages		= {041105},
  title		= {{How many eigenvalues of a Gaussian random matrix are
		  positive?}},
  url		= {http://link.aps.org/doi/10.1103/PhysRevE.83.041105},
  volume	= {83},
  year		= {2011},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevE.83.041105},
  bdsk-url-2	= {https://doi.org/10.1103/PhysRevE.83.041105}
}

@Article{	  majumdar2011a,
  author	= {Majumdar, Satya N},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/majumdar1.pdf:pdf},
  title		= {{Top eigenvalue of a random matrix : A tale of tails Plan
		  Plan :}},
  year		= {2011}
}

###Article{	  majumdar2011a,
  author	= {Majumdar, Satya N},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/majumdar1.pdf:pdf},
  title		= {{Top eigenvalue of a random matrix : A tale of tails Plan
		  Plan :}},
  year		= {2011}
}

@InProceedings{	  mandt2016variational,
  title		= {A variational analysis of stochastic gradient algorithms},
  author	= {Mandt, S. and Hoffman, M. and Blei, D.},
  booktitle	= {International Conference on Machine Learning},
  pages		= {354--363},
  year		= {2016}
}

###InProceedings{ mandt2016variational,
  author	= {Mandt, S. and Hoffman, M. and Blei, D.},
  booktitle	= {International Conference on Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {354--363},
  title		= {A variational analysis of stochastic gradient algorithms},
  year		= {2016}
}

@Article{	  mandt2017stochastic,
  title		= {Stochastic gradient descent as approximate Bayesian
		  inference},
  author	= {Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal	= {The Journal of Machine Learning Research},
  volume	= {18},
  number	= {1},
  pages		= {4873--4907},
  year		= {2017},
  publisher	= {JMLR. org}
}

@Article{	  marouf2010implicit,
  title		= {Implicit Bias and Immigration Courts},
  author	= {Marouf, Fatma E},
  journal	= {New Eng. L. Rev.},
  volume	= {45},
  pages		= {417},
  year		= {2010},
  publisher	= {HeinOnline}
}

###Article{	  marouf2010implicit,
  author	= {Marouf, Fatma E},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {New Eng. L. Rev.},
  pages		= {417},
  publisher	= {HeinOnline},
  title		= {Implicit Bias and Immigration Courts},
  volume	= {45},
  year		= {2010}
}

###Article{	  marouf2010implicit,
  title		= {Implicit Bias and Immigration Courts},
  author	= {Marouf, Fatma E},
  journal	= {New Eng. L. Rev.},
  volume	= {45},
  pages		= {417},
  year		= {2010},
  publisher	= {HeinOnline}
}

@InProceedings{	  martens2010deep,
  title		= {Deep learning via Hessian-free optimization},
  author	= {Martens, James},
  booktitle	= {Proceedings of the 27th International Conference on
		  Machine Learning (ICML-10)},
  pages		= {735--742},
  year		= {2010}
}

###InProceedings{ martens2010deep,
  author	= {Martens, James},
  booktitle	= {Proceedings of the 27th International Conference on
		  Machine Learning (ICML-10)},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {735--742},
  title		= {Deep learning via Hessian-free optimization},
  year		= {2010}
}

###InProceedings{ martens2010deep,
  title		= {Deep learning via Hessian-free optimization},
  author	= {Martens, James},
  booktitle	= {Proceedings of the 27th International Conference on
		  Machine Learning (ICML-10)},
  pages		= {735--742},
  year		= {2010}
}

@Article{	  martens2014new,
  title		= {New insights and perspectives on the natural gradient
		  method},
  author	= {Martens, James},
  journal	= {arXiv preprint arXiv:1412.1193},
  year		= {2014}
}

###Article{	  martens2014new,
  author	= {Martens, James},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1412.1193},
  title		= {New insights and perspectives on the natural gradient
		  method},
  year		= {2014}
}

@Article{	  marvcenko1967distribution,
  title		= {Distribution of eigenvalues for some sets of random
		  matrices},
  author	= {Mar{\v{c}}enko, Vladimir A and Pastur, Leonid Andreevich},
  journal	= {Mathematics of the USSR-Sbornik},
  volume	= {1},
  number	= {4},
  pages		= {457},
  year		= {1967},
  publisher	= {IOP Publishing}
}

###Article{	  marvcenko1967distribution,
  author	= {Mar{\v{c}}enko, Vladimir A and Pastur, Leonid Andreevich},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Mathematics of the USSR-Sbornik},
  number	= {4},
  pages		= {457},
  publisher	= {IOP Publishing},
  title		= {Distribution of eigenvalues for some sets of random
		  matrices},
  volume	= {1},
  year		= {1967}
}

###Article{	  marvcenko1967distribution,
  title		= {Distribution of eigenvalues for some sets of random
		  matrices},
  author	= {{Mar{\v{c}}enko}, Vladimir A and Pastur, Leonid
		  Andreevich},
  journal	= {Mathematics of the USSR-Sbornik},
  volume	= {1},
  number	= {4},
  pages		= {457},
  year		= {1967},
  publisher	= {IOP Publishing}
}

@Article{	  masters2018revisiting,
  title		= {Revisiting Small Batch Training for Deep Neural Networks},
  author	= {Masters, Dominic and Luschi, Carlo},
  journal	= {arXiv preprint arXiv:1804.07612},
  year		= {2018}
}

@Article{	  mehta,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:1409.8303v1},
  author	= {Mehta, Dhagash and Hauenstein, Jonathan D and Niemerg,
		  Matthew and Simm, Nicholas J and Stariolo, Daniel A},
  eprint	= {arXiv:1409.8303v1},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1409.8303v1.pdf:pdf},
  pages		= {1--9},
  title		= {{Energy Landscape of the Finite-Size Mean-field 2-Spin
		  Spherical Model and Topology Trivialization}}
}

###Article{	  mehta,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:1409.8303v1},
  author	= {Mehta, Dhagash and Hauenstein, Jonathan D and Niemerg,
		  Matthew and Simm, Nicholas J and Stariolo, Daniel A},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {arXiv:1409.8303v1},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1409.8303v1.pdf:pdf},
  pages		= {1--9},
  title		= {{Energy Landscape of the Finite-Size Mean-field 2-Spin
		  Spherical Model and Topology Trivialization}}
}

@Article{	  mehta2,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:1303.1520v1},
  author	= {Mehta, Dhagash and Stariolo, Daniel A and Kastner,
		  Michael},
  eprint	= {arXiv:1303.1520v1},
  pages		= {1--10},
  title		= {{Energy Landscape of the Finite-Size Mean-field 3-Spin
		  Spherical Model}}
}

###Article{	  mehta2,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:1303.1520v1},
  author	= {Mehta, Dhagash and Stariolo, Daniel A and Kastner,
		  Michael},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {arXiv:1303.1520v1},
  pages		= {1--10},
  title		= {{Energy Landscape of the Finite-Size Mean-field 3-Spin
		  Spherical Model}}
}

@Article{	  mei2016landscape,
  title		= {The landscape of empirical risk for non-convex losses},
  author	= {Mei, Song and Bai, Yu and Montanari, Andrea},
  journal	= {arXiv preprint arXiv:1607.06534},
  year		= {2016}
}

###Article{	  mei2016landscape,
  author	= {Mei, Song and Bai, Yu and Montanari, Andrea},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1607.06534},
  title		= {The landscape of empirical risk for non-convex losses},
  year		= {2016}
}

###Article{	  mei2016landscape,
  title		= {The landscape of empirical risk for non-convex losses},
  author	= {Mei, Song and Bai, Yu and Montanari, Andrea},
  journal	= {arXiv preprint arXiv:1607.06534},
  year		= {2016}
}

@Article{	  mei2018mean,
  title		= {A Mean Field View of the Landscape of Two-Layers Neural
		  Networks},
  author	= {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal	= {arXiv preprint arXiv:1804.06561},
  year		= {2018}
}

###Article{	  mei2018mean,
  title		= {A Mean Field View of the Landscape of Two-Layers Neural
		  Networks},
  author	= {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal	= {arXiv preprint arXiv:1804.06561},
  year		= {2018}
}

@Article{	  mezard2002analytic,
  title		= {Analytic and algorithmic solution of random satisfiability
		  problems},
  author	= {M{\'e}zard, Marc and Parisi, Giorgio and Zecchina,
		  Riccardo},
  journal	= {Science},
  volume	= {297},
  number	= {5582},
  pages		= {812--815},
  year		= {2002},
  publisher	= {American Association for the Advancement of Science}
}

###Article{	  mezard2002analytic,
  author	= {M{\'e}zard, Marc and Parisi, Giorgio and Zecchina,
		  Riccardo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Science},
  number	= {5582},
  pages		= {812--815},
  publisher	= {American Association for the Advancement of Science},
  title		= {Analytic and algorithmic solution of random satisfiability
		  problems},
  volume	= {297},
  year		= {2002}
}

###Article{	  mezard2002analytic,
  title		= {Analytic and algorithmic solution of random satisfiability
		  problems},
  author	= {M{\'e}zard, Marc and Parisi, Giorgio and Zecchina,
		  Riccardo},
  journal	= {Science},
  volume	= {297},
  number	= {5582},
  pages		= {812--815},
  year		= {2002},
  publisher	= {American Association for the Advancement of Science}
}

@Book{		  mezard87,
  title		= {Spin glass theory and beyond: An Introduction to the
		  Replica Method and Its Applications},
  author	= {M{\'e}zard, Marc and Parisi, Giorgio and Virasoro, Miguel},
  volume	= {9},
  year		= {1987},
  publisher	= {World Scientific Publishing Company}
}

###Book{	  mezard87,
  author	= {M{\'e}zard, Marc and Parisi, Giorgio and Virasoro, Miguel},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  publisher	= {World Scientific Publishing Company},
  title		= {Spin glass theory and beyond: An Introduction to the
		  Replica Method and Its Applications},
  volume	= {9},
  year		= {1987}
}

###Book{	  mezard87,
  title		= {Spin glass theory and beyond: An Introduction to the
		  Replica Method and Its Applications},
  author	= {{M{\'e}zard}, Marc and Parisi, Giorgio and Virasoro,
		  Miguel},
  volume	= {9},
  year		= {1987},
  publisher	= {World Scientific Publishing Company}
}

@Article{	  mitzenmacher2003,
  author	= {Mitzenmacher, Michael},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/im2004a.pdf:pdf},
  journal	= {Internet Mathematics},
  number	= {2},
  pages		= {226--251},
  title		= {{A Brief History of Generative Models for Power Law and
		  Lognormal Distributions}},
  volume	= {1},
  year		= {2003}
}

###Article{	  mitzenmacher2003,
  author	= {Mitzenmacher, Michael},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/im2004a.pdf:pdf},
  journal	= {Internet Mathematics},
  number	= {2},
  pages		= {226--251},
  title		= {{A Brief History of Generative Models for Power Law and
		  Lognormal Distributions}},
  volume	= {1},
  year		= {2003}
}

@Misc{		  mnist,
  title		= {MNIST database},
  author	= {LeCun, Yann and Cortes, Corinna and Burges Christopher J.
		  C.},
  howpublished	= {\url{http://yann.lecun.com/exdb/mnist/}},
  year		= {accessed 2017}
}

###Misc{	  mnist,
  author	= {LeCun, Yann and Cortes, Corinna and Burges Christopher J.
		  C.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  howpublished	= {\url{http://yann.lecun.com/exdb/mnist/}},
  title		= {MNIST database},
  year		= {accessed 2017}
}

@InProceedings{	  mobahi2015link,
  title		= {On the link between gaussian homotopy continuation and
		  convex envelopes},
  author	= {Mobahi, Hossein and Fisher III, John W},
  booktitle	= {International Workshop on Energy Minimization Methods in
		  Computer Vision and Pattern Recognition},
  pages		= {43--56},
  year		= {2015},
  organization	= {Springer}
}

###InProceedings{ mobahi2015link,
  author	= {Mobahi, Hossein and Fisher III, John W},
  booktitle	= {International Workshop on Energy Minimization Methods in
		  Computer Vision and Pattern Recognition},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  organization	= {Springer},
  pages		= {43--56},
  title		= {On the link between gaussian homotopy continuation and
		  convex envelopes},
  year		= {2015}
}

###InProceedings{ mobahi2015link,
  title		= {On the link between gaussian homotopy continuation and
		  convex envelopes},
  author	= {Mobahi, Hossein and Fisher III, John W},
  booktitle	= {International Workshop on Energy Minimization Methods in
		  Computer Vision and Pattern Recognition},
  pages		= {43--56},
  year		= {2015},
  organization	= {Springer}
}

@InProceedings{	  mobahi2015theoretical,
  title		= {A Theoretical Analysis of Optimization by Gaussian
		  Continuation.},
  author	= {Mobahi, Hossein and Fisher III, John W},
  booktitle	= {AAAI},
  pages		= {1205--1211},
  year		= {2015},
  organization	= {Citeseer}
}

###InProceedings{ mobahi2015theoretical,
  author	= {Mobahi, Hossein and Fisher III, John W},
  booktitle	= {AAAI},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  organization	= {Citeseer},
  pages		= {1205--1211},
  title		= {A Theoretical Analysis of Optimization by Gaussian
		  Continuation.},
  year		= {2015}
}

###InProceedings{ mobahi2015theoretical,
  title		= {A Theoretical Analysis of Optimization by Gaussian
		  Continuation.},
  author	= {Mobahi, Hossein and Fisher III, John W},
  booktitle	= {AAAI},
  pages		= {1205--1211},
  year		= {2015},
  organization	= {Citeseer}
}

@Article{	  mobahi2016training,
  title		= {Training Recurrent Neural Networks by Diffusion},
  author	= {Mobahi, Hossein},
  journal	= {arXiv preprint arXiv:1601.04114},
  year		= {2016}
}

###Article{	  mobahi2016training,
  author	= {Mobahi, Hossein},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1601.04114},
  title		= {Training Recurrent Neural Networks by Diffusion},
  year		= {2016}
}

###Article{	  mobahi2016training,
  title		= {Training Recurrent Neural Networks by Diffusion},
  author	= {Mobahi, Hossein},
  journal	= {arXiv preprint arXiv:1601.04114},
  year		= {2016}
}

@Article{	  moller1993exact,
  title		= {Exact Calculation of the Product of the Hessian Matrix of
		  Feed-Forward Network Error Functions and a Vector in 0 (N)
		  Time},
  author	= {M{\o}ller, Martin F},
  journal	= {DAIMI Report Series},
  volume	= {22},
  number	= {432},
  year		= {1993}
}

###Article{	  moller1993exact,
  author	= {M{\o}ller, Martin F},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {DAIMI Report Series},
  number	= {432},
  title		= {Exact Calculation of the Product of the Hessian Matrix of
		  Feed-Forward Network Error Functions and a Vector in 0 (N)
		  Time},
  volume	= {22},
  year		= {1993}
}

###Article{	  moller1993exact,
  title		= {Exact Calculation of the Product of the Hessian Matrix of
		  Feed-Forward Network Error Functions and a Vector in 0 (N)
		  Time},
  author	= {{M{\o}ller}, Martin F},
  journal	= {DAIMI Report Series},
  volume	= {22},
  number	= {432},
  year		= {1993}
}

@Article{	  monasson1999determining,
  title		= {Determining computational complexity from characteristic
		  ‘phase transitions’},
  author	= {Monasson, R{\'e}mi and Zecchina, Riccardo and Kirkpatrick,
		  Scott and Selman, Bart and Troyansky, Lidror},
  journal	= {Nature},
  volume	= {400},
  number	= {6740},
  pages		= {133},
  year		= {1999},
  publisher	= {Nature Publishing Group}
}

###Article{	  monasson1999determining,
  author	= {Monasson, R{\'e}mi and Zecchina, Riccardo and Kirkpatrick,
		  Scott and Selman, Bart and Troyansky, Lidror},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Nature},
  number	= {6740},
  pages		= {133},
  publisher	= {Nature Publishing Group},
  title		= {Determining computational complexity from characteristic
		  `phase transitions'},
  volume	= {400},
  year		= {1999}
}

###Article{	  monasson1999determining,
  title		= {Determining computational complexity from characteristic
		  phase transitions},
  author	= {Monasson, {R{\'e}mi} and Zecchina, Riccardo and
		  Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
  journal	= {Nature},
  volume	= {400},
  number	= {6740},
  pages		= {133},
  year		= {1999},
  publisher	= {Nature Publishing Group}
}

@Article{	  monasson95,
  title		= {Weight space structure and internal representations: a
		  direct approach to learning and generalization in
		  multilayer neural networks},
  author	= {Monasson, R{\'e}mi and Zecchina, Riccardo},
  journal	= {Physical review letters},
  volume	= {75},
  number	= {12},
  pages		= {2432},
  year		= {1995},
  publisher	= {APS}
}

###Article{	  monasson95,
  author	= {Monasson, R{\'e}mi and Zecchina, Riccardo},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical review letters},
  number	= {12},
  pages		= {2432},
  publisher	= {APS},
  title		= {Weight space structure and internal representations: a
		  direct approach to learning and generalization in
		  multilayer neural networks},
  volume	= {75},
  year		= {1995}
}

###Article{	  monasson95,
  title		= {Weight space structure and internal representations: a
		  direct approach to learning and generalization in
		  multilayer neural networks},
  author	= {Monasson, {R{\'e}mi} and Zecchina, Riccardo},
  journal	= {Physical review letters},
  volume	= {75},
  number	= {12},
  pages		= {2432},
  year		= {1995},
  publisher	= {APS}
}

@Article{	  montanarisemerjian,
  title		= {Rigorous inequalities between length and time scales in
		  glassy systems},
  author	= {Montanari, Andrea and Semerjian, Guilhem},
  journal	= {Journal of statistical physics},
  volume	= {125},
  number	= {1},
  pages		= {23},
  year		= {2006},
  publisher	= {Springer}
}

###Article{	  montanarisemerjian,
  author	= {Montanari, Andrea and Semerjian, Guilhem},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of statistical physics},
  number	= {1},
  pages		= {23},
  publisher	= {Springer},
  title		= {Rigorous inequalities between length and time scales in
		  glassy systems},
  volume	= {125},
  year		= {2006}
}

###Article{	  montanarisemerjian,
  title		= {Rigorous inequalities between length and time scales in
		  glassy systems},
  author	= {Montanari, Andrea and Semerjian, Guilhem},
  journal	= {Journal of statistical physics},
  volume	= {125},
  number	= {1},
  pages		= {23},
  year		= {2006},
  publisher	= {Springer}
}

@InProceedings{	  montufar14,
  title		= {On the number of linear regions of deep neural networks},
  author	= {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun
		  and Bengio, Yoshua},
  booktitle	= {Advances in neural information processing systems},
  pages		= {2924--2932},
  year		= {2014}
}

###InProceedings{ montufar14,
  author	= {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun
		  and Bengio, Yoshua},
  booktitle	= {Advances in neural information processing systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {2924--2932},
  title		= {On the number of linear regions of deep neural networks},
  year		= {2014}
}

###InProceedings{ montufar14,
  title		= {On the number of linear regions of deep neural networks},
  author	= {Montufar, Guido F and Pascanu, Razvan and Cho, Kyunghyun
		  and Bengio, Yoshua},
  booktitle	= {Advances in neural information processing systems},
  pages		= {2924--2932},
  year		= {2014}
}

@Article{	  morup2008algorithms,
  title		= {Algorithms for sparse nonnegative {T}ucker
		  decompositions},
  author	= {M{\o}rup, M. and Hansen, L. K. and Arnfred, S. M.},
  journal	= {Neural computation},
  volume	= {20},
  number	= {8},
  pages		= {2112--2131},
  year		= {2008}
}

###Article{	  morup2008algorithms,
  author	= {M{\o}rup, M. and Hansen, L. K. and Arnfred, S. M.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Neural computation},
  number	= {8},
  pages		= {2112--2131},
  title		= {Algorithms for sparse nonnegative {T}ucker
		  decompositions},
  volume	= {20},
  year		= {2008}
}

@Article{	  muller14,
  author	= {M{\"u}ller, Markus and Wyart, Matthieu},
  date-added	= {2015-01-05 15:07:48 +0000},
  date-modified	= {2015-06-04 20:14:00 +0000},
  doi		= {10.1146/annurev-conmatphys-031214-014614},
  journal	= {Annual Review of Condensed Matter Physics},
  number	= {1},
  pages		= {177--200},
  title		= {Marginal Stability in Structural, Spin, and Electron
		  Glasses},
  volume	= {6},
  year		= {2015},
  bdsk-url-1	= {http://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031214-014614},
  bdsk-url-2	= {http://dx.doi.org/10.1146/annurev-conmatphys-031214-014614}
}

###Article{	  muller14,
  author	= {M{\"u}ller, Markus and Wyart, Matthieu},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1146/annurev-conmatphys-031214-014614},
  journal	= {Annual Review of Condensed Matter Physics},
  number	= {1},
  pages		= {177--200},
  title		= {Marginal Stability in Structural, Spin, and Electron
		  Glasses},
  volume	= {6},
  year		= {2015},
  bdsk-url-1	= {http://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031214-014614},
  bdsk-url-2	= {http://dx.doi.org/10.1146/annurev-conmatphys-031214-014614}
}

###Article{	  muller14,
  author	= {M{\"u}ller, Markus and Wyart, Matthieu},
  date-added	= {2015-01-05 15:07:48 +0000},
  date-modified	= {2015-06-04 20:14:00 +0000},
  doi		= {10.1146/annurev-conmatphys-031214-014614},
  journal	= {Annual Review of Condensed Matter Physics},
  number	= {1},
  pages		= {177--200},
  title		= {Marginal Stability in Structural, Spin, and Electron
		  Glasses},
  volume	= {6},
  year		= {2015},
  bdsk-url-1	= {http://www.annualreviews.org/doi/abs/10.1146/annurev-conmatphys-031214-014614},
  bdsk-url-2	= {http://dx.doi.org/10.1146/annurev-conmatphys-031214-014614}
}

@TechReport{	  nberw23180,
  title		= "Human Decisions and Machine Predictions",
  author	= "Jon Kleinberg and Himabindu Lakkaraju and Jure Leskovec
		  and Jens Ludwig and Sendhil Mullainathan",
  institution	= "National Bureau of Economic Research",
  type		= "Working Paper",
  series	= "Working Paper Series",
  number	= "23180",
  year		= "2017",
  month		= "February",
  doi		= {10.3386/w23180},
  url		= "http://www.nber.org/papers/w23180"
}

###TechReport{	  nberw23180,
  author	= {Jon Kleinberg and Himabindu Lakkaraju and Jure Leskovec
		  and Jens Ludwig and Sendhil Mullainathan},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.3386/w23180},
  institution	= {National Bureau of Economic Research},
  month		= {February},
  number	= {23180},
  series	= {Working Paper Series},
  title		= {Human Decisions and Machine Predictions},
  type		= {Working Paper},
  url		= {http://www.nber.org/papers/w23180},
  year		= {2017},
  bdsk-url-1	= {http://www.nber.org/papers/w23180},
  bdsk-url-2	= {https://doi.org/10.3386/w23180}
}

@Book{		  neal1996,
  author	= {Neal, Radford M.},
  title		= {Bayesian Learning for Neural Networks},
  year		= {1996},
  isbn		= {0387947248},
  publisher	= {Springer-Verlag New York, Inc.},
  address	= {Secaucus, NJ, USA}
}

@Article{	  neal2010,
  author	= {Neal, R. M.},
  title		= {{MCMC} Using {Hamiltonian} Dynamics},
  journal	= {Handbook of {Markov chain Monte Carlo}},
  year		= {2010},
  volume	= {54}
}

###Article{	  neal2010,
  author	= {Neal, R. M.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Handbook of {Markov chain Monte Carlo}},
  title		= {{MCMC} Using {Hamiltonian} Dynamics},
  volume	= {54},
  year		= {2010}
}

@Article{	  neal2018modern,
  title		= {A Modern Take on the Bias-Variance Tradeoff in Neural
		  Networks},
  author	= {Neal, Brady and Mittal, Sarthak and Baratin, Aristide and
		  Tantia, Vinayak and Scicluna, Matthew and Lacoste-Julien,
		  Simon and Mitliagkas, Ioannis},
  journal	= {arXiv preprint arXiv:1810.08591},
  year		= {2018}
}

@Article{	  newman,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:cond-mat/0412004v3},
  author	= {Newman, M E J},
  eprint	= {0412004v3},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/0412004v3.pdf:pdf},
  number	= {1},
  primaryclass	= {arXiv:cond-mat},
  title		= {{Power laws, Pareto distributions and Zipf’s law}}
}

###Article{	  newman,
  archiveprefix	= {arXiv},
  arxivid	= {arXiv:cond-mat/0412004v3},
  author	= {Newman, M E J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  eprint	= {0412004v3},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/0412004v3.pdf:pdf},
  number	= {1},
  primaryclass	= {arXiv:cond-mat},
  title		= {{Power laws, Pareto distributions and Zipf's law}}
}

@InProceedings{	  neyshabur2017exploring,
  title		= {Exploring generalization in deep learning},
  author	= {Neyshabur, Behnam and Bhojanapalli, Srinadh and
		  McAllester, David and Srebro, Nati},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {5947--5956},
  year		= {2017}
}

@Article{	  neyshabur2017geometry,
  title		= {Geometry of optimization and implicit regularization in
		  deep learning},
  author	= {Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov,
		  Ruslan and Srebro, Nathan},
  journal	= {arXiv preprint arXiv:1705.03071},
  year		= {2017}
}

###Article{	  neyshabur2017geometry,
  title		= {Geometry of optimization and implicit regularization in
		  deep learning},
  author	= {Neyshabur, Behnam and Tomioka, Ryota and Salakhutdinov,
		  Ruslan and Srebro, Nathan},
  journal	= {arXiv preprint arXiv:1705.03071},
  year		= {2017}
}

@Article{	  neyshabur2018towards,
  title		= {Towards Understanding the Role of Over-Parametrization in
		  Generalization of Neural Networks},
  author	= {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli,
		  Srinadh and LeCun, Yann and Srebro, Nathan},
  journal	= {arXiv preprint arXiv:1805.12076},
  year		= {2018}
}

###Article{	  neyshabur2018towards,
  title		= {Towards Understanding the Role of Over-Parametrization in
		  Generalization of Neural Networks},
  author	= {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli,
		  Srinadh and LeCun, Yann and Srebro, Nathan},
  journal	= {arXiv preprint arXiv:1805.12076},
  year		= {2018}
}

@InProceedings{	  ngiam2011optimization,
  title		= {On optimization methods for deep learning},
  author	= {Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and
		  Prochnow, Bobby and Le, Quoc V and Ng, Andrew Y},
  booktitle	= {Proceedings of the 28th International Conference on
		  Machine Learning (ICML-11)},
  pages		= {265--272},
  year		= {2011}
}

###InProceedings{ ngiam2011optimization,
  author	= {Ngiam, Jiquan and Coates, Adam and Lahiri, Ahbik and
		  Prochnow, Bobby and Le, Quoc V and Ng, Andrew Y},
  booktitle	= {Proceedings of the 28th International Conference on
		  Machine Learning (ICML-11)},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {265--272},
  title		= {On optimization methods for deep learning},
  year		= {2011}
}

@Article{	  ninarello2017models,
  title		= {Models and algorithms for the next generation of glass
		  transition studies},
  author	= {Ninarello, Andrea and Berthier, Ludovic and Coslovich,
		  Daniele},
  journal	= {Physical Review X},
  volume	= {7},
  number	= {2},
  pages		= {021039},
  year		= {2017},
  publisher	= {APS}
}

###Article{	  ninarello2017models,
  author	= {Ninarello, Andrea and Berthier, Ludovic and Coslovich,
		  Daniele},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review X},
  number	= {2},
  pages		= {021039},
  publisher	= {APS},
  title		= {Models and algorithms for the next generation of glass
		  transition studies},
  volume	= {7},
  year		= {2017}
}

###Article{	  ninarello2017models,
  title		= {Models and algorithms for the next generation of glass
		  transition studies},
  author	= {Ninarello, Andrea and Berthier, Ludovic and Coslovich,
		  Daniele},
  journal	= {Physical Review X},
  volume	= {7},
  number	= {2},
  pages		= {021039},
  year		= {2017},
  publisher	= {APS}
}

@InCollection{	  nips1995_1072,
  author	= {Saad, David and Birmingham, B and Solla, Sara A},
  booktitle	= {Advances in Neural Information Processing Systems 8},
  editor	= {Touretzky, D S and Mozer, M C and Hasselmo, M E},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1072-dynamics-of-on-line-gradient-descent-learning-for-multilayer-neural-networks.pdf:pdf},
  pages		= {302--308},
  publisher	= {MIT Press},
  title		= {{Dynamics of On-Line Gradient Descent Learning for
		  Multilayer Neural Networks}},
  year		= {1996}
}

###InCollection{  nips1995_1072,
  author	= {Saad, David and Birmingham, B and Solla, Sara A},
  booktitle	= {Advances in Neural Information Processing Systems 8},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {Touretzky, D S and Mozer, M C and Hasselmo, M E},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1072-dynamics-of-on-line-gradient-descent-learning-for-multilayer-neural-networks.pdf:pdf},
  pages		= {302--308},
  publisher	= {MIT Press},
  title		= {{Dynamics of On-Line Gradient Descent Learning for
		  Multilayer Neural Networks}},
  year		= {1996}
}

@InCollection{	  nips1996_1256,
  author	= {West, Ansgar H L and Saad, David and Nabney, Ian T},
  booktitle	= {Advances in Neural Information Processing Systems 9},
  editor	= {Mozer, M C and Jordan, M I and Petsche, T},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1256-the-learning-dynamcis-of-a-universal-approximator.pdf:pdf},
  pages		= {288--294},
  publisher	= {MIT Press},
  title		= {{The Learning Dynamcis of a Universal Approximator}},
  url		= {http://papers.nips.cc/paper/1256-the-learning-dynamcis-of-a-universal-approximator.pdf},
  year		= {1997}
}

###InCollection{  nips1996_1256,
  author	= {West, Ansgar H L and Saad, David and Nabney, Ian T},
  booktitle	= {Advances in Neural Information Processing Systems 9},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {Mozer, M C and Jordan, M I and Petsche, T},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1256-the-learning-dynamcis-of-a-universal-approximator.pdf:pdf},
  pages		= {288--294},
  publisher	= {MIT Press},
  title		= {{The Learning Dynamcis of a Universal Approximator}},
  url		= {http://papers.nips.cc/paper/1256-the-learning-dynamcis-of-a-universal-approximator.pdf},
  year		= {1997},
  bdsk-url-1	= {http://papers.nips.cc/paper/1256-the-learning-dynamcis-of-a-universal-approximator.pdf}
}

@InCollection{	  nips2014_5484,
  archiveprefix	= {arXiv},
  arxivid	= {1312.6184},
  author	= {Ba, Jimmy and Caruana, Rich and Ba, Lei Jimmy},
  booktitle	= {Advances in Neural Information Processing Systems 27},
  editor	= {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N
		  D and Weinberger, K Q},
  eprint	= {1312.6184},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1312.6184v7.pdf:pdf},
  month		= dec,
  pages		= {2654--2662},
  publisher	= {Curran Associates, Inc.},
  title		= {{Do Deep Nets Really Need to be Deep?}},
  url		= {http://arxiv.org/abs/1312.6184v7
		  http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf},
  volume	= {2014},
  year		= {2014}
}

###InCollection{  nips2014_5484,
  archiveprefix	= {arXiv},
  arxivid	= {1312.6184},
  author	= {Ba, Jimmy and Caruana, Rich and Ba, Lei Jimmy},
  booktitle	= {Advances in Neural Information Processing Systems 27},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N
		  D and Weinberger, K Q},
  eprint	= {1312.6184},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1312.6184v7.pdf:pdf},
  month		= dec,
  pages		= {2654--2662},
  publisher	= {Curran Associates, Inc.},
  title		= {{Do Deep Nets Really Need to be Deep?}},
  url		= {http://arxiv.org/abs/1312.6184v7
		  http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf},
  volume	= {2014},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1312.6184v7%20http://papers.nips.cc/paper/5484-do-deep-nets-really-need-to-be-deep.pdf}
}

@InCollection{	  nips2014_5486,
  archiveprefix	= {arXiv},
  arxivid	= {1406.2572},
  author	= {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar
		  and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle	= {Advances in Neural Information Processing Systems 27},
  editor	= {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N
		  D and Weinberger, K Q},
  eprint	= {1406.2572},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1406.2572v1.pdf:pdf},
  month		= jun,
  pages		= {2933--2941},
  publisher	= {Curran Associates, Inc.},
  title		= {{Identifying and attacking the saddle point problem in
		  high-dimensional non-convex optimization}},
  url		= {http://arxiv.org/abs/1406.2572v1},
  year		= {2014}
}

###InCollection{  nips2014_5486,
  archiveprefix	= {arXiv},
  arxivid	= {1406.2572},
  author	= {Dauphin, Yann N and Pascanu, Razvan and Gulcehre, Caglar
		  and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  booktitle	= {Advances in Neural Information Processing Systems 27},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {Ghahramani, Z and Welling, M and Cortes, C and Lawrence, N
		  D and Weinberger, K Q},
  eprint	= {1406.2572},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/1406.2572v1.pdf:pdf},
  month		= jun,
  pages		= {2933--2941},
  publisher	= {Curran Associates, Inc.},
  title		= {{Identifying and attacking the saddle point problem in
		  high-dimensional non-convex optimization}},
  url		= {http://arxiv.org/abs/1406.2572v1},
  year		= {2014},
  bdsk-url-1	= {http://arxiv.org/abs/1406.2572v1}
}

@Article{	  nocedal2006numerical,
  title		= {Numerical Optimization, Second Edition},
  author	= {Nocedal, Jorge and Wright, Stephen J},
  journal	= {Numerical optimization},
  pages		= {497--528},
  year		= {2006},
  publisher	= {Springer New York}
}

###Article{	  nocedal2006numerical,
  author	= {Nocedal, Jorge and Wright, Stephen J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Numerical optimization},
  pages		= {497--528},
  publisher	= {Springer New York},
  title		= {Numerical Optimization, Second Edition},
  year		= {2006}
}

###Article{	  nocedal2006numerical,
  title		= {Numerical Optimization, Second Edition},
  author	= {Nocedal, Jorge and Wright, Stephen J},
  journal	= {Numerical optimization},
  pages		= {497--528},
  year		= {2006},
  publisher	= {Springer New York}
}

###Article{	  nocedal2006numerical,
  title		= {Numerical Optimization, Second Edition},
  author	= {Nocedal, Jorge and Wright, Stephen J},
  journal	= {Numerical optimization},
  pages		= {497--528},
  year		= {2006},
  publisher	= {Springer New York}
}

@Article{	  ohern03,
  author	= {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea
		  J. and Nagel, Sidney R.},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-11-09 08:34:09 +0000},
  doi		= {10.1103/PhysRevE.68.011306},
  journal	= {Phys. Rev. E},
  month		= {Jul},
  number	= {1},
  pages		= {011306--011324},
  publisher	= {American Physical Society},
  title		= {Jamming at zero temperature and zero applied stress: The
		  epitome of disorder},
  volume	= {68},
  year		= {2003},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevE.68.011306},
  bdsk-url-2	= {http://dx.doi.org/10.1103/PhysRevE.68.011306}
}

###Article{	  ohern03,
  author	= {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea
		  J. and Nagel, Sidney R.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevE.68.011306},
  journal	= {Phys. Rev. E},
  month		= {Jul},
  number	= {1},
  pages		= {011306--011324},
  publisher	= {American Physical Society},
  title		= {Jamming at zero temperature and zero applied stress: The
		  epitome of disorder},
  volume	= {68},
  year		= {2003},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevE.68.011306},
  bdsk-url-2	= {http://dx.doi.org/10.1103/PhysRevE.68.011306}
}

###Article{	  ohern03,
  author	= {O'Hern, Corey S. and Silbert, Leonardo E. and Liu, Andrea
		  J. and Nagel, Sidney R.},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-11-09 08:34:09 +0000},
  doi		= {10.1103/PhysRevE.68.011306},
  journal	= {Phys. Rev. E},
  month		= {Jul},
  number	= {1},
  pages		= {011306--011324},
  publisher	= {American Physical Society},
  title		= {Jamming at zero temperature and zero applied stress: The
		  epitome of disorder},
  volume	= {68},
  year		= {2003},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevE.68.011306},
  bdsk-url-2	= {http://dx.doi.org/10.1103/PhysRevE.68.011306}
}

@Article{	  olver2015sampling,
  title		= {Sampling unitary ensembles},
  author	= {Olver, Sheehan and Rao, N Raj and Trogdon, Thomas},
  journal	= {Random Matrices: Theory and Applications},
  volume	= {4},
  number	= {01},
  pages		= {1550002},
  year		= {2015},
  publisher	= {World Scientific}
}

###Article{	  olver2015sampling,
  author	= {Olver, Sheehan and Rao, N Raj and Trogdon, Thomas},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Random Matrices: Theory and Applications},
  number	= {01},
  pages		= {1550002},
  publisher	= {World Scientific},
  title		= {Sampling unitary ensembles},
  volume	= {4},
  year		= {2015}
}

###Article{	  olver2015sampling,
  title		= {Sampling unitary ensembles},
  author	= {Olver, Sheehan and Rao, N Raj and Trogdon, Thomas},
  journal	= {Random Matrices: Theory and Applications},
  volume	= {4},
  number	= {01},
  pages		= {1550002},
  year		= {2015},
  publisher	= {World Scientific}
}

@Misc{		  openmpi,
  title		= {Open {MPI}},
  howpublished	= {\url{https://www.open-mpi.org/}}
}

###Misc{	  openmpi,
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  howpublished	= {\url{https://www.open-mpi.org/}},
  title		= {Open {MPI}}
}

@Article{	  panageas2016gradient,
  title		= {Gradient descent only converges to minimizers:
		  Non-isolated critical points and invariant regions},
  author	= {Panageas, Ioannis and Piliouras, Georgios},
  journal	= {arXiv preprint arXiv:1605.00405},
  year		= {2016}
}

###Article{	  panageas2016gradient,
  author	= {Panageas, Ioannis and Piliouras, Georgios},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1605.00405},
  title		= {Gradient descent only converges to minimizers:
		  Non-isolated critical points and invariant regions},
  year		= {2016}
}

###Article{	  panageas2016gradient,
  title		= {Gradient descent only converges to minimizers:
		  Non-isolated critical points and invariant regions},
  author	= {Panageas, Ioannis and Piliouras, Georgios},
  journal	= {arXiv preprint arXiv:1605.00405},
  year		= {2016}
}

@InProceedings{	  papalexakis2015fast,
  title		= {Fast efficient and scalable core consistency diagnostic
		  for the parafac decomposition for big sparse tensors},
  author	= {Papalexakis, Evangelos E and Faloutsos, Christos},
  booktitle	= {2015 IEEE International Conference on Acoustics, Speech
		  and Signal Processing (ICASSP)},
  pages		= {5441--5445},
  year		= {2015},
  organization	= {IEEE}
}

###InProceedings{ papalexakis2015fast,
  author	= {Papalexakis, Evangelos E and Faloutsos, Christos},
  booktitle	= {2015 IEEE International Conference on Acoustics, Speech
		  and Signal Processing (ICASSP)},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {IEEE},
  pages		= {5441--5445},
  title		= {Fast efficient and scalable core consistency diagnostic
		  for the parafac decomposition for big sparse tensors},
  year		= {2015}
}

@Article{	  papalexakis2016tensors,
  title		= {Tensors for data mining and data fusion: Models,
		  applications, and scalable algorithms},
  author	= {Papalexakis, Evangelos E and Faloutsos, Christos and
		  Sidiropoulos, Nicholas D},
  journal	= {ACM Transactions on Intelligent Systems and Technology
		  (TIST)},
  volume	= {8},
  number	= {2, article 16},
  year		= {2016},
  publisher	= {ACM}
}

###Article{	  papalexakis2016tensors,
  author	= {Papalexakis, Evangelos E and Faloutsos, Christos and
		  Sidiropoulos, Nicholas D},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {ACM Transactions on Intelligent Systems and Technology
		  (TIST)},
  number	= {2, article 16},
  publisher	= {ACM},
  title		= {Tensors for data mining and data fusion: Models,
		  applications, and scalable algorithms},
  volume	= {8},
  year		= {2016}
}

@Article{	  pardalos1994optimization,
  title		= {Optimization methods for computing global minima of
		  nonconvex potential energy functions},
  author	= {Pardalos, Panos M and Shalloway, David and Xue, Guoliang},
  journal	= {Journal of Global Optimization},
  volume	= {4},
  number	= {2},
  pages		= {117--133},
  year		= {1994},
  publisher	= {Springer}
}

###Article{	  pardalos1994optimization,
  author	= {Pardalos, Panos M and Shalloway, David and Xue, Guoliang},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Journal of Global Optimization},
  number	= {2},
  pages		= {117--133},
  publisher	= {Springer},
  title		= {Optimization methods for computing global minima of
		  nonconvex potential energy functions},
  volume	= {4},
  year		= {1994}
}

###Article{	  pardalos1994optimization,
  title		= {Optimization methods for computing global minima of
		  nonconvex potential energy functions},
  author	= {Pardalos, Panos M and Shalloway, David and Xue, Guoliang},
  journal	= {Journal of Global Optimization},
  volume	= {4},
  number	= {2},
  pages		= {117--133},
  year		= {1994},
  publisher	= {Springer}
}

@InProceedings{	  patteh2013a,
  author	= {S. Patterson and Y. W. Teh},
  title		= {Stochastic Gradient {R}iemannian {L}angevin Dynamics on
		  the Probability Simplex},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {3102--3110},
  year		= {2013}
}

@Article{	  pearlmutter1994fast,
  title		= {Fast exact multiplication by the Hessian},
  author	= {Pearlmutter, Barak A},
  journal	= {Neural computation},
  volume	= {6},
  number	= {1},
  pages		= {147--160},
  year		= {1994},
  publisher	= {MIT Press}
}

###Article{	  pearlmutter1994fast,
  author	= {Pearlmutter, Barak A},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Neural computation},
  number	= {1},
  pages		= {147--160},
  publisher	= {MIT Press},
  title		= {Fast exact multiplication by the Hessian},
  volume	= {6},
  year		= {1994}
}

###Article{	  pearlmutter1994fast,
  title		= {Fast exact multiplication by the Hessian},
  author	= {Pearlmutter, Barak A},
  journal	= {Neural computation},
  volume	= {6},
  number	= {1},
  pages		= {147--160},
  year		= {1994},
  publisher	= {MIT Press}
}

@InProceedings{	  pennington2017geometry,
  title		= {Geometry of neural network loss surfaces via random matrix
		  theory},
  author	= {Pennington, Jeffrey and Bahri, Yasaman},
  booktitle	= {International Conference on Machine Learning},
  pages		= {2798--2806},
  year		= {2017}
}

@Article{	  perline2005,
  author	= {Perline, Richard},
  doi		= {10.1214/088342304000000215},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/perline-strong.pdf:pdf},
  issn		= {0883-4237},
  journal	= {Statistical Science},
  keywords	= {and phrases,distribution,extreme value theory,gumbel
		  distribu-,lognormal,mixture distributions,pareto,pareto
		  distribution,zipf laws},
  month		= feb,
  number	= {1},
  pages		= {68--88},
  title		= {{Strong, Weak and False Inverse Power Laws}},
  url		= {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1118065043/},
  volume	= {20},
  year		= {2005}
}

###Article{	  perline2005,
  author	= {Perline, Richard},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1214/088342304000000215},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/perline-strong.pdf:pdf},
  issn		= {0883-4237},
  journal	= {Statistical Science},
  keywords	= {and phrases,distribution,extreme value theory,gumbel
		  distribu-,lognormal,mixture distributions,pareto,pareto
		  distribution,zipf laws},
  month		= feb,
  number	= {1},
  pages		= {68--88},
  title		= {{Strong, Weak and False Inverse Power Laws}},
  url		= {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1118065043/},
  volume	= {20},
  year		= {2005},
  bdsk-url-1	= {http://projecteuclid.org/Dienst/getRecord?id=euclid.ss/1118065043/},
  bdsk-url-2	= {https://doi.org/10.1214/088342304000000215}
}

@Article{	  perry2016statistical,
  title		= {Statistical limits of spiked tensor models},
  author	= {Perry, Amelia and Wein, Alexander S and Bandeira, Afonso
		  S},
  journal	= {arXiv preprint arXiv:1612.07728},
  year		= {2016}
}

@Book{		  phillips81,
  author	= {Anderson, A.C.},
  date-added	= {2014-06-13 22:47:59 +0000},
  date-modified	= {2015-06-04 02:59:24 +0000},
  editor	= {W. A. Phillips},
  publisher	= {Springer, Berlin},
  series	= {Topics in Current Physics},
  title		= {Amorphous Solids: Low Temperature Properties},
  volume	= {24},
  year		= {1981}
}

###Book{	  phillips81,
  author	= {Anderson, A.C.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {W. A. Phillips},
  publisher	= {Springer, Berlin},
  series	= {Topics in Current Physics},
  title		= {Amorphous Solids: Low Temperature Properties},
  volume	= {24},
  year		= {1981}
}

###Book{	  phillips81,
  author	= {Anderson, A.C.},
  date-added	= {2014-06-13 22:47:59 +0000},
  date-modified	= {2015-06-04 02:59:24 +0000},
  editor	= {W. A. Phillips},
  publisher	= {Springer, Berlin},
  series	= {Topics in Current Physics},
  title		= {Amorphous Solids: Low Temperature Properties},
  volume	= {24},
  year		= {1981}
}

@Article{	  physrevlett.109.167203,
  author	= {Fyodorov, Yan and Nadal, Celine},
  doi		= {10.1103/PhysRevLett.109.167203},
  journal	= {Phys. Rev. Lett.},
  month		= oct,
  number	= {16},
  pages		= {167203},
  publisher	= {American Physical Society},
  title		= {{Critical Behavior of the Number of Minima of a Random
		  Landscape at the Glass Transition Point and the Tracy-Widom
		  Distribution}},
  url		= {http://link.aps.org/doi/10.1103/PhysRevLett.109.167203},
  volume	= {109},
  year		= {2012}
}

###Article{	  physrevlett.109.167203,
  author	= {Fyodorov, Yan and Nadal, Celine},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevLett.109.167203},
  journal	= {Phys. Rev. Lett.},
  month		= oct,
  number	= {16},
  pages		= {167203},
  publisher	= {American Physical Society},
  title		= {{Critical Behavior of the Number of Minima of a Random
		  Landscape at the Glass Transition Point and the Tracy-Widom
		  Distribution}},
  url		= {http://link.aps.org/doi/10.1103/PhysRevLett.109.167203},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.167203},
  bdsk-url-2	= {https://doi.org/10.1103/PhysRevLett.109.167203}
}

@Article{	  piela1989multiple,
  title		= {On the multiple-minima problem in the conformational
		  analysis of molecules: deformation of the potential energy
		  hypersurface by the diffusion equation method},
  author	= {Piela, Lucjan and Kostrowicki, Jaroslaw and Scheraga,
		  Harold A},
  journal	= {The Journal of Physical Chemistry},
  volume	= {93},
  number	= {8},
  pages		= {3339--3346},
  year		= {1989},
  publisher	= {ACS Publications}
}

###Article{	  piela1989multiple,
  author	= {Piela, Lucjan and Kostrowicki, Jaroslaw and Scheraga,
		  Harold A},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {The Journal of Physical Chemistry},
  number	= {8},
  pages		= {3339--3346},
  publisher	= {ACS Publications},
  title		= {On the multiple-minima problem in the conformational
		  analysis of molecules: deformation of the potential energy
		  hypersurface by the diffusion equation method},
  volume	= {93},
  year		= {1989}
}

###Article{	  piela1989multiple,
  title		= {On the multiple-minima problem in the conformational
		  analysis of molecules: deformation of the potential energy
		  hypersurface by the diffusion equation method},
  author	= {Piela, Lucjan and Kostrowicki, Jaroslaw and Scheraga,
		  Harold A},
  journal	= {The Journal of Physical Chemistry},
  volume	= {93},
  number	= {8},
  pages		= {3339--3346},
  year		= {1989},
  publisher	= {ACS Publications}
}

@Article{	  pinter2012,
  author	= {Pint\'{e}r, J\'{a}nos D},
  doi		= {10.1016/j.eswa.2011.06.050},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/2685.pdf:pdf},
  issn		= {09574174},
  journal	= {Expert Systems with Applications},
  keywords	= {ann implementation in mathematica,ann model calibration
		  by,artificial neural networks,global
		  optimization,illustrative numerical examples,lgo,lgo
		  for,lipschitz global optimizer,mathematica,mathoptimizer
		  professional,solver suite},
  month		= jan,
  number	= {1},
  pages		= {25--32},
  title		= {{Calibrating artificial neural networks by global
		  optimization}},
  url		= {http://linkinghub.elsevier.com/retrieve/pii/S095741741100950X},
  volume	= {39},
  year		= {2012}
}

###Article{	  pinter2012,
  author	= {Pint\'{e}r, J\'{a}nos D},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1016/j.eswa.2011.06.050},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/2685.pdf:pdf},
  issn		= {09574174},
  journal	= {Expert Systems with Applications},
  keywords	= {ann implementation in mathematica,ann model calibration
		  by,artificial neural networks,global
		  optimization,illustrative numerical examples,lgo,lgo
		  for,lipschitz global optimizer,mathematica,mathoptimizer
		  professional,solver suite},
  month		= jan,
  number	= {1},
  pages		= {25--32},
  title		= {{Calibrating artificial neural networks by global
		  optimization}},
  url		= {http://linkinghub.elsevier.com/retrieve/pii/S095741741100950X},
  volume	= {39},
  year		= {2012},
  bdsk-url-1	= {http://linkinghub.elsevier.com/retrieve/pii/S095741741100950X},
  bdsk-url-2	= {https://doi.org/10.1016/j.eswa.2011.06.050}
}

@Article{	  pnasmontanariksat,
  author	= {Krz̧aka{\l}a, Florent and Montanari, Andrea and
		  Ricci-Tersenghi, Federico and Semerjian, Guilhem and
		  Zdeborov{\'a}, Lenka},
  title		= {Gibbs states and the set of solutions of random constraint
		  satisfaction problems},
  volume	= {104},
  number	= {25},
  pages		= {10318--10323},
  year		= {2007},
  doi		= {10.1073/pnas.0703685104},
  publisher	= {National Academy of Sciences},
  abstract	= {An instance of a random constraint satisfaction problem
		  defines a random subset �� (the set of solutions) of a
		  large product space X N (the set of assignments). We
		  consider two prototypical problem ensembles (random
		  k-satisfiability and q-coloring of random regular graphs)
		  and study the uniform measure with support on S. As the
		  number of constraints per variable increases, this measure
		  first decomposes into an exponential number of pure states
		  ({\textquotedblleft}clusters{\textquotedblright}) and
		  subsequently condensates over the largest such states.
		  Above the condensation point, the mass carried by the n
		  largest states follows a Poisson-Dirichlet process. For
		  typical large instances, the two transitions are sharp. We
		  determine their precise location. Further, we provide a
		  formal definition of each phase transition in terms of
		  different notions of correlation between distinct variables
		  in the problem. The degree of correlation naturally affects
		  the performances of many search/sampling algorithms.
		  Empirical evidence suggests that local Monte Carlo Markov
		  chain strategies are effective up to the clustering phase
		  transition and belief propagation up to the condensation
		  point. Finally, refined message passing techniques (such as
		  survey propagation) may also beat this threshold.},
  issn		= {0027-8424},
  journal	= {Proceedings of the National Academy of Sciences}
}

###Article{	  pnasmontanariksat,
  abstract	= {An instance of a random constraint satisfaction problem
		  defines a random subset �� (the set of solutions) of a
		  large product space X N (the set of assignments). We
		  consider two prototypical problem ensembles (random
		  k-satisfiability and q-coloring of random regular graphs)
		  and study the uniform measure with support on S. As the
		  number of constraints per variable increases, this measure
		  first decomposes into an exponential number of pure states
		  ({\textquotedblleft}clusters{\textquotedblright}) and
		  subsequently condensates over the largest such states.
		  Above the condensation point, the mass carried by the n
		  largest states follows a Poisson-Dirichlet process. For
		  typical large instances, the two transitions are sharp. We
		  determine their precise location. Further, we provide a
		  formal definition of each phase transition in terms of
		  different notions of correlation between distinct variables
		  in the problem. The degree of correlation naturally affects
		  the performances of many search/sampling algorithms.
		  Empirical evidence suggests that local Monte Carlo Markov
		  chain strategies are effective up to the clustering phase
		  transition and belief propagation up to the condensation
		  point. Finally, refined message passing techniques (such as
		  survey propagation) may also beat this threshold.},
  author	= {Krz̧aka{\l}a, Florent and Montanari, Andrea and
		  Ricci-Tersenghi, Federico and Semerjian, Guilhem and
		  Zdeborov{\'a}, Lenka},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1073/pnas.0703685104},
  issn		= {0027-8424},
  journal	= {Proceedings of the National Academy of Sciences},
  number	= {25},
  pages		= {10318--10323},
  publisher	= {National Academy of Sciences},
  title		= {Gibbs states and the set of solutions of random constraint
		  satisfaction problems},
  volume	= {104},
  year		= {2007},
  bdsk-url-1	= {https://doi.org/10.1073/pnas.0703685104}
}

###Article{	  pnasmontanariksat,
  author	= {{Krzaka{\l}a}, Florent and Montanari, Andrea and
		  Ricci-Tersenghi, Federico and Semerjian, Guilhem and
		  {Zdeborov{\'a}}, Lenka},
  title		= {Gibbs states and the set of solutions of random constraint
		  satisfaction problems},
  volume	= {104},
  number	= {25},
  pages		= {10318--10323},
  year		= {2007},
  doi		= {10.1073/pnas.0703685104},
  publisher	= {National Academy of Sciences},
  abstract	= {An instance of a random constraint satisfaction problem
		  defines a random subset ?? (the set of solutions) of a
		  large product space X N (the set of assignments). We
		  consider two prototypical problem ensembles (random
		  k-satisfiability and q-coloring of random regular graphs)
		  and study the uniform measure with support on S. As the
		  number of constraints per variable increases, this measure
		  first decomposes into an exponential number of pure states
		  ({\textquotedblleft}clusters{\textquotedblright}) and
		  subsequently condensates over the largest such states.
		  Above the condensation point, the mass carried by the n
		  largest states follows a Poisson-Dirichlet process. For
		  typical large instances, the two transitions are sharp. We
		  determine their precise location. Further, we provide a
		  formal definition of each phase transition in terms of
		  different notions of correlation between distinct variables
		  in the problem. The degree of correlation naturally affects
		  the performances of many search/sampling algorithms.
		  Empirical evidence suggests that local Monte Carlo Markov
		  chain strategies are effective up to the clustering phase
		  transition and belief propagation up to the condensation
		  point. Finally, refined message passing techniques (such as
		  survey propagation) may also beat this threshold.},
  issn		= {0027-8424},
  journal	= {Proceedings of the National Academy of Sciences}
}

@Article{	  pouryazdian2016candecomp,
  title		= {{CANDECOMP/PARAFAC} model order selection based on
		  reconstruction error in the presence of kronecker
		  structured colored noise},
  author	= {Pouryazdian, Saeed and Beheshti, Soosan and Krishnan,
		  Sridhar},
  journal	= {Digital Signal Processing},
  volume	= {48},
  pages		= {12--26},
  year		= {2016},
  publisher	= {Elsevier}
}

###Article{	  pouryazdian2016candecomp,
  author	= {Pouryazdian, Saeed and Beheshti, Soosan and Krishnan,
		  Sridhar},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Digital Signal Processing},
  pages		= {12--26},
  publisher	= {Elsevier},
  title		= {{CANDECOMP/PARAFAC} model order selection based on
		  reconstruction error in the presence of kronecker
		  structured colored noise},
  volume	= {48},
  year		= {2016}
}

@InCollection{	  prechelt1998early,
  title		= {Early stopping-but when?},
  author	= {Prechelt, Lutz},
  booktitle	= {Neural Networks: Tricks of the trade},
  pages		= {55--69},
  year		= {1998},
  publisher	= {Springer}
}

###InCollection{  prechelt1998early,
  title		= {Early stopping-but when?},
  author	= {Prechelt, Lutz},
  booktitle	= {Neural Networks: Tricks of the trade},
  pages		= {55--69},
  year		= {1998},
  publisher	= {Springer}
}

@InProceedings{	  raghu16,
  title		= {On the Expressive Power of Deep Neural Networks},
  author	= {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya
		  Ganguli and Jascha Sohl-Dickstein},
  booktitle	= {Proceedings of the 34th International Conference on
		  Machine Learning},
  pages		= {2847--2854},
  year		= {2017},
  editor	= {Doina Precup and Yee Whye Teh},
  volume	= {70},
  series	= {Proceedings of Machine Learning Research},
  address	= {International Convention Centre, Sydney, Australia},
  month		= {06--11 Aug},
  publisher	= {PMLR},
  pdf		= {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url		= {http://proceedings.mlr.press/v70/raghu17a.html},
  abstract	= {We propose a new approach to the problem of neural network
		  expressivity, which seeks to characterize how structural
		  properties of a neural network family affect the functions
		  it is able to compute. Our approach is based on an
		  interrelated set of measures of expressivity, unified by
		  the novel notion of trajectory length, which measures how
		  the output of a network changes as the input sweeps along a
		  one-dimensional path. Our findings show that: (1) The
		  complexity of the computed function grows exponentially
		  with depth (2) All weights are not equal: trained networks
		  are more sensitive to their lower (initial) layer weights
		  (3) Trajectory regularization is a simpler alternative to
		  batch normalization, with the same performance.}
}

###InProceedings{ raghu16,
  abstract	= {We propose a new approach to the problem of neural network
		  expressivity, which seeks to characterize how structural
		  properties of a neural network family affect the functions
		  it is able to compute. Our approach is based on an
		  interrelated set of measures of expressivity, unified by
		  the novel notion of trajectory length, which measures how
		  the output of a network changes as the input sweeps along a
		  one-dimensional path. Our findings show that: (1) The
		  complexity of the computed function grows exponentially
		  with depth (2) All weights are not equal: trained networks
		  are more sensitive to their lower (initial) layer weights
		  (3) Trajectory regularization is a simpler alternative to
		  batch normalization, with the same performance.},
  address	= {International Convention Centre, Sydney, Australia},
  author	= {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya
		  Ganguli and Jascha Sohl-Dickstein},
  booktitle	= {Proceedings of the 34th International Conference on
		  Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  editor	= {Doina Precup and Yee Whye Teh},
  month		= {06--11 Aug},
  pages		= {2847--2854},
  pdf		= {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  publisher	= {PMLR},
  series	= {Proceedings of Machine Learning Research},
  title		= {On the Expressive Power of Deep Neural Networks},
  url		= {http://proceedings.mlr.press/v70/raghu17a.html},
  volume	= {70},
  year		= {2017},
  bdsk-url-1	= {http://proceedings.mlr.press/v70/raghu17a.html}
}

###InProceedings{ raghu16,
  title		= {On the Expressive Power of Deep Neural Networks},
  author	= {Maithra Raghu and Ben Poole and Jon Kleinberg and Surya
		  Ganguli and Jascha Sohl-Dickstein},
  booktitle	= {Proceedings of the 34th International Conference on
		  Machine Learning},
  pages		= {2847--2854},
  year		= {2017},
  editor	= {Doina Precup and Yee Whye Teh},
  volume	= {70},
  series	= {Proceedings of Machine Learning Research},
  address	= {International Convention Centre, Sydney, Australia},
  month		= {06--11 Aug},
  publisher	= {PMLR},
  pdf		= {http://proceedings.mlr.press/v70/raghu17a/raghu17a.pdf},
  url		= {http://proceedings.mlr.press/v70/raghu17a.html},
  abstract	= {We propose a new approach to the problem of neural network
		  expressivity, which seeks to characterize how structural
		  properties of a neural network family affect the functions
		  it is able to compute. Our approach is based on an
		  interrelated set of measures of expressivity, unified by
		  the novel notion of trajectory length, which measures how
		  the output of a network changes as the input sweeps along a
		  one-dimensional path. Our findings show that: (1) The
		  complexity of the computed function grows exponentially
		  with depth (2) All weights are not equal: trained networks
		  are more sensitive to their lower (initial) layer weights
		  (3) Trajectory regularization is a simpler alternative to
		  batch normalization, with the same performance.}
}

@Article{	  raginsky2017non,
  title		= {Non-convex learning via {Stochastic Gradient Langevin
		  Dynamics}: a nonasymptotic analysis},
  author	= {Raginsky, Maxim and Rakhlin, Alexander and Telgarsky,
		  Matus},
  journal	= {arXiv preprint arXiv:1702.03849},
  year		= {2017}
}

###Article{	  raginsky2017non,
  author	= {Raginsky, Maxim and Rakhlin, Alexander and Telgarsky,
		  Matus},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1702.03849},
  title		= {Non-convex learning via {Stochastic Gradient Langevin
		  Dynamics}: a nonasymptotic analysis},
  year		= {2017}
}

@Article{	  ramji2007refugee,
  title		= {Refugee roulette: Disparities in asylum adjudication},
  author	= {Ramji-Nogales, Jaya and Schoenholtz, Andrew I and Schrag,
		  Philip G},
  journal	= {Stanford Law Review},
  pages		= {295--411},
  year		= {2007},
  publisher	= {JSTOR}
}

###Article{	  ramji2007refugee,
  author	= {Ramji-Nogales, Jaya and Schoenholtz, Andrew I and Schrag,
		  Philip G},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Stanford Law Review},
  pages		= {295--411},
  publisher	= {JSTOR},
  title		= {Refugee roulette: Disparities in asylum adjudication},
  year		= {2007}
}

###Article{	  ramji2007refugee,
  title		= {Refugee roulette: Disparities in asylum adjudication},
  author	= {Ramji-Nogales, Jaya and Schoenholtz, Andrew I and Schrag,
		  Philip G},
  journal	= {Stanford Law Review},
  pages		= {295--411},
  year		= {2007},
  publisher	= {JSTOR}
}

@Misc{		  rcusa,
  title		= {Asylum and Detention.},
  author	= {Refugee Council USA.},
  year		= {2016},
  howpublished	= "\url{http://www.rcusa.org/asylum-and-detention}"
}

###Misc{	  rcusa,
  author	= {Refugee Council USA.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  howpublished	= {\url{http://www.rcusa.org/asylum-and-detention}},
  title		= {Asylum and Detention.},
  year		= {2016}
}

@Misc{		  ref_def,
  title		= {Refugee definition.},
  author	= {US Citizenship and Immigration Services.},
  year		= {2016},
  howpublished	= "\url{https://www.uscis.gov/humanitarian/refugees-asylum/refugees}"
}

###Misc{	  ref_def,
  author	= {US Citizenship and Immigration Services.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  howpublished	= {\url{https://www.uscis.gov/humanitarian/refugees-asylum/refugees}},
  title		= {Refugee definition.},
  year		= {2016}
}

@Article{	  reviewbb,
  title		= {Theoretical perspective on the glass transition and
		  amorphous materials},
  author	= {Berthier, Ludovic and Biroli, Giulio},
  journal	= {Reviews of Modern Physics},
  volume	= {83},
  number	= {2},
  pages		= {587},
  year		= {2011},
  publisher	= {APS}
}

###Article{	  reviewbb,
  author	= {Berthier, Ludovic and Biroli, Giulio},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Reviews of Modern Physics},
  number	= {2},
  pages		= {587},
  publisher	= {APS},
  title		= {Theoretical perspective on the glass transition and
		  amorphous materials},
  volume	= {83},
  year		= {2011}
}

###Article{	  reviewbb,
  title		= {Theoretical perspective on the glass transition and
		  amorphous materials},
  author	= {Berthier, Ludovic and Biroli, Giulio},
  journal	= {Reviews of Modern Physics},
  volume	= {83},
  number	= {2},
  pages		= {587},
  year		= {2011},
  publisher	= {APS}
}

@Article{	  reviewbckm,
  title		= {Out of equilibrium dynamics in spin-glasses and other
		  glassy systems},
  author	= {Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and
		  Kurchan, Jorge and Mezard, Marc},
  journal	= {Spin glasses and random fields},
  pages		= {161--223},
  year		= {1998},
  publisher	= {World Scientific, Singapore}
}

###Article{	  reviewbckm,
  author	= {Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and
		  Kurchan, Jorge and Mezard, Marc},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Spin glasses and random fields},
  pages		= {161--223},
  publisher	= {World Scientific, Singapore},
  title		= {Out of equilibrium dynamics in spin-glasses and other
		  glassy systems},
  year		= {1998}
}

###Article{	  reviewbckm,
  title		= {Out of equilibrium dynamics in spin-glasses and other
		  glassy systems},
  author	= {Bouchaud, Jean-Philippe and Cugliandolo, Leticia F and
		  Kurchan, Jorge and Mezard, Marc},
  journal	= {Spin glasses and random fields},
  pages		= {161--223},
  year		= {1998},
  publisher	= {World Scientific, Singapore}
}

@Article{	  reviewbray,
  title		= {Theory of phase-ordering kinetics},
  author	= {Bray, Alan J},
  journal	= {Advances in Physics},
  volume	= {51},
  number	= {2},
  pages		= {481--587},
  year		= {2002},
  publisher	= {Taylor \& Francis}
}

###Article{	  reviewbray,
  author	= {Bray, Alan J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Advances in Physics},
  number	= {2},
  pages		= {481--587},
  publisher	= {Taylor \& Francis},
  title		= {Theory of phase-ordering kinetics},
  volume	= {51},
  year		= {2002}
}

###Article{	  reviewbray,
  title		= {Theory of phase-ordering kinetics},
  author	= {Bray, Alan J},
  journal	= {Advances in Physics},
  volume	= {51},
  number	= {2},
  pages		= {481--587},
  year		= {2002},
  publisher	= {Taylor {\&} Francis}
}

@InProceedings{	  richard2014statistical,
  title		= {A statistical model for tensor PCA},
  author	= {Richard, Emile and Montanari, Andrea},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {2897--2905},
  year		= {2014}
}

@InProceedings{	  roemer2008closed,
  title		= {A closed-form solution for parallel factor ({PARAFAC})
		  analysis},
  author	= {Roemer, Florian and Haardt, Martin},
  booktitle	= {IEEE International Conference on Acoustics, Speech and
		  Signal Processing, 2008. ICASSP 2008.},
  pages		= {2365--2368},
  year		= {2008},
  organization	= {IEEE}
}

###InProceedings{ roemer2008closed,
  author	= {Roemer, Florian and Haardt, Martin},
  booktitle	= {IEEE International Conference on Acoustics, Speech and
		  Signal Processing, 2008. ICASSP 2008.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {IEEE},
  pages		= {2365--2368},
  title		= {A closed-form solution for parallel factor ({PARAFAC})
		  analysis},
  year		= {2008}
}

@Article{	  rotskoff2018neural,
  title		= {Neural networks as Interacting Particle Systems:
		  Asymptotic convexity of the Loss Landscape and Universal
		  Scaling of the Approximation Error},
  author	= {Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal	= {arXiv preprint arXiv:1805.00915},
  year		= {2018}
}

###Article{	  rotskoff2018neural,
  title		= {Neural networks as Interacting Particle Systems:
		  Asymptotic convexity of the Loss Landscape and Universal
		  Scaling of the Approximation Error},
  author	= {Rotskoff, Grant M and Vanden-Eijnden, Eric},
  journal	= {arXiv preprint arXiv:1805.00915},
  year		= {2018}
}

@Article{	  rottman2009path,
  title		= {The Path to Asylum in the US and the Determinants for Who
		  gets in and Why},
  author	= {Rottman, Andy J and Fariss, Christopher J and Poe, Steven
		  C},
  journal	= {International Migration Review},
  volume	= {43},
  number	= {1},
  pages		= {3--34},
  year		= {2009},
  publisher	= {Wiley Online Library}
}

###Article{	  rottman2009path,
  author	= {Rottman, Andy J and Fariss, Christopher J and Poe, Steven
		  C},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Migration Review},
  number	= {1},
  pages		= {3--34},
  publisher	= {Wiley Online Library},
  title		= {The Path to Asylum in the US and the Determinants for Who
		  gets in and Why},
  volume	= {43},
  year		= {2009}
}

@Article{	  saad1995line,
  title		= {On-line learning in soft committee machines},
  author	= {Saad, David and Solla, Sara A},
  journal	= {Physical Review E},
  volume	= {52},
  number	= {4},
  pages		= {4225},
  year		= {1995},
  publisher	= {APS}
}

###Article{	  saad1995line,
  title		= {On-line learning in soft committee machines},
  author	= {Saad, David and Solla, Sara A},
  journal	= {Physical Review E},
  volume	= {52},
  number	= {4},
  pages		= {4225},
  year		= {1995},
  publisher	= {APS}
}

@Article{	  saad1996dynamics,
  title		= {Dynamics of on-line gradient descent learning for
		  multilayer neural networks},
  author	= {Saad, David and Solla, Sara A},
  journal	= {NIPS},
  year		= {1996}
}

###Article{	  saad1996dynamics,
  author	= {Saad, David and Solla, Sara A},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {NIPS},
  title		= {Dynamics of on-line gradient descent learning for
		  multilayer neural networks},
  year		= {1996}
}

###Article{	  saad1996dynamics,
  title		= {Dynamics of on-line gradient descent learning for
		  multilayer neural networks},
  author	= {Saad, David and Solla, Sara A},
  journal	= {NIPS},
  year		= {1996}
}

@Article{	  sagun16,
  title		= {Singularity of the Hessian in Deep Learning},
  author	= {Sagun, Levent and Bottou, L{\'e}on and LeCun, Yann},
  journal	= {International Conference on Learning Representations},
  year		= {2017}
}

###Article{	  sagun16,
  author	= {Sagun, Levent and Bottou, L{\'e}on and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Singularity of the Hessian in Deep Learning},
  year		= {2017}
}

###Article{	  sagun16,
  title		= {Singularity of the Hessian in Deep Learning},
  author	= {Sagun, Levent and Bottou, {L{\'e}on} and LeCun, Yann},
  journal	= {International Conference on Learning Representations},
  year		= {2017}
}

@Article{	  sagun2014explorations,
  title		= {Explorations on high dimensional landscapes},
  author	= {Sagun, Levent and G\"uney, V. U\u{g}ur and G{\'{e}}rard
		  {Ben Arous} and LeCun, Yann},
  journal	= {ICLR 2015 Workshop Contribution, arXiv:1412.6615},
  year		= {2014}
}

###Article{	  sagun2014explorations,
  author	= {Sagun, Levent and G\"uney, V. U\u{g}ur and G{\'{e}}rard
		  {Ben Arous} and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {ICLR 2015 Workshop Contribution, arXiv:1412.6615},
  title		= {Explorations on high dimensional landscapes},
  year		= {2014}
}

@Article{	  sagun2015universality,
  title		= {Universality in halting time and its applications in
		  optimization},
  author	= {Sagun, Levent and Trogdon, Thomas and LeCun, Yann},
  journal	= {arXiv preprint arXiv:1511.06444},
  year		= {2015}
}

###Article{	  sagun2015universality,
  author	= {Sagun, Levent and Trogdon, Thomas and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1511.06444},
  title		= {Universality in halting time and its applications in
		  optimization},
  year		= {2015}
}

@Article{	  sagun2016gdvssgd,
  title		= {Empirical Study comparing GD and SGD},
  author	= {Sagun, Levent and Bottou, {L\'eon}},
  journal	= {Unpublished work},
  year		= {2016}
}

@Article{	  sagun2016singularity,
  title		= {Singularity of the Hessian in Deep Learning},
  author	= {Sagun, Levent and Bottou, L{\'e}on and LeCun, Yann},
  journal	= {arXiv preprint arXiv:1611.07476},
  year		= {2016}
}

###Article{	  sagun2016singularity,
  author	= {Sagun, Levent and Bottou, L{\'e}on and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1611.07476},
  title		= {Singularity of the Hessian in Deep Learning},
  year		= {2016}
}

@Article{	  sagun2017empirical,
  title		= {Empirical Analysis of the Hessian of Over-Parametrized
		  Neural Networks},
  author	= {Sagun, Levent and Evci, Utku and G\"uney, V. U\u{g}ur and
		  Dauphin, Yann and Bottou, L\'eon},
  journal	= {ICLR 2018 Workshop Contribution, arXiv:1706.04454},
  year		= {2017}
}

###Article{	  sagun2017empirical,
  author	= {Sagun, Levent and Evci, Utku and G\"uney, V. U\u{g}ur and
		  Dauphin, Yann and Bottou, L\'eon},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {ICLR 2018 Workshop Contribution, arXiv:1706.04454},
  title		= {Empirical Analysis of the Hessian of Over-Parametrized
		  Neural Networks},
  year		= {2017}
}

@Article{	  salehyan2008international,
  title		= {International relations, domestic politics, and asylum
		  admissions in the United States},
  author	= {Salehyan, Idean and Rosenblum, Marc R},
  journal	= {Political Research Quarterly},
  volume	= {61},
  number	= {1},
  pages		= {104--121},
  year		= {2008},
  publisher	= {Sage Publications}
}

###Article{	  salehyan2008international,
  author	= {Salehyan, Idean and Rosenblum, Marc R},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Political Research Quarterly},
  number	= {1},
  pages		= {104--121},
  publisher	= {Sage Publications},
  title		= {International relations, domestic politics, and asylum
		  admissions in the United States},
  volume	= {61},
  year		= {2008}
}

@Book{		  sato1999levy,
  title		= {L{\'e}vy processes and infinitely divisible distributions},
  author	= {Sato, K.},
  year		= {1999},
  publisher	= {Cambridge university press}
}

###Book{	  sato1999levy,
  author	= {Sato, K.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  publisher	= {Cambridge university press},
  title		= {L{\'e}vy processes and infinitely divisible distributions},
  year		= {1999}
}

@Article{	  saxe13,
  title		= {Exact solutions to the nonlinear dynamics of learning in
		  deep linear neural networks},
  author	= {Saxe, Andrew M and McClelland, James L and Ganguli,
		  Surya},
  journal	= {International Conference on Learning Representations},
  year		= {2014}
}

###Article{	  saxe13,
  author	= {Saxe, Andrew M and McClelland, James L and Ganguli,
		  Surya},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Exact solutions to the nonlinear dynamics of learning in
		  deep linear neural networks},
  year		= {2014}
}

###Article{	  saxe13,
  title		= {Exact solutions to the nonlinear dynamics of learning in
		  deep linear neural networks},
  author	= {Saxe, Andrew M and McClelland, James L and Ganguli,
		  Surya},
  journal	= {International Conference on Learning Representations},
  year		= {2014}
}

@Article{	  schaul2013no,
  title		= {No more pesky learning rates.},
  author	= {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  journal	= {ICML (3)},
  volume	= {28},
  pages		= {343--351},
  year		= {2013}
}

###Article{	  schaul2013no,
  author	= {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {ICML (3)},
  pages		= {343--351},
  title		= {No more pesky learning rates.},
  volume	= {28},
  year		= {2013}
}

###Article{	  schaul2013no,
  title		= {No more pesky learning rates.},
  author	= {Schaul, Tom and Zhang, Sixin and LeCun, Yann},
  journal	= {ICML (3)},
  volume	= {28},
  pages		= {343--351},
  year		= {2013}
}

@Article{	  schertzer2001fractional,
  title		= {Fractional {Fokker--Planck equation for nonlinear
		  stochastic differential equations driven by non-Gaussian
		  L}{\'e}vy stable noises},
  author	= {Schertzer, D and Larchev{\^e}que, M and Duan, J and
		  Yanovsky, VV and Lovejoy, S},
  journal	= {Journal of Mathematical Physics},
  volume	= {42},
  number	= {1},
  pages		= {200--212},
  year		= {2001},
  publisher	= {AIP}
}

###Article{	  schertzer2001fractional,
  author	= {Schertzer, D and Larchev{\^e}que, M and Duan, J and
		  Yanovsky, VV and Lovejoy, S},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Journal of Mathematical Physics},
  number	= {1},
  pages		= {200--212},
  publisher	= {AIP},
  title		= {Fractional {Fokker--Planck equation for nonlinear
		  stochastic differential equations driven by non-Gaussian
		  L}{\'e}vy stable noises},
  volume	= {42},
  year		= {2001}
}

@Article{	  schmitz2015application,
  title		= {Application of {Parallel Factor Analysis} ({PARAFAC}) to
		  electrophysiological data},
  author	= {Schmitz, S Katharina and Hasselbach, Philipp P and Ebisch,
		  Boris and Klein, Anja and Pipa, Gordon and Galuske, Ralf
		  AW},
  journal	= {Frontiers in neuroinformatics},
  volume	= {8, article 84},
  year		= {2015},
  publisher	= {Frontiers}
}

###Article{	  schmitz2015application,
  author	= {Schmitz, S Katharina and Hasselbach, Philipp P and Ebisch,
		  Boris and Klein, Anja and Pipa, Gordon and Galuske, Ralf
		  AW},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Frontiers in neuroinformatics},
  publisher	= {Frontiers},
  title		= {Application of {Parallel Factor Analysis} ({PARAFAC}) to
		  electrophysiological data},
  volume	= {8, article 84},
  year		= {2015}
}

@Article{	  shang1996,
  author	= {Shang, Y I and Wah, Benjamin W},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/10.1.1.1.6147.pdf:pdf},
  journal	= {IEEE Computer},
  number	= {3},
  pages		= {45--54},
  title		= {{Global Optimization for Neural Network Training}},
  volume	= {29},
  year		= {1996}
}

###Article{	  shang1996,
  author	= {Shang, Y I and Wah, Benjamin W},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/10.1.1.1.6147.pdf:pdf},
  journal	= {IEEE Computer},
  number	= {3},
  pages		= {45--54},
  title		= {{Global Optimization for Neural Network Training}},
  volume	= {29},
  year		= {1996}
}

@InProceedings{	  shang2015covariance,
  title		= {Covariance-Controlled Adaptive Langevin Thermostat for
		  Large-Scale Bayesian Sampling},
  author	= {Shang, X. and Zhu, Z. and Leimkuhler, B. and Storkey, A.
		  J.},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {37--45},
  year		= {2015}
}

###InProceedings{ shang2015covariance,
  author	= {Shang, X. and Zhu, Z. and Leimkuhler, B. and Storkey, A.
		  J.},
  booktitle	= {Advances in Neural Information Processing Systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {37--45},
  title		= {Covariance-Controlled Adaptive Langevin Thermostat for
		  Large-Scale Bayesian Sampling},
  year		= {2015}
}

@InProceedings{	  shashua2005non,
  title		= {Non-negative tensor factorization with applications to
		  statistics and computer vision},
  author	= {Shashua, Amnon and Hazan, Tamir},
  booktitle	= {Proceedings of the 22nd international conference on
		  Machine learning},
  pages		= {792--799},
  year		= {2005},
  organization	= {ACM}
}

###InProceedings{ shashua2005non,
  author	= {Shashua, Amnon and Hazan, Tamir},
  booktitle	= {Proceedings of the 22nd international conference on
		  Machine learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {ACM},
  pages		= {792--799},
  title		= {Non-negative tensor factorization with applications to
		  statistics and computer vision},
  year		= {2005}
}

@InCollection{	  sherrington2014,
  address	= {Cham},
  author	= {Sherrington, David},
  booktitle	= {Managing Complexity, Reducing Perplexity},
  doi		= {10.1007/978-3-319-03759-2},
  editor	= {Delitala, Marcello and {Ajmone Marsan}, Giulia},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/chp\%3A10.1007\%2F978-3-319-03759-2\_13.pdf:pdf},
  isbn		= {978-3-319-03758-5},
  keywords	= {complex systems,conceptual transfers through
		  mathematics,econophysics,hard optimization,neural
		  networks,spin glasses},
  pages		= {119--129},
  publisher	= {Springer International Publishing},
  series	= {Springer Proceedings in Mathematics \& Statistics},
  title		= {{Physics and Complexity: An Introduction}},
  url		= {http://link.springer.com/10.1007/978-3-319-03759-2},
  volume	= {67},
  year		= {2014}
}

###InCollection{  sherrington2014,
  address	= {Cham},
  author	= {Sherrington, David},
  booktitle	= {Managing Complexity, Reducing Perplexity},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1007/978-3-319-03759-2},
  editor	= {Delitala, Marcello and {Ajmone Marsan}, Giulia},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/chp\%3A10.1007\%2F978-3-319-03759-2\_13.pdf:pdf},
  isbn		= {978-3-319-03758-5},
  keywords	= {complex systems,conceptual transfers through
		  mathematics,econophysics,hard optimization,neural
		  networks,spin glasses},
  pages		= {119--129},
  publisher	= {Springer International Publishing},
  series	= {Springer Proceedings in Mathematics \& Statistics},
  title		= {{Physics and Complexity: An Introduction}},
  url		= {http://link.springer.com/10.1007/978-3-319-03759-2},
  volume	= {67},
  year		= {2014},
  bdsk-url-1	= {http://link.springer.com/10.1007/978-3-319-03759-2},
  bdsk-url-2	= {https://doi.org/10.1007/978-3-319-03759-2}
}

@Article{	  shwartz2017opening,
  title		= {Opening the Black Box of Deep Neural Networks via
		  Information},
  author	= {Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal	= {arXiv preprint arXiv:1703.00810},
  year		= {2017}
}

###Article{	  shwartz2017opening,
  author	= {Shwartz-Ziv, Ravid and Tishby, Naftali},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1703.00810},
  title		= {Opening the Black Box of Deep Neural Networks via
		  Information},
  year		= {2017}
}

###Article{	  shwartz2017opening,
  title		= {Opening the Black Box of Deep Neural Networks via
		  Information},
  author	= {Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal	= {arXiv preprint arXiv:1703.00810},
  year		= {2017}
}

@Article{	  sidiropoulos2000blind,
  title		= {Blind {PARAFAC} receivers for {DS-CDMA} systems},
  author	= {Sidiropoulos, Nicholas D and Giannakis, Georgios B and
		  Bro, Rasmus},
  journal	= {IEEE Transactions on Signal Processing},
  volume	= {48},
  number	= {3},
  pages		= {810--823},
  year		= {2000},
  publisher	= {IEEE}
}

###Article{	  sidiropoulos2000blind,
  author	= {Sidiropoulos, Nicholas D and Giannakis, Georgios B and
		  Bro, Rasmus},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {IEEE Transactions on Signal Processing},
  number	= {3},
  pages		= {810--823},
  publisher	= {IEEE},
  title		= {Blind {PARAFAC} receivers for {DS-CDMA} systems},
  volume	= {48},
  year		= {2000}
}

@InProceedings{	  sidiropoulos2004low,
  title		= {Low-rank decomposition of multi-way arrays: A signal
		  processing perspective},
  author	= {Sidiropoulos, Nikos D},
  booktitle	= {Sensor Array and Multichannel Signal Processing Workshop
		  Proceedings, 2004},
  pages		= {52--58},
  year		= {2004},
  organization	= {IEEE}
}

###InProceedings{ sidiropoulos2004low,
  author	= {Sidiropoulos, Nikos D},
  booktitle	= {Sensor Array and Multichannel Signal Processing Workshop
		  Proceedings, 2004},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  organization	= {IEEE},
  pages		= {52--58},
  title		= {Low-rank decomposition of multi-way arrays: A signal
		  processing perspective},
  year		= {2004}
}

@Article{	  silbert05,
  author	= {L. E. Silbert and A. J. Liu and S. R. Nagel},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  journal	= prl,
  pages		= {098301},
  title		= {Vibrations and Diverging Length Scales Near the Unjamming
		  Transition},
  volume	= {95},
  year		= {2005}
}

###Article{	  silbert05,
  author	= {L. E. Silbert and A. J. Liu and S. R. Nagel},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= prl,
  pages		= {098301},
  title		= {Vibrations and Diverging Length Scales Near the Unjamming
		  Transition},
  volume	= {95},
  year		= {2005}
}

###Article{	  silbert05,
  author	= {L. E. Silbert and A. J. Liu and S. R. Nagel},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  journal	= prl,
  pages		= {098301},
  title		= {Vibrations and Diverging Length Scales Near the Unjamming
		  Transition},
  volume	= {95},
  year		= {2005}
}

@Article{	  silver16,
  title		= {Mastering the game of Go with deep neural networks and
		  tree search},
  author	= {Silver, David and Huang, Aja and Maddison, Chris J and
		  Guez, Arthur and Sifre, Laurent and Van Den Driessche,
		  George and Schrittwieser, Julian and Antonoglou, Ioannis
		  and Panneershelvam, Veda and Lanctot, Marc and others},
  journal	= {Nature},
  volume	= {529},
  number	= {7587},
  pages		= {484},
  year		= {2016},
  publisher	= {Nature Publishing Group}
}

###Article{	  silver16,
  author	= {Silver, David and Huang, Aja and Maddison, Chris J and
		  Guez, Arthur and Sifre, Laurent and Van Den Driessche,
		  George and Schrittwieser, Julian and Antonoglou, Ioannis
		  and Panneershelvam, Veda and Lanctot, Marc and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Nature},
  number	= {7587},
  pages		= {484},
  publisher	= {Nature Publishing Group},
  title		= {Mastering the game of Go with deep neural networks and
		  tree search},
  volume	= {529},
  year		= {2016}
}

###Article{	  silver16,
  title		= {Mastering the game of Go with deep neural networks and
		  tree search},
  author	= {Silver, David and Huang, Aja and Maddison, Chris J and
		  Guez, Arthur and Sifre, Laurent and Van Den Driessche,
		  George and Schrittwieser, Julian and Antonoglou, Ioannis
		  and Panneershelvam, Veda and Lanctot, Marc and others},
  journal	= {Nature},
  volume	= {529},
  number	= {7587},
  pages		= {484},
  year		= {2016},
  publisher	= {Nature Publishing Group}
}

@Article{	  silver17,
  title		= {Mastering the game of Go without human knowledge},
  author	= {Silver, David and Schrittwieser, Julian and Simonyan,
		  Karen and Antonoglou, Ioannis and Huang, Aja and Guez,
		  Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew
		  and Bolton, Adrian and others},
  journal	= {Nature},
  volume	= {550},
  number	= {7676},
  pages		= {354},
  year		= {2017},
  publisher	= {Nature Publishing Group}
}

###Article{	  silver17,
  author	= {Silver, David and Schrittwieser, Julian and Simonyan,
		  Karen and Antonoglou, Ioannis and Huang, Aja and Guez,
		  Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew
		  and Bolton, Adrian and others},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Nature},
  number	= {7676},
  pages		= {354},
  publisher	= {Nature Publishing Group},
  title		= {Mastering the game of Go without human knowledge},
  volume	= {550},
  year		= {2017}
}

###Article{	  silver17,
  title		= {Mastering the game of Go without human knowledge},
  author	= {Silver, David and Schrittwieser, Julian and Simonyan,
		  Karen and Antonoglou, Ioannis and Huang, Aja and Guez,
		  Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew
		  and Bolton, Adrian and others},
  journal	= {Nature},
  volume	= {550},
  number	= {7676},
  pages		= {354},
  year		= {2017},
  publisher	= {Nature Publishing Group}
}

@Book{		  simon,
  author	= {Simon, Herbert A},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Kun\_Herbert
		  Simon\_Sciences\_of\_the\_Artificial.pdf:pdf},
  isbn		= {9780262193740},
  title		= {{The Sciences of the Artificial Third edition}}
}

###Book{	  simon,
  author	= {Simon, Herbert A},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/Kun\_Herbert
		  Simon\_Sciences\_of\_the\_Artificial.pdf:pdf},
  isbn		= {9780262193740},
  title		= {{The Sciences of the Artificial Third edition}}
}

@InProceedings{	  simsekli2016hamcmc,
  title		= {Stochastic quasi-{N}ewton {L}angevin {M}onte {Carlo}},
  author	= {\c{S}im\c{s}ekli, U. and Badeau, R. and Cemgil, A. T. and
		  Richard, G.},
  booktitle	= {ICML},
  year		= {2016}
}

###InProceedings{ simsekli2016hamcmc,
  author	= {\c{S}im\c{s}ekli, U. and Badeau, R. and Cemgil, A. T. and
		  Richard, G.},
  booktitle	= {ICML},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  title		= {Stochastic quasi-{N}ewton {L}angevin {M}onte {Carlo}},
  year		= {2016}
}

@InProceedings{	  simsekli2017parallelized,
  title		= {Parallelized Stochastic Gradient {Markov chain Monte
		  Carlo} Algorithms for Non-Negative Matrix Factorization},
  author	= {\c{S}im\c{s}ekli, U. and Durmus, A. and Badeau, R. and
		  Richard, G. and Moulines, E. and Cemgil, A. T.},
  booktitle	= {ICASSP},
  pages		= {2242--2246},
  year		= {2017}
}

###InProceedings{ simsekli2017parallelized,
  author	= {\c{S}im\c{s}ekli, U. and Durmus, A. and Badeau, R. and
		  Richard, G. and Moulines, E. and Cemgil, A. T.},
  booktitle	= {ICASSP},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {2242--2246},
  title		= {Parallelized Stochastic Gradient {Markov chain Monte
		  Carlo} Algorithms for Non-Negative Matrix Factorization},
  year		= {2017}
}

@PhDThesis{	  simsekliphdthesis,
  author	= {\c{S}im\c{s}ekli, U.},
  title		= {{Tensor Fusion: Learning in Heterogeneous and Distributed
		  Data}},
  school	= {Computer Engineering, Bo\u{g}azi\c ci University},
  year		= {2015}
}

###PhDThesis{	  simsekliphdthesis,
  author	= {\c{S}im\c{s}ekli, U.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  school	= {Computer Engineering, Bo\u{g}azi\c ci University},
  title		= {{Tensor Fusion: Learning in Heterogeneous and Distributed
		  Data}},
  year		= {2015}
}

@Article{	  simseklipsgld,
  author	= {U. \c{S}im\c{s}ekli and H. Koptagel and H. G\"{u}lda\c{s}
		  and A. T. Cemgil and F. \"{O}ztoprak and \c{S}. \.{I}. Birbil},
  title		= {Parallel Stochastic Gradient {Markov chain Monte Carlo}
		  for Matrix Factorisation Models},
  journal	= {arXiv preprint arXiv:1506.01418},
  year		= {2015},
  url		= {http://arxiv.org/abs/1506.01418}
}

###Article{	  simseklipsgld,
  author	= {U. \c{S}im\c{s}ekli and H. Koptagel and H. G\"{u}lda\c{s}
		  and A. T. Cemgil and F. \"{O}ztoprak and \c{S}. \.{I}. Birbil},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1506.01418},
  title		= {Parallel Stochastic Gradient {Markov chain Monte Carlo}
		  for Matrix Factorisation Models},
  url		= {http://arxiv.org/abs/1506.01418},
  year		= {2015},
  bdsk-url-1	= {http://arxiv.org/abs/1506.01418}
}

@Article{	  sirignano2018mean,
  title		= {Mean Field Analysis of Neural Networks},
  author	= {Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal	= {arXiv preprint arXiv:1805.01053},
  year		= {2018}
}

###Article{	  sirignano2018mean,
  title		= {Mean Field Analysis of Neural Networks},
  author	= {Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal	= {arXiv preprint arXiv:1805.01053},
  year		= {2018}
}

@Article{	  sirignano2018mean2,
  title		= {Mean Field Analysis of Neural Networks: A Central Limit
		  Theorem},
  author	= {Sirignano, Justin and Spiliopoulos, Konstantinos},
  journal	= {arXiv preprint arXiv:1808.09372},
  year		= {2018}
}

@InProceedings{	  smaragdis03,
  author	= {P. Smaragdis and J. C. Brown},
  title		= {Non-Negative Matrix Factorization for Polyphonic Music
		  Transcription},
  booktitle	= {IEEE Workshop on Applications of Signal Processing to
		  Audio and Acoustics},
  year		= {2003},
  pages		= {177--180},
  month		= oct
}

###InProceedings{ smaragdis03,
  author	= {P. Smaragdis and J. C. Brown},
  booktitle	= {IEEE Workshop on Applications of Signal Processing to
		  Audio and Acoustics},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= oct,
  pages		= {177--180},
  title		= {Non-Negative Matrix Factorization for Polyphonic Music
		  Transcription},
  year		= {2003}
}

@Article{	  smith2017don,
  title		= {Don't decay the learning rate, increase the batch size},
  author	= {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris
		  and Le, Quoc V},
  journal	= {arXiv preprint arXiv:1711.00489},
  year		= {2017}
}

@Article{	  soudry2016,
  title		= {No bad local minima: Data independent training error
		  guarantees for multilayer neural networks},
  author	= {Soudry, Daniel and Carmon, Yair},
  journal	= {arXiv preprint arXiv:1605.08361},
  year		= {2016}
}

###Article{	  soudry2016,
  author	= {Soudry, Daniel and Carmon, Yair},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1605.08361},
  title		= {No bad local minima: Data independent training error
		  guarantees for multilayer neural networks},
  year		= {2016}
}

###Article{	  soudry2016,
  title		= {No bad local minima: Data independent training error
		  guarantees for multilayer neural networks},
  author	= {Soudry, Daniel and Carmon, Yair},
  journal	= {arXiv preprint arXiv:1605.08361},
  year		= {2016}
}

@Article{	  soudry2016no,
  title		= {No bad local minima: Data independent training error
		  guarantees for multilayer neural networks},
  author	= {Soudry, Daniel and Carmon, Yair},
  journal	= {arXiv preprint arXiv:1605.08361},
  year		= {2016}
}

###Article{	  soudry2016no,
  author	= {Soudry, Daniel and Carmon, Yair},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1605.08361},
  title		= {No bad local minima: Data independent training error
		  guarantees for multilayer neural networks},
  year		= {2016}
}

@Article{	  soudry2018implicit,
  title		= {The implicit bias of gradient descent on separable data},
  author	= {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel
		  and Gunasekar, Suriya and Srebro, Nathan},
  journal	= {Journal of Machine Learning Research},
  volume	= {19},
  number	= {70},
  year		= {2018}
}

@Article{	  spigler18,
  title		= {A jamming transition from under-to over-parametrization
		  affects loss landscape and generalization},
  author	= {Spigler, Stefano and Geiger, Mario and d'Ascoli,
		  St{\'e}phane and Sagun, Levent and Biroli, Giulio and
		  Wyart, Matthieu},
  journal	= {arXiv preprint arXiv:1810.09665},
  year		= {2018}
}

@Article{	  spigler2018jamming,
  title		= {A jamming transition from under-to over-parametrization
		  affects loss landscape and generalization},
  author	= {Spigler, Stefano and Geiger, Mario and d'Ascoli,
		  St{\'e}phane and Sagun, Levent and Biroli, Giulio and
		  Wyart, Matthieu},
  journal	= {NIPS Workshop ``Integration of Deep Learning Theories''
		  contribution, arXiv preprint arXiv:1810.09665},
  year		= {2018}
}

###Article{	  spigler2018jamming,
  title		= {A jamming transition from under-to over-parametrization
		  affects loss landscape and generalization},
  author	= {Spigler, Stefano and Geiger, Mario and d'Ascoli,
		  St{\'e}phane and Sagun, Levent and Biroli, Giulio and
		  Wyart, Matthieu},
  journal	= {arXiv preprint arXiv:1810.09665},
  year		= {2018}
}

@Article{	  srivastava2014dropout,
  title		= {Dropout: a simple way to prevent neural networks from
		  overfitting},
  author	= {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky,
		  Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal	= {The Journal of Machine Learning Research},
  volume	= {15},
  number	= {1},
  pages		= {1929--1958},
  year		= {2014},
  publisher	= {JMLR. org}
}

###Article{	  srivastava2014dropout,
  title		= {Dropout: a simple way to prevent neural networks from
		  overfitting},
  author	= {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky,
		  Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  journal	= {The Journal of Machine Learning Research},
  volume	= {15},
  number	= {1},
  pages		= {1929--1958},
  year		= {2014},
  publisher	= {JMLR. org}
}

@InProceedings{	  sutskever2013importance,
  title		= {On the importance of initialization and momentum in deep
		  learning},
  author	= {Sutskever, Ilya and Martens, James and Dahl, George and
		  Hinton, Geoffrey},
  booktitle	= {International conference on machine learning},
  pages		= {1139--1147},
  year		= {2013}
}

###InProceedings{ sutskever2013importance,
  author	= {Sutskever, Ilya and Martens, James and Dahl, George and
		  Hinton, Geoffrey},
  booktitle	= {International conference on machine learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  pages		= {1139--1147},
  title		= {On the importance of initialization and momentum in deep
		  learning},
  year		= {2013}
}

###InProceedings{ sutskever2013importance,
  title		= {On the importance of initialization and momentum in deep
		  learning},
  author	= {Sutskever, Ilya and Martens, James and Dahl, George and
		  Hinton, Geoffrey},
  booktitle	= {International conference on machine learning},
  pages		= {1139--1147},
  year		= {2013}
}

@Article{	  tehthivol2014a,
  author	= {Teh, Y. W. and Thi{\'e}ry, A. and Vollmer, S.},
  title		= {Consistency and fluctuations for stochastic gradient
		  {L}angevin dynamics},
  journal	= {arXiv preprint arXiv:1409.0578},
  year		= {2014}
}

###Article{	  tehthivol2014a,
  author	= {Teh, Y. W. and Thi{\'e}ry, A. and Vollmer, S.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {arXiv preprint arXiv:1409.0578},
  title		= {Consistency and fluctuations for stochastic gradient
		  {L}angevin dynamics},
  year		= {2014}
}

@Article{	  theorem,
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/RMTGumbel.pdf:pdf},
  title		= {{Random Universality: Random Matrix Theory and Extreme
		  Value Statistics}}
}

###Article{	  theorem,
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/RMTGumbel.pdf:pdf},
  title		= {{Random Universality: Random Matrix Theory and Extreme
		  Value Statistics}}
}

@Article{	  thompson1971eigenvalues,
  title		= {On the eigenvalues of sums of Hermitian matrices},
  author	= {Thompson, Robert C and Freede, Linda J},
  journal	= {Linear Algebra and Its Applications},
  volume	= {4},
  number	= {4},
  pages		= {369--376},
  year		= {1971},
  publisher	= {Elsevier}
}

###Article{	  thompson1971eigenvalues,
  author	= {Thompson, Robert C and Freede, Linda J},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Linear Algebra and Its Applications},
  number	= {4},
  pages		= {369--376},
  publisher	= {Elsevier},
  title		= {On the eigenvalues of sums of Hermitian matrices},
  volume	= {4},
  year		= {1971}
}

###Article{	  thompson1971eigenvalues,
  title		= {On the eigenvalues of sums of Hermitian matrices},
  author	= {Thompson, Robert C and Freede, Linda J},
  journal	= {Linear Algebra and Its Applications},
  volume	= {4},
  number	= {4},
  pages		= {369--376},
  year		= {1971},
  publisher	= {Elsevier}
}

@Article{	  tkachenko99,
  author	= {Tkachenko, Alexei V. and Witten, Thomas A.},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  doi		= {10.1103/PhysRevE.60.687},
  journal	= {Phys. Rev. E},
  month		= {Jul},
  number	= {1},
  numpages	= {9},
  pages		= {687--696},
  publisher	= {American Physical Society},
  title		= {Stress propagation through frictionless granular
		  material},
  volume	= {60},
  year		= {1999},
  bdsk-url-1	= {http://dx.doi.org/10.1103/PhysRevE.60.687}
}

###Article{	  tkachenko99,
  author	= {Tkachenko, Alexei V. and Witten, Thomas A.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevE.60.687},
  journal	= {Phys. Rev. E},
  month		= {Jul},
  number	= {1},
  numpages	= {9},
  pages		= {687--696},
  publisher	= {American Physical Society},
  title		= {Stress propagation through frictionless granular
		  material},
  volume	= {60},
  year		= {1999},
  bdsk-url-1	= {http://dx.doi.org/10.1103/PhysRevE.60.687}
}

###Article{	  tkachenko99,
  author	= {Tkachenko, Alexei V. and Witten, Thomas A.},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  doi		= {10.1103/PhysRevE.60.687},
  journal	= {Phys. Rev. E},
  month		= {Jul},
  number	= {1},
  numpages	= {9},
  pages		= {687--696},
  publisher	= {American Physical Society},
  title		= {Stress propagation through frictionless granular
		  material},
  volume	= {60},
  year		= {1999},
  bdsk-url-1	= {http://dx.doi.org/10.1103/PhysRevE.60.687}
}

@Article{	  update2014,
  author	= {Update, Social Expenditure},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/OECD2014-Social-Expenditure-Update-Nov2014-8pages.pdf:pdf},
  number	= {November},
  pages		= {1--8},
  title		= {{Social spending is falling in some countries , but in
		  many others it remains at historically high levels}},
  year		= {2014}
}

###Article{	  update2014,
  author	= {Update, Social Expenditure},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/OECD2014-Social-Expenditure-Update-Nov2014-8pages.pdf:pdf},
  number	= {November},
  pages		= {1--8},
  title		= {{Social spending is falling in some countries , but in
		  many others it remains at historically high levels}},
  year		= {2014}
}

@Article{	  urban2016deep,
  title		= {Do deep convolutional nets really need to be deep and
		  convolutional?},
  author	= {Urban, Gregor and Geras, Krzysztof J and Kahou, Samira
		  Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana,
		  Rich and Mohamed, Abdelrahman and Philipose, Matthai and
		  Richardson, Matt},
  journal	= {arXiv preprint arXiv:1603.05691},
  year		= {2016}
}

###Article{	  urban2016deep,
  title		= {Do deep convolutional nets really need to be deep and
		  convolutional?},
  author	= {Urban, Gregor and Geras, Krzysztof J and Kahou, Samira
		  Ebrahimi and Aslan, Ozlem and Wang, Shengjie and Caruana,
		  Rich and Mohamed, Abdelrahman and Philipose, Matthai and
		  Richardson, Matt},
  journal	= {arXiv preprint arXiv:1603.05691},
  year		= {2016}
}

@Misc{		  uscis,
  title		= {Obtaining Asylum in the United States.},
  author	= {US Citizenship and Immigration Services.},
  year		= {2015},
  howpublished	= "\url{https://www.uscis.gov/humanitarian/refugees-asylum/asylum/obtaining-asylum-united-states}"
}

###Misc{	  uscis,
  author	= {US Citizenship and Immigration Services.},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  howpublished	= {\url{https://www.uscis.gov/humanitarian/refugees-asylum/asylum/obtaining-asylum-united-states}},
  title		= {Obtaining Asylum in the United States.},
  year		= {2015}
}

@Article{	  vanden-eijnden2008,
  abstract	= {Milestoning is a procedure to compute the time evolution
		  of complicated processes such as barrier crossing events or
		  long diffusive transitions between predefined states.
		  Milestoning reduces the dynamics to transition events
		  between intermediates (the milestones) and computes the
		  local kinetic information to describe these transitions via
		  short molecular dynamics (MD) runs between the milestones.
		  The procedure relies on the ability to reinitialize MD
		  trajectories on the milestones to get the right kinetic
		  information about the transitions. It also rests on the
		  assumptions that the transition events between successive
		  milestones and the time lags between these transitions are
		  statistically independent. In this paper, we analyze the
		  validity of these assumptions. We show that sets of optimal
		  milestones exist, i.e., sets such that successive
		  transitions are indeed statistically independent. The proof
		  of this claim relies on the results of transition path
		  theory and uses the isocommittor surfaces of the reaction
		  as milestones. For systems in the overdamped limit, we also
		  obtain the probability distribution to reinitialize the MD
		  trajectories on the milestones, and we discuss why this
		  distribution is not available in closed form for systems
		  with inertia. We explain why the time lags between
		  transitions are not statistically independent even for
		  optimal milestones, but we show that working with such
		  milestones allows one to compute mean first passage times
		  between milestones exactly. Finally, we discuss some
		  practical implications of our results and we compare
		  milestoning with Markov state models in view of our
		  findings.},
  author	= {Vanden-Eijnden, Eric and Venturoli, Maddalena and
		  Ciccotti, Giovanni and Elber, Ron},
  doi		= {10.1063/1.2996509},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/2008+Vanden-Eijnden.pdf:pdf},
  issn		= {1089-7690},
  journal	= {The Journal of chemical physics},
  keywords	= {Kinetics,Models, Chemical,Reproducibility of
		  Results,Surface Properties,Time Factors},
  month		= nov,
  number	= {17},
  pages		= {174102},
  pmid		= {19045328},
  title		= {{On the assumptions underlying milestoning.}},
  url		= {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2646510\&tool=pmcentrez\&rendertype=abstract},
  volume	= {129},
  year		= {2008}
}

###Article{	  vanden-eijnden2008,
  abstract	= {Milestoning is a procedure to compute the time evolution
		  of complicated processes such as barrier crossing events or
		  long diffusive transitions between predefined states.
		  Milestoning reduces the dynamics to transition events
		  between intermediates (the milestones) and computes the
		  local kinetic information to describe these transitions via
		  short molecular dynamics (MD) runs between the milestones.
		  The procedure relies on the ability to reinitialize MD
		  trajectories on the milestones to get the right kinetic
		  information about the transitions. It also rests on the
		  assumptions that the transition events between successive
		  milestones and the time lags between these transitions are
		  statistically independent. In this paper, we analyze the
		  validity of these assumptions. We show that sets of optimal
		  milestones exist, i.e., sets such that successive
		  transitions are indeed statistically independent. The proof
		  of this claim relies on the results of transition path
		  theory and uses the isocommittor surfaces of the reaction
		  as milestones. For systems in the overdamped limit, we also
		  obtain the probability distribution to reinitialize the MD
		  trajectories on the milestones, and we discuss why this
		  distribution is not available in closed form for systems
		  with inertia. We explain why the time lags between
		  transitions are not statistically independent even for
		  optimal milestones, but we show that working with such
		  milestones allows one to compute mean first passage times
		  between milestones exactly. Finally, we discuss some
		  practical implications of our results and we compare
		  milestoning with Markov state models in view of our
		  findings.},
  author	= {Vanden-Eijnden, Eric and Venturoli, Maddalena and
		  Ciccotti, Giovanni and Elber, Ron},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1063/1.2996509},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/2008+Vanden-Eijnden.pdf:pdf},
  issn		= {1089-7690},
  journal	= {The Journal of chemical physics},
  keywords	= {Kinetics,Models, Chemical,Reproducibility of
		  Results,Surface Properties,Time Factors},
  month		= nov,
  number	= {17},
  pages		= {174102},
  pmid		= {19045328},
  title		= {{On the assumptions underlying milestoning.}},
  url		= {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2646510\&tool=pmcentrez\&rendertype=abstract},
  volume	= {129},
  year		= {2008},
  bdsk-url-1	= {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2646510%5C&tool=pmcentrez%5C&rendertype=abstract},
  bdsk-url-2	= {https://doi.org/10.1063/1.2996509}
}

@Article{	  vanden-eijnden2009,
  abstract	= {A new milestoning procedure using Voronoi tessellations is
		  proposed. In the new procedure, the edges of Voronoi cells
		  are used as milestones, and the necessary kinetic
		  information about the transitions between the milestones is
		  calculated by running molecular dynamics (MD) simulations
		  restricted to these cells. Like the traditional milestoning
		  technique, the new procedure offers a reduced description
		  of the original dynamics and permits to efficiently compute
		  the various quantities necessary in this description.
		  However, unlike traditional milestoning, the new procedure
		  does not require to reinitialize trajectories from the
		  milestones, and thereby it avoids the approximation made in
		  traditional milestoning that the distribution for
		  reinitialization is the equilibrium one. In this paper we
		  concentrate on Markovian milestoning, which we show to be
		  valid under suitable assumptions, and we explain how to
		  estimate the rate matrix of transitions between the
		  milestones from data collected from the MD trajectories in
		  the Voronoi cells. The rate matrix can then be used to
		  compute mean first passage times between milestones and
		  reaction rates. The procedure is first illustrated on
		  test-case examples in two dimensions and then applied to
		  study the kinetics of protein insertion into a lipid
		  bilayer by means of a coarse-grained model.},
  author	= {Vanden-Eijnden, Eric and Venturoli, Maddalena},
  doi		= {10.1063/1.3129843},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/2009+Vanden-Eijnden-1.pdf:pdf},
  issn		= {1089-7690},
  journal	= {The Journal of chemical physics},
  keywords	= {Algorithms,Computer Simulation,Lipid Bilayers,Lipid
		  Bilayers: chemistry,Lipid Bilayers: metabolism,Markov
		  Chains,Membrane Proteins,Membrane Proteins:
		  chemistry,Membrane Proteins: metabolism,Models,
		  Molecular,Molecular Conformation,Time Factors},
  month		= may,
  number	= {19},
  pages		= {194101},
  pmid		= {19466815},
  title		= {{Markovian milestoning with Voronoi tessellations.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/19466815},
  volume	= {130},
  year		= {2009}
}

###Article{	  vanden-eijnden2009,
  abstract	= {A new milestoning procedure using Voronoi tessellations is
		  proposed. In the new procedure, the edges of Voronoi cells
		  are used as milestones, and the necessary kinetic
		  information about the transitions between the milestones is
		  calculated by running molecular dynamics (MD) simulations
		  restricted to these cells. Like the traditional milestoning
		  technique, the new procedure offers a reduced description
		  of the original dynamics and permits to efficiently compute
		  the various quantities necessary in this description.
		  However, unlike traditional milestoning, the new procedure
		  does not require to reinitialize trajectories from the
		  milestones, and thereby it avoids the approximation made in
		  traditional milestoning that the distribution for
		  reinitialization is the equilibrium one. In this paper we
		  concentrate on Markovian milestoning, which we show to be
		  valid under suitable assumptions, and we explain how to
		  estimate the rate matrix of transitions between the
		  milestones from data collected from the MD trajectories in
		  the Voronoi cells. The rate matrix can then be used to
		  compute mean first passage times between milestones and
		  reaction rates. The procedure is first illustrated on
		  test-case examples in two dimensions and then applied to
		  study the kinetics of protein insertion into a lipid
		  bilayer by means of a coarse-grained model.},
  author	= {Vanden-Eijnden, Eric and Venturoli, Maddalena},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1063/1.3129843},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/2009+Vanden-Eijnden-1.pdf:pdf},
  issn		= {1089-7690},
  journal	= {The Journal of chemical physics},
  keywords	= {Algorithms,Computer Simulation,Lipid Bilayers,Lipid
		  Bilayers: chemistry,Lipid Bilayers: metabolism,Markov
		  Chains,Membrane Proteins,Membrane Proteins:
		  chemistry,Membrane Proteins: metabolism,Models,
		  Molecular,Molecular Conformation,Time Factors},
  month		= may,
  number	= {19},
  pages		= {194101},
  pmid		= {19466815},
  title		= {{Markovian milestoning with Voronoi tessellations.}},
  url		= {http://www.ncbi.nlm.nih.gov/pubmed/19466815},
  volume	= {130},
  year		= {2009},
  bdsk-url-1	= {http://www.ncbi.nlm.nih.gov/pubmed/19466815},
  bdsk-url-2	= {https://doi.org/10.1063/1.3129843}
}

@Article{	  vapnik1994measuring,
  title		= {Measuring the VC-dimension of a learning machine},
  author	= {Vapnik, Vladimir and Levin, Esther and Cun, Yann Le},
  journal	= {Neural computation},
  volume	= {6},
  number	= {5},
  pages		= {851--876},
  year		= {1994},
  publisher	= {MIT Press}
}

@Article{	  venturi2018neural,
  title		= {Neural Networks with Finite Intrinsic Dimension have no
		  Spurious Valleys},
  author	= {Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal	= {arXiv preprint arXiv:1802.06384},
  year		= {2018}
}

###Article{	  venturi2018neural,
  title		= {Neural Networks with Finite Intrinsic Dimension have no
		  Spurious Valleys},
  author	= {Venturi, Luca and Bandeira, Afonso and Bruna, Joan},
  journal	= {arXiv preprint arXiv:1802.06384},
  year		= {2018}
}

@Article{	  villani2003topics,
  title		= {Topics in optimal transportation, volume 58 of{G}raduate},
  author	= {Villani, C{\'e}dric},
  journal	= {Studies in Mathematics},
  year		= {2003}
}

###Article{	  villani2003topics,
  author	= {Villani, C{\'e}dric},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {Studies in Mathematics},
  title		= {Topics in optimal transportation, volume 58 of{G}raduate},
  year		= {2003}
}

@InCollection{	  vincent2007first,
  author	= {Vincent, E. and Sawada, H. and Bofill, P. and Makino, S.
		  and Rosca, J. P.},
  title		= {First stereo audio source separation evaluation campaign:
		  data, algorithms and results},
  booktitle	= {Independent Component Analysis and Signal Separation},
  publisher	= {Springer},
  year		= {2007},
  pages		= {552--559},
  address	= london,
  month		= sep
}

###InCollection{  vincent2007first,
  address	= london,
  author	= {Vincent, E. and Sawada, H. and Bofill, P. and Makino, S.
		  and Rosca, J. P.},
  booktitle	= {Independent Component Analysis and Signal Separation},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= sep,
  pages		= {552--559},
  publisher	= {Springer},
  title		= {First stereo audio source separation evaluation campaign:
		  data, algorithms and results},
  year		= {2007}
}

@Article{	  voglis,
  author	= {Voglis, C and Lagaris, I E},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/globnn-npc.pdf:pdf},
  keywords	= {- artificial neural networks,global
		  optimization,multistart},
  pages		= {1--10},
  title		= {{A Global Optimization Approach to Neural Network
		  Training}}
}

###Article{	  voglis,
  author	= {Voglis, C and Lagaris, I E},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/globnn-npc.pdf:pdf},
  keywords	= {- artificial neural networks,global
		  optimization,multistart},
  pages		= {1--10},
  title		= {{A Global Optimization Approach to Neural Network
		  Training}}
}

@InProceedings{	  watanabe2007almost,
  title		= {Almost all learning machines are singular},
  author	= {Watanabe, Sumio},
  booktitle	= {Foundations of Computational Intelligence, 2007. FOCI
		  2007. IEEE Symposium on},
  pages		= {383--388},
  year		= {2007},
  organization	= {IEEE}
}

###InProceedings{ watanabe2007almost,
  author	= {Watanabe, Sumio},
  booktitle	= {Foundations of Computational Intelligence, 2007. FOCI
		  2007. IEEE Symposium on},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  organization	= {IEEE},
  pages		= {383--388},
  title		= {Almost all learning machines are singular},
  year		= {2007}
}

###InProceedings{ watanabe2007almost,
  title		= {Almost all learning machines are singular},
  author	= {Watanabe, Sumio},
  booktitle	= {Foundations of Computational Intelligence, 2007. FOCI
		  2007. IEEE Symposium on},
  pages		= {383--388},
  year		= {2007},
  organization	= {IEEE}
}

@InProceedings{	  welteh2011a,
  author	= {Welling, M. and Teh, Y. W},
  title		= {Bayesian Learning via Stochastic Gradient {L}angevin
		  Dynamics},
  booktitle	= {Proceedings of the 28th International Conference on
		  Machine Learning},
  year		= {2011},
  pages		= {681--688},
  month		= jun
}

###InProceedings{ welteh2011a,
  author	= {Welling, M. and Teh, Y. W},
  booktitle	= {Proceedings of the 28th International Conference on
		  Machine Learning},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  month		= jun,
  pages		= {681--688},
  title		= {Bayesian Learning via Stochastic Gradient {L}angevin
		  Dynamics},
  year		= {2011}
}

@Article{	  wen16,
  author	= {Wei Wen and Chunpeng Wu and Yandan Wang and Yiran Chen and
		  Hai Li},
  title		= {Learning Structured Sparsity in Deep Neural Networks},
  journal	= {CoRR},
  volume	= {abs/1608.03665},
  year		= {2016},
  url		= {http://arxiv.org/abs/1608.03665},
  timestamp	= {Mon, 30 Jan 2017 17:08:13 +0100},
  biburl	= {http://dblp.uni-trier.de/rec/bib/journals/corr/WenWWCL16},
  bibsource	= {dblp computer science bibliography, http://dblp.org}
}

###Article{	  wen16,
  author	= {Wei Wen and Chunpeng Wu and Yandan Wang and Yiran Chen and
		  Hai Li},
  bibsource	= {dblp computer science bibliography, http://dblp.org},
  biburl	= {http://dblp.uni-trier.de/rec/bib/journals/corr/WenWWCL16},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {CoRR},
  timestamp	= {Mon, 30 Jan 2017 17:08:13 +0100},
  title		= {Learning Structured Sparsity in Deep Neural Networks},
  url		= {http://arxiv.org/abs/1608.03665},
  volume	= {abs/1608.03665},
  year		= {2016},
  bdsk-url-1	= {http://arxiv.org/abs/1608.03665}
}

###Article{	  wen16,
  author	= {Wei Wen and Chunpeng Wu and Yandan Wang and Yiran Chen and
		  Hai Li},
  title		= {Learning Structured Sparsity in Deep Neural Networks},
  journal	= {CoRR},
  volume	= {abs/1608.03665},
  year		= {2016},
  url		= {http://arxiv.org/abs/1608.03665},
  timestamp	= {Mon, 30 Jan 2017 17:08:13 +0100},
  biburl	= {http://dblp.uni-trier.de/rec/bib/journals/corr/WenWWCL16},
  bibsource	= {dblp computer science bibliography, http://dblp.org}
}

@Book{		  wschebor,
  author	= {Azais, Jean-Marc and Wschebor, Mario},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/level.pdf:pdf},
  isbn		= {978-0-470-40933-6},
  pages		= {393},
  publisher	= {Wiley},
  title		= {{Level sets and extrema of random processes and fields}},
  url		= {http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470409339.html},
  year		= {2009}
}

###Book{	  wschebor,
  author	= {Azais, Jean-Marc and Wschebor, Mario},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/level.pdf:pdf},
  isbn		= {978-0-470-40933-6},
  pages		= {393},
  publisher	= {Wiley},
  title		= {{Level sets and extrema of random processes and fields}},
  url		= {http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470409339.html},
  year		= {2009},
  bdsk-url-1	= {http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470409339.html}
}

@InProceedings{	  wu2018sgd,
  title		= {How SGD Selects the Global Minima in Over-parameterized
		  Learning: A Dynamical Stability Perspective},
  author	= {Wu, Lei and Ma, Chao and Weinan, E},
  booktitle	= {Advances in Neural Information Processing Systems},
  pages		= {8289--8298},
  year		= {2018}
}

@Article{	  wyart05a,
  author	= {Wyart, Matthieu and Silbert, Leonardo E and Nagel, Sidney
		  R and Witten, Thomas A},
  date		= {2005},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  journal	= {Physical Review E},
  number	= {5},
  pages		= {051306},
  publisher	= {APS},
  title		= {Effects of compression on the vibrational modes of
		  marginally jammed solids},
  volume	= {72},
  year		= {2005}
}

###Article{	  wyart05a,
  author	= {Wyart, Matthieu and Silbert, Leonardo E and Nagel, Sidney
		  R and Witten, Thomas A},
  date		= {2005},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review E},
  number	= {5},
  pages		= {051306},
  publisher	= {APS},
  title		= {Effects of compression on the vibrational modes of
		  marginally jammed solids},
  volume	= {72},
  year		= {2005}
}

###Article{	  wyart05a,
  author	= {Wyart, Matthieu and Silbert, Leonardo E and Nagel, Sidney
		  R and Witten, Thomas A},
  date		= {2005},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  journal	= {Physical Review E},
  number	= {5},
  pages		= {051306},
  publisher	= {APS},
  title		= {Effects of compression on the vibrational modes of
		  marginally jammed solids},
  volume	= {72},
  year		= {2005}
}

@Article{	  wyart05b,
  author	= {M. Wyart},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-11-09 08:15:00 +0000},
  journal	= {Annales de Phys},
  number	= {3},
  pages		= {1--113},
  title		= {On the Rigidity of Amorphous Solids},
  volume	= {30},
  year		= {2005}
}

###Article{	  wyart05b,
  author	= {M. Wyart},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Annales de Phys},
  number	= {3},
  pages		= {1--113},
  title		= {On the Rigidity of Amorphous Solids},
  volume	= {30},
  year		= {2005}
}

###Article{	  wyart05b,
  author	= {M. Wyart},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-11-09 08:15:00 +0000},
  journal	= {Annales de Phys},
  number	= {3},
  pages		= {1--113},
  title		= {On the Rigidity of Amorphous Solids},
  volume	= {30},
  year		= {2005}
}

@Article{	  wyart12,
  author	= {Wyart, Matthieu},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-09-05 15:59:36 +0000},
  doi		= {10.1103/PhysRevLett.109.125502},
  issue		= {12},
  journal	= {Phys. Rev. Lett.},
  month		= {Sep},
  numpages	= {5},
  pages		= {125502},
  publisher	= {American Physical Society},
  title		= {Marginal Stability Constrains Force and Pair Distributions
		  at Random Close Packing},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.125502},
  bdsk-url-2	= {http://dx.doi.org/10.1103/PhysRevLett.109.125502}
}

###Article{	  wyart12,
  author	= {Wyart, Matthieu},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  doi		= {10.1103/PhysRevLett.109.125502},
  issue		= {12},
  journal	= {Phys. Rev. Lett.},
  month		= {Sep},
  numpages	= {5},
  pages		= {125502},
  publisher	= {American Physical Society},
  title		= {Marginal Stability Constrains Force and Pair Distributions
		  at Random Close Packing},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.125502},
  bdsk-url-2	= {http://dx.doi.org/10.1103/PhysRevLett.109.125502}
}

###Article{	  wyart12,
  author	= {Wyart, Matthieu},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-09-05 15:59:36 +0000},
  doi		= {10.1103/PhysRevLett.109.125502},
  issue		= {12},
  journal	= {Phys. Rev. Lett.},
  month		= {Sep},
  numpages	= {5},
  pages		= {125502},
  publisher	= {American Physical Society},
  title		= {Marginal Stability Constrains Force and Pair Distributions
		  at Random Close Packing},
  volume	= {109},
  year		= {2012},
  bdsk-url-1	= {http://link.aps.org/doi/10.1103/PhysRevLett.109.125502},
  bdsk-url-2	= {http://dx.doi.org/10.1103/PhysRevLett.109.125502}
}

@Article{	  xing2018walk,
  title		= {A Walk with SGD},
  author	= {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and
		  Bengio, Yoshua},
  journal	= {arXiv preprint arXiv:1802.08770},
  year		= {2018}
}

@Article{	  yan16,
  author	= {Yan, Le and DeGiuli, Eric and Wyart, Matthieu},
  date		= {2016},
  date-added	= {2016-10-13 12:49:09 +0000},
  date-modified	= {2016-10-13 12:49:27 +0000},
  journal	= {EPL (Europhysics Letters)},
  number	= {2},
  pages		= {26003},
  publisher	= {IOP Publishing},
  title		= {On variational arguments for vibrational modes near
		  jamming},
  volume	= {114},
  year		= {2016}
}

###Article{	  yan16,
  author	= {Yan, Le and DeGiuli, Eric and Wyart, Matthieu},
  date		= {2016},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {EPL (Europhysics Letters)},
  number	= {2},
  pages		= {26003},
  publisher	= {IOP Publishing},
  title		= {On variational arguments for vibrational modes near
		  jamming},
  volume	= {114},
  year		= {2016}
}

###Article{	  yan16,
  author	= {Yan, Le and DeGiuli, Eric and Wyart, Matthieu},
  date		= {2016},
  date-added	= {2016-10-13 12:49:09 +0000},
  date-modified	= {2016-10-13 12:49:27 +0000},
  journal	= {EPL (Europhysics Letters)},
  number	= {2},
  pages		= {26003},
  publisher	= {IOP Publishing},
  title		= {On variational arguments for vibrational modes near
		  jamming},
  volume	= {114},
  year		= {2016}
}

@Article{	  yau2010,
  author	= {Yau, H T and Yin, J and Ramirez, J and Tao, T and Vu, V},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/caltechshort.pdf:pdf},
  title		= {{Universality for Random Matrices and Dyson Brownian
		  Motion}},
  year		= {2010}
}

###Article{	  yau2010,
  author	= {Yau, H T and Yin, J and Ramirez, J and Tao, T and Vu, V},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  file		= {:Users/ugur/Dropbox/iclr15\_cliffhanger/Project/references/caltechshort.pdf:pdf},
  title		= {{Universality for Random Matrices and Dyson Brownian
		  Motion}},
  year		= {2010}
}

@InCollection{	  year,
  title		= {{No Title}}
}

###InCollection{  year,
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  title		= {{No Title}}
}

@InProceedings{	  yilmaz2011generalised,
  title		= {Generalised coupled tensor factorisation},
  author	= {Y{\i}lmaz, K. Y. and Cemgil, A. T. and \c{S}im\c{s}ekli, U.},
  booktitle	= {Advances in neural information processing systems},
  pages		= {2151--2159},
  year		= {2011}
}

###InProceedings{ yilmaz2011generalised,
  author	= {Y{\i}lmaz, K. Y. and Cemgil, A. T. and \c{S}im\c{s}ekli, U.},
  booktitle	= {Advances in neural information processing systems},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  pages		= {2151--2159},
  title		= {Generalised coupled tensor factorisation},
  year		= {2011}
}

@Article{	  yokota2016smooth,
  title		= {Smooth {PARAFAC} decomposition for tensor completion},
  author	= {Yokota, Tatsuya and Zhao, Qibin and Cichocki, Andrzej},
  journal	= {IEEE Transactions on Signal Processing},
  volume	= {64},
  number	= {20},
  pages		= {5423--5436},
  year		= {2016},
  publisher	= {IEEE}
}

###Article{	  yokota2016smooth,
  author	= {Yokota, Tatsuya and Zhao, Qibin and Cichocki, Andrzej},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:29 +0100},
  journal	= {IEEE Transactions on Signal Processing},
  number	= {20},
  pages		= {5423--5436},
  publisher	= {IEEE},
  title		= {Smooth {PARAFAC} decomposition for tensor completion},
  volume	= {64},
  year		= {2016}
}

@Article{	  zdeborova07,
  title		= {Phase transitions in the coloring of random graphs},
  author	= {Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal	= {Physical Review E},
  volume	= {76},
  number	= {3},
  pages		= {031131},
  year		= {2007},
  publisher	= {APS}
}

###Article{	  zdeborova07,
  author	= {Zdeborov{\'a}, Lenka and Krzakala, Florent},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Physical Review E},
  number	= {3},
  pages		= {031131},
  publisher	= {APS},
  title		= {Phase transitions in the coloring of random graphs},
  volume	= {76},
  year		= {2007}
}

###Article{	  zdeborova07,
  title		= {Phase transitions in the coloring of random graphs},
  author	= {{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal	= {Physical Review E},
  volume	= {76},
  number	= {3},
  pages		= {031131},
  year		= {2007},
  publisher	= {APS}
}

@Article{	  zdeborovareview,
  title		= {Statistical physics of inference: Thresholds and
		  algorithms},
  author	= {Zdeborov{\'a}, Lenka and Krzakala, Florent},
  journal	= {Advances in Physics},
  volume	= {65},
  number	= {5},
  pages		= {453--552},
  year		= {2016},
  publisher	= {Taylor \& Francis}
}

###Article{	  zdeborovareview,
  author	= {Zdeborov{\'a}, Lenka and Krzakala, Florent},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {Advances in Physics},
  number	= {5},
  pages		= {453--552},
  publisher	= {Taylor \& Francis},
  title		= {Statistical physics of inference: Thresholds and
		  algorithms},
  volume	= {65},
  year		= {2016}
}

###Article{	  zdeborovareview,
  title		= {Statistical physics of inference: Thresholds and
		  algorithms},
  author	= {{Zdeborov{\'a}}, Lenka and Krzakala, Florent},
  journal	= {Advances in Physics},
  volume	= {65},
  number	= {5},
  pages		= {453--552},
  year		= {2016},
  publisher	= {Taylor \& Francis}
}

@Article{	  zeravcic09,
  abstract	= {We study the vibrational modes of three-dimensional jammed
		  packings of soft ellipsoids of revolution as a function of
		  particle aspect ratio {\OE}µ and packing fraction. At the
		  jamming transition for ellipsoids, as distinct from the
		  idealized case using spheres where {\OE}µ=1, there are
		  many unconstrained and nontrivial rotational degrees of
		  freedom. These constitute a set of zero-frequency modes
		  that are gradually mobilized into a new rotational band as
		  |{\OE}µ-1| increases. Quite surprisingly, as this new band
		  is separated from zero frequency by a gap, and lies below
		  the onset frequency for translational vibrations,
		  {\oe}{\^a} * , the presence of these new degrees of freedom
		  leaves unaltered the basic scenario that the translational
		  spectrum is determined only by the average contact number.
		  Indeed, {\oe}{\^a} * depends solely on coordination as it
		  does for compressed packings of spheres. We also discuss
		  the regime of large |{\OE}µ-1|, where the two bands merge.},
  author	= {Z. Zeravcic and N. Xu and A. J. Liu and S. R. Nagel and W.
		  van Saarloos},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  journal	= epl,
  number	= {2},
  pages		= {26001},
  title		= {Excitations of ellipsoid packings near jamming},
  volume	= {87},
  year		= {2009}
}

###Article{	  zeravcic09,
  abstract	= {We study the vibrational modes of three-dimensional jammed
		  packings of soft ellipsoids of revolution as a function of
		  particle aspect ratio {\OE}µ and packing fraction. At the
		  jamming transition for ellipsoids, as distinct from the
		  idealized case using spheres where {\OE}µ=1, there are
		  many unconstrained and nontrivial rotational degrees of
		  freedom. These constitute a set of zero-frequency modes
		  that are gradually mobilized into a new rotational band as
		  |{\OE}µ-1| increases. Quite surprisingly, as this new band
		  is separated from zero frequency by a gap, and lies below
		  the onset frequency for translational vibrations,
		  {\oe}{\^a} * , the presence of these new degrees of freedom
		  leaves unaltered the basic scenario that the translational
		  spectrum is determined only by the average contact number.
		  Indeed, {\oe}{\^a} * depends solely on coordination as it
		  does for compressed packings of spheres. We also discuss
		  the regime of large |{\OE}µ-1|, where the two bands merge.},
  author	= {Z. Zeravcic and N. Xu and A. J. Liu and S. R. Nagel and W.
		  van Saarloos},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= epl,
  number	= {2},
  pages		= {26001},
  title		= {Excitations of ellipsoid packings near jamming},
  volume	= {87},
  year		= {2009}
}

###Article{	  zeravcic09,
  abstract	= {We study the vibrational modes of three-dimensional jammed
		  packings of soft ellipsoids of revolution as a function of
		  particle aspect ratio {{\OE}µ} and packing fraction. At
		  the jamming transition for ellipsoids, as distinct from the
		  idealized case using spheres where {{\OE}µ=1}, there are
		  many unconstrained and nontrivial rotational degrees of
		  freedom. These constitute a set of zero-frequency modes
		  that are gradually mobilized into a new rotational band as
		  {|{\OE}µ-1|} increases. Quite surprisingly, as this new
		  band is separated from zero frequency by a gap, and lies
		  below the onset frequency for translational vibrations,
		  {{\oe}{\^a} * }, the presence of these new degrees of
		  freedom leaves unaltered the basic scenario that the
		  translational spectrum is determined only by the average
		  contact number. Indeed, {{\oe}{\^a} *} depends solely on
		  coordination as it does for compressed packings of spheres.
		  We also discuss the regime of large {|{\OE}µ-1|}, where
		  the two bands merge.},
  author	= {Z. Zeravcic and N. Xu and A. J. Liu and S. R. Nagel and W.
		  van Saarloos},
  date-added	= {2014-02-03 23:25:36 +0000},
  date-modified	= {2014-05-22 20:13:34 +0000},
  journal	= epl,
  number	= {2},
  pages		= {26001},
  title		= {Excitations of ellipsoid packings near jamming},
  volume	= {87},
  year		= {2009}
}

@Article{	  zhang16,
  title		= {Understanding deep learning requires rethinking
		  generalization},
  author	= {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and
		  Recht, Benjamin and Vinyals, Oriol},
  journal	= {International Conference on Learning Representations},
  year		= {2017}
}

###Article{	  zhang16,
  author	= {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and
		  Recht, Benjamin and Vinyals, Oriol},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {International Conference on Learning Representations},
  title		= {Understanding deep learning requires rethinking
		  generalization},
  year		= {2017}
}

@Article{	  zhang2016understanding,
  title		= {Understanding deep learning requires rethinking
		  generalization},
  author	= {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and
		  Recht, Benjamin and Vinyals, Oriol},
  journal	= {arXiv preprint arXiv:1611.03530},
  year		= {2016}
}

###Article{	  zhang2016understanding,
  author	= {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and
		  Recht, Benjamin and Vinyals, Oriol},
  date-added	= {2018-12-12 15:35:28 +0100},
  date-modified	= {2018-12-12 15:35:28 +0100},
  journal	= {arXiv preprint arXiv:1611.03530},
  title		= {Understanding deep learning requires rethinking
		  generalization},
  year		= {2016}
}

Automatically generated by Mendeley Desktop 1.19.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@techreport{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167},
author = {Ioffe, Sergey and Szegedy, Christian},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {1502.03167},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
pmid = {15003161},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167},
year = {2015}
}
@techreport{Pennington2017a,
abstract = {Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix Y T Y , Y = f (W X), where W is a random weight matrix, X is a random data matrix, and f is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature networks on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.},
author = {Pennington, Jeffrey and Worah, Pratik},
booktitle = {Neural Information Processing Systems},
file = {::},
isbn = {0950-1991 (Print)},
issn = {10495258},
keywords = {Nonlinear random matrix,deep reinforcement learning},
number = {31},
pmid = {7555714},
title = {{Nonlinear random matrix theory for deep learning}},
url = {https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/46342.pdf},
year = {2017}
}

@article{nowlan1992simplifying,
  title={Simplifying neural networks by soft weight-sharing},
  author={Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={4},
  number={4},
  pages={473--493},
  year={1992},
  publisher={MIT Press}
}

@article{long2019size,
  title={Size-free generalization bounds for convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}

@inproceedings{du2018many,
  title={How Many Samples are Needed to Estimate a Convolutional Neural Network?},
  author={Du, Simon S and Wang, Yining and Zhai, Xiyu and Balakrishnan, Sivaraman and Salakhutdinov, Ruslan R and Singh, Aarti},
  booktitle={Advances in Neural Information Processing Systems},
  pages={373--383},
  year={2018}
}

@article{Goodfellow,
abstract = {A new generative adversarial network is developed for joint distribution matching. Distinct from most existing approaches, that only learn conditional distributions, the proposed model aims to learn a joint distribution of multiple random variables (domains). This is achieved by learning to sample from conditional distributions between the domains, while simultaneously learning to sample from the marginals of each individual domain. The proposed framework consists of multiple generators and a single softmax-based critic, all jointly trained via adversarial learning. From a simple noise source, the proposed framework allows synthesis of draws from the marginals, conditional draws given observations from a subset of random variables, or complete draws from the full joint distribution. Most examples considered are for joint analysis of two domains, with examples for three domains also presented.},
archivePrefix = {arXiv},
arxivId = {1806.02978},
author = {Pu, Yunchen and Dai, Shuyang and Gan, Zhe and Wang, Weiyao and Wang, Guoyin and Zhang, Yizhe and Henao, Ricardo and Carin, Lawrence},
eprint = {1806.02978},
file = {::},
isbn = {9781510867963},
title = {{JointGAN: Multi-Domain Joint Distribution Learning with Generative Adversarial Nets}},
url = {http://arxiv.org/abs/1806.02978},
year = {2018}
}
@article{Goyal2017,
abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves {\~{}}90{\%} scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
archivePrefix = {arXiv},
arxivId = {1706.02677},
author = {Goyal, Priya and Doll{\'{a}}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
doi = {10.1561/2400000003},
eprint = {1706.02677},
file = {::},
isbn = {9781601987167},
issn = {2167-3888},
pmid = {17460516},
title = {{Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour}},
url = {http://arxiv.org/abs/1706.02677},
year = {2017}
}
@techreport{Li2018,
abstract = {Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1808.01204},
author = {Li, Yuanzhi and Liang, Yingyu},
doi = {arXiv:1808.01204v1},
eprint = {1808.01204},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Li, Liang - 2018 - Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data.pdf:pdf},
title = {{Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data}},
url = {http://arxiv.org/abs/1808.01204},
year = {2018}
}
@techreport{Pennington2018,
abstract = {Recent work has shown that tight concentration of the entire spectrum of singular values of a deep network's input-output Jacobian around one at initialization can speed up learning by orders of magnitude. Therefore, to guide important design choices, it is important to build a full theoretical understanding of the spectra of Jacobians at initialization. To this end, we leverage powerful tools from free probability theory to provide a detailed analytic understanding of how a deep network's Jacobian spectrum depends on various hyperparameters including the nonlinearity, the weight and bias distributions, and the depth. For a variety of nonlinearities, our work reveals the emergence of new universal limiting spectral distributions that remain concentrated around one even as the depth goes to infinity.},
archivePrefix = {arXiv},
arxivId = {1802.09979},
author = {Pennington, Jeffrey and Schoenholz, Samuel S. and Ganguli, Surya},
doi = {10.1016/j.bcp.2015.06.011},
eprint = {1802.09979},
file = {::},
issn = {0006-2952},
title = {{The Emergence of Spectral Universality in Deep Networks}},
url = {http://arxiv.org/abs/1802.09979},
year = {2018}
}
@techreport{Achille2018,
abstract = {We study the topology of the space of learning tasks, which is critical to understanding transfer learning whereby a model such as a deep neural network is pre-trained on a task, and then used on a different one after some fine-tuning. First we show that using the Kolmogorov structure function we can define a distance between tasks, which is independent on any particular model used and, empirically, correlates with the semantic similarity between tasks. Then, using a path integral approximation, we show that this plays a central role in the learning dynamics of Deep Networks, and in particular in the reachability of one task from another. We show that the probability of paths connecting two tasks, is asymmetric and has a static component that depends on the geometry of the loss function, in particular on the curvature, and a dynamic component that is model dependent and relates to the ease of traversing such paths. Surprisingly, the static component corresponds to the distance derived from the Kolmogorov Structure Function. With the dynamic component, this gives strict lower bounds on the complexity necessary to learn a task starting from the solution to another. Our analysis also explains more complex phenomena where semantically similar tasks may be unreachable from one another, a phenomenon called Information Plasticity and observed in diverse learning systems such as animals and deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1810.02440},
author = {Achille, Alessandro and Mbeng, Glen and Soatto, Stefano},
doi = {arXiv:1810.02440v1},
eprint = {1810.02440},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Achille, Mbeng, Soatto - 2018 - The Dynamics of Differential Learning I Information-Dynamics and Task Reachability.pdf:pdf},
title = {{The Dynamics of Differential Learning I: Information-Dynamics and Task Reachability}},
url = {http://arxiv.org/abs/1810.02440},
year = {2018}
}
@techreport{Gotmare2018,
abstract = {The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
archivePrefix = {arXiv},
arxivId = {1810.13243},
author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
doi = {arXiv:1810.13243v1},
eprint = {1810.13243},
file = {::},
title = {{A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation}},
url = {http://arxiv.org/abs/1810.13243},
year = {2018}
}
@article{Dascoli,
abstract = {We present the first fully relativistic prediction of the electromagnetic emission from the surrounding gas of a supermassive binary black hole system approaching merger. Using a ray-tracing code to post-process data from a general relativistic 3-d MHD simulation, we generate images and spectra, and analyze the viewing angle dependence of the light emitted. When the accretion rate is relatively high, the circumbinary disk, accretion streams, and mini-disks combine to emit light in the UV/EUV bands. We posit a thermal Compton hard X-ray spectrum for coronal emission; at high accretion rates, it is almost entirely produced in the mini-disks, but at lower accretion rates it is the primary radiation mechanism in the mini-disks and accretion streams as well. Due to relativistic beaming and gravitational lensing, the angular distribution of the power radiated is strongly anisotropic, especially near the equatorial plane.},
archivePrefix = {arXiv},
arxivId = {1806.05697},
author = {D'Ascoli, St{\'{e}}phane and Noble, Scott C. and Bowen, Dennis B. and Campanelli, Manuela and Krolik, Julian H. and Mewes, Vassilios},
doi = {10.3847/1538-4357/aad8b4},
eprint = {1806.05697},
file = {::},
issn = {1538-4357},
title = {{Electromagnetic Emission from Supermassive Binary Black Holes Approaching Merger}},
url = {http://arxiv.org/abs/1806.05697},
year = {2018}
}
@techreport{L.K.Hansen1990,
abstract = {We propose several means for improving the performance and training of neural networks for classification. We use crossvali-dation as a tool for optimizing network parameters and architecture. We show further that the remaining residual "generalization" error can be reduced by invoking ensembles of similar networks. Zndex Terms-Crossvalidation, fault tolerant computing, neural networks , N-version programming.},
author = {{L. K. Hansen} and {P. Salamon}},
booktitle = {IEEE Transactions on  Pattern Analysis and Machine Intelligence},
doi = {10.1109/34.58871},
file = {::},
issn = {0162-8828},
pages = {993-- 1001},
title = {{Neural network ensembles.}},
volume = {12},
year = {1990}
}
@article{Berthier2011,
abstract = {We provide a theoretical perspective on the glass transition in molecular liquids at thermal equilibrium, on the spatially heterogeneous and aging dynamics of disordered materials, and on the rheology of soft glassy materials. We start with a broad introduction to the field and emphasize its connections with other subjects and its relevance. The important role played by computer simulations to study and understand the dynamics of systems close to the glass transition at the molecular level is spelled out. We review the recent progress on the subject of the spatially heterogeneous dynamics that characterizes structural relaxation in materials with slow dynamics. We then present the main theoretical approaches describing the glass transition in supercooled liquids, focusing on theories that have a microscopic, statistical mechanics basis. We describe both successes and failures, and critically assess the current status of each of these approaches. The physics of aging dynamics in disordered materials and the rheology of soft glassy materials are then discussed, and recent theoretical progress is described. For each section, we give an extensive overview of the most recent advances, but we also describe in some detail the important open problems that, we believe, will occupy a central place in this field in the coming years.},
archivePrefix = {arXiv},
arxivId = {1011.2578},
author = {Berthier, Ludovic and Biroli, Giulio},
doi = {10.1103/RevModPhys.83.587},
eprint = {1011.2578},
isbn = {0034-6861$\backslash$n1539-0756},
issn = {00346861},
journal = {Reviews of Modern Physics},
number = {2},
pages = {587--645},
pmid = {21179197},
title = {{Theoretical perspective on the glass transition and amorphous materials}},
volume = {83},
year = {2011}
}
@article{Babadi2014,
abstract = {In several sensory pathways, input stimuli project tosparsely active downstream populations that have more neurons than incoming axons. Here, we address the computational benefits of expansion and sparseness for clustered inputs, where different clusters represent behaviorally distinct stimuli and intracluster variability represents sensory or neuronal noise. Through analytical calculations and numerical simulations, we show that expansion implemented by feed-forward random synaptic weights amplifies variability in the incoming stimuli, and this noise enhancement increases with sparseness of the expanded representation. In addition, the low dimensionality of the input layer generates overlaps between the induced representations of different stimuli, limiting the benefit of expansion. Highly sparse expansive representations obtained through synapses that encode the clustered structure of the input reduce both intrastimulus variability and the excess overlaps between stimuli, enhancing the ability of downstream neurons to perform classification and recognition tasks. Implications for olfactory, cerebellar, and visual processing are discussed.},
author = {Babadi, Baktash and Sompolinsky, Haim},
doi = {10.1016/j.neuron.2014.07.035},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Babadi, Sompolinsky - 2014 - Article Sparseness and Expansion in Sensory Representations.pdf:pdf},
isbn = {0896-6273},
issn = {10974199},
journal = {Neuron},
number = {5},
pages = {1213--1226},
pmid = {25155954},
title = {{Sparseness and Expansion in Sensory Representations}},
url = {http://dx.},
volume = {83},
year = {2014}
}
@article{Sagun,
abstract = {We look at the eigenvalues of the Hessian of a loss function before and after training. The eigenvalue distribution is seen to be composed of two parts, the bulk which is concentrated around zero, and the edges which are scattered away from zero. We present empirical evidence for the bulk indicating how over-parametrized the system is, and for the edges that depend on the input data.},
archivePrefix = {arXiv},
arxivId = {1611.07476},
author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
doi = {10.1227/01.NEU.0000255452.20602.C9},
eprint = {1611.07476},
isbn = {9781405161251},
issn = {0148396X},
pmid = {17460516},
title = {{Eigenvalues of the Hessian in Deep Learning: Singularity and Beyond}},
url = {http://arxiv.org/abs/1611.07476},
year = {2016}
}
@techreport{Mei2018,
abstract = {Multi-layer neural networks are among the most powerful models in machine learning, yet the fundamental reasons for this success defy mathematical understanding. Learning a neural network requires to optimize a non-convex high-dimensional objective (risk function), a problem which is usually attacked using stochastic gradient descent (SGD). Does SGD converge to a global optimum of the risk or only to a local optimum? In the first case, does this happen because local minima are absent, or because SGD somehow avoids them? In the second, why do local minima reached by SGD have good generalization properties? In this paper we consider a simple case, namely two-layers neural networks, and prove that -in a suitable scaling limit- SGD dynamics is captured by a certain non-linear partial differential equation (PDE) that we call distributional dynamics (DD). We then consider several specific examples, and show how DD can be used to prove convergence of SGD to networks with nearly ideal generalization error. This description allows to 'average-out' some of the complexities of the landscape of neural networks, and can be used to prove a general convergence result for noisy SGD.},
archivePrefix = {arXiv},
arxivId = {1804.06561},
author = {Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
doi = {10.1073/pnas.1806579115},
eprint = {1804.06561},
file = {::},
isbn = {0008-5472 (Print)},
issn = {0027-8424},
pmid = {30054315},
title = {{A Mean Field View of the Landscape of Two-Layers Neural Networks}},
url = {http://arxiv.org/abs/1804.06561},
year = {2018}
}
@article{Xing2018,
abstract = {We present novel empirical observations regarding how stochastic gradient descent (SGD) navigates the loss landscape of over-parametrized deep neural networks (DNNs). These observations expose the qualitatively different roles of learning rate and batch-size in DNN optimization and generalization. Specifically we study the DNN loss surface along the trajectory of SGD by interpolating the loss surface between parameters from consecutive $\backslash$textit{\{}iterations{\}} and tracking various metrics during training. We find that the loss interpolation between parameters before and after each training iteration's update is roughly convex with a minimum ($\backslash$textit{\{}valley floor{\}}) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, SGD moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley floor. This 'bouncing between walls at a height' mechanism helps SGD traverse larger distance for small batch sizes and large learning rates which we find play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley floor, a small batch size injects noise facilitating exploration. We find this mechanism is crucial for generalization because the valley floor has barriers and this exploration above the valley floor allows SGD to quickly travel far away from the initialization point (without being affected by barriers) and find flatter regions, corresponding to better generalization.},
archivePrefix = {arXiv},
arxivId = {1802.08770},
author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
doi = {10.1038/jid.2011.482},
eprint = {1802.08770},
issn = {0022-202X},
title = {{A Walk with SGD}},
url = {http://arxiv.org/abs/1802.08770},
year = {2018}
}
@article{Chaudhari,
abstract = {This paper proposes a new optimization algorithm called Entropy-SGD for training deep neural networks that is motivated by the local geometry of the energy landscape. Local extrema with low generalization error have a large proportion of almost-zero eigenvalues in the Hessian with very few positive or negative eigenvalues. We leverage upon this observation to construct a local-entropy-based objective function that favors well-generalizable solutions lying in large flat regions of the energy landscape, while avoiding poorly-generalizable solutions located in the sharp valleys. Conceptually, our algorithm resembles two nested loops of SGD where we use Langevin dynamics in the inner loop to compute the gradient of the local entropy before each update of the weights. We show that the new objective has a smoother energy landscape and show improved generalization over SGD using uniform stability, under certain assumptions. Our experiments on convolutional and recurrent networks demonstrate that Entropy-SGD compares favorably to state-of-the-art techniques in terms of generalization error and training time.},
archivePrefix = {arXiv},
arxivId = {1611.01838},
author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
doi = {10.1109/MBE.2002.1037846},
eprint = {1611.01838},
isbn = {978-3-642-04273-7},
issn = {0300-5771},
pmid = {15737968},
title = {{Entropy-SGD: Biasing Gradient Descent Into Wide Valleys}},
url = {http://arxiv.org/abs/1611.01838},
year = {2016}
}
@techreport{Novak2018,
abstract = {In practice it is often found that large over-parameterized neural networks generalize better than their smaller counterparts, an observation that appears to conflict with classical notions of function complexity, which typically favor smaller models. In this work, we investigate this tension between complexity and generalization through an extensive empirical exploration of two natural metrics of complexity related to sensitivity to input perturbations. Our experiments survey thousands of models with various fully-connected architectures, optimizers, and other hyper-parameters, as well as four different image classification datasets. We find that trained neural networks are more robust to input perturbations in the vicinity of the training data manifold, as measured by the norm of the input-output Jacobian of the network, and that it correlates well with generalization. We further establish that factors associated with poor generalization {\$}-{\$} such as full-batch training or using random labels {\$}-{\$} correspond to lower robustness, while factors associated with good generalization {\$}-{\$} such as data augmentation and ReLU non-linearities {\$}-{\$} give rise to more robust functions. Finally, we demonstrate how the input-output Jacobian norm can be predictive of generalization at the level of individual test points.},
archivePrefix = {arXiv},
arxivId = {1802.08760},
author = {Novak, Roman and Bahri, Yasaman and Abolafia, Daniel A. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
doi = {10.1016/j.aca.2004.12.001|ISSN 0003-2670},
eprint = {1802.08760},
file = {::},
title = {{Sensitivity and Generalization in Neural Networks: an Empirical Study}},
url = {http://arxiv.org/abs/1802.08760},
year = {2018}
}
@article{Draxler,
abstract = {Training neural networks involves finding minima of a high-dimensional non-convex loss function. Knowledge of the structure of this energy landscape is sparse. Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100. Surprisingly, the paths are essentially flat in both the training and test landscapes. This implies that neural networks have enough capacity for structural changes, or that these changes are small between minima. Also, each minimum has at least one vanishing Hessian eigenvalue in addition to those resulting from trivial invariance.},
archivePrefix = {arXiv},
arxivId = {1803.00885},
author = {Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred A.},
doi = {10.1067/mse.2000.108385},
eprint = {1803.00885},
isbn = {ISBN: 978-0-444-53831-4},
issn = {10582746},
pmid = {11155298},
title = {{Essentially No Barriers in Neural Network Energy Landscape}},
url = {http://arxiv.org/abs/1803.00885},
year = {2018}
}
@techreport{Louart2018,
abstract = {This article studies the Gram random matrix model {\$}G=\backslashfrac1T\backslashSigma{\^{}}{\{}\backslashrm T{\}}\backslashSigma{\$}, {\$}\backslashSigma=\backslashsigma(WX){\$}, classically found in the analysis of random feature maps and random neural networks, where {\$}X=[x{\_}1,\backslashldots,x{\_}T]\backslashin{\{}\backslashmathbb R{\}}{\^{}}{\{}p\backslashtimes T{\}}{\$} is a (data) matrix of bounded norm, {\$}W\backslashin{\{}\backslashmathbb R{\}}{\^{}}{\{}n\backslashtimes p{\}}{\$} is a matrix of independent zero-mean unit variance entries, and {\$}\backslashsigma:{\{}\backslashmathbb R{\}}\backslashto{\{}\backslashmathbb R{\}}{\$} is a Lipschitz continuous (activation) function --- {\$}\backslashsigma(WX){\$} being understood entry-wise. By means of a key concentration of measure lemma arising from non-asymptotic random matrix arguments, we prove that, as {\$}n,p,T{\$} grow large at the same rate, the resolvent {\$}Q=(G+\backslashgamma I{\_}T){\^{}}{\{}-1{\}}{\$}, for {\$}\backslashgamma{\textgreater}0{\$}, has a similar behavior as that met in sample covariance matrix models, involving notably the moment {\$}\backslashPhi=\backslashfrac{\{}T{\}}n{\{}\backslashmathbb E{\}}[G]{\$}, which provides in passing a deterministic equivalent for the empirical spectral measure of {\$}G{\$}. Application-wise, this result enables the estimation of the asymptotic performance of single-layer random neural networks. This in turn provides practical insights into the underlying mechanisms into play in random neural networks, entailing several unexpected consequences, as well as a fast practical means to tune the network hyperparameters.},
archivePrefix = {arXiv},
arxivId = {1702.05419},
author = {Louart, Cosme and Liao, Zhenyu and Couillet, Romain},
booktitle = {Annals of Applied Probability},
doi = {10.1214/17-AAP1328},
eprint = {1702.05419},
file = {::},
issn = {10505164},
keywords = {Neural networks.,Random feature maps,Random matrix theory},
number = {2},
pages = {1190--1248},
title = {{A random matrix approach to neural networks}},
url = {https://arxiv.org/pdf/1702.05419.pdf},
volume = {28},
year = {2018}
}
@techreport{Saxe2014,
abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
archivePrefix = {arXiv},
arxivId = {1312.6120},
author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
doi = {10.1093/beheco/arw047},
eprint = {1312.6120},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Saxe, Mcclelland, Ganguli - 2014 - Exact solutions to the nonlinear dynamics of learning in deep linear neural networks.pdf:pdf},
isbn = {1312.6120},
issn = {2469-9950},
pmid = {15936304},
title = {{Exact solutions to the nonlinear dynamics of learning in deep linear neural networks}},
url = {http://arxiv.org/abs/1312.6120},
year = {2013}
}
@techreport{Dziugaite2017,
abstract = {One of the defining properties of deep learning is that models are chosen to have many more parameters than available training data. In light of this capacity for overfitting, it is remarkable that simple algorithms like SGD reliably return solutions with low test error. One roadblock to explaining these phenomena in terms of implicit regularization, structural properties of the solution, and/or easiness of the data is that many learning bounds are quantitatively vacuous when applied to networks learned by SGD in this "deep learning" regime. Logically, in order to explain generalization, we need nonvacuous bounds. We return to an idea by Langford and Caruana (2001), who used PAC-Bayes bounds to compute nonvacuous numerical bounds on generalization error for stochastic two-layer two-hidden-unit neural networks via a sensitivity analysis. By optimizing the PAC-Bayes bound directly, we are able to extend their approach and obtain nonvacuous generalization bounds for deep stochastic neural network classifiers with millions of parameters trained on only tens of thousands of examples. We connect our findings to recent and old work on flat minima and MDL-based explanations of generalization.},
archivePrefix = {arXiv},
arxivId = {1703.11008},
author = {Dziugaite, Gintare Karolina and Roy, Daniel M.},
doi = {10.1039/c2cc30952a},
eprint = {1703.11008},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Karolina Dziugaite, Roy - Unknown - Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Para.pdf:pdf},
isbn = {1077-2626 VO - PP},
issn = {10772626},
title = {{Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data}},
url = {http://arxiv.org/abs/1703.11008},
year = {2017}
}
@techreport{Raghu2016,
abstract = {We propose a new approach to the problem of neural network expressivity, which seeks to characterize how structural properties of a neural network family affect the functions it is able to compute. Our approach is based on an interrelated set of measures of expressivity, unified by the novel notion of trajectory length, which measures how the output of a network changes as the input sweeps along a one-dimensional path. Our findings can be summarized as follows: (1) The complexity of the computed function grows exponentially with depth. (2) All weights are not equal: trained networks are more sensitive to their lower (initial) layer weights. (3) Regularizing on trajectory length (trajectory regularization) is a simpler alternative to batch normalization, with the same performance.},
archivePrefix = {arXiv},
arxivId = {1606.05336},
author = {Raghu, Maithra and Poole, Ben and Kleinberg, Jon and Ganguli, Surya and Sohl-Dickstein, Jascha},
doi = {10.1093/beheco/art094},
eprint = {1606.05336},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Raghu et al. - 2017 - On the Expressive Power of Deep Neural Networks.pdf:pdf},
isbn = {2041-4889 (Electronic)},
issn = {1938-7228},
pmid = {24577083},
title = {{On the Expressive Power of Deep Neural Networks}},
url = {http://arxiv.org/abs/1606.05336},
year = {2016}
}
@article{JastrzEbski12,
abstract = {Recent work has identified that using a high learning rate or a small batch size for Stochastic Gradient Descent (SGD) based training of deep neural networks encourages finding flatter minima of the training loss towards the end of training. Moreover, measures of the flatness of minima have been shown to correlate with good generalization performance. Extending this previous work, we investigate the loss curvature through the Hessian eigenvalue spectrum in the early phase of training and find an analogous bias: even at the beginning of training, a high learning rate or small batch size influences SGD to visit flatter loss regions. In addition, the evolution of the largest eigenvalues appears to always follow a similar pattern, with a fast increase in the early phase, and a decrease or stabilization thereafter, where the peak value is determined by the learning rate and batch size. Finally, we find that by altering the learning rate just in the direction of the eigenvectors associated with the largest eigenvalues, SGD can be steered towards regions which are an order of magnitude sharper but correspond to models with similar generalization, which suggests the curvature of the endpoint found by SGD is not predictive of its generalization properties.},
archivePrefix = {arXiv},
arxivId = {1807.05031},
author = {Jastrz{\c{e}}bski, Stanis{\l}aw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
doi = {10.6028/jres.045.026},
eprint = {1807.05031},
file = {::},
issn = {0091-0635},
title = {{DNN's Sharpest Directions Along the SGD Trajectory}},
url = {http://arxiv.org/abs/1807.05031},
year = {2018}
}
@article{Sagun2014,
abstract = {Finding minima of a real valued non-convex function over a high dimensional space is a major challenge in science. We provide evidence that some such functions that are defined on high dimensional domains have a narrow band of values whose pre-image contains the bulk of its critical points. This is in contrast with the low dimensional picture in which this band is wide. Our simulations agree with the previous theoretical work on spin glasses that proves the existence of such a band when the dimension of the domain tends to infinity. Furthermore our experiments on teacher-student networks with the MNIST dataset establish a similar phenomenon in deep networks. We finally observe that both the gradient descent and the stochastic gradient descent methods can reach this level within the same number of steps.},
archivePrefix = {arXiv},
arxivId = {1412.6615},
author = {Sagun, Levent and Guney, V. Ugur and Arous, Gerard Ben and LeCun, Yann},
eprint = {1412.6615},
title = {{Explorations on high dimensional landscapes}},
url = {http://arxiv.org/abs/1412.6615},
year = {2014}
}
@techreport{Pennington2017,
abstract = {Understanding the geometry of neural network loss surfaces is important for the development of improved optimization algorithms and for building a theoretical understanding of why deep learning works. In this paper, we study the geometry in terms of the distribution of eigenvalues of the Hessian matrix at critical points of varying energy. We introduce an analytical framework and a set of tools from random matrix theory that allow us to compute an approximation of this distribution under a set of simplifying assumptions. The shape of the spectrum depends strongly on the energy and another key parameter, {\$}\phi{\$}, which measures the ratio of parameters to data points. Our analysis predicts and numerical simulations support that for critical points of small index, the number of negative eigenvalues scales like the 3/2 power of the energy. We leave as an open problem an explanation for our observation that, in the context of a certain memorization task, the energy of minimizers is well-approximated by the function {\$}1/2(1-\phi){\^{}}2{\$}.},
author = {Pennington, Jeffrey and Bahri, Yasaman},
booktitle = {Icml},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Pennington, Bahri - 2017 - Geometry of Neural Network Loss Surfaces via Random Matrix Theory.pdf:pdf},
isbn = {9781510855144},
issn = {1938-7228},
number = {18},
pages = {2798--2806},
title = {{Geometry of Neural Network Loss Surfaces via Random Matrix Theory}},
url = {http://proceedings.mlr.press/v70/pennington17a.html{\%}0Ahttp://proceedings.mlr.press/v70/arpit17a.html},
volume = {70},
year = {2017}
}
@article{Rotskoff,
abstract = {Neural networks, a central tool in machine learning, have demonstrated remarkable, high fidelity performance on image recognition and classification tasks. These successes evince an ability to accurately represent high dimensional functions, potentially of great use in computational and applied mathematics. That said, there are few rigorous results about the representation error and trainability of neural networks. Here we characterize both the error and the scaling of the error with the size of the network by reinterpreting the standard optimization algorithm used in machine learning applications, stochastic gradient descent, as the evolution of a particle system with interactions governed by a potential related to the objective or "loss" function used to train the network. We show that, when the number {\$}n{\$} of parameters is large, the empirical distribution of the particles descends on a convex landscape towards a minimizer at a rate independent of {\$}n{\$}. We establish a Law of Large Numbers and a Central Limit Theorem for the empirical distribution, which together show that the approximation error of the network universally scales as {\$}O(n{\^{}}{\{}-1{\}}){\$}. Remarkably, these properties do not depend on the dimensionality of the domain of the function that we seek to represent. Our analysis also quantifies the scale and nature of the noise introduced by stochastic gradient descent and provides guidelines for the step size and batch size to use when training a neural network. We illustrate our findings on examples in which we train neural network to learn the energy function of the continuous 3-spin model on the sphere. The approximation error scales as our analysis predicts in as high a dimension as {\$}d=25{\$}.},
archivePrefix = {arXiv},
arxivId = {1805.00915},
author = {Rotskoff, Grant M. and Vanden-Eijnden, Eric},
eprint = {1805.00915},
title = {{Neural Networks as Interacting Particle Systems: Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error}},
url = {http://arxiv.org/abs/1805.00915},
year = {2018}
}
@article{PCA,
abstract = {Principal component analysis is central to the study of multivariate data. Although one of the earliest multivariate techniques it continues to be the subject of much research, ranging from new model- based approaches to algorithmic ideas from neural networks. It is extremely versatile with applications in many disciplines. The first edition of this book was the first comprehensive text written solely on principal component analysis. The second edition updates and substantially expands the original version, and is once again the definitive text on the subject. It includes core material, current research and a wide range of applications. Its length is nearly double that of the first edition. Researchers in statistics, or in other fields that use principal component analysis, will find that the book gives an authoritative yet accessible account of the subject. It is also a valuable resource for graduate courses in multivariate analysis. The book requires some knowledge of matrix algebra. Ian Jolliffe is Professor of Statistics at the University of Aberdeen. He is author or co-author of over 60 research papers and three other books. His research interests are broad, but aspects of principal component analysis have fascinated him and kept him busy for over 30 years.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Richardson, Mark},
doi = {10.5772/2340},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {978-953-51-0195-6},
issn = {01697439},
pmid = {20931840},
title = {{Principal Component Analysis}},
url = {http://www.intechopen.com/books/principal-component-analysis},
year = {2012}
}
@techreport{Castellani2005a,
abstract = {In these notes the main theoretical concepts and techniques in the field of mean-field spin-glasses are reviewed in a compact and pedagogical way, for the benefit of the graduate and undergraduate student. One particular spin-glass model is analyzed (the p-spin spherical model) by using three different approaches. Thermodynamics, covering pure states, overlaps, overlap distribution, replica symmetry breaking, and the static transition. Dynamics, covering the generating functional method, generalized Langevin equation, equations for the correlation and the response, the Mode Coupling approximation, and the dynamical transition. And finally complexity, covering the mean-field (TAP) free energy, metastable states, entropy crisis, threshold energy, and saddles. Particular attention has been paid on the mutual consistency of the results obtained from the different methods.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0505032},
author = {Castellani, Tommaso and Cavagna, Andrea},
doi = {10.1088/1742-5468/2005/05/P05012},
eprint = {0505032},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Castellani, Cavagna - 2005 - Spin-Glass Theory for Pedestrians.pdf:pdf},
isbn = {1742-5468},
issn = {17425468},
pmid = {229586200004},
primaryClass = {cond-mat},
title = {{Spin-Glass Theory for Pedestrians}},
url = {https://arxiv.org/pdf/cond-mat/0505032.pdf},
year = {2005}
}
@article{Lipton,
abstract = {Deep learning researchers commonly suggest that converged models are stuck in local minima. More recently, some researchers observed that under reasonable assumptions, the vast majority of critical points are saddle points, not true min-ima. Both descriptions suggest that weights converge around a point in weight space, be it a local optima or merely a critical point. However, it's possible that neither interpretation is accurate. As neural networks are typically over-complete, it's easy to show the existence of vast continuous regions through weight space with equal loss. In this paper, we build on recent work empirically characterizing the error surfaces of neural networks. We analyze training paths through weight space, presenting evidence that apparent convergence of loss does not correspond to weights arriving at critical points, but instead to large movements through flat regions of weight space. While it's trivial to show that neural network error sur-faces are globally non-convex, we show that error surfaces are also locally non-convex, even after breaking symmetry with a random initialization and also after partial training.},
author = {Lipton, Zachary C},
file = {::},
title = {{Workshop track -ICLR 2016 STUCK IN A WHAT? ADVENTURES IN WEIGHT SPACE}},
url = {https://arxiv.org/pdf/1602.07320.pdf}
}
@article{Kurchan1996,
abstract = {We describe a non-Arrhenius mechanism for slowing down of dynamics that is inherent to the high dimensionality of the phase space. We show that such a mechanism is at work both in a family of mean-field spin-glass models without any domain structure and in the case of ferromagnetic domain growth. The marginality of spin-glass dynamics, as well as the existence of a `quasi equilibrium regime' can be understood within this scenario. We discuss the question of ergodicity in an out-of equilibrium situation.},
archivePrefix = {arXiv},
arxivId = {cond-mat/9510079},
author = {Kurchan, Jorge and Laloux, Laurent},
doi = {10.1088/0305-4470/29/9/009},
eprint = {9510079},
file = {::},
isbn = {0305-4470$\backslash$n1361-6447},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {9},
pages = {1929--1948},
pmid = {17077921},
primaryClass = {cond-mat},
title = {{Phase space geometry and slow dynamics}},
url = {https://arxiv.org/pdf/cond-mat/9510079.pdf},
volume = {29},
year = {1996}
}
@techreport{Zhang2016a,
abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
archivePrefix = {arXiv},
arxivId = {1611.03530},
author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
doi = {10.1039/b907724c},
eprint = {1611.03530},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Zhang et al. - 2016 - Understanding deep learning requires rethinking generalization.pdf:pdf},
isbn = {1506.02142},
issn = {1473-0197},
pmid = {88045},
title = {{Understanding deep learning requires rethinking generalization}},
url = {http://arxiv.org/abs/1611.03530},
year = {2016}
}
@article{Kadmon2016,
abstract = {Deep neural networks have received a considerable attention due to the success of their training for real world machine learning applications. They are also of great interest to the understanding of sensory processing in cortical sensory hierarchies. The purpose of this work is to advance our theoretical understanding of the computational benefits of these architectures. Using a simple model of clustered noisy inputs and a simple learning rule, we provide analytically derived recursion relations describing the propagation of the signals along the deep network. By analysis of these equations, and defining performance measures, we show that these model networks have optimal depths. We further explore the dependence of the optimal architecture on the system parameters.},
author = {Kadmon, Jonathan and Sompolinsky, Haim},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Kadmon, Sompolinsky - Unknown - Optimal Architectures in a Solvable Model of Deep Networks.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 29},
pages = {1--9},
title = {{Optimal architectures in a solvable model of deep networks}},
url = {https://papers.nips.cc/paper/6330-optimal-architectures-in-a-solvable-model-of-deep-networks.pdf},
year = {2016}
}
@article{Goodfellow2014,
abstract = {Training neural networks involves solving large-scale non-convex optimization problems. This task has long been believed to be extremely difficult, with fear of local minima and other obstacles motivating a variety of schemes to improve optimization, such as unsupervised pretraining. However, modern neural networks are able to achieve negligible training error on complex tasks, using only direct training with stochastic gradient descent. We introduce a simple analysis technique to look for evidence that such networks are overcoming local optima. We find that, in fact, on a straight path from initialization to solution, a variety of state of the art neural networks never encounter any significant obstacles.},
archivePrefix = {arXiv},
arxivId = {1412.6544},
author = {Goodfellow, Ian J. and Vinyals, Oriol and Saxe, Andrew M.},
eprint = {1412.6544},
file = {::},
title = {{Qualitatively characterizing neural network optimization problems}},
url = {http://arxiv.org/abs/1412.6544},
year = {2014}
}
@article{Choromanska,
abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
archivePrefix = {arXiv},
arxivId = {1412.0233},
author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'{e}}rard Ben and LeCun, Yann},
doi = {10.1016/0040-8166(86)90068-6},
eprint = {1412.0233},
isbn = {1412.0233},
issn = {15337928},
pmid = {12500888},
title = {{The Loss Surfaces of Multilayer Networks}},
url = {http://arxiv.org/abs/1412.0233},
volume = {38},
year = {2014}
}
@article{DAscoli2018,
abstract = {We present the first fully relativistic prediction of the electromagnetic emission from the surrounding gas of a supermassive binary black hole system approaching merger. Using a ray-tracing code to post-process data from a general relativistic 3-d MHD simulation, we generate images and spectra, and analyze the viewing angle dependence of the light emitted. When the accretion rate is relatively high, the circumbinary disk, accretion streams, and mini-disks combine to emit light in the UV/EUV bands. We posit a thermal Compton hard X-ray spectrum for coronal emission; at high accretion rates, it is almost entirely produced in the mini-disks, but at lower accretion rates it is the primary radiation mechanism in the mini-disks and accretion streams as well. Due to relativistic beaming and gravitational lensing, the angular distribution of the power radiated is strongly anisotropic, especially near the equatorial plane.},
archivePrefix = {arXiv},
arxivId = {1806.05697},
author = {D'Ascoli, St{\'{e}}phane and Noble, Scott C. and Bowen, Dennis B. and Campanelli, Manuela and Krolik, Julian H. and Mewes, Vassilios},
doi = {10.3847/1538-4357/aad8b4},
eprint = {1806.05697},
file = {::},
issn = {1538-4357},
journal = {The Astrophysical Journal},
keywords = {accretion,accretion disks,black hole physics,galaxies: nuclei,magnetohydrodynamics (MHD),radiative transfer},
pages = {140},
title = {{Electromagnetic Emission from Supermassive Binary Black Holes Approaching Merger}},
url = {http://arxiv.org/abs/1806.05697},
volume = {865},
year = {2018}
}
@article{Garipov2018,
abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
archivePrefix = {arXiv},
arxivId = {1802.10026},
author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
doi = {10.1164/rccm.201203-0508OC},
eprint = {1802.10026},
isbn = {1535-4970 (Electronic)$\backslash$r1073-449X (Linking)},
issn = {20297564},
pmid = {23144331},
title = {{Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs}},
url = {http://arxiv.org/abs/1802.10026},
year = {2018}
}
@techreport{Pennington2018a,
abstract = {An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.},
author = {Pennington, Jeffrey and Brain, Google and Worah, Pratik and Research, Google},
booktitle = {Neural Information Processing Systems (NIPS)},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Pennington et al. - Unknown - The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network.pdf:pdf},
number = {Nips},
title = {{The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network}},
year = {2018}
}
@article{Saad1995,
abstract = {We present an analytic solution to the problem of on-line gradient-descent learning for two-layer neural networks with an arbitrary number of hidden units in both teacher and student networks. PACS numbers: 87. 10.+e, 02.50. — r, 05.20. — y Layered neural networks are of interest for their abil-ity to implement input-output maps [1]. Classification and regression tasks formulated as a map from an N-dimensional input space g onto a scalar g are real-ized through a map g = fi(g), which can be modified through changes in the internal parameters [J) specifying the strength of the interneuron couplings [2,3]. Learn-ing refers to the modification of these couplings so as to bring the map f{\~{}} implemented by the network as close as possible to a desired map f. The degree of success is monitored through the generalization error, a measure of the dissimilarity between f{\~{}} and f. Learning from examples in layered neural networks is usually formulated as an optimization problem [2,3], based on the minimization of an additive learning er-ror defined over a training set composed of P inde-pendent examples ((t', ("), with (t' = f(g"), 1 {\~{}} p {\~{}} P. Statistical physics tools for investigating the prop-erties of such models, based on the use of the replica method, have been successfully applied to the analysis of single-layer perceptrons [3] and some simplified two-layer structures (e.g., committee machines [4]). Analysis of more complicated multilayer networks is hampered by technical difficulties due to the complex structure of the solutions in a space of order parameters [5], which de-scribe in this case correlations among the various neurons in the trained network, as well as their degree of special-ization toward the implementation of the desired task. A recently introduced alternative approach investigates on line learning -[6]. In this scenario the couplings are adjusted to minimize the error after the presentation of each example. The resulting changes in [J] are de-scribed as a dynamical evolution, with the number of examples playing the role of time. The average that ac-counts for the disorder introduced by the independent random selection of an example at each time step can be performed directly, without invoking the replica method. The resulting equations of motion for the relevant order parameters characterize the structure of the space of so-lutions and allow for a computation of the generalization error. While investigating the on-line learning scenario pro-posed by Biehl and Schwarze [6], we found an unexpected result: The dynamical equations for the order parameters can be obtained analytically for a general two-layer stu-dent network composed of N input units, K hidden units, and a single linear output unit, trained to perform a task defined through a teacher network of similar architecture, except that its number M of hidden units is not necessarily equal to K. Two-layer networks with an arbitrary number of hidden units have been shown to be universal approximators [1] for N-to-one dimensional maps. Our results thus describe the learning of tasks of arbitrary complexity (general M). The complexity of the student network is also arbitrary (general K, independent of M), providing a tool to investigate realizable (K = M), overrealizable (K) M), and unrealizable (K (M) learning scenarios. Such capabilities are to be contrasted with previously available results; the equations provided in [6] can only describe a committee rnachine with K = 2 hidden units learning a linearly separable task (M = 1). In this Letter we limit our discussion to the case of the soft-committee machine [6], in which all the hid-den units are connected to the output unit with positive couplings of unit strength, and only the input-to-hidden couplings are adaptive. Consider the student network: hid-den unit i receives information from input unit r through the weight J; " and its activation under presentation of an input pattern g = (gi, . . . , g{\~{}}) is x; = J; g, with J; = (J;i, . . . , J,{\~{}}) defined as the vector of incoming weights onto the ith hidden unit. The output of the student network is tr(J, g) = g, , g(J; g), where g is the activation func-tion of the hidden units, taken here to be the error function g(x) = — erf (x/{\~{}}2), and J — = [J;)i; tc is the set of input-to-hidden adaptive weights. Training examples are of the form (gt', gt'). The components of the independently drawn input vectors g{\&} are uncorrelated random variables with zero mean and unit variance. The corresponding output gP is given by a deterministic teacher whose internal structure is that of a network similar to the student except for a possible difference in the number M of hidden units. Hidden unit n in the teacher network receives input information through the weight vector B, = (B " i, . . . , B " tv), and its activation under presentation of the input pattern g{\&} is yn The corresponding output is g{\&} = g " ,g(B, gt").},
author = {Saad, David and Solla, Sara A.},
doi = {10.1103/PhysRevLett.74.4337},
file = {::},
issn = {00319007},
journal = {Physical Review Letters},
keywords = {Saad95},
mendeley-tags = {Saad95},
number = {21},
pages = {4337--4340},
title = {{Exact solution for on-line learning in multilayer neural networks}},
volume = {74},
year = {1995}
}
@article{Xiao2018a,
abstract = {In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.},
archivePrefix = {arXiv},
arxivId = {1806.05393},
author = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S. and Pennington, Jeffrey},
eprint = {1806.05393},
file = {::},
isbn = {0003-3022},
issn = {1938-7228},
title = {{Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1806.05393},
year = {2018}
}
@article{Barrat1997,
abstract = {This review presents various aspects of a mean-field spin glass model known as the p-spin spherical spin glass model, which has raised a lot of interest in the study of spin glasses, and also for its possible links with a mean-field theory of structural glasses. This preprint contains no new results and is therefore not intended to be published, but its aim is to present a collection of results and formulas concerning this very rich model. It is in fact the english translation of one of the chapters of my PhD thesis (``Quelques aspects de la dynamique hors d'equilibre des verres de spin'', ``Some aspects of the out of equilibrium dynamics of spin glasses''). A postscript version (in french) of this PhD thesis will soon be available at http://www.lpt.ens.fr .},
archivePrefix = {arXiv},
arxivId = {cond-mat/9701031},
author = {Barrat, A.},
eprint = {9701031},
file = {::},
primaryClass = {cond-mat},
title = {{The p-spin spherical spin glass model}},
url = {http://arxiv.org/abs/cond-mat/9701031},
year = {1997}
}
@article{Castellani2005,
abstract = {In these notes the main theoretical concepts and techniques in the field of mean-field spin-glasses are reviewed in a compact and pedagogical way, for the benefit of the graduate and undergraduate student. One particular spin-glass model is analyzed (the p-spin spherical model) by using three different approaches. Thermodynamics, covering pure states, overlaps, overlap distribution, replica symmetry breaking, and the static transition. Dynamics, covering the generating functional method, generalized Langevin equation, equations for the correlation and the response, the Mode Coupling approximation, and the dynamical transition. And finally complexity, covering the mean-field (TAP) free energy, metastable states, entropy crisis, threshold energy, and saddles. Particular attention has been paid on the mutual consistency of the results obtained from the different methods.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0505032},
author = {Castellani, Tommaso and Cavagna, Andrea},
doi = {10.1088/1742-5468/2005/05/P05012},
eprint = {0505032},
isbn = {1742-5468},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {Cavity and replica method,Disordered systems (theory),Slow dynamics and ageing (theory),Spin glasses (theory)},
number = {5},
pages = {215--266},
pmid = {229586200004},
primaryClass = {cond-mat},
title = {{Spin-glass theory for pedestrians}},
year = {2005}
}
@techreport{Geiger2018,
abstract = {Deep learning has been immensely successful at a variety of tasks, ranging from classification to AI. Learning corresponds to fitting training data, which is implemented by descending a very high-dimensional loss function. Understanding under which conditions neural networks do not get stuck in poor minima of the loss, and how the landscape of that loss evolves as depth is increased remains a challenge. Here we predict, and test empirically, an analogy between this landscape and the energy landscape of repulsive ellipses. We argue that in FC networks a phase transition delimits the over- and under-parametrized regimes where fitting can or cannot be achieved. In the vicinity of this transition, properties of the curvature of the minima of the loss are critical. This transition shares direct similarities with the jamming transition by which particles form a disordered solid as the density is increased, which also occurs in certain classes of computational optimization and learning problems such as the perceptron. Our analysis gives a simple explanation as to why poor minima of the loss cannot be encountered in the overparametrized regime, and puts forward the surprising result that the ability of fully connected networks to fit random data is independent of their depth. Our observations suggests that this independence also holds for real data. We also study a quantity {\$}\backslashDelta{\$} which characterizes how well ({\$}\backslashDelta{\textless}0{\$}) or badly ({\$}\backslashDelta{\textgreater}0{\$}) a datum is learned. At the critical point it is power-law distributed, {\$}P{\_}+(\backslashDelta)\backslashsim\backslashDelta{\^{}}\backslashtheta{\$} for {\$}\backslashDelta{\textgreater}0{\$} and {\$}P{\_}-(\backslashDelta)\backslashsim(-\backslashDelta){\^{}}{\{}-\backslashgamma{\}}{\$} for {\$}\backslashDelta{\textless}0{\$}, with {\$}\backslashtheta\backslashapprox0.3{\$} and {\$}\backslashgamma\backslashapprox0.2{\$}. This observation suggests that near the transition the loss landscape has a hierarchical structure and that the learning dynamics is prone to avalanche-like dynamics, with abrupt changes in the set of patterns that are learned.},
archivePrefix = {arXiv},
arxivId = {1809.09349},
author = {Geiger, Mario and Spigler, Stefano and D'Ascoli, St{\'{e}}phane and Sagun, Levent and Baity-Jesi, Marco and Biroli, Giulio and Wyart, Matthieu},
eprint = {1809.09349},
file = {::},
title = {{The jamming transition as a paradigm to understand the loss landscape of deep neural networks}},
url = {http://arxiv.org/abs/1809.09349},
year = {2018}
}
@techreport{Rotskoff2018,
abstract = {Neural networks, a central tool in machine learning, have demonstrated remarkable, high fidelity performance on image recognition and classification tasks. These successes evince an ability to accurately represent high dimensional functions, potentially of great use in computational and applied mathematics. That said, there are few rigorous results about the representation error and trainability of neural networks. Here we characterize both the error and the scaling of the error with the size of the network by reinterpreting the standard optimization algorithm used in machine learning applications, stochastic gradient descent, as the evolution of a particle system with interactions governed by a potential related to the objective or "loss" function used to train the network. We show that, when the number {\$}n{\$} of parameters is large, the empirical distribution of the particles descends on a convex landscape towards a minimizer at a rate independent of {\$}n{\$}. We establish a Law of Large Numbers and a Central Limit Theorem for the empirical distribution, which together show that the approximation error of the network universally scales as {\$}O(n{\^{}}{\{}-1{\}}){\$}. Remarkably, these properties do not depend on the dimensionality of the domain of the function that we seek to represent. Our analysis also quantifies the scale and nature of the noise introduced by stochastic gradient descent and provides guidelines for the step size and batch size to use when training a neural network. We illustrate our findings on examples in which we train neural network to learn the energy function of the continuous 3-spin model on the sphere. The approximation error scales as our analysis predicts in as high a dimension as {\$}d=25{\$}.},
archivePrefix = {arXiv},
arxivId = {1805.00915},
author = {Rotskoff, Grant M. and Vanden-Eijnden, Eric},
eprint = {1805.00915},
file = {::},
title = {{Neural Networks as Interacting Particle Systems: Asymptotic Convexity of the Loss Landscape and Universal Scaling of the Approximation Error}},
url = {http://arxiv.org/abs/1805.00915},
year = {2018}
}
@article{Smith2018,
abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate {\$}\backslashepsilon{\$} and scaling the batch size {\$}B \backslashpropto \backslashepsilon{\$}. Finally, one can increase the momentum coefficient {\$}m{\$} and scale {\$}B \backslashpropto 1/(1-m){\$}, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to {\$}76.1\backslash{\%}{\$} validation accuracy in under 30 minutes.},
archivePrefix = {arXiv},
arxivId = {1711.00489},
author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
doi = {10.1016/S0169-7161(05)80045-8},
eprint = {1711.00489},
file = {::},
isbn = {9781479982516},
issn = {0006-341X},
pmid = {10985203},
title = {{Don't Decay the Learning Rate, Increase the Batch Size}},
url = {http://arxiv.org/abs/1711.00489},
year = {2017}
}
@article{Li2017,
abstract = {Introduction Low content of cell-free mitochondrial DNA (mtDNA) in cerebrospinal fluid (CSF) is a biomarker of early stage Alzheimer's disease (AD), but whether mtDNA is altered in a rapid neurodegenerative dementia such as Creutzfeldt-Jakob disease is unknown. Methods CSF mtDNA was measured using digital polymerase chain reaction (dPCR) in two independent cohorts comprising a total of 112 patients diagnosed with sporadic Creutzfeldt-Jakob disease (sCJD), probable AD, or non-Alzheimer's type dementia. Results Patients with AD exhibit low mtDNA content in CSF compared with patients diagnosed with sCJD or with non-Alzheimer's type dementias. The CSF concentration of mtDNA does not correlate with A$\beta$, t-tau, p-tau, and 14-3-3 protein levels in CSF. Discussion Low-CSF mtDNA is not a consequence of brain damage and allows the differential diagnosis of AD from sCJD and other dementias. These results support the hypothesis that mtDNA in CSF is a pathophysiological biomarker of AD.},
archivePrefix = {arXiv},
arxivId = {1712.09913},
author = {Podlesniy, Petar and Llorens, Franc and Golanska, Ewa and Sikorska, Beata and Liberski, Pawel and Zerr, Inga and Trullas, Ramon},
doi = {10.1016/j.jalz.2015.12.011},
eprint = {1712.09913},
isbn = {1552-5279 (Electronic)$\backslash$r1552-5260 (Linking)},
issn = {15525279},
journal = {Alzheimer's and Dementia},
keywords = {Alzheimer's disease,Biomarker,Cerebrospinal fluid,Creutzfeldt-Jakob disease,Digital PCR,Mitochondrial DNA},
number = {5},
pages = {546--555},
pmid = {26806388},
title = {{Mitochondrial DNA differentiates Alzheimer's disease from Creutzfeldt-Jakob disease}},
volume = {12},
year = {2016}
}
@techreport{Krizhevsky2012a,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Hinton, Geoffrey E.},
booktitle = {Neural Information Processing Systems},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {::},
isbn = {9781627480031},
issn = {10495258},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://code.google.com/p/cuda-convnet/},
year = {2012}
}
@article{Nadler1996,
abstract = {We demonstrate that the fraction of pattern sets that can be stored in single- and hidden-layer perceptrons exhibits finite size scaling. This feature allows to estimate the critical storage capacity $\backslash$alpha{\_}c from simulations of relatively small systems. We illustrate this approach by determining $\backslash$alpha{\_}c, together with the finite size scaling exponent $\backslash$nu, for storing Gaussian patterns in committee and parity machines with binary couplings and up to K=5 hidden units.},
archivePrefix = {arXiv},
arxivId = {cond-mat/9611027},
author = {Nadler, Walter and Fink, Wolfgang},
doi = {10.1103/PhysRevLett.78.555},
eprint = {9611027},
issn = {10797114},
journal = {Physical Review Letters},
number = {3},
pages = {555--558},
pmid = {15622133},
primaryClass = {cond-mat},
title = {{Finite size scaling in neural networks}},
url = {https://arxiv.org/pdf/cond-mat/9611027.pdf},
volume = {78},
year = {1997}
}
@article{Noble2018,
abstract = {We present the first fully relativistic prediction of the electromagnetic emission from the surrounding gas of a supermassive binary black hole system approaching merger. Using a ray-tracing code to post-process data from a general relativistic 3-d MHD simulation, we generate images and spectra, and analyze the viewing angle dependence of the light emitted. When the accretion rate is relatively high, the circumbinary disk, accretion streams, and mini-disks combine to emit light in the UV/EUV bands. We posit a thermal Compton hard X-ray spectrum for coronal emission; at high accretion rates, it is almost entirely produced in the mini-disks, but at lower accretion rates it is the primary radiation mechanism in the mini-disks and accretion streams as well. Due to relativistic beaming and gravitational lensing, the angular distribution of the power radiated is strongly anisotropic, especially near the equatorial plane.},
archivePrefix = {arXiv},
arxivId = {1806.05697},
author = {D'Ascoli, St{\'{e}}phane and Noble, Scott C. and Bowen, Dennis B. and Campanelli, Manuela and Krolik, Julian H. and Mewes, Vassilios},
doi = {10.3847/1538-4357/aad8b4},
eprint = {1806.05697},
file = {::},
issn = {1538-4357},
title = {{Electromagnetic Emission from Supermassive Binary Black Holes Approaching Merger}},
url = {http://arxiv.org/abs/1806.05697},
year = {2018}
}
@article{Lee2010,
abstract = {We provide a comprehensive view of various phase transitions in random {\$}K{\$}-satisfiability problems solved by stochastic-local-search algorithms. In particular, we focus on the finite-size scaling (FSS) exponent, which is mathematically important and practically useful in analyzing finite systems. Using the FSS theory of nonequilibrium absorbing phase transitions, we show that the density of unsatisfied clauses clearly indicates the transition from the solvable (absorbing) phase to the unsolvable (active) phase as varying the noise parameter and the density of constraints. Based on the solution clustering (percolation-type) argument, we conjecture two possible values of the FSS exponent, which are confirmed reasonably well in numerical simulations for {\$}2\backslashle K \backslashle 3{\$}.},
archivePrefix = {arXiv},
arxivId = {1005.0251},
author = {Lee, Sang Hoon and Ha, Meesoon and Jeon, Chanil and Jeong, Hawoong},
doi = {10.1103/PhysRevE.82.061109},
eprint = {1005.0251},
file = {::},
issn = {15393755},
journal = {Physical Review E - Statistical, Nonlinear, and Soft Matter Physics},
number = {6},
title = {{Finite-size scaling in random K -satisfiability problems}},
url = {http://www.diva-portal.org/smash/get/diva2:375333/FULLTEXT01.pdf},
volume = {82},
year = {2010}
}
@article{Tulino2004,
author = {Tulino, Antonia and Verdu, Sergio},
file = {:Users/stephane/Dropbox/Work/References/RMT books/Random Matrix Theory and Wireless Communications.pdf:pdf},
isbn = {193301900X},
keywords = {193301900X,A. M. Tulino,Antonia M. Tulino,COMPUTERS,COMPUTERS / Data Transmission Systems / Wireless,Communications engineering / telecommunications,Data Transmission Systems,Data Transmission Systems - Wireless,Elektronik / Elektrotechnik / Nachrichtentechnik,Engineering (Electronic),Inventions {\&} Technology: General Interest,Now Publishers Inc,Random Matrix Theory and Wireless Communications (,Random matrices,S. Verdu,Science/Mathematics,TECHNOLOGY {\&} ENGINEERING,Technology {\&} Engineering / Telecommunications,Technology {\&} Industrial Arts,Telecommunications,Transmission sans fil,Wireless,Wireless communication systems},
pages = {182},
title = {{Random Matrix Theory and Wireless Communications. Foundations and Trends in Communications and Information Theory}},
url = {http://www.amazon.de/Random-Wireless-Communications-Foundations-Information/dp/193301900X},
year = {2004}
}
@article{Biroli2016,
abstract = {The aim of these lectures, given at the Les Houches Summer School of Physics "Strongly Interacting Quantum Systems Out of Equilibrium", is providing an introduction to several important and interesting facets of out of equilibrium dynamics. In recent years, there has been a boost in the research on quantum systems out of equilibrium. If fifteen years ago hard condensed matter and classical statistical physics remained rather separate research fields, now the focus on several kinds of out of equilibrium dynamics is making them closer and closer. The aim of my lectures was to present to the students the richness of this topic, insisting on the common concepts and showing that there is much to gain in considering and learning out of equilibrium dynamics as a whole research field.},
archivePrefix = {arXiv},
arxivId = {1507.05858},
author = {Biroli, Giulio},
eprint = {1507.05858},
title = {{Slow Relaxations and Non-Equilibrium Dynamics in Classical and Quantum Systems}},
url = {http://arxiv.org/abs/1507.05858},
year = {2015}
}
@techreport{Allen-Zhu2018,
abstract = {Neural networks have great success in many machine learning applications, but the fundamental learning theory behind them remains largely unsolved. Learning neural networks is NP-hard, but in practice, simple algorithms like stochastic gradient descent (SGD) often produce good solutions. Moreover, it is observed that overparameterization --- designing networks whose number of parameters is larger than statistically needed to perfectly fit the data --- improves both optimization and generalization, appearing to contradict traditional learning theory. In this work, we extend the theoretical understanding of two and three-layer neural networks in the overparameterized regime. We prove that, using overparameterized neural networks, one can (improperly) learn some notable hypothesis classes, including two and three-layer neural networks with fewer parameters. Moreover, the learning process can be simply done by SGD or its variants in polynomial time using polynomially many samples. We also show that for a fixed sample size, the generalization error of the solution found by some SGD variant can be made almost independent of the number of parameters in the overparameterized network.},
archivePrefix = {arXiv},
arxivId = {1811.04918},
author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
doi = {arXiv:1811.04918v1},
eprint = {1811.04918},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Allen-Zhu, Li, Liang - 2018 - Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers.pdf:pdf},
keywords = {AllenZhu18},
mendeley-tags = {AllenZhu18},
title = {{Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers}},
url = {http://arxiv.org/abs/1811.04918},
year = {2018}
}
@techreport{Bun2017,
abstract = {This review covers recent results concerning the estimation of large covariance matrices using tools from Random Matrix Theory (RMT). We introduce several RMT methods and analytical techniques, such as the Replica formalism and Free Probability, with an emphasis on the Mar{\v{c}}enko–Pastur equation that provides information on the resolvent of multiplicatively corrupted noisy matrices. Special care is devoted to the statistics of the eigenvectors of the empirical correlation matrix, which turn out to be crucial for many applications. We show in particular how these results can be used to build consistent “Rotationally Invariant” estimators (RIE) for large correlation matrices when there is no prior on the structure of the underlying process. The last part of this review is dedicated to some real-world applications within financial markets as a case in point. We establish empirically the efficacy of the RIE framework, which is found to be superior in this case to all previously proposed methods. The case of additively (rather than multiplicatively) corrupted noisy matrices is also dealt with in a special Appendix. Several open problems and interesting technical developments are discussed throughout the paper.},
archivePrefix = {arXiv},
arxivId = {1610.08104},
author = {Bun, Jo{\"{e}}l and Bouchaud, Jean Philippe and Potters, Marc},
booktitle = {Physics Reports},
doi = {10.1016/j.physrep.2016.10.005},
eprint = {1610.08104},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Bun, Bouchaud, Potters - Unknown - Cleaning large Correlation Matrices tools from Random Matrix Theory.pdf:pdf},
isbn = {1368520081084},
issn = {03701573},
keywords = {Correlation matrix,High dimensional statistics,Random Matrix Theory,Rotational invariant estimator,Spectral decomposition},
pages = {1--109},
pmid = {15774886},
title = {{Cleaning large correlation matrices: Tools from Random Matrix Theory}},
url = {https://arxiv.org/pdf/1610.08104.pdf},
volume = {666},
year = {2017}
}
@techreport{Papyan2018,
abstract = {Previous works observed the spectrum of the Hessian of the training loss of deep neural networks. However, the networks considered were of minuscule size. We apply state-of-the-art tools in modern high-dimensional numerical linear algebra to approximate the spectrum of the Hessian of deep nets with tens of millions of parameters. Our results corroborate previous findings, based on small-scale networks, that the Hessian exhibits 'spiked' behavior, with several outliers isolated from a continuous bulk. However we find that the bulk does not follow a simple Marchenko-Pastur distribution, as previously suggested, but rather a heavier-tailed distribution. Finally, we document the dynamics of the outliers and the bulk with varying sample size.},
archivePrefix = {arXiv},
arxivId = {1811.07062},
author = {Papyan, Vardan},
eprint = {1811.07062},
file = {::},
title = {{The Full Spectrum of Deep Net Hessians At Scale: Dynamics with Sample Size}},
url = {http://arxiv.org/abs/1811.07062},
year = {2018}
}
@article{Baity-Jesi2018,
abstract = {We analyze numerically the training dynamics of deep neural networks (DNN) by using methods developed in statistical physics of glassy systems. The two main issues we address are (1) the complexity of the loss landscape and of the dynamics within it, and (2) to what extent DNNs share similarities with glassy systems. Our findings, obtained for different architectures and datasets, suggest that during the training process the dynamics slows down because of an increasingly large number of flat directions. At large times, when the loss is approaching zero, the system diffuses at the bottom of the landscape. Despite some similarities with the dynamics of mean-field glassy systems, in particular, the absence of barrier crossing, we find distinctive dynamical behaviors in the two cases, showing that the statistical properties of the corresponding loss and energy landscapes are different. In contrast, when the network is under-parametrized we observe a typical glassy behavior, thus suggesting the existence of different phases depending on whether the network is under-parametrized or over-parametrized.},
archivePrefix = {arXiv},
arxivId = {1803.06969},
author = {Baity-Jesi, M. and Sagun, L. and Geiger, M. and Spigler, S. and Arous, G. Ben and Cammarota, C. and LeCun, Y. and Wyart, M. and Biroli, G.},
doi = {arXiv:1803.06969v2},
eprint = {1803.06969},
isbn = {9781510867963},
issn = {1938-7228},
pmid = {23258208},
title = {{Comparing Dynamics: Deep Neural Networks versus Glassy Systems}},
url = {http://arxiv.org/abs/1803.06969},
year = {2018}
}
@techreport{Erhan2010,
abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5538v1},
author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Pierre-Antoine, Manzagol and {Pascal Vincent PASCALVINCENT}, Umontrealca and Bengio, Samy},
booktitle = {Journal of Machine Learning Research},
doi = {10.1145/1756006.1756025},
eprint = {arXiv:1206.5538v1},
file = {::},
isbn = {1532-4435},
issn = {15324435},
keywords = {deep architectures,deep belief networks,non-convex optimization,stacked denoising auto-encoders,unsupervised pre-training},
pages = {625--660},
pmid = {23787338},
title = {{Why Does Unsupervised Pre-training Help Deep Learning?}},
url = {http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf},
volume = {11},
year = {2010}
}
@article{ReginaldGallagherCompSc1999,
author = {Gallagher, Marcus R.},
title = {{Multi-Layer Perceptron Error Surfaces: Visualization, Structure and Modelling Models for Iterative Global Optimization}},
url = {http://www.elec.uq.edu.au/{~}marcusg/thesis.html},
year = {2000}
}
@article{Franz,
abstract = {Random constraint satisfaction problems (CSP) have been studied extensively using statistical physics techniques. They provide a benchmark to study average case scenarios instead of the worst case one. The interplay between statistical physics of disordered systems and computer science has brought new light into the realm of computational complexity theory, by introducing the notion of clustering of solutions, related to replica symmetry breaking. However, the class of problems in which clustering has been studied often involve discrete degrees of freedom: standard random CSPs are random K-SAT (aka disordered Ising models) or random coloring problems (aka disordered Potts models). In this work we consider instead problems that involve continuous degrees of freedom. The simplest prototype of these problems is the perceptron. Here we discuss in detail the full phase diagram of the model. In the regions of parameter space where the problem is non-convex, leading to multiple disconnected clusters of solutions, the solution is critical at the SAT/UNSAT threshold and lies in the same universality class of the jamming transition of soft spheres. We show how the critical behavior at the satisfiability threshold emerges, and we compute the critical exponents associated to the approach to the transition from both the SAT and UNSAT phase. We conjecture that there is a large universality class of non-convex continuous CSPs whose SAT-UNSAT threshold is described by the same scaling solution.},
archivePrefix = {arXiv},
arxivId = {1702.06919},
author = {Franz, Silvio and Parisi, Giorgio and Sevelev, Maksim and Urbani, Pierfrancesco and Zamponi, Francesco},
doi = {10.21468/SciPostPhys.2.3.019},
eprint = {1702.06919},
issn = {2542-4653},
title = {{Universality of the SAT-UNSAT (jamming) threshold in non-convex continuous constraint satisfaction problems}},
url = {http://arxiv.org/abs/1702.06919{\%}0Ahttp://dx.doi.org/10.21468/SciPostPhys.2.3.019},
year = {2017}
}
@article{Soltanolkotabi2017,
abstract = {In this paper we study the problem of learning Rectified Linear Units (ReLUs) which are functions of the form {\$}max(0,{\textless}w,x{\textgreater}){\$} with {\$}w{\$} denoting the weight vector. We study this problem in the high-dimensional regime where the number of observations are fewer than the dimension of the weight vector. We assume that the weight vector belongs to some closed set (convex or nonconvex) which captures known side-information about its structure. We focus on the realizable model where the inputs are chosen i.i.d.{\~{}}from a Gaussian distribution and the labels are generated according to a planted weight vector. We show that projected gradient descent, when initialization at 0, converges at a linear rate to the planted model with a number of samples that is optimal up to numerical constants. Our results on the dynamics of convergence of these very shallow neural nets may provide some insights towards understanding the dynamics of deeper architectures.},
archivePrefix = {arXiv},
arxivId = {1705.04591},
author = {Soltanolkotabi, Mahdi},
doi = {10.1007/BF01721972},
eprint = {1705.04591},
file = {::},
isbn = {1166-7699},
issn = {00316768},
title = {{Learning ReLUs via Gradient Descent}},
url = {http://arxiv.org/abs/1705.04591},
year = {2017}
}
@techreport{McAllester2013,
abstract = {This tutorial gives a concise overview of existing PAC-Bayesian theory focusing on three generalization bounds. The first is an Occam bound which handles rules with finite precision parameters and which states that generalization loss is near training loss when the number of bits needed to write the rule is small compared to the sample size. The second is a PAC-Bayesian bound providing a generalization guarantee for posterior distributions rather than for individual rules. The PAC-Bayesian bound naturally handles infinite precision rule parameters, {\$}L{\_}2{\$} regularization, {\{}$\backslash$em provides a bound for dropout training{\}}, and defines a natural notion of a single distinguished PAC-Bayesian posterior distribution. The third bound is a training-variance bound --- a kind of bias-variance analysis but with bias replaced by expected training loss. The training-variance bound dominates the other bounds but is more difficult to interpret. It seems to suggest variance reduction methods such as bagging and may ultimately provide a more meaningful analysis of dropouts.},
archivePrefix = {arXiv},
arxivId = {1307.2118},
author = {McAllester, David},
doi = {10.1016/0030-4220(73)90345-9},
eprint = {1307.2118},
file = {::},
issn = {00304220},
title = {{A PAC-Bayesian Tutorial with A Dropout Bound}},
url = {http://arxiv.org/abs/1307.2118},
year = {2013}
}
@techreport{Lampinen2018,
abstract = {Much attention has been devoted recently to the generalization puzzle in deep learning: large, deep networks can generalize well, but existing theories bounding generalization error are exceedingly loose, and thus cannot explain this striking performance. Furthermore, a major hope is that knowledge may transfer across tasks, so that multi-task learning can improve generalization on individual tasks. However we lack analytic theories that can quantitatively predict how the degree of knowledge transfer depends on the relationship between the tasks. We develop an analytic theory of the nonlinear dynamics of generalization in deep linear networks, both within and across tasks. In particular, our theory provides analytic solutions to the training and testing error of deep networks as a function of training time, number of examples, network size and initialization, and the task structure and SNR. Our theory reveals that deep networks progressively learn the most important task structure first, so that generalization error at the early stopping time primarily depends on task structure and is independent of network size. This suggests any tight bound on generalization error must take into account task structure, and explains observations about real data being learned faster than random data. Intriguingly our theory also reveals the existence of a learning algorithm that proveably out-performs neural network training through gradient descent. Finally, for transfer learning, our theory reveals that knowledge transfer depends sensitively, but computably, on the SNRs and input feature alignments of pairs of tasks.},
archivePrefix = {arXiv},
arxivId = {1809.10374},
author = {Lampinen, Andrew K. and Ganguli, Surya},
doi = {10.1080/03115519808619195},
eprint = {1809.10374},
file = {::},
isbn = {0311-5518},
title = {{An analytic theory of generalization dynamics and transfer learning in deep linear networks}},
url = {http://arxiv.org/abs/1809.10374},
year = {2018}
}
@article{Goswami2016,
abstract = {We consider a " configuration model " for random XORSAT which is a random system of n equa-tions over m variables in F 2 . Each equation is of the form y 1 + y 2 + {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} + y k = b where k ≥ 3 is fixed, y 1 , y 2 , {\textperiodcentered} {\textperiodcentered} {\textperiodcentered} are variables (not necessarily distinct) and b ∈ F 2 . The equations are chosen indepen-dently and uniformly at random with replacement. It is known [5, 4, 8] that there exists $\rho$ k such that m/n = $\rho$ k is a sharp threshold for the satisfiability of this system. In this note we show that for the configuration model, the width of SAT-UNSAT transition window for random k-XORSAT is $\Theta$(n −1/2) and also derive the exact scaling function.},
archivePrefix = {arXiv},
arxivId = {arXiv:1610.07431v3},
author = {Goswami, Subhajit},
eprint = {arXiv:1610.07431v3},
file = {::},
keywords = {Finite size scaling,Random constraint satisfaction problems,and phrases Random k-XORSAT},
title = {{Finite size scaling of random XORSAT}},
url = {https://arxiv.org/pdf/1610.07431.pdf},
year = {2016}
}
@techreport{Zhang2018,
abstract = {Finding parameters that minimise a loss function is at the core of many machine learning methods. The Stochastic Gradient Descent algorithm is widely used and delivers state of the art results for many problems. Nonetheless, Stochastic Gradient Descent typically cannot find the global minimum, thus its empirical effectiveness is hitherto mysterious. We derive a correspondence between parameter inference and free energy minimisation in statistical physics. The degree of undersampling plays the role of temperature. Analogous to the energy-entropy competition in statistical physics, wide but shallow minima can be optimal if the system is undersampled, as is typical in many applications. Moreover, we show that the stochasticity in the algorithm has a non-trivial correlation structure which systematically biases it towards wide minima. We illustrate our argument with two prototypical models: image classification using deep learning, and a linear neural network where we can analytically reveal the relationship between entropy and out-of-sample error.},
archivePrefix = {arXiv},
arxivId = {1803.01927},
author = {Zhang, Yao and Saxe, Andrew M. and Advani, Madhu S. and Lee, Alpha A.},
booktitle = {Molecular Physics},
doi = {10.1080/00268976.2018.1483535},
eprint = {1803.01927},
file = {::},
isbn = {1803.01927v1},
issn = {13623028},
keywords = {Machine learning,energy landscape,high-dimensional inference,neural network},
number = {21-22},
pages = {3214--3223},
title = {{Energy–entropy competition and the effectiveness of stochastic gradient descent in machine learning}},
volume = {116},
year = {2018}
}
@article{Brito2018,
abstract = {Swap algorithms can shift the glass transition to lower temperatures, a recent unexplained observation constraining the nature of this phenomenon. Here we show that swap dynamic is governed by an effective potential describing both particle interactions as well as their ability to change size. Requiring its stability is more demanding than for the potential energy alone. This result implies that stable configurations appear at lower energies with swap dynamics, and thus at lower temperatures when the liquid is cooled. $\backslash$maa{\{} The magnitude of this effect is proportional to the width of the radii distribution, and decreases with compression for finite-range purely repulsive interaction potentials.{\}} We test these predictions numerically and discuss the implications of these findings for the glass transition.We extend these results to the case of hard spheres where swap is argued to destroy meta-stable states of the free energy coarse-grained on vibrational time scales. Our analysis unravels the soft elastic modes responsible for the speed up swap induces, and allows us to predict the structure and the vibrational properties of glass configurations reachable with swap. In particular for continuously poly-disperse systems we predict the jamming transition to be dramatically altered, as we confirm numerically. A surprising practical outcome of our analysis is new algorithm that generates ultra-stable glasses by simple descent in an appropriate effective potential.},
archivePrefix = {arXiv},
arxivId = {1801.03796},
author = {Brito, Carolina and Lerner, Edan and Wyart, Matthieu},
doi = {10.1103/PhysRevX.8.031050},
eprint = {1801.03796},
issn = {2160-3308},
keywords = {6470Pf,6520+w7722-d,PACS numbers},
title = {{Theory for Swap Acceleration near the Glass and Jamming Transitions}},
url = {http://arxiv.org/abs/1801.03796{\%}0Ahttp://dx.doi.org/10.1103/PhysRevX.8.031050},
year = {2018}
}
@article{Engel,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Engel, a and {Den Broeck}, C Van},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Quantum},
pages = {1--22},
pmid = {25246403},
title = {{Statistical Mechanics of Learning}},
url = {http://link.springer.com/10.1007/978-3-642-01805-3},
volume = {5400},
year = {2009}
}
@techreport{Livan2017,
abstract = {This is a book for absolute beginners. If you have heard about random matrix theory, commonly denoted RMT, but you do not know what that is, then welcome!, this is the place for you. Our aim is to provide a truly accessible introductory account of RMT for physicists and mathematicians at the beginning of their research career. We tried to write the sort of text we would have loved to read when we were beginning Ph.D. students ourselves. Our book is structured with light and short chapters, and the style is informal. The calculations we found most instructive are spelt out in full. Particular attention is paid to the numerical verification of most analytical results. Our book covers standard material - classical ensembles, orthogonal polynomial techniques, spectral densities and spacings - but also more advanced and modern topics - replica approach and free probability - that are not normally included in elementary accounts on RMT. This book is dedicated to the fond memory of Oriol Bohigas.},
archivePrefix = {arXiv},
arxivId = {1712.07903},
author = {Livan, Giacomo and Novaes, Marcel and Vivo, Pierpaolo},
doi = {10.1007/978-3-319-70885-0},
eprint = {1712.07903},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Livan, Novaes, Vivo - 2017 - Introduction to Random Matrices Theory and Practice.pdf:pdf},
isbn = {9783319708836},
issn = {1523-7052},
pmid = {25961722},
title = {{Introduction to Random Matrices - Theory and Practice}},
url = {http://arxiv.org/abs/1712.07903{\%}0Ahttp://dx.doi.org/10.1007/978-3-319-70885-0},
year = {2017}
}
@techreport{Mace,
abstract = {We describe the application of tools from statistical mechanics to analyse the dynamics of various classes of supervised learning rules in perceptrons. The character of this paper is mostly that of a cross between a biased non-encyclopaedic review and lecture notes: we try to present a coherent and self-contained picture of the basics of this field, to explain the ideas and tricks, to show how the predictions of the theory compare with (simulation) experiments, and to bring together scattered results. Technical details are given explicitly in an appendix. In order to avoid distraction we concentrate the references in a final section. In addition this paper contains some new results: (i) explicit solutions of the macroscopic equations that describe the error evolution for on-line and batch learning rules; (ii) an analysis of the dynamics of arbitrary macroscopic observables (for complete and incomplete training sets), leading to a general Fokker–Planck equation; and (iii) the macroscopic laws describing batch learning with complete training sets. We close the paper with a preliminary expose´ of ongoing research on the dynamics of learning for the case where the training set is incomplete (i.e. where the number of examples scales linearly with the network size).},
archivePrefix = {arXiv},
arxivId = {cond-mat/9705243},
author = {Mace, C. W.H. and Coolen, A. C.C.},
booktitle = {Statistics and Computing},
doi = {10.1023/A:1008896910704},
eprint = {9705243},
file = {::},
issn = {09603174},
keywords = {Generalization,Learning dynamics,Statistical mechanics},
number = {1},
pages = {55--88},
primaryClass = {cond-mat},
title = {{Statistical mechanical analysis of the dynamics of learning in perceptrons}},
volume = {8},
year = {1998}
}
@techreport{Burda2004,
abstract = {Using random matrix technique we determine an exact relation between the eigen-value spectrum of the covariance matrix and of its estimator. This relation can be used in practice to compute eigenvalue invariants of the covariance (correlation) matrix. Results can be applied in various problems where one experimentally estimates correlations in a system with many degrees of freedom, like for instance those in statistical physics, lattice measurements of field theory, genetics, quantitative finance and other applications of multivariate statistics. Statistical systems with many degrees of freedom appear in numerous research areas. One of the most fundamental issues in studies of such systems is the determination of correlations. In practice, one encounters frequently the following situation: one samples the system many times by carrying out independent measurements. For each sample one estimates values of the elements of the covariance matrix, and then takes the average over a set of samples. The statistical uncertainty of the average of individual elements of the matrix generically decreases with the number of independent measurements T as ∼ 1/ √ T. There are N(N + 1)/2 independent elements of the correlation matrix for a system with N degrees of freedom. Thus, naively, the total uncertainty encoded in the correlation matrix may be expected to be proportional to N(N + 1)/2 and 1/ √ T , and therefore to be large for 'non-local' quantities which depend on many elements of the correlation matrix even if one performs a large number of measurements, of the order of the number of degrees of freedom in the system. It turns out that this naive expectation is far from true. Such 'non-local' quantities occur, in particular, in the eigenvalue analysis of the correlation matrix [1]. A question which we address in this paper is how},
author = {Burda, Z and G{\"{o}}rlich, A and Jarosz, A and Jurkiewicz, J},
file = {::},
keywords = {8965Gh,correlation matrix,eigenvalue spectrum PACS: 0540Fb,random matrix theory},
number = {May 2007},
pages = {1--21},
title = {{Signal and noise in empirical correlation matrix 1}},
url = {https://arxiv.org/pdf/cond-mat/0305627.pdf},
year = {2004}
}
@techreport{Nadal2011,
abstract = {Cette th{\`{e}}se est consacr{\'{e}}e {\`{a}} l'{\'{e}}tude des matrices al{\'{e}}atoires et {\`{a}} quelques unes de leurs applications en physique, en particulier en physique statistique et en physique {\{}quantique.C'est{\}} un travail essentiellement analytique compl{\'{e}}t{\'{e}} par quelques simulations num{\'{e}}riques Monte Carlo. Dans un premier temps j'introduis la th{\'{e}}orie des matrices al{\'{e}}atoires de fa{\c{c}}on assez g{\'{e}}n{\'{e}}rale : je d{\'{e}}finis les principaux ensembles de matrices al{\'{e}}atoires (en particulier gaussiens) et d{\'{e}}cris leurs propri{\'{e}}t{\'{e}}s fondamentales (distribution des valeurs propres, densit{\'{e}}, etc). Dans un second temps je m'int{\'{e}}resse {\`{a}} des syst{\`{e}}mes physiques d'interfaces {\`{a}} l'{\'{e}}quilibre qui peuvent {\^{e}}tre mod{\'{e}}lis{\'{e}}s par des marcheurs ''vicieux'', c'est-{\`{a}}-dire des marcheurs al{\'{e}}atoires conditionn{\'{e}}s {\`{a}} ne pas se croiser. On peut montrer que la distribution des positions des marcheurs {\`{a}} un temps donn{\'{e}} est exactement celle des valeurs propres d'une matrice al{\'{e}}atoire. J'{\'{e}}tudie ensuite un probl{\`{e}}me physique qui rel{\`{e}}ve d'un domaine tr{\`{e}}s diff{\'{e}}rent, celui de l'information quantique, mais qui est {\'{e}}galement {\'{e}}troitement reli{\'{e}} aux matrices al{\'{e}}atoires: celui de l'intrication pour des {\'{e}}tats al{\'{e}}atoires dans un syst{\`{e}}me quantique bipartite (fait de deux sous-parties) de grande taille. Enfin je m'int{\'{e}}resse {\`{a}} certaines propri{\'{e}}t{\'{e}}s des matrices al{\'{e}}atoires comme la distribution du nombre de valeurs propres positives ou encore la distribution de la valeur propre maximale (loi de Tracy-Widom pr{\`{e}}s de la moyenne et grandes d{\'{e}}viations loin de la moyenne).},
author = {Nadal, C{\'{e}}line},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Nadal - Unknown - Matrices al{\'{e}}atoires et leurs applications {\`{a}} la physique statistique et quantique.pdf:pdf},
keywords = {Intrication quantique,Marcheurs vicieux,Matrices al{\'{e}}atoires,Mouvement brownien,Statistiques d'extr{\^{e}}mes},
title = {{Matrices al{\'{e}}atoires et leurs applications {\`{a}} la physique statistique et quantique}},
url = {http://tel.archives-ouvertes.fr/tel-00633266},
year = {2011}
}
@techreport{Zdeborova2018,
abstract = {Many questions of fundamental interest in todays science can be formulated as inference problems: Some partial, or noisy, observations are performed over a set of variables and the goal is to recover, or infer, the values of the variables based on the indirect information contained in the measurements. For such problems, the central scientific questions are: Under what conditions is the information contained in the measurements sufficient for a satisfactory inference to be possible? What are the most efficient algorithms for this task? A growing body of work has shown that often we can understand and locate these fundamental barriers by thinking of them as phase transitions in the sense of statistical physics. Moreover, it turned out that we can use the gained physical insight to develop new promising algorithms. Connection between inference and statistical physics is currently witnessing an impressive renaissance and we review here the current state-of-the-art, with a pedagogical focus on the Ising model which formulated as an inference problem we call the planted spin glass. In terms of applications we review two classes of problems: (i) inference of clusters on graphs and networks, with community detection as a special case and (ii) estimating a signal from its noisy linear measurements, with compressed sensing as a case of sparse estimation. Our goal is to provide a pedagogical review for researchers in physics and other fields interested in this fascinating topic.},
archivePrefix = {arXiv},
arxivId = {1511.02476},
author = {Zdeborov{\'{a}}, Lenka and Krzakala, Florent},
booktitle = {Advances in Physics},
doi = {10.1080/00018732.2016.1211393},
eprint = {1511.02476},
file = {::},
isbn = {1511.02476},
issn = {14606976},
keywords = {Bayesian inference,belief propagation,compressed sensing,phase transitions in computer science,spin glass theory,stochastic block model},
number = {5},
pages = {453--552},
title = {{Statistical physics of inference: thresholds and algorithms}},
volume = {65},
year = {2016}
}
@article{JastrzEbski,
abstract = {We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization, remains an open question. In order to tackle this problem we investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE). We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. In particular we find that the ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and that higher values of the ratio lead to wider minima and often better generalization. We confirm these findings experimentally. Further, we include experiments which show that learning rate schedules can be replaced with batch size schedules and that the ratio of learning rate to batch size is an important factor influencing the memorization process.},
archivePrefix = {arXiv},
arxivId = {1711.04623},
author = {Jastrz{\c{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
doi = {arXiv:1711.04623v3},
eprint = {1711.04623},
title = {{Three Factors Influencing Minima in SGD}},
url = {http://arxiv.org/abs/1711.04623},
year = {2017}
}
@article{Dauphin,
abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
archivePrefix = {arXiv},
arxivId = {1406.2572},
author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
doi = {10.1016/j.memsci.2004.03.009},
eprint = {1406.2572},
isbn = {1406.2572},
issn = {10495258},
title = {{Identifying and attacking the saddle point problem in high-dimensional non-convex optimization}},
url = {http://arxiv.org/abs/1406.2572},
year = {2014}
}
@article{Lecun2015,
abstract = {Deep learning (DL) is a high dimensional data reduction technique for constructing high-dimensional predictors in input-output models. DL is a form of machine learning that uses hierarchical layers of latent features. In this article, we review the state-of-the-art of deep learning from a modeling and algorithmic perspective. We provide a list of successful areas of applications in Artificial Intelligence (AI), Image Processing, Robotics and Automation. Deep learning is predictive in its nature rather then inferential and can be viewed as a black-box methodology for high-dimensional function estimation.},
archivePrefix = {arXiv},
arxivId = {1807.07987},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {1807.07987},
file = {::},
isbn = {9781450358095},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {10463930},
title = {{Deep learning}},
url = {http://colah.github.io/},
volume = {521},
year = {2015}
}
@article{He,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/CVPR.2016.90},
eprint = {1512.03385},
isbn = {978-1-4673-8851-1},
issn = {1664-1078},
pmid = {23554596},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@misc{Bun2017a,
abstract = {This review covers recent results concerning the estimation of large covariance matrices using tools from Random Matrix Theory (RMT). We introduce several RMT methods and analytical techniques, such as the Replica formalism and Free Probability, with an emphasis on the Mar{\v{c}}enko–Pastur equation that provides information on the resolvent of multiplicatively corrupted noisy matrices. Special care is devoted to the statistics of the eigenvectors of the empirical correlation matrix, which turn out to be crucial for many applications. We show in particular how these results can be used to build consistent “Rotationally Invariant” estimators (RIE) for large correlation matrices when there is no prior on the structure of the underlying process. The last part of this review is dedicated to some real-world applications within financial markets as a case in point. We establish empirically the efficacy of the RIE framework, which is found to be superior in this case to all previously proposed methods. The case of additively (rather than multiplicatively) corrupted noisy matrices is also dealt with in a special Appendix. Several open problems and interesting technical developments are discussed throughout the paper.},
archivePrefix = {arXiv},
arxivId = {1610.08104},
author = {Bun, Jo{\"{e}}l and Bouchaud, Jean Philippe and Potters, Marc},
booktitle = {Physics Reports},
doi = {10.1016/j.physrep.2016.10.005},
eprint = {1610.08104},
isbn = {1368520081084},
issn = {03701573},
keywords = {Correlation matrix,High dimensional statistics,Random Matrix Theory,Rotational invariant estimator,Spectral decomposition},
pages = {1--109},
pmid = {15774886},
title = {{Cleaning large correlation matrices: Tools from Random Matrix Theory}},
volume = {666},
year = {2017}
}
@article{Biroli2005,
abstract = {In these lecture notes I describe some of the main theoretical ideas emerged to explain the aging dynamics. This is meant to be a very short introduction to aging dynamics and no previous knowledge is assumed. I will go through simple examples that allow one to grasp the main results and predictions.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0504681},
author = {Biroli, Giulio},
doi = {10.1088/1742-5468/2005/05/P05014},
eprint = {0504681},
isbn = {1853467960},
issn = {17425468},
journal = {Journal of Statistical Mechanics: Theory and Experiment},
keywords = {Disordered systems (theory),Memory effects (theory),Slow dynamics and ageing (theory),Spin glasses (theory)},
number = {5},
pages = {291--309},
pmid = {10029903},
primaryClass = {cond-mat},
title = {{A crash course on ageing}},
year = {2005}
}
@article{Li,
abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
archivePrefix = {arXiv},
arxivId = {1804.08838},
author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
doi = {10.1182/blood-2007-10-121426.The},
eprint = {1804.08838},
title = {{Measuring the Intrinsic Dimension of Objective Landscapes}},
url = {http://arxiv.org/abs/1804.08838},
year = {2018}
}
@techreport{Dapello2018,
abstract = {The practical successes of deep neural networks have not been matched by theoret-ical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compres-sion phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlineari-ties like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
author = {Dapello, Joel and Saxe, Andrew M and Bansal, Yamini and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
doi = {10.1016/0002-9378(95)90441-7},
file = {::},
isbn = {0444565191},
issn = {00029378},
pmid = {8610776},
title = {{Modularity View project Neuroscience View project ON THE INFORMATION BOTTLENECK THEORY OF DEEP LEARNING}},
url = {https://www.researchgate.net/publication/325022755},
year = {2018}
}
@article{Burda2013,
abstract = {We review methods to calculate eigenvalue distributions of products of large random matrices. We discuss a generalization of the law of free multiplication to non-Hermitian matrices and give a couple of examples illustrating how to use these methods in practice. In particular we calculate eigenvalue densities of products of Gaussian Hermitian and non-Hermitian matrices including combinations of GUE and Ginibre matrices.},
archivePrefix = {arXiv},
arxivId = {1309.2568},
author = {Burda, Zdzislaw},
doi = {10.1088/1742-6596/473/1/012002},
eprint = {1309.2568},
file = {::},
issn = {17426596},
journal = {Journal of Physics: Conference Series},
number = {1},
pages = {12002},
title = {{Free products of large random matrices-a short review of recent developments}},
volume = {473},
year = {2013}
}
@article{Bottoua,
abstract = {During the last decade, the data sizes have grown faster than the speed of processors. In this context, the capabilities of statistical machine learning methods is limited by the computing time rather than the sample size. A more precise analysis uncovers qualitatively different tradeoffs for the case of small-scale and large-scale learning problems. The large-scale case involves the computational complexity of the underlying optimization algorithm in non-trivial ways. Unlikely optimization algorithms such as stochastic gradient descent show amazing performance for large-scale problems. In particular, second order stochastic gradient and averaged stochastic gradient are asymptotically efficient after a single pass on the training set.},
author = {Bottou, L{\'{e}}on},
isbn = {978-3-7908-2604-3},
pages = {177--186},
title = {{Large-Scale Machine Learning with Stochastic Gradient Descent BT  - Proceedings of COMPSTAT'2010}},
year = {2010}
}
@techreport{Livni2014,
abstract = {It is well-known that neural networks are computationally hard to train. On the other hand, in practice, modern day neural networks are trained efficiently using SGD and a variety of tricks that include different activation functions (e.g. ReLU), over-specification (i.e., train networks which are larger than needed), and regularization. In this paper we revisit the computational complexity of training neural networks from a modern perspective. We provide both positive and negative results, some of them yield new provably efficient and practical algorithms for training certain types of neural networks.},
archivePrefix = {arXiv},
arxivId = {1410.1141},
author = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
doi = {10.1016/0304-3975(91)90394-H},
eprint = {1410.1141},
file = {::},
isbn = {978-3-642-01439-0},
issn = {03043975},
keywords = {Livni14},
mendeley-tags = {Livni14},
title = {{On the Computational Efficiency of Training Neural Networks}},
url = {http://arxiv.org/abs/1410.1141},
year = {2014}
}
@techreport{Dasgupta2000,
abstract = {Recent theoretical work has identified random projection as a promising dimensionality reduc- tion technique for learning mixtures of Gaus- sians. Here we summarize these results and il- lustrate them by a wide variety of experiments on synthetic and real data.},
author = {Dasgupta, Sanjoy},
booktitle = {Uncertainty in artificial intelligence Conference},
doi = {10.1016/j.geomorph.2011.04.001},
file = {::},
isbn = {1-55860-709-9},
issn = {0894-6507},
pages = {143--151},
title = {{Experiments with random projection}},
url = {http://yaroslavvb.com/papers/dasgupta-experiments.pdf{\%}5Cnhttp://dl.acm.org/citation.cfm?id=2073964},
year = {2000}
}
@article{Li2017,
abstract = {Neural network training is a form of numeri-cal optimization in a high-dimensional parame-ter space. Such optimization processes are rarely visualized because of the difficulty of represent-ing high-dimensional dynamics in a visually in-telligible way. We present a simple method for rendering the optimization trajectories of deep networks with low-dimensional plots, using lin-ear projections obtained by principal component analysis. We show that such plots reveal visually distinctive properties of the training processes, and outline opportunities for future investigation.},
author = {Lorch, Eliana},
journal = {The 33rd International Conference on Machine Learning,JMLR volume 48},
title = {{Visualizing Deep Network Training Trajectories with PCA}},
url = {https://icmlviz.github.io/icmlviz2016/assets/papers/24.pdf},
year = {2016}
}
@techreport{Xiao2018,
abstract = {In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers. A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging. While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs. In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme. We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix. These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving. We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.},
archivePrefix = {arXiv},
arxivId = {1806.05393},
author = {Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel S. and Pennington, Jeffrey},
eprint = {1806.05393},
file = {::},
isbn = {0003-3022},
issn = {1938-7228},
title = {{Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1806.05393},
year = {2018}
}
@article{Ballard2017,
abstract = {Machine learning techniques are being increasingly used as flexible non-linear fitting and prediction tools in the physical sciences. Fitting functions that exhibit multiple solutions as local minima can be analysed in terms of the corresponding machine learning landscape. Methods to explore and visualise molecular potential energy landscapes can be applied to these machine learning landscapes to gain new insight into the solution space involved in training and the nature of the corresponding predictions. In particular, we can define quantities analogous to molecular structure, thermodynamics, and kinetics, and relate these emergent properties to the structure of the underlying landscape. This Perspective aims to describe these analogies with examples from recent applications, and suggest avenues for new interdisciplinary research.},
archivePrefix = {arXiv},
arxivId = {1703.07915},
author = {Ballard, Andrew J. and Das, Ritankar and Martiniani, Stefano and Mehta, Dhagash and Sagun, Levent and Stevenson, Jacob D. and Wales, David J.},
doi = {10.1039/c7cp01108c},
eprint = {1703.07915},
file = {::},
isbn = {1463-9076, 1463-9084},
issn = {14639076},
journal = {Physical Chemistry Chemical Physics},
number = {20},
pages = {12585--12603},
title = {{Energy landscapes for machine learning}},
url = {http://pubs.rsc.org/en/content/articlepdf/2017/CP/C7CP01108C},
volume = {19},
year = {2017}
}
@article{Ran2012,
abstract = {Let A be a fixed complex matrix and let u,v be two vectors. The eigenvalues of matrices A+$\tau$uv⊤($\tau$∈R) form a system of intersecting curves. The dependence of the intersections on the vectors u,v is studied. {\textcopyright} 2012 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1104.0618},
author = {Ran, Andr{\'{e}} C.M. and Wojtylak, Micha{\l}},
doi = {10.1016/j.laa.2012.02.027},
eprint = {1104.0618},
file = {::},
isbn = {1011-372X},
issn = {00243795},
journal = {Linear Algebra and Its Applications},
keywords = {Eigenvalues,Perturbations},
number = {2},
pages = {589--600},
title = {{Eigenvalues of rank one perturbations of unstructured matrices}},
url = {http://dx.doi.org/10.1016/j.laa.2012.02.027},
volume = {437},
year = {2012}
}
@techreport{LeCun1991,
abstract = {The learning time of a simple neural network model is obtained through an analytic computation of the eigenvalue spectrum for the Hessian matrix, which describes the second order properties of the cost function in the space of coupling coefficients. The form of the eigenvalue distribution suggests new techniques for accelerating the learning process, and provides a theoretical justification for the choice of centered versus biased state variables.},
author = {LeCun, Yann and Kanter, Ido and Solla, Sara A},
booktitle = {Advances in Neural Information Processing Systems 3},
file = {::},
isbn = {1558601848},
pages = {918--924},
title = {{Second Order Properties of Error Surfaces}},
url = {http://papers.nips.cc/paper/314-second-order-properties-of-error-surfaces.pdf{\%}5Cnfiles/4277/LeCun et al. - 1991 - Second Order Properties of Error Surfaces.pdf{\%}5Cnfiles/4278/314-second-order-properties-of-error-surfaces.html},
year = {1991}
}
@techreport{Schoenholz2017,
abstract = {A number of recent papers have provided evidence that practical design questions about neural networks may be tackled theoretically by studying the behavior of random networks. However, until now the tools available for analyzing random neural networks have been relatively ad-hoc. In this work, we show that the distribution of pre-activations in random neural networks can be exactly mapped onto lattice models in statistical physics. We argue that several previous investigations of stochastic networks actually studied a particular factorial approximation to the full lattice model. For random linear networks and random rectified linear networks we show that the corresponding lattice models in the wide network limit may be systematically approximated by a Gaussian distribution with covariance between the layers of the network. In each case, the approximate distribution can be diagonalized by Fourier transformation. We show that this approximation accurately describes the results of numerical simulations of wide random neural networks. Finally, we demonstrate that in each case the large scale behavior of the random networks can be approximated by an effective field theory.},
archivePrefix = {arXiv},
arxivId = {1710.06570},
author = {Schoenholz, Samuel S. and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
doi = {10.4314/ahs.v12i2.3},
eprint = {1710.06570},
file = {::},
isbn = {1729-0503 (Electronic)$\backslash$r1680-6905 (Linking)},
issn = {1729-0503 (Electronic) 1680-6905 (Linking)},
pmid = {23056012},
title = {{A Correspondence Between Random Neural Networks and Statistical Field Theory}},
url = {http://arxiv.org/abs/1710.06570},
year = {2017}
}
@article{Aubin2018,
abstract = {Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it, strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.},
archivePrefix = {arXiv},
arxivId = {1806.05451},
author = {Aubin, Benjamin and Maillard, Antoine and Barbier, Jean and Krzakala, Florent and Macris, Nicolas and Zdeborov{\'{a}}, Lenka},
eprint = {1806.05451},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Aubin et al. - 2018 - The committee machine Computational to statistical gaps in learning a two-layers neural network.pdf:pdf},
title = {{The committee machine: Computational to statistical gaps in learning a two-layers neural network}},
url = {http://arxiv.org/abs/1806.05451},
year = {2018}
}
@techreport{Cugliandolo1995,
abstract = {We present a detailed analysis for the Langevin dynamics of a spherical spin-glass model (the spherical Sherrington-Kirkpatrick model). The effects of initial conditions on the ultimate dynamical behaviour are closely examined. In addition, the effects of temperature variations in the model are studied. Somewhat surprisingly, this simple model captures some of the effects seen in laboratory spin-glasses. },
archivePrefix = {arXiv},
arxivId = {cond-mat/9502075},
author = {Cugliandolo, L. F. and Dean, D. S.},
booktitle = {Journal of Physics A: General Physics},
doi = {10.1088/0305-4470/28/15/003},
eprint = {9502075},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Cugliandolo, Dean - 1995 - Full dynamical solution for a spherical spin-glass model.pdf:pdf},
issn = {03054470},
number = {15},
pages = {4213--4234},
pmid = {19026422},
primaryClass = {cond-mat},
title = {{Full dynamical solution for a spherical spin-glass model}},
url = {https://arxiv.org/pdf/cond-mat/9502075.pdf},
volume = {28},
year = {1995}
}
@techreport{Spigler2018,
abstract = {We argue that in fully-connected networks a phase transition delimits the over- and under-parametrized regimes where fitting can or cannot be achieved. Under some general conditions, we show that this transition is sharp for the hinge loss. In the whole over-parametrized regime, poor minima of the loss are not encountered during training since the number of constraints to satisfy is too small to hamper minimization. Our findings support a link between this transition and the generalization properties of the network: as we increase the number of parameters of a given model, starting from an under-parametrized network, we observe that the generalization error displays three phases: (i) initial decay, (ii) increase until the transition point --- where it displays a cusp --- and (iii) power law decay toward a constant for the rest of the over-parametrized regime. Thereby we identify the region where the classical phenomenon of over-fitting takes place, and the region where the model keeps improving, in line with previous empirical observations for modern neural networks. The theoretical results presented here appeared elsewhere for a physics audience. The results on generalization are new.},
archivePrefix = {arXiv},
arxivId = {1810.09665},
author = {Spigler, Stefano and Geiger, Mario and D'Ascoli, St{\'{e}}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
doi = {arXiv:1810.09665v2},
eprint = {1810.09665},
file = {::},
title = {{A jamming transition from under- to over-parametrization affects loss landscape and generalization}},
url = {http://arxiv.org/abs/1810.09665},
year = {2018}
}
@techreport{Du2017,
abstract = {We consider the problem of learning a one-hidden-layer neural network with non-overlapping convolutional layer and ReLU activation, i.e., {\$}f(\backslashmathbf{\{}Z{\}}, \backslashmathbf{\{}w{\}}, \backslashmathbf{\{}a{\}}) = \backslashsum{\_}j a{\_}j\backslashsigma(\backslashmathbf{\{}w{\}}{\^{}}T\backslashmathbf{\{}Z{\}}{\_}j){\$}, in which both the convolutional weights {\$}\backslashmathbf{\{}w{\}}{\$} and the output weights {\$}\backslashmathbf{\{}a{\}}{\$} are parameters to be learned. When the labels are the outputs from a teacher network of the same architecture with fixed weights {\$}(\backslashmathbf{\{}w{\}}{\^{}}*, \backslashmathbf{\{}a{\}}{\^{}}*){\$}, we prove that with Gaussian input {\$}\backslashmathbf{\{}Z{\}}{\$}, there is a spurious local minimizer. Surprisingly, in the presence of the spurious local minimizer, gradient descent with weight normalization from randomly initialized weights can still be proven to recover the true parameters with constant probability, which can be boosted to probability {\$}1{\$} with multiple restarts. We also show that with constant probability, the same procedure could also converge to the spurious local minimum, showing that the local minimum plays a non-trivial role in the dynamics of gradient descent. Furthermore, a quantitative analysis shows that the gradient descent dynamics has two phases: it starts off slow, but converges much faster after several iterations.},
archivePrefix = {arXiv},
arxivId = {1712.00779},
author = {Du, Simon S. and Lee, Jason D. and Tian, Yuandong and Poczos, Barnabas and Singh, Aarti},
eprint = {1712.00779},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Du et al. - 2018 - Gradient Descent Learns One-hidden-layer CNN Don't be Afraid of Spurious Local Minima.pdf:pdf},
isbn = {9089325735},
keywords = {Du17},
mendeley-tags = {Du17},
title = {{Gradient Descent Learns One-hidden-layer CNN: Don't be Afraid of Spurious Local Minima}},
url = {http://arxiv.org/abs/1712.00779},
year = {2017}
}
@article{Li2015,
abstract = {We develop the method of stochastic modified equations (SME), in which stochastic gradient algorithms are approximated in the weak sense by continuous-time stochastic differential equations. We exploit the continuous formulation together with optimal control theory to derive novel adaptive hyper-parameter adjustment policies. Our algorithms have competitive performance with the added benefit of being robust to varying models and datasets. This provides a general methodology for the analysis and design of stochastic gradient algorithms.},
archivePrefix = {arXiv},
arxivId = {1511.06251},
author = {Li, Qianxiao and Tai, Cheng and E, Weinan},
eprint = {1511.06251},
isbn = {9781510855144},
issn = {1938-7228},
keywords = {dynamics,stochastic gradient algorithms,stochastic modified equations},
number = {1},
pages = {1--29},
title = {{Stochastic modified equations and adaptive stochastic gradient algorithms}},
url = {http://arxiv.org/abs/1511.06251},
volume = {1},
year = {2015}
}
@article{Bottou2016,
abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
archivePrefix = {arXiv},
arxivId = {1606.04838},
author = {Bottou, L{\'{e}}on and Curtis, Frank E. and Nocedal, Jorge},
doi = {10.1063/1.2965593},
eprint = {1606.04838},
file = {::},
isbn = {9781538634288},
issn = {1089-7690},
pmid = {18715074},
title = {{Optimization Methods for Large-Scale Machine Learning}},
url = {http://arxiv.org/abs/1606.04838},
year = {2016}
}
@techreport{Szegedy2013,
abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
archivePrefix = {arXiv},
arxivId = {1312.6199},
author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
doi = {10.1021/ct2009208},
eprint = {1312.6199},
file = {::},
isbn = {1549-9618},
issn = {15499618},
pmid = {22545027},
title = {{Intriguing properties of neural networks}},
url = {http://arxiv.org/abs/1312.6199},
year = {2013}
}
@techreport{Rahimi2009,
abstract = {Randomized neural networks are immortalized in this well-known AI Koan: In the days when Sussman was a novice, Minsky once came to him as he sat hacking at the PDP-6. "What are you doing?" asked Minsky. "I am training a randomly wired neural net to play tic-tac-toe," Sussman replied. "Why is the net wired ran-domly?" asked Minsky. Sussman replied, "I do not want it to have any precon-ceptions of how to play." Minsky then shut his eyes. "Why do you close your eyes?" Sussman asked his teacher. "So that the room will be empty," replied Minsky. At that moment, Sussman was enlightened. We analyze shallow random networks with the help of concentration of measure inequalities. Specifically, we consider architectures that compute a weighted sum of their inputs after passing them through a bank of arbitrary randomized nonlin-earities. We identify conditions under which these networks exhibit good classification performance, and bound their test error in terms of the size of the dataset and the number of random nonlinearities.},
author = {Rahimi, A and Recht, B},
booktitle = {Advances in neural information processing},
file = {::},
title = {{$\backslash$href{\{}http://dl.acm.org/citation.cfm?id=2981944{\}}{\{}Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning{\}}}},
url = {https://people.eecs.berkeley.edu/{~}brecht/papers/08.rah.rec.nips.pdf},
year = {2009}
}
@article{cifar-10,
abstract = {Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it difficult to learn a good set of filters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is significantly improved by pre-training a layer of features on a large set of unlabeled tiny images.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Krizhevsky, Alex and Hinton, Geoffrey},
doi = {10.1.1.222.9220},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Technical Report},
pages = {1--60},
pmid = {25246403},
title = {{Learning Multiple Layers of Features from Tiny Images}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:Learning+Multiple+Layers+of+Features+from+Tiny+Images{\#}0},
year = {2009}
}
@article{Schmidhuber1997,
abstract = {We present a new algorithm for ?nding low complexity neural networks with high gener- alization capability. The algorithm searches for a $\backslash$?at" minimum of the error function. A ?at minimum is a large connected region in weight-space where the error remains approxi- mately constant. An MDL-based, Bayesian argument suggests that ?at minima correspond to $\backslash$simple" networks and low expected over?tting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into under?tting and over?tting error. Unlike many previous approaches, ours does not require Gaussian assumptions and does not depend on a $\backslash$good" weight prior {\{} instead we have a prior over input/output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second order derivatives, it has backprop's order of complexity. Automatically, it e?ectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, ?at minimum search outperforms (1) conventional backprop, (2) weight decay, (3) $\backslash$optimal brain surgeon" / $\backslash$optimal brain damage". We also provide pseudo code of the algorithm (omitted from the NC-version). 1}},
author = {Schmidhuber, Jurgen},
doi = {10.1162/neco.1997.9.1.1},
file = {:Users/stephane/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - full-text(2).pdf:pdf},
isbn = {0899-7667},
issn = {08997667},
number = {1},
pmid = {9117894},
title = {{Flat minima}},
volume = {9},
year = {1997}
}
@article{Dasgupta2003,
abstract = {A result of Johnson and Lindenstrauss [13] shows that a set of n points in high dimensional Euclidean space can be mapped into an O(log n/ϵ2)-dimensional Euclidean space such that the distance between any two points changes by only a factor of (1 ± ϵ). In this note, we prove this theorem using elementary probabilistic techniques. {\textcopyright} 2002 Wiley Periodicals, Inc. Random Struct. Alg., 22: 60–65, 2002},
author = {Dasgupta, Sanjoy and Gupta, Anupam},
doi = {10.1002/rsa.10073},
file = {::},
isbn = {1098-2418},
issn = {10429832},
journal = {Random Structures and Algorithms},
number = {1},
pages = {60--65},
title = {{An Elementary Proof of a Theorem of Johnson and Lindenstrauss}},
url = {http://cseweb.ucsd.edu/{~}dasgupta/papers/jl.pdf},
volume = {22},
year = {2003}
}
@article{MiMonasson1999,
abstract = {Non-deterministic polynomial time (commonly termed ‘NP-complete') problems are relevant to many computational tasks of practical interest—such as the ‘travelling salesman problem'—but are difficult to solve: the computing time grows exponentially with problem size in the worst case. It has recently been shown that these problems exhibit ‘phase boundaries', across which dramatic changes occur in the computational difficulty and solution character—the problems become easier to solve away from the boundary. Here we report an analytic solution and experimental investigation of the phase transition in K -satisfiability, an archetypal NP-complete problem. Depending on the input parameters, the computing time may grow exponentially or polynomially with problem size; in the former case, we observe a discontinuous transition, whereas in the latter case a continuous (second-order) transition is found. The nature of these transitions may explain the differing computational costs, and suggests directions for improving the efficiency of search algorithms. Similar types of transition should occur in other combinatorial problems and in glassy or granular materials, thereby strengthening the link between computational models and properties of physical systems.},
author = {Monasson, R{\'{e}}mi and Zecchina, Riccardo and Kirkpatrick, Scott and Selman, Bart and Troyansky, Lidror},
doi = {10.1038/22055},
isbn = {0028-0836},
issn = {00280836},
journal = {Nature},
number = {6740},
pages = {133--137},
pmid = {81324900044},
title = {{Determining computational complexity from characteristic 'phase transitions'}},
url = {http://www.phys.ens.fr/{\%}7B{~}{\%}7Dmonasson/Articles/a27.pdf http://www.phys.ens.fr/{~}monasson/Articles/a27.pdf},
volume = {400},
year = {1999}
}
@techreport{Biroli2007,
abstract = {We study the statistics of the largest eigenvalue lambda{\_}max of N x N random matrices with unit variance, but power-law distributed entries, P(M{\_}{\{}ij{\}}){\~{}} |M{\_}{\{}ij{\}}|{\^{}}{\{}-1-mu{\}}. When mu {\textgreater} 4, lambda{\_}max converges to 2 with Tracy-Widom fluctuations of order N{\^{}}{\{}-2/3{\}}. When mu {\textless} 4, lambda{\_}max is of order N{\^{}}{\{}2/mu-1/2{\}} and is governed by Fr$\backslash$'echet statistics. The marginal case mu=4 provides a new class of limiting distribution that we compute explicitely. We extend these results to sample covariance matrices, and show that extreme events may cause the largest eigenvalue to significantly exceed the Marcenko-Pastur edge. Connections with Directed Polymers are briefly discussed.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0609070},
author = {Biroli, G. and Bouchaud, J. P. and Potters, M.},
booktitle = {Epl},
doi = {10.1209/0295-5075/78/10001},
eprint = {0609070},
file = {::},
issn = {02955075},
number = {1},
primaryClass = {cond-mat},
title = {{On the top eigenvalue of heavy-tailed random matrices}},
volume = {78},
year = {2007}
}
@article{Ros2018,
abstract = {We study rough high-dimensional landscapes in which an increasingly stronger preference for a given configuration emerges. Such energy landscapes arise in glass physics and inference. In particular we focus on random Gaussian functions, and on the spiked-tensor model and generalizations. We thoroughly analyze the statistical properties of the corresponding landscapes and characterize the associated geometrical phase transitions. In order to perform our study, we develop a framework based on the Kac-Rice method that allows to compute the complexity of the landscape, i.e. the logarithm of the typical number of stationary points and their Hessian. This approach generalizes the one used to compute rigorously the annealed complexity of mean-field glass models. We discuss its advantages with respect to previous frameworks, in particular the thermodynamical replica method which is shown to lead to partially incorrect predictions.},
archivePrefix = {arXiv},
arxivId = {1804.02686},
author = {Ros, Valentina and Arous, Gerard Ben and Biroli, Giulio and Cammarota, Chiara},
eprint = {1804.02686},
file = {::},
title = {{Complex energy landscapes in spiked-tensor and simple glassy models: ruggedness, arrangements of local minima and phase transitions}},
url = {http://arxiv.org/abs/1804.02686},
year = {2018}
}
@techreport{Li2018a,
abstract = {Neural networks have many successful applications, while much less theoretical understanding has been gained. Towards bridging this gap, we study the problem of learning a two-layer overparameterized ReLU neural network for multi-class classification via stochastic gradient descent (SGD) from random initialization. In the overparameterized setting, when the data comes from mixtures of well-separated distributions, we prove that SGD learns a network with a small generalization error, albeit the network has enough capacity to fit arbitrary labels. Furthermore, the analysis provides interesting insights into several aspects of learning neural networks and can be verified based on empirical studies on synthetic data and on the MNIST dataset.},
archivePrefix = {arXiv},
arxivId = {1808.01204},
author = {Li, Yuanzhi and Liang, Yingyu},
doi = {arXiv:1808.01204v1},
eprint = {1808.01204},
file = {::},
title = {{Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data}},
url = {http://arxiv.org/abs/1808.01204},
year = {2018}
}
@techreport{Gabrie2018,
abstract = {We examine a class of deep learning models with a tractable method to compute information-theoretic quantities. Our contributions are three-fold: (i) We show how entropies and mutual informations can be derived from heuristic statistical physics methods, under the assumption that weight matrices are independent and orthogonally-invariant. (ii) We extend particular cases in which this result is known to be rigorously exact by providing a proof for two-layers networks with Gaussian random weights, using the recently introduced adaptive interpolation method. (iii) We propose an experiment framework with generative models of synthetic datasets, on which we train deep neural networks with a weight constraint designed so that the assumption in (i) is verified during learning. We study the behavior of entropies and mutual informations throughout learning and conclude that, in the proposed setting, the relationship between compression and generalization remains elusive.},
archivePrefix = {arXiv},
arxivId = {1805.09785},
author = {Gabri{\'{e}}, Marylou and Manoel, Andre and Luneau, Cl{\'{e}}ment and Barbier, Jean and Macris, Nicolas and Krzakala, Florent and Zdeborov{\'{a}}, Lenka},
doi = {10.1016/0960-1686(92)90180-S},
eprint = {1805.09785},
file = {::},
issn = {09601686},
title = {{Entropy and mutual information in models of deep neural networks}},
url = {http://arxiv.org/abs/1805.09785},
year = {2018}
}
@techreport{Schoenholz,
abstract = {We study the behavior of untrained neural networks whose weights and biases are randomly distributed using mean field theory. We show the existence of depth scales that naturally limit the maximum depth of signal propagation through these random networks. Our main practical result is to show that random networks may be trained precisely when information can travel through them. Thus, the depth scales that we identify provide bounds on how deep a network may be trained for a specific choice of hyperparameters. As a corollary to this, we argue that in networks at the edge of chaos, one of these depth scales diverges. Thus arbitrarily deep networks may be trained only sufficiently close to criticality. We show that the presence of dropout destroys the order-to-chaos critical point and therefore strongly limits the maximum trainable depth for random networks. Finally, we develop a mean field theory for backpropagation and we show that the ordered and chaotic phases correspond to regions of vanishing and exploding gradient respectively.},
archivePrefix = {arXiv},
arxivId = {1611.01232},
author = {Schoenholz, Samuel S. and Gilmer, Justin and Ganguli, Surya and Sohl-Dickstein, Jascha},
doi = {10.1017/CBO9781107415324.004},
eprint = {1611.01232},
file = {::},
isbn = {9788578110796},
issn = {1098-6596},
pmid = {25246403},
title = {{Deep Information Propagation}},
url = {http://arxiv.org/abs/1611.01232},
year = {2016}
}
@techreport{Advani2017,
abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant "high-dimensional" regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
archivePrefix = {arXiv},
arxivId = {1710.03667},
author = {Advani, Madhu S. and Saxe, Andrew M.},
doi = {10.1007/s10641-015-0445-x},
eprint = {1710.03667},
file = {::},
issn = {1521-3773},
keywords = {Advani17,Generalization error,Neural networks,Random matrix theory},
mendeley-tags = {Advani17},
title = {{High-dimensional dynamics of generalization error in neural networks}},
url = {http://arxiv.org/abs/1710.03667},
year = {2017}
}
@article{Cugliandolo2002,
abstract = {These lecture notes can be read in two ways. The first two Sections contain a review of the phenomenology of several physical systems with slow nonequilibrium dynamics. In the Conclusions we summarize the scenario derived from the solution to some solvable models (p-spin and the like) that are intimately connected to the mode coupling approach (and similar ones) to super-cooled liquids. At the end we list a number of open problems of great relevance in this context. These Sections can be read independently of the body of the paper where we present some of the basic analytic techniques used to study the out of equilibrium dynamics of classical and quantum models with and without disorder. The technical part starts wIth a brief discussion of the role played by the environment and quenched disorder in the dynamics of classical and quantum systems. Later on we expand on the dynamic functional methods and the diagrammatic expansions and resummations used to derive macroscopic equations from the microscopic dynamics. We show why the macroscopic dynamic equations for disordered models and those resulting from self-consistent approximations to non-disordered ones coincide. We review some generic properties of the slow out of equilibrium dynamics like the modifications of FDT and their link to effective temperatures, some generic scaling forms of the correlation functions, etc. Finally we solve a family of mean-field glassy models. The connection between the dynamic treatment and the analysis of the free-energy landscape is also presented. We use pedagogical examples all along these lectures to illustrate the properties and results.},
archivePrefix = {arXiv},
arxivId = {cond-mat/0210312},
author = {Cugliandolo, Leticia F.},
doi = {10.1007/BFb0104821},
eprint = {0210312},
isbn = {978-3-540-40141-4},
pmid = {16122123},
primaryClass = {cond-mat},
title = {{Dynamics of glassy systems}},
url = {http://arxiv.org/abs/cond-mat/0210312},
year = {2002}
}
@article{Barrat1996,
abstract = {We study the out-of-equilibrium dynamics of several models exhibiting$\backslash$n       ageing. We attempt to identify various types of ageing systems using a$\backslash$n       phase space point of view. We introduce a trial classification, based$\backslash$n       on the overlap between two replicas of a system, which evolve together$\backslash$n       until a certain waiting time, and are then totally decoupled. In this$\backslash$n       way we investigate two types of systems, domain growth problems and$\backslash$n       spin glasses, and we show that they behave differently.},
author = {Barrat, A. and Burioni, R. and M{\'{e}}zard, M.},
doi = {10.1088/0305-4470/29/7/005},
isbn = {Part 1},
issn = {03054470},
journal = {Journal of Physics A: Mathematical and General},
number = {7},
pages = {1311--1330},
title = {{Ageing classification in glassy dynamics}},
volume = {29},
year = {1996}
}
@article{Mehta2018,
abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias-variance tradeoff, overfitting, regularization, and generalization before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton-proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists maybe able to contribute. (Notebooks are available at https://physics.bu.edu/{\~{}}pankajm/MLnotebooks.html )},
archivePrefix = {arXiv},
arxivId = {1803.08823},
author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
doi = {10.1016/B978-0-08-044132-0.50011-3},
eprint = {1803.08823},
isbn = {0080441327},
issn = {0021-9606},
title = {{A high-bias, low-variance introduction to Machine Learning for physicists}},
url = {http://arxiv.org/abs/1803.08823},
year = {2018}
}
@article{Zamponi2014,
abstract = {These lecture notes focus on the mean field theory of spin glasses, with particular emphasis on the presence of a very large number of metastable states in these systems. This phenomenon, and some of its physical consequences, will be discussed in details for fully-connected models and for models defined on random lattices. This will be done using the replica and cavity methods. These notes have been prepared for a course of the PhD program in Statistical Mechanics at SISSA, Trieste and at the University of Rome "Sapienza". Part of the material is reprinted from other lecture notes, and when this is done a reference is obviously provided to the original.},
archivePrefix = {arXiv},
arxivId = {1008.4844},
author = {Zamponi, Francesco},
eprint = {1008.4844},
isbn = {1478643770823},
title = {{Mean field theory of spin glasses}},
url = {http://arxiv.org/abs/1008.4844},
year = {2010}
}
@techreport{Li2018b,
abstract = {We present a formal measure-theoretical theory of neural networks (NN) built on probability coupling theory. Our main contributions are summarized as follows. * Built on the formalism of probability coupling theory, we derive an algorithm framework, named Hierarchical Measure Group and Approximate System (HMGAS), nicknamed S-System, that is designed to learn the complex hierarchical, statistical dependency in the physical world. * We show that NNs are special cases of S-System when the probability kernels assume certain exponential family distributions. Activation Functions are derived formally. We further endow geometry on NNs through information geometry, show that intermediate feature spaces of NNs are stochastic manifolds, and prove that "distance" between samples is contracted as layers stack up. * S-System shows NNs are inherently stochastic, and under a set of realistic boundedness and diversity conditions, it enables us to prove that for large size nonlinear deep NNs with a class of losses, including the hinge loss, all local minima are global minima with zero loss errors, and regions around the minima are flat basins where all eigenvalues of Hessians are concentrated around zero, using tools and ideas from mean field theory, random matrix theory, and nonlinear operator equations. * S-System, the information-geometry structure and the optimization behaviors combined completes the analog between Renormalization Group (RG) and NNs. It shows that a NN is a complex adaptive system that estimates the statistic dependency of microscopic object, e.g., pixels, in multiple scales. Unlike clear-cut physical quantity produced by RG in physics, e.g., temperature, NNs renormalize/recompose manifolds emerging through learning/optimization that divide the sample space into highly semantically meaningful groups that are dictated by supervised labels (in supervised NNs).},
archivePrefix = {arXiv},
arxivId = {1811.12783},
author = {Li, Shuai},
doi = {arXiv:1811.12783v1},
eprint = {1811.12783},
file = {::},
title = {{Measure, Manifold, Learning, and Optimization: A Theory Of Neural Networks}},
url = {http://arxiv.org/abs/1811.12783},
year = {2018}
}
@article{Poole2016,
abstract = {The state of the art of the field of feature interactions in telecommunications services is reviewed, concentrating on three major research trends: software engineering approaches, formal methods, and on line techniques. Then, the impact of the new, emerging architectures on the feature interaction problem is considered. A forecast is made about how research in feature interactions needs to readjust to address the new challenges posed by the emerging architectures. {\textcopyright} 2002 Elsevier Science B.V. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {1606.05340},
author = {Calder, Muffy and Kolberg, Mario and Magill, Evan H. and Reiff-Marganiec, Stephan},
doi = {10.1016/S1389-1286(02)00352-3},
eprint = {1606.05340},
file = {::},
isbn = {1786464551},
issn = {13891286},
journal = {Computer Networks},
keywords = {Feature interaction,Incompatibility,Telecommunications services},
number = {1},
pages = {115--141},
title = {{Feature interaction: A critical review and considered forecast}},
volume = {41},
year = {2003}
}
@techreport{Seung1992,
abstract = {Learning from examples in feedforward neural networks is studied within a statistical-mechanical framework. Training is assumed to be stochastic, leading to a Gibbs distribution of networks characterized by a temperature parameter T. Learning of realizable rules as well as of unrealizable rules is considered. In the latter case, the target rule cannot be perfectly realized by a network of the given architecture. Two useful approximate theories of learning from examples are studied: the high-temperature limit and the annealed approximation. Exact treatment of the quenched disorder generated by the random sampling of the examples leads to the use of the replica theory. Of primary interest is the generalization curve, namely, the average generalization error e{\~{}} versus the number of examples P used for training. The theory implies that, for a reduction in eg that remains finite in the large-N limit, P should generally scale as nN, where N is the number of independently adjustable weights in the network. We show that for smooth networks, i.e. , those with continuously varying weights and smooth transfer functions, the generalization curve asymptotically obeys an inverse power law. In contrast, for nonsmooth networks other behaviors can appear, depending on the nature of the nonlinearities as well as the realizability of the rule. In particular, a discontinuous learning transition from a state of poor to a state of perfect generalization can occur in nonsmooth networks learning realizable rules. We illustrate both gradual and continuous learning with a detailed analytical and numerical study of several single-layer perceptron models. Comparing with the exact replica theory of perceptron learning, we find that for realizable rules the high-temperature and annealed theories provide very good approximations to the generalization performance. Assuming this to hold for multilayer networks as well, we propose a classification of possible asymptotic forms of learning curves in general realizable models. For unrealizable rules we find that the above approximations fail in general to predict correctly the shapes of the generalization curves. Another indication of the important role of quenched disorder for unrealizable rules is that the generalization error is not necessarily a monotonically increasing function of temperature. Also, unrealizable rules can possess genuine spin-glass phases indicative of degenerate minima separated by high barriers.},
author = {Seung, H S and Sompolinsky, H and Tishby, N},
booktitle = {PHYSICAL REVIEW A},
file = {::},
keywords = {Seung92},
mendeley-tags = {Seung92},
pages = {15},
title = {{Statistical mechanics of learning from examples}},
url = {https://pdfs.semanticscholar.org/2498/a4e1755f047accc06a6e0fab0b0eb1b37ae0.pdf},
volume = {45},
year = {1992}
}
@article{Bottou,
abstract = {... Proper learning rates ensure that this algorithm converges to$\backslash$na local minimum of the cost ... are updated after the presentation$\backslash$nof each example, according to the gradient of the ... A convergence$\backslash$ntheorem for the stochastic back- propagation algorithm for one hidden$\backslash$nlayered networks ...},
author = {Bottou, L},
doi = {10.1093/emboj/16.9.2333},
isbn = {0010-7824},
issn = {0261-4189},
journal = {Proceedings of Neuro-Nımes},
pmid = {9171347},
title = {{Stochastic gradient learning in neural networks}},
url = {http://leon.bottou.org/publications/pdf/nimes-1991.pdf},
year = {1991}
}
@techreport{Li2018c,
abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
archivePrefix = {arXiv},
arxivId = {1804.08838},
author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
doi = {10.1182/blood-2007-10-121426.The},
eprint = {1804.08838},
file = {::},
title = {{Measuring the Intrinsic Dimension of Objective Landscapes}},
url = {http://arxiv.org/abs/1804.08838},
year = {2018}
}
@techreport{Engel2009,
abstract = {applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Engel, a and {Den Broeck}, C Van},
booktitle = {Quantum},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {::},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {Engel09},
mendeley-tags = {Engel09},
pages = {1--22},
pmid = {25246403},
title = {{Statistical Mechanics of Learning}},
url = {http://link.springer.com/10.1007/978-3-642-01805-3},
volume = {5400},
year = {2009}
}
@article{Hinton2012,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel Rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
eprint = {1207.0580},
file = {::},
isbn = {1053-5888},
issn = {10535888},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
pmid = {13057166},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
volume = {29},
year = {2012}
}
@techreport{Crisanti,
abstract = {In this work we study of the dynamics of large size random neural networks. Different methods have been developed to analyse their behavior, most of them rely on heuristic methods based on Gaussian assumptions regarding the fluctuations in the limit of infinite sizes. These approaches, however, do not justify the underlying assumptions systematically. Furthermore, they are incapable of deriving in general the stability of the derived mean field equations, and they are not amenable to analysis of finite size corrections. Here we present a systematic method based on Path Integrals which overcomes these limitations. We apply the method to a large non-linear rate based neural network with random asymmetric connectivity matrix. We derive the Dynamic Mean Field (DMF) equations for the system, and derive the Lyapunov exponent of the system. Although the main results are well known, here for the first time, we calculate the spectrum of fluctuations around the mean field equations from which we derive the general stability conditions for the DMF states. The methods presented here, can be applied to neural networks with more complex dynamics and architectures. In addition, the theory can be used to compute systematic finite size corrections to the mean field equations.},
archivePrefix = {arXiv},
arxivId = {1809.06042},
author = {Crisanti, A. and Sompolinsky, H.},
doi = {10.1103/PhysRevE.98.062120},
eprint = {1809.06042},
file = {::},
issn = {2470-0045},
title = {{Path Integral Approach to Random Neural Networks}},
url = {http://arxiv.org/abs/1809.06042},
year = {2018}
}
@techreport{Chitour2018,
abstract = {In this article we present a geometric framework to analyze convergence of gradient descent trajectories in the context of neural networks. In the case of linear networks of an arbitrary number of hidden layers, we characterize appropriate quantities which are conserved along the gradient descent system (GDS). We use them to prove boundedness of every trajectory of the GDS, which implies convergence to a critical point. We further focus on the local behavior in the neighborhood of each critical points and perform a study on the associated basin of attractions so as to measure the "possibility" of converging to saddle points and local minima.},
archivePrefix = {arXiv},
arxivId = {1811.03568},
author = {Chitour, Yacine and Liao, Zhenyu and Couillet, Romain},
eprint = {1811.03568},
file = {::},
title = {{A Geometric Approach of Gradient Descent Algorithms in Neural Networks}},
url = {http://arxiv.org/abs/1811.03568},
year = {2018}
}

@article{geiger2019scaling,
  title={Scaling description of generalization with number of parameters in deep learning},
  author={Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and d'Ascoli, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  journal={arXiv preprint arXiv:1901.01608},
  year={2019}
}

@article{belkin2018reconciling,
  title={Reconciling modern machine learning and the bias-variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={arXiv preprint arXiv:1812.11118},
  year={2018}
}

@article{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  journal={arXiv preprint arXiv:1802.08246},
  year={2018}
}

@inproceedings{ba2014deep,
  title={Do deep nets really need to be deep?},
  author={Ba, Jimmy and Caruana, Rich},
  booktitle={Advances in neural information processing systems},
  pages={2654--2662},
  year={2014}
}

@article{novak2018bayesian,
  title={Bayesian deep convolutional networks with many channels are gaussian processes},
  author={Novak, Roman and Xiao, Lechao and Lee, Jaehoon and Bahri, Yasaman and Yang, Greg and Hron, Jiri and Abolafia, Daniel A and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1810.05148},
  year={2018}
}

@article{golatkar2019time,
  title={Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence},
  author={Golatkar, Aditya and Achille, Alessandro and Soatto, Stefano},
  journal={arXiv preprint arXiv:1905.13277},
  year={2019}
}

@article{achille2017critical,
  title={Critical learning periods in deep neural networks},
  author={Achille, Alessandro and Rovere, Matteo and Soatto, Stefano},
  journal={arXiv preprint arXiv:1711.08856},
  year={2017}
}