\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbeel \& Ng(2004)Abbeel and Ng]{abbeel2004apprenticeship}
Abbeel, P. and Ng, A.~Y.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In \emph{Proceedings of the twenty-first international conference on Machine learning}, pp.\ ~1, 2004.

\bibitem[Adeniji et~al.(2023)Adeniji, Xie, Sferrazza, Seo, James, and Abbeel]{adeniji2023language}
Adeniji, A., Xie, A., Sferrazza, C., Seo, Y., James, S., and Abbeel, P.
\newblock Language reward modulation for pretraining reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2308.12270}, 2023.

\bibitem[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew, Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving}
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., et~al.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019.

\bibitem[B{\i}y{\i}k et~al.(2022{\natexlab{a}})B{\i}y{\i}k, Losey, Palan, Landolfi, Shevchuk, and Sadigh]{biyik2022learning}
B{\i}y{\i}k, E., Losey, D.~P., Palan, M., Landolfi, N.~C., Shevchuk, G., and Sadigh, D.
\newblock Learning reward functions from diverse sources of human feedback: Optimally integrating demonstrations and preferences.
\newblock \emph{The International Journal of Robotics Research}, 41\penalty0 (1):\penalty0 45--67, 2022{\natexlab{a}}.

\bibitem[B{\i}y{\i}k et~al.(2022{\natexlab{b}})B{\i}y{\i}k, Talati, and Sadigh]{biyik2022aprel}
B{\i}y{\i}k, E., Talati, A., and Sadigh, D.
\newblock Aprel: A library for active preference-based reward learning algorithms.
\newblock In \emph{2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI)}, pp.\  613--617. IEEE, 2022{\natexlab{b}}.

\bibitem[B{\i}y{\i}k et~al.(2023)B{\i}y{\i}k, Huynh, Kochenderfer, and Sadigh]{biyik2023active}
B{\i}y{\i}k, E., Huynh, N., Kochenderfer, M.~J., and Sadigh, D.
\newblock Active preference-based gaussian process regression for reward learning and optimization.
\newblock \emph{The International Journal of Robotics Research}, pp.\  02783649231208729, 2023.

\bibitem[Bradley \& Terry(1952)Bradley and Terry]{bradley1952rank}
Bradley, R.~A. and Terry, M.~E.
\newblock Rank analysis of incomplete block designs: I. the method of paired comparisons.
\newblock \emph{Biometrika}, 39\penalty0 (3/4):\penalty0 324--345, 1952.

\bibitem[Brohan et~al.(2023)Brohan, Chebotar, Finn, Hausman, Herzog, Ho, Ibarz, Irpan, Jang, Julian, et~al.]{brohan2023can}
Brohan, A., Chebotar, Y., Finn, C., Hausman, K., Herzog, A., Ho, D., Ibarz, J., Irpan, A., Jang, E., Julian, R., et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock In \emph{Conference on Robot Learning}, pp.\  287--318. PMLR, 2023.

\bibitem[Chen et~al.(2023)Chen, Tippur, Wu, Kumar, Adelson, and Agrawal]{chen2023visual}
Chen, T., Tippur, M., Wu, S., Kumar, V., Adelson, E., and Agrawal, P.
\newblock Visual dexterity: In-hand reorientation of novel and complex object shapes.
\newblock \emph{Science Robotics}, 8\penalty0 (84):\penalty0 eadc9244, 2023.

\bibitem[Gu et~al.(2023)Gu, Xiang, Li, Ling, Liu, Mu, Tang, Tao, Wei, Yao, et~al.]{gu2023maniskill2}
Gu, J., Xiang, F., Li, X., Ling, Z., Liu, X., Mu, T., Tang, Y., Tao, S., Wei, X., Yao, Y., et~al.
\newblock Maniskill2: A unified benchmark for generalizable manipulation skills.
\newblock \emph{arXiv preprint arXiv:2302.04659}, 2023.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\  1861--1870. PMLR, 2018.

\bibitem[Hadfield-Menell et~al.(2017)Hadfield-Menell, Milli, Abbeel, Russell, and Dragan]{hadfield2017inverse}
Hadfield-Menell, D., Milli, S., Abbeel, P., Russell, S.~J., and Dragan, A.
\newblock Inverse reward design.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ho \& Ermon(2016)Ho and Ermon]{ho2016generative}
Ho, J. and Ermon, S.
\newblock Generative adversarial imitation learning.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Hoegerman \& Losey(2023)Hoegerman and Losey]{hoegerman2023reward}
Hoegerman, J. and Losey, D.
\newblock Reward learning with intractable normalizing functions.
\newblock \emph{IEEE Robotics and Automation Letters}, 2023.

\bibitem[Hu et~al.(2023)Hu, Mu, Yu, Ding, Wu, Shao, Chen, Wang, Qiao, and Luo]{hu2023tree}
Hu, M., Mu, Y., Yu, X., Ding, M., Wu, S., Shao, W., Chen, Q., Wang, B., Qiao, Y., and Luo, P.
\newblock Tree-planner: Efficient close-loop task planning with large language models.
\newblock \emph{arXiv preprint arXiv:2310.08582}, 2023.

\bibitem[Huang et~al.(2022)Huang, Abbeel, Pathak, and Mordatch]{huang2022language}
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
\newblock Language models as zero-shot planners: Extracting actionable knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\  9118--9147. PMLR, 2022.

\bibitem[Ke et~al.(2021)Ke, Choudhury, Barnes, Sun, Lee, and Srinivasa]{ke2021imitation}
Ke, L., Choudhury, S., Barnes, M., Sun, W., Lee, G., and Srinivasa, S.
\newblock Imitation learning as f-divergence minimization.
\newblock In \emph{Algorithmic Foundations of Robotics XIV: Proceedings of the Fourteenth Workshop on the Algorithmic Foundations of Robotics 14}, pp.\  313--329. Springer, 2021.

\bibitem[Kwon et~al.(2023)Kwon, Xie, Bullard, and Sadigh]{kwon2023reward}
Kwon, M., Xie, S.~M., Bullard, K., and Sadigh, D.
\newblock Reward design with language models.
\newblock \emph{arXiv preprint arXiv:2303.00001}, 2023.

\bibitem[Lee et~al.(2021)Lee, Smith, and Abbeel]{lee2021pebble}
Lee, K., Smith, L., and Abbeel, P.
\newblock Pebble: Feedback-efficient interactive reinforcement learning via relabeling experience and unsupervised pre-training.
\newblock \emph{arXiv preprint arXiv:2106.05091}, 2021.

\bibitem[Liang et~al.(2023)Liang, Huang, Xia, Xu, Hausman, Ichter, Florence, and Zeng]{liang2023code}
Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P., and Zeng, A.
\newblock Code as policies: Language model programs for embodied control.
\newblock In \emph{2023 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  9493--9500. IEEE, 2023.

\bibitem[Liu et~al.(2023)Liu, Bahety, and Song]{liu2023reflect}
Liu, Z., Bahety, A., and Song, S.
\newblock Reflect: Summarizing robot experiences for failure explanation and correction.
\newblock \emph{arXiv preprint arXiv:2306.15724}, 2023.

\bibitem[Luce(1959)]{luce1959individual}
Luce, R.
\newblock \emph{Individual Choice Behavior: A Theoretical Analysis}.
\newblock Wiley, 1959.
\newblock URL \url{https://books.google.com.sg/books?id=c519AAAAMAAJ}.

\bibitem[Ma et~al.(2023)Ma, Liang, Wang, Huang, Bastani, Jayaraman, Zhu, Fan, and Anandkumar]{ma2023eureka}
Ma, Y.~J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu, Y., Fan, L., and Anandkumar, A.
\newblock Eureka: Human-level reward design via coding large language models.
\newblock \emph{arXiv preprint arXiv:2310.12931}, 2023.

\bibitem[Makoviichuk \& Makoviychuk(2021)Makoviichuk and Makoviychuk]{rl-games2021}
Makoviichuk, D. and Makoviychuk, V.
\newblock rl-games: A high-performance framework for reinforcement learning.
\newblock \url{https://github.com/Denys88/rl_games}, May 2021.

\bibitem[Makoviychuk et~al.(2021)Makoviychuk, Wawrzyniak, Guo, Lu, Storey, Macklin, Hoeller, Rudin, Allshire, Handa, et~al.]{makoviychuk2021isaac}
Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., Hoeller, D., Rudin, N., Allshire, A., Handa, A., et~al.
\newblock Isaac gym: High performance gpu-based physics simulation for robot learning.
\newblock \emph{arXiv preprint arXiv:2108.10470}, 2021.

\bibitem[Manchester et~al.(2011)Manchester, Mettin, Iida, and Tedrake]{manchester2011stable}
Manchester, I.~R., Mettin, U., Iida, F., and Tedrake, R.
\newblock Stable dynamic walking over uneven terrain.
\newblock \emph{The International Journal of Robotics Research}, 30\penalty0 (3):\penalty0 265--279, 2011.

\bibitem[Mehta \& Losey(2022)Mehta and Losey]{mehta2022unified}
Mehta, S.~A. and Losey, D.~P.
\newblock Unified learning from demonstrations, corrections, and preferences during physical human-robot interaction.
\newblock \emph{arXiv preprint arXiv:2207.03395}, 2022.

\bibitem[Ng et~al.(2000)Ng, Russell, et~al.]{ng2000algorithms}
Ng, A.~Y., Russell, S., et~al.
\newblock Algorithms for inverse reinforcement learning.
\newblock In \emph{Icml}, volume~1, pp.\ ~2, 2000.

\bibitem[Palan et~al.(2019)Palan, Landolfi, Shevchuk, and Sadigh]{palan2019learning}
Palan, M., Landolfi, N.~C., Shevchuk, G., and Sadigh, D.
\newblock Learning reward functions by integrating human demonstrations and preferences.
\newblock \emph{arXiv preprint arXiv:1906.08928}, 2019.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and Dormann]{stable-baselines3}
Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., and Dormann, N.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (268):\penalty0 1--8, 2021.
\newblock URL \url{http://jmlr.org/papers/v22/20-1364.html}.

\bibitem[Sadigh et~al.(2017)Sadigh, Dragan, Sastry, and Seshia]{sadigh2017active}
Sadigh, D., Dragan, A.~D., Sastry, S., and Seshia, S.~A.
\newblock \emph{Active preference-based learning of reward functions}.
\newblock 2017.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Singh et~al.(2023)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox, Thomason, and Garg]{singh2023progprompt}
Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D., Thomason, J., and Garg, A.
\newblock Progprompt: Generating situated robot task plans using large language models.
\newblock In \emph{2023 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  11523--11530. IEEE, 2023.

\bibitem[Valsecchi et~al.(2020)Valsecchi, Grandia, and Hutter]{valsecchi2020quadrupedal}
Valsecchi, G., Grandia, R., and Hutter, M.
\newblock Quadrupedal locomotion on uneven terrain with sensorized feet.
\newblock \emph{IEEE Robotics and Automation Letters}, 5\penalty0 (2):\penalty0 1548--1555, 2020.

\bibitem[Wang et~al.(2023)Wang, Xian, Chen, Wang, Wang, Fragkiadaki, Erickson, Held, and Gan]{wang2023robogen}
Wang, Y., Xian, Z., Chen, F., Wang, T.-H., Wang, Y., Fragkiadaki, K., Erickson, Z., Held, D., and Gan, C.
\newblock Robogen: Towards unleashing infinite data for automated robot learning via generative simulation.
\newblock \emph{arXiv preprint arXiv:2311.01455}, 2023.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Xie et~al.(2023)Xie, Zhao, Wu, Liu, Luo, Zhong, Yang, and Yu]{xie2023text2reward}
Xie, T., Zhao, S., Wu, C.~H., Liu, Y., Luo, Q., Zhong, V., Yang, Y., and Yu, T.
\newblock Text2reward: Automated dense reward function generation for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2309.11489}, 2023.

\bibitem[Yu et~al.(2023)Yu, Gileadi, Fu, Kirmani, Lee, Arenas, Chiang, Erez, Hasenclever, Humplik, et~al.]{yu2023language}
Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.-H., Arenas, M.~G., Chiang, H.-T.~L., Erez, T., Hasenclever, L., Humplik, J., et~al.
\newblock Language to rewards for robotic skill synthesis.
\newblock \emph{arXiv preprint arXiv:2306.08647}, 2023.

\end{thebibliography}
