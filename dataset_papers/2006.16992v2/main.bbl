\begin{thebibliography}{73}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Arjovsky, M., Shah, A., and Bengio, Y.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{ICML}, 2016.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv}, 2016.

\bibitem[Bai et~al.(2020)Bai, Koltun, and Kolter]{bai2020multiscale}
Bai, S., Koltun, V., and Kolter, J.~Z.
\newblock Multiscale deep equilibrium models.
\newblock \emph{arXiv}, 2020.

\bibitem[Balduzzi et~al.(2017)Balduzzi, Frean, Leary, Lewis, Ma, and
  McWilliams]{balduzzi2017shattered}
Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D., and McWilliams, B.
\newblock The shattered gradients problem: If resnets are the answer, then what
  is the question?
\newblock In \emph{ICML}, 2017.

\bibitem[Bansal et~al.(2018)Bansal, Chen, and Wang]{bansal2018can}
Bansal, N., Chen, X., and Wang, Z.
\newblock Can we gain more from orthogonality regularizations in training deep
  networks?
\newblock In \emph{NIPS}, 2018.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Brock, A., Donahue, J., and Simonyan, K.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock In \emph{ICLR}, 2019.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock \emph{arXiv}, 2020.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and
  Usunier]{cisse2017parseval}
Cisse, M., Bojanowski, P., Grave, E., Dauphin, Y., and Usunier, N.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{ICML}, 2017.

\bibitem[Clevert et~al.(2016)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Clevert, D.-A., Unterthiner, T., and Hochreiter, S.
\newblock Fast and accurate deep network learning by exponential linear units
  (elus).
\newblock In \emph{ICLR}, 2016.

\bibitem[Dai et~al.(2017)Dai, Qi, Xiong, Li, Zhang, Hu, and
  Wei]{dai2017deformable}
Dai, J., Qi, H., Xiong, Y., Li, Y., Zhang, G., Hu, H., and Wei, Y.
\newblock Deformable convolutional networks.
\newblock In \emph{ICCV}, 2017.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Denker et~al.(1989)Denker, Gardner, Graf, Henderson, Howard, Hubbard,
  Jackel, Baird, and Guyon]{denker1989neural}
Denker, J.~S., Gardner, W., Graf, H.~P., Henderson, D., Howard, R.~E., Hubbard,
  W., Jackel, L.~D., Baird, H.~S., and Guyon, I.
\newblock Neural network recognizer for hand-written zip code digits.
\newblock In \emph{NIPS}, 1989.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{AISTATS}, 2010.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola,
  A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv}, 2017.

\bibitem[Hanin \& Rolnick(2018)Hanin and Rolnick]{hanin2018start}
Hanin, B. and Rolnick, D.
\newblock How to start training: The effect of initialization and architecture.
\newblock In \emph{NIPS}, 2018.

\bibitem[Harandi \& Fernando(2016)Harandi and Fernando]{harandi2016generalized}
Harandi, M. and Fernando, B.
\newblock Generalized backpropagation,$\backslash$'$\{$E$\}$ tude de cas:
  Orthogonality.
\newblock \emph{arXiv}, 2016.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{ICCV}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{CVPR}, 2016.

\bibitem[He et~al.(2020)He, Fan, Wu, Xie, and Girshick]{he2019momentum}
He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In \emph{CVPR}, 2020.

\bibitem[Hu et~al.(2020)Hu, Xiao, and Pennington]{hu2020provable}
Hu, W., Xiao, L., and Pennington, J.
\newblock Provable benefit of orthogonal initialization in optimizing deep
  linear networks.
\newblock In \emph{ICLR}, 2020.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{CVPR}, 2017.

\bibitem[Huang et~al.(2018)Huang, Liu, Lang, Yu, Wang, and
  Li]{huang2018orthogonal}
Huang, L., Liu, X., Lang, B., Yu, A.~W., Wang, Y., and Li, B.
\newblock Orthogonal weight normalization: Solution to optimization over
  multiple dependent stiefel manifolds in deep neural networks.
\newblock In \emph{AAAI}, 2018.

\bibitem[Huang et~al.(2020)Huang, Liu, Zhu, Wan, Yuan, Li, and
  Shao]{huang2020controllable}
Huang, L., Liu, L., Zhu, F., Wan, D., Yuan, Z., Li, B., and Shao, L.
\newblock Controllable orthogonalization in training dnns.
\newblock In \emph{CVPR}, 2020.

\bibitem[Huang et~al.(2019)Huang, Ng, Liu, Mason, Zhuang, and
  Liu]{huang2019sndcnn}
Huang, Z., Ng, T., Liu, L., Mason, H., Zhuang, X., and Liu, D.
\newblock Sndcnn: Self-normalizing deep cnns with scaled exponential linear
  units for speech recognition.
\newblock \emph{arXiv}, 2019.

\bibitem[Ioffe \& Szegedy(2015)Ioffe and Szegedy]{ioffe2015batch}
Ioffe, S. and Szegedy, C.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{ICML}, 2015.

\bibitem[Jia et~al.(2017)Jia, Tao, Gao, and Xu]{jia2017improving}
Jia, K., Tao, D., Gao, S., and Xu, X.
\newblock Improving training of deep neural networks via singular value
  bounding.
\newblock In \emph{CVPR}, 2017.

\bibitem[Jia et~al.(2019)Jia, Li, Wen, Liu, and Tao]{jia2019orthogonal}
Jia, K., Li, S., Wen, Y., Liu, T., and Tao, D.
\newblock Orthogonal deep neural networks.
\newblock \emph{PAMI}, 2019.

\bibitem[Klambauer et~al.(2017)Klambauer, Unterthiner, Mayr, and
  Hochreiter]{klambauer2017self}
Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S.
\newblock Self-normalizing neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  971--980, 2017.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, A., Sutskever, I., and Hinton, G.~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, 2012.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{le2015simple}
Le, Q.~V., Jaitly, N., and Hinton, G.~E.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv}, 2015.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.,
  and Jackel, L.~D.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1989.

\bibitem[LeCun et~al.(1990)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1990handwritten}
LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,
  W.~E., and Jackel, L.~D.
\newblock Handwritten digit recognition with a back-propagation network.
\newblock In \emph{NIPS}, 1990.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 1998.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
LeCun, Y.~A., Bottou, L., Orr, G.~B., and M{\"u}ller, K.-R.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}. Springer, 2012.

\bibitem[Lezcano-Casado \& Mart{\'\i}nez-Rubio(2019)Lezcano-Casado and
  Mart{\'\i}nez-Rubio]{lezcano2019cheap}
Lezcano-Casado, M. and Mart{\'\i}nez-Rubio, D.
\newblock Cheap orthogonal constraints in neural networks: A simple
  parametrization of the orthogonal and unitary group.
\newblock \emph{arXiv preprint}, 2019.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Li, and Todorovic]{li2019efficient}
Li, J., Li, F., and Todorovic, S.
\newblock Efficient riemannian optimization on the stiefel manifold via the
  cayley transform.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Haque, Anil, Lucas, Grosse, and
  Jacobsen]{li2019preventing}
Li, Q., Haque, S., Anil, C., Lucas, J., Grosse, R.~B., and Jacobsen, J.-H.
\newblock Preventing gradient attenuation in lipschitz constrained
  convolutional networks.
\newblock In \emph{NIPS}, 2019{\natexlab{b}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{ECCV}, 2014.

\bibitem[Liu et~al.(2020)Liu, Zhu, Fu, de~Melo, and Elgammal]{liu2019oogan}
Liu, B., Zhu, Y., Fu, Z., de~Melo, G., and Elgammal, A.
\newblock Oogan: Disentangling gan with one-hot sampling and orthogonal
  regularization.
\newblock In \emph{AAAI}, 2020.

\bibitem[Luo et~al.(2018)Luo, Ren, Peng, Zhang, and Li]{luo2018differentiable}
Luo, P., Ren, J., Peng, Z., Zhang, R., and Li, J.
\newblock Differentiable learning-to-normalize via switchable normalization.
\newblock \emph{arXiv}, 2018.

\bibitem[Maas et~al.(2013)Maas, Hannun, and Ng]{maas2013rectifier}
Maas, A.~L., Hannun, A.~Y., and Ng, A.~Y.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{ICML}, 2013.

\bibitem[Mishkin \& Matas(2016)Mishkin and Matas]{mishkin2015all}
Mishkin, D. and Matas, J.
\newblock All you need is a good init.
\newblock In \emph{ICLR}, 2016.

\bibitem[Nair \& Hinton(2010)Nair and Hinton]{nair2010rectified}
Nair, V. and Hinton, G.~E.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In \emph{ICML}, 2010.

\bibitem[Nica(2013)]{nica2013mazur}
Nica, B.
\newblock The mazur-ulam theorem.
\newblock \emph{arXiv}, 2013.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{ICML}, 2013.

\bibitem[Pennington et~al.(2018)Pennington, Schoenholz, and
  Ganguli]{pennington2018emergence}
Pennington, J., Schoenholz, S.~S., and Ganguli, S.
\newblock The emergence of spectral universality in deep networks.
\newblock In \emph{AISTATS}, 2018.

\bibitem[Poole et~al.(2016)Poole, Lahiri, Raghu, Sohl-Dickstein, and
  Ganguli]{poole2016exponential}
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J., and Ganguli, S.
\newblock Exponential expressivity in deep neural networks through transient
  chaos.
\newblock In \emph{NIPS}, 2016.

\bibitem[Qiu et~al.(2018)Qiu, Xu, and Cai]{qiu2018frelu}
Qiu, S., Xu, X., and Cai, B.
\newblock Frelu: Flexible rectified linear units for improving convolutional
  neural networks.
\newblock In \emph{ICPR}, 2018.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Ren, S., He, K., Girshick, R., and Sun, J.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In \emph{NIPS}, 2015.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{IJCV}, 2015.

\bibitem[Saxe et~al.(2013)Saxe, McClelland, and Ganguli]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{arXiv}, 2013.

\bibitem[Schoenholz et~al.(2016)Schoenholz, Gilmer, Ganguli, and
  Sohl-Dickstein]{schoenholz2016deep}
Schoenholz, S.~S., Gilmer, J., Ganguli, S., and Sohl-Dickstein, J.
\newblock Deep information propagation.
\newblock \emph{arXiv}, 2016.

\bibitem[Simonyan \& Zisserman(2015)Simonyan and Zisserman]{simonyan2014very}
Simonyan, K. and Zisserman, A.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In \emph{ICLR}, 2015.

\bibitem[Singh \& Krishnan(2020)Singh and Krishnan]{singh2019filter}
Singh, S. and Krishnan, S.
\newblock Filter response normalization layer: Eliminating batch dependence in
  the training of deep neural networks.
\newblock In \emph{CVPR}, 2020.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015highway}
Srivastava, R.~K., Greff, K., and Schmidhuber, J.
\newblock Highway networks.
\newblock \emph{arXiv}, 2015.

\bibitem[Sun et~al.(2019)Sun, Wang, Liu, Miller, Efros, and Hardt]{sun2019test}
Sun, Y., Wang, X., Liu, Z., Miller, J., Efros, A.~A., and Hardt, M.
\newblock Test-time training for out-of-distribution generalization.
\newblock \emph{arXiv}, 2019.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{CVPR}, 2015.

\bibitem[Szegedy et~al.(2017)Szegedy, Ioffe, Vanhoucke, and
  Alemi]{szegedy2017inception}
Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A.~A.
\newblock Inception-v4, inception-resnet and the impact of residual connections
  on learning.
\newblock In \emph{AAAI}, 2017.

\bibitem[Taki(2017)]{taki2017deep}
Taki, M.
\newblock Deep residual networks and weight initialization.
\newblock \emph{arXiv}, 2017.

\bibitem[Tarnowski et~al.(2019)Tarnowski, Warchol, Jastrzebski, Tabor, and
  Nowak]{tarnowski2019dynamical}
Tarnowski, W., Warchol, P., Jastrzebski, S., Tabor, J., and Nowak, M.
\newblock Dynamical isometry is achieved in residual networks in a universal
  way for any activation function.
\newblock In \emph{AISTATS}, 2019.

\bibitem[Trottier et~al.(2017)Trottier, Gigu, Chaib-draa,
  et~al.]{trottier2017parametric}
Trottier, L., Gigu, P., Chaib-draa, B., et~al.
\newblock Parametric exponential linear unit for deep convolutional neural
  networks.
\newblock In \emph{ICMLA}, 2017.

\bibitem[Wang et~al.(2020)Wang, Chen, Chakraborty, and Yu]{wang2019orthogonal}
Wang, J., Chen, Y., Chakraborty, R., and Yu, S.
\newblock Orthogonal convolutional neural networks.
\newblock In \emph{CVPR}, 2020.

\bibitem[Wu \& He(2018)Wu and He]{wu2018group}
Wu, Y. and He, K.
\newblock Group normalization.
\newblock In \emph{ECCV}, 2018.

\bibitem[Wu et~al.(2019)Wu, Kirillov, Massa, Lo, and
  Girshick]{wu2019detectron2}
Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Girshick, R.
\newblock Detectron2.
\newblock \url{https://github.com/facebookresearch/detectron2}, 2019.

\bibitem[Xiang \& Li(2017)Xiang and Li]{xiang2017effects}
Xiang, S. and Li, H.
\newblock On the effects of batch and weight normalization in generative
  adversarial networks.
\newblock \emph{arXiv}, 2017.

\bibitem[Xiao et~al.(2018)Xiao, Bahri, Sohl-Dickstein, Schoenholz, and
  Pennington]{xiao2018dynamical}
Xiao, L., Bahri, Y., Sohl-Dickstein, J., Schoenholz, S.~S., and Pennington, J.
\newblock Dynamical isometry and a mean field theory of cnns: How to train
  10,000-layer vanilla convolutional neural networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Xie et~al.(2017{\natexlab{a}})Xie, Xiong, and Pu]{xie2017all}
Xie, D., Xiong, J., and Pu, S.
\newblock All you need is beyond a good init: Exploring better solution for
  training extremely deep convolutional neural networks with orthonormality and
  modulation.
\newblock In \emph{CVPR}, 2017{\natexlab{a}}.

\bibitem[Xie et~al.(2017{\natexlab{b}})Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Xie, S., Girshick, R., Doll{\'a}r, P., Tu, Z., and He, K.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{CVPR}, 2017{\natexlab{b}}.

\bibitem[Xu et~al.(2015)Xu, Wang, Chen, and Li]{xu2015empirical}
Xu, B., Wang, N., Chen, T., and Li, M.
\newblock Empirical evaluation of rectified activations in convolutional
  network.
\newblock \emph{arXiv}, 2015.

\bibitem[Zagoruyko \& Komodakis(2017)Zagoruyko and
  Komodakis]{zagoruyko2017diracnets}
Zagoruyko, S. and Komodakis, N.
\newblock Diracnets: Training very deep neural networks without
  skip-connections.
\newblock \emph{arXiv}, 2017.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Niwa, and
  Kleijn]{zhang2019approximated}
Zhang, G., Niwa, K., and Kleijn, W.
\newblock Approximated orthonormal normalisation in training neural networks.
\newblock \emph{arXiv}, 2019{\natexlab{a}}.

\bibitem[Zhang et~al.(2018)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Zhang, H., Cisse, M., Dauphin, Y.~N., and Lopez-Paz, D.
\newblock mixup: Beyond empirical risk minimization.
\newblock In \emph{ICLR}, 2018.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Dauphin, and
  Ma]{zhang2019fixup}
Zhang, H., Dauphin, Y.~N., and Ma, T.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In \emph{ICLR}, 2019{\natexlab{b}}.

\end{thebibliography}
