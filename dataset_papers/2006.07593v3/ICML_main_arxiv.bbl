\begin{thebibliography}{75}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Back(1996)]{back1996evolutionary}
Back, T.
\newblock \emph{Evolutionary algorithms in theory and practice: evolution
  strategies, evolutionary programming, genetic algorithms}.
\newblock Oxford university press, 1996.

\bibitem[Baker et~al.(2017)Baker, Gupta, Naik, and Raskar]{baker2016designing}
Baker, B., Gupta, O., Naik, N., and Raskar, R.
\newblock Designing neural network architectures using reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2017.

\bibitem[Berg et~al.(1984)Berg, Christensen, and Ressel]{berg1984harmonic}
Berg, C., Christensen, J. P.~R., and Ressel, P.
\newblock \emph{Harmonic analysis on semigroups}.
\newblock Springer-Verlag, 1984.

\bibitem[Bergstra et~al.(2011)Bergstra, Bardenet, Bengio, and
  K{\'e}gl]{Bergstra_2011Algorithms}
Bergstra, J.~S., Bardenet, R., Bengio, Y., and K{\'e}gl, B.
\newblock Algorithms for hyper-parameter optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2546--2554, 2011.

\bibitem[Burkard \& Cela(1999)Burkard and Cela]{burkard1999}
Burkard, R.~E. and Cela, E.
\newblock Linear assignment problems and extensions.
\newblock In \emph{Handbook of combinatorial optimization}, pp.\  75--149.
  Springer, 1999.

\bibitem[Contal et~al.(2013)Contal, Buffoni, Robicquet, and
  Vayatis]{Contal_2013Parallel}
Contal, E., Buffoni, D., Robicquet, A., and Vayatis, N.
\newblock Parallel gaussian process optimization with upper confidence bound
  and pure exploration.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases}, pp.\
   225--240. Springer, 2013.

\bibitem[Desautels et~al.(2014)Desautels, Krause, and
  Burdick]{Desautels_2014Parallelizing}
Desautels, T., Krause, A., and Burdick, J.~W.
\newblock Parallelizing exploration-exploitation tradeoffs in gaussian process
  bandit optimization.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 3873--3923, 2014.

\bibitem[Do~Ba et~al.(2011)Do~Ba, Nguyen, Nguyen, and
  Rubinfeld]{do2011sublinear}
Do~Ba, K., Nguyen, H.~L., Nguyen, H.~N., and Rubinfeld, R.
\newblock Sublinear time algorithms for {E}arth {M}over's distance.
\newblock \emph{Theory of Computing Systems}, 48\penalty0 (2):\penalty0
  428--442, 2011.

\bibitem[Dong \& Yang(2019)Dong and Yang]{dong2019searching}
Dong, X. and Yang, Y.
\newblock Searching for a robust neural architecture in four gpu hours.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1761--1770, 2019.

\bibitem[Dong \& Yang(2020)Dong and Yang]{NASBench201}
Dong, X. and Yang, Y.
\newblock Nas-bench-201: Extending the scope of reproducible neural
  architecture search.
\newblock \emph{International Conference on Learning Representation}, 2020.

\bibitem[Elsken et~al.(2019{\natexlab{a}})Elsken, Metzen, and
  Hutter]{elsken2019efficient}
Elsken, T., Metzen, J.~H., and Hutter, F.
\newblock Efficient multi-objective neural architecture search via lamarckian
  evolution.
\newblock \emph{International Conference on Learning Representation},
  2019{\natexlab{a}}.

\bibitem[Elsken et~al.(2019{\natexlab{b}})Elsken, Metzen, and
  Hutter]{elsken2019neural}
Elsken, T., Metzen, J.~H., and Hutter, F.
\newblock Neural architecture search: A survey.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (55):\penalty0 1--21, 2019{\natexlab{b}}.

\bibitem[Falkner et~al.(2018)Falkner, Klein, and Hutter]{falkner2018bohb}
Falkner, S., Klein, A., and Hutter, F.
\newblock Bohb: Robust and efficient hyperparameter optimization at scale.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1436--1445, 2018.

\bibitem[Flamary \& Courty(2017)Flamary and Courty]{flamary2017pot}
Flamary, R. and Courty, N.
\newblock Pot python optimal transport library.
\newblock \emph{GitHub: https://github. com/rflamary/POT}, 2017.

\bibitem[Frazier(2018)]{frazier2018tutorial}
Frazier, P.~I.
\newblock A tutorial on {B}ayesian optimization.
\newblock \emph{arXiv preprint arXiv:1807.02811}, 2018.

\bibitem[Gao et~al.(2010)Gao, Xiao, Tao, and Li]{gao2010survey}
Gao, X., Xiao, B., Tao, D., and Li, X.
\newblock A survey of graph edit distance.
\newblock \emph{Pattern Analysis and applications}, 13\penalty0 (1):\penalty0
  113--129, 2010.

\bibitem[Garnett et~al.(2010)Garnett, Osborne, and
  Roberts]{garnett2010bayesian}
Garnett, R., Osborne, M.~A., and Roberts, S.~J.
\newblock Bayesian optimization for sensor set selection.
\newblock In \emph{Proceedings of the 9th ACM/IEEE international conference on
  information processing in sensor networks}, pp.\  209--219, 2010.

\bibitem[Ginsbourger et~al.(2010)Ginsbourger, Le~Riche, and
  Carraro]{Ginsbourger_2010Kriging}
Ginsbourger, D., Le~Riche, R., and Carraro, L.
\newblock Kriging is well-suited to parallelize optimization.
\newblock In \emph{Computational Intelligence in Expensive Optimization
  Problems}, pp.\  131--162. Springer, 2010.

\bibitem[Gonz{\'a}lez et~al.(2016)Gonz{\'a}lez, Dai, Hennig, and
  Lawrence]{Gonzalez_2015Batch}
Gonz{\'a}lez, J., Dai, Z., Hennig, P., and Lawrence, N.~D.
\newblock Batch {B}ayesian optimization via local penalization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  648--657, 2016.

\bibitem[Gopakumar et~al.(2018)Gopakumar, Gupta, Rana, Nguyen, and
  Venkatesh]{gopakumar2018algorithmic_NIPS}
Gopakumar, S., Gupta, S., Rana, S., Nguyen, V., and Venkatesh, S.
\newblock Algorithmic assurance: An active approach to algorithmic testing
  using {B}ayesian optimisation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5465--5473, 2018.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2017)Hern{\'a}ndez-Lobato, Requeima,
  Pyzer-Knapp, and Aspuru-Guzik]{Hernandez_2017Parallel}
Hern{\'a}ndez-Lobato, J.~M., Requeima, J., Pyzer-Knapp, E.~O., and
  Aspuru-Guzik, A.
\newblock Parallel and distributed {T}hompson sampling for large-scale
  accelerated exploration of chemical space.
\newblock \emph{In International Conference on Machine Learning}, pp.\
  1470--1479, 2017.

\bibitem[Jin et~al.(2018)Jin, Song, and Hu]{jin2018auto}
Jin, H., Song, Q., and Hu, X.
\newblock Auto-keras: Efficient neural architecture search with network
  morphism.
\newblock 2018.

\bibitem[Kandasamy et~al.(2018)Kandasamy, Neiswanger, Schneider, Poczos, and
  Xing]{kandasamy2018neural}
Kandasamy, K., Neiswanger, W., Schneider, J., Poczos, B., and Xing, E.~P.
\newblock Neural architecture search with bayesian optimisation and optimal
  transport.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2016--2025, 2018.

\bibitem[Kathuria et~al.(2016)Kathuria, Deshpande, and
  Kohli]{Kathuria_NIPS2016Batched}
Kathuria, T., Deshpande, A., and Kohli, P.
\newblock Batched {G}aussian process bandit optimization via determinantal
  point processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4206--4214, 2016.

\bibitem[Kondor \& Lafferty(2002)Kondor and Lafferty]{kondor2002diffusion}
Kondor, R. and Lafferty, J.
\newblock Diffusion kernels on graphs and other discrete input spaces.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  315--322, 2002.

\bibitem[Kulesza \& Taskar(2011)Kulesza and Taskar]{Kulesza_2011kDPP}
Kulesza, A. and Taskar, B.
\newblock k-dpps: Fixed-size determinantal point processes.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning}, pp.\  1193--1200, 2011.

\bibitem[Kulesza et~al.(2012)Kulesza, Taskar,
  et~al.]{Kulesza_2012Determinantal}
Kulesza, A., Taskar, B., et~al.
\newblock Determinantal point processes for machine learning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  5\penalty0 (2--3):\penalty0 123--286, 2012.

\bibitem[Le \& Nguyen(2021)Le and Nguyen]{le2021ept}
Le, T. and Nguyen, T.
\newblock Entropy partial transport with tree metrics: Theory and practice.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3835--3843, 2021.

\bibitem[Le et~al.(2019{\natexlab{a}})Le, Huynh, Ho, Phung, and
  Yamada]{le2019wb}
Le, T., Huynh, V., Ho, N., Phung, D., and Yamada, M.
\newblock On scalable variant of wasserstein barycenter.
\newblock \emph{arXiv preprint arXiv:1910.04483}, 2019{\natexlab{a}}.

\bibitem[Le et~al.(2019{\natexlab{b}})Le, Yamada, Fukumizu, and
  Cuturi]{le2019tree}
Le, T., Yamada, M., Fukumizu, K., and Cuturi, M.
\newblock Tree-sliced variants of {W}asserstein distances.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  12283--12294, 2019{\natexlab{b}}.

\bibitem[Le et~al.(2021)Le, Ho, and Yamada]{le2021fba}
Le, T., Ho, N., and Yamada, M.
\newblock Flow-based alignment approaches for probability measures in different
  spaces.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3934--3942, 2021.

\bibitem[Li \& Jamieson(2018)Li and Jamieson]{li2018hyperband}
Li, L. and Jamieson, K.
\newblock Hyperband: A novel bandit-based approach to hyperparameter
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 18:\penalty0 1--52,
  2018.

\bibitem[Liu et~al.(2018)Liu, Simonyan, Vinyals, Fernando, and
  Kavukcuoglu]{liu2018hierarchical}
Liu, H., Simonyan, K., Vinyals, O., Fernando, C., and Kavukcuoglu, K.
\newblock Hierarchical representations for efficient architecture search.
\newblock \emph{International Conference on Learning Representation}, 2018.

\bibitem[Liu et~al.(2019)Liu, Simonyan, and Yang]{liu2019darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock Darts: Differentiable architecture search.
\newblock \emph{International Conference on Learning Representation}, 2019.

\bibitem[Luo et~al.(2018)Luo, Tian, Qin, Chen, and Liu]{luo2018neural}
Luo, R., Tian, F., Qin, T., Chen, E., and Liu, T.-Y.
\newblock Neural architecture optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7816--7827, 2018.

\bibitem[M{\'e}moli(2011)]{memoli2011gromov}
M{\'e}moli, F.
\newblock Gromov--wasserstein distances and the metric approach to object
  matching.
\newblock \emph{Foundations of computational mathematics}, 11\penalty0
  (4):\penalty0 417--487, 2011.

\bibitem[M{\'e}moli et~al.(2019)M{\'e}moli, Smith, and Wan]{memoli2019gromov}
M{\'e}moli, F., Smith, Z., and Wan, Z.
\newblock Gromov-hausdorff distances on $ p $-metric spaces and ultrametric
  spaces.
\newblock \emph{arXiv preprint arXiv:1912.00564}, 2019.

\bibitem[M{\'e}moli et~al.(2021)M{\'e}moli, Munk, Wan, and
  Weitkamp]{memoli2021ultrametric}
M{\'e}moli, F., Munk, A., Wan, Z., and Weitkamp, C.
\newblock The ultrametric gromov-wasserstein distance.
\newblock \emph{arXiv preprint arXiv:2101.05756}, 2021.

\bibitem[Messmer \& Bunke(1998)Messmer and Bunke]{messmer1998new}
Messmer, B.~T. and Bunke, H.
\newblock A new algorithm for error-tolerant subgraph isomorphism detection.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 20\penalty0 (5):\penalty0 493--504, 1998.

\bibitem[Nguyen \& Osborne(2020)Nguyen and Osborne]{nguyen2019knowing}
Nguyen, V. and Osborne, M.~A.
\newblock Knowing the what but not the where in {B}ayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7317--7326. PMLR, 2020.

\bibitem[Nguyen et~al.(2016)Nguyen, Rana, Gupta, Li, and
  Venkatesh]{Nguyen_ICDM2016Budgeted}
Nguyen, V., Rana, S., Gupta, S.~K., Li, C., and Venkatesh, S.
\newblock Budgeted batch {B}ayesian optimization.
\newblock In \emph{16th International Conference on Data Mining (ICDM)}, pp.\
  1107--1112, 2016.

\bibitem[Nguyen et~al.(2020)Nguyen, Schulze, and Osborne]{boil}
Nguyen, V., Schulze, S., and Osborne, M.~A.
\newblock Bayesian optimization for iterative learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Parker-Holder et~al.(2020)Parker-Holder, Nguyen, and Roberts]{pb2}
Parker-Holder, J., Nguyen, V., and Roberts, S.~J.
\newblock Provably efficient online hyperparameter optimization with
  population-based bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Peyr\'e \& Cuturi(2019)Peyr\'e and Cuturi]{PeyreCuturiBook}
Peyr\'e, G. and Cuturi, M.
\newblock Computational optimal transport.
\newblock \emph{Foundations and Trends in Machine Learning}, 11\penalty0
  (5-6):\penalty0 355--607, 2019.

\bibitem[Pham et~al.(2018)Pham, Guan, Zoph, Le, and Dean]{pham2018efficient}
Pham, H., Guan, M., Zoph, B., Le, Q., and Dean, J.
\newblock Efficient neural architecture search via parameters sharing.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4095--4104, 2018.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{Rahimi_2007Random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1177--1184, 2007.

\bibitem[Rana et~al.(2017)Rana, Li, Gupta, Nguyen, and
  Venkatesh]{Rana_ICML2017High}
Rana, S., Li, C., Gupta, S., Nguyen, V., and Venkatesh, S.
\newblock High dimensional {B}ayesian optimization with elastic {G}aussian
  process.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  2883--2891, 2017.

\bibitem[Rasmussen(2006)]{Rasmussen_2006gaussian}
Rasmussen, C.~E.
\newblock Gaussian processes for machine learning.
\newblock 2006.

\bibitem[Real et~al.(2017)Real, Moore, Selle, Saxena, Suematsu, Tan, Le, and
  Kurakin]{real2017large}
Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y.~L., Tan, J., Le,
  Q.~V., and Kurakin, A.
\newblock Large-scale evolution of image classifiers.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  2902--2911, 2017.

\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{real2019regularized}
Real, E., Aggarwal, A., Huang, Y., and Le, Q.~V.
\newblock Regularized evolution for image classifier architecture search.
\newblock In \emph{Proceedings of the AAAI conference on Artificial
  Intelligence}, volume~33, pp.\  4780--4789, 2019.

\bibitem[Ru et~al.(2020)Ru, Alvi, Nguyen, Osborne, and Roberts]{cocabo}
Ru, B., Alvi, A.~S., Nguyen, V., Osborne, M.~A., and Roberts, S.~J.
\newblock Bayesian optimisation over multiple continuous and categorical
  inputs.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Ru et~al.(2021)Ru, Wan, Dong, and Osborne]{ru_iclr2021}
Ru, B., Wan, X., Dong, X., and Osborne, M.~A.
\newblock Interpretable neural architecture search via bayesian optimisation
  with weisfeiler-lehman kernels.
\newblock \emph{International Conference on Learning Representation}, 2021.

\bibitem[Sciuto et~al.(2019)Sciuto, Yu, Jaggi, Musat, and
  Salzmann]{sciuto2019evaluating}
Sciuto, C., Yu, K., Jaggi, M., Musat, C., and Salzmann, M.
\newblock Evaluating the search phase of neural architecture search.
\newblock \emph{arXiv preprint arXiv:1902.08142}, 2019.

\bibitem[Semple \& Steel(2003)Semple and Steel]{semple2003phylogenetics}
Semple, C. and Steel, M.
\newblock Phylogenetics.
\newblock \emph{Oxford Lecture Series in Mathematics and its Applications},
  2003.

\bibitem[Shah et~al.(2018)Shah, Wu, Lu, Zhang, Sasidharan, DeMar, Guok,
  Macauley, Pouyoul, Kim, et~al.]{shah2018amoebanet}
Shah, S. A.~R., Wu, W., Lu, Q., Zhang, L., Sasidharan, S., DeMar, P., Guok, C.,
  Macauley, J., Pouyoul, E., Kim, J., et~al.
\newblock Amoebanet: An sdn-enabled network service for big data science.
\newblock \emph{Journal of Network and Computer Applications}, 119:\penalty0
  70--82, 2018.

\bibitem[Shahriari et~al.(2016)Shahriari, Swersky, Wang, Adams, and
  de~Freitas]{Shahriari_2016Taking}
Shahriari, B., Swersky, K., Wang, Z., Adams, R.~P., and de~Freitas, N.
\newblock Taking the human out of the loop: A review of {B}ayesian
  optimization.
\newblock \emph{Proceedings of the IEEE}, 104\penalty0 (1):\penalty0 148--175,
  2016.

\bibitem[Smola \& Kondor(2003)Smola and Kondor]{smola2003kernels}
Smola, A.~J. and Kondor, R.
\newblock Kernels and regularization on graphs.
\newblock In \emph{Learning theory and kernel machines}, pp.\  144--158.
  Springer, 2003.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{Snoek_2012Practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical {B}ayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2951--2959, 2012.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{Snoek_2015Scalable}
Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N.,
  Patwary, M., Prabhat, M., and Adams, R.
\newblock Scalable {B}ayesian optimization using deep neural networks.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pp.\  2171--2180, 2015.

\bibitem[Springenberg et~al.(2016)Springenberg, Klein, Falkner, and
  Hutter]{Springenberg_2016Bayesian}
Springenberg, J.~T., Klein, A., Falkner, S., and Hutter, F.
\newblock Bayesian optimization with robust bayesian neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4134--4142, 2016.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{Srinivas_2010Gaussian}
Srinivas, N., Krause, A., Kakade, S., and Seeger, M.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning}, pp.\  1015--1022, 2010.

\bibitem[Suganuma et~al.(2017)Suganuma, Shirakawa, and
  Nagao]{suganuma2017genetic}
Suganuma, M., Shirakawa, S., and Nagao, T.
\newblock A genetic programming approach to designing convolutional neural
  network architectures.
\newblock In \emph{Proceedings of the Genetic and Evolutionary Computation
  Conference}, pp.\  497--504, 2017.

\bibitem[Villani(2003)]{villani2003topics}
Villani, C.
\newblock \emph{Topics in optimal transportation}.
\newblock American Mathematical Soc., 2003.

\bibitem[Vishwanathan et~al.(2010)Vishwanathan, Schraudolph, Kondor, and
  Borgwardt]{vishwanathan2010graph}
Vishwanathan, S. V.~N., Schraudolph, N.~N., Kondor, R., and Borgwardt, K.~M.
\newblock Graph kernels.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1201--1242, 2010.

\bibitem[Wallis et~al.(2001)Wallis, Shoubridge, Kraetz, and
  Ray]{wallis2001graph}
Wallis, W.~D., Shoubridge, P., Kraetz, M., and Ray, D.
\newblock Graph distances using graph union.
\newblock \emph{Pattern Recognition Letters}, 22\penalty0 (6-7):\penalty0
  701--704, 2001.

\bibitem[Wan et~al.(2021)Wan, Nguyen, Ha, Ru, Lu, and Osborne]{wan2021think}
Wan, X., Nguyen, V., Ha, H., Ru, B., Lu, C., and Osborne, M.~A.
\newblock Think global and act local: {B}ayesian optimisation over
  high-dimensional categorical and mixed search spaces.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Wang et~al.(2020)Wang, Zhao, Jinnai, Tian, and Fonseca]{alphax}
Wang, L., Zhao, Y., Jinnai, Y., Tian, Y., and Fonseca, R.
\newblock Alphax: exploring neural architectures with deep neural networks and
  monte carlo tree search.
\newblock \emph{The Thirty-Fourth AAAI Conference on Artificial Intelligence},
  2020.

\bibitem[Wang et~al.(2018)Wang, Gehring, Kohli, and Jegelka]{wang2018batched}
Wang, Z., Gehring, C., Kohli, P., and Jegelka, S.
\newblock Batched large-scale {B}ayesian optimization in high-dimensional
  spaces.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  745--754, 2018.

\bibitem[White et~al.(2021)White, Neiswanger, and Savani]{white2019bananas}
White, C., Neiswanger, W., and Savani, Y.
\newblock Bananas: Bayesian optimization with neural architectures for neural
  architecture search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, 2021.

\bibitem[Xie \& Yuille(2017)Xie and Yuille]{xie2017genetic}
Xie, L. and Yuille, A.
\newblock Genetic cnn.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1379--1388, 2017.

\bibitem[Xie et~al.(2019)Xie, Zheng, Liu, and Lin]{xie2018snas}
Xie, S., Zheng, H., Liu, C., and Lin, L.
\newblock Snas: stochastic neural architecture search.
\newblock \emph{International Conference on Learning Representation}, 2019.

\bibitem[Yao et~al.(2020)Yao, Xu, Tu, and Zhu]{yao2020efficient}
Yao, Q., Xu, J., Tu, W.-W., and Zhu, Z.
\newblock Efficient neural architecture search via proximal iterations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2020.

\bibitem[Ying et~al.(2019)Ying, Klein, Christiansen, Real, Murphy, and
  Hutter]{ying2019bench}
Ying, C., Klein, A., Christiansen, E., Real, E., Murphy, K., and Hutter, F.
\newblock Nas-bench-101: Towards reproducible neural architecture search.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7105--7114, 2019.

\bibitem[Zhong et~al.(2018)Zhong, Yan, Wu, Shao, and Liu]{zhong2018practical}
Zhong, Z., Yan, J., Wu, W., Shao, J., and Liu, C.-L.
\newblock Practical block-wise neural network architecture generation.
\newblock In \emph{Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pp.\  2423--2432, 2018.

\bibitem[Zoph \& Le(2017)Zoph and Le]{zoph2016neural}
Zoph, B. and Le, Q.~V.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{International Conference on Representation Learning}, 2017.

\end{thebibliography}
