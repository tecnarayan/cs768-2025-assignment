\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anil et~al.(2022)Anil, Wu, Andreassen, Lewkowycz, Misra, Ramasesh, Slone, Gur{-}Ari, Dyer, and Neyshabur]{TFLength}
Anil, C., Wu, Y., Andreassen, A., Lewkowycz, A., Misra, V., Ramasesh, V.~V., Slone, A., Gur{-}Ari, G., Dyer, E., and Neyshabur, B.
\newblock Exploring length generalization in large language models.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Banayeeanzade et~al.(2021)Banayeeanzade, Mirzaiezadeh, Hasani, and Soleymani]{GeMCL}
Banayeeanzade, M., Mirzaiezadeh, R., Hasani, H., and Soleymani, M.
\newblock Generative vs. discriminative: Rethinking the meta-continual learning.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Beaulieu et~al.(2020)Beaulieu, Frati, Miconi, Lehman, Stanley, Clune, and Cheney]{ANML}
Beaulieu, S., Frati, L., Miconi, T., Lehman, J., Stanley, K.~O., Clune, J., and Cheney, N.
\newblock Learning to continually learn.
\newblock In \emph{{ECAI}}, 2020.

\bibitem[Bishop(2006)]{PRML}
Bishop, C.~M.
\newblock \emph{Pattern Recognition and Machine Learning}.
\newblock Berlin, Heidelberg, 2006.
\newblock ISBN 0387310738.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{GPT3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Dokania, Ajanthan, and Torr]{Chaudhry18Riemannian}
Chaudhry, A., Dokania, P.~K., Ajanthan, T., and Torr, P. H.~S.
\newblock Riemannian walk for incremental learning: Understanding forgetting and intransigence.
\newblock In \emph{{ECCV}}, 2018.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Rohrbach, Elhoseiny, Ajanthan, Dokania, Torr, and Ranzato]{TinyMemory}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.~K., Torr, P. H.~S., and Ranzato, M.
\newblock Continual learning with tiny episodic memories.
\newblock \emph{CoRR}, abs/1902.10486, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarl{\'{o}}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{Performer}
Choromanski, K.~M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl{\'{o}}s, T., Hawkins, P., Davis, J.~Q., Mohiuddin, A., Kaiser, L., Belanger, D.~B., Colwell, L.~J., and Weller, A.
\newblock Rethinking attention with {Performers}.
\newblock In \emph{ICLR}, 2021.

\bibitem[Darmois(1935)]{Darmois1935}
Darmois, G.
\newblock Sur les lois de probabilit{\'e}a estimation exhaustive.
\newblock \emph{CR Acad. Sci. Paris}, 260\penalty0 (1265):\penalty0 85, 1935.

\bibitem[Farajtabar et~al.(2020)Farajtabar, Azizan, Mott, and Li]{Farajtabar20Orthogonal}
Farajtabar, M., Azizan, N., Mott, A., and Li, A.
\newblock Orthogonal gradient descent for continual learning.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Farquhar \& Gal(2019)Farquhar and Gal]{Farquahr19Bayesian}
Farquhar, S. and Gal, Y.
\newblock A unifying {Bayesian} view of continual learning.
\newblock \emph{CoRR}, abs/1902.06494, 2019.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{MAML}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Finn et~al.(2019)Finn, Rajeswaran, Kakade, and Levine]{Finn19Online}
Finn, C., Rajeswaran, A., Kakade, S.~M., and Levine, S.
\newblock Online meta-learning.
\newblock In \emph{ICML}, 2019.

\bibitem[Fisher(1934)]{Fisher1934}
Fisher, R.~A.
\newblock Two new properties of mathematical likelihood.
\newblock \emph{Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character}, 144\penalty0 (852):\penalty0 285--307, 1934.

\bibitem[Garnelo et~al.(2018{\natexlab{a}})Garnelo, Rosenbaum, Maddison, Ramalho, Saxton, Shanahan, Teh, Rezende, and Eslami]{CNP}
Garnelo, M., Rosenbaum, D., Maddison, C., Ramalho, T., Saxton, D., Shanahan, M., Teh, Y.~W., Rezende, D.~J., and Eslami, S. M.~A.
\newblock Conditional neural processes.
\newblock In \emph{ICML}, 2018{\natexlab{a}}.

\bibitem[Garnelo et~al.(2018{\natexlab{b}})Garnelo, Schwarz, Rosenbaum, Viola, Rezende, Eslami, and Teh]{NP}
Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D.~J., Eslami, S. M.~A., and Teh, Y.~W.
\newblock Neural processes.
\newblock \emph{CoRR}, abs/1807.01622, 2018{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget{-}Abadie, Mirza, Xu, Warde{-}Farley, Ozair, Courville, and Bengio]{GAN}
Goodfellow, I.~J., Pouget{-}Abadie, J., Mirza, M., Xu, B., Warde{-}Farley, D., Ozair, S., Courville, A.~C., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Gordon et~al.(2019)Gordon, Bronskill, Bauer, Nowozin, and Turner]{ML-PIP}
Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R.~E.
\newblock Meta-learning probabilistic inference for prediction.
\newblock In \emph{ICLR}, 2019.

\bibitem[Guo et~al.(2016)Guo, Zhang, Hu, He, and Gao]{MSCeleb}
Guo, Y., Zhang, L., Hu, Y., He, X., and Gao, J.
\newblock {MS-Celeb-1M}: {A} dataset and benchmark for large-scale face recognition.
\newblock In \emph{Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part {III}}, 2016.

\bibitem[Gupta et~al.(2020)Gupta, Yadav, and Paull]{La-MAML}
Gupta, G., Yadav, K., and Paull, L.
\newblock Look-ahead meta learning for continual learning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Harrison et~al.(2018)Harrison, Sharma, and Pavone]{ALPaCA}
Harrison, J., Sharma, A., and Pavone, M.
\newblock Meta-learning priors for efficient online {Bayesian} regression.
\newblock In \emph{Algorithmic Foundations of Robotics XIII, Proceedings of the 13th Workshop on the Algorithmic Foundations of Robotics, {WAFR} 2018, M{\'{e}}rida, Mexico, December 9-11, 2018}, 2018.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{DDPM}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Javed \& White(2019)Javed and White]{OML}
Javed, K. and White, M.
\newblock Meta-learning representations for continual learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Jerfel et~al.(2019)Jerfel, Grant, Griffiths, and Heller]{Jerfel19Reconciling}
Jerfel, G., Grant, E., Griffiths, T., and Heller, K.~A.
\newblock Reconciling meta-learning and continual learning with online mixtures of tasks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{LinearTF}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock {Transformer}s are {RNNs}: Fast autoregressive {Transformer}s with linear attention.
\newblock In \emph{ICML}, 2020.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{VAE}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {Bayes}.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kirkpatrick et~al.(2016)Kirkpatrick, Pascanu, Rabinowitz, Veness, Desjardins, Rusu, Milan, Quan, Ramalho, Grabska{-}Barwinska, Hassabis, Clopath, Kumaran, and Hadsell]{EWC}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N.~C., Veness, J., Desjardins, G., Rusu, A.~A., Milan, K., Quan, J., Ramalho, T., Grabska{-}Barwinska, A., Hassabis, D., Clopath, C., Kumaran, D., and Hadsell, R.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{CoRR}, abs/1612.00796, 2016.

\bibitem[Knoblauch et~al.(2020)Knoblauch, Husain, and Diethe]{Knoblauch2020OptimalCL}
Knoblauch, J., Husain, H., and Diethe, T.
\newblock Optimal continual learning has perfect memory and is {NP}-hard.
\newblock In \emph{ICML}, 2020.

\bibitem[Koopman(1936)]{Koopman1936}
Koopman, B.~O.
\newblock On distributions admitting a sufficient statistic.
\newblock \emph{Transactions of the American Mathematical society}, 39\penalty0 (3):\penalty0 399--409, 1936.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{Omniglot}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350:\penalty0 1332 -- 1338, 2015.

\bibitem[Lee et~al.(2023)Lee, Son, and Kim]{CL-Seq}
Lee, S., Son, J., and Kim, G.
\newblock Recasting continual learning as sequence modeling.
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Liu et~al.(2011)Liu, Yin, Wang, and Wang]{CASIA}
Liu, C., Yin, F., Wang, D., and Wang, Q.
\newblock {CASIA} online and offline chinese handwriting databases.
\newblock In \emph{ICDAR}, 2011.

\bibitem[Lopez{-}Paz \& Ranzato(2017)Lopez{-}Paz and Ranzato]{GEM}
Lopez{-}Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Murphy(2022)]{MurphyML1}
Murphy, K.~P.
\newblock \emph{Probabilistic Machine Learning: An introduction}.
\newblock 2022.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{VCL}
Nguyen, C.~V., Li, Y., Bui, T.~D., and Turner, R.~E.
\newblock Variational continual learning.
\newblock In \emph{ICLR}, 2018.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{Reptile}
Nichol, A., Achiam, J., and Schulman, J.
\newblock On first-order meta-learning algorithms.
\newblock \emph{CoRR}, abs/1803.02999, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, K{\"{o}}pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{PyTorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., K{\"{o}}pf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.
\newblock {PyTorch}: An imperative style, high-performance deep learning library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Pitman(1936)]{Pitman1936}
Pitman, E. J.~G.
\newblock Sufficient statistics and intrinsic accuracy.
\newblock In \emph{Mathematical Proceedings of the Cambridge Philosophical Society}, number~4. Cambridge University Press, 1936.

\bibitem[Requeima et~al.(2019)Requeima, Gordon, Bronskill, Nowozin, and Turner]{CNAP}
Requeima, J., Gordon, J., Bronskill, J., Nowozin, S., and Turner, R.~E.
\newblock Fast and flexible multi-task classification using conditional neural adaptive processes.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Riemer et~al.(2019)Riemer, Cases, Ajemian, Liu, Rish, Tu, and Tesauro]{RiemerCALRTT19}
Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., and Tesauro, G.
\newblock Learning to learn without forgetting by maximizing transfer and minimizing interference.
\newblock In \emph{ICLR}, 2019.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{PN}
Snell, J., Swersky, K., and Zemel, R.~S.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Son et~al.(2023)Son, Lee, and Kim]{MCL-Survey}
Son, J., Lee, S., and Kim, G.
\newblock When meta-learning meets online and continual learning: {A} survey.
\newblock \emph{CoRR}, abs/2311.05241, 2023.

\bibitem[Tay et~al.(2022)Tay, Dehghani, Bahri, and Metzler]{Tay2020EfficientTA}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient {Transformer}s: A survey.
\newblock \emph{ACM Comput. Surv.}, 55\penalty0 (6), 2022.
\newblock ISSN 0360-0300.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Volpp et~al.(2021)Volpp, Fl{\"{u}}renbrock, Gro{\ss}berger, Daniel, and Neumann]{Volpp21BayesContext}
Volpp, M., Fl{\"{u}}renbrock, F., Gro{\ss}berger, L., Daniel, C., and Neumann, G.
\newblock {Bayesian} context aggregation for neural processes.
\newblock In \emph{ICLR}, 2021.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{SI}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{ICML}, 2017.

\end{thebibliography}
