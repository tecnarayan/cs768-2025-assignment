\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2019{\natexlab{a}})Aljundi, Belilovsky, Tuytelaars,
  Charlin, Caccia, Lin, and Page-Caccia]{NIPS2019_9357}
Aljundi, R., Belilovsky, E., Tuytelaars, T., Charlin, L., Caccia, M., Lin, M.,
  and Page-Caccia, L.
\newblock Online continual learning with maximal interfered retrieval.
\newblock \emph{Advances in Neural Information Processing Systems 32}, pp.\
  11849--11860, 2019{\natexlab{a}}.

\bibitem[Aljundi et~al.(2019{\natexlab{b}})Aljundi, Kelchtermans, and
  Tuytelaars]{aljundi2019taskfree}
Aljundi, R., Kelchtermans, K., and Tuytelaars, T.
\newblock Task-free continual learning.
\newblock \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2019{\natexlab{b}}.

\bibitem[Aljundi et~al.(2019{\natexlab{c}})Aljundi, Lin, Goujaud, and
  Bengio]{aljundi2019gradient}
Aljundi, R., Lin, M., Goujaud, B., and Bengio, Y.
\newblock Gradient based sample selection for online continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30},
  2019{\natexlab{c}}.

\bibitem[Ambrosio et~al.(2008)Ambrosio, Gigli, and Savare]{gradientflow}
Ambrosio, L., Gigli, N., and Savare, G.
\newblock Gradient flows: In metric spaces and in the space of probability
  measures.
\newblock \emph{(Lectures in Mathematics. ETH)}, 2008.

\bibitem[Bass(2011)]{bass_2011}
Bass, R.~F.
\newblock \emph{Stochastic Processes}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 2011.
\newblock \doi{10.1017/CBO9780511997044}.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017evaluating}
Carlini, N. and Wagner, D.
\newblock Towards evaluating the robustness of neural networks.
\newblock \emph{2017 IEEE Symposium on Security and Privacy (SP)}, 2017.

\bibitem[Chaudhry et~al.(2019{\natexlab{a}})Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{AGEM19}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with a-gem.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2019{\natexlab{a}}.

\bibitem[Chaudhry et~al.(2019{\natexlab{b}})Chaudhry, Rohrbach, Elhoseiny,
  Ajanthan, Dokania, Torr, and Ranzato]{ERRing19}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.~K., Torr,
  P. H.~S., and Ranzato, M.
\newblock Continual learning with tiny episodic memories.
\newblock \emph{https://arxiv.org/abs/1902.10486}, 2019{\natexlab{b}}.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{pmlr-v32-cheni14}
Chen, T., Fox, E., and Guestrin, C.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, 2014.

\bibitem[Chewi et~al.(2020)Chewi, Le~Gouic, Lu, Maunu, and
  Rigollet]{NEURIPS2020_SVGD}
Chewi, S., Le~Gouic, T., Lu, C., Maunu, T., and Rigollet, P.
\newblock Svgd as a kernelized wasserstein gradient flow of the chi-squared
  divergence.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Chrysakis \& Moens(2020)Chrysakis and Moens]{pmlr-v119-chrysakis20a}
Chrysakis, A. and Moens, M.-F.
\newblock Online continual learning from imbalanced data.
\newblock \emph{Proceedings of the 37th International Conference on Machine
  Learning}, 119:\penalty0 1952--1961, 2020.

\bibitem[He et~al.(2019)He, Sygnowski, Galashov, Rusu, Teh, and
  Pascanu]{he2019task}
He, X., Sygnowski, J., Galashov, A., Rusu, A.~A., Teh, Y.~W., and Pascanu, R.
\newblock Task agnostic continual learning via meta learning.
\newblock \emph{https://arxiv.org/abs/1906.05201}, 2019.

\bibitem[Jin et~al.(2021)Jin, Sadhu, Du, and Ren]{jin2021gradientbased}
Jin, X., Sadhu, A., Du, J., and Ren, X.
\newblock Gradient-based editing of memory examples for online task-free
  continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Jordan et~al.(1998)Jordan, Kinderlehrer, , and Otto.]{SGLDWGF}
Jordan, R., Kinderlehrer, D., , and Otto., F.
\newblock The variational formulation of the fokkerâ€“planck equation.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 1998.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and Hadsell]{EWC16}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D.,
  Clopath, C., Kumaran, D., and Hadsell, R.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the national academy of sciences}, 2017.

\bibitem[Krizhevsky(2009)]{cifar100}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report}, 2009.

\bibitem[Lee et~al.(2020)Lee, Ha, Zhang, and Kim]{lee2020neural}
Lee, S., Ha, J., Zhang, D., and Kim, G.
\newblock A neural dirichlet process mixture model for task-free continual
  learning.
\newblock In \emph{Proceedings of the 17th International Conference on Machine
  Learning}, 2020.

\bibitem[Liu et~al.(2019)Liu, Zhuo, and Zhu]{MCMCflow}
Liu, C., Zhuo, J., and Zhu, J.
\newblock Understanding mcmc dynamics as flows on the wasserstein spac.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2019.

\bibitem[Liu(2017)]{liu2017stein}
Liu, Q.
\newblock Stein variational gradient descent as gradient flow.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Liu \& Wang(2016)Liu and Wang]{liu2019stein}
Liu, Q. and Wang, D.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Lopez-Paz \& Ranzato(2017)Lopez-Paz and Ranzato]{lopezpaz2017gradient}
Lopez-Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Ma et~al.(2015)Ma, Chen, and Fox]{ma2015complete}
Ma, Y.-A., Chen, T., and Fox, E.~B.
\newblock A complete recipe for stochastic gradient mcmc.
\newblock \emph{Advances in Neural Information Processing Systems}, 2015.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2019deep}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Miersemann(2012)]{variation}
Miersemann, E.
\newblock \emph{Calculus of Variations, Lecture Notes}.
\newblock Leipzig University, 2012.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{nguyen2017variational}
Nguyen, C.~V., Li, Y., Bui, T.~D., and Turner, R.~E.
\newblock Variational continual learning.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2018.

\bibitem[Pham et~al.(2021)Pham, Liu, Sahoo, and HOI]{pham2021contextual}
Pham, Q., Liu, C., Sahoo, D., and HOI, S.
\newblock Contextual transformation networks for online continual learning.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\bibitem[Rahimian \& Mehrotra(2019)Rahimian and
  Mehrotra]{rahimian2019distributionally}
Rahimian, H. and Mehrotra, S.
\newblock Distributionally robust optimization: A review.
\newblock 2019.

\bibitem[Riemer et~al.(2019)Riemer, Cases, Ajemian, Liu, Rish, Tu, and
  Tesauro]{riemer2018learning}
Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., and Tesauro, G.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock \emph{International Conference on Learning Representations}, 2019.

\bibitem[Sagawa et~al.(2020)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2020distributionally}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Saha et~al.(2021)Saha, Garg, and Roy]{saha2021gradient}
Saha, G., Garg, I., and Roy, K.
\newblock Gradient projection memory for continual learning.
\newblock \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, kavukcuoglu, and
  Wierstra]{NIPS2016_90e13578}
Vinyals, O., Blundell, C., Lillicrap, T., kavukcuoglu, k., and Wierstra, D.
\newblock Matching networks for one shot learning.
\newblock 29, 2016.

\bibitem[von Oswald et~al.(2019)von Oswald, Henning, Sacramento, and
  Grewe]{oswald2019continual}
von Oswald, J., Henning, C., Sacramento, J., and Grewe, B.~F.
\newblock Continual learning with hypernetworks.
\newblock \emph{https://arxiv.org/abs/1906.00695}, 2019.

\bibitem[Wang et~al.(2021)Wang, Duan, Fang, Suo, and Gao]{Wang_2021_ICCV}
Wang, Z., Duan, T., Fang, L., Suo, Q., and Gao, M.
\newblock Meta learning on a sequence of imbalanced domains with difficulty
  awareness.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  8947--8957, 2021.

\bibitem[Wang et~al.(2022)Wang, Shen, Duan, Zhan, Fang, and
  Gao]{Wang_2022_CVPR}
Wang, Z., Shen, L., Duan, T., Zhan, D., Fang, L., and Gao, M.
\newblock Learning to learn and remember super long multi-domain task sequence.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  7982--7992, 2022.

\bibitem[Welling \& Teh(2011)Welling and Teh]{SGLD11}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2011.

\bibitem[Xu et~al.(2020)Xu, Dan, Khim, and Ravikumar]{imbalanceclass}
Xu, Z., Dan, C., Khim, J., and Ravikumar, P.
\newblock Class-weighted classification: Trade-offs and robust approaches.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2020.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence.
\newblock \emph{https://arxiv.org/abs/1703.04200}, 2017.

\bibitem[Zeno et~al.(2019)Zeno, Golan, Hoffer, and Soudry]{zeno2019task}
Zeno, C., Golan, I., Hoffer, E., and Soudry, D.
\newblock Task agnostic continual learning using online variational bayes.
\newblock \emph{https://arxiv.org/abs/1803.10123}, 2019.

\bibitem[Zhai et~al.(2021)Zhai, Dan, Kolter, and Ravikumar]{zhai2021doro}
Zhai, R., Dan, C., Kolter, J.~Z., and Ravikumar, P.
\newblock Doro: Distributional and outlier robust optimization.
\newblock \emph{Proceedings of the International Conference on Machine
  Learning}, 2021.

\end{thebibliography}
