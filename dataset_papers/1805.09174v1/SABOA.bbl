\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2012)Agarwal, Negahban, and
  Wainwright]{AgarwalNegahbanWainwright2012}
A.~Agarwal, S.~Negahban, and M.~J. Wainwright.
\newblock Stochastic optimization and sparse statistical recovery: Optimal
  algorithms for high dimensions.
\newblock In \emph{Advances in Neural Information Processing Systems 25}, pages
  1538--1546. Curran Associates, Inc., 2012.

\bibitem[Audibert(2008)]{Audibert2008}
J.-Y. Audibert.
\newblock Progressive mixture rules are deviation suboptimal.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing
  {S}ystems}, pages 41--48, 2008.

\bibitem[Bunea et~al.(2007)Bunea, Tsybakov, Wegkamp, et~al.]{bunea2007}
F.~Bunea, A.~Tsybakov, M.~Wegkamp, et~al.
\newblock Sparsity oracle inequalities for the lasso.
\newblock \emph{Electronic Journal of Statistics}, 1:\penalty0 169--194, 2007.

\bibitem[Catoni(1999)]{Catoni1999}
O.~Catoni.
\newblock Universal aggregation rules with exact bias bounds.
\newblock \emph{preprint}, 510, 1999.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{Cesa-BianchiLugosi2006}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.

\bibitem[Cesa-Bianchi et~al.(2007)Cesa-Bianchi, Mansour, and
  Stoltz]{CesaBianchiMansourStoltz2007}
N.~Cesa-Bianchi, Y.~Mansour, and G.~Stoltz.
\newblock Improved second-order bounds for prediction with expert advice.
\newblock \emph{Machine Learning}, 66\penalty0 (2-3):\penalty0 321--352, 2007.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and
  Chandra]{DuchiEtAl2008}
J.~Duchi, S.~Shalev-Shwartz, Y.~Singer, and T.~Chandra.
\newblock Efficient projections onto the $\ell_1$-ball for learning in high
  dimensions.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 272--279. ACM, 2008.

\bibitem[Duchi et~al.(2010)Duchi, Shalev-Shwartz, Singer, and
  Tewari]{duchi2010composite}
J.~C. Duchi, S.~Shalev-Shwartz, Y.~Singer, and A.~Tewari.
\newblock Composite objective mirror descent.
\newblock In \emph{COLT}, pages 14--26, 2010.

\bibitem[Foster et~al.(2016)Foster, Kale, and Karloff]{foster2016online}
D.~Foster, S.~Kale, and H.~Karloff.
\newblock Online sparse linear regression.
\newblock In \emph{Conference on Learning Theory}, pages 960--970, 2016.

\bibitem[Foster et~al.(2017)Foster, Kale, Mohri, and Sridharan]{Foster2017}
D.~J. Foster, S.~Kale, M.~Mohri, and K.~Sridharan.
\newblock Parameter-free online learning via model selection.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 30}, pages 6020--6030. Curran Associates,
  Inc., 2017.

\bibitem[Freund and Schapire(1997)]{freund1997decision}
Y.~Freund and R.~E. Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Gaillard and Wintenberger(2017)]{GaillardWintenberger2017}
P.~Gaillard and O.~Wintenberger.
\newblock {Sparse Accelerated Exponential Weights}.
\newblock In \emph{{20th International Conference on Artificial Intelligence
  and Statistics (AISTATS)}}, Apr. 2017.

\bibitem[Gerchinovitz(2011)]{Gerchinovitz2011}
S.~Gerchinovitz.
\newblock \emph{Prediction of individual sequences and prediction in the
  statistical framework: some links around sparse regression and aggregation
  techniques}.
\newblock PhD thesis, Universit\'{e} Paris-Sud 11, Orsay, 2011.

\bibitem[Hazan(2016)]{Hazan2016}
E.~Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007}
E.~Hazan, A.~Agarwal, and S.~Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Kale et~al.(2017)Kale, Karnin, Liang, and P{\'a}l]{kale2017adaptive}
S.~Kale, Z.~Karnin, T.~Liang, and D.~P{\'a}l.
\newblock Adaptive feature selection: Computationally efficient online sparse
  linear regression under rip.
\newblock \emph{arXiv preprint arXiv:1706.04690}, 2017.

\bibitem[Kivinen and Warmuth(1997)]{KivinenWarmuth1997}
J.~Kivinen and M.~K. Warmuth.
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock \emph{Information and Computation}, 132\penalty0 (1):\penalty0 1--63,
  1997.

\bibitem[Koolen and Van~Erven(2015)]{vanErvenKoolen2015}
W.~M. Koolen and T.~Van~Erven.
\newblock Second-order quantile methods for experts and combinatorial games.
\newblock In \emph{COLT}, volume~40, pages 1155--1175, 2015.

\bibitem[Koolen et~al.(2016)Koolen, Gr{\"u}nwald, and van
  Erven]{koolen2016combining}
W.~M. Koolen, P.~Gr{\"u}nwald, and T.~van Erven.
\newblock Combining adversarial guarantees and stochastic fast rates in online
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4457--4465, 2016.

\bibitem[Langford et~al.(2009)Langford, Li, and Zhang]{langford2009sparse}
J.~Langford, L.~Li, and T.~Zhang.
\newblock Sparse online learning via truncated gradient.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (Mar):\penalty0 777--801, 2009.

\bibitem[Littlestone and Warmuth(1994)]{littlestone1994weighted}
N.~Littlestone and M.~K. Warmuth.
\newblock The weighted majority algorithm.
\newblock \emph{Information and computation}, 108\penalty0 (2):\penalty0
  212--261, 1994.

\bibitem[Mehta(2016)]{mehta2016fast}
N.~A. Mehta.
\newblock Fast rates with high probability in exp-concave statistical learning.
\newblock \emph{arXiv preprint arXiv:1605.01288}, 2016.

\bibitem[Rakhlin and Sridharan(2015)]{rakhlin2015online}
A.~Rakhlin and K.~Sridharan.
\newblock Online nonparametric regression with general loss functions.
\newblock \emph{arXiv preprint arXiv:1501.06598}, 2015.

\bibitem[Rigollet and Tsybakov(2011)]{RigolletTsybakov2011}
P.~Rigollet and A.~Tsybakov.
\newblock Exponential screening and optimal rates of sparse estimation.
\newblock \emph{The Annals of Statistics}, pages 731--771, 2011.

\bibitem[Roulet and d'Aspremont(2017)]{roulet2017sharpness}
V.~Roulet and A.~d'Aspremont.
\newblock Sharpness, restart and acceleration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1119--1129, 2017.

\bibitem[Steinhardt et~al.(2014)Steinhardt, Wager, and Liang]{Steinhardt2014}
J.~Steinhardt, S.~Wager, and P.~Liang.
\newblock The statistics of streaming sparse regression.
\newblock \emph{arXiv preprint arXiv:1412.4182}, 2014.

\bibitem[Van~Erven et~al.(2015)Van~Erven, Gr{\"u}nwald, Mehta, Reid, and
  Williamson]{van2015fast}
T.~Van~Erven, P.~D. Gr{\"u}nwald, N.~A. Mehta, M.~D. Reid, and R.~C.
  Williamson.
\newblock Fast rates in statistical and online learning.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 1793--1861,
  2015.

\bibitem[Vovk(1998)]{vovk1998game}
V.~Vovk.
\newblock A game of prediction with expert advice.
\newblock \emph{Journal of Computer and System Sciences}, 56\penalty0
  (2):\penalty0 153--173, 1998.

\bibitem[Vovk(1990)]{vovk1990aggregating}
V.~G. Vovk.
\newblock Aggregating strategies.
\newblock \emph{Proc. of Computational Learning Theory, 1990}, 1990.

\bibitem[Wainwright(2009)]{wainwright2009sharp}
M.~J. Wainwright.
\newblock Sharp thresholds for high-dimensional and noisy sparsity recovery
  using $\ell_1$-constrained quadratic programming (lasso).
\newblock \emph{IEEE transactions on information theory}, 55\penalty0
  (5):\penalty0 2183--2202, 2009.

\bibitem[Wintenberger(2014)]{Wintenberger2014}
O.~Wintenberger.
\newblock Optimal learning with bernstein online aggregation.
\newblock \emph{\emph{Extended version available at arXiv:1404.1356 [stat.
  ML]}}, 2014.

\bibitem[Xiao(2010)]{Xiao2010}
L.~Xiao.
\newblock Dual averaging methods for regularized stochastic learning and online
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Oct):\penalty0 2543--2596, 2010.

\bibitem[Yang(2004)]{Yang2004}
Y.~Yang.
\newblock Combining forecasting procedures: some theoretical results.
\newblock \emph{Econometric Theory}, 20\penalty0 (01):\penalty0 176--222, 2004.

\bibitem[Zhang et~al.(2014)Zhang, Wainwright, and Jordan]{Zhang2014}
Y.~Zhang, M.~J. Wainwright, and M.~I. Jordan.
\newblock Lower bounds on the performance of polynomial-time algorithms for
  sparse linear regression.
\newblock In \emph{Conference on Learning Theory}, pages 921--948, 2014.

\bibitem[Zinkevich(2003)]{Zinkevich2003}
M.~Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning, ICML 2003}, 2003.

\bibitem[Łojasiewicz(1963)]{Loja63}
S.~Łojasiewicz.
\newblock Une propri{\'e}t{\'e} topologique des sous-ensembles analytiques
  r{\'e}els.
\newblock \emph{Les {\'e}quations aux d{\'e}riv{\'e}es partielles}, pages
  87--89, 1963.

\bibitem[Łojasiewicz(1993)]{Loja93}
S.~Łojasiewicz.
\newblock Sur la g{\'e}om{\'e}trie semi-et sous-analytique.
\newblock \emph{Annales de l'institut Fourier}, 43\penalty0 (5):\penalty0
  1575--1595, 1993.

\end{thebibliography}
