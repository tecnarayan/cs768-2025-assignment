\begin{thebibliography}{10}

\bibitem{akiyama2021learnability}
{\sc S.~Akiyama and T.~Suzuki}, {\em On learnability via gradient method for
  two-layer relu neural networks in teacher-student setting}, arXiv e-prints,
  (2021), pp.~arXiv--2106.

\bibitem{allen2019convergence}
{\sc Z.~Allen-Zhu, Y.~Li, and Z.~Song}, {\em A convergence theory for deep
  learning via over-parameterization}, in International Conference on Machine
  Learning, PMLR, 2019, pp.~242--252.

\bibitem{ambrosio2008gradient}
{\sc L.~Ambrosio, N.~Gigli, and G.~Savaré}, {\em Gradient flows: in metric
  spaces and in the space of probability measures}, Lectures in mathematics ETH
  Zürich,  (2008).

\bibitem{aronszajn1950theory}
{\sc N.~Aronszajn}, {\em Theory of reproducing kernels}, Transactions of the
  American mathematical society, 68 (1950), pp.~337--404.

\bibitem{bach2017breaking}
{\sc F.~Bach}, {\em Breaking the curse of dimensionality with convex neural
  networks}, The Journal of Machine Learning Research, 18 (2017), pp.~629--681.

\bibitem{barron_universal_1993}
{\sc A.~Barron}, {\em Universal approximation bounds for superpositions of a
  sigmoidal function}, IEEE Transactions on Information Theory, 39 (1993),
  pp.~930--945.

\bibitem{bartlett2018gradient}
{\sc P.~Bartlett, D.~Helmbold, and P.~Long}, {\em Gradient descent with
  identity initialization efficiently learns positive definite linear
  transformations by deep residual networks}, in International Conference on
  Machine Learning, PMLR, 2018, pp.~521--530.

\bibitem{beg2005computing}
{\sc M.~F. Beg, M.~I. Miller, A.~Trouv{\'e}, and L.~Younes}, {\em Computing
  large deformation metric mappings via geodesic flows of diffeomorphisms},
  International journal of computer vision, 61 (2005), pp.~139--157.

\bibitem{belkin2019reconciling}
{\sc M.~Belkin, D.~Hsu, S.~Ma, and S.~Mandal}, {\em Reconciling modern
  machine-learning practice and the classical bias--variance trade-off},
  Proceedings of the National Academy of Sciences, 116 (2019),
  pp.~15849--15854.

\bibitem{bolte_characterizations_2009}
{\sc J.~Bolte, A.~Daniilidis, O.~Ley, and L.~Mazet}, {\em Characterizations of
  Łojasiewicz inequalities: {Subgradient} flows, talweg, convexity},
  Transactions of the American Mathematical Society, 362 (2009),
  pp.~3319--3363.

\bibitem{chen2018neural}
{\sc R.~T.~Q. Chen, Y.~Rubanova, J.~Bettencourt, and D.~Duvenaud}, {\em Neural
  ordinary differential equations}, Advances in Neural Information Processing
  Systems,  (2018).

\bibitem{chen2020much}
{\sc Z.~Chen, Y.~Cao, D.~Zou, and Q.~Gu}, {\em How much over-parameterization
  is sufficient to learn deep relu networks?}, in International Conference on
  Learning Representations, 2020.

\bibitem{chizat2021sparse}
{\sc L.~Chizat}, {\em Sparse optimization on measures with over-parameterized
  gradient descent}, Mathematical Programming,  (2021), pp.~1--46.

\bibitem{chizat2018global}
{\sc L.~Chizat and F.~Bach}, {\em On the global convergence of gradient descent
  for over-parameterized models using optimal transport}, Advances in Neural
  Information Processing Systems, 31 (2018), pp.~3036--3046.

\bibitem{chizat2019lazy}
{\sc L.~Chizat, E.~Oyallon, and F.~Bach}, {\em On lazy training in
  differentiable programming}, in NeurIPS 2019-33rd Conference on Neural
  Information Processing Systems, 2019, pp.~2937--2947.

\bibitem{du2019gradient}
{\sc S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai}, {\em Gradient descent finds
  global minima of deep neural networks}, in International Conference on
  Machine Learning, PMLR, 2019, pp.~1675--1685.

\bibitem{du2018gradient}
{\sc S.~S. Du, X.~Zhai, B.~Poczos, and A.~Singh}, {\em Gradient descent
  provably optimizes over-parameterized neural networks}, in International
  Conference on Learning Representations, 2018.

\bibitem{dupont2019augmented}
{\sc E.~Dupont, A.~Doucet, and Y.~W. Teh}, {\em Augmented neural odes}, in
  Proceedings of the 33rd International Conference on Neural Information
  Processing Systems, 2019, pp.~3140--3150.

\bibitem{e_mean-field_2019}
{\sc W.~E, J.~Han, and Q.~Li}, {\em A mean-field optimal control formulation of
  deep learning}, Research in the Mathematical Sciences, 6 (2019), p.~10.

\bibitem{e_barron_2021}
{\sc W.~E, C.~Ma, and L.~Wu}, {\em The {Barron} {Space} and the
  {Flow}-{Induced} {Function} {Spaces} for {Neural} {Network} {Models}},
  Constructive Approximation,  (2021).

\bibitem{fang2021modeling}
{\sc C.~Fang, J.~Lee, P.~Yang, and T.~Zhang}, {\em Modeling from features: a
  mean-field framework for over-parameterized deep neural networks}, in
  Conference on Learning Theory, PMLR, 2021, pp.~1887--1936.

\bibitem{frei2021proxy}
{\sc S.~Frei and Q.~Gu}, {\em Proxy convexity: A unified framework for the
  analysis of neural networks trained by gradient descent}, in Thirty-Fifth
  Conference on Neural Information Processing Systems, 2021.

\bibitem{hale_ordinary_2009}
{\sc J.~K. Hale}, {\em Ordinary differential equations}, Dover Publications,
  Mineola, N.Y, dover ed~ed., 2009.
\newblock OCLC: ocn294885198.

\bibitem{hardt_identity_2018}
{\sc M.~Hardt and T.~Ma}, {\em Identity {Matters} in {Deep} {Learning}},
  arXiv:1611.04231 [cs, stat],  (2018).
\newblock arXiv: 1611.04231.

\bibitem{he2016deep}
{\sc K.~He, X.~Zhang, S.~Ren, and J.~Sun}, {\em Deep residual learning for
  image recognition}, in Proceedings of the IEEE conference on computer vision
  and pattern recognition, 2016, pp.~770--778.

\bibitem{jacot_neural_2021}
{\sc A.~Jacot, F.~Gabriel, and C.~Hongler}, {\em Neural tangent kernel:
  convergence and generalization in neural networks (invited paper)}, in
  Proceedings of the 53rd {Annual} {ACM} {SIGACT} {Symposium} on {Theory} of
  {Computing}, Virtual Italy, June 2021, ACM, pp.~6--6.

\bibitem{javanmard_analysis_2020}
{\sc A.~Javanmard, M.~Mondelli, and A.~Montanari}, {\em Analysis of a two-layer
  neural network via displacement convexity}, The Annals of Statistics, 48
  (2020).

\bibitem{review_multivariate}
{\sc B.~M.~G. Kibria and A.~Joarder}, {\em A short review of multivariate
  t-distribution}, Journal of Statistical Research ISSN, 40 (2006),
  pp.~256--422.

\bibitem{kobyzev_normalizing_2020}
{\sc I.~Kobyzev, S.~Prince, and M.~Brubaker}, {\em Normalizing {Flows}: {An}
  {Introduction} and {Review} of {Current} {Methods}}, IEEE Transactions on
  Pattern Analysis and Machine Intelligence,  (2020), pp.~1--1.

\bibitem{lacoste2019quantifying}
{\sc A.~Lacoste, A.~Luccioni, V.~Schmidt, and T.~Dandres}, {\em Quantifying the
  carbon emissions of machine learning}, arXiv preprint arXiv:1910.09700,
  (2019).

\bibitem{laforgia2010some}
{\sc A.~Laforgia and P.~Natalini}, {\em Some inequalities for modified bessel
  functions}, Journal of Inequalities and Applications, 2010 (2010), pp.~1--10.

\bibitem{lee2019wide}
{\sc J.~Lee, L.~Xiao, S.~Schoenholz, Y.~Bahri, R.~Novak, J.~Sohl-Dickstein, and
  J.~Pennington}, {\em Wide neural networks of any depth evolve as linear
  models under gradient descent}, Advances in neural information processing
  systems, 32 (2019), pp.~8572--8583.

\bibitem{li2018learning}
{\sc Y.~Li and Y.~Liang}, {\em Learning overparameterized neural networks via
  stochastic gradient descent on structured data}, Advances in neural
  information processing systems,  (2018).

\bibitem{li2017convergence}
{\sc Y.~Li and Y.~Yuan}, {\em Convergence analysis of two-layer neural networks
  with relu activation}, Advances in Neural Information Processing Systems, 30
  (2017), pp.~597--607.

\bibitem{liu2020linearity}
{\sc C.~Liu, L.~Zhu, and M.~Belkin}, {\em On the linearity of large non-linear
  models: when and why the tangent kernel is constant}, Advances in Neural
  Information Processing Systems, 33 (2020).

\bibitem{liu_loss_2021}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em Loss landscapes and
  optimization in over-parameterized non-linear systems and neural networks},
  arXiv:2003.00307 [cs, math, stat],  (2021).
\newblock arXiv: 2003.00307.

\bibitem{lojasiewicz1982trajectoires}
{\sc S.~Lojasiewicz}, {\em Sur les trajectoires du gradient d’une fonction
  analytique}, Seminari di geometria, 1983 (1982), pp.~115--117.

\bibitem{lu2020mean}
{\sc Y.~Lu, C.~Ma, Y.~Lu, J.~Lu, and L.~Ying}, {\em A mean field analysis of
  deep resnet and beyond: Towards provably optimization via
  overparameterization from depth}, in International Conference on Machine
  Learning, PMLR, 2020, pp.~6426--6436.

\bibitem{lu2018beyond}
{\sc Y.~Lu, A.~Zhong, Q.~Li, and B.~Dong}, {\em Beyond finite layer neural
  networks: Bridging deep architectures and numerical differential equations},
  in International Conference on Machine Learning, PMLR, 2018, pp.~3276--3285.

\bibitem{mei2019mean}
{\sc S.~Mei, T.~Misiakiewicz, and A.~Montanari}, {\em Mean-field theory of
  two-layers neural networks: dimension-free bounds and kernel limit}, in
  Conference on Learning Theory, PMLR, 2019, pp.~2388--2464.

\bibitem{mei2018mean}
{\sc S.~Mei, A.~Montanari, and P.-M. Nguyen}, {\em A mean field view of the
  landscape of two-layer neural networks}, Proceedings of the National Academy
  of Sciences, 115 (2018), pp.~E7665--E7671.

\bibitem{neyshabur2017implicit}
{\sc B.~Neyshabur}, {\em Implicit regularization in deep learning}, arXiv
  preprint arXiv:1709.01953,  (2017).

\bibitem{nguyen_proof_2021}
{\sc Q.~Nguyen}, {\em On the {Proof} of {Global} {Convergence} of {Gradient}
  {Descent} for {Deep} {ReLU} {Networks} with {Linear} {Widths}},
  arXiv:2101.09612 [cs, stat],  (2021).
\newblock arXiv: 2101.09612.

\bibitem{Niethammer2011}
{\sc M.~Niethammer, Y.~Huang, and F.-X. Vialard}, {\em Geodesic regression for
  image time-series}, in Medical Image Computing and Computer-Assisted
  Intervention -- MICCAI 2011: 14th International Conference, Toronto, Canada,
  September 18-22, 2011, Proceedings, Part II, G.~Fichtinger, A.~Martel, and
  T.~Peters, eds., Berlin, Heidelberg, 2011, Springer Berlin Heidelberg,
  pp.~655--662.

\bibitem{paszke2017automatic}
{\sc A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer}, {\em Automatic differentiation in
  pytorch},  (2017).

\bibitem{pham2020global}
{\sc H.~T. Pham and P.-M. Nguyen}, {\em Global convergence of three-layer
  neural networks in the mean field regime}, in International Conference on
  Learning Representations, 2020.

\bibitem{pontryagin1987mathematical}
{\sc L.~S. Pontryagin}, {\em Mathematical theory of optimal processes}, CRC
  press, 1987.

\bibitem{rahimi2007random}
{\sc A.~Rahimi and B.~Recht}, {\em Random features for large-scale kernel
  machines}, in Proceedings of the 20th International Conference on Neural
  Information Processing Systems, 2007, pp.~1177--1184.

\bibitem{rahimi2008weighted}
\leavevmode\vrule height 2pt depth -1.6pt width 23pt, {\em Weighted sums of
  random kitchen sinks: Replacing minimization with randomization in learning},
  Advances in Neural Information Processing Systems, 21 (2008), pp.~1313--1320.

\bibitem{rudin2017fourier}
{\sc W.~Rudin}, {\em Fourier analysis on groups}, Courier Dover Publications,
  2017.

\bibitem{salman2018deep}
{\sc H.~Salman, P.~Yadollahpour, T.~Fletcher, and K.~Batmanghelich}, {\em Deep
  diffeomorphic normalizing flows}, arXiv e-prints,  (2018), pp.~arXiv--1810.

\bibitem{scholkopf2002learning}
{\sc B.~Sch{\"o}lkopf, A.~J. Smola, F.~Bach, et~al.}, {\em Learning with
  kernels: support vector machines, regularization, optimization, and beyond},
  2002.

\bibitem{sriperumbudur2015optimal}
{\sc B.~Sriperumbudur and Z.~Szabo}, {\em Optimal rates for random fourier
  features}, Advances in Neural Information Processing Systems, 28 (2015),
  pp.~1144--1152.

\bibitem{sutherland2015error}
{\sc D.~J. Sutherland and J.~Schneider}, {\em On the error of random fourier
  features}, in Proceedings of the Thirty-First Conference on Uncertainty in
  Artificial Intelligence, 2015, pp.~862--871.

\bibitem{trouve1998diffeomorphisms}
{\sc A.~Trouv{\'e}}, {\em Diffeomorphisms groups and pattern matching in image
  analysis}, International journal of computer vision, 28 (1998), pp.~213--221.

\bibitem{vialard2020shooting}
{\sc F.-X. Vialard, R.~Kwitt, S.~Wei, and M.~Niethammer}, {\em A shooting
  formulation of deep learning}, Advances in Neural Information Processing
  Systems, 33 (2020).

\bibitem{williams2006gaussian}
{\sc C.~K. Williams and C.~E. Rasmussen}, {\em Gaussian processes for machine
  learning}, vol.~2, MIT press Cambridge, MA, 2006.

\bibitem{younes_shapes_2010}
{\sc L.~Younes}, {\em Shapes and {Diffeomorphisms}}, vol.~171 of Applied
  {Mathematical} {Sciences}, Springer Berlin Heidelberg, Berlin, Heidelberg,
  2010.

\bibitem{zagoruyko2016wide}
{\sc S.~Zagoruyko and N.~Komodakis}, {\em Wide residual networks}, in British
  Machine Vision Conference 2016, British Machine Vision Association, 2016.

\bibitem{zhang2021understanding}
{\sc C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals}, {\em
  Understanding deep learning (still) requires rethinking generalization},
  Communications of the ACM, 64 (2021), pp.~107--115.

\bibitem{zhang2018fixup}
{\sc H.~Zhang, Y.~N. Dauphin, and T.~Ma}, {\em Fixup initialization: Residual
  learning without normalization}, in International Conference on Learning
  Representations, 2018.

\bibitem{zhou2021local}
{\sc M.~Zhou, R.~Ge, and C.~Jin}, {\em A local convergence theory for mildly
  over-parameterized two-layer neural network}, in COLT, 2021.

\bibitem{zou_gradient_2020}
{\sc D.~Zou, Y.~Cao, D.~Zhou, and Q.~Gu}, {\em Gradient descent optimizes
  over-parameterized deep {ReLU} networks}, Machine Learning, 109 (2020),
  pp.~467--492.

\bibitem{zou2019global}
{\sc D.~Zou, P.~M. Long, and Q.~Gu}, {\em On the global convergence of training
  deep linear resnets}, in International Conference on Learning
  Representations, 2019.

\end{thebibliography}
