\begin{thebibliography}{83}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Quionero-Candela et~al.(2009)Quionero-Candela, Sugiyama, Schwaighofer,
  and Lawrence]{edit:Quinonero-Candela+etal:2009}
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D.
  Lawrence.
\newblock \emph{Dataset Shift in Machine Learning}.
\newblock The MIT Press, 2009.

\bibitem[Sugiyama and Kawanabe(2012)]{book/mit/sugiyama2012machine}
Masashi Sugiyama and Motoaki Kawanabe.
\newblock \emph{Machine {L}earning in {N}on-stationary {E}nvironments:
  Introduction to {C}ovariate {S}hift {A}daptation}.
\newblock The MIT Press, 2012.

\bibitem[Dietterich(2017)]{TGD:robust-AI}
Thomas~G. Dietterich.
\newblock Steps toward robust artificial intelligence.
\newblock \emph{{AI} Magazine}, 38\penalty0 (3):\penalty0 3--24, 2017.

\bibitem[Bengio et~al.(2021)Bengio, Lecun, and Hinton]{CACM'21:DL-AI}
Yoshua Bengio, Yann Lecun, and Geoffrey Hinton.
\newblock Deep learning for {AI}.
\newblock \emph{Communication of {ACM}}, 64\penalty0 (7):\penalty0 58â€“65,
  2021.

\bibitem[Zhou(2022)]{nsr'22:Open-Survey}
Zhi-Hua Zhou.
\newblock {Open-environment machine learning}.
\newblock \emph{National Science Review}, 9\penalty0 (8):\penalty0 nwac123,
  2022.

\bibitem[G{\'{o}}mez{-}Villa et~al.(2017)G{\'{o}}mez{-}Villa, Salazar, and
  Vargas{-}Bonilla]{ECOI'17:wild}
Alexander G{\'{o}}mez{-}Villa, Augusto Salazar, and Jes{\'{u}}s~Francisco
  Vargas{-}Bonilla.
\newblock Towards automatic wild animal monitoring: Identification of animal
  species in camera-trap images using very deep convolutional neural networks.
\newblock \emph{Ecological Informatics}, 41:\penalty0 24--32, 2017.

\bibitem[Norouzzadeh et~al.(2018)Norouzzadeh, Nguyen, Kosmala, Swanson, Palmer,
  Packer, and Clune]{PNAS'18:Species}
Mohammad~Sadegh Norouzzadeh, Anh Nguyen, Margaret Kosmala, Alexandra Swanson,
  Meredith~S. Palmer, Craig Packer, and Jeff Clune.
\newblock Automatically identifying, counting, and describing wild animals in
  camera-trap images with deep learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (25):\penalty0 E5716--E5725, 2018.

\bibitem[Saerens et~al.(2002)Saerens, Latinne, and
  Decaestecker]{DBLP:journals/neco/SaerensLD02}
Marco Saerens, Patrice Latinne, and Christine Decaestecker.
\newblock Adjusting the outputs of a classifier to new a priori probabilities:
  {A} simple procedure.
\newblock \emph{Neural Computation}, 14\penalty0 (1):\penalty0 21--41, 2002.

\bibitem[Zhang et~al.(2013)Zhang, Sch{\"{o}}lkopf, Muandet, and
  Wang]{conf/icml/ZhangSMW13}
Kun Zhang, Bernhard Sch{\"{o}}lkopf, Krikamol Muandet, and Zhikun Wang.
\newblock Domain adaptation under target and conditional shift.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning {(ICML)}}, pages 819--827, 2013.

\bibitem[du~Plessis and Sugiyama(2014)]{DBLP:journals/nn/PlessisS14}
Marthinus~Christoffel du~Plessis and Masashi Sugiyama.
\newblock Semi-supervised learning of class balance under class-prior change by
  distribution matching.
\newblock \emph{Neural Networks}, 50:\penalty0 110--119, 2014.

\bibitem[Nguyen et~al.(2015)Nguyen, du~Plessis, and
  Sugiyama]{DBLP:conf/acml/NguyenPS15}
Tuan~Duong Nguyen, Marthinus~Christoffel du~Plessis, and Masashi Sugiyama.
\newblock Continuous target shift adaptation in supervised learning.
\newblock In \emph{Proceedings of the 7th Asian Conference on Machine Learning
  ({ACML})}, pages 285--300, 2015.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{conf/icml/LiptonWS18}
Zachary~C. Lipton, Yu{-}Xiang Wang, and Alexander~J. Smola.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 3128--3136, 2018.

\bibitem[Azizzadenesheli et~al.(2019)Azizzadenesheli, Liu, Yang, and
  Anandkumar]{conf/iclr/Azizzadenesheli19}
Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar.
\newblock Regularized learning for domain adaptation under label shifts.
\newblock In \emph{Proceedings of the 7th International Conference on Learning
  Representations (ICLR)}, 2019.

\bibitem[Garg et~al.(2020)Garg, Wu, Balakrishnan, and
  Lipton]{DBLP:conf/nips/GargWBL20}
Saurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary~C. Lipton.
\newblock A unified view of label shift estimation.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pages 3290--3300, 2020.

\bibitem[Hazan(2016)]{book'16:Hazan-OCO}
Elad Hazan.
\newblock Introduction to {O}nline {C}onvex {O}ptimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Wu et~al.(2021{\natexlab{a}})Wu, Guo, Su, and
  Weinberger]{NIPS'21:Online-LS}
Ruihan Wu, Chuan Guo, Yi~Su, and Kilian~Q. Weinberger.
\newblock Online adaptation to label distribution shift.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS)}, pages 11340--11351, 2021{\natexlab{a}}.

\bibitem[Besbes et~al.(2015)Besbes, Gur, and Zeevi]{OR'15:dynamic-function-VT}
Omar Besbes, Yonatan Gur, and Assaf~J. Zeevi.
\newblock Non-stationary stochastic optimization.
\newblock \emph{Operations Research}, 63\penalty0 (5):\penalty0 1227--1244,
  2015.

\bibitem[Zhang et~al.(2018{\natexlab{a}})Zhang, Lu, and
  Zhou]{NIPS'18:Zhang-Ader}
Lijun Zhang, Shiyin Lu, and Zhi-Hua Zhou.
\newblock Adaptive online learning in dynamic environments.
\newblock In \emph{Advances in Neural Information Processing Systems 31
  (NeurIPS)}, pages 1330--1340, 2018{\natexlab{a}}.

\bibitem[Zhao et~al.(2020{\natexlab{a}})Zhao, Zhang, Zhang, and
  Zhou]{NIPS'20:sword}
Peng Zhao, Yu-Jie Zhang, Lijun Zhang, and Zhi-Hua Zhou.
\newblock Dynamic regret of convex and smooth functions.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pages 12510--12520, 2020{\natexlab{a}}.

\bibitem[Zinkevich(2003)]{ICML'03:zinkvich}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML)}, pages 928--936, 2003.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{book/Cambridge/cesa2006prediction}
Nicol\`{o} Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, {L}earning, and {G}ames}.
\newblock Cambridge {U}niversity {P}ress, 2006.

\bibitem[Bachem et~al.(2017)Bachem, Lucic, and Krause]{arXiv'17:coreset}
Olivier Bachem, Mario Lucic, and Andreas Krause.
\newblock Practical coreset constructions for machine learning.
\newblock arXiv:1703.06476, 2017.

\bibitem[Mirzasoleiman et~al.(2020)Mirzasoleiman, Bilmes, and
  Leskovec]{ICML'20:coreset}
Baharan Mirzasoleiman, Jeff~A. Bilmes, and Jure Leskovec.
\newblock Coresets for data-efficient training of machine learning models.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, pages 6950--6960, 2020.

\bibitem[Wu et~al.(2021{\natexlab{b}})Wu, Xu, Liu, and Zhou]{TKDE'21:RKME}
Xi-Zhu Wu, Wenkai Xu, Song Liu, and Zhi-Hua Zhou.
\newblock Model reuse with reduced kernel mean embedding specification.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering},
  35:\penalty0 699--710, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Yan, Zhao, and
  Zhou]{AAAI'21:UnseenJob}
Yu-Jie Zhang, Yu-Hu Yan, Peng Zhao, and Zhi-Hua Zhou.
\newblock Towards enabling learnware to handle unseen jobs.
\newblock In \emph{Proceedings of the 35th AAAI Conference on Artificial
  Intelligence (AAAI)}, pages 10964--10972, 2021{\natexlab{a}}.

\bibitem[Muandet et~al.(2017)Muandet, Fukumizu, Sriperumbudur, and
  Sch{\"{o}}lkopf]{FTML'17:KME}
Krikamol Muandet, Kenji Fukumizu, Bharath~K. Sriperumbudur, and Bernhard
  Sch{\"{o}}lkopf.
\newblock Kernel mean embedding of distributions: {A} review and beyond.
\newblock \emph{Foundations and Trends in Machine Learning}, 10\penalty0
  (1-2):\penalty0 1--141, 2017.

\bibitem[Jadbabaie et~al.(2015)Jadbabaie, Rakhlin, Shahrampour, and
  Sridharan]{AISTATS'15:dynamic-optimistic}
Ali Jadbabaie, Alexander Rakhlin, Shahin Shahrampour, and Karthik Sridharan.
\newblock Online optimization : Competing with dynamic comparators.
\newblock In \emph{Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 398--406, 2015.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Zhao, and Zhou]{UAI'20:Simple}
Yu{-}Jie Zhang, Peng Zhao, and Zhi{-}Hua Zhou.
\newblock A simple online algorithm for competing with dynamic comparators.
\newblock In \emph{Proceedings of the 36th Conference on Uncertainty in
  Artificial Intelligence (UAI)}, pages 390--399, 2020{\natexlab{a}}.

\bibitem[Abernethy et~al.(2008{\natexlab{a}})Abernethy, Bartlett, Rakhlin, and
  Tewari]{COLT'08:lower-bound}
Jacob Abernethy, Peter~L Bartlett, Alexander Rakhlin, and Ambuj Tewari.
\newblock Optimal strategies and minimax lower bounds for online convex games.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning Theory
  (COLT)}, pages 415--423, 2008{\natexlab{a}}.

\bibitem[Cesa{-}Bianchi et~al.(1997)Cesa{-}Bianchi, Freund, Haussler, Helmbold,
  Schapire, and Warmuth]{JACM'97:doubling-trick}
Nicol{\`{o}} Cesa{-}Bianchi, Yoav Freund, David Haussler, David~P. Helmbold,
  Robert~E. Schapire, and Manfred~K. Warmuth.
\newblock How to use expert advice.
\newblock \emph{Journal of the ACM}, 44\penalty0 (3):\penalty0 427--485, 1997.

\bibitem[Auer et~al.(2002)Auer, Cesa{-}Bianchi, and
  Gentile]{JCSS'02:Auer-self-confident}
Peter Auer, Nicol{\`{o}} Cesa{-}Bianchi, and Claudio Gentile.
\newblock Adaptive and self-confident on-line learning algorithms.
\newblock \emph{Journal of Computer and System Sciences}, 64\penalty0
  (1):\penalty0 48--75, 2002.

\bibitem[Freund and Schapire(1997)]{JCSS'97:boosting}
Yoav Freund and Robert~E. Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of Computer and System Sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Roughgarden(2020)]{book'20:beyond-worst-case}
Tim Roughgarden, editor.
\newblock \emph{Beyond the {W}orst-{C}ase {A}nalysis of {A}lgorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Zhao et~al.(2020{\natexlab{b}})Zhao, Cai, and Zhou]{MLJ'20:Condor}
Peng Zhao, Le-Wen Cai, and Zhi-Hua Zhou.
\newblock Handling concept drift via model reuse.
\newblock \emph{Machine Learning}, 109\penalty0 (3):\penalty0 533--568,
  2020{\natexlab{b}}.

\bibitem[Chiang et~al.(2012)Chiang, Yang, Lee, Mahdavi, Lu, Jin, and
  Zhu]{COLT'12:variation-Yang}
Chao-Kai Chiang, Tianbao Yang, Chia-Jung Lee, Mehrdad Mahdavi, Chi-Jen Lu, Rong
  Jin, and Shenghuo Zhu.
\newblock Online optimization with gradual variations.
\newblock In \emph{Proceedings of the 25th Conference On Learning Theory
  (COLT)}, pages 6.1--6.20, 2012.

\bibitem[Rakhlin and Sridharan(2013)]{conf/colt/RakhlinS13}
Alexander Rakhlin and Karthik Sridharan.
\newblock Online learning with predictable sequences.
\newblock In \emph{Proceedings of the 26th Conference On Learning Theory
  (COLT)}, pages 993--1019, 2013.

\bibitem[Kulis and Bartlett(2010)]{ICML'10:Implict-OL}
Brian Kulis and Peter~L. Bartlett.
\newblock Implicit online learning.
\newblock In \emph{Proceedings of the 27th International Conference on Machine
  Learning (ICML)}, pages 575--582, 2010.

\bibitem[Campolongo and Orabona(2020)]{NIPS'20:Implict-V_T}
Nicol{\`{o}} Campolongo and Francesco Orabona.
\newblock Temporal variability in implicit online learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  (NeurIPS)}, pages 12377--12387, 2020.

\bibitem[Gjoreski et~al.(2018)Gjoreski, Ciliberto, Wang, Morales, Mekki,
  Valentin, and Roggen]{DBLP:journals/access/GjoreskiCWMMVR18}
Hristijan Gjoreski, Mathias Ciliberto, Lin Wang, Francisco
  Javier~Ord{\'{o}}{\~{n}}ez Morales, Sami Mekki, Stefan Valentin, and Daniel
  Roggen.
\newblock The university of {Sussex-Huawei} locomotion and transportation
  dataset for multimodal analytics with mobile devices.
\newblock \emph{{IEEE} Access}, 6:\penalty0 42592--42604, 2018.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf]{journals/corr/abs-1910-01108}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock {DistilBERT}, a distilled version of {BERT:} smaller, faster, cheaper
  and lighter.
\newblock arXiv:1910.01108, 2019.

\bibitem[Helber et~al.(2018)Helber, Bischke, Dengel, and
  Borth]{DBLP:journals/staeors/HelberBDB19}
Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth.
\newblock Introducing {EuroSAT}: A novel dataset and deep learning benchmark
  for land use and land cover classification.
\newblock In \emph{Proceedings of the IEEE International Geoscience and Remote
  Sensing Symposium ({IGARSS})}, pages 204--207, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{DBLP:conf/cvpr/HeZRS16}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the {IEEE} Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 770--778, 2016.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{DBLP:journals/pieee/LeCunBBH98}
Yann LeCun, L{\'{e}}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the {IEEE}}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and
  Vollgraf]{DBLP:journals/corr/abs-1708-07747}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock {Fashion-MNIST}: A novel image dataset for benchmarking machine
  learning algorithms.
\newblock arXiv:1708.07747, 2017.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Darlow et~al.(2018)Darlow, Crowley, Antoniou, and
  Storkey]{DBLP:journals/corr/abs-1810-03505}
Luke~Nicholas Darlow, Elliot~J. Crowley, Antreas Antoniou, and Amos~J. Storkey.
\newblock {CINIC-10} is not {ImageNet} or {CIFAR-10}.
\newblock arXiv:1810.03505, 2018.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, pages 248--255, 2009.

\bibitem[Shalev-Shwartz(2012)]{book'12:Shai-OCO}
Shai Shalev-Shwartz.
\newblock Online {L}earning and {O}nline {C}onvex {O}ptimization.
\newblock \emph{Foundations and Trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2012.

\bibitem[Abernethy et~al.(2008{\natexlab{b}})Abernethy, Bartlett, Rakhlin, and
  Tewari]{DBLP:conf/colt/AbernethyBRT08}
Jacob~D. Abernethy, Peter~L. Bartlett, Alexander Rakhlin, and Ambuj Tewari.
\newblock Optimal stragies and minimax lower bounds for online convex games.
\newblock In \emph{Proceedings of the 21st Conference On Learning Theory
  (COLT)}, pages 415--424, 2008{\natexlab{b}}.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{journals/ml/HazanAK07}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Hazan and Kale(2008)]{COLT'08:Hazan-variation}
Elad Hazan and Satyen Kale.
\newblock Extracting certainty from uncertainty: Regret bounded by variation in
  costs.
\newblock In \emph{Proceedings of the 21st Annual Conference on Learning Theory
  (COLT)}, pages 57--68, 2008.

\bibitem[Blanchard et~al.(2010)Blanchard, Lee, and
  Scott]{journals/jmlr/BlanchardLS10}
Gilles Blanchard, Gyemin Lee, and Clayton Scott.
\newblock Semi-supervised novelty detection.
\newblock \emph{Journal of Machine Learning Research}, 11:\penalty0 2973--3009,
  2010.

\bibitem[du~Plessis et~al.(2017)du~Plessis, Niu, and
  Sugiyama]{MLJ'17:class-prior}
Marthinus~Christoffel du~Plessis, Gang Niu, and Masashi Sugiyama.
\newblock Class-prior estimation for learning from positive and unlabeled data.
\newblock \emph{Machine Learning}, 106\penalty0 (4):\penalty0 463--492, 2017.

\bibitem[Ramaswamy et~al.(2016)Ramaswamy, Scott, and
  Tewari]{conf/icml/RamaswamyST16}
Harish~G. Ramaswamy, Clayton Scott, and Ambuj Tewari.
\newblock Mixture proportion estimation via kernel embeddings of distributions.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning ({ICML})}, pages 2052--2060, 2016.

\bibitem[Scott(2015)]{conf/aistats/Scott15}
Clayton Scott.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{Proceedings of the 18th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 838--846, 2015.

\bibitem[du~Plessis et~al.(2014)du~Plessis, Niu, and
  Sugiyama]{conf/nips/PlessisNS14}
Marthinus~Christoffel du~Plessis, Gang Niu, and Masashi Sugiyama.
\newblock Analysis of learning from positive and unlabeled data.
\newblock In \emph{Advances in Neural Information Processing Systems 27
  ({NeurIPS})}, pages 703--711, 2014.

\bibitem[du~Plessis et~al.(2015)du~Plessis, Niu, and
  Sugiyama]{conf/icml/PlessisNS15}
Marthinus~Christoffel du~Plessis, Gang Niu, and Masashi Sugiyama.
\newblock Convex formulation for learning from positive and unlabeled data.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML)}, pages 1386--1394, 2015.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Zhao, Ma, and
  Zhou]{NeurIPS'20:EULAC}
Yu-Jie Zhang, Peng Zhao, Lanjihong Ma, and Zhi-Hua Zhou.
\newblock An unbiased risk estimator for learning with augmented classes.
\newblock In \emph{Advances in Neural Information Processing Systems 33
  ({NeurIPS})}, pages 10247--10258, 2020{\natexlab{b}}.

\bibitem[Yang et~al.(2016)Yang, Zhang, Jin, and Yi]{ICML'16:Yang-smooth}
Tianbao Yang, Lijun Zhang, Rong Jin, and Jinfeng Yi.
\newblock Tracking slowly moving clairvoyant: Optimal dynamic regret of online
  learning with true and noisy gradient.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning (ICML)}, pages 449--457, 2016.

\bibitem[Zhang et~al.(2017)Zhang, Yang, Yi, Jin, and Zhou]{NIPS:2017:Zhang}
Lijun Zhang, Tianbao Yang, Jinfeng Yi, Rong Jin, and Zhi-Hua Zhou.
\newblock Improved dynamic regret for non-degenerate functions.
\newblock In \emph{Advance in Neural Information Processing Systems 30 (NIPS)},
  pages 732--741, 2017.

\bibitem[Zhang et~al.(2018{\natexlab{b}})Zhang, Yang, Jin, and
  Zhou]{ICML'18:zhang-dynamic-adaptive}
Lijun Zhang, Tianbao Yang, Rong Jin, and Zhi-Hua Zhou.
\newblock Dynamic regret of strongly adaptive methods.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, pages 5877--5886, 2018{\natexlab{b}}.

\bibitem[Baby and Wang(2019)]{conf/nips/BabyW19}
Dheeraj Baby and Yu{-}Xiang Wang.
\newblock Online forecasting of total-variation-bounded sequences.
\newblock In \emph{Advances in Neural Information Processing Systems 32
  (NeurIPS)}, pages 11069--11079, 2019.

\bibitem[Chen et~al.(2019)Chen, Wang, and Wang]{OR'19:V_T-pq}
Xi~Chen, Yining Wang, and Yu-Xiang Wang.
\newblock Non-stationary stochastic optimization under ${L}_{p, q}$-variation
  measures.
\newblock \emph{Operations Research}, 67\penalty0 (6):\penalty0 1752--1765,
  2019.

\bibitem[Zhao and Zhang(2021{\natexlab{a}})]{L4DC'21:SC-Smooth}
Peng Zhao and Lijun Zhang.
\newblock Improved analysis for dynamic regret of strongly convex and smooth
  functions.
\newblock In \emph{Proceedings of the 3rd Conference on Learning for Dynamics
  and Control (L4DC)}, pages 48--59, 2021{\natexlab{a}}.

\bibitem[Mokhtari et~al.(2016)Mokhtari, Shahrampour, Jadbabaie, and
  Ribeiro]{CDC'16:dynamic-sc}
Aryan Mokhtari, Shahin Shahrampour, Ali Jadbabaie, and Alejandro Ribeiro.
\newblock Online optimization in dynamic environments: Improved regret rates
  for strongly convex problems.
\newblock In \emph{Proceedings of the 55th IEEE Conference on Decision and
  Control (CDC)}, pages 7195--7201, 2016.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Lu, and Yang]{AISTATS'20:Zhang}
Lijun Zhang, Shiyin Lu, and Tianbao Yang.
\newblock Minimizing dynamic regret and adaptive regret simultaneously.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 309--319,
  2020{\natexlab{c}}.

\bibitem[Cutkosky(2020)]{ICML'20:Ashok}
Ashok Cutkosky.
\newblock Parameter-free, dynamic, and strongly-adaptive online learning.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, pages 2250--2259, 2020.

\bibitem[Zhao et~al.(2021{\natexlab{a}})Zhao, Zhang, Zhang, and
  Zhou]{arXiv'21:Sword++}
Peng Zhao, Yu{-}Jie Zhang, Lijun Zhang, and Zhi{-}Hua Zhou.
\newblock Adaptivity and non-stationarity: Problem-dependent dynamic regret for
  online convex optimization.
\newblock arXiv:2112.14368, 2021{\natexlab{a}}.

\bibitem[Zhao et~al.(2021{\natexlab{b}})Zhao, Wang, Zhang, and
  Zhou]{JMLR'21:BCO}
Peng Zhao, Guanghui Wang, Lijun Zhang, and Zhi-Hua Zhou.
\newblock Bandit convex optimization in non-stationary environments.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (125):\penalty0 1 -- 45, 2021{\natexlab{b}}.

\bibitem[Baby and Wang(2021)]{conf/colt/BabyW21}
Dheeraj Baby and Yu{-}Xiang Wang.
\newblock Optimal dynamic regret in exp-concave online learning.
\newblock In \emph{Proceedings of the 34th Annual Conference on Learning Theory
  (COLT)}, pages 359--409, 2021.

\bibitem[Zhao et~al.(2022{\natexlab{a}})Zhao, Wang, and
  Zhou]{AISTATS'22:memory}
Peng Zhao, Yu-Xiang Wang, and Zhi-Hua Zhou.
\newblock Non-stationary online learning with memory and non-stochastic
  control.
\newblock In \emph{Proceedings of the 25th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 2101--2133,
  2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Jiang, Lu, and
  Yang]{NeurIPS:2021:Zhang:A}
Lijun Zhang, Wei Jiang, Shiyin Lu, and Tianbao Yang.
\newblock Revisiting smoothed online learning.
\newblock In \emph{Advances in Neural Information Processing Systems 34
  (NeurIPS)}, pages 13599--13612, 2021{\natexlab{b}}.

\bibitem[Zhao et~al.(2022{\natexlab{b}})Zhao, Li, and Zhou]{ICML'22:mdp}
Peng Zhao, Long-Fei Li, and Zhi-Hua Zhou.
\newblock Dynamic regret of online markov decision processes.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning (ICML)}, pages 26865--26894, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Zhao, Luo, and Zhou]{ICML'22:TVgame}
Mengxiao Zhang, Peng Zhao, Haipeng Luo, and Zhi-Hua Zhou.
\newblock No-regret learning in time-varying zero-sum games.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning (ICML))}, pages 26772--26808, 2022.

\bibitem[Jacobsen and Cutkosky(2022)]{colt'22:parameter-free-mirror-descent}
Andrew Jacobsen and Ashok Cutkosky.
\newblock Parameter-free mirror descent.
\newblock In \emph{Proceedings of the 35th Annual Conference on Learning Theory
  (COLT)}, pages 4160--4211, 2022.

\bibitem[Tropp(2012)]{FCM'12:Matrix-Concentration}
Joel~A. Tropp.
\newblock User-friendly tail bounds for sums of random matrices.
\newblock \emph{Foundations of Computational Mathematics}, 12\penalty0
  (4):\penalty0 389--434, 2012.

\bibitem[Hsu et~al.(2012)Hsu, Kakade, and Zhang]{JCSS'12:Hsu}
Daniel~J. Hsu, Sham~M. Kakade, and Tong Zhang.
\newblock A spectral algorithm for learning hidden markov models.
\newblock \emph{Journal of Computer and System Sciences}, 78\penalty0
  (5):\penalty0 1460--1480, 2012.

\bibitem[Zhang et~al.(2020{\natexlab{d}})Zhang, Zhao, and
  Jin]{AAAI'20:DynamicOMD}
Teng Zhang, Peng Zhao, and Hai Jin.
\newblock Optimal margin distribution learning in dynamic environments.
\newblock In \emph{Proceedings of the 34th {AAAI} Conference on Artificial
  Intelligence (AAAI)}, pages 6821--6828, 2020{\natexlab{d}}.

\bibitem[Zhou et~al.(2022)Zhou, Zhao, Zhang, Wang, Chang, Wang, and
  Zhu]{AISTATS'22:OnlineContinualAdaptation}
Shiji Zhou, Han Zhao, Shanghang Zhang, Lianzhe Wang, Heng Chang, Zhi Wang, and
  Wenwu Zhu.
\newblock Online continual adaptation with active self-training.
\newblock In \emph{Proceedings of the 25th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, pages 8852--8883, 2022.

\bibitem[Zhao and Zhang(2021{\natexlab{b}})]{L4DC'21:sc_smooth}
Peng Zhao and Lijun Zhang.
\newblock Improved analysis for dynamic regret of strongly convex and smooth
  functions.
\newblock In \emph{Proceedings of the 3rd Conference on Learning for Dynamics
  and Control (L4DC)}, pages 48--59, 2021{\natexlab{b}}.

\bibitem[Barbakh and Fyfe(2008)]{DBLP:journals/ijns/BarbakhF08}
Wesam Barbakh and Colin Fyfe.
\newblock Online clustering algorithms.
\newblock \emph{International Journal of Neural Systems}, 18\penalty0
  (3):\penalty0 185--194, 2008.

\bibitem[Horn and Johnson(2012)]{book'12:Matrix}
Roger~A. Horn and Charles~R. Johnson.
\newblock \emph{Matrix {A}nalysis}.
\newblock Cambridge University Press, second edition, 2012.

\bibitem[Syrgkanis et~al.(2015)Syrgkanis, Agarwal, Luo, and
  Schapire]{NIPS'15:fast-rate-game}
Vasilis Syrgkanis, Alekh Agarwal, Haipeng Luo, and Robert~E. Schapire.
\newblock Fast convergence of regularized learning in games.
\newblock In \emph{Advances in Neural Information Processing Systems 28
  (NIPS)}, pages 2989--2997, 2015.

\end{thebibliography}
