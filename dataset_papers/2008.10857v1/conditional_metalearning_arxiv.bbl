\begin{thebibliography}{10}

\bibitem{abernethy2009new}
J.~Abernethy, F.~Bach, T.~Evgeniou, and J.-P. Vert.
\newblock A new approach to collaborative filtering: Operator estimation with
  spectral regularization.
\newblock {\em Journal of Machine Learning Research}, 10(Mar):803--826, 2009.

\bibitem{argyriou2013learning}
A.~Argyriou, S.~Cl{\'e}men{\c{c}}on, and R.~Zhang.
\newblock Learning the graph of relations among multiple tasks.
\newblock 2013.

\bibitem{argyriou2008convex}
A.~Argyriou, T.~Evgeniou, and M.~Pontil.
\newblock Convex multi-task feature learning.
\newblock {\em Machine Learning}, 73(3):243--272, 2008.

\bibitem{argyriou2008algorithm}
A.~Argyriou, A.~Maurer, and M.~Pontil.
\newblock An algorithm for transfer learning in a heterogeneous environment.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 71--85. Springer, 2008.

\bibitem{balcan2019provable}
M.-F. Balcan, M.~Khodak, and A.~Talwalkar.
\newblock Provable guarantees for gradient-based meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  424--433, 2019.

\bibitem{bauschke2011convex}
H.~H. Bauschke and P.~L. Combettes.
\newblock {\em Convex Analysis and Monotone Operator theory in Hilbert Spaces},
  volume 408.
\newblock Springer, 2011.

\bibitem{baxter2000model}
J.~Baxter.
\newblock A model of inductive bias learning.
\newblock {\em J. Artif. Intell. Res.}, 12(149--198):3, 2000.

\bibitem{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock {\em Journal of machine learning research}, 2(Mar):499--526, 2002.

\bibitem{cai2020weighted}
T.~T. Cai, T.~Liang, and A.~Rakhlin.
\newblock Weighted message passing and minimum energy flow for heterogeneous
  stochastic block models with side information.
\newblock {\em Journal of Machine Learning Research}, 21(11):1--34, 2020.

\bibitem{cavallanti2010linear}
G.~Cavallanti, N.~Cesa-Bianchi, and C.~Gentile.
\newblock Linear algorithms for online multitask classification.
\newblock {\em Journal of Machine Learning Research}, 11:2901--2934, 2010.

\bibitem{ciliberto2020general}
C.~Ciliberto, L.~Rosasco, and A.~Rudi.
\newblock A general framework for consistent structured prediction with
  implicit loss embeddings.
\newblock {\em arXiv preprint arXiv:2002.05424}, 2020.

\bibitem{cutkosky2018black}
A.~Cutkosky and F.~Orabona.
\newblock Black-box reductions for parameter-free online learning in banach
  spaces.
\newblock In {\em Proceedings of the 31st Conference On Learning Theory},
  volume~75, 2018.

\bibitem{denevi2019learning}
G.~Denevi, C.~Ciliberto, R.~Grazzi, and M.~Pontil.
\newblock Learning-to-learn stochastic gradient descent with biased
  regularization.
\newblock In {\em International Conference on Machine Learning}, pages
  1566--1575, 2019.

\bibitem{denevi2019online}
G.~Denevi, D.~Stamos, C.~Ciliberto, and M.~Pontil.
\newblock Online-within-online meta-learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13089--13099, 2019.

\bibitem{evgeniou2004regularized}
T.~Evgeniou and M.~Pontil.
\newblock Regularized multi--task learning.
\newblock In {\em Proceedings of the tenth ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 109--117, 2004.

\bibitem{fallah2019convergence}
A.~Fallah, A.~Mokhtari, and A.~Ozdaglar.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock {\em arXiv preprint arXiv:1908.10400}, 2019.

\bibitem{pmlr-v70-finn17a}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  1126--1135. PMLR, 2017.

\bibitem{finn2019online}
C.~Finn, A.~Rajeswaran, S.~Kakade, and S.~Levine.
\newblock Online meta-learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1920--1930, 2019.

\bibitem{jacob2009clustered}
L.~Jacob, J.-p. Vert, and F.~R. Bach.
\newblock Clustered multi-task learning: A convex formulation.
\newblock In {\em Advances in neural information processing systems}, pages
  745--752, 2009.

\bibitem{jerfel2019reconciling}
G.~Jerfel, E.~Grant, T.~Griffiths, and K.~A. Heller.
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9119--9130, 2019.

\bibitem{ji2020multi}
K.~Ji, J.~Yang, and Y.~Liang.
\newblock Multi-step model-agnostic meta-learning: Convergence and improved
  algorithms.
\newblock {\em arXiv preprint arXiv:2002.07836}, 2020.

\bibitem{khodak2019adaptive}
M.~Khodak, M.-F.~F. Balcan, and A.~S. Talwalkar.
\newblock Adaptive gradient-based meta-learning methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5915--5926, 2019.

\bibitem{kivinen2004online}
J.~Kivinen, A.~J. Smola, and R.~C. Williamson.
\newblock Online learning with kernels.
\newblock {\em IEEE transactions on signal processing}, 52(8):2165--2176, 2004.

\bibitem{lenk1996hierarchical}
P.~J. Lenk, W.~S. DeSarbo, P.~E. Green, and M.~R. Young.
\newblock Hierarchical bayes conjoint analysis: Recovery of partworth
  heterogeneity from reduced experimental designs.
\newblock {\em Marketing Science}, 15(2):173--191, 1996.

\bibitem{maurer2006}
A.~Maurer.
\newblock The rademacher complexity of linear transformation classes.
\newblock In {\em International Conference on Computational Learning Theory},
  pages 65--78, 2006.

\bibitem{maurer2009transfer}
A.~Maurer.
\newblock Transfer bounds for linear feature learning.
\newblock {\em Machine Learning}, 75(3):327--350, 2009.

\bibitem{maurer2012transfer}
A.~Maurer and M.~Pontil.
\newblock Transfer learning in a heterogeneous environment.
\newblock In {\em 2012 3rd International Workshop on Cognitive Information
  Processing (CIP)}, pages 1--6. IEEE, 2012.

\bibitem{Andrew}
A.~M. McDonald, M.~Pontil, and D.~Stamos.
\newblock New perspectives on k-support and cluster norms.
\newblock {\em Journal of Machine Learning Research}, 17(155):1--38, 2016.

\bibitem{nichol2018first}
A.~Nichol, J.~Achiam, and J.~Schulman.
\newblock On first-order meta-learning algorithms.
\newblock {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{orabona2016coin}
F.~Orabona and D.~P{\'a}l.
\newblock Coin betting and parameter-free online learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  577--585, 2016.

\bibitem{pentina2014pac}
A.~Pentina and C.~Lampert.
\newblock A {PAC}-{B}ayesian bound for lifelong learning.
\newblock In {\em International Conference on Machine Learning}, pages
  991--999, 2014.

\bibitem{rahimi2008random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in neural information processing systems}, pages
  1177--1184, 2008.

\bibitem{rusu2018meta}
A.~A. Rusu, D.~Rao, J.~Sygnowski, O.~Vinyals, R.~Pascanu, S.~Osindero, and
  R.~Hadsell.
\newblock Meta-learning with latent embedding optimization.
\newblock {\em arXiv preprint arXiv:1807.05960}, 2018.

\bibitem{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock {\em Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem{singh2012online}
A.~Singh, N.~Ahuja, and P.~Moulin.
\newblock Online learning with kernels: Overcoming the growing sum problem.
\newblock In {\em 2012 IEEE International Workshop on Machine Learning for
  Signal Processing}, pages 1--6. IEEE, 2012.

\bibitem{tripuraneni2020provable}
N.~Tripuraneni, C.~Jin, and M.~I. Jordan.
\newblock Provable meta-learning of linear representations.
\newblock {\em arXiv preprint arXiv:2002.11684}, 2020.

\bibitem{vuorio2019multimodal}
R.~Vuorio, S.-H. Sun, H.~Hu, and J.~J. Lim.
\newblock Multimodal model-agnostic meta-learning via task-aware modulation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1--12, 2019.

\bibitem{wang2020structured}
R.~Wang, Y.~Demiris, and C.~Ciliberto.
\newblock A structured prediction approach for conditional meta-learning.
\newblock {\em arXiv preprint arXiv:2002.08799}, 2020.

\bibitem{yao2019hierarchically}
H.~Yao, Y.~Wei, J.~Huang, and Z.~Li.
\newblock Hierarchically structured meta-learning.
\newblock {\em arXiv preprint arXiv:1905.05301}, 2019.

\end{thebibliography}
