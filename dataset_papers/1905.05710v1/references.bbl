\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baxter \& Bartlett(2001)Baxter and Bartlett]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research}, 15:\penalty0
  319--350, 2001.

\bibitem[Deisenroth et~al.(2013)Deisenroth, Neumann, Peters,
  et~al.]{deisenroth2013survey}
Deisenroth, M.~P., Neumann, G., Peters, J., et~al.
\newblock A survey on policy search for robotics.
\newblock \emph{Foundations and Trends{\textregistered} in Robotics},
  2\penalty0 (1--2):\penalty0 1--142, 2013.

\bibitem[Duan et~al.(2016)Duan, Chen, Houthooft, Schulman, and
  Abbeel]{duan2016benchmarking}
Duan, Y., Chen, X., Houthooft, R., Schulman, J., and Abbeel, P.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1329--1338, 2016.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{espeholt2018impala}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1406--1415, 2018.

\bibitem[Greensmith et~al.(2004)Greensmith, Bartlett, and
  Baxter]{greensmith2004variance}
Greensmith, E., Bartlett, P.~L., and Baxter, J.
\newblock Variance reduction techniques for gradient estimates in reinforcement
  learning.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Nov):\penalty0 1471--1530, 2004.

\bibitem[Ilyas et~al.(2018)Ilyas, Engstrom, Santurkar, Tsipras, Janoos,
  Rudolph, and Madry]{ilyas2018deep}
Ilyas, A., Engstrom, L., Santurkar, S., Tsipras, D., Janoos, F., Rudolph, L.,
  and Madry, A.
\newblock Are deep policy gradient algorithms truly policy gradient algorithms?
\newblock \emph{arXiv preprint arXiv:1811.02553}, 2018.

\bibitem[Jie \& Abbeel(2010)Jie and Abbeel]{jie2010connection}
Jie, T. and Abbeel, P.
\newblock On a connection between importance sampling and the likelihood ratio
  policy gradient.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1000--1008, 2010.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Metelli et~al.(2018)Metelli, Papini, Faccio, and
  Restelli]{metelli2018policy}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.
\newblock Policy optimization via importance sampling.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pp.\  5442--5454, 2018.

\bibitem[Meuleau et~al.(2000)Meuleau, Peshkin, Kaelbling, and
  Kim]{meuleau2000off}
Meuleau, N., Peshkin, L., Kaelbling, L.~P., and Kim, K.-E.
\newblock Off-policy policy search.
\newblock \emph{MIT Articical Intelligence Laboratory}, 2000.

\bibitem[Munos(2006)]{munos2006policy}
Munos, R.
\newblock Policy gradient in continuous time.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0
  (May):\penalty0 771--791, 2006.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pp.\  1054--1062, 2016.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and
  Van~Roy]{osband2016deep}
Osband, I., Blundell, C., Pritzel, A., and Van~Roy, B.
\newblock Deep exploration via bootstrapped {DQN}.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pp.\  4026--4034, 2016.

\bibitem[Peshkin \& Shelton(2002)Peshkin and Shelton]{peshkin2002learning}
Peshkin, L. and Shelton, C.~R.
\newblock Learning from scarce experience.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  498--505. Morgan Kaufmann Publishers Inc., 2002.

\bibitem[Peters \& Schaal(2008{\natexlab{a}})Peters and
  Schaal]{peters2008natural}
Peters, J. and Schaal, S.
\newblock Natural actor-critic.
\newblock \emph{Neurocomputing}, 71\penalty0 (7-9):\penalty0 1180--1190,
  2008{\natexlab{a}}.

\bibitem[Peters \& Schaal(2008{\natexlab{b}})Peters and
  Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697,
  2008{\natexlab{b}}.

\bibitem[Plappert et~al.(2017)Plappert, Houthooft, Dhariwal, Sidor, Chen, Chen,
  Asfour, Abbeel, and Andrychowicz]{plappert2017parameter}
Plappert, M., Houthooft, R., Dhariwal, P., Sidor, S., Chen, R.~Y., Chen, X.,
  Asfour, T., Abbeel, P., and Andrychowicz, M.
\newblock Parameter space noise for exploration.
\newblock \emph{arXiv preprint arXiv:1706.01905}, 2017.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Precup, D., Sutton, R.~S., and Singh, S.~P.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  759--766. Citeseer, 2000.

\bibitem[Schaul et~al.(2015)Schaul, Quan, Antonoglou, and
  Silver]{schaul2015prioritized}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{arXiv preprint arXiv:1511.05952}, 2015.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sehnke et~al.(2008)Sehnke, Osendorfer, R{\"u}ckstie{\ss}, Graves,
  Peters, and Schmidhuber]{sehnke2008policy}
Sehnke, F., Osendorfer, C., R{\"u}ckstie{\ss}, T., Graves, A., Peters, J., and
  Schmidhuber, J.
\newblock Policy gradients with parameter-based exploration for control.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pp.\  387--396. Springer, 2008.

\bibitem[Shelton(2001)]{shelton2001policy}
Shelton, C.~R.
\newblock Policy improvement for {POMDPs} using normalized importance sampling.
\newblock In \emph{Proceedings of the Seventeenth conference on Uncertainty in
  artificial intelligence (UAI)}, pp.\  496--503. Morgan Kaufmann Publishers
  Inc., 2001.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{ICML}, 2014.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2951--2959, 2012.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pp.\  1057--1063, 2000.

\bibitem[Wawrzynski \& Pacut(2007)Wawrzynski and
  Pacut]{wawrzynski2007truncated}
Wawrzynski, P. and Pacut, A.
\newblock Truncated importance sampling for reinforcement learning with
  experience replay.
\newblock \emph{Proc. CSIT Int. Multiconf}, pp.\  305--315, 2007.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock In \emph{Reinforcement Learning}, pp.\  5--32. Springer, 1992.

\bibitem[Zhao et~al.(2013)Zhao, Hachiya, Tangkaratt, Morimoto, and
  Sugiyama]{zhao2013efficient}
Zhao, T., Hachiya, H., Tangkaratt, V., Morimoto, J., and Sugiyama, M.
\newblock Efficient sample reuse in policy gradients with parameter-based
  exploration.
\newblock \emph{Neural computation}, 25\penalty0 (6):\penalty0 1512--1547,
  2013.

\end{thebibliography}
