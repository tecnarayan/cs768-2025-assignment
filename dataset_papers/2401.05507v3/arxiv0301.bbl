\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[01ai(2023)]{2023yi}
01ai.
\newblock Yi.
\newblock \url{https://01.ai/}, 2023.

\bibitem[Anthropic(2023)]{claude2023}
Anthropic.
\newblock Claude-2.1.
\newblock \url{https://www.anthropic.com/index/claude-2-1}, 2023.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Bai et~al.(2023)Bai, Bai, Chu, Cui, Dang, Deng, Fan, Ge, Han, Huang, et~al.]{bai2023qwen}
Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et~al.
\newblock Qwen technical report.
\newblock \emph{arXiv preprint arXiv:2309.16609}, 2023.

\bibitem[Chan et~al.(2023)Chan, Chen, Su, Yu, Xue, Zhang, Fu, and Liu]{chan2023chateval}
Chan, C.-M., Chen, W., Su, Y., Yu, J., Xue, W., Zhang, S., Fu, J., and Liu, Z.
\newblock Chateval: Towards better llm-based evaluators through multi-agent debate.
\newblock \emph{arXiv preprint arXiv:2308.07201}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2023)Chen, Lin, Sch{\"a}rli, and Zhou]{chen2023teaching}
Chen, X., Lin, M., Sch{\"a}rli, N., and Zhou, D.
\newblock Teaching large language models to self-debug.
\newblock \emph{arXiv preprint arXiv:2304.05128}, 2023.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Covington(2016)]{covington2016analytics}
Covington, D.
\newblock \emph{Analytics: Data Science, Data Analysis, and Predictive Analytics for Business}.
\newblock CreateSpace Independent Publishing Platform, 2016.

\bibitem[DeepSeek(2023)]{deepseek-coder}
DeepSeek.
\newblock Deepseek coder: Let the code write itself.
\newblock \url{https://github.com/deepseek-ai/DeepSeek-Coder}, 2023.

\bibitem[Fu et~al.(2023)Fu, Ng, Jiang, and Liu]{fu2023gptscore}
Fu, J., Ng, S.-K., Jiang, Z., and Liu, P.
\newblock Gptscore: Evaluate as you desire.
\newblock \emph{arXiv preprint arXiv:2302.04166}, 2023.

\bibitem[Goyal et~al.(2017)Goyal, Khot, Summers-Stay, Batra, and Parikh]{goyal2017making}
Goyal, Y., Khot, T., Summers-Stay, D., Batra, D., and Parikh, D.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering, 2017.

\bibitem[Hardy \& Bryman(2004)Hardy and Bryman]{hardy2004handbook}
Hardy, M.~A. and Bryman, A.
\newblock Handbook of data analysis.
\newblock 2004.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Kadavath, Mazeika, Arora, Guo, Burns, Puranik, He, Song, and Steinhardt]{hendrycks2021measuring}
Hendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S., He, H., Song, D., and Steinhardt, J.
\newblock Measuring coding challenge competence with apps, 2021.

\bibitem[Huang \& Chang(2023)Huang and Chang]{huang2023reasoning}
Huang, J. and Chang, K. C.-C.
\newblock Towards reasoning in large language models: A survey, 2023.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems}, 35:\penalty0 22199--22213, 2022.

\bibitem[Koop(2022)]{koop2022analysis}
Koop, G.
\newblock \emph{Analysis of financial data}.
\newblock John Wiley \& Sons Inc., 2022.

\bibitem[Kwon et~al.(2023)Kwon, Li, Zhuang, Sheng, Zheng, Yu, Gonzalez, Zhang, and Stoica]{kwon2023efficient}
Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C.~H., Gonzalez, J.~E., Zhang, H., and Stoica, I.
\newblock Efficient memory management for large language model serving with pagedattention.
\newblock In \emph{Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles}, 2023.

\bibitem[Lai et~al.(2023)Lai, Li, Wang, Zhang, Zhong, Zettlemoyer, Yih, Fried, Wang, and Yu]{lai2023ds}
Lai, Y., Li, C., Wang, Y., Zhang, T., Zhong, R., Zettlemoyer, L., Yih, W.-t., Fried, D., Wang, S., and Yu, T.
\newblock Ds-1000: A natural and reliable benchmark for data science code generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\  18319--18345. PMLR, 2023.

\bibitem[Li et~al.(2023)Li, Song, Yu, Yu, Li, Huang, and Li]{li2023api}
Li, M., Song, F., Yu, B., Yu, H., Li, Z., Huang, F., and Li, Y.
\newblock Api-bank: A benchmark for tool-augmented llms.
\newblock \emph{arXiv preprint arXiv:2304.08244}, 2023.

\bibitem[Li et~al.(2022)Li, Choi, Chung, Kushman, Schrittwieser, Leblond, Eccles, Keeling, Gimeno, Dal~Lago, et~al.]{li2022competition}
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal~Lago, A., et~al.
\newblock Competition-level code generation with alphacode.
\newblock \emph{Science}, 378\penalty0 (6624):\penalty0 1092--1097, 2022.

\bibitem[Liang et~al.(2023)Liang, Wang, Huang, Wu, Wu, Lu, Ma, and Li]{liang2023unleashing}
Liang, X., Wang, B., Huang, H., Wu, S., Wu, P., Lu, L., Ma, Z., and Li, Z.
\newblock Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system.
\newblock \emph{arXiv preprint arXiv:2304.13343}, 2023.

\bibitem[Liu et~al.(2023)Liu, Sarkar, Negreanu, Zorn, Williams, Toronto, and Gordon]{liu2023wants}
Liu, M.~X., Sarkar, A., Negreanu, C., Zorn, B., Williams, J., Toronto, N., and Gordon, A.~D.
\newblock “what it wants me to say”: Bridging the abstraction gap between end-user programmers and code-generating large language models.
\newblock In \emph{Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems}, pp.\  1--31, 2023.

\bibitem[Lu et~al.(2021)Lu, Guo, Ren, Huang, Svyatkovskiy, Blanco, Clement, Drain, Jiang, Tang, et~al.]{lu2021codexglue}
Lu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang, D., Tang, D., et~al.
\newblock Codexglue: A machine learning benchmark dataset for code understanding and generation.
\newblock \emph{arXiv preprint arXiv:2102.04664}, 2021.

\bibitem[Lucas(2023)]{2023openinterpreter}
Lucas, K.
\newblock Open interpreter.
\newblock \url{https://github.com/KillianLucas/open-interpreter}, 2023.

\bibitem[Luo et~al.(2023)Luo, Xu, Zhao, Sun, Geng, Hu, Tao, Ma, Lin, and Jiang]{luo2023wizardcoder}
Luo, Z., Xu, C., Zhao, P., Sun, Q., Geng, X., Hu, W., Tao, C., Ma, J., Lin, Q., and Jiang, D.
\newblock Wizardcoder: Empowering code large language models with evol-instruct.
\newblock \emph{arXiv preprint arXiv:2306.08568}, 2023.

\bibitem[Mathew et~al.(2021)Mathew, Karatzas, and Jawahar]{mathew2021docvqa}
Mathew, M., Karatzas, D., and Jawahar, C.~V.
\newblock Docvqa: A dataset for vqa on document images, 2021.

\bibitem[MiniMax(2023)]{minimax2023abab5.5}
MiniMax.
\newblock Abab5.5.
\newblock \url{https://api.minimax.chat/}, 2023.

\bibitem[Mistral.ai(2023)]{mistral2023}
Mistral.ai.
\newblock Mistral.
\newblock \url{https://mistral.ai/product/}, 2023.

\bibitem[Nakajima(2023)]{2023babyagi}
Nakajima, Y.
\newblock Babyagi.
\newblock \url{https://github.com/yoheinakajima/babyagi}, 2023.

\bibitem[OpenAI(2023{\natexlab{a}})]{openai2023gpt3.5}
OpenAI.
\newblock Openai models - openai api.
\newblock \url{https://platform.openai.com/docs/models/gpt-3-5}, 2023{\natexlab{a}}.

\bibitem[OpenAI(2023{\natexlab{b}})]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/pdf/2303.08774.pdf}.

\bibitem[Patil et~al.(2023)Patil, Zhang, Wang, and Gonzalez]{patil2023gorilla}
Patil, S.~G., Zhang, T., Wang, X., and Gonzalez, J.~E.
\newblock Gorilla: Large language model connected with massive apis.
\newblock \emph{arXiv preprint arXiv:2305.15334}, 2023.

\bibitem[Phind(2023)]{phind2023phindcodellama}
Phind.
\newblock Phind code llama.
\newblock \url{https://www.phind.com/blog/code-llama-beats-gpt4}, 2023.

\bibitem[Qiao et~al.(2023)Qiao, Li, Zhang, He, Kang, Zhang, Yang, Dong, Zhang, Wang, et~al.]{qiao2023taskweaver}
Qiao, B., Li, L., Zhang, X., He, S., Kang, Y., Zhang, C., Yang, F., Dong, H., Zhang, J., Wang, L., et~al.
\newblock Taskweaver: A code-first agent framework.
\newblock \emph{arXiv preprint arXiv:2311.17541}, 2023.

\bibitem[Qin et~al.(2023{\natexlab{a}})Qin, Hu, Lin, Chen, Ding, Cui, Zeng, Huang, Xiao, Han, Fung, Su, Wang, Qian, Tian, Zhu, Liang, Shen, Xu, Zhang, Ye, Li, Tang, Yi, Zhu, Dai, Yan, Cong, Lu, Zhao, Huang, Yan, Han, Sun, Li, Phang, Yang, Wu, Ji, Liu, and Sun]{qin2023tool}
Qin, Y., Hu, S., Lin, Y., Chen, W., Ding, N., Cui, G., Zeng, Z., Huang, Y., Xiao, C., Han, C., Fung, Y.~R., Su, Y., Wang, H., Qian, C., Tian, R., Zhu, K., Liang, S., Shen, X., Xu, B., Zhang, Z., Ye, Y., Li, B., Tang, Z., Yi, J., Zhu, Y., Dai, Z., Yan, L., Cong, X., Lu, Y., Zhao, W., Huang, Y., Yan, J., Han, X., Sun, X., Li, D., Phang, J., Yang, C., Wu, T., Ji, H., Liu, Z., and Sun, M.
\newblock Tool learning with foundation models, 2023{\natexlab{a}}.

\bibitem[Qin et~al.(2023{\natexlab{b}})Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, Zhao, Hong, Tian, Xie, Zhou, Gerstein, Li, Liu, and Sun]{qin2023toolllm}
Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., Zhao, S., Hong, L., Tian, R., Xie, R., Zhou, J., Gerstein, M., Li, D., Liu, Z., and Sun, M.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis, 2023{\natexlab{b}}.

\bibitem[Reddy \& Aggarwal(2015)Reddy and Aggarwal]{reddy2015healthcare}
Reddy, C.~K. and Aggarwal, C.~C.
\newblock \emph{Healthcare data analytics}, volume~36.
\newblock CRC Press, 2015.

\bibitem[Reworkd(2023)]{2023agentgpt}
Reworkd.
\newblock Agentgpt.
\newblock \url{https://github.com/reworkd/AgentGPT}, 2023.

\bibitem[Rozière et~al.(2023)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve]{rozière2023code}
Rozière, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C.~C., Grattafiori, A., Xiong, W., Défossez, A., Copet, J., Azhar, F., Touvron, H., Martin, L., Usunier, N., Scialom, T., and Synnaeve, G.
\newblock Code llama: Open foundation models for code.
\newblock 2023.
\newblock URL \url{https://arxiv.org/pdf/2308.12950.pdf}.

\bibitem[Sabina \& Zalta(2020)Sabina and Zalta]{sabina2020scientific}
Sabina, L. and Zalta, E.~N.
\newblock Scientific research and big data.
\newblock \emph{The Stanford Encyclopedia of Philosophy (Summer 2020 Edition)}, 2020.

\bibitem[Sun et~al.(2023)Sun, Zhuang, Kong, Dai, and Zhang]{sun2023adaplanner}
Sun, H., Zhuang, Y., Kong, L., Dai, B., and Zhang, C.
\newblock Adaplanner: Adaptive planning from feedback with language models.
\newblock \emph{arXiv preprint arXiv:2305.16653}, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Team(2023{\natexlab{a}})]{2023internlm}
Team, I.
\newblock Internlm: A multilingual language model with progressively enhanced capabilities.
\newblock \url{https://github.com/InternLM/InternLM}, 2023{\natexlab{a}}.

\bibitem[Team(2023{\natexlab{b}})]{xagent2023}
Team, X.
\newblock Xagent: An autonomous agent for complex task solving, 2023{\natexlab{b}}.

\bibitem[Team(2023{\natexlab{c}})]{xwin-lm}
Team, X.-L.
\newblock Xwin-lm, 9 2023{\natexlab{c}}.
\newblock URL \url{https://github.com/Xwin-LM/Xwin-LM}.

\bibitem[Torantulino(2023)]{2023autogpt}
Torantulino.
\newblock Autogpt.
\newblock \url{https://github.com/Significant-Gravitas/AutoGPT}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Liang, Meng, Shi, Li, Xu, Qu, and Zhou]{wang2023chatgpt}
Wang, J., Liang, Y., Meng, F., Shi, H., Li, Z., Xu, J., Qu, J., and Zhou, J.
\newblock Is chatgpt a good nlg evaluator? a preliminary study.
\newblock \emph{arXiv preprint arXiv:2303.04048}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Ma, Feng, Zhang, Yang, Zhang, Chen, Tang, Chen, Lin, et~al.]{wang2023survey}
Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et~al.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{arXiv preprint arXiv:2308.11432}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Li, Chen, Cai, Zhu, Lin, Cao, Liu, Liu, and Sui]{wang2023large}
Wang, P., Li, L., Chen, L., Cai, Z., Zhu, D., Lin, B., Cao, Y., Liu, Q., Liu, T., and Sui, Z.
\newblock Large language models are not fair evaluators, 2023{\natexlab{c}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V., Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24824--24837, 2022.

\bibitem[Wu et~al.(2023)Wu, Bansal, Zhang, Wu, Li, Zhu, Jiang, Zhang, Zhang, Liu, Awadallah, White, Burger, and Wang]{wu2023autogen}
Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu, E., Jiang, L., Zhang, X., Zhang, S., Liu, J., Awadallah, A.~H., White, R.~W., Burger, D., and Wang, C.
\newblock Autogen: Enabling next-gen llm applications via multi-agent conversation framework.
\newblock 2023.

\bibitem[Xi et~al.(2023)Xi, Chen, Guo, He, Ding, Hong, Zhang, Wang, Jin, Zhou, et~al.]{xi2023rise}
Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B., Zhang, M., Wang, J., Jin, S., Zhou, E., et~al.
\newblock The rise and potential of large language model based agents: A survey.
\newblock \emph{arXiv preprint arXiv:2309.07864}, 2023.

\bibitem[Xie et~al.(2023)Xie, Zhou, Cheng, Shi, Weng, Liu, Hua, Zhao, Liu, Liu, et~al.]{xie2023openagents}
Xie, T., Zhou, F., Cheng, Z., Shi, P., Weng, L., Liu, Y., Hua, T.~J., Zhao, J., Liu, Q., Liu, C., et~al.
\newblock Openagents: An open platform for language agents in the wild.
\newblock \emph{arXiv preprint arXiv:2310.10634}, 2023.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Liu, Shen, Han, Li, Yue, Peng, Liu, Yao, and Xu]{xu2023gentopia}
Xu, B., Liu, X., Shen, H., Han, Z., Li, Y., Yue, M., Peng, Z., Liu, Y., Yao, Z., and Xu, D.
\newblock Gentopia: A collaborative platform for tool-augmented llms, 2023{\natexlab{a}}.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Peng, Lei, Mukherjee, Liu, and Xu]{xu2023rewoo}
Xu, B., Peng, Z., Lei, B., Mukherjee, S., Liu, Y., and Xu, D.
\newblock Rewoo: Decoupling reasoning from observations for efficient augmented language models.
\newblock \emph{arXiv preprint arXiv:2305.18323}, 2023{\natexlab{b}}.

\bibitem[Yang et~al.(2023)Yang, Xiao, Wang, Zhang, Bian, Yin, Lv, Pan, Wang, Yan, Yang, Deng, Wang, Liu, Ai, Dong, Zhao, Xu, Sun, Zhang, Liu, Ji, Xie, Dai, Fang, Su, Song, Liu, Ru, Ma, Wang, Liu, Lin, Nie, Guo, Sun, Zhang, Li, Li, Cheng, Chen, Zeng, Wang, Chen, Men, Yu, Pan, Shen, Wang, Li, Jiang, Gao, Zhang, Zhou, and Wu]{yang2023baichuan}
Yang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin, C., Lv, C., Pan, D., Wang, D., Yan, D., Yang, F., Deng, F., Wang, F., Liu, F., Ai, G., Dong, G., Zhao, H., Xu, H., Sun, H., Zhang, H., Liu, H., Ji, J., Xie, J., Dai, J., Fang, K., Su, L., Song, L., Liu, L., Ru, L., Ma, L., Wang, M., Liu, M., Lin, M., Nie, N., Guo, P., Sun, R., Zhang, T., Li, T., Li, T., Cheng, W., Chen, W., Zeng, X., Wang, X., Chen, X., Men, X., Yu, X., Pan, X., Shen, Y., Wang, Y., Li, Y., Jiang, Y., Gao, Y., Zhang, Y., Zhou, Z., and Wu, Z.
\newblock Baichuan 2: Open large-scale language models, 2023.

\bibitem[Yang et~al.(2024)Yang, Liu, Wu, Yang, Fung, Li, Huang, Cao, Wang, Wang, et~al.]{yang2024if}
Yang, K., Liu, J., Wu, J., Yang, C., Fung, Y.~R., Li, S., Huang, Z., Cao, X., Wang, X., Wang, Y., et~al.
\newblock If llm is the wizard, then code is the wand: A survey on how code empowers large language models to serve as intelligent agents.
\newblock \emph{arXiv preprint arXiv:2401.00812}, 2024.

\bibitem[Yao et~al.(2022{\natexlab{a}})Yao, Chen, Yang, and Narasimhan]{yao2022webshop}
Yao, S., Chen, H., Yang, J., and Narasimhan, K.
\newblock Webshop: Towards scalable real-world web interaction with grounded language agents.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 20744--20757, 2022{\natexlab{a}}.

\bibitem[Yao et~al.(2022{\natexlab{b}})Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022{\natexlab{b}}.

\bibitem[Yao et~al.(2023)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2023react}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock 2023.
\newblock URL \url{https://arxiv.org/pdf/2210.03629.pdf}.

\bibitem[Zan et~al.(2023)Zan, Chen, Zhang, Lu, Wu, Guan, Yongji, and Lou]{zan-etal-2023-large}
Zan, D., Chen, B., Zhang, F., Lu, D., Wu, B., Guan, B., Yongji, W., and Lou, J.-G.
\newblock Large language models meet {NL}2{C}ode: A survey.
\newblock In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  7443--7464, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.411}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.411}.

\bibitem[Zeng et~al.(2022)Zeng, Liu, Du, Wang, Lai, Ding, Yang, Xu, Zheng, Xia, et~al.]{zeng2022glm}
Zeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M., Yang, Z., Xu, Y., Zheng, W., Xia, X., et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock \emph{arXiv preprint arXiv:2210.02414}, 2022.

\bibitem[Zeng et~al.(2023)Zeng, Liu, Lu, Wang, Liu, Dong, and Tang]{zeng2023agenttuning}
Zeng, A., Liu, M., Lu, R., Wang, B., Liu, X., Dong, Y., and Tang, J.
\newblock Agenttuning: Enabling generalized agent abilities for llms, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Zhao, Kang, and Liu]{zhang2023memory}
Zhang, K., Zhao, F., Kang, Y., and Liu, X.
\newblock Memory-augmented llm personalization with short-and long-term memory coordination.
\newblock \emph{arXiv preprint arXiv:2309.11696}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Chen, Liu, Liao, Gong, Yu, Li, and Wang]{zhang2023unifying}
Zhang, Z., Chen, C., Liu, B., Liao, C., Gong, Z., Yu, H., Li, J., and Wang, R.
\newblock Unifying the perspectives of nlp and software engineering: A survey on language models for code, 2023{\natexlab{b}}.

\bibitem[Zhao et~al.(2023)Zhao, Jin, and Cheng]{zhao2023depth}
Zhao, P., Jin, Z., and Cheng, N.
\newblock An in-depth survey of large language model-based artificial intelligence agents.
\newblock \emph{arXiv preprint arXiv:2309.14365}, 2023.

\bibitem[Zhou et~al.(2023)Zhou, Xu, Zhu, Zhou, Lo, Sridhar, Cheng, Ou, Bisk, Fried, Alon, and Neubig]{zhou2023webarena}
Zhou, S., Xu, F.~F., Zhu, H., Zhou, X., Lo, R., Sridhar, A., Cheng, X., Ou, T., Bisk, Y., Fried, D., Alon, U., and Neubig, G.
\newblock Webarena: A realistic web environment for building autonomous agents, 2023.

\end{thebibliography}
