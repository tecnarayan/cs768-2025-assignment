\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balduzzi et~al.(2019)Balduzzi, Garnelo, Bachrach, Czarnecki, Perolat,
  Jaderberg, and Graepel]{rectified_psro}
D.~Balduzzi, M.~Garnelo, Y.~Bachrach, W.~Czarnecki, J.~Perolat, M.~Jaderberg,
  and T.~Graepel.
\newblock Open-ended learning in symmetric zero-sum games.
\newblock In \emph{International Conference on Machine Learning}, pages
  434--443, 2019.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{dota}
C.~Berner, G.~Brockman, B.~Chan, V.~Cheung, P.~D{\k{e}}biak, C.~Dennison,
  D.~Farhi, Q.~Fischer, S.~Hashme, C.~Hesse, et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bowling and Veloso(2002)]{bowling2002multiagent}
M.~Bowling and M.~Veloso.
\newblock Multiagent learning using a variable learning rate.
\newblock \emph{Artificial Intelligence}, 136\penalty0 (2):\penalty0 215--250,
  2002.

\bibitem[Brown and Sandholm(2018)]{brown2018superhuman}
N.~Brown and T.~Sandholm.
\newblock Superhuman ai for heads-up no-limit poker: Libratus beats top
  professionals.
\newblock \emph{Science}, 359\penalty0 (6374):\penalty0 418--424, 2018.

\bibitem[Brown et~al.(2019)Brown, Lerer, Gross, and Sandholm]{deep_cfr}
N.~Brown, A.~Lerer, S.~Gross, and T.~Sandholm.
\newblock Deep counterfactual regret minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  793--802, 2019.

\bibitem[Christodoulou(2019)]{discrete_sac}
P.~Christodoulou.
\newblock Soft actor-critic for discrete action settings.
\newblock \emph{arXiv preprint arXiv:1910.07207}, 2019.

\bibitem[Fudenberg et~al.(1998)Fudenberg, Drew, Levine, and
  Levine]{theory_of_learning_in_games}
D.~Fudenberg, F.~Drew, D.~K. Levine, and D.~K. Levine.
\newblock \emph{The theory of learning in games}.
\newblock The MIT Press, 1998.

\bibitem[Gruslys et~al.(2020)Gruslys, Lanctot, Munos, Timbers, Schmid, Perolat,
  Morrill, Zambaldi, Lespiau, Schultz, et~al.]{gruslys2020advantage}
A.~Gruslys, M.~Lanctot, R.~Munos, F.~Timbers, M.~Schmid, J.~Perolat,
  D.~Morrill, V.~Zambaldi, J.-B. Lespiau, J.~Schultz, et~al.
\newblock The advantage regret-matching actor-critic.
\newblock \emph{arXiv preprint arXiv:2008.12234}, 2020.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
T.~Haarnoja, A.~Zhou, P.~Abbeel, and S.~Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1861--1870, 2018.

\bibitem[Heinrich and Silver(2016)]{nfsp}
J.~Heinrich and D.~Silver.
\newblock Deep reinforcement learning from self-play in imperfect-information
  games.
\newblock \emph{arXiv preprint arXiv:1603.01121}, 2016.

\bibitem[Heinrich et~al.(2015)Heinrich, Lanctot, and Silver]{xfp}
J.~Heinrich, M.~Lanctot, and D.~Silver.
\newblock Fictitious self-play in extensive-form games.
\newblock In \emph{International Conference on Machine Learning}, pages
  805--813, 2015.

\bibitem[Jaderberg et~al.(2019)Jaderberg, Czarnecki, Dunning, Marris, Lever,
  Castaneda, Beattie, Rabinowitz, Morcos, Ruderman, et~al.]{pbt}
M.~Jaderberg, W.~M. Czarnecki, I.~Dunning, L.~Marris, G.~Lever, A.~G.
  Castaneda, C.~Beattie, N.~C. Rabinowitz, A.~S. Morcos, A.~Ruderman, et~al.
\newblock Human-level performance in 3d multiplayer games with population-based
  reinforcement learning.
\newblock \emph{Science}, 364\penalty0 (6443):\penalty0 859--865, 2019.

\bibitem[Jug and Schadd(2009)]{stratego_bots}
S.~Jug and M.~Schadd.
\newblock The 3rd stratego computer world championship.
\newblock \emph{Icga Journal}, 32\penalty0 (4):\penalty0 233, 2009.

\bibitem[Lanctot et~al.(2017)Lanctot, Zambaldi, Gruslys, Lazaridou, Tuyls,
  P{\'e}rolat, Silver, and Graepel]{psro}
M.~Lanctot, V.~Zambaldi, A.~Gruslys, A.~Lazaridou, K.~Tuyls, J.~P{\'e}rolat,
  D.~Silver, and T.~Graepel.
\newblock A unified game-theoretic approach to multiagent reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4190--4203, 2017.

\bibitem[Li(2017)]{deep_rl}
Y.~Li.
\newblock Deep reinforcement learning: An overview.
\newblock \emph{arXiv preprint arXiv:1701.07274}, 2017.

\bibitem[Liang et~al.(2018)Liang, Liaw, Nishihara, Moritz, Fox, Goldberg,
  Gonzalez, Jordan, and Stoica]{rllib}
E.~Liang, R.~Liaw, R.~Nishihara, P.~Moritz, R.~Fox, K.~Goldberg, J.~Gonzalez,
  M.~Jordan, and I.~Stoica.
\newblock Rllib: Abstractions for distributed reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3053--3062, 2018.

\bibitem[McMahan et~al.(2003)McMahan, Gordon, and Blum]{double_oracle}
H.~B. McMahan, G.~J. Gordon, and A.~Blum.
\newblock Planning in the presence of cost functions controlled by an
  adversary.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pages 536--543, 2003.

\bibitem[Moritz et~al.(2018)Moritz, Nishihara, Wang, Tumanov, Liaw, Liang,
  Elibol, Yang, Paul, Jordan, et~al.]{ray}
P.~Moritz, R.~Nishihara, S.~Wang, A.~Tumanov, R.~Liaw, E.~Liang, M.~Elibol,
  Z.~Yang, W.~Paul, M.~I. Jordan, et~al.
\newblock Ray: A distributed framework for emerging $\{$AI$\}$ applications.
\newblock In \emph{13th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 18)}, pages 561--577, 2018.

\bibitem[Muller et~al.(2020)Muller, Omidshafiei, Rowland, Tuyls, Perolat, Liu,
  Hennes, Marris, Lanctot, Hughes, et~al.]{muller2019generalized}
P.~Muller, S.~Omidshafiei, M.~Rowland, K.~Tuyls, J.~Perolat, S.~Liu, D.~Hennes,
  L.~Marris, M.~Lanctot, E.~Hughes, et~al.
\newblock A generalized training approach for multiagent learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Nisan et~al.(2007)Nisan, Roughgarden, Tardos, and Vazirani]{agt}
N.~Nisan, T.~Roughgarden, E.~Tardos, and V.~V. Vazirani.
\newblock \emph{Algorithmic Game Theory}.
\newblock Cambridge University Press, 2007.

\bibitem[Schadd and Winands(2009)]{stratego_bot}
M.~Schadd and M.~Winands.
\newblock Quiescence search for stratego.
\newblock In \emph{Proceedings of the 21st Benelux Conference on Artificial
  Intelligence. Eindhoven, the Netherlands}, 2009.

\bibitem[Shoham and Leyton-Brown(2008)]{shoham2008multiagent}
Y.~Shoham and K.~Leyton-Brown.
\newblock \emph{Multiagent systems: Algorithmic, game-theoretic, and logical
  foundations}.
\newblock Cambridge University Press, 2008.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, Driessche,
  Graepel, and Hassabis]{silver_2017}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~Lillicrap, F.~Hui,
  L.~Sifre, G.~v.~d. Driessche, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 550\penalty0 (7676):\penalty0 354--359, 10 2017.
\newblock ISSN 0028-0836.
\newblock \doi{10.1038/nature24270}.
\newblock URL \url{http:https://doi.org/10.1038/nature24270}.

\bibitem[Srinivasan et~al.(2018)Srinivasan, Lanctot, Zambaldi, P{\'e}rolat,
  Tuyls, Munos, and Bowling]{qpg}
S.~Srinivasan, M.~Lanctot, V.~Zambaldi, J.~P{\'e}rolat, K.~Tuyls, R.~Munos, and
  M.~Bowling.
\newblock Actor-critic policy optimization in partially observable multiagent
  environments.
\newblock In \emph{Advances in neural information processing systems}, pages
  3422--3435, 2018.

\bibitem[Steinberger et~al.(2020)Steinberger, Lerer, and
  Brown]{steinberger2020dream}
E.~Steinberger, A.~Lerer, and N.~Brown.
\newblock Dream: Deep regret minimization with advantage baselines and
  model-free learning.
\newblock \emph{arXiv preprint arXiv:2006.10410}, 2020.

\bibitem[Taylor and Jonker(1978)]{replicator}
P.~D. Taylor and L.~B. Jonker.
\newblock Evolutionary stable strategies and game dynamics.
\newblock \emph{Mathematical biosciences}, 40\penalty0 (1-2):\penalty0
  145--156, 1978.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{alphastar}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Zinkevich et~al.(2008)Zinkevich, Johanson, Bowling, and Piccione]{cfr}
M.~Zinkevich, M.~Johanson, M.~Bowling, and C.~Piccione.
\newblock Regret minimization in games with incomplete information.
\newblock In \emph{Advances in neural information processing systems}, pages
  1729--1736, 2008.

\end{thebibliography}
