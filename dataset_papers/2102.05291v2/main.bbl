\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Amid et~al.(2019{\natexlab{a}})Amid, Warmuth, Anil, and
  Koren]{amid2019robust}
Amid, E., Warmuth, M.~K., Anil, R., and Koren, T.
\newblock Robust bi-tempered logistic loss based on bregman divergences.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  14987--14996, 2019{\natexlab{a}}.

\bibitem[Amid et~al.(2019{\natexlab{b}})Amid, Warmuth, and
  Srinivasan]{amid2019two}
Amid, E., Warmuth, M.~K., and Srinivasan, S.
\newblock Two-temperature logistic regression based on the tsallis divergence.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pp.\  2388--2396. PMLR, 2019{\natexlab{b}}.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and
  Vincent]{bengio2013representation}
Bengio, Y., Courville, A., and Vincent, P.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 35\penalty0 (8):\penalty0 1798--1828, 2013.

\bibitem[Berthon et~al.(2021)Berthon, Han, Niu, Liu, and
  Sugiyama]{berthon2020confidence}
Berthon, A., Han, B., Niu, G., Liu, T., and Sugiyama, M.
\newblock Confidence scores make instance-dependent label-noise learning
  possible.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, ICML, 2021.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Cheng et~al.(2020)Cheng, Zhu, Li, Gong, Sun, and Liu]{sieve2020}
Cheng, H., Zhu, Z., Li, X., Gong, Y., Sun, X., and Liu, Y.
\newblock Learning with instance-dependent label noise: A sample sieve
  approach, 2020.

\bibitem[Feldman(2020)]{feldman2020does}
Feldman, V.
\newblock Does learning require memorization? a short tale about a long tail.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  954--959, 2020.

\bibitem[Gao et~al.(2016)Gao, Yang, and Zhou]{gao2016resistance}
Gao, W., Yang, B.-B., and Zhou, Z.-H.
\newblock On the resistance of nearest neighbor to random noisy labels.
\newblock \emph{arXiv preprint arXiv:1607.07526}, 2016.

\bibitem[Ghosh et~al.(2017)Ghosh, Kumar, and Sastry]{ghosh2017robust}
Ghosh, A., Kumar, H., and Sastry, P.
\newblock Robust loss functions under label noise for deep neural networks.
\newblock In \emph{Thirty-First AAAI Conference on Artificial Intelligence},
  2017.

\bibitem[Gong et~al.(2018)Gong, Li, Meng, Miao, and Liu]{gong2018decomposition}
Gong, M., Li, H., Meng, D., Miao, Q., and Liu, J.
\newblock Decomposition-based evolutionary multiobjective optimization to
  self-paced learning.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 23\penalty0
  (2):\penalty0 288--302, 2018.

\bibitem[Han et~al.(2018)Han, Yao, Yu, Niu, Xu, Hu, Tsang, and
  Sugiyama]{han2018co}
Han, B., Yao, Q., Yu, X., Niu, G., Xu, M., Hu, W., Tsang, I., and Sugiyama, M.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8527--8537, 2018.

\bibitem[Han et~al.(2020)Han, Yao, Liu, Niu, Tsang, Kwok, and
  Sugiyama]{han2020survey}
Han, B., Yao, Q., Liu, T., Niu, G., Tsang, I.~W., Kwok, J.~T., and Sugiyama, M.
\newblock A survey of label-noise representation learning: Past, present and
  future.
\newblock \emph{arXiv preprint arXiv:2011.04406}, 2020.

\bibitem[Han et~al.(2019)Han, Luo, and Wang]{han2019deep}
Han, J., Luo, P., and Wang, X.
\newblock Deep self-learning from noisy labels.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  5138--5147, 2019.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hoeffding(1963)]{hoeffding1963probability}
Hoeffding, W.
\newblock Probability inequalities for sums of bounded random variables.
\newblock \emph{Journal of the American Statistical Association}, 58\penalty0
  (301):\penalty0 13--30, 1963.
\newblock ISSN 01621459.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{horn2012matrix}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Ji et~al.(2019)Ji, Henriques, and Vedaldi]{ji2019invariant}
Ji, X., Henriques, J.~F., and Vedaldi, A.
\newblock Invariant information clustering for unsupervised image
  classification and segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9865--9874, 2019.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2017mentornet}
Jiang, L., Zhou, Z., Leung, T., Li, L.-J., and Fei-Fei, L.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2304--2313. PMLR, 2018.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Zhai, and
  Beyer]{kolesnikov2019revisiting}
Kolesnikov, A., Zhai, X., and Beyer, L.
\newblock Revisiting self-supervised visual representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  1920--1929, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[Li et~al.(2020)Li, Zhang, Xu, Dickerson, and Ba]{li2020noisy}
Li, J., Zhang, M., Xu, K., Dickerson, J.~P., and Ba, J.
\newblock Noisy labels can induce good representations.
\newblock \emph{arXiv preprint arXiv:2012.12896}, 2020.

\bibitem[Li et~al.(2021)Li, Liu, Han, Niu, and Sugiyama]{li2021provably}
Li, X., Liu, T., Han, B., Niu, G., and Sugiyama, M.
\newblock Provably end-to-end label-noise learning without anchor points.
\newblock \emph{arXiv preprint arXiv:2102.02400}, 2021.

\bibitem[Liu et~al.(2012)Liu, Peng, and Ihler]{liu2012variational}
Liu, Q., Peng, J., and Ihler, A.
\newblock Variational inference for crowdsourcing.
\newblock In \emph{Proceedings of the 25th International Conference on Neural
  Information Processing Systems-Volume 1}, pp.\  692--700, 2012.

\bibitem[Liu \& Tao(2015)Liu and Tao]{liu2015classification}
Liu, T. and Tao, D.
\newblock Classification with noisy labels by importance reweighting.
\newblock \emph{IEEE Transactions on pattern analysis and machine
  intelligence}, 38\penalty0 (3):\penalty0 447--461, 2015.

\bibitem[Liu(2021)]{liu2021importance}
Liu, Y.
\newblock The importance of understanding instance-level noisy labels.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, ICML '21, 2021.

\bibitem[Liu \& Chen(2017)Liu and Chen]{liu2017machine}
Liu, Y. and Chen, Y.
\newblock Machine-learning aided peer prediction.
\newblock In \emph{Proceedings of the 2017 ACM Conference on Economics and
  Computation}, pp.\  63--80, 2017.

\bibitem[Liu \& Guo(2020)Liu and Guo]{liu2019peer}
Liu, Y. and Guo, H.
\newblock Peer loss functions: Learning from noisy labels without knowing noise
  rates.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML '20, 2020.

\bibitem[Liu \& Liu(2015)Liu and Liu]{liu2015online}
Liu, Y. and Liu, M.
\newblock An online learning approach to improving the quality of
  crowd-sourcing.
\newblock \emph{ACM SIGMETRICS Performance Evaluation Review}, 43\penalty0
  (1):\penalty0 217--230, 2015.

\bibitem[Liu et~al.(2020)Liu, Wang, and Chen]{liu2020surrogate}
Liu, Y., Wang, J., and Chen, Y.
\newblock Surrogate scoring rules.
\newblock In \emph{Proceedings of the 21st ACM Conference on Economics and
  Computation}, pp.\  853--871, 2020.

\bibitem[Lukasik et~al.(2020)Lukasik, Bhojanapalli, Menon, and
  Kumar]{lukasik2020does}
Lukasik, M., Bhojanapalli, S., Menon, A., and Kumar, S.
\newblock Does label smoothing mitigate label noise?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6448--6458. PMLR, 2020.

\bibitem[Natarajan et~al.(2013)Natarajan, Dhillon, Ravikumar, and
  Tewari]{natarajan2013learning}
Natarajan, N., Dhillon, I.~S., Ravikumar, P.~K., and Tewari, A.
\newblock Learning with noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1196--1204, 2013.

\bibitem[Northcutt et~al.(2021)Northcutt, Jiang, and
  Chuang]{northcutt2021confident}
Northcutt, C., Jiang, L., and Chuang, I.
\newblock Confident learning: Estimating uncertainty in dataset labels.
\newblock \emph{Journal of Artificial Intelligence Research}, 70:\penalty0
  1373--1411, 2021.

\bibitem[Northcutt et~al.(2017)Northcutt, Wu, and
  Chuang]{northcutt2017learning}
Northcutt, C.~G., Wu, T., and Chuang, I.~L.
\newblock Learning with confident examples: Rank pruning for robust
  classification with noisy labels.
\newblock \emph{UAI}, 2017.

\bibitem[Patrini et~al.(2017)Patrini, Rozza, Krishna~Menon, Nock, and
  Qu]{patrini2017making}
Patrini, G., Rozza, A., Krishna~Menon, A., Nock, R., and Qu, L.
\newblock Making deep neural networks robust to label noise: A loss correction
  approach.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1944--1952, 2017.

\bibitem[Scott(2015)]{scott2015rate}
Scott, C.
\newblock A rate of convergence for mixture proportion estimation, with
  application to learning from noisy labels.
\newblock In \emph{AISTATS}, 2015.

\bibitem[Shu et~al.(2020)Shu, Zhao, Chen, Xu, and Meng]{shu2020learning}
Shu, J., Zhao, Q., Chen, K., Xu, Z., and Meng, D.
\newblock Learning adaptive loss for robust learning with noisy labels.
\newblock \emph{arXiv preprint arXiv:2002.06482}, 2020.

\bibitem[Van~Rooyen \& Williamson(2017)Van~Rooyen and
  Williamson]{van2017theory}
Van~Rooyen, B. and Williamson, R.~C.
\newblock A theory of learning with corrupted labels.
\newblock \emph{J. Mach. Learn. Res.}, 18\penalty0 (1):\penalty0 8501--8550,
  2017.

\bibitem[Wang et~al.(2021)Wang, Liu, and Levy]{wang2020fair}
Wang, J., Liu, Y., and Levy, C.
\newblock Fair classification with group-dependent label noise.
\newblock FAccT, pp.\  526â€“536, New York, NY, USA, 2021.

\bibitem[Wang et~al.(2019)Wang, Ma, Chen, Luo, Yi, and
  Bailey]{wang2019symmetric}
Wang, Y., Ma, X., Chen, Z., Luo, Y., Yi, J., and Bailey, J.
\newblock Symmetric cross entropy for robust learning with noisy labels.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  322--330, 2019.

\bibitem[Wei et~al.(2020)Wei, Feng, Chen, and An]{wei2020combating}
Wei, H., Feng, L., Chen, X., and An, B.
\newblock Combating noisy labels by agreement: A joint training method with
  co-regularization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13726--13735, 2020.

\bibitem[Wei \& Liu(2021)Wei and Liu]{wei2021when}
Wei, J. and Liu, Y.
\newblock When optimizing f-divergence is robust with label noise.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Wei et~al.(2021)Wei, Liu, Liu, Niu, and Liu]{wei2021understanding}
Wei, J., Liu, H., Liu, T., Niu, G., and Liu, Y.
\newblock Understanding (generalized) label smoothing when learning with noisy
  labels.
\newblock 2021.

\bibitem[Xia et~al.(2019)Xia, Liu, Wang, Han, Gong, Niu, and
  Sugiyama]{xia2019anchor}
Xia, X., Liu, T., Wang, N., Han, B., Gong, C., Niu, G., and Sugiyama, M.
\newblock Are anchor points really indispensable in label-noise learning?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6838--6849, 2019.

\bibitem[Xia et~al.(2020{\natexlab{a}})Xia, Liu, Han, Wang, Deng, Li, and
  Mao]{xia2020extended}
Xia, X., Liu, T., Han, B., Wang, N., Deng, J., Li, J., and Mao, Y.
\newblock Extended {T}: Learning with mixed closed-set and open-set noisy
  labels.
\newblock \emph{arXiv preprint arXiv:2012.00932}, 2020{\natexlab{a}}.

\bibitem[Xia et~al.(2020{\natexlab{b}})Xia, Liu, Han, Wang, Gong, Liu, Niu,
  Tao, and Sugiyama]{xia2020parts}
Xia, X., Liu, T., Han, B., Wang, N., Gong, M., Liu, H., Niu, G., Tao, D., and
  Sugiyama, M.
\newblock Part-dependent label noise: Towards instance-dependent label noise.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7597--7610, 2020{\natexlab{b}}.

\bibitem[Xia et~al.(2021)Xia, Liu, Han, Gong, Wang, Ge, and
  Chang]{xia2021robust}
Xia, X., Liu, T., Han, B., Gong, C., Wang, N., Ge, Z., and Chang, Y.
\newblock Robust early-learning: Hindering the memorization of noisy labels.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Xiao et~al.(2015)Xiao, Xia, Yang, Huang, and Wang]{xiao2015learning}
Xiao, T., Xia, T., Yang, Y., Huang, C., and Wang, X.
\newblock Learning from massive noisy labeled data for image classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  2691--2699, 2015.

\bibitem[Xu et~al.(2019)Xu, Cao, Kong, and Wang]{xu2019l_dmi}
Xu, Y., Cao, P., Kong, Y., and Wang, Y.
\newblock L\_dmi: A novel information-theoretic loss function for training deep
  nets robust to label noise.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Yao et~al.(2020{\natexlab{a}})Yao, Yang, Han, Niu, and
  Kwok]{yao2020searching}
Yao, Q., Yang, H., Han, B., Niu, G., and Kwok, J.~T.
\newblock Searching to exploit memorization effect in learning with noisy
  labels.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, ICML '20, 2020{\natexlab{a}}.

\bibitem[Yao et~al.(2020{\natexlab{b}})Yao, Liu, Han, Gong, Deng, Niu, and
  Sugiyama]{yao2020dual}
Yao, Y., Liu, T., Han, B., Gong, M., Deng, J., Niu, G., and Sugiyama, M.
\newblock Dual t: Reducing estimation error for transition matrix in
  label-noise learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7260--7271, 2020{\natexlab{b}}.

\bibitem[Yu et~al.(2019)Yu, Han, Yao, Niu, Tsang, and Sugiyama]{yu2019does}
Yu, X., Han, B., Yao, J., Niu, G., Tsang, I., and Sugiyama, M.
\newblock How does disagreement help generalization against label corruption?
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pp.\  7164--7173. PMLR, 09--15 Jun 2019.

\bibitem[Zhang \& Sabuncu(2018)Zhang and Sabuncu]{zhang2018generalized}
Zhang, Z. and Sabuncu, M.
\newblock Generalized cross entropy loss for training deep neural networks with
  noisy labels.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  8778--8788, 2018.

\bibitem[Zhu et~al.(2021)Zhu, Liu, and Liu]{zhu2020second}
Zhu, Z., Liu, T., and Liu, Y.
\newblock A second-order approach to learning with instance-dependent label
  noise.
\newblock In \emph{The IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, June 2021.

\end{thebibliography}
