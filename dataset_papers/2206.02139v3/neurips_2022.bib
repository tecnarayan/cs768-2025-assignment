@article{ma2022multiscale,
  title={The Multiscale Structure of Neural Network Loss Functions: The Effect on Optimization and Origin},
  author={Ma, Chao and Wu, Lei and Ying, Lexing},
  journal={arXiv preprint arXiv:2204.11326},
  year={2022}
}
@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  journal={},
  year={2009},
  publisher={Citeseer}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}
@article{lakshminarayanan2020deep,
  title={Deep Gated Networks: A framework to understand training and generalisation in deep learning},
  author={Lakshminarayanan, Chandrashekar and Singh, Amit Vikram},
  journal={arXiv preprint arXiv:2002.03996},
  year={2020}
}
@article{wright1999numerical,
  title={Numerical optimization},
  author={Wright, Stephen and Nocedal, Jorge and others},
  journal={Springer Science},
  volume={35},
  number={67-68},
  pages={7},
  year={1999}
}
@article{du2018algorithmic,
  title={Algorithmic regularization in learning deep homogeneous models: Layers are automatically balanced},
  author={Du, Simon S and Hu, Wei and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@inproceedings{phuong2020inductive,
  title={The inductive bias of ReLU networks on orthogonally separable data},
  author={Phuong, Mary and Lampert, Christoph H},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{mei2019mean,
  title={Mean-field theory of two-layers neural networks: dimension-free bounds and kernel limit},
  author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  booktitle={Conference on Learning Theory},
  pages={2388--2464},
  year={2019},
  organization={PMLR}
}
@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}
@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{ahn2022understanding,
  title={Understanding the unstable convergence of gradient descent},
  author={Ahn, Kwangjun and Zhang, Jingzhao and Sra, Suvrit},
  journal={arXiv preprint arXiv:2204.01050},
  year={2022}
}
@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}
@inproceedings{tian2017analytical,
  title={An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis},
  author={Tian, Yuandong},
  booktitle={International conference on machine learning},
  pages={3404--3413},
  year={2017},
  organization={PMLR}
}
@article{cho2009kernel,
  title={Kernel methods for deep learning},
  author={Cho, Youngmin and Saul, Lawrence},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}
@inproceedings{zhong2017recovery,
  title={Recovery guarantees for one-hidden-layer neural networks},
  author={Zhong, Kai and Song, Zhao and Jain, Prateek and Bartlett, Peter L and Dhillon, Inderjit S},
  booktitle={International conference on machine learning},
  pages={4140--4149},
  year={2017},
  organization={PMLR}
}
@article{li2017convergence,
  title={Convergence analysis of two-layer neural networks with relu activation},
  author={Li, Yuanzhi and Yuan, Yang},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{ding2019sub,
  title={Sub-optimal local minima exist for neural networks with almost all non-linear activations},
  author={Ding, Tian and Li, Dawei and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1911.01413},
  year={2019}
}
@article{sun2020global,
  title={The global landscape of neural networks: An overview},
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={95--108},
  year={2020},
  publisher={IEEE}
}
@article{weinan2019analysis,
  title={Analysis of the gradient descent algorithm for a deep neural network model with skip-connections},
  author={E, Weinan and Ma, Chao and Wang, Qingcan and Wu, Lei},
  journal={arXiv preprint arXiv:1904.05263},
  year={2019}
}
@inproceedings{yehudai2020learning,
  title={Learning a single neuron with gradient methods},
  author={Yehudai, Gilad and Ohad, Shamir},
  booktitle={Conference on Learning Theory},
  pages={3756--3786},
  year={2020},
  organization={PMLR}
}
@article{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{wu2022learning,
  title={Learning a Single Neuron for Non-monotonic Activation Functions},
  author={Wu, Lei},
  journal={arXiv preprint arXiv:2202.08064},
  year={2022}
}
@article{cheridito2022proof,
  title={A proof of convergence for gradient descent in the training of artificial neural networks for constant target functions},
  author={Cheridito, Patrick and Jentzen, Arnulf and Riekert, Adrian and Rossmannek, Florian},
  journal={Journal of Complexity},
  pages={101646},
  year={2022},
  publisher={Elsevier}
}
@article{jentzen2021proof,
  title={A proof of convergence for the gradient descent optimization method with random initializations in the training of neural networks with ReLU activation for piecewise linear target functions},
  author={Jentzen, Arnulf and Riekert, Adrian},
  journal={arXiv preprint arXiv:2108.04620},
  year={2021}
}
@inproceedings{li2020learning,
  title={Learning over-parametrized two-layer neural networks beyond ntk},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Conference on learning theory},
  pages={2613--2682},
  year={2020},
  organization={PMLR}
}
@inproceedings{zhou2021local,
  title={A local convergence theory for mildly over-parameterized two-layer neural network},
  author={Zhou, Mo and Ge, Rong and Jin, Chi},
  booktitle={Conference on Learning Theory},
  pages={4577--4632},
  year={2021},
  organization={PMLR}
}
@article{chatterji2021doesA,
  title={When does gradient descent with logistic loss find interpolating two-layer networks?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={159},
  pages={1--48},
  year={2021}
}
@inproceedings{chatterji2021doesB,
  title={When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?},
  author={Chatterji, Niladri S and Long, Philip M and Bartlett, Peter},
  booktitle={Conference on Learning Theory},
  pages={927--1027},
  year={2021},
  organization={PMLR}
}
@article{lyu2021gradient,
  title={Gradient Descent on Two-layer Nets: Margin Maximization and Simplicity Bias},
  author={Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}
@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}
@inproceedings{safran2021effects,
  title={The effects of mild over-parameterization on the optimization landscape of shallow relu neural networks},
  author={Safran, Itay M and Yehudai, Gilad and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={3889--3934},
  year={2021},
  organization={PMLR}
}
@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}
@article{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}
@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}
@article{zou2018stochastic,
  title={Stochastic gradient descent optimizes over-parameterized deep relu networks},
  author={Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:1811.08888},
  year={2018}
}
@article{ma2020quenching,
  title={The quenching-activation behavior of the gradient descent dynamics for two-layer neural network models},
  author={Ma, Chao and Wu, Lei and E, Weinan},
  journal={arXiv preprint arXiv:2006.14450},
  year={2020}
}
@inproceedings{safran2018spurious,
  title={Spurious local minima are common in two-layer relu neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={International conference on machine learning},
  pages={4433--4441},
  year={2018},
  organization={PMLR}
}
@article{livni2014computational,
  title={On the computational efficiency of training neural networks},
  author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}
@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}
@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{zhang2017understanding,
  title={Understanding deep learning requires rethinking generalization (2016)},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2017}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}
@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}
@article{wu2018sgd,
  title={How sgd selects the global minima in over-parameterized learning: A dynamical stability perspective},
  author={Wu, Lei and Ma, Chao and E, Weinan},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={8279--8288},
  year={2018}
}
@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}
@article{bassily2020stability,
  title={Stability of stochastic gradient descent on nonsmooth convex losses},
  author={Bassily, Raef and Feldman, Vitaly and Guzm{\'a}n, Crist{\'o}bal and Talwar, Kunal},
  journal={arXiv preprint arXiv:2006.06914},
  year={2020}
}
@inproceedings{bousquet2020sharper,
  title={Sharper bounds for uniformly stable algorithms},
  author={Bousquet, Olivier and Klochkov, Yegor and Zhivotovskiy, Nikita},
  booktitle={Conference on Learning Theory},
  pages={610--626},
  year={2020},
  organization={PMLR}
}
@article{rogers1978finite,
  title={A finite sample distribution-free performance bound for local discrimination rules},
  author={Rogers, William H and Wagner, Terry J},
  journal={The Annals of Statistics},
  pages={506--514},
  year={1978},
  publisher={JSTOR}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}
@article{bartlett2017spectrally,
  title={Spectrally-normalized margin bounds for neural networks},
  author={Bartlett, Peter and Foster, Dylan J and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1706.08498},
  year={2017}
}
@article{ma2020towards,
  title={Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don't},
  author={Ma, Chao and Wojtowytsch, Stephan and Wu, Lei and E, Weinan},
  journal={arXiv preprint arXiv:2009.10713},
  year={2020}
}
@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}
@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}
@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1474--1520},
  year={2019},
  publisher={JMLR. org}
}
@article{lin2019generalization,
  title={Generalization bounds for convolutional neural networks},
  author={Lin, Shan and Zhang, Jingwei},
  journal={arXiv preprint arXiv:1910.01487},
  year={2019}
}
@article{bottou1991stochastic,
  title={Stochastic gradient learning in neural networks},
  author={Bottou, L{\'e}on and others},
  journal={Proceedings of Neuro-N{\i}mes},
  volume={91},
  number={8},
  pages={12},
  year={1991},
  publisher={Nimes}
}
@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}
@article{wu2019global,
  title={Global convergence of gradient descent for deep linear residual networks},
  author={Wu, Lei and Wang, Qingcan and Ma, Chao},
  journal={arXiv preprint arXiv:1911.00645},
  year={2019}
}
@article{asadi2018chaining,
  title={Chaining mutual information and tightening generalization bounds},
  author={Asadi, Amir R and Abbe, Emmanuel and Verd{\'u}, Sergio},
  journal={arXiv preprint arXiv:1806.03803},
  year={2018}
}
@inproceedings{arora2018stronger,
  title={Stronger generalization bounds for deep nets via a compression approach},
  author={Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  booktitle={International Conference on Machine Learning},
  pages={254--263},
  year={2018},
  organization={PMLR}
}
@inproceedings{raginsky2017non,
  title={Non-convex learning via stochastic gradient langevin dynamics: a nonasymptotic analysis},
  author={Raginsky, Maxim and Rakhlin, Alexander and Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1674--1703},
  year={2017},
  organization={PMLR}
}
@inproceedings{welling2011bayesian,
  title={Bayesian learning via stochastic gradient Langevin dynamics},
  author={Welling, Max and Teh, Yee W},
  booktitle={Proceedings of the 28th international conference on machine learning (ICML-11)},
  pages={681--688},
  year={2011},
  organization={Citeseer}
}
@inproceedings{zhang2017hitting,
  title={A hitting time analysis of stochastic gradient langevin dynamics},
  author={Zhang, Yuchen and Liang, Percy and Charikar, Moses},
  booktitle={Conference on Learning Theory},
  pages={1980--2022},
  year={2017},
  organization={PMLR}
}
@article{chen2018stability,
  title={Stability and convergence trade-off of iterative optimization algorithms},
  author={Chen, Yuansi and Jin, Chi and Yu, Bin},
  journal={arXiv preprint arXiv:1804.01619},
  year={2018}
}
@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}
@article{neyshabur2017exploring,
  title={Exploring generalization in deep learning},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  journal={arXiv preprint arXiv:1706.08947},
  year={2017}
}
@article{kraskov2004estimating,
  title={Estimating mutual information},
  author={Kraskov, Alexander and St{\"o}gbauer, Harald and Grassberger, Peter},
  journal={Physical review E},
  volume={69},
  number={6},
  pages={066138},
  year={2004},
  publisher={APS}
}
@inproceedings{steinke2020reasoning,
  title={Reasoning about generalization via conditional mutual information},
  author={Steinke, Thomas and Zakynthinou, Lydia},
  booktitle={Conference on Learning Theory},
  pages={3437--3452},
  year={2020},
  organization={PMLR}
}
@article{haghifam2020sharpened,
  title={Sharpened generalization bounds based on conditional mutual information and an application to noisy, iterative algorithms},
  author={Haghifam, Mahdi and Negrea, Jeffrey and Khisti, Ashish and Roy, Daniel M and Dziugaite, Gintare Karolina},
  journal={arXiv preprint arXiv:2004.12983},
  year={2020}
}
@article{xu2017information,
  title={Information-theoretic analysis of generalization capability of learning algorithms},
  author={Xu, Aolin and Raginsky, Maxim},
  journal={arXiv preprint arXiv:1705.07809},
  year={2017}
}
@article{bu2020tightening,
  title={Tightening Mutual Information-Based Bounds on Generalization Error},
  author={Bu, Yuheng and Zou, Shaofeng and Veeravalli, Venugopal V},
  journal={IEEE Journal on Selected Areas in Information Theory},
  volume={1},
  number={1},
  pages={121--130},
  year={2020},
  publisher={IEEE}
}
@inproceedings{tu2019understanding,
  title={Understanding generalization in recurrent neural networks},
  author={Tu, Zhuozhuo and He, Fengxiang and Tao, Dacheng},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}@inproceedings{hardt2016train,
  title={Train faster, generalize better: Stability of stochastic gradient descent},
  author={Hardt, Moritz and Recht, Ben and Singer, Yoram},
  booktitle={International Conference on Machine Learning},
  pages={1225--1234},
  year={2016},
  organization={PMLR}
}
@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}
@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  year={2019}
}
@article{weinan2020comparative,
  title={A comparative analysis of optimization and generalization properties of two-layer neural network and random feature models under gradient descent dynamics},
  author={E, Weinan and Ma, Chao and Wu, Lei},
  journal={Science China Mathematics},
  volume={63},
  number={7},
  pages={1235},
  year={2020},
  publisher={Science China Press}
}
@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{shwartz2017opening,
  title={Opening the black box of deep neural networks via information},
  author={Shwartz-Ziv, Ravid and Tishby, Naftali},
  journal={arXiv preprint arXiv:1703.00810},
  year={2017}
}
@inproceedings{liang2019fisher,
  title={Fisher-rao metric, geometry, and complexity of neural networks},
  author={Liang, Tengyuan and Poggio, Tomaso and Rakhlin, Alexander and Stokes, James},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={888--896},
  year={2019},
  organization={PMLR}
}

@article{weinan2021barron,
  title={The Barron Space and the Flow-Induced Function Spaces for Neural Network Models},
  author={Weinan, E and Ma, Chao and Wu, Lei},
  journal={Constructive Approximation},
  pages={1--38},
  year={2021},
  publisher={Springer}
}
@article{zhou2020towards,
  title={Towards theoretically understanding why sgd generalizes better than adam in deep learning},
  author={Zhou, Pan and Feng, Jiashi and Ma, Chao and Xiong, Caiming and Hoi, Steven and E, Weinan},
  journal={arXiv preprint arXiv:2010.05627},
  year={2020}
}
@article{li2020complexity,
  title={Complexity measures for neural networks with general activation functions using path-based norms},
  author={Li, Zhong and Ma, Chao and Wu, Lei},
  journal={arXiv preprint arXiv:2009.06132},
  year={2020}
}
@article{ma2019priori,
  title={A priori estimates of the population risk for residual networks},
  author={Ma, Chao and Wang, Qingcan and E, Weinan},
  journal={arXiv preprint arXiv:1903.02154},
  year={2019}
}
@article{ma2018priori,
  title={A priori estimates of the population risk for two-layer neural networks},
  author={Ma, Chao and Wu, Lei and E, Weinan},
  journal={arXiv preprint arXiv:1810.06397},
  year={2018}
}
@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={Journal of Machine Learning Research},
  volume={21},
  year={2020},
  publisher={MICROTOME PUBL}
}
@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}
@article{ma2021sobolev,
  title={The Sobolev Regularization Effect of Stochastic Gradient Descent},
  author={Ma, Chao and Ying, Lexing},
  journal={arXiv preprint arXiv:2105.13462},
  year={2021}
}
@article{mustafa2021fine,
  title={Fine-grained Generalization Analysis of Structured Output Prediction},
  author={Mustafa, Waleed and Lei, Yunwen and Ledent, Antoine and Kloft, Marius},
  journal={arXiv preprint arXiv:2106.00115},
  year={2021}
}
@article{li2018tighter,
  title={On tighter generalization bound for deep neural networks: Cnns, resnets, and beyond},
  author={Li, Xingguo and Lu, Junwei and Wang, Zhaoran and Haupt, Jarvis and Zhao, Tuo},
  journal={arXiv preprint arXiv:1806.05159},
  year={2018}
}
@article{ledent2019norm,
  title={Norm-based generalisation bounds for multi-class convolutional neural networks},
  author={Ledent, Antoine and Mustafa, Waleed and Lei, Yunwen and Kloft, Marius},
  journal={arXiv preprint arXiv:1905.12430},
  year={2019},
  publisher={Technical report}
}
@inproceedings{zhou2018understanding,
  title={Understanding generalization and optimization performance of deep CNNs},
  author={Zhou, Pan and Feng, Jiashi},
  booktitle={International Conference on Machine Learning},
  pages={5960--5969},
  year={2018},
  organization={PMLR}
}
@article{long2019generalization,
  title={Generalization bounds for deep convolutional neural networks},
  author={Long, Philip M and Sedghi, Hanie},
  journal={arXiv preprint arXiv:1905.12600},
  year={2019}
}
@inproceedings{golowich2018size,
  title={Size-independent sample complexity of neural networks},
  author={Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle={Conference On Learning Theory},
  pages={297--299},
  year={2018},
  organization={PMLR}
}
@inproceedings{mou2018generalization,
  title={Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints},
  author={Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  booktitle={Conference on Learning Theory},
  pages={605--638},
  year={2018},
  organization={PMLR}
}
@article{asadi2020chaining,
  title={Chaining Meets Chain Rule: Multilevel Entropic Regularization and Training of Neural Networks.},
  author={Asadi, Amir R and Abbe, Emmanuel},
  journal={J. Mach. Learn. Res.},
  volume={21},
  pages={139--1},
  year={2020}
}
@inproceedings{pensia2018generalization,
  title={Generalization error bounds for noisy, iterative algorithms},
  author={Pensia, Ankit and Jog, Varun and Loh, Po-Ling},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)},
  pages={546--550},
  year={2018},
  organization={IEEE}
}
@inproceedings{fang2019sharp,
  title={Sharp analysis for nonconvex sgd escaping from saddle points},
  author={Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle={Conference on Learning Theory},
  pages={1192--1234},
  year={2019},
  organization={PMLR}
}
@article{zhou2018convergence,
  title={On the convergence of adaptive gradient methods for nonconvex optimization},
  author={Zhou, Dongruo and Chen, Jinghui and Cao, Yuan and Tang, Yiqi and Yang, Ziyan and Gu, Quanquan},
  journal={arXiv preprint arXiv:1808.05671},
  year={2018}
}
@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{reddi2019convergence,
  title={On the convergence of adam and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}
@inproceedings{ge2015escaping,
  title={Escaping from saddle points—online stochastic gradient for tensor decomposition},
  author={Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  booktitle={Conference on learning theory},
  pages={797--842},
  year={2015},
  organization={PMLR}
}
@inproceedings{kleinberg2018alternative,
  title={An alternative view: When does SGD escape local minima?},
  author={Kleinberg, Bobby and Li, Yuanzhi and Yuan, Yang},
  booktitle={International Conference on Machine Learning},
  pages={2698--2707},
  year={2018},
  organization={PMLR}
}
@article{ma2020qualitative,
  title={A qualitative study of the dynamic behavior of adaptive gradient algorithms},
  author={Ma, Chao and Wu, Lei and Weinan, E },
  journal={arXiv preprint arXiv:2009.06125},
  year={2020}
}
@article{allen2017katyusha,
  title={Katyusha: The first direct acceleration of stochastic gradient methods},
  author={Allen-Zhu, Zeyuan},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={8194--8244},
  year={2017},
  publisher={JMLR. org}
}
@article{li2020accelerated,
author = {Li, Huan and Fang, Cong and Lin, Zhouchen},
year = {2020},
month = {07},
pages = {1-16},
title = {Accelerated First-Order Optimization Algorithms for Machine Learning},
volume = {PP},
journal = {Proceedings of the IEEE},
doi = {10.1109/JPROC.2020.3007634}
}
@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}
@article{cai2019gram,
  title={Gram-Gauss-Newton Method: Learning Overparameterized Neural Networks for Regression Problems},
  author={Cai, Tianle and Gao, Ruiqi and Hou, Jikai and Chen, Siyu and Wang, Dong and He, Di and Zhang, Zhihua and Wang, Liwei},
  journal={arXiv preprint arXiv:1905.11675},
  year={2019}
}
@article{zhang2019fast,
  title={Fast convergence of natural gradient descent for overparameterized neural networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}
@article{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  journal={arXiv preprint arXiv:1807.01695},
  year={2018}
}
@article{johnson2013accelerating,
  title={Accelerating stochastic gradient descent using predictive variance reduction},
  author={Johnson, Rie and Zhang, Tong},
  journal={Advances in neural information processing systems},
  volume={26},
  pages={315--323},
  year={2013},
  publisher={Citeseer}
}
@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}
@article{barakat2019convergence,
  title={Convergence analysis of a momentum algorithm with adaptive step size for non convex optimization},
  author={Barakat, Anas and Bianchi, Pascal},
  journal={arXiv preprint arXiv:1911.07596},
  year={2019}
}
@inproceedings{staib2019escaping,
  title={Escaping saddle points with adaptive gradient methods},
  author={Staib, Matthew and Reddi, Sashank and Kale, Satyen and Kumar, Sanjiv and Sra, Suvrit},
  booktitle={International Conference on Machine Learning},
  pages={5956--5965},
  year={2019},
  organization={PMLR}
}
@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
