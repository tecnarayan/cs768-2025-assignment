\begin{thebibliography}{68}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(1996)Anderson, Kevrekidis, and
  Rico-Martinez]{anderson1996comparison}
Anderson, J., Kevrekidis, I., and Rico-Martinez, R.
\newblock A comparison of recurrent training algorithms for time series
  analysis and system identification.
\newblock \emph{Computers \& Chemical Engineering}, 20:\penalty0 S751--S756,
  1996.

\bibitem[Ascher \& Petzold(1998)Ascher and Petzold]{ascher1998computer}
Ascher, U.~M. and Petzold, L.~R.
\newblock \emph{Computer methods for ordinary differential equations and
  differential-algebraic equations}, volume~61.
\newblock SIAM, 1998.

\bibitem[Bai et~al.(2018)Bai, Kolter, and Koltun]{bai2018empirical}
Bai, S., Kolter, J.~Z., and Koltun, V.
\newblock An empirical evaluation of generic convolutional and recurrent
  networks for sequence modeling.
\newblock \emph{arXiv preprint arXiv:1803.01271}, 2018.

\bibitem[Basodi et~al.(2020)Basodi, Ji, Zhang, and Pan]{basodi2020gradient}
Basodi, S., Ji, C., Zhang, H., and Pan, Y.
\newblock Gradient amplification: An efficient way to train deep neural
  networks.
\newblock \emph{Big Data Mining and Analytics}, 3\penalty0 (3):\penalty0
  196--207, 2020.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{basri2020frequency}
Basri, R., Galun, M., Geifman, A., Jacobs, D., Kasten, Y., and Kritchman, S.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  685--694. PMLR, 2020.

\bibitem[Botev et~al.(2021)Botev, Jaegle, Wirnsberger, Hennes, and
  Higgins]{botev2021priors}
Botev, A., Jaegle, A., Wirnsberger, P., Hennes, D., and Higgins, I.
\newblock Which priors matter? benchmarking models for learning latent
  dynamics.
\newblock In \emph{Thirty-fifth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track (Round 1)}, 2021.

\bibitem[Cawley \& Talbot(2010)Cawley and Talbot]{cawley2010over}
Cawley, G.~C. and Talbot, N.~L.
\newblock On over-fitting in model selection and subsequent selection bias in
  performance evaluation.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2079--2107, 2010.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neuralode}
Chen, R. T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.~K.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Chen et~al.(2021)Chen, Amos, and Nickel]{chen2021eventfn}
Chen, R. T.~Q., Amos, B., and Nickel, M.
\newblock Learning neural event functions for ordinary differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Clevert et~al.(2015)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Clevert, D.-A., Unterthiner, T., and Hochreiter, S.
\newblock {Fast and accurate deep network learning by exponential linear units
  (ELUs)}.
\newblock \emph{arXiv preprint arXiv:1511.07289}, 2015.

\bibitem[Cohen \& Grossberg(1983)Cohen and Grossberg]{cohen1983absolute}
Cohen, M.~A. and Grossberg, S.
\newblock Absolute stability of global pattern formation and parallel memory
  storage by competitive neural networks.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics},
  SMC-13\penalty0 (5):\penalty0 815--826, 1983.

\bibitem[Dormand \& Prince(1980)Dormand and Prince]{dormand1980family}
Dormand, J.~R. and Prince, P.~J.
\newblock {A family of embedded Runge-Kutta formulae}.
\newblock \emph{{Journal of Computational and Applied Mathematics}}, 6\penalty0
  (1):\penalty0 19--26, 1980.

\bibitem[Farahmand et~al.(2017)Farahmand, Pourazarm, and
  Nikovski]{farahmand2017random}
Farahmand, A.-m., Pourazarm, S., and Nikovski, D.
\newblock Random projection filter bank for time series data.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Finlay et~al.(2020)Finlay, Jacobsen, Nurbekyan, and
  Oberman]{finlay2020train}
Finlay, C., Jacobsen, J.-H., Nurbekyan, L., and Oberman, A.
\newblock {How to train your neural ODE: the world of Jacobian and kinetic
  regularization}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3154--3164. PMLR, 2020.

\bibitem[Gholami et~al.(2019)Gholami, Keutzer, and Biros]{gholami2019anode}
Gholami, A., Keutzer, K., and Biros, G.
\newblock {ANODE: Unconditionally accurate memory-efficient gradients for
  neural ODEs}.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2019.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  249--256. JMLR Workshop and Conference Proceedings, 2010.

\bibitem[Goel et~al.(2022)Goel, Gu, Donahue, and R{\'e}]{goel2022sashimi}
Goel, K., Gu, A., Donahue, C., and R{\'e}, C.
\newblock Itâ€™s raw! {A}udio generation with state-space models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7616--7633. PMLR, 2022.

\bibitem[Gonz{\'a}lez-Garc{\'\i}a et~al.(1998)Gonz{\'a}lez-Garc{\'\i}a,
  Rico-Mart{\`\i}nez, and Kevrekidis]{gonzalez1998identification}
Gonz{\'a}lez-Garc{\'\i}a, R., Rico-Mart{\`\i}nez, R., and Kevrekidis, I.~G.
\newblock Identification of distributed parameter systems: A neural net based
  approach.
\newblock \emph{Computers \& chemical engineering}, 22:\penalty0 S965--S968,
  1998.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{goodfellow2013empirical}
Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}]{gu2020hippo}
Gu, A., Dao, T., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock {HiPPO: Recurrent memory with optimal polynomial projections}.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1474--1487, 2020.

\bibitem[Gu et~al.(2022)Gu, Goel, and Re]{gu2022efficiently}
Gu, A., Goel, K., and Re, C.
\newblock Efficiently modeling long sequences with structured state spaces.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Gusak et~al.(2020)Gusak, Markeeva, Daulbaev, Katrutsa, Cichocki, and
  Oseledets]{gusak2020towards}
Gusak, J., Markeeva, L., Daulbaev, T., Katrutsa, A., Cichocki, A., and
  Oseledets, I.
\newblock {Towards understanding normalization in neural ODEs}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{IEEE International Conference on Computer Vision}, pp.\
  1026--1034, 2015.

\bibitem[Huang et~al.(2020)Huang, Sun, and Wang]{huang2020learning}
Huang, Z., Sun, Y., and Wang, W.
\newblock Learning continuous system dynamics from irregularly-sampled partial
  observations.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16177--16187, 2020.

\bibitem[Jung(2022)]{jung2022automated}
Jung, D.
\newblock Automated design of grey-box recurrent neural networks for fault
  diagnosis using structural models and causal information.
\newblock In \emph{Learning for Dynamics and Control Conference}, pp.\  8--20.
  PMLR, 2022.

\bibitem[Khalil(2002)]{khalil2020nonlinear}
Khalil, H.~K.
\newblock \emph{Nonlinear Systems}.
\newblock Prentice Hall, 3rd edition, 2002.

\bibitem[Kidger et~al.(2020)Kidger, Morrill, Foster, and
  Lyons]{kidger2020neural}
Kidger, P., Morrill, J., Foster, J., and Lyons, T.
\newblock Neural controlled differential equations for irregular time series.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  6696--6707, 2020.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{diederik2014auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Krishnapriyan et~al.(2022)Krishnapriyan, Queiruga, Erichson, and
  Mahoney]{krishnapriyan2022learning}
Krishnapriyan, A.~S., Queiruga, A.~F., Erichson, N.~B., and Mahoney, M.~W.
\newblock Learning continuous models for continuous physics.
\newblock \emph{arXiv preprint arXiv:2202.08494}, 2022.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Le et~al.(2015)Le, Jaitly, and Hinton]{le2015simple}
Le, Q.~V., Jaitly, N., and Hinton, G.~E.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock \emph{arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Lipman et~al.(2023)Lipman, Chen, Ben-Hamu, Nickel, and
  Le]{lipman2023flow}
Lipman, Y., Chen, R. T.~Q., Ben-Hamu, H., Nickel, M., and Le, M.
\newblock Flow matching for generative modeling.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Ljung(1999)]{ljung1999system}
Ljung, L.
\newblock \emph{System identification: Theory for the User}.
\newblock Prentice Hall, 2nd edition edition, 1999.

\bibitem[Luko{\v{s}}evi{\v{c}}ius \& Jaeger(2009)Luko{\v{s}}evi{\v{c}}ius and
  Jaeger]{lukovsevivcius2009reservoir}
Luko{\v{s}}evi{\v{c}}ius, M. and Jaeger, H.
\newblock Reservoir computing approaches to recurrent neural network training.
\newblock \emph{Computer science review}, 3\penalty0 (3):\penalty0 127--149,
  2009.

\bibitem[Massaroli et~al.(2020)Massaroli, Poli, Park, Yamashita, and
  Asama]{massaroli2020dissecting}
Massaroli, S., Poli, M., Park, J., Yamashita, A., and Asama, H.
\newblock {Dissecting neural ODEs}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  3952--3963, 2020.

\bibitem[Mezzadri(2006)]{mezzadri2006generate}
Mezzadri, F.
\newblock How to generate random matrices from the classical compact groups.
\newblock \emph{arXiv preprint math-ph/0609050}, 2006.

\bibitem[Mohammadi et~al.(2023)Mohammadi, Westny, Jung, and
  Krysander]{mohammadi2023analysis}
Mohammadi, A., Westny, T., Jung, D., and Krysander, M.
\newblock Analysis of numerical integration in {RNN}-based residuals for fault
  diagnosis of dynamic systems.
\newblock \emph{arXiv preprint arXiv:2305.04670}, 2023.

\bibitem[Oord et~al.(2016)Oord, Dieleman, Zen, Simonyan, Vinyals, Graves,
  Kalchbrenner, Senior, and Kavukcuoglu]{oord2016wavenet}
Oord, A. v.~d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A.,
  Kalchbrenner, N., Senior, A., and Kavukcuoglu, K.
\newblock Wavenet: A generative model for raw audio.
\newblock \emph{arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[Ott et~al.(2021)Ott, Katiyar, Hennig, and Tiemann]{ott2021resnet}
Ott, K., Katiyar, P., Hennig, P., and Tiemann, M.
\newblock {ResNet after all: Neural ODEs and their numerical solution}.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Pascanu et~al.(2013)Pascanu, Mikolov, and
  Bengio]{pascanu2013difficulty}
Pascanu, R., Mikolov, T., and Bengio, Y.
\newblock On the difficulty of training recurrent neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1310--1318. PMLR, 2013.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock {PyTorch: An imperative style, high-performance deep learning
  library}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Queiruga et~al.(2020)Queiruga, Erichson, Taylor, and
  Mahoney]{queiruga2020continuous}
Queiruga, A.~F., Erichson, N.~B., Taylor, D., and Mahoney, M.~W.
\newblock Continuous-in-depth neural networks.
\newblock \emph{arXiv preprint arXiv:2008.02389}, 2020.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock On the spectral bias of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5301--5310. PMLR, 2019.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1278--1286. PMLR, 2014.

\bibitem[Rico-Martinez \& Kevrekidis(1993)Rico-Martinez and
  Kevrekidis]{rico1993continuous}
Rico-Martinez, R. and Kevrekidis, I.~G.
\newblock Continuous time modeling of nonlinear systems: A neural network-based
  approach.
\newblock In \emph{IEEE International Conference on Neural Networks}, pp.\
  1522--1525. IEEE, 1993.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015unet}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock {U-Net: Convolutional networks for biomedical image segmentation}.
\newblock In \emph{International Conference on Medical Image Computing and
  Computer-Assisted Intervention}, pp.\  234--241. Springer, 2015.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Rubanova, Y., Chen, R.~T., and Duvenaud, D.~K.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Stewart(1980)]{stewart1980efficient}
Stewart, G.~W.
\newblock The efficient generation of random orthogonal matrices with an
  application to condition estimators.
\newblock \emph{SIAM Journal on Numerical Analysis}, 17\penalty0 (3):\penalty0
  403--409, 1980.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  7537--7547, 2020.

\bibitem[Teerapittayanon et~al.(2016)Teerapittayanon, McDanel, and
  Kung]{anon2016branchynet}
Teerapittayanon, S., McDanel, B., and Kung, H.-T.
\newblock Branchynet: Fast inference via early exiting from deep neural
  networks.
\newblock In \emph{IEEE International Conference on Pattern Recognition}, pp.\
  2464--2469, 2016.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In \emph{IEEE/RSJ international Conference on Intelligent Robots and
  Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Toth et~al.(2020)Toth, Rezende, Jaegle, Racani{\`e}re, Botev, and
  Higgins]{toth2019hamiltonian}
Toth, P., Rezende, D.~J., Jaegle, A., Racani{\`e}re, S., Botev, A., and
  Higgins, I.
\newblock Hamiltonian generative networks.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Verma et~al.(2024)Verma, Heinonen, and Garg]{verma2024climode}
Verma, Y., Heinonen, M., and Garg, V.
\newblock Clim{ODE}: Climate forecasting with physics-informed neural {ODE}s.
\newblock In \emph{International Conference on Learning Representations}, 2024.

\bibitem[Vidulin et~al.(2010)Vidulin, Lustrek, Kaluza, Piltaver, and
  Krivec]{vidulin2010localization}
Vidulin, V., Lustrek, M., Kaluza, B., Piltaver, R., and Krivec, J.
\newblock {Localization Data for Person Activity}.
\newblock UCI Machine Learning Repository, 2010.
\newblock {DOI}: https://doi.org/10.24432/C57G8X.

\bibitem[Vito(2016)]{saverio2016air}
Vito, S.
\newblock {Air Quality}.
\newblock UCI Machine Learning Repository, 2016.
\newblock {DOI}: https://doi.org/10.24432/C59K5F.

\bibitem[Westny et~al.(2023)Westny, Oskarsson, Olofsson, and
  Frisk]{westny2023graph}
Westny, T., Oskarsson, J., Olofsson, B., and Frisk, E.
\newblock {MTP-GO}: Graph-based probabilistic multi-agent trajectory prediction
  with neural {ODEs}.
\newblock \emph{IEEE Transactions on Intelligent Vehicles}, 8\penalty0
  (9):\penalty0 4223--4236, 2023.

\bibitem[Xia et~al.(2021)Xia, Suliafu, Ji, Nguyen, Bertozzi, Osher, and
  Wang]{xia2021heavy}
Xia, H., Suliafu, V., Ji, H., Nguyen, T., Bertozzi, A., Osher, S., and Wang, B.
\newblock Heavy ball neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  18646--18659, 2021.

\bibitem[Yan et~al.(2019)Yan, Du, Tan, and Feng]{yan2019robustness}
Yan, H., Du, J., Tan, V.~Y., and Feng, J.
\newblock On robustness of neural ordinary differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yildiz et~al.(2019)Yildiz, Heinonen, and
  Lahdesmaki]{yildiz2019ode2vae}
Yildiz, C., Heinonen, M., and Lahdesmaki, H.
\newblock {ODE2VAE: Deep generative second order ODEs with Bayesian neural
  networks}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Gao, Unterman, and
  Arodz]{zhang2020approximation}
Zhang, H., Gao, X., Unterman, J., and Arodz, T.
\newblock {Approximation capabilities of neural ODEs and invertible residual
  networks}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11086--11095. PMLR, 2020.

\bibitem[Zhang et~al.(2019)Zhang, Yao, Gholami, Gonzalez, Keutzer, Mahoney, and
  Biros]{zhang2019anodev2}
Zhang, T., Yao, Z., Gholami, A., Gonzalez, J.~E., Keutzer, K., Mahoney, M.~W.,
  and Biros, G.
\newblock {ANODEV2: A coupled neural ODE framework}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Zhong et~al.(2021)Zhong, Dey, and Chakraborty]{zhong2021benchmarking}
Zhong, Y.~D., Dey, B., and Chakraborty, A.
\newblock Benchmarking energy-conserving neural networks for learning dynamics
  from data.
\newblock In \emph{Learning for dynamics and control}, pp.\  1218--1229. PMLR,
  2021.

\bibitem[Zhu et~al.(2022)Zhu, Jin, Zhu, and Tang]{zhu2022numerical}
Zhu, A., Jin, P., Zhu, B., and Tang, Y.
\newblock On numerical integration in neural ordinary differential equations.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  27527--27547. PMLR, 2022.

\bibitem[Zhuang et~al.(2020)Zhuang, Dvornek, Li, Tatikonda, Papademetris, and
  Duncan]{zhuang2020adaptive}
Zhuang, J., Dvornek, N., Li, X., Tatikonda, S., Papademetris, X., and Duncan,
  J.
\newblock {Adaptive checkpoint adjoint method for gradient estimation in neural
  ODE}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11639--11649. PMLR, 2020.

\end{thebibliography}
