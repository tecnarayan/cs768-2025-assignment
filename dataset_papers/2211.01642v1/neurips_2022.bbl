\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2019)Achille, Paolini, and
  Soatto]{achille2019information}
Alessandro Achille, Giovanni Paolini, and Stefano Soatto.
\newblock Where is the information in a deep neural network?
\newblock \emph{arXiv preprint arXiv:1905.12213}, 2019.

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Shrivastava, Gupta, Goyal,
  Zettlemoyer, and Gupta]{aghajanyan2020better}
Armen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke
  Zettlemoyer, and Sonal Gupta.
\newblock Better fine-tuning by reducing representational collapse.
\newblock In \emph{ICLR}, 2021.

\bibitem[Amari(1996)]{amari1996neural}
Shun-ichi Amari.
\newblock Neural learning in structured parameter spaces-natural riemannian
  gradient.
\newblock In \emph{NeurIPS}, 1996.

\bibitem[Belinkov et~al.(2019)Belinkov, Poliak, Shieber, Van~Durme, and
  Rush]{belinkov2019adversarial}
Yonatan Belinkov, Adam Poliak, Stuart~M Shieber, Benjamin Van~Durme, and
  Alexander~M Rush.
\newblock On adversarial removal of hypothesis-only bias in natural language
  inference.
\newblock In \emph{StarSem}, 2019.

\bibitem[Bentivogli et~al.(2009)Bentivogli, Clark, Dagan, and
  Giampiccolo]{bentivogli2009fifth}
Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In \emph{TAC}, 2009.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and
  Manning]{bowman2015large}
Samuel~R Bowman, Gabor Angeli, Christopher Potts, and Christopher~D Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{EMNLP}, 2015.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Cer et~al.(2017)Cer, Diab, Agirre, Lopez-Gazpio, and
  Specia]{cer2017semeval}
Daniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.
\newblock Semeval-2017 task 1: Semantic textual similarity-multilingual and
  cross-lingual focused evaluation.
\newblock \emph{arXiv preprint arXiv:1708.00055}, 2017.

\bibitem[Clark et~al.(2020)Clark, Luong, Le, and Manning]{clark2020electra}
Kevin Clark, Minh-Thang Luong, Quoc~V Le, and Christopher~D Manning.
\newblock Electra: Pre-training text encoders as discriminators rather than
  generators.
\newblock In \emph{ICLR}, 2020.

\bibitem[Crowley et~al.(2018)Crowley, Turner, Storkey, and
  O'Boyle]{crowley2018pruning}
Elliot~J Crowley, Jack Turner, Amos Storkey, and Michael O'Boyle.
\newblock Pruning neural networks: is it time to nip it in the bud?
\newblock 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2018.

\bibitem[Dodge et~al.(2020)Dodge, Ilharco, Schwartz, Farhadi, Hajishirzi, and
  Smith]{dodge2020fine}
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi,
  and Noah Smith.
\newblock Fine-tuning pretrained language models: Weight initializations, data
  orders, and early stopping.
\newblock \emph{arXiv preprint arXiv:2002.06305}, 2020.

\bibitem[Dolan and Brockett(2005)]{dolan2005automatically}
Bill Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{IWP}, 2005.

\bibitem[Fisher(1922)]{fisher1922mathematical}
Ronald~A Fisher.
\newblock On the mathematical foundations of theoretical statistics.
\newblock \emph{Philosophical transactions of the Royal Society of London.
  Series A, containing papers of a mathematical or physical character},
  222\penalty0 (594-604):\penalty0 309--368, 1922.

\bibitem[He et~al.(2021)He, Liu, Gao, and Chen]{he2020deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention.
\newblock In \emph{ICLR}, 2021.

\bibitem[Howard and Ruder(2018)]{howard2018universal}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock In \emph{ACL}, 2018.

\bibitem[Khot et~al.(2018)Khot, Sabharwal, and Clark]{khot2018scitail}
Tushar Khot, Ashish Sabharwal, and Peter Clark.
\newblock Scitail: A textual entailment dataset from science question
  answering.
\newblock In \emph{AAAI}, 2018.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock In \emph{PNAS}, 2017.

\bibitem[Lee et~al.(2020)Lee, Cho, and Kang]{lee2019mixout}
Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.
\newblock Mixout: Effective regularization to finetune large-scale pretrained
  language models.
\newblock In \emph{ICLR}, 2020.

\bibitem[Lewis et~al.(2019)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock \emph{arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov2019decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization (2017).
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2019.

\bibitem[Mahabadi et~al.(2021)Mahabadi, Belinkov, and
  Henderson]{mahabadi2021variational}
Rabeeh~Karimi Mahabadi, Yonatan Belinkov, and James Henderson.
\newblock Variational information bottleneck for effective low-resource
  fine-tuning.
\newblock In \emph{ICLR}, 2021.

\bibitem[Marelli et~al.(2014)Marelli, Menini, Baroni, Bentivogli, Bernardi, and
  Zamparelli]{marelli2014sick}
Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella
  Bernardi, and Roberto Zamparelli.
\newblock A sick cure for the evaluation of compositional distributional
  semantic models.
\newblock In \emph{LREC}, 2014.

\bibitem[Martens(2014)]{martens2014new}
James Martens.
\newblock New insights and perspectives on the natural gradient method.
\newblock \emph{arXiv preprint arXiv:1412.1193}, 2014.

\bibitem[Mosbach et~al.(2021)Mosbach, Andriushchenko, and
  Klakow]{mosbach2020stability}
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow.
\newblock On the stability of fine-tuning bert: Misconceptions, explanations,
  and strong baselines.
\newblock In \emph{ICLR}, 2021.

\bibitem[Pascanu and Bengio(2013)]{pascanu2013revisiting}
Razvan Pascanu and Yoshua Bengio.
\newblock Revisiting natural gradient for deep networks.
\newblock \emph{arXiv preprint arXiv:1301.3584}, 2013.

\bibitem[Phang et~al.(2018)Phang, F{\'e}vry, and Bowman]{phang2018sentence}
Jason Phang, Thibault F{\'e}vry, and Samuel~R Bowman.
\newblock Sentence encoders on stilts: Supplementary training on intermediate
  labeled-data tasks.
\newblock \emph{arXiv preprint arXiv:1811.01088}, 2018.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{rajpurkar2016squad}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Shankar~Iyer and Csernai.(2017)]{Iyer2017}
Nikhil~Dandekar Shankar~Iyer and Kornel Csernai.
\newblock First quora dataset release: Question pairs.
\newblock \emph{\url{https://tinyurl.com/y2y8u5ed}}, 2017.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Singh and Alistarh(2020)]{singh2020woodfisher}
Sidak~Pal Singh and Dan Alistarh.
\newblock Woodfisher: Efficient second-order approximation for neural network
  compression.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D Manning,
  Andrew~Y Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{EMNLP}, 2013.

\bibitem[Theis et~al.(2018)Theis, Korshunova, Tejani, and
  Husz{\'a}r]{theis2018faster}
Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz{\'a}r.
\newblock Faster gaze prediction with dense networks and fisher pruning.
\newblock \emph{arXiv preprint arXiv:1801.05787}, 2018.

\bibitem[Tu et~al.(2016)Tu, Berisha, Woolf, Seo, and Cao]{tu2016ranking}
Ming Tu, Visar Berisha, Martin Woolf, Jae-sun Seo, and Yu~Cao.
\newblock Ranking the parameters of deep neural networks using the fisher
  information.
\newblock In \emph{ICASSP}, 2016.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel~R
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{ICLR}, 2019.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{warstadt2019neural}
Alex Warstadt, Amanpreet Singh, and Samuel~R Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 625--641, 2019.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{NAACL}, 2018.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, et~al.]{wolf2020transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R{\'e}mi Louf, Morgan Funtowicz,
  et~al.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Wu et~al.(2021)Wu, Li, Wang, Meng, Qin, Chen, Zhang, Liu,
  et~al.]{wu2021r}
Lijun Wu, Juntao Li, Yue Wang, Qi~Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan
  Liu, et~al.
\newblock R-drop: regularized dropout for neural networks.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Xu et~al.(2021)Xu, Luo, Zhang, Tan, Chang, Huang, and
  Huang]{xu2021raise}
Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang,
  and Fei Huang.
\newblock Raise a child in large language model: Towards effective and
  generalizable fine-tuning.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Zhang et~al.(2021)Zhang, Wu, Katiyar, Weinberger, and
  Artzi]{zhang2020revisiting}
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Revisiting few-sample bert fine-tuning.
\newblock In \emph{ICLR}, 2021.

\end{thebibliography}
