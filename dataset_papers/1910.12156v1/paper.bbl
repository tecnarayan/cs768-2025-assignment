\begin{thebibliography}{10}

\bibitem{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages 22--31,
  2017.

\bibitem{adolphs2018non}
Leonard Adolphs.
\newblock Non convex-concave saddle point optimization.
\newblock Master's thesis, ETH Zurich, 2018.

\bibitem{altman1999constrained}
Eitan Altman.
\newblock {\em Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem{ammar2015safe}
Haitham~Bou Ammar, Rasul Tutunov, and Eric Eaton.
\newblock Safe policy search for lifelong reinforcement learning with sublinear
  regret.
\newblock In {\em International Conference on Machine Learning}, pages
  2361--2369, 2015.

\bibitem{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and
  Dan Man{\'e}.
\newblock Concrete problems in ai safety.
\newblock {\em arXiv preprint arXiv:1606.06565}, 2016.

\bibitem{anderson2007optimal}
Brian~DO Anderson and John~B Moore.
\newblock {\em Optimal control: linear quadratic methods}.
\newblock Courier Corporation, 2007.

\bibitem{bai2019provably}
Yu~Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang.
\newblock Provably efficient q-learning with low switching cost.
\newblock {\em arXiv preprint arXiv:1905.12849}, 2019.

\bibitem{bellemare2013arcade}
Marc~G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, 47:253--279, 2013.

\bibitem{berkenkamp2017safe}
Felix Berkenkamp, Matteo Turchetta, Angela Schoellig, and Andreas Krause.
\newblock Safe model-based reinforcement learning with stability guarantees.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  908--918, 2017.

\bibitem{borkar1997stochastic}
Vivek~S Borkar.
\newblock Stochastic approximation with two time scales.
\newblock {\em Systems \& Control Letters}, 29(5):291--294, 1997.

\bibitem{boutilier1996planning}
Craig Boutilier.
\newblock Planning, learning and coordination in multiagent decision processes.
\newblock In {\em Proceedings of the 6th conference on Theoretical aspects of
  rationality and knowledge}, pages 195--210. Morgan Kaufmann Publishers Inc.,
  1996.

\bibitem{bradtke1993reinforcement}
Steven~J Bradtke.
\newblock Reinforcement learning applied to linear quadratic regulation.
\newblock In {\em Advances in neural information processing systems}, pages
  295--302, 1993.

\bibitem{bradtke1994adaptive}
Steven~J Bradtke, B~Erik Ydstie, and Andrew~G Barto.
\newblock Adaptive linear quadratic control using policy iteration.
\newblock In {\em Proceedings of the American control conference}, volume~3,
  pages 3475--3475. Citeseer, 1994.

\bibitem{breiman2001random}
Leo Breiman.
\newblock Random forests.
\newblock {\em Machine learning}, 45(1):5--32, 2001.

\bibitem{busoniu2008comprehensive}
Lucian Busoniu, Robert Babuska, and Bart De~Schutter.
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock {\em IEEE Transactions on Systems, Man, And Cybernetics-Part C:
  Applications and Reviews, 38 (2), 2008}, 2008.

\bibitem{cai2019neural}
Qi~Cai, Zhuoran Yang, Jason~D Lee, and Zhaoran Wang.
\newblock Neural temporal-difference learning converges to global optima.
\newblock {\em arXiv preprint arXiv:1905.10027}, 2019.

\bibitem{chen2018communication}
Tianyi Chen, Kaiqing Zhang, Georgios~B Giannakis, and Tamer Ba{\c{s}}ar.
\newblock Communication-efficient distributed reinforcement learning.
\newblock {\em arXiv preprint arXiv:1812.03239}, 2018.

\bibitem{chow2017risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock {\em Journal of Machine Learning Research}, 18(167):1--167, 2017.

\bibitem{chow2018lyapunov}
Yinlam Chow, Ofir Nachum, Edgar Duenez-Guzman, and Mohammad Ghavamzadeh.
\newblock A lyapunov-based approach to safe reinforcement learning.
\newblock {\em arXiv preprint arXiv:1805.07708}, 2018.

\bibitem{dann2014policy}
Christoph Dann, Gerhard Neumann, and Jan Peters.
\newblock Policy evaluation with temporal differences: A survey and comparison.
\newblock {\em The Journal of Machine Learning Research}, 15(1):809--883, 2014.

\bibitem{dean2017sample}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock {\em arXiv preprint arXiv:1710.01688}, 2017.

\bibitem{dunford1958linear}
Nelson Dunford and Jacob~T Schwartz.
\newblock {\em Linear operators part I: general theory}, volume~7.
\newblock Interscience publishers New York, 1958.

\bibitem{evans2005introduction}
Lawrence~C Evans.
\newblock An introduction to mathematical optimal control theory.
\newblock {\em Lecture Notes, University of California, Department of
  Mathematics, Berkeley}, 2005.

\bibitem{fazel2018global}
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In {\em International Conference on Machine Learning}, pages
  1466--1475, 2018.

\bibitem{fisac2018general}
Jaime~F Fisac, Anayo~K Akametalu, Melanie~N Zeilinger, Shahab Kaynama, Jeremy
  Gillula, and Claire~J Tomlin.
\newblock A general safety framework for learning-based control in uncertain
  robotic systems.
\newblock {\em IEEE Transactions on Automatic Control}, 2018.

\bibitem{garcia2015comprehensive}
Javier Garc{\i}a and Fernando Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 16(1):1437--1480, 2015.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem{grondman2012survey}
Ivo Grondman, Lucian Busoniu, Gabriel~AD Lopes, and Robert Babuska.
\newblock A survey of actor-critic reinforcement learning: Standard and natural
  policy gradients.
\newblock {\em IEEE Transactions on Systems, Man, and Cybernetics, Part C
  (Applications and Reviews)}, 42(6):1291--1307, 2012.

\bibitem{he2019xbart}
Jingyu He, Saar Yalov, and P~Richard Hahn.
\newblock X{BART}: Accelerated {B}ayesian additive regression trees.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 1130--1138, 2019.

\bibitem{he2019scalable}
Jingyu He, Saar Yalov, Jared Murray, and P~Richard Hahn.
\newblock Stochastic tree ensembles for regularized supervised learning.
\newblock {\em Technical report}, 2019.

\bibitem{huang2018learning}
Jessie Huang, Fa~Wu, Doina Precup, and Yang Cai.
\newblock Learning safe policies with expert guidance.
\newblock {\em arXiv preprint arXiv:1805.08313}, 2018.

\bibitem{kelley2017general}
John~L Kelley.
\newblock {\em General topology}.
\newblock Courier Dover Publications, 2017.

\bibitem{konda2000actor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  1008--1014, 2000.

\bibitem{kretchmar2002parallel}
R~Matthew Kretchmar.
\newblock Parallel reinforcement learning.
\newblock In {\em The 6th World Conference on Systemics, Cybernetics, and
  Informatics}. Citeseer, 2002.

\bibitem{lacotte2018risk}
Jonathan Lacotte, Yinlam Chow, Mohammad Ghavamzadeh, and Marco Pavone.
\newblock Risk-sensitive generative adversarial imitation learning.
\newblock {\em arXiv preprint arXiv:1808.04468}, 2018.

\bibitem{lee2018modular}
Dennis Lee, Haoran Tang, Jeffrey~O Zhang, Huazhe Xu, Trevor Darrell, and Pieter
  Abbeel.
\newblock Modular architecture for starcraft ii with deep reinforcement
  learning.
\newblock In {\em Fourteenth Artificial Intelligence and Interactive Digital
  Entertainment Conference}, 2018.

\bibitem{li2017deep}
Yuxi Li.
\newblock Deep reinforcement learning: An overview.
\newblock {\em arXiv preprint arXiv:1701.07274}, 2017.

\bibitem{liu2018stochastic}
An~Liu, Vincent Lau, and Borna Kananian.
\newblock Stochastic successive convex approximation for non-convex constrained
  stochastic optimization.
\newblock {\em arXiv preprint arXiv:1801.08266}, 2018.

\bibitem{liu2019neural}
Boyi Liu, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural proximal/trust region policy optimization attains globally
  optimal policy.
\newblock {\em arXiv preprint arXiv:1906.10306}, 2019.

\bibitem{mnih2016asynchronous}
Volodymyr Mnih, Adria~Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
  Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In {\em International conference on machine learning}, pages
  1928--1937, 2016.

\bibitem{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518(7540):529, 2015.

\bibitem{nair2015massively}
Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon,
  Alessandro De~Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles
  Beattie, Stig Petersen, et~al.
\newblock Massively parallel methods for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1507.04296}, 2015.

\bibitem{paternain2018stochastic}
Santiago Paternain.
\newblock {\em Stochastic Control Foundations of Autonomous Behavior}.
\newblock PhD thesis, University of Pennsylvania, 2018.

\bibitem{peng2017multiagent}
Peng Peng, Ying Wen, Yaodong Yang, Quan Yuan, Zhenkun Tang, Haitao Long, and
  Jun Wang.
\newblock Multiagent bidirectionally-coordinated nets: Emergence of human-level
  coordination in learning to play starcraft combat games.
\newblock {\em arXiv preprint arXiv:1703.10069}, 2017.

\bibitem{pirotta2015policy}
Matteo Pirotta, Marcello Restelli, and Luca Bascetta.
\newblock Policy gradient in lipschitz markov decision processes.
\newblock {\em Machine Learning}, 100(2-3):255--283, 2015.

\bibitem{prashanth2016variance}
LA~Prashanth and Mohammad Ghavamzadeh.
\newblock Variance-constrained actor-critic algorithms for discounted and
  average reward mdps.
\newblock {\em Machine Learning}, 105(3):367--417, 2016.

\bibitem{recht2018tour}
Benjamin Recht.
\newblock A tour of reinforcement learning: The view from continuous control.
\newblock {\em Annual Review of Control, Robotics, and Autonomous Systems},
  2018.

\bibitem{rhee2015unbiased}
Chang-han Rhee and Peter~W Glynn.
\newblock Unbiased estimation with square root convergence for sde models.
\newblock {\em Operations Research}, 63(5):1026--1043, 2015.

\bibitem{ruszczynski1980feasible}
Andrzej Ruszczy{\'n}ski.
\newblock Feasible direction methods for stochastic programming problems.
\newblock {\em Mathematical Programming}, 19(1):220--229, 1980.

\bibitem{scharpff2016solving}
Joris Scharpff, Diederik~M Roijers, Frans~A Oliehoek, Matthijs~TJ Spaan, and
  Mathijs~Michiel de~Weerdt.
\newblock Solving transition-independent multi-agent mdps with sparse
  interactions.
\newblock In {\em AAAI}, pages 3174--3180, 2016.

\bibitem{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem{scutari2013decomposition}
Gesualdo Scutari, Francisco Facchinei, Peiran Song, Daniel~P Palomar, and
  Jong-Shi Pang.
\newblock Decomposition by partial linearization: Parallel optimization of
  multi-agent systems.
\newblock {\em IEEE Transactions on Signal Processing}, 62(3):641--656, 2013.

\bibitem{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em nature}, 529(7587):484, 2016.

\bibitem{silver2018general}
David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew
  Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan Kumaran, Thore
  Graepel, et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock {\em Science}, 362(6419):1140--1144, 2018.

\bibitem{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock {\em Nature}, 550(7676):354, 2017.

\bibitem{sun2018tstarbots}
Peng Sun, Xinghai Sun, Lei Han, Jiechao Xiong, Qing Wang, Bo~Li, Yang Zheng,
  Ji~Liu, Yongsheng Liu, Han Liu, et~al.
\newblock Tstarbots: Defeating the cheating level builtin ai in starcraft ii in
  the full game.
\newblock {\em arXiv preprint arXiv:1809.07193}, 2018.

\bibitem{sun2016majorization}
Ying Sun, Prabhu Babu, and Daniel~P Palomar.
\newblock Majorization-minimization algorithms in signal processing,
  communications, and machine learning.
\newblock {\em IEEE Transactions on Signal Processing}, 65(3):794--816, 2016.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem{tessler2018reward}
Chen Tessler, Daniel~J Mankowitz, and Shie Mannor.
\newblock Reward constrained policy optimization.
\newblock {\em arXiv preprint arXiv:1805.11074}, 2018.

\bibitem{turchetta2016safe}
Matteo Turchetta, Felix Berkenkamp, and Andreas Krause.
\newblock Safe exploration in finite markov decision processes with gaussian
  processes.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4312--4320, 2016.

\bibitem{vinyals2017starcraft}
Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander~Sasha
  Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K{\"u}ttler, John
  Agapiou, Julian Schrittwieser, et~al.
\newblock Starcraft ii: A new challenge for reinforcement learning.
\newblock {\em arXiv preprint arXiv:1708.04782}, 2017.

\bibitem{wai2018multi}
Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, and Mingyi Hong.
\newblock Multi-agent reinforcement learning via double averaging primal-dual
  optimization.
\newblock {\em arXiv preprint arXiv:1806.00877}, 2018.

\bibitem{wang2019neural}
Lingxiao Wang, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock {\em arXiv preprint arXiv:1909.01150}, 2019.

\bibitem{wen2018constrained}
Min Wen and Ufuk Topcu.
\newblock Constrained cross-entropy method for safe reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7461--7471, 2018.

\bibitem{xu2018macro}
Sijia Xu, Hongyu Kuang, Zhi Zhuang, Renjie Hu, Yang Liu, and Huyang Sun.
\newblock Macro action selection with deep reinforcement learning in starcraft.
\newblock {\em arXiv preprint arXiv:1812.00336}, 2018.

\bibitem{yang2016parallel}
Yang Yang, Gesualdo Scutari, Daniel~P Palomar, and Marius Pesavento.
\newblock A parallel decomposition method for nonconvex stochastic multi-agent
  optimization problems.
\newblock {\em IEEE Transactions on Signal Processing}, 64(11):2949--2964,
  2016.

\bibitem{yang2019global}
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang.
\newblock On the global convergence of actor-critic: A case for linear
  quadratic regulator with ergodic cost.
\newblock {\em arXiv preprint arXiv:1907.06246}, 2019.

\bibitem{zhang2018finite}
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Ba{\c{s}}ar.
\newblock Finite-sample analyses for fully decentralized multi-agent
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1812.02783}, 2018.

\end{thebibliography}
