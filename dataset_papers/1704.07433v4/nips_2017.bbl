\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alain et~al.(2015)Alain, Lamb, Sankar, Courville, and
  Bengio]{alain2015variance}
G.~Alain, A.~Lamb, C.~Sankar, A.~Courville, and Y.~Bengio.
\newblock Variance reduction in {SGD} by distributed importance sampling.
\newblock \emph{arXiv preprint arXiv:1511.06481}, 2015.

\bibitem[Amari et~al.(2000)Amari, Park, and Fukumizu]{amari2000adaptive}
S.-I. Amari, H.~Park, and K.~Fukumizu.
\newblock Adaptive method of realizing natural gradient learning for multilayer
  perceptrons.
\newblock \emph{Neural Computation}, 12\penalty0 (6):\penalty0 1399--1409,
  2000.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, and de~Freitas]{andrychowicz2016learning}
M.~Andrychowicz, M.~Denil, S.~Gomez, M.~W. Hoffman, D.~Pfau, T.~Schaul, and
  N.~de~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{NIPS}, 2016.

\bibitem[Avramova(2015)]{avramova2015curriculum}
V.~Avramova.
\newblock Curriculum learning with deep convolutional neural networks, 2015.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{bengio2009curriculum}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston.
\newblock Curriculum learning.
\newblock In \emph{ICML}, 2009.

\bibitem[Bordes et~al.(2005)Bordes, Ertekin, Weston, and
  Bottou]{bordes2005fast}
A.~Bordes, S.~Ertekin, J.~Weston, and L.~Bottou.
\newblock Fast kernel classifiers with online and active learning.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Sep):\penalty0 1579--1619, 2005.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, et~al.]{bubeck2012regret}
S.~Bubeck, N.~Cesa-Bianchi, et~al.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  5\penalty0 (1):\penalty0 1--122, 2012.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, and
  LeCun]{chaudhari2016entropy}
P.~Chaudhari, A.~Choromanska, S.~Soatto, and Y.~LeCun.
\newblock {Entropy-SGD}: Biasing gradient descent into wide valleys.
\newblock In \emph{ICLR}, 2017.

\bibitem[Collobert et~al.(2011)Collobert, Weston, Bottou, Karlen, Kavukcuoglu,
  and Kuksa]{collobert2011natural}
R.~Collobert, J.~Weston, L.~Bottou, M.~Karlen, K.~Kavukcuoglu, and P.~Kuksa.
\newblock Natural language processing (almost) from scratch.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Aug):\penalty0 2493--2537, 2011.

\bibitem[Druck and McCallum(2011)]{druck2011toward}
G.~Druck and A.~McCallum.
\newblock Toward interactive training and evaluation.
\newblock In \emph{Proceedings of the 20th ACM international conference on
  Information and knowledge management}, pages 947--956. ACM, 2011.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Gao et~al.(2015)Gao, Jagadish, and Ooi]{gao2015active}
J.~Gao, H.~Jagadish, and B.~C. Ooi.
\newblock Active sampler: Light-weight accelerator for complex data analytics
  at scale.
\newblock \emph{arXiv preprint arXiv:1512.03880}, 2015.

\bibitem[Gopal(2016)]{gopal2016adaptive}
S.~Gopal.
\newblock Adaptive sampling for {SGD} by exploiting side information.
\newblock In \emph{ICML}, 2016.

\bibitem[Guillory et~al.(2009)Guillory, Chastain, and
  Bilmes]{guillory2009active}
A.~Guillory, E.~Chastain, and J.~A. Bilmes.
\newblock Active learning as non-convex optimization.
\newblock In \emph{AISTATS}, 2009.

\bibitem[Gulcehre et~al.(2017)Gulcehre, Moczulski, Visin, and
  Bengio]{gulcehre2016mollifying}
C.~Gulcehre, M.~Moczulski, F.~Visin, and Y.~Bengio.
\newblock Mollifying networks.
\newblock In \emph{ICLR}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{NIPS Deep Learning Workshop}, 2014.

\bibitem[Hinton(2007)]{hinton2007recognize}
G.~E. Hinton.
\newblock To recognize shapes, first learn to generate images.
\newblock \emph{Progress in brain research}, 165:\penalty0 535--547, 2007.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and
  Lengyel]{houlsby2011bayesian}
N.~Houlsby, F.~Husz{\'a}r, Z.~Ghahramani, and M.~Lengyel.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{arXiv preprint arXiv:1112.5745}, 2011.

\bibitem[Hovy et~al.(2006)Hovy, Marcus, Palmer, Ramshaw, and
  Weischedel]{hovy2006ontonotes}
E.~Hovy, M.~Marcus, M.~Palmer, L.~Ramshaw, and R.~Weischedel.
\newblock {OntoNotes}: the 90\% solution.
\newblock In \emph{HLT-NAACL}, 2006.

\bibitem[Johnson and Zhang(2013)]{johnson2013accelerating}
R.~Johnson and T.~Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{NIPS}, 2013.

\bibitem[Kim(2014)]{DBLP:conf/emnlp/Kim14}
Y.~Kim.
\newblock Convolutional neural networks for sentence classification.
\newblock In \emph{EMNLP}, 2014.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
M.~P. Kumar, B.~Packer, and D.~Koller.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{NIPS}, 2010.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2016)Lee, Yang, and Lin]{lee2016toward}
G.-H. Lee, S.-W. Yang, and S.-D. Lin.
\newblock Toward implicit sample noise modeling: Deviation-driven matrix
  factorization.
\newblock \emph{arXiv preprint arXiv:1610.09274}, 2016.

\bibitem[Li and Roth(2002)]{li2002learning}
X.~Li and D.~Roth.
\newblock Learning question classifiers.
\newblock In \emph{COLING}, 2002.

\bibitem[Loshchilov and Hutter(2015)]{loshchilov2015online}
I.~Loshchilov and F.~Hutter.
\newblock Online batch selection for faster training of neural networks.
\newblock \emph{arXiv preprint arXiv:1511.06343}, 2015.

\bibitem[MacKay(1992)]{mackay1992information}
D.~J. MacKay.
\newblock Information-based objective functions for active data selection.
\newblock \emph{Neural computation}, 4\penalty0 (4):\penalty0 590--604, 1992.

\bibitem[Mandt et~al.(2016{\natexlab{a}})Mandt, Hoffman, and
  Blei]{mandt2016variational_SGD}
S.~Mandt, M.~D. Hoffman, and D.~M. Blei.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In \emph{ICML}, 2016{\natexlab{a}}.

\bibitem[Mandt et~al.(2016{\natexlab{b}})Mandt, McInerney, Abrol, Ranganath,
  and Blei]{mandt2016variational}
S.~Mandt, J.~McInerney, F.~Abrol, R.~Ranganath, and D.~Blei.
\newblock Variational tempering.
\newblock In \emph{AISTATS}, 2016{\natexlab{b}}.

\bibitem[Meng et~al.(2015)Meng, Zhao, and Jiang]{meng2015objective}
D.~Meng, Q.~Zhao, and L.~Jiang.
\newblock What objective does self-paced learning indeed optimize?
\newblock \emph{arXiv preprint arXiv:1511.06049}, 2015.

\bibitem[Mu et~al.(2016)Mu, Liu, Liu, and Fan]{mu2016stochastic}
Y.~Mu, W.~Liu, X.~Liu, and W.~Fan.
\newblock Stochastic gradient made stable: A manifold propagation approach for
  large-scale optimization.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2016.

\bibitem[Northcutt et~al.(2017)Northcutt, Wu, and
  Chuang]{northcutt2017learning}
C.~G. Northcutt, T.~Wu, and I.~L. Chuang.
\newblock Learning with confident examples: Rank pruning for robust
  classification with noisy labels.
\newblock \emph{arXiv preprint arXiv:1705.01936}, 2017.

\bibitem[Pi et~al.(2016)Pi, Li, Zhang, Meng, Wu, Xiao, and Zhuang]{pi2016self}
T.~Pi, X.~Li, Z.~Zhang, D.~Meng, F.~Wu, J.~Xiao, and Y.~Zhuang.
\newblock Self-paced boost learning for classification.
\newblock In \emph{IJCAI}, 2016.

\bibitem[Pregibon(1982)]{pregibon1982resistant}
D.~Pregibon.
\newblock Resistant fits for some commonly used logistic models with medical
  applications.
\newblock \emph{Biometrics}, pages 485--498, 1982.

\bibitem[Qian(1999)]{qian1999momentum}
N.~Qian.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \emph{Neural networks}, 12\penalty0 (1):\penalty0 145--151, 1999.

\bibitem[Rennie(2005)]{rennie2005regularized}
J.~D. Rennie.
\newblock Regularized logistic regression is strictly convex.
\newblock \emph{Unpublished manuscript. URL:
  \url{people.csail.mit.edu/jrennie/writing/convexLR.pdf}}, 2005.

\bibitem[Schaul et~al.(2013)Schaul, Zhang, and LeCun]{schaul2013no}
T.~Schaul, S.~Zhang, and Y.~LeCun.
\newblock No more pesky learning rates.
\newblock \emph{ICML}, 2013.

\bibitem[Schein and Ungar(2007)]{schein2007active}
A.~I. Schein and L.~H. Ungar.
\newblock Active learning for logistic regression: an evaluation.
\newblock \emph{Machine Learning}, 68\penalty0 (3):\penalty0 235--265, 2007.

\bibitem[Schohn and Cohn(2000)]{schohn2000less}
G.~Schohn and D.~Cohn.
\newblock Less is more: Active learning with support vector machines.
\newblock In \emph{ICML}, 2000.

\bibitem[Settles(2010)]{settles2010active}
B.~Settles.
\newblock Active learning literature survey.
\newblock \emph{University of Wisconsin, Madison}, 52\penalty0
  (55-66):\penalty0 11, 2010.

\bibitem[Shrivastava et~al.(2016)Shrivastava, Gupta, and
  Girshick]{shrivastava2016training}
A.~Shrivastava, A.~Gupta, and R.~Girshick.
\newblock Training region-based object detectors with online hard example
  mining.
\newblock In \emph{CVPR}, 2016.

\bibitem[Strubell et~al.(2017)Strubell, Verga, Belanger, and
  McCallum]{strubell2017fast}
E.~Strubell, P.~Verga, D.~Belanger, and A.~McCallum.
\newblock Fast and accurate sequence labeling with iterated dilated
  convolutions.
\newblock \emph{arXiv preprint arXiv:1702.02098}, 2017.

\bibitem[Tjong Kim~Sang and De~Meulder(2003)]{tjong2003introduction}
E.~F. Tjong Kim~Sang and F.~De~Meulder.
\newblock Introduction to the conll-2003 shared task: Language-independent
  named entity recognition.
\newblock In \emph{HLT-NAACL}, 2003.

\bibitem[Wang et~al.(2013)Wang, Chen, Smola, and Xing]{wang2013variance}
C.~Wang, X.~Chen, A.~J. Smola, and E.~P. Xing.
\newblock Variance reduction for stochastic gradient optimization.
\newblock In \emph{NIPS}, 2013.

\bibitem[Wang et~al.(2016)Wang, Kucukelbir, and Blei]{wang2016reweighted}
Y.~Wang, A.~Kucukelbir, and D.~M. Blei.
\newblock Reweighted data for robust probabilistic models.
\newblock \emph{arXiv preprint arXiv:1606.03860}, 2016.

\bibitem[Xiao and Zhang(2014)]{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zhao and Zhang(2014)]{ZhaoZ14}
P.~Zhao and T.~Zhang.
\newblock Stochastic optimization with importance sampling.
\newblock \emph{arXiv preprint arXiv:1412.2753}, 2014.

\end{thebibliography}
