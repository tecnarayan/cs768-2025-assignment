\begin{thebibliography}{54}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{Raffel2019ExploringTL}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{ArXiv}, abs/1910.10683, 2019.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{Strubell2019EnergyAP}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock In \emph{ACL}, 2019.

\bibitem[Han et~al.(2016{\natexlab{a}})Han, Liu, Mao, Pu, Pedram, Horowitz, and
  Dally]{Han2016EIEEI}
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark Horowitz, and
  William~J. Dally.
\newblock Eie: Efficient inference engine on compressed deep neural network.
\newblock In \emph{ISCA}, 2016{\natexlab{a}}.

\bibitem[{Horowitz}(2014)]{6757323}
M.~{Horowitz}.
\newblock 1.1 computing's energy problem (and what we can do about it).
\newblock In \emph{ISSCC}, 2014.

\bibitem[Han et~al.(2015)Han, Pool, Tran, and Dally]{Han2015LearningBW}
Song Han, Jeff Pool, John Tran, and William~J. Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In \emph{NIPS}, 2015.

\bibitem[Han et~al.(2016{\natexlab{b}})Han, Mao, and Dally]{Han2016DeepCC}
Song Han, Huizi Mao, and William~J. Dally.
\newblock Deep compression: Compressing deep neural network with pruning,
  trained quantization and huffman coding.
\newblock In \emph{ICLR}, 2016{\natexlab{b}}.

\bibitem[Guo et~al.(2016)Guo, Yao, and Chen]{Guo2016DynamicNS}
Yiwen Guo, Anbang Yao, and Yurong Chen.
\newblock Dynamic network surgery for efficient dnns.
\newblock In \emph{NIPS}, 2016.

\bibitem[Gale et~al.(2019)Gale, Elsen, and Hooker]{Gale2019TheSO}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock \emph{ArXiv}, abs/1902.09574, 2019.

\bibitem[Frankle et~al.(2020)Frankle, Dziugaite, Roy, and
  Carbin]{Frankle2019LinearMC}
Jonathan Frankle, G.~Dziugaite, D.~M. Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock In \emph{ICML}, 2020.

\bibitem[Bengio et~al.(2013)Bengio, L{\'e}onard, and
  Courville]{Bengio2013EstimatingOP}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron~C. Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock \emph{ArXiv}, abs/1308.3432, 2013.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017AttentionIA}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{NIPS}, 2017.

\bibitem[Louizos et~al.(2017)Louizos, Welling, and
  Kingma]{Louizos2017LearningSN}
Christos Louizos, Max Welling, and Diederik~P. Kingma.
\newblock Learning sparse neural networks through l0 regularization.
\newblock In \emph{ICLR}, 2017.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{N18-1101}
Adina Williams, Nikita Nangia, and Samuel Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{NAACL}, 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and
  Liang]{Rajpurkar2016SQuAD10}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Mallya and Lazebnik(2018)]{Mallya2018PiggybackAM}
Arun Mallya and Svetlana Lazebnik.
\newblock Piggyback: Adding multiple tasks to a single, fixed network by
  learning to mask.
\newblock \emph{ArXiv}, abs/1801.06519, 2018.

\bibitem[Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and
  Rastegari]{Ramanujan2019WhatsHI}
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and
  Mohammad Rastegari.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{CVPR}, 2020.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{LeCun1989OptimalBD}
Yann LeCun, John~S. Denker, and Sara~A. Solla.
\newblock Optimal brain damage.
\newblock In \emph{NIPS}, 1989.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{Hassibi1993OptimalBS}
Babak Hassibi, David~G. Stork, and Gregory~J. Wolff.
\newblock Optimal brain surgeon: Extensions and performance comparisons.
\newblock In \emph{NIPS}, 1993.

\bibitem[Theis et~al.(2018)Theis, Korshunova, Tejani, and
  Husz{\'a}r]{Theis2018FasterGP}
Lucas Theis, Iryna Korshunova, Alykhan Tejani, and Ferenc Husz{\'a}r.
\newblock Faster gaze prediction with dense networks and fisher pruning.
\newblock \emph{ArXiv}, abs/1801.05787, 2018.

\bibitem[Ding et~al.(2019)Ding, Ding, Zhou, Guo, Liu, and
  Han]{Ding2019GlobalSM}
Xiaohan Ding, Guiguang Ding, Xiangxin Zhou, Yuchen Guo, Ji~Liu, and Jungong
  Han.
\newblock Global sparse momentum sgd for pruning very deep neural networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Lee et~al.(2019)Lee, Ajanthan, and Torr]{Lee2019SNIPSN}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip H.~S. Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In \emph{ICLR}, 2019.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and
  Wolf]{Sanh2019DistilBERTAD}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock In \emph{NeurIPS EMC2 Workshop}, 2019.

\bibitem[Tang et~al.(2019)Tang, Lu, Liu, Mou, Vechtomova, and
  Lin]{Tang2019DistillingTK}
Raphael Tang, Yao Lu, Linqing Liu, Lili Mou, Olga Vechtomova, and Jimmy Lin.
\newblock Distilling task-specific knowledge from bert into simple neural
  networks.
\newblock \emph{ArXiv}, abs/1903.12136, 2019.

\bibitem[Fan et~al.(2020{\natexlab{a}})Fan, Grave, and
  Joulin]{Fan2019ReducingTD}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{ICLR}, 2020{\natexlab{a}}.

\bibitem[Sajjad et~al.(2020)Sajjad, Dalvi, Durrani, and
  Nakov]{Sajjad2020PoorMB}
Hassan Sajjad, Fahim Dalvi, Nadir Durrani, and Preslav Nakov.
\newblock Poor man's bert: Smaller and faster transformer models.
\newblock \emph{ArXiv}, abs/2004.03844, 2020.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{Michel2019AreSH}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Wang et~al.(2019)Wang, Wohlwend, and Lei]{Wang2019StructuredPO}
Ziheng Wang, Jeremy Wohlwend, and Tao Lei.
\newblock Structured pruning of large language models.
\newblock \emph{ArXiv}, abs/1910.04732, 2019.

\bibitem[Yu et~al.(2020)Yu, Edunov, Tian, and Morcos]{Yu2019PlayingTL}
Haonan Yu, Sergey Edunov, Yuandong Tian, and Ari~S. Morcos.
\newblock Playing the lottery with rewards and multiple languages: lottery
  tickets in rl and nlp.
\newblock In \emph{ICLR}, 2020.

\bibitem[Dettmers and Zettlemoyer(2019)]{Dettmers2019SparseNF}
Tim Dettmers and L.~Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock \emph{ArXiv}, abs/1907.04840, 2019.

\bibitem[Fan et~al.(2020{\natexlab{b}})Fan, Stock, Graham, Grave, Gribonval,
  J{\'e}gou, and Joulin]{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herv{\'e} J{\'e}gou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock \emph{ArXiv}, abs/2004.07320, 2020{\natexlab{b}}.

\bibitem[Zafrir et~al.(2019)Zafrir, Boudoukh, Izsak, and
  Wasserblat]{Zafrir2019Q8BERTQ8}
Ofir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe Wasserblat.
\newblock Q8bert: Quantized 8bit bert.
\newblock In \emph{NeurIPS EMC2 Workshop}, 2019.

\bibitem[Gong et~al.(2014)Gong, Liu, Yang, and Bourdev]{Gong2014CompressingDC}
Yunchao Gong, Liu Liu, Ming Yang, and Lubomir~D. Bourdev.
\newblock Compressing deep convolutional networks using vector quantization.
\newblock \emph{ArXiv}, abs/1412.6115, 2014.

\bibitem[Li et~al.(2020)Li, Wallace, Shen, Lin, Keutzer, Klein, and
  Gonzalez]{Li2020TrainLT}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joseph~E. Gonzalez.
\newblock Train large, then compress: Rethinking model size for efficient
  training and inference of transformers.
\newblock In \emph{ICML}, 2020.

\bibitem[Zhu and Gupta(2018)]{Zhu2018ToPO}
Michael Zhu and Suyog Gupta.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock In \emph{ICLR}, 2018.

\bibitem[Gordon et~al.(2020)Gordon, Duh, and Andrews]{Gordon2020CompressingBS}
Mitchell~A. Gordon, Kevin Duh, and Nicholas Andrews.
\newblock Compressing bert: Studying the effects of weight pruning on transfer
  learning.
\newblock In \emph{RepL4NLP@ACL}, 2020.

\bibitem[Ruder et~al.(2019)Ruder, Peters, Swayamdipta, and
  Wolf]{ruder-etal-2019-transfer}
Sebastian Ruder, Matthew~E. Peters, Swabha Swayamdipta, and Thomas Wolf.
\newblock Transfer learning in natural language processing.
\newblock In \emph{NAACL}, 2019.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{ArXiv}, abs/1907.11692, 2019.

\bibitem[Iyer et~al.(2017)Iyer, Dandekar, and Csernai]{WinNT}
Shankar Iyer, Nikhil Dandekar, and Kornel Csernai.
\newblock First quora dataset release: Question pairs, 2017.
\newblock URL
  \url{https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs}.

\bibitem[Guo et~al.(2019)Guo, Liu, Mungall, Lin, and Wang]{Guo2019ReweightedPP}
Fu-Ming Guo, Sijia Liu, Finlay~S. Mungall, Xue~Lian Lin, and Yanzhi Wang.
\newblock Reweighted proximal pruning for large-scale language representation.
\newblock \emph{ArXiv}, abs/1909.12486, 2019.

\bibitem[Parikh and Boyd(2014)]{Parikh2014ProximalA}
Neal Parikh and Stephen~P. Boyd.
\newblock Proximal algorithms.
\newblock \emph{Found. Trends Optim.}, 1:\penalty0 127--239, 2014.

\bibitem[Turc et~al.(2019)Turc, Chang, Lee, and Toutanova]{Turc2019WellReadSL}
Iulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Well-read students learn better: The impact of student initialization
  on knowledge distillation.
\newblock \emph{ArXiv}, abs/1908.08962, 2019.

\bibitem[McCarley(2019)]{McCarley2019PruningAB}
J.~Scott McCarley.
\newblock Pruning a bert-based question answering model.
\newblock \emph{ArXiv}, abs/1910.06360, 2019.

\bibitem[Bucila et~al.(2006)Bucila, Caruana, and
  Niculescu-Mizil]{Bucila2006ModelC}
Cristian Bucila, Rich Caruana, and Alexandru Niculescu-Mizil.
\newblock Model compression.
\newblock In \emph{KDD}, 2006.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{Hinton2015DistillingTK}
Geoffrey~E. Hinton, Oriol Vinyals, and Jeffrey Dean.
\newblock Distilling the knowledge in a neural network.
\newblock In \emph{NIPS}, 2014.

\bibitem[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{Jiao2019TinyBERTDB}
Xiaoqi Jiao, Y.~Yin, Lifeng Shang, Xin Jiang, Xusong Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{ArXiv}, abs/1909.10351, 2019.

\bibitem[He et~al.(2018)He, Lin, Liu, Wang, Li, and Han]{He2018AMCAF}
Yihui He, Ji~Lin, Zhijian Liu, Hanrui Wang, Li-Jia Li, and Song Han.
\newblock Amc: Automl for model compression and acceleration on mobile devices.
\newblock In \emph{ECCV}, 2018.

\bibitem[Bach et~al.(2011)Bach, Jenatton, Mairal, and
  Obozinski]{BachSparsity2011}
Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski.
\newblock Structured sparsity through convex optimization.
\newblock \emph{Statistical Science}, 27, 09 2011.
\newblock \doi{10.1214/12-STS394}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv}, abs/2005.14165, 2020.

\bibitem[Lu et~al.(2018)Lu, Mardziel, Wu, Amancharla, and
  Datta]{Lu2018GenderBI}
Kaiji Lu, Piotr Mardziel, Fangjing Wu, Preetam Amancharla, and Anupam Datta.
\newblock Gender bias in neural natural language processing.
\newblock \emph{ArXiv}, abs/1807.11714, 2018.

\bibitem[Vig et~al.(2020)Vig, Gehrmann, Belinkov, Qian, Nevo, Singer, and
  Shieber]{Vig2020CausalMA}
Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo,
  Yaron Singer, and Stuart~M. Shieber.
\newblock Causal mediation analysis for interpreting neural nlp: The case of
  gender bias.
\newblock \emph{ArXiv}, abs/2004.12265, 2020.

\bibitem[Wallace et~al.(2019)Wallace, Feng, Kandpal, Gardner, and
  Singh]{Wallace2019UniversalAT}
Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
\newblock Universal adversarial triggers for attacking and analyzing nlp.
\newblock In \emph{EMNLP/IJCNLP}, 2019.

\bibitem[Wolf et~al.(2019)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, and Brew]{Wolf2019HuggingFacesTS}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, R'emi Louf, Morgan Funtowicz, and
  Jamie Brew.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{ArXiv}, abs/1910.03771, 2019.

\end{thebibliography}
