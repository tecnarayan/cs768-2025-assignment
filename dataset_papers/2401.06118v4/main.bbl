\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Babenko \& Lempitsky(2014)Babenko and Lempitsky]{aq}
Babenko, A. and Lempitsky, V.
\newblock Additive quantization for extreme vector compression.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  931--938, 2014.

\bibitem[Besag(1986)]{besag1986statistical}
Besag, J.
\newblock On the statistical analysis of dirty pictures.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 48\penalty0 (3):\penalty0 259--279, 1986.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O'Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O'Brien, K., Hallahan, E., Khan, M.~A., Purohit, S., Prashanth, U.~S., Raff, E., et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock \emph{arXiv preprint arXiv:2304.01373}, 2023.

\bibitem[Blalock \& Guttag(2021)Blalock and Guttag]{maddness2021}
Blalock, D. and Guttag, J.
\newblock Multiplying matrices without multiplying.
\newblock In \emph{International Conference on Machine Learning}, pp.\  992--1004. PMLR, 2021.

\bibitem[Burton et~al.(1983)Burton, Shore, and Buck]{vq1}
Burton, D., Shore, J., and Buck, J.
\newblock A generalization of isolated word recognition using vector quantization.
\newblock In \emph{ICASSP '83. IEEE International Conference on Acoustics, Speech, and Signal Processing}, volume~8, pp.\  1021--1024, 1983.
\newblock \doi{10.1109/ICASSP.1983.1171915}.

\bibitem[Chee et~al.(2023)Chee, Cai, Kuleshov, and Sa]{chee2023quip}
Chee, J., Cai, Y., Kuleshov, V., and Sa, C.~D.
\newblock Quip: 2-bit quantization of large language models with guarantees, 2023.

\bibitem[Chen et~al.(2019)Chen, Wang, and Pan]{Chen_Wang_Pan_2019}
Chen, S., Wang, W., and Pan, S.~J.
\newblock Deep neural network quantization via layer-wise optimization using limited training data.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, 33\penalty0 (01):\penalty0 3329--3336, Jul. 2019.
\newblock \doi{10.1609/aaai.v33i01.33013329}.
\newblock URL \url{https://ojs.aaai.org/index.php/AAAI/article/view/4206}.

\bibitem[Chen et~al.(2010)Chen, Guan, and Wang]{rvq}
Chen, Y., Guan, T., and Wang, C.
\newblock Approximate nearest neighbor search by residual vector quantization.
\newblock \emph{Sensors}, 10\penalty0 (12):\penalty0 11259--11273, 2010.

\bibitem[Clark et~al.(2018)Clark, Cowhey, Etzioni, Khot, Sabharwal, Schoenick, and Tafjord]{arc_allenai}
Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O.
\newblock Think you have solved question answering? try arc, the ai2 reasoning challenge.
\newblock \emph{arXiv preprint arXiv:1803.05457}, 2018.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{CoRR}, abs/2110.14168, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Computer(2023)]{together2023redpajama}
Computer, T.
\newblock Redpajama: an open dataset for training large language models, 2023.
\newblock URL \url{https://github.com/togethercomputer/RedPajama-Data}.

\bibitem[Dettmers \& Zettlemoyer(2022)Dettmers and Zettlemoyer]{dettmers2022case}
Dettmers, T. and Zettlemoyer, L.
\newblock The case for 4-bit precision: k-bit inference scaling laws.
\newblock \emph{arXiv preprint arXiv:2212.09720}, 2022.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{dettmers2022llm}
Dettmers, T., Lewis, M., Belkada, Y., and Zettlemoyer, L.
\newblock {LLM}.int8(): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022}, 2022.

\bibitem[Dettmers et~al.(2023{\natexlab{a}})Dettmers, Pagnoni, Holtzman, and Zettlemoyer]{dettmers2023qlora}
Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.
\newblock {QLoRA}: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}, 2023{\natexlab{a}}.

\bibitem[Dettmers et~al.(2023{\natexlab{b}})Dettmers, Svirschevski, Egiazarian, Kuznedelev, Frantar, Ashkboos, Borzunov, Hoefler, and Alistarh]{dettmers2023spqr}
Dettmers, T., Svirschevski, R., Egiazarian, V., Kuznedelev, D., Frantar, E., Ashkboos, S., Borzunov, A., Hoefler, T., and Alistarh, D.
\newblock Spqr: A sparse-quantized representation for near-lossless llm weight compression.
\newblock \emph{arXiv preprint arXiv:2306.03078}, 2023{\natexlab{b}}.

\bibitem[Fern{\'a}ndez-Marqu{\'e}s et~al.(2023)Fern{\'a}ndez-Marqu{\'e}s, AbouElhamayed, Lane, and Abdelfattah]{FernndezMarqus2023AreWT}
Fern{\'a}ndez-Marqu{\'e}s, J., AbouElhamayed, A.~F., Lane, N.~D., and Abdelfattah, M.~S.
\newblock Are we there yet? product quantization and its hardware acceleration.
\newblock \emph{ArXiv}, abs/2305.18334, 2023.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:258967539}.

\bibitem[Frantar \& Alistarh(2023)Frantar and Alistarh]{frantar2023qmoe}
Frantar, E. and Alistarh, D.
\newblock Qmoe: Practical sub-1-bit compression of trillion-parameter models.
\newblock \emph{arXiv preprint arXiv:2310.16795}, 2023.

\bibitem[Frantar et~al.(2022{\natexlab{a}})Frantar, Ashkboos, Hoefler, and Alistarh]{frantar2022gptq}
Frantar, E., Ashkboos, S., Hoefler, T., and Alistarh, D.
\newblock Gptq: Accurate post-training quantization for generative pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2210.17323}, 2022{\natexlab{a}}.

\bibitem[Frantar et~al.(2022{\natexlab{b}})Frantar, Singh, and Alistarh]{frantar2022obc}
Frantar, E., Singh, S.~P., and Alistarh, D.
\newblock {Optimal Brain Compression}: A framework for accurate post-training quantization and pruning.
\newblock \emph{arXiv preprint arXiv:2208.11580}, 2022{\natexlab{b}}.
\newblock Accepted to NeurIPS 2022, to appear.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and Zou]{eval-harness}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Ge et~al.(2013)Ge, He, Ke, and Sun]{opq}
Ge, T., He, K., Ke, Q., and Sun, J.
\newblock Optimized product quantization.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 36\penalty0 (4):\penalty0 744--755, 2013.

\bibitem[Gholami et~al.(2021)Gholami, Kim, Dong, Yao, Mahoney, and Keutzer]{gholami2021survey}
Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M.~W., and Keutzer, K.
\newblock A survey of quantization methods for efficient neural network inference.
\newblock \emph{arXiv preprint arXiv:2103.13630}, 2021.

\bibitem[Gray(1984)]{vq2}
Gray, R.
\newblock Vector quantization.
\newblock \emph{IEEE ASSP Magazine}, 1\penalty0 (2):\penalty0 4--29, 1984.
\newblock \doi{10.1109/MASSP.1984.1162229}.

\bibitem[Guo et~al.(2016)Guo, Kumar, Choromanski, and Simcha]{guo2016quantization}
Guo, R., Kumar, S., Choromanski, K., and Simcha, D.
\newblock Quantization based fast inner product search.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  482--490. PMLR, 2016.

\bibitem[Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock \emph{CoRR}, abs/2009.03300, 2020.
\newblock URL \url{https://arxiv.org/abs/2009.03300}.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network, 2015.

\bibitem[Jegou et~al.(2010)Jegou, Douze, and Schmid]{pq}
Jegou, H., Douze, M., and Schmid, C.
\newblock Product quantization for nearest neighbor search.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 33\penalty0 (1):\penalty0 117--128, 2010.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
Jiang, A.~Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Hanna, E.~B., Bressand, F., et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kim et~al.(2023)Kim, Hooper, Gholami, Dong, Li, Shen, Mahoney, and Keutzer]{kim2023squeezellm}
Kim, S., Hooper, C., Gholami, A., Dong, Z., Li, X., Shen, S., Mahoney, M.~W., and Keutzer, K.
\newblock Squeezellm: Dense-and-sparse quantization.
\newblock \emph{arXiv preprint arXiv:2306.07629}, 2023.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2015.

\bibitem[Li et~al.(2017)Li, Ni, Zhang, Yang, and Gao]{li2017performance}
Li, Z., Ni, B., Zhang, W., Yang, X., and Gao, W.
\newblock Performance guaranteed network acceleration via high-order residual quantization, 2017.

\bibitem[Lin et~al.(2023)Lin, Tang, Tang, Yang, Dang, and Han]{lin2023awq}
Lin, J., Tang, J., Tang, H., Yang, S., Dang, X., and Han, S.
\newblock Awq: Activation-aware weight quantization for llm compression and acceleration.
\newblock \emph{arXiv preprint arXiv:2306.00978}, 2023.

\bibitem[Malinovskii et~al.(2024)Malinovskii, Mazur, Ilin, Kuznedelev, Burlachenko, Yi, Alistarh, and Richtarik]{malinovskii2024pv}
Malinovskii, V., Mazur, D., Ilin, I., Kuznedelev, D., Burlachenko, K., Yi, K., Alistarh, D., and Richtarik, P.
\newblock Pv-tuning: Beyond straight-through estimation for extreme llm compression.
\newblock \emph{arXiv preprint arXiv:2405.14852}, 2024.

\bibitem[Martinez et~al.(2016)Martinez, Clement, Hoos, and Little]{lsq}
Martinez, J., Clement, J., Hoos, H.~H., and Little, J.~J.
\newblock Revisiting additive quantization.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14}, pp.\  137--153. Springer, 2016.

\bibitem[Martinez et~al.(2018)Martinez, Zakhmi, Hoos, and Little]{lsq++}
Martinez, J., Zakhmi, S., Hoos, H.~H., and Little, J.~J.
\newblock Lsq++: Lower running time and higher recall in multi-codebook quantization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision (ECCV)}, pp.\  491--506, 2018.

\bibitem[McCarter \& Dronen(2022)McCarter and Dronen]{McCarter2022LookupsAN}
McCarter, C. and Dronen, N.
\newblock Look-ups are not (yet) all you need for deep learning inference.
\newblock \emph{ArXiv}, abs/2207.05808, 2022.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:250491319}.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and Socher]{wikitext103}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Nagel et~al.(2020)Nagel, Amjad, Van~Baalen, Louizos, and Blankevoort]{nagel2020up}
Nagel, M., Amjad, R.~A., Van~Baalen, M., Louizos, C., and Blankevoort, T.
\newblock Up or down? {A}daptive rounding for post-training quantization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Norouzi \& Fleet(2013)Norouzi and Fleet]{norouzi13}
Norouzi, M. and Fleet, D.~J.
\newblock Cartesian k-means.
\newblock In \emph{Proceedings of the IEEE Conference on computer Vision and Pattern Recognition}, pp.\  3017--3024, 2013.

\bibitem[Ozan et~al.(2016)Ozan, Kiranyaz, and Gabbouj]{competitveq}
Ozan, E.~C., Kiranyaz, S., and Gabbouj, M.
\newblock Competitive quantization for approximate nearest neighbor search.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 28\penalty0 (11):\penalty0 2884--2894, 2016.
\newblock \doi{10.1109/TKDE.2016.2597834}.

\bibitem[Park et~al.(2022)Park, Park, Kwon, Kim, Lee, and Lee]{park2022nuqmm}
Park, G., Park, B., Kwon, S.~J., Kim, B., Lee, Y., and Lee, D.
\newblock {nuQmm}: Quantized matmul for efficient inference of large-scale generative language models.
\newblock \emph{arXiv preprint arXiv:2206.09557}, 2022.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.
\newblock {PyTorch}: An imperative style, high-performance deep learning library.
\newblock In \emph{Conference on Neural Information Processing Systems (NeurIPS)}. 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{C4}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi]{DBLP:journals/cacm/winogrande2021}
Sakaguchi, K., Bras, R.~L., Bhagavatula, C., and Choi, Y.
\newblock Winogrande: an adversarial winograd schema challenge at scale.
\newblock \emph{Commun. {ACM}}, 64\penalty0 (9):\penalty0 99--106, 2021.
\newblock \doi{10.1145/3474381}.
\newblock URL \url{https://doi.org/10.1145/3474381}.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon, Gall{\'e}, et~al.]{scao2022bloom}
Scao, T.~L., Fan, A., Akiki, C., Pavlick, E., Ili{\'c}, S., Hesslow, D., Castagn{\'e}, R., Luccioni, A.~S., Yvon, F., Gall{\'e}, M., et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}, 2022.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Shazeer, N.
\newblock Glu variants improve transformer, 2020.

\bibitem[Tata \& Patel(2003)Tata and Patel]{tata2003piqa}
Tata, S. and Patel, J.~M.
\newblock {PiQA}: An algebra for querying protein data sets.
\newblock In \emph{International Conference on Scientific and Statistical Database Management}, 2003.

\bibitem[{TII UAE}(2023)]{falcon2023}
{TII UAE}.
\newblock The {Falcon} family of large language models.
\newblock \url{https://huggingface.co/tiiuae/falcon-40b}, May 2023.

\bibitem[Touvron et~al.(2023)Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023.

\bibitem[Tseng et~al.(2024)Tseng, Chee, Sun, Kuleshov, and Sa]{quip-sharp}
Tseng, A., Chee, J., Sun, Q., Kuleshov, V., and Sa, C.~D.
\newblock Quip\#: Even better llm quantization with hadamard incoherence and lattice codebooks, 2024.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Xiao et~al.(2022)Xiao, Lin, Seznec, Demouth, and Han]{xiao2022smoothquant}
Xiao, G., Lin, J., Seznec, M., Demouth, J., and Han, S.
\newblock Smoothquant: Accurate and efficient post-training quantization for large language models.
\newblock \emph{arXiv preprint arXiv:2211.10438}, 2022.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and He]{yao2022zeroquant}
Yao, Z., Aminabadi, R.~Y., Zhang, M., Wu, X., Li, C., and He, Y.
\newblock Zeroquant: Efficient and affordable post-training quantization for large-scale transformers.
\newblock \emph{arXiv preprint arXiv:2206.01861}, 2022.

\bibitem[Zellers et~al.(2019)Zellers, Holtzman, Bisk, Farhadi, and Choi]{DBLP:conf/acl/hellaswag2019}
Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock In Korhonen, A., Traum, D.~R., and M{\`{a}}rquez, L. (eds.), \emph{Proceedings of the 57th Conference of the Association for Computational Linguistics, {ACL} 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers}, pp.\  4791--4800. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/p19-1472}.
\newblock URL \url{https://doi.org/10.18653/v1/p19-1472}.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{rmsnorm}
Zhang, B. and Sennrich, R.
\newblock Root mean square layer normalization.
\newblock \emph{CoRR}, abs/1910.07467, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.07467}.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin, et~al.]{zhang2022opt}
Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.~V., et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\bibitem[Zhang et~al.(2014)Zhang, Du, and Wang]{compositeq}
Zhang, T., Du, C., and Wang, J.
\newblock Composite quantization for approximate nearest neighbor search.
\newblock In \emph{International Conference on Machine Learning}, pp.\  838--846. PMLR, 2014.

\bibitem[Zhou et~al.(2017)Zhou, Wang, Wen, He, and Zou]{Zhou2017}
Zhou, S.-C., Wang, Y.-Z., Wen, H., He, Q.-Y., and Zou, Y.-H.
\newblock Balanced quantization: An effective and efficient approach to quantized neural networks.
\newblock \emph{Journal of Computer Science and Technology}, 32\penalty0 (4):\penalty0 667--682, Jul 2017.
\newblock ISSN 1860-4749.
\newblock \doi{10.1007/s11390-017-1750-y}.
\newblock URL \url{https://doi.org/10.1007/s11390-017-1750-y}.

\end{thebibliography}
