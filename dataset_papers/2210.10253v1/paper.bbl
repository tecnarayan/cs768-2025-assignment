\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2019)Alayrac, Uesato, Huang, Fawzi, Stanforth, and
  Kohli]{alayrac2019labels}
J.-B. Alayrac, J.~Uesato, P.-S. Huang, A.~Fawzi, R.~Stanforth, and P.~Kohli.
\newblock Are labels required for improving adversarial robustness?
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Allingham et~al.(2021)Allingham, Wenzel, Mariet, Mustafa, Puigcerver,
  Houlsby, Jerfel, Fortuin, Lakshminarayanan, Snoek, Tran, Ruiz, and
  Jenatton]{allingham2021}
J.~U. Allingham, F.~Wenzel, Z.~E. Mariet, B.~Mustafa, J.~Puigcerver,
  N.~Houlsby, G.~Jerfel, V.~Fortuin, B.~Lakshminarayanan, J.~Snoek, D.~Tran,
  C.~R. Ruiz, and R.~Jenatton.
\newblock Sparse moes meet efficient ensembles.
\newblock \emph{CoRR}, abs/2110.03360, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.03360}.

\bibitem[Bubeck and Sellke(2021)]{bubeck2021universal}
S.~Bubeck and M.~Sellke.
\newblock A universal law of robustness via isoperimetry.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,
  Tsipras, Goodfellow, Madry, and Kurakin]{carlini2019evaluating}
N.~Carlini, A.~Athalye, N.~Papernot, W.~Brendel, J.~Rauber, D.~Tsipras,
  I.~Goodfellow, A.~Madry, and A.~Kurakin.
\newblock On evaluating adversarial robustness.
\newblock \emph{arXiv preprint arXiv:1902.06705}, 2019.

\bibitem[Croce and Hein(2020)]{pmlr-v119-croce20b}
F.~Croce and M.~Hein.
\newblock Reliable evaluation of adversarial robustness with an ensemble of
  diverse parameter-free attacks.
\newblock In H.~D. III and A.~Singh, editors, \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pages 2206--2216. PMLR,
  13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/croce20b.html}.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.-J. Li, K.~Li, and L.~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255. Ieee, 2009.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{vit2020}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, et~al.
\newblock {An image is worth 16x16 words: Transformers for image recognition at
  scale}.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, et~al.]{du2022glam}
N.~Du, Y.~Huang, A.~M. Dai, S.~Tong, D.~Lepikhin, Y.~Xu, M.~Krikun, Y.~Zhou,
  A.~W. Yu, O.~Firat, et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5547--5569. PMLR, 2022.

\bibitem[Eigen et~al.(2013)Eigen, Ranzato, and Sutskever]{eigen2013learning}
D.~Eigen, M.~Ranzato, and I.~Sutskever.
\newblock Learning factored representations in a deep mixture of experts.
\newblock \emph{arXiv preprint arXiv:1312.4314}, 2013.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{switch}
W.~Fedus, B.~Zoph, and N.~Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (120):\penalty0 1--39, 2022.
\newblock URL \url{http://jmlr.org/papers/v23/21-0998.html}.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and Szegedy]{fgsm}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Gowal et~al.(2020)Gowal, Qin, Uesato, Mann, and
  Kohli]{gowal2020uncovering}
S.~Gowal, C.~Qin, J.~Uesato, T.~Mann, and P.~Kohli.
\newblock Uncovering the limits of adversarial training against norm-bounded
  adversarial examples.
\newblock \emph{arXiv preprint arXiv:2010.03593}, 2020.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, Friedman, and
  Friedman]{hastie2009elements}
T.~Hastie, R.~Tibshirani, J.~H. Friedman, and J.~H. Friedman.
\newblock \emph{The elements of statistical learning: data mining, inference,
  and prediction}, volume~2.
\newblock Springer, 2009.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and
  Hinton]{jacobs1991adaptive}
R.~A. Jacobs, M.~I. Jordan, S.~J. Nowlan, and G.~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural computation}, 3\penalty0 (1):\penalty0 79--87, 1991.

\bibitem[Jordan and Jacobs(1994)]{jordan1994hierarchical}
M.~I. Jordan and R.~A. Jacobs.
\newblock Hierarchical mixtures of experts and the em algorithm.
\newblock \emph{Neural Computation}, 6\penalty0 (2):\penalty0 181--214, 1994.
\newblock \doi{10.1162/neco.1994.6.2.181}.

\bibitem[Kovanic(1979)]{kovanic1979pseudoinverse}
P.~Kovanic.
\newblock On the pseudoinverse of a sum of symmetric matrices with applications
  to estimation.
\newblock \emph{Kybernetika}, 15\penalty0 (5):\penalty0 341--348, 1979.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem[Lepikhin et~al.(2020)Lepikhin, Lee, Xu, Chen, Firat, Huang, Krikun,
  Shazeer, and Chen]{gshard2020}
D.~Lepikhin, H.~Lee, Y.~Xu, D.~Chen, O.~Firat, Y.~Huang, M.~Krikun, N.~Shazeer,
  and Z.~Chen.
\newblock {GShard: Scaling giant models with conditional computation and
  automatic sharding}.
\newblock \emph{arXiv preprint arXiv:2006.16668}, 2020.

\bibitem[Lewis et~al.(2021)Lewis, Bhosale, Dettmers, Goyal, and
  Zettlemoyer]{lewis2021base}
M.~Lewis, S.~Bhosale, T.~Dettmers, N.~Goyal, and L.~Zettlemoyer.
\newblock Base layers: Simplifying training of large, sparse models.
\newblock In M.~Meila and T.~Zhang, editors, \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 6265--6274. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/lewis21a.html}.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Patterson et~al.(2021)Patterson, Gonzalez, Le, Liang, Munguia,
  Rothchild, So, Texier, and Dean]{patterson2021carbon}
D.~Patterson, J.~Gonzalez, Q.~Le, C.~Liang, L.-M. Munguia, D.~Rothchild, D.~So,
  M.~Texier, and J.~Dean.
\newblock Carbon emissions and large neural network training.
\newblock \emph{arXiv preprint arXiv:2104.10350}, 2021.

\bibitem[Riquelme et~al.(2021)Riquelme, Puigcerver, Mustafa, Neumann, Jenatton,
  Susano~Pinto, Keysers, and Houlsby]{vmoe2021}
C.~Riquelme, J.~Puigcerver, B.~Mustafa, M.~Neumann, R.~Jenatton,
  A.~Susano~Pinto, D.~Keysers, and N.~Houlsby.
\newblock {Scaling Vision with Sparse Mixture of Experts}.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W.
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 8583--8595. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf}.

\bibitem[Shazeer et~al.(2017)Shazeer, Mirhoseini, Maziarz, Davis, Le, Hinton,
  and Dean]{shazeer2017outrageously}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~Le, G.~Hinton, and J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock \emph{arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{jft300m}
C.~Sun, A.~Shrivastava, S.~Singh, and A.~Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, Oct 2017.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and
  R.~Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Xie and Yuille(2020)]{xie2019intriguing}
C.~Xie and A.~Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Xue et~al.(2021)Xue, Shi, Wei, Lou, Liu, and You]{xue2021go}
F.~Xue, Z.~Shi, F.~Wei, Y.~Lou, Y.~Liu, and Y.~You.
\newblock Go wider instead of deeper.
\newblock \emph{arXiv preprint arXiv:2107.11817}, 2021.

\bibitem[Yuksel et~al.(2012)Yuksel, Wilson, and Gader]{Yuksel2012}
S.~E. Yuksel, J.~N. Wilson, and P.~D. Gader.
\newblock Twenty years of mixture of experts.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  23\penalty0 (8):\penalty0 1177--1193, 2012.
\newblock \doi{10.1109/TNNLS.2012.2200299}.

\end{thebibliography}
