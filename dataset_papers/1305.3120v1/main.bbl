\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Beck \& Teboulle(2009)Beck and Teboulle]{beck}
Beck, A. and Teboulle, M.
\newblock {A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems}.
\newblock \emph{SIAM J. Imaging Sci.}, 2\penalty0 (1):\penalty0 183--202, 2009.

\bibitem[Bertsekas(1999)]{bertsekas}
Bertsekas, D.P.
\newblock \emph{Nonlinear programming}.
\newblock Athena Scientific Belmont, 1999.
\newblock 2nd edition.

\bibitem[B{\"o}hning \& Lindsay(1988)B{\"o}hning and Lindsay]{bohning}
B{\"o}hning, D. and Lindsay, B.~G.
\newblock Monotonicity of quadratic-approximation algorithms.
\newblock \emph{Ann. I. Stat. Math.}, 40\penalty0 (4):\penalty0 641--663, 1988.

\bibitem[Borwein \& Lewis(2006)Borwein and Lewis]{borwein}
Borwein, J.M. and Lewis, A.S.
\newblock \emph{Convex analysis and nonlinear optimization: theory and
  examples}.
\newblock Springer, 2006.

\bibitem[Bottou(2010)]{bottou5}
Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{Proc. COMPSTAT}, 2010.

\bibitem[Bradley et~al.(2011)Bradley, Kyrola, Bickson, and Guestrin]{bradley2}
Bradley, J.K., Kyrola, A., Bickson, D., and Guestrin, C.
\newblock Parallel coordinate descent for l1-regularized loss minimization.
\newblock In \emph{Proc. ICML}, 2011.

\bibitem[Cand\`es et~al.(2008)Cand\`es, Wakin, and Boyd]{candes4}
Cand\`es, E.J., Wakin, M., and Boyd, S.P.
\newblock Enhancing sparsity by reweighted $\ell_1$ minimization.
\newblock \emph{J. Fourier Anal. Appl.}, 14\penalty0 (5):\penalty0 877--905,
  2008.

\bibitem[Collins et~al.(2002)Collins, Schapire, and Singer]{collins}
Collins, M., Schapire, R.E., and Singer, Y.
\newblock Logistic regression, {AdaBoost and Bregman} distances.
\newblock \emph{Mach. Learn.}, 48\penalty0 (1):\penalty0 253--285, 2002.


\bibitem[Daubechies et~al.(2004)Daubechies, Defrise, and {De Mol}]{daubechies}
Daubechies, I., Defrise, M., and {De Mol}, C.
\newblock An iterative thresholding algorithm for linear inverse problems with
  a sparsity constraint.
\newblock \emph{Commun. Pur. Appl. Math.}, 57\penalty0 (11):\penalty0
  1413--1457, 2004.

\bibitem[Della~Pietra et~al.(2001)Della~Pietra, Della~Pietra, and
  Lafferty]{pietra}
Della~Pietra, S., Della~Pietra, V., and Lafferty, J.
\newblock Duality and auxiliary functions for {B}regman distances.
\newblock Technical report, CMU-CS-01-109, 2001.

\bibitem[Fan et~al.(2008)Fan, Chang, Hsieh, Wang, and Lin]{fan2}
Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J.
\newblock {LIBLINEAR}: A library for large linear classification.
\newblock \emph{J. Mach. Learn. Res.}, 9:\penalty0 1871--1874, 2008.

\bibitem[Gasso et~al.(2009)Gasso, Rakotomamonjy, and Canu]{gasso}
Gasso, G., Rakotomamonjy, A., and Canu, S.
\newblock Recovering sparse signals with non-convex penalties and {DC}
  programming.
\newblock \emph{IEEE T. Signal Process.}, 57\penalty0 (12):\penalty0
  4686--4698, 2009.

\bibitem[Harchaoui et~al.(2013)Harchaoui, Juditsky, and Nemirovski]{harchaoui3}
Harchaoui, Z., Juditsky, A., and Nemirovski, A.
\newblock Conditional gradient algorithms for norm-regularized smooth convex
  optimization.
\newblock \emph{preprint arXiv:1302.2325v4}, 2013.

\bibitem[Hazan \& Kale(2012)Hazan and Kale]{hazan}
Hazan, E. and Kale, S.
\newblock Projection-free online learning.
\newblock In \emph{Proc. ICML}, 2012.

\bibitem[Horst \& Thoai(1999)Horst and Thoai]{horst}
Horst, R. and Thoai, N.V.
\newblock {DC} programming: overview.
\newblock \emph{J. Optim. Theory App.}, 103\penalty0 (1):\penalty0 1--43, 1999.

\bibitem[Juditsky \& Nemirovski(2011)Juditsky and Nemirovski]{juditsky}
Juditsky, A. and Nemirovski, A.
\newblock First order methods for nonsmooth convex large-scale optimization,
  {I}: General purpose methods.
\newblock In \emph{Optimization for Machine Learning}. MIT Press, 2011.

\bibitem[Khan et~al.(2010)Khan, Marlin, Bouchard, and Murphy]{khan}
Khan, E., Marlin, B., Bouchard, G., and Murphy, K.
\newblock Variational bounds for mixed-data factor analysis.
\newblock In \emph{Adv. NIPS}, 2010.

\bibitem[Lacoste-Julien et~al.(2013)Lacoste-Julien, Jaggi, Schmidt, and
  Pletscher]{lacoste}
Lacoste-Julien, S., Jaggi, M., Schmidt, M., and Pletscher, P.
\newblock Block-coordinate {Frank-Wolfe} optimization for structural {SVMs}.
\newblock In \emph{Proc. ICML}, 2013.

\bibitem[Lange et~al.(2000)Lange, Hunter, and Yang]{lange2}
Lange, K., Hunter, D.R., and Yang, I.
\newblock Optimization transfer using surrogate objective functions.
\newblock \emph{J. Comput. Graph. Stat.}, 9\penalty0 (1):\penalty0 1--20, 2000.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{leroux}
Le~Roux, N., Schmidt, M., and Bach, F.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In \emph{Adv. NIPS}, 2012.

\bibitem[Lee \& Seung(2001)Lee and Seung]{lee2}
Lee, D.D. and Seung, H.S.
\newblock Algorithms for non-negative matrix factorization.
\newblock In \emph{Adv. NIPS}, 2001.

\bibitem[Mairal et~al.(2010)Mairal, Bach, Ponce, and Sapiro]{mairal7}
Mairal, J., Bach, F., Ponce, J., and Sapiro, G.
\newblock Online learning for matrix factorization and sparse coding.
\newblock \emph{J. Mach. Learn. Res.}, 11:\penalty0 19--60, 2010.

\bibitem[Neal \& Hinton(1998)Neal and Hinton]{neal}
Neal, R.M. and Hinton, G.E.
\newblock {A view of the EM algorithm that justifies incremental, sparse, and
  other variants}.
\newblock \emph{Learning in graphical models}, 89:\penalty0 355--368, 1998.

\bibitem[Nesterov(2004)]{nesterov4}
Nesterov, Y.
\newblock \emph{Introductory lectures on convex optimization}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem[Nesterov(2007)]{nesterov}
Nesterov, Y.
\newblock Gradient methods for minimizing composite objective functions.
\newblock Technical report, CORE Discussion Paper, 2007.

\bibitem[Nesterov(2012)]{nesterov6}
Nesterov, Y.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock \emph{SIAM J. Optimiz.}, 22\penalty0 (2):\penalty0 341--362, 2012.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{nesterov7}
Nesterov, Y. and Polyak, B.T.
\newblock Cubic regularization of {N}ewton method and its global performance.
\newblock \emph{Math. Program.}, 108\penalty0 (1):\penalty0 177--205, 2006.

\bibitem[Richt{\'a}rik \& Tak{\'a}{\v{c}}(2012)Richt{\'a}rik and
  Tak{\'a}{\v{c}}]{richtarik}
Richt{\'a}rik, P. and Tak{\'a}{\v{c}}, M.
\newblock Iteration complexity of randomized block coordinate descent methods
  for minimizing a composite function.
\newblock \emph{Math. Program.}, 2012.

\bibitem[Seeger \& Wipf(2010)Seeger and Wipf]{seeger}
Seeger, M.W. and Wipf, D.P.
\newblock Variational {B}ayesian inference techniques.
\newblock \emph{IEEE Signal Proc. Mag.}, 27\penalty0 (6):\penalty0 81--91,
  2010.

\bibitem[Shalev-Schwartz \& Zhang(2012)Shalev-Schwartz and Zhang]{shalev2}
Shalev-Schwartz, S. and Zhang, T.
\newblock Proximal stochastic dual coordinate ascent.
\newblock \emph{preprint arXiv 1211.2717v1}, 2012.

\bibitem[Shalev-Shwartz \& Tewari(2009)Shalev-Shwartz and Tewari]{shalev}
Shalev-Shwartz, S. and Tewari, A.
\newblock Stochastic methods for $\ell_1$ regularized loss minimization.
\newblock In \emph{Proc. ICML}, 2009.

\bibitem[Tseng \& Yun(2009)Tseng and Yun]{tseng}
Tseng, P. and Yun, S.
\newblock A coordinate gradient descent method for nonsmooth separable
  minimization.
\newblock \emph{Math. Program.}, 117:\penalty0 387--423, 2009.

\bibitem[Wainwright \& Jordan(2008)Wainwright and Jordan]{wainwright2}
Wainwright, M.J. and Jordan, M.I.
\newblock Graphical models, exponential families, and variational inference.
\newblock \emph{Found. Trends Mach. Learn.}, 1\penalty0 (1-2):\penalty0 1--305,
  2008.

\bibitem[Wright et~al.(2009)Wright, Nowak, and Figueiredo]{wright}
Wright, S., Nowak, R., and Figueiredo, M.
\newblock Sparse reconstruction by separable approximation.
\newblock \emph{IEEE T. Signal Process.}, 57\penalty0 (7):\penalty0 2479--2493,
  2009.

\bibitem[Zhang(2003)]{zhang3}
Zhang, T.
\newblock Sequential greedy approximation for certain convex optimization
  problems.
\newblock \emph{IEEE T. Inform. Theory}, 49\penalty0 (3):\penalty0 682--691,
  2003.

\bibitem[Zhang et~al.(2012)Zhang, Yu, and Schuurmans]{zhang4}
Zhang, X., Yu, Y., and Schuurmans, D.
\newblock Accelerated training for matrix-norm regularization: a boosting
  approach.
\newblock In \emph{Adv. NIPS}, 2012.

\end{thebibliography}
