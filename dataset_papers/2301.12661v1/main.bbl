\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baevski et~al.(2020)Baevski, Zhou, Mohamed, and
  Auli]{baevski2020wav2vec}
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M.
\newblock wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Baevski et~al.(2022)Baevski, Hsu, Xu, Babu, Gu, and
  Auli]{baevski2022data2vec}
Baevski, A., Hsu, W.-N., Xu, Q., Babu, A., Gu, J., and Auli, M.
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock \emph{arXiv preprint arXiv:2202.03555}, 2022.

\bibitem[Benhamdi et~al.(2017)Benhamdi, Babouri, and
  Chiky]{benhamdi2017personalized}
Benhamdi, S., Babouri, A., and Chiky, R.
\newblock Personalized recommender system for e-learning environment.
\newblock \emph{Education and Information Technologies}, 22\penalty0
  (4):\penalty0 1455--1477, 2017.

\bibitem[Bittner et~al.(2014)Bittner, Salamon, Tierney, Mauch, Cannam, and
  Bello]{bittner2014medleydb}
Bittner, R.~M., Salamon, J., Tierney, M., Mauch, M., Cannam, C., and Bello,
  J.~P.
\newblock Medleydb: A multitrack dataset for annotation-intensive mir research.
\newblock In \emph{ISMIR}, volume~14, pp.\  155--160, 2014.

\bibitem[Borsos et~al.(2022)Borsos, Marinier, Vincent, Kharitonov, Pietquin,
  Sharifi, Teboul, Grangier, Tagliasacchi, and Zeghidour]{borsos2022audiolm}
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi,
  M., Teboul, O., Grangier, D., Tagliasacchi, M., and Zeghidour, N.
\newblock Audiolm: a language modeling approach to audio generation.
\newblock \emph{arXiv preprint arXiv:2209.03143}, 2022.

\bibitem[Chen et~al.(2020{\natexlab{a}})Chen, Xie, Vedaldi, and
  Zisserman]{chen2020vggsound}
Chen, H., Xie, W., Vedaldi, A., and Zisserman, A.
\newblock Vggsound: A large-scale audio-visual dataset.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  721--725. IEEE,
  2020{\natexlab{a}}.

\bibitem[Chen et~al.(2020{\natexlab{b}})Chen, Tan, Li, Liu, Qin, Liu,
  et~al.]{chen2021adaspeech}
Chen, M., Tan, X., Li, B., Liu, Y., Qin, T., Liu, T.-Y., et~al.
\newblock Adaspeech: Adaptive text to speech for custom voice.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{b}}.

\bibitem[Deshmukh et~al.(2022)Deshmukh, Elizalde, and Wang]{deshmukh2022audio}
Deshmukh, S., Elizalde, B., and Wang, H.
\newblock Audio retrieval with wavtext5k and clap training.
\newblock \emph{arXiv preprint arXiv:2209.14275}, 2022.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dhariwal \& Nichol(2021)Dhariwal and Nichol]{dhariwal2021diffusion}
Dhariwal, P. and Nichol, A.
\newblock Diffusion models beat gans on image synthesis.
\newblock In \emph{Proc. of NeurIPS}, volume~34, 2021.

\bibitem[Ding et~al.(2022)Ding, Zheng, Hong, and Tang]{ding2022cogview2}
Ding, M., Zheng, W., Hong, W., and Tang, J.
\newblock Cogview2: Faster and better text-to-image generation via hierarchical
  transformers.
\newblock \emph{arXiv preprint arXiv:2204.14217}, 2022.

\bibitem[Drossos et~al.(2020)Drossos, Lipping, and Virtanen]{drossos2020clotho}
Drossos, K., Lipping, S., and Virtanen, T.
\newblock Clotho: An audio captioning dataset.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pp.\  736--740. IEEE, 2020.

\bibitem[Elizalde et~al.(2022)Elizalde, Deshmukh, Ismail, and
  Wang]{elizalde2022clap}
Elizalde, B., Deshmukh, S., Ismail, M.~A., and Wang, H.
\newblock Clap: Learning audio concepts from natural language supervision.
\newblock \emph{arXiv preprint arXiv:2206.04769}, 2022.

\bibitem[Gal et~al.(2022)Gal, Alaluf, Atzmon, Patashnik, Bermano, Chechik, and
  Cohen-Or]{gal2022image}
Gal, R., Alaluf, Y., Atzmon, Y., Patashnik, O., Bermano, A.~H., Chechik, G.,
  and Cohen-Or, D.
\newblock An image is worth one word: Personalizing text-to-image generation
  using textual inversion.
\newblock \emph{arXiv preprint arXiv:2208.01618}, 2022.

\bibitem[Gan et~al.(2020)Gan, Huang, Chen, Tenenbaum, and
  Torralba]{gan2020foley}
Gan, C., Huang, D., Chen, P., Tenenbaum, J.~B., and Torralba, A.
\newblock Foley music: Learning to generate music from videos.
\newblock In \emph{European Conference on Computer Vision}, pp.\  758--775.
  Springer, 2020.

\bibitem[Gemmeke et~al.(2017)Gemmeke, Ellis, Freedman, Jansen, Lawrence, Moore,
  Plakal, and Ritter]{gemmeke2017audio}
Gemmeke, J.~F., Ellis, D.~P., Freedman, D., Jansen, A., Lawrence, W., Moore,
  R.~C., Plakal, M., and Ritter, M.
\newblock Audio set: An ontology and human-labeled dataset for audio events.
\newblock In \emph{2017 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pp.\  776--780. IEEE, 2017.

\bibitem[Gong et~al.(2022)Gong, Lai, Chung, and Glass]{gong2022ssast}
Gong, Y., Lai, C.-I., Chung, Y.-A., and Glass, J.
\newblock Ssast: Self-supervised audio spectrogram transformer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  10699--10709, 2022.

\bibitem[Goodfellow et~al.(2020)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2020generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial networks.
\newblock \emph{Communications of the ACM}, 63\penalty0 (11):\penalty0
  139--144, 2020.

\bibitem[He et~al.(2022)He, Chen, Xie, Li, Doll{\'a}r, and
  Girshick]{he2022masked}
He, K., Chen, X., Xie, S., Li, Y., Doll{\'a}r, P., and Girshick, R.
\newblock Masked autoencoders are scalable vision learners.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  16000--16009, 2022.

\bibitem[Hessel et~al.(2021)Hessel, Holtzman, Forbes, Bras, and
  Choi]{hessel2021clipscore}
Hessel, J., Holtzman, A., Forbes, M., Bras, R.~L., and Choi, Y.
\newblock Clipscore: A reference-free evaluation metric for image captioning.
\newblock \emph{arXiv preprint arXiv:2104.08718}, 2021.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Ho \& Salimans(2022)Ho and Salimans]{ho2022classifier}
Ho, J. and Salimans, T.
\newblock Classifier-free diffusion guidance.
\newblock \emph{arXiv preprint arXiv:2207.12598}, 2022.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Proc. of NeurIPS}, 2020.

\bibitem[Hong et~al.(2022)Hong, Ding, Zheng, Liu, and Tang]{hong2022cogvideo}
Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J.
\newblock Cogvideo: Large-scale pretraining for text-to-video generation via
  transformers.
\newblock \emph{arXiv preprint arXiv:2205.15868}, 2022.

\bibitem[Hsu et~al.(2020)Hsu, Harwath, Song, and Glass]{hsu2020text}
Hsu, W.-N., Harwath, D., Song, C., and Glass, J.
\newblock Text-free image-to-speech synthesis using learned segmental units.
\newblock \emph{arXiv preprint arXiv:2012.15454}, 2020.

\bibitem[Hsu et~al.(2021)Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov, and
  Mohamed]{hsu2021hubert}
Hsu, W.-N., Bolte, B., Tsai, Y.-H.~H., Lakhotia, K., Salakhutdinov, R., and
  Mohamed, A.
\newblock Hubert: Self-supervised speech representation learning by masked
  prediction of hidden units.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29:\penalty0 3451--3460, 2021.

\bibitem[Huang et~al.(2022)Huang, Ren, Liu, Cui, and
  Zhao]{huang2022generspeech}
Huang, R., Ren, Y., Liu, J., Cui, C., and Zhao, Z.
\newblock Generspeech: Towards style transfer for generalizable out-of-domain
  text-to-speech synthesis.
\newblock \emph{arXiv preprint arXiv:2205.07211}, 2022.

\bibitem[Iashin \& Rahtu(2021)Iashin and Rahtu]{iashin2021taming}
Iashin, V. and Rahtu, E.
\newblock Taming visually guided sound generation.
\newblock \emph{arXiv preprint arXiv:2110.08791}, 2021.

\bibitem[Kim et~al.(2019)Kim, Kim, Lee, and Kim]{kim2019audiocaps}
Kim, C.~D., Kim, B., Lee, H., and Kim, G.
\newblock Audiocaps: Generating captions for audios in the wild.
\newblock In \emph{Proceedings of the 2019 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  119--132, 2019.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 10215--10224, 2018.

\bibitem[Koepke et~al.(2022)Koepke, Oncescu, Henriques, Akata, and
  Albanie]{koepke2022audio}
Koepke, A.~S., Oncescu, A.-M., Henriques, J., Akata, Z., and Albanie, S.
\newblock Audio retrieval with natural language queries: A benchmark study.
\newblock \emph{IEEE Transactions on Multimedia}, 2022.

\bibitem[Kong et~al.(2020)Kong, Kim, and Bae]{kong2020hifi}
Kong, J., Kim, J., and Bae, J.
\newblock Hifi-gan: Generative adversarial networks for efficient and high
  fidelity speech synthesis.
\newblock \emph{arXiv preprint arXiv:2010.05646}, 2020.

\bibitem[Koutini et~al.(2021)Koutini, Schl{\"u}ter, Eghbal-zadeh, and
  Widmer]{koutini2021efficient}
Koutini, K., Schl{\"u}ter, J., Eghbal-zadeh, H., and Widmer, G.
\newblock Efficient training of audio transformers with patchout.
\newblock \emph{arXiv preprint arXiv:2110.05069}, 2021.

\bibitem[Kreuk et~al.(2022)Kreuk, Synnaeve, Polyak, Singer, D{\'e}fossez,
  Copet, Parikh, Taigman, and Adi]{kreuk2022audiogen}
Kreuk, F., Synnaeve, G., Polyak, A., Singer, U., D{\'e}fossez, A., Copet, J.,
  Parikh, D., Taigman, Y., and Adi, Y.
\newblock Audiogen: Textually guided audio generation.
\newblock \emph{arXiv preprint arXiv:2209.15352}, 2022.

\bibitem[Liu et~al.(2020)Liu, Jiang, Song, Huang, and Yang]{liu2020rethinking}
Liu, H., Jiang, B., Song, Y., Huang, W., and Yang, C.
\newblock Rethinking image inpainting via a mutual encoder-decoder with feature
  equalizations.
\newblock In \emph{European Conference on Computer Vision}, pp.\  725--741.
  Springer, 2020.

\bibitem[Mart{\'\i}n-Morat{\'o} \& Mesaros(2021)Mart{\'\i}n-Morat{\'o} and
  Mesaros]{martin2021ground}
Mart{\'\i}n-Morat{\'o}, I. and Mesaros, A.
\newblock What is the ground truth? reliability of multi-annotator data for
  audio tagging.
\newblock In \emph{2021 29th European Signal Processing Conference (EUSIPCO)},
  pp.\  76--80. IEEE, 2021.

\bibitem[Meng et~al.(2021)Meng, He, Song, Song, Wu, Zhu, and
  Ermon]{meng2021sdedit}
Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon, S.
\newblock Sdedit: Guided image synthesis and editing with stochastic
  differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Nazeri et~al.(2019)Nazeri, Ng, Joseph, Qureshi, and
  Ebrahimi]{nazeri2019edgeconnect}
Nazeri, K., Ng, E., Joseph, T., Qureshi, F.~Z., and Ebrahimi, M.
\newblock Edgeconnect: Generative image inpainting with adversarial edge
  learning.
\newblock \emph{arXiv preprint arXiv:1901.00212}, 2019.

\bibitem[Nichol et~al.(2021)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew,
  Sutskever, and Chen]{nichol2021glide}
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
  Sutskever, I., and Chen, M.
\newblock Glide: Towards photorealistic image generation and editing with
  text-guided diffusion models.
\newblock \emph{arXiv preprint arXiv:2112.10741}, 2021.

\bibitem[Piczak(2015)]{piczak2015esc}
Piczak, K.~J.
\newblock Esc: Dataset for environmental sound classification.
\newblock In \emph{Proceedings of the 23rd ACM international conference on
  Multimedia}, pp.\  1015--1018, 2015.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, Liu, et~al.]{raffel2020exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., Liu, P.~J., et~al.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21\penalty0 (140):\penalty0 1--67, 2020.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{ramesh2021zero}
Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and
  Sutskever, I.
\newblock Zero-shot text-to-image generation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8821--8831. PMLR, 2021.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and
  Ommer]{rombach2022high}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  10684--10695, 2022.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{International Conference on Medical image computing and
  computer-assisted intervention}, pp.\  234--241. Springer, 2015.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton,
  Ghasemipour, Ayan, Mahdavi, Lopes, et~al.]{saharia2022photorealistic}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
  S. K.~S., Ayan, B.~K., Mahdavi, S.~S., Lopes, R.~G., et~al.
\newblock Photorealistic text-to-image diffusion models with deep language
  understanding.
\newblock \emph{arXiv preprint arXiv:2205.11487}, 2022.

\bibitem[Salamon et~al.(2014)Salamon, Jacoby, and
  Bello]{Salamon:UrbanSound:ACMMM:14}
Salamon, J., Jacoby, C., and Bello, J.~P.
\newblock A dataset and taxonomy for urban sound research.
\newblock In \emph{22nd {ACM} International Conference on Multimedia
  (ACM-MM'14)}, pp.\  1041--1044, Orlando, FL, USA, Nov. 2014.

\bibitem[Singer et~al.(2022)Singer, Polyak, Hayes, Yin, An, Zhang, Hu, Yang,
  Ashual, Gafni, et~al.]{singer2022make}
Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang,
  H., Ashual, O., Gafni, O., et~al.
\newblock Make-a-video: Text-to-video generation without text-video data.
\newblock \emph{arXiv preprint arXiv:2209.14792}, 2022.

\bibitem[Song et~al.(2020)Song, Meng, and Ermon]{song2020denoising}
Song, J., Meng, C., and Ermon, S.
\newblock Denoising diffusion implicit models.
\newblock In \emph{Proc. of ICLR}, 2020.

\bibitem[Su et~al.(2020)Su, Liu, and Shlizerman]{su2020audeo}
Su, K., Liu, X., and Shlizerman, E.
\newblock Audeo: Audio generation for a silent performance video.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3325--3337, 2020.

\bibitem[Suvorov et~al.(2022)Suvorov, Logacheva, Mashikhin, Remizova, Ashukha,
  Silvestrov, Kong, Goka, Park, and Lempitsky]{suvorov2022resolution}
Suvorov, R., Logacheva, E., Mashikhin, A., Remizova, A., Ashukha, A.,
  Silvestrov, A., Kong, N., Goka, H., Park, K., and Lempitsky, V.
\newblock Resolution-robust large mask inpainting with fourier convolutions.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pp.\  2149--2159, 2022.

\bibitem[Taylor(2009)]{taylor2009text}
Taylor, P.
\newblock \emph{Text-to-speech synthesis}.
\newblock Cambridge university press, 2009.

\bibitem[Van Den~Oord et~al.(2017)Van Den~Oord, Vinyals, et~al.]{van2017neural}
Van Den~Oord, A., Vinyals, O., et~al.
\newblock Neural discrete representation learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Xu et~al.(2022)Xu, Li, Baevski, Auli, Galuba, Metze, Feichtenhofer,
  et~al.]{xu2022masked}
Xu, H., Li, J., Baevski, A., Auli, M., Galuba, W., Metze, F., Feichtenhofer,
  C., et~al.
\newblock Masked autoencoders that listen.
\newblock \emph{arXiv preprint arXiv:2207.06405}, 2022.

\bibitem[Xu et~al.(2020)Xu, Dinkel, Wu, and Yu]{xu2020crnn}
Xu, X., Dinkel, H., Wu, M., and Yu, K.
\newblock A crnn-gru based reinforcement learning approach to audio captioning.
\newblock In \emph{DCASE}, pp.\  225--229, 2020.

\bibitem[Yang et~al.(2022)Yang, Yu, Wang, Wang, Weng, Zou, and
  Yu]{yang2022diffsound}
Yang, D., Yu, J., Wang, H., Wang, W., Weng, C., Zou, Y., and Yu, D.
\newblock Diffsound: Discrete diffusion model for text-to-sound generation.
\newblock \emph{arXiv preprint arXiv:2207.09983}, 2022.

\bibitem[Zeghidour et~al.(2021)Zeghidour, Luebs, Omran, Skoglund, and
  Tagliasacchi]{zeghidour2021soundstream}
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M.
\newblock Soundstream: An end-to-end neural audio codec.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 30:\penalty0 495--507, 2021.

\bibitem[Zen et~al.(2019)Zen, Dang, Clark, Zhang, Weiss, Jia, Chen, and
  Wu]{zen2019libritts}
Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R.~J., Jia, Y., Chen, Z., and
  Wu, Y.
\newblock Libritts: A corpus derived from librispeech for text-to-speech.
\newblock \emph{arXiv preprint arXiv:1904.02882}, 2019.

\end{thebibliography}
