\begin{thebibliography}{10}

\bibitem{attouch2019rate}
Hedy Attouch, Zaki Chbani, and Hassan Riahi.
\newblock Rate of convergence of the nesterov accelerated gradient method in
  the subcritical case $\alpha\leq 3$.
\newblock {\em ESAIM: Control, Optimisation and Calculus of Variations}, 25:2,
  2019.

\bibitem{baldassarre2012multi}
Luca Baldassarre, Lorenzo Rosasco, Annalisa Barla, and Alessandro Verri.
\newblock Multi-output learning via spectral filtering.
\newblock {\em Machine learning}, 87(3):259--301, 2012.

\bibitem{Blanchard2018}
Gilles Blanchard and Nicole M{\"u}cke.
\newblock Optimal rates for regularization of statistical inverse learning
  problems.
\newblock {\em Foundations of Computational Mathematics}, 18(4):971--1013, Aug
  2018.

\bibitem{bottou2008tradeoffs}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Advances in neural information processing systems}, pages
  161--168, 2008.

\bibitem{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em Journal of machine learning research}, 2(Mar):499--526, 2002.

\bibitem{buhlmann2003boosting}
Peter B{\"u}hlmann and Bin Yu.
\newblock Boosting with the l 2 loss: regression and classification.
\newblock {\em Journal of the American Statistical Association},
  98(462):324--339, 2003.

\bibitem{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock {\em Foundations of Computational Mathematics}, 7(3):331--368, 2007.

\bibitem{charles2017stability}
Zachary Charles and Dimitris Papailiopoulos.
\newblock Stability and generalization of learning algorithms that converge to
  global optima.
\newblock {\em arXiv preprint arXiv:1710.08402}, 2017.

\bibitem{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock {\em arXiv preprint arXiv:1804.01619}, 2018.

\bibitem{devolder2014first}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock {\em Mathematical Programming}, 146(1-2):37--75, 2014.

\bibitem{engl1996regularization}
Heinz~Werner Engl, Martin Hanke, and Andreas Neubauer.
\newblock {\em Regularization of inverse problems}, volume 375.
\newblock Springer Science \& Business Media, 1996.

\bibitem{fujii1993norm}
Junichi Fujii, Masatoshi Fujii, Takayuki Furuta, and Ritsuo Nakamoto.
\newblock Norm inequalities equivalent to heinz inequality.
\newblock {\em Proceedings of the American Mathematical Society},
  118(3):827--830, 1993.

\bibitem{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9461--9471, 2018.

\bibitem{hardt2015train}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:1509.01240}, 2015.

\bibitem{lecun2012efficient}
Yann~A LeCun, L{\'e}on Bottou, Genevieve~B Orr, and Klaus-Robert M{\"u}ller.
\newblock Efficient backprop.
\newblock In {\em Neural networks: Tricks of the trade}, pages 9--48. Springer,
  2012.

\bibitem{lin2016generalization}
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Generalization properties and implicit regularization for multiple
  passes sgm.
\newblock In {\em International Conference on Machine Learning}, pages
  2340--2348, 2016.

\bibitem{lin2018optimal}
Junhong Lin and Volkan Cevher.
\newblock Optimal convergence for distributed learning with stochastic gradient
  methods and spectral-regularization algorithms.
\newblock {\em stat}, 1050:22, 2018.

\bibitem{lin2016optimal}
Junhong Lin and Lorenzo Rosasco.
\newblock Optimal learning for multi-pass stochastic gradient methods.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4556--4564, 2016.

\bibitem{lin2018optimal_spectral}
Junhong Lin, Alessandro Rudi, Lorenzo Rosasco, and Volkan Cevher.
\newblock Optimal rates for spectral algorithms with least-squares regression
  over hilbert spaces.
\newblock {\em Applied and Computational Harmonic Analysis}, 2018.

\bibitem{mathe2006regularization}
Peter Math{\'e} and Sergei Pereverzev.
\newblock Regularization of some linear ill-posed problems with discretized
  random noisy data.
\newblock {\em Mathematics of Computation}, 75(256):1913--1929, 2006.

\bibitem{mathe2002moduli}
Peter Math{\'e} and Sergei~V Pereverzev.
\newblock Moduli of continuity for operator valued functions.
\newblock {\em Numerical Functional Analysis and Optimization},
  23(5-6):623--631, 2002.

\bibitem{nesterov1983method}
Yurii~E Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In {\em Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem{neubauer2017nesterov}
Andreas Neubauer.
\newblock On nesterov acceleration for landweber iteration of linear ill-posed
  problems.
\newblock {\em Journal of Inverse and Ill-posed Problems}, 25(3):381--390,
  2017.

\bibitem{polyak1987introduction}
Boris~T Polyak.
\newblock Introduction to optimization.
\newblock Technical report, 1987.

\bibitem{raskutti2014early}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Early stopping and non-parametric regression: an optimal
  data-dependent stopping rule.
\newblock {\em The Journal of Machine Learning Research}, 15(1):335--366, 2014.

\bibitem{rosasco2015learning}
Lorenzo Rosasco and Silvia Villa.
\newblock Learning with incremental iterative regularization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1630--1638, 2015.

\bibitem{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em The Journal of Machine Learning Research}, 19(1):2822--2878,
  2018.

\bibitem{steinwart2008support}
Ingo Steinwart and Andreas Christmann.
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{su2014differential}
Weijie Su, Stephen Boyd, and Emmanuel Candes.
\newblock A differential equation for modeling nesterovâ€™s accelerated
  gradient method: Theory and insights.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2510--2518, 2014.

\bibitem{szeg1939orthogonal}
Gabor Szeg.
\newblock {\em Orthogonal polynomials}, volume~23.
\newblock American Mathematical Soc., 1939.

\bibitem{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock {\em Constructive Approximation}, 26(2):289--315, 2007.

\bibitem{zavriev1993heavy}
SK~Zavriev and FV~Kostyuk.
\newblock Heavy-ball method in nonconvex optimization problems.
\newblock {\em Computational Mathematics and Modeling}, 4(4):336--341, 1993.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\end{thebibliography}
