\begin{thebibliography}{}

\bibitem[Akhtar et~al., 2018]{akhtar2018oboe}
Akhtar, Z., Nam, Y.~S., Govindan, R., Rao, S., Chen, J., Katz-Bassett, E.,
  Ribeiro, B., Zhan, J., and Zhang, H. (2018).
\newblock Oboe: Auto-tuning video abr algorithms to network conditions.
\newblock In {\em Proceedings of the 2018 Conference of the ACM Special
  Interest Group on Data Communication}, pages 44--58.

\bibitem[Al-Shedivat et~al., 2017]{al2017continuous}
Al-Shedivat, M., Bansal, T., Burda, Y., Sutskever, I., Mordatch, I., and
  Abbeel, P. (2017).
\newblock Continuous adaptation via meta-learning in nonstationary and
  competitive environments.
\newblock {\em arXiv preprint arXiv:1710.03641}.

\bibitem[Alegre et~al., 2021]{alegre2021minimum}
Alegre, L.~N., Bazzan, A.~L., and da~Silva, B.~C. (2021).
\newblock Minimum-delay adaptation in non-stationary reinforcement learning via
  online high-confidence change-point detection.
\newblock {\em arXiv preprint arXiv:2105.09452}.

\bibitem[Bellemare et~al., 2012]{bellemare2012ale}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M. (2012).
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock {\em Journal of Artificial Intelligence Research}, Vol. 47:253--279.
\newblock cite arxiv:1207.4708.

\bibitem[Buesing et~al., 2018]{buesing2018learning}
Buesing, L., Weber, T., Racaniere, S., Eslami, S., Rezende, D., Reichert,
  D.~P., Viola, F., Besse, F., Gregor, K., Hassabis, D., et~al. (2018).
\newblock Learning and querying fast generative models for reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1802.03006}.

\bibitem[Chandak et~al., 2020a]{chandak2020towards}
Chandak, Y., Jordan, S., Theocharous, G., White, M., and Thomas, P.~S. (2020a).
\newblock Towards safe policy improvement for non-stationary mdps.
\newblock {\em Advances in Neural Information Processing Systems},
  33:9156--9168.

\bibitem[Chandak et~al., 2020b]{chandak2020optimizing}
Chandak, Y., Theocharous, G., Shankar, S., White, M., Mahadevan, S., and
  Thomas, P. (2020b).
\newblock Optimizing for the future in non-stationary mdps.
\newblock In {\em International Conference on Machine Learning}, pages
  1414--1425. PMLR.

\bibitem[Cho et~al., 2014]{cho2014learning}
Cho, K., Van~Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
  Schwenk, H., and Bengio, Y. (2014).
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock {\em arXiv preprint arXiv:1406.1078}.

\bibitem[Doerr et~al., 2018]{doerr2018probabilistic}
Doerr, A., Daniel, C., Schiegg, M., Duy, N.-T., Schaal, S., Toussaint, M., and
  Sebastian, T. (2018).
\newblock Probabilistic recurrent state-space models.
\newblock In {\em International Conference on Machine Learning}, pages
  1280--1289. PMLR.

\bibitem[Duff, 2002]{duff2002optimal}
Duff, M.~O. (2002).
\newblock {\em Optimal Learning: Computational procedures for Bayes-adaptive
  Markov decision processes}.
\newblock University of Massachusetts Amherst.

\bibitem[Fellows et~al., 2021]{fellows2021bayesian}
Fellows, M., Hartikainen, K., and Whiteson, S. (2021).
\newblock Bayesian bellman operators.
\newblock {\em Advances in Neural Information Processing Systems}, 34.

\bibitem[Feng et~al., 2022]{feng2022factored}
Feng, F., Huang, B., Zhang, K., and Magliacane, S. (2022).
\newblock Factored adaptation for non-stationary reinforcement learning.
\newblock {\em arXiv preprint arXiv:2203.16582}.

\bibitem[Ha and Schmidhuber, 2018]{ha2018world}
Ha, D. and Schmidhuber, J. (2018).
\newblock World models.
\newblock {\em arXiv preprint arXiv:1803.10122}.

\bibitem[Hafner et~al., 2019a]{hafner2019dream}
Hafner, D., Lillicrap, T., Ba, J., and Norouzi, M. (2019a).
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock {\em arXiv preprint arXiv:1912.01603}.

\bibitem[Hafner et~al., 2019b]{hafner2019learning}
Hafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D., Lee, H., and
  Davidson, J. (2019b).
\newblock Learning latent dynamics for planning from pixels.

\bibitem[Hallak et~al., 2015]{hallak2015contextual}
Hallak, A., Di~Castro, D., and Mannor, S. (2015).
\newblock Contextual markov decision processes.
\newblock {\em arXiv preprint arXiv:1502.02259}.

\bibitem[Han et~al., 2019]{han2019variational}
Han, D., Doya, K., and Tani, J. (2019).
\newblock Variational recurrent models for solving partially observable control
  tasks.

\bibitem[Hausknecht and Stone, 2015]{hausknecht2015deep}
Hausknecht, M. and Stone, P. (2015).
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In {\em 2015 aaai fall symposium series}.

\bibitem[Hessel et~al., 2017]{hessel2017rainbow}
Hessel, M., Modayil, J., van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2017).
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock cite arxiv:1710.02298Comment: Under review as a conference paper at
  AAAI 2018.

\bibitem[Humplik et~al., 2019]{humplik2019meta}
Humplik, J., Galashov, A., Hasenclever, L., Ortega, P.~A., Teh, Y.~W., and
  Heess, N. (2019).
\newblock Meta reinforcement learning as task inference.
\newblock {\em arXiv preprint arXiv:1905.06424}.

\bibitem[Kaelbling et~al., 1998]{kaelbling1998planning}
Kaelbling, L.~P., Littman, M.~L., and Cassandra, A.~R. (1998).
\newblock Planning and acting in partially observable stochastic domains.
\newblock {\em Artificial intelligence}, 101(1-2):99--134.

\bibitem[Kamienny et~al., 2020]{kamienny2020learning}
Kamienny, P.-A., Pirotta, M., Lazaric, A., Lavril, T., Usunier, N., and
  Denoyer, L. (2020).
\newblock Learning adaptive exploration strategies in dynamic environments
  through informed policy regularization.
\newblock {\em arXiv preprint arXiv:2005.02934}.

\bibitem[Karl et~al., 2016]{karl2016deep}
Karl, M., Soelch, M., Bayer, J., and Van~der Smagt, P. (2016).
\newblock Deep variational bayes filters: Unsupervised learning of state space
  models from raw data.
\newblock {\em arXiv preprint arXiv:1605.06432}.

\bibitem[Krishnan et~al., 2015]{krishnan2015deep}
Krishnan, R.~G., Shalit, U., and Sontag, D. (2015).
\newblock Deep kalman filters.
\newblock {\em arXiv preprint arXiv:1511.05121}.

\bibitem[Kumar et~al., 2021]{kumar2021rma}
Kumar, A., Fu, Z., Pathak, D., and Malik, J. (2021).
\newblock Rma: Rapid motor adaptation for legged robots.

\bibitem[Mnih et~al., 2013]{mnih2013atari}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M. (2013).
\newblock Playing atari with deep reinforcement learning.
\newblock cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013.

\bibitem[Nagabandi et~al., 2018]{nagabandi2018deep}
Nagabandi, A., Finn, C., and Levine, S. (2018).
\newblock Deep online learning via meta-learning: Continual adaptation for
  model-based rl.
\newblock {\em arXiv preprint arXiv:1812.07671}.

\bibitem[OpenNetLab, 2021]{alphartc}
OpenNetLab (2021).
\newblock Alphartc.
\newblock In {\em https://github.com/OpenNetLab/AlphaRTC/}.

\bibitem[Poiani et~al., 2021]{poiani2021meta}
Poiani, R., Tirinzoni, A., and Restelli, M. (2021).
\newblock Meta-reinforcement learning by tracking task non-stationarity.
\newblock {\em arXiv preprint arXiv:2105.08834}.

\bibitem[Rakelly et~al., 2019]{rakelly2019efficient}
Rakelly, K., Zhou, A., Finn, C., Levine, S., and Quillen, D. (2019).
\newblock Efficient off-policy meta-reinforcement learning via probabilistic
  context variables.
\newblock In {\em International conference on machine learning}, pages
  5331--5340. PMLR.

\bibitem[Ren et~al., 2022]{ren2022reinforcement}
Ren, H., Sootla, A., Jafferjee, T., Shen, J., Wang, J., and Bou-Ammar, H.
  (2022).
\newblock Reinforcement learning in presence of discrete markovian context
  evolution.
\newblock {\em arXiv preprint arXiv:2202.06557}.

\bibitem[Schulman et~al., 2015]{schulman2015trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M.~I., and Moritz, P. (2015).
\newblock Trust region policy optimization.
\newblock In Bach, F.~R. and Blei, D.~M., editors, {\em ICML}, volume~37 of
  {\em JMLR Workshop and Conference Proceedings}, pages 1889--1897. JMLR.org.

\bibitem[Schulman et~al., 2017a]{schulman2017ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017a).
\newblock Proximal policy optimization algorithms.
\newblock {\em CoRR}, abs/1707.06347.

\bibitem[Schulman et~al., 2017b]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017b).
\newblock Proximal policy optimization algorithms.
\newblock {\em arXiv preprint arXiv:1707.06347}.

\bibitem[Sodhani et~al., 2021]{sodhani2021block}
Sodhani, S., Meier, F., Pineau, J., and Zhang, A. (2021).
\newblock Block contextual mdps for continual learning.
\newblock {\em arXiv preprint arXiv:2110.06972}.

\bibitem[Todorov et~al., 2012]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y. (2012).
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ international conference on intelligent robots
  and systems}, pages 5026--5033. IEEE.

\bibitem[Xie et~al., 2021]{xie2021deep}
Xie, A., Harrison, J., and Finn, C. (2021).
\newblock Deep reinforcement learning amidst continual structured
  non-stationarity.
\newblock In {\em International Conference on Machine Learning}, pages
  11393--11403. PMLR.

\bibitem[Xie et~al., 2022]{xie2022robust}
Xie, A., Sodhani, S., Finn, C., Pineau, J., and Zhang, A. (2022).
\newblock Robust policy learning over multiple uncertainty sets.
\newblock {\em arXiv preprint arXiv:2202.07013}.

\bibitem[Zhang and Duffield, 2001]{zhang2001constancy}
Zhang, Y. and Duffield, N. (2001).
\newblock On the constancy of internet path properties.
\newblock In {\em Proceedings of the 1st ACM SIGCOMM Workshop on Internet
  Measurement}, pages 197--211.

\bibitem[Zhao et~al., 2020]{zhao2020meld}
Zhao, T.~Z., Nagabandi, A., Rakelly, K., Finn, C., and Levine, S. (2020).
\newblock Meld: Meta-reinforcement learning from images via latent state
  models.
\newblock {\em arXiv preprint arXiv:2010.13957}.

\bibitem[Zintgraf et~al., 2020]{zintgraf2020varibad}
Zintgraf, L., Shiarlis, K., Igl, M., Schulze, S., Gal, Y., Hofmann, K., and
  Whiteson, S. (2020).
\newblock Varibad: A very good method for bayes-adaptive deep rl via
  meta-learning.
\newblock In {\em International Conference on Learning Representation (ICLR)}.

\end{thebibliography}
