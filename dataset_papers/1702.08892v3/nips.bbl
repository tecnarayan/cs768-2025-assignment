\begin{thebibliography}{10}

\bibitem{tensorflow}
M.~Abadi, P.~Barham, J.~Chen, Z.~Chen, A.~Davis, J.~Dean, M.~Devin,
  S.~Ghemawat, G.~Irving, M.~Isard, et~al.
\newblock Tensorflow: A system for large-scale machine learning.
\newblock {\em arXiv:1605.08695}, 2016.

\bibitem{abbeel2004apprenticeship}
P.~Abbeel and A.~Y. Ng.
\newblock Apprenticeship learning via inverse reinforcement learning.
\newblock In {\em Proceedings of the twenty-first international conference on
  Machine learning}, page~1. ACM, 2004.

\bibitem{antos2008learning}
A.~Antos, C.~Szepesv{\'a}ri, and R.~Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning}, 71(1):89--129, 2008.

\bibitem{mellowmax}
K.~Asadi and M.~L. Littman.
\newblock A new softmax operator for reinforcement learning.
\newblock {\em arXiv:1612.05628}, 2016.

\bibitem{azaretal11}
M.~G. Azar, V.~G{\'o}mez, and H.~J. Kappen.
\newblock Dynamic policy programming with function approximation.
\newblock {\em AISTATS}, 2011.

\bibitem{azar}
M.~G. Azar, V.~G{\'o}mez, and H.~J. Kappen.
\newblock Dynamic policy programming.
\newblock {\em JMLR}, 13(Nov), 2012.

\bibitem{azaretal12}
M.~G. Azar, V.~G{\'o}mez, and H.~J. Kappen.
\newblock Optimal control as a graphical model inference problem.
\newblock {\em Mach. Learn. J.}, 87, 2012.

\bibitem{bertsekas95}
D.~P. Bertsekas.
\newblock {\em Dynamic Programming and Optimal Control}, volume~2.
\newblock Athena Scientific, 1995.

\bibitem{borweinlewis00}
J.~Borwein and A.~Lewis.
\newblock {\em Convex Analysis and Nonlinear Optimization}.
\newblock Springer, 2000.

\bibitem{Brockman}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock Open{AI} {G}ym.
\newblock arXiv:1606.01540, 2016.

\bibitem{fox}
R.~Fox, A.~Pakman, and N.~Tishby.
\newblock G-learning: Taming the noise in reinforcement learning via soft
  updates.
\newblock {\em UAI}, 2016.

\bibitem{reactor}
A.~Gruslys, M.~G. Azar, M.~G. Bellemare, and R.~Munos.
\newblock The reactor: A sample-efficient actor-critic architecture.
\newblock {\em arXiv preprint arXiv:1704.04651}, 2017.

\bibitem{gu2016deep}
S.~Gu, E.~Holly, T.~Lillicrap, and S.~Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock {\em ICRA}, 2016.

\bibitem{guetal17}
S.~Gu, T.~Lillicrap, Z.~Ghahramani, R.~E. Turner, and S.~Levine.
\newblock {Q-Prop}: Sample-efficient policy gradient with an off-policy critic.
\newblock {\em ICLR}, 2017.

\bibitem{haarnojaetal17}
T.~Haarnoja, H.~Tang, P.~Abbeel, and S.~Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock arXiv:1702.08165, 2017.

\bibitem{ho2016generative}
J.~Ho and S.~Ermon.
\newblock Generative adversarial imitation learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4565--4573, 2016.

\bibitem{lstm}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Comput.}, 1997.

\bibitem{huang2015approximate}
D.-A. Huang, A.-m. Farahmand, K.~M. Kitani, and J.~A. Bagnell.
\newblock Approximate maxent inverse optimal control and its application for
  mental simulation of human interactions.
\newblock 2015.

\bibitem{kakade01}
S.~Kakade.
\newblock A natural policy gradient.
\newblock {\em NIPS}, 2001.

\bibitem{kappen2005path}
H.~J. Kappen.
\newblock Path integrals and symmetry breaking for optimal control theory.
\newblock {\em Journal of statistical mechanics: theory and experiment},
  2005(11):P11011, 2005.

\bibitem{adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em ICLR}, 2015.

\bibitem{kober2013}
J.~Kober, J.~A. Bagnell, and J.~Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock {\em IJRR}, 2013.

\bibitem{levine2016end}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em JMLR}, 17(39), 2016.

\bibitem{li2010}
L.~Li, W.~Chu, J.~Langford, and R.~E. Schapire.
\newblock A contextual-bandit approach to personalized news article
  recommendation.
\newblock 2010.

\bibitem{lillicrap2015continuous}
T.~P. Lillicrap, J.~J. Hunt, A.~Pritzel, N.~Heess, T.~Erez, Y.~Tassa,
  D.~Silver, and D.~Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em ICLR}, 2016.

\bibitem{littman}
M.~L. Littman.
\newblock {\em Algorithms for sequential decision making}.
\newblock PhD thesis, Brown University, 1996.

\bibitem{mnih2016asynchronous}
V.~Mnih, A.~P. Badia, M.~Mirza, A.~Graves, T.~P. Lillicrap, T.~Harley,
  D.~Silver, and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock {\em ICML}, 2016.

\bibitem{atarinature}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 2015.

\bibitem{munos2016safe}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock {\em NIPS}, 2016.

\bibitem{urex}
O.~Nachum, M.~Norouzi, and D.~Schuurmans.
\newblock Improving policy gradient by exploring under-appreciated rewards.
\newblock {\em ICLR}, 2017.

\bibitem{pgq2017}
B.~O'Donoghue, R.~Munos, K.~Kavukcuoglu, and V.~Mnih.
\newblock {PGQ}: Combining policy gradient and {Q}-learning.
\newblock {\em ICLR}, 2017.

\bibitem{peng1996incremental}
J.~Peng and R.~J. Williams.
\newblock Incremental multi-step {Q}-learning.
\newblock {\em Machine learning}, 22(1-3):283--290, 1996.

\bibitem{petersetal10}
J.~Peters, K.~M\"{u}ling, and Y.~Altun.
\newblock Relative entropy policy search.
\newblock {\em AAAI}, 2010.

\bibitem{precup2000eligibility}
D.~Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock {\em Computer Science Department Faculty Publication Series},
  page~80, 2000.

\bibitem{precup2001off}
D.~Precup, R.~S. Sutton, and S.~Dasgupta.
\newblock Off-policy temporal-difference learning with function approximation.
\newblock 2001.

\bibitem{pdqn}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock {\em ICLR}, 2016.

\bibitem{schulmanetal17}
J.~Schulman, X.~Chen, and P.~Abbeel.
\newblock Equivalence between policy gradients and soft {Q}-learning.
\newblock arXiv:1704.06440, 2017.

\bibitem{trpo2015}
J.~Schulman, S.~Levine, P.~Moritz, M.~Jordan, and P.~Abbeel.
\newblock Trust region policy optimization.
\newblock {\em ICML}, 2015.

\bibitem{schulmaniclr2016}
J.~Schulman, P.~Moritz, S.~Levine, M.~Jordan, and P.~Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock {\em ICLR}, 2016.

\bibitem{silver14ddpg}
D.~Silver, G.~Lever, N.~Heess, T.~Degris, D.~Wierstra, and M.~Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock {\em ICML}, 2014.

\bibitem{suttonbook_2nd_ed}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Introduction to Reinforcement Learning}.
\newblock MIT Press, 2nd edition, 2017.
\newblock Preliminary Draft.

\bibitem{sutton1999policy}
R.~S. Sutton, D.~A. McAllester, S.~P. Singh, Y.~Mansour, et~al.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock {\em NIPS}, 1999.

\bibitem{tesauro1995}
G.~Tesauro.
\newblock Temporal difference learning and {TD}-gammon.
\newblock {\em CACM}, 1995.

\bibitem{theocharous2015}
G.~Theocharous, P.~S. Thomas, and M.~Ghavamzadeh.
\newblock Personalized ad recommendation systems for life-time value
  optimization with guarantees.
\newblock {\em IJCAI}, 2015.

\bibitem{todorov2006linearly}
E.~Todorov.
\newblock Linearly-solvable {Markov} decision problems.
\newblock {\em NIPS}, 2006.

\bibitem{todorov10}
E.~Todorov.
\newblock Policy gradients in linearly-solvable {MDP}s.
\newblock {\em NIPS}, 2010.

\bibitem{tsitsiklisvanroy97}
J.~N. Tsitsiklis and B.~{Van Roy}.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock {\em IEEE Transactions on Automatic Control}, 42(5), 1997.

\bibitem{acer}
Z.~Wang, V.~Bapst, N.~Heess, V.~Mnih, R.~Munos, K.~Kavukcuoglu, and
  N.~de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock {\em ICLR}, 2017.

\bibitem{wangetal16}
Z.~Wang, N.~de~Freitas, and M.~Lanctot.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock {\em ICLR}, 2016.

\bibitem{qlearning}
C.~J. Watkins.
\newblock {\em Learning from delayed rewards}.
\newblock PhD thesis, University of Cambridge England, 1989.

\bibitem{watkins1992q}
C.~J. Watkins and P.~Dayan.
\newblock Q-learning.
\newblock {\em Machine learning}, 8(3-4):279--292, 1992.

\bibitem{williams92}
R.~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Mach. Learn. J.}, 1992.

\bibitem{williams1991function}
R.~J. Williams and J.~Peng.
\newblock Function optimization using connectionist reinforcement learning
  algorithms.
\newblock {\em Connection Science}, 1991.

\bibitem{ziebart2010modeling}
B.~D. Ziebart.
\newblock {\em Modeling purposeful adaptive behavior with the principle of
  maximum causal entropy}.
\newblock PhD thesis, CMU, 2010.

\bibitem{ziebart2008maximum}
B.~D. Ziebart, A.~L. Maas, J.~A. Bagnell, and A.~K. Dey.
\newblock Maximum entropy inverse reinforcement learning.
\newblock {\em AAAI}, 2008.

\end{thebibliography}
