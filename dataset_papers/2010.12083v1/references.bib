@misc{zhu2020visionlanguage,
      title={Vision-Language Navigation with Self-Supervised Auxiliary Reasoning Tasks}, 
      author={Fengda Zhu and Yi Zhu and Xiaojun Chang and Xiaodan Liang},
      year={2020},
      eprint={1911.07883},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cui2020empathic,
      title={The EMPATHIC Framework for Task Learning from Implicit Human Feedback}, 
      author={Yuchen Cui and Qiping Zhang and Alessandro Allievi and Peter Stone and Scott Niekum and W. Bradley Knox},
      year={2020},
      eprint={2009.13649},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{campbell2019probabilistic,
      title={Probabilistic Multimodal Modeling for Human-Robot Interaction Tasks}, 
      author={Joseph Campbell and Simon Stepputtis and Heni Ben Amor},
      year={2019},
      eprint={1908.04955},
      archivePrefix={arXiv},
      primaryClass={cs.RO}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{marzoev2020unnatural,
      title={Unnatural Language Processing: Bridging the Gap Between Synthetic and Natural Language Data}, 
      author={Alana Marzoev and Samuel Madden and M. Frans Kaashoek and Michael Cafarella and Jacob Andreas},
      year={2020},
      eprint={2004.13645},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{Codevilla2018EndtoEndDV,
  title={End-to-End Driving Via Conditional Imitation Learning},
  author={Felipe Codevilla and Matthias M{\"u}ller and Alexey Dosovitskiy and Antonio L{\'o}pez and Vladlen Koltun},
  journal={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  year={2018},
  pages={1-9}
}

@article{ding2019,
author    = {Yiming Ding and Carlos Florensa and Mariano Phielipp and Pieter Abbeel},
title     = {Goal-conditioned Imitation Learning},
journal   = {Advances in Neural Information Processing Systems},
year      = {2019},
url       = {http://arxiv.org/abs/1906.05838}
}

@article{Wahid2019,
abstract = {Learned Neural Network based policies have shown promising results for robot navigation. However, most of these approaches fall short of being used on a real robot due to the extensive simulated training they require. These simulations lack the visuals and dynamics of the real world, which makes it infeasible to deploy on a real robot. We present a novel Neural Net based policy, NavNet, which allows for easy deployment on a real robot. It consists of two sub policies -- a high level policy which can understand real images and perform long range planning expressed in high level commands; a low level policy that can translate the long range plan into low level commands on a specific platform in a safe and robust manner. For every new deployment, the high level policy is trained on an easily obtainable scan of the environment modeling its visuals and layout. We detail the design of such an environment and how one can use it for training a final navigation policy. Further, we demonstrate a learned low-level policy. We deploy the model in a large office building and test it extensively, achieving {\$}0.80{\$} success rate over long navigation runs and outperforming SLAM-based models in the same settings.},
archivePrefix = {arXiv},
arxivId = {1903.09870},
author = {Wahid, Ayzaan and Toshev, Alexander and Fiser, Marek and Lee, Tsang-Wei Edward},
eprint = {1903.09870},
file = {:home/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Wahid et al. - 2019 - Long Range Neural Navigation Policies for the Real World.pdf:pdf},
journal = {IEEE International Conference on Intelligent Robots and Systems},
month = {mar},
pages = {82--89},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Long Range Neural Navigation Policies for the Real World}},
url = {http://arxiv.org/abs/1903.09870},
year = {2019}
}


@article{Hristov2019,
archivePrefix = {arXiv},
arxivId = {1907.13627},
author = {Hristov, Yordan and Angelov, Daniel and Burke, Michael and Lascarides, Alex and Ramamoorthy, Subramanian},
eprint = {1907.13627},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hristov et al. - 2019 - Disentangled Relational Representations for Explaining and Learning from Demonstration.pdf:pdf},
mendeley-groups = {ICLR Reviews},
month = {jul},
title = {{Disentangled Relational Representations for Explaining and Learning from Demonstration}},
url = {http://arxiv.org/abs/1907.13627},
year = {2019}
}

@misc{highway_network,
Author = {Rupesh Kumar Srivastava and Klaus Greff and JÃ¼rgen Schmidhuber},
Title = {Highway Networks},
Year = {2015},
Eprint = {arXiv:1505.00387},
}

@techreport{Sugita2005,
author = {Sugita, Yuuya and Tani, Jun},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sugita, Tani BSI RIKEN - 2005 - Learning Semantic Combinatoriality from the Interaction between Linguistic and Behavioral Processes.pdf:pdf},
mendeley-groups = {ICLR Reviews},
title = {{Learning Semantic Combinatoriality from the Interaction between Linguistic and Behavioral Processes}},
year = {2005}
}

@techreport{Chang,
archivePrefix = {arXiv},
arxivId = {1910.10628},
author = {Chang, Jonathan and Kumar, Nishanth and Hastings, Sean and Gokaslan, Aaron and Romeres, Diego and Jha, Devesh and Nikovski, Daniel and Konidaris, George and Tellex, Stefanie},
eprint = {1910.10628},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chang et al. - Unknown - Learning Deep Parameterized Skills from Demonstration for Re-targetable Visuomotor Control.pdf:pdf},
mendeley-groups = {ICLR Reviews},
title = {{Learning Deep Parameterized Skills from Demonstration for Re-targetable Visuomotor Control}},
url = {http://arxiv.org/abs/1910.10628},
year = {2019}
}

@techreport{Krantz,
abstract = {We develop a language-guided navigation task set in a continuous 3D environment where agents must execute low-level actions to follow natural language navigation directions. By being situated in continuous environments, this setting lifts a number of assumptions implicit in prior work that represents environments as a sparse graph of panoramas with edges corresponding to navigability. Specifically, our setting drops the presumptions of known environment topologies, short-range oracle navigation, and perfect agent localization. To contextualize this new task, we develop models that mirror many of the advances made in prior settings as well as single-modality baselines. While some of these techniques transfer, we find significantly lower absolute performance in the continuous setting-suggesting that performance in prior 'navigation-graph' settings may be inflated by the strong implicit assumptions.},
archivePrefix = {arXiv},
arxivId = {2004.02857v2},
author = {Krantz, Jacob and Wijmans, Erik and Majumdar, Arjun and Batra, Dhruv and Lee, Stefan},
eprint = {2004.02857v2},
file = {:home/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Krantz et al. - Unknown - Beyond the Nav-Graph Vision-and-Language Navigation in Continuous Environments(2).pdf:pdf},
isbn = {2004.02857v2},
keywords = {Embodied Agents,Vision-and-Language Navigation},
title = {{Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments}},
year = {2020}
}

@techreport{Kuo,
abstract = {We demonstrate how a sampling-based robotic planner can be augmented to learn to understand a sequence of natural language commands in a continuous configuration space to move and manipulate objects. Our approach combines a deep network structured according to the parse of a complex command that includes objects, verbs, spatial relations, and attributes, with a sampling-based planner, RRT. A recurrent hierarchical deep network controls how the planner explores the environment, determines when a planned path is likely to achieve a goal, and estimates the confidence of each move to trade off exploitation and exploration between the network and the planner. Planners are designed to have near-optimal behavior when information about the task is missing, while networks learn to exploit observations which are available from the environment, making the two naturally complementary. Combining the two enables generalization to new maps, new kinds of obstacles, and more complex sentences that do not occur in the training set. Little data is required to train the model despite it jointly acquiring a CNN that extracts features from the environment as it learns the meanings of words. The model provides a level of interpretability through the use of attention maps allowing users to see its reasoning steps despite being an end-to-end model. This end-to-end model allows robots to learn to follow natural language commands in challenging continuous environments.},
archivePrefix = {arXiv},
arxivId = {2002.05201v2},
author = {Kuo, Yen-Ling and Katz, Boris and Barbu, Andrei},
eprint = {2002.05201v2},
file = {:home/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Kuo, Katz, Barbu - Unknown - Deep compositional robotic planners that follow natural language commands.pdf:pdf},
title = {{Deep compositional robotic planners that follow natural language commands}},
year = {2020}
}



@inproceedings{Abolghasemi,
  title={Pay attention!-robustifying a deep visuomotor policy through task-focused visual attention},
  author={Abolghasemi, Pooya and Mazaheri, Amir and Shah, Mubarak and Boloni, Ladislau},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4254--4262},
  year={2019},
}

@misc{1506.01497,
Author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
Title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
Year = {2015},
Eprint = {arXiv:1506.01497},
}

@techreport{Bisk,
author = {Bisk, Yonatan and Yuret, Deniz and Marcu, Daniel},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bisk, Yuret, Marcu - Unknown - Natural Language Communication with Robots.pdf:pdf},
mendeley-groups = {Background},
title = {{Natural Language Communication with Robots}},
url = {http://nlg.isi.edu/},
year = {2016}
}

@techreport{ArkanCan2019,
author = {{Arkan Can}, Ozan and {Zuidberg Dos Martires}, Pedro and Leuven, Ku and Gaal, Julian and Loutf{\"{i}}Loutf{\"{i}}, Amy and {De Raedt Leuven Deniz Yuret}, Luc KU and Saffiott{\"{i}}Saffiott{\"{i}}, Alessandro},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Arkan Can et al. - 2019 - Learning from Implicit Information in Natural Language Instructions for Robotic Manipulations.pdf:pdf},
mendeley-groups = {Background},
pages = {29--39},
title = {{Learning from Implicit Information in Natural Language Instructions for Robotic Manipulations}},
url = {https://spacy.io/},
year = {2019}
}

@techreport{Matuszek2017,
author = {Matuszek, Cynthia},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Matuszek - 2017 - Grounded Language Learning Where Robotics and NLP Meet.pdf:pdf},
keywords = {Machine Learning Applications: Other Applications,Natural Language Processing: Natural Language Processing,Robotics: Human Robot Interaction,Robotics: Learning in Robotics},
mendeley-groups = {Background},
title = {{Grounded Language Learning: Where Robotics and NLP Meet *}},
year = {2017}
}

@techreport{Alomari,
author = {Alomari, Muhannad and Duckworth, Paul and Hogg, David C and Cohn, Anthony G},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Alomari et al. - Unknown - Natural Language Acquisition and Grounding for Embodied Robotic Systems AAAI17.pdf:pdf},
keywords = {Special Track on Cognitive Systems},
mendeley-groups = {Background},
title = {{Natural Language Acquisition and Grounding for Embodied Robotic Systems}},
year = {2017},
url = {www.aaai.org}
}

@techreport{GibsonHunt2014,
author = {{Gibson Hunt}, Gail and Reinhard, Susan and Greene, Rick and Whiting, C Grace and Feinberg, Lynn Friss and Choula, Rita and Green, Jordan and Houser, Ari},
file = {:home/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Gibson Hunt et al. - 2014 - Caregiving in the U.S. 2015 Report Acknowledgments Co-Directors.pdf:pdf},
keywords = {AARP,NAC,National Alliance for Caregiving,PPI,Public Policy Institute,caregiver,caregiving,hospital,nursing,stress,studies,study},
title = {{Caregiving in the U.S. 2015 Report Acknowledgments Co-Directors}},
year = {2014}
}


@article{Burke2019,
abstract = {Hybrid systems are a compact and natural mechanism with which to address problems in robotics. This work introduces an approach to learning hybrid systems from demonstrations, with an emphasis on extracting models that are explicitly verifiable and easily interpreted by robot operators. We fit a sequence of controllers using sequential importance sampling under a generative switching proportional controller task model. Here, we parameterise controllers using a proportional gain and a visually verifiable joint angle goal. Inference under this model is challenging, but we address this by introducing an attribution prior extracted from a neural end-to-end visuomotor control model. Given the sequence of controllers comprising a task, we simplify the trace using grammar parsing strategies, taking advantage of the sequence compositionality, before grounding the controllers by training perception networks to predict goals given images. Using this approach, we are successfully able to induce a program for a visuomotor reaching task involving loops and conditionals from a single demonstration and a neural end-to-end model. In addition, we are able to discover the program used for a tower building task. We argue that computer program-like control systems are more interpretable than alternative end-to-end learning approaches, and that hybrid systems inherently allow for better generalisation across task configurations.},
archivePrefix = {arXiv},
arxivId = {1902.10657},
author = {Burke, Michael and Penkov, Svetlin and Ramamoorthy, Subramanian},
doi = {10.15607/RSS.2019.XV.015},
eprint = {1902.10657},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Burke, Penkov, Ramamoorthy - 2019 - From explanation to synthesis Compositional program induction for learning from demonstration.pdf:pdf},
mendeley-groups = {ICLR Reviews},
month = {feb},
title = {{From explanation to synthesis: Compositional program induction for learning from demonstration}},
url = {http://arxiv.org/abs/1902.10657 http://dx.doi.org/10.15607/RSS.2019.XV.015},
year = {2019}
}

@misc{Misra2018,
abstract = {We propose to directly map raw visual observations and text input to actions for instruction execution. While existing approaches assume access to structured environment representations or use a pipeline of separately trained models, we learn a single model to jointly reason about linguistic and visual input. We use reinforcement learning in a contextual bandit setting to train a neural network agent. To guide the agent's exploration, we use reward shaping with different forms of supervision. Our approach does not require intermediate representations, planning procedures, or training different models. We evaluate in a simulated environment, and show significant improvements over supervised learning and common reinforcement learning variants.},
archivePrefix = {arXiv},
arxivId = {1704.08795},
author = {Misra, Dipendra and Langford, John and Artzi, Yoav},
doi = {10.18653/v1/d17-1106},
eprint = {1704.08795},
month = {jan},
pages = {1004--1015},
publisher = {Association for Computational Linguistics (ACL)},
title = {{Mapping Instructions and Visual Observations to Actions with Reinforcement Learning}},
url = {https://arxiv.org/abs/1704.08795},
year = {2018}
}

@techreport{Anderson,
archivePrefix = {arXiv},
arxivId = {1707.07998v3},
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
eprint = {1707.07998v3},
file = {:home/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Anderson et al. - Unknown - Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering.pdf:pdf},
mendeley-groups = {Background},
title = {{Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering}},
year = {2017},
url = {http://www.panderson.me/up-down-attention}
}

@misc{1612.08083,
Author = {Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
Title = {Language Modeling with Gated Convolutional Networks},
Year = {2016},
Eprint = {arXiv:1612.08083},
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Old stuff from NeurIPS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{san,
  author    = {Zichao Yang and
               Xiaodong He and
               Jianfeng Gao and
               Li Deng and
               Alexander J. Smola},
  title     = {Stacked Attention Networks for Image Question Answering},
  journal   = {CoRR},
  volume    = {abs/1511.02274},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.02274},
  archivePrefix = {arXiv},
  eprint    = {1511.02274},
  timestamp = {Mon, 13 Aug 2018 16:47:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/YangHGDS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{loss_combination,
Author = {Stephen James and Michael Bloesch and Andrew J. Davison},
Title = {Task-Embedded Control Networks for Few-Shot Imitation Learning},
Year = {2018},
Eprint = {arXiv:1810.03237},
}

@inproceedings{pennington2014glove,
  author = {Jeffrey Pennington and Richard Socher and Christopher D. Manning},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  title = {GloVe: Global Vectors for Word Representation},
  year = {2014},
  pages = {1532--1543},
  url = {http://www.aclweb.org/anthology/D14-1162},
}

@article{ntp,
  author    = {Danfei Xu and
               Suraj Nair and
               Yuke Zhu and
               Julian Gao and
               Animesh Garg and
               Li Fei{-}Fei and
               Silvio Savarese},
  title     = {Neural Task Programming: Learning to Generalize Across Hierarchical
               Tasks},
  journal   = {CoRR},
  volume    = {abs/1710.01813},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.01813},
  archivePrefix = {arXiv},
  eprint    = {1710.01813},
  timestamp = {Mon, 22 Jul 2019 14:55:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-01813},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{deep_embedding,
Author = {Jaeyong Sung and Ian Lenz and Ashutosh Saxena},
Title = {Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories},
Year = {2015},
Eprint = {arXiv:1509.07831},
}

@misc{translation_parameters,
Author = {Emmanouil Antonios Platanios and Mrinmaya Sachan and Graham Neubig and Tom Mitchell},
Title = {Contextual Parameter Generation for Universal Neural Machine Translation},
Year = {2018},
Eprint = {arXiv:1808.08493},
}

@misc{convlng,
Author = {Nal Kalchbrenner and Edward Grefenstette and Phil Blunsom},
Title = {A Convolutional Neural Network for Modelling Sentences},
Year = {2014},
Eprint = {arXiv:1404.2188},
}

@misc{cnp,
Author = {Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo J. Rezende and S. M. Ali Eslami},
Title = {Conditional Neural Processes},
Year = {2018},
Eprint = {arXiv:1807.01613},
}

@misc{cnmp,
Author = {M. Yunus Seker, Mert Imre, Justus Piatery, Emre Ugur},
Title = {Conditional Neural Movement Primitives},
Year = {2019},
}

@misc{visual_learning_1,
Author = {Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},
Title = {Watch, Try, Learn: Meta-Learning from Demonstrations and Reward},
Year = {2019},
Eprint = {arXiv:1906.03352},
}

@article{visual_learning_2,
  author    = {Yan Duan and
               Marcin Andrychowicz and
               Bradly C. Stadie and
               Jonathan Ho and
               Jonas Schneider and
               Ilya Sutskever and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {One-Shot Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1703.07326},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.07326},
  archivePrefix = {arXiv},
  eprint    = {1703.07326},
  timestamp = {Mon, 13 Aug 2018 16:47:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DuanASHSSAZ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vqa_1,
Author = {Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Judy Hoffman and Li Fei-Fei and C. Lawrence Zitnick and Ross Girshick},
Title = {Inferring and Executing Programs for Visual Reasoning},
Year = {2017},
Eprint = {arXiv:1705.03633},
}

@misc{vqa_2,
Author = {Kexin Yi and Jiajun Wu and Chuang Gan and Antonio Torralba and Pushmeet Kohli and Joshua B. Tenenbaum},
Title = {Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding},
Year = {2018},
}

@article{resnet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  archivePrefix = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/HeZRS15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



@article{pagel2011,
	Author = {Mark Pagel},
	Date-Added = {2017-04-28 17:04:48 +0000},
	Date-Modified = {2017-04-28 17:06:22 +0000},
	Journal = {TED Global},
	Title = {How language transformed humanity},
	Year = {2011}}

@article{balch1994communication,
	Author = {Balch, Tucker and Arkin, Ronald C},
	Journal = {Autonomous robots},
	Number = {1},
	Pages = {27--52},
	Publisher = {Springer},
	Title = {Communication in reactive multiagent robotic systems},
	Volume = {1},
	Year = {1994}}

@inproceedings{barbulescu2010distributed,
	Author = {Barbulescu, Laura and Rubinstein, Zachary B and Smith, Stephen F and Zimmerman, Terry L},
	Booktitle = {Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1-Volume 1},
	Organization = {International Foundation for Autonomous Agents and Multiagent Systems},
	Pages = {1331--1338},
	Title = {Distributed coordination of mobile agent teams: the advantage of planning ahead},
	Year = {2010}}

@article{shehory1998multi,
	Author = {Shehory, Onn and Sycara, Katia and Jha, Somesh},
	Journal = {Intelligent agents IV agent theories, architectures, and languages},
	Pages = {143--154},
	Publisher = {Springer},
	Title = {Multi-agent coordination through coalition formation},
	Year = {1998}}
%%

@inproceedings{mericcli2014interactive,
	Author = {Meri{\c{c}}li, Cetin and Klee, Steven D and Paparian, Jack and Veloso, Manuela},
	Booktitle = {Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems},
	Organization = {International Foundation for Autonomous Agents and Multiagent Systems},
	Pages = {1069--1076},
	Title = {An interactive approach for situated task specification through verbal instructions},
	Year = {2014}}

@inproceedings{she2014teaching,
	Author = {She, Lanbo and Cheng, Yu and Chai, Joyce Y and Jia, Yunyi and Yang, Shaohua and Xi, Ning},
	Booktitle = {Robot and Human Interactive Communication, 2014 RO-MAN: The 23rd IEEE International Symposium on},
	Organization = {IEEE},
	Pages = {868--873},
	Title = {Teaching robots new actions through natural language instructions},
	Year = {2014}}

@inproceedings{she2014back,
	Author = {She, Lanbo and Yang, Shaohua and Cheng, Yu and Jia, Yunyi and Chai, Joyce Y and Xi, Ning},
	Booktitle = {15th Annual Meeting of the Special Interest Group on Discourse and Dialogue},
	Title = {Back to the blocks world: Learning new actions through situated human-robot dialogue},
	Volume = {89},
	Year = {2014}}

@article{cuayahuitl2015robot,
	Author = {Cuay{\'a}huitl, Heriberto},
	Journal = {Proceedings of the New Frontiers in Human-Robot Interaction},
	Title = {Robot learning from verbal interaction: a brief survey},
	Year = {2015}}

@inproceedings{gemignani2015teaching,
	Author = {Gemignani, Guglielmo and Bastianelli, Emanuele and Nardi, Daniele},
	Booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
	Organization = {International Foundation for Autonomous Agents and Multiagent Systems},
	Pages = {851--859},
	Title = {Teaching robots parametrized executable plans through spoken interaction},
	Year = {2015}}



@inproceedings{marcoewerton_icra_2015,
author = {Ewerton, M and Neumann, G and Lioutikov, R and {Ben Amor}, H and Peters, J and Maeda, G},
booktitle = {Proceedings of the International Conference on Robotics and Automation (ICRA)},
title = {{Learning Multiple Collaborative Tasks with a Mixture of Interaction Primitives}},
year = {2015}
}

@INPROCEEDINGS{Heni2014ICRA,
  author = {Ben Amor, H. and Neumann, G. and Kamthe, S. and Kroemer, O. and Peters,
	J.},
  title = {Interaction Primitives for Human-Robot Cooperation Tasks },
  booktitle = {Proceedings of 2014 IEEE International Conference on Robotics and
	Automation (ICRA)},
  year = {2014}
}

@inproceedings{BenAmor2013,
author = {{Ben Amor}, Heni and Vogt, David and Ewerton, Marco and Berger, Erik and Jung, Bernhard and Peters, Jan},
booktitle = {2013 IEEE/RSJ International Conference on Intelligent Robots and Systems},
isbn = {978-1-4673-6358-7},
issn = {21530858},
month = nov,
pages = {3257--3264},
publisher = {IEEE},
title = {{Learning responsive robot behavior by imitation}},
year = {2013}
}

@inproceedings{amor2012generalization,
  title={Generalization of human grasping for multi-fingered robot hands},
  author={Amor, Heni Ben and Kroemer, Oliver and Hillenbrand, Ulrich and Neumann, Gerhard and Peters, Jan},
  booktitle={Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ International Conference on},
  pages={2043--2050},
  year={2012},
  organization={IEEE}
}

@inproceedings{berger2013inferring,
  title={Inferring guidance information in cooperative human-robot tasks},
  author={Berger, Erik and Vogt, David and Haji-Ghassemi, Nooshin and Jung, Bernhard and Amor, Heni Ben},
  booktitle={Humanoid Robots (Humanoids), 2013 13th IEEE-RAS International Conference on},
  pages={124--129},
  year={2013},
  organization={IEEE}
}


@inproceedings{amor2009kinesthetic,
  title={Kinesthetic bootstrapping: Teaching motor skills to humanoid robots through physical interaction},
  author={Amor, Heni Ben and Berger, Erik and Vogt, David and Jung, Bernhard},
  booktitle={Annual Conference on Artificial Intelligence},
  pages={492--499},
  year={2009},
  organization={Springer Berlin Heidelberg}
}

@article{wang2013probabilistic,
  title={Probabilistic movement modeling for intention inference in human--robot interaction},
  author={Wang, Zhikun and M{\"u}lling, Katharina and Deisenroth, Marc Peter and Ben Amor, Heni and Vogt, David and Sch{\"o}lkopf, Bernhard and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={7},
  pages={841--858},
  year={2013},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{maeda2014learning,
  title={Learning interaction for collaborative tasks with probabilistic movement primitives},
  author={Maeda, Guilherme and Ewerton, Marco and Lioutikov, Rudolf and Amor, Heni Ben and Peters, Jan and Neumann, Gerhard},
  booktitle={Humanoid Robots (Humanoids), 2014 14th IEEE-RAS International Conference on},
  pages={527--534},
  year={2014},
  organization={IEEE}
}



@inproceedings{Atkeson:1997:RLD:645526.657285,
 author = {Atkeson, Christopher G. and Schaal, Stefan},
 title = {Robot Learning From Demonstration},
 booktitle = {Proceedings of the Fourteenth International Conference on Machine Learning},
 series = {ICML '97},
 year = {1997},
 isbn = {1-55860-486-3},
 pages = {12--20},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=645526.657285},
 acmid = {657285},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA}
} 

@article{argall2009survey,
  title={A survey of robot learning from demonstration},
  author={Argall, Brenna D and Chernova, Sonia and Veloso, Manuela and Browning, Brett},
  journal={Robotics and autonomous systems},
  volume={57},
  number={5},
  pages={469--483},
  year={2009},
  publisher={North-Holland}
}

@article{kingma2015adam,
  title={Adam: A Method for Stochastic Optimization. 2015 ICLR},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2015}
}

@article{lopaovska,
author = {Lopatovska, Irene and Rink, Katrina and Knight, Ian and Raines, Kieran and Cosenza, Kevin and Williams, Harriet and Sorsche, Perachya and Hirsch, David and Li, Qi and Martinez, Adrianna},
year = {2018},
month = {03},
pages = {096100061875941},
title = {Talk to me: Exploring user interactions with the Amazon Alexa},
journal = {Journal of Librarianship and Information Science},
doi = {10.1177/0961000618759414}
}

@article{williams2017state,
  title={The state-of-the-art in autonomous wheelchairs controlled through natural language: A survey},
  author={Williams, Tom and Scheutz, Matthias},
  journal={Robotics and Autonomous Systems},
  volume={96},
  pages={171--183},
  year={2017},
  publisher={Elsevier}
}


@article{kress2009temporal,
  title={Temporal-logic-based reactive mission and motion planning},
  author={Kress-Gazit, Hadas and Fainekos, Georgios E and Pappas, George J},
  journal={IEEE transactions on robotics},
  volume={25},
  number={6},
  pages={1370--1381},
  year={2009},
  publisher={IEEE}
}

@inproceedings{antol2015vqa,
  title={Vqa: Visual question answering},
  author={Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2425--2433},
  year={2015}
}


@INPROCEEDINGS{Gopalan-RSS-18, 
    AUTHOR    = {Nakul Gopalan AND Dilip Arumugam AND Lawson Wong AND Stefanie Tellex}, 
    TITLE     = {Sequence-to-Sequence Language Grounding of Non-Markovian Task Specifications}, 
    BOOKTITLE = {Proceedings of Robotics: Science and Systems}, 
    YEAR      = {2018}, 
    ADDRESS   = {Pittsburgh, Pennsylvania}, 
    MONTH     = {June}, 
    DOI       = {10.15607/RSS.2018.XIV.067} 
} 

@inproceedings{raman2012temporal,
  title={Temporal logic robot mission planning for slow and fast actions},
  author={Raman, Vasumathi and Finucane, Cameron and Kress-Gazit, Hadas},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={251--256},
  year={2012},
  organization={IEEE}
}


@article{khansari2011learning,
  title={Learning stable nonlinear dynamical systems with gaussian mixture models},
  author={Khansari-Zadeh, S Mohammad and Billard, Aude},
  journal={IEEE Transactions on Robotics},
  volume={27},
  number={5},
  pages={943--957},
  year={2011},
  publisher={IEEE}
}

@article{ijspeert2013dynamical,
  title={Dynamical movement primitives: learning attractor models for motor behaviors},
  author={Ijspeert, Auke Jan and Nakanishi, Jun and Hoffmann, Heiko and Pastor, Peter and Schaal, Stefan},
  journal={Neural computation},
  volume={25},
  number={2},
  pages={328--373},
  year={2013},
  publisher={MIT Press}
}

@inproceedings{paraschos2013probabilistic,
  title={Probabilistic movement primitives},
  author={Paraschos, Alexandros and Daniel, Christian and Peters, Jan R and Neumann, Gerhard},
  booktitle={Advances in neural information processing systems},
  pages={2616--2624},
  year={2013}
}

@incollection{billard2008robot,
  title={Robot programming by demonstration},
  author={Billard, A. and Calinon, S. and Dillmann, R. and Schaal, S.},
  booktitle={Springer handbook of robotics},
  pages={1371--1394},
  year={2008},
  publisher={Springer Berlin Heidelberg}
}

@inproceedings{Mericli:2014:IAS:2617388.2617416,
 author = {Mericli, Cetin and Klee, Steven D. and Paparian, Jack and Veloso, Manuela},
 title = {An Interactive Approach for Situated Task Specification Through Verbal Instructions},
 booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems},
 series = {AAMAS '14},
 year = {2014},
 isbn = {978-1-4503-2738-1},
 location = {Paris, France},
 pages = {1069--1076},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2617388.2617416},
 acmid = {2617416},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {human-robot interaction, robot task specification}
} 

@inproceedings{Gemignani:2015:TRP:2772879.2773262,
 author = {Gemignani, Guglielmo and Bastianelli, Emanuele and Nardi, Daniele},
 title = {Teaching Robots Parametrized Executable Plans Through Spoken Interaction},
 booktitle = {Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems},
 series = {AAMAS '15},
 year = {2015},
 isbn = {978-1-4503-3413-6},
 location = {Istanbul, Turkey},
 pages = {851--859},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2772879.2773262},
 acmid = {2773262},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {communication and teamwork, human-robot interaction, robot planning and plan execution}
} 

@inproceedings{Nicolescu:2003:NMR:860575.860614,
 author = {Nicolescu, Monica N. and Mataric, Maja J.},
 title = {Natural Methods for Robot Task Learning: Instructive Demonstrations, Generalization and Practice},
 booktitle = {Proceedings of the Second International Joint Conference on Autonomous Agents and Multiagent Systems},
 series = {AAMAS '03},
 year = {2003},
 isbn = {1-58113-683-8},
 location = {Melbourne, Australia},
 pages = {241--248},
 numpages = {8},
 url = {http://doi.acm.org/10.1145/860575.860614},
 doi = {10.1145/860575.860614},
 acmid = {860614},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {human-robot interaction, learning by demonstration, robotics}
} 

@inproceedings{pomerleau1989alvinn,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  booktitle={Advances in neural information processing systems},
  pages={305--313},
  year={1989}
}

@InProceedings{finn17a,
  title = 	 {One-Shot Visual Imitation Learning via Meta-Learning},
  author = 	 {Chelsea Finn and Tianhe Yu and Tianhao Zhang and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 1st Annual Conference on Robot Learning},
  pages = 	 {357--368},
  year = 	 {2017},
  editor = 	 {Sergey Levine and Vincent Vanhoucke and Ken Goldberg},
  volume = 	 {78},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {},
  month = 	 {13--15 Nov},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v78/finn17a/finn17a.pdf},
  url = 	 {http://proceedings.mlr.press/v78/finn17a.html},
  abstract = 	 {In order for a robot to be a generalist that can perform a wide range of jobs, it must be able to acquire a wide variety of skills quickly and efficiently in complex unstructured environments. High-capacity models such as deep neural networks can enable a robot to represent complex skills, but learning each skill from scratch then becomes infeasible. In this work, we present a meta-imitation learning method that enables a robot to learn how to learn more efficiently, allowing it to acquire new skills from just a single demonstration. Unlike prior methods for one-shot imitation, our method can scale to raw pixel inputs and requires data from significantly fewer prior tasks for effective learning of new skills. Our experiments on both simulated and real robot platforms demonstrate the ability to learn new tasks, end-to-end, from a single visual demonstration.}
}

@article{Cederborg:2013:LMG:2712980.2713055,
 author = {Cederborg, Thomas and Oudeyer, Pierre-Yves},
 title = {From Language to Motor Gavagai: Unified Imitation Learning of Multiple Linguistic and Nonlinguistic Sensorimotor Skills},
 journal = {IEEE Trans. on Auton. Ment. Dev.},
 issue_date = {September 2013},
 volume = {5},
 number = {3},
 month = sep,
 year = {2013},
 issn = {1943-0604},
 pages = {222--239},
 numpages = {18},
 url = {http://dx.doi.org/10.1109/TAMD.2013.2279277},
 doi = {10.1109/TAMD.2013.2279277},
 acmid = {2713055},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA}
}

@article{akgun2015simultaneously,
  title={Simultaneously learning actions and goals from demonstration},
  author={Akgun, Baris and Thomaz, Andrea},
  journal={Autonomous Robots},
  pages={1--17},
  year={2015},
  publisher={Springer US}
}

@article{calinon2010learning,
  title={Learning and reproduction of gestures by imitation},
  author={Calinon, Sylvain and D'halluin, Florent and Sauser, Eric L and Caldwell, Darwin G and Billard, Aude G},
  journal={IEEE Robotics \& Automation Magazine},
  volume={17},
  number={2},
  pages={44--54},
  year={2010},
  publisher={IEEE}
}

@book{Calinon:2009:RPD:1795482,
 author = {Calinon, Sylvain},
 title = {Robot Programming by Demonstration},
 year = {2009},
 isbn = {1439808678, 9781439808672},
 edition = {1st},
 publisher = {CRC Press, Inc.},
 address = {Boca Raton, FL, USA}
} 

@book{Koller:2009:PGM:1795555,
 author = {Koller, Daphne and Friedman, Nir},
 title = {Probabilistic Graphical Models: Principles and Techniques - Adaptive Computation and Machine Learning},
 year = {2009},
 isbn = {0262013193, 9780262013192},
 publisher = {The MIT Press},
}

@inproceedings{pais2013learning,
  title={Learning robot skills through motion segmentation and constraints extraction},
  author={Pais, Lucia and Umezawa, Keisuke and Nakamura, Yoshihiko and Billard, Aude},
  booktitle={HRI Workshop on Collaborative Manipulation},
  year={2013}
}

@INPROCEEDINGS{7354120,
author={A. X. Lee and A. Gupta and H. Lu and S. Levine and P. Abbeel},
booktitle={2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
title={Learning from multiple demonstrations using trajectory-aware non-rigid registration with applications to deformable object manipulation},
year={2015},
pages={5265-5272},
keywords={generalisation (artificial intelligence);image registration;learning by example;learning systems;manipulators;robot vision;trajectory control;deformable object manipulation;generalization;learning from demonstration;nonlinear transformation;nonrigid point cloud registration;object grasping;towel folding;trajectory-aware nonrigid registration;visual variation;Probabilistic logic;Registers;Robots;Splines (mathematics);Three-dimensional displays;Trajectory;Yttrium},
doi={10.1109/IROS.2015.7354120},
month={Sept},}

@inproceedings{rybski2007interactive,
  title={Interactive robot task training through dialog and demonstration},
  author={Rybski, Paul E and Yoon, Kevin and Stolarz, Jeremy and Veloso, Manuela M},
  booktitle={Proceedings of the ACM/IEEE international conference on Human-robot interaction},
  pages={49--56},
  year={2007},
  organization={ACM}
}


@Article{Tellex2014,
author="Tellex, Stefanie
and Thaker, Pratiksha
and Joseph, Joshua
and Roy, Nicholas",
title="Learning perceptually grounded word meanings from unaligned parallel data",
journal="Machine Learning",
year="2014",
month="Feb",
day="01",
volume="94",
number="2",
pages="151--167",
abstract="In order for robots to effectively understand natural language commands, they must be able to acquire meaning representations that can be mapped to perceptual features in the external world. Previous approaches to learning these grounded meaning representations require detailed annotations at training time. In this paper, we present an approach to grounded language acquisition which is capable of jointly learning a policy for following natural language commands such as ``Pick up the tire pallet,'' as well as a mapping between specific phrases in the language and aspects of the external world; for example the mapping between the words ``the tire pallet'' and a specific object in the environment. Our approach assumes a parametric form for the policy that the robot uses to choose actions in response to a natural language command that factors based on the structure of the language. We use a gradient method to optimize model parameters. Our evaluation demonstrates the effectiveness of the model on a corpus of commands given to a robotic forklift by untrained users.",
issn="1573-0565",
doi="10.1007/s10994-013-5383-2",
url="https://doi.org/10.1007/s10994-013-5383-2"
}

@inproceedings{Mericli2014,
 author = {Mericli, Cetin and Klee, Steven D. and Paparian, Jack and Veloso, Manuela},
 title = {An Interactive Approach for Situated Task Specification Through Verbal Instructions},
 booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-agent Systems},
 series = {AAMAS '14},
 year = {2014},
 isbn = {978-1-4503-2738-1},
 location = {Paris, France},
 pages = {1069--1076},
 numpages = {8},
 url = {http://dl.acm.org/citation.cfm?id=2617388.2617416},
 acmid = {2617416},
 publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
 address = {Richland, SC},
 keywords = {human-robot interaction, robot task specification},
}


@article{Gal2015DropoutB,
Author = {Yarin Gal and Zoubin Ghahramani},
Title = {Dropout as a {B}ayesian Approximation: Representing Model Uncertainty in Deep Learning},
Year = {2015},
Journal = {arXiv:1506.02142},
}

@article{schaal1999imitation,
  title={Is imitation learning the route to humanoid robots?},
  author={Schaal, Stefan},
  journal={Trends in cognitive sciences},
  volume={3},
  number={6},
  pages={233--242},
  year={1999},
  publisher={Elsevier}
}

@book{calinon2009robot,
  title={Robot programming by demonstration},
  author={Calinon, Sylvain},
  year={2009},
  publisher={EPFL Press}
}

@inproceedings{amor2014interaction,
  title={Interaction primitives for human-robot cooperation tasks},
  author={Amor, Heni Ben and Neumann, Gerhard and Kamthe, Sanket and Kroemer, Oliver and Peters, Jan},
  booktitle={2014 IEEE international conference on robotics and automation (ICRA)},
  pages={2831--2837},
  year={2014},
  organization={IEEE}
}

@article{mulling2013learning,
  title={Learning to select and generalize striking movements in robot table tennis},
  author={M{\"u}lling, Katharina and Kober, Jens and Kroemer, Oliver and Peters, Jan},
  journal={The International Journal of Robotics Research},
  volume={32},
  number={3},
  pages={263--279},
  year={2013},
  publisher={Sage Publications Sage UK: London, England}
}

@inproceedings{chalodhorn2007learning,
  title={Learning to Walk through Imitation.},
  author={Chalodhorn, Rawichote and Grimes, David B and Grochow, Keith and Rao, Rajesh PN},
  booktitle={IJCAI},
  volume={7},
  pages={2084--2090},
  year={2007}
}

@misc{loss_combination,
Author = {Stephen James and Michael Bloesch and Andrew J. Davison},
Title = {Task-Embedded Control Networks for Few-Shot Imitation Learning},
Year = {2018},
Eprint = {arXiv:1810.03237},
}

@article{ntp,
  author    = {Danfei Xu and
               Suraj Nair and
               Yuke Zhu and
               Julian Gao and
               Animesh Garg and
               Li Fei{-}Fei and
               Silvio Savarese},
  title     = {Neural Task Programming: Learning to Generalize Across Hierarchical
               Tasks},
  journal   = {CoRR},
  volume    = {abs/1710.01813},
  year      = {2017},
  url       = {http://arxiv.org/abs/1710.01813},
  archivePrefix = {arXiv},
  eprint    = {1710.01813},
  timestamp = {Mon, 22 Jul 2019 14:55:30 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1710-01813},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{deep_embedding,
Author = {Jaeyong Sung and Ian Lenz and Ashutosh Saxena},
Title = {Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories},
Year = {2015},
Eprint = {arXiv:1509.07831},
}

@misc{translation_parameters,
Author = {Emmanouil Antonios Platanios and Mrinmaya Sachan and Graham Neubig and Tom Mitchell},
Title = {Contextual Parameter Generation for Universal Neural Machine Translation},
Year = {2018},
Eprint = {arXiv:1808.08493},
}

@misc{convlng,
Author = {Nal Kalchbrenner and Edward Grefenstette and Phil Blunsom},
Title = {A Convolutional Neural Network for Modelling Sentences},
Year = {2014},
Eprint = {arXiv:1404.2188},
}

@misc{cnp,
Author = {Marta Garnelo and Dan Rosenbaum and Chris J. Maddison and Tiago Ramalho and David Saxton and Murray Shanahan and Yee Whye Teh and Danilo J. Rezende and S. M. Ali Eslami},
Title = {Conditional Neural Processes},
Year = {2018},
Eprint = {arXiv:1807.01613},
}

@misc{cnmp,
Author = {M. Yunus Seker, Mert Imre, Justus Piatery, Emre Ugur},
Title = {Conditional Neural Movement Primitives},
Year = {2019},
}

@misc{visual_learning_1,
Author = {Allan Zhou and Eric Jang and Daniel Kappler and Alex Herzog and Mohi Khansari and Paul Wohlhart and Yunfei Bai and Mrinal Kalakrishnan and Sergey Levine and Chelsea Finn},
Title = {Watch, Try, Learn: Meta-Learning from Demonstrations and Reward},
Year = {2019},
Eprint = {arXiv:1906.03352},
}


@article{Arumugam2019,
abstract = {Language grounding is broadly defined as the problem of mapping natural language instructions to robot behavior. To truly be effective, these language grounding systems must be accurate in their selection of behavior, efficient in the robot's realization of that selected behavior, and capable of generalizing beyond commands and environment configurations only seen at training time. One choice that is crucial to the success of a language grounding model is the choice of representation used to capture the objective specified by the input command. Prior work has been varied in its use of explicit goal representations, with some approaches lacking a representation altogether, resulting in models that infer whole sequences of robot actions, while other approaches map to carefully constructed logical form representations. While many of the models in either category are reasonably accurate, they fail to offer either efficient execution or any generalization without requiring a large amount of manual specification. In this work, we take a first step towards language grounding models that excel across accuracy, efficiency, and generalization through the construction of simple, semantic goal representations within Markov decision processes. We propose two related semantic goal representations that take advantage of the hierarchical structure of tasks and the compositional nature of language respectively, and present multiple grounding models for each. We validate these ideas empirically with results collected from following text instructions within a simulated mobile-manipulator domain, as well as demonstrations of a physical robot responding to spoken instructions in real time. Our grounding models tie abstraction in language commands to a hierarchical planner for the robot's execution, enabling a response-time speed-up of several orders of magnitude over baseline planners within sufficiently large domains. Concurrently, our grounding models for generalization infer elements of the semantic representation that are subsequently combined to form a complete goal description, enabling the interpretation of commands involving novel combinations never seen during training. Taken together, our results show that the design of semantic goal representation has powerful implications for the accuracy, efficiency, and generalization capabilities of language grounding models.},
author = {Arumugam, Dilip and Karamcheti, Siddharth and Gopalan, Nakul and Williams, {\textperiodcentered} Edward C and Rhee, Mina and Lawson, {\textperiodcentered} and Wong, L S and Tellex, Stefanie},
doi = {10.1007/s10514-018-9792-8},
journal = {Autonomous Robots},
mendeley-groups = {ICLR Reviews},
pages = {449--468},
title = {{Grounding natural language instructions to semantic goal representations for abstraction and generalization}},
url = {https://doi.org/10.1007/s10514-018-9792-8},
volume = {43},
year = {2019}
}

@incollection{NIPS2019_8297,
title = {ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks},
author = {Lu, Jiasen and Batra, Dhruv and Parikh, Devi and Lee, Stefan},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {13--23},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@incollection{NIPS2019_8329,
title = {Chasing Ghosts: Instruction Following as Bayesian State Tracking},
author = {Anderson, Peter and Shrivastava, Ayush and Parikh, Devi and Batra, Dhruv and Lee, Stefan},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {369--379},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@article{Sung2015,
archivePrefix = {arXiv},
arxivId = {1509.07831},
author = {Sung, Jaeyong and Lenz, Ian and Saxena, Ashutosh},
eprint = {1509.07831},
file = {:home/local/ASUAD/sstepput/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sung, Lenz, Saxena - 2015 - Deep Multimodal Embedding Manipulating Novel Objects with Point-clouds, Language and Trajectories.pdf:pdf},
mendeley-groups = {Language},
month = {sep},
title = {{Deep Multimodal Embedding: Manipulating Novel Objects with Point-clouds, Language and Trajectories}},
url = {http://arxiv.org/abs/1509.07831},
year = {2015}
}

@article{visual_learning_2,
  author    = {Yan Duan and
               Marcin Andrychowicz and
               Bradly C. Stadie and
               Jonathan Ho and
               Jonas Schneider and
               Ilya Sutskever and
               Pieter Abbeel and
               Wojciech Zaremba},
  title     = {One-Shot Imitation Learning},
  journal   = {CoRR},
  volume    = {abs/1703.07326},
  year      = {2017},
  url       = {http://arxiv.org/abs/1703.07326},
  archivePrefix = {arXiv},
  eprint    = {1703.07326},
  timestamp = {Mon, 13 Aug 2018 16:47:41 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/DuanASHSSAZ17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vqa_1,
Author = {Justin Johnson and Bharath Hariharan and Laurens van der Maaten and Judy Hoffman and Li Fei-Fei and C. Lawrence Zitnick and Ross Girshick},
Title = {Inferring and Executing Programs for Visual Reasoning},
Year = {2017},
Eprint = {arXiv:1705.03633},
}

@misc{vqa_2,
Author = {Kexin Yi and Jiajun Wu and Chuang Gan and Antonio Torralba and Pushmeet Kohli and Joshua B. Tenenbaum},
Title = {Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding},
Year = {2018},
}

@misc{gal2015dropout,
    title={Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
    author={Yarin Gal and Zoubin Ghahramani},
    year={2015},
    eprint={1506.02142},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{james2019pyrep,
  title={PyRep: Bringing V-REP to Deep Robot Learning},
  author={James, Stephen and Freese, Marc and Davison, Andrew J.},
  journal={arXiv preprint arXiv:1906.11176},
  year={2019}
}

@inproceedings{coppeliaSim,
author={E. Rohmer and S. P. N. Singh and M. Freese},
title={CoppeliaSim (formerly V-REP): a Versatile and Scalable Robot Simulation Framework},
booktitle={Proc. of The International Conference on Intelligent Robots and Systems (IROS)},
year={2013},
note={www.coppeliarobotics.com}

}