% Encoding: UTF-8

@Article{FastISTA,
  author   = {Beck, Amir and Teboulle, Marc},
  title    = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
  journal  = {SIAM Journal on Imaging Sciences},
  year     = {2009},
  volume   = {2},
  number   = {1},
  pages    = {183-202},
  doi      = {doi:10.1137/080716542},
  keywords = {90C25,90C06,65F22},
  type     = {Journal Article},
  url      = {http://epubs.siam.org/doi/abs/10.1137/080716542},
}

@Article{RPCA,
  author  = {Cand\`{e}s, Emmanuel J. and Li, Xiaodong and Ma, Yi and Wright, John},
  title   = {Robust principal component analysis?},
  journal = {J. ACM},
  year    = {2011},
  volume  = {58},
  number  = {3},
  pages   = {1-37},
  issn    = {0004-5411},
  doi     = {10.1145/1970392.1970395},
  type    = {Journal Article},
}

@Article{,
  author   = {Fang, X. and Xu, Y. and Li, X. and Lai, Z. and Wong, W. K.},
  title    = {Robust Semi-Supervised Subspace Clustering via Non-Negative Low-Rank Representation},
  journal  = {Cybernetics, IEEE Transactions on},
  year     = {2015},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  doi      = {10.1109/TCYB.2015.2454521},
  issn     = {2168-2267},
  keywords = {Clustering algorithms Clustering methods Matrix converters Optimization Principal component analysis Robustness Sparse matrices Affinity matrix low-rank representation (LRR) subspace clustering supervision information},
  type     = {Journal Article},
}

@InProceedings{,
  author    = {Karasuyama, Masayuki and Mamitsuka, Hiroshi},
  title     = {Manifold-based Similarity Adaptation for Label Propagation},
  booktitle = {Advances in Neural Information Processing Systems 26 (NIPS 2013)},
  type      = {Conference Proceedings},
}

@Article{LRR_PAMI_2013,
  author   = {Liu, G. and Lin, Z. and Yan, S. and Sun, J. and Yu, Y. and Ma, Y.},
  title    = {Robust Recovery of Subspace Structures by Low-Rank Representation},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2013},
  volume   = {35},
  number   = {1},
  pages    = {171-184},
  doi      = {10.1109/TPAMI.2012.88},
  issn     = {0162-8828},
  keywords = {convex programming pattern clustering sparse matrices LRR convex program data sample vectors dictionary low-rank representation objective function outlier removal robust subspace structure recovery row space sparse error correction subspace clustering problem subspace membership Data models Dictionaries Noise Optimization Polynomials Robustness Vectors outlier detection segmentation subspace clustering Algorithms Artificial Intelligence Computer Simulation Models, Theoretical Pattern Recognition, Automated Signal Processing, Computer-Assisted},
  type     = {Journal Article},
}

@InProceedings{LatentLRR,
  author    = {Liu, G. and Yan, S.},
  title     = {Latent Low-Rank Representation for subspace segmentation and feature extraction},
  booktitle = {2011 International Conference on Computer Vision},
  pages     = {1615-1622},
  doi       = {10.1109/ICCV.2011.6126422},
  isbn      = {1550-5499},
  keywords  = {convex programming data encapsulation data structures feature extraction image segmentation matrix algebra minimisation vectors LatLRR convex problem corrupted data data vectors dictionary dimension reduction based methods hidden data latent low-rank representation multiple subspace data structures nuclear norm minimization problem observed data matrix salient features state-of-the-art algorithms subspace segmentation algorithm unsupervised feature extraction algorithm Dictionaries Motion segmentation Noise Robustness Strontium},
  type      = {Conference Proceedings},
}

@Article{,
  author  = {Parsons, Lance and Haque, Ehtesham and Liu, Huan},
  title   = {Subspace clustering for high dimensional data: a review},
  journal = {SIGKDD Explor. Newsl.},
  year    = {2004},
  volume  = {6},
  number  = {1},
  pages   = {90-105},
  doi     = {10.1145/1007730.1007731},
  issn    = {1931-0145},
  type    = {Journal Article},
}

@Article{,
  author   = {Patel, V. M. and Hien Van, Nguyen and Vidal, R.},
  title    = {Latent Space Sparse and Low-Rank Subspace Clustering},
  journal  = {Selected Topics in Signal Processing, IEEE Journal of},
  year     = {2015},
  volume   = {9},
  number   = {4},
  pages    = {691-701},
  doi      = {10.1109/JSTSP.2015.2402643},
  issn     = {1932-4553},
  keywords = {data reduction matrix algebra optimisation pattern clustering cluster labels data clustering data projection dimensionality reduction kernel methods latent space sparse low-dimensional latent space low-rank coefficients low-rank subspace clustering optimization methods similarity matrix spectral clustering Clustering algorithms Clustering methods Cost function Kernel Signal processing algorithms Sparse matrices Dimension reduction manifold clustering sparse subspace clustering subspace clustering},
  type     = {Journal Article},
}

@Article{,
  author   = {Peng, Yong and Lu, Bao-Liang and Wang, Suhang},
  title    = {Enhanced low-rank representation via sparse manifold adaption for semi-supervised learning},
  journal  = {Neural Networks},
  year     = {2015},
  volume   = {65},
  pages    = {1-17},
  abstract = {Constructing an informative and discriminative graph plays an important role in various pattern recognition tasks such as clustering and classification. Among the existing graph-based learning models, low-rank representation (LRR) is a very competitive one, which has been extensively employed in spectral clustering and semi-supervised learning (SSL). In SSL, the graph is composed of both labeled and unlabeled samples, where the edge weights are calculated based on the LRR coefficients. However, most of existing LRR related approaches fail to consider the geometrical structure of data, which has been shown beneficial for discriminative tasks. In this paper, we propose an enhanced LRR via sparse manifold adaption, termed manifold low-rank representation (MLRR), to learn low-rank data representation. MLRR can explicitly take the data local manifold structure into consideration, which can be identified by the geometric sparsity idea; specifically, the local tangent space of each data point was sought by solving a sparse representation objective. Therefore, the graph to depict the relationship of data points can be built once the manifold information is obtained. We incorporate a regularizer into LRR to make the learned coefficients preserve the geometric constraints revealed in the data space. As a result, MLRR combines both the global information emphasized by low-rank property and the local information emphasized by the identified manifold structure. Extensive experimental results on semi-supervised classification tasks demonstrate that MLRR is an excellent method in comparison with several state-of-the-art graph construction approaches.},
  doi      = {http://dx.doi.org/10.1016/j.neunet.2015.01.001},
  issn     = {0893-6080},
  keywords = {Low-rank representation Sparse manifold adaption Graph construction Semi-supervised learning Face recognition},
  type     = {Journal Article},
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608015000027},
}

@article{Zhang2012,
   author = {Zhang, Teng and Szlam, Arthur and Wang, Yi and Lerman, Gilad},
   title = {Hybrid Linear Modeling via Local Best-Fit Flats},
   journal = {International Journal of Computer Vision},
   volume = {100},
   number = {3},
   pages = {217-240},
   abstract = {We present a simple and fast geometric method for modeling data by a union of affine subspaces. The method begins by forming a collection of local best-fit affine subspaces, i.e., subspaces approximating the data in local neighborhoods. The correct sizes of the local neighborhoods are determined automatically by the Jones’ β 2 numbers (we prove under certain geometric conditions that our method finds the optimal local neighborhoods). The collection of subspaces is further processed by a greedy selection procedure or a spectral method to generate the final model. We discuss applications to tracking-based motion segmentation and clustering of faces under different illuminating conditions. We give extensive experimental evidence demonstrating the state of the art accuracy and speed of the suggested algorithms on these problems and also on synthetic hybrid linear data as well as the MNIST handwritten digits data; and we demonstrate how to use our algorithms for fast determination of the number of affine subspaces.},
   ISSN = {1573-1405},
   DOI = {10.1007/s11263-012-0535-6},
   url = {http://dx.doi.org/10.1007/s11263-012-0535-6},
   year = {2012},
   type = {Journal Article}
}

@Article{,
  author   = {Zhao, Miaoyun and Jiao, Licheng and Ma, Wenping and Liu, Hongying and Yang, Shuyuan},
  title    = {Classification and saliency detection by semi-supervised low-rank representation},
  journal  = {Pattern Recognition},
  year     = {2016},
  volume   = {51},
  pages    = {281-294},
  abstract = {In the area of pattern recognition, Low Rank Representation (LRR) is an efficient method in recovering the subspace structure of the dataset. However, LRR is unsupervised. Without any label information, LRR constructs an informative graph which is then combined with the mature graph-based semi-supervised learning (GSSL) framework to complete the classification task. In this paper, we propose a new low rank learning method which constructs the low rank representation matrix utilizing label information to obtain a more informative graph. This method integrates the low rank graph construction and the label information propagation processes together. Thus the optimization of the low rank representation and the soft label prediction function are calculated iteratively at the same time. We name this method as Semi-Supervised Low Rank Learning (SSLRL). It enhanced the classification performance of traditional LRR-Graph based SSL by 5–30% and the running time is reduced from hundreds to less than ten seconds. Based on this method, a new outlier detection strategy is presented. This strategy succeeds with an AUC of at least 93% even if the detection condition of LRR is not satisfied. The effectiveness of SSLRL is demonstrated in semi-supervised classification, outlier detection, and salient detection tasks. These extensive experimental results highlight the outperforming of our method over state-of-the-art methods.},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2015.09.008},
  issn     = {0031-3203},
  keywords = {Low rank representation Semi-supervised learning Outlier detection Saliency detection},
  type     = {Journal Article},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320315003398},
}

@InProceedings{,
  author    = {Zhuang, L. and Gao, H. and Lin, Z. and Ma, Y. and Zhang, X. and Yu, N.},
  title     = {Non-negative low rank and sparse graph for semi-supervised learning},
  booktitle = {Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on},
  pages     = {2328-2335},
  doi       = {10.1109/CVPR.2012.6247944},
  isbn      = {1063-6919},
  keywords  = {data structures graph theory learning (artificial intelligence) matrix algebra pattern classification pattern clustering NNLRS-graph clustering task data structure representation discriminative analysis locally linear structure machine learning tasks nonnegative low rank-and-sparse graph nonnegative low-rank-and-sparse matrix semisupervised classification semisupervised learning subspaces structure Databases Educational institutions Noise Optimization Sparse matrices Strontium Vectors},
  type      = {Conference Proceedings},
}

@InProceedings{MC_Universal2014,
  author    = {Bhojanapalli, Srinadh and Jain, Prateek},
  title     = {Universal Matrix Completion},
  booktitle = {Proceedings of The 31st International Conference on Machine Learning},
  year      = {2014},
  pages     = {1881-1889},
  publisher = {JMLR},
  type      = {Conference Proceedings},
}

@Article{CaiCandesShen2010,
  author   = {Cai, Jian-Feng and Cand\`{e}s, Emmanuel J. and Shen, Zuowei},
  title    = {A Singular Value Thresholding Algorithm for Matrix Completion},
  journal  = {SIAM Journal on Optimization},
  year     = {2010},
  volume   = {20},
  number   = {4},
  pages    = {1956-1982},
  doi      = {doi:10.1137/080738970},
  keywords = {90C25,15A83,65K05},
  type     = {Journal Article},
  url      = {http://epubs.siam.org/doi/abs/10.1137/080738970},
}

@Article{CandesPlan2010,
  author   = {Cand\`{e}s, E. J. and Plan, Y.},
  title    = {Matrix Completion With Noise},
  journal  = {Proceedings of the IEEE},
  year     = {2010},
  volume   = {98},
  number   = {6},
  pages    = {925-936},
  issn     = {0018-9219},
  doi      = {10.1109/JPROC.2009.2035722},
  keywords = {data integrity matrix algebra minimisation noise signal sampling compressed sensing convex optimization problem data constraints low rank matrices matrix completion nuclear norm minimization Collaboration Computer vision Filtering Frequency Linear matrix inequalities Machine learning Motion pictures Noise level Remote sensing duality in optimization low-rank matrices nuclear-norm minimization oracle inequalities semidefinite programming},
  type     = {Journal Article},
}

@Article{CandesRecht2009,
  author   = {Cand\`{e}s, Emmanuel J. and Recht, Benjamin},
  title    = {Exact Matrix Completion via Convex Optimization},
  journal  = {Foundations of Computational Mathematics},
  year     = {2009},
  volume   = {9},
  number   = {6},
  pages    = {717-772},
  issn     = {1615-3383},
  abstract = {We consider a problem of considerable practical interest: the recovery of a data matrix from a sampling of its entries. Suppose that we observe m entries selected uniformly at random from a matrix M. Can we complete the matrix and recover the entries that we have not seen?},
  doi      = {10.1007/s10208-009-9045-5},
  type     = {Journal Article},
  url      = {http://dx.doi.org/10.1007/s10208-009-9045-5},
}

@Misc{Netflix2006,
  author = {Netflix},
  title  = {Netflx Prize},
  year   = {2006},
  type   = {Web Page},
}

@Unpublished{ZhangYangJinEtAl2015,
  author  = {Zhang, Lijun and Yang, Tianbao and Jin, Rong and Zhou, Zhi-Hua},
  title   = {Analysis of Nuclear Norm Regularization for Full-rank Matrix Completion},
  year    = {2015},
  address = {arXiv:1504.06817v1 [cs.LG]},
  type    = {Unpublished Work},
}

@InProceedings{,
  author    = {Patel, V. M. and Vidal, R.},
  title     = {Kernel sparse subspace clustering},
  booktitle = {2014 IEEE International Conference on Image Processing (ICIP)},
  pages     = {2849-2853},
  doi       = {10.1109/ICIP.2014.7025576},
  isbn      = {1522-4880},
  keywords  = {computer vision data handling pattern clustering Kernel sparse subspace clustering data points data representation image processing kernel sparse representations kernel trick nonlinear manifolds nonlinear mappings Clustering algorithms Conferences Kernel Manifolds Pattern recognition Signal processing algorithms Subspace clustering kernel methods non-linear subspace clustering sparse subspace clustering},
  type      = {Conference Proceedings},
}

@InProceedings{,
  author    = {Goh, A. and Vidal, R.},
  title     = {Segmenting Motions of Different Types by Unsupervised Manifold Clustering},
  booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  pages     = {1-6},
  doi       = {10.1109/CVPR.2007.383235},
  isbn      = {1063-6919},
  keywords  = {image motion analysis image segmentation image sequences motion sequences multiple affine multiple motions segmentation nonlinear dimensionality reduction perspective views unsupervised manifold clustering Algorithm design and analysis Analysis of variance Clustering algorithms Computer vision Data analysis Functional analysis Motion segmentation Null space Testing Vectors},
  type      = {Conference Proceedings},
}

@Article{RKLRR,
  author   = {Xiao, S. and Tan, M. and Xu, D. and Dong, Z. Y.},
  title    = {Robust Kernel Low-Rank Representation},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2015},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  doi      = {10.1109/TNNLS.2015.2472284},
  issn     = {2162-237X},
  keywords = {Closed-form solutions Convergence Face Kernel Linear programming Optimization Robustness Low-rank representation (LRR) kernel methods.},
  type     = {Journal Article},
}

@Article{SSC_PAMIN_2013,
  author   = {Elhamifar, E. and Vidal, R.},
  title    = {Sparse Subspace Clustering: Algorithm, Theory, and Applications},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2013},
  volume   = {35},
  number   = {11},
  pages    = {2765-2781},
  doi      = {10.1109/TPAMI.2013.57},
  issn     = {0162-8828},
  keywords = {computational complexity convex programming data structures minimisation pattern clustering convex relaxation data point clustering data point representation face clustering general NP-hard problem high-dimensional data collection minimization program motion segmentation sparse optimization program sparse representation sparse subspace clustering algorithm spectral clustering framework synthetic data Clustering algorithms Computer vision Face Noise Optimization Sparse matrices Vectors $(ell_1)$-minimization High-dimensional data clustering intrinsic low-dimensionality principal angles spectral clustering subspaces Algorithms Artificial Intelligence Biometry Humans Image Interpretation, Computer-Assisted Pattern Recognition, Automated Sample Size},
  type     = {Journal Article},
}

@Article{SC_tutorial2011,
  author   = {Vidal, R.},
  title    = {Subspace Clustering},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2011},
  volume   = {28},
  number   = {2},
  pages    = {52-68},
  doi      = {10.1109/MSP.2010.939739},
  issn     = {1053-5888},
  keywords = {computer vision face recognition image motion analysis image segmentation pattern clustering affine subspace face clustering problem high-dimensional data set clustering linear subspace motion segmentation subspace clustering Clustering algorithms Data models Noise Polynomials Principal component analysis Signal processing algorithms Subspace constraints},
  type     = {Journal Article},
}

@Article{,
  author   = {Yang, Allen Y. and Wright, John and Ma, Yi and Sastry, S. Shankar},
  title    = {Unsupervised segmentation of natural images via lossy data compression},
  journal  = {Computer Vision and Image Understanding},
  year     = {2008},
  volume   = {110},
  number   = {2},
  pages    = {212-225},
  abstract = {In this paper, we cast natural-image segmentation as a problem of clustering texture features as multivariate mixed data. We model the distribution of the texture features using a mixture of Gaussian distributions. Unlike most existing clustering methods, we allow the mixture components to be degenerate or nearly-degenerate. We contend that this assumption is particularly important for mid-level image segmentation, where degeneracy is typically introduced by using a common feature representation for different textures in an image. We show that such a mixture distribution can be effectively segmented by a simple agglomerative clustering algorithm derived from a lossy data compression approach. Using either 2D texture filter banks or simple fixed-size windows to obtain texture features, the algorithm effectively segments an image by minimizing the overall coding length of the feature vectors. We conduct comprehensive experiments to measure the performance of the algorithm in terms of visual evaluation and a variety of quantitative indices for image segmentation. The algorithm compares favorably against other well-known image-segmentation methods on the Berkeley image database.},
  doi      = {http://dx.doi.org/10.1016/j.cviu.2007.07.005},
  issn     = {1077-3142},
  keywords = {Image segmentation Texture segmentation Lossy compression Mixture of Gaussian distributions Clustering},
  type     = {Journal Article},
  url      = {http://www.sciencedirect.com/science/article/pii/S1077314207001300},
}

@Article{Luxburg2007,
  author   = {Luxburg, Ulrike},
  title    = {A tutorial on spectral clustering},
  journal  = {Statistics and Computing},
  year     = {2007},
  volume   = {17},
  number   = {4},
  pages    = {395-416},
  abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
  doi      = {10.1007/s11222-007-9033-z},
  issn     = {1573-1375},
  type     = {Journal Article},
  url      = {http://dx.doi.org/10.1007/s11222-007-9033-z},
}

@Article{Vidal2007,
  author   = {Vidal, René and Tron, Roberto and Hartley, Richard},
  title    = {Multiframe Motion Segmentation with Missing Data Using PowerFactorization and GPCA},
  journal  = {International Journal of Computer Vision},
  year     = {2007},
  volume   = {79},
  number   = {1},
  pages    = {85-105},
  abstract = {We consider the problem of segmenting multiple rigid-body motions from point correspondences in multiple affine views. We cast this problem as a subspace clustering problem in which point trajectories associated with each motion live in a linear subspace of dimension two, three or four. Our algorithm involves projecting all point trajectories onto a 5-dimensional subspace using the SVD, the PowerFactorization method, or RANSAC, and fitting multiple linear subspaces representing different rigid-body motions to the points in ℝ5 using GPCA. Unlike previous work, our approach does not restrict the motion subspaces to be four-dimensional and independent. Instead, it deals gracefully with all the spectrum of possible affine motions: from two-dimensional and partially dependent to four-dimensional and fully independent. Our algorithm can handle the case of missing data, meaning that point tracks do not have to be visible in all images, by using the PowerFactorization method to project the data. In addition, our method can handle outlying trajectories by using RANSAC to perform the projection. We compare our approach to other methods on a database of 167 motion sequences with full motions, independent motions, degenerate motions, partially dependent motions, missing data, outliers, etc. On motion sequences with complete data our method achieves a misclassification error of less that 5% for two motions and 29% for three motions.},
  doi      = {10.1007/s11263-007-0099-z},
  issn     = {1573-1405},
  type     = {Journal Article},
  url      = {http://dx.doi.org/10.1007/s11263-007-0099-z},
}

@InBook{Yan2006,
  pages     = {94-106},
  title     = {A General Framework for Motion Segmentation: Independent, Articulated, Rigid, Non-rigid, Degenerate and Non-degenerate},
  publisher = {Springer Berlin Heidelberg},
  year      = {2006},
  author    = {Yan, Jingyu and Pollefeys, Marc},
  editor    = {Leonardis, Aleš and Bischof, Horst and Pinz, Axel},
  type      = {Book Section},
  address   = {Berlin, Heidelberg},
  booktitle = {Computer Vision – ECCV 2006: 9th European Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part IV},
  doi       = {10.1007/11744085_8},
  isbn      = {978-3-540-33839-0},
  url       = {http://dx.doi.org/10.1007/11744085_8},
}

@Article{,
  author   = {Vidal, R. and Yi, Ma and Sastry, S.},
  title    = {Generalized principal component analysis (GPCA)},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2005},
  volume   = {27},
  number   = {12},
  pages    = {1945-1959},
  doi      = {10.1109/TPAMI.2005.244},
  issn     = {0162-8828},
  keywords = {computer vision expectation-maximisation algorithm geometry polynomials principal component analysis algebro-geometric solution expectation maximization generalized principal component analysis homogeneous polynomials iterative techniques polynomial factorization subspace segmentation Application software Clustering algorithms Iterative algorithms Kernel Machine learning Matrix decomposition Motion segmentation Index Terms- Principal component analysis (PCA) Veronese map dimensionality reduction dynamic scenes and motion segmentation. temporal video segmentation Algorithms Artificial Intelligence Cluster Analysis Computer Simulation Image Enhancement Image Interpretation, Computer-Assisted Imaging, Three-Dimensional Information Storage and Retrieval Models, Statistical Pattern Recognition, Automated},
  type     = {Journal Article},
}

@Article{ref1,
  author   = {Gear, C.W.},
  title    = {Multibody Grouping from Motion Images},
  journal  = {International Journal of Computer Vision},
  year     = {1998},
  volume   = {29},
  number   = {2},
  pages    = {133-150},
  abstract = {We want to deduce, from a sequence of noisy two-dimensional images of a scene of several rigid bodies moving independently in three dimensions, the number of bodies and the grouping of given feature points in the images to the bodies. Prior processing is assumed to have identified features or points common to all frames and the images are assumed to be created by orthographic projection (i.e., perspective effects are minimal). We describe a computationally inexpensive algorithm that can determine which points or features belong to which rigid body using the fact that, with exact observations in orthographic projection, points on a single body lie in a three or less dimensional linear manifold of frame space. If there are enough observations and independent motions, these manifolds can be viewed as a set linearly independent, four or less dimensional subspaces. We show that the row echelon canonical form provides direct information on the grouping of points to these subspaces. Treatment of the noise is the most difficult part of the problem. This paper uses a statistical approach to estimate the grouping of points to subspaces in the presence of noise by computing which partition has the maximum likelihood. The input data is assumed to be contaminated with independent Gaussian noise. The algorithm can base its estimates on a user-supplied standard deviation of the noise, or it can estimate the noise from the data. The algorithm can also be used to estimate the probability of a user-specified partition so that the hypothesis can be combined with others using Bayesian statistics.},
  doi      = {10.1023/a:1008026310903},
  issn     = {1573-1405},
  type     = {Journal Article},
  url      = {http://dx.doi.org/10.1023/A:1008026310903},
}

@Article{MC_MultiLlabel2015PAMI,
  author   = {Cabral, R. and Torre, F. D. l. and Costeira, J. P. and Bernardino, A.},
  title    = {Matrix Completion for Weakly-Supervised Multi-Label Image Classification},
  journal  = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year     = {2015},
  volume   = {37},
  number   = {1},
  pages    = {121-135},
  doi      = {10.1109/TPAMI.2014.2343234},
  issn     = {0162-8828},
  keywords = {image classification image segmentation learning (artificial intelligence) matrix algebra background noise bounding boxes discriminative methods labeling errors low-rank matrix completion problem manual labeling manual segmentations matrix completion multiple-instance learning methods object classifiers optimal spatial enclosure partial occlusions pixelwise segmentations semantic segmentation state-of-the-art classification algorithms training images visual concepts visual recognition weakly-supervised multilabel image classification Histograms Minimization Pattern analysis Semantics Training Vectors Weakly-supervised learning multi-label image classification nuclear norm rank minimization segmentation},
  type     = {Journal Article},
}

@Article{LuoMCmultilabel,
  author   = {Luo, Y. and Liu, T. and Tao, D. and Xu, C.},
  title    = {Multiview Matrix Completion for Multilabel Image Classification},
  journal  = {IEEE Transactions on Image Processing},
  year     = {2015},
  volume   = {24},
  number   = {8},
  pages    = {2355-2368},
  doi      = {10.1109/TIP.2015.2421309},
  issn     = {1057-7149},
  keywords = {computational complexity feature extraction image classification least squares approximations matrix algebra AP loss MC-based image classification MVMC framework average precision loss features concatenate learning process least squares loss formulation multilabel image classification multiview MC framework multiview matrix completion ranking- based criteria single-view feature time complexity web-based image analytics-based application Minimization Optimization Robustness Support vector machines average precision matrix completion multi-label multi-view transductive},
  type     = {Journal Article},
}

@Article{Lin2014,
  author   = {Lin, Zhouchen and Liu, Risheng and Li, Huan},
  title    = {Linearized alternating direction method with parallel splitting and adaptive penalty for separable convex programs in machine learning},
  journal  = {Machine Learning},
  year     = {2014},
  volume   = {99},
  number   = {2},
  pages    = {287-325},
  abstract = {Many problems in machine learning and other fields can be (re)formulated as linearly constrained separable convex programs. In most of the cases, there are multiple blocks of variables. However, the traditional alternating direction method (ADM) and its linearized version (LADM, obtained by linearizing the quadratic penalty term) are for the two-block case and cannot be naively generalized to solve the multi-block case. So there is great demand on extending the ADM based methods for the multi-block case. In this paper, we propose LADM with parallel splitting and adaptive penalty (LADMPSAP) to solve multi-block separable convex programs efficiently. When all the component objective functions have bounded subgradients, we obtain convergence results that are stronger than those of ADM and LADM, e.g., allowing the penalty parameter to be unbounded and proving the sufficient and necessary conditions for global convergence. We further propose a simple optimality measure and reveal the convergence rate of LADMPSAP in an ergodic sense. For programs with extra convex set constraints, with refined parameter estimation we devise a practical version of LADMPSAP for faster convergence. Finally, we generalize LADMPSAP to handle programs with more difficult objective functions by linearizing part of the objective function as well. LADMPSAP is particularly suitable for sparse representation and low-rank recovery problems because its subproblems have closed form solutions and the sparsity and low-rankness of the iterates can be preserved during the iteration. It is also highly parallelizable and hence fits for parallel or distributed computing. Numerical experiments testify to the advantages of LADMPSAP in speed and numerical accuracy.},
  doi      = {10.1007/s10994-014-5469-5},
  issn     = {1573-0565},
  type     = {Journal Article},
  url      = {http://dx.doi.org/10.1007/s10994-014-5469-5},
}

@Article{,
  author   = {Yang, Junfeng and Zhang, Yin},
  title    = {Alternating Direction Algorithms for $\ell_1$-Problems in Compressive Sensing},
  journal  = {SIAM Journal on Scientific Computing},
  year     = {2011},
  volume   = {33},
  number   = {1},
  pages    = {250-278},
  doi      = {doi:10.1137/090777761},
  keywords = {65F22,65J22,65K10,90C25,90C06},
  type     = {Journal Article},
  url      = {http://epubs.siam.org/doi/abs/10.1137/090777761},
}

@Article{Gorski2007,
  author   = {Gorski, Jochen and Pfeuffer, Frank and Klamroth, Kathrin},
  title    = {Biconvex sets and optimization with biconvex functions: a survey and extensions},
  journal  = {Mathematical Methods of Operations Research},
  year     = {2007},
  volume   = {66},
  number   = {3},
  pages    = {373-407},
  abstract = {The problem of optimizing a biconvex function over a given (bi)convex or compact set frequently occurs in theory as well as in industrial applications, for example, in the field of multifacility location or medical image registration. Thereby, a function $$f:X\times Y\to{\mathbb{R}}$$ is called biconvex, if f(x,y) is convex in y for fixed x∈X, and f(x,y) is convex in x for fixed y∈Y. This paper presents a survey of existing results concerning the theory of biconvex sets and biconvex functions and gives some extensions. In particular, we focus on biconvex minimization problems and survey methods and algorithms for the constrained as well as for the unconstrained case. Furthermore, we state new theoretical results for the maximum of a biconvex function over biconvex sets.},
  doi      = {10.1007/s00186-007-0161-1},
  issn     = {1432-5217},
  type     = {Journal Article},
  url      = {http://dx.doi.org/10.1007/s00186-007-0161-1},
}

@InProceedings{YangRobinsonVidal,
  author    = {Yang, Congyuan and Robinson, Daniel and Vidal, Rene},
  title     = {Sparse Subspace Clustering with Missing Entries},
  booktitle = {The 32 nd International Conference on Machine Learning},
  year      = {2015},
  volume    = {37},
  publisher = {JMLR W\&CP},
  type      = {Conference Proceedings},
}

@Article{Su:2009:SCF:1592474.1722966,
  author     = {Su, Xiaoyuan and Khoshgoftaar, Taghi M.},
  title      = {A Survey of Collaborative Filtering Techniques},
  journal    = {Adv. in Artif. Intell.},
  year       = {2009},
  volume     = {2009},
  pages      = {4:2--4:2},
  month      = jan,
  acmid      = {1722966},
  address    = {New York, NY, United States},
  articleno  = {4},
  doi        = {10.1155/2009/421425},
  issn       = {1687-7470},
  issue_date = {January 2009},
  numpages   = {1},
  publisher  = {Hindawi Publishing Corp.},
  url        = {http://dx.doi.org/10.1155/2009/421425},
}

@Article{MC_TNNR_PAMI2013,
  author    = {Yao Hu and Debing Zhang and Jieping Ye and Xuelong Li and Xiaofei He},
  title     = {Fast and Accurate Matrix Completion via Truncated Nuclear Norm Regularization},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year      = {2013},
  volume    = {35},
  number    = {9},
  pages     = {2117-2130},
  address   = {Los Alamitos, CA, USA},
  doi       = {http://doi.ieeecomputersociety.org/10.1109/TPAMI.2012.271},
  issn      = {0162-8828},
  publisher = {IEEE Computer Society},
}

@Article{MCGross2011,
  author   = {D. Gross},
  title    = {Recovering Low-Rank Matrices From Few Coefficients in Any Basis},
  journal  = {IEEE Transactions on Information Theory},
  year     = {2011},
  volume   = {57},
  number   = {3},
  pages    = {1548-1566},
  month    = {March},
  doi      = {10.1109/TIT.2011.2104999},
  issn     = {0018-9448},
  keywords = {computational complexity;matrix algebra;degree of incoherence;low-rank matrix recovery;matrix completion;multiplicative constants;randomly sampled expansion coefficients;Coherence;Compressed sensing;Convex functions;Eigenvalues and eigenfunctions;Linear matrix inequalities;Random variables;Compressed sensing;matrix completion;matrix recovery;operator large-deviation bound;quantum-state tomography},
}

@Article{MC_Candes2010TIT,
  author     = {Cand\`{e}s, Emmanuel J. and Tao, Terence},
  title      = {The Power of Convex Relaxation: Near-optimal Matrix Completion},
  journal    = {IEEE Trans. Inf. Theor.},
  year       = {2010},
  volume     = {56},
  number     = {5},
  pages      = {2053--2080},
  month      = may,
  issn       = {0018-9448},
  acmid      = {1823678},
  address    = {Piscataway, NJ, USA},
  doi        = {10.1109/TIT.2010.2044061},
  issue_date = {May 2010},
  keywords   = {Duality in optimization, duality in optimization, free probability, low-rank matrices, matrix completion, nuclear norm minimization, random matrices and techniques from random matrix theory, semidefinite programming},
  numpages   = {28},
  publisher  = {IEEE Press},
  url        = {http://dx.doi.org/10.1109/TIT.2010.2044061},
}

@InProceedings{MC_icml2014c1_chenc14,
  author    = {Yudong Chen and Srinadh Bhojanapalli and Sujay Sanghavi and Rachel Ward},
  title     = {Coherent Matrix Completion},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  year      = {2014},
  pages     = {674-682},
  publisher = {JMLR Workshop and Conference Proceedings},
}

@InCollection{MC_NIPS2015_6022,
  author    = {Gunasekar, Suriya and Banerjee, Arindam and Ghosh, Joydeep},
  title     = {Unified View of Matrix Completion under General Structural Constraints},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {1180--1188},
  url       = {http://papers.nips.cc/paper/6022-unified-view-of-matrix-completion-under-general-structural-constraints.pdf},
}

@Article{MC_corrupcolumn2011,
  author        = {{Chen}, Y. and {Xu}, H. and {Caramanis}, C. and {Sanghavi}, S.},
  title         = {{Matrix completion with column manipulation: Near-optimal sample-robustness-rank tradeoffs}},
  journal       = {ArXiv e-prints},
  year          = {2011},
  month         = feb,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2011arXiv1102.2254C},
  archiveprefix = {arXiv},
  eprint        = {1102.2254},
  keywords      = {Statistics - Machine Learning, Computer Science - Information Theory},
  primaryclass  = {stat.ML},
}

@Article{MC_robust_2014arXiv,
  author        = {{Klopp}, O. and {Lounici}, K. and {Tsybakov}, A.~B.},
  title         = {{Robust Matrix Completion}},
  journal       = {ArXiv e-prints},
  year          = {2014},
  month         = dec,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1412.8132K},
  archiveprefix = {arXiv},
  eprint        = {1412.8132},
  keywords      = {Mathematics - Statistics Theory},
  primaryclass  = {math.ST},
}

@PhdThesis{MC_Srebro2004,
  author    = {Srebro, Nathan},
  title     = {Learning with Matrix Factorizations},
  year      = {2004},
  address   = {Cambridge, MA, USA},
  note      = {AAI0807530},
  publisher = {Massachusetts Institute of Technology},
}

@Article{MC_MF_Wen2012,
  author   = {Wen, Zaiwen and Yin, Wotao and Zhang, Yin},
  title    = {Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm},
  journal  = {Mathematical Programming Computation},
  year     = {2012},
  volume   = {4},
  number   = {4},
  pages    = {333--361},
  abstract = {The matrix completion problem is to recover a low-rank matrix from a subset of its entries. The main solution strategy for this problem has been based on nuclear-norm minimization which requires computing singular value decompositions---a task that is increasingly costly as matrix sizes and ranks increase. To improve the capacity of solving large-scale problems, we propose a low-rank factorization model and construct a nonlinear successive over-relaxation (SOR) algorithm that only requires solving a linear least squares problem per iteration. Extensive numerical experiments show that the algorithm can reliably solve a wide range of problems at a speed at least several times faster than many nuclear-norm minimization algorithms. In addition, convergence of this nonlinear SOR algorithm to a stationary point is analyzed.},
  doi      = {10.1007/s12532-012-0044-1},
  issn     = {1867-2957},
  url      = {http://dx.doi.org/10.1007/s12532-012-0044-1},
}

@Article{MC_ADMM2014,
  author  = {Y. Shen and Z. Wen and Y. Zhang},
  title   = {Augmented Lagrangian alternating direction method for matrix separation based on low-rank factorization},
  journal = {Optimization Methods and Software},
  year    = {2014},
  volume  = {29},
  number  = {2},
  pages   = {239-263},
  doi     = {10.1080/10556788.2012.700713},
  eprint  = { http://dx.doi.org/10.1080/10556788.2012.700713 },
  url     = { 
        http://dx.doi.org/10.1080/10556788.2012.700713
    
},
}

@Article{MC_Riemannian2013,
  author  = {Bart Vandereycken},
  title   = {Low-Rank Matrix Completion by Riemannian Optimization},
  journal = {SIAM Journal on Optimization},
  year    = {2013},
  volume  = {23},
  number  = {2},
  pages   = {1214-1236},
  doi     = {10.1137/110845768},
  eprint  = { http://dx.doi.org/10.1137/110845768 },
  url     = { 
        http://dx.doi.org/10.1137/110845768
    
},
}

@Article{MC_OptSpace_2009,
  author        = {{Keshavan}, R.~H. and {Oh}, S.},
  title         = {{A Gradient Descent Algorithm on the Grassman Manifold for Matrix Completion}},
  journal       = {ArXiv e-prints},
  year          = {2009},
  month         = oct,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2009arXiv0910.5260K},
  archiveprefix = {arXiv},
  eprint        = {0910.5260},
  keywords      = {Computer Science - Numerical Analysis, Computer Science - Learning},
  primaryclass  = {cs.NA},
}

@InProceedings{MC_GROUSE_2010,
  author    = {L. Balzano and R. Nowak and B. Recht},
  title     = {Online identification and tracking of subspaces from highly incomplete information},
  booktitle = {Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on},
  year      = {2010},
  pages     = {704-711},
  month     = {Sept},
  doi       = {10.1109/ALLERTON.2010.5706976},
  keywords  = {Internet;gradient methods;iterative methods;matrix algebra;symbol manipulation;GROUSE;Grassmanian rank-one update subspace estimation;Grassmannian manifold;gradient descent algorithm;linear algebraic manipulations;low-rank matrix completion problem;online identification;online incremental algorithm;subspace update;subspaces tracking;Algorithm design and analysis;Approximation methods;Cost function;Estimation;Manifolds;Noise;Steady-state},
}

@Article{MC_ADM_Chen09062011,
  author   = {Chen, Caihua and He, Bingsheng and Yuan, Xiaoming},
  title    = {Matrix completion via an alternating direction method},
  journal  = {IMA Journal of Numerical Analysis},
  year     = {2011},
  abstract = {The matrix completion problem is to complete an unknown matrix from a small number of entries, and it captures many applications in diversified areas. Recently, it was shown that completing a low-rank matrix can be successfully accomplished by solving its convex relaxation problem using the nuclear norm. This paper shows that the alternating direction method (ADM) is applicable for completing a low-rank matrix including the noiseless case, the noisy case and the positive semidefinite case. The ADM approach for the matrix completion problem is easily implementable and very efficient. Numerical comparisons of the ADM approach with some state-of-the-art methods are reported.},
  doi      = {10.1093/imanum/drq039},
  eprint   = {http://imajna.oxfordjournals.org/content/early/2011/06/08/imanum.drq039.full.pdf+html},
  url      = {http://imajna.oxfordjournals.org/content/early/2011/06/08/imanum.drq039.abstract},
}

@InProceedings{MC_ShattenP_AAAI125165,
  author    = {Nie, Feiping and Huang, Heng and Ding, Chris},
  title     = {Low-rank Matrix Recovery via Efficient {Schatten} P-norm Minimization},
  booktitle = {Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence},
  year      = {2012},
  series    = {AAAI'12},
  pages     = {655--661},
  publisher = {AAAI Press},
  acmid     = {2900822},
  location  = {Toronto, Ontario, Canada},
  numpages  = {7},
  url       = {http://dl.acm.org/citation.cfm?id=2900728.2900822},
}

@Article{MC_GSVT_2014,
  author        = {{Lu}, C. and {Zhu}, C. and {Xu}, C. and {Yan}, S. and {Lin}, Z.},
  title         = {{Generalized Singular Value Thresholding}},
  journal       = {ArXiv e-prints},
  year          = {2014},
  month         = dec,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2014arXiv1412.2231L},
  archiveprefix = {arXiv},
  eprint        = {1412.2231},
  keywords      = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Computer Science - Numerical Analysis, Mathematics - Numerical Analysis},
  primaryclass  = {cs.CV},
}

@InBook{SC_LSR_Lu2012,
  chapter   = {Robust and Efficient Subspace Segmentation via Least Squares Regression},
  pages     = {347--360},
  title     = {Computer Vision -- ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part VII},
  publisher = {Springer Berlin Heidelberg},
  year      = {2012},
  author    = {Lu, Can-Yi and Min, Hai and Zhao, Zhong-Qiu and Zhu, Lin and Huang, De-Shuang and Yan, Shuicheng},
  editor    = {Fitzgibbon, Andrew and Lazebnik, Svetlana and Perona, Pietro and Sato, Yoichi and Schmid, Cordelia},
  address   = {Berlin, Heidelberg},
  doi       = {10.1007/978-3-642-33786-4_26},
  isbn      = {978-3-642-33786-4},
  url       = {http://dx.doi.org/10.1007/978-3-642-33786-4_26},
}

@InCollection{LADMMAP_NIPS2011,
  author    = {Lin, Zhouchen and Risheng Liu and Zhixun Su},
  title     = {Linearized Alternating Direction Method with Adaptive Penalty for Low-Rank Representation},
  booktitle = {Advances in Neural Information Processing Systems 24},
  publisher = {Curran Associates, Inc.},
  year      = {2011},
  editor    = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
  pages     = {612--620},
  url       = {http://papers.nips.cc/paper/4434-linearized-alternating-direction-method-with-adaptive-penalty-for-low-rank-representation.pdf},
}

@Article{LALM_LADMM,
  author     = {Yang, Junfeng and Yuan, Xiaoming},
  title      = {Linearized augmented {L}agrangian and alternating direction methods for nuclear norm minimization},
  journal    = {Math. Comp.},
  year       = {2013},
  volume     = {82},
  number     = {281},
  pages      = {301--329},
  coden      = {MCMPAF},
  doi        = {10.1090/S0025-5718-2012-02598-1},
  fjournal   = {Mathematics of Computation},
  issn       = {0025-5718},
  mrclass    = {90C25 (90C06)},
  mrnumber   = {2983026},
  mrreviewer = {Lei-Hong Zhang},
  url        = {http://dx.doi.org/10.1090/S0025-5718-2012-02598-1},
}

@Article{ProximalA,
  author     = {Parikh, Neal and Boyd, Stephen},
  title      = {Proximal Algorithms},
  journal    = {Found. Trends Optim.},
  year       = {2014},
  volume     = {1},
  number     = {3},
  pages      = {127--239},
  month      = jan,
  acmid      = {2693613},
  address    = {Hanover, MA, USA},
  doi        = {10.1561/2400000003},
  issn       = {2167-3888},
  issue_date = {January 2014},
  numpages   = {113},
  publisher  = {Now Publishers Inc.},
  url        = {http://dx.doi.org/10.1561/2400000003},
}

@Article{Dataset_ExtendYaleB,
  author  = {Kuang-Chih, Lee and Ho, J. and Kriegman, D. J.},
  title   = {Acquiring linear subspaces for face recognition under variable lighting},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year    = {2005},
  volume  = {27},
  number  = {5},
  pages   = {684-698},
  doi     = {10.1109/TPAMI.2005.92},
  issn    = {0162-8828},
  type    = {Journal Article},
}

@TechReport{Dataset_COIL20,
  author      = {Nene, S. A. and Nayar, S. K. and Murase, H.},
  title       = {Columbia object image library (COIL-20)},
  institution = {Columbia University},
  year        = {1996},
  type        = {Report},
}

@InProceedings{Dataset_MITCBCL,
  author    = {B. Weyrauch and B. Heisele and J. Huang and V. Blanz},
  title     = {Component-Based Face Recognition with 3D Morphable Models},
  booktitle = {Computer Vision and Pattern Recognition Workshop, 2004. CVPRW '04. Conference on},
  year      = {2004},
  pages     = {85-85},
  month     = {June},
  doi       = {10.1109/CVPR.2004.41},
  keywords  = {Biology computing;Computer vision;Detectors;Face detection;Face recognition;Image databases;Image recognition;Lighting;Object detection;Support vector machines},
}

@Article{Web_NCDC,
  title   = {National Climatic Data Center},
  journal = {http://www.ncdc.noaa.gov},
}

@Article{Web_JesterJoke,
  title   = {Jester jokes datasets},
  journal = {http://eigentaste.berkeley.edu/dataset/},
}

@Article{Web_MovieLens1M,
  title   = {{MovieLens} Dataset},
  journal = {https://grouplens.org/datasets/movielens/},
}

@Article{LinChenMa2013,
  author  = {Lin, Zhouchen and Chen, Minming and Ma, Yi},
  title   = {The Augmented Lagrange Multiplier Method for Exact Recovery of Corrupted Low-Rank Matrices},
  journal = {arXiv:1009.5055v3 [math.OC]},
  year    = {2010},
}

@Article{ErikssonBalzanoNowak2011,
  author    = {Eriksson, Brian and Balzano, Laura and Nowak, Robert D.},
  title     = {High-Rank Matrix Completion and Subspace Clustering with Missing Data},
  journal   = {CoRR},
  year      = {2011},
  volume    = {abs/1112.5629},
  added-at  = {2015-05-07T00:00:00.000+0200},
  biburl    = {http://www.bibsonomy.org/bibtex/261172e2bad9b4ec34ae977dba81446c3/dblp},
  ee        = {http://arxiv.org/abs/1112.5629},
  interhash = {0372a9ef4aca67b5ffe504afb4858e14},
  intrahash = {61172e2bad9b4ec34ae977dba81446c3},
  keywords  = {dblp},
  timestamp = {2015-06-18T03:54:10.000+0200},
  url       = {http://dblp.uni-trier.de/db/journals/corr/corr1112.html#abs-1112-5629},
}

@Article{MC_Recht2011:SAM,
  author     = {Recht, Benjamin},
  title      = {A Simpler Approach to Matrix Completion},
  journal    = {J. Mach. Learn. Res.},
  year       = {2011},
  volume     = {12},
  pages      = {3413--3430},
  month      = dec,
  acmid      = {2185803},
  issn       = {1532-4435},
  issue_date = {2/1/2011},
  numpages   = {18},
  publisher  = {JMLR.org},
  url        = {http://dl.acm.org/citation.cfm?id=1953048.2185803},
}

@Proceedings{,
}

@InProceedings{MC_IRRN,
  author    = {C. Lu and J. Tang and S. Yan and Z. Lin},
  title     = {Generalized Nonconvex Nonsmooth Low-Rank Minimization},
  booktitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2014},
  pages     = {4130-4137},
  month     = {June},
  doi       = {10.1109/CVPR.2014.526},
  issn      = {1063-6919},
  keywords  = {concave programming;minimisation;concave function;generalized nonconvex minimization;iteratively reweighted nuclear norm algorithm;low rank matrix recovery;nonconvex penalty function;nonsmooth low rank minimization;surrogate functions;weight vector;weighted singular value thresholding problem;Algorithm design and analysis;Convergence;Convex functions;Educational institutions;Minimization;Programming;Vectors},
}

@Proceedings{,
}

@Article{KPCA1998,
  author  = {Schölkopf, Bernhard and Smola, Alexander and Müller, Klaus-Robert},
  title   = {Nonlinear Component Analysis as a Kernel Eigenvalue Problem},
  journal = {Neural Computation},
  year    = {1998},
  volume  = {10},
  number  = {5},
  pages   = {1299-1319},
  doi     = {10.1162/089976698300017467},
  issn    = {0899-7667},
  type    = {Journal Article},
  url     = {http://dx.doi.org/10.1162/089976698300017467},
}

@Article{L-BFGS,
  author   = {Liu, Dong C. and Nocedal, Jorge},
  title    = {On the limited memory BFGS method for large scale optimization},
  journal  = {Mathematical Programming},
  year     = {1989},
  volume   = {45},
  number   = {1},
  pages    = {503--528},
  abstract = {We study the numerical performance of a limited memory quasi-Newton method for large scale optimization, which we call the L-BFGS method. We compare its performance with that of the method developed by Buckley and LeNir (1985), which combines cycles of BFGS steps and conjugate direction steps. Our numerical tests indicate that the L-BFGS method is faster than the method of Buckley and LeNir, and is better able to use additional storage to accelerate convergence. We show that the L-BFGS method can be greatly accelerated by means of a simple scaling. We then compare the L-BFGS method with the partitioned quasi-Newton method of Griewank and Toint (1982a). The results show that, for some problems, the partitioned quasi-Newton method is clearly superior to the L-BFGS method. However we find that for other problems the L-BFGS method is very competitive due to its low iteration cost. We also study the convergence properties of the L-BFGS method, and prove global convergence on uniformly convex problems.},
  doi      = {10.1007/BF01589116},
  issn     = {1436-4646},
  url      = {http://dx.doi.org/10.1007/BF01589116},
}

@InCollection{NIPS2010_3932,
  author    = {Andrew Goldberg and Ben Recht and Junming Xu and Robert Nowak and Zhu, Xiaojin},
  title     = {Transduction with Matrix Completion: Three Birds with One Stone},
  booktitle = {Advances in Neural Information Processing Systems 23},
  publisher = {Curran Associates, Inc.},
  year      = {2010},
  pages     = {757--765},
}

@Misc{minFunc,
  author       = {M. Schmidt},
  title        = {minFunc: unconstrained differentiable multivariate optimization in Matlab},
  howpublished = {http://www.cs.ubc.ca/~schmidtm/Software/minFunc. html},
  year         = {2005},
}

@Article{Kallas20133066,
  author   = {Maya Kallas and Paul Honeine and Cédric Richard and Clovis Francis and Hassan Amoud},
  title    = {Non-negativity constraints on the pre-image for pattern recognition with kernel machines},
  journal  = {Pattern Recognition},
  year     = {2013},
  volume   = {46},
  number   = {11},
  pages    = {3066 - 3080},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2013.03.021},
  issn     = {0031-3203},
  keywords = {Kernel machines, Machine learning, SVM, Kernel PCA, Pre-image problem, Non-negativity constraints, Nonlinear denoising, Pattern recognition },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320313001507},
}

@Article{PreImageKPCA2010TNN,
  author   = {W. S. Zheng and J. Lai and P. C. Yuen},
  title    = {Penalized Preimage Learning in Kernel Principal Component Analysis},
  journal  = {IEEE Transactions on Neural Networks},
  year     = {2010},
  volume   = {21},
  number   = {4},
  pages    = {551-570},
  month    = {April},
  doi      = {10.1109/TNN.2009.2039647},
  issn     = {1045-9227},
  keywords = {face recognition;image reconstruction;learning (artificial intelligence);principal component analysis;Laplacian penalty;convexity constraint;face image data sets;face image denoising;facial expression normalization;feature vector;illumination normalization;image preprocessing;image reconstruction;kernel principal component analysis;mean square error;occlusion;optimization function;penalized preimage learning;pointwise conditional mutual information;ridge penalty;visual quality;Constraint optimization;Image denoising;Image reconstruction;Kernel;Laplace equations;Lighting;Mean square error methods;Mutual information;Principal component analysis;Turning;Kernel;kernel principal component analysis (KPCA);locality preservation;penalty function;preimage problem;Algorithms;Artificial Intelligence;Computer Simulation;Humans;Image Interpretation, Computer-Assisted;Pattern Recognition, Automated;Principal Component Analysis},
}

@Article{PreImageKPCA2004TNN,
  author   = {J. T. Y. Kwok and I. W. H. Tsang},
  title    = {The pre-image problem in kernel methods},
  journal  = {IEEE Transactions on Neural Networks},
  year     = {2004},
  volume   = {15},
  number   = {6},
  pages    = {1517-1525},
  month    = {Nov},
  doi      = {10.1109/TNN.2004.837781},
  issn     = {1045-9227},
  keywords = {image denoising;linear algebra;principal component analysis;distance constraint;feature space;feature vector;kernel clustering;kernel method;kernel principal component analysis;linear algebra;preimage problem;Clustering algorithms;Constraint optimization;Image denoising;Kernel;Linear algebra;Noise reduction;Optimization methods;Performance evaluation;Principal component analysis;Space technology;Kernel principal component analysis (PCA);multidimensional scaling (MDS);pre-image;Algorithms;Artificial Intelligence;Cluster Analysis;Computer Simulation;Decision Support Techniques;Feedback;Image Interpretation, Computer-Assisted;Information Storage and Retrieval;Neural Networks (Computer);Pattern Recognition, Automated;Principal Component Analysis},
}

@Article{Data_MNIST1998,
  author   = {Y. Lecun and L. Bottou and Y. Bengio and P. Haffner},
  title    = {Gradient-based learning applied to document recognition},
  journal  = {Proceedings of the IEEE},
  year     = {1998},
  volume   = {86},
  number   = {11},
  pages    = {2278-2324},
  month    = {Nov},
  doi      = {10.1109/5.726791},
  issn     = {0018-9219},
  keywords = {backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis},
}

@Misc{SVM-KMToolbox,
  author       = {S. Canu and Y. Grandvalet and V. Guigue and A. Rakotomamonjy},
  title        = {SVM and Kernel Methods Matlab Toolbox},
  howpublished = {Perception Syst�mes et Information, INSA de Rouen, Rouen, France},
  year         = {2005},
}

@Article{ML_KNN,
  author   = {Min-Ling Zhang and Zhi-Hua Zhou},
  title    = {ML-KNN: A lazy learning approach to multi-label learning},
  journal  = {Pattern Recognition},
  year     = {2007},
  volume   = {40},
  number   = {7},
  pages    = {2038 - 2048},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2006.12.019},
  issn     = {0031-3203},
  keywords = {Machine learning, Multi-label learning, Lazy learning, K-nearest neighbor, Functional genomics, Natural scene classification, Text categorization },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320307000027},
}

@Article{ML_RBF,
  author   = {Zhang, Min-Ling},
  title    = {Ml-rbf: RBF Neural Networks for Multi-Label Learning},
  journal  = {Neural Processing Letters},
  year     = {2009},
  volume   = {29},
  number   = {2},
  pages    = {61--74},
  abstract = {Multi-label learning deals with the problem where each instance is associated with multiple labels simultaneously. The task of this learning paradigm is to predict the label set for each unseen instance, through analyzing training instances with known label sets. In this paper, a neural network based multi-label learning algorithm named Ml-rbf is proposed, which is derived from the traditional radial basis function (RBF) methods. Briefly, the first layer of an Ml-rbf neural network is formed by conducting clustering analysis on instances of each possible class, where the centroid of each clustered groups is regarded as the prototype vector of a basis function. After that, second layer weights of the Ml-rbf neural network are learned by minimizing a sum-of-squares error function. Specifically, information encoded in the prototype vectors corresponding to all classes are fully exploited to optimize the weights corresponding to each specific class. Experiments on three real-world multi-label data sets show that Ml-rbf achieves highly competitive performance to other well-established multi-label learning algorithms.},
  doi      = {10.1007/s11063-009-9095-3},
  issn     = {1573-773X},
  url      = {http://dx.doi.org/10.1007/s11063-009-9095-3},
}

@InProceedings{Mika99kernelpca,
  author    = {Sebastian Mika and Bernhard Schölkopf and Alex Smola and Klaus-Robert Müller and Matthias Scholz and Gunnar Rätsch},
  title     = {Kernel PCA and De-Noising in Feature Spaces},
  booktitle = {Advances in Neural Information Processing Systems 11},
  year      = {1999},
  pages     = {536--542},
  publisher = {MIT Press},
}

@InProceedings{NLMC2016,
  author    = {Alameda-Pineda, Xavier and Ricci, Elisa and Yan, Yan and Sebe, Nicu},
  title     = {Recognizing Emotions From Abstract Paintings Using Non-Linear Matrix Completion},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2016},
  pages     = {5240--5248},
}

@Article{KPCA2014,
  author   = {A. Papaioannou and S. Zafeiriou},
  title    = {Principal Component Analysis With Complex Kernel: The Widely Linear Model},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2014},
  volume   = {25},
  number   = {9},
  pages    = {1719-1726},
  month    = {Sept},
  doi      = {10.1109/TNNLS.2013.2285783},
  issn     = {2162-237X},
  keywords = {Hilbert spaces;digital filters;principal component analysis;regression analysis;CRKHS;Euler data representation;classification frameworks;complex kernel;complex reproducing kernel Hilbert spaces;digital filters;dimensionality reduction;nonlinear complex representations;principal component analysis;regression frameworks;robust reconstruction;widely linear model;Covariance matrices;Eigenvalues and eigenfunctions;Image reconstruction;Kernel;Principal component analysis;Robustness;Vectors;Complex kernels;machine vision;pattern recognition;principal component analysis (PCA);principal component analysis (PCA).},
}

@Article{Multi-label2013,
  author   = {Y. Luo and D. Tao and C. Xu and C. Xu and H. Liu and Y. Wen},
  title    = {Multiview Vector-Valued Manifold Regularization for Multilabel Image Classification},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2013},
  volume   = {24},
  number   = {5},
  pages    = {709-722},
  month    = {May},
  doi      = {10.1109/TNNLS.2013.2238682},
  issn     = {2162-237X},
  keywords = {computer vision;feature extraction;image classification;matrix algebra;vectors;MIR Flickr datasets;MV3MR;PASCAL datasets;VOC' 07 datasets;complementary property;computer vision;image datasets;intrinsic local geometry discovery;label relationship;matrix-valued kernel construction;multilabel image classification;multiview vector-valued manifold regularization;vector-valued function;visual features;Correlation;Image color analysis;Kernel;Laplace equations;Manifolds;Optimization;Support vector machines;Image classification;manifold;multilabel;multiview;semisupervised},
}

@Article{Multi-label2015,
  author   = {Q. Wu and Y. Ye and H. Zhang and T. W. S. Chow and S. S. Ho},
  title    = {ML-TREE: A Tree-Structure-Based Approach to Multilabel Learning},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2015},
  volume   = {26},
  number   = {3},
  pages    = {430-443},
  month    = {March},
  doi      = {10.1109/TNNLS.2014.2315296},
  issn     = {2162-237X},
  keywords = {learning (artificial intelligence);pattern classification;support vector machines;trees (mathematics);Friedman tests;ML-Tree algorithm;Nemenyi tests;automatic label relationships discovery;child nodes;hierarchical tree model;multilabel learning;multilabel prediction;one-against-all SVM classifiers;predictive label cooccurrence;predictive label transmission;predictive label vector;training samples;tree-structure-based approach;Clustering algorithms;Correlation;Partitioning algorithms;Prediction algorithms;Support vector machines;Training;Vectors;Hierarchical tree model;multilabel classification;multilabel learning;tree-based classification;tree-based classification.},
}

@Book{PCA_Jolliffe2002,
  title     = {Principal Component Analysis},
  publisher = {Springer-Verlag New York},
  year      = {2002},
  author    = {Jolliffe, I.T.},
  series    = {Springer Series in Statistics},
  isbn      = {9780387954424},
  doi       = {10.1007/b98835},
  lccn      = {2002019560},
  url       = {https://books.google.com.hk/books?id=\_olByCrhjwIC},
}

@InProceedings{Si:2016:GIM:2939672.2939809,
  author    = {Si, Si and Chiang, Kai-Yang and Hsieh, Cho-Jui and Rao, Nikhil and Dhillon, Inderjit S.},
  title     = {Goal-Directed Inductive Matrix Completion},
  booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2016},
  series    = {KDD '16},
  pages     = {1165--1174},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2939809},
  doi       = {10.1145/2939672.2939809},
  isbn      = {978-1-4503-4232-2},
  keywords  = {matrix completion},
  location  = {San Francisco, California, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2939672.2939809},
}

@InProceedings{liu2016kernelized,
  author    = {Liu, Xinyue and Aggarwal, Charu and Li, Yu-Feng and Kong, Xiangnan and Sun, Xinyuan and Sathe, Saket},
  title     = {Kernelized matrix factorization for collaborative filtering},
  booktitle = {SIAM Conference on Data Mining},
  year      = {2016},
  pages     = {399--416},
}

@InProceedings{icml2014c2_wanga14,
  author    = {Zheng Wang and Ming-jun Lai and Zhaosong Lu and Wei Fan and Hasan Davulcu and Jieping Ye},
  title     = {Rank-One Matrix Pursuit for Matrix Completion},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  year      = {2014},
  pages     = {91-99},
}

@InBook{KMC2012,
  pages     = {403-414},
  title     = {Kernelized Probabilistic Matrix Factorization: Exploiting Graphs and Side Information},
  year      = {2012},
  author    = {Tinghui Zhou and Hanhuai Shan and Arindam Banerjee and Guillermo Sapiro},
  booktitle = {Proceedings of the 2012 SIAM International Conference on Data Mining},
}

@Article{IMC2013,
  author  = {Jain, Prateek and Dhillon, Inderjit S},
  title   = {Provable inductive matrix completion},
  journal = {arXiv preprint arXiv:1306.0626},
  year    = {2013},
}

@Article{TIP_MC_201501,
  author  = {K. H. Jin and J. C. Ye},
  title   = {Annihilating Filter-Based Low-Rank Hankel Matrix Approach for Image Inpainting},
  journal = {IEEE Transactions on Image Processing},
  year    = {2015},
  volume  = {24},
  number  = {11},
  pages   = {3498-3511},
  month   = {Nov},
  doi     = {10.1109/TIP.2015.2446943},
  issn    = {1057-7149},
}

@Article{TIP_MC_2016,
  author  = {Q. Liu and Z. Lai and Z. Zhou and F. Kuang and Z. Jin},
  title   = {A Truncated Nuclear Norm Regularization Method Based on Weighted Residual Error for Matrix Completion},
  journal = {IEEE Transactions on Image Processing},
  year    = {2016},
  volume  = {25},
  number  = {1},
  pages   = {316-330},
  month   = {Jan},
}

@Article{TIP_MC_TNNM,
  author  = {C. Lee and E. Y. Lam},
  title   = {Computationally Efficient Truncated Nuclear Norm Minimization for High Dynamic Range Imaging},
  journal = {IEEE Transactions on Image Processing},
  year    = {2016},
  volume  = {25},
  number  = {9},
  pages   = {4145-4157},
  month   = {Sept},
}

@Article{Liu2014218,
  author   = {Lu Liu and Wei Huang and Di-Rong Chen},
  title    = {Exact minimum rank approximation via {Schatten} p-norm minimization},
  journal  = {Journal of Computational and Applied Mathematics},
  year     = {2014},
  volume   = {267},
  pages    = {218 - 227},
  doi      = {http://dx.doi.org/10.1016/j.cam.2014.02.015},
  issn     = {0377-0427},
  keywords = {Rank minimization, Schatten p -norm, Singular value decomposition, Restricted isometry constant, Random matrix, Majorization minimization },
  url      = {http://www.sciencedirect.com/science/article/pii/S0377042714001010},
}

@Article{2012arXiv1209.0377Y,
  author        = {{Yue}, M.-C. and {Man-Cho So}, A.},
  title         = {{A Perturbation Inequality for the Schatten-$p$ Quasi-Norm and Its Applications to Low-Rank Matrix Recovery}},
  journal       = {ArXiv e-prints},
  year          = {2012},
  month         = sep,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2012arXiv1209.0377Y},
  archiveprefix = {arXiv},
  eprint        = {1209.0377},
  keywords      = {Mathematics - Optimization and Control, Computer Science - Information Theory},
  primaryclass  = {math.OC},
}

@Article{MC_TSP_2015_Mard,
  author  = {M. Mardani and G. Mateos and G. B. Giannakis},
  title   = {Subspace Learning and Imputation for Streaming Big Data Matrices and Tensors},
  journal = {IEEE Transactions on Signal Processing},
  year    = {2015},
  volume  = {63},
  number  = {10},
  pages   = {2663-2677},
  month   = {May},
  doi     = {10.1109/TSP.2015.2417491},
  issn    = {1053-587X},
}

@Article{MC_TSP_2015_Chou,
  author   = {S. Chouvardas and Y. Kopsinis and S. Theodoridis},
  title    = {Robust Subspace Tracking With Missing Entries: The Set-Theoretic Approach},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2015},
  volume   = {63},
  number   = {19},
  pages    = {5060-5070},
  month    = {Oct},
  doi      = {10.1109/TSP.2015.2449254},
  issn     = {1053-587X},
  keywords = {gradient methods;matrix algebra;object tracking;set theory;APSM based algorithm;adaptive projected subgradient method;corrupted data purification;observation vectors;outlier detection mechanism;outlier vector estimation;prediction procedure;robust matrix completion;robust subspace estimation;robust subspace tracking;set theoretic approach;sparsity-promoting greedy algorithm;time instance;zero level set;Algorithm design and analysis;Cost function;Estimation;Noise;Prediction algorithms;Robustness;Signal processing algorithms;APSM;Robust subspace tracking;greedy algorithms},
}

@Article{MC_TSP_2016_Cao,
  author   = {Y. Cao and Y. Xie},
  title    = {Poisson Matrix Recovery and Completion},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {6},
  pages    = {1609-1620},
  month    = {March},
  doi      = {10.1109/TSP.2015.2500192},
  issn     = {1053-587X},
  keywords = {Gaussian noise;Poisson distribution;matrix algebra;maximum likelihood estimation;signal processing;stochastic processes;vectors;Poisson distribution;Poisson likelihood function;Poisson noise;compressed measurement;iterative algorithm;low-rank Poisson matrix recovery theory;matrix completion;maximum likelihood;signal-to-noise requirement;sparse vector recovery;subGaussian characteristics;word length 1 bit;Approximation algorithms;Compressed sensing;Noise measurement;Random variables;Signal to noise ratio;Sparse matrices;Upper bound;Estimation;Poisson noise;information theoretic bounds;low-rank matrix recovery;matrix completion},
}

@Article{LiuLi2016,
  author   = {G. Liu and P. Li},
  title    = {Low-Rank Matrix Completion in the Presence of High Coherence},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {PP},
  number   = {99},
  pages    = {1-1},
  doi      = {10.1109/TSP.2016.2586753},
  issn     = {1053-587X},
  keywords = {Coherence;Dictionaries;Distributed databases;Electronic mail;Indexes;Manifolds;Matrix decomposition},
}

@Article{MC_TSP_2016_Zhao,
  author   = {L. Zhao and P. Babu and D. P. Palomar},
  title    = {Efficient Algorithms on Robust Low-Rank Matrix Completion Against Outliers},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {18},
  pages    = {4767-4780},
  month    = {Sept},
  doi      = {10.1109/TSP.2016.2572049},
  issn     = {1053-587X},
  keywords = {Big Data;Gaussian noise;data analysis;matrix algebra;parallel processing;Big Data system analytics;additive Gaussian noise;bilinear factorization formulation;dense outlier;parallel computing;robust low-rank matrix completion;sparse spike-like outlier;Approximation algorithms;Convergence;Gaussian noise;Manganese;Principal component analysis;Robustness;Signal processing algorithms;Matrix completion;factorization formulation;parallel algorithm;robust loss functions},
}

@Article{MC_TSP_2016_Yang,
  author   = {L. Yang and J. Fang and H. Li and B. Zeng},
  title    = {An Iterative Reweighted Method for Tucker Decomposition of Incomplete Tensors},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {18},
  pages    = {4817-4829},
  month    = {Sept},
  doi      = {10.1109/TSP.2016.2572047},
  issn     = {1053-587X},
  keywords = {data analysis;iterative methods;tensors;Tucker decomposition;computational complexity;data analysis;group-based log-sum penalty functional;iterative optimization;iterative reweighted method;low-rank incomplete tensor decomposition;model complexity;multilinear low-rank structure;multilinear operations;objective function;over-relaxed monotone fast iterative shrinkage-thresholding technique;surrogate function;Complexity theory;Electronic mail;Iterative methods;Matrix decomposition;Optimization;Signal processing algorithms;Tensile stress;Tucker decomposition;iterative reweighted method;low rank tensor decomposition;tensor completion},
}

@Article{MC_TSP_2016_Xin,
  author  = {B. Xin and Y. Wang and W. Gao and D. Wipf},
  title   = {Exploring Algorithmic Limits of Matrix Rank Minimization Under Affine Constraints},
  journal = {IEEE Transactions on Signal Processing},
  year    = {2016},
  volume  = {64},
  number  = {19},
  pages   = {4960-4974},
  month   = {Oct},
  doi     = {10.1109/TSP.2016.2551697},
  issn    = {1053-587X},
}

@Article{MC_TSP_2016_Huang,
  author   = {K. Huang and N. D. Sidiropoulos and A. P. Liavas},
  title    = {A Flexible and Efficient Algorithmic Framework for Constrained Matrix and Tensor Factorization},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {19},
  pages    = {5052-5065},
  month    = {Oct},
  doi      = {10.1109/TSP.2016.2576427},
  issn     = {1053-587X},
  keywords = {matrix decomposition;signal processing;tensors;AO-ADMM;alternating direction method;alternating optimization;block coordinate descent-type methods;computation caching;dictionary learning;general algorithmic framework;machine learning;matrix factorization;matrix/tensor completion;multipliers;non-negative matrix;signal processing;tensor factorization;Complexity theory;Loss measurement;Matrix decomposition;Optimization;Signal processing;Signal processing algorithms;Tensile stress;Constrained matrix/tensor factorization;PARAFAC;alternating direction method of multipliers;alternating optimization;canonical polyadic decomposition;dictionary learning;matrix/tensor completion;non-negative matrix/tensor factorization},
}

@Article{MC_TSP_2016_Velas,
  author   = {J. Velasco and D. Pizarro and J. Macias-Guarasa and A. Asaei},
  title    = {TDOA Matrices: Algebraic Properties and Their Application to Robust Denoising With Missing Data},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {20},
  pages    = {5242-5254},
  month    = {Oct},
  doi      = {10.1109/TSP.2016.2593690},
  issn     = {1053-587X},
  keywords = {Estimation;Matrix decomposition;Noise measurement;Noise reduction;Pollution measurement;Robustness;Sensors;TDOA denoising;TDOA estimation;matrix completion;missing data;skew-symmetric matrices},
}

@Article{MC_2016_Sun,
  author   = {M. Sundin and C. R. Rojas and M. Jansson and S. Chatterjee},
  title    = {Relevance Singular Vector Machine for Low-Rank Matrix Reconstruction},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {20},
  pages    = {5327-5339},
  month    = {Oct},
  doi      = {10.1109/TSP.2016.2597121},
  issn     = {1053-587X},
  keywords = {Bayes methods;Convex functions;Covariance matrices;Estimation;Signal processing algorithms;Sparse matrices;Standards;Bayes methods;Machine learning;compressed sensing},
}

@Article{MC_TSP_2016_Yokota,
  author   = {T. Yokota and Q. Zhao and A. Cichocki},
  title    = {Smooth PARAFAC Decomposition for Tensor Completion},
  journal  = {IEEE Transactions on Signal Processing},
  year     = {2016},
  volume   = {64},
  number   = {20},
  pages    = {5423-5436},
  month    = {Oct},
  doi      = {10.1109/TSP.2016.2586759},
  issn     = {1053-587X},
  keywords = {Image color analysis;Interpolation;Matrix decomposition;Minimization;TV;Tensile stress;Visualization;CP model;PARAFAC model;Tensor completion for images;low-rank tensor approximation;quadratic variation;smoothness;total variation (TV)},
}

@Article{li2016structured,
  author    = {Li, Chun-Guang and Vidal, Rene},
  title     = {A Structured Sparse Plus Structured Low-Rank Framework for Subspace Clustering and Completion},
  journal   = {IEEE Transactions on Signal Processing},
  year      = {2016},
  volume    = {64},
  number    = {24},
  pages     = {6557--6570},
  publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC 445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
}

@Article{liu2016low,
  author    = {Liu, Guangcan and Li, Ping},
  title     = {Low-rank matrix completion in the presence of high coherence},
  journal   = {IEEE Transactions on Signal Processing},
  year      = {2016},
  volume    = {64},
  number    = {21},
  pages     = {5623--5633},
  publisher = {IEEE},
}

@Article{,
  author   = {Zhao, Miaoyun and Jiao, Licheng and Ma, Wenping and Liu, Hongying and Yang, Shuyuan},
  title    = {Classification and saliency detection by semi-supervised low-rank representation},
  journal  = {Pattern Recognition},
  year     = {2016},
  volume   = {51},
  pages    = {281-294},
  abstract = {In the area of pattern recognition, Low Rank Representation (LRR) is an efficient method in recovering the subspace structure of the dataset. However, LRR is unsupervised. Without any label information, LRR constructs an informative graph which is then combined with the mature graph-based semi-supervised learning (GSSL) framework to complete the classification task. In this paper, we propose a new low rank learning method which constructs the low rank representation matrix utilizing label information to obtain a more informative graph. This method integrates the low rank graph construction and the label information propagation processes together. Thus the optimization of the low rank representation and the soft label prediction function are calculated iteratively at the same time. We name this method as Semi-Supervised Low Rank Learning (SSLRL). It enhanced the classification performance of traditional LRR-Graph based SSL by 5–30% and the running time is reduced from hundreds to less than ten seconds. Based on this method, a new outlier detection strategy is presented. This strategy succeeds with an AUC of at least 93% even if the detection condition of LRR is not satisfied. The effectiveness of SSLRL is demonstrated in semi-supervised classification, outlier detection, and salient detection tasks. These extensive experimental results highlight the outperforming of our method over state-of-the-art methods.},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2015.09.008},
  issn     = {0031-3203},
  keywords = {Low rank representation Semi-supervised learning Outlier detection Saliency detection},
  type     = {Journal Article},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320315003398},
}

@Article{Liu2013284,
  author   = {Yuanyuan Liu and L.C. Jiao and Fanhua Shang},
  title    = {An efficient matrix factorization based low-rank representation for subspace clustering},
  journal  = {Pattern Recognition},
  year     = {2013},
  volume   = {46},
  number   = {1},
  pages    = {284 - 292},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2012.06.011},
  issn     = {0031-3203},
  keywords = {Nuclear norm minimization (NNM), Low rank representation, Alternating direction method (ADM), Matrix tri-factorization, Positive semidefinite (PSD) },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320312002816},
}

@Article{He20161041,
  author   = {Zhi He and Lin Liu and Suhong Zhou and Yi Shen},
  title    = {Learning group-based sparse and low-rank representation for hyperspectral image classification},
  journal  = {Pattern Recognition},
  year     = {2016},
  volume   = {60},
  pages    = {1041 - 1056},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2016.04.009},
  issn     = {0031-3203},
  keywords = {Classification, Hyperspectral image (HSI), Dictionary learning, Sparse representation, Low-rank representation },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320316300425},
}

@Article{Zhao2016281,
  author   = {Miaoyun Zhao and Licheng Jiao and Wenping Ma and Hongying Liu and Shuyuan Yang},
  title    = {Classification and saliency detection by semi-supervised low-rank representation},
  journal  = {Pattern Recognition},
  year     = {2016},
  volume   = {51},
  pages    = {281 - 294},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2015.09.008},
  issn     = {0031-3203},
  keywords = {Low rank representation, Semi-supervised learning, Outlier detection, Saliency detection },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320315003398},
}

@Article{Liu2013163,
  author   = {Yuanyuan Liu and L.C. Jiao and Fanhua Shang},
  title    = {A fast tri-factorization method for low-rank matrix recovery and completion},
  journal  = {Pattern Recognition},
  year     = {2013},
  volume   = {46},
  number   = {1},
  pages    = {163 - 173},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2012.07.003},
  issn     = {0031-3203},
  keywords = {Rank minimization, Nuclear norm minimization, Matrix completion, Low-rank and sparse decomposition, Low rank representation },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320312002981},
}

@Article{Li2016890,
  author   = {Yongqiang Li and Baoyuan Wu and Bernard Ghanem and Yongping Zhao and Hongxun Yao and Qiang Ji},
  title    = {Facial action unit recognition under incomplete data based on multi-label learning with missing labels},
  journal  = {Pattern Recognition},
  year     = {2016},
  volume   = {60},
  pages    = {890 - 900},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2016.07.009},
  issn     = {0031-3203},
  keywords = {Face action unit recognition, Incomplete data, Multi-label learning },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320316301583},
}

@Article{Wu20152279,
  author   = {Baoyuan Wu and Siwei Lyu and Bao-Gang Hu and Qiang Ji},
  title    = {Multi-label learning with missing labels for image annotation and facial action unit recognition},
  journal  = {Pattern Recognition},
  year     = {2015},
  volume   = {48},
  number   = {7},
  pages    = {2279 - 2289},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2015.01.022},
  issn     = {0031-3203},
  keywords = {Multi-label learning, Missing labels, Image annotation, Facial action unit recognition },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320315000412},
}

@Article{Liu201685,
  author   = {Zhun-ga Liu and Quan Pan and Jean Dezert and Arnaud Martin},
  title    = {Adaptive imputation of missing values for incomplete pattern classification},
  journal  = {Pattern Recognition},
  year     = {2016},
  volume   = {52},
  pages    = {85 - 95},
  doi      = {http://dx.doi.org/10.1016/j.patcog.2015.10.001},
  issn     = {0031-3203},
  keywords = {Belief function, Classification, Missing values, SOM, K-NN },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320315003684},
}

@Article{Nie2015,
  author   = {Nie, Feiping and Wang, Hua and Huang, Heng and Ding, Chris},
  title    = {Joint {Schatten} p-norm and lp-norm robust matrix completion for missing value recovery},
  journal  = {Knowledge and Information Systems},
  year     = {2015},
  volume   = {42},
  number   = {3},
  pages    = {525--544},
  abstract = {The low-rank matrix completion problem is a fundamental machine learning and data mining problem with many important applications. The standard low-rank matrix completion methods relax the rank minimization problem by the trace norm minimization. However, this relaxation may make the solution seriously deviate from the original solution. Meanwhile, most completion methods minimize the squared prediction errors on the observed entries, which is sensitive to outliers. In this paper, we propose a new robust matrix completion method to address these two problems. The joint Schatten                                                                           {\$}{\$}p{\$}{\$}                                                            p                                                      -norm and                                                                           {\$}{\$}{\backslash}ell {\_}p{\$}{\$}                                                                                    ℓ                        p                                                                            -norm are used to better approximate the rank minimization problem and enhance the robustness to outliers. The extensive experiments are performed on both synthetic data and real-world applications in collaborative filtering prediction and social network link recovery. All empirical results show that our new method outperforms the standard matrix completion methods.},
  doi      = {10.1007/s10115-013-0713-z},
  issn     = {0219-3116},
  url      = {http://dx.doi.org/10.1007/s10115-013-0713-z},
}

@Article{Fan2017290,
  author   = {Jicong Fan and Tommy W.S. Chow},
  title    = {Matrix completion by least-square, low-rank, and sparse self-representations},
  journal  = {Pattern Recognition},
  year     = {2017},
  volume   = {71},
  pages    = {290 - 305},
  doi      = {https://doi.org/10.1016/j.patcog.2017.05.013},
  issn     = {0031-3203},
  keywords = {Matrix completion, Missing value, Low-rank and sparse representations, Image inpainting, Collaborative filtering },
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320317302030},
}

@Article{Fan201736,
  author   = {Jicong Fan and Tommy W.S. Chow},
  title    = {Sparse subspace clustering for data with missing entries and high-rank matrix completion},
  journal  = {Neural Networks},
  year     = {2017},
  volume   = {93},
  pages    = {36 - 44},
  doi      = {https://doi.org/10.1016/j.neunet.2017.04.005},
  issn     = {0893-6080},
  keywords = {Subspace clustering, Sparse representation, Missing entries, High-rank, Matrix completion },
  url      = {http://www.sciencedirect.com/science/article/pii/S0893608017300850},
}

@Article{imageinpainting2014,
  author   = {C. Guillemot and O. Le Meur},
  title    = {Image Inpainting : Overview and Recent Advances},
  journal  = {IEEE Signal Processing Magazine},
  year     = {2014},
  volume   = {31},
  number   = {1},
  pages    = {127-144},
  month    = {Jan},
  doi      = {10.1109/MSP.2013.2273004},
  issn     = {1053-5888},
  keywords = {image restoration;IBR;art restoration;cameras;disocclusion;image inpainting;image restoration;image-based rendering;impaired image transmission;object removal;text overlays;Image color analysis;Image edge detection;Mathematical model;Smoothing methods;Tensile stress},
}

@Article{HOTV2011,
  author  = {Carola-Bibiane Schönlieb and Andrea Bertozzi},
  title   = {Unconditionally stable schemes for higher order inpainting},
  journal = {Communications in Mathematical Sciences},
  year    = {2011},
  volume  = {9},
  number  = {2},
  pages   = {413-457},
}

@Article{Benning2013,
  author   = {Benning, Martin and Brune, Christoph and Burger, Martin and M{\"u}ller, Jahn},
  title    = {Higher-Order TV method---enhancement via Bregman iteration},
  journal  = {Journal of Scientific Computing},
  year     = {2013},
  volume   = {54},
  number   = {2},
  pages    = {269--310},
  abstract = {In this work we analyze and compare two recent variational models for image denoising and improve their reconstructions by applying a Bregman iteration strategy. One of the standard techniques in image denoising, the ROF-model (cf. Rudin et al. in Physica D 60:259--268, 1992), is well known for recovering sharp edges of a signal or image, but also for producing staircase-like artifacts. In order to overcome these model-dependent deficiencies, total variation modifications that incorporate higher-order derivatives have been proposed (cf. Chambolle and Lions in Numer. Math. 76:167--188, 1997; Bredies et al. in SIAM J. Imaging Sci. 3(3):492--526, 2010). These models reduce staircasing for reasonable parameter choices. However, the combination of derivatives of different order leads to other undesired side effects, which we shall also highlight in several examples.},
  doi      = {10.1007/s10915-012-9650-3},
  issn     = {1573-7691},
  url      = {http://dx.doi.org/10.1007/s10915-012-9650-3},
}

@InProceedings{zhang2011sparse,
  author       = {Zhang, Lei and Yang, Meng and Feng, Xiangchu},
  title        = {Sparse representation or collaborative representation: Which helps face recognition?},
  booktitle    = {Computer vision (ICCV), 2011 IEEE international conference on},
  year         = {2011},
  pages        = {471--478},
  organization = {IEEE},
}

@Article{wright2009robust,
  author    = {Wright, John and Yang, Allen Y and Ganesh, Arvind and Sastry, S Shankar and Ma, Yi},
  title     = {Robust face recognition via sparse representation},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2009},
  volume    = {31},
  number    = {2},
  pages     = {210--227},
  publisher = {IEEE},
}

@Article{MA2017,
  author   = {Jianghong Ma and Zhaoyang Tian and Haijun Zhang and Tommy W.S. Chow},
  title    = {Multi-Label Low-dimensional Embedding with Missing Labels},
  journal  = {Knowledge-Based Systems},
  year     = {2017},
  doi      = {https://doi.org/10.1016/j.knosys.2017.09.005},
  issn     = {0950-7051},
  keywords = {Label imputation, Low rank, Instance-wise label correlation, Inductive classifier},
  url      = {http://www.sciencedirect.com/science/article/pii/S0950705117304136},
}

@Article{MA2018336,
  author   = {Jianghong Ma and Tommy W.S. Chow},
  title    = {Robust non-negative sparse graph for semi-supervised multi-label learning with missing labels},
  journal  = {Information Sciences},
  year     = {2018},
  volume   = {422},
  number   = {Supplement C},
  pages    = {336 - 351},
  doi      = {https://doi.org/10.1016/j.ins.2017.08.061},
  issn     = {0020-0255},
  keywords = {Label recovery, Semi-supervised setting, Missing labels, Semantic structure, Semantic correlation},
  url      = {http://www.sciencedirect.com/science/article/pii/S0020025517309088},
}

@Article{FAN2014205,
  author   = {Jicong Fan and S. Joe Qin and Youqing Wang},
  title    = {Online monitoring of nonlinear multivariate industrial processes using filtering KICA–PCA},
  journal  = {Control Engineering Practice},
  year     = {2014},
  volume   = {22},
  pages    = {205 - 216},
  doi      = {https://doi.org/10.1016/j.conengprac.2013.06.017},
  issn     = {0967-0661},
  keywords = {Process monitoring, KICA–PCA, Variance of independent component, EWMA, Variable contribution analysis, TE process},
  url      = {http://www.sciencedirect.com/science/article/pii/S0967066113001275},
}

@Article{FAN2017540,
  author   = {Jicong Fan and Tommy Chow},
  title    = {Deep learning based matrix completion},
  journal  = {Neurocomputing},
  year     = {2017},
  volume   = {266},
  number   = {Supplement C},
  pages    = {540 - 549},
  doi      = {https://doi.org/10.1016/j.neucom.2017.05.074},
  issn     = {0925-2312},
  keywords = {Matrix completion, AutoEncoder, deep learning, out-of-sample extension, image inpainting, collaborative filtering},
  url      = {http://www.sciencedirect.com/science/article/pii/S0925231217309621},
}

@Article{Fan2017,
  author   = {Fan, Jicong and Chow, Tommy W. S. and Zhao, Mingbo and Ho, John K. L.},
  title    = {Nonlinear Dimensionality Reduction for Data with Disconnected Neighborhood Graph},
  journal  = {Neural Processing Letters},
  year     = {2017},
  month    = {Aug},
  abstract = {Neighborhood graph based nonlinear dimensionality reduction algorithms, such as Isomap and LLE, perform well under an assumption that the neighborhood graph is connected. However, for datasets consisting of multiple clusters or lying on multiple manifolds, the neighborhood graphs are often disconnected, or in other words, have multiple connected components. Neighborhood graph based dimensionality reduction techniques cannot recognize both the local and global properties of such datasets. In this paper, a new method, called enhanced neighborhood graph, is proposed to solve the problem. The concept is to add edges to the neighborhood graph adaptively and iteratively until it becomes connected. Nonlinear dimensionality reduction can then be performed based on the enhanced neighborhood graph. As a result, both local and global properties of the data can be exactly recognized. In this study, thorough simulations on synthetic datasets and natural datasets are conducted. The experimental results corroborate that the proposed method provides significant improvements on dimensionality reduction for data with disconnected neighborhood graph.},
  day      = {03},
  doi      = {10.1007/s11063-017-9676-5},
  issn     = {1573-773X},
  url      = {https://doi.org/10.1007/s11063-017-9676-5},
}

@Article{6471823,
  author   = {Y. Chen and A. Jalali and S. Sanghavi and C. Caramanis},
  title    = {Low-Rank Matrix Recovery From Errors and Erasures},
  journal  = {IEEE Transactions on Information Theory},
  year     = {2013},
  volume   = {59},
  number   = {7},
  pages    = {4324-4337},
  month    = {July},
  doi      = {10.1109/TIT.2013.2249572},
  issn     = {0018-9448},
  keywords = {matrix decomposition;pattern recognition;erasure patterns;error patterns;low-rank matrix decomposition;low-rank matrix recovery;sparse matrix decomposition;Collaboration;Information theory;Matrix decomposition;Principal component analysis;Sparse matrices;Standards;Technological innovation;Low-rank;matrix decomposition;robustness;sparsity;statistical learning},
}

@Article{7064749,
  author   = {Y. Chen},
  title    = {Incoherence-Optimal Matrix Completion},
  journal  = {IEEE Transactions on Information Theory},
  year     = {2015},
  volume   = {61},
  number   = {5},
  pages    = {2909-2923},
  month    = {May},
  doi      = {10.1109/TIT.2015.2415195},
  issn     = {0018-9448},
  keywords = {computational complexity;singular value decomposition;sparse matrices;ℓ∞,2-norm;incoherence optimal matrix completion problem;incoherence parameter;joint incoherence condition;low rank plus sparse matrix decomposition;matrix decomposition problem;planted clique conjecture;polynomial- time algorithm;sample complexity bound;semidefinite matrix recovery;semisupervised clustering;singular value decomposition projection;structured matrix completion;Complexity theory;Information theory;Joints;Matrix decomposition;Sparse matrices;Standards;Vectors;Matrix completion;computational barrier;incoherence;nuclear norm minimization;robust PCA},
}

@Article{YPlan2012,
  author   = {Y.C. Eldar and D. Needell and Y. Plan},
  title    = {Uniqueness conditions for low-rank matrix recovery},
  journal  = {Applied and Computational Harmonic Analysis},
  year     = {2012},
  volume   = {33},
  number   = {2},
  pages    = {309 - 314},
  doi      = {https://doi.org/10.1016/j.acha.2012.04.002},
  issn     = {1063-5203},
  keywords = {Rank minimization, Nuclear-norm minimization, Low-rank matrix recovery, Random matrices, Compressed sensing},
  url      = {http://www.sciencedirect.com/science/article/pii/S1063520312000528},
}

@Article{Review_LRMC,
  author   = {M. A. Davenport and J. Romberg},
  title    = {An Overview of Low-Rank Matrix Recovery From Incomplete Observations},
  journal  = {IEEE Journal of Selected Topics in Signal Processing},
  year     = {2016},
  volume   = {10},
  number   = {4},
  pages    = {608-622},
  month    = {June},
  doi      = {10.1109/JSTSP.2016.2539100},
  issn     = {1932-4553},
  keywords = {matrix algebra;signal processing;incomplete observations;indirect observations;low-rank matrix recovery;machine learning;signal processing;Analytical models;Computational modeling;Context;Matrix decomposition;Sensor arrays;Signal processing;Signal processing algorithms;Blind deconvolution;low-rank matrices;matrix completion;matrix recovery algorithms;phase retrieval},
}

@Article{5466511,
  author   = {R. H. Keshavan and A. Montanari and S. Oh},
  title    = {Matrix Completion From a Few Entries},
  journal  = {IEEE Transactions on Information Theory},
  year     = {2010},
  volume   = {56},
  number   = {6},
  pages    = {2980-2998},
  month    = {June},
  doi      = {10.1109/TIT.2010.2046205},
  issn     = {0018-9448},
  keywords = {matrix algebra;signal reconstruction;Feige-Ofek;Friedman-Kahn-Szemeredi;OptSpace;massive data sets;matrix completion;reconstruction algorithm;sparse random matrices;Collaboration;Information filtering;Information filters;Mathematical model;Motion pictures;Optimization methods;Reconstruction algorithms;Root mean square;Sparse matrices;Watches;Gradient descent;low rank;manifold optimization;matrix completion;phase transition;spectral methods},
}

@InProceedings{hardt2014understanding,
  author       = {Hardt, Moritz},
  title        = {Understanding alternating minimization for matrix completion},
  booktitle    = {2014 IEEE 55th Annual Symposium on Foundations of Computer Science (FOCS), },
  year         = {2014},
  pages        = {651--660},
  organization = {IEEE},
}

@InProceedings{pmlr-v70-ongie17a,
  author    = {Greg Ongie and Rebecca Willett and Robert D. Nowak and Laura Balzano},
  title     = {Algebraic Variety Models for High-Rank Matrix Completion},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  year      = {2017},
  pages     = {2691--2700},
  address   = {Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
}

@Article{FANNLMC,
  author   = {Jicong Fan and Tommy W.S. Chow},
  title    = {Non-linear matrix completion},
  journal  = {Pattern Recognition},
  year     = {2018},
  volume   = {77},
  pages    = {378 - 394},
  doi      = {https://doi.org/10.1016/j.patcog.2017.10.014},
  issn     = {0031-3203},
  keywords = {Matrix completion, Low-rank, Kernel, Schatten -norm, Image inpainting, Single-/multi-label classification, Non-linear denoising},
  url      = {http://www.sciencedirect.com/science/article/pii/S0031320317304028},
}

@InProceedings{pimentel2016information,
  author    = {Pimentel-Alarcon, Daniel and Nowak, Robert},
  title     = {The information-theoretic requirements of subspace clustering with missing data},
  booktitle = {International Conference on Machine Learning},
  year      = {2016},
  pages     = {802--810},
}

@InProceedings{eriksson2012high,
  author    = {Eriksson, Brian and Balzano, Laura and Nowak, Robert},
  title     = {High-rank matrix completion},
  booktitle = {Artificial Intelligence and Statistics},
  year      = {2012},
  pages     = {373--381},
}

@InProceedings{yang2015sparse,
  author    = {Yang, Congyuan and Robinson, Daniel and Vidal, Rene},
  title     = {Sparse subspace clustering with missing entries},
  booktitle = {International Conference on Machine Learning},
  year      = {2015},
  pages     = {2463--2472},
}

@InCollection{NIPS2016_6357,
  author    = {Elhamifar, Ehsan},
  title     = {High-Rank Matrix Completion and Clustering under Self-Expressive Models},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {73--81},
  url       = {http://papers.nips.cc/paper/6357-high-rank-matrix-completion-and-clustering-under-self-expressive-models.pdf},
}

@Article{adam2014,
  author  = {Kingma, Diederik P and Ba, Jimmy},
  title   = {Adam: A method for stochastic optimization},
  journal = {arXiv preprint arXiv:1412.6980},
  year    = {2014},
}

@Article{LADMC,
  author        = {{Ongie}, G. and {Balzano}, L. and {Pimentel-Alarc{\'o}n}, D. and {Willett}, R. and {Nowak}, R.~D.},
  title         = {{Tensor Methods for Nonlinear Matrix Completion}},
  journal       = {ArXiv e-prints},
  year          = {2018},
  month         = apr,
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl        = {http://adsabs.harvard.edu/abs/2018arXiv180410266O},
  archiveprefix = {arXiv},
  eprint        = {1804.10266},
  keywords      = {Statistics - Machine Learning, Computer Science - Learning},
  primaryclass  = {stat.ML},
}

@Article{pascoe2014inverse,
  author    = {Pascoe, JE},
  title     = {The inverse function theorem and the Jacobian conjecture for free analysis},
  journal   = {Mathematische Zeitschrift},
  year      = {2014},
  volume    = {278},
  number    = {3-4},
  pages     = {987--994},
  publisher = {Springer},
}

@Article{bass1982,
  author    = {Bass, Hyman and Connell, Edwin H. and Wright, David},
  title     = {The Jacobian conjecture: Reduction of degree and formal expansion of the inverse},
  journal   = {Bull. Amer. Math. Soc. (N.S.)},
  year      = {1982},
  volume    = {7},
  number    = {2},
  pages     = {287--330},
  month     = {09},
  fjournal  = {Bulletin (New Series) of the American Mathematical Society},
  publisher = {American Mathematical Society},
  url       = {https://projecteuclid.org:443/euclid.bams/1183549636},
}

@Book{van2012polynomial,
  title     = {Polynomial Automorphisms: and the Jacobian Conjecture},
  publisher = {Birkh{\"a}user},
  year      = {2012},
  author    = {Van den Essen, Arno},
  volume    = {190},
}

@InProceedings{4269999,
  author    = {R. Tron and R. Vidal},
  title     = {A Benchmark for the Comparison of 3-D Motion Segmentation Algorithms},
  booktitle = {2007 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2007},
  pages     = {1-8},
  month     = {June},
  doi       = {10.1109/CVPR.2007.382974},
  issn      = {1063-6919},
  keywords  = {computer vision;image segmentation;image sequences;motion estimation;3D motion segmentation algorithm;image sequence;Application software;Benchmark testing;Cameras;Computer vision;Databases;Image segmentation;Layout;Motion segmentation;Statistical analysis;Traffic control},
}

@Article{castella2008inversion,
  author    = {Castella, Marc},
  title     = {Inversion of polynomial systems and separation of nonlinear mixtures of finite-alphabet sources},
  journal   = {IEEE Transactions on Signal Processing},
  year      = {2008},
  volume    = {56},
  number    = {8},
  pages     = {3905--3917},
  publisher = {IEEE},
}

@InBook{Cox2007,
  pages     = {439--508},
  title     = {The Dimension of a Variety},
  publisher = {Springer New York},
  year      = {2007},
  author    = {Cox, David and Little, John and O'Shea, Donal},
  address   = {New York, NY},
  isbn      = {978-0-387-35651-8},
  abstract  = {The most important invariant of a linear subspace of affine space is its dimension. For affine varieties, we have seen numerous examples which have a clearly defined dimension, at least from a naive point of view. In this chapter, we will carefully define the dimension of any affine or projective variety and show how to compute it. We will also show that this notion accords well with what we would expect intuitively. In keeping with our general philosophy, we consider the computational side of dimension theory right from the outset.},
  booktitle = {Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra},
  doi       = {10.1007/978-0-387-35651-8_9},
  url       = {https://doi.org/10.1007/978-0-387-35651-8_9},
}

@Book{eisenbud_harris_2016,
  title     = {3264 and All That: A Second Course in Algebraic Geometry},
  publisher = {Cambridge University Press},
  year      = {2016},
  author    = {Eisenbud, David and Harris, Joe},
  doi       = {10.1017/CBO9781139062046},
  place     = {Cambridge},
}

@Book{IntroApproxTheory,
  title     = {Introduction to approximation theory},
  publisher = {McGraw-Hill},
  year      = {1966},
  author    = {Cheney, Elliott Ward},
}

@Article{Mironenko2011,
  author   = {Mironenko, A. V.},
  title    = {On the Jackson-Stechkin inequality for algebraic polynomials},
  journal  = {Proceedings of the Steklov Institute of Mathematics},
  year     = {2011},
  volume   = {273},
  number   = {1},
  pages    = {116},
  month    = {Jun},
  issn     = {1531-8605},
  abstract = {The Jackson-Stechkin inequality is considered, which estimates the value of the best uniform approximation of a continuous function by algebraic polynomials on a closed interval in terms of values of the modulus of continuity of the approximated function. A variant of the inequality with second-order modulus of continuity is proved, in which the argument of the modulus of continuity and the constant are specified explicitly.},
  day      = {18},
  doi      = {10.1134/S0081543811050129},
  url      = {https://doi.org/10.1134/S0081543811050129},
}

@Article{babenko2017jackson,
  author  = {Babenko, AG and Kryakin, Yu V},
  title   = {On the Jackson constants for algebraic approximation of continuous functions},
  journal = {arXiv preprint arXiv:1705.05614},
  year    = {2017},
}

@InProceedings{jin2016provable,
  author    = {Jin, Chi and Kakade, Sham M and Netrapalli, Praneeth},
  title     = {Provable efficient online matrix completion via non-convex stochastic gradient descent},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2016},
  pages     = {4520--4528},
}

@Article{sun2016guaranteed,
  author    = {Sun, Ruoyu and Luo, Zhi-Quan},
  title     = {Guaranteed matrix completion via non-convex factorization},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2016},
  volume    = {62},
  number    = {11},
  pages     = {6535--6579},
  publisher = {IEEE},
}

@InProceedings{srebro2005rank,
  author       = {Srebro, Nathan and Shraibman, Adi},
  title        = {Rank, trace-norm and max-norm},
  booktitle    = {International Conference on Computational Learning Theory},
  year         = {2005},
  pages        = {545--560},
  organization = {Springer},
}

@InProceedings{foygel2011concentration,
  author    = {Foygel, Rina and Srebro, Nathan},
  title     = {Concentration-based guarantees for low-rank matrix reconstruction},
  booktitle = {Proceedings of the 24th Annual Conference on Learning Theory},
  year      = {2011},
  pages     = {315--340},
}

@InProceedings{shang2016tractable,
  author    = {Shang, Fanhua and Liu, Yuanyuan and Cheng, James},
  title     = {Tractable and scalable {Schatten} quasi-norm approximations for rank minimization},
  booktitle = {Artificial Intelligence and Statistics},
  year      = {2016},
  pages     = {620--629},
}

@Article{bolte2014proximal,
  author    = {Bolte, J{\'e}r{\^o}me and Sabach, Shoham and Teboulle, Marc},
  title     = {Proximal alternating linearized minimization for nonconvex and nonsmooth problems},
  journal   = {Mathematical Programming},
  year      = {2014},
  volume    = {146},
  number    = {1-2},
  pages     = {459--494},
  publisher = {Springer},
}

@Article{wen2017linear,
  author    = {Wen, Bo and Chen, Xiaojun and Pong, Ting Kei},
  title     = {Linear convergence of proximal gradient algorithm with extrapolation for a class of nonconvex nonsmooth minimization problems},
  journal   = {SIAM Journal on Optimization},
  year      = {2017},
  volume    = {27},
  number    = {1},
  pages     = {124--145},
  publisher = {SIAM},
}

@Article{FAN2018SFMC,
  author   = {J. Fan and M. Zhao and T. W. S. Chow},
  title    = {Matrix completion via sparse factorization solved by accelerated proximal alternating linearized minimization},
  journal  = {IEEE Transactions on Big Data},
  year     = {2018},
  pages    = {1-1},
  issn     = {2332-7790},
  doi      = {10.1109/TBDATA.2018.2871476},
  keywords = {Sparse matrices;Matrix decomposition;Acceleration;Minimization;Optimization;Collaboration;Big Data;matrix completion;sparse factorization;low-rank;accelerated proximal alternating linearized minimization;nonconvex optimization;collaborative filtering},
}

@Article{ADMM,
  author  = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
  title   = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
  journal = {Found. Trends Mach. Learn.},
  year    = {2011},
  volume  = {3},
  number  = {1},
  pages   = {1-122},
  issn    = {1935-8237},
  doi     = {10.1561/2200000016},
  type    = {Journal Article},
}

@Article{wang2015global,
  author    = {Wang, Yu and Yin, Wotao and Zeng, Jinshan},
  title     = {Global convergence of ADMM in nonconvex nonsmooth optimization},
  journal   = {Journal of Scientific Computing},
  year      = {2015},
  pages     = {1--35},
  publisher = {Springer},
}

@Article{gao2018admm,
  author  = {Gao, Wenbo and Goldfarb, Donald and Curtis, Frank E},
  title   = {ADMM for Multiaffine Constrained Optimization},
  journal = {arXiv preprint arXiv:1802.09592},
  year    = {2018},
}

@Article{cai2014direct,
  author  = {Cai, Xingju and Han, Deren and Yuan, Xiaoming},
  title   = {The direct extension of ADMM for three-block separable convex minimization models is convergent when one function is strongly convex},
  journal = {Optimization Online},
  year    = {2014},
  volume  = {229},
  pages   = {230},
}

@Article{liu2017linearized,
  author  = {Liu, Qinghua and Shen, Xinyue and Gu, Yuantao},
  title   = {Linearized admm for non-convex non-smooth optimization with convergence analysis},
  journal = {arXiv preprint arXiv:1705.02502},
  year    = {2017},
}

@Article{GRCAR2010203,
  author   = {Joseph F. Grcar},
  title    = {A matrix lower bound},
  journal  = {Linear Algebra and its Applications},
  year     = {2010},
  volume   = {433},
  number   = {1},
  pages    = {203 - 220},
  issn     = {0024-3795},
  abstract = {Four essentially different interpretations of a lower bound for linear operators are shown to be equivalent for matrices (involving inequalities, convex sets, minimax problems, and quotient spaces). Properties stated by von Neumann in a restricted case are satisfied by the lower bound. Applications are made to rank reduction, s-numbers, condition numbers, and pseudospectra. In particular, the matrix lower bound is the distance to the nearest matrix with strictly contained row or column spaces, and it occurs in a condition number formula for any consistent system of linear equations, including those that are underdetermined.},
  doi      = {https://doi.org/10.1016/j.laa.2010.02.014},
  keywords = {Matrix inequalities, Distance to lower rank, Functional analysis in matrix theory, Condition number},
  url      = {http://www.sciencedirect.com/science/article/pii/S0024379510000923},
}

@InProceedings{taylor2016training,
  author    = {Taylor, Gavin and Burmeister, Ryan and Xu, Zheng and Singh, Bharat and Patel, Ankit and Goldstein, Tom},
  title     = {Training neural networks without gradients: A scalable admm approach},
  booktitle = {International Conference on Machine Learning},
  year      = {2016},
  pages     = {2722--2731},
}

@InProceedings{gunasekar2017implicit,
  author    = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  title     = {Implicit regularization in matrix factorization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {6151--6159},
}

@InProceedings{srebro2005maximum,
  author    = {Srebro, Nathan and Rennie, Jason and Jaakkola, Tommi S.},
  title     = {Maximum-margin matrix factorization},
  booktitle = {Advances in neural information processing systems},
  year      = {2005},
  pages     = {1329--1336},
}

@InProceedings{rennie2005fast,
  author       = {Rennie, Jasson DM and Srebro, Nathan},
  title        = {Fast maximum margin matrix factorization for collaborative prediction},
  booktitle    = {Proceedings of the 22nd international conference on Machine learning},
  year         = {2005},
  pages        = {713--719},
  organization = {ACM},
}

@Article{wang2012stability,
  author  = {Wang, Yu-Xiang and Xu, Huan},
  title   = {Stability of matrix factorization for collaborative filtering},
  journal = {arXiv preprint arXiv:1206.4640},
  year    = {2012},
}

@Article{negahban2012restricted,
  author  = {Negahban, Sahand and Wainwright, Martin J.},
  title   = {Restricted strong convexity and weighted matrix completion: Optimal bounds with noise},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {May},
  pages   = {1665--1697},
}

@Article{raskutti2011minimax,
  author    = {Raskutti, Garvesh and Wainwright, Martin J. and Yu, Bin},
  title     = {Minimax rates of estimation for high-dimensional linear regression over $\ell_q$-balls},
  journal   = {IEEE transactions on information theory},
  year      = {2011},
  volume    = {57},
  number    = {10},
  pages     = {6976--6994},
  publisher = {IEEE},
}

@Article{ahlswede2002strong,
  author    = {Ahlswede, Rudolf and Winter, Andreas},
  title     = {Strong converse for identification via quantum channels},
  journal   = {IEEE Transactions on Information Theory},
  year      = {2002},
  volume    = {48},
  number    = {3},
  pages     = {569--579},
  publisher = {IEEE},
}

@InProceedings{lee2010practical,
  author    = {Lee, Jason D. and Recht, Ben and Srebro, Nathan and Tropp, Joel and Salakhutdinov, Ruslan R.},
  title     = {Practical large-scale optimization for max-norm regularization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2010},
  pages     = {1297--1305},
}

@Article{mohan2012iterative,
  author  = {Mohan, Karthik and Fazel, Maryam},
  title   = {Iterative reweighted algorithms for matrix rank minimization},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {Nov},
  pages   = {3441--3473},
}

@InProceedings{pimentel2017random,
  author    = {Pimentel-Alarc{\'o}n, Daniel and Nowak, Robert},
  title     = {Random Consensus Robust {PCA}},
  booktitle = {Artificial Intelligence and Statistics},
  year      = {2017},
  pages     = {344--352},
}

@InProceedings{feng2013online,
  author    = {Feng, Jiashi and Xu, Huan and Yan, Shuicheng},
  title     = {Online robust {PCA} via stochastic optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2013},
  pages     = {404--412},
}

@InProceedings{zhao2014robust,
  author    = {Zhao, Qian and Meng, Deyu and Xu, Zongben and Zuo, Wangmeng and Zhang, Lei},
  title     = {Robust principal component analysis with complex noise},
  booktitle = {International Conference on Machine Learning},
  year      = {2014},
  pages     = {55--63},
}

@InCollection{NIPS2010_4102,
  author    = {Nathan Srebro and Salakhutdinov, Ruslan R},
  title     = {Collaborative Filtering in a Non-Uniform World: Learning with the Weighted Trace Norm},
  booktitle = {Advances in Neural Information Processing Systems 23},
  publisher = {Curran Associates, Inc.},
  year      = {2010},
  editor    = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
  pages     = {2056--2064},
  url       = {http://papers.nips.cc/paper/4102-collaborative-filtering-in-a-non-uniform-world-learning-with-the-weighted-trace-norm.pdf},
}

@Article{toh2010accelerated,
  author  = {Toh, Kim-Chuan and Yun, Sangwoon},
  title   = {An accelerated proximal gradient algorithm for nuclear norm regularized linear least squares problems},
  journal = {Pacific Journal of optimization},
  year    = {2010},
  volume  = {6},
  number  = {615-640},
  pages   = {15},
}

@Book{wainwright2019high,
  title     = {High-dimensional statistics: A non-asymptotic viewpoint},
  publisher = {Cambridge University Press},
  year      = {2019},
  author    = {Wainwright, Martin J.},
  volume    = {48},
}

@Article{klein2005concentration,
  author    = {Klein, Thierry and Rio, Emmanuel and others},
  title     = {Concentration around the mean for maxima of empirical processes},
  journal   = {The Annals of Probability},
  year      = {2005},
  volume    = {33},
  number    = {3},
  pages     = {1060--1077},
  publisher = {Institute of Mathematical Statistics},
}

@Article{lee2005acquiring,
  author    = {Lee, Kuang-Chih and Ho, Jeffrey and Kriegman, David J},
  title     = {Acquiring linear subspaces for face recognition under variable lighting},
  journal   = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
  year      = {2005},
  number    = {5},
  pages     = {684--698},
  publisher = {IEEE},
}

@Article{8425659,
  author  = {T. {Bouwmans} and S. {Javed} and H. {Zhang} and Z. {Lin} and R. {Otazo}},
  title   = {On the Applications of Robust {PCA} in Image and Video Processing},
  journal = {Proceedings of the IEEE},
  year    = {2018},
  volume  = {106},
  number  = {8},
  pages   = {1427-1457},
  month   = {Aug},
  issn    = {0018-9219},
  doi     = {10.1109/JPROC.2018.2853589},
}

@Article{shamir2014matrix,
  author    = {Shamir, Ohad and Shalev-Shwartz, Shai},
  title     = {Matrix completion with the trace norm: learning, bounding, and transducing},
  journal   = {The Journal of Machine Learning Research},
  year      = {2014},
  volume    = {15},
  number    = {1},
  pages     = {3401--3423},
  publisher = {JMLR. org},
}

@Article{Web_MovieLens100K,
  title   = {MovieLens 100K Dataset},
  journal = {http://grouplens.org/datasets/movielens/100k/},
}

@Article{udell2019big,
  author    = {Udell, Madeleine and Townsend, Alex},
  title     = {Why Are Big Data Matrices Approximately Low Rank?},
  journal   = {SIAM Journal on Mathematics of Data Science},
  year      = {2019},
  volume    = {1},
  number    = {1},
  pages     = {144--160},
  publisher = {SIAM},
}

@Article{udell2016generalized,
  author    = {Udell, Madeleine and Horn, Corinne and Zadeh, Reza and Boyd, Stephen and others},
  title     = {Generalized low rank models},
  journal   = {Foundations and Trends{\textregistered} in Machine Learning},
  year      = {2016},
  volume    = {9},
  number    = {1},
  pages     = {1--118},
  publisher = {Now Publishers, Inc.},
}

@InProceedings{Fan_2019_CVPR,
  author    = {Fan, Jicong and Udell, Madeleine},
  title     = {Online High Rank Matrix Completion},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2019},
  month     = {June},
}

@Article{rkpca_fan,
  author   = {J. {Fan} and T. W. S. {Chow}},
  title    = {Exactly Robust Kernel Principal Component Analysis},
  journal  = {IEEE Transactions on Neural Networks and Learning Systems},
  year     = {2019},
  pages    = {1-13},
  issn     = {2162-237X},
  doi      = {10.1109/TNNLS.2019.2909686},
  keywords = {Sparse matrices;Principal component analysis;Minimization;Kernel;Matrix decomposition;Feature extraction;Optimization;High rank;kernel;low rank;noise removal;robust principal component analysis (RPCA);sparse;subspace clustering.},
}

@InProceedings{gu2014weighted,
  author    = {Gu, Shuhang and Zhang, Lei and Zuo, Wangmeng and Feng, Xiangchu},
  title     = {Weighted nuclear norm minimization with application to image denoising},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year      = {2014},
  pages     = {2862--2869},
}

@InProceedings{tan2014riemannian,
  author    = {Tan, Mingkui and Tsang, Ivor W and Wang, Li and Vandereycken, Bart and Pan, Sinno Jialin},
  title     = {Riemannian pursuit for big matrix recovery},
  booktitle = {International Conference on Machine Learning},
  year      = {2014},
  pages     = {1539--1547},
}

@Article{shang2017bilinear,
  author    = {Shang, Fanhua and Cheng, James and Liu, Yuanyuan and Luo, Zhi-Quan and Lin, Zhouchen},
  title     = {Bilinear factor matrix norm minimization for robust PCA: Algorithms and applications},
  journal   = {IEEE transactions on pattern analysis and machine intelligence},
  year      = {2017},
  volume    = {40},
  number    = {9},
  pages     = {2066--2080},
  publisher = {IEEE},
}

@Comment{jabref-meta: databaseType:bibtex;}
