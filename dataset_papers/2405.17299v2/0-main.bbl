\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{a19f}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock {Fine-Grained Analysis of Optimization and Generalization for
  Overparameterized Two-Layer Neural Networks}.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  322--332. PMLR, 09--15
  Jun 2019.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, and Lacoste-Julien]{a17a}
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.~S.,
  Maharaj, T., Fischer, A., Courville, A., Bengio, Y., and Lacoste-Julien, S.
\newblock {A Closer Look at Memorization in Deep Networks}.
\newblock In Precup, D. and Teh, Y.~W. (eds.), \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pp.\  233--242. PMLR, 06--11 Aug 2017.

\bibitem[Awasthi et~al.(2023)Awasthi, Tang, and Vijayaraghavan]{a23a}
Awasthi, P., Tang, A., and Vijayaraghavan, A.
\newblock {Agnostic Learning of General ReLU Activation Using Gradient
  Descent}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Ba et~al.(2022)Ba, Erdogdu, Suzuki, Wang, Wu, and Yang]{ba22h}
Ba, J., Erdogdu, M.~A., Suzuki, T., Wang, Z., Wu, D., and Yang, G.
\newblock {High-dimensional Asymptotics of Feature Learning: How One Gradient
  Step Improves the Representation}.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  37932--37946. Curran Associates, Inc., 2022.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{b20f}
Basri, R., Galun, M., Geifman, A., Jacobs, D., Kasten, Y., and Kritchman, S.
\newblock {Frequency Bias in Neural Networks for Input of Non-Uniform Density}.
\newblock In III, H.~D. and Singh, A. (eds.), \emph{Proceedings of the 37th
  International Conference on Machine Learning}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  685--694. PMLR, 13--18
  Jul 2020.

\bibitem[Boursier \& Flammarion(2024)Boursier and Flammarion]{b24e}
Boursier, E. and Flammarion, N.
\newblock Early alignment in two-layer networks training is a two-edged sword,
  2024.
\newblock arXiv:2401.10791 [cs.LG].

\bibitem[Boursier et~al.(2022)Boursier, Pillaud-Vivien, and Flammarion]{b22g}
Boursier, E., Pillaud-Vivien, L., and Flammarion, N.
\newblock Gradient flow dynamics of shallow {ReLU} networks for square loss and
  orthogonal inputs.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  20105--20118. Curran Associates, Inc., 2022.

\bibitem[Bowman \& Montufar(2022)Bowman and Montufar]{b22i}
Bowman, B. and Montufar, G.
\newblock {Implicit Bias of MSE Gradient Optimization in Underparameterized
  Neural Networks}.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Brutzkus \& Globerson(2019)Brutzkus and Globerson]{b19w}
Brutzkus, A. and Globerson, A.
\newblock {Why do Larger Models Generalize Better? A Theoretical Perspective
  via the XOR Problem}.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  822--830. PMLR, 09--15
  Jun 2019.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{b18s}
Brutzkus, A., Globerson, A., Malach, E., and Shalev-Shwartz, S.
\newblock {SGD Learns Over-parameterized Networks that Provably Generalize on
  Linearly Separable Data}.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chistikov et~al.(2023)Chistikov, Englert, and Lazic]{c23l}
Chistikov, D., Englert, M., and Lazic, R.
\newblock {Learning a Neuron by a Shallow ReLU Network: Dynamics and Implicit
  Bias for Correlated Inputs}.
\newblock In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and
  Levine, S. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~36, pp.\  23748--23760. Curran Associates, Inc., 2023.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{c18o}
Chizat, L. and Bach, F.
\newblock {On the Global Convergence of Gradient Descent for Over-parameterized
  Models using Optimal Transport}.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{c19o}
Chizat, L., Oyallon, E., and Bach, F.
\newblock {On Lazy Training in Differentiable Programming}.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d\textquotesingle
  Alch\'{e}-Buc, F., Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Cybenko(1989)]{c89a}
Cybenko, G.
\newblock Approximation by superpositions of a sigmoidal function.
\newblock \emph{Mathematics of control, signals and systems}, 2\penalty0
  (4):\penalty0 303--314, 1989.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{d18a}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock {Algorithmic Regularization in Learning Deep Homogeneous Models:
  Layers are Automatically Balanced}.
\newblock In Bengio, S., Wallach, H., Larochelle, H., Grauman, K.,
  Cesa-Bianchi, N., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Englert \& Lazic(2022)Englert and Lazic]{e22a}
Englert, M. and Lazic, R.
\newblock {Adversarial Reprogramming Revisited}.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  28588--28600. Curran Associates, Inc., 2022.

\bibitem[Frei et~al.(2023{\natexlab{a}})Frei, Chatterji, and Bartlett]{f23r}
Frei, S., Chatterji, N.~S., and Bartlett, P.~L.
\newblock {Random Feature Amplification: Feature Learning and Generalization in
  Neural Networks}.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (303):\penalty0 1--49, 2023{\natexlab{a}}.

\bibitem[Frei et~al.(2023{\natexlab{b}})Frei, Vardi, Bartlett, Srebro, and
  Hu]{f23i}
Frei, S., Vardi, G., Bartlett, P., Srebro, N., and Hu, W.
\newblock {Implicit Bias in Leaky ReLU Networks Trained on High-Dimensional
  Data}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023{\natexlab{b}}.

\bibitem[Ge et~al.(2021)Ge, Ren, Wang, and Zhou]{g21u}
Ge, R., Ren, Y., Wang, X., and Zhou, M.
\newblock {Understanding Deflation Process in Over-parametrized Tensor
  Decomposition}.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  1299--1311. Curran Associates, Inc., 2021.

\bibitem[Geirhos et~al.(2019)Geirhos, Rubisch, Michaelis, Bethge, Wichmann, and
  Brendel]{ge18i}
Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.~A., and
  Brendel, W.
\newblock {ImageNet-trained CNNs are biased towards texture; increasing shape
  bias improves accuracy and robustness.}
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{g20s}
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge,
  M., and Wichmann, F.~A.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2\penalty0 (11):\penalty0
  665--673, 2020.

\bibitem[Hartman(2002)]{h02o}
Hartman, P.
\newblock \emph{Ordinary Differential Equations}.
\newblock Society for Industrial and Applied Mathematics, second edition, 2002.
\newblock \doi{10.1137/1.9780898719222}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{h16d}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock {Deep Residual Learning for Image Recognition}.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, June 2016.

\bibitem[Hermann \& Lampinen(2020)Hermann and Lampinen]{h20w}
Hermann, K. and Lampinen, A.
\newblock What shapes feature representations? exploring datasets,
  architectures, and training.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  9995--10006. Curran Associates, Inc., 2020.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{h89m}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.
\newblock ISSN 0893-6080.
\newblock \doi{10.1016/0893-6080(89)90020-8}.

\bibitem[Jain et~al.(2023)Jain, Lawrence, Moitra, and Madry]{j23d}
Jain, S., Lawrence, H., Moitra, A., and Madry, A.
\newblock {Distilling Model Failures as Directions in Latent Space}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Ji \& Telgarsky(2020)Ji and Telgarsky]{j20d}
Ji, Z. and Telgarsky, M.
\newblock Directional convergence and alignment in deep learning.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  17176--17186. Curran Associates, Inc., 2020.

\bibitem[Kalimeris et~al.(2019)Kalimeris, Kaplun, Nakkiran, Edelman, Yang,
  Barak, and Zhang]{k19s}
Kalimeris, D., Kaplun, G., Nakkiran, P., Edelman, B., Yang, T., Barak, B., and
  Zhang, H.
\newblock {SGD on Neural Networks Learns Functions of Increasing Complexity}.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d~Alche-Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Kou et~al.(2023)Kou, Chen, and Gu]{k23i}
Kou, Y., Chen, Z., and Gu, Q.
\newblock {Implicit Bias of Gradient Descent for Two-layer ReLU and Leaky ReLU
  Networks on Nearly-orthogonal Data}.
\newblock In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and
  Levine, S. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~36, pp.\  30167--30221. Curran Associates, Inc., 2023.

\bibitem[Kumar \& Haupt(2024)Kumar and Haupt]{k24d}
Kumar, A. and Haupt, J.
\newblock {Directional Convergence Near Small Initializations and Saddles in
  Two-Homogeneous Neural Networks}.
\newblock \emph{Transactions on Machine Learning Research}, 2024.
\newblock ISSN 2835-8856.

\bibitem[Li et~al.(2020)Li, Ma, and Zhang]{l20l}
Li, Y., Ma, T., and Zhang, H.~R.
\newblock {Learning Over-Parametrized Two-Layer Neural Networks beyond NTK}.
\newblock In Abernethy, J. and Agarwal, S. (eds.), \emph{Proceedings of Thirty
  Third Conference on Learning Theory}, volume 125 of \emph{Proceedings of
  Machine Learning Research}, pp.\  2613--2682. PMLR, 09--12 Jul 2020.

\bibitem[Loi(2010)]{l10l}
Loi, T.~L.
\newblock {Lecture 1: O-minimal structures}.
\newblock In \emph{The Japanese-Australian Workshop on Real and Complex
  Singularities: JARCS III}, volume~43, pp.\  19--31. Australian National
  University, Mathematical Sciences Institute, 2010.

\bibitem[Luo et~al.(2021{\natexlab{a}})Luo, Ma, Xu, and Zhang]{l21t}
Luo, T., Ma, Z., Xu, Z.-Q.~J., and Zhang, Y.
\newblock {Theory of the Frequency Principle for General Deep Neural Networks}.
\newblock \emph{CSIAM Transactions on Applied Mathematics}, 2\penalty0
  (3):\penalty0 484--507, 2021{\natexlab{a}}.
\newblock ISSN 2708-0579.
\newblock \doi{10.4208/csiam-am.SO-2020-0005}.

\bibitem[Luo et~al.(2021{\natexlab{b}})Luo, Xu, Ma, and Zhang]{l21p}
Luo, T., Xu, Z.-Q.~J., Ma, Z., and Zhang, Y.
\newblock {Phase Diagram for Two-layer ReLU Neural Networks at Infinite-width
  Limit}.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (71):\penalty0 1--47, 2021{\natexlab{b}}.

\bibitem[Lyu \& Li(2020)Lyu and Li]{l20g}
Lyu, K. and Li, J.
\newblock {Gradient Descent Maximizes the Margin of Homogeneous Neural
  Networks}.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{l21g}
Lyu, K., Li, Z., Wang, R., and Arora, S.
\newblock {Gradient Descent on Two-layer Nets: Margin Maximization and
  Simplicity Bias}.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  12978--12991. Curran Associates, Inc., 2021.

\bibitem[Lyu et~al.(2024)Lyu, Jin, Li, Du, Lee, and Hu]{l24d}
Lyu, K., Jin, J., Li, Z., Du, S.~S., Lee, J.~D., and Hu, W.
\newblock Dichotomy of early and late phase implicit biases can provably induce
  grokking.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Maennel et~al.(2018)Maennel, Bousquet, and Gelly]{m18g}
Maennel, H., Bousquet, O., and Gelly, S.
\newblock {Gradient Descent Quantizes ReLU Network Features}, 2018.
\newblock arXiv:1803.08367 [stat.ML].

\bibitem[McCoy et~al.(2019)McCoy, Pavlick, and Linzen]{m19r}
McCoy, T., Pavlick, E., and Linzen, T.
\newblock {Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in
  Natural Language Inference}.
\newblock In Korhonen, A., Traum, D., and M{\`a}rquez, L. (eds.),
  \emph{Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pp.\  3428--3448, Florence, Italy, July 2019.
  Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1334}.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{m18a}
Mei, S., Montanari, A., and Nguyen, P.-M.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.
\newblock \doi{10.1073/pnas.1806579115}.

\bibitem[Min et~al.(2024)Min, Mallada, and Vidal]{m24e}
Min, H., Mallada, E., and Vidal, R.
\newblock {Early Neuron Alignment in Two-layer ReLU Networks with Small
  Initialization}.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Morwani et~al.(2023)Morwani, Batra, Jain, and Netrapalli]{m23s}
Morwani, D., Batra, J., Jain, P., and Netrapalli, P.
\newblock {Simplicity Bias in 1-Hidden Layer Neural Networks}.
\newblock In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and
  Levine, S. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~36, pp.\  8048--8075. Curran Associates, Inc., 2023.

\bibitem[Papyan et~al.(2020)Papyan, Han, and Donoho]{p20p}
Papyan, V., Han, X.~Y., and Donoho, D.~L.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (40):\penalty0 24652--24663, 2020.
\newblock \doi{10.1073/pnas.2015509117}.

\bibitem[Pellegrini \& Biroli(2020)Pellegrini and Biroli]{p20a}
Pellegrini, F. and Biroli, G.
\newblock An analytic theory of shallow networks dynamics for hinge loss
  classification.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  5356--5367. Curran Associates, Inc., 2020.

\bibitem[Perez et~al.(2018)Perez, Strub, de~Vries, Dumoulin, and
  Courville]{p18f}
Perez, E., Strub, F., de~Vries, H., Dumoulin, V., and Courville, A.
\newblock {FiLM: Visual Reasoning with a General Conditioning Layer}.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  32\penalty0 (1), Apr. 2018.
\newblock \doi{10.1609/aaai.v32i1.11671}.

\bibitem[Phuong \& Lampert(2021)Phuong and Lampert]{p21t}
Phuong, M. and Lampert, C.~H.
\newblock The inductive bias of {ReLU} networks on orthogonally separable data.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{r19o}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock {On the Spectral Bias of Neural Networks}.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  5301--5310. PMLR,
  09--15 Jun 2019.

\bibitem[Refinetti et~al.(2023)Refinetti, Ingrosso, and Goldt]{r23n}
Refinetti, M., Ingrosso, A., and Goldt, S.
\newblock Neural networks trained with {SGD} learn distributions of increasing
  complexity.
\newblock In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
  and Scarlett, J. (eds.), \emph{Proceedings of the 40th International
  Conference on Machine Learning}, volume 202 of \emph{Proceedings of Machine
  Learning Research}, pp.\  28843--28863. PMLR, 23--29 Jul 2023.

\bibitem[Safran et~al.(2022)Safran, Vardi, and Lee]{s22o}
Safran, I., Vardi, G., and Lee, J.~D.
\newblock {On the Effective Number of Linear Regions in Shallow Univariate ReLU
  Networks: Convergence Guarantees and Implicit Bias}.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  32667--32679. Curran Associates, Inc., 2022.

\bibitem[Sarussi et~al.(2021)Sarussi, Brutzkus, and Globerson]{s21t}
Sarussi, R., Brutzkus, A., and Globerson, A.
\newblock {Towards Understanding Learning in Neural Networks with Linear
  Teachers}.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pp.\  9313--9322. PMLR,
  18--24 Jul 2021.

\bibitem[Shah et~al.(2020)Shah, Tamuly, Raghunathan, Jain, and
  Netrapalli]{s20p}
Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P.
\newblock {The Pitfalls of Simplicity Bias in Neural Networks}.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
  (eds.), \emph{Advances in Neural Information Processing Systems}, volume~33,
  pp.\  9573--9585. Curran Associates, Inc., 2020.

\bibitem[Telgarsky(2023)]{t23f}
Telgarsky, M.
\newblock Feature selection and low test error in shallow low-rotation {ReLU}
  networks.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Valle-Perez et~al.(2019)Valle-Perez, Camargo, and Louis]{v18d}
Valle-Perez, G., Camargo, C.~Q., and Louis, A.~A.
\newblock Deep learning generalizes because the parameter-function map is
  biased towards simple functions.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Vardi et~al.(2022)Vardi, Shamir, and Srebro]{v22o}
Vardi, G., Shamir, O., and Srebro, N.
\newblock {On Margin Maximization in Linear and ReLU Networks}.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  37024--37036. Curran Associates, Inc., 2022.

\bibitem[Wang \& Ma(2023)Wang and Ma]{w23u}
Wang, M. and Ma, C.
\newblock {Understanding Multi-phase Optimization Dynamics and Rich Nonlinear
  Behaviors of ReLU Networks}.
\newblock In Oh, A., Neumann, T., Globerson, A., Saenko, K., Hardt, M., and
  Levine, S. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~36, pp.\  35654--35747. Curran Associates, Inc., 2023.

\bibitem[Wei et~al.(2019)Wei, Lee, Liu, and Ma]{w19r}
Wei, C., Lee, J.~D., Liu, Q., and Ma, T.
\newblock {Regularization Matters: Generalization and Optimization of Neural
  Nets v.s. their Induced Kernel}.
\newblock In Wallach, H., Larochelle, H., Beygelzimer, A., d~Alche-Buc, F.,
  Fox, E., and Garnett, R. (eds.), \emph{Advances in Neural Information
  Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Xu \& Du(2023)Xu and Du]{x23o}
Xu, W. and Du, S.
\newblock {Over-Parameterization Exponentially Slows Down Gradient Descent for
  Learning a Single Neuron}.
\newblock In Neu, G. and Rosasco, L. (eds.), \emph{Proceedings of Thirty Sixth
  Conference on Learning Theory}, volume 195 of \emph{Proceedings of Machine
  Learning Research}, pp.\  1155--1198. PMLR, 12--15 Jul 2023.

\bibitem[Xu et~al.(2019)Xu, Zhang, and Xiao]{x18t}
Xu, Z.-Q.~J., Zhang, Y., and Xiao, Y.
\newblock {Training Behavior of Deep Neural Network in Frequency Domain}.
\newblock In Gedeon, T., Wong, K.~W., and Lee, M. (eds.), \emph{Neural
  Information Processing}, pp.\  264--274, Cham, 2019. Springer International
  Publishing.
\newblock ISBN 978-3-030-36708-4.

\bibitem[Zhou et~al.(2022)Zhou, Qixuan, Luo, Zhang, and Xu]{z22t}
Zhou, H., Qixuan, Z., Luo, T., Zhang, Y., and Xu, Z.-Q.
\newblock {Towards Understanding the Condensation of Neural Networks at Initial
  Training}.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  2184--2196. Curran Associates, Inc., 2022.

\bibitem[Zhu et~al.(2021)Zhu, Ding, Zhou, Li, You, Sulam, and Qu]{z21n}
Zhu, Z., Ding, T., Zhou, J., Li, X., You, C., Sulam, J., and Qu, Q.
\newblock {A Geometric Analysis of Neural Collapse with Unconstrained
  Features}.
\newblock In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
  J.~W. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  29820--29834. Curran Associates, Inc., 2021.

\end{thebibliography}
