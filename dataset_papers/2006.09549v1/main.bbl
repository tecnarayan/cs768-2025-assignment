\begin{thebibliography}{10}

\bibitem{akrout2019deep}
Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, and
  Douglas~B Tweed.
\newblock Deep learning without weight transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  974--982, 2019.

\bibitem{ba2016fast}
J.~Ba, G.~E. Hinton, V.~Mnih, J.~Z. Leibo, and C.~Ionescu.
\newblock Using fast weights to attend to the recent past.
\newblock In D.~D. Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 29}, pages
  4331--4339. 2016.

\bibitem{bartunov2018assessing}
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey~E Hinton,
  and Timothy Lillicrap.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9368--9378, 2018.

\bibitem{bengio1991learning}
Y.~Bengio, S.~Bengio, and J.~Cloutier.
\newblock Learning a synaptic learning rule.
\newblock {\em In}, 1991, 1991.

\bibitem{bengio2014auto}
Yoshua Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock {\em arXiv preprint arXiv:1407.7906}, 2014.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em {\em International Conference on Machine Learning}}, 2017.

\bibitem{finn2017meta}
Chelsea Finn and Sergey Levine.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock {\em arXiv preprint arXiv:1710.11622}, 2017.

\bibitem{guerguiev2019spike}
Jordan Guerguiev, Konrad~P Kording, and Blake~A Richards.
\newblock Spike-based causal inference for weight alignment.
\newblock {\em arXiv preprint arXiv:1910.01689}, 2019.

\bibitem{guerguiev2017towards}
Jordan Guerguiev, Timothy~P Lillicrap, and Blake~A Richards.
\newblock Towards deep learning with segregated dendrites.
\newblock {\em ELife}, 6:e22901, 2017.

\bibitem{hige2015heterosynaptic}
Toshihide Hige, Yoshinori Aso, Mehrab~N Modi, Gerald~M Rubin, and Glenn~C
  Turner.
\newblock Heterosynaptic plasticity underlies aversive olfactory learning in
  drosophila.
\newblock {\em Neuron}, 88(5):985--998, 2015.

\bibitem{javed2019meta}
Khurram Javed and Martha White.
\newblock Meta-learning representations for continual learning.
\newblock {\em Advances in Neural Information Processing Systems}, 2019.

\bibitem{jiang2019models}
Linnie Jiang and Ashok Litwin-Kumar.
\newblock Models of heterogeneous dopamine signaling in an insect learning and
  memory center.
\newblock {\em bioRxiv}, page 737064, 2019.

\bibitem{kell2018task}
Alexander~JE Kell, Daniel~LK Yamins, Erica~N Shook, Sam~V Norman-Haignere, and
  Josh~H McDermott.
\newblock A task-optimized neural network replicates human auditory behavior,
  predicts brain responses, and reveals a cortical processing hierarchy.
\newblock {\em Neuron}, 98(3):630--644, 2018.

\bibitem{kingma2014adam}
Diederik~P. Kingma and A~Jimmy Ba.~Adam:.
\newblock {\em method for stochastic optimization}.
\newblock 2014.

\bibitem{kunin2020two}
Daniel Kunin, Aran Nayebi, Javier Sagastuy-Brena, Surya Ganguli, Jon Bloom, and
  Daniel~LK Yamins.
\newblock Two routes to scalable credit assignment without weight symmetry.
\newblock {\em arXiv preprint arXiv:2003.01513}, 2020.

\bibitem{lake2015human}
Brenden~M Lake, Ruslan Salakhutdinov, and Joshua~B Tenenbaum.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338, 2015.

\bibitem{lake2017building}
Brenden~M Lake, Tomer~D Ullman, Joshua~B Tenenbaum, and Samuel~J Gershman.
\newblock Building machines that learn and think like people.
\newblock {\em Behavioral and brain sciences}, 40, 2017.

\bibitem{lansdell2019learning}
Benjamin~James Lansdell, Prashanth Prakash, and Konrad~Paul Kording.
\newblock Learning to solve the credit assignment problem.
\newblock {\em arXiv preprint arXiv:1906.00889}, 2019.

\bibitem{lillicrap2016random}
Timothy~P Lillicrap, Daniel Cownden, Douglas~B Tweed, and Colin~J Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock {\em Nature communications}, 7:13276, 2016.

\bibitem{lindsey2019unified}
Jack Lindsey, Samuel~A Ocko, Surya Ganguli, and Stephane Deny.
\newblock A unified theory of early visual representations from retina to
  cortex through anatomically constrained deep {CNNs}.
\newblock {\em arXiv preprint arXiv:1901.00945}, 2019.

\bibitem{metz_meta-learning_2019}
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein.
\newblock Meta-learning update rules for unsupervised representation learning.
\newblock {\em arXiv:1804.00222 [cs, stat]}, February 2019.

\bibitem{michaels2019neural}
Jonathan~A. Michaels, Stefan Schaffelhofer, Andres Agudelo-Toro, and
  Hansj{\"o}rg Scherberger.
\newblock A neural network model of flexible grasp movement generation.
\newblock {\em bioRxiv}, 2019.

\bibitem{miconi2018differentiable}
Thomas Miconi, Jeff Clune, and Kenneth~O Stanley.
\newblock Differentiable plasticity: training plastic neural networks with
  backpropagation.
\newblock {\em arXiv preprint arXiv:1804.02464}, 2018.

\bibitem{miconi2018backpropamine}
Thomas Miconi, Aditya Rawal, Jeff Clune, and Kenneth~O Stanley.
\newblock Backpropamine: training self-modifying neural networks with
  differentiable neuromodulated plasticity.
\newblock 2018.

\bibitem{nokland2016direct}
Arild N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1037--1045, 2016.

\bibitem{oja1982simplified}
Erkki Oja.
\newblock Simplified neuron model as a principal component analyzer.
\newblock {\em Journal of mathematical biology}, 15(3):267--273, 1982.

\bibitem{parisi2019continual}
German~I Parisi, Ronald Kemker, Jose~L Part, Christopher Kanan, and Stefan
  Wermter.
\newblock Continual lifelong learning with neural networks: A review.
\newblock {\em Neural Networks}, 2019.

\bibitem{raghu2019rapid}
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock {\em arXiv preprint arXiv:1909.09157}, 2019.

\bibitem{sandbrink2020taskdriven}
Kai~J. Sandbrink, Pranav Mamidanna, Claudio Michaelis, Mackenzie~Weygandt
  Mathis, Matthias Bethge, and Alexander Mathis.
\newblock Task-driven hierarchical deep neural network models of the
  proprioceptive pathway.
\newblock {\em bioRxiv}, 2020.

\bibitem{vinyals2016matching}
O.~Vinyals, C.~Blundell, T.~Lillicrap, D.~Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock {\em In}, pages 3630--3638, 2016.

\bibitem{wang2016learning}
J.~X. Wang, Z.~Kurth{-}Nelson, D.~Tirumala, H.~Soyer, J.~Z. Leibo, R.~Munos,
  C.~Blundell, D.~Kumaran, and M.~Botvinick.
\newblock Learning to reinforcement learn, 2016.

\bibitem{wang2018prefrontal}
Jane~X Wang, Zeb Kurth-Nelson, Dharshan Kumaran, Dhruva Tirumala, Hubert Soyer,
  Joel~Z Leibo, Demis Hassabis, and Matthew Botvinick.
\newblock Prefrontal cortex as a meta-reinforcement learning system.
\newblock {\em Nature neuroscience}, 21(6):860--868, 2018.

\bibitem{yamins2014performance}
Daniel~LK Yamins, Ha~Hong, Charles~F Cadieu, Ethan~A Solomon, Darren Seibert,
  and James~J DiCarlo.
\newblock Performance-optimized hierarchical models predict neural responses in
  higher visual cortex.
\newblock {\em Proceedings of the National Academy of Sciences},
  111(23):8619--8624, 2014.

\bibitem{zador2019critique}
Anthony~M. Zador.
\newblock A critique of pure learning and what artificial neural networks can
  learn from animal brains.
\newblock {\em Nature Communications}, 10(1):3770, August 2019.

\end{thebibliography}
