\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Schuurmans, and
  Norouzi]{agarwal19striving}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock Striving for simplicity in off-policy deep reinforcement learning.
\newblock \emph{CoRR}, abs/1907.04543, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.04543}.

\bibitem[{Antos} et~al.(2007){Antos}, {Szepesvari}, and {Munos}]{antos07value}
Andr\"as {Antos}, Csaa {Szepesvari}, and Remi {Munos}.
\newblock Value-iteration based fitted policy iteration: Learning with a single
  trajectory.
\newblock In \emph{2007 IEEE International Symposium on Approximate Dynamic
  Programming and Reinforcement Learning}, pages 330--337, April 2007.
\newblock \doi{10.1109/ADPRL.2007.368207}.

\bibitem[Antos et~al.(2008)Antos, Szepesv\'{a}ri, and Munos]{anots08fitted}
Andr\'{a}s Antos, Csaba Szepesv\'{a}ri, and R\'{e}mi Munos.
\newblock Fitted q-iteration in continuous action-space mdps.
\newblock In \emph{Advances in Neural Information Processing Systems 20}, pages
  9--16. Curran Associates, Inc., 2008.

\bibitem[Bennett et~al.(2007)Bennett, Lanning, et~al.]{bennett2007netflix}
James Bennett, Stan Lanning, et~al.
\newblock The netflix prize.
\newblock 2007.

\bibitem[Bertsekas and Tsitsiklis(1996)]{bertsekas1996ndp}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock \emph{Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[Byrd and Lipton()]{byrd19is}
Jonathon Byrd and Zachary Lipton.
\newblock What is the effect of importance weighting in deep learning?
\newblock In \emph{ICML 2019}.

\bibitem[de~Bruin et~al.(2015)de~Bruin, Kober, Tuyls, and
  Babuska]{deBruin2015importance}
Tim de~Bruin, Jens Kober, Karl Tuyls, and Robert Babuska.
\newblock The importance of experience replay database composition in deep
  reinforcement learning.
\newblock 01 2015.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet_cvpr09}
Jia Deng, Wei Dong, Richard~S. Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet: A Large-Scale Hierarchical Image Database}.
\newblock In \emph{CVPR09}, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, et~al.]{impala2018}
Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Volodymir Mnih, Tom
  Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et~al.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning (ICML)}, 2018.

\bibitem[Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri, and
  Munos]{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  568--576, 2010.

\bibitem[Fu et~al.(2019)Fu, Kumar, Soh, and Levine]{fu2019diagnosing}
Justin Fu, Aviral Kumar, Matthew Soh, and Sergey Levine.
\newblock Diagnosing bottlenecks in deep q-learning algorithms.
\newblock \emph{arXiv preprint arXiv:1902.10250}, 2019.

\bibitem[Fujimoto et~al.(2018{\natexlab{a}})Fujimoto, Meger, and
  Precup]{fujimoto2018off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock \emph{arXiv preprint arXiv:1812.02900}, 2018{\natexlab{a}}.

\bibitem[Fujimoto et~al.(2018{\natexlab{b}})Fujimoto, van Hoof, and
  Meger]{fujimoto18addressing}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In Jennifer Dy and Andreas Krause, editors, \emph{Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of
  \emph{Proceedings of Machine Learning Research}, pages 1587--1596. PMLR,
  2018{\natexlab{b}}.

\bibitem[Gao et~al.(2018)Gao, Xu, Lin, Yu, Levine, and
  Darrell]{DBLP:conf/iclr/GaoXLYLD18}
Yang Gao, Huazhe Xu, Ji~Lin, Fisher Yu, Sergey Levine, and Trevor Darrell.
\newblock Reinforcement learning from imperfect demonstrations.
\newblock In \emph{{ICLR} (Workshop)}. OpenReview.net, 2018.

\bibitem[Gelada and Bellemare(2019)]{gelada2019off}
Carles Gelada and Marc~G. Bellemare.
\newblock Off-policy deep reinforcement learning by bootstrapping the covariate
  shift.
\newblock \emph{CoRR}, abs/1901.09455, 2019.

\bibitem[Grau-Moya et~al.(2019)Grau-Moya, Leibfried, and
  Vrancx]{grau-moya2018soft}
Jordi Grau-Moya, Felix Leibfried, and Peter Vrancx.
\newblock Soft q-learning with mutual-information regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=HyEtjoCqFX}.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch\"{o}lkopf, and
  Smola]{gretton2012kernel}
Arthur Gretton, Karsten~M. Borgwardt, Malte~J. Rasch, Bernhard Sch\"{o}lkopf,
  and Alexander Smola.
\newblock A kernel two-sample test.
\newblock \emph{J. Mach. Learn. Res.}, 13:\penalty0 723--773, March 2012.
\newblock ISSN 1532-4435.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=2188385.2188410}.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{arXiv preprint arXiv:1801.01290}, 2018.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{2016 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 770--778, 2016.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot,
  Horgan, Quan, Sendonaris, Osband, et~al.]{hester2018dqfd}
Todd Hester, Matej Vecerik, Olivier Pietquin, Marc Lanctot, Tom Schaul, Bilal
  Piot, Dan Horgan, John Quan, Andrew Sendonaris, Ian Osband, et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jacques19way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson,
  {\`{A}}gata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind~W. Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{CoRR}, abs/1907.00456, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.00456}.

\bibitem[Kakade and Langford(2002)]{kakade2002cpi}
Sham Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{Proceedings of the Nineteenth International Conference on
  Machine Learning}, pages 267--274. Morgan Kaufmann Publishers Inc., 2002.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke, and
  Levine]{kalashnikov18qtopt}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog,
  Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
  Vanhoucke, and Sergey Levine.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Proceedings of The 2nd Conference on Robot Learning},
  volume~87 of \emph{Proceedings of Machine Learning Research}, pages 651--673.
  PMLR, 2018.

\bibitem[Laroche et~al.(2019)Laroche, Trichelair, and
  Des~Combes]{laroche2019spibb}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Levine(2018)]{levine2018rlasinference}
Sergey Levine.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock \emph{CoRR}, abs/1805.00909, 2018.
\newblock URL \url{http://arxiv.org/abs/1805.00909}.

\bibitem[Mahmood et~al.(2015)Mahmood, Yu, White, and
  Sutton]{mahmood2015emphatic}
A~Rupam Mahmood, Huizhen Yu, Martha White, and Richard~S Sutton.
\newblock Emphatic temporal-difference learning.
\newblock \emph{arXiv preprint arXiv:1507.01569}, 2015.

\bibitem[Munos(2003)]{munos2003errorapi}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, pages 560--567. AAAI Press,
  2003.

\bibitem[Munos(2005)]{munos2005erroravi}
R{\'e}mi Munos.
\newblock Error bounds for approximate value iteration.
\newblock In \emph{Proceedings of the National Conference on Artificial
  Intelligence}, 2005.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
R{\'e}mi Munos, Tom Stepleton, Anna Harutyunyan, and Marc Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1054--1062, 2016.

\bibitem[Precup et~al.(2001)Precup, Sutton, and Dasgupta]{precup2001offpol}
Doina Precup, Richard~S. Sutton, and Sanjoy Dasgupta.
\newblock {Off-policy temporal-difference learning with function
  approximation}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2001.

\bibitem[Schaal(1999)]{Schaal99isimitation}
Stefan Schaal.
\newblock Is imitation learning the route to humanoid robots?, 1999.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{Schaul2016PrioritizedER}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock \emph{CoRR}, abs/1511.05952, 2016.

\bibitem[Scherrer et~al.(2015)Scherrer, Ghavamzadeh, Gabillon, Lesner, and
  Geist]{bruno2015approximate}
Bruno Scherrer, Mohammad Ghavamzadeh, Victor Gabillon, Boris Lesner, and
  Matthieu Geist.
\newblock Approximate modified policy iteration and its application to the game
  of tetris.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 1629--1676,
  2015.
\newblock URL \url{http://jmlr.org/papers/v16/scherrer15a.html}.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trpo}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 1889--1897, Lille,
  France, 07--09 Jul 2015. PMLR.

\bibitem[Sutton and Barto(2018)]{suttonrlbook}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock Second edition, 2018.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In \emph{IROS}, pages 5026--5033, 2012.

\bibitem[Wu et~al.()Wu, Winston, Kaushik, and Lipton]{wu19domain}
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton.
\newblock Domain adaptation with asymmetrically-relaxed distribution alignment.
\newblock In \emph{ICML 2019}.

\bibitem[Yu et~al.(2018)Yu, Xian, Chen, Liu, Liao, Madhavan, and
  Darrell]{yu2018bdd}
Fisher Yu, Wenqi Xian, Yingying Chen, Fangchen Liu, Mike Liao, Vashisht
  Madhavan, and Trevor Darrell.
\newblock {BDD100K:} {A} diverse driving video database with scalable
  annotation tooling.
\newblock \emph{CoRR}, abs/1805.04687, 2018.
\newblock URL \url{http://arxiv.org/abs/1805.04687}.

\end{thebibliography}
