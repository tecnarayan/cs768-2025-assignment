\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Dean et~al.(2012)Dean, Corrado, Monga, Chen, Devin, Mao, aurelio
  Ranzato, Senior, Tucker, Yang, Le, and Ng]{distbelief}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Marc\textquotesingle aurelio Ranzato, Andrew Senior, Paul Tucker, Ke~Yang,
  Quoc~V. Le, and Andrew~Y. Ng.
\newblock Large scale distributed deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1223--1231. 2012.

\bibitem[Abadi et~al.(2016)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, et~al.]{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock \emph{arXiv preprint:1603.04467}, 2016.

\bibitem[Coates et~al.(2013)Coates, Huval, Wang, Wu, Catanzaro, and
  Andrew]{coates2013deep}
Adam Coates, Brody Huval, Tao Wang, David Wu, Bryan Catanzaro, and Ng~Andrew.
\newblock Deep learning with cots hpc systems.
\newblock In \emph{International Conference on Machine Learning}, pages
  1337--1345, 2013.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  693--701, 2011.

\bibitem[Chilimbi et~al.(2014)Chilimbi, Suzue, Apacible, and
  Kalyanaraman]{chilimbi2014project}
Trishul~M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In \emph{OSDI}, volume~14, pages 571--582, 2014.

\bibitem[Xing et~al.(2015)Xing, Ho, Dai, Kim, Wei, Lee, Zheng, Xie, Kumar, and
  Yu]{xing2015petuum}
Eric~P Xing, Qirong Ho, Wei Dai, Jin~Kyu Kim, Jinliang Wei, Seunghak Lee, Xun
  Zheng, Pengtao Xie, Abhimanu Kumar, and Yaoliang Yu.
\newblock Petuum: A new platform for distributed machine learning on big data.
\newblock \emph{IEEE Transactions on Big Data}, 1\penalty0 (2):\penalty0
  49--67, 2015.

\bibitem[Moritz et~al.(2015)Moritz, Nishihara, Stoica, and
  Jordan]{moritz2015sparknet}
Philipp Moritz, Robert Nishihara, Ion Stoica, and Michael~I Jordan.
\newblock Sparknet: Training deep networks in spark.
\newblock \emph{arXiv preprint:1511.06051}, 2015.

\bibitem[Chen et~al.(2015)Chen, Li, Li, Lin, Wang, Wang, Xiao, Xu, Zhang, and
  Zhang]{chen2015mxnet}
Tianqi Chen, Mu~Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
  Bing Xu, Chiyuan Zhang, and Zheng Zhang.
\newblock Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock \emph{arXiv preprint:1512.01274}, 2015.

\bibitem[Zhang et~al.(2015)Zhang, Choromanska, and LeCun]{zhang2015deep}
Sixin Zhang, Anna~E Choromanska, and Yann LeCun.
\newblock Deep learning with elastic averaging sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  685--693, 2015.

\bibitem[Li(2017)]{li2017scaling}
Mu~Li.
\newblock \emph{Scaling Distributed Machine Learning with System and Algorithm
  Co-design}.
\newblock PhD thesis, Carnegie Mellon University, 2017.

\bibitem[Li et~al.(2014{\natexlab{a}})Li, Andersen, Park, Smola, Ahmed,
  Josifovski, Long, Shekita, and Su]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{OSDI}, volume~14, pages 583--598, 2014{\natexlab{a}}.

\bibitem[Li et~al.(2014{\natexlab{b}})Li, Andersen, Smola, and
  Yu]{li2014communication}
Mu~Li, David~G Andersen, Alexander~J Smola, and Kai Yu.
\newblock Communication efficient distributed machine learning with the
  parameter server.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  19--27, 2014{\natexlab{b}}.

\bibitem[Ho et~al.(2013)Ho, Cipar, Cui, Lee, Kim, Gibbons, Gibson, Ganger, and
  Xing]{ho2013more}
Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin~Kyu Kim, Phillip~B
  Gibbons, Garth~A Gibson, Greg Ganger, and Eric~P Xing.
\newblock More effective distributed ml via a stale synchronous parallel
  parameter server.
\newblock In \emph{Advances in neural information processing systems}, pages
  1223--1231, 2013.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  2595--2603, 2010.

\bibitem[Pan et~al.(2017)Pan, Chen, Monga, Bengio, and
  Jozefowicz]{pan2017revisiting}
Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
\newblock Revisiting distributed synchronous sgd.
\newblock \emph{arXiv preprint:1702.05800}, 2017.

\bibitem[Zhang et~al.(2016)Zhang, Gupta, Lian, and Liu]{Zhang2016Staleness}
Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji~Liu.
\newblock Staleness-aware async-sgd for distributed deep learning.
\newblock In \emph{Proceedings of the Twenty-Fifth International Joint
  Conference on Artificial Intelligence}, IJCAI'16, pages 2350--2356. AAAI
  Press, 2016.
\newblock ISBN 978-1-57735-770-4.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=3060832.3060950}.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Wen et~al.(2016)Wen, Wu, Wang, Chen, and Li]{wen2016learning}
Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Learning structured sparsity in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2074--2082, 2016.

\bibitem[Park et~al.(2017)Park, Li, Wen, Tang, Li, Chen, and
  Dubey]{park2017faster}
J~Park, S~Li, W~Wen, PTP Tang, H~Li, Y~Chen, and P~Dubey.
\newblock Faster cnns with direct sparse convolutions and guided pruning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{hubara2016binarized}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4107--4115, 2016.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European Conference on Computer Vision}, pages 525--542.
  Springer, 2016.

\bibitem[Zhou et~al.(2016)Zhou, Wu, Ni, Zhou, Wen, and Zou]{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock \emph{arXiv preprint arXiv:1606.06160}, 2016.

\bibitem[Wen et~al.(2017)Wen, He, Rajbhandari, Wang, Liu, Hu, Chen, and
  Li]{wen2017iclrlearning}
Wei Wen, Yuxiong He, Samyam Rajbhandari, Wenhan Wang, Fang Liu, Bin Hu, Yiran
  Chen, and Hai Li.
\newblock Learning intrinsic sparse structures within long short-term memory.
\newblock \emph{arXiv:1709.05027}, 2017.

\bibitem[Ott et~al.(2016)Ott, Lin, Zhang, Liu, and Bengio]{ott2016recurrent}
Joachim Ott, Zhouhan Lin, Ying Zhang, Shih-Chii Liu, and Yoshua Bengio.
\newblock Recurrent neural networks with limited numerical precision.
\newblock \emph{arXiv:1608.06902}, 2016.

\bibitem[Lin et~al.(2015)Lin, Courbariaux, Memisevic, and
  Bengio]{lin2015neural}
Zhouhan Lin, Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio.
\newblock Neural networks with few multiplications.
\newblock \emph{arXiv:1510.03009}, 2015.

\bibitem[Bradley et~al.(2011)Bradley, Kyrola, Bickson, and
  Guestrin]{bradley2011parallel}
Joseph~K Bradley, Aapo Kyrola, Danny Bickson, and Carlos Guestrin.
\newblock Parallel coordinate descent for l1-regularized loss minimization.
\newblock \emph{arXiv preprint arXiv:1105.5379}, 2011.

\bibitem[Aji and Heafield(2017)]{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock \emph{arXiv preprint:1704.05021}, 2017.

\bibitem[Seide et~al.(2014)Seide, Fu, Droppo, Li, and Yu]{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In \emph{Interspeech}, pages 1058--1062, 2014.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{tomiokaqsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1707--1718, 2017.

\bibitem[Gupta et~al.(2015)Gupta, Agrawal, Gopalakrishnan, and
  Narayanan]{gupta2015deep}
Suyog Gupta, Ankur Agrawal, Kailash Gopalakrishnan, and Pritish Narayanan.
\newblock Deep learning with limited numerical precision.
\newblock In \emph{ICML}, pages 1737--1746, 2015.

\bibitem[Garg and Khandekar(2009)]{garg2009gradient}
Rahul Garg and Rohit Khandekar.
\newblock Gradient descent with sparsification: an iterative algorithm for
  sparse recovery with restricted isometry property.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 337--344. ACM, 2009.

\bibitem[Suresh et~al.(2016)Suresh, Yu, McMahan, and
  Kumar]{suresh2016distributed}
Ananda~Theertha Suresh, Felix~X Yu, H~Brendan McMahan, and Sanjiv Kumar.
\newblock Distributed mean estimation with limited communication.
\newblock \emph{arXiv:1611.00429}, 2016.

\bibitem[Bottou(1998)]{bottou1998online}
L{\'e}on Bottou.
\newblock Online learning and stochastic approximations.
\newblock \emph{On-line learning in neural networks}, 17\penalty0 (9):\penalty0
  142, 1998.

\bibitem[Neelakantan et~al.(2015)Neelakantan, Vilnis, Le, Sutskever, Kaiser,
  Kurach, and Martens]{neelakantan2015adding}
Arvind Neelakantan, Luke Vilnis, Quoc~V Le, Ilya Sutskever, Lukasz Kaiser,
  Karol Kurach, and James Martens.
\newblock Adding gradient noise improves learning for very deep networks.
\newblock \emph{arXiv preprint:1511.06807}, 2015.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Alex_NIPS2012_4824}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1097--1105. 2012.

\bibitem[Qian(1999)]{qian1999momentum}
Ning Qian.
\newblock On the momentum term in gradient descent learning algorithms.
\newblock \emph{Neural networks}, 12\penalty0 (1):\penalty0 145--151, 1999.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint:1412.6980}, 2014.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{Kaiming_ResNet_ICCV}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[Ioffe and Szegedy(2015)]{batchnorm_2015}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint:1502.03167}, 2015.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2818--2826, 2016.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{GoogleNet_2015}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock \emph{arXiv preprint:1409.4842}, 2015.

\bibitem[Simonyan and Zisserman(2014)]{Vggnet_2014}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint:1409.1556}, 2014.

\bibitem[Yan et~al.(2015)Yan, Ruwase, He, and Chilimbi]{fengKDD15}
Feng Yan, Olatunji Ruwase, Yuxiong He, and Trishul~M. Chilimbi.
\newblock Performance modeling and scalability optimization of distributed deep
  learning systems.
\newblock In \emph{Proceedings of the 21th {ACM} {SIGKDD} International
  Conference on Knowledge Discovery and Data Mining, Sydney, NSW, Australia,
  August 10-13, 2015}, pages 1355--1364, 2015.
\newblock \doi{10.1145/2783258.2783270}.
\newblock URL \url{http://doi.acm.org/10.1145/2783258.2783270}.

\bibitem[Snavely et~al.(2002)Snavely, Carrington, Wolter, Labarta, Badia, and
  Purkayastha]{snavely2002framework}
Allan Snavely, Laura Carrington, Nicole Wolter, Jesus Labarta, Rosa Badia, and
  Avi Purkayastha.
\newblock A framework for performance modeling and prediction.
\newblock In \emph{Supercomputing, ACM/IEEE 2002 Conference}, pages 21--21.
  IEEE, 2002.

\end{thebibliography}
