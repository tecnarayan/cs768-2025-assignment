\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aslan et~al.(2013)Aslan, Cheng, Zhang, and
  Schuurmans]{aslan2013convex}
{\"O}.~Aslan, H.~Cheng, X.~Zhang, and D.~Schuurmans.
\newblock Convex two-layer modeling.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2985--2993, 2013.

\bibitem[Aslan et~al.(2014)Aslan, Zhang, and Schuurmans]{aslan2014convex}
{\"O}.~Aslan, X.~Zhang, and D.~Schuurmans.
\newblock Convex deep learning via normalized kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3275--3283, 2014.

\bibitem[Bach(2014)]{bach2014breaking}
F.~Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{arXiv preprint arXiv:1412.8690}, 2014.

\bibitem[Bartlett and Mendelson(2003)]{bartlett2003rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and {G}aussian complexities: Risk bounds and structural
  results.
\newblock \emph{The Journal of Machine Learning Research}, 3:\penalty0
  463--482, 2003.

\bibitem[Bengio et~al.(2005)Bengio, Roux, Vincent, Delalleau, and
  Marcotte]{bengio2005convex}
Y.~Bengio, N.~L. Roux, P.~Vincent, O.~Delalleau, and P.~Marcotte.
\newblock Convex neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  123--130, 2005.

\bibitem[Blum and Rivest(1992)]{blum1992training}
A.~L. Blum and R.~L. Rivest.
\newblock Training a 3-node neural network is {NP}-complete.
\newblock \emph{Neural Networks}, 5\penalty0 (1):\penalty0 117--127, 1992.

\bibitem[Bottou(1998)]{bottou1998online}
L.~Bottou.
\newblock Online learning and stochastic approximations.
\newblock \emph{On-line learning in neural networks}, 17\penalty0 (9):\penalty0
  142, 1998.

\bibitem[Bruna and Mallat(2013)]{bruna2013invariant}
J.~Bruna and S.~Mallat.
\newblock Invariant scattering convolution networks.
\newblock \emph{Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, 35\penalty0 (8):\penalty0 1872--1886, 2013.

\bibitem[Chan et~al.(2015)Chan, Jia, Gao, Lu, Zeng, and Ma]{chan2015pcanet}
T.-H. Chan, K.~Jia, S.~Gao, J.~Lu, Z.~Zeng, and Y.~Ma.
\newblock Pcanet: A simple deep learning baseline for image classification?
\newblock \emph{IEEE Transactions on Image Processing}, 24\penalty0
  (12):\penalty0 5017--5032, 2015.

\bibitem[Chen and Manning(2014)]{chen2014fast}
D.~Chen and C.~D. Manning.
\newblock A fast and accurate dependency parser using neural networks.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, volume~1, pages 740--750, 2014.

\bibitem[Choromanska et~al.(2014)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2014loss}
A.~Choromanska, M.~Henaff, M.~Mathieu, G.~B. Arous, and Y.~LeCun.
\newblock The loss surface of multilayer networks.
\newblock \emph{ArXiv:1412.0233}, 2014.

\bibitem[Coates et~al.(2010)Coates, Lee, and Ng]{coates2010analysis}
A.~Coates, H.~Lee, and A.~Y. Ng.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock \emph{Ann Arbor}, 1001\penalty0 (48109):\penalty0 2, 2010.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
A.~Daniely, R.~Frostig, and Y.~Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock \emph{arXiv preprint arXiv:1602.05897}, 2016.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphin2014identifying}
Y.~N. Dauphin, R.~Pascanu, C.~Gulcehre, K.~Cho, S.~Ganguli, and Y.~Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2933--2941, 2014.

\bibitem[Drineas and Mahoney(2005)]{drineas2005nystrom}
P.~Drineas and M.~W. Mahoney.
\newblock On the {N}ystr{\"o}m method for approximating a {G}ram matrix for
  improved kernel-based learning.
\newblock \emph{The Journal of Machine Learning Research}, 6:\penalty0
  2153--2175, 2005.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and
  Chandra]{duchi2008efficient}
J.~Duchi, S.~Shalev-Shwartz, Y.~Singer, and T.~Chandra.
\newblock Efficient projections onto the $\ell_1$-ball for learning in high
  dimensions.
\newblock In \emph{Proceedings of the 25th International Conference on Machine
  Learning}, pages 272--279. ACM, 2008.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{The Journal of Machine Learning Research}, 12:\penalty0
  2121--2159, 2011.

\bibitem[Eldar and Kutyniok(2012)]{eldar2012compressed}
Y.~C. Eldar and G.~Kutyniok.
\newblock \emph{Compressed sensing: theory and applications}.
\newblock Cambridge University Press, 2012.

\bibitem[Fahlman(1988)]{fahlman1988empirical}
S.~E. Fahlman.
\newblock An empirical study of learning speed in back-propagation networks.
\newblock \emph{Journal of Heuristics}, 1988.

\bibitem[Haeffele and Vidal(2015)]{haeffele2015global}
B.~D. Haeffele and R.~Vidal.
\newblock Global optimality in tensor factorization, deep learning, and beyond.
\newblock \emph{arXiv preprint arXiv:1506.07540}, 2015.

\bibitem[Hinton et~al.(2012)Hinton, Deng, Yu, Dahl, Mohamed, Jaitly, Senior,
  Vanhoucke, Nguyen, Sainath, et~al.]{hinton2012deep}
G.~Hinton, L.~Deng, D.~Yu, G.~E. Dahl, A.-r. Mohamed, N.~Jaitly, A.~Senior,
  V.~Vanhoucke, P.~Nguyen, T.~N. Sainath, et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock \emph{Signal Processing Magazine, IEEE}, 29\penalty0 (6):\penalty0
  82--97, 2012.

\bibitem[Isa et~al.(2010)Isa, Saad, Omar, Osman, Ahmad, and
  Sakim]{isa2010suitable}
I.~Isa, Z.~Saad, S.~Omar, M.~Osman, K.~Ahmad, and H.~M. Sakim.
\newblock Suitable mlp network activation functions for breast cancer and
  thyroid disease detection.
\newblock In \emph{2010 Second International Conference on Computational
  Intelligence, Modelling and Simulation}, pages 39--44, 2010.

\bibitem[Janzamin et~al.(2015)Janzamin, Sedghi, and
  Anandkumar]{janzamin2015generalization}
M.~Janzamin, H.~Sedghi, and A.~Anandkumar.
\newblock Generalization bounds for neural networks through tensor
  factorization.
\newblock \emph{ArXiv:1506.08473}, 2015.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master Thesis}, 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1097--1105, 2012.

\bibitem[Lawrence et~al.(1997)Lawrence, Giles, Tsoi, and
  Back]{lawrence1997face}
S.~Lawrence, C.~L. Giles, A.~C. Tsoi, and A.~D. Back.
\newblock Face recognition: A convolutional neural-network approach.
\newblock \emph{Neural Networks, IEEE Transactions on}, 8\penalty0
  (1):\penalty0 98--113, 1997.

\bibitem[Le et~al.(2013)Le, Sarl{\'o}s, and Smola]{le2013fastfood}
Q.~Le, T.~Sarl{\'o}s, and A.~Smola.
\newblock Fastfood-approximating kernel expansions in loglinear time.
\newblock In \emph{Proceedings of the International Conference on Machine
  Learning}, 2013.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
R.~Livni, S.~Shalev-Shwartz, and O.~Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem[Mairal et~al.(2014)Mairal, Koniusz, Harchaoui, and
  Schmid]{mairal2014convolutional}
J.~Mairal, P.~Koniusz, Z.~Harchaoui, and C.~Schmid.
\newblock Convolutional kernel networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2627--2635, 2014.

\bibitem[Minsker(2011)]{minsker2011some}
S.~Minsker.
\newblock On some extensions of {B}ernstein's inequality for self-adjoint
  operators.
\newblock \emph{arXiv preprint arXiv:1112.5448}, 2011.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Rahimi and Recht(2007)]{rahimi2007random}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1177--1184, 2007.

\bibitem[Safran and Shamir(2015)]{safran2015quality}
I.~Safran and O.~Shamir.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock \emph{arXiv preprint arXiv:1511.04210}, 2015.

\bibitem[Sedghi and Anandkumar(2014)]{sedghi2014provable}
H.~Sedghi and A.~Anandkumar.
\newblock Provable methods for training neural networks with sparse
  connectivity.
\newblock \emph{ArXiv:1412.2693}, 2014.

\bibitem[Shalev-Shwartz et~al.(2011)Shalev-Shwartz, Shamir, and
  Sridharan]{shalev2011learning}
S.~Shalev-Shwartz, O.~Shamir, and K.~Sridharan.
\newblock Learning kernel-based halfspaces with the 0-1 loss.
\newblock \emph{SIAM Journal on Computing}, 40\penalty0 (6):\penalty0
  1623--1646, 2011.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
D.~Silver, A.~Huang, C.~J. Maddison, A.~Guez, L.~Sifre, G.~Van Den~Driessche,
  J.~Schrittwieser, I.~Antonoglou, V.~Panneershelvam, M.~Lanctot, et~al.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Sohn and Lee(2012)]{sohn2012learning}
K.~Sohn and H.~Lee.
\newblock Learning invariant representations with local transformations.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning (ICML-12)}, pages 1311--1318, 2012.

\bibitem[Sopena et~al.(1999)Sopena, Romero, and Alquezar]{sopena1999neural}
J.~M. Sopena, E.~Romero, and R.~Alquezar.
\newblock Neural networks with periodic and monotonic activation functions: a
  comparative study in classification problems.
\newblock In \emph{ICANN 99}, pages 323--328, 1999.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Steinwart and Christmann(2008)]{SteChr08}
I.~Steinwart and A.~Christmann.
\newblock \emph{Support vector machines}.
\newblock Springer, New York, 2008.

\bibitem[Tang(2013)]{tang2013deep}
Y.~Tang.
\newblock Deep learning using linear support vector machines.
\newblock \emph{arXiv preprint arXiv:1306.0239}, 2013.

\bibitem[VariationsMNIST()]{WinNT}
VariationsMNIST.
\newblock Variations on the {MNIST} digits.
\newblock
  \url{http://www.iro.umontreal.ca/~lisa/twiki/bin/view.cgi/Public/MnistVariations},
  2007.

\bibitem[Vincent et~al.(2010)Vincent, Larochelle, Lajoie, Bengio, and
  Manzagol]{vincent2010stacked}
P.~Vincent, H.~Larochelle, I.~Lajoie, Y.~Bengio, and P.-A. Manzagol.
\newblock Stacked denoising autoencoders: Learning useful representations in a
  deep network with a local denoising criterion.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  3371--3408, 2010.

\bibitem[Wang et~al.(2012)Wang, Wu, Coates, and Ng]{wang2012end}
T.~Wang, D.~J. Wu, A.~Coates, and A.~Y. Ng.
\newblock End-to-end text recognition with convolutional neural networks.
\newblock In \emph{Pattern Recognition (ICPR), 2012 21st International
  Conference on}, pages 3304--3308. IEEE, 2012.

\bibitem[Xiao and Zhang(2014)]{xiao2014proximal}
L.~Xiao and T.~Zhang.
\newblock A proximal stochastic gradient method with progressive variance
  reduction.
\newblock \emph{SIAM Journal on Optimization}, 24\penalty0 (4):\penalty0
  2057--2075, 2014.

\bibitem[Zhang et~al.(2015)Zhang, Lee, Wainwright, and
  Jordan]{zhang2015learning}
Y.~Zhang, J.~D. Lee, M.~J. Wainwright, and M.~I. Jordan.
\newblock Learning halfspaces and neural networks with random initialization.
\newblock \emph{arXiv preprint arXiv:1511.07948}, 2015.

\bibitem[Zhang et~al.(2016)Zhang, Lee, and Jordan]{zhang2015ell_1}
Y.~Zhang, J.~D. Lee, and M.~I. Jordan.
\newblock $\ell_1$-regularized neural networks are improperly learnable in
  polynomial time.
\newblock In \emph{Proceedings on the 33rd International Conference on Machine
  Learning}, 2016.

\end{thebibliography}
