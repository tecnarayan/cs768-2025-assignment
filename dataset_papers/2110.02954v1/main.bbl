\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, Allen-Zhu, Bullins, Hazan, and
  Ma]{agarwal2017finding}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima faster than gradient descent.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 1195--1199, 2017.

\bibitem[Allen-Zhu(2018)]{allen2018natasha2}
Zeyuan Allen-Zhu.
\newblock Natasha 2: faster non-convex optimization than sgd.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2680--2691, 2018.

\bibitem[Arjevani et~al.(2020)Arjevani, Carmon, Duchi, Foster, Sekhari, and
  Sridharan]{arjevani2020second}
Yossi Arjevani, Yair Carmon, John~C Duchi, Dylan~J Foster, Ayush Sekhari, and
  Karthik Sridharan.
\newblock Second-order information in non-convex stochastic optimization: Power
  and limitations.
\newblock In \emph{Conference on Learning Theory}, pages 242--299. PMLR, 2020.

\bibitem[Bach(2010)]{bach2010self}
Francis Bach.
\newblock Self-concordant analysis for logistic regression.
\newblock \emph{Electronic Journal of Statistics}, 4:\penalty0 384--414, 2010.

\bibitem[Boyd and Vandenberghe(2004)]{boyd2004convex}
Stephen Boyd and Lieven Vandenberghe.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Bullins(2020)]{bullins2020highly}
Brian Bullins.
\newblock Highly smooth minimization of non-smooth problems.
\newblock In \emph{Conference on Learning Theory}, pages 988--1030. PMLR, 2020.

\bibitem[Carmon et~al.(2018)Carmon, Duchi, Hinder, and
  Sidford]{carmon2018accelerated}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for nonconvex optimization.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (2):\penalty0
  1751--1772, 2018.

\bibitem[Carmon et~al.(2020)Carmon, Jambulapati, Jiang, Jin, Lee, Sidford, and
  Tian]{carmon2020acceleration}
Yair Carmon, Arun Jambulapati, Qijia Jiang, Yujia Jin, Yin~Tat Lee, Aaron
  Sidford, and Kevin Tian.
\newblock Acceleration with a ball optimization oracle.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Chang and Lin(2011)]{CC01a}
Chih-Chung Chang and Chih-Jen Lin.
\newblock {LIBSVM}: A library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:\penalty0 27:1--27:27, 2011.
\newblock Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}.

\bibitem[Coppola(2015)]{Coppola2015:IPM}
Greg Coppola.
\newblock \emph{Iterative parameter mixing for distributed large-margin
  training of structured predictors for natural language processing}.
\newblock PhD thesis, The University of Edinburgh, 2015.

\bibitem[Cotter et~al.(2011)Cotter, Shamir, Srebro, and
  Sridharan]{cotter2011better}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~24, pages 1647--1655, 2011.

\bibitem[Crane and Roosta(2019)]{crane2019dingo}
Rixon Crane and Fred Roosta.
\newblock Dingo: Distributed {N}ewton-type method for gradient-norm
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jan):\penalty0 165--202, 2012.

\bibitem[Dua and Graff(2017)]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Ghadimi and Lan(2012)]{ghadimi2012optimal}
Saeed Ghadimi and Guanghui Lan.
\newblock Optimal stochastic approximation algorithms for strongly convex
  stochastic composite optimization i: A generic algorithmic framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (4):\penalty0
  1469--1492, 2012.

\bibitem[Gupta et~al.(2021)Gupta, Ghosh, Derezinski, Khanna, Ramchandran, and
  Mahoney]{gupta2021localnewton}
Vipul Gupta, Avishek Ghosh, Michal Derezinski, Rajiv Khanna, Kannan
  Ramchandran, and Michael Mahoney.
\newblock {LocalNewton}: Reducing communication bottleneck for distributed
  learning.
\newblock \emph{arXiv preprint arXiv:2105.07320}, 2021.

\bibitem[Islamov et~al.(2021)Islamov, Qian, and
  Richt{\'a}rik]{islamov2021distributed}
Rustem Islamov, Xun Qian, and Peter Richt{\'a}rik.
\newblock Distributed second order methods with fast rates and compressed
  communication.
\newblock \emph{arXiv preprint arXiv:2102.07158}, 2021.

\bibitem[Karimireddy et~al.(2018)Karimireddy, Stich, and
  Jaggi]{karimireddy2018global}
Sai~Praneeth Karimireddy, Sebastian~U Stich, and Martin Jaggi.
\newblock Global linear convergence of {N}ewton's method without
  strong-convexity or lipschitz gradients.
\newblock \emph{arXiv preprint arXiv:1806.00413}, 2018.

\bibitem[Karimireddy et~al.(2019)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2019scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank~J Reddi,
  Sebastian~U Stich, and Ananda~Theertha Suresh.
\newblock {SCAFFOLD}: Stochastic controlled averaging for on-device federated
  learning.
\newblock \emph{arXiv preprint arXiv:1910.06378}, 2019.

\bibitem[Khaled et~al.(2019)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2019first}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock First analysis of local gd on heterogeneous data.
\newblock \emph{arXiv preprint arXiv:1909.04715}, 2019.

\bibitem[Khaled et~al.(2020)Khaled, Mishchenko, and
  Richt{\'a}rik]{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local sgd on identical and heterogeneous data.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4519--4529. PMLR, 2020.

\bibitem[Koloskova et~al.(2020)Koloskova, Loizou, Boreiri, Jaggi, and
  Stich]{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and Sebastian
  Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In \emph{International Conference on Machine Learning}, pages
  5381--5393. PMLR, 2020.

\bibitem[Lan(2012)]{lan2012optimal}
Guanghui Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1-2):\penalty0
  365--397, 2012.

\bibitem[Monteiro and Svaiter(2013)]{monteiro2013accelerated}
Renato~DC Monteiro and Benar~Fux Svaiter.
\newblock An accelerated hybrid proximal extragradient method for convex
  optimization and its implications to second-order methods.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (2):\penalty0
  1092--1125, 2013.

\bibitem[Nesterov(1998)]{nesterov1998introductory}
Yurii Nesterov.
\newblock Introductory lectures on convex programming volume i: Basic course.
\newblock \emph{Lecture notes}, 3\penalty0 (4):\penalty0 5, 1998.

\bibitem[Nesterov(2019)]{nesterov2019implementable}
Yurii Nesterov.
\newblock Implementable tensor methods in unconstrained convex optimization.
\newblock \emph{Mathematical Programming}, pages 1--27, 2019.

\bibitem[Nesterov and Nemirovskii(1994)]{nesterov1994interior}
Yurii Nesterov and Arkadii Nemirovskii.
\newblock \emph{Interior-point polynomial algorithms in convex programming}.
\newblock SIAM, 1994.

\bibitem[Nesterov and Polyak(2006)]{nesterov2006cubic}
Yurii Nesterov and Boris~T Polyak.
\newblock Cubic regularization of {N}ewton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Nocedal and Wright(2006)]{nocedal2006numerical}
Jorge Nocedal and Stephen Wright.
\newblock \emph{Numerical optimization}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Pearlmutter(1994)]{pearlmutter1994fast}
Barak~A Pearlmutter.
\newblock Fast exact multiplication by the {H}essian.
\newblock \emph{Neural Computation}, 6\penalty0 (1):\penalty0 147--160, 1994.

\bibitem[Reddi et~al.(2016)Reddi, Kone{\v{c}}n{\`y}, Richt{\'a}rik,
  P{\'o}cz{\'o}s, and Smola]{reddi2016aide}
Sashank~J Reddi, Jakub Kone{\v{c}}n{\`y}, Peter Richt{\'a}rik, Barnab{\'a}s
  P{\'o}cz{\'o}s, and Alex Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock \emph{arXiv preprint arXiv:1608.06879}, 2016.

\bibitem[Shamir et~al.(2014)Shamir, Srebro, and Zhang]{shamir2014communication}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  {N}ewton-type method.
\newblock In \emph{International Conference on Machine Learning}, pages
  1000--1008. PMLR, 2014.

\bibitem[Stich(2019)]{stich2019local}
Sebastian~U Stich.
\newblock Local sgd converges fast and communicates little.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wang et~al.(2018)Wang, Roosta, Xu, and Mahoney]{wang2018giant}
Shusen Wang, Fred Roosta, Peng Xu, and Michael~W Mahoney.
\newblock Giant: Globally improved approximate {N}ewton method for distributed
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 2332--2342, 2018.

\bibitem[Woodworth et~al.(2020{\natexlab{a}})Woodworth, Patel, Stich, Dai,
  Bullins, Mcmahan, Shamir, and Srebro]{woodworth2020local}
Blake Woodworth, Kumar~Kshitij Patel, Sebastian Stich, Zhen Dai, Brian Bullins,
  Brendan Mcmahan, Ohad Shamir, and Nathan Srebro.
\newblock Is local sgd better than minibatch sgd?
\newblock In \emph{International Conference on Machine Learning}, pages
  10334--10343. PMLR, 2020{\natexlab{a}}.

\bibitem[Woodworth et~al.(2021)Woodworth, Bullins, Shamir, and
  Srebro]{woodworth2021min}
Blake Woodworth, Brian Bullins, Ohad Shamir, and Nathan Srebro.
\newblock The min-max complexity of distributed stochastic convex optimization
  with intermittent communication.
\newblock In \emph{Conference on Learning Theory}. PMLR, 2021.

\bibitem[Woodworth et~al.(2018)Woodworth, Wang, Smith, McMahan, and
  Srebro]{woodworth2018graph}
Blake~E Woodworth, Jialei Wang, Adam Smith, Brendan McMahan, and Nati Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 8496--8506, 2018.

\bibitem[Woodworth et~al.(2020{\natexlab{b}})Woodworth, Patel, and
  Srebro]{woodworth2020minibatch}
Blake~E Woodworth, Kumar~Kshitij Patel, and Nati Srebro.
\newblock Minibatch vs local sgd for heterogeneous distributed learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6281--6292, 2020{\natexlab{b}}.

\bibitem[Yuan and Ma(2020)]{yuan2020federated}
Honglin Yuan and Tengyu Ma.
\newblock Federated accelerated stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Zhang and Xiao(2015)]{zhang2015disco}
Yuchen Zhang and Lin Xiao.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In \emph{International Conference on Machine Learning}, pages
  362--370. PMLR, 2015.

\bibitem[Zhang et~al.(2012)Zhang, Wainwright, and
  Duchi]{zhang2012communication}
Yuchen Zhang, Martin~J Wainwright, and John~C Duchi.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1502--1510, 2012.

\bibitem[Zhang et~al.(2013)Zhang, Duchi, and
  Wainwright]{zhang2013communication}
Yuchen Zhang, John~C Duchi, and Martin~J Wainwright.
\newblock Communication-efficient algorithms for statistical optimization.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 3321--3363, 2013.

\bibitem[Zhou and Cong(2018)]{Zhou2018:Kaveraging}
Fan Zhou and Guojing Cong.
\newblock On the convergence properties of a k-step averaging stochastic
  gradient descent algorithm for nonconvex optimization.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence, {IJCAI-18}}, pages 3219--3227.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018.
\newblock \doi{10.24963/ijcai.2018/447}.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2595--2603, 2010.

\end{thebibliography}
