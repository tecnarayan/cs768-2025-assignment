
@incollection{leqi_human-aligned_2019,
	title = {On {Human}-{Aligned} {Risk} {Minimization}},
	url = {http://papers.nips.cc/paper/9642-on-human-aligned-risk-minimization.pdf},
	urldate = {2020-03-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Leqi, Liu and Prasad, Adarsh and Ravikumar, Pradeep K},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {15055--15064},
	file = {NIPS Full Text PDF:/Users/cassidy/Zotero/storage/M997YHH8/Leqi et al. - 2019 - On Human-Aligned Risk Minimization.pdf:application/pdf;NIPS Snapshot:/Users/cassidy/Zotero/storage/DIXC2YNS/9642-on-human-aligned-risk-minimization.html:text/html},
}

@article{hiranandani_performance_2019,
	title = {Performance {Metric} {Elicitation} from {Pairwise} {Classifier} {Comparisons}},
	url = {http://arxiv.org/abs/1806.01827},
	abstract = {Given a binary prediction problem, which performance metric should the classifier optimize? We address this question by formalizing the problem of Metric Elicitation. The goal of metric elicitation is to discover the performance metric of a practitioner, which reflects her innate rewards (costs) for correct (incorrect) classification. In particular, we focus on eliciting binary classification performance metrics from pairwise feedback, where a practitioner is queried to provide relative preference between two classifiers. By exploiting key geometric properties of the space of confusion matrices, we obtain provably query efficient algorithms for eliciting linear and linear-fractional performance metrics. We further show that our method is robust to feedback and finite sample noise.},
	urldate = {2020-11-02},
	journal = {arXiv:1806.01827 [cs, stat]},
	author = {Hiranandani, Gaurush and Boodaghians, Shant and Mehta, Ruta and Koyejo, Oluwasanmi},
	month = jan,
	year = {2019},
	note = {arXiv: 1806.01827},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The paper to appear in AISTATS 2019. 35 pages, 6 figures, 3 tables},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/UMWMCQNT/Hiranandani et al. - 2019 - Performance Metric Elicitation from Pairwise Class.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/B5Z9H6DR/1806.html:text/html},
}

@incollection{hiranandani_multiclass_2019,
	title = {Multiclass {Performance} {Metric} {Elicitation}},
	url = {http://papers.nips.cc/paper/9133-multiclass-performance-metric-elicitation.pdf},
	urldate = {2020-11-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Hiranandani, Gaurush and Boodaghians, Shant and Mehta, Ruta and Koyejo, Oluwasanmi O},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {9356--9365},
	file = {NIPS Full Text PDF:/Users/cassidy/Zotero/storage/GNILNCW3/Hiranandani et al. - 2019 - Multiclass Performance Metric Elicitation.pdf:application/pdf;NIPS Snapshot:/Users/cassidy/Zotero/storage/4LIYELUE/9133-multiclass-performance-metric-elicitation.html:text/html},
}

@article{evans_learning_2015,
	title = {Learning the {Preferences} of {Ignorant}, {Inconsistent} {Agents}},
	url = {http://arxiv.org/abs/1512.05832},
	abstract = {An important use of machine learning is to learn what people value. What posts or photos should a user be shown? Which jobs or activities would a person find rewarding? In each case, observations of people's past choices can inform our inferences about their likes and preferences. If we assume that choices are approximately optimal according to some utility function, we can treat preference inference as Bayesian inverse planning. That is, given a prior on utility functions and some observed choices, we invert an optimal decision-making process to infer a posterior distribution on utility functions. However, people often deviate from approximate optimality. They have false beliefs, their planning is sub-optimal, and their choices may be temporally inconsistent due to hyperbolic discounting and other biases. We demonstrate how to incorporate these deviations into algorithms for preference inference by constructing generative models of planning for agents who are subject to false beliefs and time inconsistency. We explore the inferences these models make about preferences, beliefs, and biases. We present a behavioral experiment in which human subjects perform preference inference given the same observations of choices as our model. Results show that human subjects (like our model) explain choices in terms of systematic deviations from optimal behavior and suggest that they take such deviations into account when inferring preferences.},
	urldate = {2021-01-15},
	journal = {arXiv:1512.05832 [cs]},
	author = {Evans, Owain and Stuhlmueller, Andreas and Goodman, Noah D.},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.05832},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: AAAI 2016},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/43UDRMN8/Evans et al. - 2015 - Learning the Preferences of Ignorant, Inconsistent.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/YLL4X3XE/1512.html:text/html},
}

@article{dawes_clinical_1989,
	title = {Clinical {Versus} {Actuarial} {Judgment}},
	volume = {243},
	issn = {0036-8075},
	url = {https://www.jstor.org/stable/1703476},
	abstract = {Professionals are frequently consulted to diagnose and predict human behavior; optimal treatment and planning often hinge on the consultant's judgmental accuracy. The consultant may rely on one of two contrasting approaches to decision-making--the clinical and actuarial methods. Research comparing these two approaches shows the actuarial method to be superior. Factors underlying the greater accuracy of actuarial methods, sources of resistance to the scientific findings, and the benefits of increased reliance on actuarial approaches are discussed.},
	number = {4899},
	urldate = {2021-02-05},
	journal = {Science},
	author = {Dawes, Robyn M. and Faust, David and Meehl, Paul E.},
	year = {1989},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {1668--1674},
}

@article{swartz_inverse_2006,
	title = {Inverse {Decision} {Theory}},
	volume = {101},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/abs/10.1198/016214505000000998},
	doi = {10.1198/016214505000000998},
	abstract = {Identifying an optimal decision rule using Bayesian decision theory requires priors, likelihoods, and losses. In many medical settings, we can develop priors and likelihoods, but specifying losses can be difficult, especially when considering both patient outcomes and economic costs. If there is a widely accepted treatment strategy, then we can consider the inverse problem and find a region in the space of losses where the procedure is optimal. We call this approach inverse decision theory (IDT). We apply IDT to the standard of care for diagnosis and treatment of precancerous lesions of the cervix, and consider an alternative procedure that has been proposed. We use a Bayesian approach to estimate the probabilities associated with the diagnostic tests and make inferences about the region in loss space where these medical procedures are optimal. In particular, we find evidence supporting the current standard of care.},
	number = {473},
	urldate = {2021-02-18},
	journal = {Journal of the American Statistical Association},
	author = {Swartz, Richard J and Cox, Dennis D and Cantor, Scott B and Davies, Kalatu and Follen, Michele},
	month = mar,
	year = {2006},
	note = {Publisher: Taylor \& Francis},
	pages = {1--8},
	file = {Snapshot:/Users/cassidy/Zotero/storage/XL8SUQSU/016214505000000998.html:text/html},
}

@techreport{mullainathan_machine_2019,
	title = {A {Machine} {Learning} {Approach} to {Low}-{Value} {Health} {Care}: {Wasted} {Tests}, {Missed} {Heart} {Attacks} and {Mis}-predictions},
	shorttitle = {A machine learning approach to low-value health care},
	institution = {National Bureau of Economic Research},
	author = {Mullainathan, Sendhil and Obermeyer, Ziad},
	year = {2019},
	file = {Full Text:/Users/cassidy/Zotero/storage/LS8PRJ5V/Mullainathan and Obermeyer - 2019 - A machine learning approach to low-value health ca.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/2IY5AFAR/w26168.html:text/html},
}

@article{zadimoghaddam_efficiently_2012,
	title = {Efficiently {Learning} from {Revealed} {Preference}},
	url = {http://arxiv.org/abs/1211.4150},
	abstract = {In this paper, we consider the revealed preferences problem from a learning perspective. Every day, a price vector and a budget is drawn from an unknown distribution, and a rational agent buys his most preferred bundle according to some unknown utility function, subject to the given prices and budget constraint. We wish not only to find a utility function which rationalizes a finite set of observations, but to produce a hypothesis valuation function which accurately predicts the behavior of the agent in the future. We give efficient algorithms with polynomial sample-complexity for agents with linear valuation functions, as well as for agents with linearly separable, concave valuation functions with bounded second derivative.},
	urldate = {2021-03-01},
	journal = {arXiv:1211.4150 [cs]},
	author = {Zadimoghaddam, Morteza and Roth, Aaron},
	month = nov,
	year = {2012},
	note = {arXiv: 1211.4150},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Computer Science - Data Structures and Algorithms},
	annote = {Comment: Extended abstract appears in WINE 2012},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/2EDNE249/Zadimoghaddam and Roth - 2012 - Efficiently Learning from Revealed Preference.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/6FBZMITV/1211.html:text/html},
}

@article{hiranandani_fair_2020,
	title = {Fair {Performance} {Metric} {Elicitation}},
	url = {http://arxiv.org/abs/2006.12732},
	abstract = {What is a fair performance metric? We consider the choice of fairness metrics through the lens of metric elicitation -- a principled framework for selecting performance metrics that best reflect implicit preferences. The use of metric elicitation enables a practitioner to tune the performance and fairness metrics to the task, context, and population at hand. Specifically, we propose a novel strategy to elicit group-fair performance metrics for multiclass classification problems with multiple sensitive groups that also includes selecting the trade-off between predictive performance and fairness violation. The proposed elicitation strategy requires only relative preference feedback and is robust to both finite sample and feedback noise.},
	urldate = {2021-04-09},
	journal = {arXiv:2006.12732 [cs, stat]},
	author = {Hiranandani, Gaurush and Narasimhan, Harikrishna and Koyejo, Oluwasanmi},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.12732},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The paper to appear at NeurIPS 2020. This version includes the camera-ready edits. 31 pages, 6 figures, and 2 tables},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/RUAPDYHS/Hiranandani et al. - 2020 - Fair Performance Metric Elicitation.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/ZH5LVNK3/2006.html:text/html},
}

@book{russell_human_2019,
	title = {Human {Compatible}: {Artificial} {Intelligence} and the {Problem} of {Control}},
	shorttitle = {Human {Compatible}},
	abstract = {"The most important book on AI this year." --The Guardian"Mr. Russell's exciting book goes deep, while sparkling with dry witticisms." --The Wall Street Journal"The most important book I have read in quite some time" (Daniel Kahneman); "A must-read" (Max Tegmark); "The book we've all been waiting for" (Sam Harris)A leading artificial intelligence researcher lays out a new approach to AI that will enable us to coexist successfully with increasingly intelligent machinesIn the popular imagination, superhuman artificial intelligence is an approaching tidal wave that threatens not just jobs and human relationships, but civilization itself. Conflict between humans and machines is seen as inevitable and its outcome all too predictable.In this groundbreaking book, distinguished AI researcher Stuart Russell argues that this scenario can be avoided, but only if we rethink AI from the ground up. Russell begins by exploring the idea of intelligence in humans and in machines. He describes the near-term benefits we can expect, from intelligent personal assistants to vastly accelerated scientific research, and outlines the AI breakthroughs that still have to happen before we reach superhuman AI. He also spells out the ways humans are already finding to misuse AI, from lethal autonomous weapons to viral sabotage.If the predicted breakthroughs occur and superhuman AI emerges, we will have created entities far more powerful than ourselves. How can we ensure they never, ever, have power over us?  Russell suggests that we can rebuild AI on a new foundation, according to which machines are designed to be inherently uncertain about the human preferences they are required to satisfy. Such machines would be humble, altruistic, and committed to pursue our objectives, not theirs. This new foundation would allow us to create machines that are provably deferential and provably beneficial.},
	language = {English},
	publisher = {Penguin Books},
	author = {Russell, Stuart},
	month = oct,
	year = {2019},
}

@article{freedman_choice_2021,
	title = {Choice {Set} {Misspecification} in {Reward} {Inference}},
	url = {http://arxiv.org/abs/2101.07691},
	abstract = {Specifying reward functions for robots that operate in environments without a natural reward signal can be challenging, and incorrectly specified rewards can incentivise degenerate or dangerous behavior. A promising alternative to manually specifying reward functions is to enable robots to infer them from human feedback, like demonstrations or corrections. To interpret this feedback, robots treat as approximately optimal a choice the person makes from a choice set, like the set of possible trajectories they could have demonstrated or possible corrections they could have made. In this work, we introduce the idea that the choice set itself might be difficult to specify, and analyze choice set misspecification: what happens as the robot makes incorrect assumptions about the set of choices from which the human selects their feedback. We propose a classification of different kinds of choice set misspecification, and show that these different classes lead to meaningful differences in the inferred reward and resulting performance. While we would normally expect misspecification to hurt, we find that certain kinds of misspecification are neither helpful nor harmful (in expectation). However, in other situations, misspecification can be extremely harmful, leading the robot to believe the opposite of what it should believe. We hope our results will allow for better prediction and response to the effects of misspecification in real-world reward inference.},
	urldate = {2021-04-16},
	journal = {arXiv:2101.07691 [cs]},
	author = {Freedman, Rachel and Shah, Rohin and Dragan, Anca},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.07691},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction},
	annote = {Comment: Presented at the IJCAI-PRICAI 2020 Workshop on Artificial Intelligence Safety},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/2ULHPGZX/Freedman et al. - 2021 - Choice Set Misspecification in Reward Inference.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/QDZBR95G/2101.html:text/html},
}

@inproceedings{ziebart_maximum_2008,
	title = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}},
	volume = {8},
	booktitle = {Aaai},
	publisher = {Chicago, IL, USA},
	author = {Ziebart, Brian D. and Maas, Andrew L. and Bagnell, J. Andrew and Dey, Anind K.},
	year = {2008},
	pages = {1433--1438},
	file = {Full Text:/Users/cassidy/Zotero/storage/UM52ABKW/Ziebart et al. - 2008 - Maximum entropy inverse reinforcement learning..pdf:application/pdf},
}

@inproceedings{ramachandran_bayesian_2007,
	title = {Bayesian {Inverse} {Reinforcement} {Learning}.},
	volume = {7},
	booktitle = {{IJCAI}},
	author = {Ramachandran, Deepak and Amir, Eyal},
	year = {2007},
	pages = {2586--2591},
	file = {Full Text:/Users/cassidy/Zotero/storage/58WQIZT4/Ramachandran and Amir - 2007 - Bayesian Inverse Reinforcement Learning..pdf:application/pdf},
}

@article{fu_learning_2017,
	title = {Learning {Robust} {Rewards} with {Adversarial} {Inverse} {Reinforcement} {Learning}},
	journal = {arXiv preprint arXiv:1710.11248},
	author = {Fu, Justin and Luo, Katie and Levine, Sergey},
	year = {2017},
	file = {Full Text:/Users/cassidy/Zotero/storage/ESLT66EB/Fu et al. - 2017 - Learning robust rewards with adversarial inverse r.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/9WSC7XRZ/1710.html:text/html},
}

@inproceedings{abbeel_apprenticeship_2004,
	title = {Apprenticeship {Learning} via {Inverse} {Reinforcement} {Learning}},
	booktitle = {Proceedings of the twenty-first international conference on {Machine} learning},
	author = {Abbeel, Pieter and Ng, Andrew Y.},
	year = {2004},
	pages = {1},
	file = {Full Text:/Users/cassidy/Zotero/storage/TG2D9ESI/Abbeel and Ng - 2004 - Apprenticeship learning via inverse reinforcement .pdf:application/pdf},
}

@inproceedings{ng_algorithms_2000,
	title = {Algorithms for {Inverse} {Reinforcement} {Learning}.},
	volume = {1},
	booktitle = {{ICML}},
	author = {Ng, Andrew Y. and Russell, Stuart J.},
	year = {2000},
	pages = {2},
}

@incollection{rust_structural_1994,
	title = {Structural {Estimation} of {Markov} {Decision} {Processes}},
	volume = {4},
	isbn = {978-0-444-88766-5},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1573441205800200},
	language = {en},
	urldate = {2021-05-10},
	booktitle = {Handbook of {Econometrics}},
	publisher = {Elsevier},
	author = {Rust, John},
	year = {1994},
	doi = {10.1016/S1573-4412(05)80020-0},
	pages = {3081--3143},
	file = {Rust - 1994 - Chapter 51 Structural estimation of markov decisio.pdf:/Users/cassidy/Zotero/storage/CGL4YW97/Rust - 1994 - Chapter 51 Structural estimation of markov decisio.pdf:application/pdf},
}

@inproceedings{baker_goal_2007,
	title = {Goal {Inference} as {Inverse} {Planning}},
	volume = {29},
	booktitle = {Proceedings of the {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	author = {Baker, Chris L. and Tenenbaum, Joshua B. and Saxe, Rebecca R.},
	year = {2007},
	note = {Issue: 29},
	file = {Full Text:/Users/cassidy/Zotero/storage/4SDXTQDQ/Baker et al. - 2007 - Goal inference as inverse planning.pdf:application/pdf},
}

@article{luce_choice_1977,
	title = {The {Choice} {Axiom} {After} {Twenty} {Years}},
	volume = {15},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/0022249677900323},
	doi = {10.1016/0022-2496(77)90032-3},
	language = {en},
	number = {3},
	urldate = {2021-05-11},
	journal = {Journal of Mathematical Psychology},
	author = {Luce, R. Duncan},
	month = jun,
	year = {1977},
	pages = {215--233},
}

@article{shepard_stimulus_1957,
	title = {Stimulus and {Response} {Generalization}: {A} {Stochastic} {Model} {Relating} {Generalization} to {Distance} in {Psychological} {Space}},
	volume = {22},
	issn = {1860-0980},
	shorttitle = {Stimulus and response generalization},
	url = {https://doi.org/10.1007/BF02288967},
	doi = {10.1007/BF02288967},
	abstract = {A mathematical model is developed in an attempt to relate errors in multiple stimulus-response situations to psychological inter-stimulus and inter response distances. The fundamental assumptions are (a) that the stimulus and response confusions go on independently of each other, (b) that the probability of a stimulus confusion is an exponential decay function of the psychological distance between the stimuli, and (c) that the probability of a response confusion is an exponential decay function of the psychological distance between the responses. The problem of the operational definition of psychological distance is considered in some detail.},
	language = {en},
	number = {4},
	urldate = {2021-05-11},
	journal = {Psychometrika},
	author = {Shepard, Roger N.},
	month = dec,
	year = {1957},
	pages = {325--345},
}

@phdthesis{davies_inverse_2005,
	address = {Houston, Texas},
	title = {Inverse {Decision} {Theory} with {Medical} {Applications}},
	url = {https://scholarship.rice.edu/handle/1911/18756},
	urldate = {2021-05-23},
	school = {Rice University},
	author = {Davies, Kalatu},
	month = may,
	year = {2005},
}

@article{simoiu_problem_2017,
	title = {The {Problem} of {Infra}-{Marginality} in {Outcome} {Tests} for {Discrimination}},
	volume = {11},
	issn = {1932-6157},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-11/issue-3/The-problem-of-infra-marginality-in-outcome-tests-for-discrimination/10.1214/17-AOAS1058.full},
	doi = {10.1214/17-AOAS1058},
	abstract = {Outcome tests are a popular method for detecting bias in lending, hiring, and policing decisions. These tests operate by comparing the success rate of decisions across groups. For example, if loans made to minority applicants are observed to be repaid more often than loans made to whites, it suggests that only exceptionally qualiﬁed minorities are granted loans, indicating discrimination. Outcome tests, however, are known to suﬀer from the problem of infra-marginality: even absent discrimination, the repayment rates for minority and white loan recipients might diﬀer if the two groups have diﬀerent risk distributions. Thus, at least in theory, outcome tests can fail to accurately detect discrimination. We develop a new statistical test of discrimination—the threshold test—that mitigates the problem of infra-marginality by jointly estimating decision thresholds and risk distributions. Applying our test to a dataset of 4.5 million police stops in North Carolina, we ﬁnd that the problem of infra-marginality is more than a theoretical possibility, and can cause the outcome test to yield misleading results in practice.},
	language = {en},
	number = {3},
	urldate = {2021-05-24},
	journal = {The Annals of Applied Statistics},
	author = {Simoiu, Camelia and Corbett-Davies, Sam and Goel, Sharad},
	month = sep,
	year = {2017},
	file = {Simoiu et al. - 2017 - The problem of infra-marginality in outcome tests .pdf:/Users/cassidy/Zotero/storage/72TNZE4J/Simoiu et al. - 2017 - The problem of infra-marginality in outcome tests .pdf:application/pdf},
}

@book{barocas_fairness_2019,
	title = {Fairness and {Machine} {Learning}},
	publisher = {fairmlbook.org},
	author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
	year = {2019},
	annote = {http://www.fairmlbook.org},
}

@article{choi_inverse_2011,
	title = {Inverse {Reinforcement} {Learning} in {Partially} {Observable} {Environments}},
	volume = {12},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v12/choi11a.html},
	number = {21},
	urldate = {2021-05-24},
	journal = {Journal of Machine Learning Research},
	author = {Choi, Jaedeug and Kim, Kee-Eung},
	year = {2011},
	pages = {691--730},
	file = {Full Text PDF:/Users/cassidy/Zotero/storage/R3MA84NP/Choi and Kim - 2011 - Inverse Reinforcement Learning in Partially Observ.pdf:application/pdf},
}

@inproceedings{chinaei_inverse_2012,
	title = {An {Inverse} {Reinforcement} {Learning} {Algorithm} for {Partially} {Observable} {Domains} with {Application} on {Healthcare} {Dialogue} {Management}},
	volume = {1},
	doi = {10.1109/ICMLA.2012.31},
	abstract = {In this paper, we propose an algorithm for learning a reward model from an expert policy in partially observable Markov decision processes (POMDPs). The problem is formulated as inverse reinforcement learning (IRL) in the POMDP framework. The proposed algorithm then uses the expert trajectories to find an unknown reward model-based on the known POMDP model components. Similar to previous IRL work in Markov Decision Processes (MDPs), our algorithm maximizes the sum of the margin between the expert policy and the intermediate candidate policies. However, in contrast to previous work, the expert and intermediate candidate policy values are approximated using the beliefs recovered from the expert trajectories, specifically by approximating expert belief transitions. We apply our IRL algorithm to a healthcare dialogue POMDP where the POMDP model components are estimated from real dialogues. Our experimental results show that the proposed algorithm is able to learn a reward model that accounts for the expert policy.},
	author = {Chinaei, Hamid R. and Chaib-Draa, Brahim},
	month = dec,
	year = {2012},
	keywords = {Testing, Approximation algorithms, Approximation methods, dialogue management, Equations, Inverse reinforcement learning, Mathematical model, partially observable Markov decision processes, Trajectory, Vectors},
	pages = {144--149},
	file = {IEEE Xplore Abstract Record:/Users/cassidy/Zotero/storage/VL3GB5R2/6406603.html:text/html},
}

@article{hertwig_descriptionexperience_2009,
	title = {The {Description}–{Experience} {Gap} in {Risky} {Choice}},
	volume = {13},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661309002125},
	doi = {10.1016/j.tics.2009.09.004},
	abstract = {According to a common conception in behavioral decision research, two cognitive processes—overestimation and overweighting—operate to increase the impact of rare events on people's choices. Supportive findings stem primarily from investigations in which people learn about options via descriptions thereof. Recently, a number of researchers have begun to investigate risky choice in settings in which people learn about options by experiential sampling over time. This article reviews work across three experiential paradigms. Converging findings show that when people make decisions based on experience, rare events tend to have less impact than they deserve according to their objective probabilities. Striking similarities in human and animal experience-based choices, ways of modeling these choices, and their implications for risk and precautionary behavior are discussed.},
	language = {en},
	number = {12},
	urldate = {2021-05-24},
	journal = {Trends in Cognitive Sciences},
	author = {Hertwig, Ralph and Erev, Ido},
	month = dec,
	year = {2009},
	pages = {517--523},
	file = {Full Text:/Users/cassidy/Zotero/storage/I2EAJA8S/Hertwig and Erev - 2009 - The description–experience gap in risky choice.pdf:application/pdf;ScienceDirect Snapshot:/Users/cassidy/Zotero/storage/W4DZBAFH/S1364661309002125.html:text/html},
}

@article{bhatia_agnostic_2021,
	title = {Agnostic {Learning} with {Unknown} {Utilities}},
	url = {http://arxiv.org/abs/2104.08482},
	abstract = {Traditional learning approaches for classification implicitly assume that each mistake has the same cost. In many real-world problems though, the utility of a decision depends on the underlying context \$x\$ and decision \$y\$. However, directly incorporating these utilities into the learning objective is often infeasible since these can be quite complex and difficult for humans to specify. We formally study this as agnostic learning with unknown utilities: given a dataset \$S = {\textbackslash}\{x\_1, {\textbackslash}ldots, x\_n{\textbackslash}\}\$ where each data point \$x\_i {\textbackslash}sim {\textbackslash}mathcal\{D\}\$, the objective of the learner is to output a function \$f\$ in some class of decision functions \${\textbackslash}mathcal\{F\}\$ with small excess risk. This risk measures the performance of the output predictor \$f\$ with respect to the best predictor in the class \${\textbackslash}mathcal\{F\}\$ on the unknown underlying utility \$u{\textasciicircum}*\$. This utility \$u{\textasciicircum}*\$ is not assumed to have any specific structure. This raises an interesting question whether learning is even possible in our setup, given that obtaining a generalizable estimate of utility \$u{\textasciicircum}*\$ might not be possible from finitely many samples. Surprisingly, we show that estimating the utilities of only the sampled points{\textasciitilde}\$S\$ suffices to learn a decision function which generalizes well. We study mechanisms for eliciting information which allow a learner to estimate the utilities \$u{\textasciicircum}*\$ on the set \$S\$. We introduce a family of elicitation mechanisms by generalizing comparisons, called the \$k\$-comparison oracle, which enables the learner to ask for comparisons across \$k\$ different inputs \$x\$ at once. We show that the excess risk in our agnostic learning framework decreases at a rate of \$O{\textbackslash}left({\textbackslash}frac\{1\}\{k\} {\textbackslash}right)\$. This result brings out an interesting accuracy-elicitation trade-off -- as the order \$k\$ of the oracle increases, the comparative queries become harder to elicit from humans but allow for more accurate learning.},
	urldate = {2021-05-24},
	journal = {arXiv:2104.08482 [cs, stat]},
	author = {Bhatia, Kush and Bartlett, Peter L. and Dragan, Anca D. and Steinhardt, Jacob},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.08482},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 30 pages; published as a conference paper at ITCS 2021},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/TCTTYMRD/Bhatia et al. - 2021 - Agnostic learning with unknown utilities.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/IM3JLTJV/2104.html:text/html},
}

@article{valiant_theory_1984,
	title = {A {Theory} of the {Learnable}},
	volume = {27},
	number = {11},
	journal = {Communications of the ACM},
	author = {Valiant, Leslie G.},
	year = {1984},
	note = {Publisher: ACM New York, NY, USA},
	pages = {1134--1142},
	file = {Full Text:/Users/cassidy/Zotero/storage/CXJ8JBSW/Valiant - 1984 - A theory of the learnable.pdf:application/pdf},
}

@article{ehrenfeucht_general_1989,
	title = {A {General} {Lower} {Bound} on the {Number} of {Examples} {Needed} for {Learning}},
	volume = {82},
	number = {3},
	journal = {Information and Computation},
	author = {Ehrenfeucht, Andrzej and Haussler, David and Kearns, Michael and Valiant, Leslie},
	year = {1989},
	note = {Publisher: Elsevier},
	pages = {247--261},
	file = {Snapshot:/Users/cassidy/Zotero/storage/XE63WFGL/0890540189900023.html:text/html},
}

@article{jeon_reward-rational_2020,
	title = {Reward-{Rational} ({Implicit}) {Choice}: {A} {Unifying} {Formalism} for {Reward} {Learning}},
	shorttitle = {Reward-rational (implicit) choice},
	url = {http://arxiv.org/abs/2002.04833},
	abstract = {It is often difficult to hand-specify what the correct reward function is for a task, so researchers have instead aimed to learn reward functions from human behavior or feedback. The types of behavior interpreted as evidence of the reward function have expanded greatly in recent years. We've gone from demonstrations, to comparisons, to reading into the information leaked when the human is pushing the robot away or turning it off. And surely, there is more to come. How will a robot make sense of all these diverse types of behavior? Our key insight is that different types of behavior can be interpreted in a single unifying formalism - as a reward-rational choice that the human is making, often implicitly. The formalism offers both a unifying lens with which to view past work, as well as a recipe for interpreting new sources of information that are yet to be uncovered. We provide two examples to showcase this: interpreting a new feedback type, and reading into how the choice of feedback itself leaks information about the reward.},
	urldate = {2021-05-24},
	journal = {arXiv:2002.04833 [cs]},
	author = {Jeon, Hong Jun and Milli, Smitha and Dragan, Anca D.},
	month = dec,
	year = {2020},
	note = {arXiv: 2002.04833},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Robotics},
	annote = {Comment: Published at NeurIPS 2020},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/A842T8NF/Jeon et al. - 2020 - Reward-rational (implicit) choice A unifying form.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/QZ5NV55D/2002.html:text/html},
}

@book{devroye_probabilistic_2013,
	title = {A {Probabilistic} {Theory} of {Pattern} {Recognition}},
	volume = {31},
	publisher = {Springer Science \& Business Media},
	author = {Devroye, Luc and Györfi, László and Lugosi, Gábor},
	year = {2013},
	file = {Full Text:/Users/cassidy/Zotero/storage/VKT3FHL2/Devroye et al. - 2013 - A probabilistic theory of pattern recognition.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/JGEYGW7S/Y5bxBwAAQBAJ.html:text/html},
}

@article{blumer_learnability_1989,
	title = {Learnability and the {Vapnik}-{Chervonenkis} {Dimension}},
	volume = {36},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/76359.76371},
	doi = {10.1145/76359.76371},
	abstract = {Valiant’s learnability model is extended to learning classesof concepts defined by regions in Euclidean space E”. The methods in this paper lead to a unified treatment of some of Valiant’s results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the VapnikChervonenkis dimension, a simple combinatorial parameter of the classof concepts to be learned. Using this parameter, the complexity and closure properties of learnable classesare analyzed, and the necessary and sufftcient conditions are provided for feasible learnability.},
	language = {en},
	number = {4},
	urldate = {2021-05-25},
	journal = {Journal of the ACM},
	author = {Blumer, Anselm and Ehrenfeucht, A. and Haussler, David and Warmuth, Manfred K.},
	month = oct,
	year = {1989},
	pages = {929--965},
	file = {Blumer et al. - 1989 - Learnability and the Vapnik-Chervonenkis dimension.pdf:/Users/cassidy/Zotero/storage/Z6D56B29/Blumer et al. - 1989 - Learnability and the Vapnik-Chervonenkis dimension.pdf:application/pdf},
}

@book{vapnik_estimation_2006,
	address = {New York},
	series = {Information {Science} and {Statistics}},
	title = {Estimation of {Dependences} {Based} on {Empirical} {Data}},
	isbn = {978-0-387-30865-4},
	url = {https://www.springer.com/gp/book/9780387308654},
	abstract = {Twenty-?ve years have passed since the publication of the Russian version of the book Estimation of Dependencies Based on Empirical Data (EDBED for short). Twen- ?ve years is a long period of time. During these years many things have happened. Looking back, one can see how rapidly life and technology have changed, and how slow and dif?cult it is to change the theoretical foundation of the technology and its philosophy. I pursued two goals writing this Afterword: to update the technical results presented in EDBED (the easy goal) and to describe a general picture of how the new ideas developed over these years (a much more dif?cult goal). The picture which I would like to present is a very personal (and therefore very biased) account of the development of one particular branch of science, Empirical - ference Science. Such accounts usually are not included in the content of technical publications. I have followed this rule in all of my previous books. But this time I would like to violate it for the following reasons. First of all, for me EDBED is the important milestone in the development of empirical inference theory and I would like to explain why. S- ond, during these years, there were a lot of discussions between supporters of the new 1 paradigm (now it is called the VC theory ) and the old one (classical statistics).},
	language = {en},
	urldate = {2021-05-25},
	publisher = {Springer-Verlag},
	author = {Vapnik, V.},
	year = {2006},
	doi = {10.1007/0-387-34239-7},
	file = {Snapshot:/Users/cassidy/Zotero/storage/3MYCF6FV/9780387308654.html:text/html},
}

@article{kearns_toward_1994,
	title = {Toward {Efficient} {Agnostic} {Learning}},
	volume = {17},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00993468},
	doi = {10.1007/BF00993468},
	abstract = {In this paper we initiate an investigation of generalizations of the Probably Approximately Correct (PAC) learning model that attempt to significantly weaken the target function assumptions. The ultimate goal in this direction is informally termedagnostic learning, in which we make virtually no assumptions on the target function. The name derives from the fact that as designers of learning algorithms, we give up the belief that Nature (as represented by the target function) has a simple or succinct explanation. We give a number of positive and negative results that provide an initial outline of the possibilities for agnostic learning. Our results include hardness results for the most obvious generalization of the PAC model to an agnostic setting, an efficient and general agnostic learning method based on dynamic programming, relationships between loss functions for agnostic learning, and an algorithm for a learning problem that involves hidden variables.},
	language = {en},
	number = {2},
	urldate = {2021-05-26},
	journal = {Machine Learning},
	author = {Kearns, Michael J. and Schapire, Robert E. and Sellie, Linda M.},
	month = nov,
	year = {1994},
	pages = {115--141},
	file = {Springer Full Text PDF:/Users/cassidy/Zotero/storage/CMG5D6YY/Kearns et al. - 1994 - Toward efficient agnostic learning.pdf:application/pdf},
}

@article{haussler_decision_1992,
	title = {Decision {Theoretic} {Generalizations} of the {PAC} {Model} for {Neural} {Net} and {Other} {Learning} {Applications}},
	volume = {100},
	issn = {0890-5401},
	url = {https://www.sciencedirect.com/science/article/pii/089054019290010D},
	doi = {10.1016/0890-5401(92)90010-D},
	abstract = {We describe a generalization of the PAC learning model that is based on statistical decision theory. In this model the learner receives randomly drawn examples, each example consisting of an instance x ∈ X and an outcome y ∈ Y, and tries to find a decision rule h: X → A, where h ∈ H, that specifies the appropriate action a ∈ A to take for each instance x in order to minimize the expectation of a loss l(y, a). Here X, Y, and A are arbitrary sets, l is a real-valued function, and examples are generated according to an arbitrary joint distribution on X × Y. Special cases include the problem of learning a function from X into Y, the problem of learning the conditional probability distribution on Y given X (regression), and the problem of learning a distribution on X (density estimation). We give theorems on the uniform convergence of empirical loss estimates to true expected loss rates for certain decision rule spaces H, and show how this implies learnability with bounded sample size, disregarding computational complexity. As an application, we give distribution-independent upper bounds on the sample size needed for learning with feedforward neural networks. Our theorems use a generalized notion of VC dimension that applies to classes of real-valued functions, adapted from Vapnik and Pollard's work, and a notion of capacity and metric dimension for classes of functions that map into a bounded metric space.},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Information and Computation},
	author = {Haussler, David},
	month = sep,
	year = {1992},
	pages = {78--150},
	file = {ScienceDirect Full Text PDF:/Users/cassidy/Zotero/storage/4BU5CTTQ/Haussler - 1992 - Decision theoretic generalizations of the PAC mode.pdf:application/pdf;ScienceDirect Snapshot:/Users/cassidy/Zotero/storage/KHAVHPTC/089054019290010D.html:text/html},
}

@inproceedings{gleave_quantifying_2021,
	title = {Quantifying {Differences} in {Reward} {Functions}},
	url = {https://openreview.net/forum?id=LwEQnp6CYev},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Gleave, Adam and Dennis, Michael and Legg, Shane and Russell, Stuart and Leike, Jan},
	year = {2021},
}

@article{armstrong_occams_2018,
	title = {Occam's {Razor} is {Insufficient} to {Infer} the {Preferences} of {Irrational} {Agents}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html},
	language = {en},
	urldate = {2021-05-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Armstrong, Stuart and Mindermann, Sören},
	year = {2018},
	file = {Full Text PDF:/Users/cassidy/Zotero/storage/URZWLWX9/Armstrong and Mindermann - 2018 - Occam's razor is insufficient to infer the prefere.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/ZIDRTL4K/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html:text/html},
}

@article{vapnik_principles_1991,
	title = {Principles of {Risk} {Minimization} for {Learning} {Theory}},
	volume = {4},
	url = {https://proceedings.neurips.cc/paper/1991/hash/ff4d5fbbafdf976cfdc032e3bde78de5-Abstract.html},
	language = {en},
	urldate = {2021-05-26},
	journal = {Advances in Neural Information Processing Systems},
	author = {Vapnik, V.},
	year = {1991},
	file = {Full Text PDF:/Users/cassidy/Zotero/storage/6G487NE5/Vapnik - 1991 - Principles of Risk Minimization for Learning Theor.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/VFQ6SEJE/ff4d5fbbafdf976cfdc032e3bde78de5-Abstract.html:text/html},
}

@article{kleinberg_inherent_2016,
	title = {Inherent {Trade}-{Offs} in the {Fair} {Determination} of {Risk} {Scores}},
	url = {http://arxiv.org/abs/1609.05807},
	abstract = {Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.},
	urldate = {2021-05-26},
	journal = {arXiv:1609.05807 [cs, stat]},
	author = {Kleinberg, Jon and Mullainathan, Sendhil and Raghavan, Manish},
	month = nov,
	year = {2016},
	note = {arXiv: 1609.05807},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in Proceedings of Innovations in Theoretical Computer Science (ITCS), 2017},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/QWVR4D8I/Kleinberg et al. - 2016 - Inherent Trade-Offs in the Fair Determination of R.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/HRAZPGXD/1609.html:text/html},
}

@inproceedings{liu_implicit_2019,
	title = {The {Implicit} {Fairness} {Criterion} of {Unconstrained} {Learning}},
	url = {http://proceedings.mlr.press/v97/liu19f.html},
	abstract = {We clarify what fairness guarantees we can and cannot expect to follow from unconstrained machine learning. Specifically, we show that in many settings, unconstrained learning on its own implies gr...},
	language = {en},
	urldate = {2021-05-26},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Liu, Lydia T. and Simchowitz, Max and Hardt, Moritz},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {4051--4060},
	file = {Full Text PDF:/Users/cassidy/Zotero/storage/KME2QHPU/Liu et al. - 2019 - The Implicit Fairness Criterion of Unconstrained L.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/NAM8G3UA/liu19f.html:text/html},
}

@article{binswanger_attitudes_1980,
	title = {Attitudes {Toward} {Risk}: {Experimental} {Measurement} in {Rural} {India}},
	volume = {62},
	copyright = {© 1980 Agricultural and Applied Economics Association},
	issn = {1467-8276},
	shorttitle = {Attitudes {Toward} {Risk}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/1240194},
	doi = {https://doi.org/10.2307/1240194},
	abstract = {Attitudes toward risk were measured in 240 households using two methods: an interview method eliciting certainty equivalents and an experimental gambling approach with real payoffs which, at their maximum, exceeded monthly incomes of unskilled laborers. The interview method is subject to interviewer bias and its results were totally inconsistent with the experimental measures of risk aversion. Experimental measures indicate that, at high payoff levels, virtually all individuals are moderately risk-averse with little variation according to personal characteristics. Wealth tends to reduce risk aversion slightly, but its effect is not statistically significant.},
	language = {en},
	number = {3},
	urldate = {2021-05-28},
	journal = {American Journal of Agricultural Economics},
	author = {Binswanger, Hans P.},
	year = {1980},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/1240194},
	keywords = {India, psychological experiments, risk aversion, semi-arid tropics},
	pages = {395--407},
	file = {Submitted Version:/Users/cassidy/Zotero/storage/55EEX9E2/Binswanger - 1980 - Attitudes Toward Risk Experimental Measurement in.pdf:application/pdf},
}

@article{holt_risk_2002,
	title = {Risk {Aversion} and {Incentive} {Effects}},
	volume = {92},
	issn = {0002-8282},
	url = {https://www.jstor.org/stable/3083270},
	number = {5},
	urldate = {2021-05-28},
	journal = {The American Economic Review},
	author = {Holt, Charles A. and Laury, Susan K.},
	year = {2002},
	note = {Publisher: American Economic Association},
	pages = {1644--1655},
}

@article{csermely_how_2016,
	title = {How to {Reveal} {People}'s {Preferences}: {Comparing} {Time} {Consistency} and {Predictive} {Power} of {Multiple} {Price} {List} {Risk} {Elicitation} {Methods}},
	volume = {53},
	issn = {0895-5646},
	shorttitle = {How to reveal people's preferences},
	doi = {10.1007/s11166-016-9247-6},
	abstract = {The question of how to measure and classify people's risk preferences is of substantial importance in the field of economics. Inspired by the multitude of ways used to elicit risk preferences, we conduct a holistic investigation of the most prevalent method, the multiple price list (MPL) and its derivations. In our experiment, we find that revealed preferences differ under various versions of MPLs as well as yield unstable results within a 30-minute time frame. We determine the most stable elicitation method with the highest forecast accuracy by using multiple measures of within-method consistency and by using behavior in two economically relevant games as benchmarks. A derivation of the well-known method by Holt and Laury (American Economic Review 92(5):1644-1655, 2002), where the highest payoff is varied instead of probabilities, emerges as the best MPL method in both dimensions. As we pinpoint each MPL characteristic's effect on the revealed preference and its consistency, our results have implications for preference elicitation procedures in general.},
	language = {eng},
	number = {2},
	journal = {Journal of Risk and Uncertainty},
	author = {Csermely, Tamás and Rabas, Alexander},
	year = {2016},
	pmid = {28405057},
	pmcid = {PMC5366177},
	keywords = {MPL, Multiple price list, Revealed preferences, Risk, Risk preference elicitation methods},
	pages = {107--136},
	file = {Full Text:/Users/cassidy/Zotero/storage/X4RUZUGS/Csermely and Rabas - 2016 - How to reveal people's preferences Comparing time.pdf:application/pdf},
}

@article{cohen_experimental_1987,
	title = {Experimental {Comparison} of {Individual} {Behavior} {Under} {Risk} and {Under} {Uncertainty} for {Gains} and for {Losses}},
	volume = {39},
	issn = {0749-5978},
	url = {https://www.sciencedirect.com/science/article/pii/0749597887900434},
	doi = {10.1016/0749-5978(87)90043-4},
	abstract = {This study concerns individual decision making under risk and under non-probabilized uncertainty. It is based on a sample of 134 college students. These subjects had to perform binary choices offering prospects of fairly large gains or losses and knew that some payments would become effective. Data analysis leads to the following main conclusions: (a) subjects' behavior on the gain side and their behavior on the loss side are totally unrelated, which fact in particular suffices to disprove the “reflection effect” hypothesis proposed by Kahneman and Tversky; (b) conventional, model independent definitions of risk attitudes are inadequate; (c) on the gain side, subjects generally take the exact probabilities of the events into account, whereas on the loss side, they appear to have recourse only to coarser categories of plausibility; (d) correlatively, on the gain side, subjects discriminate between situations of risk and of uncertainty and are, on the whole, pessimists, whereas on the loss side, they do not differentiate between those two situations.},
	language = {en},
	number = {1},
	urldate = {2021-05-28},
	journal = {Organizational Behavior and Human Decision Processes},
	author = {Cohen, Michele and Jaffray, Jean-Yves and Said, Tanios},
	month = feb,
	year = {1987},
	pages = {1--22},
	file = {ScienceDirect Snapshot:/Users/cassidy/Zotero/storage/N5RNJG3V/0749597887900434.html:text/html},
}

@article{kahneman_prospect_1979,
	title = {Prospect {Theory}: {An} {Analysis} of {Decision} under {Risk}},
	volume = {47},
	issn = {0012-9682},
	shorttitle = {Prospect {Theory}},
	url = {https://www.jstor.org/stable/1914185},
	doi = {10.2307/1914185},
	abstract = {This paper presents a critique of expected utility theory as a descriptive model of decision making under risk, and develops an alternative model, called prospect theory. Choices among risky prospects exhibit several pervasive effects that are inconsistent with the basic tenets of utility theory. In particular, people underweight outcomes that are merely probable in comparison with outcomes that are obtained with certainty. This tendency, called the certainty effect, contributes to risk aversion in choices involving sure gains and to risk seeking in choices involving sure losses. In addition, people generally discard components that are shared by all prospects under consideration. This tendency, called the isolation effect, leads to inconsistent preferences when the same choice is presented in different forms. An alternative theory of choice is developed, in which value is assigned to gains and losses rather than to final assets and in which probabilities are replaced by decision weights. The value function is normally concave for gains, commonly convex for losses, and is generally steeper for losses than for gains. Decision weights are generally lower than the corresponding probabilities, except in the range of low probabilities. Overweighting of low probabilities may contribute to the attractiveness of both insurance and gambling.},
	number = {2},
	urldate = {2021-05-28},
	journal = {Econometrica},
	author = {Kahneman, Daniel and Tversky, Amos},
	year = {1979},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {263--291},
	file = {Submitted Version:/Users/cassidy/Zotero/storage/H2558IH8/Kahneman and Tversky - 1979 - Prospect Theory An Analysis of Decision under Ris.pdf:application/pdf},
}

@inproceedings{shah_feasibility_2019,
	title = {On the {Feasibility} of {Learning}, {Rather} than {Assuming}, {Human} {Biases} for {Reward} {Inference}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Shah, Rohin and Gundotra, Noah and Abbeel, Pieter and Dragan, Anca},
	year = {2019},
	pages = {5670--5679},
	file = {Full Text:/Users/cassidy/Zotero/storage/ZISJZN2T/Shah et al. - 2019 - On the feasibility of learning, rather than assumi.pdf:application/pdf;Snapshot:/Users/cassidy/Zotero/storage/944F2AGZ/shah19a.html:text/html},
}

@techreport{matousek_probablistic_2008,
	address = {Prague, Czech Republic},
	type = {Lecture {Notes}},
	title = {The {Probablistic} {Method}},
	institution = {Charles University},
	author = {Matoušek, Jiří and Vondrák, Jan},
	month = mar,
	year = {2008},
}

@article{angluin_fast_1979,
	title = {Fast {Probabilistic} {Algorithms} for {Hamiltonian} {Circuits} and {Matchings}},
	volume = {18},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/002200007990045X},
	doi = {10.1016/0022-0000(79)90045-X},
	abstract = {We describe and analyse three simple efficient algorithms with good probabilistic behaviour; two algorithms with run times of O(n(log n)2) which almost certainly find directed (undirected) Hamiltonian circuits in random graphs of at least cn log n edges, and an algorithm with a run time of O(n log n) which almost certainly finds a perfect matching in a random graph of at least cn log n edges. Auxiliary propositions regarding conversion between input distributions and the “de-randomization” of randomized algorithms are proved. A new model, the random access computer (RAC), is introduced specifically to treat run times in low-level complexity.},
	language = {en},
	number = {2},
	urldate = {2021-06-18},
	journal = {Journal of Computer and System Sciences},
	author = {Angluin, D. and Valiant, L. G.},
	month = apr,
	year = {1979},
	pages = {155--193},
	file = {ScienceDirect Full Text PDF:/Users/cassidy/Zotero/storage/YT6PERH7/Angluin and Valiant - 1979 - Fast probabilistic algorithms for hamiltonian circ.pdf:application/pdf;ScienceDirect Snapshot:/Users/cassidy/Zotero/storage/QLPXSPAH/002200007990045X.html:text/html},
}

@article{zhang_safe_2021,
	title = {Safe {Occlusion}-aware {Autonomous} {Driving} via {Game}-{Theoretic} {Active} {Perception}},
	url = {http://arxiv.org/abs/2105.08169},
	abstract = {Autonomous vehicles interacting with other traffic participants heavily rely on the perception and prediction of other agents' behaviors to plan safe trajectories. However, as occlusions limit the vehicle's perception ability, reasoning about potential hazards beyond the field-of-view is one of the most challenging issues in developing autonomous driving systems. This paper introduces a novel analytical approach that poses the problem of safe trajectory planning under occlusions as a hybrid zero-sum dynamic game between the autonomous vehicle (evader), and an initially hidden traffic participant (pursuer). Due to occlusions, the pursuer's state is initially unknown to the evader and may later be discovered by the vehicle's sensors. The analysis yields optimal strategies for both players as well as the set of initial conditions from which the autonomous vehicle is guaranteed to avoid collisions. We leverage this theoretical result to develop a novel trajectory planning framework for autonomous driving that provides worst-case safety guarantees while minimizing conservativeness by accounting for the vehicle's ability to actively avoid other road users as soon as they are detected in future observations. Our framework is agnostic to the driving environment and suitable for various motion planners. We demonstrate our algorithm on challenging urban and highway driving scenarios using the open-source CARLA simulator. The experimental results can be found in https://youtu.be/Cdm1T6Iv7GI.},
	urldate = {2021-06-23},
	journal = {arXiv:2105.08169 [cs, eess]},
	author = {Zhang, Zixu and Fisac, Jaime F.},
	month = may,
	year = {2021},
	note = {arXiv: 2105.08169},
	keywords = {Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
	annote = {Comment: To be appeared in Robotics: Science and Systems (RSS), 2021},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/QPFZ48EZ/Zhang and Fisac - 2021 - Safe Occlusion-aware Autonomous Driving via Game-T.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/YITLG8VT/2105.html:text/html},
}

@article{rosasco_are_2004,
	title = {Are loss functions all the same?},
	volume = {16},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/089976604773135104},
	doi = {10.1162/089976604773135104},
	abstract = {In this letter, we investigate the impact of choosing different loss functions from the viewpoint of statistical learning theory. We introduce a convexity assumption, which is met by all loss functions commonly used in the literature, and study how the bound on the estimation error changes with the loss. We also derive a general result on the minimizer of the expected risk for a convex loss function in the case of classification. The main outcome of our analysis is that for classification, the hinge loss appears to be the loss of choice. Other things being equal, the hinge loss leads to a convergence rate practically indistinguishable from the logistic loss rate and much better than the square loss rate. Furthermore, if the hypothesis space is sufficiently rich, the bounds obtained for the hinge loss are not loosened by the thresholding stage.},
	number = {5},
	urldate = {2021-10-25},
	journal = {Neural Computation},
	author = {Rosasco, Lorenzo and De Vito, Ernesto and Caponnetto, Andrea and Piana, Michele and Verri, Alessandro},
	month = may,
	year = {2004},
	pages = {1063--1076},
	file = {Submitted Version:/Users/cassidy/Zotero/storage/7WI4IFBV/Rosasco et al. - 2004 - Are loss functions all the same.pdf:application/pdf},
}

@article{mindermann_active_2019,
	title = {Active {Inverse} {Reward} {Design}},
	url = {http://arxiv.org/abs/1809.03060},
	abstract = {Designers of AI agents often iterate on the reward function in a trial-and-error process until they get the desired behavior, but this only guarantees good behavior in the training environment. We propose structuring this process as a series of queries asking the user to compare between different reward functions. Thus we can actively select queries for maximum informativeness about the true reward. In contrast to approaches asking the designer for optimal behavior, this allows us to gather additional information by eliciting preferences between suboptimal behaviors. After each query, we need to update the posterior over the true reward function from observing the proxy reward function chosen by the designer. The recently proposed Inverse Reward Design (IRD) enables this. Our approach substantially outperforms IRD in test environments. In particular, it can query the designer about interpretable, linear reward functions and still infer non-linear ones.},
	urldate = {2021-10-26},
	journal = {arXiv:1809.03060 [cs, stat]},
	author = {Mindermann, Sören and Shah, Rohin and Gleave, Adam and Hadfield-Menell, Dylan},
	month = nov,
	year = {2019},
	note = {arXiv: 1809.03060},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/82CY8H6W/Mindermann et al. - 2019 - Active Inverse Reward Design.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/NE9UYYLW/1809.html:text/html},
}

@article{biyik_asking_2019,
	title = {Asking {Easy} {Questions}: {A} {User}-{Friendly} {Approach} to {Active} {Reward} {Learning}},
	shorttitle = {Asking {Easy} {Questions}},
	url = {http://arxiv.org/abs/1910.04365},
	abstract = {Robots can learn the right reward function by querying a human expert. Existing approaches attempt to choose questions where the robot is most uncertain about the human's response; however, they do not consider how easy it will be for the human to answer! In this paper we explore an information gain formulation for optimally selecting questions that naturally account for the human's ability to answer. Our approach identifies questions that optimize the trade-off between robot and human uncertainty, and determines when these questions become redundant or costly. Simulations and a user study show our method not only produces easy questions, but also ultimately results in faster reward learning.},
	urldate = {2021-10-26},
	journal = {arXiv:1910.04365 [cs]},
	author = {Bıyık, Erdem and Palan, Malayandi and Landolfi, Nicholas C. and Losey, Dylan P. and Sadigh, Dorsa},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.04365},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Proceedings of the 3rd Conference on Robot Learning (CoRL), October 2019},
	file = {arXiv Fulltext PDF:/Users/cassidy/Zotero/storage/8LHA8DSN/Bıyık et al. - 2019 - Asking Easy Questions A User-Friendly Approach to.pdf:application/pdf;arXiv.org Snapshot:/Users/cassidy/Zotero/storage/4Q7NPUM5/1910.html:text/html},
}

@inproceedings{biyik_batch_2018,
	title = {Batch {Active} {Preference}-{Based} {Learning} of {Reward} {Functions}},
	url = {https://proceedings.mlr.press/v87/biyik18a.html},
	abstract = {Data generation and labeling are usually an expensive part of learning for robotics. While active learning methods are commonly used to tackle the former problem, preference-based learning is a concept that attempts to solve the latter by querying users with preference questions. In this paper, we will develop a new algorithm, batch active preference-based learning, that enables efficient learning of reward functions using as few data samples as possible while still having short query generation times. We introduce several approximations to the batch active learning problem, and provide theoretical guarantees for the convergence of our algorithms. Finally, we present our experimental results for a variety of robotics tasks in simulation. Our results suggest that our batch active learning algorithm requires only a few queries that are computed in a short amount of time. We then showcase our algorithm in a study to learn human users’ preferences.},
	language = {en},
	urldate = {2021-10-26},
	booktitle = {Proceedings of {The} 2nd {Conference} on {Robot} {Learning}},
	publisher = {PMLR},
	author = {Biyik, Erdem and Sadigh, Dorsa},
	month = oct,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {519--528},
	file = {Full Text PDF:/Users/cassidy/Zotero/storage/DYNB5PFE/Biyik and Sadigh - 2018 - Batch Active Preference-Based Learning of Reward F.pdf:application/pdf},
}
