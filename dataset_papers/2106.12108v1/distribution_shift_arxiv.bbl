\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar et~al.(2021)Abnar, Berg, Ghiasi, Dehghani, Kalchbrenner, and
  Sedghi]{abnar2021gradual}
Samira Abnar, Rianne van~den Berg, Golnaz Ghiasi, Mostafa Dehghani, Nal
  Kalchbrenner, and Hanie Sedghi.
\newblock Gradual domain adaptation in the wild: When intermediate
  distributions are absent.
\newblock \emph{arXiv preprint arXiv:2106.06080}, 2021.

\bibitem[Ahuja et~al.(2020)Ahuja, Wang, Dhurandhar, Shanmugam, and
  Varshney]{ahuja2020empirical}
Kartik Ahuja, Jun Wang, Amit Dhurandhar, Karthikeyan Shanmugam, and Kush~R
  Varshney.
\newblock Empirical or invariant risk minimization? a sample complexity
  perspective.
\newblock \emph{arXiv preprint arXiv:2010.16412}, 2020.

\bibitem[Albuquerque et~al.(2019)Albuquerque, Monteiro, Darvishi, Falk, and
  Mitliagkas]{albuquerque2019generalizing}
Isabela Albuquerque, João Monteiro, Mohammad Darvishi, Tiago~H. Falk, and
  Ioannis Mitliagkas.
\newblock Generalizing to unseen domains via distribution matching.
\newblock \emph{arXiv preprint arXiv:1911.00804}, 2019.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Baktashmotlagh et~al.(2013)Baktashmotlagh, Harandi, Lovell, and
  Salzmann]{baktashmotlagh2013unsupervised}
Mahsa Baktashmotlagh, Mehrtash~T Harandi, Brian~C Lovell, and Mathieu Salzmann.
\newblock Unsupervised domain adaptation by domain invariant projection.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 769--776, 2013.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine learning}, 79\penalty0 (1-2):\penalty0 151--175, 2010.

\bibitem[Blaker(2000)]{blaker2000minimax}
Helge Blaker.
\newblock Minimax estimation in linear regression under restrictions.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (1):\penalty0 35--55, 2000.

\bibitem[Cai et~al.(2021)Cai, Gao, Lee, and Lei]{cai2021theory}
Tianle Cai, Ruiqi Gao, Jason~D Lee, and Qi~Lei.
\newblock A theory of label propagation for subpopulation shift.
\newblock \emph{arXiv preprint arXiv:2102.11203}, 2021.

\bibitem[Chen et~al.(2021)Chen, Rosenfeld, Sellke, Ma, and Risteski]{Chen2021}
Yining Chen, Elan Rosenfeld, Mark Sellke, Tengyu Ma, and Andrej Risteski.
\newblock {Iterative Feature Matching: Toward Provable Domain Generalization
  with Logarithmic Environments}.
\newblock \emph{ArXiv e-prints}, abs/2106.09913, June 2021.
\newblock URL \url{http://arxiv.org/abs/2106.09913v1}.

\bibitem[Cortes and Mohri(2011)]{cortes2011domain}
Corinna Cortes and Mehryar Mohri.
\newblock Domain adaptation in regression.
\newblock In \emph{Algorithmic Learning Theory}, pages 308--323. Springer,
  2011.

\bibitem[Cortes and Mohri(2014)]{cortes2014domain}
Corinna Cortes and Mehryar Mohri.
\newblock Domain adaptation and sample bias correction theory and algorithm for
  regression.
\newblock \emph{Theoretical Computer Science}, 519:\penalty0 103--126, 2014.

\bibitem[Cortes et~al.(2010)Cortes, Mansour, and Mohri]{cortes2010learning}
Corinna Cortes, Yishay Mansour, and Mehryar Mohri.
\newblock Learning bounds for importance weighting.
\newblock In \emph{Advances in neural information processing systems}, pages
  442--450, 2010.

\bibitem[Cortes et~al.(2019)Cortes, Mohri, and Medina]{cortes2019adaptation}
Corinna Cortes, Mehryar Mohri, and Andr{\'e}s~Munoz Medina.
\newblock Adaptation based on generalized discrepancy.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 1--30, 2019.

\bibitem[David et~al.(2010)David, Lu, Luu, and P{\'a}l]{david2010impossibility}
Shai~Ben David, Tyler Lu, Teresa Luu, and D{\'a}vid P{\'a}l.
\newblock Impossibility theorems for domain adaptation.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pages 129--136, 2010.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[Donoho(1994)]{donoho1994statistical}
David~L Donoho.
\newblock Statistical estimation and optimal recovery.
\newblock \emph{The Annals of Statistics}, pages 238--270, 1994.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Ginosar et~al.(2015)Ginosar, Rakelly, Sachs, Yin, and
  Efros]{ginosar2015century}
Shiry Ginosar, Kate Rakelly, Sarah Sachs, Brian Yin, and Alexei~A Efros.
\newblock A century of portraits: A visual historical record of american high
  school yearbooks.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision Workshops}, pages 1--7, 2015.

\bibitem[Glorot et~al.(2011)Glorot, Bordes, and Bengio]{glorot2011domain}
Xavier Glorot, Antoine Bordes, and Yoshua Bengio.
\newblock Domain adaptation for large-scale sentiment classification: A deep
  learning approach.
\newblock In \emph{ICML}, 2011.

\bibitem[Gong et~al.(2012)Gong, Shi, Sha, and Grauman]{gong2012geodesic}
Boqing Gong, Yuan Shi, Fei Sha, and Kristen Grauman.
\newblock Geodesic flow kernel for unsupervised domain adaptation.
\newblock In \emph{2012 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 2066--2073. IEEE, 2012.

\bibitem[Gong et~al.(2013)Gong, Grauman, and Sha]{gong2013connecting}
Boqing Gong, Kristen Grauman, and Fei Sha.
\newblock Connecting the dots with landmarks: Discriminatively learning
  domain-invariant features for unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pages
  222--230, 2013.

\bibitem[Gopalan et~al.(2011)Gopalan, Li, and Chellappa]{gopalan2011domain}
Raghuraman Gopalan, Ruonan Li, and Rama Chellappa.
\newblock Domain adaptation for object recognition: An unsupervised approach.
\newblock In \emph{2011 international conference on computer vision}, pages
  999--1006. IEEE, 2011.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch\"{o}lkopf, and
  Smola]{gretton2012kernel}
Arthur Gretton, Karsten~M. Borgwardt, Malte~J. Rasch, Bernhard Sch\"{o}lkopf,
  and Alexander Smola.
\newblock A kernel two-sample test.
\newblock \emph{J. Mach. Learn. Res.}, 13\penalty0 (null):\penalty0 723–773,
  March 2012.
\newblock ISSN 1532-4435.

\bibitem[Hanneke and Kpotufe(2019)]{hanneke2019value}
Steve Hanneke and Samory Kpotufe.
\newblock On the value of target data in transfer learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9871--9881, 2019.

\bibitem[Heckman(1979)]{heckman1979sample}
James~J Heckman.
\newblock Sample selection bias as a specification error.
\newblock \emph{Econometrica: Journal of the econometric society}, pages
  153--161, 1979.

\bibitem[Huang et~al.(2006)Huang, Gretton, Borgwardt, Sch{\"o}lkopf, and
  Smola]{huang2006correcting}
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch{\"o}lkopf, and
  Alex Smola.
\newblock Correcting sample selection bias by unlabeled data.
\newblock \emph{Advances in neural information processing systems},
  19:\penalty0 601--608, 2006.

\bibitem[Huang et~al.(2007)Huang, Gretton, Borgwardt, Sch{\"o}lkopf, and
  Smola]{huang2007correcting}
Jiayuan Huang, Arthur Gretton, Karsten Borgwardt, Bernhard Sch{\"o}lkopf, and
  Alex~J Smola.
\newblock Correcting sample selection bias by unlabeled data.
\newblock In \emph{Advances in neural information processing systems}, pages
  601--608, 2007.

\bibitem[Jiang and Zhai(2007)]{jiang2007instance}
Jing Jiang and ChengXiang Zhai.
\newblock Instance weighting for domain adaptation in nlp.
\newblock In \emph{Proceedings of the 45th annual meeting of the association of
  computational linguistics}, pages 264--271, 2007.

\bibitem[Johnstone(2011)]{johnstone2011gaussian}
Iain~M Johnstone.
\newblock Gaussian estimation: Sequence and wavelet models.
\newblock \emph{Unpublished manuscript}, 2011.

\bibitem[Juditsky et~al.(2009)Juditsky, Nemirovski,
  et~al.]{juditsky2009nonparametric}
Anatoli~B Juditsky, Arkadi~S Nemirovski, et~al.
\newblock Nonparametric estimation by convex programming.
\newblock \emph{The Annals of Statistics}, 37\penalty0 (5A):\penalty0
  2278--2300, 2009.

\bibitem[Kalan et~al.(2020)Kalan, Fabian, Avestimehr, and
  Soltanolkotabi]{kalan2020minimax}
Seyed Mohammadreza~Mousavi Kalan, Zalan Fabian, A~Salman Avestimehr, and Mahdi
  Soltanolkotabi.
\newblock Minimax lower bounds for transfer learning with linear and one-hidden
  layer neural networks.
\newblock \emph{arXiv preprint arXiv:2006.10581}, 2020.

\bibitem[Kamath et~al.(2021)Kamath, Tangella, Sutherland, and
  Srebro]{pmlr-v130-kamath21a}
Pritish Kamath, Akilesh Tangella, Danica Sutherland, and Nathan Srebro.
\newblock Does invariant risk minimization capture invariance?
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, \emph{Proceedings of
  The 24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of \emph{Proceedings of Machine Learning Research}, pages
  4069--4077. PMLR, 13--15 Apr 2021.
\newblock URL \url{http://proceedings.mlr.press/v130/kamath21a.html}.

\bibitem[Kanamori et~al.(2009)Kanamori, Hido, and Sugiyama]{kanamori2009least}
Takafumi Kanamori, Shohei Hido, and Masashi Sugiyama.
\newblock A least-squares approach to direct importance estimation.
\newblock \emph{The Journal of Machine Learning Research}, 10:\penalty0
  1391--1445, 2009.

\bibitem[Kanamori et~al.(2011)Kanamori, Suzuki, and Sugiyama]{kanamori2011f}
Takafumi Kanamori, Taiji Suzuki, and Masashi Sugiyama.
\newblock $ f $-divergence estimation and two-sample homogeneity test under
  semiparametric density-ratio models.
\newblock \emph{IEEE transactions on information theory}, 58\penalty0
  (2):\penalty0 708--720, 2011.

\bibitem[Kumar et~al.(2020)Kumar, Ma, and Liang]{kumar2020understanding}
Ananya Kumar, Tengyu Ma, and Percy Liang.
\newblock Understanding self-training for gradual domain adaptation.
\newblock \emph{arXiv preprint arXiv:2002.11361}, 2020.

\bibitem[Li et~al.(2018)Li, Pan, Wang, and Kot]{li2018domain}
Haoliang Li, Sinno~Jialin Pan, Shiqi Wang, and Alex~C Kot.
\newblock Domain generalization with adversarial feature learning.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5400--5409, 2018.

\bibitem[Lin et~al.(2002)Lin, Lee, and Wahba]{lin2002support}
Yi~Lin, Yoonkyung Lee, and Grace Wahba.
\newblock Support vector machines for classification in nonstandard situations.
\newblock \emph{Machine learning}, 46\penalty0 (1):\penalty0 191--202, 2002.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{lipton2018detecting}
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{International conference on machine learning}, pages
  3122--3130. PMLR, 2018.

\bibitem[Long et~al.(2013)Long, Wang, Ding, Sun, and Yu]{long2013transfer}
Mingsheng Long, Jianmin Wang, Guiguang Ding, Jiaguang Sun, and Philip~S Yu.
\newblock Transfer feature learning with joint distribution adaptation.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2200--2207, 2013.

\bibitem[Long et~al.(2017)Long, Zhu, Wang, and Jordan]{long2017deep}
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael~I Jordan.
\newblock Deep transfer learning with joint adaptation networks.
\newblock In \emph{International conference on machine learning}, pages
  2208--2217. PMLR, 2017.

\bibitem[Lu et~al.(2020)Lu, Nott, Olson, Todeschini, Vahabi, Carmon, and
  Schmidt]{lu2020harder}
Shangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi,
  Yair Carmon, and Ludwig Schmidt.
\newblock Harder or different? a closer look at distribution shift in dataset
  reproduction.
\newblock In \emph{ICML Workshop on Uncertainty and Robustness in Deep
  Learning}, 2020.

\bibitem[Menon and Ong(2016)]{menon2016linking}
Aditya Menon and Cheng~Soon Ong.
\newblock Linking losses for density ratio and class-probability estimation.
\newblock In \emph{International Conference on Machine Learning}, pages
  304--313. PMLR, 2016.

\bibitem[Murphy(2012)]{murphy2012machine}
Kevin~P Murphy.
\newblock \emph{Machine learning: a probabilistic perspective}.
\newblock MIT press, 2012.

\bibitem[Pan and Yang(2009)]{pan2009survey}
Sinno~Jialin Pan and Qiang Yang.
\newblock A survey on transfer learning.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  22\penalty0 (10):\penalty0 1345--1359, 2009.

\bibitem[Pan et~al.(2010)Pan, Tsang, Kwok, and Yang]{pan2010domain}
Sinno~Jialin Pan, Ivor~W Tsang, James~T Kwok, and Qiang Yang.
\newblock Domain adaptation via transfer component analysis.
\newblock \emph{IEEE Transactions on Neural Networks}, 22\penalty0
  (2):\penalty0 199--210, 2010.

\bibitem[Quionero-Candela et~al.(2009)Quionero-Candela, Sugiyama, Schwaighofer,
  and Lawrence]{quionero2009dataset}
Joaquin Quionero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D
  Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock 2009.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Rosenfeld et~al.(2021)Rosenfeld, Ravikumar, and
  Risteski]{rosenfeld2021the}
Elan Rosenfeld, Pradeep Ravikumar, and Andrej Risteski.
\newblock The risks of invariant risk minimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=BbNIbVPJ-42}.

\bibitem[Saerens et~al.(2002)Saerens, Latinne, and
  Decaestecker]{saerens2002adjusting}
Marco Saerens, Patrice Latinne, and Christine Decaestecker.
\newblock Adjusting the outputs of a classifier to new a priori probabilities:
  a simple procedure.
\newblock \emph{Neural computation}, 14\penalty0 (1):\penalty0 21--41, 2002.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock \emph{arXiv preprint arXiv:1911.08731}, 2019.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Srivastava et~al.(2020)Srivastava, Hashimoto, and
  Liang]{srivastava2020robustness}
Megha Srivastava, Tatsunori Hashimoto, and Percy Liang.
\newblock Robustness to spurious correlations via human annotations.
\newblock \emph{arXiv preprint arXiv:2007.06661}, 2020.

\bibitem[Storkey(2009)]{storkey2009training}
Amos Storkey.
\newblock When training and test sets are different: characterizing learning
  transfer.
\newblock \emph{Dataset shift in machine learning}, pages 3--28, 2009.

\bibitem[Sugiyama et~al.(2008)Sugiyama, Suzuki, Nakajima, Kashima, von
  B{\"u}nau, and Kawanabe]{sugiyama2008direct}
Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima, Hisashi Kashima, Paul von
  B{\"u}nau, and Motoaki Kawanabe.
\newblock Direct importance estimation for covariate shift adaptation.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  60\penalty0 (4):\penalty0 699--746, 2008.

\bibitem[Sugiyama et~al.(2012)Sugiyama, Suzuki, and
  Kanamori]{sugiyama2012density}
Masashi Sugiyama, Taiji Suzuki, and Takafumi Kanamori.
\newblock Density-ratio matching under the bregman divergence: a unified
  framework of density-ratio estimation.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  64\penalty0 (5):\penalty0 1009--1044, 2012.

\bibitem[Sun and Saenko(2016)]{sun2016deep}
Baochen Sun and Kate Saenko.
\newblock Deep coral: Correlation alignment for deep domain adaptation.
\newblock In \emph{European conference on computer vision}, pages 443--450.
  Springer, 2016.

\bibitem[Sun et~al.(2016)Sun, Qu, and Wright]{sun2016geometric}
Ju~Sun, Qing Qu, and John Wright.
\newblock A geometric analysis of phase retrieval.
\newblock In \emph{Information Theory (ISIT), 2016 IEEE International Symposium
  on}, pages 2379--2383. IEEE, 2016.

\bibitem[Sun et~al.(2011)Sun, Chattopadhyay, Panchanathan, and Ye]{sun2011two}
Qian Sun, Rita Chattopadhyay, Sethuraman Panchanathan, and Jieping Ye.
\newblock A two-stage weighting framework for multi-source domain adaptation.
\newblock In \emph{Advances in neural information processing systems}, pages
  505--513, 2011.

\bibitem[Tachet~des Combes et~al.(2020)Tachet~des Combes, Zhao, Wang, and
  Gordon]{tachet2020domain}
Remi Tachet~des Combes, Han Zhao, Yu-Xiang Wang, and Geoffrey~J Gordon.
\newblock Domain adaptation with conditional distribution matching and
  generalized label shift.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 19276--19289. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/dfbfa7ddcfffeb581f50edcf9a0204bb-Paper.pdf}.

\bibitem[Uehara et~al.(2016)Uehara, Sato, Suzuki, Nakayama, and
  Matsuo]{uehara2016generative}
Masatoshi Uehara, Issei Sato, Masahiro Suzuki, Kotaro Nakayama, and Yutaka
  Matsuo.
\newblock Generative adversarial nets from a density ratio estimation
  perspective.
\newblock \emph{arXiv preprint arXiv:1610.02920}, 2016.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wang and Schneider(2014)]{wang2014flexible}
Xuezhi Wang and Jeff Schneider.
\newblock Flexible transfer learning under support and model shift.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1898--1906, 2014.

\bibitem[Wang and Schneider(2015)]{wang2015generalization}
Xuezhi Wang and Jeff~G Schneider.
\newblock Generalization bounds for transfer learning under model shift.
\newblock In \emph{UAI}, pages 922--931, 2015.

\bibitem[Wang et~al.(2014)Wang, Huang, and Schneider]{wang2014active}
Xuezhi Wang, Tzu-Kuo Huang, and Jeff Schneider.
\newblock Active transfer learning under model shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  1305--1313, 2014.

\bibitem[Weiss et~al.(2016)Weiss, Khoshgoftaar, and Wang]{weiss2016survey}
Karl Weiss, Taghi~M Khoshgoftaar, and DingDing Wang.
\newblock A survey of transfer learning.
\newblock \emph{Journal of Big data}, 3\penalty0 (1):\penalty0 9, 2016.

\bibitem[Wu et~al.(2019)Wu, Winston, Kaushik, and Lipton]{wu2019domain}
Yifan Wu, Ezra Winston, Divyansh Kaushik, and Zachary Lipton.
\newblock Domain adaptation with asymmetrically-relaxed distribution alignment.
\newblock In \emph{International Conference on Machine Learning}, pages
  6872--6881. PMLR, 2019.

\bibitem[Zadrozny(2004)]{zadrozny2004learning}
Bianca Zadrozny.
\newblock Learning and evaluating classifiers under sample selection bias.
\newblock In \emph{Proceedings of the twenty-first international conference on
  Machine learning}, page 114, 2004.

\bibitem[Zhang et~al.(2013)Zhang, Sch{\"o}lkopf, Muandet, and
  Wang]{zhang2013domain}
Kun Zhang, Bernhard Sch{\"o}lkopf, Krikamol Muandet, and Zhikun Wang.
\newblock Domain adaptation under target and conditional shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  819--827, 2013.

\bibitem[Zhao et~al.(2019)Zhao, Combes, Zhang, and Gordon]{zhao2019learning}
Han Zhao, Remi Tachet~des Combes, Kun Zhang, and Geoffrey~J Gordon.
\newblock On learning invariant representation for domain adaptation.
\newblock \emph{arXiv preprint arXiv:1901.09453}, 2019.

\end{thebibliography}
