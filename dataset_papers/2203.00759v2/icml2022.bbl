\begin{thebibliography}{34}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2021)Aghajanyan, Gupta, Shrivastava, Chen,
  Zettlemoyer, and Gupta]{aghajanyan2021muppet}
Aghajanyan, A., Gupta, A., Shrivastava, A., Chen, X., Zettlemoyer, L., and
  Gupta, S.
\newblock Muppet: Massive multi-task representations with pre-finetuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  5799--5811, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.468}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.468}.

\bibitem[Aribandi et~al.(2021)Aribandi, Tay, Schuster, Rao, Zheng, Mehta,
  Zhuang, Tran, Bahri, Ni, Gupta, Hui, Ruder, and Metzler]{aribandi2021ext5}
Aribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H.~S., Mehta, S.~V.,
  Zhuang, H., Tran, V.~Q., Bahri, D., Ni, J., Gupta, J., Hui, K., Ruder, S.,
  and Metzler, D.
\newblock Ext5: Towards extreme multi-task scaling for transfer learning, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.~F., and Lin,
  H. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  1877--1901. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf}.

\bibitem[Dehghani et~al.(2021)Dehghani, Arnab, Beyer, Vaswani, and
  Tay]{dehghani2021efficiency}
Dehghani, M., Arnab, A., Beyer, L., Vaswani, A., and Tay, Y.
\newblock The efficiency misnomer.
\newblock \emph{arXiv preprint arXiv:2110.12894}, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1423}.
\newblock URL \url{https://aclanthology.org/N19-1423}.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{ha2017HyperNetwork}
Ha, D., Dai, A.~M., and Le, Q.~V.
\newblock Hypernetworks.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=rkpACe1lx}.

\bibitem[Hambardzumyan et~al.(2021)Hambardzumyan, Khachatrian, and
  May]{hambardzumyan2021warp}
Hambardzumyan, K., Khachatrian, H., and May, J.
\newblock {WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pp.\  4921--4933,
  Online, August 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.acl-long.381}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.381}.

\bibitem[He et~al.(2021)He, Zhou, Ma, Berg-Kirkpatrick, and
  Neubig]{he2021unified}
He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G.
\newblock Towards a unified view of parameter-efficient transfer learning,
  2021.

\bibitem[Houlsby et~al.(2019{\natexlab{a}})Houlsby, Giurgiu, Jastrzebski,
  Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{houlsby2019adapter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In Chaudhuri, K. and Salakhutdinov, R. (eds.), \emph{Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2790--2799. PMLR,
  09--15 Jun 2019{\natexlab{a}}.
\newblock URL \url{https://proceedings.mlr.press/v97/houlsby19a.html}.

\bibitem[Houlsby et~al.(2019{\natexlab{b}})Houlsby, Giurgiu, Jastrzebski,
  Morrone, De~Laroussilhe, Gesmundo, Attariyan, and
  Gelly]{houlsby2019parameter}
Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De~Laroussilhe, Q.,
  Gesmundo, A., Attariyan, M., and Gelly, S.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2790--2799. PMLR, 2019{\natexlab{b}}.

\bibitem[Karimi~Mahabadi et~al.(2021)Karimi~Mahabadi, Ruder, Dehghani, and
  Henderson]{karimi-mahabadi-etal-2021-parameter}
Karimi~Mahabadi, R., Ruder, S., Dehghani, M., and Henderson, J.
\newblock Parameter-efficient multi-task fine-tuning for transformers via
  shared hypernetworks.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, August 2021.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Lester, B., Al-Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Lewis et~al.(2020)Lewis, Liu, Goyal, Ghazvininejad, Mohamed, Levy,
  Stoyanov, and Zettlemoyer]{lewis2020bart}
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,
  Stoyanov, V., and Zettlemoyer, L.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  7871--7880, 2020.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Li, X.~L. and Liang, P.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, He, Chen, and Gao]{liu2019MDNN}
Liu, X., He, P., Chen, W., and Gao, J.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4487--4496, Florence, Italy, July
  2019{\natexlab{a}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P19-1441}.
\newblock URL \url{https://aclanthology.org/P19-1441}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, He, Chen, and Gao]{liu2019multi}
Liu, X., He, P., Chen, W., and Gao, J.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  4487--4496, 2019{\natexlab{b}}.

\bibitem[Liu et~al.(2021)Liu, Zheng, Du, Ding, Qian, Yang, and
  Tang]{liu2021gpt}
Liu, X., Zheng, Y., Du, Z., Ding, M., Qian, Y., Yang, Z., and Tang, J.
\newblock Gpt understands, too, 2021.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Sanh et~al.(2021)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai,
  Chaffin, Stiegler, Scao, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla,
  Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey,
  Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, Fevry, Fries, Teehan,
  Biderman, Gao, Bers, Wolf, and Rush]{sanh2021multitask}
Sanh, V., Webson, A., Raffel, C., Bach, S.~H., Sutawika, L., Alyafeai, Z.,
  Chaffin, A., Stiegler, A., Scao, T.~L., Raja, A., Dey, M., Bari, M.~S., Xu,
  C., Thakker, U., Sharma, S.~S., Szczechla, E., Kim, T., Chhablani, G., Nayak,
  N., Datta, D., Chang, J., Jiang, M. T.-J., Wang, H., Manica, M., Shen, S.,
  Yong, Z.~X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma,
  A., Santilli, A., Fevry, T., Fries, J.~A., Teehan, R., Biderman, S., Gao, L.,
  Bers, T., Wolf, T., and Rush, A.~M.
\newblock Multitask prompted training enables zero-shot task generalization,
  2021.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{shazeer2018adafactor}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4596--4604. PMLR, 2018.

\bibitem[Shazeer et~al.(2018)Shazeer, Cheng, Parmar, Tran, Vaswani,
  Koanantakool, Hawkins, Lee, Hong, Young, et~al.]{shazeer2018mesh}
Shazeer, N., Cheng, Y., Parmar, N., Tran, D., Vaswani, A., Koanantakool, P.,
  Hawkins, P., Lee, H., Hong, M., Young, C., et~al.
\newblock Mesh-tensorflow: Deep learning for supercomputers.
\newblock \emph{arXiv preprint arXiv:1811.02084}, 2018.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Shin, T., Razeghi, Y., Logan~IV, R.~L., Wallace, E., and Singh, S.
\newblock {A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with
  {A}utomatically {G}enerated {P}rompts.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pp.\  4222--4235, Online, November
  2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-main.346}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-main.346}.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Lample, Jegou, and
  Joulin]{sukhbaatar2019augmenting}
Sukhbaatar, S., Grave, E., Lample, G., Jegou, H., and Joulin, A.
\newblock Augmenting self-attention with persistent memory.
\newblock \emph{arXiv preprint arXiv:1907.01470}, 2019.

\bibitem[Tay et~al.(2020)Tay, Zhao, Bahri, Metzler, and Juan]{tay2020hypergrid}
Tay, Y., Zhao, Z., Bahri, D., Metzler, D., and Juan, D.-C.
\newblock Hypergrid: Efficient multi-task transformers with grid-wise
  decomposable hyper projections.
\newblock \emph{arXiv preprint arXiv:2007.05891}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[von Oswald et~al.(2019)von Oswald, Henning, Sacramento, and
  Grewe]{von2019continual}
von Oswald, J., Henning, C., Sacramento, J., and Grewe, B.~F.
\newblock Continual learning with hypernetworks.
\newblock \emph{arXiv preprint arXiv:1906.00695}, 2019.

\bibitem[Wang et~al.(2018)Wang, Singh, Michael, Hill, Levy, and
  Bowman]{wang2018glue}
Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S.~R.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock \emph{arXiv preprint arXiv:1804.07461}, 2018.

\bibitem[Wang et~al.(2019)Wang, Pruksachatkun, Nangia, Singh, Michael, Hill,
  Levy, and Bowman]{wang2019superglue}
Wang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F.,
  Levy, O., and Bowman, S.~R.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock \emph{arXiv preprint arXiv:1905.00537}, 2019.

\bibitem[Wang et~al.(2020)Wang, Zhao, Dai, Fifty, Lin, Hong, and
  Chi]{wang2020small}
Wang, Y., Zhao, Z., Dai, B., Fifty, C., Lin, D., Hong, L., and Chi, E.~H.
\newblock Small towers make big differences, 2020.

\bibitem[Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021flan}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,
  A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners, 2021.

\bibitem[Wu et~al.(2020)Wu, Zhang, and Ré]{Wu2020Understanding}
Wu, S., Zhang, H.~R., and Ré, C.
\newblock Understanding and improving information transfer in multi-task
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SylzhkBtDB}.

\bibitem[Yosinski et~al.(2014)Yosinski, Clune, Bengio, and
  Lipson]{yosinski2014transferable}
Yosinski, J., Clune, J., Bengio, Y., and Lipson, H.
\newblock How transferable are features in deep neural networks?
\newblock In \emph{Advances in neural information processing systems}, pp.\
  3320--3328, 2014.

\bibitem[Zaken et~al.(2021)Zaken, Ravfogel, and Goldberg]{zaken2021bitfit}
Zaken, E.~B., Ravfogel, S., and Goldberg, Y.
\newblock Bitfit: Simple parameter-efficient fine-tuning for transformer-based
  masked language-models.
\newblock \emph{arXiv preprint arXiv:2106.10199}, 2021.

\end{thebibliography}
