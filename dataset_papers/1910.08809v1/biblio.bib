% Encoding: UTF-8

@book{sutton2018reinforcement,
	Author = {Sutton, Richard S and Barto, Andrew G},
	Date-Added = {2019-01-04 13:31:11 +0100},
	Date-Modified = {2019-01-04 13:31:11 +0100},
	Publisher = {MIT press},
	Title = {Reinforcement learning: An introduction},
	Year = {2018}}

@article{mnih_human-level_2015,
	title = {Human-level control through deep reinforcement learning},
	volume = {518},
	pages = {529},
	number = {7540},
	journaltitle = {Nature},
	author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg},
	year = {2015},
}

@article{silver_mastering_2017,
	title = {Mastering the game of go without human knowledge},
	volume = {550},
	pages = {354},
	number = {7676},
	journaltitle = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian},
	year = {2017},
}


@inproceedings{dolgov_computationally-efficient_2005,
	Address = {Utrecht, The Netherlands},
	Author = {Dolgov, Dmitri A. and Durfee, Edmund H.},
	Booktitle = {Proceedings of the {Fourth} {International} {Joint} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} ({AAMAS}-05)},
	Date-Added = {2018-04-17 07:32:36 +0000},
	Date-Modified = {2018-04-17 07:32:36 +0000},
	File = {aamas05-UM-dolgov-final.pdf:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/XNGSPCE4/aamas05-UM-dolgov-final.pdf:application/pdf},
	Month = jul,
	Pages = {657--664},
	Title = {Computationally-{Efficient} {Combinatorial} {Auctions} for {Resource} {Allocation} in {Weakly}-{Coupled} {MDPs}},
	Year = {2005}}

@inproceedings{guestrin_coordinated_2002,
	Author = {Guestrin, Carlos and Lagoudakis, Michail and Parr, Ronald},
	Booktitle = {{ICML}},
	Date-Added = {2018-04-17 07:32:36 +0000},
	Date-Modified = {2018-04-17 07:32:36 +0000},
	File = {icml02.pdf:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/8I5RG5IH/icml02.pdf:application/pdf},
	Pages = {227--234},
	Title = {Coordinated reinforcement learning},
	Volume = {2},
	Year = {2002}}

@article{hosseini_coordinated_2014,
	Author = {Hosseini, Hadi and Hoey, Jesse and Cohen, Robin},
	Date-Added = {2018-04-17 07:32:36 +0000},
	Date-Modified = {2018-04-17 07:32:36 +0000},
	File = {1407.1584.pdf:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/XST374QX/1407.1584.pdf:application/pdf},
	Journal = {CoRR},
	Title = {A {Coordinated} {MDP} {Approach} to {Multi}-{Agent} {Planning} for {Resource} {Allocation}, with {Applications} to {Healthcare}},
	Volume = {abs/1407.1584},
	Year = {2014},
	Bdsk-Url-1 = {http://arxiv.org/abs/1407.1584}}

@inproceedings{meuleau1998solving,
	Abstract = {We present a technique for computing approximately optimal solutions to stochastic resource allocation problems modeled as Markov decision processes (MDPS). We exploit two key properties to avoid explicitly enumerating the very large state and action spaces associated with these problems. First, the problems are composed of multiple tasks whose utilities are independent. Second, the actions taken with respect to (or resources allocated to) a task do not influence the status of any other task. We can therefore view each task as an MDP. However, these MDPS are weakly coupled by resource constraints: actions selected for one MDP restrict the actions available to others. We describe heuristic techniques for dealing with several classes of constraints that use the solutions for individual MDPS to construct an approximate global solution. We demonstrate this technique on problems involving thousands of tasks, approximating the solution to problems that are far beyond the reach of standard methods.},
	Author = {Meuleau, Nicolas and Hauskrecht, Milos and Kim, Kee-Eung and Peshkin, Leonid and Kaelbling, Leslie Pack and Dean, Thomas and Boutilier, Craig},
	Date-Added = {2018-04-17 07:32:36 +0000},
	Date-Modified = {2018-04-17 07:32:55 +0000},
	File = {MeuleauetalAAAI-98.pdf:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/N23JUA3D/MeuleauetalAAAI-98.pdf:application/pdf;SCOPUS Snapshot:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/6LXY9HJZ/display.html:text/html},
	Pages = {165--172},
	Title = {Solving very large weakly coupled {Markov} decision processes},
	Year = {1998}}

@inproceedings{singh_how_1998,
	Author = {Singh, Satinder P. and Cohn, David},
	Booktitle = {Advances in neural information processing systems},
	Date-Added = {2018-04-17 07:32:36 +0000},
	Date-Modified = {2018-04-17 07:32:36 +0000},
	File = {1420-how-to-dynamically-merge-markov-decision-processes.pdf:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/WDRTTTU3/1420-how-to-dynamically-merge-markov-decision-processes.pdf:application/pdf},
	Pages = {1057--1063},
	Title = {How to dynamically merge {Markov} decision processes},
	Year = {1998}}

@inproceedings{tesauro_online_2005,
	Author = {Tesauro, Gerald},
	Booktitle = {{AAAI}},
	Date-Added = {2018-04-17 07:32:36 +0000},
	Date-Modified = {2018-04-17 07:32:36 +0000},
	File = {AAAI05-140.pdf:/Users/lazaric/Library/Application Support/Zotero/Profiles/zdca9jfu.default/zotero/storage/96UV3XF8/AAAI05-140.pdf:application/pdf},
	Pages = {886--891},
	Title = {Online resource allocation using decompositional reinforcement learning},
	Volume = {5},
	Year = {2005}}

@inproceedings{proper2009solving,
	Address = {Richland, SC},
	Author = {Proper, Scott and Tadepalli, Prasad},
	Booktitle = {Proceedings of {The} 8th {International} {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems} - {Volume} 1},
	Date-Added = {2018-04-17 07:30:50 +0000},
	Date-Modified = {2018-04-17 07:30:53 +0000},
	Isbn = {978-0-9817381-6-1},
	Keywords = {Alessandro, assignment problem, coordination graphs, Markov decision processes, reinforcement learning},
	Pages = {681--688},
	Publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
	Series = {{AAMAS} '09},
	Title = {Solving {Multiagent} {Assignment} {Markov} {Decision} {Processes}},
	Year = {2009},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1558013.1558107}}

@inproceedings{maillard2013optimal,
	Address = {Atlanta, USA, United States},
	Author = {Maillard, Odalric-Ambrym and Nguyen, Phuong and Ortner, Ronald and Ryabko, Daniil},
	Booktitle = {{ICML - 30th International Conference on Machine Learning}},
	Date-Added = {2017-07-07 10:23:27 +0000},
	Date-Modified = {2017-07-07 10:23:29 +0000},
	Hal_Id = {hal-00778586},
	Hal_Version = {v1},
	Pages = {543-551},
	Pdf = {https://hal.inria.fr/hal-00778586/file/icml1_iblb_cr-corrected.pdf},
	Series = {JMLR W\&CP},
	Title = {{Optimal Regret Bounds for Selecting the State Representation in Reinforcement Learning}},
	Volume = {28(1)},
	Year = {2013},
	Bdsk-Url-1 = {https://hal.inria.fr/hal-00778586}}

@inproceedings{nguyen2013online,
	Acmid = {3042875},
	Author = {Nguyen, Trung Thanh and Li, Zhuoru and Silander, Tomi and Leong, Tze-Yun},
	Booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	Date-Added = {2017-07-03 09:46:19 +0000},
	Date-Modified = {2017-07-03 09:46:22 +0000},
	Location = {Atlanta, GA, USA},
	Pages = {I-498--I-506},
	Publisher = {JMLR.org},
	Series = {ICML'13},
	Title = {Online Feature Selection for Model-based Reinforcement Learning},
	Year = {2013},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3042817.3042875}}

@inproceedings{guestrin2003generalizing,
	Acmid = {1630803},
	Address = {San Francisco, CA, USA},
	Author = {Guestrin, Carlos and Koller, Daphne and Gearhart, Chris and Kanodia, Neal},
	Booktitle = {Proceedings of the 18th International Joint Conference on Artificial Intelligence},
	Date-Added = {2017-07-03 09:45:29 +0000},
	Date-Modified = {2017-07-03 09:45:31 +0000},
	Location = {Acapulco, Mexico},
	Numpages = {8},
	Pages = {1003--1010},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Series = {IJCAI'03},
	Title = {Generalizing Plans to New Environments in Relational MDPs},
	Year = {2003},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1630659.1630803}}

@inproceedings{diuk2008object,
  title={An object-oriented representation for efficient reinforcement learning},
  author={Diuk, Carlos and Cohen, Andre and Littman, Michael L},
  booktitle={Proceedings of the 25th international conference on Machine learning},
  pages={240--247},
  year={2008},
  organization={ACM}
}


@misc{hallak2015off-policy,
	Author = {Assaf Hallak and Fran{\c c}ois Schnitzler and Timothy Mann and Shie Mannor},
	Date-Added = {2017-07-01 16:53:38 +0000},
	Date-Modified = {2017-07-01 16:53:40 +0000},
	Eprint = {arXiv:1502.03255},
	Title = {Off-policy evaluation for MDPs with unknown structure},
	Year = {2015}}

@article{guo2017sample,
	Author = {Zhaohan Daniel Guo and Emma Brunskill},
	Bibsource = {dblp computer science bibliography, http://dblp.org},
	Biburl = {http://dblp.uni-trier.de/rec/bib/journals/corr/GuoB17},
	Date-Added = {2017-07-01 16:41:44 +0000},
	Date-Modified = {2017-07-01 16:41:45 +0000},
	Journal = {CoRR},
	Timestamp = {Wed, 07 Jun 2017 14:41:49 +0200},
	Title = {Sample Efficient Feature Selection for Factored MDPs},
	Volume = {abs/1703.03454},
	Year = {2017},
	Bdsk-Url-1 = {http://arxiv.org/abs/1703.03454}}

@inproceedings{chakraborty2011structure,
	Acmid = {3104575},
	Address = {USA},
	Author = {Chakraborty, Doran and Stone, Peter},
	Booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	Date-Added = {2017-07-01 16:33:57 +0000},
	Date-Modified = {2017-07-01 16:33:58 +0000},
	Isbn = {978-1-4503-0619-5},
	Location = {Bellevue, Washington, USA},
	Numpages = {8},
	Pages = {737--744},
	Publisher = {Omnipress},
	Series = {ICML'11},
	Title = {Structure Learning in Ergodic Factored MDPs Without Knowledge of the Transition Function's In-degree},
	Year = {2011},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=3104482.3104575}}

@inproceedings{diuk2009the-adaptive,
	Acmid = {1553406},
	Address = {New York, NY, USA},
	Author = {Diuk, Carlos and Li, Lihong and Leffler, Bethany R.},
	Booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
	Date-Added = {2017-07-01 16:29:58 +0000},
	Date-Modified = {2017-07-01 16:29:58 +0000},
	Doi = {10.1145/1553374.1553406},
	Isbn = {978-1-60558-516-1},
	Location = {Montreal, Quebec, Canada},
	Numpages = {8},
	Pages = {249--256},
	Publisher = {ACM},
	Series = {ICML '09},
	Title = {The Adaptive K-meteorologists Problem and Its Application to Structure Learning and Feature Selection in Reinforcement Learning},
	Year = {2009},
	Bdsk-Url-1 = {http://doi.acm.org/10.1145/1553374.1553406},
	Bdsk-Url-2 = {http://dx.doi.org/10.1145/1553374.1553406}}

@inproceedings{strehl2007efficient,
	Acmid = {1619749},
	Author = {Strehl, Alexander L. and Diuk, Carlos and Littman, Michael L.},
	Booktitle = {Proceedings of the 22Nd National Conference on Artificial Intelligence - Volume 1},
	Date-Added = {2017-06-30 16:21:34 +0000},
	Date-Modified = {2017-06-30 16:21:35 +0000},
	Isbn = {978-1-57735-323-2},
	Location = {Vancouver, British Columbia, Canada},
	Numpages = {6},
	Pages = {645--650},
	Publisher = {AAAI Press},
	Series = {AAAI'07},
	Title = {Efficient Structure Learning in Factored-state MDPs},
	Year = {2007},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1619645.1619749}}

@article{jaksch2010near-optimal,
	Author = {Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
	Date-Added = {2017-06-30 16:17:52 +0000},
	Date-Modified = {2017-06-30 16:17:53 +0000},
	Journal = {J. Mach. Learn. Res.},
	Month = aug,
	Pages = {1563--1600},
	Title = {Near-optimal Regret Bounds for Reinforcement Learning},
	Volume = {11},
	Year = {2010},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1756006.1859902}}

@incollection{osband2014near-optimal,
	Author = {Ian Osband and Benjamin V. Roy},
	Booktitle = {Advances in Neural Information Processing Systems 27},
	Date-Added = {2017-06-30 16:16:42 +0000},
	Date-Modified = {2017-06-30 16:16:44 +0000},
	Editor = {Z. Ghahramani and M. Welling and C. Cortes and N.d. Lawrence and K.q. Weinberger},
	Pages = {604--612},
	Publisher = {Curran Associates, Inc.},
	Title = {Near-optimal Reinforcement Learning in Factored MDPs},
	Year = {2014},
	Bdsk-Url-1 = {http://papers.nips.cc/paper/5445-near-optimal-reinforcement-learning-in-factored-mdps.pdf}}

@inproceedings{kearns1999efficient,
	Acmid = {1624325},
	Address = {San Francisco, CA, USA},
	Author = {Kearns, Michael and Koller, Daphne},
	Booktitle = {Proceedings of the 16th International Joint Conference on Artificial Intelligence - Volume 2},
	Date-Added = {2017-06-30 16:13:21 +0000},
	Date-Modified = {2017-06-30 16:15:32 +0000},
	Location = {Stockholm, Sweden},
	Numpages = {8},
	Pages = {740--747},
	Publisher = {Morgan Kaufmann Publishers Inc.},
	Series = {IJCAI'99},
	Title = {Efficient Reinforcement Learning in Factored MDPs},
	Url = {http://dl.acm.org/citation.cfm?id=1624312.1624325},
	Year = {1999},
	Bdsk-Url-1 = {http://dl.acm.org/citation.cfm?id=1624312.1624325}}
    
    
 @techreport{GleixnerEiflerGallyetal.2017,
  author      = {Ambros Gleixner and Leon Eifler and Tristan Gally and Gerald Gamrath and Patrick Gemander and Robert Lion Gottwald and Gregor Hendel and Christopher Hojny and Thorsten Koch and Matthias Miltenberger and Benjamin M{\"u}ller and Marc E. Pfetsch and Christian Puchert and Daniel Rehfeldt and Franziska Schl{\"o}sser and Felipe Serrano and Yuji Shinano and Jan Merlin Viernickel and Stefan Vigerske and Dieter Weninger and Jonas T. Witt and Jakob Witzig},
  title       = {The SCIP Optimization Suite 5.0},
  institution = {ZIB},
  address     = {Takustr. 7, 14195 Berlin},
  number      = {17-61},
  language    = {eng},
  urn         = {urn:nbn:de:0297-zib-66297},
  year        = {2017}
}

@article{frank1956algorithm,
  title={An algorithm for quadratic programming},
    author={Frank, Marguerite and Wolfe, Philip},
  journal={Naval Research Logistics (NRL)},
  volume={3},
  number={1-2},
  pages={95--110},
  year={1956},
  publisher={Wiley Online Library}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year={2016}
}

@article{tian2017elf, 
  title={ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games},
  author={Yuandong Tian and Qucheng Gong and Wenling Shang and Yuxin Wu and C. Lawrence Zitnick},
  journal={Advances in Neural Information Processing Systems (NIPS)},
  year={2017}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{wang2018non,
  title={Non-local neural networks},
  author={Wang, Xiaolong and Girshick, Ross and Gupta, Abhinav and He, Kaiming},
  booktitle={The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2018}
}
@misc{Glop,
  title={The Glop Linear Solver},
  howpublished = "\url{https://developers.google.com/optimization/lp/glop}"
}

@article{busoniu2008comprehensive,
  title={A comprehensive survey of multiagent reinforcement learning},
  author={Busoniu, Lucian and Babuska, Robert and De Schutter, Bart},
  journal={IEEE Transactions on Systems, Man, And Cybernetics-Part C: Applications and Reviews, 38 (2), 2008},
  year={2008},
  publisher={IEEE}
}

@article{sanner2012practical,
  title={Practical linear value-approximation techniques for first-order MDPs},
  author={Sanner, Scott and Boutilier, Craig},
  journal={arXiv preprint arXiv:1206.6879},
  year={2012}
}

@article{zambaldi2018relational,
  title={Relational Deep Reinforcement Learning},
  author={Zambaldi, Vinicius and Raposo, David and Santoro, Adam and Bapst, Victor and Li, Yujia and Babuschkin, Igor and Tuyls, Karl and Reichert, David and Lillicrap, Timothy and Lockhart, Edward and others},
  journal={arXiv preprint arXiv:1806.01830},
  year={2018}
}

@inproceedings{sukhbaatar2016learning,
  title={Learning multiagent communication with backpropagation},
  author={Sukhbaatar, Sainbayar and Fergus, Rob and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2244--2252},
  year={2016}
}

@article{usunier2016episodic,
  title={Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks},
  author={Usunier, Nicolas and Synnaeve, Gabriel and Lin, Zeming and Chintala, Soumith},
  journal={arXiv preprint arXiv:1609.02993},
  year={2016}
}

@incollection{foerster_learning_2016,
	title = {Learning to Communicate with Deep Multi-Agent Reinforcement Learning},
	pages = {2137--2145},
	booktitle = {Advances in Neural Information Processing Systems 29},
	publisher = {Curran Associates, Inc.},
	author = {Foerster, Jakob and Assael, Ioannis Alexandros and de Freitas, Nando and Whiteson, Shimon},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	urldate = {2018-03-05},
	year = {2016},
	file = {NIPS Full Text PDF:/Users/alcinos/Zotero/storage/PUSM9NHT/Foerster et al. - 2016 - Learning to Communicate with Deep Multi-Agent Rein.pdf:application/pdf;NIPS Snapshort:/Users/alcinos/Zotero/storage/K6MRYHF7/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.html:text/html}
}%

@inproceedings{foerster2018counterfactual,
  title={Counterfactual multi-agent policy gradients},
  author={Foerster, Jakob N and Farquhar, Gregory and Afouras, Triantafyllos and Nardelli, Nantas and Whiteson, Shimon},
  booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
  year={2018}
}

@article{rashid2018qmix,
  title={QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning},
  author={Rashid, Tabish and Samvelyan, Mikayel and de Witt, Christian Schroeder and Farquhar, Gregory and Foerster, Jakob and Whiteson, Shimon},
  journal={arXiv preprint arXiv:1803.11485},
  year={2018}
}

@article{wawrzynski2015control,
  title={Control policy with autocorrelated noise in reinforcement learning for robotics},
  author={Wawrzynski, Pawel},
  journal={International Journal of Machine Learning and Computing},
  volume={5},
  number={2},
  pages={91},
  year={2015},
  publisher={IACSIT Press}
}


@inproceedings{churchill_analysis_2017,
	title = {An Analysis of Model-Based Heuristic Search Techniques for {StarCraft} Combat Scenarios},
	publisher = {{AAAI} Publications, Thirteenth Artificial Intelligence and Interactive â€¦},
	author = {Churchill, David and Lin, Zeming and Synnaeve, Gabriel},
	year = {2017},
	file = {Full Text:/Users/alcinos/Zotero/storage/IBARKD35/Churchill et al. - 2017 - An Analysis of Model-Based Heuristic Search Techni.pdf:application/pdf}
}

@inproceedings{churchill_fast_2012,
	title = {Fast Heuristic Search for {RTS} Game Combat Scenarios.},
	pages = {112--117},
	booktitle = {{AIIDE}},
	author = {Churchill, David and Saffidine, Abdallah and Buro, Michael},
	year = {2012}
}

@inproceedings{churchill_portfolio_2013,
	title = {Portfolio greedy search and simulation for large-scale combat in {StarCraft}},
	pages = {1--8},
	booktitle = {Computational Intelligence in Games ({CIG}), 2013 {IEEE} Conference on},
	publisher = {{IEEE}},
	author = {Churchill, David and Buro, Michael},
	year = {2013},
	file = {Full Text:/Users/alcinos/Zotero/storage/NSAPMWHQ/Churchill and Buro - 2013 - Portfolio greedy search and simulation for large-s.pdf:application/pdf;Snapshot:/Users/alcinos/Zotero/storage/3PQTYPSN/6633643.html:text/html}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{wilcoxon1945individual,
  title={Individual comparisons by ranking methods},
  author={Wilcoxon, Frank},
  journal={Biometrics bulletin},
  volume={1},
  number={6},
  pages={80--83},
  year={1945},
  publisher={JSTOR}
}

@article{yang2018mean,
  title={Mean Field Multi-Agent Reinforcement Learning},
  author={Yang, Yaodong and Luo, Rui and Li, Minne and Zhou, Ming and Zhang, Weinan and Wang, Jun},
  journal={arXiv preprint arXiv:1802.05438},
  year={2018}
}

@article{Jiang2018GraphCR,
  title={Graph Convolutional Reinforcement Learning for Multi-Agent Cooperation},
  author={Jiechuan Jiang and I{\~n}igo Fern{\'a}ndez del Amo and Zongqing Lu},
  journal={CoRR},
  year={2018},
  volume={abs/1810.09202}
}
@incollection{NIPS2018_7956,
title = {Learning Attentional Communication for Multi-Agent Cooperation},
author = {Jiang, Jiechuan and Lu, Zongqing},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {7265--7275},
year = {2018},
publisher = {Curran Associates, Inc.},
}
@incollection{NIPS2017_7217,
title = {Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments},
author = {Lowe, Ryan and WU, YI and Tamar, Aviv and Harb, Jean and Pieter Abbeel, OpenAI and Mordatch, Igor},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6379--6390},
year = {2017},
publisher = {Curran Associates, Inc.},
}
@inproceedings{lin18,
 author = {Lin, Kaixiang and Zhao, Renyu and Xu, Zhe and Zhou, Jiayu},
 title = {Efficient Large-Scale Fleet Management via Multi-Agent Deep Reinforcement Learning},
 booktitle = {Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \&\#38; Data Mining},
 series = {KDD '18},
 year = {2018},
 isbn = {978-1-4503-5552-0},
 location = {London, United Kingdom},
 pages = {1774--1783},
 numpages = {10},
 doi = {10.1145/3219819.3219993},
 acmid = {3219993},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {deep reinforcement learning, fleet management, multi-agent reinforcement learning},
} 


@inproceedings{
singh_individualized_2019,
title={Individualized Controlled Continuous Communication Model for Multiagent Cooperative and Competitive Tasks},
author={Amanpreet Singh and Tushar Jain and Sainbayar Sukhbaatar},
booktitle={International Conference on Learning Representations},
year={2019},
}



@inproceedings{synnaeve2011bayesian,
  title={A Bayesian model for RTS units control applied to StarCraft},
  author={Synnaeve, Gabriel and Bessiere, Pierre},
  booktitle={Computational Intelligence and Games (CIG), 2011 IEEE Conference on},
  pages={190--196},
  year={2011},
  organization={IEEE}
}

@article{ontanon2013survey,
  title={A survey of real-time strategy game AI research and competition in StarCraft},
  author={Ontan{\'o}n, Santiago and Synnaeve, Gabriel and Uriarte, Alberto and Richoux, Florian and Churchill, David and Preuss, Mike},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={5},
  number={4},
  pages={293--311},
  year={2013},
  publisher={IEEE}
}

@inproceedings{uriarte2012kiting,
  title={Kiting in RTS games using influence maps},
  author={Uriarte, Alberto and Ontan{\'o}n, Santiago},
  booktitle={Eighth Artificial Intelligence and Interactive Digital Entertainment Conference},
  year={2012}
}

@article{lillicrap_continuous_2015,
	title = {Continuous control with deep reinforcement learning},
	journaltitle = {{arXiv}:1509.02971 [cs, stat]},
	author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
	date = {2015-09-09},
	eprinttype = {arxiv},
	eprint = {1509.02971},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}
