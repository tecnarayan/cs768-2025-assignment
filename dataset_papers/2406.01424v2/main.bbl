\begin{thebibliography}{71}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Ahn et~al.(2023)Ahn, Cheng, Daneshmand, and
  Sra}]{ahn2023transformers}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8ed3d610ea4b68e7afb30ea7d01422c6-Abstract-Conference.html}
  {Transformers learn to implement preconditioned gradient descent for
  in-context learning}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou}]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
  2022.
\newblock \href {https://openreview.net/forum?id=0g0X4H8yN4I} {What learning
  algorithm is in-context learning? {Investigations} with linear models}.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}.

\bibitem[{Aky{\"u}rek et~al.(2024)Aky{\"u}rek, Wang, Kim, and
  Andreas}]{akyurek2024context}
Ekin Aky{\"u}rek, Bailin Wang, Yoon Kim, and Jacob Andreas. 2024.
\newblock \href {https://arxiv.org/abs/2401.12973} {In-context language
  learning: Arhitectures and algorithms}.
\newblock \emph{arXiv preprint arXiv:2401.12973}.

\bibitem[{Amari(1972)}]{amari1992rnns}
Shun-ichi Amari. 1972.
\newblock \href {https://doi.org/10.1109/T-C.1972.223477} {Learning patterns
  and pattern sequences by self-organizing nets of threshold elements}.
\newblock \emph{IEEE Transactions on Computers}, C-21(11):1197--1206.

\bibitem[{Ba et~al.(2016)Ba, Kiros, and Hinton}]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton. 2016.
\newblock \href {https://arxiv.org/abs/1607.06450} {Layer normalization}.
\newblock \emph{arXiv preprint arXiv:1607.06450}.

\bibitem[{Bahdanau et~al.(2015)Bahdanau, Cho, and Bengio}]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
\newblock \href {https://arxiv.org/abs/1409.0473} {Neural machine translation
  by jointly learning to align and translate}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Bai et~al.(2023)Bai, Chen, Wang, Xiong, and
  Mei}]{bai2024transformers}
Yu~Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/b2e63e36c57e153b9015fece2352a9f9-Abstract-Conference.html}
  {Transformers as statisticians: Provable in-context learning with in-context
  algorithm selection}.
\newblock In \emph{Advances in neural information processing systems}.

\bibitem[{Barron(1993)}]{barron1993universal}
Andrew~R Barron. 1993.
\newblock \href {https://doi.org/10.1109/18.256500} {Universal approximation
  bounds for superpositions of a sigmoidal function}.
\newblock \emph{IEEE Transactions on Information Theory}, 39(3):930--945.

\bibitem[{Beltagy et~al.(2020)Beltagy, Peters, and
  Cohan}]{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan. 2020.
\newblock \href {https://arxiv.org/abs/2004.05150} {Longformer: The
  long-document transformer}.
\newblock \emph{arXiv preprint arXiv:2004.05150}.

\bibitem[{Bengio et~al.(1994)Bengio, Simard, and Frasconi}]{bengio1994learning}
Yoshua Bengio, Patrice Simard, and Paolo Frasconi. 1994.
\newblock \href {https://ieeexplore.ieee.org/document/279181} {Learning
  long-term dependencies with gradient descent is difficult}.
\newblock \emph{IEEE Transactions on Neural Networks}, 5(2):157--166.

\bibitem[{Blum and Li(1991)}]{blum1991approximation}
Edward~K Blum and Leong~Kwan Li. 1991.
\newblock Approximation theory and feedforward networks.
\newblock \emph{Neural networks}, 4(4):511--515.

\bibitem[{Botev et~al.(2024)Botev, De, Smith, Fernando, Muraru, Haroun,
  Berrada, Pascanu, Sessa, Dadashi et~al.}]{botev2024recurrentgemma}
Aleksandar Botev, Soham De, Samuel~L Smith, Anushan Fernando, George-Cristian
  Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier~Giuseppe Sessa,
  Robert Dadashi, et~al. 2024.
\newblock \href {https://arxiv.org/abs/2404.07839} {{RecurrentGemma}: Moving
  past transformers for efficient open language models}.
\newblock \emph{arXiv preprint arXiv:2404.07839}.

\bibitem[{Boyd and Chua(1985)}]{boyd1985fading}
Stephen Boyd and Leon Chua. 1985.
\newblock Fading memory and the problem of approximating nonlinear operators
  with {Volterra} series.
\newblock \emph{IEEE Transactions on Circuits and Systems}, 32(11):1150--1161.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell et~al.}]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al. 2020.
\newblock \href
  {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
  {Language models are few-shot learners}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Cho et~al.(2014)Cho, van Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio}]{cho2014learning}
Kyunghyun Cho, Bart van Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014.
\newblock \href {https://aclanthology.org/D14-1179} {Learning phrase
  representations using {RNN} encoder{--}decoder for statistical machine
  translation}.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}.

\bibitem[{Coda-Forno et~al.(2023)Coda-Forno, Binz, Akata, Botvinick, Wang, and
  Schulz}]{coda2023meta}
Julian Coda-Forno, Marcel Binz, Zeynep Akata, Matt Botvinick, Jane Wang, and
  Eric Schulz. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/cda04d7ea67ea1376bf8c6962d8541e0-Abstract-Conference.html}
  {Meta-in-context learning in large language models}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  65189--65201.

\bibitem[{Conmy et~al.(2023)Conmy, Mavor-Parker, Lynch, Heimersheim, and
  Garriga-Alonso}]{conmy2023towards}
Arthur Conmy, Augustine Mavor-Parker, Aengus Lynch, Stefan Heimersheim, and
  Adri{\`a} Garriga-Alonso. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html}
  {Towards automated circuit discovery for mechanistic interpretability}.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Cybenko(1989)}]{cybenko1989approximation}
George Cybenko. 1989.
\newblock \href {https://doi.org/10.1007/BF02551274} {Approximation by
  superpositions of a sigmoidal function}.
\newblock \emph{Mathematics of control, signals and systems}, 2(4):303--314.

\bibitem[{Dai et~al.(2023)Dai, Sun, Dong, Hao, Ma, Sui, and Wei}]{dai2023gpt}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.
  2023.
\newblock \href {https://aclanthology.org/2023.findings-acl.247} {Why can {GPT}
  learn in-context? {Language} models secretly perform gradient descent as
  meta-optimizers}.
\newblock In \emph{Findings of the Association for Computational Linguistics:
  ACL 2023}.

\bibitem[{De et~al.(2024)De, Smith, Fernando, Botev, Cristian-Muraru, Gu,
  Haroun, Berrada, Chen, Srinivasan et~al.}]{de2024griffin}
Soham De, Samuel~L Smith, Anushan Fernando, Aleksandar Botev, George
  Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen,
  Srivatsan Srinivasan, et~al. 2024.
\newblock \href {https://arxiv.org/abs/2402.19427} {Griffin: Mixing gated
  linear recurrences with local attention for efficient language models}.
\newblock \emph{arXiv preprint arXiv:2402.19427}.

\bibitem[{Friedman et~al.(2023)Friedman, Wettig, and
  Chen}]{friedman2024learning}
Dan Friedman, Alexander Wettig, and Danqi Chen. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/995f693b73050f90977ed2828202645c-Abstract-Conference.html}
  {Learning transformer programs}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Fu et~al.(2023{\natexlab{a}})Fu, Dao, Saab, Thomas, Rudra, and
  Re}]{fu2023hungry}
Daniel~Y Fu, Tri Dao, Khaled~Kamal Saab, Armin~W Thomas, Atri Rudra, and
  Christopher Re. 2023{\natexlab{a}}.
\newblock \href {https://openreview.net/forum?id=COZDy0WYGg} {{Hungry Hungry
  Hippos}: {Towards} language modeling with state space models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Fu et~al.(2023{\natexlab{b}})Fu, Chen, Jia, and
  Sharan}]{fu2023transformers}
Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2310.17086} {Transformers learn
  higher-order optimization methods for in-context learning: A study with
  linear models}.
\newblock \emph{arXiv preprint arXiv:2310.17086}.

\bibitem[{Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant}]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant. 2022.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html}
  {What can transformers learn in-context? {A} case study of simple function
  classes}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Geiger et~al.(2024)Geiger, Wu, Potts, Icard, and
  Goodman}]{geiger2024finding}
Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah
  Goodman. 2024.
\newblock Finding alignments between interpretable causal variables and
  distributed neural representations.
\newblock In \emph{Causal Learning and Reasoning}.

\bibitem[{Gers et~al.(2000)Gers, Schmidhuber, and Cummins}]{gers2000learning}
Felix~A Gers, J{\"u}rgen Schmidhuber, and Fred Cummins. 2000.
\newblock Learning to forget: Continual prediction with lstm.
\newblock \emph{Neural computation}, 12(10):2451--2471.

\bibitem[{Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and
  Papailiopoulos}]{giannou23looped}
Angeliki Giannou, Shashank Rajput, Jy-Yong Sohn, Kangwook Lee, Jason~D. Lee,
  and Dimitris Papailiopoulos. 2023.
\newblock \href {https://proceedings.mlr.press/v202/giannou23a.html} {Looped
  transformers as programmable computers}.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Girosi and Poggio(1989)}]{girosi1989representation}
Federico Girosi and Tomaso Poggio. 1989.
\newblock \href {https://doi.org/10.1162/neco.1989.1.4.465} {Representation
  properties of networks: {Kolmogorov's} theorem is irrelevant}.
\newblock \emph{Neural Computation}, 1(4):465--469.

\bibitem[{Gu and Dao(2023)}]{gu2023mamba}
Albert Gu and Tri Dao. 2023.
\newblock \href {https://arxiv.org/abs/2312.00752} {Mamba: Linear-time sequence
  modeling with selective state spaces}.
\newblock \emph{arXiv preprint arXiv:2312.00752}.

\bibitem[{Gu et~al.(2020)Gu, Dao, Ermon, Rudra, and R{\'e}}]{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html}
  {{HiPPO}: {Recurrent} memory with optimal polynomial projections}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Gu et~al.(2021)Gu, Goel, and Re}]{gu2021efficiently}
Albert Gu, Karan Goel, and Christopher Re. 2021.
\newblock \href {https://openreview.net/forum?id=uYLFoz1vlAC} {Efficiently
  modeling long sequences with structured state spaces}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Gupta et~al.(2022)Gupta, Gu, and Berant}]{gupta2022diagonal}
Ankit Gupta, Albert Gu, and Jonathan Berant. 2022.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html}
  {Diagonal state spaces are as effective as structured state spaces}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Han et~al.(2023)Han, Wang, Zhao, and Ji}]{han2023context}
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji. 2023.
\newblock \href {https://arxiv.org/abs/2305.12766} {In-context learning of
  large language models explained as kernel regression}.
\newblock \emph{arXiv preprint arXiv:2305.12766}.

\bibitem[{He et~al.(2016)He, Zhang, Ren, and Sun}]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
\newblock \href
  {https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html}
  {Deep residual learning for image recognition}.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  pattern Recognition}.

\bibitem[{Hochreiter and Schmidhuber(1997)}]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber. 1997.
\newblock \href {https://ieeexplore.ieee.org/abstract/document/6795963} {Long
  short-term memory}.
\newblock \emph{Neural Computation}, 9(8):1735--1780.

\bibitem[{Hornik et~al.(1989)Hornik, Stinchcombe, and
  White}]{hornik1989multilayer}
Kurt Hornik, Maxwell Stinchcombe, and Halbert White. 1989.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural networks}, 2(5):359--366.

\bibitem[{Jayakumar et~al.(2020)Jayakumar, Czarnecki, Menick, Schwarz, Rae,
  Osindero, Teh, Harley, and Pascanu}]{jayakumar2020multiplicative}
Siddhant~M Jayakumar, Wojciech~M Czarnecki, Jacob Menick, Jonathan Schwarz,
  Jack Rae, Simon Osindero, Yee~Whye Teh, Tim Harley, and Razvan Pascanu. 2020.
\newblock \href {https://openreview.net/forum?id=rylnK6VtDH} {Multiplicative
  interactions and where to find them}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Kolmogorov(1957)}]{kolmogorov1957representation}
Andrei~Nikolaevich Kolmogorov. 1957.
\newblock On the representation of continuous functions of many variables by
  superposition of continuous functions of one variable and addition.
\newblock In \emph{Doklady Akademii Nauk}, volume 114, pages 953--956. Russian
  Academy of Sciences.

\bibitem[{{La Malfa} et~al.(2023){La Malfa}, Petrov, Frieder, Weinhuber,
  Burnell, Nazar, Cohn, Shadbolt, and Wooldridge}]{lamalfa2023language}
Emanuele {La Malfa}, Aleksandar Petrov, Simon Frieder, Christoph Weinhuber,
  Ryan Burnell, Raza Nazar, Anthony~G. Cohn, Nigel Shadbolt, and Michael
  Wooldridge. 2023.
\newblock \href {https://arxiv.org/abs/2309.16573} {{Language Models as a
  Service}: Overview of a new paradigm and its challenges}.
\newblock \emph{arXiv preprint arXiv:2309.16573}.

\bibitem[{La~Malfa et~al.(2024)La~Malfa, Weinhuber, Torre, Lin, Cohn, Shadbolt,
  and Wooldridge}]{la2024code}
Emanuele La~Malfa, Christoph Weinhuber, Orazio Torre, Fangru Lin, Anthony Cohn,
  Nigel Shadbolt, and Michael Wooldridge. 2024.
\newblock \href {https://arxiv.org/abs/2401.09074} {Code simulation challenges
  for large language models}.
\newblock \emph{arXiv preprint arXiv:2401.09074}.

\bibitem[{Lee et~al.(2024)Lee, Jiang, and Berg-Kirkpatrick}]{lee2023exploring}
Ivan Lee, Nan Jiang, and Taylor Berg-Kirkpatrick. 2024.
\newblock \href {https://openreview.net/forum?id=Qwq4cpLtoX} {Exploring the
  relationship between model architecture and in-context learning ability}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Li et~al.(2023)Li, Ildiz, Papailiopoulos, and
  Oymak}]{li2023transformers}
Yingcong Li, Muhammed~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
  2023.
\newblock \href {https://proceedings.mlr.press/v202/li23l.html} {Transformers
  as algorithms: Generalization and stability in in-context learning}.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Lindner et~al.(2023)Lindner, Kram{\'a}r, Farquhar, Rahtz, McGrath,
  and Mikulik}]{lindner2024tracr}
David Lindner, J{\'a}nos Kram{\'a}r, Sebastian Farquhar, Matthew Rahtz, Tom
  McGrath, and Vladimir Mikulik. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/771155abaae744e08576f1f3b4b7ac0d-Abstract-Conference.html}
  {Tracr: {Compiled} transformers as a laboratory for interpretability}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Liu et~al.(2023)Liu, Yuan, Fu, Jiang, Hayashi, and
  Neubig}]{liu2023pretrain}
Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and
  Graham Neubig. 2023.
\newblock \href {https://dl.acm.org/doi/full/10.1145/3560815} {Pre-train,
  prompt, and predict: A systematic survey of prompting methods in natural
  language processing}.
\newblock \emph{ACM Computing Surveys}.

\bibitem[{Liu and Chilton(2022)}]{liu2022design}
Vivian Liu and Lydia~B Chilton. 2022.
\newblock \href {https://doi.org/10.1145/3491102.3501825} {Design guidelines
  for prompt engineering text-to-image generative models}.
\newblock In \emph{Proceedings of the 2022 CHI Conference on Human Factors in
  Computing Systems}.

\bibitem[{Meurer et~al.(2017)Meurer, Smith, Paprocki, \v{C}ert\'{i}k,
  Kirpichev, Rocklin, Kumar, Ivanov, Moore, Singh, Rathnayake, Vig, Granger,
  Muller, Bonazzi, Gupta, Vats, Johansson, Pedregosa, Curry, Terrel,
  Rou\v{c}ka, Saboo, Fernando, Kulal, Cimrman, and Scopatz}]{SymPy}
Aaron Meurer, Christopher~P. Smith, Mateusz Paprocki, Ond\v{r}ej
  \v{C}ert\'{i}k, Sergey~B. Kirpichev, Matthew Rocklin, AMiT Kumar, Sergiu
  Ivanov, Jason~K. Moore, Sartaj Singh, Thilina Rathnayake, Sean Vig, Brian~E.
  Granger, Richard~P. Muller, Francesco Bonazzi, Harsh Gupta, Shivam Vats,
  Fredrik Johansson, Fabian Pedregosa, Matthew~J. Curry, Andy~R. Terrel,
  \v{S}t\v{e}p\'{a}n Rou\v{c}ka, Ashutosh Saboo, Isuru Fernando, Sumith Kulal,
  Robert Cimrman, and Anthony Scopatz. 2017.
\newblock \href {https://doi.org/10.7717/peerj-cs.103} {{SymPy}: Symbolic
  computing in {Python}}.
\newblock \emph{PeerJ Computer Science}, 3.

\bibitem[{Omlin and Giles(1996)}]{omlin1996constructing}
Christian~W Omlin and C~Lee Giles. 1996.
\newblock \href {https://dl.acm.org/doi/abs/10.1145/235809.235811}
  {Constructing deterministic finite-state automata in recurrent neural
  networks}.
\newblock \emph{Journal of the ACM (JACM)}, 43(6):937--972.

\bibitem[{Orvieto et~al.(2023)Orvieto, Smith, Gu, Fernando, Gulcehre, Pascanu,
  and De}]{orvieto2023resurrecting}
Antonio Orvieto, Samuel~L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre,
  Razvan Pascanu, and Soham De. 2023.
\newblock \href {https://arxiv.org/abs/2303.06349} {Resurrecting recurrent
  neural networks for long sequences}.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Petrov et~al.(2024{\natexlab{a}})Petrov, Torr, and
  Bibi}]{petrov2024universal}
Aleksandar Petrov, Philip~HS Torr, and Adel Bibi. 2024{\natexlab{a}}.
\newblock \href {https://arxiv.org/abs/2402.14753} {Prompting a pretrained
  transformer can be a universal approximator}.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Petrov et~al.(2024{\natexlab{b}})Petrov, Torr, and
  Bibi}]{petrov2023prompting}
Aleksandar Petrov, Philip~HS Torr, and Adel Bibi. 2024{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2310.19698} {When do prompting and
  prefix-tuning work? {A} theory of capabilities and limitations}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Sahoo et~al.(2024)Sahoo, Singh, Saha, Jain, Mondal, and
  Chadha}]{sahoo2024systematic}
Pranab Sahoo, Ayush~Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and
  Aman Chadha. 2024.
\newblock \href {https://arxiv.org/abs/2402.07927} {A systematic survey of
  prompt engineering in large language models: Techniques and applications}.
\newblock \emph{arXiv preprint arXiv:2402.07927}.

\bibitem[{Sanford et~al.(2024)Sanford, Hsu, and
  Telgarsky}]{sanford2024transformers}
Clayton Sanford, Daniel Hsu, and Matus Telgarsky. 2024.
\newblock \href {https://arxiv.org/abs/2402.09268} {Transformers, parallel
  computation, and logarithmic depth}.
\newblock \emph{arXiv preprint arXiv:2402.09268}.

\bibitem[{Scarselli and Tsoi(1998)}]{scarselli1998universal}
Franco Scarselli and Ah~Chung Tsoi. 1998.
\newblock Universal approximation using feedforward neural networks: A survey
  of some existing methods, and some new results.
\newblock \emph{Neural networks}, 11(1):15--37.

\bibitem[{Sch{\"a}fer and Zimmermann(2006)}]{schafer2006recurrent}
Anton~Maximilian Sch{\"a}fer and Hans~Georg Zimmermann. 2006.
\newblock Recurrent neural networks are universal approximators.
\newblock In \emph{Artificial Neural Networks--ICANN 2006: 16th International
  Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16},
  pages 632--640. Springer.

\bibitem[{Shazeer(2019)}]{shazeer2019fast}
Noam Shazeer. 2019.
\newblock \href {https://arxiv.org/abs/1911.02150} {Fast transformer decoding:
  One write-head is all you need}.
\newblock \emph{arXiv preprint arXiv:1911.02150}.

\bibitem[{Su et~al.(2024)Su, Ahmed, Lu, Pan, Bo, and Liu}]{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
  2024.
\newblock \href {https://arxiv.org/abs/2104.09864} {Roformer: {Enhanced}
  transformer with rotary position embedding}.
\newblock \emph{Neurocomputing}, 568.

\bibitem[{Telgarsky(2015)}]{telgarsky2015representation}
Matus Telgarsky. 2015.
\newblock \href {https://arxiv.org/abs/1509.08101} {Representation benefits of
  deep feedforward networks}.
\newblock \emph{arXiv preprint arXiv:1509.08101}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones,
  Gomez, Kaiser, and Polosukhin}]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock \href
  {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
  {Attention is all you need}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Virtanen et~al.(2020)Virtanen, Gommers, Oliphant, Haberland, Reddy,
  Cournapeau, Burovski, Peterson, Weckesser, Bright, {van der Walt}, Brett,
  Wilson, Millman, Mayorov, Nelson, Jones, Kern, Larson, Carey, Polat, Feng,
  Moore, {VanderPlas}, Laxalde, Perktold, Cimrman, Henriksen, Quintero, Harris,
  Archibald, Ribeiro, Pedregosa, {van Mulbregt}, and {SciPy 1.0
  Contributors}}]{SciPy}
Pauli Virtanen, Ralf Gommers, Travis~E. Oliphant, Matt Haberland, Tyler Reddy,
  David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan
  Bright, St{\'e}fan~J. {van der Walt}, Matthew Brett, Joshua Wilson, K.~Jarrod
  Millman, Nikolay Mayorov, Andrew R.~J. Nelson, Eric Jones, Robert Kern, Eric
  Larson, C~J Carey, {\.I}lhan Polat, Yu~Feng, Eric~W. Moore, Jake
  {VanderPlas}, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen,
  E.~A. Quintero, Charles~R. Harris, Anne~M. Archibald, Ant{\^o}nio~H. Ribeiro,
  Fabian Pedregosa, Paul {van Mulbregt}, and {SciPy 1.0 Contributors}. 2020.
\newblock \href {https://doi.org/10.1038/s41592-019-0686-2} {{SciPy} 1.0:
  Fundamental algorithms for scientific computing in {Python}}.
\newblock \emph{Nature Methods}, 17:261--272.

\bibitem[{von Oswald et~al.(2023{\natexlab{a}})von Oswald, Niklasson, Randazzo,
  Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov}]{oswald23transformers}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo\~ao Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
  2023{\natexlab{a}}.
\newblock \href {https://proceedings.mlr.press/v202/von-oswald23a.html}
  {Transformers learn in-context by gradient descent}.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{von Oswald et~al.(2023{\natexlab{b}})von Oswald, Niklasson, Schlegel,
  Kobayashi, Zucchet, Scherrer, Miller, Sandler, Vladymyrov, Pascanu, and
  Sacramento}]{von2023uncovering}
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi,
  Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler, Max Vladymyrov,
  Razvan Pascanu, and Jo\~ao Sacramento. 2023{\natexlab{b}}.
\newblock \href {https://arxiv.org/abs/2309.05858} {Uncovering
  mesa-optimization algorithms in transformers}.
\newblock \emph{arXiv preprint arXiv:2309.05858}.

\bibitem[{Wang and Xue(2023)}]{wang2024state}
Shida Wang and Beichen Xue. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/ea8608c6258450e75b3443ec8022fb2e-Abstract-Conference.html}
  {State-space models with layer-wise nonlinearity are universal approximators
  with exponential decaying memory}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wang et~al.(2023)Wang, Chauhan, Wang, and
  Hsieh}]{wang2024universality}
Yihan Wang, Jatin Chauhan, Wei Wang, and Cho-Jui Hsieh. 2023.
\newblock \href
  {https://proceedings.neurips.cc/paper_files/paper/2023/hash/eef6aecfe050b556c6a48d9c16b15558-Abstract-Conference.html}
  {Universality and limitations of prompt tuning}.
\newblock \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Wei et~al.(2021)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le}]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester,
  Nan Du, Andrew~M Dai, and Quoc~V Le. 2021.
\newblock \href {https://openreview.net/forum?id=gEZrGCozdqR} {Finetuned
  language models are zero-shot learners}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Weiss et~al.(2021)Weiss, Goldberg, and Yahav}]{weiss2021thinking}
Gail Weiss, Yoav Goldberg, and Eran Yahav. 2021.
\newblock \href {http://proceedings.mlr.press/v139/weiss21a.html} {Thinking
  like transformers}.
\newblock In \emph{International Conference on Machine Learning}.

\bibitem[{Xie et~al.(2022)Xie, Raghunathan, Liang, and Ma}]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma. 2022.
\newblock \href {https://openreview.net/forum?id=RdJVFCHjUMI} {An explanation
  of in-context learning as implicit {Bayesian} inference}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Yun et~al.(2019)Yun, Bhojanapalli, Rawat, Reddi, and
  Kumar}]{yun2019transformers}
Chulhee Yun, Srinadh Bhojanapalli, Ankit~Singh Rawat, Sashank Reddi, and Sanjiv
  Kumar. 2019.
\newblock \href {https://arxiv.org/abs/1912.10077} {Are transformers universal
  approximators of sequence-to-sequence functions?}
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Zhang and Sennrich(2019)}]{zhang2019root}
Biao Zhang and Rico Sennrich. 2019.
\newblock \href
  {https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html}
  {Root mean square layer normalization}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Zhang et~al.(2023)Zhang, Frei, and Bartlett}]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett. 2023.
\newblock \href {https://arxiv.org/abs/2306.09927} {Trained transformers learn
  linear models in-context}.
\newblock \emph{arXiv preprint arXiv:2306.09927}.

\bibitem[{Zhou et~al.(2024)Zhou, Bradley, Littwin, Razin, Saremi, Susskind,
  Bengio, and Nakkiran}]{zhou2023algorithms}
Hattie Zhou, Arwen Bradley, Etai Littwin, Noam Razin, Omid Saremi, Josh
  Susskind, Samy Bengio, and Preetum Nakkiran. 2024.
\newblock \href {https://arxiv.org/abs/2310.16028} {What algorithms can
  transformers learn? {A} study in length generalization}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ziyin et~al.(2020)Ziyin, Hartwig, and Ueda}]{ziyin2020neural}
Liu Ziyin, Tilman Hartwig, and Masahito Ueda. 2020.
\newblock \href
  {https://proceedings.neurips.cc/paper/2020/hash/1160453108d3e537255e9f7b931f4e90-Abstract.html}
  {Neural networks fail to learn periodic functions and how to fix it}.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\end{thebibliography}
