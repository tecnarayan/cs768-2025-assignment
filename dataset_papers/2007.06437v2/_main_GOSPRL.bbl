\begin{thebibliography}{10}

\bibitem{agarwal2019optimality}
A.~Agarwal, S.~Kakade, and L.~F. Yang.
\newblock Model-based reinforcement learning with a generative model is minimax
  optimal.
\newblock In {\em Proceedings of Thirty Third Conference on Learning Theory},
  volume 125 of {\em Proceedings of Machine Learning Research}. PMLR, 2020.

\bibitem{andrychowicz2017hindsight}
M.~Andrychowicz, F.~Wolski, A.~Ray, J.~Schneider, R.~Fong, P.~Welinder,
  B.~McGrew, J.~Tobin, P.~Abbeel, and W.~Zaremba.
\newblock Hindsight experience replay.
\newblock In {\em Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 5055--5065, 2017.

\bibitem{audibert2010best}
J.-Y. Audibert and S.~Bubeck.
\newblock Best arm identification in multi-armed bandits.
\newblock In {\em COLT - 23th Conference on Learning Theory}, 2010.

\bibitem{audibert2009exploration}
J.-Y. Audibert, R.~Munos, and C.~Szepesv{\'a}ri.
\newblock Exploration--exploitation tradeoff using variance estimates in
  multi-armed bandits.
\newblock {\em Theoretical Computer Science}, 410(19):1876--1902, 2009.

\bibitem{azar2013minimax}
M.~G. Azar, R.~Munos, and H.~J. Kappen.
\newblock Minimax pac bounds on the sample complexity of reinforcement learning
  with a generative model.
\newblock {\em Machine learning}, 91(3):325--349, 2013.

\bibitem{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR, 2017.

\bibitem{bartlett2019scale-free}
P.~Bartlett, V.~Gabillon, J.~Healey, and M.~Valko.
\newblock Scale-free adaptive planning for deterministic dynamics \& discounted
  rewards.
\newblock In {\em International Conference on Machine Learning}, pages
  495--504. PMLR, 2019.

\bibitem{bartlett2009regal}
P.~L. Bartlett and A.~Tewari.
\newblock Regal: a regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In {\em Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, 2009.

\bibitem{bertsekas1995dynamic}
D.~P. Bertsekas.
\newblock {\em Dynamic programming and optimal control}, volume~1.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem{bertsekas1991analysis}
D.~P. Bertsekas and J.~N. Tsitsiklis.
\newblock An analysis of stochastic shortest path problems.
\newblock {\em Mathematics of Operations Research}, 16(3):580--595, 1991.

\bibitem{bertsekas2013stochastic}
D.~P. Bertsekas and H.~Yu.
\newblock Stochastic shortest path problems under weak conditions.
\newblock {\em Lab. for Information and Decision Systems Report LIDS-P-2909,
  MIT}, 2013.

\bibitem{bhatnagar2009natural}
S.~Bhatnagar, R.~S. Sutton, M.~Ghavamzadeh, and M.~Lee.
\newblock Natural actor--critic algorithms.
\newblock {\em Automatica}, 45(11):2471--2482, 2009.

\bibitem{brafman2002r}
R.~I. Brafman and M.~Tennenholtz.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 3(Oct):213--231, 2002.

\bibitem{carpentier2011upper}
A.~Carpentier, A.~Lazaric, M.~Ghavamzadeh, R.~Munos, and P.~Auer.
\newblock Upper-confidence-bound algorithms for active learning in multi-armed
  bandits.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  2011.

\bibitem{chen2018scalable}
Y.~Chen, L.~Li, and M.~Wang.
\newblock Scalable bilinear $\pi$ learning using state and action features.
\newblock {\em arXiv preprint arXiv:1804.10328}, 2018.

\bibitem{chen2016stochastic}
Y.~Chen and M.~Wang.
\newblock Stochastic primal-dual methods and sample complexity of reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1612.02516}, 2016.

\bibitem{cheung2019exploration}
W.~C. Cheung.
\newblock Exploration-exploitation trade-off in reinforcement learning on
  online markov decision processes with global concave rewards.
\newblock {\em arXiv preprint arXiv:1905.06466}, 2019.

\bibitem{cheung2019regret}
W.~C. Cheung.
\newblock Regret minimization for reinforcement learning with vectorial
  feedback and complex objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  724--734, 2019.

\bibitem{dekel2014bandits}
O.~Dekel, J.~Ding, T.~Koren, and Y.~Peres.
\newblock Bandits with switching costs: ${T}^{2/3}$ regret.
\newblock In {\em Proceedings of the forty-sixth annual ACM symposium on Theory
  of computing}, pages 459--467, 2014.

\bibitem{d1963probabilistic}
F.~d'Epenoux.
\newblock A probabilistic production and inventory problem.
\newblock {\em Management Science}, 10(1):98--108, 1963.

\bibitem{florensa2018automatic}
C.~Florensa, D.~Held, X.~Geng, and P.~Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In {\em International conference on machine learning}, pages
  1515--1528. PMLR, 2018.

\bibitem{improved_analysis_UCRL2B}
R.~Fruit, M.~Pirotta, and A.~Lazaric.
\newblock Improved analysis of ucrl2 with empirical bernstein inequality.
\newblock {\em arXiv preprint arXiv:2007.05456}, 2020.

\bibitem{fruit2018efficient}
R.~Fruit, M.~Pirotta, A.~Lazaric, and R.~Ortner.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In {\em ICML 2018-The 35th International Conference on Machine
  Learning}, volume~80, pages 1578--1586, 2018.

\bibitem{garcelon2020conservative}
E.~Garcelon, M.~Ghavamzadeh, A.~Lazaric, and M.~Pirotta.
\newblock Conservative exploration in reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1431--1441. PMLR, 2020.

\bibitem{grill2016blazing}
J.-B. Grill, M.~Valko, and R.~Munos.
\newblock Blazing the trails before beating the path: Sample-efficient
  monte-carlo planning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4680--4688, 2016.

\bibitem{hazan2019provably}
E.~Hazan, S.~Kakade, K.~Singh, and A.~Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In {\em International Conference on Machine Learning}, pages
  2681--2691, 2019.

\bibitem{hsu2015mixing}
D.~J. Hsu, A.~Kontorovich, and C.~Szepesv{\'a}ri.
\newblock Mixing time estimation in reversible markov chains from a single
  sample path.
\newblock In {\em Advances in neural information processing systems}, pages
  1459--1467, 2015.

\bibitem{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Journal of Machine Learning Research}, 11(Apr):1563--1600, 2010.

\bibitem{jin2018is-q-learning}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is q-learning provably efficient?
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem{jin2020reward}
C.~Jin, A.~Krishnamurthy, M.~Simchowitz, and T.~Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4870--4879. PMLR, 2020.

\bibitem{kaufmann2021adaptive}
E.~Kaufmann, P.~M{\'e}nard, O.~D. Domingues, A.~Jonsson, E.~Leurent, and
  M.~Valko.
\newblock Adaptive reward-free exploration.
\newblock In {\em Algorithmic Learning Theory}, pages 865--891. PMLR, 2021.

\bibitem{kazerouni2017conservative}
A.~Kazerouni, M.~Ghavamzadeh, Y.~Abbasi, and B.~Van~Roy.
\newblock Conservative contextual linear bandits.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3910--3919, 2017.

\bibitem{kearns2002sparse}
M.~Kearns, Y.~Mansour, and A.~Y. Ng.
\newblock A sparse sampling algorithm for near-optimal planning in large markov
  decision processes.
\newblock {\em Machine learning}, 49(2-3):193--208, 2002.

\bibitem{kearns2002near}
M.~Kearns and S.~Singh.
\newblock Near-optimal reinforcement learning in polynomial time.
\newblock {\em Machine learning}, 49(2-3):209--232, 2002.

\bibitem{kearns2000approximate}
M.~J. Kearns, Y.~Mansour, and A.~Y. Ng.
\newblock Approximate planning in large pomdps via reusable trajectories.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1001--1007, 2000.

\bibitem{koren2017bandits}
T.~Koren, R.~Livni, and Y.~Mansour.
\newblock Bandits with movement costs and adaptive pricing.
\newblock In {\em Conference on Learning Theory}, pages 1242--1268. PMLR, 2017.

\bibitem{lattimore2019bandit}
T.~Lattimore and C.~Szepesv{\'a}ri.
\newblock {\em Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem{li2020breaking}
G.~Li, Y.~Wei, Y.~Chi, Y.~Gu, and Y.~Chen.
\newblock Breaking the sample size barrier in model-based reinforcement
  learning with a generative model.
\newblock {\em Advances in Neural Information Processing Systems}, 2020.

\bibitem{menard2020fast}
P.~M{\'e}nard, O.~D. Domingues, A.~Jonsson, E.~Kaufmann, E.~Leurent, and
  M.~Valko.
\newblock Fast active learning for pure exploration in reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7599--7608. PMLR, 2021.

\bibitem{ortner2020regret}
R.~Ortner.
\newblock Regret bounds for reinforcement learning via markov chain
  concentration.
\newblock {\em Journal of Artificial Intelligence Research}, 67:115--128, 2020.

\bibitem{paulin2015concentration}
D.~Paulin.
\newblock Concentration inequalities for markov chains by marton couplings and
  spectral methods.
\newblock {\em Electronic Journal of Probability}, 20, 2015.

\bibitem{pong2020skew}
V.~Pong, M.~Dalal, S.~Lin, A.~Nair, S.~Bahl, and S.~Levine.
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  7783--7792. PMLR, 2020.

\bibitem{puterman2014markov}
M.~L. Puterman.
\newblock {\em Markov Decision Processes.: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem{jian2019exploration}
J.~Qian, R.~Fruit, M.~Pirotta, and A.~Lazaric.
\newblock Exploration bonus for regret minimization in discrete and continuous
  average reward mdps.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4891--4900, 2019.

\bibitem{cohen2020near}
A.~Rosenberg, A.~Cohen, Y.~Mansour, and H.~Kaplan.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock In {\em International Conference on Machine Learning}, pages
  8210--8219. PMLR, 2020.

\bibitem{sidford2018near}
A.~Sidford, M.~Wang, X.~Wu, L.~Yang, and Y.~Ye.
\newblock Near-optimal time and sample complexities for solving markov decision
  processes with a generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5186--5196, 2018.

\bibitem{strehl2009reinforcement}
A.~L. Strehl, L.~Li, and M.~L. Littman.
\newblock Reinforcement learning in finite mdps: Pac analysis.
\newblock {\em Journal of Machine Learning Research}, 10(Nov):2413--2444, 2009.

\bibitem{strehl2008analysis}
A.~L. Strehl and M.~L. Littman.
\newblock An analysis of model-based interval estimation for markov decision
  processes.
\newblock {\em Journal of Computer and System Sciences}, 74(8):1309--1331,
  2008.

\bibitem{szorenyi2014optimistic}
B.~Sz{\"o}r{\'e}nyi, G.~Kedenburg, and R.~Munos.
\newblock Optimistic planning in markov decision processes using a generative
  model.
\newblock {\em Advances in Neural Information Processing Systems},
  27:1035--1043, 2014.

\bibitem{tarbouriech2019no}
J.~Tarbouriech, E.~Garcelon, M.~Valko, M.~Pirotta, and A.~Lazaric.
\newblock No-regret exploration in goal-oriented reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  9428--9437. PMLR, 2020.

\bibitem{tarbouriech2019active}
J.~Tarbouriech and A.~Lazaric.
\newblock Active exploration in markov decision processes.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 974--982, 2019.

\bibitem{tarbouriech2020improved}
J.~Tarbouriech, M.~Pirotta, M.~Valko, and A.~Lazaric.
\newblock Improved sample complexity for incremental autonomous exploration in
  mdps.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, pages 11273--11284, 2020.

\bibitem{tarbou}
J.~Tarbouriech, M.~Pirotta, M.~Valko, and A.~Lazaric.
\newblock Sample complexity bounds for stochastic shortest path with a
  generative model.
\newblock In {\em Algorithmic Learning Theory}, pages 1157--1178. PMLR, 2021.

\bibitem{tarbouriech2020active}
J.~Tarbouriech, S.~Shekhar, M.~Pirotta, M.~Ghavamzadeh, and A.~Lazaric.
\newblock Active model estimation in markov decision processes.
\newblock In {\em Conference on Uncertainty in Artificial Intelligence}, pages
  1019--1028. PMLR, 2020.

\bibitem{trevizan2016heuristic}
F.~Trevizan, S.~Thi{\'e}baux, P.~Santana, and B.~Williams.
\newblock Heuristic search in dual space for constrained stochastic shortest
  path problems.
\newblock In {\em Twenty-Sixth International Conference on Automated Planning
  and Scheduling}, 2016.

\bibitem{vial2021regret}
D.~Vial, A.~Parulekar, S.~Shakkottai, and R.~Srikant.
\newblock Regret bounds for stochastic shortest path problems with linear
  function approximation.
\newblock {\em arXiv preprint arXiv:2105.01593}, 2021.

\bibitem{wainwright}
M.~Wainwright.
\newblock Course on mathematical statistics, chapter 2: Basic tail and
  concentration bounds. {U}niversity of {C}alifornia at {B}erkeley,
  {D}epartment of {S}tatistics, 2015.

\bibitem{wang2017primal}
M.~Wang.
\newblock Primal-dual $\pi$ learning: Sample complexity and sublinear run time
  for ergodic markov decision problems.
\newblock {\em arXiv preprint arXiv:1710.06100}, 2017.

\bibitem{wang2019q}
Y.~Wang, K.~Dong, X.~Chen, and L.~Wang.
\newblock Q-learning with ucb exploration is sample efficient for
  infinite-horizon mdp.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{wei2019model}
C.-Y. Wei, M.~J. Jahromi, H.~Luo, H.~Sharma, and R.~Jain.
\newblock Model-free reinforcement learning in infinite-horizon average-reward
  markov decision processes.
\newblock In {\em International Conference on Machine Learning}, pages
  10170--10180. PMLR, 2020.

\bibitem{zanette2019tighter}
A.~Zanette and E.~Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In {\em International Conference on Machine Learning}, pages
  7304--7312, 2019.

\bibitem{zanette2019almost}
A.~Zanette, M.~J. Kochenderfer, and E.~Brunskill.
\newblock Almost horizon-free structure-aware best policy identification with a
  generative model.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5626--5635, 2019.

\bibitem{zhang2020taskagnostic}
X.~Zhang, Y.~Ma, and A.~Singla.
\newblock Task-agnostic exploration in reinforcement learning.
\newblock In {\em 34th Conference on Neural Information Processing Systems},
  pages 11734--11743, 2020.

\bibitem{zhang2020nearly}
Z.~Zhang, S.~S. Du, and X.~Ji.
\newblock Nearly minimax optimal reward-free reinforcement learning.
\newblock {\em arXiv preprint arXiv:2010.05901}, 2020.

\bibitem{zhang2019regret}
Z.~Zhang and X.~Ji.
\newblock Regret minimization for reinforcement learning by evaluating the
  optimal bias function.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2823--2832, 2019.

\bibitem{zhang2020almost}
Z.~Zhang, Y.~Zhou, and X.~Ji.
\newblock Almost optimal model-free reinforcement learning via
  reference-advantage decomposition.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\end{thebibliography}
