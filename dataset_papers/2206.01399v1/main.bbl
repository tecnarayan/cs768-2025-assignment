\begin{thebibliography}{84}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allwein et~al.(2000)Allwein, Schapire, and
  Singer]{algo_comparison:Allwein00}
Erin Allwein, Robert~E. Schapire, and Yoram Singer.
\newblock Reducing multiclass to binary: A unifying approach for margin
  classifiers.
\newblock \emph{Journal of Machine Learning Research}, 1:\penalty0 113--141,
  2000.

\bibitem[Azizan et~al.(2020)Azizan, Lale, and
  Hassibi]{implicit_bias:azizan2020study}
Navid Azizan, Sahin Lale, and Babak Hassibi.
\newblock A study of generalization of stochastic mirror descent algorithms on
  overparameterized nonlinear models.
\newblock In \emph{ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 3132--3136. IEEE, 2020.

\bibitem[Bartlett and Mendelson(2002)]{margin:bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{reg:bartlett2020benign}
Peter~L Bartlett, Philip~M Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{survey:bartlett2021deep}
Peter~L Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock \emph{Acta numerica}, 30:\penalty0 87--201, 2021.

\bibitem[Belkin(2021)]{survey:belkin2021fit}
Mikhail Belkin.
\newblock Fit without fear: remarkable mathematical phenomena of deep learning
  through the prism of interpolation.
\newblock \emph{Acta Numerica}, 30:\penalty0 203--248, 2021.

\bibitem[Belkin et~al.(2018)Belkin, Ma, and Mandal]{reg:belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock \emph{ICML}, 2018.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and
  Mandal]{reg:belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Belkin et~al.(2020)Belkin, Hsu, and Xu]{reg:belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 2\penalty0
  (4):\penalty0 1167--1180, 2020.

\bibitem[Bibas et~al.(2019)Bibas, Fogel, and Feder]{reg:bibas}
Koby Bibas, Yaniv Fogel, and Meir Feder.
\newblock A new look at an old problem: {A} universal learning approach to
  linear regression.
\newblock \emph{CoRR}, abs/1905.04708, 2019.
\newblock URL \url{http://arxiv.org/abs/1905.04708}.

\bibitem[Bosman et~al.(2020)Bosman, Engelbrecht, and
  Helbig]{nets_loss_function:Bos20}
Anna Bosman, Andries Engelbrecht, and Mardé Helbig.
\newblock Visualising basins of attraction for the cross-entropy and the
  squared error neural network loss functions.
\newblock \emph{Neurocomputing}, 400, 03 2020.
\newblock \doi{10.1016/j.neucom.2020.02.113}.

\bibitem[Bredensteiner and Bennett(1999)]{algo_multiclass:Bred99}
Erin~J. Bredensteiner and Kristin~P. Bennett.
\newblock Multicategory classification by support vector machines.
\newblock \emph{Computational Optimization and Applications}, 12, 1999.
\newblock \doi{10.1023/A:1008663629662}.

\bibitem[Chatterji and Long(2021)]{binary_classification:chatterji2021finite}
Niladri~S Chatterji and Philip~M Long.
\newblock Finite-sample analysis of interpolating linear classifiers in the
  overparameterized regime.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (129):\penalty0 1--30, 2021.

\bibitem[Chen and Sun(2006)]{consistency:Chen06}
Di-Rong Chen and Tao Sun.
\newblock Consistency of multiclass empirical risk minimization methods based
  on convex loss.
\newblock \emph{Journal of Machine Learning Research}, 7:\penalty0 2435–2447,
  dec 2006.
\newblock ISSN 1532-4435.

\bibitem[Choromanska et~al.(2013)Choromanska, Agarwal, and
  Langford]{extreme:choromanska2013extreme}
Anna Choromanska, Alekh Agarwal, and John Langford.
\newblock Extreme multi class classification.
\newblock In \emph{NIPS Workshop: eXtreme Classification, submitted}, volume~1,
  pages 2--1, 2013.

\bibitem[Cortes et~al.(2016)Cortes, Kuznetsov, Mohri, and
  Yang]{multi_class_theory:Cortes16}
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang.
\newblock Structured prediction theory based on factor graph complexity.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/535ab76633d94208236a2e829ea6d888-Paper.pdf}.

\bibitem[Cover and Thomas(2006)]{Cover2006}
Thomas~M. Cover and Joy~A. Thomas.
\newblock \emph{Elements of Information Theory 2nd Edition (Wiley Series in
  Telecommunications and Signal Processing)}.
\newblock Wiley-Interscience, July 2006.
\newblock ISBN 0471241954.

\bibitem[Crammer et~al.(2001)Crammer, Singer, Cristianini, Shawe-taylor, and
  Williamson]{algo_multiclass:Crammer01}
Koby Crammer, Yoram Singer, Nello Cristianini, John Shawe-taylor, and Bob
  Williamson.
\newblock On the algorithmic implementation of multiclass kernel-based vector
  machines.
\newblock \emph{Journal of Machine Learning Research}, 2:\penalty0 265–292,
  2001.

\bibitem[D'Amour et~al.(2020)D'Amour, Heller, Moldovan, Adlam, Alipanahi,
  Beutel, Chen, Deaton, Eisenstein, Hoffman, et~al.]{d2020underspecification}
Alexander D'Amour, Katherine Heller, Dan Moldovan, Ben Adlam, Babak Alipanahi,
  Alex Beutel, Christina Chen, Jonathan Deaton, Jacob Eisenstein, Matthew~D
  Hoffman, et~al.
\newblock Underspecification presents challenges for credibility in modern
  machine learning.
\newblock \emph{arXiv preprint arXiv:2011.03395}, 2020.

\bibitem[Dar et~al.(2021)Dar, Muthukumar, and Baraniuk]{survey:dar2021farewell}
Yehuda Dar, Vidya Muthukumar, and Richard~G Baraniuk.
\newblock A farewell to the bias-variance tradeoff? an overview of the theory
  of overparameterized machine learning.
\newblock \emph{arXiv preprint arXiv:2109.02355}, 2021.

\bibitem[Demirkaya et~al.(2020)Demirkaya, Chen, and
  Oymak]{nets_loss_function:Dem20}
Ahmet Demirkaya, Jiasi Chen, and Samet Oymak.
\newblock Exploring the role of loss functions in multiclass classification.
\newblock In \emph{2020 54th Annual Conference on Information Sciences and
  Systems (CISS)}, pages 1--5, 2020.
\newblock \doi{10.1109/CISS48834.2020.1570627167}.

\bibitem[Deng et~al.(2021)Deng, Kammoun, and
  Thrampoulidis]{classification_cgmt:deng21}
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis.
\newblock {A model of double descent for high-dimensional binary linear
  classification}.
\newblock \emph{Information and Inference: A Journal of the IMA}, 04 2021.
\newblock ISSN 2049-8772.
\newblock \doi{10.1093/imaiai/iaab002}.
\newblock URL \url{https://doi.org/10.1093/imaiai/iaab002}.
\newblock iaab002.

\bibitem[Dietterich and Bakiri(1994)]{algo_multiclass:Dietterich95}
Thomas~G Dietterich and Ghulum Bakiri.
\newblock Solving multiclass learning problems via error-correcting output
  codes.
\newblock \emph{Journal of Artificial Intelligence Research}, 2\penalty0
  (1):\penalty0 263–286, 1994.
\newblock ISSN 1076-9757.

\bibitem[Engl et~al.(1996)Engl, Hanke, and
  Neubauer]{implicit_bias:engl1996regularization}
Heinz~Werner Engl, Martin Hanke, and Andreas Neubauer.
\newblock \emph{Regularization of inverse problems}, volume 375.
\newblock Springer Science \& Business Media, 1996.

\bibitem[F{\"u}rnkranz(2002)]{algo_comparison:Johannes02}
Johannes F{\"u}rnkranz.
\newblock Round robin classification.
\newblock \emph{Journal of Machine Learning Research}, 2:\penalty0 721--747,
  2002.

\bibitem[Gajowniczek et~al.(2017)Gajowniczek, Chmielewski, Or{\l}owski, and
  Z{\k{a}}bkowski]{nets_loss_function:Kry17}
Krzysztof Gajowniczek, Leszek Chmielewski, Arkadiusz Or{\l}owski, and Tomasz
  Z{\k{a}}bkowski.
\newblock Generalized entropy cost function in neural networks.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pages 128--136, 10 2017.
\newblock ISBN 978-3-319-68611-0.
\newblock \doi{10.1007/978-3-319-68612-7_15}.

\bibitem[Gallager(1968)]{gallager1968information}
Robert~G Gallager.
\newblock \emph{Information theory and reliable communication}, volume 588.
\newblock Springer, 1968.

\bibitem[Geiger et~al.(2019)Geiger, Spigler, d'Ascoli, Sagun, Baity-Jesi,
  Biroli, and Wyart]{reg:geiger2019jamming}
Mario Geiger, Stefano Spigler, St{\'e}phane d'Ascoli, Levent Sagun, Marco
  Baity-Jesi, Giulio Biroli, and Matthieu Wyart.
\newblock Jamming transition as a paradigm to understand the loss landscape of
  deep neural networks.
\newblock \emph{Physical Review E}, 100\penalty0 (1):\penalty0 012115, 2019.

\bibitem[Guermeur(2002)]{multi_class_theory:Gue02}
Yann Guermeur.
\newblock {Combining Discriminant Models with New Multi-Class SVMs}.
\newblock \emph{Pattern Anal. Appl.}, 5:\penalty0 168--179, 06 2002.
\newblock \doi{10.1007/s100440200015}.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{implicit_bias:gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1832--1841, 2018.

\bibitem[Hastie et~al.(2019)Hastie, Montanari, Rosset, and
  Tibshirani]{reg:hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock \emph{arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[{Hou} et~al.(2016){Hou}, {Yu}, and
  {Samaras}]{nets_loss_function:Hou16}
Le~{Hou}, Chen-Ping {Yu}, and Dimitris {Samaras}.
\newblock {Squared Earth Mover's Distance-based Loss for Training Deep Neural
  Networks}.
\newblock \emph{arXiv e-prints}, art. arXiv:1611.05916, November 2016.

\bibitem[Hsu et~al.(2021)Hsu, Muthukumar, and
  Xu]{binary_classification:hsu2021proliferation}
Daniel Hsu, Vidya Muthukumar, and Ji~Xu.
\newblock On the proliferation of support vectors in high dimensions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 91--99. PMLR, 2021.

\bibitem[(https://mathoverflow.net/users/36721/iosif pinelis)()]{Marginproof}
Iosif~Pinelis (https://mathoverflow.net/users/36721/iosif pinelis).
\newblock Concentration and anti-concentration of gap between largest and
  second largest value in gaussian iid sample.
\newblock MathOverflow.
\newblock URL \url{https://mathoverflow.net/q/379688}.
\newblock URL:https://mathoverflow.net/q/379688 (version: 2020-12-25).

\bibitem[{Hui} and {Belkin}(2020)]{nets_loss_function:Hui20}
Like {Hui} and Mikhail {Belkin}.
\newblock {Evaluation of Neural Architectures Trained with Square Loss vs
  Cross-Entropy in Classification Tasks}.
\newblock \emph{arXiv e-prints}, art. arXiv:2006.07322, June 2020.

\bibitem[Ji and Telgarsky(2019)]{implicit_bias:ji2019}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pages 1772--1798, 2019.

\bibitem[Kammoun and
  AlouiniFellow(2021)]{classification_cgmt:kammoun2021precise}
Abla Kammoun and Mohamed-Slim AlouiniFellow.
\newblock On the precise error analysis of support vector machines.
\newblock \emph{IEEE Open Journal of Signal Processing}, 2:\penalty0 99--118,
  2021.

\bibitem[Kini and Thrampoulidis(2020)]{classification_cgmt:kini2020analytic}
Ganesh~Ramachandra Kini and Christos Thrampoulidis.
\newblock Analytic study of double descent in binary classification: The impact
  of loss.
\newblock In \emph{2020 IEEE International Symposium on Information Theory
  (ISIT)}, pages 2527--2532. IEEE, 2020.

\bibitem[Kini et~al.(2021)Kini, Paraskevas, Oymak, and
  Thrampoulidis]{label_imbalance}
Ganesh~Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
  Thrampoulidis.
\newblock Label-imbalanced and group-sensitive classification under
  overparameterization.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 18970--18983. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/9dfcf16f0adbc5e2a55ef02db36bac7f-Paper.pdf}.

\bibitem[Kline and Berardi(2005)]{nets_loss_function:Kline05}
Doug~M. Kline and Victor~L. Berardi.
\newblock Revisiting squared-error and cross-entropy functions for training
  neural network classifiers.
\newblock \emph{Neural Computing \& Applications}, 14:\penalty0 310--318, 2005.

\bibitem[Kobak et~al.(2020)Kobak, Lomond, and Sanchez]{reg:kobak2020optimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez.
\newblock The optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 169--1,
  2020.

\bibitem[Koltchinskii and Panchenko(2002)]{multi_class_theory:Kolt02}
Vladimir Koltchinskii and Dmitry Panchenko.
\newblock Empirical margin distributions and bounding the generalization error
  of combined classifiers.
\newblock \emph{The Annals of Statistics}, 30\penalty0 (1):\penalty0 1--50,
  2002.
\newblock ISSN 00905364.
\newblock URL \url{http://www.jstor.org/stable/2700001}.

\bibitem[Kumar and Sastry(2018)]{nets_loss_function:Kum18}
Himanshu Kumar and P.~Shanti Sastry.
\newblock Robust loss functions for learning multi-class classifiers.
\newblock \emph{2018 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)}, pages 687--692, 2018.

\bibitem[Kuznetsov et~al.(2014)Kuznetsov, Mohri, and
  Syed]{multi_class_theory:Kuz14}
Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed.
\newblock Multi-class deep boosting.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.~Lawrence, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~27. Curran Associates, Inc., 2014.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2014/file/7bb060764a818184ebb1cc0d43d382aa-Paper.pdf}.

\bibitem[Kuznetsov et~al.(2015)Kuznetsov, Mohri, and
  Syed]{multi_class_theory:Kuznetsov2015RademacherCM}
Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed.
\newblock Rademacher complexity margin bounds for learning with a large number
  of classes.
\newblock In \emph{ICML Workshop on Extreme Classification: Learning with a
  Very Large Number of Labels}, 2015.

\bibitem[Lee et~al.(2004)Lee, Lin, and Wahba]{algo_multiclass:Lee04}
Yoonkyung Lee, Yi~Lin, and Grace Wahba.
\newblock Multicategory support vector machines: Theory and application to the
  classification of microarray data and satellite radiance data.
\newblock \emph{Journal of the American Statistical Association}, 99\penalty0
  (465):\penalty0 67--81, 2004.
\newblock \doi{10.1198/016214504000000098}.
\newblock URL \url{https://doi.org/10.1198/016214504000000098}.

\bibitem[Lei et~al.(2015)Lei, Dogan, Binder, and
  Kloft]{multi_class_theory:Lei15}
Yunwen Lei, Urun Dogan, Alexander Binder, and Marius Kloft.
\newblock {Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds
  to Novel Algorithms}.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2015/file/3a029f04d76d32e79367c4b3255dda4d-Paper.pdf}.

\bibitem[Lei et~al.(2019)Lei, Dogan, Zhou, and Kloft]{multi_class_theory:Lei19}
Yunwen Lei, Ürün Dogan, Ding-Xuan Zhou, and Marius Kloft.
\newblock Data-dependent generalization bounds for multi-class classification.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (5):\penalty0 2995--3021, 2019.
\newblock \doi{10.1109/TIT.2019.2893916}.

\bibitem[Li et~al.(2018)Li, Liu, Yin, Zhang, Ding, and
  Wang]{multi_class_theory:Li18}
Jian Li, Yong Liu, Rong Yin, Hua Zhang, Lizhong Ding, and Weiping Wang.
\newblock Multi-class learning: From theory to algorithm.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/1141938ba2c2b13f5505d7c424ebae5f-Paper.pdf}.

\bibitem[Li and Wei(2021)]{reg:li2021minimum}
Yue Li and Yuting Wei.
\newblock Minimum $\ell_1$-norm interpolators: Precise asymptotics and multiple
  descent.
\newblock \emph{arXiv preprint arXiv:2110.09502}, 2021.

\bibitem[Maurer(2016)]{multi_class_theory:Mau16}
Andreas Maurer.
\newblock { A vector-contraction inequality for Rademacher complexities}.
\newblock In Hans Ulrich~Simon Ronald~Ortner and Sandra Zilles, editors,
  \emph{Algorithmic Learning Theory}, pages 3--17. Springer International
  Publishing, 2016.

\bibitem[Mei and Montanari(2019)]{reg:mei2019generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: {P}recise
  asymptotics and double descent curve.
\newblock \emph{arXiv preprint arXiv:1908.05355}, 2019.

\bibitem[Mitra(2019)]{reg:mitra2019understanding}
Partha~P Mitra.
\newblock Understanding overfitting peaks in generalization error: Analytical
  risk curves for $\ell_2 $ and $\ell_1 $ penalized interpolation.
\newblock \emph{arXiv preprint arXiv:1906.03667}, 2019.

\bibitem[Montanari et~al.(2019)Montanari, Ruan, Sohn, and
  Yan]{classification_cgmt:montanari2019generalization}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock \emph{arXiv preprint arXiv:1911.01544}, 2019.

\bibitem[Muthukumar et~al.(2020)Muthukumar, Vodrahalli, Subramanian, and
  Sahai]{reg:muthukumar2020harmless}
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai.
\newblock Harmless interpolation of noisy data in regression.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  1\penalty0 (1):\penalty0 67--83, 2020.

\bibitem[Muthukumar et~al.(2021)Muthukumar, Narang, Subramanian, Belkin, Hsu,
  and Sahai]{binary:Muth20}
Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin,
  Daniel~J. Hsu, and Anant Sahai.
\newblock Classification vs regression in overparameterized regimes: Does the
  loss function matter?
\newblock \emph{Journal of Machine Learning Research}, 22:\penalty0
  222:1--222:69, 2021.

\bibitem[Nacson et~al.(2019)Nacson, Lee, Gunasekar, Savarese, Srebro, and
  Soudry]{implicit_bias:nacson2019convergence}
Mor~Shpigel Nacson, Jason Lee, Suriya Gunasekar, Pedro Henrique~Pamplona
  Savarese, Nathan Srebro, and Daniel Soudry.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence
  and Statistics}, pages 3420--3428, 2019.

\bibitem[{Nakkiran}(2019)]{reg:nakkiran}
Preetum {Nakkiran}.
\newblock {More Data Can Hurt for Linear Regression: Sample-wise Double
  Descent}.
\newblock \emph{arXiv e-prints}, art. arXiv:1912.07242, December 2019.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{reg:neyshabur2014search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Pires and {Szepesv{\'a}ri}(2016)]{consistency:Pires16}
Bernardo~{\'A}vila Pires and Csaba {Szepesv{\'a}ri}.
\newblock {Multiclass Classification Calibration Functions}.
\newblock \emph{arXiv e-prints}, art. arXiv:1609.06385, September 2016.

\bibitem[Pires et~al.(2013)Pires, Ghavamzadeh, and
  Szepesv\'{a}ri]{consistency:Pires2013CostsensitiveMC}
Bernardo~\'{A}vila Pires, Mohammad Ghavamzadeh, and Csaba Szepesv\'{a}ri.
\newblock Cost-sensitive multiclass classification risk bounds.
\newblock In \emph{Proceedings of the 30th International Conference on
  International Conference on Machine Learning - Volume 28}, pages 1391--1399,
  2013.

\bibitem[Rawat et~al.(2019)Rawat, Chen, Yu, Suresh, and
  Kumar]{extreme:rawat2019sampled}
Ankit~Singh Rawat, Jiecao Chen, Felix Xinnan~X Yu, Ananda~Theertha Suresh, and
  Sanjiv Kumar.
\newblock Sampled softmax with random fourier features.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Richards et~al.(2021)Richards, Mourtada, and
  Rosasco]{reg:richards2021asymptotics}
Dominic Richards, Jaouad Mourtada, and Lorenzo Rosasco.
\newblock Asymptotics of ridge (less) regression under general source
  condition.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3889--3897. PMLR, 2021.

\bibitem[Rifkin and Klautau(2004)]{algo_comparison:Rifkin04}
Ryan Rifkin and Aldebaro Klautau.
\newblock In defense of one-vs-all classification.
\newblock \emph{Journal of Machine Learning Research}, 5:\penalty0 101--141, 12
  2004.

\bibitem[Rudelson and Vershynin(2013)]{rudelson2013hanson}
Mark Rudelson and Roman Vershynin.
\newblock {Hanson-Wright inequality and sub-Gaussian concentration}.
\newblock \emph{Electronic Communications in Probability}, 18:\penalty0 1--9,
  2013.

\bibitem[Salehi et~al.(2019)Salehi, Abbasi, and
  Hassibi]{classification_cgmt:salehi2019impact}
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi.
\newblock The impact of regularization on high-dimensional logistic regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Shamir(2022)]{binary_classification:shamir2022implicit}
Ohad Shamir.
\newblock The implicit bias of benign overfitting.
\newblock \emph{arXiv preprint arXiv:2201.11489}, 2022.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{implicit_bias:srebro}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Taheri et~al.(2020)Taheri, Pedarsani, and
  Thrampoulidis]{classification_cgmt:taheri2020sharp}
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis.
\newblock Sharp asymptotics and optimal performance for inference in binary
  models.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3739--3749. PMLR, 2020.

\bibitem[Taheri et~al.(2021)Taheri, Pedarsani, and
  Thrampoulidis]{classification_cgmt:taheri2021fundamental}
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis.
\newblock Fundamental limits of ridge-regularized empirical risk minimization
  in high dimensions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2773--2781. PMLR, 2021.

\bibitem[Tewari and Bartlett(2005)]{consistency:Tewari05}
Ambuj Tewari and Peter Bartlett.
\newblock On the consistency of multiclass classification methods.
\newblock \emph{Journal of Machine Learning Research}, 8:\penalty0 143--157, 01
  2005.

\bibitem[Thrampoulidis et~al.(2020)Thrampoulidis, Oymak, and
  Soltanolkotabi]{multi_class_theory:Thram20}
Christos Thrampoulidis, Samet Oymak, and Mahdi Soltanolkotabi.
\newblock Theoretical insights into multiclass classification: A
  high-dimensional asymptotic view.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 8907--8920. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/6547884cea64550284728eb26b0947ef-Paper.pdf}.

\bibitem[Tripuraneni et~al.(2021)Tripuraneni, Adlam, and
  Pennington]{covariate_shift}
Nilesh Tripuraneni, Ben Adlam, and Jeffrey Pennington.
\newblock Overparameterization improves robustness to covariate shift in high
  dimensions.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 13883--13897. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/73fed7fd472e502d8908794430511f4d-Paper.pdf}.

\bibitem[Tsigler and Bartlett(2020)]{reg:tsigler2020benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[Wang et~al.(2021)Wang, Donhauser, and Yang]{reg:wang2021tight}
Guillaume Wang, Konstantin Donhauser, and Fanny Yang.
\newblock Tight bounds for minimum $\ell_1$-norm interpolation of noisy data.
\newblock \emph{arXiv preprint arXiv:2111.05987}, 2021.

\bibitem[Wang and Thrampoulidis(2021)]{binary_classification:wang2021benign}
Ke~Wang and Christos Thrampoulidis.
\newblock {Benign overfitting in binary classification of Gaussian mixtures}.
\newblock In \emph{ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 4030--4034. IEEE, 2021.

\bibitem[{Wang} et~al.(2021){Wang}, {Muthukumar}, and
  {Thrampoulidis}]{multi_class_theory:Wang21}
Ke~{Wang}, Vidya {Muthukumar}, and Christos {Thrampoulidis}.
\newblock {Benign Overfitting in Multiclass Classification: All Roads Lead to
  Interpolation}.
\newblock \emph{arXiv e-prints}, art. arXiv:2106.10865, June 2021.

\bibitem[Weston and Watkins(1998)]{algo_multiclass:Weston98}
Jason Weston and Chris Watkins.
\newblock Multi-class support vector machines.
\newblock Technical report, 1998.

\bibitem[Woodworth et~al.(2019)Woodworth, Gunasekar, Lee, Soudry, and
  Srebro]{implicit_bias:woodworth2019kernel}
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and deep regimes in overparametrized models.
\newblock \emph{arXiv preprint arXiv:1906.05827}, 2019.

\bibitem[Wu and Xu(2020)]{reg:wu2020optimal}
Denny Wu and Ji~Xu.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10112--10123, 2020.

\bibitem[Wu et~al.(2020)Wu, Zou, Braverman, and
  Gu]{implicit_bias:wu2020direction}
Jingfeng Wu, Difan Zou, Vladimir Braverman, and Quanquan Gu.
\newblock Direction matters: On the implicit bias of stochastic gradient
  descent with moderate learning rate.
\newblock \emph{arXiv preprint arXiv:2011.02538}, 2020.

\bibitem[Yen et~al.(2016)Yen, Huang, Ravikumar, Zhong, and
  Dhillon]{extreme:yen2016pd}
Ian En-Hsu Yen, Xiangru Huang, Pradeep Ravikumar, Kai Zhong, and Inderjit
  Dhillon.
\newblock Pd-sparse: A primal and dual sparse approach to extreme multiclass
  and multilabel classification.
\newblock In \emph{International conference on machine learning}, pages
  3069--3077. PMLR, 2016.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{reg:zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang(2004)]{consistency:Zhang2004StatisticalAO}
Tong Zhang.
\newblock Statistical analysis of some multi-category large margin
  classification methods.
\newblock \emph{Journal of Machine Learning Research}, 5:\penalty0 1225--1251,
  2004.

\end{thebibliography}
