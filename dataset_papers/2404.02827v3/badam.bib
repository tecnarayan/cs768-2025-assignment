@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
  journal={arXiv preprint arXiv:2305.18290},
  year={2023}
}

@article{peng2023instruction,
  title={Instruction tuning with {GPT}-4},
  author={Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2304.03277},
  year={2023}
}

@article{achiam2023gpt,
  title={{GPT}-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{touvron2023llama,
  title={{Llama} 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with {GPT}-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@inproceedings{volske2017tl,
  title={Tl; dr: Mining reddit to learn automatic summarization},
  author={V{\"o}lske, Michael and Potthast, Martin and Syed, Shahbaz and Stein, Benno},
  booktitle={Proceedings of the Workshop on New Frontiers in Summarization},
  pages={59--63},
  year={2017}
}

@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}

@article{mtbench,
  title={Judging {LLM}-as-a-judge with {MT}-bench and chatbot arena},
  author={Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{liu2019roberta,
  title={{RoBERTa}: A robustly optimized {BERT} pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{lomo,
  title={Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author={Lv, Kai and Yang, Yuqing and Liu, Tengxiao and Gao, Qinghui and Guo, Qipeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2306.09782},
  year={2023}
}

@article{mezo,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{sung2022lst,
  title     = {LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning},
  author    = {Yi-Lin Sung and Jaemin Cho and Mohit Bansal},
  journal   = {NeurIPS},
  year      = {2022}
}

@article{tseng2001convergence,
  title={Convergence of a block coordinate descent method for nondifferentiable minimization},
  author={Tseng, Paul},
  journal={Journal of Optimization Theory and Applications},
  volume={109},
  pages={475--494},
  year={2001},
  publisher={Springer}
}

@article{wright2015coordinate,
  title={Coordinate descent algorithms},
  author={Wright, Stephen J},
  journal={Mathematical Programming},
  volume={151},
  number={1},
  pages={3--34},
  year={2015},
  publisher={Springer}
}

@article{nesterov2012efficiency,
  title={Efficiency of coordinate descent methods on huge-scale optimization problems},
  author={Nesterov, Yu},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={2},
  pages={341--362},
  year={2012},
  publisher={SIAM}
}

@article{chang2011libsvm,
  title={{LIBSVM}: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on Intelligent Systems and Technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@inproceedings{lester2021power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    year = "2021",
    journal = "Association for Computational Linguistics"
}

@inproceedings{relora,
  title={{R}e{L}o{RA}: High-Rank Training Through Low-Rank Updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{chainoflora,
  title={Chain of {LoRA}: Efficient fine-tuning of language models via residual learning},
  author={Xia, Wenhan and Qin, Chengwei and Hazan, Elad},
  journal={arXiv preprint arXiv:2401.04151},
  year={2024}
}

@inproceedings{nola,
  title={{NOLA}: Networks as Linear Combination of Low Rank Random Basis},
  author={Koohpayegani, Soroush Abbasi and Navaneet, KL and Nooralinejad, Parsa and Kolouri, Soheil and Pirsiavash, Hamed},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@inproceedings{vera,
title={{V}e{RA}: Vector-based Random Matrix Adaptation},
author={Dawid Jan Kopiczko and Tijmen Blankevoort and Yuki M Asano},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024}
}

@article{qlora,
  title={{QLoRA}: Efficient finetuning of quantized {LLM}s},
  author={Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{galore,
  title={GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{liu2023winnertakeall,
  title   = {Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model},
  author  = {Zirui Liu and Guanchu Wang and Shaochen Zhong and Zhaozhuo Xu and Daochen Zha and Ruixiang Tang and Zhimeng Jiang and Kaixiong Zhou and Vipin Chaudhary and Shuai Xu and Xia Hu},
  year    = {2023},
  journal = {NEURIPS}
}

@article{lora,
  title={{LoRA}: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{adapter,
  title={Parameter-efficient transfer learning for {NLP}},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}

@inproceedings{prefix,
  title={Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing},
  pages={4582--4597},
  year={2021}
}

@inproceedings{prompting,
  title={The Power of Scale for Parameter-Efficient Prompt Tuning},
  author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages={3045--3059},
  year={2021}
}

@article{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford {A}lpaca: An Instruction-following {LLaMA} model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository}
}

@article{unsloth,
  author = {Daniel Han and Michael Han},
  title = {Unsloth},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository}
}

@article{wang2019superglue,
  title={Super{GLUE}: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{chen2016training,
  title={Training deep nets with sublinear memory cost},
  author={Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal={arXiv preprint arXiv:1604.06174},
  year={2016}
}

@article{chakrabarti2024block,
  title={Block-coordinate methods and restarting for solving extensive-form games},
  author={Chakrabarti, Darshan and Diakonikolas, Jelena and Kroer, Christian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{cai2023cyclic,
  title={Cyclic block coordinate descent with variance reduction for composite nonconvex optimization},
  author={Cai, Xufeng and Song, Chaobing and Wright, Stephen and Diakonikolas, Jelena},
  booktitle={International Conference on Machine Learning},
  pages={3469--3494},
  year={2023},
  organization={PMLR}
}

@article{zhao2022randomized,
  title={Randomized Coordinate Subgradient Method for Nonsmooth Optimization},
  author={Zhao, Lei and Chen, Ding and Zhu, Daoli and Li, Xiao},
  journal={arXiv preprint arXiv:2206.14981},
  year={2022}
}

@article{chorobura2023random,
  title={Random coordinate descent methods for nonseparable composite optimization},
  author={Chorobura, Flavia and Necoara, Ion},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={3},
  pages={2160--2190},
  year={2023},
  publisher={SIAM}
}

@article{zhang2024scaling,
  title={When Scaling Meets {LLM} Finetuning: The Effect of Data, Model and Finetuning Method},
  author={Zhang, Biao and Liu, Zhongtao and Cherry, Colin and Firat, Orhan},
  journal={The Twelfth International Conference on Learning Representations},
  year={2024}
}

@article{he2021towards,
  title     = {Towards a Unified View of Parameter-Efficient Transfer Learning},
  author    = {Junxian He and Chunting Zhou and Xuezhe Ma and Taylor Berg-Kirkpatrick and Graham Neubig},
  journal   = {International Conference on Learning Representations},
  year      = {2021}
}

@article{zheng2024llamafactory,
  title={Llama{F}actory: Unified Efficient Fine-Tuning of 100+ Language Models},
  author={Yaowei Zheng and Richong Zhang and Junhao Zhang and Yanhan Ye and Zheyan Luo and Yongqiang Ma},
  journal={arXiv preprint arXiv:2403.13372},
  year={2024},
  url={http://arxiv.org/abs/2403.13372}
}

@article{hinton2006fast,
  title={A fast learning algorithm for deep belief nets},
  author={Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
  journal={Neural Computation},
  volume={18},
  number={7},
  pages={1527--1554},
  year={2006},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}


@article{bengio2006greedy,
  title={Greedy layer-wise training of deep networks},
  author={Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  journal={Advances in Neural Information Processing Systems},
  volume={19},
  year={2006}
}

@book{ortega1970iterative,
  title={Iterative Solution of Nonlinear Equations in Several Variables},
  author={Ortega, J.M. and Rheinboldt, W.C.},
  volume={30},
  year={1970},
  publisher={SIAM}
}

@article{luo1992convergence,
  title={On the convergence of the coordinate descent method for convex differentiable minimization},
  author={Luo, Zhi-Quan and Tseng, Paul},
  journal={Journal of Optimization Theory and Applications},
  volume={72},
  number={1},
  pages={7--35},
  year={1992},
  publisher={Springer}
}

@article{bertsekas1989parallel,
  title={Parallel and distributed computation},
  author={Bertsekas, Dimitri P.  and Tsitsiklis, John N. },
  journal={Prentice-Hall},
  year={1989},
  publisher={Old Tappan, NJ (USA); Prentice Hall Inc.}
}

@article{beck2013convergence,
  title={On the convergence of block coordinate descent type methods},
  author={Beck, Amir and Tetruashvili, Luba},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2037--2060},
  year={2013},
  publisher={SIAM}
}

@article{richtarik2014iteration,
  title={Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function},
  author={Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
  journal={Mathematical Programming},
  volume={144},
  number={1},
  pages={1--38},
  year={2014},
  publisher={Springer}
}

@article{lu2015complexity,
  title={On the complexity analysis of randomized block-coordinate descent methods},
  author={Lu, Zhaosong and Xiao, Lin},
  journal={Mathematical Programming},
  volume={152},
  pages={615--642},
  year={2015},
  publisher={Springer}
}

@article{song2023cyclic,
  title={Cyclic Coordinate Dual Averaging with Extrapolation},
  author={Song, Chaobing and Diakonikolas, Jelena},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={4},
  pages={2935--2961},
  year={2023},
  publisher={SIAM}
}


@article{chen2016direct,
  title={The direct extension of {ADMM} for multi-block convex minimization problems is not necessarily convergent},
  author={Chen, Caihua and He, Bingsheng and Ye, Yinyu and Yuan, Xiaoming},
  journal={Mathematical Programming},
  volume={155},
  number={1},
  pages={57--79},
  year={2016},
  publisher={Springer}
}

@article{sun2015expected,
  title={On the expected convergence of randomly permuted {ADMM}},
  author={Sun, Ruoyu and Luo, Zhi-Quan and Ye, Yinyu},
  journal={arXiv preprint arXiv:1503.06387},
  volume={4},
  number={6},
  year={2015}
}

@article{sun2021worst,
  title={Worst-case complexity of cyclic coordinate descent: O (n\^{} 2)  gap with randomized version},
  author={Sun, Ruoyu and Ye, Yinyu},
  journal={Mathematical Programming},
  volume={185},
  pages={487--520},
  year={2021},
  publisher={Springer}
}

@article{mihic2021managing,
  title={Managing randomization in the multi-block alternating direction method of multipliers for quadratic optimization},
  author={Mihi{\'c}, Kre{\v{s}}imir and Zhu, Mingxi and Ye, Yinyu},
  journal={Mathematical Programming Computation},
  volume={13},
  number={2},
  pages={339--413},
  year={2021},
  publisher={Springer}
}

@article{gurbuzbalaban2017cyclic,
  title={When cyclic coordinate descent outperforms randomized coordinate descent},
  author={Gurbuzbalaban, Mert and Ozdaglar, Asuman and Parrilo, Pablo A and Vanli, Nuri},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{belilovsky2019greedy,
  title={Greedy layerwise learning can scale to imagenet},
  author={Belilovsky, Eugene and Eickenberg, Michael and Oyallon, Edouard},
  booktitle={International Conference on Machine Learning},
  pages={583--593},
  year={2019}
}

@article{pan2024lisa,
  title={{LISA}: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning},
  author={Pan, Rui and Liu, Xiang and Diao, Shizhe and Pi, Renjie and Zhang, Jipeng and Han, Chi and Zhang, Tong},
  journal={arXiv preprint arXiv:2403.17919},
  year={2024}
}

@misc{
zhu2024lift,
title={{LIFT}: Efficient Layer-wise Fine-tuning for Large Model Models},
author={Ligeng Zhu and Lanxiang Hu and Ji Lin and Song Han},
year={2024},
url={https://openreview.net/forum?id=u0INlprg3U}
}

@article{liu2024hift,
  title={{HiFT}: A Hierarchical Full Parameter Fine-Tuning Strategy},
  author={Liu, Yongkang and Zhang, Yiqun and Li, Qian and Feng, Shi and Wang, Daling and Zhang, Yifei and Sch{\"u}tze, Hinrich},
  journal={arXiv preprint arXiv:2401.15207},
  year={2024}
}

@article{llama3,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{zhao2024galore,
  title={Galore: Memory-efficient {LLM} training by gradient low-rank projection},
  author={Zhao, Jiawei and Zhang, Zhenyu and Chen, Beidi and Wang, Zhangyang and Anandkumar, Anima and Tian, Yuandong},
  journal={arXiv preprint arXiv:2403.03507},
  year={2024}
}

@article{li2024convergence,
  title={Convergence of {A}dam under relaxed assumptions},
  author={Li, Haochuan and Rakhlin, Alexander and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{zhang2022adam,
  title={Adam can converge without any modification on update rules},
  author={Zhang, Yushun and Chen, Congliang and Shi, Naichen and Sun, Ruoyu and Luo, Zhi-Quan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28386--28399},
  year={2022}
}

@article{defossez2022a,
title={A Simple Convergence Proof of Adam and Adagrad},
author={Alexandre D{\'e}fossez and Leon Bottou and Francis Bach and Nicolas Usunier},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2022},
url={https://openreview.net/forum?id=ZPQhzTSWA7}
}

@article{guo2021novel,
  title   = {A Novel Convergence Analysis for Algorithms of the Adam Family and Beyond},
  author  = {Zhishuai Guo and Yi Xu and Wotao Yin and Rong Jin and Tianbao Yang},
  year    = {2021},
  journal = {arXiv preprint arXiv: 2104.14840}
}

@article{ren2021zerooffload,
  title     = {ZeRO-Offload: Democratizing Billion-Scale Model Training},
  author    = {Jie Ren and Samyam Rajbhandari and Reza Yazdani Aminabadi and Olatunji Ruwase and Shuangyang Yang and Minjia Zhang and Dong Li and Yuxiong He},
  journal   = {USENIX Annual Technical Conference},
  year      = {2021}
}

@article{ren2021zero,
  title={ZeRO-Offload: Democratizing Billion-Scale Model Training},
  author={Ren, Jie and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Ruwase, Olatunji and Yang, Shuangyan and Zhang, Minjia and Li, Dong and He, Yuxiong},
  journal={arXiv preprint arXiv:2101.06840},
  year={2021}
}

@article{gpt-3,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{instructgpt,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{wang2024closing,
  title={Closing the gap between the upper bound and lower bound of Adam's iteration complexity},
  author={Wang, Bohan and Fu, Jingwen and Zhang, Huishuai and Zheng, Nanning and Chen, Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@inproceedings{rajbhandari2021zero,
  title={Zero-infinity: Breaking the gpu memory wall for extreme scale deep learning},
  author={Rajbhandari, Samyam and Ruwase, Olatunji and Rasley, Jeff and Smith, Shaden and He, Yuxiong},
  booktitle={Proceedings of the international conference for high performance computing, networking, storage and analysis},
  pages={1--14},
  year={2021}
}

@inproceedings{jiant,
  title={jiant: A software toolkit for research on general-purpose text understanding models},
  author={Pruksachatkun, Yada and Yeres, Phil and Liu, Haokun and Phang, Jason and Htut, Phu Mon and Wang, Alex and Tenney, Ian and Bowman, Samuel R},
  booktitle={58th Annual Meeting of the Association for Computational Linguistics, ACL 2020},
  pages={109--117},
  year={2020}
}

@article{nutini2022let,
  title={Let's make block coordinate descent converge faster: faster greedy rules, message-passing, active-set complexity, and superlinear convergence},
  author={Nutini, Julie and Laradji, Issam and Schmidt, Mark},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={131},
  pages={1--74},
  year={2022}
}

@article{zhuang2023efficiently,
  title={Efficiently measuring the cognitive ability of {LLM}s: An adaptive testing perspective},
  author={Zhuang, Yan and Liu, Qi and Ning, Yuting and Huang, Weizhe and Lv, Rui and Huang, Zhenya and Zhao, Guanhao and Zhang, Zheng and Mao, Qingyang and Wang, Shijin and others},
  journal={arXiv preprint arXiv:2306.10512},
  year={2023}
}

@inproceedings{zhuang2023open,
  title={Open-source Large Language Models are Strong Zero-shot Query Likelihood Models for Document Ranking},
  author={Zhuang, Shengyao and Liu, Bing and Koopman, Bevan and Zuccon, Guido},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{yang2024harnessing,
  title={Harnessing the power of {LLM}s in practice: A survey on {ChatGPT} and beyond},
  author={Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
  journal={ACM Transactions on Knowledge Discovery from Data},
  volume={18},
  number={6},
  pages={1--32},
  year={2024}
}

@article{yue2023mammoth,
  title={Mammoth: Building math generalist models through hybrid instruction tuning},
  author={Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal={arXiv preprint arXiv:2309.05653},
  year={2023}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{math,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@inproceedings{aqua,
    title = "Program Induction by Rationale Generation: Learning to Solve and Explain Algebraic Word Problems",
    author = "Ling, Wang  and
      Yogatama, Dani  and
      Dyer, Chris  and
      Blunsom, Phil",
    editor = "Barzilay, Regina  and
      Kan, Min-Yen",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P17-1015",
    doi = "10.18653/v1/P17-1015",
    pages = "158--167",
}

@inproceedings{numglue,
    title = "{N}um{GLUE}: A Suite of Fundamental yet Challenging Mathematical Reasoning Tasks",
    author = "Mishra, Swaroop  and
      Mitra, Arindam  and
      Varshney, Neeraj  and
      Sachdeva, Bhavdeep  and
      Clark, Peter  and
      Baral, Chitta  and
      Kalyan, Ashwin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    pages = "3505--3523",
}

@inproceedings{sat-math,
    title = "{AGIE}val: A Human-Centric Benchmark for Evaluating Foundation Models",
    author = "Zhong, Wanjun  and
      Cui, Ruixiang  and
      Guo, Yiduo  and
      Liang, Yaobo  and
      Lu, Shuai  and
      Wang, Yanlin  and
      Saied, Amin  and
      Chen, Weizhu  and
      Duan, Nan",
    booktitle = "Findings of the Association for Computational Linguistics: NAACL 2024",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    pages = "2299--2314",
}

@article{mmlu,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2009.03300},
  year={2020}
}

@article{li2023starcoder,
  title={Starcoder: may the source be with you!},
  author={Li, Raymond and Allal, Loubna Ben and Zi, Yangtian and Muennighoff, Niklas and Kocetkov, Denis and Mou, Chenghao and Marone, Marc and Akiki, Christopher and Li, Jia and Chim, Jenny and others},
  journal={arXiv preprint arXiv:2305.06161},
  year={2023}
}

@article{adam-mini,
  title={Adam-mini: Use fewer learning rates to gain more},
  author={Zhang, Yushun and Chen, Congliang and Li, Ziniu and Ding, Tian and Wu, Chenwei and Ye, Yinyu and Luo, Zhi-Quan and Sun, Ruoyu},
  journal={arXiv preprint arXiv:2406.16793},
  year={2024}
}