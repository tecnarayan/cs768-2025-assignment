\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghbalou and Staerman(2023)]{aghbalou2023hypothesis}
Anass Aghbalou and Guillaume Staerman.
\newblock Hypothesis transfer learning with surrogate classification losses: Generalization bounds through algorithmic stability.
\newblock In \emph{International Conference on Machine Learning}, pages 280--303. PMLR, 2023.

\bibitem[Bastani(2021)]{bastani2021predicting}
Hamsa Bastani.
\newblock Predicting with proxies: Transfer learning in high dimension.
\newblock \emph{Management Science}, 67\penalty0 (5):\penalty0 2964--2984, 2021.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzev, and Rosasco]{bauer2007regularization}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Cai and Pu(2022)]{cai2022transfer}
T~Tony Cai and Hongming Pu.
\newblock Transfer learning for nonparametric regression: Non-asymptotic minimax analysis and adaptive procedure.
\newblock \emph{arXiv preprint arXiv:0000.0000}, 2022.

\bibitem[Caponnetto and De~Vito(2007)]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito.
\newblock Optimal rates for the regularized least-squares algorithm.
\newblock \emph{Foundations of Computational Mathematics}, 7:\penalty0 331--368, 2007.

\bibitem[Cheng and Shang(2015)]{cheng2015joint}
Guang Cheng and Zuofeng Shang.
\newblock Joint asymptotics for semi-nonparametric regression models with partially linear structure.
\newblock \emph{The Annals of Statistics}, 43\penalty0 (3):\penalty0 1351--1390, 2015.

\bibitem[De~Vito et~al.(2010)De~Vito, Pereverzyev, and Rosasco]{de2010adaptive}
Ernesto De~Vito, Sergei Pereverzyev, and Lorenzo Rosasco.
\newblock Adaptive kernel methods using the balancing principle.
\newblock \emph{Foundations of Computational Mathematics}, 10:\penalty0 455--479, 2010.

\bibitem[DeVore and Sharpley(1993)]{devore1993besov}
Ronald~A DeVore and Robert~C Sharpley.
\newblock Besov spaces on domains in r\^{}$\{$d$\}$.
\newblock \emph{Transactions of the American Mathematical Society}, 335\penalty0 (2):\penalty0 843--864, 1993.

\bibitem[Dicker et~al.(2017)Dicker, Foster, and Hsu]{dicker2017kernel}
Lee~H Dicker, Dean~P Foster, and Daniel Hsu.
\newblock Kernel ridge vs. principal component regression: Minimax bounds and the qualification of regularization operators.
\newblock 2017.

\bibitem[Du et~al.(2017)Du, Koushik, Singh, and P{\'o}czos]{du2017hypothesis}
Simon~S Du, Jayanth Koushik, Aarti Singh, and Barnab{\'a}s P{\'o}czos.
\newblock Hypothesis transfer learning via transformation functions.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Du et~al.(2020)Du, Hu, Kakade, Lee, and Lei]{du2020few}
Simon~S Du, Wei Hu, Sham~M Kakade, Jason~D Lee, and Qi~Lei.
\newblock Few-shot learning via learning the representation, provably.
\newblock \emph{arXiv preprint arXiv:2002.09434}, 2020.

\bibitem[Duan and Wang(2022)]{duan2022adaptive}
Yaqi Duan and Kaizheng Wang.
\newblock Adaptive and robust multi-task learning.
\newblock \emph{arXiv preprint arXiv:2202.05250}, 2022.

\bibitem[Eberts and Steinwart(2013)]{eberts2013optimal}
Mona Eberts and Ingo Steinwart.
\newblock {Optimal regression rates for SVMs using Gaussian kernels}.
\newblock \emph{Electronic Journal of Statistics}, 7\penalty0 (none):\penalty0 1 -- 42, 2013.
\newblock \doi{10.1214/12-EJS760}.
\newblock URL \url{https://doi.org/10.1214/12-EJS760}.

\bibitem[Fasshauer and Ye(2011)]{fasshauer2011reproducing}
Gregory~E Fasshauer and Qi~Ye.
\newblock Reproducing kernels of generalized sobolev spaces via a green function approach with distributional operators.
\newblock \emph{Numerische Mathematik}, 119:\penalty0 585--611, 2011.

\bibitem[Fischer and Steinwart(2020)]{fischer2020sobolev}
Simon Fischer and Ingo Steinwart.
\newblock Sobolev norm learning rates for regularized least-squares algorithms.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 8464--8501, 2020.

\bibitem[Gao et~al.(2008)Gao, Fan, Jiang, and Han]{gao2008knowledge}
Jing Gao, Wei Fan, Jing Jiang, and Jiawei Han.
\newblock Knowledge transfer via multiple model local structure mapping.
\newblock In \emph{Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining}, pages 283--291, 2008.

\bibitem[Geer(2000)]{geer2000empirical}
Sara~A Geer.
\newblock \emph{Empirical Processes in M-estimation}, volume~6.
\newblock Cambridge university press, 2000.

\bibitem[Hamm and Steinwart(2021)]{hamm2021adaptive}
Thomas Hamm and Ingo Steinwart.
\newblock Adaptive learning rates for support vector machines working on data with low intrinsic dimension.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (6):\penalty0 3153--3180, 2021.

\bibitem[Kanagawa et~al.(2018)Kanagawa, Hennig, Sejdinovic, and Sriperumbudur]{kanagawa2018gaussian}
Motonobu Kanagawa, Philipp Hennig, Dino Sejdinovic, and Bharath~K Sriperumbudur.
\newblock Gaussian processes and kernel methods: A review on connections and equivalences.
\newblock \emph{arXiv preprint arXiv:1807.02582}, 2018.

\bibitem[Kuzborskij and Orabona(2013)]{kuzborskij2013stability}
Ilja Kuzborskij and Francesco Orabona.
\newblock Stability and hypothesis transfer learning.
\newblock In \emph{International Conference on Machine Learning}, pages 942--950. PMLR, 2013.

\bibitem[Kuzborskij and Orabona(2017)]{kuzborskij2017fast}
Ilja Kuzborskij and Francesco Orabona.
\newblock Fast rates by transferring from auxiliary hypotheses.
\newblock \emph{Machine Learning}, 106:\penalty0 171--195, 2017.

\bibitem[Lepskii(1991)]{lepskii1991problem}
OV~Lepskii.
\newblock On a problem of adaptive estimation in gaussian white noise.
\newblock \emph{Theory of Probability \& Its Applications}, 35\penalty0 (3):\penalty0 454--466, 1991.

\bibitem[Li et~al.(2022)Li, Cai, and Li]{li2022transfer}
Sai Li, T~Tony Cai, and Hongzhe Li.
\newblock Transfer learning for high-dimensional linear regression: Prediction, estimation and minimax optimality.
\newblock \emph{Journal of the Royal Statistical Society Series B: Statistical Methodology}, 84\penalty0 (1):\penalty0 149--173, 2022.

\bibitem[Li and Yuan(2019)]{li2019optimality}
Tong Li and Ming Yuan.
\newblock On the optimality of gaussian kernel based nonparametric tests against smooth alternatives.
\newblock \emph{arXiv preprint arXiv:1909.03302}, 2019.

\bibitem[Li and Bilmes(2007)]{li2007bayesian}
Xiao Li and Jeff Bilmes.
\newblock A bayesian divergence prior for classiffier adaptation.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 275--282. PMLR, 2007.

\bibitem[Li et~al.(2023)Li, Zhang, and Lin]{li2023saturation}
Yicheng Li, Haobo Zhang, and Qian Lin.
\newblock On the saturation effect of kernel ridge regression.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.

\bibitem[Lin and Reimherr(2022)]{lin2022transfer}
Haotian Lin and Matthew Reimherr.
\newblock On transfer learning in functional linear regression.
\newblock \emph{arXiv preprint arXiv:2206.04277}, 2022.

\bibitem[Ma et~al.(2022)Ma, Pathak, and Wainwright]{ma2022optimally}
Cong Ma, Reese Pathak, and Martin~J Wainwright.
\newblock Optimally tackling covariate shift in rkhs-based nonparametric regression.
\newblock \emph{arXiv preprint arXiv:2205.02986}, 2022.

\bibitem[Mendelson and Neeman(2010)]{mendelson2010regularization}
Shahar Mendelson and Joseph Neeman.
\newblock Regularization in kernel learning.
\newblock 2010.

\bibitem[Orabona et~al.(2009)Orabona, Castellini, Caputo, Fiorilla, and Sandini]{orabona2009model}
Francesco Orabona, Claudio Castellini, Barbara Caputo, Angelo~Emanuele Fiorilla, and Giulio Sandini.
\newblock Model adaptation with least-squares svm for adaptive hand prosthetics.
\newblock In \emph{2009 IEEE international conference on robotics and automation}, pages 2897--2903. IEEE, 2009.

\bibitem[Stein(1999)]{stein1999interpolation}
Michael~L Stein.
\newblock \emph{Interpolation of spatial data: some theory for kriging}.
\newblock Springer Science \& Business Media, 1999.

\bibitem[Steinwart and Christmann(2008)]{steinwart2008support}
Ingo Steinwart and Andreas Christmann.
\newblock \emph{Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Steinwart et~al.(2006)Steinwart, Hush, and Scovel]{steinwart2006explicit}
Ingo Steinwart, Don Hush, and Clint Scovel.
\newblock An explicit description of the reproducing kernel hilbert spaces of gaussian rbf kernels.
\newblock \emph{IEEE Transactions on Information Theory}, 52\penalty0 (10):\penalty0 4635--4643, 2006.

\bibitem[Steinwart et~al.(2009)Steinwart, Hush, Scovel, et~al.]{steinwart2009optimal}
Ingo Steinwart, Don~R Hush, Clint Scovel, et~al.
\newblock Optimal rates for regularized least squares regression.
\newblock In \emph{COLT}, pages 79--93, 2009.

\bibitem[Tian and Feng(2022)]{tian2022transfer}
Ye~Tian and Yang Feng.
\newblock Transfer learning under high-dimensional generalized linear models.
\newblock \emph{Journal of the American Statistical Association}, pages 1--14, 2022.

\bibitem[Tian et~al.(2023)Tian, Gu, and Feng]{tian2023learning}
Ye~Tian, Yuqi Gu, and Yang Feng.
\newblock Learning from similar linear representations: Adaptivity, minimaxity, and robustness.
\newblock \emph{arXiv preprint arXiv:2303.17765}, 2023.

\bibitem[Tripuraneni et~al.(2020)Tripuraneni, Jordan, and Jin]{tripuraneni2020theory}
Nilesh Tripuraneni, Michael Jordan, and Chi Jin.
\newblock On the theory of transfer learning: The importance of task diversity.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 7852--7862, 2020.

\bibitem[Varshamov(1957)]{varshamov1957estimate}
Rom~Rubenovich Varshamov.
\newblock Estimate of the number of signals in error correcting codes.
\newblock \emph{Docklady Akad. Nauk, SSSR}, 117:\penalty0 739--741, 1957.

\bibitem[Wang(2023)]{wang2023pseudo}
Kaizheng Wang.
\newblock Pseudo-labeling for kernel ridge regression under covariate shift.
\newblock \emph{arXiv preprint arXiv:2302.10160}, 2023.

\bibitem[Wang and Jing(2022)]{wang2022gaussian}
Wenjia Wang and Bing-Yi Jing.
\newblock Gaussian process regression: Optimality, robustness, and relationship with kernel ridge regression.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (193):\penalty0 1--67, 2022.

\bibitem[Wang and Schneider(2015)]{wang2015generalization}
Xuezhi Wang and Jeff~G Schneider.
\newblock Generalization bounds for transfer learning under model shift.
\newblock In \emph{UAI}, pages 922--931, 2015.

\bibitem[Wang et~al.(2016)Wang, Oliva, Schneider, and P{\'o}czos]{wang2016nonparametric}
Xuezhi Wang, Junier~B Oliva, Jeff~G Schneider, and Barnab{\'a}s P{\'o}czos.
\newblock Nonparametric risk and stability analysis for multi-task learning problems.
\newblock In \emph{IJCAI}, pages 2146--2152, 2016.

\bibitem[Wendland(2004)]{wendland2004scattered}
Holger Wendland.
\newblock \emph{Scattered data approximation}, volume~17.
\newblock Cambridge university press, 2004.

\bibitem[Xu and Tewari(2021)]{xu2021representation}
Ziping Xu and Ambuj Tewari.
\newblock Representation learning beyond linear prediction functions.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 4792--4804, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Li, Lu, and Lin]{zhang2023optimality}
Haobo Zhang, Yicheng Li, Weihao Lu, and Qian Lin.
\newblock On the optimality of misspecified kernel ridge regression.
\newblock \emph{arXiv preprint arXiv:2305.07241}, 2023.

\bibitem[Zhang et~al.(2022)Zhang, Blanchet, Ghosh, and Squillante]{zhang2022class}
Xuhui Zhang, Jose Blanchet, Soumyadip Ghosh, and Mark~S Squillante.
\newblock A class of geometric structures in transfer learning: Minimax bounds and optimality.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 3794--3820. PMLR, 2022.

\end{thebibliography}
