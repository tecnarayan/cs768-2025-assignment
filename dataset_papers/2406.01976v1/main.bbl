\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{layernorm}
Ba, L.~J., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{CoRR}, abs/1607.06450, 2016.
\newblock URL \url{http://arxiv.org/abs/1607.06450}.

\bibitem[Berglund et~al.(2024)Berglund, Tong, Kaufmann, Balesni, Stickland, Korbak, and Evans]{lmreverse}
Berglund, L., Tong, M., Kaufmann, M., Balesni, M., Stickland, A.~C., Korbak, T., and Evans, O.
\newblock The reversal curse: {LLM}s trained on {\textquotedblleft}a is b{\textquotedblright} fail to learn {\textquotedblleft}b is a{\textquotedblright}.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024}, 2024.
\newblock URL \url{https://openreview.net/forum?id=GPKTIktA0k}.

\bibitem[Biesialska et~al.(2020)Biesialska, Biesialska, and Costa{-}juss{\`{a}}]{clnlpreview}
Biesialska, M., Biesialska, K., and Costa{-}juss{\`{a}}, M.~R.
\newblock Continual lifelong learning in natural language processing: {A} survey.
\newblock In \emph{Proceedings of the 28th International Conference on Computational Linguistics, {COLING} 2020}, pp.\  6523--6541. International Committee on Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.COLING-MAIN.574}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.coling-main.574}.

\bibitem[Chaudhry et~al.(2019)Chaudhry, Ranzato, Rohrbach, and Elhoseiny]{agem}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with {A-GEM}.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019}. OpenReview.net, 2019.
\newblock URL \url{https://openreview.net/forum?id=Hkf2\_sC5FX}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert{-}Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{codex}
Chen, M., Tworek, J., Jun, H., Yuan, Q., de~Oliveira~Pinto, H.~P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.~P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert{-}Voss, A., Guss, W.~H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.~N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating large language models trained on code.
\newblock \emph{CoRR}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Chen et~al.(2020)Chen, Hou, Cui, Che, Liu, and Yu]{continuallmtask3}
Chen, S., Hou, Y., Cui, Y., Che, W., Liu, T., and Yu, X.
\newblock Recall and learn: Fine-tuning deep pretrained language models with less forgetting.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2020}, pp.\  7870--7881. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.emnlp-main.634}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.emnlp-main.634}.

\bibitem[Chrysostomou \& Aletras(2021)Chrysostomou and Aletras]{salientattention}
Chrysostomou, G. and Aletras, N.
\newblock Enjoy the salience: Towards better transformer-based faithful explanations with word salience.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021}, pp.\  8189--8200. Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/V1/2021.EMNLP-MAIN.645}.
\newblock URL \url{https://doi.org/10.18653/v1/2021.emnlp-main.645}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, Webson, Gu, Dai, Suzgun, Chen, Chowdhery, Narang, Mishra, Yu, Zhao, Huang, Dai, Yu, Petrov, Chi, Dean, Devlin, Roberts, Zhou, Le, and Wei]{flan}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang, X., Dehghani, M., Brahma, S., Webson, A., Gu, S.~S., Dai, Z., Suzgun, M., Chen, X., Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao, V.~Y., Huang, Y., Dai, A.~M., Yu, H., Petrov, S., Chi, E.~H., Dean, J., Devlin, J., Roberts, A., Zhou, D., Le, Q.~V., and Wei, J.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{CoRR}, abs/2210.11416, 2022.
\newblock \doi{10.48550/arXiv.2210.11416}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2210.11416}.

\bibitem[Clark et~al.(2019)Clark, Yatskar, and Zettlemoyer]{productofexperts}
Clark, C., Yatskar, M., and Zettlemoyer, L.
\newblock Don{'}t take the easy way out: Ensemble based methods for avoiding known dataset biases.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pp.\  4069--4082. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/v1/D19-1418}.
\newblock URL \url{https://aclanthology.org/D19-1418}.

\bibitem[Clark et~al.(2020)Clark, Yatskar, and Zettlemoyer]{productofexperts3}
Clark, C., Yatskar, M., and Zettlemoyer, L.
\newblock Learning to model and ignore dataset bias with mixed capacity ensembles.
\newblock In \emph{Findings of the Association for Computational Linguistics: {EMNLP} 2020}, volume {EMNLP} 2020 of \emph{Findings of {ACL}}, pp.\  3031--3045. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/V1/2020.FINDINGS-EMNLP.272}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.findings-emnlp.272}.

\bibitem[Dong et~al.(2023)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, Li, and Sui]{iclsurvey}
Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., Li, L., and Sui, Z.
\newblock A survey on in-context learning.
\newblock \emph{CoRR}, abs/2301.00234, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.00234}.

\bibitem[Du et~al.(2024)Du, He, Zou, Tao, and Hu]{shortcutlm}
Du, M., He, F., Zou, N., Tao, D., and Hu, X.
\newblock Shortcut learning of large language models in natural language understanding.
\newblock \emph{Commun. {ACM}}, 67\penalty0 (1):\penalty0 110--120, 2024.
\newblock \doi{10.1145/3596490}.
\newblock URL \url{https://doi.org/10.1145/3596490}.

\bibitem[Dunn et~al.(2017)Dunn, Sagun, Higgins, G{\"{u}}ney, Cirik, and Cho]{searchqa}
Dunn, M., Sagun, L., Higgins, M., G{\"{u}}ney, V.~U., Cirik, V., and Cho, K.
\newblock Searchqa: {A} new q{\&}a dataset augmented with context from a search engine.
\newblock \emph{CoRR}, abs/1704.05179, 2017.
\newblock URL \url{http://arxiv.org/abs/1704.05179}.

\bibitem[Feng et~al.(2022)Feng, Zhang, and Song]{attentionmod}
Feng, A., Zhang, X., and Song, X.
\newblock Unrestricted attention may not be all you need-masked attention mechanism focuses better on relevant parts in aspect-based sentiment analysis.
\newblock \emph{{IEEE} Access}, 10:\penalty0 8518--8528, 2022.
\newblock \doi{10.1109/ACCESS.2022.3142178}.
\newblock URL \url{https://doi.org/10.1109/ACCESS.2022.3142178}.

\bibitem[Fisch et~al.(2019)Fisch, Talmor, Jia, Seo, Choi, and Chen]{mrqa}
Fisch, A., Talmor, A., Jia, R., Seo, M., Choi, E., and Chen, D.
\newblock {MRQA} 2019 shared task: Evaluating generalization in reading comprehension.
\newblock In \emph{Proceedings of the 2nd Workshop on Machine Reading for Question Answering, MRQA@EMNLP 2019}, pp.\  1--13. Association for Computational Linguistics, 2019.
\newblock \doi{10.18653/V1/D19-5801}.
\newblock URL \url{https://doi.org/10.18653/v1/D19-5801}.

\bibitem[Gao et~al.(2021)Gao, Tow, Biderman, Black, DiPofi, Foster, Golding, Hsu, McDonell, Muennighoff, Phang, Reynolds, Tang, Thite, Wang, Wang, and Zou]{lmevalharness}
Gao, L., Tow, J., Biderman, S., Black, S., DiPofi, A., Foster, C., Golding, L., Hsu, J., McDonell, K., Muennighoff, N., Phang, J., Reynolds, L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation.
\newblock \url{https://github.com/EleutherAI/lm-evaluation-harness}, September 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5371628}.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel, Bethge, and Wichmann]{shortcut}
Geirhos, R., Jacobsen, J., Michaelis, C., Zemel, R.~S., Brendel, W., Bethge, M., and Wichmann, F.~A.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nat. Mach. Intell.}, 2\penalty0 (11):\penalty0 665--673, 2020.
\newblock \doi{10.1038/s42256-020-00257-z}.
\newblock URL \url{https://doi.org/10.1038/s42256-020-00257-z}.

\bibitem[Gupta et~al.(2023)Gupta, Th{\'e}rien, Ibrahim, Richter, Anthony, Belilovsky, Rish, and Lesort]{rewarmup}
Gupta, K., Th{\'e}rien, B., Ibrahim, A., Richter, M.~L., Anthony, Q.~G., Belilovsky, E., Rish, I., and Lesort, T.
\newblock Continual pre-training of large language models: How to re-warm your model?
\newblock In \emph{Workshop on Efficient Systems for Foundation Models @ ICML2023}, 2023.
\newblock URL \url{https://openreview.net/forum?id=pg7PUJe0Tl}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021}, 2021.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Hu et~al.(2023)Hu, Mitchell, Manning, and Finn]{tokenmask}
Hu, N., Mitchell, E., Manning, C.~D., and Finn, C.
\newblock Meta-learning online adaptation of language models.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023}, pp.\  4418--4432. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.268}.

\bibitem[Jain et~al.(2023)Jain, Salman, Khaddaj, Wong, Park, and Madry]{dataselectiontransfer}
Jain, S., Salman, H., Khaddaj, A., Wong, E., Park, S.~M., and Madry, A.
\newblock A data-based perspective on transfer learning.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2023}, pp.\  3613--3622. {IEEE}, 2023.
\newblock \doi{10.1109/CVPR52729.2023.00352}.
\newblock URL \url{https://doi.org/10.1109/CVPR52729.2023.00352}.

\bibitem[Jang et~al.(2022)Jang, Ye, Yang, Shin, Han, Kim, Choi, and Seo]{continuallmknowledge}
Jang, J., Ye, S., Yang, S., Shin, J., Han, J., Kim, G., Choi, S.~J., and Seo, M.
\newblock Towards continual knowledge learning of language models.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022}. OpenReview.net, 2022.
\newblock URL \url{https://openreview.net/forum?id=vfsRB5MImo9}.

\bibitem[Jin et~al.(2021)Jin, Pan, Oufattole, Weng, Fang, and Szolovits]{medqa}
Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P.
\newblock What disease does this patient have? a large-scale open domain question answering dataset from medical exams.
\newblock \emph{Applied Sciences}, 11\penalty0 (14), 2021.
\newblock ISSN 2076-3417.
\newblock \doi{10.3390/app11146421}.
\newblock URL \url{https://www.mdpi.com/2076-3417/11/14/6421}.

\bibitem[Jin et~al.(2022)Jin, Zhang, Zhu, Xiao, Li, Wei, Arnold, and Ren]{continualpretraining}
Jin, X., Zhang, D., Zhu, H., Xiao, W., Li, S., Wei, X., Arnold, A.~O., and Ren, X.
\newblock Lifelong pretraining: Continually adapting language models to emerging corpora.
\newblock In \emph{Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, {NAACL} 2022}, pp.\  4764--4780. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/v1/2022.naacl-main.351}.
\newblock URL \url{https://doi.org/10.18653/v1/2022.naacl-main.351}.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{triviaqa}
Joshi, M., Choi, E., Weld, D.~S., and Zettlemoyer, L.
\newblock Triviaqa: {A} large scale distantly supervised challenge dataset for reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, {ACL} 2017, Volume 1: Long Papers}, pp.\  1601--1611. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/P17-1147}.
\newblock URL \url{https://doi.org/10.18653/v1/P17-1147}.

\bibitem[Katharopoulos \& Fleuret(2018)Katharopoulos and Fleuret]{importancesamplingdl}
Katharopoulos, A. and Fleuret, F.
\newblock Not all samples are created equal: Deep learning with importance sampling.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning, {ICML} 2018}, volume~80 of \emph{Proceedings of Machine Learning Research}, pp.\  2530--2539. {PMLR}, 2018.
\newblock URL \url{http://proceedings.mlr.press/v80/katharopoulos18a.html}.

\bibitem[Ke et~al.(2023)Ke, Shao, Lin, Konishi, Kim, and Liu]{continualpretraining2}
Ke, Z., Shao, Y., Lin, H., Konishi, T., Kim, G., and Liu, B.
\newblock Continual pre-training of language models.
\newblock In \emph{The Eleventh International Conference on Learning Representations, {ICLR} 2023}. OpenReview.net, 2023.
\newblock URL \url{https://openreview.net/pdf?id=m\_GDIItaI3o}.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, Toutanova, Jones, Kelcey, Chang, Dai, Uszkoreit, Le, and Petrov]{naturalquestions}
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A.~P., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., Toutanova, K., Jones, L., Kelcey, M., Chang, M., Dai, A.~M., Uszkoreit, J., Le, Q., and Petrov, S.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Trans. Assoc. Comput. Linguistics}, 7:\penalty0 452--466, 2019.
\newblock \doi{10.1162/TACL\_A\_00276}.
\newblock URL \url{https://doi.org/10.1162/tacl\_a\_00276}.

\bibitem[Lester et~al.(2021)Lester, Al{-}Rfou, and Constant]{prompttuning}
Lester, B., Al{-}Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2021}, pp.\  3045--3059. Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/V1/2021.EMNLP-MAIN.243}.
\newblock URL \url{https://doi.org/10.18653/v1/2021.emnlp-main.243}.

\bibitem[Lewkowycz et~al.(2022)Lewkowycz, Andreassen, Dohan, Dyer, Michalewski, Ramasesh, Slone, Anil, Schlag, Gutman{-}Solo, Wu, Neyshabur, Gur{-}Ari, and Misra]{minerva}
Lewkowycz, A., Andreassen, A., Dohan, D., Dyer, E., Michalewski, H., Ramasesh, V.~V., Slone, A., Anil, C., Schlag, I., Gutman{-}Solo, T., Wu, Y., Neyshabur, B., Gur{-}Ari, G., and Misra, V.
\newblock Solving quantitative reasoning problems with language models.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/18abbeef8cfe9203fdf9053c9c4fe191-Abstract-Conference.html}.

\bibitem[Li \& Liang(2021)Li and Liang]{prefixtuning}
Li, X.~L. and Liang, P.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pp.\  4582--4597. Association for Computational Linguistics, 2021.
\newblock \doi{10.18653/v1/2021.acl-long.353}.
\newblock URL \url{https://aclanthology.org/2021.acl-long.353}.

\bibitem[Liu et~al.(2022)Liu, Ji, Fu, Tam, Du, Yang, and Tang]{multilayerprefixtuning}
Liu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., and Tang, J.
\newblock {P}-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks.
\newblock In \emph{Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)}, pp.\  61--68. Association for Computational Linguistics, 2022.
\newblock \doi{10.18653/v1/2022.acl-short.8}.
\newblock URL \url{https://aclanthology.org/2022.acl-short.8}.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, and Roberts]{flanv2}
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.~W., Tay, Y., Zhou, D., Le, Q.~V., Zoph, B., Wei, J., and Roberts, A.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2023}, volume 202 of \emph{Proceedings of Machine Learning Research}, pp.\  22631--22648. {PMLR}, 2023.
\newblock URL \url{https://proceedings.mlr.press/v202/longpre23a.html}.

\bibitem[Loshchilov \& Hutter(2019)Loshchilov and Hutter]{adamw}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{7th International Conference on Learning Representations, {ICLR} 2019}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bkg6RiCqY7}.

\bibitem[Mangrulkar et~al.(2022)Mangrulkar, Gugger, Debut, Belkada, Paul, and Bossan]{peft}
Mangrulkar, S., Gugger, S., Debut, L., Belkada, Y., Paul, S., and Bossan, B.
\newblock Peft: State-of-the-art parameter-efficient fine-tuning methods.
\newblock \url{https://github.com/huggingface/peft}, 2022.

\bibitem[McCann et~al.(2018)McCann, Keskar, Xiong, and Socher]{decathlon}
McCann, B., Keskar, N.~S., Xiong, C., and Socher, R.
\newblock The natural language decathlon: Multitask learning as question answering.
\newblock \emph{CoRR}, abs/1806.08730, 2018.
\newblock URL \url{http://arxiv.org/abs/1806.08730}.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock Technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe]{instructgpt}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P.~F., Leike, J., and Lowe, R.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{NeurIPS}, 2022.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html}.

\bibitem[Parisi et~al.(2019)Parisi, Kemker, Part, Kanan, and Wermter]{clreview}
Parisi, G.~I., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S.
\newblock Continual lifelong learning with neural networks: {A} review.
\newblock \emph{Neural Networks}, 113:\penalty0 54--71, 2019.
\newblock \doi{10.1016/j.neunet.2019.01.012}.
\newblock URL \url{https://doi.org/10.1016/j.neunet.2019.01.012}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu]{t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21:\penalty0 140:1--140:67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing}, pp.\  2383--2392. Association for Computational Linguistics, 2016.
\newblock \doi{10.18653/v1/D16-1264}.
\newblock URL \url{http://aclweb.org/anthology/D16-1264}.

\bibitem[Sanh et~al.(2021)Sanh, Wolf, Belinkov, and Rush]{productofexperts2}
Sanh, V., Wolf, T., Belinkov, Y., and Rush, A.~M.
\newblock Learning from others' mistakes: Avoiding dataset biases without modeling them.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021}, 2021.
\newblock URL \url{https://openreview.net/forum?id=Hf3qXoiNkR}.

\bibitem[Sanh et~al.(2022)Sanh, Webson, Raffel, Bach, Sutawika, Alyafeai, Chaffin, Stiegler, Raja, Dey, Bari, Xu, Thakker, Sharma, Szczechla, Kim, Chhablani, Nayak, Datta, Chang, Jiang, Wang, Manica, Shen, Yong, Pandey, Bawden, Wang, Neeraj, Rozen, Sharma, Santilli, F{\'{e}}vry, Fries, Teehan, Scao, Biderman, Gao, Wolf, and Rush]{t0}
Sanh, V., Webson, A., Raffel, C., Bach, S.~H., Sutawika, L., Alyafeai, Z., Chaffin, A., Stiegler, A., Raja, A., Dey, M., Bari, M.~S., Xu, C., Thakker, U., Sharma, S.~S., Szczechla, E., Kim, T., Chhablani, G., Nayak, N.~V., Datta, D., Chang, J., Jiang, M.~T., Wang, H., Manica, M., Shen, S., Yong, Z.~X., Pandey, H., Bawden, R., Wang, T., Neeraj, T., Rozen, J., Sharma, A., Santilli, A., F{\'{e}}vry, T., Fries, J.~A., Teehan, R., Scao, T.~L., Biderman, S., Gao, L., Wolf, T., and Rush, A.~M.
\newblock Multitask prompted training enables zero-shot task generalization.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022}, 2022.
\newblock URL \url{https://openreview.net/forum?id=9Vrb9D0WI4}.

\bibitem[Shi et~al.(2023)Shi, Darrell, and Wang]{topdownattention}
Shi, B., Darrell, T., and Wang, X.
\newblock Top-down visual attention from analysis by synthesis.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition, {CVPR} 2023}, pp.\  2102--2112. {IEEE}, 2023.
\newblock \doi{10.1109/CVPR52729.2023.00209}.
\newblock URL \url{https://doi.org/10.1109/CVPR52729.2023.00209}.

\bibitem[Singhal et~al.(2023)Singhal, Tu, Gottweis, Sayres, Wulczyn, Hou, Clark, Pfohl, Cole{-}Lewis, Neal, Schaekermann, Wang, Amin, Lachgar, Mansfield, Prakash, Green, Dominowska, y~Arcas, Tomasev, Liu, Wong, Semturs, Mahdavi, Barral, Webster, Corrado, Matias, Azizi, Karthikesalingam, and Natarajan]{medpalm2}
Singhal, K., Tu, T., Gottweis, J., Sayres, R., Wulczyn, E., Hou, L., Clark, K., Pfohl, S., Cole{-}Lewis, H., Neal, D., Schaekermann, M., Wang, A., Amin, M., Lachgar, S., Mansfield, P.~A., Prakash, S., Green, B., Dominowska, E., y~Arcas, B.~A., Tomasev, N., Liu, Y., Wong, R., Semturs, C., Mahdavi, S.~S., Barral, J.~K., Webster, D.~R., Corrado, G.~S., Matias, Y., Azizi, S., Karthikesalingam, A., and Natarajan, V.
\newblock Towards expert-level medical question answering with large language models.
\newblock \emph{CoRR}, abs/2305.09617, 2023.
\newblock \doi{10.48550/arXiv.2305.09617}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2305.09617}.

\bibitem[Sun et~al.(2022)Sun, Shao, Qian, Huang, and Qiu]{blackboxtuning}
Sun, T., Shao, Y., Qian, H., Huang, X., and Qiu, X.
\newblock Black-box tuning for language-model-as-a-service.
\newblock In \emph{International Conference on Machine Learning, {ICML} 2022}, volume 162 of \emph{Proceedings of Machine Learning Research}, pp.\  20841--20855. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/sun22e.html}.

\bibitem[Thrun(1998)]{lifelonglearning}
Thrun, S.
\newblock Lifelong learning algorithms.
\newblock In \emph{Learning to Learn}, pp.\  181--209. Springer, 1998.
\newblock \doi{10.1007/978-1-4615-5529-2\_8}.
\newblock URL \url{https://doi.org/10.1007/978-1-4615-5529-2\_8}.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`{e}}re, Goyal, Hambro, Azhar, Rodriguez, Joulin, Grave, and Lample]{llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M., Lacroix, T., Rozi{\`{e}}re, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{CoRR}, abs/2302.13971, 2023{\natexlab{a}}.
\newblock \doi{10.48550/arXiv.2302.13971}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, Bikel, Blecher, Canton{-}Ferrer, Chen, Cucurull, Esiobu, Fernandes, Fu, Fu, Fuller, Gao, Goswami, Goyal, Hartshorn, Hosseini, Hou, Inan, Kardas, Kerkez, Khabsa, Kloumann, Korenev, Koura, Lachaux, Lavril, Lee, Liskovich, Lu, Mao, Martinet, Mihaylov, Mishra, Molybog, Nie, Poulton, Reizenstein, Rungta, Saladi, Schelten, Silva, Smith, Subramanian, Tan, Tang, Taylor, Williams, Kuan, Xu, Yan, Zarov, Zhang, Fan, Kambadur, Narang, Rodriguez, Stojnic, Edunov, and Scialom]{llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Canton{-}Ferrer, C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.~S., Lachaux, M., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E.~M., Subramanian, R., Tan, X.~E., Tang, B., Taylor, R., Williams, A., Kuan, J.~X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{CoRR}, abs/2307.09288, 2023{\natexlab{b}}.
\newblock \doi{10.48550/arXiv.2307.09288}.
\newblock URL \url{https://doi.org/10.48550/arXiv.2307.09288}.

\bibitem[Trischler et~al.(2017)Trischler, Wang, Yuan, Harris, Sordoni, Bachman, and Suleman]{newsqa}
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K.
\newblock Newsqa: {A} machine comprehension dataset.
\newblock In \emph{Proceedings of the 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL 2017}, pp.\  191--200. Association for Computational Linguistics, 2017.
\newblock \doi{10.18653/V1/W17-2623}.
\newblock URL \url{https://doi.org/10.18653/v1/w17-2623}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Ichter, Xia, Chi, Le, and Zhou]{cot}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock In \emph{Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022}, 2022.
\newblock URL \url{http://papers.nips.cc/paper\_files/paper/2022/hash/9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html}.

\bibitem[Weng et~al.(2023)Weng, Zhu, Xia, Li, He, Liu, Sun, Liu, and Zhao]{selfverification}
Weng, Y., Zhu, M., Xia, F., Li, B., He, S., Liu, S., Sun, B., Liu, K., and Zhao, J.
\newblock Large language models are better reasoners with self-verification.
\newblock In \emph{Findings of the Association for Computational Linguistics: {EMNLP} 2023}, pp.\  2550--2575. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.167}.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{hf}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, {EMNLP} 2020 - Demos}, pp.\  38--45. Association for Computational Linguistics, 2020.
\newblock \doi{10.18653/v1/2020.emnlp-demos.6}.
\newblock URL \url{https://doi.org/10.18653/v1/2020.emnlp-demos.6}.

\bibitem[Xie et~al.(2023)Xie, Santurkar, Ma, and Liang]{dataselectlm}
Xie, S.~M., Santurkar, S., Ma, T., and Liang, P.
\newblock Data selection for language models via importance resampling.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=uPSQv0leAu}.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.~W., Salakhutdinov, R., and Manning, C.~D.
\newblock Hotpotqa: {A} dataset for diverse, explainable multi-hop question answering.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, pp.\  2369--2380. Association for Computational Linguistics, 2018.
\newblock \doi{10.18653/V1/D18-1259}.
\newblock URL \url{https://doi.org/10.18653/v1/d18-1259}.

\bibitem[Yao et~al.(2023)Yao, Yu, Zhao, Shafran, Griffiths, Cao, and Narasimhan]{tot}
Yao, S., Yu, D., Zhao, J., Shafran, I., Griffiths, T.~L., Cao, Y., and Narasimhan, K.~R.
\newblock Tree of thoughts: Deliberate problem solving with large language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=5Xc1ecxO1h}.

\bibitem[Zhang \& Wu(2024)Zhang and Wu]{dissectlm}
Zhang, X. and Wu, J.
\newblock Dissecting learning and forgetting in language model finetuning.
\newblock In \emph{The Twelfth International Conference on Learning Representations, {ICLR} 2024}, 2024.
\newblock URL \url{https://openreview.net/forum?id=tmsqb6WpLz}.

\bibitem[Zhong et~al.(2023)Zhong, Wu, Manning, Potts, and Chen]{kediteval}
Zhong, Z., Wu, Z., Manning, C.~D., Potts, C., and Chen, D.
\newblock Mquake: Assessing knowledge editing in language models via multi-hop questions.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, {EMNLP} 2023}, pp.\  15686--15702. Association for Computational Linguistics, 2023.
\newblock URL \url{https://aclanthology.org/2023.emnlp-main.971}.

\end{thebibliography}
