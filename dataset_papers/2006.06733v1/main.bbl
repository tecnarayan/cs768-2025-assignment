\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjevani and Shamir(2015)]{arjevani2015communication}
Y.~Arjevani and O.~Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NIPS)}, 2015.

\bibitem[Arjevani and Shamir(2016)]{arjevani2016iteration}
Y.~Arjevani and O.~Shamir.
\newblock On the iteration complexity of oblivious first-order optimization
  algorithms.
\newblock In \emph{International Conferences on Machine Learning (ICML)}, 2016.

\bibitem[Arjevani et~al.(2020)Arjevani, Shamir, and Srebro]{arjevani2018tight}
Y.~Arjevani, O.~Shamir, and N.~Srebro.
\newblock A tight convergence analysis for stochastic gradient descent with
  delayed updates.
\newblock In \emph{Proceedings of the 31st International Conference on
  Algorithmic Learning Theory}, volume 117, pages 111--132, 2020.

\bibitem[Auzinger and Melenk(2011)]{lecturenote}
W.~Auzinger and J.~Melenk.
\newblock Iterative solution of large linear systems.
\newblock \emph{Lecture Note}, 2011.

\bibitem[Aybat and G{\"u}rb{\"u}zbalaban(2017)]{aybat2017decentralized}
N.~S. Aybat and M.~G{\"u}rb{\"u}zbalaban.
\newblock Decentralized computation of effective resistances and acceleration
  of consensus algorithms.
\newblock In \emph{2017 IEEE Global Conference on Signal and Information
  Processing (GlobalSIP)}, pages 538--542. IEEE, 2017.

\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and
  Zilberstein]{bernstein2002complexity}
D.~S. Bernstein, R.~Givan, N.~Immerman, and S.~Zilberstein.
\newblock The complexity of decentralized control of markov decision processes.
\newblock \emph{Mathematics of operations research}, 27\penalty0 (4):\penalty0
  819--840, 2002.

\bibitem[Bertsekas(2014)]{bertsekas2014constrained}
D.~P. Bertsekas.
\newblock \emph{Constrained optimization and Lagrange multiplier methods}.
\newblock Academic press, 2014.

\bibitem[Can et~al.(2019)Can, Soori, Aybat, Dehvani, and
  G\"urb\"uzbalaban]{can2019decentralized}
B.~Can, S.~Soori, N.~S. Aybat, M.~M. Dehvani, and M.~G\"urb\"uzbalaban.
\newblock Decentralized computation of effective resistances and acceleration
  of distributed optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1907.13110}, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Agarwal, and Wainwright]{duchi2011dual}
J.~C. Duchi, A.~Agarwal, and M.~J. Wainwright.
\newblock Dual averaging for distributed optimization: Convergence analysis and
  network scaling.
\newblock \emph{IEEE Transactions on Automatic control}, 57\penalty0
  (3):\penalty0 592--606, 2011.

\bibitem[Dvinskikh and Gasnikov(2019)]{dvinskikh2019decentralized}
D.~Dvinskikh and A.~Gasnikov.
\newblock Decentralized and parallelized primal and dual accelerated methods
  for stochastic convex programming problems.
\newblock \emph{arXiv preprint arXiv:1904.09015}, 2019.

\bibitem[Fallah et~al.(2019)Fallah, G\"urb\"uzbalaban, Ozdaglar, Simsekli, and
  Zhu]{fallah2019robust}
A.~Fallah, M.~G\"urb\"uzbalaban, A.~Ozdaglar, U.~Simsekli, and L.~Zhu.
\newblock Robust distributed accelerated stochastic gradient methods for
  multi-agent networks.
\newblock \emph{arXiv preprint arXiv:1910.08701}, 2019.

\bibitem[GDPR(2016)]{voigt2017gdpr}
GDPR.
\newblock The eu general data protection regulation (gdpr).
\newblock 2016.

\bibitem[G{\"u}ler(1992)]{guler1992new}
O.~G{\"u}ler.
\newblock New proximal point algorithms for convex minimization.
\newblock \emph{SIAM Journal on Optimization}, 2\penalty0 (4):\penalty0
  649--664, 1992.

\bibitem[Hendrikx et~al.(2020)Hendrikx, Bach, and
  Massoulie]{hendrikx2020optimal}
H.~Hendrikx, F.~Bach, and L.~Massoulie.
\newblock An optimal algorithm for decentralized finite sum optimization, 2020.

\bibitem[Jakoveti{\'c} et~al.(2014{\natexlab{a}})Jakoveti{\'c}, Moura, and
  Xavier]{jakovetic2014linear}
D.~Jakoveti{\'c}, J.~M. Moura, and J.~Xavier.
\newblock Linear convergence rate of a class of distributed augmented
  lagrangian algorithms.
\newblock \emph{IEEE Transactions on Automatic Control}, 60\penalty0
  (4):\penalty0 922--936, 2014{\natexlab{a}}.

\bibitem[Jakoveti{\'c} et~al.(2014{\natexlab{b}})Jakoveti{\'c}, Xavier, and
  Moura]{jakovetic2014fast}
D.~Jakoveti{\'c}, J.~Xavier, and J.~M. Moura.
\newblock Fast distributed gradient methods.
\newblock \emph{IEEE Transactions on Automatic Control}, 59\penalty0
  (5):\penalty0 1131--1146, 2014{\natexlab{b}}.

\bibitem[Jochems et~al.(2016)Jochems, Deist, Van~Soest, Eble, Bulens, Coucke,
  Dries, Lambin, and Dekker]{jochems2016distributed}
A.~Jochems, T.~M. Deist, J.~Van~Soest, M.~Eble, P.~Bulens, P.~Coucke, W.~Dries,
  P.~Lambin, and A.~Dekker.
\newblock Distributed learning: developing a predictive model based on data
  from multiple hospitals without data leaving the hospital--a real life proof
  of concept.
\newblock \emph{Radiotherapy and Oncology}, 121\penalty0 (3):\penalty0
  459--467, 2016.

\bibitem[Jochems et~al.(2017)Jochems, Deist, El~Naqa, Kessler, Mayo, Reeves,
  Jolly, Matuszak, Ten~Haken, van Soest, et~al.]{jochems2017developing}
A.~Jochems, T.~M. Deist, I.~El~Naqa, M.~Kessler, C.~Mayo, J.~Reeves, S.~Jolly,
  M.~Matuszak, R.~Ten~Haken, J.~van Soest, et~al.
\newblock Developing and validating a survival prediction model for nsclc
  patients through distributed learning across 3 countries.
\newblock \emph{International Journal of Radiation Oncology* Biology* Physics},
  99\penalty0 (2):\penalty0 344--352, 2017.

\bibitem[Kang et~al.(2015)Kang, Kang, and Jung]{kang2015inexact}
M.~Kang, M.~Kang, and M.~Jung.
\newblock Inexact accelerated augmented lagrangian methods.
\newblock \emph{Computational Optimization and Applications}, 62\penalty0
  (2):\penalty0 373--404, 2015.

\bibitem[Konečný et~al.(2016)Konečný, McMahan, Yu, Richtarik, Suresh, and
  Bacon]{google2016federated}
J.~Konečný, H.~B. McMahan, F.~X. Yu, P.~Richtarik, A.~T. Suresh, and
  D.~Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock In \emph{NIPS Workshop on Private Multi-Party Machine Learning},
  2016.
\newblock URL \url{https://arxiv.org/abs/1610.05492}.

\bibitem[LeCun et~al.(2010)LeCun, Cortes, and Burges]{lecun2010mnist}
Y.~LeCun, C.~Cortes, and C.~Burges.
\newblock Mnist handwritten digit database.
\newblock \emph{ATT Labs [Online]}, 2, 2010.
\newblock URL \url{http://yann.lecun.com/exdb/mnist}.

\bibitem[Li et~al.(2018)Li, Fang, Yin, and Lin]{li2018sharp}
H.~Li, C.~Fang, W.~Yin, and Z.~Lin.
\newblock A sharp convergence rate analysis for distributed accelerated
  gradient methods.
\newblock \emph{arXiv preprint arXiv:1810.01053}, 2018.

\bibitem[Lin et~al.(2017)Lin, Mairal, and Harchaoui]{lin2017catalyst}
H.~Lin, J.~Mairal, and Z.~Harchaoui.
\newblock Catalyst acceleration for first-order convex optimization: from
  theory to practice.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 7854--7907, 2017.

\bibitem[Mairal(2016)]{mairal2016end}
J.~Mairal.
\newblock End-to-end kernel learning with supervised convolutional kernel
  networks.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NIPS)}, 2016.

\bibitem[Mao et~al.(2017)Mao, You, Zhang, Huang, and Letaief]{mao2017survey}
Y.~Mao, C.~You, J.~Zhang, K.~Huang, and K.~B. Letaief.
\newblock A survey on mobile edge computing: The communication perspective.
\newblock \emph{IEEE Communications Surveys \& Tutorials}, 19\penalty0
  (4):\penalty0 2322--2358, 2017.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
B.~McMahan, E.~Moore, D.~Ramage, S.~Hampson, and B.~A. y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 1273--1282,
  2017.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson,
  et~al.]{mcmahan2016communication}
H.~B. McMahan, E.~Moore, D.~Ramage, S.~Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Nedelcu et~al.(2014)Nedelcu, Necoara, and
  Tran-Dinh]{nedelcu2014computational}
V.~Nedelcu, I.~Necoara, and Q.~Tran-Dinh.
\newblock Computational complexity of inexact gradient augmented lagrangian
  methods: application to constrained mpc.
\newblock \emph{SIAM Journal on Control and Optimization}, 52\penalty0
  (5):\penalty0 3109--3134, 2014.

\bibitem[Nedic and Ozdaglar(2009)]{nedic2009distributed}
A.~Nedic and A.~Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock \emph{IEEE Transactions on Automatic Control}, 54\penalty0
  (1):\penalty0 48--61, 2009.

\bibitem[Nedic et~al.(2017)Nedic, Olshevsky, and Shi]{nedic2017achieving}
A.~Nedic, A.~Olshevsky, and W.~Shi.
\newblock Achieving geometric convergence for distributed optimization over
  time-varying graphs.
\newblock \emph{SIAM Journal on Optimization}, 27\penalty0 (4):\penalty0
  2597--2633, 2017.

\bibitem[Nedi{\'c} et~al.(2017)Nedi{\'c}, Olshevsky, Shi, and
  Uribe]{nedic2017geometrically}
A.~Nedi{\'c}, A.~Olshevsky, W.~Shi, and C.~A. Uribe.
\newblock Geometrically convergent distributed optimization with uncoordinated
  step-sizes.
\newblock In \emph{2017 American Control Conference (ACC)}, pages 3950--3955.
  IEEE, 2017.

\bibitem[Nesterov(2004)]{nesterov2004introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization}, volume~87.
\newblock Springer Science \& Business Media, 2004.

\bibitem[Panait and Luke(2005)]{panait2005cooperative}
L.~Panait and S.~Luke.
\newblock Cooperative multi-agent learning: The state of the art.
\newblock \emph{Autonomous agents and multi-agent systems}, 11\penalty0
  (3):\penalty0 387--434, 2005.

\bibitem[Qu and Li(2017)]{qu2017harnessing}
G.~Qu and N.~Li.
\newblock Harnessing smoothness to accelerate distributed optimization.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 5\penalty0
  (3):\penalty0 1245--1260, 2017.

\bibitem[Rockafellar(1976)]{rockafellar1976augmented}
R.~T. Rockafellar.
\newblock Augmented lagrangians and applications of the proximal point
  algorithm in convex programming.
\newblock \emph{Mathematics of operations research}, 1\penalty0 (2):\penalty0
  97--116, 1976.

\bibitem[Rockafellar and Wets(2009)]{rockafellar2009variational}
R.~T. Rockafellar and R.~J.-B. Wets.
\newblock \emph{Variational analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Scaman et~al.(2017)Scaman, Bach, Bubeck, Lee, and
  Massouli{\'e}]{scaman2017optimal}
K.~Scaman, F.~Bach, S.~Bubeck, Y.~T. Lee, and L.~Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In \emph{International Conferences on Machine Learning (ICML)}, 2017.

\bibitem[Scaman et~al.(2018)Scaman, Bach, Bubeck, Massouli{\'e}, and
  Lee]{scaman2018optimal}
K.~Scaman, F.~Bach, S.~Bubeck, L.~Massouli{\'e}, and Y.~T. Lee.
\newblock Optimal algorithms for non-smooth distributed optimization in
  networks.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NIPS)}, 2018.

\bibitem[Schmidt et~al.(2011)Schmidt, Roux, and Bach]{schmidt2011convergence}
M.~Schmidt, N.~L. Roux, and F.~R. Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NIPS)}, 2011.

\bibitem[Shi et~al.(2014)Shi, Ling, Yuan, Wu, and Yin]{shi2014linear}
W.~Shi, Q.~Ling, K.~Yuan, G.~Wu, and W.~Yin.
\newblock On the linear convergence of the {ADMM} in decentralized consensus
  optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 62\penalty0
  (7):\penalty0 1750--1761, 2014.

\bibitem[Shi et~al.(2015)Shi, Ling, Wu, and Yin]{shi2015extra}
W.~Shi, Q.~Ling, G.~Wu, and W.~Yin.
\newblock Extra: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock \emph{SIAM Journal on Optimization}, 25\penalty0 (2):\penalty0
  944--966, 2015.

\bibitem[Shi et~al.(2016)Shi, Cao, Zhang, Li, and Xu]{shi2016edge}
W.~Shi, J.~Cao, Q.~Zhang, Y.~Li, and L.~Xu.
\newblock Edge computing: Vision and challenges.
\newblock \emph{IEEE internet of things journal}, 3\penalty0 (5):\penalty0
  637--646, 2016.

\bibitem[Shokri and Shmatikov(2015)]{shokri2015privacy}
R.~Shokri and V.~Shmatikov.
\newblock Privacy-preserving deep learning.
\newblock In \emph{Proceedings of the 22nd ACM SIGSAC conference on computer
  and communications security}, pages 1310--1321, 2015.

\bibitem[Sun et~al.(2019)Sun, Daneshmand, and Scutari]{sun2019convergence}
Y.~Sun, A.~Daneshmand, and G.~Scutari.
\newblock Convergence rate of distributed optimization algorithms based on
  gradient tracking.
\newblock \emph{arXiv preprint arXiv:1905.02637}, 2019.

\bibitem[Uribe et~al.(2020)Uribe, Lee, Gasnikov, and Nedi{\'c}]{uribe2020dual}
C.~A. Uribe, S.~Lee, A.~Gasnikov, and A.~Nedi{\'c}.
\newblock A dual approach for optimal algorithms in distributed optimization
  over networks.
\newblock \emph{Optimization Methods and Software}, pages 1--40, 2020.

\bibitem[Woodworth et~al.(2018)Woodworth, Wang, Smith, McMahan, and
  Srebro]{woodworth2018graph}
B.~E. Woodworth, J.~Wang, A.~Smith, B.~McMahan, and N.~Srebro.
\newblock Graph oracle models, lower bounds, and gaps for parallel stochastic
  optimization.
\newblock In \emph{Proceedings of Advances in Neural Information Processing
  Systems (NIPS)}, 2018.

\bibitem[Xiao et~al.(2007)Xiao, Boyd, and Kim]{xiao2007distributed}
L.~Xiao, S.~Boyd, and S.-J. Kim.
\newblock Distributed average consensus with least-mean-square deviation.
\newblock \emph{Journal of parallel and distributed computing}, 67\penalty0
  (1):\penalty0 33--46, 2007.

\bibitem[Xu et~al.(2020)Xu, Tian, Sun, and Scutari]{xu2019accelerated}
J.~Xu, Y.~Tian, Y.~Sun, and G.~Scutari.
\newblock Accelerated primal-dual algorithms for distributed smooth convex
  optimization over networks.
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2020.

\bibitem[Yan and He(2020)]{yan2020bregman}
S.~Yan and N.~He.
\newblock Bregman augmented lagrangian and its acceleration, 2020.

\bibitem[Yuan et~al.(2016)Yuan, Ling, and Yin]{yuan2016convergence}
K.~Yuan, Q.~Ling, and W.~Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (3):\penalty0
  1835--1854, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Uribe, Mokhtari, and
  Jadbabaie]{zhang2019achieving}
J.~Zhang, C.~A. Uribe, A.~Mokhtari, and A.~Jadbabaie.
\newblock Achieving acceleration in distributed optimization via direct
  discretization of the heavy-ball ode.
\newblock In \emph{2019 American Control Conference (ACC)}, pages 3408--3413.
  IEEE, 2019.

\end{thebibliography}
