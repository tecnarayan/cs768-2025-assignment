\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bars et~al.(2022)Bars, Bellet, Tommasi, and Kermarrec]{yes_topology}
B.~Le Bars, A.~Bellet, M.~Tommasi, and AM. Kermarrec.
\newblock Yes, topology matters in decentralized optimization: Refined
  convergence and topology learning under heterogeneous data.
\newblock \emph{arXiv preprint arxiv:2204.04452}, 2022.
\newblock URL \url{https://arxiv.org/abs/2204.04452}.

\bibitem[Bianchi and Jakubowicz(2012)]{bianchi2012convergence}
Pascal Bianchi and J{\'e}r{\'e}mie Jakubowicz.
\newblock Convergence of a multi-agent projected stochastic gradient algorithm
  for non-convex optimization.
\newblock \emph{IEEE transactions on automatic control}, 58\penalty0
  (2):\penalty0 391--405, 2012.

\bibitem[Boyd et~al.(2004)Boyd, Diaconis, and Xiao]{boyd2004fastest}
Stephen Boyd, Persi Diaconis, and Lin Xiao.
\newblock Fastest mixing markov chain on a graph.
\newblock \emph{SIAM review}, 46\penalty0 (4):\penalty0 667--689, 2004.

\bibitem[Brown et~al.(2022)Brown, Hod, and Kalemaj]{brown2020performative}
Gavin Brown, Shlomi Hod, and Iden Kalemaj.
\newblock Performative prediction in a stateful world.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Caldas et~al.(2019)Caldas, Duddu, Wu, Li, Konečný, McMahan, Smith,
  and Talwalkar]{caldas2019leaf}
Sebastian Caldas, Sai Meher~Karthik Duddu, Peter Wu, Tian Li, Jakub Konečný,
  H.~Brendan McMahan, Virginia Smith, and Ameet Talwalkar.
\newblock Leaf: A benchmark for federated settings.
\newblock In \emph{NeurIPS Workshop on Federated Learning for Data Privacy and
  Confidentiality}, 2019.

\bibitem[Chen et~al.(2021)Chen, Zhang, Giannakis, and
  Basar]{chen2021communication}
Tianyi Chen, Kaiqing Zhang, Georgios~B Giannakis, and Tamer Basar.
\newblock Communication-efficient policy gradient methods for distributed
  reinforcement learning.
\newblock \emph{IEEE Transactions on Control of Network Systems}, 2021.

\bibitem[Dong et~al.(2018)Dong, Roth, Schutzman, Waggoner, and
  Wu]{dong2018strategic}
Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo~Waggoner, and Zhiwei~Steven Wu.
\newblock Strategic classification from revealed preferences.
\newblock In \emph{Proceedings of the 2018 ACM Conference on Economics and
  Computation}, pages 55--70, 2018.

\bibitem[Drusvyatskiy and Xiao(2020)]{drusvyatskiy2020stochastic}
Dmitriy Drusvyatskiy and Lin Xiao.
\newblock Stochastic optimization with decision-dependent distributions.
\newblock \emph{ArXiv preprint arxiv:2011.11173}, 2020.

\bibitem[Granas and Dugundji(2003)]{granas2003fixed}
Andrzej Granas and James Dugundji.
\newblock \emph{Fixed point theory}, volume~14.
\newblock Springer, 2003.

\bibitem[Hardt et~al.(2016)Hardt, Megiddo, Papadimitriou, and Wootters]{Hardt}
Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters.
\newblock Strategic classification.
\newblock In \emph{ITCS}, page 111–122, 2016.

\bibitem[Hopkins(1999)]{spambase94}
Reeber Hopkins, Mark.
\newblock {Spambase}.
\newblock UCI Machine Learning Repository, 1999.

\bibitem[Izzo et~al.(2021)Izzo, Ying, and Zou]{izzo2021learn}
Zachary Izzo, Lexing Ying, and James Zou.
\newblock How to learn when data reacts to your model: performative gradient
  descent.
\newblock In \emph{ICML}, pages 4641--4650. PMLR, 2021.

\bibitem[Izzo et~al.(2022)Izzo, Zou, and Ying]{izzo2022learn}
Zachary Izzo, James Zou, and Lexing Ying.
\newblock How to learn when data gradually reacts to your model.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Karimi et~al.(2019)Karimi, Miasojedow, Moulines, and
  Wai]{karimi2019non}
Belhal Karimi, Blazej Miasojedow, Eric Moulines, and Hoi-To Wai.
\newblock Non-asymptotic analysis of biased stochastic approximation scheme.
\newblock In \emph{Conference on Learning Theory}, pages 1944--1974. PMLR,
  2019.

\bibitem[Kleinberg and Raghavan(2020)]{kleinberg2020classifiers}
Jon Kleinberg and Manish Raghavan.
\newblock How do classifiers induce agents to invest effort strategically?
\newblock \emph{ACM Transactions on Economics and Computation (TEAC)},
  8\penalty0 (4):\penalty0 1--23, 2020.

\bibitem[Koloskova et~al.(2019)Koloskova, Stich, and
  Jaggi]{koloskova2019decentralized}
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In \emph{International Conference on Machine Learning}, pages
  3478--3487. PMLR, 2019.

\bibitem[Kong et~al.(2021)Kong, Lin, Koloskova, Jaggi, and
  Stich]{kong2021consensus}
Lingjing Kong, Tao Lin, Anastasia Koloskova, Martin Jaggi, and Sebastian Stich.
\newblock Consensus control for decentralized deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5686--5696. PMLR, 2021.

\bibitem[Lan et~al.(2020)Lan, Lee, and Zhou]{lan2020communication}
Guanghui Lan, Soomin Lee, and Yi~Zhou.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock \emph{Mathematical Programming}, 180\penalty0 (1):\penalty0 237--284,
  2020.

\bibitem[Li and Wai(2022)]{li2021state}
Qiang Li and Hoi-To Wai.
\newblock State dependent performative prediction with stochastic
  approximation.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{lian2017decentralized}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Mendler-D{\"u}nner et~al.(2020)Mendler-D{\"u}nner, Perdomo, Zrnic, and
  Hardt]{mendler2020}
Celestine Mendler-D{\"u}nner, Juan Perdomo, Tijana Zrnic, and Moritz Hardt.
\newblock Stochastic optimization for performative prediction.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4929--4939, 2020.

\bibitem[Miller et~al.(2021)Miller, Perdomo, and Zrnic]{miller2021}
John Miller, Juan~C. Perdomo, and Tijana Zrnic.
\newblock Outside the echo chamber: Optimizing the performative risk.
\newblock In \emph{ICML}, 2021.

\bibitem[Moulines and Bach(2011)]{moulines2011non}
Eric Moulines and Francis Bach.
\newblock Non-asymptotic analysis of stochastic approximation algorithms for
  machine learning.
\newblock In \emph{Advances in neural information processing systems},
  volume~24, 2011.

\bibitem[Narang et~al.(2022)Narang, Faulkner, Drusvyatskiy, Fazel, and
  Ratliff]{narang2022multiplayer}
Adhyyan Narang, Evan Faulkner, Dmitriy Drusvyatskiy, Maryam Fazel, and
  Lillian~J. Ratliff.
\newblock Multiplayer performative prediction: Learning in decision-dependent
  games.
\newblock In \emph{AISTATS}, 2022.

\bibitem[Perdomo et~al.(2020)Perdomo, Zrnic, Mendler-D{\"u}nner, and
  Hardt]{perdomo2020performative}
Juan Perdomo, Tijana Zrnic, Celestine Mendler-D{\"u}nner, and Moritz Hardt.
\newblock Performative prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  7599--7609. PMLR, 2020.

\bibitem[Piliouras and Yu(2022)]{piliouras2022multi}
Georgios Piliouras and Fang-Yi Yu.
\newblock Multi-agent performative prediction: From global stability and
  optimality to chaos.
\newblock \emph{arXiv preprint arXiv:2201.10483}, 2022.

\bibitem[Pu et~al.(2021)Pu, Olshevsky, and Paschalidis]{pu2021sharp}
Shi Pu, Alexander Olshevsky, and Ioannis~Ch Paschalidis.
\newblock A sharp estimate on the transient time of distributed stochastic
  gradient descent.
\newblock \emph{IEEE Transactions on Automatic Control}, 2021.

\bibitem[Qui{\~n}onero-Candela et~al.(2008)Qui{\~n}onero-Candela, Sugiyama,
  Schwaighofer, and Lawrence]{quinonero2008dataset}
Joaquin Qui{\~n}onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D
  Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock MIT Press, 2008.

\bibitem[Ram et~al.(2010)Ram, Nedi{\'c}, and
  Veeravalli]{sundhar2010distributed}
S~Sundhar Ram, Angelia Nedi{\'c}, and Venugopal~V Veeravalli.
\newblock Distributed stochastic subgradient projection algorithms for convex
  optimization.
\newblock \emph{Journal of optimization theory and applications}, 147\penalty0
  (3):\penalty0 516--545, 2010.

\bibitem[Ray et~al.(2022)Ray, Ratliff, Drusvyatskiy, and
  Fazel]{ray2022decision}
Mitas Ray, Lillian~J Ratliff, Dmitriy Drusvyatskiy, and Maryam Fazel.
\newblock Decision-dependent risk minimization in geometrically decaying
  dynamic environments.
\newblock In \emph{AAAI Conference on Artificial Intelligence, To appear},
  2022.

\bibitem[Sayed(2014)]{sayed2014adaptation}
Ali~H Sayed.
\newblock Adaptation, learning, and optimization over networks.
\newblock \emph{Foundations and Trends in Machine Learning}, 7:\penalty0
  311--801, 2014.

\bibitem[Tang et~al.(2018)Tang, Lian, Yan, Zhang, and Liu]{tang2018d}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock $d^2$: Decentralized training over decentralized data.
\newblock In \emph{International Conference on Machine Learning}, pages
  4848--4856. PMLR, 2018.

\bibitem[Wood et~al.(2021)Wood, Bianchin, and Dall’Anese]{wood2021online}
Killian Wood, Gianluca Bianchin, and Emiliano Dall’Anese.
\newblock Online projected gradient descent for stochastic optimization with
  decision-dependent distributions.
\newblock \emph{IEEE Control Systems Letters}, 6:\penalty0 1646--1651, 2021.

\bibitem[Yuan and Alghunaim(2021)]{yuan2021removing}
Kun Yuan and Sulaiman~A Alghunaim.
\newblock Removing data heterogeneity influence enhances network topology
  dependence of decentralized sgd.
\newblock \emph{arXiv preprint arXiv:2105.08023}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Yang, Liu, Zhang, and Basar]{zhang2018fully}
Kaiqing Zhang, Zhuoran Yang, Han Liu, Tong Zhang, and Tamer Basar.
\newblock Fully decentralized multi-agent reinforcement learning with networked
  agents.
\newblock In \emph{International Conference on Machine Learning}, pages
  5872--5881. PMLR, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Koppel, Zhu, and Basar]{zhang2020global}
Kaiqing Zhang, Alec Koppel, Hao Zhu, and Tamer Basar.
\newblock Global convergence of policy gradient methods to (almost) locally
  optimal policies.
\newblock \emph{SIAM Journal on Control and Optimization}, 58\penalty0
  (6):\penalty0 3586--3612, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Yang, and Ba{\c{s}}ar]{zhang2021multi}
Kaiqing Zhang, Zhuoran Yang, and Tamer Ba{\c{s}}ar.
\newblock Multi-agent reinforcement learning: A selective overview of theories
  and algorithms.
\newblock \emph{Handbook of Reinforcement Learning and Control}, pages
  321--384, 2021.

\bibitem[Zrnic et~al.(2021)Zrnic, Mazumdar, Sastry, and Jordan]{zrnic2021leads}
Tijana Zrnic, Eric Mazumdar, Shankar Sastry, and Michael Jordan.
\newblock Who leads and who follows in strategic classification?
\newblock In \emph{NeurIPS}, volume~34, 2021.

\end{thebibliography}
