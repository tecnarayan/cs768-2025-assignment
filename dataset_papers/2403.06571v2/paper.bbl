\begin{thebibliography}{101}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2014)Agarwal, Hsu, Kale, Langford, Li, and Schapire]{agarwal2014taming}
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire.
\newblock Taming the monster: A fast and simple algorithm for contextual bandits.
\newblock In \emph{International Conference on Machine Learning}, 2014.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock {FLAMBE}: Structural complexity and representation learning of low rank {MDP}s.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation, and distribution shift.
\newblock \emph{The Journal of Machine Learning Research}, 2021.

\bibitem[Amortila et~al.(2024)Amortila, Foster, Jiang, Sekhari, and Xie]{amortila2023harnessing}
Philip Amortila, Dylan~J Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie.
\newblock Harnessing density ratios for online reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2024.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 2008.

\bibitem[Ash et~al.(2022)Ash, Zhang, Goel, Krishnamurthy, and Kakade]{ash2021anti}
Jordan~T Ash, Cyril Zhang, Surbhi Goel, Akshay Krishnamurthy, and Sham Kakade.
\newblock Anti-concentrated confidence bonuses for scalable exploration.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Auer(2002)]{auer2002using}
Peter Auer.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \emph{Journal of Machine Learning Research}, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Badia et~al.(2020{\natexlab{a}})Badia, Piot, Kapturowski, Sprechmann, Vitvitskyi, Guo, and Blundell]{badia2020agent57}
Adri{\`a}~Puigdom{\`e}nech Badia, Bilal Piot, Steven Kapturowski, Pablo Sprechmann, Alex Vitvitskyi, Zhaohan~Daniel Guo, and Charles Blundell.
\newblock Agent57: Outperforming the atari human benchmark.
\newblock In \emph{International conference on machine learning}, 2020{\natexlab{a}}.

\bibitem[Badia et~al.(2020{\natexlab{b}})Badia, Sprechmann, Vitvitskyi, Guo, Piot, Kapturowski, Tieleman, Arjovsky, Pritzel, Bolt, and Blundell]{badia2020never}
Adri{\`a}~Puigdom{\`e}nech Badia, Pablo Sprechmann, Alex Vitvitskyi, Daniel Guo, Bilal Piot, Steven Kapturowski, Olivier Tieleman, Martin Arjovsky, Alexander Pritzel, Andrew Bolt, and Charles Blundell.
\newblock Never give up: Learning directed exploration strategies.
\newblock In \emph{International Conference on Learning Representations}, 2020{\natexlab{b}}.

\bibitem[Bagnell et~al.(2003)Bagnell, Kakade, Schneider, and Ng]{bagnell2003policy}
James Bagnell, Sham~M Kakade, Jeff Schneider, and Andrew Ng.
\newblock Policy search by dynamic programming.
\newblock \emph{Advances in Neural Information Processing Systems}, 2003.

\bibitem[Bellemare et~al.(2016)Bellemare, Srinivasan, Ostrovski, Schaul, Saxton, and Munos]{bellemare2016unifying}
Marc Bellemare, Sriram Srinivasan, Georg Ostrovski, Tom Schaul, David Saxton, and R\'{e}mi Munos.
\newblock Unifying count-based exploration and intrinsic motivation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Block and Polyanskiy(2023)]{block2023sample}
Adam Block and Yury Polyanskiy.
\newblock The sample complexity of approximate rejection sampling with applications to smoothed online learning.
\newblock In \emph{Conference on Learning Theory}, 2023.

\bibitem[Block et~al.(2022)Block, Dagan, Golowich, and Rakhlin]{block2022smoothed}
Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin.
\newblock Smoothed online learning is as easy as statistical learning.
\newblock In \emph{Conference on Learning Theory}, 2022.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider, Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba.
\newblock {OpenAI} gym.
\newblock \emph{arXiv:1606.01540}, 2016.

\bibitem[Burda et~al.(2018)Burda, Edwards, Storkey, and Klimov]{burda2018exploration}
Yuri Burda, Harrison Edwards, Amos Storkey, and Oleg Klimov.
\newblock Exploration by random network distillation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Mei, and Bai]{chen2022unified}
Fan Chen, Song Mei, and Yu~Bai.
\newblock Unified algorithms for {RL} with decision-estimation coefficients: No-regret, {PAC}, and reward-free learning.
\newblock \emph{arXiv:2209.11745}, 2022{\natexlab{a}}.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Chen and Jiang(2022)]{chen2022offline}
Jinglin Chen and Nan Jiang.
\newblock Offline reinforcement learning under value and density-ratio realizability: {T}he power of gaps.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2022.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Modi, Krishnamurthy, Jiang, and Agarwal]{chen2022statistical}
Jinglin Chen, Aditya Modi, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock On the statistical efficiency of reward-free exploration in non-linear rl.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{b}}.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.
\newblock In \emph{Conference on Learning Theory}, 2008.

\bibitem[Dann et~al.(2018)Dann, Jiang, Krishnamurthy, Agarwal, Langford, and Schapire]{dann2018oracle}
Christoph Dann, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E Schapire.
\newblock On oracle-efficient {PAC} {RL} with rich observations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Daskalakis et~al.(2023)Daskalakis, Golowich, Haghtalab, and Shetty]{daskalakis2023smooth}
Constantinos Daskalakis, Noah Golowich, Nika Haghtalab, and Abhishek Shetty.
\newblock Smooth nash equilibria: Algorithms and complexity.
\newblock In \emph{Innovations in Theoretical Computer Science}, 2023.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and Langford]{du2019latent}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford.
\newblock Provably efficient {RL} with rich observations via latent state decoding.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and Wang]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization in {RL}.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Dudik et~al.(2011)Dudik, Hsu, Kale, Karampatziakis, Langford, Reyzin, and Zhang]{dudik2011efficient}
Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John Langford, Lev Reyzin, and Tong Zhang.
\newblock Efficient optimal learning for contextual bandits.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2011.

\bibitem[Ecoffet et~al.(2019)Ecoffet, Huizinga, Lehman, Stanley, and Clune]{ecoffet2019go}
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth~O Stanley, and Jeff Clune.
\newblock Go-explore: a new approach for hard-exploration problems.
\newblock \emph{arXiv:1901.10995}, 2019.

\bibitem[Ecoffet et~al.(2021)Ecoffet, Huizinga, Lehman, Stanley, and Clune]{ecoffet2021first}
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth~O Stanley, and Jeff Clune.
\newblock First return, then explore.
\newblock \emph{Nature}, 2021.

\bibitem[Farahmand et~al.(2010)Farahmand, Szepesv{\'a}ri, and Munos]{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock \emph{Advances in Neural Information Processing Systems}, 2010.

\bibitem[Foster and Krishnamurthy(2021)]{foster2021efficient}
Dylan~J Foster and Akshay Krishnamurthy.
\newblock Efficient first-order contextual bandits: Prediction, allocation, and triangular discrimination.
\newblock \emph{Neural Information Processing Systems}, 2021.

\bibitem[Foster and Rakhlin(2020)]{foster2020beyond}
Dylan~J Foster and Alexander Rakhlin.
\newblock Beyond {UCB}: Optimal and efficient contextual bandits with regression oracles.
\newblock \emph{International Conference on Machine Learning}, 2020.

\bibitem[Foster and Rakhlin(2023)]{foster2023lecture}
Dylan~J Foster and Alexander Rakhlin.
\newblock Foundations of reinforcement learning and interactive decision making.
\newblock \emph{arXiv:2312.16730}, 2023.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv:2112.13487}, 2021.

\bibitem[Foster et~al.(2022)Foster, Krishnamurthy, Simchi-Levi, and Xu]{foster2022offline}
Dylan~J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu.
\newblock Offline reinforcement learning: Fundamental barriers for value function approximation.
\newblock In \emph{Conference on Learning Theory}, 2022.

\bibitem[Foster et~al.(2023)Foster, Golowich, and Han]{foster2023tight}
Dylan~J Foster, Noah Golowich, and Yanjun Han.
\newblock Tight guarantees for interactive decision making with the decision-estimation coefficient.
\newblock \emph{Conference on Learning Theory}, 2023.

\bibitem[Foster et~al.(2024)Foster, Han, Qian, and Rakhlin]{foster2023online}
Dylan~J. Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin.
\newblock Online estimation via offline estimation: An information-theoretic framework.
\newblock \emph{In preparation}, 2024.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural networks.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, 2010.

\bibitem[Golowich et~al.(2023)Golowich, Rohatgi, and Moitra]{golowich2023exploring}
Noah Golowich, Dhruv Rohatgi, and Ankur Moitra.
\newblock Exploring and learning in sparse linear mdps without computationally intractable oracles.
\newblock \emph{arXiv:2309.09457}, 2023.

\bibitem[Guo et~al.(2022)Guo, Thakoor, P{\^\i}slar, Avila~Pires, Altch{\'e}, Tallec, Saade, Calandriello, Grill, Tang, Valko, Muno, Azar, and Piot]{guo2022byol}
Zhaohan Guo, Shantanu Thakoor, Miruna P{\^\i}slar, Bernardo Avila~Pires, Florent Altch{\'e}, Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, Michal Valko, R\'{e}mi Muno, Mohammad~Gheshlaghi Azar, and Bilal Piot.
\newblock Byol-explore: Exploration by bootstrapped prediction.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Haghtalab et~al.(2020)Haghtalab, Roughgarden, and Shetty]{haghtalab2020smoothed}
Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty.
\newblock Smoothed analysis of online and differentially private learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Haghtalab et~al.(2022{\natexlab{a}})Haghtalab, Han, Shetty, and Yang]{haghtalab2022oracle}
Nika Haghtalab, Yanjun Han, Abhishek Shetty, and Kunhe Yang.
\newblock Oracle-efficient online learning for smoothed adversaries.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022{\natexlab{a}}.

\bibitem[Haghtalab et~al.(2022{\natexlab{b}})Haghtalab, Roughgarden, and Shetty]{haghtalab2022smoothed}
Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty.
\newblock Smoothed analysis with adaptive adversaries.
\newblock In \emph{Symposium on Foundations of Computer Science (FOCS)}, 2022{\natexlab{b}}.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and Van~Soest]{hazan2019provably}
Elad Hazan, Sham Kakade, Karan Singh, and Abby Van~Soest.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Huang et~al.(2023)Huang, Chen, and Jiang]{huang2023reinforcement}
Audrey Huang, Jinglin Chen, and Nan Jiang.
\newblock Reinforcement learning in low-rank {MDP}s with density features.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Jiang and Huang(2020)]{jiang2020minimax}
Nan Jiang and Jiawei Huang.
\newblock Minimax value interval for off-policy evaluation and policy optimization.
\newblock \emph{Neural Information Processing Systems}, 2020.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E Schapire.
\newblock Contextual decision processes with low {Bellman} rank are {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \emph{Conference on Learning Theory}, 2020{\natexlab{b}}.

\bibitem[Jin et~al.(2021{\natexlab{a}})Jin, Liu, and Miryoosefi]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of {RL} problems, and sample-efficient algorithms.
\newblock \emph{Neural Information Processing Systems}, 2021{\natexlab{a}}.

\bibitem[Jin et~al.(2021{\natexlab{b}})Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline {RL}?
\newblock In \emph{International Conference on Machine Learning}, 2021{\natexlab{b}}.

\bibitem[Kakade(2003)]{kakade2003sample}
Sham~Machandranath Kakade.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock University College London, 2003.

\bibitem[Katdare et~al.(2023)Katdare, Jiang, and Driggs-Campbell]{katdare2023marginalized}
Pulkit Katdare, Nan Jiang, and Katherine Driggs-Campbell.
\newblock Marginalized importance sampling for off-environment policy evaluation.
\newblock In \emph{Conference on Robot Learning}, 2023.

\bibitem[Kiefer and Wolfowitz(1960)]{kiefer1960equivalence}
Jack Kiefer and Jacob Wolfowitz.
\newblock The equivalence of two extremum problems.
\newblock \emph{Canadian Journal of Mathematics}, 1960.

\bibitem[Kingma and Ba(2015)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Jens Kober, J~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 2013.

\bibitem[Li et~al.(2023)Li, Zhan, Lee, Chi, and Chen]{li2023reward}
Gen Li, Wenhao Zhan, Jason~D Lee, Yuejie Chi, and Yuxin Chen.
\newblock Reward-agnostic fine-tuning: Provable statistical benefits of hybrid reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023.

\bibitem[Li et~al.(2016)Li, Monroe, Ritter, Jurafsky, Galley, and Gao]{li2016deep}
Jiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky, Michel Galley, and Jianfeng Gao.
\newblock Deep reinforcement learning for dialogue generation.
\newblock In \emph{Conference on Empirical Methods in Natural Language Processing}, 2016.

\bibitem[Li et~al.(2010)Li, Chu, Langford, and Schapire]{li2010contextual}
Lihong Li, Wei Chu, John Langford, and Robert~E Schapire.
\newblock A contextual-bandit approach to personalized news article recommendation.
\newblock In \emph{International Conference on World Wide Web}, 2010.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa, Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Viano, and Cevher]{liu2023provable}
Fanghui Liu, Luca Viano, and Volkan Cevher.
\newblock Provable benefits of general coverage conditions in efficient online {RL} with function approximation.
\newblock \emph{International Conference on Machine Learning}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: {I}nfinite-horizon off-policy estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Lu, Xiong, Zhong, Hu, Zhang, Zheng, Yang, and Wang]{liu2023maximize}
Zhihan Liu, Miao Lu, Wei Xiong, Han Zhong, Hao Hu, Shenao Zhang, Sirui Zheng, Zhuoran Yang, and Zhaoran Wang.
\newblock Maximize to explore: {O}ne objective function fusing estimation, planning, and exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2023{\natexlab{b}}.

\bibitem[Martin et~al.(2017)Martin, Sasikumar, Everitt, and Hutter]{martin2017count}
Jarryd Martin, Suraj~Narayanan Sasikumar, Tom Everitt, and Marcus Hutter.
\newblock Count-based exploration in feature space for reinforcement learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence}, 2017.

\bibitem[Mhammedi et~al.(2023{\natexlab{a}})Mhammedi, Block, Foster, and Rakhlin]{mhammedi2023efficient}
Zakaria Mhammedi, Adam Block, Dylan~J Foster, and Alexander Rakhlin.
\newblock Efficient model-free exploration in low-rank {MDPs}.
\newblock \emph{Advances in Neural Information Processing Systems}, 2023{\natexlab{a}}.

\bibitem[Mhammedi et~al.(2023{\natexlab{b}})Mhammedi, Foster, and Rakhlin]{mhammedi2023representation}
Zakaria Mhammedi, Dylan~J Foster, and Alexander Rakhlin.
\newblock Representation learning with multi-step inverse kinematics: An efficient and optimal approach to rich-observation {RL}.
\newblock \emph{International Conference on Machine Learning}, 2023{\natexlab{b}}.

\bibitem[Misra et~al.(2020)Misra, Henaff, Krishnamurthy, and Langford]{misra2019kinematic}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation reinforcement learning.
\newblock In \emph{International conference on machine learning}, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness, Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 2015.

\bibitem[Modi et~al.(2024)Modi, Chen, Krishnamurthy, Jiang, and Agarwal]{modi2021model}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{Journal of Machine Learning Research}, 2024.

\bibitem[Munos(2003)]{munos2003error}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In \emph{International Conference on Machine Learning}, 2003.

\bibitem[Nguyen et~al.(2010)Nguyen, Wainwright, and Jordan]{nguyen2010estimating}
XuanLong Nguyen, Martin~J Wainwright, and Michael~I Jordan.
\newblock Estimating divergence functionals and the likelihood ratio by convex risk minimization.
\newblock \emph{IEEE Transactions on Information Theory}, 2010.

\bibitem[Ozdaglar et~al.(2023)Ozdaglar, Pattathil, Zhang, and Zhang]{ozdaglar2023revisiting}
Asuman~E Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang.
\newblock Revisiting the linear-programming framework for offline {RL} with general function approximation.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga, Desmaison, K\"{o}pf, Yang, DeVito, Raison, Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\"{o}pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Pathak et~al.(2017)Pathak, Agrawal, Efros, and Darrell]{pathak2017curiosity}
Deepak Pathak, Pulkit Agrawal, Alexei~A Efros, and Trevor Darrell.
\newblock Curiosity-driven exploration by self-supervised prediction.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Rashidinejad et~al.(2021)Rashidinejad, Zhu, Ma, Jiao, and Russell]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A tale of pessimism.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Rashidinejad et~al.(2023)Rashidinejad, Zhu, Yang, Russell, and Jiao]{rashidinejad2022optimal}
Paria Rashidinejad, Hanlin Zhu, Kunhe Yang, Stuart Russell, and Jiantao Jiao.
\newblock Optimal conservative offline {RL} with general function approximation via augmented lagrangian.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Rendle et~al.(2010)Rendle, Freudenthaler, and Schmidt{-}Thieme]{RendleFS10}
Steffen Rendle, Christoph Freudenthaler, and Lars Schmidt{-}Thieme.
\newblock Factorizing personalized {M}arkov chains for next-basket recommendation.
\newblock In \emph{International Conference on World Wide Web}, 2010.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, Dieleman, Grewe, Nham, Kalchbrenner, Sutskever, Lillicrap, Leach, Kavukcuoglu, Graepel, and Hassabis]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis.
\newblock Mastering the game of {G}o with deep neural networks and tree search.
\newblock \emph{Nature}, 2016.

\bibitem[Sion(1958)]{sion1958minimax}
Maurice Sion.
\newblock On general minimax theorems.
\newblock \emph{Pacific Journal of Mathematics}, 1958.

\bibitem[Song et~al.(2022)Song, Zhou, Sekhari, Bagnell, Krishnamurthy, and Sun]{song2022hybrid}
Yuda Song, Yifei Zhou, Ayush Sekhari, Drew Bagnell, Akshay Krishnamurthy, and Wen Sun.
\newblock Hybrid {RL}: Using both offline and online data can make {RL} efficient.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and Langford]{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and exponential improvements over model-free approaches.
\newblock In \emph{Conference on Learning Theory}, 2019.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and Mansour]{sutton1999policy}
Richard~S Sutton, David McAllester, Satinder Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 1999.

\bibitem[Tang et~al.(2017)Tang, Houthooft, Foote, Stooke, Xi~Chen, Duan, Schulman, DeTurck, and Abbeel]{tang2017exploration}
Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi~Chen, Yan Duan, John Schulman, Filip DeTurck, and Pieter Abbeel.
\newblock \# exploration: A study of count-based exploration for deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Uehara et~al.(2020)Uehara, Huang, and Jiang]{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and {Q}-function learning for off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Uehara et~al.(2021)Uehara, Imaizumi, Jiang, Kallus, Sun, and Xie]{uehara2021finite}
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie.
\newblock Finite sample analysis of minimax offline reinforcement learning: Completeness, fast rates and first-order efficiency.
\newblock \emph{arXiv:2102.02981}, 2021.

\bibitem[Uehara et~al.(2022)Uehara, Zhang, and Sun]{uehara2022representation}
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun.
\newblock Representation learning for online and offline {RL} in low-rank mdps.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Wagenmaker and Jamieson(2022)]{wagenmaker2022instance}
Andrew Wagenmaker and Kevin~G Jamieson.
\newblock Instance-dependent near-optimal policy identification in linear mdps via online experiment design.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Wagenmaker and Pacchiano(2023)]{wagenmaker2023leveraging}
Andrew Wagenmaker and Aldo Pacchiano.
\newblock Leveraging offline data in online reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, 2023.

\bibitem[Wagenmaker et~al.(2022)Wagenmaker, Simchowitz, and Jamieson]{wagenmaker2022beyond}
Andrew~J Wagenmaker, Max Simchowitz, and Kevin Jamieson.
\newblock Beyond no regret: Instance-dependent {PAC} reinforcement learning.
\newblock In \emph{Conference on Learning Theory}, 2022.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Du, Yang, and Salakhutdinov]{wang2020reward}
Ruosong Wang, Simon~S Du, Lin Yang, and Russ~R Salakhutdinov.
\newblock On reward-free reinforcement learning with linear function approximation.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Salakhutdinov, and Yang]{wang2020provably}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation: Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020{\natexlab{b}}.

\bibitem[Xie and Jiang(2020)]{xie2020q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A theoretical comparison.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence}, 2020.

\bibitem[Xie and Jiang(2021)]{xie2021batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro, and Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Xie et~al.(2023)Xie, Foster, Bai, Jiang, and Kakade]{xie2023role}
Tengyang Xie, Dylan~J Foster, Yu~Bai, Nan Jiang, and Sham~M Kakade.
\newblock The role of coverage in online reinforcement learning.
\newblock \emph{International Conference on Learning Representations}, 2023.

\bibitem[Yang et~al.(2020)Yang, Nachum, Dai, Li, and Schuurmans]{yang2020off}
Mengjiao Yang, Ofir Nachum, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yao et~al.(2014)Yao, Szepesv{\'{a}}ri, Pires, and Zhang]{YaoSPZ14}
Hengshuai Yao, Csaba Szepesv{\'{a}}ri, Bernardo~{\'{A}}vila Pires, and Xinhua Zhang.
\newblock Pseudo-{MDP}s and factored linear action models.
\newblock In \emph{Symposium on Adaptive Dynamic Programming and Reinforcement Learning}, 2014.

\bibitem[Zanette et~al.(2021)Zanette, Wainwright, and Brunskill]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Zhan et~al.(2022)Zhan, Huang, Huang, Jiang, and Lee]{zhan2022offline}
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee.
\newblock Offline reinforcement learning with realizability and single-policy concentrability.
\newblock In \emph{Conference on Learning Theory}, 2022.

\bibitem[Zhang et~al.(2022)Zhang, Song, Uehara, Wang, Agarwal, and Sun]{zhang2022efficient}
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun.
\newblock Efficient reinforcement learning in block {MDP}s: A model-free representation learning approach.
\newblock In \emph{International Conference on Machine Learning}, 2022.

\end{thebibliography}
