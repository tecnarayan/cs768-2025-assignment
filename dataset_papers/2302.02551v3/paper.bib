@InProceedings{pmlr-v37-plessis15,
  title = 	 {Convex Formulation for Learning from Positive and Unlabeled Data},
  author = 	 {Marthinus Du Plessis and Gang Niu and Masashi Sugiyama},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1386--1394},
  year = 	 {2015},
  editor = 	 {Francis Bach and David Blei},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/plessis15.pdf},
  url = 	 {http://proceedings.mlr.press/v37/plessis15.html},
  abstract = 	 {We discuss binary classification from only from positive and unlabeled data (PU classification), which is conceivable in various real-world machine learning problems. Since unlabeled data consists of both positive and negative data, simply separating positive and unlabeled data yields a biased solution. Recently, it was shown that the bias can be canceled by using a particular non-convex loss such as the ramp loss. However, classifier training with a non-convex loss is not straightforward in practice. In this paper, we discuss a convex formulation for PU classification that can still cancel the bias. The key idea is to use different loss functions for positive and unlabeled samples. However, in this setup, the hinge loss is not permissible. As an alternative, we propose the double hinge loss. Theoretically, we prove that the estimators converge to the optimal solutions at the optimal parametric rate. Experimentally, we demonstrate that PU classification with the double hinge loss performs as accurate as the non-convex method, with a much lower computational cost.}
}

@inproceedings{zhang2005simple,
  title={A simple probabilistic approach to learning from positive and unlabeled examples},
  author={Zhang, Dell and Lee, Wee Sun},
  booktitle={Proceedings of the 5th Annual UK Workshop on Computational Intelligence (UKCI)},
  pages={83--87},
  year={2005}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Artificial Intelligence and Statistics (AISTATS)},
  pages={249--256},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@article{dvoretzky1956asymptotic,
  title={Asymptotic minimax character of the sample distribution function and of the classical multinomial estimator},
  author={Dvoretzky, Aryeh and Kiefer, Jack and Wolfowitz, Jacob},
  journal={The Annals of Mathematical Statistics},
  pages={642--669},
  year={1956},
  publisher={JSTOR}
}

@inproceedings{NEURIPS2019a9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
year = {2019},
}

@inproceedings{abadi2016tensorflow,
  title={Tensorflow: A system for large-scale machine learning},
  author={Abadi, Mart{\'\i}n and Barham, Paul and Chen, Jianmin and Chen, Zhifeng and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Irving, Geoffrey and Isard, Michael and others},
  booktitle={12th USENIX Symposium on Operating Systems Design and Implementation},
  year={2016}
}


@inproceedings{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}


@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Association for Computational Linguistics (ACL)},
  year={2011}
}

@article{springenberg2014striving,
  title={Striving for simplicity: The all convolutional net},
  author={Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1412.6806},
  year={2014}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  year={2018},
  publisher={JMLR. org}
}

@inproceedings{Peters:2018,
  author={Peters, Matthew E. and  Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  title={Deep contextualized word representations},
  booktitle={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2018}
}
@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press}
}

@inproceedings{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={North American Chapter of the Association for Computational Linguistics (NAACL)},
  year={2019}
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics",
    pages = "38--45"
}

@inproceedings{liu2002partially,
  title={Partially supervised classification of text documents},
  author={Liu, Bing and Lee, Wee Sun and Yu, Philip S and Li, Xiaoli},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2002}
}
@inproceedings{li2003learning,
  title={Learning to classify texts using positive and unlabeled data},
  author={Li, Xiaoli and Liu, Bing},
  booktitle={International Joint Conference on Artificial Intelligence (IJCAI)},
  year={2003},
  organization={Citeseer}
}

@inproceedings{lee2003learning,
  title={Learning with positive and unlabeled examples using weighted logistic regression},
  author={Lee, Wee Sun and Liu, Bing},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2003}
}

@inproceedings{liu2003building,
  title={Building text classifiers using positive and unlabeled examples},
  author={Liu, Bing and Dai, Yang and Li, Xiaoli and Lee, Wee Sun and Yu, Philip S},
  booktitle={International Conference on Data Mining (ICDM)},
  year={2003},
}


@article{bekker2020learning,
  title={Learning from positive and unlabeled data: a survey.},
  author={Bekker, Jessa and Davis, Jesse},
  journal={Machine Learning},
  year={2020},
  publisher={Springer}
}
@inproceedings{scott2009novelty,
  title={Novelty detection: Unlabeled data definitely help},
  author={Scott, Clayton and Blanchard, Gilles},
  booktitle={Artificial intelligence and Statistics (AISTATS)},
  year={2009},
  organization={PMLR}
}

@inproceedings{sanderson2014class,
  title={Class proportion estimation with application to multiclass anomaly rejection},
  author={Sanderson, Tyler and Scott, Clayton},
  booktitle={Artificial Intelligence and Statistics (AISTATS)},
  pages={850--858},
  year={2014}
}

@article{blanchard2010semi,
  title={Semi-supervised novelty detection},
  author={Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={11},
  pages={2973--3009},
  year={2010},
  publisher={JMLR. org}
}

@inproceedings{elkan2008learning,
  title={Learning classifiers from only positive and unlabeled data},
  author={Elkan, Charles and Noto, Keith},
  booktitle={International Conference Knowledge Discovery and Data Mining (KDD)},
  pages={213--220},
  year={2008}
}

@inproceedings{christoffel2016class,
  title={Class-prior estimation for learning from positive and unlabeled data},
  author={Christoffel, Marthinus and Niu, Gang and Sugiyama, Masashi},
  booktitle={Asian Conference on Machine Learning},
  pages={221--236},
  year={2016}
}

@inproceedings{scott2015rate,
  title={A rate of convergence for mixture proportion estimation, with application to learning from noisy labels},
  author={Scott, Clayton},
  booktitle={Artificial Intelligence and Statistics},
  pages={838--846},
  year={2015}
}

@article{kato2018alternate,
  title={Alternate estimation of a classifier and the class-prior from positive and unlabeled data},
  author={Kato, Masahiro and Xu, Liyuan and Niu, Gang and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:1809.05710},
  year={2018}
}

@article{du2014analysis,
  title={Analysis of learning from positive and unlabeled data},
  author={Du Plessis, Marthinus C and Niu, Gang and Sugiyama, Masashi},
  journal={Advances in neural information processing systems},
  volume={27},
  pages={703--711},
  year={2014}
}

@inproceedings{du2015convex,
  title={Convex formulation for learning from positive and unlabeled data},
  author={Du Plessis, Marthinus and Niu, Gang and Sugiyama, Masashi},
  booktitle={International conference on machine learning},
  pages={1386--1394},
  year={2015}
}

@inproceedings{kiryo2017positive,
  title={Positive-unlabeled learning with non-negative risk estimator},
  author={Kiryo, Ryuichi and Niu, Gang and Du Plessis, Marthinus C and Sugiyama, Masashi},
  booktitle={Advances in neural information processing systems},
  pages={1675--1685},
  year={2017}
}

@article{ivanov2019dedpul,
  title={{DEDPUL}: Difference-of-Estimated-Densities-based Positive-Unlabeled Learning},
  author={Ivanov, Dmitry},
  journal={arXiv preprint arXiv:1902.06965},
  year={2019}
}

@article{hou2017generative,
  title={Generative adversarial positive-unlabelled learning},
  author={Hou, Ming and Chaib-Draa, Brahim and Li, Chao and Zhao, Qibin},
  journal={arXiv preprint arXiv:1711.08054},
  year={2017}
}

@inproceedings{ramaswamy2016mixture,
  title={Mixture proportion estimation via kernel embeddings of distributions},
  author={Ramaswamy, Harish and Scott, Clayton and Tewari, Ambuj},
  booktitle={International conference on machine learning},
  pages={2052--2060},
  year={2016}
}


@article{sansone2018efficient,
  title={Efficient training for positive unlabeled learning},
  author={Sansone, Emanuele and De Natale, Francesco GB and Zhou, Zhi-Hua},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={11},
  pages={2584--2598},
  year={2018},
  publisher={IEEE}
}

@article{jain2016nonparametric,
  title={Nonparametric semi-supervised learning of class proportions},
  author={Jain, Shantanu and White, Martha and Trosset, Michael W and Radivojac, Predrag},
  journal={arXiv preprint arXiv:1601.01944},
  year={2016}
}

@inproceedings{letouzey2000learning,
  title={Learning from positive and unlabeled examples},
  author={Letouzey, Fabien and Denis, Fran{\c{c}}ois and Gilleron, R{\'e}mi},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={71--85},
  year={2000},
  organization={Springer}
}

@inproceedings{de1999positive,
  title={Positive and unlabeled examples help learning},
  author={De Comit{\'e}, Francesco and Denis, Fran{\c{c}}ois and Gilleron, R{\'e}mi and Letouzey, Fabien},
  booktitle={International Conference on Algorithmic Learning Theory (ALT)},
  year={1999},
  organization={Springer}
}

@article{gershgorin1931uber,
  title={Uber die abgrenzung der eigenwerte einer matrix},
  author={Gershgorin, Semyon Aranovich},
  journal={Izv. Akad. Nauk. USSR Otd. Fiz.-Mat. Nauk (in German)},
  number={6},
  pages={749--754},
  year={1931},
  publisher={}
}


@inproceedings{gardner2018gpytorch,
  title={Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration},
  author={Gardner, Jacob and Pleiss, Geoff and Weinberger, Kilian Q and Bindel, David and Wilson, Andrew G},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@inproceedings{wang2019exact,
  title={Exact Gaussian Processes on a Million Data Points},
  author={Wang, Ke Alexander and Pleiss, Geoff and Gardner, Jacob R and Tyree, Stephen and Weinberger, Kilian Q and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{gretton2012kernel,
  title={A kernel two-sample test},
  author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
  journal={Journal of Machine Learning Research (JMLR)},
  volume={13},
  number={Mar},
  pages={723--773},
  year={2012}
}
@inproceedings{bekker2018estimating,
  title={Estimating the class prior in positive and unlabeled data through decision tree induction},
  author={Bekker, Jessa and Davis, Jesse},
  booktitle={Assosication for the Advancement of Artificial Intelligence (AAAI)},
  year={2018}
}

@inproceedings{denis1998pac,
  title={PAC learning from positive statistical queries},
  author={Denis, Fran{\c{c}}ois},
  booktitle={International Conference on Algorithmic Learning Theory (ALT)},
  year={1998},
  organization={Springer}
}

@article{du2014class,
  title={Class prior estimation from positive and unlabeled data},
  author={Du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
  journal={IEICE TRANSACTIONS on Information and Systems},
  volume={97},
  number={5},
  pages={1358--1362},
  year={2014},
  publisher={The Institute of Electronics, Information and Communication Engineers}
}

@article{kato2018alternate,
  title={Alternate estimation of a classifier and the class-prior from positive and unlabeled data},
  author={Kato, Masahiro and Xu, Liyuan and Niu, Gang and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:1809.05710},
  year={2018}
}

@inproceedings{hido2008inlier,
  title={Inlier-based outlier detection via direct density ratio estimation},
  author={Hido, Shohei and Tsuboi, Yuta and Kashima, Hisashi and Sugiyama, Masashi and Kanamori, Takafumi},
  booktitle={International Conference on Data Mining (ICDM)},
  pages={223--232},
  year={2008},
  organization={IEEE}
}

@article{du2014semi,
  title={Semi-supervised learning of class balance under class-prior change by distribution matching},
  author={Du Plessis, Marthinus Christoffel and Sugiyama, Masashi},
  journal={Neural Networks},
  volume={50},
  pages={110--119},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{vaicenavicius2019evaluating,
  title={Evaluating model calibration in classification},
  author={Vaicenavicius, Juozas and Widmann, David and Andersson, Carl and Lindsten, Fredrik and Roll, Jacob and Sch{\"o}n, Thomas B},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}


@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}

@inproceedings{azizzadenesheli2019regularized,
  title={Regularized learning for domain adaptation under label shifts},
  author={Azizzadenesheli, Kamyar and Liu, Anqi and Yang, Fanny and Anandkumar, Animashree},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}


@article{balakrishnan2017statistical,
  title={Statistical guarantees for the EM algorithm: From population to sample-based analysis},
  author={Balakrishnan, Sivaraman and Wainwright, Martin J and Yu, Bin and others},
  journal={The Annals of Statistics},
  volume={45},
  number={1},
  pages={77--120},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{wu2016convergence,
  title={On the convergence of the {EM} algorithm: A data-adaptive analysis},
  author={Wu, Chong and Yang, Can and Zhao, Hongyu and Zhu, Ji},
  journal={arXiv preprint arXiv:1611.00519},
  year={2016}
}

@article{wang2014high,
  title={High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality},
  author={Wang, Zhaoran and Gu, Quanquan and Ning, Yang and Liu, Han},
  journal={arXiv preprint arXiv:1412.8729},
  year={2014}
}

@article{celeux2001component,
  title={A component-wise EM algorithm for mixtures},
  author={Celeux, Gilles and Chr{\'e}tien, St{\'e}phane and Forbes, Florence and Mkhadri, Abdallah},
  journal={Journal of Computational and Graphical Statistics},
  volume={10},
  number={4},
  pages={697--712},
  year={2001},
  publisher={Taylor \& Francis}
}

@article{redner1984mixture,
  title={Mixture densities, maximum likelihood and the EM algorithm},
  author={Redner, Richard A and Walker, Homer F},
  journal={SIAM review},
  volume={26},
  number={2},
  pages={195--239},
  year={1984},
  publisher={SIAM}
}
@article{wu1983convergence,
  title={On the convergence properties of the EM algorithm},
  author={Wu, CF Jeff and others},
  journal={The Annals of statistics},
  volume={11},
  number={1},
  pages={95--103},
  year={1983},
  publisher={Institute of Mathematical Statistics}
}

@article{spokoiny2012parametric,
  title={Parametric estimation. Finite sample theory},
  author={Spokoiny, Vladimir and others},
  journal={The Annals of Statistics},
  volume={40},
  number={6},
  pages={2877--2909},
  year={2012},
  publisher={Institute of Mathematical Statistics}
}

@article{gershgorin1931uber,
  title={Uber die abgrenzung der eigenwerte einer matrix},
  author={Gershgorin, Semyon Aranovich},
  journal={Izv. Akad. Nauk. USSR Otd. Fiz.-Mat. Nauk (in German)},
  number={6},
  pages={749--754},
  year={1931},
  publisher={}
}

@book{horn2012matrix,
  title={Matrix analysis},
  author={Horn, Roger A and Johnson, Charles R},
  year={2012},
  publisher={Cambridge university press}
}


@article{balakrishnan2017statistical,
  title={Statistical guarantees for the EM algorithm: From population to sample-based analysis},
  author={Balakrishnan, Sivaraman and Wainwright, Martin J and Yu, Bin and others},
  journal={The Annals of Statistics},
  volume={45},
  number={1},
  pages={77--120},
  year={2017},
  publisher={Institute of Mathematical Statistics}
}

@article{liu2020early,
  title={Early-learning regularization prevents memorization of noisy labels},
  author={Liu, Sheng and Niles-Weed, Jonathan and Razavian, Narges and Fernandez-Granda, Carlos},
  journal={arXiv preprint arXiv:2007.00151},
  year={2020}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}


@article{li2019gradient,
  title={Gradient descent with early stopping is provably robust to label noise for overparameterized neural networks},
  author={Li, Mingchen and Soltanolkotabi, Mahdi and Oymak, Samet},
  journal={arXiv preprint arXiv:1903.11680},
  year={2019}
}


@article{wu2016convergence,
  title={On the convergence of the em algorithm: A data-adaptive analysis},
  author={Wu, Chong and Yang, Can and Zhao, Hongyu and Zhu, Ji},
  journal={arXiv preprint arXiv:1611.00519},
  year={2016}
}
@article{wang2014high,
  title={High dimensional expectation-maximization algorithm: Statistical optimization and asymptotic normality},
  author={Wang, Zhaoran and Gu, Quanquan and Ning, Yang and Liu, Han},
  journal={arXiv preprint arXiv:1412.8729},
  year={2014}
}
@article{celeux2001component,
  title={A component-wise EM algorithm for mixtures},
  author={Celeux, Gilles and Chr{\'e}tien, St{\'e}phane and Forbes, Florence and Mkhadri, Abdallah},
  journal={Journal of Computational and Graphical Statistics},
  volume={10},
  number={4},
  year={2001},
  publisher={Taylor \& Francis}
}
@article{redner1984mixture,
  title={Mixture densities, maximum likelihood and the EM algorithm},
  author={Redner, Richard A and Walker, Homer F},
  journal={SIAM review},
  volume={26},
  number={2},
  year={1984},
  publisher={SIAM}
}
@article{wu1983convergence,
  title={On the convergence properties of the EM algorithm},
  author={Wu, CF Jeff and others},
  journal={The Annals of statistics},
  volume={11},
  number={1},
  pages={95--103},
  year={1983},
  publisher={Institute of Mathematical Statistics}
}

@article{beck2003mirror,
  title={Mirror descent and nonlinear projected subgradient methods for convex optimization},
  author={Beck, Amir and Teboulle, Marc},
  journal={Operations Research Letters},
  volume={31},
  number={3},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{kumar2019verified,
  title={Verified uncertainty calibration},
  author={Kumar, Ananya and Liang, Percy S and Ma, Tengyu},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@article{stein1981estimation,
  title={Estimation of the mean of a multivariate normal distribution},
  author={Stein, Charles M},
  journal={The annals of Statistics},
  pages={1135--1151},
  year={1981},
  publisher={JSTOR}
}
@article{tropp2015introduction,
  title={An introduction to matrix concentration inequalities},
  author={Tropp, Joel A and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@inproceedings{alexandari2019adapting,
  title={Adapting to Label Shift with Bias-Corrected Calibration},
  author={Alexandari, Amr and Kundaje, Anshul and Shrikumar, Avanti},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021}
}

@book{van2000asymptotic,
  title={Asymptotic statistics},
  author={van der Vaart, Aad W},
  publisher={Cambridge university press}
}

@inproceedings{kuleshov2015calibrated,
  title={Calibrated structured prediction},
  author={Kuleshov, Volodymyr and Liang, Percy S},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2015}
}
@inproceedings{pires2012statistical,
  title={Statistical linear estimation with penalized estimators: an application to reinforcement learning},
  author={Pires, Bernardo Avila and Szepesv{\'a}ri, Csaba},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2012}
}
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2017}
}
@article{platt1999probabilistic,
  title={Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods},
  author={Platt, John and others},
  journal={Advances in large margin classifiers},
  volume={10},
  number={3},
  year={1999},
  publisher={Cambridge, MA}
}
@article{andersen1982cox,
  title={Cox's regression model for counting processes: a large sample study},
  author={Andersen, Per Kragh and Gill, Richard D},
  journal={The annals of statistics},
  year={1982},
  publisher={JSTOR}
}

@inproceedings{van2020scan,
  title={Scan: Learning to classify images without labels},
  author={Van Gansbeke, Wouter and Vandenhende, Simon and Georgoulis, Stamatios and Proesmans, Marc and Van Gool, Luc},
  booktitle={European Conference on Computer Vision},
  year={2020},
  organization={Springer}
}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Adversarial examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{szegedy2013intriguing,
  title={{Intriguing Properties of Neural Networks}},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2014}
}

@inproceedings{DBLP:conf/kdd/ZugnerAG18,
  author    = {Daniel Z{\"{u}}gner and
               Amir Akbarnejad and
               Stephan G{\"{u}}nnemann},
  title     = {{Adversarial Attacks on Neural Networks for Graph Data}},
  booktitle = {International Conference on Knowledge Discovery {\&} Data Mining (KDD)},
  year      = {2018}
 }


% =============================================================
% Data shift
% =============================================================

@book{sugiyama2017dataset,
  title={{Dataset Shift in Machine Learning}},
  author={Sugiyama, Masashi and Lawrence, Neil D and Schwaighofer, Anton and others},
  year={2017},
  publisher={The MIT Press}
}

@inproceedings{lipton2018detecting,
  title={{Detecting and Correcting for Label Shift with Black Box Predictors}},
  author={Lipton, Zachary C and Wang, Yu-Xiang and Smola, Alex},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

% =============================================================
% Change point detection
% =============================================================

% First works in this area

@article{page1957problems,
  title={{On Problems in Which a Change in a Parameter Occurs at an Unknown Point}},
  author={Page, ES},
  journal={Biometrika},
  year={1957},
  publisher={JSTOR}
}

@article{picard1985testing,
  title={{Testing and Estimating Change-Points in Time Series}},
  author={Picard, Dominique},
  journal={Advances in Applied Probability},
  year={1985},
  publisher={Cambridge University Press}
}

% More recent works

@article{truong2018review,
  title={{A Review of Change Point Detection Methods}},
  author={Truong, Charles and Oudre, Laurent and Vayatis, Nicolas},
  journal={arXiv Preprint arXiv:1801.00718},
  year={2018}
}

@article{aminikhanghahi2017survey,
  title={{A Survey of Methods for Time Series Change Point Detection}},
  author={Aminikhanghahi, Samaneh and Cook, Diane J},
  journal={Knowledge and Information Systems},
  year={2017},
}

@article{xie2013change,
  title={{Change-Point Detection for High-Dimensional Time Series with Missing Data}},
  author={Xie, Yao and Huang, Jiaji and Willett, Rebecca},
  journal={IEEE Journal of Selected Topics in Signal Processing},
  year={2013},
}

@inproceedings{mazhar2018bayesian,
  title={{Bayesian Model Selection for Change Point Detection and Clustering}},
  author={Mazhar, Othmane and Rojas, Cristian and Fischione, Carlo and Hesamzadeh, Mohammad Reza},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018}
}

@article{niu2016multiple,
  title={{Multiple Change-Point Detection: A Selective Overview}},
  author={Niu, Yue S and Hao, Ning and Zhang, Heping and others},
  journal={Statistical Science},
  year={2016},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{li2015m,
  title={{M-Statistic for Kernel Change-Point Detection}},
  author={Li, Shuang and Xie, Yao and Dai, Hanjun and Song, Le},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2015}
}

@article{song2017parallelization,
  title={{Parallelization of Change Point Detection}},
  author={Song, Nancy and Yang, Haw},
  journal={The Journal of Physical Chemistry},
  year={2017},
  publisher={ACS Publications}
}

@article{adams2007bayesian,
  title={{Bayesian Online Changepoint Detection}},
  author={Adams, Ryan Prescott and MacKay, David JC},
  journal={arXiv Preprint arXiv:0710.3742},
  year={2007}
}

@article{mohammad2006bayesian,
  title={{Bayesian Approach to Change Points Detection in Time Series}},
  author={Mohammad-Djafari, Ali and F{\'e}ron, Olivier},
  journal={International Journal of Imaging Systems and Technology},
  year={2006},
  publisher={Wiley Online Library}
}

@article{sinn2012detecting,
  title={{Detecting Change-Points in Time Series by Maximum Mean Discrepancy of Ordinal Pattern Distributions}},
  author={Sinn, Mathieu and Ghodsi, Ali and Keller, Karsten},
  journal={arXiv Preprint arXiv:1210.4903},
  year={2012}
}

@article{ding2017multiple,
  title={{Multiple Change Point Analysis: Fast Implementation and Strong Consistency}},
  author={Ding, Jie and Xiang, Yu and Shen, Lu and Tarokh, Vahid},
  journal={IEEE Transactions on Signal Processing},
  year={2017}
}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% bib from icml label shift
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Most relevant papers / theory
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@article{rosenbaum1983central,
  title={{The Central Role of the Propensity Score in Observational Studies for Causal Effects}},
  author={Rosenbaum, Paul R and Rubin, Donald B},
  journal={Biometrika},
  year={1983},
  publisher={Oxford University Press}
}

@article{kang2007demystifying,
  title={{Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data}},
  author={Kang, Joseph DY and Schafer, Joseph L},
  journal={Statistical Science},
  year={2007},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{zaremba2013b,
	title={{B-Test: A Non-Parametric, Low Variance Kernel Two-Sample Test}},
	author={Zaremba, Wojciech and Gretton, Arthur and Blaschko, Matthew},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	year={2013}
}

@article{gretton2012kernel,
	title={{A Kernel Two-Sample Test}},
	author={Gretton, Arthur and Borgwardt, Karsten M and Rasch, Malte J and Sch{\"o}lkopf, Bernhard and Smola, Alexander},
	journal={Journal of Machine Learning Research (JMLR)},
	year={2012}
}

@inproceedings{ramdas2015decreasing,
	title={{On the Decreasing Power of Kernel and Distance Based Nonparametric Hypothesis Tests in High Dimensions}},
	author={Ramdas, Aaditya and Reddi, Sashank Jakkam and P{\'o}czos, Barnab{\'a}s and Singh, Aarti and Wasserman, Larry A},
	booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
	year={2015}
}

@inproceedings{scholkopf2012causal,
  title={{On Causal and Anticausal Learning}},
  author={Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2012},
}

@inproceedings{zhang2013domain,
	title={{Domain Adaptation Under Target and Conditional Shift}},
	author={Zhang, Kun and Sch{\"o}lkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
	booktitle={International Conference on Machine Learning (ICML)},
	year={2013}
}

@inproceedings{smola2007hilbert,
	title={{A Hilbert Space Embedding for Distributions}},
	author={Smola, Alex and Gretton, Arthur and Song, Le and Sch{\"o}lkopf, Bernhard},
	booktitle={International Conference on Algorithmic Learning Theory},
	year={2007},
	organization={Springer}
}

@article{bickel2009discriminative,
	title={{Discriminative Learning Under Covariate Shift}},
	author={Bickel, Steffen and Br{\"u}ckner, Michael and Scheffer, Tobias},
	journal={Journal of Machine Learning Research (JMLR)},
	year={2009}
}
@article{gretton2009covariate,
	title={{Covariate Shift by Kernel Mean Matching}},
	author={Gretton, Arthur and Smola, Alexander J and Huang, Jiayuan and Schmittfull, Marcel and Borgwardt, Karsten M and Sch{\"o}lkopf, Bernhard},
	year={2009},
	journal={Journal of Machine Learning Research (JMLR)}
}

@article{song2013kernel,
	title={{Kernel Embeddings of Conditional Distributions: A Unified Kernel Framework for Nonparametric Inference in Graphical Models}},
	author={Song, Le and Fukumizu, Kenji and Gretton, Arthur},
	journal={IEEE Signal Processing Magazine},
	year={2013},
}

@inproceedings{cortes2010learning,
	title={{Learning Bounds for Importance Weighting}},
	author={Cortes, Corinna and Mansour, Yishay and Mohri, Mehryar},
	booktitle={Advances in Neural Information Processing Systems (NIPS)},
	year={2010}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background on covariate shift
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{shimodaira2000improving,
  title={{Improving Predictive Inference Under Covariate Shift by Weighting the Log-Likelihood Function}},
  author={Shimodaira, Hidetoshi},
  journal={Journal of Statistical Planning and Inference},
  year={2000},
}

@inproceedings{sugiyama2008direct,
  title={{Direct Importance Estimation with Model Selection and Its Application to Covariate Shift Adaptation}},
  author={Sugiyama, Masashi and Nakajima, Shinichi and Kashima, Hisashi and Buenau, Paul V and Kawanabe, Motoaki},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2008}
}

@inproceedings{huang2007correcting,
  title={{Correcting Sample Selection Bias by Unlabeled Data}},
  author={Huang, Jiayuan and Gretton, Arthur and Borgwardt, Karsten M and Sch{\"o}lkopf, Bernhard and Smola, Alex J},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2007}
}

@inproceedings{zadrozny2004learning,
  title={{Learning and Evaluating Classifiers Under Sample Selection Bias}},
  author={Zadrozny, Bianca},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2004}
}

%
% A whole book on non-stationary data
% but only addresses covariate-shift
%
@book{sugiyama2012machine,
  title={{Machine Learning in Non-Stationary Environments: Introduction to Covariate Shift Adaptation}},
  author={Sugiyama, Masashi and Kawanabe, Motoaki},
  year={2012}
}

@misc{heckman1977sample,
  title={{Sample Selection Bias as a Specification Error (With an Application to the Estimation of Labor Supply Functions)}},
  author={Heckman, James J},
  year={1977},
  publisher={National Bureau of Economic Research Cambridge, MA}
}

@inproceedings{chan2005word,
	title={{Word Sense Disambiguation with Distribution Estimation}},
	author={Chan, Yee Seng and Ng, Hwee Tou},
	booktitle={International Joint Conference on Artificial intelligence (IJCAI)},
	year={2005},
}

@inproceedings{dudik2006correcting,
  title={{Correcting Sample Selection Bias in Maximum Entropy Density Estimation}},
  author={Dud{\'\i}k, Miroslav and Phillips, Steven J and Schapire, Robert E},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2006}
}

@article{lin2002support,
  title={{Support Vector Machines for Classification in Nonstandard Situations}},
  author={Lin, Yi and Lee, Yoonkyung and Wahba, Grace},
  journal={Machine Learning},
  year={2002},
}

@article{sugiyama2005input,
  title={{Input-Dependent Estimation of Generalization Error Under Covariate Shift}},
  author={Sugiyama, Masashi and M{\"u}ller, Klaus-Robert},
  journal={Statistics \& Decisions},
  year={2005}
}

@article{sugiyama2007covariate,
  title={Covariate shift adaptation by importance weighted cross validation},
  author={Sugiyama, Masashi and Krauledat, Matthias and M{\~A}{\v{z}}ller, Klaus-Robert},
  journal={Journal of Machine Learning Research (JMLR)},
  year={2007}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Background on label shift
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Kun's paper (above
%
@article{manski1977estimation,
  title={{The Estimation of Choice Probabilities from Choice Based Samples}},
  author={Manski, Charles F and Lerman, Steven R},
  journal={Econometrica: Journal of the Econometric Society},
  year={1977},
}
@article{storkey2009training,
  title={{When Training and Test Sets Are Different: Characterizing Learning Transfer}},
  author={Storkey, Amos},
  journal={Dataset Shift in Machine Learning},
  year={2009},
  publisher={MIT Press}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  Motivating applications
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{beijbom2012automated,
  title={{Automated Annotation of Coral Reef Survey Images}},
  author={Beijbom, Oscar and Edmunds, Peter J and Kline, David I and Mitchell, B Greg and Kriegman, David},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2012},
}

%
%  Google flu
%
@article{ginsberg2009detecting,
  title={{Detecting Influenza Epidemics Using Search Engine Query Data}},
  author={Ginsberg, Jeremy and Mohebbi, Matthew H and Patel, Rajan S and Brammer, Lynnette and Smolinski, Mark S and Brilliant, Larry},
  journal={Nature},
  year={2009},
}


@inproceedings{elkan2001foundations,
  title={{The Foundations of Cost-Sensitive Learning}},
  author={Elkan, Charles},
  booktitle={International Joint Conference on Artificial intelligence (IJCAI)},
  year={2001},
}

@book{bishop1995neural,
  title={{Neural Networks for Pattern Recognition}},
  author={Bishop, Christopher M},
  year={1995},
  publisher={Oxford University Press}
}

@book{bishop2006pattern,
  title={{Pattern Recognition and Machine Learning}},
  author={Bishop, Christopher M},
  year={2006},
  publisher={Springer}
}

@book{murphy2012machine,
  title={{Machine Learning: A Probabilistic Perspective}},
  author={Murphy, Kevin P},
  year={2012},
  publisher={MIT Press}
}

@book{goodfellow2016deep,
  title={{Deep Learning}},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT Press}
}

@book{mohri2018foundations,
  title={{Foundations of Machine Learning}},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT Press}
}

@book{friedman2001elements,
  title={{The Elements of Statistical Learning}},
  author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
  year={2001},
  publisher={Springer Series in Statistics New York}
}

@book{james2013introduction,
  title={{An Introduction to Statistical Learning}},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  volume={112},
  year={2013},
  publisher={Springer}
}

@book{georgii2015stochastik,
  title={{Stochastik: Einf{\"u}hrung in die Wahrscheinlichkeitstheorie und Statistik}},
  author={Georgii, Hans-Otto},
  year={2015},
  publisher={Walter de Gruyter GmbH \& Co KG}
}

@inproceedings{zhu2010cognitive,
  title={{Cognitive Models of Test-Item Effects in Human Category Learning}},
  author={Zhu, Xiaojin and Gibson, Bryan R and Jun, Kwang-Sung and Rogers, Timothy T and Harrison, Joseph and Kalish, Chuck},
  booktitle={International Conference on Machine Leanring (ICML)},
  year={2010}
}

@inproceedings{deng2009imagenet,
  title={{ImageNet: A Large-Scale Hierarchical Image Database}},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2009},
}

%
%   PRECEDENT AMONG CLASSICAL STATS PAPERS
%

@article{buck1966comparison,
  title={{Comparison of a Screening Test and a Reference Test in Epidemiologic Studies. A Probabilistic Model for the Comparison of Diagnostic Tests}},
  author={Buck, AA and Gart, JJ and others},
  journal={American Journal of Epidemiology},
  year={1966}
}

@article{saerens2002adjusting,
  title={{Adjusting the Outputs of a Classifier to New a Priori Probabilities: A Simple Procedure}},
  author={Saerens, Marco and Latinne, Patrice and Decaestecker, Christine},
  journal={Neural Computation},
  year={2002},
  publisher={MIT Press}
}

@article{forman2008quantifying,
  title={{Quantifying Counts and Costs via Classification}},
  author={Forman, George},
  journal={Data Mining and Knowledge Discovery},
  year={2008},
  publisher={Springer}
}


@article{ramdas2016classification,
  title={{Classification Accuracy as a Proxy for Two Sample Testing}},
  author={Ramdas, Aaditya and Singh, Aarti and Wasserman, Larry},
  journal={arXiv Preprint arXiv:1602.02210},
  year={2016}
}

@inproceedings{sculley2014machine,
  title={{Machine Learning: The High-Interest Credit Card of Technical Debt}},
  author={Sculley, D and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
  year={2014},
  booktitle={SE4ML: Software Engineering for Machine Learning (NIPS 2014 Workshop)}
}

% Dimensionality reduction

@article{shlens2014tutorial,
  title={{A Tutorial on Principal Component Analysis}},
  author={Shlens, Jonathon},
  journal={arXiv Preprint arXiv:1404.1100},
  year={2014}
}

@techreport{rumelhart1985learning,
  title={{Learning Internal Representations by Error Propagation}},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  year={1985},
  institution={California Univ San Diego La Jolla Inst for Cognitive Science}
}

@inproceedings{bingham2001random,
  title={{Random Projection in Dimensionality Reduction: Applications to Image and Text Data}},
  author={Bingham, Ella and Mannila, Heikki},
  booktitle={International Conference on Knowledge Discovery {\&} Data Mining (KDD)},
  year={2001}
}

% Anomaly, outlier, novelty detection

@inproceedings{scholkopf2000support,
  title={{Support Vector Method for Novelty Detection}},
  author={Sch{\"o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex J and Shawe-Taylor, John and Platt, John C},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2000}
}

@inproceedings{hawkins2002outlier,
  title={{Outlier Detection Using Replicator Neural Networks}},
  author={Hawkins, Simon and He, Hongxing and Williams, Graham and Baxter, Rohan},
  booktitle={International Conference on Data Warehousing and Knowledge Discovery},
  year={2002},
  organization={Springer}
}

@article{fisher1936use,
  title={{The Use of Multiple Measurements in Taxonomic Problems}},
  author={Fisher, Ronald A},
  journal={Annals of Eugenics},
  year={1936},
  publisher={Wiley Online Library}
}

% Out-of-distribution

@inproceedings{hendrycks2016baseline,
  title={{A Baseline for Detecting Misclassified and Out-Of-Distribution Examples in Neural Networks}},
  author={Hendrycks, Dan and Gimpel, Kevin},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2017}
}

@article{shafaei2018does,
  title={{Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased Evaluation of Outlier Detectors}},
  author={Shafaei, Alireza and Schmidt, Mark and Little, James J},
  journal={arXiv Preprint arXiv:1809.04729},
  year={2018}
}

@inproceedings{liang2017enhancing,
  title={{Enhancing the Reliability of Out-Of-Distribution Image Detection in Neural Networks}},
  author={Liang, Shiyu and Li, Yixuan and Srikant, R},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{shalev2018out,
  title={{Out-Of-Distribution Detection Using Multiple Semantic Label Representations}},
  author={Shalev, Gabi and Adi, Yossi and Keshet, Joseph},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@article{choi2018generative,
  title={{Generative Ensembles for Robust Anomaly Detection}},
  author={Choi, Hyunsun and Jang, Eric},
  journal={arXiv Preprint arXiv:1810.01392},
  year={2018}
}

@article{alemi2018uncertainty,
  title={{Uncertainty in the Variational Information Bottleneck}},
  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V},
  journal={arXiv Preprint arXiv:1807.00906},
  year={2018}
}

@inproceedings{hendrycks2018deep,
  title={{Deep Anomaly Detection with Outlier Exposure}},
  author={Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas G},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

% Shift


% Domain adapt

@inproceedings{long2013transfer,
  title={{Transfer Feature Learning with Joint Distribution Adaptation}},
  author={Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2013}
}

% Tests

% Univariate

@article{chakravarty1967handbook,
  title={{Handbook of Methods of Applied Statistics}},
  author={Chakravarty, Indra M and Roy, JD and Laha, Radha Govind},
  year={1967},
  publisher={McGraw-Hill}
}

% Multivariate Tests

@article{friedman1979multivariate,
  title={{Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests}},
  author={Friedman, Jerome H and Rafsky, Lawrence C},
  journal={The Annals of Statistics},
  year={1979},
  publisher={JSTOR}
}

@article{friedman1983graph,
  title={{Graph-Theoretic Measures of Multivariate Association and Prediction}},
  author={Friedman, Jerome H and Rafsky, Lawrence C and others},
  journal={The Annals of Statistics},
  year={1983},
  publisher={Institute of Mathematical Statistics}
}



@article{szekely2013energy,
  title={{Energy Statistics: A Class of Statistics Based on Distances}},
  author={Sz{\'e}kely, G{\'a}bor J and Rizzo, Maria L},
  journal={Journal of Statistical Planning and Inference},
  year={2013},
}


@article{chandola2009anomaly,
  title={{Anomaly Detection: A Survey}},
  author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  journal={ACM Computing Surveys (CSUR)},
  year={2009},
}

@inproceedings{ben2010impossibility,
  title={{Impossibility Theorems for Domain Adaptation}},
  author={Ben-David, Shai and Lu, Tyler and Luu, Teresa and P{\'a}l, D{\'a}vid},
  booktitle={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  year={2010}
}

@inproceedings{schlegl2017unsupervised,
  title={{Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery}},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  booktitle={International Conference on Information Processing in Medical Imaging},
  year={2017},
}

@article{markou2003novelty,
  title={{Novelty Detection: A Review: Part 1: Statistical Approaches}},
  author={Markou, Markos and Singh, Sameer},
  journal={Signal Processing},
  year={2003},
}

@inproceedings{breunig2000lof,
  title={{LOF: Identifying Density-Based Local Outliers}},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={ACM SIGMOD Record},
  year={2000},
}

@inproceedings{liu2008isolation,
  title={{Isolation Forest}},
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={International Conference on Data Mining (ICDM)},
  year={2008},
}

@article{bland1995multiple,
  title={{Multiple Significance Tests: The Bonferroni Method}},
  author={Bland, J Martin and Altman, Douglas G},
  journal={BMJ},
  year={1995},
  publisher={British Medical Journal Publishing Group}
}

@article{simes1986improved,
  title={{An Improved Bonferroni Procedure for Multiple Tests of Significance}},
  author={Simes, R John},
  journal={Biometrika},
  year={1986},
  publisher={Oxford University Press}
}


@inproceedings{he2016deep,
  title={{Deep Residual Learning for Image Recognition}},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Computer Vision and Pattern Recognition (CVPR)},
  year={2016}
}

@article{vovk2018combining,
  title={{Combining $p$-Values via Averaging}},
  author={Vovk, Vladimir and Wang, Ruodu},
  journal={arXiv Preprint arXiv:1212.4966},
  year={2018}
}

@article{zaykin2002truncated,
  title={{Truncated Product Method for Combining $p$-Values}},
  author={Zaykin, Dmitri V and Zhivotovsky, Lev A and Westfall, Peter H and Weir, Bruce S},
  journal={Genetic Epidemiology: The Official Publication of the International Genetic Epidemiology Society},
  year={2002},
  publisher={Wiley Online Library}
}

@article{loughin2004systematic,
  title={{A Systematic Comparison of Methods for Combining $p$-Values from Independent Tests}},
  author={Loughin, Thomas M},
  journal={Computational Statistics \& Data Analysis},
  year={2004},
}

@article{heard2018choosing,
  title={{Choosing Between Methods of Combining-Values}},
  author={Heard, Nicholas A and Rubin-Delanchy, Patrick},
  journal={Biometrika},
  year={2018},
  publisher={Oxford University Press}
}

@inproceedings{goodfellow2014explaining,
  title={{Explaining and Harnessing Adversarial Examples}},
  author={Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2014}
}

@article{howard2018uniform,
  title={{Uniform, Nonparametric, Non-Asymptotic Confidence Sequences}},
  author={Howard, Steven R and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
  journal={arXiv Preprint arXiv:1810.08240},
  year={2018}
}

@inproceedings{lee2017training,
  title={{Training Confidence-Calibrated Classifiers for Detecting Out-Of-Distribution Samples}},
  author={Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{gretton2012optimal,
  title={{Optimal Kernel Choice for Large-Scale Two-Sample Tests}},
  author={Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2012}
}

@article{achlioptas2003database,
  title={{Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins}},
  author={Achlioptas, Dimitris},
  journal={Journal of Computer and System Sciences},
  volume={66},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{li2006very,
  title={{Very Sparse Random Projections}},
  author={Li, Ping and Hastie, Trevor J and Church, Kenneth W},
  booktitle={Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  year={2006},
  organization={ACM}
}

@techreport{krizhevsky2009learning,
  title={{Learning Multiple Layers of Features from Tiny Images}},
  author={Krizhevsky, Alex and Hinton, Geoffrey},
  year={2009},
  institution={Citeseer}
}

@article{lecun1998mnist,
  title={{Gradient-Based Learning Applied to Document Recognition}},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  year={1998},
  publisher={IEEE}
}

@book{wasserman2013all,
  title={{All of Statistics: A Concise Course in Statistical Inference}},
  author={Wasserman, Larry},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{vsidak1967rectangular,
  title={{Rectangular Confidence Regions for the Means of Multivariate Normal Distributions}},
  author={{\v{S}}id{\'a}k, Zbyn{\v{e}}k},
  journal={Journal of the American Statistical Association},
  volume={62},
  year={1967},
  publisher={Taylor \& Francis}
}

@article{tukey1949comparing,
  title={{Comparing Individual Means in the Analysis of Variance}},
  author={Tukey, John W and others},
  journal={Biometrics},
  volume={5},
  year={1949}
}

@article{holm1979simple,
  title={{A Simple Sequentially Rejective Multiple Test Procedure}},
  author={Holm, Sture},
  journal={Scandinavian Journal of Statistics},
  year={1979},
  publisher={JSTOR}
}

@article{hochberg1988sharper,
  title={{A Sharper Bonferroni Procedure for Multiple Tests of Significance}},
  author={Hochberg, Yosef}},
  journal={Biometrika},
  volume={75},
  year={1988},
  publisher={Oxford University Press}
}

@article{benjamini1995controlling,
  title={{Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing}},
  author={Benjamini, Yoav and Hochberg, Yosef},
  journal={Journal of the Royal Statistical Society},
  volume={57},
  year={1995},
  publisher={Wiley Online Library}
}

@article{benjamini2001control,
  title={{The Control of the False Discovery Rate in Multiple Testing Under Dependency}},
  author={Benjamini, Yoav and Yekutieli, Daniel and others},
  journal={The Annals of Statistics},
  volume={29},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}

@article{pearson1901liii,
  title={{On Lines and Planes of Closest Fit to Systems of Points in Space}},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={2},
  year={1901},
  publisher={Taylor \& Francis}
}

@article{robbins1951stochastic,
  title={{A Stochastic Approximation Method}},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  year={1951},
  publisher={JSTOR}
}

@article{kingma2014adam,
  title={{Adam: A Method for Stochastic Optimization}},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv Preprint arXiv:1412.6980},
  year={2014}
}

@article{duchi2011adaptive,
  title={{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  year={2011}
}

@article{zeiler2012adadelta,
  title={{ADADELTA: An Adaptive Learning Rate Method}},
  author={Zeiler, Matthew D},
  journal={arXiv Preprint arXiv:1212.5701},
  year={2012}
}

@article{nene1996object,
  title={{Columbia Object Image Library (COIL-100)}},
  author={Nene, Sameer A and Nayar, Shree K and Murase, Hiroshi},
  year={1996}
}


% Newly added


@article{bojarski2016end,
  title={{End to End Learning for Self-Driving Cars}},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv Preprint arXiv:1604.07316},
  year={2016}
}

@article{lakhani2017deep,
  title={{Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks}},
  author={Lakhani, Paras and Sundaram, Baskaran},
  journal={Radiology},
  volume={284},
  year={2017},
  publisher={Radiological Society of North America}
}

@inproceedings{stone2008autotagging,
  title={{Autotagging Facebook: Social Network Context Improves Photo Annotation}},
  author={Stone, Zak and Zickler, Todd and Darrell, Trevor},
  booktitle={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  year={2008},
  organization={IEEE}
}

@inproceedings{cheng2016wide,
  title={{Wide \& Deep Learning for Recommender Systems}},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
  year={2016},
  organization={ACM}
}

@inproceedings{covington2016deep,
  title={{Deep Neural Networks for YouTube Recommendations}},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM Conference on Recommender Systems},
  year={2016},
  organization={ACM}
}

@inproceedings{graves2013speech,
  title={{Speech Recognition with Deep Recurrent Neural Networks}},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
  year={2013},
  organization={IEEE}
}

@article{hinton2012deep,
  title={{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Kingsbury, Brian and others},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  year={2012}
}

@inproceedings{sutskever2014sequence,
  title={{Sequence to Sequence Learning with Neural Networks}},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2014}
}

@book{mitchell1997machine,
  title={{Machine Learning}},
  author={Tom Mitchell},
  year={1997}
}

@book{geron2017hands,
  title={{Hands-On Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems}},
  author={G{\'e}ron, Aur{\'e}lien},
  year={2017},
  publisher={O'Reilly Media, Inc.}
}

@book{scholkopf2001learning,
  title={{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT Press}
}

@article{mercer1909xvi,
  title={{Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations}},
  author={Mercer, James},
  journal={Philosophical Transactions},
  volume={209},
  year={1909},
  publisher={The Royal Society London}
}

@article{rumelhart1988learning,
  title={{Learning Representations by Back-Propagating Errors}},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive Modeling},
  volume={5},
  year={1988}
}

@book{bishop1995neural,
  title={{Neural Networks for Pattern Recognition}},
  author={Bishop, Christopher M and others},
  year={1995},
  publisher={Oxford University Press}
}

@article{abiodun2018state,
  title={{State-Of-The-Art in Artificial Neural Network Applications: A Survey}},
  author={Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
  journal={Heliyon},
  volume={4},
  year={2018},
  publisher={Elsevier}
}

@article{hubel1959receptive,
  title={{Receptive Fields of Single Neurones in the Cat's Striate Cortex}},
  author={Hubel, David H and Wiesel, Torsten N},
  journal={The Journal of Physiology},
  volume={148},
  year={1959},
  publisher={Wiley Online Library}
}

@article{lecun1989backpropagation,
  title={{Backpropagation Applied to Handwritten Zip Code Recognition}},
  author={LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  journal={Neural Computation},
  volume={1},
  year={1989},
  publisher={MIT Press}
}

@book{leskovec2014mining,
  title={{Mining of Massive Datasets}},
  author={Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year={2014},
  publisher={Cambridge University Press}
}

@inproceedings{lee2001algorithms,
  title={{Algorithms for Non-Negative Matrix Factorization}},
  author={Lee, Daniel D and Seung, H Sebastian},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2001}
}

@inproceedings{gionis1999similarity,
  title={{Similarity Search in High Dimensions via Hashing}},
  author={Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev and others},
  booktitle={Very Large Data Bases (VLDB)},
  volume={99},
  year={1999}
}

@article{johnson1984extensions,
  title={{Extensions of Lipschitz Mappings into a Hilbert Space}},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary Mathematics},
  volume={26},
  year={1984}
}

@article{kingma2013auto,
  title={{Auto-Encoding Variational Bayes}},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv Preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{ballard1987modular,
  title={{Modular Learning in Neural Networks}},
  author={Ballard, Dana H},
  booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
  year={1987}
}

@article{makhzani2013k,
  title={{K-Sparse Autoencoders}},
  author={Makhzani, Alireza and Frey, Brendan},
  journal={arXiv Preprint arXiv:1312.5663},
  year={2013}
}

@book{cox2006principles,
  title={{Principles of Statistical Inference}},
  author={Cox, David Roxbee},
  year={2006},
  publisher={Cambridge University Press}
}

@book{lehmann2006testing,
  title={{Testing Statistical Hypotheses}},
  author={Lehmann, Erich L and Romano, Joseph P},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{greenwood1996guide,
  title={{A Guide to Chi-Squared Testing}},
  author={Greenwood, Priscilla E and Nikulin, Michael S},
  volume={280},
  year={1996},
  publisher={John Wiley \& Sons}
}

@article{massey1951kolmogorov,
  title={{The Kolmogorov-Smirnov Test for Goodness of Fit}},
  author={Massey Jr, Frank J},
  journal={Journal of the American statistical Association},
  volume={46},
  year={1951},
  publisher={Taylor \& Francis}
}

@book{rupert2012simultaneous,
  title={{Simultaneous Statistical Inference}},
  author={Rupert Jr, G and others},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@techreport{hochberg1987multiple,
  title={{Multiple Comparison Procedures}},
  author={Hochberg, Yosef},
  year={1987},
  institution={John Wiley \& Sons}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@article{scikitlearn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@online{xiao2017,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@article{netzer2011reading,
  title={{Reading Digits in Natural Images With Unsupervised Feature Learning}},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

@inproceedings{wu2019domain,
  title={Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment},
  author={Wu, Yifan and Winston, Ezra and Kaushik, Divyansh and Lipton, Zachary},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}
@article{ben2010theory,
  title={A theory of learning from different domains},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
  journal={Machine learning},
  volume={79},
  number={1-2},
  year={2010},
  publisher={Springer}
}


%%% DA work empirical 

@inproceedings{long2015learning,
  title={Learning transferable features with deep adaptation networks},
  author={Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael},
  booktitle={International conference on machine learning},
  pages={97--105},
  year={2015},
  organization={PMLR}
}


@article{ganin2016domain,
  title={Domain-adversarial training of neural networks},
  author={Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, Fran{\c{c}}ois and Marchand, Mario and Lempitsky, Victor},
  journal={The journal of machine learning research},
  year={2016},
}

@inproceedings{sun2016deep,
  title={Deep coral: Correlation alignment for deep domain adaptation},
  author={Sun, Baochen and Saenko, Kate},
  booktitle={European conference on computer vision},
  year={2016},
  organization={Springer}
}

@incollection{sun2017correlation,
  title={Correlation alignment for unsupervised domain adaptation},
  author={Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  booktitle={Domain Adaptation in Computer Vision Applications},
  year={2017},
  publisher={Springer}
}

@inproceedings{long2017deep,
  title={Deep transfer learning with joint adaptation networks},
  author={Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I},
  booktitle={International conference on machine learning},
  year={2017},
  organization={PMLR}
}

@inproceedings{saito2018maximum,
  title={Maximum classifier discrepancy for unsupervised domain adaptation},
  author={Saito, Kuniaki and Watanabe, Kohei and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018}
}

@inproceedings{zhang2018collaborative,
  title={Collaborative and adversarial network for unsupervised domain adaptation},
  author={Zhang, Weichen and Ouyang, Wanli and Li, Wen and Xu, Dong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2018}
}

@inproceedings{zhang2019bridging,
  title={Bridging theory and algorithm for domain adaptation},
  author={Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  year={2019},
  organization={PMLR}
}

@article{sohn2020fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and Raffel, Colin A and Cubuk, Ekin Dogus and Kurakin, Alexey and Li, Chun-Liang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{6365193,
  author={Scheirer, Walter J. and de Rezende Rocha, Anderson and Sapkota, Archana and Boult, Terrance E.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Toward Open Set Recognition}, 
  year={2013}
  }
%%%

@inproceedings{byrd2019effect,
  title={What is the Effect of Importance Weighting in Deep Learning?},
  author={Byrd, Jonathon and Lipton, Zachary C},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@inproceedings{blair1985problem,
  title={Problem complexity and method efficiency in optimization (AS Nemirovsky and DB Yudin)},
  author={Nemirovski,Arkadi and Yudin,},
  year={1983},
  publisher={Wiley}
}

@book{geer2000empirical,
  title={Empirical Processes in M-estimation},
  author={van de Geer, Sara},
  volume={6},
  year={2000},
  publisher={Cambridge university press}
}

@incollection{van1996weak,
  title={Weak convergence},
  author={van der Vaart, Aad W and Wellner, Jon A},
  booktitle={Weak convergence and empirical processes},
  year={1996},
  publisher={Springer}
}

@inproceedings{garg2020labelshift,
title={ A Unified View of Label Shift Estimation},
author={Garg, Saurabh and Wu, Yifan and Balakrishnan, Sivaraman and Lipton, Zachary},
year={2020},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)}
}

@inproceedings{garg2021PUlearning,
title={Mixture Proportion Estimation and {PU} Learning: A Modern Approach},
author={Garg, Saurabh and Wu, Yifan and Smola, Alex and Balakrishnan, Sivaraman and Lipton, Zachary},
year={2021},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)} 
}

@inproceedings{garg2021RATT,
title={ {RATT}: Leveraging Unlabeled Data to Guarantee Generalization},
author={Garg, Saurabh and Balakrishnan, Sivaraman and Kolter, Zico and Lipton, Zachary},
year={2021},
booktitle={International Conference on Machine Learning (ICML)}
}

@inproceedings{garg2022ATC,
title={Leveraging Unlabeled Data to Predict Out-of-Distribution Performance},
author={Garg, Saurabh and Balakrishnan, Sivaraman and Lipton, Zachary and Neyshabur, Behnam and Sedghi, Hanie},
year={2022},
booktitle={International Conference on Learning Representations (ICLR)}
}


@article{ji2021predicting,
  title={Predicting Unreliable Predictions by Shattering a Neural Network},
  author={Ji, Xu and Pascanu, Razvan and Hjelm, Devon and Vedaldi, Andrea and Lakshminarayanan, Balaji and Bengio, Yoshua},
  journal={arXiv preprint arXiv:2106.08365},
  year={2021}
}

@inproceedings{lakshminarayanan2016simple,
  title={Simple and scalable predictive uncertainty estimation using deep ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2016}
}

@inproceedings{ovadia2019can,
  title={Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift},
  author={Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, David and Nowozin, Sebastian and Dillon, Joshua V and Lakshminarayanan, Balaji and Snoek, Jasper},    booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@inproceedings{zhang2020hybrid,
  title={Hybrid models for open set recognition},
  author={Zhang, Hongjie and Li, Ang and Guo, Jie and Guo, Yanwen},
  booktitle={European Conference on Computer Vision},
  pages={102--117},
  year={2020},
  organization={Springer}
}

@article{geifman2017selective,
  title={Selective classification for deep neural networks},
  author={Geifman, Yonatan and El-Yaniv, Ran},
  journal={arXiv preprint arXiv:1705.08500},
  year={2017}
}

@inproceedings{jiang2018trust,
  title={To Trust Or Not To Trust A Classifier.},
  author={Jiang, Heinrich and Kim, Been and Guan, Melody Y and Gupta, Maya R},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  pages={5546--5557},
  year={2018}
}

@inproceedings{rabanser2018failing,
  title={Failing loudly: An empirical study of methods for detecting dataset shift},
  author={Rabanser, Stephan and G{\"u}nnemann, Stephan and Lipton, Zachary C},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@inproceedings{venkateswara2017deep,
  title={Deep hashing network for unsupervised domain adaptation},
  author={Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5018--5027},
  year={2017}
}

@inproceedings{redko2019optimal,
  title={Optimal transport for multi-source domain adaptation under target shift},
  author={Redko, Ievgen and Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={849--858},
  year={2019},
  organization={PMLR}
}


@inproceedings{
anonymous2022the,
title={The Needle in the haystack: Out-distribution aware Self-training in an Open-World Setting},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=f9JwVXMJ1Up},
note={under review}
}

@inproceedings{saito2020universal,
  title={Universal domain adaptation through self supervision},
  author={Saito, Kuniaki and Kim, Donghyun and Sclaroff, Stan and Saenko, Kate},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard S},
  journal={arXiv preprint arXiv:1703.05175},
  year={2017}
}

@inproceedings{fu2020learning,
  title={Learning to detect open classes for universal domain adaptation},
  author={Fu, Bo and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin},
  booktitle={European Conference on Computer Vision},
  year={2020},
  organization={Springer}
}

@article{augustin2020out,
  title={Out-distribution aware Self-training in an Open World Setting},
  author={Augustin, Maximilian and Hein, Matthias},
  journal={arXiv preprint arXiv:2012.12372},
  year={2020}
}

@article{bousmalis2016domain,
  title={Domain separation networks},
  author={Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={343--351},
  year={2016}
}

@inproceedings{baktashmotlagh2018learning,
  title={Learning factorized representations for open-set domain adaptation},
  author={Baktashmotlagh, Mahsa and Faraki, Masoud and Drummond, Tom and Salzmann, Mathieu},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

@article{lian2019known,
  title={Known-class aware self-ensemble for open set domain adaptation},
  author={Lian, Qing and Li, Wen and Chen, Lin and Duan, Lixin},
  journal={arXiv preprint arXiv:1905.01068},
  year={2019}
}

@inproceedings{cao2019learning,
  title={Learning to transfer examples for partial domain adaptation},
  author={Cao, Zhangjie and You, Kaichao and Long, Mingsheng and Wang, Jianmin and Yang, Qiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2985--2994},
  year={2019}
}

@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}

@article{alom2019breast,
  title={Breast cancer classification from histopathological images with inception recurrent residual convolutional neural network},
  author={Alom, Md Zahangir and Yakopcic, Chris and Nasrin, Mst and Taha, Tarek M and Asari, Vijayan K and others},
  journal={Journal of digital imaging},
  year={2019},
  publisher={Springer}
}

@article{liao2016deep,
  title={A deep learning approach to universal skin disease classification},
  author={Liao, Haofu},
  journal={University of Rochester Department of Computer Science, CSC},
  year={2016}
}
@inproceedings{panareda2017open,
  title={Open set domain adaptation},
  author={Panareda Busto, Pau and Gall, Juergen},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={754--763},
  year={2017}
}

@article{peng2018syn2real,
  title={Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation},
  author={Peng, Xingchao and Usman, Ben and Saito, Kuniaki and Kaushik, Neela and Hoffman, Judy and Saenko, Kate},
  journal={arXiv preprint arXiv:1806.09755},
  year={2018}
}

@inproceedings{liu2019separate,
  title={Separate to adapt: Open set domain adaptation via progressive separation},
  author={Liu, Hong and Cao, Zhangjie and Long, Mingsheng and Wang, Jianmin and Yang, Qiang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2927--2936},
  year={2019}
}

@inproceedings{saito2018open,
  title={Open set domain adaptation by backpropagation},
  author={Saito, Kuniaki and Yamamoto, Shohei and Ushiku, Yoshitaka and Harada, Tatsuya},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={153--168},
  year={2018}
}

@inproceedings{tan2019weakly,
  title={Weakly supervised open-set domain adaptation by dual-domain collaboration},
  author={Tan, Shuhan and Jiao, Jiening and Zheng, Wei-Shi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5394--5403},
  year={2019}
}

@article{belghazi2021classifiers,
  title={What classifiers know what they don't?},
  author={Belghazi, Mohamed Ishmael and Lopez-Paz, David},
  journal={arXiv preprint arXiv:2107.06217},
  year={2021}
}

@inproceedings{jain2014multi,
  title={Multi-class open set recognition using probability of inclusion},
  author={Jain, Lalit P and Scheirer, Walter J and Boult, Terrance E},
  booktitle={European Conference on Computer Vision},
  pages={393--409},
  year={2014},
  organization={Springer}
}


@article{kechaou2020open,
  title={Open Set Domain Adaptation using Optimal Transport},
  author={Kechaou, Marwa and H{\'e}rault, Romain and Alaya, Mokhtar Z and Gasso, Gilles},
  journal={arXiv preprint arXiv:2010.01045},
  year={2020}
}

@inproceedings{xu2017multi,
  title={Multi-Positive and Unlabeled Learning.},
  author={Xu, Yixing and Xu, Chang and Xu, Chao and Tao, Dacheng},
  booktitle={IJCAI},
  pages={3182--3188},
  year={2017}
}

@article{ortiz2019cdot,
  title={CDOT: Continuous Domain Adaptation using Optimal Transport},
  author={Ortiz-Jimenez, Guillermo and Gheche, Mireille El and Simou, Effrosyni and Maretic, Hermina Petric and Frossard, Pascal},
  journal={arXiv preprint arXiv:1909.11448},
  year={2019}
}

@inproceedings{wulfmeier2018incremental,
  title={Incremental adversarial domain adaptation for continually changing environments},
  author={Wulfmeier, Markus and Bewley, Alex and Posner, Ingmar},
  booktitle={2018 IEEE International conference on robotics and automation (ICRA)},
  pages={4489--4495},
  year={2018},
  organization={IEEE}
}

@article{goel2020model,
  title={Model patching: Closing the subgroup performance gap with data augmentation},
  author={Goel, Karan and Gu, Albert and Li, Yixuan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2008.06775},
  year={2020}
}

@inproceedings{kumar2020understanding,
  title={Understanding self-training for gradual domain adaptation},
  author={Kumar, Ananya and Ma, Tengyu and Liang, Percy},
  booktitle={International Conference on Machine Learning},
  pages={5468--5479},
  year={2020},
  organization={PMLR}
}

@inproceedings{hoffman2018cycada,
  title={Cycada: Cycle-consistent adversarial domain adaptation},
  author={Hoffman, Judy and Tzeng, Eric and Park, Taesung and Zhu, Jun-Yan and Isola, Phillip and Saenko, Kate and Efros, Alexei and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={1989--1998},
  year={2018},
  organization={PMLR}
}

@inproceedings{palladino2020unsupervised,
  title={Unsupervised domain adaptation via CycleGAN for white matter hyperintensity segmentation in multicenter MR images},
  author={Palladino, Juli{\'a}n Alberto and Slezak, Diego Fernandez and Ferrante, Enzo},
  booktitle={16th International Symposium on Medical Information Processing and Analysis},
  volume={11583},
  pages={1158302},
  year={2020},
  organization={International Society for Optics and Photonics}
}

@inproceedings{xie2020self,
  title={Self-Supervised CycleGAN for Object-Preserving Image-to-Image Domain Adaptation},
  author={Xie, Xinpeng and Chen, Jiawei and Li, Yuexiang and Shen, Linlin and Ma, Kai and Zheng, Yefeng},
  booktitle={Computer Vision--ECCV 2020: 16th European Conference, Glasgow, UK, August 23--28, 2020, Proceedings, Part XX 16},
  pages={498--513},
  year={2020},
  organization={Springer}
}

@article{schneider2020improving,
  title={Improving robustness against common corruptions by covariate shift adaptation},
  author={Schneider, Steffen and Rusak, Evgenia and Eck, Luisa and Bringmann, Oliver and Brendel, Wieland and Bethge, Matthias},
  journal={arXiv preprint arXiv:2006.16971},
  year={2020}
}

@article{zhang2021predicting,
  title={On Predicting Generalization using GANs},
  author={Zhang, Yi and Gupta, Arushi and Saunshi, Nikunj and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2111.14212},
  year={2021}
}

@article{zhang2021memo,
  title={MEMO: Test Time Robustness via Adaptation and Augmentation},
  author={Zhang, Marvin and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2110.09506},
  year={2021}
}

@inproceedings{
wang2021tent,
title={Tent: Fully Test-Time Adaptation by Entropy Minimization},
author={Dequan Wang and Evan Shelhamer and Shaoteng Liu and Bruno Olshausen and Trevor Darrell},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=uXl3bZLkr3c}
}

@inproceedings{
mohta2021the,
title={The impact of domain shift on the calibration of fine-tuned models},
author={Jay Mohta and Colin Raffel},
booktitle={NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications},
year={2021},
url={https://openreview.net/forum?id=dZ7MVojplmi}
}

@inproceedings{
yu2021an,
title={An Empirical Study of Pre-trained Vision Models on Out-of-distribution Generalization},
author={Yaodong Yu and Heinrich Jiang and Dara Bahri and Hossein Mobahi and Seungyeon Kim and Ankit Singh Rawat and Andreas Veit and Yi Ma},
booktitle={NeurIPS 2021 Workshop on Distribution Shifts: Connecting Methods and Applications},
year={2021},
url={https://openreview.net/forum?id=z-LBrGmZaNs}
}

@inproceedings{
anonymous2022optimal,
title={Optimal Representations for Covariate Shift},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=Rf58LPCwJj0},
note={under review}
}

@inproceedings{
anonymous2022how,
title={How does Contrastive Pre-training Connect Disparate Domains?},
author={Anonymous},
booktitle={Submitted to The Tenth International Conference on Learning Representations },
year={2022},
url={https://openreview.net/forum?id=vBn2OXZuQCF},
note={under review}
}

@article{wang2020continuously,
  title={Continuously indexed domain adaptation},
  author={Wang, Hao and He, Hao and Katabi, Dina},
  journal={arXiv preprint arXiv:2007.01807},
  year={2020}
}

@inproceedings{cao2021open,
  title={Open-world semi-supervised learning},
  author={Cao, Kaidi and Brbic, Maria and Leskovec, Jure},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2022}
}

@inproceedings{cao2020concept,
  title={Concept learners for few-shot learning},
  author={Cao, Kaidi and Brbic, Maria and Leskovec, Jure},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{tabula2020single,
  title={A single cell transcriptomic atlas characterizes aging tissues in the mouse},
  author={Tabula Muris Consortium and others},
  journal={Nature},
  volume={583},
  number={7817},
  year={2020},
  publisher={NIH Public Access}
}

@article{vaze2021open,
  title={Open-set recognition: A good closed-set classifier is all you need},
  author={Vaze, Sagar and Han, Kai and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:2110.06207},
  year={2021}
}

@inproceedings{bucci2020effectiveness,
  title={On the effectiveness of image rotation for open set domain adaptation},
  author={Bucci, Silvia and Loghmani, Mohammad Reza and Tommasi, Tatiana},
  booktitle={European Conference on Computer Vision},
  year={2020},
  organization={Springer}
}

@inproceedings{shu2020learning,
  title={Learning from multi-class positive and unlabeled data},
  author={Shu, Senlin and Lin, Zhuoyi and Yan, Yan and Li, Li},
  booktitle={2020 IEEE International Conference on Data Mining (ICDM)},
  year={2020},
  organization={IEEE}
}

@inproceedings{boult2019learning,
  title={Learning and the unknown: Surveying steps toward open world recognition},
  author={Boult, Terrance E and Cruz, Steve and Dhamija, Akshay Raj and Gunther, Manuel and Henrydoss, James and Scheirer, Walter J},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  year={2019}
}

@inproceedings{bendale2015towards,
  title={Towards open world recognition},
  author={Bendale, Abhijit and Boult, Terrance},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1893--1902},
  year={2015}
}
% Anomaly, outlier, novelty detection

@inproceedings{scholkopf2000support,
  title={{Support Vector Method for Novelty Detection}},
  author={Sch{\"o}lkopf, Bernhard and Williamson, Robert C and Smola, Alex J and Shawe-Taylor, John and Platt, John C},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2000}
}

@inproceedings{hawkins2002outlier,
  title={{Outlier Detection Using Replicator Neural Networks}},
  author={Hawkins, Simon and He, Hongxing and Williams, Graham and Baxter, Rohan},
  booktitle={International Conference on Data Warehousing and Knowledge Discovery},
  year={2002},
  organization={Springer}
}

@article{fisher1936use,
  title={{The Use of Multiple Measurements in Taxonomic Problems}},
  author={Fisher, Ronald A},
  journal={Annals of Eugenics},
  year={1936},
  publisher={Wiley Online Library}
}

@inproceedings{santurkar2020breeds,
  title={Breeds: Benchmarks for subpopulation shift},
  author={Santurkar, Shibani and Tsipras, Dimitris and Madry, Aleksander},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  number={3},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{spanhol2015dataset,
  title={A dataset for breast cancer histopathological image classification},
  author={Spanhol, Fabio A and Oliveira, Luiz S and Petitjean, Caroline and Heutte, Laurent},
  journal={Ieee transactions on biomedical engineering},
  volume={63},
  number={7},
  pages={1455--1462},
  year={2015},
  publisher={IEEE}
}

% Out-of-distribution

@article{shafaei2018does,
  title={{Does Your Model Know the Digit 6 Is Not a Cat? A Less Biased Evaluation of Outlier Detectors}},
  author={Shafaei, Alireza and Schmidt, Mark and Little, James J},
  journal={arXiv Preprint arXiv:1809.04729},
  year={2018}
}

@inproceedings{liang2017enhancing,
  title={{Enhancing the Reliability of Out-Of-Distribution Image Detection in Neural Networks}},
  author={Liang, Shiyu and Li, Yixuan and Srikant, R},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{shalev2018out,
  title={{Out-Of-Distribution Detection Using Multiple Semantic Label Representations}},
  author={Shalev, Gabi and Adi, Yossi and Keshet, Joseph},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2018}
}

@article{choi2018generative,
  title={{Generative Ensembles for Robust Anomaly Detection}},
  author={Choi, Hyunsun and Jang, Eric},
  journal={arXiv Preprint arXiv:1810.01392},
  year={2018}
}

@article{alemi2018uncertainty,
  title={{Uncertainty in the Variational Information Bottleneck}},
  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V},
  journal={arXiv Preprint arXiv:1807.00906},
  year={2018}
}

@inproceedings{hendrycks2018deep,
  title={{Deep Anomaly Detection with Outlier Exposure}},
  author={Hendrycks, Dan and Mazeika, Mantas and Dietterich, Thomas G},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2019}
}

% Shift


% Domain adapt

@inproceedings{long2013transfer,
  title={{Transfer Feature Learning with Joint Distribution Adaptation}},
  author={Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S},
  booktitle={International Conference on Computer Vision (ICCV)},
  year={2013}
}

% Tests

% Univariate

@article{chakravarty1967handbook,
  title={{Handbook of Methods of Applied Statistics}},
  author={Chakravarty, Indra M and Roy, JD and Laha, Radha Govind},
  year={1967},
  publisher={McGraw-Hill}
}

% Multivariate Tests

@article{friedman1979multivariate,
  title={{Multivariate Generalizations of the Wald-Wolfowitz and Smirnov Two-Sample Tests}},
  author={Friedman, Jerome H and Rafsky, Lawrence C},
  journal={The Annals of Statistics},
  year={1979},
  publisher={JSTOR}
}

@article{friedman1983graph,
  title={{Graph-Theoretic Measures of Multivariate Association and Prediction}},
  author={Friedman, Jerome H and Rafsky, Lawrence C and others},
  journal={The Annals of Statistics},
  year={1983},
  publisher={Institute of Mathematical Statistics}
}



@article{szekely2013energy,
  title={{Energy Statistics: A Class of Statistics Based on Distances}},
  author={Sz{\'e}kely, G{\'a}bor J and Rizzo, Maria L},
  journal={Journal of Statistical Planning and Inference},
  year={2013},
}


@article{chandola2009anomaly,
  title={{Anomaly Detection: A Survey}},
  author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  journal={ACM Computing Surveys (CSUR)},
  year={2009},
}


@inproceedings{saenko2010adapting,
  title={Adapting visual category models to new domains},
  author={Saenko, Kate and Kulis, Brian and Fritz, Mario and Darrell, Trevor},
  booktitle={European conference on computer vision},
  pages={213--226},
  year={2010},
  organization={Springer}
}

@inproceedings{peng2019moment,
  title={Moment matching for multi-source domain adaptation},
  author={Peng, Xingchao and Bai, Qinxun and Xia, Xide and Huang, Zijun and Saenko, Kate and Wang, Bo},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1406--1415},
  year={2019}
}

@inproceedings{zhang2020coping,
  title={Coping with label shift via distributionally robust optimisation},
  author={Zhang, Jingzhao and Menon, Aditya and Veit, Andreas and Bhojanapalli, Srinadh and Kumar, Sanjiv and Sra, Suvrit},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year={2021}
}

@inproceedings{wilds2021,
  title = {{WILDS}: A Benchmark of in-the-Wild Distribution Shifts},
  author = {Pang Wei Koh and Shiori Sagawa and Henrik Marklund and Sang Michael Xie and Marvin Zhang and Akshay Balsubramani and Weihua Hu and Michihiro Yasunaga and Richard Lanas Phillips and Irena Gao and Tony Lee and Etienne David and Ian Stavness and Wei Guo and Berton A. Earnshaw and Imran S. Haque and Sara Beery and Jure Leskovec and Anshul Kundaje and Emma Pierson and Sergey Levine and Chelsea Finn and Percy Liang},
  booktitle = {International Conference on Machine Learning (ICML)},
  year = {2021}
}

@inproceedings{sagawa2021extending,
  title = {Extending the WILDS Benchmark for Unsupervised Adaptation}, 
  author = {Shiori Sagawa and Pang Wei Koh and Tony Lee and Irena Gao and Sang Michael Xie and Kendrick Shen and Ananya Kumar and Weihua Hu and Michihiro Yasunaga and Henrik Marklund and Sara Beery and Etienne David and Ian Stavness and Wei Guo and Jure Leskovec and Kate Saenko and Tatsunori Hashimoto and Sergey Levine and Chelsea Finn and Percy Liang},
  booktitle = {NeurIPS Workshop on Distribution Shifts},
  year = {2021}
}

@inproceedings{schlegl2017unsupervised,
  title={{Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery}},
  author={Schlegl, Thomas and Seeb{\"o}ck, Philipp and Waldstein, Sebastian M and Schmidt-Erfurth, Ursula and Langs, Georg},
  booktitle={International Conference on Information Processing in Medical Imaging},
  year={2017},
}

@article{markou2003novelty,
  title={{Novelty Detection: A Review: Part 1: Statistical Approaches}},
  author={Markou, Markos and Singh, Sameer},
  journal={Signal Processing},
  year={2003},
}

@inproceedings{breunig2000lof,
  title={{LOF: Identifying Density-Based Local Outliers}},
  author={Breunig, Markus M and Kriegel, Hans-Peter and Ng, Raymond T and Sander, J{\"o}rg},
  booktitle={ACM SIGMOD Record},
  year={2000},
}

@inproceedings{liu2008isolation,
  title={{Isolation Forest}},
  author={Liu, Fei Tony and Ting, Kai Ming and Zhou, Zhi-Hua},
  booktitle={International Conference on Data Mining (ICDM)},
  year={2008},
}

@article{bland1995multiple,
  title={{Multiple Significance Tests: The Bonferroni Method}},
  author={Bland, J Martin and Altman, Douglas G},
  journal={BMJ},
  year={1995},
  publisher={British Medical Journal Publishing Group}
}

@article{simes1986improved,
  title={{An Improved Bonferroni Procedure for Multiple Tests of Significance}},
  author={Simes, R John},
  journal={Biometrika},
  year={1986},
  publisher={Oxford University Press}
}


@article{vovk2018combining,
  title={{Combining $p$-Values via Averaging}},
  author={Vovk, Vladimir and Wang, Ruodu},
  journal={arXiv Preprint arXiv:1212.4966},
  year={2018}
}

@article{zaykin2002truncated,
  title={{Truncated Product Method for Combining $p$-Values}},
  author={Zaykin, Dmitri V and Zhivotovsky, Lev A and Westfall, Peter H and Weir, Bruce S},
  journal={Genetic Epidemiology: The Official Publication of the International Genetic Epidemiology Society},
  year={2002},
  publisher={Wiley Online Library}
}

@article{loughin2004systematic,
  title={{A Systematic Comparison of Methods for Combining $p$-Values from Independent Tests}},
  author={Loughin, Thomas M},
  journal={Computational Statistics \& Data Analysis},
  year={2004},
}

@article{heard2018choosing,
  title={{Choosing Between Methods of Combining-Values}},
  author={Heard, Nicholas A and Rubin-Delanchy, Patrick},
  journal={Biometrika},
  year={2018},
  publisher={Oxford University Press}
}

@inproceedings{goodfellow2014explaining,
  title={{Explaining and Harnessing Adversarial Examples}},
  author={Goodfellow, Ian and Shlens, Jonathon and Szegedy, Christian},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2014}
}

@article{howard2018uniform,
  title={{Uniform, Nonparametric, Non-Asymptotic Confidence Sequences}},
  author={Howard, Steven R and Ramdas, Aaditya and McAuliffe, Jon and Sekhon, Jasjeet},
  journal={arXiv Preprint arXiv:1810.08240},
  year={2018}
}

@inproceedings{lee2017training,
  title={{Training Confidence-Calibrated Classifiers for Detecting Out-Of-Distribution Samples}},
  author={Lee, Kimin and Lee, Honglak and Lee, Kibok and Shin, Jinwoo},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{gretton2012optimal,
  title={{Optimal Kernel Choice for Large-Scale Two-Sample Tests}},
  author={Gretton, Arthur and Sejdinovic, Dino and Strathmann, Heiko and Balakrishnan, Sivaraman and Pontil, Massimiliano and Fukumizu, Kenji and Sriperumbudur, Bharath K},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2012}
}

@article{achlioptas2003database,
  title={{Database-Friendly Random Projections: Johnson-Lindenstrauss with Binary Coins}},
  author={Achlioptas, Dimitris},
  journal={Journal of Computer and System Sciences},
  volume={66},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{li2006very,
  title={{Very Sparse Random Projections}},
  author={Li, Ping and Hastie, Trevor J and Church, Kenneth W},
  booktitle={Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD)},
  year={2006},
  organization={ACM}
}

@book{wasserman2013all,
  title={{All of Statistics: A Concise Course in Statistical Inference}},
  author={Wasserman, Larry},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{vsidak1967rectangular,
  title={{Rectangular Confidence Regions for the Means of Multivariate Normal Distributions}},
  author={{\v{S}}id{\'a}k, Zbyn{\v{e}}k},
  journal={Journal of the American Statistical Association},
  volume={62},
  year={1967},
  publisher={Taylor \& Francis}
}

@article{tukey1949comparing,
  title={{Comparing Individual Means in the Analysis of Variance}},
  author={Tukey, John W and others},
  journal={Biometrics},
  volume={5},
  year={1949}
}

@article{holm1979simple,
  title={{A Simple Sequentially Rejective Multiple Test Procedure}},
  author={Holm, Sture},
  journal={Scandinavian Journal of Statistics},
  year={1979},
  publisher={JSTOR}
}

@article{hochberg1988sharper,
  title={{A Sharper Bonferroni Procedure for Multiple Tests of Significance}},
  author={Hochberg, Yosef}},
  journal={Biometrika},
  volume={75},
  year={1988},
  publisher={Oxford University Press}
}

@article{benjamini1995controlling,
  title={{Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing}},
  author={Benjamini, Yoav and Hochberg, Yosef},
  journal={Journal of the Royal Statistical Society},
  volume={57},
  year={1995},
  publisher={Wiley Online Library}
}

@article{benjamini2001control,
  title={{The Control of the False Discovery Rate in Multiple Testing Under Dependency}},
  author={Benjamini, Yoav and Yekutieli, Daniel and others},
  journal={The Annals of Statistics},
  volume={29},
  year={2001},
  publisher={Institute of Mathematical Statistics}
}

@article{pearson1901liii,
  title={{On Lines and Planes of Closest Fit to Systems of Points in Space}},
  author={Pearson, Karl},
  journal={The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science},
  volume={2},
  year={1901},
  publisher={Taylor \& Francis}
}

@article{robbins1951stochastic,
  title={{A Stochastic Approximation Method}},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The Annals of Mathematical Statistics},
  year={1951},
  publisher={JSTOR}
}

@article{kingma2014adam,
  title={{Adam: A Method for Stochastic Optimization}},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv Preprint arXiv:1412.6980},
  year={2014}
}

@article{duchi2011adaptive,
  title={{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  year={2011}
}

@article{zeiler2012adadelta,
  title={{ADADELTA: An Adaptive Learning Rate Method}},
  author={Zeiler, Matthew D},
  journal={arXiv Preprint arXiv:1212.5701},
  year={2012}
}

@article{nene1996object,
  title={{Columbia Object Image Library (COIL-100)}},
  author={Nene, Sameer A and Nayar, Shree K and Murase, Hiroshi},
  year={1996}
}


% Newly added


@article{bojarski2016end,
  title={{End to End Learning for Self-Driving Cars}},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv Preprint arXiv:1604.07316},
  year={2016}
}

@article{lakhani2017deep,
  title={{Deep Learning at Chest Radiography: Automated Classification of Pulmonary Tuberculosis by Using Convolutional Neural Networks}},
  author={Lakhani, Paras and Sundaram, Baskaran},
  journal={Radiology},
  volume={284},
  year={2017},
  publisher={Radiological Society of North America}
}

@inproceedings{stone2008autotagging,
  title={{Autotagging Facebook: Social Network Context Improves Photo Annotation}},
  author={Stone, Zak and Zickler, Todd and Darrell, Trevor},
  booktitle={IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops},
  year={2008},
  organization={IEEE}
}

@inproceedings{cheng2016wide,
  title={{Wide \& Deep Learning for Recommender Systems}},
  author={Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and others},
  booktitle={Proceedings of the 1st Workshop on Deep Learning for Recommender Systems},
  year={2016},
  organization={ACM}
}

@inproceedings{covington2016deep,
  title={{Deep Neural Networks for YouTube Recommendations}},
  author={Covington, Paul and Adams, Jay and Sargin, Emre},
  booktitle={Proceedings of the 10th ACM Conference on Recommender Systems},
  year={2016},
  organization={ACM}
}

@inproceedings{graves2013speech,
  title={{Speech Recognition with Deep Recurrent Neural Networks}},
  author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  booktitle={IEEE International Conference on Acoustics, Speech and Signal Processing},
  year={2013},
  organization={IEEE}
}

@article{hinton2012deep,
  title={{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
  author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Kingsbury, Brian and others},
  journal={IEEE Signal Processing Magazine},
  volume={29},
  year={2012}
}

@inproceedings{sutskever2014sequence,
  title={{Sequence to Sequence Learning with Neural Networks}},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2014}
}

@book{mitchell1997machine,
  title={{Machine Learning}},
  author={Tom Mitchell},
  year={1997}
}

@book{geron2017hands,
  title={{Hands-On Machine Learning with Scikit-Learn and Tensorflow: Concepts, Tools, and Techniques to Build Intelligent Systems}},
  author={G{\'e}ron, Aur{\'e}lien},
  year={2017},
  publisher={O'Reilly Media, Inc.}
}

@book{scholkopf2001learning,
  title={{Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond}},
  author={Sch{\"o}lkopf, Bernhard and Smola, Alexander J},
  year={2001},
  publisher={MIT Press}
}

@article{mercer1909xvi,
  title={{Functions of Positive and Negative Type, and Their Connection the Theory of Integral Equations}},
  author={Mercer, James},
  journal={Philosophical Transactions},
  volume={209},
  year={1909},
  publisher={The Royal Society London}
}

@article{rumelhart1988learning,
  title={{Learning Representations by Back-Propagating Errors}},
  author={Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J and others},
  journal={Cognitive Modeling},
  volume={5},
  year={1988}
}

@book{bishop1995neural,
  title={{Neural Networks for Pattern Recognition}},
  author={Bishop, Christopher M and others},
  year={1995},
  publisher={Oxford University Press}
}

@article{abiodun2018state,
  title={{State-Of-The-Art in Artificial Neural Network Applications: A Survey}},
  author={Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat AbdElatif and Arshad, Humaira},
  journal={Heliyon},
  volume={4},
  year={2018},
  publisher={Elsevier}
}

@article{hubel1959receptive,
  title={{Receptive Fields of Single Neurones in the Cat's Striate Cortex}},
  author={Hubel, David H and Wiesel, Torsten N},
  journal={The Journal of Physiology},
  volume={148},
  year={1959},
  publisher={Wiley Online Library}
}


@book{leskovec2014mining,
  title={{Mining of Massive Datasets}},
  author={Leskovec, Jure and Rajaraman, Anand and Ullman, Jeffrey David},
  year={2014},
  publisher={Cambridge University Press}
}

@inproceedings{lee2001algorithms,
  title={{Algorithms for Non-Negative Matrix Factorization}},
  author={Lee, Daniel D and Seung, H Sebastian},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  year={2001}
}

@inproceedings{gionis1999similarity,
  title={{Similarity Search in High Dimensions via Hashing}},
  author={Gionis, Aristides and Indyk, Piotr and Motwani, Rajeev and others},
  booktitle={Very Large Data Bases (VLDB)},
  volume={99},
  year={1999}
}

@article{johnson1984extensions,
  title={{Extensions of Lipschitz Mappings into a Hilbert Space}},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary Mathematics},
  volume={26},
  year={1984}
}

@article{kingma2013auto,
  title={{Auto-Encoding Variational Bayes}},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv Preprint arXiv:1312.6114},
  year={2013}
}

@inproceedings{ballard1987modular,
  title={{Modular Learning in Neural Networks}},
  author={Ballard, Dana H},
  booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
  year={1987}
}

@article{makhzani2013k,
  title={{K-Sparse Autoencoders}},
  author={Makhzani, Alireza and Frey, Brendan},
  journal={arXiv Preprint arXiv:1312.5663},
  year={2013}
}

@book{cox2006principles,
  title={{Principles of Statistical Inference}},
  author={Cox, David Roxbee},
  year={2006},
  publisher={Cambridge University Press}
}

@book{lehmann2006testing,
  title={{Testing Statistical Hypotheses}},
  author={Lehmann, Erich L and Romano, Joseph P},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{greenwood1996guide,
  title={{A Guide to Chi-Squared Testing}},
  author={Greenwood, Priscilla E and Nikulin, Michael S},
  volume={280},
  year={1996},
  publisher={John Wiley \& Sons}
}

@article{massey1951kolmogorov,
  title={{The Kolmogorov-Smirnov Test for Goodness of Fit}},
  author={Massey Jr, Frank J},
  journal={Journal of the American statistical Association},
  volume={46},
  year={1951},
  publisher={Taylor \& Francis}
}

@book{rupert2012simultaneous,
  title={{Simultaneous Statistical Inference}},
  author={Rupert Jr, G and others},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@techreport{hochberg1987multiple,
  title={{Multiple Comparison Procedures}},
  author={Hochberg, Yosef},
  year={1987},
  institution={John Wiley \& Sons}
}

@misc{chollet2015keras,
  title={Keras},
  author={Chollet, Fran\c{c}ois and others},
  year={2015},
  howpublished={\url{https://keras.io}},
}

@article{scikitlearn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@online{xiao2017,
  author       = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title        = {{Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms}},
  date         = {2017-08-28},
  year         = {2017},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  eprint       = {cs.LG/1708.07747},
}

@article{netzer2011reading,
  title={{Reading Digits in Natural Images With Unsupervised Feature Learning}},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  year={2011}
}
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


@article{cortes2014domain,
  title={Domain adaptation and sample bias correction theory and algorithm for regression},
  author={Cortes, Corinna and Mohri, Mehryar},
  journal={Theoretical Computer Science},
  volume={519},
  year={2014},
  publisher={Elsevier}
}

@inproceedings{you2019universal,
  title={Universal domain adaptation},
  author={You, Kaichao and Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={2720--2729},
  year={2019}
}

@inproceedings{xu2021joint,
  title={Joint partial optimal transport for open set domain adaptation},
  author={Xu, Renjun and Liu, Pelen and Zhang, Yin and Cai, Fang and Wang, Jindong and Liang, Shuoying and Ying, Heting and Yin, Jianwei},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={2540--2546},
  year={2021}
}

@article{courty2017joint,
  title={Joint distribution optimal transportation for domain adaptation},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Habrard, Amaury and Rakotomamonjy, Alain},
  journal={arXiv preprint arXiv:1705.08848},
  year={2017}
}


@inproceedings{damodaran2018deepjdot,
  title={Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation},
  author={Damodaran, Bharath Bhushan and Kellenberger, Benjamin and Flamary, R{\'e}mi and Tuia, Devis and Courty, Nicolas},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={447--463},
  year={2018}
}

@article{courty2016optimal,
  title={Optimal transport for domain adaptation},
  author={Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis and Rakotomamonjy, Alain},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={39},
  number={9},
  pages={1853--1865},
  year={2016},
  publisher={IEEE}
}

@inproceedings{redko2019optimal,
  title={Optimal transport for multi-source domain adaptation under target shift},
  author={Redko, Ievgen and Courty, Nicolas and Flamary, R{\'e}mi and Tuia, Devis},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={849--858},
  year={2019},
  organization={PMLR}
}


@article{tachet2020domain,
  title={Domain adaptation with conditional distribution matching and generalized label shift},
  author={Tachet des Combes, Remi and Zhao, Han and Wang, Yu-Xiang and Gordon, Geoffrey J},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{byrd2019effect,
  title={What is the Effect of Importance Weighting in Deep Learning?},
  author={Byrd, Jonathon and Lipton, Zachary C},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019}
}

@inproceedings{blair1985problem,
  title={Problem complexity and method efficiency in optimization (AS Nemirovsky and DB Yudin)},
  author={Nemirovski,Arkadi and Yudin,},
  year={1983},
  publisher={Wiley}
}

@book{geer2000empirical,
  title={Empirical Processes in M-estimation},
  author={van de Geer, Sara},
  volume={6},
  year={2000},
  publisher={Cambridge university press}
}

@incollection{van1996weak,
  title={Weak convergence},
  author={van der Vaart, Aad W and Wellner, Jon A},
  booktitle={Weak convergence and empirical processes},
  year={1996},
  publisher={Springer}
}

@inproceedings{rabanser2018failing,
  title={Failing loudly: An empirical study of methods for detecting dataset shift},
  author={Rabanser, Stephan and G{\"u}nnemann, Stephan and Lipton, Zachary C},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2019}
}

@inproceedings{chen2020self,
  title={Self-pu: Self boosted and calibrated positive-unlabeled training},
  author={Chen, Xuxi and Chen, Wuyang and Chen, Tianlong and Yuan, Ye and Gong, Chen and Chen, Kewei and Wang, Zhangyang},
  booktitle={International Conference on Machine Learning},
  pages={1510--1519},
  year={2020},
  organization={PMLR}
}

@misc{tajwar2021true,
    title={No True State-of-the-Art? OOD Detection Methods are Inconsistent across Datasets},
    author={Fahim Tajwar and Ananya Kumar and Sang Michael Xie and Percy Liang},
    year={2021},
    eprint={2109.05554},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}


@inproceedings{recht2019imagenet,
  title={Do imagenet classifiers generalize to imagenet?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  booktitle={International Conference on Machine Learning},
  pages={5389--5400},
  year={2019},
  organization={PMLR}
}


@inproceedings{zhao2019learning,
  title={On learning invariant representations for domain adaptation},
  author={Zhao, Han and Des Combes, Remi Tachet and Zhang, Kun and Gordon, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={7523--7532},
  year={2019},
  organization={PMLR}
}


@inproceedings{prabhu2021sentry,
  title={Sentry: Selective entropy optimization via committee consistency for unsupervised domain adaptation},
  author={Prabhu, Viraj and Khare, Shivam and Kartik, Deeksha and Hoffman, Judy},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={8558--8567},
  year={2021}
}

@article{li2020rethinking,
  title={Rethinking distributional matching based domain adaptation},
  author={Li, Bo and Wang, Yezhen and Che, Tong and Zhang, Shanghang and Zhao, Sicheng and Xu, Pengfei and Zhou, Wei and Bengio, Yoshua and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2006.13352},
  year={2020}
}


@article{tachet_domain_2020,
	title = {Domain {Adaptation} with {Conditional} {Distribution} {Matching} and {Generalized} {Label} {Shift}},
	url = {http://arxiv.org/abs/2003.04475},
	abstract = {Adversarial learning has demonstrated good performance in the unsupervised domain adaptation setting, by learning domain-invariant representations. However, recent work has shown limitations of this approach when label distributions differ between the source and target domains. In this paper, we propose a new assumption, generalized label shift (\$GLS\$), to improve robustness against mismatched label distributions. \$GLS\$ states that, conditioned on the label, there exists a representation of the input that is invariant between the source and target domains. Under \$GLS\$, we provide theoretical guarantees on the transfer performance of any classifier. We also devise necessary and sufficient conditions for \$GLS\$ to hold, by using an estimation of the relative class weights between domains and an appropriate reweighting of samples. Our weight estimation method could be straightforwardly and generically applied in existing domain adaptation (DA) algorithms that learn domain-invariant representations, with small computational overhead. In particular, we modify three DA algorithms, JAN, DANN and CDAN, and evaluate their performance on standard and artificial DA tasks. Our algorithms outperform the base versions, with vast improvements for large label distribution mismatches. Our code is available at https://tinyurl.com/y585xt6j.},
	urldate = {2022-01-03},
	journal = {arXiv:2003.04475 [cs, stat]},
	author = {Tachet, Remi and Zhao, Han and Wang, Yu-Xiang and Gordon, Geoff},
	month = dec,
	year = {2020},
	note = {arXiv: 2003.04475},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	annote = {Comment: Appeared in NeurIPS 2020},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/XUZXICXY/Tachet et al. - 2020 - Domain Adaptation with Conditional Distribution Ma.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/627FIASQ/2003.html:text/html},
}

@article{rakotomamonjy_optimal_2021,
	title = {Optimal {Transport} for {Conditional} {Domain} {Matching} and {Label} {Shift}},
	url = {http://arxiv.org/abs/2006.08161},
	abstract = {We address the problem of unsupervised domain adaptation under the setting of generalized target shift (joint class-conditional and label shifts). For this framework, we theoretically show that, for good generalization, it is necessary to learn a latent representation in which both marginals and class-conditional distributions are aligned across domains. For this sake, we propose a learning problem that minimizes importance weighted loss in the source domain and a Wasserstein distance between weighted marginals. For a proper weighting, we provide an estimator of target label proportion by blending mixture estimation and optimal matching by optimal transport. This estimation comes with theoretical guarantees of correctness under mild assumptions. Our experimental results show that our method performs better on average than competitors across a range domain adaptation problems including {\textbackslash}emph\{digits\},{\textbackslash}emph\{VisDA\} and {\textbackslash}emph\{Office\}. Code for this paper is available at {\textbackslash}url\{https://github.com/arakotom/mars\_domain\_adaptation\}.},
	urldate = {2022-01-04},
	journal = {arXiv:2006.08161 [cs, stat]},
	author = {Rakotomamonjy, Alain and Flamary, RÃ©mi and Gasso, Gilles and Alaya, Mokhtar Z. and Berar, Maxime and Courty, Nicolas},
	month = oct,
	year = {2021},
	note = {arXiv: 2006.08161},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/AFGY72NL/Rakotomamonjy et al. - 2021 - Optimal Transport for Conditional Domain Matching .pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/K8KTR7P2/2006.html:text/html},
}

@article{le_label_2021,
	title = {On {Label} {Shift} in {Domain} {Adaptation} via {Wasserstein} {Distance}},
	url = {http://arxiv.org/abs/2110.15520},
	abstract = {We study the label shift problem between the source and target domains in general domain adaptation (DA) settings. We consider transformations transporting the target to source domains, which enable us to align the source and target examples. Through those transformations, we define the label shift between two domains via optimal transport and develop theory to investigate the properties of DA under various DA settings (e.g., closed-set, partial-set, open-set, and universal settings). Inspired from the developed theory, we propose Label and Data Shift Reduction via Optimal Transport (LDROT) which can mitigate the data and label shifts simultaneously. Finally, we conduct comprehensive experiments to verify our theoretical findings and compare LDROT with state-of-the-art baselines.},
	urldate = {2022-01-04},
	journal = {arXiv:2110.15520 [cs, stat]},
	author = {Le, Trung and Do, Dat and Nguyen, Tuan and Nguyen, Huy and Bui, Hung and Ho, Nhat and Phung, Dinh},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.15520},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
	annote = {Comment: 35 pages, 7 figures, 6 tables},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/UDDEMW3S/Le et al. - 2021 - On Label Shift in Domain Adaptation via Wasserstei.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/NFP5G9GM/2110.html:text/html},
}

@article{gong_domain_nodate,
	title = {Domain {Adaptation} with {Conditional} {Transferable} {Components}},
	abstract = {Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let X and Y denote the features and target, respectively, previous work on domain adaptation mainly considers the covariate shift situation where the distribution of the features P (X) changes across domains while the conditional distribution P (Y {\textbar}X) stays the same. To reduce domain discrepancy, recent methods try to ï¬nd invariant components T (X) that have similar P (T (X)) on different domains by explicitly minimizing a distribution discrepancy measure. However, it is not clear if P (Y {\textbar}T (X)) in different domains is also similar when P (Y {\textbar}X) changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identiï¬able, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where P (X{\textbar}Y ) and P (Y ) both change in a causal system in which Y is the cause for X. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution P (T (X){\textbar}Y ) is invariant after proper location-scale (LS) transformations, and identify how P (Y ) changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method.},
	language = {en},
	author = {Gong, Mingming and Zhang, Kun and Liu, Tongliang and Tao, Dacheng and Glymour, Clark and Scholkopf, Bernhard},
	pages = {10},
	file = {Gong et al. - Domain Adaptation with Conditional Transferable Co.pdf:/Users/sgarg2/Zotero/storage/TD88H4LM/Gong et al. - Domain Adaptation with Conditional Transferable Co.pdf:application/pdf},
}

@article{gong_causal_nodate,
	title = {Causal {Generative} {Domain} {Adaptation} {Networks}},
	abstract = {An essential problem in domain adaptation is to understand and make use of distribution changes across domains. For this purpose, we ï¬rst propose a ï¬exible Generative Domain Adaptation Network (G-DAN) with speciï¬c latent variables to capture changes in the generating process of features across domains. By explicitly modeling the changes, one can even generate data in new domains using the generating process with new values for the latent variables in G-DAN. In practice, the process to generate all features together may involve high-dimensional latent variables, requiring dealing with distributions in high dimensions and making it diï¬cult to learn domain changes from few source domains. Interestingly, by further making use of the causal representation of joint distributions, we then decompose the joint distribution into separate modules, each of which involves diï¬erent low-dimensional latent variables and can be learned separately, leading to a Causal G-DAN (CG-DAN). This improves both statistical and computational eï¬ciency of the learning procedure. Finally, by matching the feature distribution in the target domain, we can recover the target-domain joint distribution and derive the learning machine for the target domain. We demonstrate the eï¬cacy of both G-DAN and CG-DAN in domain generation and cross-domain prediction on both synthetic and real data experiments.},
	language = {en},
	author = {Gong, Mingming and Zhang, Kun and Huang, Biwei and Tao, Dacheng and Batmanghelich, Kayhan},
	pages = {18},
	file = {Gong et al. - Causal Generative Domain Adaptation Networks.pdf:/Users/sgarg2/Zotero/storage/JQ22993R/Gong et al. - Causal Generative Domain Adaptation Networks.pdf:application/pdf},
}

@article{zhang_domain_nodate,
	title = {Domain {Adaptation} under {Target} and {Conditional} {Shift}},
	abstract = {Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios: (1) the marginal PY changes, while the conditional PX{\textbar}Y stays the same (target shift), (2) the marginal PY is ï¬xed, while the conditional PX{\textbar}Y changes with certain constraints (conditional shift), and (3) the marginal PY changes, and the conditional PX{\textbar}Y changes with constraints (generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to ï¬nd the learning machine that works well on test data, and propose to estimate the weights or transformations by reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and realworld data sets demonstrate the eï¬ectiveness of the proposed framework.},
	language = {en},
	author = {Zhang, Kun and Scholkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
	pages = {9},
	file = {Zhang et al. - Domain Adaptation under Target and Conditional Shi.pdf:/Users/sgarg2/Zotero/storage/C2G4XKEB/Zhang et al. - Domain Adaptation under Target and Conditional Shi.pdf:application/pdf},
}

@incollection{ferrari_deep_2018,
	address = {Cham},
	title = {Deep {Domain} {Generalization} via {Conditional} {Invariant} {Adversarial} {Networks}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_38},
	abstract = {Domain generalization aims to learn a classiï¬cation model from multiple source domains and generalize it to unseen target domains. A critical problem in domain generalization involves learning domaininvariant representations. Let X and Y denote the features and the labels, respectively. Under the assumption that the conditional distribution P (Y {\textbar}X) remains unchanged across domains, earlier approaches to domain generalization learned the invariant representation T (X) by minimizing the discrepancy of the marginal distribution P (T (X)). However, such an assumption of stable P (Y {\textbar}X) does not necessarily hold in practice. In addition, the representation learning function T (X) is usually constrained to a simple linear transformation or shallow networks. To address the above two drawbacks, we propose an end-to-end conditional invariant deep domain generalization approach by leveraging deep neural networks for domain-invariant representation learning. The domain-invariance property is guaranteed through a conditional invariant adversarial network that can learn domain-invariant representations w.r.t. the joint distribution P (T (X), Y ) if the target domain data are not severely class unbalanced. We perform various experiments to demonstrate the eï¬ectiveness of the proposed method.},
	language = {en},
	urldate = {2022-06-20},
	booktitle = {Computer {Vision} â {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01267-0_38},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {647--663},
	file = {Li et al. - 2018 - Deep Domain Generalization via Conditional Invaria.pdf:/Users/sgarg2/Zotero/storage/6GYEGJQQ/Li et al. - 2018 - Deep Domain Generalization via Conditional Invaria.pdf:application/pdf},
}

@misc{he_domain_2022,
	title = {Domain {Adaptation} with {Factorizable} {Joint} {Shift}},
	url = {http://arxiv.org/abs/2203.02902},
	abstract = {Existing domain adaptation (DA) usually assumes the domain shift comes from either the covariates or the labels. However, in real-world applications, samples selected from different domains could have biases in both the covariates and the labels. In this paper, we propose a new assumption, Factorizable Joint Shift (FJS), to handle the co-existence of sampling bias in covariates and labels. Although allowing for the shift from both sides, FJS assumes the independence of the bias between the two factors. We provide theoretical and empirical understandings about when FJS degenerates to prior assumptions and when it is necessary. We further propose Joint Importance Aligning (JIA), a discriminative learning objective to obtain joint importance estimators for both supervised and unsupervised domain adaptation. Our method can be seamlessly incorporated with existing domain adaptation algorithms for better importance estimation and weighting on the training data. Experiments on a synthetic dataset demonstrate the advantage of our method.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {He, Hao and Yang, Yuzhe and Wang, Hao},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.02902
arXiv:2203.02902 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 6 pages, 5 figures, ICML 2021 Workshop},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/659QSPTJ/He et al. - 2022 - Domain Adaptation with Factorizable Joint Shift.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/4E5CIP85/2203.html:text/html},
}

@misc{bouvier_hidden_2019,
	title = {Hidden {Covariate} {Shift}: {A} {Minimal} {Assumption} {For} {Domain} {Adaptation}},
	shorttitle = {Hidden {Covariate} {Shift}},
	url = {http://arxiv.org/abs/1907.12299},
	abstract = {Unsupervised Domain Adaptation aims to learn a model on a source domain with labeled data in order to perform well on unlabeled data of a target domain. Current approaches focus on learning {\textbackslash}textit\{Domain Invariant Representations\}. It relies on the assumption that such representations are well-suited for learning the supervised task in the target domain. We rather believe that a better and minimal assumption for performing Domain Adaptation is the {\textbackslash}textit\{Hidden Covariate Shift\} hypothesis. Such approach consists in learning a representation of the data such that the label distribution conditioned on this representation is domain invariant. From the Hidden Covariate Shift assumption, we derive an optimization procedure which learns to match an estimated joint distribution on the target domain and a re-weighted joint distribution on the source domain. The re-weighting is done in the representation space and is learned during the optimization procedure. We show on synthetic data and real world data that our approach deals with both {\textbackslash}textit\{Target Shift\} and {\textbackslash}textit\{Concept Drift\}. We report state-of-the-art performances on Amazon Reviews dataset {\textbackslash}cite\{blitzer2007biographies\} demonstrating the viability of this approach.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Bouvier, Victor and Very, Philippe and Hudelot, CÃ©line and Chastagnol, ClÃ©ment},
	month = jul,
	year = {2019},
	note = {Number: arXiv:1907.12299
arXiv:1907.12299 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/LMUI68CY/Bouvier et al. - 2019 - Hidden Covariate Shift A Minimal Assumption For D.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/UJ6GUIMG/1907.html:text/html},
}

@misc{luo_generalized_2022,
	title = {Generalized {Label} {Shift} {Correction} via {Minimum} {Uncertainty} {Principle}: {Theory} and {Algorithm}},
	shorttitle = {Generalized {Label} {Shift} {Correction} via {Minimum} {Uncertainty} {Principle}},
	url = {http://arxiv.org/abs/2202.13043},
	abstract = {As a fundamental problem in machine learning, dataset shift induces a paradigm to learn and transfer knowledge under changing environment. Previous methods assume the changes are induced by covariate, which is less practical for complex real-world data. We consider the Generalized Label Shift (GLS), which provides an interpretable insight into the learning and transfer of desirable knowledge. Current GLS methods: 1) are not well-connected with the statistical learning theory; 2) usually assume the shifting conditional distributions will be matched with an implicit transformation, but its explicit modeling is unexplored. In this paper, we propose a conditional adaptation framework to deal with these challenges. From the perspective of learning theory, we prove that the generalization error of conditional adaptation is lower than previous covariate adaptation. Following the theoretical results, we propose the minimum uncertainty principle to learn conditional invariant transformation via discrepancy optimization. Specifically, we propose the {\textbackslash}textit\{conditional metric operator\} on Hilbert space to characterize the distinctness of conditional distributions. For finite observations, we prove that the empirical estimation is always well-defined and will converge to underlying truth as sample size increases. The results of extensive experiments demonstrate that the proposed model achieves competitive performance under different GLS scenarios.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Luo, You-Wei and Ren, Chuan-Xian},
	month = feb,
	year = {2022},
	note = {Number: arXiv:2202.13043
arXiv:2202.13043 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: 16 pages},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/Q4GJ775R/Luo and Ren - 2022 - Generalized Label Shift Correction via Minimum Unc.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/MIZQZWIK/2202.html:text/html},
}

@misc{dhouib_connecting_2022,
	title = {Connecting sufficient conditions for domain adaptation: source-guided uncertainty, relaxed divergences and discrepancy localization},
	shorttitle = {Connecting sufficient conditions for domain adaptation},
	url = {http://arxiv.org/abs/2203.05076},
	abstract = {Recent advances in domain adaptation establish that requiring a low risk on the source domain and equal feature marginals degrade the adaptation's performance. At the same time, empirical evidence shows that incorporating an unsupervised target domain term that pushes decision boundaries away from the high-density regions, along with relaxed alignment, improves adaptation. In this paper, we theoretically justify such observations via a new bound on the target risk, and we connect two notions of relaxation for divergence, namely \${\textbackslash}beta-\$relaxed divergences and localization. This connection allows us to incorporate the source domain's categorical structure into the relaxation of the considered divergence, provably resulting in a better handling of the label shift case in particular.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Dhouib, Sofien and Maghsudi, Setareh},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.05076
arXiv:2203.05076 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/TIMNRX8W/Dhouib and Maghsudi - 2022 - Connecting sufficient conditions for domain adapta.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/7DD6IWV4/2203.html:text/html},
}

@misc{tong_adversarial_2022,
	title = {Adversarial {Support} {Alignment}},
	url = {http://arxiv.org/abs/2203.08908},
	abstract = {We study the problem of aligning the supports of distributions. Compared to the existing work on distribution alignment, support alignment does not require the densities to be matched. We propose symmetric support difference as a divergence measure to quantify the mismatch between supports. We show that select discriminators (e.g. discriminator trained for Jensen-Shannon divergence) are able to map support differences as support differences in their one-dimensional output space. Following this result, our method aligns supports by minimizing a symmetrized relaxed optimal transport cost in the discriminator 1D space via an adversarial process. Furthermore, we show that our approach can be viewed as a limit of existing notions of alignment by increasing transportation assignment tolerance. We quantitatively evaluate the method across domain adaptation tasks with shifts in label distributions. Our experiments show that the proposed method is more robust against these shifts than other alignment-based baselines.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Tong, Shangyuan and Garipov, Timur and Zhang, Yang and Chang, Shiyu and Jaakkola, Tommi S.},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2203.08908
arXiv:2203.08908 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Accepted to ICLR 2022},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/T6IP5ENX/Tong et al. - 2022 - Adversarial Support Alignment.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/UUTD6F3G/2203.html:text/html},
}

@article{chen_domain_nodate,
	title = {Domain adaptation under structural causal models},
	abstract = {Domain adaptation (DA) arises as an important problem in statistical machine learning when the source data used to train a model is diï¬erent from the target data used to test the model. Recent advances in DA have mainly been application-driven and have largely relied on the idea of a common subspace for source and target data. To understand the empirical successes and failures of DA methods, we propose a theoretical framework via structural causal models that enables analysis and comparison of the prediction performance of DA methods. This framework also allows us to itemize the assumptions needed for the DA methods to have a low target error. Additionally, with insights from our theory, we propose a new DA method called CIRM that outperforms existing DA methods when both the covariates and label distributions are perturbed in the target data. We complement the theoretical analysis with extensive simulations to show the necessity of the devised assumptions. Reproducible synthetic and real data experiments are also provided to illustrate the strengths and weaknesses of DA methods when parts of the assumptions in our theory are violated.},
	language = {en},
	author = {Chen, Yuansi and Buhlmann, Peter},
	pages = {80},
	file = {Chen and Buhlmann - Domain adaptation under structural causal models.pdf:/Users/sgarg2/Zotero/storage/LEU78XKE/Chen and Buhlmann - Domain adaptation under structural causal models.pdf:application/pdf},
}

@misc{kirchmeyer_mapping_2022,
	title = {Mapping conditional distributions for domain adaptation under generalized target shift},
	url = {http://arxiv.org/abs/2110.15057},
	abstract = {We consider the problem of unsupervised domain adaptation (UDA) between a source and a target domain under conditional and label shift a.k.a Generalized Target Shift (GeTarS). Unlike simpler UDA settings, few works have addressed this challenging problem. Recent approaches learn domain-invariant representations, yet they have practical limitations and rely on strong assumptions that may not hold in practice. In this paper, we explore a novel and general approach to align pretrained representations, which circumvents existing drawbacks. Instead of constraining representation invariance, it learns an optimal transport map, implemented as a NN, which maps source representations onto target ones. Our approach is flexible and scalable, it preserves the problem's structure and it has strong theoretical guarantees under mild assumptions. In particular, our solution is unique, matches conditional distributions across domains, recovers target proportions and explicitly controls the target generalization risk. Through an exhaustive comparison on several datasets, we challenge the state-of-the-art in GeTarS.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Kirchmeyer, Matthieu and Rakotomamonjy, Alain and de Bezenac, Emmanuel and Gallinari, Patrick},
	month = mar,
	year = {2022},
	note = {Number: arXiv:2110.15057
arXiv:2110.15057 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/8ZBKAD83/Kirchmeyer et al. - 2022 - Mapping conditional distributions for domain adapt.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/CW4Y7965/2110.html:text/html},
}

@misc{manders_adversarial_2019,
	title = {Adversarial {Alignment} of {Class} {Prediction} {Uncertainties} for {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1804.04448},
	abstract = {We consider unsupervised domain adaptation: given labelled examples from a source domain and unlabelled examples from a related target domain, the goal is to infer the labels of target examples. Under the assumption that features from pre-trained deep neural networks are transferable across related domains, domain adaptation reduces to aligning source and target domain at class prediction uncertainty level. We tackle this problem by introducing a method based on adversarial learning which forces the label uncertainty predictions on the target domain to be indistinguishable from those on the source domain. Pre-trained deep neural networks are used to generate deep features having high transferability across related domains. We perform an extensive experimental analysis of the proposed method over a wide set of publicly available pre-trained deep neural networks. Results of our experiments on domain adaptation tasks for image classification show that class prediction uncertainty alignment with features extracted from pre-trained deep neural networks provides an efficient, robust and effective method for domain adaptation.},
	urldate = {2022-06-21},
	publisher = {arXiv},
	author = {Manders, Jeroen and van Laarhoven, Twan and Marchiori, Elena},
	month = jan,
	year = {2019},
	note = {Number: arXiv:1804.04448
arXiv:1804.04448 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: To appear in ICPRAM 2019},
	file = {arXiv Fulltext PDF:/Users/sgarg2/Zotero/storage/42USAJH9/Manders et al. - 2019 - Adversarial Alignment of Class Prediction Uncertai.pdf:application/pdf;arXiv.org Snapshot:/Users/sgarg2/Zotero/storage/W75M8IFT/1804.html:text/html},
}

@article{adel_unsupervised_nodate,
	title = {Unsupervised {Domain} {Adaptation} with a {Relaxed} {Covariate} {Shift} {Assumption}},
	abstract = {Domain adaptation addresses learning tasks where training is performed on data from one domain whereas testing is performed on data belonging to a different but related domain. Assumptions about the relationship between the source and target domains should lead to tractable solutions on the one hand, and be realistic on the other hand. Here we propose a generative domain adaptation model that allows for modelling different assumptions about this relationship, among which is a newly introduced assumption that replaces covariate shift with a possibly more realistic assumption without losing tractability due to the efï¬cient variational inference procedure developed. In addition to the ability to model less restrictive relationships between source and target, modelling can be performed without any target labeled data (unsupervised domain adaptation). We also provide a Rademacher complexity bound of the proposed algorithm. We evaluate the model on the Amazon reviews and the CVC pedestrian detection datasets.},
	language = {en},
	author = {Adel, Tameem and Zhao, Han and Wong, Alexander},
	pages = {7},
	file = {Adel et al. - Unsupervised Domain Adaptation with a Relaxed Cova.pdf:/Users/sgarg2/Zotero/storage/WEAZSIVE/Adel et al. - Unsupervised Domain Adaptation with a Relaxed Cova.pdf:application/pdf},
}

@inproceedings{liu_adversarial_2021,
	address = {Montreal, QC, Canada},
	title = {Adversarial {Unsupervised} {Domain} {Adaptation} with {Conditional} and {Label} {Shift}: {Infer}, {Align} and {Iterate}},
	isbn = {978-1-66542-812-5},
	shorttitle = {Adversarial {Unsupervised} {Domain} {Adaptation} with {Conditional} and {Label} {Shift}},
	url = {https://ieeexplore.ieee.org/document/9710205/},
	doi = {10.1109/ICCV48922.2021.01020},
	abstract = {In this work, we propose an adversarial unsupervised domain adaptation (UDA) method under inherent conditional and label shifts, in which we aim to align the distributions w.r.t. both p(x{\textbar}y) and p(y). Since labels are inaccessible in a target domain, conventional adversarial UDA methods assume that p(y) is invariant across domains and rely on aligning p(x) as an alternative to the p(x{\textbar}y) alignment. To address this, we provide a thorough theoretical and empirical analysis of the conventional adversarial UDA methods under both conditional and label shifts, and propose a novel and practical alternative optimization scheme for adversarial UDA. Specifically, we infer the marginal p(y) and align p(x{\textbar}y) iteratively at the training stage, and precisely align the posterior p(y{\textbar}x) at the testing stage. Our experimental results demonstrate its effectiveness on both classification and segmentation UDA and partial UDA.},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Liu, Xiaofeng and Guo, Zhenhua and Li, Site and Xing, Fangxu and You, Jane and Kuo, C.-C. Jay and El Fakhri, Georges and Woo, Jonghye},
	month = oct,
	year = {2021},
	pages = {10347--10356},
	file = {Liu et al. - 2021 - Adversarial Unsupervised Domain Adaptation with Co.pdf:/Users/sgarg2/Zotero/storage/CE2AXPJN/Liu et al. - 2021 - Adversarial Unsupervised Domain Adaptation with Co.pdf:application/pdf},
}

@inproceedings{li_learning_2021,
	address = {Nashville, TN, USA},
	title = {Learning {Invariant} {Representations} and {Risks} for {Semi}-supervised {Domain} {Adaptation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578015/},
	doi = {10.1109/CVPR46437.2021.00116},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Bo and Wang, Yezhen and Zhang, Shanghang and Li, Dongsheng and Keutzer, Kurt and Darrell, Trevor and Zhao, Han},
	month = jun,
	year = {2021},
	pages = {1104--1113},
	file = {Li et al. - 2021 - Learning Invariant Representations and Risks for S.pdf:/Users/sgarg2/Zotero/storage/UNLA7CB2/Li et al. - 2021 - Learning Invariant Representations and Risks for S.pdf:application/pdf},
}

@article{chen_re-weighted_nodate,
	title = {Re-weighted {Adversarial} {Adaptation} {Network} for {Unsupervised} {Domain} {Adaptation}},
	abstract = {Unsupervised Domain Adaptation (UDA) aims to transfer domain knowledge from existing well-deï¬ned tasks to new ones where labels are unavailable. In the real-world applications, as the domain (task) discrepancies are usually uncontrollable, it is signiï¬cantly motivated to match the feature distributions even if the domain discrepancies are disparate. Additionally, as no label is available in the target domain, how to successfully adapt the classiï¬er from the source to the target domain still remains an open question. In this paper, we propose the Re-weighted Adversarial Adaptation Network (RAAN) to reduce the feature distribution divergence and adapt the classiï¬er when domain discrepancies are disparate. Speciï¬cally, to alleviate the need of common supports in matching the feature distribution, we choose to minimize optimal transport (OT) based EarthMover (EM) distance and reformulate it to a minimax objective function. Utilizing this, RAAN can be trained in an end-to-end and adversarial manner. To further adapt the classiï¬er, we propose to match the label distribution and embed it into the adversarial training. Finally, after extensive evaluation of our method using UDA datasets of varying difï¬culty, RAAN achieved the state-of-the-art results and outperformed other methods by a large margin when the domain shifts are disparate.},
	language = {en},
	author = {Chen, Qingchao and Liu, Yang and Wang, Zhaowen and Wassell, Ian and Chetty, Kevin},
	pages = {10},
	file = {Chen et al. - Re-weighted Adversarial Adaptation Network for Uns.pdf:/Users/sgarg2/Zotero/storage/7U5A7HMD/Chen et al. - Re-weighted Adversarial Adaptation Network for Uns.pdf:application/pdf},
}


@inproceedings{garg2022domain,
  title={Domain Adaptation under Open Set Label Shift},
  author={Garg, Saurabh and Balakrishnan, Sivaraman and Lipton, Zachary Chase},
  booktitle={ICML 2022: Workshop on Spurious Correlations, Invariance and Stability},
  year={2022}
}


@inproceedings{li_learning_2021,
	address = {Nashville, TN, USA},
	title = {Learning {Invariant} {Representations} and {Risks} for {Semi}-supervised {Domain} {Adaptation}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578015/},
	doi = {10.1109/CVPR46437.2021.00116},
	language = {en},
	urldate = {2022-06-21},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Bo and Wang, Yezhen and Zhang, Shanghang and Li, Dongsheng and Keutzer, Kurt and Darrell, Trevor and Zhao, Han},
	month = jun,
	year = {2021},
	pages = {1104--1113},
	file = {Li et al. - 2021 - Learning Invariant Representations and Risks for S.pdf:/Users/sgarg2/Zotero/storage/UNLA7CB2/Li et al. - 2021 - Learning Invariant Representations and Risks for S.pdf:application/pdf},
}

@inproceedings{chen2018re,
  title={Re-weighted adversarial adaptation network for unsupervised domain adaptation},
  author={Chen, Qingchao and Liu, Yang and Wang, Zhaowen and Wassell, Ian and Chetty, Kevin},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={7976--7985},
  year={2018}
}

@inproceedings{tan2020class,
  title={Class-imbalanced domain adaptation: An empirical odyssey},
  author={Tan, Shuhan and Peng, Xingchao and Saenko, Kate},
  booktitle={European Conference on Computer Vision},
  pages={585--602},
  year={2020},
  organization={Springer}
}

@inproceedings{yan2017mind,
  title={Mind the class weight bias: Weighted maximum mean discrepancy for unsupervised domain adaptation},
  author={Yan, Hongliang and Ding, Yukang and Li, Peihua and Wang, Qilong and Xu, Yong and Zuo, Wangmeng},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2272--2281},
  year={2017}
}

@article{xu2022controlling,
  title={Controlling directions orthogonal to a classifier},
  author={Xu, Yilun and He, Hao and Shen, Tianxiao and Jaakkola, Tommi},
  journal={arXiv preprint arXiv:2201.11259},
  year={2022}
}

@inproceedings{djolonga2021robustness,
  title={On robustness and transferability of convolutional neural networks},
  author={Djolonga, Josip and Yung, Jessica and Tschannen, Michael and Romijnders, Rob and Beyer, Lucas and Kolesnikov, Alexander and Puigcerver, Joan and Minderer, Matthias and D'Amour, Alexander and Moldovan, Dan and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16458--16468},
  year={2021}
}

@book{quinonero2008dataset,
  title={Dataset shift in machine learning},
  author={Quinonero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D},
  year={2008},
  publisher={Mit Press}
}

@inproceedings{torralba2011unbiased,
  title={Unbiased look at dataset bias},
  author={Torralba, Antonio and Efros, Alexei A},
  booktitle={CVPR 2011},
  pages={1521--1528},
  year={2011},
  organization={IEEE}
}

@article{berthelot2021adamatch,
  title={Adamatch: A unified approach to semi-supervised learning and domain adaptation},
  author={Berthelot, David and Roelofs, Rebecca and Sohn, Kihyuk and Carlini, Nicholas and Kurakin, Alex},
  journal={arXiv preprint arXiv:2106.04732},
  year={2021}
}


@inproceedings{sun2020test,
  title={Test-time training with self-supervision for generalization under distribution shifts},
  author={Sun, Yu and Wang, Xiaolong and Liu, Zhuang and Miller, John and Efros, Alexei and Hardt, Moritz},
  booktitle={International conference on machine learning},
  pages={9229--9248},
  year={2020},
  organization={PMLR}
}

@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}

@article{recht2018cifar,
  title={Do cifar-10 classifiers generalize to cifar-10?},
  author={Recht, Benjamin and Roelofs, Rebecca and Schmidt, Ludwig and Shankar, Vaishaal},
  journal={arXiv preprint arXiv:1806.00451},
  year={2018}
}
@article{torralba2008tinyimages, 
  author = {Antonio Torralba and Rob Fergus and William T. Freeman}, 
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title = {80 Million Tiny Images: A Large Data Set for Nonparametric Object and Scene Recognition}, 
  year = {2008}, 
  volume = {30}, 
  number = {11}, 
  pages = {1958-1970}
}

@inproceedings{christie2018functional,
  title={Functional Map of the World},
  author={Christie, Gordon and Fendley, Neil and Wilson, James and Mukherjee, Ryan},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},  
  year={2018}
}


@article{bandi2018detection,
  title={From detection of individual metastases to classification of lymph node status at the patient level: the CAMELYON17 challenge},
  author={Bandi, Peter and Geessink, Oscar and Manson, Quirine and Van Dijk, Marcory and Balkenhol, Maschenka and Hermsen, Meyke and Bejnordi, Babak Ehteshami and Lee, Byungjae and Paeng, Kyunghyun and Zhong, Aoxiao and others},
  journal={IEEE Transactions on Medical Imaging},
  year={2018},
  publisher={IEEE}
}

@inproceedings{taylor2019rxrx1,
  author = {Taylor, J. and Earnshaw, B. and Mabey, B. and Victors, M. and  Yosinski, J.},
  title = {RxRx1: An Image Set for Cellular Morphological Variation Across Many Experimental Batches.},
  year = {2019},
  booktitle = {International Conference on Learning Representations (ICLR)},
  booksubtitle = {AI for Social Good Workshop},
}

@article{beery2020iwildcam,
    title={The iWildCam 2020 Competition Dataset},
    author={Beery, Sara and Cole, Elijah and Gjoka, Arvi},
    journal={arXiv preprint arXiv:2004.10340},
    year={2020}
}

@article{vapnik1999overview,
  title={An overview of statistical learning theory},
  author={Vapnik, Vladimir N},
  journal={IEEE transactions on neural networks},
  volume={10},
  number={5},
  pages={988--999},
  year={1999},
  publisher={IEEE}
}

@article{long2018conditional,
  title={Conditional adversarial domain adaptation},
  author={Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{lee2013pseudo,
  title={Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks},
  author={Lee, Dong-Hyun and others},
  booktitle={Workshop on challenges in representation learning, ICML},
  volume={3},
  number={2},
  pages={896},
  year={2013}
}

@article{caron2020unsupervised,
  title={Unsupervised learning of visual features by contrasting cluster assignments},
  author={Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9912--9924},
  year={2020}
}

@article{rosenfeld2022domain,
  title={Domain-adjusted regression or: Erm may already learn features sufficient for out-of-distribution generalization},
  author={Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
  journal={arXiv preprint arXiv:2202.06856},
  year={2022}
}

@article{li2016revisiting,
  title={Revisiting batch normalization for practical domain adaptation},
  author={Li, Yanghao and Wang, Naiyan and Shi, Jianping and Liu, Jiaying and Hou, Xiaodi},
  journal={arXiv preprint arXiv:1603.04779},
  year={2016}
}

@inproceedings{sun2016return,
  title={Return of frustratingly easy domain adaptation},
  author={Sun, Baochen and Feng, Jiashi and Saenko, Kate},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{
kumar2022finetuning,
title={Fine-Tuning can Distort Pretrained Features and Underperform Out-of-Distribution},
author={Ananya Kumar and Aditi Raghunathan and Robbie Matthew Jones and Tengyu Ma and Percy Liang},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=UYneFzXSJWh}
}

@article{devries2017improved,
  title={Improved regularization of convolutional neural networks with cutout},
  author={DeVries, Terrance and Taylor, Graham W},
  journal={arXiv preprint arXiv:1708.04552},
  year={2017}
}


@inproceedings{cubuk2020randaugment,
  title={Randaugment: Practical automated data augmentation with a reduced search space},
  author={Cubuk, Ekin D and Zoph, Barret and Shlens, Jonathon and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops},
  pages={702--703},
  year={2020}
}


@article{gidaris2018unsupervised,
  title={Unsupervised representation learning by predicting image rotations},
  author={Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  journal={arXiv preprint arXiv:1803.07728},
  year={2018}
}

@inproceedings{erhan2010does,
  title={Why does unsupervised pre-training help deep learning?},
  author={Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={201--208},
  year={2010},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{he2022masked,
  title={Masked autoencoders are scalable vision learners},
  author={He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={16000--16009},
  year={2022}
}


@inproceedings{he2020momentum,
  title={Momentum contrast for unsupervised visual representation learning},
  author={He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9729--9738},
  year={2020}
}

@article{taori2020measuring,
  title={Measuring robustness to natural distribution shifts in image classification},
  author={Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={18583--18599},
  year={2020}
}

@article{gulrajani2020search,
  title={In search of lost domain generalization},
  author={Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:2007.01434},
  year={2020}
}

@article{wenzel2022assaying,
  title={Assaying Out-Of-Distribution Generalization in Transfer Learning},
  author={Wenzel, Florian and Dittadi, Andrea and Gehler, Peter Vincent and Simon-Gabriel, Carl-Johann and Horn, Max and Zietlow, Dominik and Kernert, David and Russell, Chris and Brox, Thomas and Schiele, Bernt and others},
  journal={arXiv preprint arXiv:2207.09239},
  year={2022}
}


% clip citations
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},
}

@inproceedings{jia2021scaling,
  title={Scaling up visual and vision-language representation learning with noisy text supervision},
  author={Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},
}

@article{pham2021scaling,
  title={Combined scaling for open-vocabulary image classification},
  author={Pham, Hieu and Dai, Zihang and Ghiasi, Golnaz and Kawaguchi, Kenji and Liu, Hanxiao and Yu, Adams Wei and Yu, Jiahui and Chen, Yi-Ting and Luong, Minh-Thang and Wu, Yonghui and others},
  journal={arXiv preprint arXiv: 2111.10050},
  year={2021}
}

@misc{rosanne2022cupl,
  author = {Pratt, Sarah and Liu, Rosanne and Farhadi, Ali},
  
  title = {What does a platypus look like? Generating customized prompts for zero-shot image classification},
  
  year = {2022},
}

@article{gao2021clip,
  title={CLIP-Adapter: Better Vision-Language Models with Feature Adapters},
  author={Gao, Peng and Geng, Shijie and Zhang, Renrui and Ma, Teli and Fang, Rongyao and Zhang, Yongfeng and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2110.04544},
  year={2021}
}

@inproceedings{Cho2022CLIPReward,
  title     = {Fine-grained Image Captioning with CLIP Reward},
  author    = {Jaemin Cho and Seunghyun Yoon and Ajinkya Kale and Franck Dernoncourt and Trung Bui and Mohit Bansal},
  booktitle = {Findings of NAACL},
  year      = {2022}
}

@article{gao2020making,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@inproceedings{modelsoup,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International Conference on Machine Learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}


@article{zhang2021pointclip,
  title={PointCLIP: Point Cloud Understanding by CLIP},
  author={Zhang, Renrui and Guo, Ziyu and Zhang, Wei and Li, Kunchang and Miao, Xupeng and Cui, Bin and Qiao, Yu and Gao, Peng and Li, Hongsheng},
  journal={arXiv preprint arXiv:2112.02413},
  year={2021}
}

@article{zhang2021tip,
  title={Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling},
  author={Zhang, Renrui and Fang, Rongyao and Gao, Peng and Zhang, Wei and Li, Kunchang and Dai, Jifeng and Qiao, Yu and Li, Hongsheng},
  journal={arXiv preprint arXiv:2111.03930},
  year={2021}
}

@article{ilharco2022patching,
  title={Patching open-vocabulary models by interpolating weights},
  author={Ilharco, Gabriel and Wortsman, Mitchell and Gadre, Samir Yitzhak and Song, Shuran and Hajishirzi, Hannaneh and Kornblith, Simon and Farhadi, Ali and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2208.05592},
  year={2022}
}

@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2019},
}



@inproceedings{zhou2022cocoop,
    title={Conditional Prompt Learning for Vision-Language Models},
    author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
    booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2022}
}

@article{zhou2022coop,
    title={Learning to Prompt for Vision-Language Models},
    author={Zhou, Kaiyang and Yang, Jingkang and Loy, Chen Change and Liu, Ziwei},
    journal={International Journal of Computer Vision (IJCV)},
    year={2022}
}

@misc{huang2022upl,
  
  author = {Huang, Tony and Chu, Jack and Wei, Fangyun},

  title = {Unsupervised Prompt Learning for Vision-Language Models},
  
  publisher = {arXiv},
  
  year = {2022},

}

@article{wortsman2021robust,
  title={Robust fine-tuning of zero-shot models},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Kim, Jong Wook and Li, Mike and Kornblith, Simon and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Hajishirzi, Hannaneh and Farhadi, Ali and Namkoong, Hongseok and Schmidt, Ludwig},
  journal={arXiv preprint arXiv:2109.01903},
  year={2021}
}

@misc{kadavath2022know,
  author = {Kadavath, Saurav and Conerly, Tom and Askell, Amanda and Henighan, Tom and Drain, Dawn and Perez, Ethan and Schiefer, Nicholas and Dodds, Zac Hatfield and DasSarma, Nova and Tran-Johnson, Eli and Johnston, Scott and El-Showk, Sheer and Jones, Andy and Elhage, Nelson and Hume, Tristan and Chen, Anna and Bai, Yuntao and Bowman, Sam and Fort, Stanislav and Ganguli, Deep and Hernandez, Danny and Jacobson, Josh and Kernion, Jackson and Kravec, Shauna and Lovitt, Liane and Ndousse, Kamal and Olsson, Catherine and Ringer, Sam and Amodei, Dario and Brown, Tom and Clark, Jack and Joseph, Nicholas and Mann, Ben and McCandlish, Sam and Olah, Chris and Kaplan, Jared},
  
  title = {Language Models (Mostly) Know What They Know},
  
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2207.05221},
  year = {2022},
  
}


@misc{santurkar2022clip,
  
  author = {Santurkar, Shibani and Dubois, Yann and Taori, Rohan and Liang, Percy and Hashimoto, Tatsunori},

  
  title = {Is a Caption Worth a Thousand Images? A Controlled Study for Representation Learning},
  
  publisher = {arXiv},
  
  year = {2022},
}

@misc{ding2022conti,
  
  author = {Ding, Yuxuan and Liu, Lingqiao and Tian, Chunna and Yang, Jingyuan and Ding, Haoxuan},
  
  title = {Don't Stop Learning: Towards Continual Learning for the CLIP Model},
  
  publisher = {arXiv},
  
  year = {2022},
}

@inproceedings{larochelle2008zero,
  title={Zero-data learning of new tasks.},
  author={Larochelle, Hugo and Erhan, Dumitru and Bengio, Yoshua},
  booktitle={Association for the Advancement of Artificial Intelligence (AAAI)},
  year={2008}
}

@article{kuo2022qa,
  author={Kuo, Chia-Chih and Chen, Kuan-Yu},
  journal={IEEE Access}, 
  title={Toward Zero-Shot and Zero-Resource Multilingual Question Answering}, 
  year={2022},
}

@misc{shen2022klite,
  
  author = {Shen, Sheng and Li, Chunyuan and Hu, Xiaowei and Xie, Yujia and Yang, Jianwei and Zhang, Pengchuan and Rohrbach, Anna and Gan, Zhe and Wang, Lijuan and Yuan, Lu and Liu, Ce and Keutzer, Kurt and Darrell, Trevor and Gao, Jianfeng},
  
  title = {K-LITE: Learning Transferable Visual Models with External Knowledge},
  
  publisher = {arXiv},
  
  year = {2022},
}

@inproceedings{zhai2022lit,
  title={Lit: Zero-shot transfer with locked-image text tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2022}
}



@article{jia2022visual,
  title={Visual prompt tuning},
  author={Jia, Menglin and Tang, Luming and Chen, Bor-Chun and Cardie, Claire and Belongie, Serge and Hariharan, Bharath and Lim, Ser-Nam},
  journal={arXiv preprint arXiv:2203.12119},
  year={2022}
}
@inproceedings{bujwid2021large,
      title = {Large-Scale Zero-Shot Image Classification from Rich and Diverse Textual Descriptions},
      author = {Bujwid, Sebastian  and Sullivan, Josephine},
      booktitle = {Proceedings of the Third Workshop on Beyond Vision and LANguage: inTEgrating Real-world kNowledge (LANTERN)},
      year = {2021},
  }

@article{CAO2020107488,
title = {Zero-shot Handwritten Chinese Character Recognition with hierarchical decomposition embedding},
journal = {Pattern Recognition},
year = {2020},
author = {Zhong Cao and Jiang Lu and Sen Cui and Changshui Zhang},
}



@misc{tewel2021i2t,
  
  author = {Tewel, Yoad and Shalev, Yoav and Schwartz, Idan and Wolf, Lior},
  
  title = {ZeroCap: Zero-Shot Image-to-Text Generation for Visual-Semantic Arithmetic},
  
  publisher = {arXiv},
  
  year = {2021},
}

@article{yi2022exploring,
  title={Exploring hierarchical graph representation for large-scale zero-shot image classification},
  author={Yi, Kai and Shen, Xiaoqian and Gou, Yunhao and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2203.01386},
  year={2022}
}

@misc{yu2022esper,
  
  author = {Yu, Youngjae and Chung, Jiwan and Yun, Heeseung and Hessel, Jack and Park, JaeSung and Lu, Ximing and Ammanabrolu, Prithviraj and Zellers, Rowan and Bras, Ronan Le and Kim, Gunhee and Choi, Yejin},
  
  title = {Multimodal Knowledge Alignment with Reinforcement Learning},
  
  publisher = {arXiv},
  
  year = {2022},
}

@inproceedings{mensink2014costa,
  title={Costa: Co-occurrence statistics for zero-shot classification},
  author={Mensink, Thomas and Gavves, Efstratios and Snoek, Cees GM},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  year={2014}
}

@article{TURKOGLU2021112603,
title = {Crop mapping from image time series: Deep learning with multi-scale label hierarchies},
journal = {Remote Sensing of Environment},
year = {2021},
author = {Mehmet Ozgur Turkoglu and Stefano D'Aronco and Gregor Perich and Frank Liebisch and Constantin Streit and Konrad Schindler and Jan Dirk Wegner},
}

@inproceedings{chen2021hsva,
  title={Hsva: Hierarchical semantic-visual adaptation for zero-shot learning},
  author={Chen, Shiming and Xie, GuoSen and Liu, Yang and Peng, Qinmu and Sun, Baigui and Li, Hao and You, Xinge and Shao, Ling},
  journal={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{al2015transfer,
  title={How to transfer? zero-shot object recognition via hierarchical transfer of semantic attributes},
  author={Al-Halah, Ziad and Stiefelhagen, Rainer},
  booktitle={IEEE Winter Conference on Applications of Computer Vision},
  year={2015},
}


@article{DIMITROVSKI20112436,
title = {Hierarchical annotation of medical images},
journal = {Pattern Recognition},
year = {2011},
author = {Ivica Dimitrovski and Dragi Kocev and Suzana Loskovska and SaÅ¡o DÅ¾eroski},
}

@article{chalkidis2020empirical,
  title={An empirical study on large-scale multi-label text classification including few and zero-shot labels},
  author={Chalkidis, Ilias and Fergadiotis, Manos and Kotitsas, Sotiris and Malakasiotis, Prodromos and Aletras, Nikolaos and Androutsopoulos, Ion},
  journal={arXiv preprint arXiv:2010.01653},
  year={2020}
}

@article{Silla2010ASO,
  title={A survey of hierarchical classification across different application domains},
  author={Carlos Nascimento Silla and Alex Alves Freitas},
  journal={Data Mining and Knowledge Discovery},
  year={2010},
}

@article{liu2021improving,
  title={Improving pretrained models for zero-shot multi-label text classification through reinforced label hierarchy reasoning},
  author={Liu, Hui and Zhang, Danqing and Yin, Bing and Zhu, Xiaodan},
  journal={arXiv preprint arXiv:2104.01666},
  year={2021}
}

@article{zeng2022socraticmodels,
    title={Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language},
    author={Andy Zeng and Maria Attarian and Brian Ichter and Krzysztof Choromanski and Adrian Wong and Stefan Welker and Federico Tombari and Aveek Purohit and Michael Ryoo and Vikas Sindhwani and Johnny Lee and Vincent Vanhoucke and Pete Florence},
    journal={arXiv},
    year={2022}
}


@misc{ren2022open,
  
  author = {Ren, Shuhuai and Li, Lei and Ren, Xuancheng and Zhao, Guangxiang and Sun, Xu},
  
  title = {Rethinking the Openness of CLIP},
  
  publisher = {arXiv},
  
  year = {2022},
}


@inproceedings{fergus2010hypo,
author={Fergus, Rob
and Bernal, Hector
and Weiss, Yair
and Torralba, Antonio",
editor="Daniilidis, Kostas
and Maragos, Petros
and Paragios, Nikos},
title={Semantic Label Sharing for Learning with Many Categories},
booktitle={European Conference on Computer Vision (ECCV)},
year={2010},
}

@inproceedings{minderer2021revisiting,
  title={Revisiting the calibration of modern neural networks},
  author={Minderer, Matthias and Djolonga, Josip and Romijnders, Rob and Hubis, Frances and Zhai, Xiaohua and Houlsby, Neil and Tran, Dustin and Lucic, Mario},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021}
}


@article{shen2021much,
  title={How Much Can CLIP Benefit Vision-and-Language Tasks?},
  author={Shen, Sheng and Li, Liunian Harold and Tan, Hao and Bansal, Mohit and Rohrbach, Anna and Chang, Kai-Wei and Yao, Zhewei and Keutzer, Kurt},
  journal={arXiv preprint arXiv:2107.06383},
  year={2021}
}

@article{gadre2022clip,
  title={CLIP on Wheels: Zero-Shot Object Navigation as Object Localization and Exploration},
  author={Gadre, Samir Yitzhak and Wortsman, Mitchell and Ilharco, Gabriel and Schmidt, Ludwig and Song, Shuran},
  journal={arXiv preprint arXiv:2203.10421},
  year={2022}
}

% datasets

@TECHREPORT{cifar100,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    institution = {},
    year = {2009}
}

@inproceedings{food101,
  title = {Food-101 -- Mining Discriminative Components with Random Forests},
  author = {Bossard, Lukas and Guillaumin, Matthieu and Van Gool, Luc},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2014}
}

@article{fruits360,
author = {Horea MureÅan and Mihai Oltean},
title = {Fruit recognition from images using deep learning},
journal = {Acta Universitatis Sapientiae, Informatica},
year = {2018},
}

@inproceedings{fashion1M,
  title={Learning from Massive Noisy Labeled Data for Image Classification},
  author={Xiao, Tong and Xia, Tian and Yang, Yi and Huang, Chang and Wang, Xiaogang},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2015}
}

@misc{fashionmnist,
  author = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title   = {Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  year  = {2017},
}

@misc{lsun,
  
  author = {Yu, Fisher and Seff, Ari and Zhang, Yinda and Song, Shuran and Funkhouser, Thomas and Xiao, Jianxiong},
  
  title = {LSUN: Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop},
  
  publisher = {arXiv},
  
  year = {2015},
}

@inproceedings{office31,
author={Saenko, Kate
and Kulis, Brian
and Fritz, Mario
and Darrell, Trevor},
title={Adapting Visual Category Models to New Domains},
booktitle={European Conference on Computer Vision (ECCV)},
year={2010}
}

@inproceedings{officehome,
  title={Deep hashing network for unsupervised domain adaptation},
  author={Venkateswara, Hemanth and Eusebio, Jose and Chakraborty, Shayok and Panchanathan, Sethuraman},
  booktitle={Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2017}
}

@inproceedings{objectnet,
author={ Barbu, Andrei and  Mayo, David and  Alverio, Julian and  Luo, William and  Wang, Christopher and  Gutfreund, Dan and  Tenenbaum, Josh and  Katz, Boris},
year={2019},
title={Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models},
booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
}

@article{eurosat1,
  title={Eurosat: A novel dataset and deep learning benchmark for land use and land cover classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  journal={IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
  year={2019}
}

@inproceedings{eurosat2,
  title={Introducing EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},
  author={Helber, Patrick and Bischke, Benjamin and Dengel, Andreas and Borth, Damian},
  year={2018}
}

@article{resisc45,
   title={Remote Sensing Image Scene Classification: Benchmark and State of the Art},
   journal={Proceedings of the IEEE},
   author={Cheng, Gong and Han, Junwei and Lu, Xiaoqiang},
   year={2017},
}

@inproceedings{sun397,
author={J. {Xiao} and J. {Hays} and K. A. {Ehinger} and A. {Oliva} and A. {Torralba} },
booktitle={2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
title={SUN database: Large-scale scene recognition from abbey to zoo},
year={2010},}