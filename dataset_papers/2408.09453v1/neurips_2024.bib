#====== RNNs ======#

@inproceedings{arjovsky2016unitary,
  title={Unitary evolution recurrent neural networks},
  author={Arjovsky, Martin and Shah, Amar and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1120--1128},
  year={2016},
  organization={PMLR}
}

#====== Transformers ======#

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{zhang2023efficient,
  title={Efficient long-range transformers: You need to attend more, but not necessarily at every layer},
  author={Zhang, Qingru and Ram, Dhananjay and Hawkins, Cole and Zha, Sheng and Zhao, Tuo},
  journal={arXiv preprint arXiv:2310.12442},
  year={2023}
}

#====== Convolutional Models ======#



@article{bai2018empirical,
  title={An empirical evaluation of generic convolutional and recurrent networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1803.01271},
  year={2018}
}

@article{oord2016wavenet,
  title={Wavenet: A generative model for raw audio},
  author={Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1609.03499},
  year={2016}
}

@article{bai2018trellis,
  title={Trellis networks for sequence modeling},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={arXiv preprint arXiv:1810.06682},
  year={2018}
}

@article{romero2021ckconv,
  title={Ckconv: Continuous kernel convolution for sequential data},
  author={Romero, David W and Kuzina, Anna and Bekkers, Erik J and Tomczak, Jakub M and Hoogendoorn, Mark},
  journal={arXiv preprint arXiv:2102.02611},
  year={2021}
}

@article{fu2023simple,
  title={Simple hardware-efficient long convolutions for sequence modeling},
  author={Fu, Daniel Y and Epstein, Elliot L and Nguyen, Eric and Thomas, Armin W and Zhang, Michael and Dao, Tri and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2302.06646},
  year={2023}
}

@article{li2022makes,
  title={What Makes Convolutional Models Great on Long Sequence Modeling?},
  author={Li, Yuhong and Cai, Tianle and Zhang, Yi and Chen, Deming and Dey, Debadeepta},
  journal={arXiv preprint arXiv:2210.09298},
  year={2022}
}

@article{knigge2023modelling,
  title={Modelling Long Range Dependencies in ND: From Task-Specific to a General Purpose CNN},
  author={Knigge, David M and Romero, David W and Gu, Albert and Gavves, Efstratios and Bekkers, Erik J and Tomczak, Jakub M and Hoogendoorn, Mark and Sonke, Jan-Jakob},
  journal={arXiv preprint arXiv:2301.10540},
  year={2023}
}

@article{poli2023hyena,
  title={Hyena hierarchy: Towards larger convolutional language models},
  author={Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2302.10866},
  year={2023}
}

@inproceedings{shi2023sequence,
  title={Sequence modeling with multiresolution convolutional memory},
  author={Shi, Jiaxin and Wang, Ke Alexander and Fox, Emily},
  booktitle={International Conference on Machine Learning},
  pages={31312--31327},
  year={2023},
  organization={PMLR}
}

#====== State Space Models ======#

@article{gu2020hippo,
  title={Hippo: Recurrent memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1474--1487},
  year={2020}
}

@article{gu2022train,
  title={How to train your hippo: State space models with generalized orthogonal basis projections},
  author={Gu, Albert and Johnson, Isys and Timalsina, Aman and Rudra, Atri and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2206.12037},
  year={2022}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@article{gu2022parameterization,
  title={On the parameterization and initialization of diagonal state space models},
  author={Gu, Albert and Goel, Karan and Gupta, Ankit and R{\'e}, Christopher},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={35971--35983},
  year={2022}
}

@article{smith2022simplified,
  title={Simplified state space layers for sequence modeling},
  author={Smith, Jimmy TH and Warrington, Andrew and Linderman, Scott W},
  journal={arXiv preprint arXiv:2208.04933},
  year={2022}
}

@article{hasani2022liquid,
  title={Liquid structural state-space models},
  author={Hasani, Ramin and Lechner, Mathias and Wang, Tsun-Hsuan and Chahine, Makram and Amini, Alexander and Rus, Daniela},
  journal={arXiv preprint arXiv:2209.12951},
  year={2022}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

% CNNs - Vision

@inproceedings{liu2022convnet,
  title={A convnet for the 2020s},
  author={Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11976--11986},
  year={2022}
}

@inproceedings{ding2022scaling,
  title={Scaling up your kernels to 31x31: Revisiting large kernel design in cnns},
  author={Ding, Xiaohan and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11963--11975},
  year={2022}
}

@article{liu2022more,
  title={More convnets in the 2020s: Scaling up kernels beyond 51x51 using sparsity},
  author={Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Chen, Xuxi and Xiao, Qiao and Wu, Boqian and K{\"a}rkk{\"a}inen, Tommi and Pechenizkiy, Mykola and Mocanu, Decebal and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2207.03620},
  year={2022}
}

@article{ding2023unireplknet,
  title={Unireplknet: A universal perception large-kernel convnet for audio, video, point cloud, time-series and image recognition},
  author={Ding, Xiaohan and Zhang, Yiyuan and Ge, Yixiao and Zhao, Sijie and Song, Lin and Yue, Xiangyu and Shan, Ying},
  journal={arXiv preprint arXiv:2311.15599},
  year={2023}
}

#====== Structural Reparameterization ======#

@inproceedings{ding2022repmlpnet,
  title={Repmlpnet: Hierarchical vision mlp with re-parameterized locality},
  author={Ding, Xiaohan and Chen, Honghao and Zhang, Xiangyu and Han, Jungong and Ding, Guiguang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={578--587},
  year={2022}
}

@inproceedings{ding2019acnet,
  title={Acnet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks},
  author={Ding, Xiaohan and Guo, Yuchen and Ding, Guiguang and Han, Jungong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1911--1920},
  year={2019}
}



% Other

@article{tay2020long,
  title={Long range arena: A benchmark for efficient transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}

@article{fu2023flashfftconv,
  title={FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores},
  author={Fu, Daniel Y and Kumbong, Hermann and Nguyen, Eric and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2311.05908},
  year={2023}
}

@article{massaroli2023laughing,
  title={Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions},
  author={Massaroli, Stefano and Poli, Michael and Fu, Daniel Y and Kumbong, Hermann and Parnichkun, Rom N and Timalsina, Aman and Romero, David W and McIntyre, Quinn and Chen, Beidi and Rudra, Atri and others},
  journal={arXiv preprint arXiv:2310.18780},
  year={2023}
}

@inproceedings{koutnik2014clockwork,
  title={A clockwork rnn},
  author={Koutnik, Jan and Greff, Klaus and Gomez, Faustino and Schmidhuber, Juergen},
  booktitle={International conference on machine learning},
  pages={1863--1871},
  year={2014},
  organization={PMLR}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{mallat1989theory,
  title={A theory for multiresolution signal decomposition: the wavelet representation},
  author={Mallat, Stephane G},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={11},
  number={7},
  pages={674--693},
  year={1989},
  publisher={Ieee}
}

@inproceedings{ding2021repvgg,
  title={Repvgg: Making vgg-style convnets great again},
  author={Ding, Xiaohan and Zhang, Xiangyu and Ma, Ningning and Han, Jungong and Ding, Guiguang and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13733--13742},
  year={2021}
}

@inproceedings{ding2021resrep,
  title={Resrep: Lossless cnn pruning via decoupling remembering and forgetting},
  author={Ding, Xiaohan and Hao, Tianxiang and Tan, Jianchao and Liu, Ji and Han, Jungong and Guo, Yuchen and Ding, Guiguang},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={4510--4520},
  year={2021}
}

@inproceedings{hu2022online,
  title={Online convolutional re-parameterization},
  author={Hu, Mu and Feng, Junyi and Hua, Jiashen and Lai, Baisheng and Huang, Jianqiang and Gong, Xiaojin and Hua, Xian-Sheng},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={568--577},
  year={2022}
}



@article{ma2022mega,
  title={Mega: moving average equipped gated attention},
  author={Ma, Xuezhe and Zhou, Chunting and Kong, Xiang and He, Junxian and Gui, Liangke and Neubig, Graham and May, Jonathan and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2209.10655},
  year={2022}
}

@article{de2020batch,
  title={Batch normalization biases residual blocks towards the identity function in deep networks},
  author={De, Soham and Smith, Sam},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={19964--19975},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{warden2018speech,
  title={Speech commands: A dataset for limited-vocabulary speech recognition},
  author={Warden, Pete},
  journal={arXiv preprint arXiv:1804.03209},
  year={2018}
}

@article{russakovsky2015imagenet,
  title={Imagenet large scale visual recognition challenge},
  author={Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and others},
  journal={International journal of computer vision},
  volume={115},
  pages={211--252},
  year={2015},
  publisher={Springer}
}

@article{ma2024megalodon,
  title={Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length},
  author={Ma, Xuezhe and Yang, Xiaomeng and Xiong, Wenhan and Chen, Beidi and Yu, Lili and Zhang, Hao and May, Jonathan and Zettlemoyer, Luke and Levy, Omer and Zhou, Chunting},
  journal={arXiv preprint arXiv:2404.08801},
  year={2024}
}

@article{zuo2022efficient,
  title={Efficient long sequence modeling via state space augmented transformer},
  author={Zuo, Simiao and Liu, Xiaodong and Jiao, Jian and Charles, Denis and Manavoglu, Eren and Zhao, Tuo and Gao, Jianfeng},
  journal={arXiv preprint arXiv:2212.08136},
  year={2022}
}

@article{fathi2023block,
  title={Block-state transformer},
  author={Fathi, Mahan and Pilault, Jonathan and Bacon, Pierre-Luc and Pal, Christopher and Firat, Orhan and Goroshin, Ross},
  journal={arXiv preprint arXiv:2306.09539},
  year={2023}
}

@article{amos2023never,
  title={Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors},
  author={Amos, Ido and Berant, Jonathan and Gupta, Ankit},
  journal={arXiv preprint arXiv:2310.02980},
  year={2023}
}

@article{romero2021flexconv,
  title={Flexconv: Continuous kernel convolutions with differentiable kernel sizes},
  author={Romero, David W and Bruintjes, Robert-Jan and Tomczak, Jakub M and Bekkers, Erik J and Hoogendoorn, Mark and van Gemert, Jan C},
  journal={arXiv preprint arXiv:2110.08059},
  year={2021}
}

@inproceedings{gu2020improving,
  title={Improving the gating mechanism of recurrent neural networks},
  author={Gu, Albert and Gulcehre, Caglar and Paine, Thomas and Hoffman, Matt and Pascanu, Razvan},
  booktitle={International Conference on Machine Learning},
  pages={3800--3809},
  year={2020},
  organization={PMLR}
}

@inproceedings{trinh2018learning,
  title={Learning longer-term dependencies in rnns with auxiliary losses},
  author={Trinh, Trieu and Dai, Andrew and Luong, Thang and Le, Quoc},
  booktitle={International Conference on Machine Learning},
  pages={4965--4974},
  year={2018},
  organization={PMLR}
}

@article{erichson2020lipschitz,
  title={Lipschitz recurrent neural networks},
  author={Erichson, N Benjamin and Azencot, Omri and Queiruga, Alejandro and Hodgkinson, Liam and Mahoney, Michael W},
  journal={arXiv preprint arXiv:2006.12070},
  year={2020}
}

@inproceedings{nonaka2021depth,
  title={In-depth benchmarking of deep neural network architectures for ECG diagnosis},
  author={Nonaka, Naoki and Seita, Jun},
  booktitle={Machine Learning for Healthcare Conference},
  pages={414--439},
  year={2021},
  organization={PMLR}
}

@article{chetlur2014cudnn,
  title={cudnn: Efficient primitives for deep learning},
  author={Chetlur, Sharan and Woolley, Cliff and Vandermersch, Philippe and Cohen, Jonathan and Tran, John and Catanzaro, Bryan and Shelhamer, Evan},
  journal={arXiv preprint arXiv:1410.0759},
  year={2014}
}
