\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{FKSLX21}

\bibitem[AMS07]{antos2007fitted}
Andr{\'a}s Antos, R{\'e}mi Munos, and Csaba Szepesv{\'a}ri.
\newblock Fitted {Q}-iteration in continuous action-space {M}{D}{P}s.
\newblock 2007.

\bibitem[ASM08]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with {B}ellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock {\em Machine Learning}, 71(1):89--129, 2008.

\bibitem[ASN20]{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  104--114. PMLR, 2020.

\bibitem[BB96]{bradtke1996linear}
Steven~J Bradtke and Andrew~G Barto.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock {\em Machine learning}, 22(1-3):33--57, 1996.

\bibitem[Ber95a]{Bertsekas_dyn1}
D.~P. Bertsekas.
\newblock {\em Dynamic programming and stochastic control}, volume~1.
\newblock Athena Scientific, Belmont, MA, 1995.

\bibitem[Ber95b]{Bertsekas_dyn2}
D.P. Bertsekas.
\newblock {\em Dynamic programming and stochastic control}, volume~2.
\newblock Athena Scientific, Belmont, MA, 1995.

\bibitem[BGB20]{buckman2020importance}
Jacob Buckman, Carles Gelada, and Marc~G Bellemare.
\newblock The importance of pessimism in fixed-dataset policy optimization.
\newblock {\em arXiv preprint arXiv:2009.06799}, 2020.

\bibitem[BLL{\etalchar{+}}11]{beygelzimer2011contextual}
Alina Beygelzimer, John Langford, Lihong Li, Lev Reyzin, and Robert Schapire.
\newblock Contextual bandit algorithms with supervised learning guarantees.
\newblock In {\em Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 19--26. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[BT96]{bertsekas1996neuro}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock {\em Neuro-dynamic programming}.
\newblock Athena Scientific, 1996.

\bibitem[CJ19]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1042--1051, 2019.

\bibitem[CQ22]{chen2022well}
Xiaohong Chen and Zhengling Qi.
\newblock On well-posedness and minimax optimal rates of nonparametric
  q-function estimation in off-policy evaluation.
\newblock {\em arXiv preprint arXiv:2201.06169}, 2022.

\bibitem[DJL21]{duan2021risk}
Yaqi Duan, Chi Jin, and Zhiyuan Li.
\newblock Risk bounds and rademacher complexity in batch reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2103.13883}, 2021.

\bibitem[DW20]{duan2020minimax}
Yaqi Duan and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock {\em arXiv preprint arXiv:2002.09516}, 2020.

\bibitem[Eva10]{evans2010partial}
Lawrence~C Evans.
\newblock {\em Partial differential equations}, volume~19.
\newblock American Mathematical Soc., 2010.

\bibitem[FCG18]{farajtabar2018more}
Mehrdad Farajtabar, Yinlam Chow, and Mohammad Ghavamzadeh.
\newblock More robust doubly robust off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  1447--1456. PMLR, 2018.

\bibitem[FGSM16]{farahmand2016regularized}
Amir-massoud Farahmand, Mohammad Ghavamzadeh, Csaba Szepesv{\'a}ri, and Shie
  Mannor.
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock {\em The Journal of Machine Learning Research}, 17(1):4809--4874,
  2016.

\bibitem[FKSLX21]{foster2021offline}
Dylan~J. Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu.
\newblock Offline reinforcement learning: Fundamental barriers for value
  function approximation, 2021.

\bibitem[Fle84]{fletcher1984computational}
Clive~AJ Fletcher.
\newblock Computational {G}alerkin methods.
\newblock In {\em Computational {G}alerkin methods}, pages 72--85. Springer,
  1984.

\bibitem[FRTL20]{feng2020accountable}
Yihao Feng, Tongzheng Ren, Ziyang Tang, and Qiang Liu.
\newblock Accountable off-policy evaluation with kernel bellman statistics.
\newblock In {\em International Conference on Machine Learning}, pages
  3102--3111. PMLR, 2020.

\bibitem[FSM10]{farahmand2010error}
Amir-massoud Farahmand, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Error propagation for approximate policy and value iteration.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2010.

\bibitem[Gal15]{galerkin1915series}
Boris~Grigoryevich Galerkin.
\newblock Series solution of some problems of elastic equilibrium of rods and
  plates.
\newblock {\em Vestnik inzhenerov i tekhnikov}, 19(7):897--908, 1915.

\bibitem[Haz21]{hazan2021introduction}
Elad Hazan.
\newblock Introduction to online convex optimization, 2021.

\bibitem[HJD{\etalchar{+}}21]{hao2021bootstrapping}
Botao Hao, Xiang Ji, Yaqi Duan, Hao Lu, Csaba Szepesv{\'a}ri, and Mengdi Wang.
\newblock Bootstrapping statistical inference for off-policy evaluation.
\newblock {\em arXiv preprint arXiv:2102.03607}, 2021.

\bibitem[JGS{\etalchar{+}}19]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock {\em arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[JH20]{jiang2020minimax}
Nan Jiang and Jiawei Huang.
\newblock Minimax value interval for off-policy evaluation and policy
  optimization.
\newblock {\em arXiv preprint arXiv:2002.02081}, 2020.

\bibitem[JKA{\etalchar{+}}17]{jiang17contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E.
  Schapire.
\newblock Contextual decision processes with low {B}ellman rank are
  {PAC}-learnable.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em International
  Conference on Machine Learning (ICML)}, volume~70 of {\em Proceedings of
  Machine Learning Research}, pages 1704--1713, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[JL16]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem[JYW21]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In {\em International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem[K{\etalchar{+}}03]{kakade2003sample}
Sham~Machandranath Kakade et~al.
\newblock {\em On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[KFTL19]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock {\em arXiv preprint arXiv:1906.00949}, 2019.

\bibitem[KHSL21]{kumar2021should}
Aviral Kumar, Joey Hong, Anikait Singh, and Sergey Levine.
\newblock Should i run offline reinforcement learning or behavioral cloning?
\newblock In {\em Deep RL Workshop NeurIPS 2021}, 2021.

\bibitem[KRNJ20]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2005.05951}, 2020.

\bibitem[KU19]{kallus2019efficiently}
Nathan Kallus and Masatoshi Uehara.
\newblock Efficiently breaking the curse of horizon in off-policy evaluation
  with double reinforcement learning.
\newblock {\em arXiv preprint arXiv:1909.05850}, 2019.

\bibitem[LLTZ18]{liu2018breaking}
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5356--5366, 2018.

\bibitem[LP03]{lagoudakis2003least}
Michail~G Lagoudakis and Ronald Parr.
\newblock Least-squares policy iteration.
\newblock {\em Journal of machine learning research}, 4(Dec):1107--1149, 2003.

\bibitem[LSAB20]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch reinforcement learning without great exploration.
\newblock {\em arXiv preprint arXiv:2007.08202}, 2020.

\bibitem[LTDC19]{laroche2019safe}
Romain Laroche, Paul Trichelair, and Remi~Tachet Des~Combes.
\newblock Safe policy improvement with baseline bootstrapping.
\newblock In {\em International Conference on Machine Learning}, pages
  3652--3661. PMLR, 2019.

\bibitem[LTND21]{lee2021model}
Jonathan~N Lee, George Tucker, Ofir Nachum, and Bo~Dai.
\newblock Model selection in batch policy optimization.
\newblock {\em arXiv preprint arXiv:2112.12320}, 2021.

\bibitem[MS08]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock {\em Journal of Machine Learning Research}, 9(May):815--857, 2008.

\bibitem[Mun03]{munos2003error}
R{\'e}mi Munos.
\newblock Error bounds for approximate policy iteration.
\newblock In {\em ICML}, volume~3, pages 560--567, 2003.

\bibitem[Mun05]{munos2005error}
R{\'e}mi Munos.
\newblock Error bounds for approximate value iteration.
\newblock In {\em AAAI Conference on Artificial Intelligence (AAAI)}, 2005.

\bibitem[ND20]{nachum2020reinforcement}
Ofir Nachum and Bo~Dai.
\newblock Reinforcement learning via {F}enchel-{R}ockafellar duality.
\newblock {\em arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[NDGL20]{nair2020accelerating}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock {\em arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[NDK{\etalchar{+}}19]{nachum2019algaedice}
Ofir Nachum, Bo~Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale
  Schuurmans.
\newblock Algaedice: Policy gradient from arbitrary experience.
\newblock {\em arXiv preprint arXiv:1912.02074}, 2019.

\bibitem[Pre00]{precup2000eligibility}
Doina Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock {\em Computer Science Department Faculty Publication Series},
  page~80, 2000.

\bibitem[Put94]{puterman1994markov}
Martin~L. Puterman.
\newblock {\em Markov Decision Processes: Discrete Stochastic Dynamic
  Programming}.
\newblock John Wiley \& Sons, Inc., New York, NY, USA, 1994.

\bibitem[Rep17]{repin2017one}
Sergey Repin.
\newblock One hundred years of the {G}alerkin method.
\newblock {\em Computational Methods in Applied Mathematics}, 17(3):351--357,
  2017.

\bibitem[RZM{\etalchar{+}}21]{rashidinejad2021bridging}
Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell.
\newblock Bridging offline reinforcement learning and imitation learning: A
  tale of pessimism.
\newblock {\em arXiv preprint arXiv:2103.12021}, 2021.

\bibitem[SB18]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT Press, 2018.

\bibitem[SSB{\etalchar{+}}20]{siegel2020keep}
Noah~Y Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin
  Riedmiller.
\newblock Keep doing what worked: Behavioral modelling priors for offline
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[TB16]{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2139--2148, 2016.

\bibitem[TFL{\etalchar{+}}19]{tang2019doubly}
Ziyang Tang, Yihao Feng, Lihong Li, Dengyong Zhou, and Qiang Liu.
\newblock Doubly robust bias reduction in infinite horizon off-policy
  estimation.
\newblock {\em arXiv preprint arXiv:1910.07186}, 2019.

\bibitem[UHJ20]{uehara2020minimax}
Masatoshi Uehara, Jiawei Huang, and Nan Jiang.
\newblock Minimax weight and q-function learning for off-policy evaluation.
\newblock In {\em International Conference on Machine Learning}, pages
  9659--9668. PMLR, 2020.

\bibitem[UIJ{\etalchar{+}}21]{uehara2021finite}
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and
  Tengyang Xie.
\newblock Finite sample analysis of minimax offline reinforcement learning:
  Completeness, fast rates and first-order efficiency.
\newblock {\em arXiv preprint arXiv:2102.02981}, 2021.

\bibitem[US21]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage, 2021.

\bibitem[VJY21]{voloshin2021minimax}
Cameron Voloshin, Nan Jiang, and Yisong Yue.
\newblock Minimax model learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1612--1620. PMLR, 2021.

\bibitem[Wai19]{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[WFK20]{wang2020statistical}
Ruosong Wang, Dean~P Foster, and Sham~M Kakade.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock {\em arXiv preprint arXiv:2010.11895}, 2020.

\bibitem[WN{\.Z}{\etalchar{+}}20]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad {\.Z}o{\l}na, Jost~Tobias Springenberg,
  Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock {\em arXiv preprint arXiv:2006.15134}, 2020.

\bibitem[WTN19]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[XCJ{\etalchar{+}}21]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock {B}ellman-consistent pessimism for offline reinforcement learning.
\newblock {\em arXiv preprint arXiv:2106.06926}, 2021.

\bibitem[XJ20a]{xie2020batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock {\em arXiv preprint arXiv:2008.04990}, 2020.

\bibitem[XJ20b]{xie2020Q}
Tengyang Xie and Nan Jiang.
\newblock Q* approximation schemes for batch reinforcement learning: A
  theoretical comparison.
\newblock volume 124 of {\em Proceedings of Machine Learning Research}, pages
  550--559, Virtual, 03--06 Aug 2020. PMLR.

\bibitem[XMW19]{xie2019towards}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9668--9678, 2019.

\bibitem[YB10]{YuBer10}
H.~Yu and D.~P. Bertsekas.
\newblock Error bounds for approximations from projected linear equations.
\newblock {\em Mathematics of Operations Research}, 35(2):306--329, 2010.

\bibitem[YBW20]{yin2020near}
Ming Yin, Yu~Bai, and Yu-Xiang Wang.
\newblock Near optimal provable uniform convergence in off-policy evaluation
  for reinforcement learning.
\newblock {\em arXiv preprint arXiv:2007.03760}, 2020.

\bibitem[YND{\etalchar{+}}20]{yang2020off}
Mengjiao Yang, Ofir Nachum, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Off-policy evaluation via the regularized lagrangian.
\newblock {\em arXiv preprint arXiv:2007.03438}, 2020.

\bibitem[YQCC21]{yang2021pessimistic}
Chao-Han~Huck Yang, Zhengling Qi, Yifan Cui, and Pin-Yu Chen.
\newblock Pessimistic model selection for offline deep reinforcement learning,
  2021.

\bibitem[YTY{\etalchar{+}}20]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine,
  Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock {\em arXiv preprint arXiv:2005.13239}, 2020.

\bibitem[YW20]{yin2020asymptotically}
Ming Yin and Yu-Xiang Wang.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3948--3958. PMLR, 2020.

\bibitem[YW21]{yin2021towards}
Ming Yin and Yu-Xiang Wang.
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock {\em arXiv preprint arXiv:2110.08695}, 2021.

\bibitem[YWDW]{yinnear}
Ming Yin, Yu-Xiang Wang, Yaqi Duan, and Mengdi Wang.
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.

\bibitem[Zan20]{zanette2020exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: Batch rl
  can be exponentially harder than online {R}{L}.
\newblock {\em arXiv preprint arXiv:2012.08005}, 2020.

\bibitem[ZDLS20]{zhang2020gendice}
Ruiyi Zhang, Bo~Dai, Lihong Li, and Dale Schuurmans.
\newblock Gendice: Generalized offline estimation of stationary values.
\newblock {\em arXiv preprint arXiv:2002.09072}, 2020.

\bibitem[ZHH{\etalchar{+}}22]{zhan2022offline}
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason~D Lee.
\newblock Offline reinforcement learning with realizability and single-policy
  concentrability.
\newblock {\em arXiv preprint arXiv:2202.04634}, 2022.

\bibitem[ZJZ21]{zhang2021optimal}
Zihan Zhang, Xiangyang Ji, and Yuan Zhou.
\newblock Almost optimal batch-regret tradeoff for batch linear contextual
  bandits, 2021.

\bibitem[ZLW20]{zhang2020gradientdice}
Shangtong Zhang, Bo~Liu, and Shimon Whiteson.
\newblock Gradientdice: Rethinking generalized offline estimation of stationary
  values.
\newblock In {\em International Conference on Machine Learning}, pages
  11194--11203. PMLR, 2020.

\bibitem[ZSU{\etalchar{+}}22]{zhang2022efficient}
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen
  Sun.
\newblock Efficient reinforcement learning in block {M}{D}{P}s: {A} model-free
  representation learning approach, 2022.

\bibitem[ZWB21]{zanette2021provable}
Andrea Zanette, Martin~J Wainwright, and Emma Brunskill.
\newblock Provable benefits of actor-critic methods for offline reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2108.08812}, 2021.

\end{thebibliography}
