@article{xie2020batch,
  title={Batch Value-function Approximation with Only Realizability},
  author={Xie, Tengyang and Jiang, Nan},
  journal={arXiv preprint arXiv:2008.04990},
  year={2020}
}

@inproceedings{hsu2012random,
  title={Random design analysis of ridge regression},
  author={Hsu, Daniel and Kakade, Sham M and Zhang, Tong},
  booktitle={Conference on learning theory},
  pages={9--1},
  year={2012}
}


@article{yin2020near,
  title={Near optimal provable uniform convergence in off-policy evaluation for reinforcement learning},
  author={Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2007.03760},
  year={2020}
}


@article{chen2019information,
  title={Information-theoretic considerations in batch reinforcement learning},
  author={Chen, Jinglin and Jiang, Nan},
  journal={arXiv preprint arXiv:1905.00360},
  year={2019}
}

@article{scherrer2010should,
  title={Should one compute the temporal difference fix point or minimize the bellman residual? {T}he unified oblique projection view},
  author={Scherrer, Bruno},
  journal={arXiv preprint arXiv:1011.4362},
  year={2010}
}

@inproceedings{geist2017bellman,
  title={Is the {B}ellman residual a bad proxy?},
  author={Geist, Matthieu and Piot, Bilal and Pietquin, Olivier},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3205--3214},
  year={2017}
}

@inproceedings{scherrer2014approximate,
  title={Approximate policy iteration schemes: {A} comparison},
  author={Scherrer, Bruno},
  booktitle={International Conference on Machine Learning},
  pages={1314--1322},
  year={2014}
}

@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  volume={2},
  pages={267--274},
  year={2002}
}

@article{kumar2020conservative,
  title={Conservative {Q}-Learning for Offline Reinforcement Learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@article{yu2020mopo,
  title={{MOPO: M}odel-based Offline Policy Optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={arXiv preprint arXiv:2005.13239},
  year={2020}
}

@article{luo2018algorithmic,
  title={Algorithmic framework for model-based deep reinforcement learning with theoretical guarantees},
  author={Luo, Yuping and Xu, Huazhe and Li, Yuanzhi and Tian, Yuandong and Darrell, Trevor and Ma, Tengyu},
  journal={arXiv preprint arXiv:1807.03858},
  year={2018}
}

@article{xu2020error,
  title={Error Bounds of Imitating Policies and Environments},
  author={Xu, Tian and Li, Ziniu and Yu, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@techreport{agarwal2019reinforcement,
  title={{Reinforcement learning: Theory and algorithms}},
  author={Agarwal, Alekh and Jiang, Nan and Kakade, Sham M},
  year={2019},
  institution={Technical Report, Department of Computer Science, University of Washington}
}

@article{pananjady2020instance,
  title={Instance-dependent $\ell_\infty$-bounds for policy evaluation in tabular reinforcement learning},
  author={Pananjady, Ashwin and Wainwright, Martin J},
  journal={IEEE Transactions on Information Theory},
  year={2020},
  publisher={IEEE}
}

@article{okamoto1959some,
  title={Some inequalities relating to the partial sum of binomial probabilities},
  author={Okamoto, Masashi},
  journal={Annals of the institute of Statistical Mathematics},
  volume={10},
  number={1},
  pages={29--35},
  year={1959},
  publisher={Springer}
}

@book{anthony2009neural,
  title={Neural network learning: {T}heoretical foundations},
  author={Anthony, Martin and Bartlett, Peter L},
  year={2009},
  publisher={cambridge university press}
}

@article{rajaraman2020toward,
  title={Toward the Fundamental Limits of Imitation Learning},
  author={Rajaraman, Nived and Yang, Lin F and Jiao, Jiantao and Ramachandran, Kannan},
  journal={arXiv preprint arXiv:2009.05990},
  year={2020}
}

@inproceedings{ross2010efficient,
  title={Efficient reductions for imitation learning},
  author={Ross, St{\'e}phane and Bagnell, Drew},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={661--668},
  year={2010}
}

@article{mcallester2003concentration,
  title={Concentration inequalities for the missing mass and for histogram rule error},
  author={McAllester, David and Ortiz, Luis},
  journal={Journal of Machine Learning Research},
  volume={4},
  number={Oct},
  pages={895--911},
  year={2003}
}

@article{levine2020offline,
  title={Offline reinforcement learning: {T}utorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{jiao2018minimax,
  title={Minimax estimation of the $\ell_1$ distance},
  author={Jiao, Jiantao and Han, Yanjun and Weissman, Tsachy},
  journal={IEEE Transactions on Information Theory},
  volume={64},
  number={10},
  pages={6672--6706},
  year={2018},
  publisher={IEEE}
}

@book{mitzenmacher2017probability,
  title={Probability and computing: {R}andomization and probabilistic techniques in algorithms and data analysis},
  author={Mitzenmacher, Michael and Upfal, Eli},
  year={2017},
  publisher={Cambridge university press}
}

@article{azar2013minimax,
  title={Minimax {PAC} bounds on the sample complexity of reinforcement learning with a generative model},
  author={Azar, Mohammad Gheshlaghi and Munos, R{\'e}mi and Kappen, Hilbert J},
  journal={Machine learning},
  volume={91},
  number={3},
  pages={325--349},
  year={2013},
  publisher={Springer}
}

@article{zhang2020reinforcement,
  title={Is reinforcement learning more difficult than bandits? {A} near-optimal algorithm escaping the curse of horizon},
  author={Zhang, Zihan and Ji, Xiangyang and Du, Simon S},
  journal={arXiv preprint arXiv:2009.13503},
  year={2020}
}

@inproceedings{agarwal2020model,
  title={Model-based reinforcement learning with a generative model is minimax optimal},
  author={Agarwal, Alekh and Kakade, Sham and Yang, Lin F},
  booktitle={Conference on Learning Theory},
  pages={67--83},
  year={2020},
  organization={PMLR}
}

@article{sidford2018near,
  title={Near-optimal time and sample complexities for solving discounted {M}arkov decision process with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  journal={arXiv preprint arXiv:1806.01492},
  year={2018}
}

@article{maurer2009empirical,
  title={Empirical {B}ernstein bounds and sample variance penalization},
  author={Maurer, Andreas and Pontil, Massimiliano},
  journal={arXiv preprint arXiv:0907.3740},
  year={2009}
}

@inproceedings{azar2013sample,
  title={On the sample complexity of reinforcement learning with a generative model},
  author={Azar, MG and Munos, R and Kappen, HJ},
  booktitle={Proceedings of the International Conference on Machine Learning},
  volume={29},
  pages={1--11},
  year={2013}
}


@article{bellman1959functional,
  title={Functional approximations and dynamic programming},
  author={Bellman, Richard and Dreyfus, Stuart},
  journal={Mathematical Tables and Other Aids to Computation},
  pages={247--251},
  year={1959},
  publisher={JSTOR}
}

@article{jin2020pessimism,
  title={Is Pessimism Provably Efficient for Offline {RL}?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2012.15085},
  year={2020}
}

@article{hu2021fast,
  title={Fast Rates for the Regret of Offline Reinforcement Learning},
  author={Hu, Yichun and Kallus, Nathan and Uehara, Masatoshi},
  journal={arXiv preprint arXiv:2102.00479},
  year={2021}
}

@article{slivkins2019introduction,
  title={Introduction to multi-armed bandits},
  author={Slivkins, Aleksandrs},
  journal={arXiv preprint arXiv:1904.07272},
  year={2019}
}

@article{liu2020provably,
  title={Provably good batch reinforcement learning without great exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:2007.08202},
  year={2020}
}

@article{kumar2019stabilizing,
  title={Stabilizing off-policy {Q}-learning via bootstrapping error reduction},
  author={Kumar, Aviral and Fu, Justin and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:1906.00949},
  year={2019}
}  

@article{nachum2020reinforcement,
  title={Reinforcement learning via {F}enchel-{R}ockafellar duality},
  author={Nachum, Ofir and Dai, Bo},
  journal={arXiv preprint arXiv:2001.01866},
  year={2020}
}
@inproceedings{fujimoto2019off,
  title={Off-policy deep reinforcement learning without exploration},
  author={Fujimoto, Scott and Meger, David and Precup, Doina},
  booktitle={International Conference on Machine Learning},
  pages={2052--2062},
  year={2019},
  organization={PMLR}
}

@inproceedings{nadjahi2019safe,
  title={Safe policy improvement with soft baseline bootstrapping},
  author={Nadjahi, Kimia and Laroche, Romain and des Combes, R{\'e}mi Tachet},
  booktitle={Joint European Conference on Machine Learning and Knowledge Discovery in Databases},
  pages={53--68},
  year={2019},
  organization={Springer}
}

@inproceedings{laroche2019safe,
  title={Safe policy improvement with baseline bootstrapping},
  author={Laroche, Romain and Trichelair, Paul and Des Combes, Remi Tachet},
  booktitle={International Conference on Machine Learning},
  pages={3652--3661},
  year={2019},
  organization={PMLR}
}

@article{peng2019advantage,
  title={Advantage-weighted regression: {S}imple and scalable off-policy reinforcement learning},
  author={Peng, Xue Bin and Kumar, Aviral and Zhang, Grace and Levine, Sergey},
  journal={arXiv preprint arXiv:1910.00177},
  year={2019}
}


@article{siegel2020keep,
  title={Keep doing what worked: {B}ehavioral modelling priors for offline reinforcement learning},
  author={Siegel, Noah Y and Springenberg, Jost Tobias and Berkenkamp, Felix and Abdolmaleki, Abbas and Neunert, Michael and Lampe, Thomas and Hafner, Roland and Riedmiller, Martin},
  journal={arXiv preprint arXiv:2002.08396},
  year={2020}
}

@article{ghasemipour2020emaq,
  title={{EMaQ}: {E}xpected-max {Q}-learning operator for simple yet effective offline and online {RL}},
  author={Ghasemipour, Seyed Kamyar Seyed and Schuurmans, Dale and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2007.11091},
  year={2020}
}

@article{kidambi2020morel,
  title={{MOReL}: {M}odel-based offline reinforcement learning},
  author={Kidambi, Rahul and Rajeswaran, Aravind and Netrapalli, Praneeth and Joachims, Thorsten},
  journal={arXiv preprint arXiv:2005.05951},
  year={2020}
}

@article{gottesman2018evaluating,
  title={Evaluating reinforcement learning algorithms in observational health settings},
  author={Gottesman, Omer and Johansson, Fredrik and Meier, Joshua and Dent, Jack and Lee, Donghun and Srinivasan, Srivatsan and Zhang, Linying and Ding, Yi and Wihl, David and Peng, Xuefeng and others},
  journal={arXiv preprint arXiv:1805.12298},
  year={2018}
}

@article{nie2020learning,
  title={Learning when-to-treat policies},
  author={Nie, Xinkun and Brunskill, Emma and Wager, Stefan},
  journal={Journal of the American Statistical Association},
  pages={1--18},
  year={2020},
  publisher={Taylor \& Francis}
}

@inproceedings{wang2018supervised,
  title={Supervised reinforcement learning with recurrent neural network for dynamic treatment recommendation},
  author={Wang, Lu and Zhang, Wei and He, Xiaofeng and Zha, Hongyuan},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2447--2456},
  year={2018}
}

@article{gottesman2019guidelines,
  title={Guidelines for reinforcement learning in healthcare},
  author={Gottesman, Omer and Johansson, Fredrik and Komorowski, Matthieu and Faisal, Aldo and Sontag, David and Doshi-Velez, Finale and Celi, Leo Anthony},
  journal={Nature medicine},
  volume={25},
  number={1},
  pages={16--18},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{yin2021near,
  title={Near-Optimal Offline Reinforcement Learning via Double Variance Reduction},
  author={Yin, Ming and Bai, Yu and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2102.01748},
  year={2021}
}

@article{munos2007performance,
  title={Performance bounds in $\ell_p$-norm for approximate value iteration},
  author={Munos, R{\'e}mi},
  journal={SIAM journal on control and optimization},
  volume={46},
  number={2},
  pages={541--561},
  year={2007},
  publisher={SIAM}
}

@inproceedings{szepesvari2005finite,
  title={Finite time bounds for sampling based fitted value iteration},
  author={Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  booktitle={Proceedings of the 22nd international conference on Machine learning},
  pages={880--887},
  year={2005}
}

@article{antos2008learning,
  title={Learning near-optimal policies with {B}ellman-residual minimization based fitted policy iteration and a single sample path},
  author={Antos, Andr{\'a}s and Szepesv{\'a}ri, Csaba and Munos, R{\'e}mi},
  journal={Machine Learning},
  volume={71},
  number={1},
  pages={89--129},
  year={2008},
  publisher={Springer}
}

@inproceedings{farahmand2010error,
  title={Error propagation for approximate policy and value iteration},
  author={Farahmand, Amir Massoud and Munos, R{\'e}mi and Szepesv{\'a}ri, Csaba},
  booktitle={Advances in Neural Information Processing Systems},
  year={2010}
}

@article{jiang2019value,
  title={On value functions and the agent-environment boundary},
  author={Jiang, Nan},
  journal={arXiv preprint arXiv:1905.13341},
  year={2019}
}

@inproceedings{uehara2020minimax,
  title={Minimax weight and {Q}-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  booktitle={International Conference on Machine Learning},
  pages={9659--9668},
  year={2020},
  organization={PMLR}
}

@article{feng2019kernel,
  title={A kernel loss for solving the {B}ellman equation},
  author={Feng, Yihao and Li, Lihong and Liu, Qiang},
  journal={arXiv preprint arXiv:1905.10506},
  year={2019}
}

@inproceedings{agarwal2020optimality,
  title={Optimality and approximation with policy gradient methods in {M}arkov decision processes},
  author={Agarwal, Alekh and Kakade, Sham M and Lee, Jason D and Mahajan, Gaurav},
  booktitle={Conference on Learning Theory},
  pages={64--66},
  year={2020},
  organization={PMLR}
}

@article{wang2020statistical,
  title={What are the Statistical Limits of Offline RL with Linear Function Approximation?},
  author={Wang, Ruosong and Foster, Dean P and Kakade, Sham M},
  journal={arXiv preprint arXiv:2010.11895},
  year={2020}
}

@article{zanette2020exponential,
  title={Exponential Lower Bounds for Batch Reinforcement Learning: {Batch RL} can be Exponentially Harder than Online {RL}},
  author={Zanette, Andrea},
  journal={arXiv preprint arXiv:2012.08005},
  year={2020}
}

@article{szepesvari2010algorithms,
  title={Algorithms for reinforcement learning},
  author={Szepesv{\'a}ri, Csaba},
  journal={Synthesis lectures on artificial intelligence and machine learning},
  volume={4},
  number={1},
  pages={1--103},
  year={2010},
  publisher={Morgan \& Claypool Publishers}
}

@article{yurtsever2020survey,
  title={A survey of autonomous driving: {C}ommon practices and emerging technologies},
  author={Yurtsever, Ekim and Lambert, Jacob and Carballo, Alexander and Takeda, Kazuya},
  journal={IEEE Access},
  volume={8},
  pages={58443--58469},
  year={2020},
  publisher={IEEE}
}

@article{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}

@inproceedings{sun2018fast,
  title={A fast integrated planning and control framework for autonomous driving via imitation learning},
  author={Sun, Liting and Peng, Cheng and Zhan, Wei and Tomizuka, Masayoshi},
  booktitle={Dynamic Systems and Control Conference},
  volume={51913},
  pages={V003T37A012},
  year={2018},
  organization={American Society of Mechanical Engineers}
}

@article{pan2017agile,
  title={Agile autonomous driving using end-to-end deep imitation learning},
  author={Pan, Yunpeng and Cheng, Ching-An and Saigol, Kamil and Lee, Keuntaek and Yan, Xinyan and Theodorou, Evangelos and Boots, Byron},
  journal={arXiv preprint arXiv:1709.07174},
  year={2017}
}

@inproceedings{kendall2019learning,
  title={Learning to drive in a day},
  author={Kendall, Alex and Hawke, Jeffrey and Janz, David and Mazur, Przemyslaw and Reda, Daniele and Allen, John-Mark and Lam, Vinh-Dieu and Bewley, Alex and Shah, Amar},
  booktitle={2019 International Conference on Robotics and Automation (ICRA)},
  pages={8248--8254},
  year={2019},
  organization={IEEE}
}
@inproceedings{codevilla2018end,
  title={End-to-end driving via conditional imitation learning},
  author={Codevilla, Felipe and M{\"u}ller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={4693--4700},
  year={2018},
  organization={IEEE}
}

@incollection{lange2012batch,
  title={Batch reinforcement learning},
  author={Lange, Sascha and Gabel, Thomas and Riedmiller, Martin},
  booktitle={Reinforcement learning},
  pages={45--73},
  year={2012},
  publisher={Springer}
}

@article{fu2020d4rl,
  title={{D4RL: D}atasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{silver2016mastering,
  title={Mastering the game of {Go} with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

@article{silver2017mastering,
  title={Mastering the game of {G}o without human knowledge},
  author={Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal={nature},
  volume={550},
  number={7676},
  pages={354--359},
  year={2017},
  publisher={Nature Publishing Group}
}

@article{vinyals2017starcraft,
  title={Starcraft {II}: {A} new challenge for reinforcement learning},
  author={Vinyals, Oriol and Ewalds, Timo and Bartunov, Sergey and Georgiev, Petko and Vezhnevets, Alexander Sasha and Yeo, Michelle and Makhzani, Alireza and K{\"u}ttler, Heinrich and Agapiou, John and Schrittwieser, Julian and others},
  journal={arXiv preprint arXiv:1708.04782},
  year={2017}
}

@article{mnih2013playing,
  title={Playing {Atari} with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@inproceedings{swaminathan2017off,
  title={Off-policy evaluation for slate recommendation},
  author={Swaminathan, Adith and Krishnamurthy, Akshay and Agarwal, Alekh and Dud{\'\i}k, Miroslav and Langford, John and Jose, Damien and Zitouni, Imed},
  booktitle={Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages={3635--3645},
  year={2017}
}
@inproceedings{gilotte2018offline,
  title={Offline {A/B} testing for recommender systems},
  author={Gilotte, Alexandre and Calauz{\`e}nes, Cl{\'e}ment and Nedelec, Thomas and Abraham, Alexandre and Doll{\'e}, Simon},
  booktitle={Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining},
  pages={198--206},
  year={2018}
}

@article{strehl2010learning,
  title={Learning from logged implicit exploration data},
  author={Strehl, Alex and Langford, John and Kakade, Sham and Li, Lihong},
  journal={arXiv preprint arXiv:1003.0120},
  year={2010}
}

@inproceedings{garcin2014offline,
  title={Offline and online evaluation of news recommender systems at swissinfo.ch},
  author={Garcin, Florent and Faltings, Boi and Donatsch, Olivier and Alazzawi, Ayar and Bruttin, Christophe and Huber, Amr},
  booktitle={Proceedings of the 8th ACM Conference on Recommender systems},
  pages={169--176},
  year={2014}
}

@inproceedings{thomas2017predictive,
  title={Predictive Off-Policy Policy Evaluation for Nonstationary Decision Problems, with Applications to Digital Marketing.},
  author={Thomas, Philip S and Theocharous, Georgios and Ghavamzadeh, Mohammad and Durugkar, Ishan and Brunskill, Emma},
  booktitle={AAAI},
  pages={4740--4745},
  year={2017}
}

@inproceedings{theocharous2015ad,
  title={Ad recommendation systems for life-time value optimization},
  author={Theocharous, Georgios and Thomas, Philip S and Ghavamzadeh, Mohammad},
  booktitle={Proceedings of the 24th International Conference on World Wide Web},
  pages={1305--1310},
  year={2015}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@misc{neurips_tutorial,
	title        = {Offline Reinforcement Learning: 
    {F}rom Algorithms to Practical Challenges},
	author       = {Kumar, Aviral and Levine, Sergey},
	booktitle={{NeurIPS 2020 Tutorial}},
	year = {2020},
	howpublished = {\url{https://sites.google.com/view/offlinerltutorial-neurips2020/home}}
}

@book{bickel1993efficient,
  title={Efficient and Adaptive Estimation for Semiparametric Models},
  author={Bickel, Peter J and Klaassen, Chris AJ and Bickel, Peter J and Ritov, Yaâ€™acov and Klaassen, J and Wellner, Jon A and Ritov, YA'Acov},
  volume={4},
  year={1993},
  publisher={Johns Hopkins University Press Baltimore}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{he2020minimax,
  title={Minimax Optimal Reinforcement Learning for Discounted {MDP}s},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2010.00587},
  year={2020}
}

@article{zhou2020nearly,
  title={Nearly Minimax Optimal Reinforcement Learning for Linear Mixture {M}arkov Decision Processes},
  author={Zhou, Dongruo and Gu, Quanquan and Szepesvari, Csaba},
  journal={arXiv preprint arXiv:2012.08507},
  year={2020}
}


@article{wu2021nearly,
  title={Nearly Minimax Optimal Regret for Learning Infinite-horizon Average-reward {MDP}s with Linear Function Approximation},
  author={Wu, Yue and Zhou, Dongruo and Gu, Quanquan},
  journal={arXiv preprint arXiv:2102.07301},
  year={2021}
}

@article{bubeck2011pure,
  title={Pure exploration in finitely-armed and continuous-armed bandits},
  author={Bubeck, S{\'e}bastien and Munos, R{\'e}mi and Stoltz, Gilles},
  journal={Theoretical Computer Science},
  volume={412},
  number={19},
  pages={1832--1852},
  year={2011},
  publisher={Elsevier}
}

@inproceedings{dudik2011efficient,
  title={Efficient optimal learning for contextual bandits},
  author={Dudik, M and Hsu, D and Kale, S and Karampatziakis, N and Langford, J and Reyzin, L and Zhang, T},
  booktitle={Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence, UAI 2011},
  pages={169},
  year={2011}
}

@inproceedings{agarwal2014taming,
  title={Taming the monster: {A} fast and simple algorithm for contextual bandits},
  author={Agarwal, Alekh and Hsu, Daniel and Kale, Satyen and Langford, John and Li, Lihong and Schapire, Robert},
  booktitle={International Conference on Machine Learning},
  pages={1638--1646},
  year={2014},
  organization={PMLR}
}


@inproceedings{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={4868--4878},
  year={2018}
}

@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{massart2006risk,
  title={Risk bounds for statistical learning},
  author={Massart, Pascal and N{\'e}d{\'e}lec, {\'E}lodie and others},
  journal={The Annals of Statistics},
  volume={34},
  number={5},
  pages={2326--2366},
  year={2006},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{lattimore2012pac,
  title={{PAC} bounds for discounted {MDP}s},
  author={Lattimore, Tor and Hutter, Marcus},
  booktitle={International Conference on Algorithmic Learning Theory},
  pages={320--334},
  year={2012},
  organization={Springer}
}

@article{zhang2020model,
  title={Model-free reinforcement learning: {F}rom clipped pseudo-regret to sample complexity},
  author={Zhang, Zihan and Zhou, Yuan and Ji, Xiangyang},
  journal={arXiv preprint arXiv:2006.03864},
  year={2020}
}

@article{lagoudakis2003least,
  title={Least-squares policy iteration},
  author={Lagoudakis, Michail G and Parr, Ronald},
  journal={The Journal of Machine Learning Research},
  volume={4},
  pages={1107--1149},
  year={2003},
  publisher={JMLR. org}
}

@inproceedings{munos2003error,
  title={Error bounds for approximate policy iteration},
  author={Munos, R{\'e}mi},
  booktitle={Proceedings of the Twentieth International Conference on International Conference on Machine Learning},
  pages={560--567},
  year={2003}
}

@article{wang2020critic,
  title={Critic Regularized Regression},
  author={Wang, Ziyu and Novikov, Alexander and Zolna, Konrad and Merel, Josh S and Springenberg, Jost Tobias and Reed, Scott E and Shahriari, Bobak and Siegel, Noah and Gulcehre, Caglar and Heess, Nicolas and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{fujimoto2019benchmarking,
  title={Benchmarking batch deep reinforcement learning algorithms},
  author={Fujimoto, Scott and Conti, Edoardo and Ghavamzadeh, Mohammad and Pineau, Joelle},
  journal={arXiv preprint arXiv:1910.01708},
  year={2019}
}

@inproceedings{nachum2019dualdice,
	title        = {Dualdice: {B}ehavior-agnostic estimation of discounted stationary distribution corrections},
	author       = {Nachum, Ofir and Chow, Yinlam and Dai, Bo and Li, Lihong},
	year         = {2019},
	booktitle    = {Advances in Neural Information Processing Systems},
	pages        = {2315--2325}
}

@article{nachum2019algaedice,
	title        = {{AlgaeDICE: P}olicy Gradient from Arbitrary Experience},
	author       = {Nachum, Ofir and Dai, Bo and Kostrikov, Ilya and Chow, Yinlam and Li, Lihong and Schuurmans, Dale},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1912.02074}
}

@inproceedings{Zhang2020GenDICE:,
	title        = {{GenDICE: G}eneralized Offline Estimation of Stationary Values},
	author       = {Ruiyi Zhang and Bo Dai and Lihong Li and Dale Schuurmans},
	year         = {2020},
	booktitle    = {International Conference on Learning Representations}
}

@article{zhang2020gradientdice,
	title        = {{GradientDICE: R}ethinking Generalized Offline Estimation of Stationary Values},
	author       = {Zhang, Shantong and Liu, Bo and Whiteson, Shimon},
	year         = {2020},
	journal      = {arXiv preprint arXiv:2001.11113}
}

@article{wu2019behavior,
	title        = {Behavior Regularized Offline Reinforcement Learning},
	author       = {Wu, Yifan and Tucker, George and Nachum, Ofir},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1911.11361}
}


@article{jaques2019way,
	title        = {Way off-policy batch deep reinforcement learning of implicit human preferences in dialog},
	author       = {Jaques, Natasha and Ghandeharioun, Asma and Shen, Judy Hanwen and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang and Picard, Rosalind},
	year         = {2019},
	journal      = {arXiv preprint arXiv:1907.00456}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}

@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016},
  organization={PMLR}
}

@inproceedings{farajtabar2018more,
  title={More robust doubly robust off-policy evaluation},
  author={Farajtabar, Mehrdad and Chow, Yinlam and Ghavamzadeh, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={1447--1456},
  year={2018},
  organization={PMLR}
}

@inproceedings{liu2018breaking,
  title={Breaking the curse of horizon: {I}nfinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={5361--5371},
  year={2018}
}

@inproceedings{tang2019doubly,
  title={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},
  author={Tang, Ziyang and Feng, Yihao and Li, Lihong and Zhou, Dengyong and Liu, Qiang},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{kallus2020doubly,
  title={Doubly Robust Off-Policy Value and Gradient Estimation for Deterministic Policies},
  author={Kallus, Nathan and Uehara, Masatoshi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{jiang2020minimax,
  title={Minimax Value Interval for Off-Policy Evaluation and Policy Optimization},
  author={Jiang, Nan and Huang, Jiawei},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@inproceedings{duan2020minimax,
  title={Minimax-optimal off-policy evaluation with linear function approximation},
  author={Duan, Yaqi and Jia, Zeyu and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={2701--2709},
  year={2020},
  organization={PMLR}
}



@article{gulcehre2020rl,
  title={{RL} unplugged: {B}enchmarks for offline reinforcement learning},
  author={Gulcehre, Caglar and Wang, Ziyu and Novikov, Alexander and Paine, Tom Le and Colmenarejo, Sergio G{\'o}mez and Zolna, Konrad and Agarwal, Rishabh and Merel, Josh and Mankowitz, Daniel and Paduraru, Cosmin and others},
  journal={arXiv preprint arXiv:2006.13888},
  year={2020}
}

@inproceedings{antos2007fitted,
  title={Fitted {Q}-iteration in continuous action-space MDPs},
  author={Antos, Andras and Munos, R{\'e}mi and Szepesvari, Csaba},
  booktitle={Neural Information Processing Systems},
  year={2007}
}

@inproceedings{liu2019neural,
  title={Neural trust region/proximal policy optimization attains globally optimal policy},
  author={Liu, Boyi and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={Neural Information Processing Systems},
  year={2019}
}

@inproceedings{wang2019neural,
  title={Neural Policy Gradient Methods: {G}lobal Optimality and Rates of Convergence},
  author={Wang, Lingxiao and Cai, Qi and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{liao2020batch,
  title={Batch Policy Learning in Average Reward {M}arkov Decision Processes},
  author={Liao, Peng and Qi, Zhengling and Murphy, Susan},
  journal={arXiv preprint arXiv:2007.11771},
  year={2020}
}

@article{zhang2020variational,
  title={Variational policy gradient method for reinforcement learning with general utilities},
  author={Zhang, Junyu and Koppel, Alec and Bedi, Amrit Singh and Szepesvari, Csaba and Wang, Mengdi},
  journal={arXiv preprint arXiv:2007.02151},
  year={2020}
}

@inproceedings{yang2019sample,
  title={Sample-optimal parametric {Q}-learning using linearly additive features},
  author={Yang, Lin and Wang, Mengdi},
  booktitle={International Conference on Machine Learning},
  pages={6995--7004},
  year={2019},
  organization={PMLR}
}

@article{li2020sample,
  title={Sample complexity of asynchronous Q-learning: Sharper analysis and variance reduction},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={arXiv preprint arXiv:2006.03041},
  year={2020}
}

@article{wainwright2019variance,
  title={Variance-reduced $ Q $-learning is minimax optimal},
  author={Wainwright, Martin J},
  journal={arXiv preprint arXiv:1906.04697},
  year={2019}
}

@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2},
  pages={2818--2826},
  year={2015}
}

@inproceedings{jiang2017contextual,
  title={Contextual decision processes with low {B}ellman rank are {PAC}-learnable},
  author={Jiang, Nan and Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John and Schapire, Robert E},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2017},
  organization={PMLR}
}

@inproceedings{krishnamurthy2016pac,
  title={{PAC} reinforcement learning with rich observations},
  author={Krishnamurthy, Akshay and Agarwal, Alekh and Langford, John},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={1848--1856},
  year={2016}
}

@article{domingues2020episodic,
  title={Episodic Reinforcement Learning in Finite {MDPs: M}inimax Lower Bounds Revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  journal={arXiv preprint arXiv:2010.03531},
  year={2020}
}


@misc{robert1990ash,
  title={Information Theory},
  author={Robert, B Ash.},
  year={1990},
  publisher={Dover Publications Inc., New York}
}


@article{gilbert1952comparison,
  title={A comparison of signalling alphabets},
  author={Gilbert, Edgar N},
  journal={The Bell system technical journal},
  volume={31},
  number={3},
  pages={504--522},
  year={1952},
  publisher={Nokia Bell Labs}
}

@article{varshamov1957estimate,
  title={Estimate of the number of signals in error correcting codes},
  author={Varshamov, Rom Rubenovich},
  journal={Docklady Akad. Nauk, SSSR},
  volume={117},
  pages={739--741},
  year={1957}
}

@article{fano1961transmission,
  title={Transmission of information: A statistical theory of communications},
  author={Fano, Robert M},
  journal={American Journal of Physics},
  volume={29},
  number={11},
  pages={793--794},
  year={1961},
  publisher={American Association of Physics Teachers}
}

@inproceedings{cai2020provably,
  title={Provably efficient exploration in policy optimization},
  author={Cai, Qi and Yang, Zhuoran and Jin, Chi and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={1283--1294},
  year={2020},
  organization={PMLR}
}

@inproceedings{du2020good,
  title={Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?},
  author={Du, Simon S and Kakade, Sham M and Wang, Ruosong and Yang, Lin F},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{ma2021minimax,
  title={Minimax Off-Policy Evaluation for Multi-Armed Bandits},
  author={Ma, Cong and Zhu, Banghua and Jiao, Jiantao and Wainwright, Martin J},
  journal={arXiv preprint arXiv:2101.07781},
  year={2021}
}
@article{puterman1990markov,
  title={Markov decision processes},
  author={Puterman, Martin L},
  journal={Handbooks in operations research and management science},
  volume={2},
  pages={331--434},
  year={1990},
  publisher={Elsevier}
}

@article{yu2021combo,
  title={{COMBO: C}onservative Offline Model-Based Policy Optimization},
  author={Yu, Tianhe and Kumar, Aviral and Rafailov, Rafael and Rajeswaran, Aravind and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:2102.08363},
  year={2021}
}

@article{koh2020wilds,
  title={{WILDS: A} Benchmark of in-the-Wild Distribution Shifts},
  author={Koh, Pang Wei and Sagawa, Shiori and Marklund, Henrik and Xie, Sang Michael and Zhang, Marvin and Balsubramani, Akshay and Hu, Weihua and Yasunaga, Michihiro and Phillips, Richard Lanas and Beery, Sara and others},
  journal={arXiv preprint arXiv:2012.07421},
  year={2020}
}

@inproceedings{sidford2018variance,
  title={Variance reduced value iteration and faster algorithms for solving {M}arkov decision processes},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Ye, Yinyu},
  booktitle={Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms},
  pages={770--787},
  year={2018},
  organization={SIAM}
}

@article{li2020breaking,
  title={Breaking the sample size barrier in model-based reinforcement learning with a generative model},
  author={Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  journal={arXiv preprint arXiv:2005.12900},
  year={2020}
}

@article{buckman2020importance,
  title={The importance of pessimism in fixed-dataset policy optimization},
  author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2009.06799},
  year={2020}
}

@article{uehara2021finite,
  title={Finite Sample Analysis of Minimax Offline Reinforcement Learning: {C}ompleteness, Fast Rates and First-Order Efficiency},
  author={Uehara, Masatoshi and Imaizumi, Masaaki and Jiang, Nan and Kallus, Nathan and Sun, Wen and Xie, Tengyang},
  journal={arXiv preprint arXiv:2102.02981},
  year={2021}
}

@article{hao2020sparse,
  title={Sparse Feature Selection Makes Batch Reinforcement Learning More Sample Efficient},
  author={Hao, Botao and Duan, Yaqi and Lattimore, Tor and Szepesv{\'a}ri, Csaba and Wang, Mengdi},
  journal={arXiv preprint arXiv:2011.04019},
  year={2020}
}

@book{le2012asymptotic,
  title={{Asymptotic Methods in Statistical Decision Theory}},
  author={Le Cam, Lucien},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@incollection{yu1997assouad,
  title={Assouad, fano, and le cam},
  author={Yu, Bin},
  booktitle={{Festschrift for Lucien Le Cam}},
  pages={423--435},
  year={1997},
  publisher={Springer}
}
