
@misc{tosh_contrastive_2020,
	title = {Contrastive estimation reveals topic posterior information to linear models},
	url = {http://arxiv.org/abs/2003.02234},
	doi = {10.48550/arXiv.2003.02234},
	abstract = {Contrastive learning is an approach to representation learning that utilizes naturally occurring similar and dissimilar pairs of data points to find useful embeddings of data. In the context of document classification under topic modeling assumptions, we prove that contrastive learning is capable of recovering a representation of documents that reveals their underlying topic posterior information to linear models. We apply this procedure in a semi-supervised setup and demonstrate empirically that linear classifiers with these representations perform well in document classification tasks with very few training examples.},
	urldate = {2022-08-10},
	publisher = {arXiv},
	author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
	month = mar,
	year = {2020},
	note = {arXiv:2003.02234 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mcinnes_umap_2018,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection}},
	volume = {3},
	issn = {2475-9066},
	shorttitle = {{UMAP}},
	url = {https://joss.theoj.org/papers/10.21105/joss.00861},
	doi = {10.21105/joss.00861},
	abstract = {McInnes et al., (2018). UMAP: Uniform Manifold Approximation and Projection. Journal of Open Source Software, 3(29), 861, https://doi.org/10.21105/joss.00861},
	language = {en},
	number = {29},
	urldate = {2022-08-09},
	journal = {Journal of Open Source Software},
	author = {McInnes, Leland and Healy, John and Saul, Nathaniel and Großberger, Lukas},
	month = sep,
	year = {2018},
	pages = {861},
}

@misc{yeh_decoupled_2022,
	title = {Decoupled {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2110.06848},
	abstract = {Contrastive learning (CL) is one of the most successful paradigms for self-supervised learning (SSL). In a principled way, it considers two augmented "views" of the same image as positive to be pulled closer, and all other images as negative to be pushed further apart. However, behind the impressive success of CL-based techniques, their formulation often relies on heavy-computation settings, including large sample batches, extensive training epochs, etc. We are thus motivated to tackle these issues and establish a simple, efficient, yet competitive baseline of contrastive learning. Specifically, we identify, from theoretical and empirical studies, a noticeable negative-positive-coupling (NPC) effect in the widely used InfoNCE loss, leading to unsuitable learning efficiency concerning the batch size. By removing the NPC effect, we propose decoupled contrastive learning (DCL) loss, which removes the positive term from the denominator and significantly improves the learning efficiency. DCL achieves competitive performance with less sensitivity to sub-optimal hyperparameters, requiring neither large batches in SimCLR, momentum encoding in MoCo, or large epochs. We demonstrate with various benchmarks while manifesting robustness as much less sensitive to suboptimal hyperparameters. Notably, SimCLR with DCL achieves 68.2\% ImageNet-1K top-1 accuracy using batch size 256 within 200 epochs pre-training, outperforming its SimCLR baseline by 6.4\%. Further, DCL can be combined with the SOTA contrastive learning method, NNCLR, to achieve 72.3\% ImageNet-1K top-1 accuracy with 512 batch size in 400 epochs, which represents a new SOTA in contrastive learning. We believe DCL provides a valuable baseline for future contrastive SSL studies.},
	urldate = {2022-08-03},
	publisher = {arXiv},
	author = {Yeh, Chun-Hsiao and Hong, Cheng-Yao and Hsu, Yen-Chi and Liu, Tyng-Luh and Chen, Yubei and LeCun, Yann},
	month = jul,
	year = {2022},
	note = {arXiv:2110.06848 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{desai_virtex_2021,
	title = {{VirTex}: {Learning} {Visual} {Representations} from {Textual} {Annotations}},
	shorttitle = {{VirTex}},
	url = {http://arxiv.org/abs/2006.06666},
	abstract = {The de-facto approach to many vision tasks is to start from pretrained visual representations, typically learned via supervised training on ImageNet. Recent methods have explored unsupervised pretraining to scale to vast quantities of unlabeled images. In contrast, we aim to learn high-quality visual representations from fewer images. To this end, we revisit supervised pretraining, and seek data-efficient alternatives to classification-based pretraining. We propose VirTex -- a pretraining approach using semantically dense captions to learn visual representations. We train convolutional networks from scratch on COCO Captions, and transfer them to downstream recognition tasks including image classification, object detection, and instance segmentation. On all tasks, VirTex yields features that match or exceed those learned on ImageNet -- supervised or unsupervised -- despite using up to ten times fewer images.},
	urldate = {2022-07-28},
	publisher = {arXiv},
	author = {Desai, Karan and Johnson, Justin},
	month = sep,
	year = {2021},
	note = {arXiv:2006.06666 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@misc{assran_masked_2022,
	title = {Masked {Siamese} {Networks} for {Label}-{Efficient} {Learning}},
	url = {http://arxiv.org/abs/2204.07141},
	abstract = {We propose Masked Siamese Networks (MSN), a self-supervised learning framework for learning image representations. Our approach matches the representation of an image view containing randomly masked patches to the representation of the original unmasked image. This self-supervised pre-training strategy is particularly scalable when applied to Vision Transformers since only the unmasked patches are processed by the network. As a result, MSNs improve the scalability of joint-embedding architectures, while producing representations of a high semantic level that perform competitively on low-shot image classification. For instance, on ImageNet-1K, with only 5,000 annotated images, our base MSN model achieves 72.4\% top-1 accuracy, and with 1\% of ImageNet-1K labels, we achieve 75.7\% top-1 accuracy, setting a new state-of-the-art for self-supervised learning on this benchmark. Our code is publicly available.},
	urldate = {2022-07-27},
	publisher = {arXiv},
	author = {Assran, Mahmoud and Caron, Mathilde and Misra, Ishan and Bojanowski, Piotr and Bordes, Florian and Vincent, Pascal and Joulin, Armand and Rabbat, Michael and Ballas, Nicolas},
	month = apr,
	year = {2022},
	note = {arXiv:2204.07141 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@misc{williamson_information_2022,
	title = {Information {Processing} {Equalities} and the {Information}-{Risk} {Bridge}},
	url = {http://arxiv.org/abs/2207.11987},
	doi = {10.48550/arXiv.2207.11987},
	abstract = {We introduce two new classes of measures of information for statistical experiments which generalise and subsume \${\textbackslash}phi\$-divergences, integral probability metrics, \${\textbackslash}mathfrak\{N\}\$-distances (MMD), and \$(f,{\textbackslash}Gamma)\$ divergences between two or more distributions. This enables us to derive a simple geometrical relationship between measures of information and the Bayes risk of a statistical decision problem, thus extending the variational \${\textbackslash}phi\$-divergence representation to multiple distributions in an entirely symmetric manner. The new families of divergence are closed under the action of Markov operators which yields an information processing equality which is a refinement and generalisation of the classical data processing inequality. This equality gives insight into the significance of the choice of the hypothesis class in classical risk minimization.},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Williamson, Robert C. and Cranko, Zac},
	month = jul,
	year = {2022},
	note = {arXiv:2207.11987 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, E.4, G.3, I.5, Statistics - Machine Learning},
}

@article{russell_rationality_nodate,
	title = {Rationality and {Intelligence}},
	abstract = {The long-term goal of our ﬁeld is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, beneﬁts from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. The concept of rational agency has long been considered a leading candidate to fulﬁll this role. This paper outlines a gradual evolution in the formal conception of rationality that brings it closer to our informal conception of intelligence and simultaneously reduces the gap between theory and practice. Some directions for future research are indicated.},
	language = {en},
	author = {Russell, Stuart},
	pages = {27},
}

@incollection{karni_axiomatic_2014,
	title = {Axiomatic {Foundations} of {Expected} {Utility} and {Subjective} {Probability}},
	volume = {1},
	isbn = {978-0-444-53685-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444536853000015},
	abstract = {This chapter provides a critical review of the theories of decision making under risk and under uncertainty and the notion of choice-based subjective probabilities. It includes formal statements and discussions of the various models, including their analytical frameworks, the corresponding axiomatic foundations, and the representations.},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Handbook of the {Economics} of {Risk} and {Uncertainty}},
	publisher = {Elsevier},
	author = {Karni, Edi},
	year = {2014},
	doi = {10.1016/B978-0-444-53685-3.00001-5},
	pages = {1--39},
}

@article{genewein_bounded_2015,
	title = {Bounded {Rationality}, {Abstraction}, and {Hierarchical} {Decision}-{Making}: {An} {Information}-{Theoretic} {Optimality} {Principle}},
	volume = {2},
	issn = {2296-9144},
	shorttitle = {Bounded {Rationality}, {Abstraction}, and {Hierarchical} {Decision}-{Making}},
	url = {https://www.frontiersin.org/articles/10.3389/frobt.2015.00027},
	abstract = {Abstraction and hierarchical information processing are hallmarks of human and animal intelligence underlying the unrivaled flexibility of behavior in biological systems. Achieving such flexibility in artificial systems is challenging, even with more and more computational power. Here, we investigate the hypothesis that abstraction and hierarchical information processing might in fact be the consequence of limitations in information-processing power. In particular, we study an information-theoretic framework of bounded rational decision-making that trades off utility maximization against information-processing costs. We apply the basic principle of this framework to perception-action systems with multiple information-processing nodes and derive bounded-optimal solutions. We show how the formation of abstractions and decision-making hierarchies depends on information-processing costs. We illustrate the theoretical ideas with example simulations and conclude by formalizing a mathematically unifying optimization principle that could potentially be extended to more complex systems.},
	urldate = {2022-07-26},
	journal = {Frontiers in Robotics and AI},
	author = {Genewein, Tim and Leibfried, Felix and Grau-Moya, Jordi and Braun, Daniel Alexander},
	year = {2015},
}

@article{lipman_information_1995,
	title = {Information {Processing} and {Bounded} {Rationality}: {A} {Survey}},
	volume = {28},
	issn = {0008-4085},
	shorttitle = {Information {Processing} and {Bounded} {Rationality}},
	url = {https://www.jstor.org/stable/136022},
	doi = {10.2307/136022},
	abstract = {This paper surveys recent attempts to formulate a plausible and tractable model of bounded rationality. I focus in particular on models that view bounded rationality as stemming from limited information processing. I discuss partitional models (such as computability, automata, perceptrons, and optimal networks), non-partitional models, and axiomatic approaches. /// Transformation de l'information et rationalité limitée: une revue de la littérature. Ce mémoire examine certaines tentatives récentes pour formuler un modèle plausible et utilisable de la rationalité limitée. L'auteur s'attache en particulier aux modèles qui présentent la rationalité limitée comme un phénomène émanant de la limitation dans la capacité à transformer l'information. L'auteur discute les modèles qu'on appelle `partitionnels' (computabilité, automates, perceptrons, réseaux optimaux), les modèles `non partitionnels' ainsi que les approches axiomatiques.},
	number = {1},
	urldate = {2022-07-26},
	journal = {The Canadian Journal of Economics / Revue canadienne d'Economique},
	author = {Lipman, Barton L.},
	year = {1995},
	note = {Publisher: [Wiley, Canadian Economics Association]},
	pages = {42--67},
}

@misc{shwartz-ziv_what_2022,
	title = {What {Do} {We} {Maximize} in {Self}-{Supervised} {Learning}?},
	url = {http://arxiv.org/abs/2207.10081},
	abstract = {In this paper, we examine self-supervised learning methods, particularly VICReg, to provide an information-theoretical understanding of their construction. As a first step, we demonstrate how information-theoretic quantities can be obtained for a deterministic network, offering a possible alternative to prior work that relies on stochastic models. This enables us to demonstrate how VICReg can be (re)discovered from first principles and its assumptions about data distribution. Furthermore, we empirically demonstrate the validity of our assumptions, confirming our novel understanding of VICReg. Finally, we believe that the derivation and insights we obtain can be generalized to many other SSL methods, opening new avenues for theoretical and practical understanding of SSL and transfer learning.},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Shwartz-Ziv, Ravid and Balestriero, Randall and LeCun, Yann},
	month = jul,
	year = {2022},
	note = {arXiv:2207.10081 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@incollection{braha_information_2006,
	address = {Berlin, Heidelberg},
	title = {Information {Theory} ― {The} {Bridge} {Connecting} {Bounded} {Rational} {Game} {Theory} and {Statistical} {Physics}},
	isbn = {978-3-540-32831-5},
	url = {http://link.springer.com/10.1007/3-540-32834-3_12},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Complex {Engineered} {Systems}},
	publisher = {Springer Berlin Heidelberg},
	author = {Wolpert, David H.},
	editor = {Braha, Dan and Minai, Ali A. and Bar-Yam, Yaneer},
	year = {2006},
	doi = {10.1007/3-540-32834-3_12},
	note = {Series Title: Understanding Complex Systems},
	pages = {262--290},
}

@article{simon_theories_1972,
	title = {Theories of bounded rationality, decision and organization},
	journal = {CBR a. R. Radner. Amsterdam, NorthHolland},
	author = {Simon, Herbert A},
	year = {1972},
}

@incollection{simon_bounded_1990,
	address = {London},
	series = {The {New} {Palgrave}},
	title = {Bounded {Rationality}},
	isbn = {978-1-349-20568-4},
	url = {https://doi.org/10.1007/978-1-349-20568-4_5},
	abstract = {The term ‘bounded rationality’ is used to designate rational choice that takes into account the cognitive limitations of the decision-maker — limitations of both knowledge and computational capacity. Bounded rationality is a central theme in the behavioural approach to economics, which is deeply concerned with the ways in which the actual decision–making process influences the decisions that are reached.},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Utility and {Probability}},
	publisher = {Palgrave Macmillan UK},
	author = {Simon, Herbert A.},
	editor = {Eatwell, John and Milgate, Murray and Newman, Peter},
	year = {1990},
	doi = {10.1007/978-1-349-20568-4_5},
	pages = {15--18},
}

@incollection{simon_bounded_1990-1,
	address = {London},
	series = {The {New} {Palgrave}},
	title = {Bounded {Rationality}},
	isbn = {978-1-349-20568-4},
	url = {https://doi.org/10.1007/978-1-349-20568-4_5},
	abstract = {The term ‘bounded rationality’ is used to designate rational choice that takes into account the cognitive limitations of the decision-maker — limitations of both knowledge and computational capacity. Bounded rationality is a central theme in the behavioural approach to economics, which is deeply concerned with the ways in which the actual decision–making process influences the decisions that are reached.},
	language = {en},
	urldate = {2022-07-26},
	booktitle = {Utility and {Probability}},
	publisher = {Palgrave Macmillan UK},
	author = {Simon, Herbert A.},
	editor = {Eatwell, John and Milgate, Murray and Newman, Peter},
	year = {1990},
	doi = {10.1007/978-1-349-20568-4_5},
	pages = {15--18},
}

@article{simon_behavioral_1955,
	title = {A {Behavioral} {Model} of {Rational} {Choice}},
	volume = {69},
	issn = {0033-5533},
	url = {https://www.jstor.org/stable/1884852},
	doi = {10.2307/1884852},
	abstract = {Introduction, 99.--I. Some general features of rational choice, 100.--II. The essential simplifications, 103.--III. Existence and uniqueness of solutions, 111.--IV. Further comments on dynamics, 113.--V. Conclusion, 114.--Appendix, 115.},
	number = {1},
	urldate = {2022-07-26},
	journal = {The Quarterly Journal of Economics},
	author = {Simon, Herbert A.},
	year = {1955},
	note = {Publisher: Oxford University Press},
	pages = {99--118},
}

@misc{ortega_information-theoretic_2015,
	title = {Information-{Theoretic} {Bounded} {Rationality}},
	url = {http://arxiv.org/abs/1512.06789},
	abstract = {Bounded rationality, that is, decision-making and planning under resource limitations, is widely regarded as an important open problem in artificial intelligence, reinforcement learning, computational neuroscience and economics. This paper offers a consolidated presentation of a theory of bounded rationality based on information-theoretic ideas. We provide a conceptual justification for using the free energy functional as the objective function for characterizing bounded-rational decisions. This functional possesses three crucial properties: it controls the size of the solution space; it has Monte Carlo planners that are exact, yet bypass the need for exhaustive search; and it captures model uncertainty arising from lack of evidence or from interacting with other agents having unknown intentions. We discuss the single-step decision-making case, and show how to extend it to sequential decisions using equivalence transformations. This extension yields a very general class of decision problems that encompass classical decision rules (e.g. EXPECTIMAX and MINIMAX) as limit cases, as well as trust- and risk-sensitive planning.},
	urldate = {2022-07-26},
	publisher = {arXiv},
	author = {Ortega, Pedro A. and Braun, Daniel A. and Dyer, Justin and Kim, Kee-Eung and Tishby, Naftali},
	month = dec,
	year = {2015},
	note = {arXiv:1512.06789 [cs, math, stat]},
	keywords = {Computer Science - Artificial Intelligence, Electrical Engineering and Systems Science - Systems and Control, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{horvitz_reasoning_1987,
	title = {Reasoning about beliefs and actions under computational resource constraints},
	abstract = {Although many investigators arm a desire to build reasoning systems that behave consistently with the axiomatic basis dened by probability theory and utility theory, limited resources for engineering and computation can make a complete normative anal-ysis impossible. We attempt to move discussion beyond the debate over the scope of problems that can be handled eectively to cases where it is clear that there are in-sucient computational resources to perform an analysis deemed as complete. Under these conditions, we stress the importance of considering the expected costs and benets of applying alternative approximation procedures and heuristics for computation and knowledge acquisition. We discuss how knowledge about the structure of user utility can be used to control value tradeos for tailoring inference to alternative contexts. We address the notion of real-time rationality, focusing on the application of knowledge about the expected timewise-renement abilities of reasoning strategies to balance the bene ts of additional computation with the costs of acting with a partial result. We dis-cuss the benets of applying decision theory to control the solution of dicult problems given limitations and uncertainty in reasoning resources. 1},
	booktitle = {in {Proceedings} of the 1989 {Workshop} on {Uncertainty} and {AI}},
	author = {Horvitz, Eric J.},
	year = {1987},
}

@article{mullainathan_memory-based_2002,
	title = {A {Memory}-{Based} {Model} of {Bounded} {Rationality}},
	volume = {117},
	issn = {0033-5533},
	url = {https://www.jstor.org/stable/4132488},
	abstract = {In order to investigate the impact of limited memory on human behavior, I develop a model of memory grounded in psychological and biological research. I assume that people take their memories as accurate and use them to make inferences. The resulting model predicts both over- and underreaction but provides enough structure to predict when each effect dominates. I then use this framework to study the consumption decision. The results match empirical work on consumption predictability as well as differences in the marginal propensity to consume from different income streams. Most importantly, because it ties the extent of bias to a measurable aspect of the stochastic process being forecasted, the model makes testable empirical predictions.},
	number = {3},
	urldate = {2022-07-25},
	journal = {The Quarterly Journal of Economics},
	author = {Mullainathan, Sendhil},
	year = {2002},
	note = {Publisher: Oxford University Press},
	pages = {735--774},
}

@misc{noauthor_memory-based_nodate,
	title = {A memory-based model of bounded rationality - {Google} {Search}},
	url = {https://www.google.com/search?q=A+memory-based+model+of+bounded+rationality&oq=A+memory-based+model+of+bounded+rationality&aqs=chrome..69i57j0i390l5j69i64l3.340j0j4&sourceid=chrome&ie=UTF-8},
	urldate = {2022-07-25},
}

@misc{russell_provably_1995,
	title = {Provably {Bounded}-{Optimal} {Agents}},
	url = {http://arxiv.org/abs/cs/9505103},
	abstract = {Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Russell, S. J. and Subramanian, D.},
	month = apr,
	year = {1995},
	note = {arXiv:cs/9505103},
	keywords = {Computer Science - Artificial Intelligence},
}

@article{good_rational_1952,
	title = {Rational {Decisions}},
	volume = {14},
	url = {http://www.jstor.org/stable/2984087},
	number = {1},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Good, I. J.},
	year = {1952},
	pages = {107--114},
}

@article{good_rational_1952-1,
	title = {Rational {Decisions}},
	volume = {14},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984087},
	abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
	number = {1},
	urldate = {2022-07-25},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Good, I. J.},
	year = {1952},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {107--114},
}

@article{neyman_bounded_1985,
	title = {Bounded complexity justifies cooperation in the finitely repeated prisoners' dilemma},
	volume = {19},
	issn = {0165-1765},
	url = {https://www.sciencedirect.com/science/article/pii/0165176585900266},
	doi = {10.1016/0165-1765(85)90026-6},
	abstract = {Cooperation in the finitely repeated prisoner's dilemma is justified, without departure from strict utility maximization or complete information, but under the assumption that there are bounds (possibly very large) to the complexity of the strategies that the players may use.},
	language = {en},
	number = {3},
	urldate = {2022-07-25},
	journal = {Economics Letters},
	author = {Neyman, Abraham},
	month = jan,
	year = {1985},
	pages = {227--229},
}

@misc{halpern_decision_2013,
	title = {Decision {Theory} with {Resource}-{Bounded} {Agents}},
	url = {http://arxiv.org/abs/1308.3780},
	doi = {10.48550/arXiv.1308.3780},
	abstract = {There have been two major lines of research aimed at capturing resource-bounded players in game theory. The first, initiated by Rubinstein, charges an agent for doing costly computation; the second, initiated by Neyman, does not charge for computation, but limits the computation that agents can do, typically by modeling agents as finite automata. We review recent work on applying both approaches in the context of decision theory. For the first approach, we take the objects of choice in a decision problem to be Turing machines, and charge players for the ``complexity'' of the Turing machine chosen (e.g., its running time). This approach can be used to explain well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on) and belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions) as the outcomes of quite rational decisions. For the second approach, we model people as finite automata, and provide a simple algorithm that, on a problem that captures a number of settings of interest, provably performs optimally as the number of states in the automaton increases.},
	urldate = {2022-07-25},
	publisher = {arXiv},
	author = {Halpern, Joseph Y. and Pass, Rafael and Seeman, Lior},
	month = aug,
	year = {2013},
	note = {arXiv:1308.3780 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
}

@article{conlisk_why_1996,
	title = {Why {Bounded} {Rationality}?},
	volume = {34},
	issn = {0022-0515},
	url = {https://www.jstor.org/stable/2729218},
	number = {2},
	urldate = {2022-07-25},
	journal = {Journal of Economic Literature},
	author = {Conlisk, John},
	year = {1996},
	note = {Publisher: American Economic Association},
	pages = {669--700},
}

@article{rabin_psychology_1998,
	title = {Psychology and {Economics}},
	volume = {36},
	issn = {0022-0515},
	url = {https://www.jstor.org/stable/2564950},
	number = {1},
	urldate = {2022-07-25},
	journal = {Journal of Economic Literature},
	author = {Rabin, Matthew},
	year = {1998},
	note = {Publisher: American Economic Association},
	pages = {11--46},
}

@article{harre_information_2021,
	title = {Information {Theory} for {Agents} in {Artificial} {Intelligence}, {Psychology}, and {Economics}},
	volume = {23},
	issn = {1099-4300},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8001993/},
	doi = {10.3390/e23030310},
	abstract = {This review looks at some of the central relationships between artificial intelligence, psychology, and economics through the lens of information theory, specifically focusing on formal models of decision-theory. In doing so we look at a particular approach that each field has adopted and how information theory has informed the development of the ideas of each field. A key theme is expected utility theory, its connection to information theory, the Bayesian approach to decision-making and forms of (bounded) rationality. What emerges from this review is a broadly unified formal perspective derived from three very different starting points that reflect the unique principles of each field. Each of the three approaches reviewed can, in principle at least, be implemented in a computational model in such a way that, with sufficient computational power, they could be compared with human abilities in complex tasks. However, a central critique that can be applied to all three approaches was first put forward by Savage in The Foundations of Statistics and recently brought to the fore by the economist Binmore: Bayesian approaches to decision-making work in what Savage called ‘small worlds’ but cannot work in ‘large worlds’. This point, in various different guises, is central to some of the current debates about the power of artificial intelligence and its relationship to human-like learning and decision-making. Recent work on artificial intelligence has gone some way to bridging this gap but significant questions remain to be answered in all three fields in order to make progress in producing realistic models of human decision-making in the real world in which we live in.},
	number = {3},
	urldate = {2022-07-24},
	journal = {Entropy},
	author = {Harré, Michael S.},
	month = mar,
	year = {2021},
	pmid = {33800724},
	pmcid = {PMC8001993},
	pages = {310},
}

@incollection{jumarie_critical_1990,
	address = {Berlin, Heidelberg},
	series = {Springer {Series} in {Synergetics}},
	title = {A {Critical} {Review} of {Shannon} {Information} {Theory}},
	isbn = {978-3-642-84017-3},
	url = {https://doi.org/10.1007/978-3-642-84017-3_3},
	abstract = {In the preceding chapter, we summarized the basic elements of information theory, and we now proceed to examine and analyze the main characteristics of this theory. The term “critical” in the title of the chapter implies simply that we shall present a review of the main features for and against the theory. To support the theory in its present form, one can mention Shannon results on the capacity of a channel, the Boltzmann equation, and the fact that one can prove the central limit theorem in probability by using the properties of entropy only. Against the present form of the theory we have the apparent discrepancy between discrete entropy and continuous entropy, the absence of a concept of negative information to describe information lost, and the fact that the model does not take explicitly into account syntax and semantics. In the present chapter, we shall review these features and one of our conclusions will be as follows: Contrary to what some scientists are inclined to believe, we maintain that the continuous entropy is soundly defined, and that it merely remains to exhibit the differences in physical nature between discrete entropy and continuous entropy.},
	language = {en},
	urldate = {2022-07-25},
	booktitle = {Relative {Information}: {Theories} and {Applications}},
	publisher = {Springer},
	author = {Jumarie, Guy},
	editor = {Jumarie, Guy},
	year = {1990},
	doi = {10.1007/978-3-642-84017-3_3},
	keywords = {Boltzmann Equation, Central Limit Theorem, Negative Information, Shannon Entropy, Transmission Error},
	pages = {44--65},
}

@article{rivoire_informations_2016,
	title = {Informations in {Models} of {Evolutionary} {Dynamics}},
	volume = {162},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-015-1381-z},
	doi = {10.1007/s10955-015-1381-z},
	abstract = {Biological organisms adapt to changes by processing informations from different sources, most notably from their ancestors and from their environment. We review an approach to quantify these informations by analyzing mathematical models of evolutionary dynamics and show how explicit results are obtained for a solvable subclass of these models. In several limits, the results coincide with those obtained in studies of information processing for communication, gambling or thermodynamics. In the most general case, however, information processing by biological populations shows unique features that motivate the analysis of speciﬁc models.},
	language = {en},
	number = {5},
	urldate = {2022-07-25},
	journal = {Journal of Statistical Physics},
	author = {Rivoire, Olivier},
	month = mar,
	year = {2016},
	pages = {1324--1352},
}

@misc{noauthor_information_2022,
	title = {Information economics},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Information_economics&oldid=1095456816},
	abstract = {Information economics or the economics of information is the branch of microeconomics that studies how information and information systems affect an economy and economic decisions.One application considers information embodied in certain types of commodities that are "expensive to produce but cheap to reproduce." Examples include computer software (e.g., Microsoft Windows), pharmaceuticals, and technical books.  Once information is recorded "on paper, in a computer, or on a compact disc, it can be reproduced and used by a second person essentially for free."  Without the basic research, initial production of high-information commodities may be too unprofitable to market, a type of market failure. Government subsidization of basic research has been suggested as a way to mitigate the problem.The subject of "information economics" is treated under Journal of Economic Literature classification code JEL D8 – Information, Knowledge, and Uncertainty. The present article reflects topics included in that code. There are several subfields of information economics. Information as signal has been described as a kind of negative measure of uncertainty. It includes complete and scientific knowledge as special cases. The first insights in information economics related to the economics of information goods.
In recent decades, there have been influential advances in the study of information asymmetries and their implications for contract theory, including market failure as a possibility.Information economics is formally related to game theory as two different types of games that may apply, including games with perfect information, complete information, and incomplete information. Experimental and game-theory methods have been developed to model and test theories of information economics, including potential public-policy applications such as mechanism design to elicit information-sharing and otherwise welfare-enhancing behavior.An example of game theory in practice would be if two potential employees are going for the same promotion at work and are conversing with their employee about the job. However, one employee may have more information about what the role would entail then the other. Whilst the less informed employee may be willing to accept a lower pay rise for the new job, the other may have more knowledge on what the role's hours and commitment would take and would expect a higher pay. This is a clear use of incomplete information to give one person the advantage in a given scenario. If they talk about the promotion with each other in a process called colluding there may be the expectation that both will have equally informed knowledge about the job. However the employee with more information may mis-inform the other one about the value of the job for the work that is involved and make the promotion appear less appealing and hence not worth it. This brings into action the incentives behind information economics and highlights non-cooperative games.},
	language = {en},
	urldate = {2022-07-25},
	journal = {Wikipedia},
	month = jun,
	year = {2022},
	note = {Page Version ID: 1095456816},
}

@article{tops_information_nodate,
	title = {Information {Theoretical} {Optimization} {Techniques}},
	language = {en},
	author = {Tops, Flemming},
	pages = {20},
}

@article{merhav_optimum_nodate,
	title = {On {Optimum} {Strategies} for {Minimizing} the {Exponential} {Moments} of a {Loss} {Function}},
	abstract = {We consider a general problem of ﬁnding a strategy that minimizes the exponential moment of a given cost function, with an emphasis on its relation to the more common criterion of minimization the expectation of the ﬁrst moment of the same cost function. In particular, the basic observation that we make and use is about simple suﬃcient conditions for a strategy to be optimum in the exponential moment sense. This observation may be useful in various situations, and application examples are given. We also examine the asymptotic regime and investigate universal asymptotically optimum strategies in light of the aforementioned suﬃcient conditions, as well as phenomena of irregularities, or phase transitions, in the behavior of the asymptotic performance, which can be viewed and understood from a statistical–mechanical perspective. Finally, we propose a new route for deriving lower bounds on exponential moments of certain cost functions (like the square error in estimation problems) on the basis of well known lower bounds on their expectations.},
	language = {en},
	author = {Merhav, Neri},
	pages = {31},
}

@misc{mraginsky_information_2012,
	title = {Information theory in economics, {Part} {I}: {Rational} inattention},
	shorttitle = {Information theory in economics, {Part} {I}},
	url = {https://infostructuralist.wordpress.com/2012/06/01/information-theory-in-economics-part-i-rational-inattention/},
	abstract = {Economic activity involves making decisions. In order to make decisions, agents need information. Thus, the problem of acquisition, transmission, and uses of information has been occupying the econ…},
	language = {en},
	urldate = {2022-07-25},
	journal = {The Information Structuralist},
	author = {mraginsky, Author},
	month = jun,
	year = {2012},
}

@book{noauthor_theory_nodate,
	title = {Theory of {Information} and its {Value}},
	url = {https://link.springer.com/book/10.1007/978-3-030-22833-0},
	abstract = {This book is an English version of R.L. Stratonovich’s Theory of Information. Unifying theories of information, optimization, and statistical physics, the value of information theory has gained recognition in data science, machine learning, and artificial intelligence.},
	language = {en},
	urldate = {2022-07-25},
}

@article{wiener_cybernetics_1961,
	title = {Cybernetics: {Control} and {Communication} in the {Animal} and the {Machine}–2nd},
	author = {Wiener, Norbert},
	year = {1961},
	note = {Publisher: CUMINCAD},
}

@inproceedings{knuth_information_2011,
	title = {Information {Physics}: {The} {New} {Frontier}},
	shorttitle = {Information {Physics}},
	url = {http://arxiv.org/abs/1009.5161},
	doi = {10.1063/1.3573644},
	abstract = {At this point in time, two major areas of physics, statistical mechanics and quantum mechanics, rest on the foundations of probability and entropy. The last century saw several significant fundamental advances in our understanding of the process of inference, which make it clear that these are inferential theories. That is, rather than being a description of the behavior of the universe, these theories describe how observers can make optimal predictions about the universe. In such a picture, information plays a critical role. What is more is that little clues, such as the fact that black holes have entropy, continue to suggest that information is fundamental to physics in general. In the last decade, our fundamental understanding of probability theory has led to a Bayesian revolution. In addition, we have come to recognize that the foundations go far deeper and that Cox's approach of generalizing a Boolean algebra to a probability calculus is the first specific example of the more fundamental idea of assigning valuations to partially-ordered sets. By considering this as a natural way to introduce quantification to the more fundamental notion of ordering, one obtains an entirely new way of deriving physical laws. I will introduce this new way of thinking by demonstrating how one can quantify partially-ordered sets and, in the process, derive physical laws. The implication is that physical law does not reflect the order in the universe, instead it is derived from the order imposed by our description of the universe. Information physics, which is based on understanding the ways in which we both quantify and process information about the world around us, is a fundamentally new approach to science.},
	urldate = {2022-07-25},
	author = {Knuth, Kevin H.},
	year = {2011},
	note = {arXiv:1009.5161 [cond-mat, physics:math-ph]},
	keywords = {Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Mathematical Physics},
	pages = {3--19},
}

@article{perdigao_debates_2020,
	title = {Debates: {Does} {Information} {Theory} {Provide} a {New} {Paradigm} for {Earth} {Science}? {Emerging} {Concepts} and {Pathways} of {Information} {Physics}},
	volume = {56},
	issn = {1944-7973},
	shorttitle = {Debates},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2019WR025270},
	doi = {10.1029/2019WR025270},
	abstract = {Entropy and Information are key concepts not only in Information Theory but also in Physics: historically in the fields of Thermodynamics, Statistical and Analytical Mechanics, and, more recently, in the field of Information Physics. In this paper we argue that Information Physics reconciles and generalizes statistical, geometric, and mechanistic views on information. We start by demonstrating how the use and interpretation of Entropy and Information coincide in Information Theory, Statistical Thermodynamics, and Analytical Mechanics, and how this can be taken advantage of when addressing Earth Science problems in general and hydrological problems in particular. In the second part we discuss how Information Physics provides ways to quantify Information and Entropy from fundamental physical principles. This extends their use to cases where the preconditions to calculate Entropy in the classical manner as an aggregate statistical measure are not met. Indeed, these preconditions are rarely met in the Earth Sciences due either to limited observations or the far-from-equilibrium nature of evolving systems. Information Physics therefore offers new opportunities for improving the treatment of Earth Science problems.},
	language = {en},
	number = {2},
	urldate = {2022-07-25},
	journal = {Water Resources Research},
	author = {Perdigão, Rui A.P. and Ehret, Uwe and Knuth, Kevin H. and Wang, Jingfeng},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2019WR025270},
	keywords = {complex systems, entropy, information physics, information theory, kinematic geometry, thermodynamics},
	pages = {e2019WR025270},
}

@book{dretske_knowledge_1981,
	address = {Cambridge, MA, USA},
	title = {Knowledge and the {Flow} of {Information}},
	isbn = {978-0-262-04063-1},
	language = {en},
	publisher = {MIT Press},
	author = {Dretske, Fred},
	month = may,
	year = {1981},
}

@book{harris_theory_1991,
	title = {A {Theory} of {Language} and {Information}: {A} {Mathematical} {Approach}},
	isbn = {978-0-19-824224-6},
	shorttitle = {A {Theory} of {Language} and {Information}},
	abstract = {Written by one of the most respected figures in American linguistics, this book develops an approach to the analysis of language on a mathematical model. Harris presents a formal theory of language structure, in which syntax is characterized as an orderly system of departure from random combinings of sounds, words, and all the elements of language. He argues that the combining of words in a sentence constitutes a mathematical object, and that each departure from randomness is a contribution to the structure and meaning of a sentence. Discussing the differences in the structure and content of language, mathematics, and music, Harris shows that the use of language in a science constitutes a distinguishable sub-language. Remarkable and compelling, Harris's magnum opus will be considered the classical analysis of the structuring of information and development of language.},
	language = {en},
	publisher = {Clarendon Press},
	author = {Harris, Zellig Sabbettai and Harris, Senior Research Scientist Center for the Social Sciences Columbia University {and} Professor Emeritus Zellig},
	year = {1991},
	note = {Google-Books-ID: ij1sAAAAIAAJ},
}

@article{shannon_bandwagon_1956,
	title = {The bandwagon ({Edtl}.)},
	volume = {2},
	issn = {2168-2712},
	doi = {10.1109/TIT.1956.1056774},
	number = {1},
	journal = {IRE Transactions on Information Theory},
	author = {Shannon, C.},
	month = mar,
	year = {1956},
	note = {Conference Name: IRE Transactions on Information Theory},
	keywords = {Automation, Cybernetics, Decoding, Humans, Information theory, Maintenance engineering, Mathematics, Psychology, Research and development, Testing},
	pages = {3--3},
}

@article{maasoumi_1_compendium_1993,
	title = {A compendium to information theory in economics and econometrics},
	volume = {12},
	issn = {0747-4938},
	url = {https://www.tandfonline.com/doi/abs/10.1080/07474939308800260},
	doi = {10.1080/07474939308800260},
	abstract = {An extensive synthesis is provided of the concepts, measures and techniques of Information Theory (IT). After an axiomatic description of the basic definitions of “information functions”, “entropy” or uncertainty and the maximum entropy principle, the paper demonstrates the power of IT as both an interpretive and techinically productive tool. It is argued that this power and universality is promarily due to the common need for (i) measures of distance and discrimination and, (ii) appropriate partitioning- aggregation properties. IT offers a very suggestive unification for a bewildering and arbitrary set of approaches that have evolved in different disciplines. Applications are discussed or indicated. These applications have relevance to economics, finance, industrial organization, marketing, statistical ingerence and model selection, political science and communication. A main focus of the discussion is the generative power of IT measures in statistical examinations of unknown distributions and random phenomena. Measures of concentration and inequality, aggregation functions and index numbers, tests of nested and non\_nested hypotheses, and measures of volatility, movility and divergence are presented. Extending the author's previous work, estimation of unknown regression functions, densities and score functions is examined based on the maximum entropy principle. Some empirical examples are cited.},
	number = {2},
	urldate = {2022-07-24},
	journal = {Econometric Reviews},
	author = {Maasoumi 1, Esfandiar},
	month = jan,
	year = {1993},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/07474939308800260},
	keywords = {Information theory, MLE, adaptive estimation, aggregation, distance functions, entropy, inequality, nonparametrics, tests, uncertainty},
	pages = {137--181},
}

@article{adami_information_2004,
	title = {Information theory in molecular biology},
	volume = {1},
	issn = {1571-0645},
	url = {https://www.sciencedirect.com/science/article/pii/S157106450400003X},
	doi = {10.1016/j.plrev.2004.01.002},
	abstract = {This article introduces the physics of information in the context of molecular biology and genomics. Entropy and information, the two central concepts of Shannon's theory of information and communication, are often confused with each other but play transparent roles when applied to statistical ensembles (i.e., identically prepared sets) of symbolic sequences. Such an approach can distinguish between entropy and information in genes, predict the secondary structure of ribozymes, and detect the covariation between residues in folded proteins. We also review applications to molecular sequence and structure analysis, and introduce new tools in the characterization of resistance mutations, and in drug design.},
	language = {en},
	number = {1},
	urldate = {2022-07-24},
	journal = {Physics of Life Reviews},
	author = {Adami, Christoph},
	month = apr,
	year = {2004},
	pages = {3--22},
}

@article{dimitrov_information_2011,
	title = {Information theory in neuroscience},
	volume = {30},
	issn = {0929-5313},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3736735/},
	doi = {10.1007/s10827-011-0314-3},
	number = {1},
	urldate = {2022-07-24},
	journal = {Journal of computational neuroscience},
	author = {Dimitrov, Alexander G. and Lazar, Aurel A. and Victor, Jonathan D.},
	month = feb,
	year = {2011},
	pmid = {21279429},
	pmcid = {PMC3736735},
	pages = {1--5},
}

@inproceedings{schroeder_difference_2017,
	title = {The {Difference} that {Makes} a {Difference} for the {Conceptualization} of {Information}},
	url = {https://www.mdpi.com/2504-3900/1/3/221},
	doi = {10.3390/IS4SI-2017-04043},
	abstract = {Information is a subject of multiple efforts of conceptualization leading to controversies. Not frequently sufficient effort is made to formulate the concept of information in a way leading to its formal mathematical theory. Discussions of conceptualizations of information usually are focusing on the articulation of definitions, but not on their consequences for theoretical studies. This paper compares two conceptualizations of information exploring their mathematical theories. One of these concepts and its mathematical theory were introduced in earlier publications of the author. Information was defined in terms of the opposition of one and many and its theory was formulated in terms of closure spaces. The other concept of information was formulated in a rather open-ended way by Bateson as “any difference that makes a difference”. There are some similarities between Bateson’s concept of information and that of MacKay. In this paper a mathematical theory is formulated for this alternative approach to information founded on the concept of a difference in terms of generalized orthogonality relation. Finally, the mathematical formalisms for both approaches are compared and related. In conclusion of that comparison the approach to information founded on the concept of difference is a special case for the approach based on one-and-many opposition.},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {Proceedings of the {IS4SI} 2017 {Summit} {DIGITALISATION} {FOR} {A} {SUSTAINABLE} {SOCIETY}, {Gothenburg}, {Sweden}, 12–16 {June} 2017.},
	publisher = {MDPI},
	author = {Schroeder, Marcin J.},
	month = jun,
	year = {2017},
	pages = {221},
}

@article{floridi_what_2002,
	title = {{WHAT} {IS} {THE} {PHILOSOPHY} {OF} {INFORMATION}?},
	volume = {33},
	issn = {0026-1068},
	url = {https://www.jstor.org/stable/24439320},
	abstract = {Computational and information-theoretic research in philosophy has become increasingly fertile and pervasive, giving rise to a wealth of interesting results. In consequence, a new and vitally important field has emerged, the philosophy of information (PI). This essay is the first attempt to analyse the nature of PI systematically. PI is defined as the philosophical field concerned with the critical investigation of the conceptual nature and basic principles of information, including its dynamics, utilisation, and sciences, and the elaboration and application of information-theoretic and computational methodologies to philosophical problems. I argue that PI is a mature discipline for three reasons: it represents an autonomous field of research; it provides an innovative approach to both traditional and new philosophical topics; and it can stand beside other branches of philosophy, offering a systematic treatment of the conceptual foundations of the world of information and the information society.},
	number = {1/2},
	urldate = {2022-07-24},
	journal = {Metaphilosophy},
	author = {FLORIDI, LUCIANO},
	year = {2002},
	note = {Publisher: Wiley},
	pages = {123--145},
}

@article{degroot_changes_1984,
	title = {Changes in {Utility} as {Information}},
	volume = {17},
	issn = {0040-5833},
	url = {https://www.proquest.com/docview/1303208810/citation/D164F76E2A084283PQ/1},
	language = {English},
	number = {3},
	urldate = {2022-07-24},
	journal = {Theory and Decision},
	author = {DeGroot, Morris H.},
	month = nov,
	year = {1984},
	note = {Num Pages: 17
Place: Dordrecht, Netherlands
Publisher: Kluwer Academic Publishers},
	keywords = {Social Sciences: Comprehensive Works},
	pages = {287--303},
}

@article{raiffa_applied_nodate,
	title = {Applied {Statistical} {Decision} {Theory}},
	author = {Raiffa, Howard and Schlaifer, Robert},
	pages = {395},
}

@incollection{degroot_concepts_1986,
	address = {Dordrecht},
	series = {Theory and {Decision} {Library}},
	title = {Concepts of {Information} {Based} on {Utility}},
	isbn = {978-94-009-4616-3},
	url = {https://doi.org/10.1007/978-94-009-4616-3_17},
	abstract = {The central topic of this paper is the measurement of the amount of information about some parameter ö that is present in a set of data X. The parameter ö can be any quantity such that a decision maker (DM) is uncertain about its value. We follow a Bayesian approach and assume that the DM can represent his uncertainty at any stage of the learning process in terms of a subjective probability distribution over the parameter space Ω of all possible values of ö. This distribution, in turn, will be represented by a generalized probability density function (gpdf) ξ with respect to some fixed σ-finite measure λ on Ω.},
	language = {en},
	urldate = {2022-07-24},
	booktitle = {Recent {Developments} in the {Foundations} of {Utility} and {Risk} {Theory}},
	publisher = {Springer Netherlands},
	author = {DeGroot, Morris H.},
	editor = {Daboni, L. and Montesano, A. and Lines, M.},
	year = {1986},
	doi = {10.1007/978-94-009-4616-3_17},
	pages = {265--275},
}

@article{hirshleifer_analytics_1979,
	title = {The {Analytics} of {Uncertainty} and {Information}-{An} {Expository} {Survey}},
	volume = {17},
	issn = {0022-0515},
	url = {https://www.jstor.org/stable/2723720},
	number = {4},
	urldate = {2022-07-24},
	journal = {Journal of Economic Literature},
	author = {Hirshleifer, J. and Riley, John G.},
	year = {1979},
	note = {Publisher: American Economic Association},
	pages = {1375--1421},
}

@article{artandi_information_1973,
	title = {Information concepts and their utility},
	volume = {24},
	issn = {1097-4571},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asi.4630240403},
	doi = {10.1002/asi.4630240403},
	abstract = {The concept of information is examined within the framework of the Mathematical Theory of Communication and semiotics, the study of signs and sign systems. The implications of these theories for the better understanding of information as we deal with this concept in the context of information systems are discussed.},
	language = {en},
	number = {4},
	urldate = {2022-07-24},
	journal = {Journal of the American Society for Information Science},
	author = {Artandi, Susan},
	year = {1973},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asi.4630240403},
	pages = {242--245},
}

@article{losee_discipline_1997,
	title = {A discipline independent definition of information},
	volume = {48},
	issn = {1097-4571},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-4571%28199703%2948%3A3%3C254%3A%3AAID-ASI6%3E3.0.CO%3B2-W},
	doi = {10.1002/(SICI)1097-4571(199703)48:3<254::AID-ASI6>3.0.CO;2-W},
	abstract = {Information may be defined as the characteristics of the output of a process, these being informative about the process and the input. This discipline independent definition may be applied to all domains, from physics to epistemology. Hierarchies of processes, linked together, provide a communication channel between each of the corresponding functions and layers in the hierarchies. Models of communication (Shannon), perception, observation, belief, and knowledge are suggested that are consistent with this conceptual framework of information as the value of the output of any process in a hierarchy of processes. Misinformation and errors are considered. © 1997 John Wiley \& Sons, Inc.},
	language = {en},
	number = {3},
	urldate = {2022-07-24},
	journal = {Journal of the American Society for Information Science},
	author = {Losee, Robert M.},
	year = {1997},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/\%28SICI\%291097-4571\%28199703\%2948\%3A3\%3C254\%3A\%3AAID-ASI6\%3E3.0.CO\%3B2-W},
	pages = {254--269},
}

@book{noauthor_economic_nodate,
	title = {Economic {Information}, {Decision}, and {Prediction}},
	url = {https://link.springer.com/book/10.1007/978-94-010-9278-4},
	language = {en},
	urldate = {2022-07-23},
}

@misc{noauthor_theory_nodate-1,
	title = {Theory of {Information}},
	url = {https://www.worldscientific.com/doi/epdf/10.1142/7048},
	language = {en},
	urldate = {2022-07-23},
	doi = {10.1142/7048},
}

@book{burgin_theory_2009,
	title = {Theory of {Information}: {Fundamentality}, {Diversity} and {Unification}},
	isbn = {978-981-283-548-2 978-981-283-549-9},
	shorttitle = {Theory of {Information}},
	url = {http://www.worldscientific.com/worldscibooks/10.1142/7048},
	language = {en},
	urldate = {2022-07-23},
	publisher = {WORLD SCIENTIFIC},
	author = {Burgin, Mark},
	month = dec,
	year = {2009},
	doi = {10.1142/7048},
}

@article{logan_what_2012,
	title = {What {Is} {Information}?: {Why} {Is} {It} {Relativistic} and {What} {Is} {Its} {Relationship} to {Materiality}, {Meaning} and {Organization}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2078-2489},
	shorttitle = {What {Is} {Information}?},
	url = {https://www.mdpi.com/2078-2489/3/1/68},
	doi = {10.3390/info3010068},
	abstract = {We review the historic development of concept of information including the relationship of Shannon information and entropy and the criticism of Shannon information because of its lack of a connection to meaning. We review the work of Kauffman, Logan et al. that shows that Shannon information fails to describe biotic information. We introduce the notion of the relativity of information and show that the concept of information depends on the context of where and how it is being used. We examine the relationship of information to meaning and materiality within information theory, cybernetics and systems biology. We show there exists a link between information and organization in biotic systems and in the various aspects of human culture including language, technology, science, economics and governance.},
	language = {en},
	number = {1},
	urldate = {2022-07-23},
	journal = {Information},
	author = {Logan, Robert K.},
	month = mar,
	year = {2012},
	note = {Number: 1
Publisher: Molecular Diversity Preservation International},
	keywords = {Shannon, biology, information, language, meaning, organization, relativity},
	pages = {68--91},
}

@article{mac_kay_information_1969,
	title = {Information, mechanism and meaning.},
	author = {Mac Kay, Donald M},
	year = {1969},
	note = {Publisher: ERIC},
}

@misc{meister_typical_2022,
	title = {Typical {Decoding} for {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2202.00666},
	abstract = {Despite achieving incredibly low perplexities on myriad natural language corpora, today's language models still often underperform when used to generate text. This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language as a communication channel ({\textbackslash}`a la Shannon, 1948) can provide new insights into the behaviors of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, and do so in a simultaneously efficient and error-minimizing manner; they choose each word in a string with this (perhaps subconscious) goal in mind. We propose that generation from probabilistic models should mimic this behavior. Rather than always choosing words from the high-probability region of the distribution--which have a low Shannon information content--we sample from the set of words with information content close to the conditional entropy of our model, i.e., close to the expected information content. This decision criterion can be realized through a simple and efficient implementation, which we call typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance in terms of quality while consistently reducing the number of degenerate repetitions.},
	urldate = {2022-07-22},
	publisher = {arXiv},
	author = {Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2202.00666 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{rivoire_informations_2016-1,
	title = {Informations in {Models} of {Evolutionary} {Dynamics}},
	volume = {162},
	issn = {0022-4715, 1572-9613},
	url = {http://link.springer.com/10.1007/s10955-015-1381-z},
	doi = {10.1007/s10955-015-1381-z},
	abstract = {Biological organisms adapt to changes by processing informations from different sources, most notably from their ancestors and from their environment. We review an approach to quantify these informations by analyzing mathematical models of evolutionary dynamics and show how explicit results are obtained for a solvable subclass of these models. In several limits, the results coincide with those obtained in studies of information processing for communication, gambling or thermodynamics. In the most general case, however, information processing by biological populations shows unique features that motivate the analysis of speciﬁc models.},
	language = {en},
	number = {5},
	urldate = {2022-07-22},
	journal = {Journal of Statistical Physics},
	author = {Rivoire, Olivier},
	month = mar,
	year = {2016},
	pages = {1324--1352},
}

@misc{mraginsky_information_2022,
	title = {Information and {Control} in {Biology}, {Part} 1: {Preliminary} {Considerations}},
	shorttitle = {Information and {Control} in {Biology}, {Part} 1},
	url = {https://infostructuralist.wordpress.com/2022/01/11/information-and-control-in-biology-part-1-preliminary-considerations/},
	abstract = {Disclaimer: I am not a biologist, but I have become interested in biology and related matters over the past couple of years. One reason is obviously the pandemic, so the talk of biology, viruses, m…},
	language = {en},
	urldate = {2022-07-22},
	journal = {The Information Structuralist},
	author = {mraginsky, Author},
	month = jan,
	year = {2022},
}

@misc{john_how_2015,
	title = {How informative is the concept of biological information?},
	url = {https://3quarksdaily.com/3quarksdaily/2015/05/how-informative-is-the-concept-of-biological-information.html},
	abstract = {by Yohan J. John We are routinely told that we live in a brave new Information Age. Every aspect of human life — commerce, entertainment, education, and perhaps even the shape of consciousness itself — seems to be undergoing an information-driven revolution. The tools for storing and sharing information are becoming faster, more ubiquitous, and…},
	language = {en-US},
	urldate = {2022-07-22},
	journal = {3 Quarks Daily},
	author = {John, Yohan},
	month = may,
	year = {2015},
}

@misc{wen_mechanism_2022,
	title = {The {Mechanism} of {Prediction} {Head} in {Non}-contrastive {Self}-supervised {Learning}},
	url = {http://arxiv.org/abs/2205.06226},
	doi = {10.48550/arXiv.2205.06226},
	abstract = {Recently the surprising discovery of the Bootstrap Your Own Latent (BYOL) method by Grill et al. shows the negative term in contrastive loss can be removed if we add the so-called prediction head to the network. This initiated the research of non-contrastive self-supervised learning. It is mysterious why even when there exist trivial collapsed global optimal solutions, neural networks trained by (stochastic) gradient descent can still learn competitive representations. This phenomenon is a typical example of implicit bias in deep learning and remains little understood. In this work, we present our empirical and theoretical discoveries on non-contrastive self-supervised learning. Empirically, we find that when the prediction head is initialized as an identity matrix with only its off-diagonal entries being trainable, the network can learn competitive representations even though the trivial optima still exist in the training objective. Theoretically, we present a framework to understand the behavior of the trainable, but identity-initialized prediction head. Under a simple setting, we characterized the substitution effect and acceleration effect of the prediction head. The substitution effect happens when learning the stronger features in some neurons can substitute for learning these features in other neurons through updating the prediction head. And the acceleration effect happens when the substituted features can accelerate the learning of other weaker features to prevent them from being ignored. These two effects enable the neural networks to learn all the features rather than focus only on learning the stronger features, which is likely the cause of the dimensional collapse phenomenon. To the best of our knowledge, this is also the first end-to-end optimization guarantee for non-contrastive methods using nonlinear neural networks with a trainable prediction head and normalization.},
	urldate = {2022-07-22},
	publisher = {arXiv},
	author = {Wen, Zixin and Li, Yuanzhi},
	month = may,
	year = {2022},
	note = {arXiv:2205.06226 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hewitt_conditional_2021,
	title = {Conditional probing: measuring usable information beyond a baseline},
	shorttitle = {Conditional probing},
	url = {http://arxiv.org/abs/2109.09234},
	doi = {10.48550/arXiv.2109.09234},
	abstract = {Probing experiments investigate the extent to which neural representations make properties -- like part-of-speech -- predictable. One suggests that a representation encodes a property if probing that representation produces higher accuracy than probing a baseline representation like non-contextual word embeddings. Instead of using baselines as a point of comparison, we're interested in measuring information that is contained in the representation but not in the baseline. For example, current methods can detect when a representation is more useful than the word identity (a baseline) for predicting part-of-speech; however, they cannot detect when the representation is predictive of just the aspects of part-of-speech not explainable by the word identity. In this work, we extend a theory of usable information called \${\textbackslash}mathcal\{V\}\$-information and propose conditional probing, which explicitly conditions on the information in the baseline. In a case study, we find that after conditioning on non-contextual word embeddings, properties like part-of-speech are accessible at deeper layers of a network than previously thought.},
	urldate = {2022-07-22},
	publisher = {arXiv},
	author = {Hewitt, John and Ethayarajh, Kawin and Liang, Percy and Manning, Christopher D.},
	month = sep,
	year = {2021},
	note = {arXiv:2109.09234 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{ethayarajh_understanding_2022,
	title = {Understanding {Dataset} {Difficulty} with \${\textbackslash}mathcal\{{V}\}\$-{Usable} {Information}},
	url = {https://proceedings.mlr.press/v162/ethayarajh22a.html},
	abstract = {Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty—w.r.t. a model \${\textbackslash}mathcal\{V\}\$—as the lack of \${\textbackslash}mathcal\{V\}\$-usable information (Xu et al., 2019), where a lower value indicates a more difficult dataset for \${\textbackslash}mathcal\{V\}\$. We further introduce pointwise \${\textbackslash}mathcal\{V\}\$-information (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, \${\textbackslash}mathcal\{V\}\$-usable information and PVI also permit the converse: for a given model \${\textbackslash}mathcal\{V\}\$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.},
	language = {en},
	urldate = {2022-07-22},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {5988--6008},
}

@misc{bergstrom_transmission_2008,
	title = {The transmission sense of information},
	url = {http://arxiv.org/abs/0810.4168},
	abstract = {Biologists rely heavily on the language of information, coding, and transmission that is commonplace in the field of information theory as developed by Claude Shannon, but there is open debate about whether such language is anything more than facile metaphor. Philosophers of biology have argued that when biologists talk about information in genes and in evolution, they are not talking about the sort of information that Shannon's theory addresses. First, philosophers have suggested that Shannon theory is only useful for developing a shallow notion of correlation, the so-called "causal sense" of information. Second they typically argue that in genetics and evolutionary biology, information language is used in a "semantic sense," whereas semantics are deliberately omitted from Shannon theory. Neither critique is well-founded. Here we propose an alternative to the causal and semantic senses of information: a transmission sense of information, in which an object X conveys information if the function of X is to reduce, by virtue of its sequence properties, uncertainty on the part of an agent who observes X. The transmission sense not only captures much of what biologists intend when they talk about information in genes, but also brings Shannon's theory back to the fore. By taking the viewpoint of a communications engineer and focusing on the decision problem of how information is to be packaged for transport, this approach resolves several problems that have plagued the information concept in biology, and highlights a number of important features of the way that information is encoded, stored, and transmitted as genetic sequence.},
	urldate = {2022-07-22},
	publisher = {arXiv},
	author = {Bergstrom, C. T. and Rosvall, M.},
	month = oct,
	year = {2008},
	note = {arXiv:0810.4168 [q-bio]},
	keywords = {Quantitative Biology - Genomics, Quantitative Biology - Populations and Evolution},
}

@incollection{cutsuridis_information_2011,
	address = {New York, NY},
	title = {Information {Theory} of {Decisions} and {Actions}},
	isbn = {978-1-4419-1451-4 978-1-4419-1452-1},
	url = {http://link.springer.com/10.1007/978-1-4419-1452-1_19},
	abstract = {The perception-action cycle is often deﬁned as “the circular ﬂow of information between an organism and its environment in the course of a sensory guided sequence of actions towards a goal” (Fuster 2001, 2006). The question we address in this paper is in what sense this “ﬂow of information” can be described by Shannon’s measures of information introduced in his mathematical theory of communication. We provide an afﬁrmative answer to this question using an intriguing analogy between Shannon’s classical model of communication and the Perception-Action-Cycle. In particular, decision and action sequences turn out to be directly analogous to codes in communication, and their complexity — the minimal number of (binary) decisions required for reaching a goal — directly bounded by information measures, as in communication. This analogy allows us to extend the standard Reinforcement Learning framework. The latter considers the future expected reward in the course of a behaviour sequence towards a goal (value-to-go). Here, we additionally incorporate a measure of information associated with this sequence: the cumulated information processing cost or bandwidth required to specify the future decision and action sequence (information-to-go).},
	language = {en},
	urldate = {2022-07-21},
	booktitle = {Perception-{Action} {Cycle}},
	publisher = {Springer New York},
	author = {Tishby, Naftali and Polani, Daniel},
	editor = {Cutsuridis, Vassilis and Hussain, Amir and Taylor, John G.},
	year = {2011},
	doi = {10.1007/978-1-4419-1452-1_19},
	pages = {601--636},
}

@misc{wang_importance_2022,
	title = {On the {Importance} of {Asymmetry} for {Siamese} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2204.00613},
	abstract = {Many recent self-supervised frameworks for visual representation learning are based on certain forms of Siamese networks. Such networks are conceptually symmetric with two parallel encoders, but often practically asymmetric as numerous mechanisms are devised to break the symmetry. In this work, we conduct a formal study on the importance of asymmetry by explicitly distinguishing the two encoders within the network -- one produces source encodings and the other targets. Our key insight is keeping a relatively lower variance in target than source generally benefits learning. This is empirically justified by our results from five case studies covering different variance-oriented designs, and is aligned with our preliminary theoretical analysis on the baseline. Moreover, we find the improvements from asymmetric designs generalize well to longer training schedules, multiple other frameworks and newer backbones. Finally, the combined effect of several asymmetric designs achieves a state-of-the-art accuracy on ImageNet linear probing and competitive results on downstream transfer. We hope our exploration will inspire more research in exploiting asymmetry for Siamese representation learning.},
	urldate = {2022-07-20},
	publisher = {arXiv},
	author = {Wang, Xiao and Fan, Haoqi and Tian, Yuandong and Kihara, Daisuke and Chen, Xinlei},
	month = apr,
	year = {2022},
	note = {arXiv:2204.00613 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{han_contrastive_2022,
	title = {Contrastive {Principal} {Component} {Learning}: {Modeling} {Similarity} by {Augmentation} {Overlap}},
	shorttitle = {Contrastive {Principal} {Component} {Learning}},
	url = {http://arxiv.org/abs/2206.00471},
	abstract = {Traditional self-supervised contrastive learning methods learn embeddings by pulling views of the same sample together and pushing views of different samples away. Since views of a sample are usually generated via data augmentations, the semantic relationship between samples is ignored. Based on the observation that semantically similar samples are more likely to have similar augmentations, we propose to measure similarity via the distribution of augmentations, i.e., how much the augmentations of two samples overlap. To handle the dimensional and computational complexity, we propose a novel Contrastive Principal Component Learning (CPCL) method composed of a contrastive-like loss and an on-the-fly projection loss to efficiently perform PCA on the augmentation feature, which encodes the augmentation distribution. By CPCL, the learned low-dimensional embeddings theoretically preserve the similarity of augmentation distribution between samples. Empirical results show our method can achieve competitive results against various traditional contrastive learning methods on different benchmarks.},
	urldate = {2022-07-18},
	publisher = {arXiv},
	author = {Han, Lu and Ye, Han-Jia and Zhan, De-Chuan},
	month = jun,
	year = {2022},
	note = {arXiv:2206.00471 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{girosi_approximation_nodate,
	title = {Approximation {Error} and {Approximation} {Theory}},
	language = {en},
	author = {Girosi, Federico},
	pages = {37},
}

@article{barron_approximation_1994,
	title = {Approximation and estimation bounds for artificial neural networks},
	volume = {14},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/BF00993164},
	doi = {10.1007/BF00993164},
	abstract = {For a common class of artificial neural networks, the mean integrated squared error between the estimated network and a target functionf is shown to be bounded by\$\$O{\textbackslash}left( \{{\textbackslash}frac\{\{{\textbackslash}mathop c{\textbackslash}nolimits\_f{\textasciicircum}2 \}\}\{n\}\} {\textbackslash}right) + O{\textbackslash}left( \{{\textbackslash}frac\{\{nd\}\}\{N\}{\textbackslash}log  N\} {\textbackslash}right)\$\$wheren is the number of nodes,d is the input dimension of the function,N is the number of training observations, andCfis the first absolute moment of the Fourier magnitude distribution off. The two contributions to this total risk are the approximation error and the estimation error. Approximation error refers to the distance between the target function and the closest neural network function of a given architecture and estimation error refers to the distance between this ideal network function and an estimated network function. Withn ∼ Cf(N/(d logN))1/2 nodes, the order of the bound on the mean integrated squared error is optimized to beO(Cf((d/N) logN)1/2). The bound demonstrates surprisingly favorable properties of network estimation compared to traditional series and nonparametric curve estimation techniques in the case thatd is moderately large. Similar bounds are obtained when the number of nodesn is not preselected as a function ofCf(which is generally not knowna priori), but rather the number of nodes is optimized from the observed data by the use of a complexity regularization or minimum description length criterion. The analysis involves Fourier techniques for the approximation error, metric entropy considerations for the estimation error, and a calculation of the index of resolvability of minimum complexity estimation of the family of networks.},
	language = {en},
	number = {1},
	urldate = {2022-07-17},
	journal = {Machine Learning},
	author = {Barron, Andrew R.},
	month = jan,
	year = {1994},
	keywords = {Neural nets, approximation theory, complexity regularization, estimation theory, statistical risk},
	pages = {115--133},
}

@misc{duchi_multiclass_2017,
	title = {Multiclass {Classification}, {Information}, {Divergence}, and {Surrogate} {Risk}},
	url = {http://arxiv.org/abs/1603.00126},
	abstract = {We provide a unifying view of statistical information measures, multi-way Bayesian hypothesis testing, loss functions for multi-class classification problems, and multi-distribution \$f\$-divergences, elaborating equivalence results between all of these objects, and extending existing results for binary outcome spaces to more general ones. We consider a generalization of \$f\$-divergences to multiple distributions, and we provide a constructive equivalence between divergences, statistical information (in the sense of DeGroot), and losses for multiclass classification. A major application of our results is in multi-class classification problems in which we must both infer a discriminant function \${\textbackslash}gamma\$---for making predictions on a label \$Y\$ from datum \$X\$---and a data representation (or, in the setting of a hypothesis testing problem, an experimental design), represented as a quantizer \${\textbackslash}mathsf\{q\}\$ from a family of possible quantizers \${\textbackslash}mathsf\{Q\}\$. In this setting, we characterize the equivalence between loss functions, meaning that optimizing either of two losses yields an optimal discriminant and quantizer \${\textbackslash}mathsf\{q\}\$, complementing and extending earlier results of Nguyen et. al. to the multiclass case. Our results provide a more substantial basis than standard classification calibration results for comparing different losses: we describe the convex losses that are consistent for jointly choosing a data representation and minimizing the (weighted) probability of error in multiclass classification problems.},
	urldate = {2022-07-15},
	publisher = {arXiv},
	author = {Duchi, John C. and Khosravi, Khashayar and Ruan, Feng},
	month = sep,
	year = {2017},
	note = {arXiv:1603.00126 [cs, math, stat]},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
}

@misc{huang_towards_2022,
	title = {Towards the {Generalization} of {Contrastive} {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2111.00743},
	abstract = {Recently, self-supervised learning has attracted great attention, since it only requires unlabeled data for training. Contrastive learning is one popular method for self-supervised learning and has achieved promising empirical performance. However, the theoretical understanding of its generalization ability is still limited. To this end, we define a kind of \$({\textbackslash}sigma,{\textbackslash}delta)\$-measure to mathematically quantify the data augmentation, and then provide an upper bound of the downstream classification error based on the measure. We show that the generalization ability of contrastive self-supervised learning depends on three key factors: alignment of positive samples, divergence of class centers, and concentration of augmented data. The first two factors can be optimized by contrastive algorithms, while the third one is priorly determined by pre-defined data augmentation. With the above theoretical findings, we further study two canonical contrastive losses, InfoNCE and cross-correlation loss, and prove that both of them are indeed able to satisfy the first two factors. Moreover, we empirically verify the third factor by conducting various experiments on the real-world dataset, and show that our theoretical inferences on the relationship between the data augmentation and the generalization of contrastive self-supervised learning agree with the empirical observations.},
	urldate = {2022-07-11},
	publisher = {arXiv},
	author = {Huang, Weiran and Yi, Mingyang and Zhao, Xuyang},
	month = may,
	year = {2022},
	note = {arXiv:2111.00743 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{fathony_distributionally_2018,
	title = {Distributionally {Robust} {Graphical} {Models}},
	volume = {31},
	url = {https://proceedings.neurips.cc/paper/2018/hash/79a3308b13cd31f096d8a4a34f96b66b-Abstract.html},
	abstract = {In many structured prediction problems, complex relationships between variables are compactly defined using graphical structures. The most prevalent graphical prediction methods---probabilistic graphical models and large margin methods---have their own distinct strengths but also possess significant drawbacks. Conditional random fields (CRFs)  are Fisher consistent, but they do not permit integration of customized loss metrics into their learning process. Large-margin models, such as structured support vector machines (SSVMs), have the flexibility to incorporate customized loss metrics, but lack Fisher consistency guarantees. We present adversarial graphical models (AGM), a distributionally robust approach for constructing a predictor that performs robustly for a class of data distributions defined using a graphical structure. Our approach enjoys both the flexibility of incorporating customized loss metrics into its design as well as the statistical guarantee of Fisher consistency. We present exact learning and prediction algorithms for AGM with time complexity similar to existing graphical models and show the practical benefits of our approach with experiments.},
	urldate = {2022-07-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fathony, Rizal and Rezaei, Ashkan and Bashiri, Mohammad Ali and Zhang, Xinhua and Ziebart, Brian},
	year = {2018},
}

@misc{agarwal_reductions_2018,
	title = {A {Reductions} {Approach} to {Fair} {Classification}},
	url = {http://arxiv.org/abs/1803.02453},
	abstract = {We present a systematic approach for achieving fairness in a binary classification setting. While we focus on two well-known quantitative definitions of fairness, our approach encompasses many other previously studied definitions as special cases. The key idea is to reduce fair classification to a sequence of cost-sensitive classification problems, whose solutions yield a randomized classifier with the lowest (empirical) error subject to the desired constraints. We introduce two reductions that work for any representation of the cost-sensitive classifier and compare favorably to prior baselines on a variety of data sets, while overcoming several of their disadvantages.},
	urldate = {2022-07-10},
	publisher = {arXiv},
	author = {Agarwal, Alekh and Beygelzimer, Alina and Dudík, Miroslav and Langford, John and Wallach, Hanna},
	month = jul,
	year = {2018},
	note = {arXiv:1803.02453 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{chen_robust_2016,
	title = {Robust covariate shift regression},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Chen, Xiangli and Monfort, Mathew and Liu, Anqi and Ziebart, Brian D},
	year = {2016},
	pages = {1270--1279},
}

@article{chen_robust_nodate,
	title = {Robust {Covariate} {Shift} {Regression}},
	language = {en},
	author = {Chen, Xiangli and Monfort, Mathew and Liu, Anqi and Ziebart, Brian D},
	pages = {10},
}

@misc{weston_support_1999,
	title = {Support {Vector} {Machines} for {Multi}-{Class} {Pattern} {Recognition}},
	abstract = {. The solution of binary classification problems using support vector machines (SVMs) is well developed, but multi-class problems with more than two classes have typically been solved by combining independently produced binary classifiers. We propose a formulation of the SVM that enables a multi-class pattern recognition problem to be solved in a single optimisation. We also propose a similar generalization of linear programming machines. We report experiments using bench-mark datasets in which these two methods achieve a reduction in the number of support vectors and kernel calculations needed.},
	author = {Weston, J. and Watkins, C.},
	year = {1999},
}

@article{lee_multicategory_2004,
	title = {Multicategory {Support} {Vector} {Machines}: {Theory} and {Application} to the {Classification} of {Microarray} {Data} and {Satellite} {Radiance} {Data}},
	volume = {99},
	issn = {0162-1459, 1537-274X},
	shorttitle = {Multicategory {Support} {Vector} {Machines}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214504000000098},
	doi = {10.1198/016214504000000098},
	language = {en},
	number = {465},
	urldate = {2022-07-07},
	journal = {Journal of the American Statistical Association},
	author = {Lee, Yoonkyung and Lin, Yi and Wahba, Grace},
	month = mar,
	year = {2004},
	pages = {67--81},
}

@article{crammer_algorithmic_2001,
	title = {On the {Algorithmic} {Implementation} of {Multiclass} {Kernel}-based {Vector} {Machines}},
	volume = {2},
	issn = {ISSN 1533-7928},
	url = {https://jmlr.csail.mit.edu/papers/v2/crammer01a},
	abstract = {In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines.  Our starting point is a generalized notion of the margin to multiclass problems.  Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function.  Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors.  By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size.  We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence.  We then discuss technical details that yield significant running time improvements for large datasets.  Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods.  Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.},
	number = {Dec},
	urldate = {2022-07-07},
	journal = {Journal of Machine Learning Research},
	author = {Crammer, Koby and Singer, Yoram},
	year = {2001},
	pages = {265--292},
}

@misc{lugosi_mean_2019,
	title = {Mean estimation and regression under heavy-tailed distributions--a survey},
	url = {http://arxiv.org/abs/1906.04280},
	abstract = {We survey some of the recent advances in mean estimation and regression function estimation. In particular, we describe sub-Gaussian mean estimators for possibly heavy-tailed data both in the univariate and multivariate settings. We focus on estimators based on median-of-means techniques but other methods such as the trimmed mean and Catoni's estimator are also reviewed. We give detailed proofs for the cornerstone results. We dedicate a section on statistical learning problems--in particular, regression function estimation--in the presence of possibly heavy-tailed data.},
	urldate = {2022-07-07},
	publisher = {arXiv},
	author = {Lugosi, Gabor and Mendelson, Shahar},
	month = jun,
	year = {2019},
	note = {arXiv:1906.04280 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{tewari_author_nodate,
	title = {Author and {Co}-author {Contact} {Information}},
	abstract = {We present an overview of learning theory including its statistical and computational aspects. We start by giving a probabilistic formulation of learning problems including classiﬁcation and regression where the learner knows nothing about the probability distribution generating the data. We then consider the principle of empirical risk minimization (ERM) that chooses a function from a given class based on its performance on the observed data. Learning guarantees for ERM are shown to be intimately connected with the uniform law of large numbers. A uniform law of large numbers ensures that empirical means converge to true expectations uniformly over a function class. Tools such as Rademacher complexity, covering numbers, and combinatorial dimensions known as the Vapnik-Chervonenkis (VC) and fat shattering dimensions are introduced. After considering the case of learning using a ﬁxed function class, we turn to the problem of model selection: how to choose a function class from a family based on available data? We also survey alternative techniques for studying generalization ability of learning algorithms including sample compression, algorithmic stability, and the PAC-Bayesian theorem. After dealing with statistical issues, we study computational models of learning such the basic and agnostic PAC learning models, the statistical query, and the mistake bound model. Finally, we point out extensions of the basic theory beyond the probabilistic setting of a learner passively learning a single task from independent and identically distributed samples.},
	language = {en},
	author = {Tewari, Ambuj and Bartlett, Peter L},
	pages = {50},
}

@article{nelder_generalized_1972,
	title = {Generalized {Linear} {Models}},
	volume = {135},
	issn = {0035-9238},
	url = {https://www.jstor.org/stable/2344614},
	doi = {10.2307/2344614},
	abstract = {The technique of iterative weighted linear regression can be used to obtain maximum likelihood estimates of the parameters with observations distributed according to some exponential family and systematic effects that can be made linear by a suitable transformation. A generalization of the analysis of variance is given for these models using log-likelihoods. These generalized linear models are illustrated by examples relating to four distributions; the Normal, Binomial (probit analysis, etc.), Poisson (contingency tables) and gamma (variance components). The implications of the approach in designing statistics courses are discussed.},
	number = {3},
	urldate = {2022-07-05},
	journal = {Journal of the Royal Statistical Society. Series A (General)},
	author = {Nelder, J. A. and Wedderburn, R. W. M.},
	year = {1972},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {370--384},
}

@inproceedings{fathony_efficient_2018,
	title = {Efficient and {Consistent} {Adversarial} {Bipartite} {Matching}},
	url = {https://proceedings.mlr.press/v80/fathony18a.html},
	abstract = {Many important structured prediction problems, including learning to rank items, correspondence-based natural language processing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations. Existing structured prediction approaches have significant drawbacks when applied under the constraints of perfect bipartite matchings. Exponential family probabilistic models, such as the conditional random field (CRF), provide statistical consistency guarantees, but suffer computationally from the need to compute the normalization term of its distribution over matchings, which is a \#P-hard matrix permanent computation. In contrast, the structured support vector machine (SSVM) provides computational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the optimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable potential functions). We propose adversarial bipartite matching to avoid both of these limitations. We develop this approach algorithmically, establish its computational efficiency and Fisher consistency properties, and apply it to matching problems that demonstrate its empirical benefits.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Fathony, Rizal and Behpour, Sima and Zhang, Xinhua and Ziebart, Brian},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {1457--1466},
}

@misc{kivinen_boosting_1999,
	title = {Boosting as {Entropy} {Projection}},
	abstract = {We consider the AdaBoost procedure for boosting  weak learners. In AdaBoost, a key step is choosing  a new distribution on the training examples based  on the old distribution and the mistakes made by  the present weak hypothesis. We show how AdaBoost  's choice of the new distribution can be seen  as an approximate solution to the following problem:  Find a new distribution that is closest to the  old distribution subject to the constraint that the  new distribution is orthogonal to the vector of mistakes  of the current weak hypothesis. The distance  (or divergence) between distributions is measured  by the relative entropy. Alternatively, we could say  that AdaBoost approximately projects the distribution  vector onto a hyperplane dened by the mistake  vector. We show that this new view of AdaBoost  as an entropy projection is dual to the usual  view of AdaBoost as minimizing the normalization  factors of the updated distributions.},
	author = {Kivinen, Jyrki and Warmuth, Manfred K.},
	year = {1999},
}

@article{shen_dual_2010,
	title = {On the {Dual} {Formulation} of {Boosting} {Algorithms}},
	volume = {32},
	issn = {0162-8828},
	url = {http://arxiv.org/abs/0901.3590},
	doi = {10.1109/TPAMI.2010.47},
	abstract = {We study boosting algorithms from a new perspective. We show that the Lagrange dual problems of AdaBoost, LogitBoost and soft-margin LPBoost with generalized hinge loss are all entropy maximization problems. By looking at the dual problems of these boosting algorithms, we show that the success of boosting algorithms can be understood in terms of maintaining a better margin distribution by maximizing margins and at the same time controlling the margin variance.We also theoretically prove that, approximately, AdaBoost maximizes the average margin, instead of the minimum margin. The duality formulation also enables us to develop column generation based optimization algorithms, which are totally corrective. We show that they exhibit almost identical classification results to that of standard stage-wise additive boosting algorithms but with much faster convergence rates. Therefore fewer weak classifiers are needed to build the ensemble using our proposed optimization technique.},
	number = {12},
	urldate = {2022-07-05},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shen, Chunhua and Li, Hanxi},
	month = dec,
	year = {2010},
	note = {arXiv:0901.3590 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {2216--2231},
}

@misc{altun_exponential_2004,
	title = {Exponential {Families} for {Conditional} {Random} {Fields}},
	abstract = {In this paper we define conditional random fields in reproducing kernel Hilbert spaces and show connections to Gaussian Process classification. More specifically, we prove decomposition results for undirected graphical models and we give constructions for kernels. Finally we present efficient means of solving the optimization problem using reduced rank decompositions and we show how stationarity can be exploited efficiently in the optimization process.},
	author = {Altun, Yasemin and Smola, Alex J. and Hofmann, Thomas},
	year = {2004},
}

@incollection{neal_priors_1996,
	address = {New York, NY},
	series = {Lecture {Notes} in {Statistics}},
	title = {Priors for {Infinite} {Networks}},
	isbn = {978-1-4612-0745-0},
	url = {https://doi.org/10.1007/978-1-4612-0745-0_2},
	abstract = {In this chapter, I show that priors over network parameters can be defined in such a way that the corresponding priors over functions computed by the network reach reasonable limits as the number of hidden units goes to infinity. When using such priors,there is thus no need to limit the size of the network in order to avoid “overfitting”. The infinite network limit also provides insight into the properties of different priors. A Gaussian prior for hidden-to-output weights results in a Gaussian process prior for functions,which may be smooth, Brownian, or fractional Brownian. Quite different effects can be obtained using priors based on non-Gaussian stable distributions. In networks with more than one hidden layer, a combination of Gaussian and non-Gaussian priors appears most interesting.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Bayesian {Learning} for {Neural} {Networks}},
	publisher = {Springer},
	author = {Neal, Radford M.},
	editor = {Neal, Radford M.},
	year = {1996},
	doi = {10.1007/978-1-4612-0745-0_2},
	keywords = {Gaussian Process, Hide Layer, Hide Unit, Output Unit, Prior Distribution},
	pages = {29--53},
}

@inproceedings{le_heteroscedastic_2005,
	address = {Bonn, Germany},
	title = {Heteroscedastic {Gaussian} process regression},
	isbn = {978-1-59593-180-1},
	url = {http://portal.acm.org/citation.cfm?doid=1102351.1102413},
	doi = {10.1145/1102351.1102413},
	abstract = {This paper presents an algorithm to estimate simultaneously both mean and variance of a non parametric regression problem. The key point is that we are able to estimate variance locally unlike standard Gaussian Process regression or SVMs. This means that our estimator adapts to the local noise. The problem is cast in the setting of maximum a posteriori estimation in exponential families. Unlike previous work, we obtain a convex optimization problem which can be solved via Newton’s method.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 22nd international conference on {Machine} learning  - {ICML} '05},
	publisher = {ACM Press},
	author = {Le, Quoc V. and Smola, Alex J. and Canu, Stéphane},
	year = {2005},
	pages = {489--496},
}

@article{phillips_modeling_2008,
	title = {Modeling of species distributions with {Maxent}: new extensions and a comprehensive evaluation},
	volume = {31},
	issn = {1600-0587},
	shorttitle = {Modeling of species distributions with {Maxent}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.0906-7590.2008.5203.x},
	doi = {10.1111/j.0906-7590.2008.5203.x},
	abstract = {Accurate modeling of geographic distributions of species is crucial to various applications in ecology and conservation. The best performing techniques often require some parameter tuning, which may be prohibitively time-consuming to do separately for each species, or unreliable for small or biased datasets. Additionally, even with the abundance of good quality data, users interested in the application of species models need not have the statistical knowledge required for detailed tuning. In such cases, it is desirable to use “default settings”, tuned and validated on diverse datasets. Maxent is a recently introduced modeling technique, achieving high predictive accuracy and enjoying several additional attractive properties. The performance of Maxent is influenced by a moderate number of parameters. The first contribution of this paper is the empirical tuning of these parameters. Since many datasets lack information about species absence, we present a tuning method that uses presence-only data. We evaluate our method on independently collected high-quality presence-absence data. In addition to tuning, we introduce several concepts that improve the predictive accuracy and running time of Maxent. We introduce “hinge features” that model more complex relationships in the training data; we describe a new logistic output format that gives an estimate of probability of presence; finally we explore “background sampling” strategies that cope with sample selection bias and decrease model-building time. Our evaluation, based on a diverse dataset of 226 species from 6 regions, shows: 1) default settings tuned on presence-only data achieve performance which is almost as good as if they had been tuned on the evaluation data itself; 2) hinge features substantially improve model performance; 3) logistic output improves model calibration, so that large differences in output values correspond better to large differences in suitability; 4) “target-group” background sampling can give much better predictive performance than random background sampling; 5) random background sampling results in a dramatic decrease in running time, with no decrease in model performance.},
	language = {en},
	number = {2},
	urldate = {2022-07-05},
	journal = {Ecography},
	author = {Phillips, Steven J. and Dudík, Miroslav},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.0906-7590.2008.5203.x},
	pages = {161--175},
}

@incollection{dudik_performance_2004,
	address = {Berlin, Heidelberg},
	title = {Performance {Guarantees} for {Regularized} {Maximum} {Entropy} {Density} {Estimation}},
	volume = {3120},
	isbn = {978-3-540-22282-8 978-3-540-27819-1},
	url = {http://link.springer.com/10.1007/978-3-540-27819-1_33},
	abstract = {We consider the problem of estimating an unknown probability distribution from samples using the principle of maximum entropy (maxent). To alleviate overﬁtting with a very large number of features, we propose applying the maxent principle with relaxed constraints on the expectations of the features. By convex duality, this turns out to be equivalent to ﬁnding the Gibbs distribution minimizing a regularized version of the empirical log loss. We prove nonasymptotic bounds showing that, with respect to the true underlying distribution, this relaxed version of maxent produces density estimates that are almost as good as the best possible. These bounds are in terms of the deviation of the feature empirical averages relative to their true expectations, a number that can be bounded using standard uniform-convergence techniques. In particular, this leads to bounds that drop quickly with the number of samples, and that depend very moderately on the number or complexity of the features. We also derive and prove convergence for both sequential-update and parallel-update algorithms. Finally, we brieﬂy describe experiments on data relevant to the modeling of species geographical distributions.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Learning {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dudík, Miroslav and Phillips, Steven J. and Schapire, Robert E.},
	year = {2004},
	doi = {10.1007/978-3-540-27819-1_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {472--486},
}

@article{dudik_maximum_2007,
	title = {Maximum {Entropy} {Density} {Estimation} with {Generalized} {Regularization} and an {Application} to {Species} {Distribution} {Modeling}},
	language = {en},
	author = {Dudık, Miroslav and Phillips, Steven J and Schapire, Robert E},
	year = {2007},
	pages = {44},
}

@misc{zhou_regularized_2015,
	title = {Regularized {Minimax} {Conditional} {Entropy} for {Crowdsourcing}},
	url = {http://arxiv.org/abs/1503.07240},
	abstract = {There is a rapidly increasing interest in crowdsourcing for data labeling. By crowdsourcing, a large number of labels can be often quickly gathered at low cost. However, the labels provided by the crowdsourcing workers are usually not of high quality. In this paper, we propose a minimax conditional entropy principle to infer ground truth from noisy crowdsourced labels. Under this principle, we derive a unique probabilistic labeling model jointly parameterized by worker ability and item difficulty. We also propose an objective measurement principle, and show that our method is the only method which satisfies this objective measurement principle. We validate our method through a variety of real crowdsourcing datasets with binary, multiclass or ordinal labels.},
	urldate = {2022-07-05},
	publisher = {arXiv},
	author = {Zhou, Dengyong and Liu, Qiang and Platt, John C. and Meek, Christopher and Shah, Nihar B.},
	month = mar,
	year = {2015},
	note = {arXiv:1503.07240 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{dudik_hierarchical_2007,
	address = {Corvalis, Oregon},
	title = {Hierarchical maximum entropy density estimation},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273528},
	doi = {10.1145/1273496.1273528},
	abstract = {We study the problem of simultaneously estimating several densities where the datasets are organized into overlapping groups, such as a hierarchy. For this problem, we propose a maximum entropy formulation, which systematically incorporates the groups and allows us to share the strength of prediction across similar datasets. We derive general performance guarantees, and show how some previous approaches, such as hierarchical shrinkage and hierarchical priors, can be derived as special cases. We demonstrate the proposed technique on synthetic data and in a realworld application to modeling the geographic distributions of species hierarchically grouped in a taxonomy. Speciﬁcally, we model the geographic distributions of species in the Australian wet tropics and Northeast New South Wales. In these regions, small numbers of samples per species signiﬁcantly hinder effective prediction. Substantial beneﬁts are obtained by combining information across taxonomic groups.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 24th international conference on {Machine} learning - {ICML} '07},
	publisher = {ACM Press},
	author = {Dudik, Miroslav and Blei, David M. and Schapire, Robert E.},
	year = {2007},
	pages = {249--256},
}

@inproceedings{liang_learning_2009,
	address = {Montreal, Quebec, Canada},
	title = {Learning from measurements in exponential families},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553457},
	doi = {10.1145/1553374.1553457},
	abstract = {Given a model family and a set of unlabeled examples, one could either label speciﬁc examples or state general constraints—both provide information about the desired model. In general, what is the most cost-eﬀective way to learn? To address this question, we introduce measurements, a general class of mechanisms for providing information about a target model. We present a Bayesian decision-theoretic framework, which allows us to both integrate diverse measurements and choose new measurements to make. We use a variational inference algorithm, which exploits exponential family duality. The merits of our approach are demonstrated on two sequence labeling tasks.},
	language = {en},
	urldate = {2022-07-05},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Liang, Percy and Jordan, Michael I. and Klein, Dan},
	year = {2009},
	pages = {1--8},
}

@inproceedings{zhou_learning_2012,
	title = {Learning from the {Wisdom} of {Crowds} by {Minimax} {Entropy}},
	volume = {25},
	url = {https://papers.nips.cc/paper/2012/hash/46489c17893dfdcf028883202cefd6d1-Abstract.html},
	abstract = {An important way to make large training sets is to gather noisy labels from crowds of nonexperts. We propose a minimax entropy principle to improve the quality of these labels. Our method assumes that labels are generated by a probability distribution over workers, items, and labels. By maximizing the entropy of this distribution, the method naturally infers item confusability and worker expertise. We infer the ground truth by minimizing the entropy of this distribution, which we show minimizes the Kullback-Leibler (KL) divergence between the probability distribution and the unknown truth. We show that a simple coordinate descent scheme can optimize minimax entropy. Empirically, our results are substantially better than previously published methods for the same problem.},
	urldate = {2022-07-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Zhou, Dengyong and Basu, Sumit and Mao, Yi and Platt, John},
	year = {2012},
}

@article{friedlander_minimizing_2006,
	title = {On minimizing distortion and relative entropy},
	volume = {52},
	issn = {1557-9654},
	doi = {10.1109/TIT.2005.860448},
	abstract = {A common approach for estimating a probability mass function w when given a prior q and moment constraints given by Aw/spl les/b is to minimize the relative entropy between w and q subject to the set of linear constraints. In such cases, the solution w is known to have exponential form. We consider the case in which the linear constraints are noisy, uncertain, infeasible, or otherwise "soft." A solution can then be obtained by minimizing both the relative entropy and violation of the constraints Aw/spl les/b. A penalty parameter /spl sigma/ weights the relative importance of these two objectives. We show that this penalty formulation also yields a solution w with exponential form. If the distortion is based on an /spl lscr//sub p/ norm, then the exponential form of w is shown to have exponential decay parameters that are bounded as a function of /spl sigma/. We also state conditions under which the solution w to the penalty formulation will result in zero distortion, so that the moment constraints hold exactly. These properties are useful in choosing penalty parameters, evaluating the impact of chosen penalty parameters, and proving properties about methods that use such penalty formulations.},
	number = {1},
	journal = {IEEE Transactions on Information Theory},
	author = {Friedlander, M.P. and Gupta, M.R.},
	month = jan,
	year = {2006},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Computer science, Constraint optimization, Convex optimization, Councils, Entropy, Equations, Inverse problems, Kullback–Leibler distance, Nonlinear distortion, Power engineering and energy, Random variables, Scientific computing, cross-entropy, exact penalty, function, inverse problem, maximum entropy, moment constraint, relative entropy},
	pages = {238--245},
}

@inproceedings{erkan_semi-supervised_2010,
	title = {Semi-{Supervised} {Learning} via {Generalized} {Maximum} {Entropy}},
	url = {https://proceedings.mlr.press/v9/erkan10a.html},
	abstract = {Various supervised inference methods can be analyzed as convex duals of the generalized maximum entropy (MaxEnt) framework. Generalized MaxEnt aims to find a distribution that maximizes an entropy function while respecting prior information represented as potential functions in miscellaneous forms of constraints and/or penalties. We extend this framework to semi-supervised learning by incorporating unlabeled data via modifications to these potential functions reflecting structural assumptions on the data geometry. The proposed approach leads to a family of discriminative semi-supervised algorithms, that are convex, scalable, inherently multi-class, easy to implement, and that can be kernelized naturally. Experimental evaluation of special cases shows the competitiveness of our methodology.},
	language = {en},
	urldate = {2022-07-04},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Erkan, Ayse and Altun, Yasemin},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {209--216},
}

@book{devroye_probabilistic_1996,
	address = {New York, NY},
	series = {Stochastic {Modelling} and {Applied} {Probability}},
	title = {A {Probabilistic} {Theory} of {Pattern} {Recognition}},
	volume = {31},
	isbn = {978-1-4612-6877-2 978-1-4612-0711-5},
	url = {http://link.springer.com/10.1007/978-1-4612-0711-5},
	language = {en},
	urldate = {2022-07-04},
	publisher = {Springer New York},
	author = {Devroye, Luc and Györfi, László and Lugosi, Gábor},
	editor = {Karatzas, I. and Yor, M.},
	year = {1996},
	doi = {10.1007/978-1-4612-0711-5},
}

@misc{atito_sit_2021,
	title = {{SiT}: {Self}-supervised {vIsion} {Transformer}},
	shorttitle = {{SiT}},
	url = {http://arxiv.org/abs/2104.03602},
	abstract = {Self-supervised learning methods are gaining increasing traction in computer vision due to their recent success in reducing the gap with supervised learning. In natural language processing (NLP) self-supervised learning and transformers are already the methods of choice. The recent literature suggests that the transformers are becoming increasingly popular also in computer vision. So far, the vision transformers have been shown to work well when pretrained either using a large scale supervised data or with some kind of co-supervision, e.g. in terms of teacher network. These supervised pretrained vision transformers achieve very good results in downstream tasks with minimal changes. In this work we investigate the merits of self-supervised learning for pretraining image/vision transformers and then using them for downstream classification tasks. We propose Self-supervised vIsion Transformers (SiT) and discuss several self-supervised training mechanisms to obtain a pretext model. The architectural flexibility of SiT allows us to use it as an autoencoder and work with multiple self-supervised tasks seamlessly. We show that a pretrained SiT can be finetuned for a downstream classification task on small scale datasets, consisting of a few thousand images rather than several millions. The proposed approach is evaluated on standard datasets using common protocols. The results demonstrate the strength of the transformers and their suitability for self-supervised learning. We outperformed existing self-supervised learning methods by large margin. We also observed that SiT is good for few shot learning and also showed that it is learning useful representation by simply training a linear classifier on top of the learned features from SiT. Pretraining, finetuning, and evaluation codes will be available under: https://github.com/Sara-Ahmed/SiT.},
	urldate = {2022-06-22},
	publisher = {arXiv},
	author = {Atito, Sara and Awais, Muhammad and Kittler, Josef},
	month = nov,
	year = {2021},
	note = {Number: arXiv:2104.03602
arXiv:2104.03602 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@incollection{altun_unifying_2006,
	address = {Berlin, Heidelberg},
	title = {Unifying {Divergence} {Minimization} and {Statistical} {Inference} {Via} {Convex} {Duality}},
	volume = {4005},
	isbn = {978-3-540-35294-5 978-3-540-35296-9},
	url = {http://link.springer.com/10.1007/11776420_13},
	abstract = {In this paper we unify divergence minimization and statistical inference by means of convex duality. In the process of doing so, we prove that the dual of approximate maximum entropy estimation is maximum a posteriori estimation. Moreover, our treatment leads to stability and convergence bounds for many statistical learning problems. Finally, we show how an algorithm by Zhang can be used to solve this class of optimization problems eﬃciently.},
	language = {en},
	urldate = {2022-06-13},
	booktitle = {Learning {Theory}},
	publisher = {Springer Berlin Heidelberg},
	author = {Altun, Yasemin and Smola, Alex},
	year = {2006},
	doi = {10.1007/11776420_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {139--153},
}

@article{rockafellar_characterization_1966,
	title = {Characterization of the subdifferentials of convex functions.},
	volume = {17},
	issn = {0030-8730},
	url = {https://projecteuclid.org/journals/pacific-journal-of-mathematics/volume-17/issue-3/Characterization-of-the-subdifferentials-of-convex-functions/pjm/1102994514.full},
	abstract = {Pacific Journal of Mathematics},
	number = {3},
	urldate = {2022-06-21},
	journal = {Pacific Journal of Mathematics},
	author = {Rockafellar, R. T.},
	month = jan,
	year = {1966},
	note = {Publisher: Pacific Journal of Mathematics, A Non-profit Corporation},
	keywords = {46.45, 47.80},
	pages = {497--510},
}

@misc{jebara_feature_2013,
	title = {Feature {Selection} and {Dualities} in {Maximum} {Entropy} {Discrimination}},
	url = {http://arxiv.org/abs/1301.3865},
	abstract = {Incorporating feature selection into a classification or regression method often carries a number of advantages. In this paper we formalize feature selection specifically from a discriminative perspective of improving classification/regression accuracy. The feature selection method is developed as an extension to the recently proposed maximum entropy discrimination (MED) framework. We describe MED as a flexible (Bayesian) regularization approach that subsumes, e.g., support vector classification, regression and exponential family models. For brevity, we restrict ourselves primarily to feature selection in the context of linear classification/regression methods and demonstrate that the proposed approach indeed carries substantial improvements in practice. Moreover, we discuss and develop various extensions of feature selection, including the problem of dealing with example specific but unobserved degrees of freedom -- alignments or invariants.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Jebara, Tony S. and Jaakkola, Tommi S.},
	month = jan,
	year = {2013},
	note = {Number: arXiv:1301.3865
arXiv:1301.3865 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zhang_coping_2021,
	title = {Coping with {Label} {Shift} via {Distributionally} {Robust} {Optimisation}},
	url = {http://arxiv.org/abs/2010.12230},
	abstract = {The label shift problem refers to the supervised learning setting where the train and test label distributions do not match. Existing work addressing label shift usually assumes access to an {\textbackslash}emph\{unlabelled\} test sample. This sample may be used to estimate the test label distribution, and to then train a suitably re-weighted classifier. While approaches using this idea have proven effective, their scope is limited as it is not always feasible to access the target domain; further, they require repeated retraining if the model is to be deployed in {\textbackslash}emph\{multiple\} test environments. Can one instead learn a {\textbackslash}emph\{single\} classifier that is robust to arbitrary label shifts from a broad family? In this paper, we answer this question by proposing a model that minimises an objective based on distributionally robust optimisation (DRO). We then design and analyse a gradient descent-proximal mirror ascent algorithm tailored for large-scale problems to optimise the proposed objective. \%, and establish its convergence. Finally, through experiments on CIFAR-100 and ImageNet, we show that our technique can significantly improve performance over a number of baselines in settings where label shift is present.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Zhang, Jingzhao and Menon, Aditya and Veit, Andreas and Bhojanapalli, Srinadh and Kumar, Sanjiv and Sra, Suvrit},
	month = aug,
	year = {2021},
	note = {Number: arXiv:2010.12230
arXiv:2010.12230 [cs, math]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{duchi_learning_2020,
	title = {Learning {Models} with {Uniform} {Performance} via {Distributionally} {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1810.08750},
	doi = {10.48550/arXiv.1810.08750},
	abstract = {A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts, or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition, and providing good tail performance, the distributionally robust approach often exhibits improved performance.},
	urldate = {2022-06-20},
	publisher = {arXiv},
	author = {Duchi, John and Namkoong, Hongseok},
	month = jul,
	year = {2020},
	note = {Number: arXiv:1810.08750
arXiv:1810.08750 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{tao_siamese_2022,
	title = {Siamese {Image} {Modeling} for {Self}-{Supervised} {Vision} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2206.01204},
	abstract = {Self-supervised learning (SSL) has delivered superior performance on a variety of downstream vision tasks. Two main-stream SSL frameworks have been proposed, i.e., Instance Discrimination (ID) and Masked Image Modeling (MIM). ID pulls together the representations of different views from the same image, while avoiding feature collapse. It does well on linear probing but is inferior in detection performance. On the other hand, MIM reconstructs the original content given a masked image. It excels at dense prediction but fails to perform well on linear probing. Their distinctions are caused by neglecting the representation requirements of either semantic alignment or spatial sensitivity. Specifically, we observe that (1) semantic alignment demands semantically similar views to be projected into nearby representation, which can be achieved by contrasting different views with strong augmentations; (2) spatial sensitivity requires to model the local structure within an image. Predicting dense representations with masked image is therefore beneficial because it models the conditional distribution of image content. Driven by these analysis, we propose Siamese Image Modeling (SIM), which predicts the dense representations of an augmented view, based on another masked view from the same image but with different augmentations. Our method uses a Siamese network with two branches. The online branch encodes the first view, and predicts the second view's representation according to the relative positions between these two views. The target branch produces the target by encoding the second view. In this way, we are able to achieve comparable linear probing and dense prediction performances with ID and MIM, respectively. We also demonstrate that decent linear probing result can be obtained without a global loss. Code shall be released.},
	urldate = {2022-06-16},
	publisher = {arXiv},
	author = {Tao, Chenxin and Zhu, Xizhou and Huang, Gao and Qiao, Yu and Wang, Xiaogang and Dai, Jifeng},
	month = jun,
	year = {2022},
	note = {Number: arXiv:2206.01204
arXiv:2206.01204 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{nozawa_empirical_2022,
	title = {Empirical {Evaluation} and {Theoretical} {Analysis} for {Representation} {Learning}: {A} {Survey}},
	shorttitle = {Empirical {Evaluation} and {Theoretical} {Analysis} for {Representation} {Learning}},
	doi = {10.48550/arXiv.2204.08226},
	abstract = {The evaluation survey is the extended version of Nozawa and Sato [1], and it is shown that there exist various ways to evaluate representation learning algorithms depending on the application because of the flexibility of representation learning. Representation learning enables us to automatically extract generic feature representations from a dataset to solve another machine learning task. Recently, extracted feature representations by a representation learning algorithm and a simple predictor have exhibited state-of-the-art performance on several machine learning tasks. Despite its remarkable progress, there exist various ways to evaluate representation learning algorithms depending on the application because of the ﬂexibility of representation learning. To understand the current representation learning, we review evaluation methods of representation learning algorithms and theoretical analyses. On the basis of our evaluation survey, we also discuss the future direction of representation learning. Note that this survey is the extended version of Nozawa and Sato [1].},
	journal = {ArXiv},
	author = {Nozawa, Kento and Sato, Issei},
	year = {2022},
}

@article{slater_lagrange_1959,
	title = {Lagrange {Multipliers} {Revisited}},
	url = {https://elischolar.library.yale.edu/cowles-discussion-paper-series/304},
	journal = {Cowles Foundation Discussion Papers},
	author = {Slater, Morton},
	month = oct,
	year = {1959},
}

@article{ben-david_difficulty_2003,
	title = {On the difficulty of approximately maximizing agreements},
	volume = {66},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000003000382},
	doi = {10.1016/S0022-0000(03)00038-2},
	abstract = {We address the computational complexity of learning in the agnostic framework. For a variety of common concept classes we prove that, unless P=NP, there is no polynomial time approximation scheme for finding a member in the class that approximately maximizes the agreement with a given training sample. In particular our results apply to the classes of monomials, axis-aligned hyper-rectangles, closed balls and monotone monomials. For each of these classes, we prove the NP-hardness of approximating maximal agreement to within some fixed constant (independent of the sample size and of the dimensionality of the sample space). For the class of half-spaces, we prove that, for any ε{\textgreater}0, it is NP-hard to approximately maximize agreements to within a factor of (418/415−ε), improving on the best previously known constant for this problem, and using a simpler proof. An interesting feature of our proofs is that, for each of the classes we discuss, we find patterns of training examples that, while being hard for approximating agreement within that concept class, allow efficient agreement maximization within other concept classes. These results bring up a new aspect of the model selection problem—they imply that the choice of hypothesis class for agnostic learning from among those considered in this paper can drastically effect the computational complexity of the learning process.},
	language = {en},
	number = {3},
	urldate = {2022-06-15},
	journal = {Journal of Computer and System Sciences},
	author = {Ben-David, Shai and Eiron, Nadav and Long, Philip M.},
	month = may,
	year = {2003},
	keywords = {Axis-aligned hyper-rectangles, Balls, Computational learning theory, Half-spaces, Hardness, Inapproximability, Machine learning, Monomials, Neural networks},
	pages = {496--514},
}

@inproceedings{sutter_robust_2021,
	title = {Robust {Generalization} despite {Distribution} {Shift} via {Minimum} {Discriminating} {Information}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/f86890095c957e9b949d11d15f0d0cd5-Abstract.html},
	abstract = {Training models that perform well under distribution shifts is a central challenge in machine learning. In this paper, we introduce a modeling framework where, in addition to training data, we have partial structural knowledge of the shifted test distribution. We employ the principle of minimum discriminating information to embed the available prior knowledge, and use distributionally robust optimization to account for uncertainty due to the limited samples. By leveraging large deviation results, we obtain explicit generalization bounds with respect to the unknown shifted distribution. Lastly, we demonstrate the versatility of our framework by demonstrating it on two rather distinct applications: (1) training classifiers on systematically biased data and (2) off-policy evaluation in Markov Decision Processes.},
	urldate = {2022-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sutter, Tobias and Krause, Andreas and Kuhn, Daniel},
	year = {2021},
	pages = {29754--29767},
}

@techreport{fathony_consistent_2019,
	title = {Consistent {Robust} {Adversarial} {Prediction} for {General} {Multiclass} {Classification}},
	url = {http://arxiv.org/abs/1812.07526},
	abstract = {We propose a robust adversarial prediction framework for general multiclass classification. Our method seeks predictive distributions that robustly optimize non-convex and non-continuous multiclass loss metrics against the worst-case conditional label distributions (the adversarial distributions) that (approximately) match the statistics of the training data. Although the optimized loss metrics are non-convex and non-continuous, the dual formulation of the framework is a convex optimization problem that can be recast as a risk minimization model with a prescribed convex surrogate loss we call the adversarial surrogate loss. We show that the adversarial surrogate losses fill an existing gap in surrogate loss construction for general multiclass classification problems, by simultaneously aligning better with the original multiclass loss, guaranteeing Fisher consistency, enabling a way to incorporate rich feature spaces via the kernel trick, and providing competitive performance in practice.},
	number = {arXiv:1812.07526},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Fathony, Rizal and Asif, Kaiser and Liu, Anqi and Bashiri, Mohammad Ali and Xing, Wei and Behpour, Sima and Zhang, Xinhua and Ziebart, Brian D.},
	month = nov,
	year = {2019},
	note = {arXiv:1812.07526 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{wang_distributionally_2020,
	title = {Distributionally {Robust} {Learning} for {Uncertainty} {Calibration} under {Domain} {Shift}},
	url = {http://arxiv.org/abs/2010.05784},
	abstract = {We propose a framework for learning calibrated uncertainties under domain shifts. We consider the case where the source (training) distribution differs from the target (test) distribution. We detect such domain shifts through the use of a binary domain classifier and integrate it with the task network and train them jointly end-to-end. The binary domain classifier yields a density ratio that reflects the closeness of a target (test) sample to the source (training) distribution. We employ it to adjust the uncertainty of prediction in the task network. This idea of using the density ratio is based on the distributionally robust learning (DRL) framework, which accounts for the domain shift through adversarial risk minimization. We demonstrate that our method generates calibrated uncertainties that benefit many downstream tasks, such as unsupervised domain adaptation (UDA) and semi-supervised learning (SSL). In these tasks, methods like self-training and FixMatch use uncertainties to select confident pseudo-labels for re-training. Our experiments show that the introduction of DRL leads to significant improvements in cross-domain performance. We also demonstrate that the estimated density ratios show agreement with the human selection frequencies, suggesting a positive correlation with a proxy of human perceived uncertainties.},
	number = {arXiv:2010.05784},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Wang, Haoxuan and Liu, Anqi and Yu, Zhiding and Yan, Junchi and Yue, Yisong and Anandkumar, Anima},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05784 [cs]
version: 1
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{hashimoto_fairness_nodate,
	title = {Fairness {Without} {Demographics} in {Repeated} {Loss} {Minimization}},
	abstract = {Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity—minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we ﬁrst show that the status quo of empirical risk minimization (ERM) ampliﬁes representation disparity over time, which can even make initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity ampliﬁcation on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task.},
	language = {en},
	author = {Hashimoto, Tatsunori B and Srivastava, Megha and Namkoong, Hongseok and Liang, Percy},
	pages = {10},
}

@inproceedings{zhai_doro_2021,
	title = {{DORO}: {Distributional} and {Outlier} {Robust} {Optimization}},
	shorttitle = {{DORO}},
	url = {https://proceedings.mlr.press/v139/zhai21a.html},
	abstract = {Many machine learning tasks involve subpopulation shift where the testing data distribution is a subpopulation of the training distribution. For such settings, a line of recent work has proposed the use of a variant of empirical risk minimization(ERM) known as distributionally robust optimization (DRO). In this work, we apply DRO to real, large-scale tasks with subpopulation shift, and observe that DRO performs relatively poorly, and moreover has severe instability. We identify one direct cause of this phenomenon: sensitivity of DRO to outliers in the datasets. To resolve this issue, we propose the framework of DORO, for Distributional and Outlier Robust Optimization. At the core of this approach is a refined risk function which prevents DRO from overfitting to potential outliers. We instantiate DORO for the Cressie-Read family of Rényi divergence, and delve into two specific instances of this family: CVaR and \${\textbackslash}chi{\textasciicircum}2\$-DRO. We theoretically prove the effectiveness of the proposed method, and empirically show that DORO improves the performance and stability of DRO with experiments on large modern datasets, thereby positively addressing the open question raised by Hashimoto et al., 2018. Codes are available at https://github.com/RuntianZ/doro.},
	language = {en},
	urldate = {2022-06-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zhai, Runtian and Dan, Chen and Kolter, Zico and Ravikumar, Pradeep},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {12345--12355},
}

@techreport{taskesen_distributionally_2020,
	title = {A {Distributionally} {Robust} {Approach} to {Fair} {Classification}},
	url = {http://arxiv.org/abs/2007.09530},
	abstract = {We propose a distributionally robust logistic regression model with an unfairness penalty that prevents discrimination with respect to sensitive attributes such as gender or ethnicity. This model is equivalent to a tractable convex optimization problem if a Wasserstein ball centered at the empirical distribution on the training data is used to model distributional uncertainty and if a new convex unfairness measure is used to incentivize equalized opportunities. We demonstrate that the resulting classifier improves fairness at a marginal loss of predictive accuracy on both synthetic and real datasets. We also derive linear programming-based confidence bounds on the level of unfairness of any pre-trained classifier by leveraging techniques from optimal uncertainty quantification over Wasserstein balls.},
	number = {arXiv:2007.09530},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Taskesen, Bahar and Nguyen, Viet Anh and Kuhn, Daniel and Blanchet, Jose},
	month = jul,
	year = {2020},
	note = {arXiv:2007.09530 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ziebart_maximum_nodate,
	title = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}},
	abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
	language = {en},
	author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	pages = {6},
}

@techreport{rezaei_robust_2021,
	title = {Robust {Fairness} under {Covariate} {Shift}},
	url = {http://arxiv.org/abs/2010.05166},
	abstract = {Making predictions that are fair with regard to protected group membership (race, gender, age, etc.) has become an important requirement for classification algorithms. Existing techniques derive a fair model from sampled labeled data relying on the assumption that training and testing data are identically and independently drawn (iid) from the same distribution. In practice, distribution shift can and does occur between training and testing datasets as the characteristics of individuals interacting with the machine learning system change. We investigate fairness under covariate shift, a relaxation of the iid assumption in which the inputs or covariates change while the conditional label distribution remains the same. We seek fair decisions under these assumptions on target data with unknown labels. We propose an approach that obtains the predictor that is robust to the worst-case in terms of target performance while satisfying target fairness requirements and matching statistical properties of the source data. We demonstrate the benefits of our approach on benchmark prediction tasks.},
	number = {arXiv:2010.05166},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Rezaei, Ashkan and Liu, Anqi and Memarrast, Omid and Ziebart, Brian},
	month = feb,
	year = {2021},
	doi = {10.48550/arXiv.2010.05166},
	note = {arXiv:2010.05166 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{memarrast_fairness_2021,
	title = {Fairness for {Robust} {Learning} to {Rank}},
	url = {http://arxiv.org/abs/2112.06288},
	abstract = {While conventional ranking systems focus solely on maximizing the utility of the ranked items to users, fairness-aware ranking systems additionally try to balance the exposure for different protected attributes such as gender or race. To achieve this type of group fairness for ranking, we derive a new ranking system based on the first principles of distributional robustness. We formulate a minimax game between a player choosing a distribution over rankings to maximize utility while satisfying fairness constraints against an adversary seeking to minimize utility while matching statistics of the training data. We show that our approach provides better utility for highly fair rankings than existing baseline methods.},
	number = {arXiv:2112.06288},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Memarrast, Omid and Rezaei, Ashkan and Fathony, Rizal and Ziebart, Brian},
	month = dec,
	year = {2021},
	note = {arXiv:2112.06288 [cs, stat]
type: article},
	keywords = {Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{liu_robust_2017,
	title = {Robust {Covariate} {Shift} {Prediction} with {General} {Losses} and {Feature} {Views}},
	url = {http://arxiv.org/abs/1712.10043},
	abstract = {Covariate shift relaxes the widely-employed independent and identically distributed (IID) assumption by allowing different training and testing input distributions. Unfortunately, common methods for addressing covariate shift by trying to remove the bias between training and testing distributions using importance weighting often provide poor performance guarantees in theory and unreliable predictions with high variance in practice. Recently developed methods that construct a predictor that is inherently robust to the difficulties of learning under covariate shift are restricted to minimizing logloss and can be too conservative when faced with high-dimensional learning tasks. We address these limitations in two ways: by robustly minimizing various loss functions, including non-convex ones, under the testing distribution; and by separately shaping the influence of covariate shift according to different feature-based views of the relationship between input variables and example labels. These generalizations make robust covariate shift prediction applicable to more task scenarios. We demonstrate the benefits on classification under covariate shift tasks.},
	number = {arXiv:1712.10043},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Liu, Anqi and Ziebart, Brian D.},
	month = dec,
	year = {2017},
	note = {arXiv:1712.10043 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rezaei_fairness_2020,
	title = {Fairness for {Robust} {Log} {Loss} {Classification}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {http://arxiv.org/abs/1903.03910},
	doi = {10.1609/aaai.v34i04.6002},
	abstract = {Developing classification methods with high accuracy that also avoid unfair treatment of different groups has become increasingly important for data-driven decision making in social applications. Many existing methods enforce fairness constraints on a selected classifier (e.g., logistic regression) by directly forming constrained optimizations. We instead re-derive a new classifier from the first principles of distributional robustness that incorporates fairness criteria into a worst-case logarithmic loss minimization. This construction takes the form of a minimax game and produces a parametric exponential family conditional distribution that resembles truncated logistic regression. We present the theoretical benefits of our approach in terms of its convexity and asymptotic convergence. We then demonstrate the practical advantages of our approach on three benchmark fairness datasets.},
	number = {04},
	urldate = {2022-06-14},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Rezaei, Ashkan and Fathony, Rizal and Memarrast, Omid and Ziebart, Brian},
	month = apr,
	year = {2020},
	note = {arXiv:1903.03910 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {5511--5518},
}

@techreport{grunwald_entropy_2008,
	title = {Entropy {Concentration} and the {Empirical} {Coding} {Game}},
	url = {http://arxiv.org/abs/0809.1017},
	abstract = {We give a characterization of Maximum Entropy/Minimum Relative Entropy inference by providing two `strong entropy concentration' theorems. These theorems unify and generalize Jaynes' `concentration phenomenon' and Van Campenhout and Cover's `conditional limit theorem'. The theorems characterize exactly in what sense a prior distribution Q conditioned on a given constraint, and the distribution P, minimizing the relative entropy D(P {\textbar}{\textbar}Q) over all distributions satisfying the constraint, are `close' to each other. We then apply our theorems to establish the relationship between entropy concentration and a game-theoretic characterization of Maximum Entropy Inference due to Topsoe and others.},
	number = {arXiv:0809.1017},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Grunwald, Peter},
	month = sep,
	year = {2008},
	note = {arXiv:0809.1017 [cs, math, stat]
type: article},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Methodology},
}

@techreport{mazuelas_minimax_2020,
	title = {Minimax {Classification} with 0-1 {Loss} and {Performance} {Guarantees}},
	url = {http://arxiv.org/abs/2010.07964},
	abstract = {Supervised classification techniques use training samples to find classification rules with small expected 0-1 loss. Conventional methods achieve efficient learning and out-of-sample generalization by minimizing surrogate losses over specific families of rules. This paper presents minimax risk classifiers (MRCs) that do not rely on a choice of surrogate loss and family of rules. MRCs achieve efficient learning and out-of-sample generalization by minimizing worst-case expected 0-1 loss w.r.t. uncertainty sets that are defined by linear constraints and include the true underlying distribution. In addition, MRCs' learning stage provides performance guarantees as lower and upper tight bounds for expected 0-1 loss. We also present MRCs' finite-sample generalization bounds in terms of training size and smallest minimax risk, and show their competitive classification performance w.r.t. state-of-the-art techniques using benchmark datasets.},
	number = {arXiv:2010.07964},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Mazuelas, Santiago and Zanoni, Andrea and Perez, Aritz},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2010.07964},
	note = {arXiv:2010.07964 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{mazuelas_minimax_2022,
	title = {Minimax risk classifiers with 0-1 loss},
	url = {http://arxiv.org/abs/2201.06487},
	abstract = {Supervised classification techniques use training samples to learn a classification rule with small expected 0-1 loss (error probability). Conventional methods enable tractable learning and provide out-of-sample generalization by using surrogate losses instead of the 0-1 loss and considering specific families of rules (hypothesis classes). This paper presents minimax risk classifiers (MRCs) that minimize the worst-case 0-1 loss over general classification rules and provide tight performance guarantees at learning. We show that MRCs are strongly universally consistent using feature mappings given by characteristic kernels. The paper also proposes efficient optimization techniques for MRC learning and shows that the methods presented can provide accurate classification together with tight performance guarantees in practice.},
	number = {arXiv:2201.06487},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Mazuelas, Santiago and Romero, Mauricio and Grünwald, Peter},
	month = apr,
	year = {2022},
	note = {arXiv:2201.06487 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{asif_adversarial_nodate,
	title = {Adversarial {Cost}-{Sensitive} {Classiﬁcation}},
	abstract = {In many classiﬁcation settings, mistakes incur different application-dependent penalties based on the predicted and actual class labels. Costsensitive classiﬁers minimizing these penalties are needed. We propose a robust minimax approach for producing classiﬁers that directly minimize the cost of mistakes as a convex optimization problem. This is in contrast to previous methods that minimize the empirical risk using a convex surrogate for the cost of mistakes, since minimizing the empirical risk of the actual cost-sensitive loss is generally intractable. By treating properties of the training data as uncertain, our approach avoids these computational difﬁculties. We develop theory and algorithms for our approach and demonstrate its beneﬁts on cost-sensitive classiﬁcation tasks.},
	language = {en},
	author = {Asif, Kaiser and Xing, Wei and Behpour, Sima and Ziebart, Brian D},
	pages = {10},
}

@inproceedings{fathony_adversarial_2016,
	title = {Adversarial {Multiclass} {Classification}: {A} {Risk} {Minimization} {Perspective}},
	volume = {29},
	shorttitle = {Adversarial {Multiclass} {Classification}},
	url = {https://proceedings.neurips.cc/paper/2016/hash/ad13a2a07ca4b7642959dc0c4c740ab6-Abstract.html},
	abstract = {Recently proposed adversarial classification methods have shown promising results for cost sensitive and multivariate losses. In contrast with empirical risk minimization (ERM) methods, which use convex surrogate losses to approximate the desired non-convex target loss function, adversarial methods minimize non-convex losses by treating the properties of the training data as being uncertain and worst case within a minimax game. Despite this difference in formulation, we recast adversarial classification under zero-one loss as an ERM method with a novel prescribed loss function. We demonstrate a number of theoretical and practical advantages over the very closely related hinge loss ERM methods. This establishes adversarial classification under the zero-one loss as a method that fills the long standing gap in multiclass hinge loss classification, simultaneously guaranteeing Fisher consistency and universal consistency, while also providing dual parameter sparsity and high accuracy predictions in practice.},
	urldate = {2022-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Fathony, Rizal and Liu, Anqi and Asif, Kaiser and Ziebart, Brian},
	year = {2016},
}

@inproceedings{lebanon_boosting_2001,
	title = {Boosting and {Maximum} {Likelihood} for {Exponential} {Models}},
	volume = {14},
	url = {https://proceedings.neurips.cc/paper/2001/hash/71e09b16e21f7b6919bbfc43f6a5b2f0-Abstract.html},
	abstract = {We derive an equivalence between AdaBoost and the dual of a convex optimization problem, showing that the only difference between mini- mizing the exponential loss used by AdaBoost and maximum likelihood for exponential models is that the latter requires the model to be normal- ized to form a conditional probability distribution over labels. In addi- tion to establishing a simple and easily understood connection between the two methods, this framework enables us to derive new regularization procedures for boosting that directly correspond to penalized maximum likelihood. Experiments on UCI datasets support our theoretical analy- sis and give additional insight into the relationship between boosting and logistic regression.},
	urldate = {2022-06-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Lebanon, Guy and Lafferty, John},
	year = {2001},
}

@inproceedings{cortes_structural_2015,
	title = {Structural maxent models},
	booktitle = {Proceedings of the {Thirty}-{Second} {International} {Conference} on {Machine} {Learning} ({ICML} 2015)},
	author = {Cortes, Corinna and Kuznetsov, Vitaly and Mohri, Mehryar and Syed, Umar},
	year = {2015},
}

@article{smith_derivation_1974,
	title = {A {Derivation} of {Entropy} and the {Maximum} {Entropy} {Criterion} in the {Context} of {Decision} {Problems}},
	volume = {SMC-4},
	issn = {2168-2909},
	doi = {10.1109/TSMC.1974.5409109},
	abstract = {In this paper the entropy functional is derived in the context of decision making under uncertainty. It is shown that the mathematical description of a decision problem can be partitioned into two parts: a probability distribution p and a convex set LN(A) that describes the remaining structure of the problem. The question of selecting a general representative form for this convex set is explored, and a set of assumptions is proposed that specifies a form for such a representative set LN. Corresponding to this set LN, the entropy of the probability p can be interpreted as an information measure with respect to decision problems. In addition, it is shown that if this set LN is substituted for the set LN(A) in a given decision problem, the use of the maximum entropy criterion for probability selection corresponds to a minimax solution to the representative form of the problem. To the extent that the set LN resembles the set LN(A) in a given decision problem, the use of the maximum entropy criterion for probability selection corresponds to a minimax criterion for decision making.},
	number = {2},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics},
	author = {Smith, Stephen A.},
	month = mar,
	year = {1974},
	note = {Conference Name: IEEE Transactions on Systems, Man, and Cybernetics},
	keywords = {Decision making, Entropy, Loss measurement, Minimax techniques, Optimal control, Positron emission tomography, Probability distribution, Uncertainty},
	pages = {157--163},
}

@article{sutter_generalized_nodate,
	title = {Generalized maximum entropy estimation},
	abstract = {We consider the problem of estimating a probability distribution that maximizes the entropy while satisfying a ﬁnite number of moment constraints, possibly corrupted by noise. Based on duality of convex programming, we present a novel approximation scheme using a smoothed fast gradient method that is equipped with explicit bounds on the approximation error. We further demonstrate how the presented scheme can be used for approximating the chemical master equation through the zero-information moment closure method, and for an approximate dynamic programming approach in the context of constrained Markov decision processes with uncountable state and action spaces.},
	language = {en},
	author = {Sutter, Tobias and Sutter, David and Esfahani, Peyman Mohajerin and Lygeros, John},
	pages = {29},
}

@book{mohri_foundations_2012,
	address = {Cambridge, MA, USA},
	series = {Adaptive {Computation} and {Machine} {Learning} series},
	title = {Foundations of {Machine} {Learning}},
	isbn = {978-0-262-01825-8},
	abstract = {Fundamental topics in machine learning are presented along with theoretical and conceptual tools for the discussion and proof of algorithms.},
	language = {en},
	publisher = {MIT Press},
	author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
	editor = {Bach, Francis},
	month = aug,
	year = {2012},
}

@article{della_pietra_inducing_1997,
	title = {Inducing features of random fields},
	volume = {19},
	issn = {01628828},
	url = {http://ieeexplore.ieee.org/document/588021/},
	doi = {10.1109/34.588021},
	abstract = {We present a technique for constructing random ﬁelds from a set of training samples. The learning paradigm builds increasingly complex ﬁelds by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the ﬁeld and an iterative scaling algorithm is used to estimate the optimal values of the weights.},
	language = {en},
	number = {4},
	urldate = {2022-06-14},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Della Pietra, S. and Della Pietra, V. and Lafferty, J.},
	month = apr,
	year = {1997},
	pages = {380--393},
}

@article{singh_lecture_nodate,
	title = {Lecture 8: {September} 26},
	author = {Singh, Aarti},
}

@article{uffink_can_1995,
	title = {Can the maximum entropy principle be explained as a consistency requirement?},
	volume = {26},
	issn = {1355-2198},
	url = {https://www.sciencedirect.com/science/article/pii/1355219895000151},
	doi = {10.1016/1355-2198(95)00015-1},
	abstract = {The principle of maximum entropy is a general method to assign values to probability distributions on the basis of partial information. This principle, introduced by Jaynes in 1957, forms an extension of the classical principle of insufficient reason. It has been further generalized, both in mathematical formulation and in intended scope, into the principle of maximum relative entropy or of minimum information. It has been claimed that these principles are singled out as unique methods of statistical inference that agree with certain compelling consistency requirements. This paper reviews these consistency arguments and the surrounding controversy. It is shown that the uniqueness proofs are flawed, or rest on unreasonably strong assumptions. A more general class of inference rules, maximizing the so-called Rényi entropies, is exhibited which also fulfill the reasonable part of the consistency assumptions.},
	language = {en},
	number = {3},
	urldate = {2022-06-14},
	journal = {Studies in History and Philosophy of Science Part B: Studies in History and Philosophy of Modern Physics},
	author = {Uffink, Jos},
	month = dec,
	year = {1995},
	pages = {223--261},
}

@techreport{allahverdyan_maximum_2020,
	title = {Maximum {Entropy} competes with {Maximum} {Likelihood}},
	url = {http://arxiv.org/abs/2012.09430},
	abstract = {Maximum entropy (MAXENT) method has a large number of applications in theoretical and applied machine learning, since it provides a convenient non-parametric tool for estimating unknown probabilities. The method is a major contribution of statistical physics to probabilistic inference. However, a systematic approach towards its validity limits is currently missing. Here we study MAXENT in a Bayesian decision theory set-up, i.e. assuming that there exists a well-defined prior Dirichlet density for unknown probabilities, and that the average Kullback-Leibler (KL) distance can be employed for deciding on the quality and applicability of various estimators. These allow to evaluate the relevance of various MAXENT constraints, check its general applicability, and compare MAXENT with estimators having various degrees of dependence on the prior, viz. the regularized maximum likelihood (ML) and the Bayesian estimators. We show that MAXENT applies in sparse data regimes, but needs specific types of prior information. In particular, MAXENT can outperform the optimally regularized ML provided that there are prior rank correlations between the estimated random quantity and its probabilities.},
	number = {arXiv:2012.09430},
	urldate = {2022-06-14},
	institution = {arXiv},
	author = {Allahverdyan, A. E. and Martirosyan, N. H.},
	month = dec,
	year = {2020},
	note = {arXiv:2012.09430 [cond-mat, physics:physics, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Statistical Mechanics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@misc{noauthor_wayback_2006,
	title = {Wayback {Machine}},
	url = {https://web.archive.org/web/20060603144738/http://www.phys.uu.nl/~wwwgrnsl/jos/mepabst/mep.pdf},
	urldate = {2022-06-14},
	month = jun,
	year = {2006},
}

@article{harremoes_maximum_2001,
	title = {Maximum {Entropy} {Fundamentals}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1099-4300},
	url = {https://www.mdpi.com/1099-4300/3/3/191},
	doi = {10.3390/e3030191},
	abstract = {In its modern formulation, the Maximum Entropy Principle was promoted by E.T. Jaynes, starting in the mid-fifties. The principle dictates that one should look for a distribution, consistent with available information, which maximizes the entropy. However, this principle focuses only on distributions and it appears advantageous to bring information theoretical thinking more prominently into play by also focusing on the "observer" and on coding. This view was brought forward by the second named author in the late seventies and is the view we will follow-up on here. It leads to the consideration of a certain game, the Code Length Game and, via standard game theoretical thinking, to a principle of Game Theoretical Equilibrium. This principle is more basic than the Maximum Entropy Principle in the sense that the search for one type of optimal strategies in the Code Length Game translates directly into the search for distributions with maximum entropy. In the present paper we offer a self-contained and comprehensive treatment of fundamentals of both principles mentioned, based on a study of the Code Length Game. Though new concepts and results are presented, the reading should be instructional and accessible to a rather wide audience, at least if certain mathematical details are left aside at a rst reading. The most frequently studied instance of entropy maximization pertains to the Mean Energy Model which involves a moment constraint related to a given function, here taken to represent "energy". This type of application is very well known from the literature with hundreds of applications pertaining to several different elds and will also here serve as important illustration of the theory. But our approach reaches further, especially regarding the study of continuity properties of the entropy function, and this leads to new results which allow a discussion of models with so-called entropy loss. These results have tempted us to speculate over the development of natural languages. In fact, we are able to relate our theoretical findings to the empirically found Zipf's law which involves statistical aspects of words in a language. The apparent irregularity inherent in models with entropy loss turns out to imply desirable stability properties of languages.},
	language = {en},
	number = {3},
	urldate = {2022-06-14},
	journal = {Entropy},
	author = {Harremoës, Peter and Topsøe, Flemming},
	month = sep,
	year = {2001},
	note = {Number: 3
Publisher: Molecular Diversity Preservation International},
	keywords = {Nash equilibrium code, Zipf's law, continuity of entropy, entropy loss, exponential family, game theoretical equilibrium, hyperbolic distributions, information topology, maximum entropy, minimum risk, partition function},
	pages = {191--226},
}

@article{csiszar_why_1991,
	title = {Why {Least} {Squares} and {Maximum} {Entropy}? {An} {Axiomatic} {Approach} to {Inference} for {Linear} {Inverse} {Problems}},
	volume = {19},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Why {Least} {Squares} and {Maximum} {Entropy}?},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-19/issue-4/Why-Least-Squares-and-Maximum-Entropy-An-Axiomatic-Approach-to/10.1214/aos/1176348385.full},
	doi = {10.1214/aos/1176348385},
	abstract = {An attempt is made to determine the logically consistent rules for selecting a vector from any feasible set defined by linear constraints, when either all \$n\$-vectors or those with positive components or the probability vectors are permissible. Some basic postulates are satisfied if and only if the selection rule is to minimize a certain function which, if a "prior guess" is available, is a measure of distance from the prior guess. Two further natural postulates restrict the permissible distances to the author's \$f\$-divergences and Bregman's divergences, respectively. As corollaries, axiomatic characterizations of the methods of least squares and minimum discrimination information are arrived at. Alternatively, the latter are also characterized by a postulate of composition consistency. As a special case, a derivation of the method of maximum entropy from a small set of natural axioms is obtained.},
	number = {4},
	urldate = {2022-06-14},
	journal = {The Annals of Statistics},
	author = {Csiszar, Imre},
	month = dec,
	year = {1991},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62A99, 68T01, 92C55, 94A17, Image reconstruction, Selection rules, linear constraitns, logically consistent inference, minimum discrimination information, nonlinear projection, nonsymmetric distance},
	pages = {2032--2066},
}

@article{paris_common_nodate,
	title = {Common {Sense} and {Maximum} {Entropy}},
	abstract = {This paper concerns the question of how to draw inferences common sensically from uncertain knowledge. Since the early work of Shore and Johnson (1980), Paris and Vencovská (1990), and Csiszár (1989), it has been known that the Maximum Entropy Inference Process is the only inference process which obeys certain common sense principles of uncertain reasoning. In this paper we consider the present status of this result and argue that within the rather narrow context in which we work this complete and consistent mode of uncertain reasoning is actually characterised by the observance of just a single common sense principle (or slogan).},
	language = {en},
	author = {Paris, Jeff},
	pages = {20},
}

@article{paris_defense_1997,
	title = {In defense of the maximum entropy inference process},
	volume = {17},
	issn = {0888613X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0888613X97000145},
	doi = {10.1016/S0888-613X(97)00014-5},
	abstract = {This paper is a sequel to an earlier result of the authors that in making inferences from certain probabilistic knowledge bases the maximum entropy inference process, ME, is the only inference process respecting "common sense." This result was criticized on the grounds that the probabilistic knowledge bases considered are unnatural and that ignorance of dependence shouM not be identified with statistical independence. We argue against these criticisms and also against the more general criticism that ME is representation dependent. In a final section, however, we provide a criticism of our own o f ME, and o f inference processes in general, namely that they fail to satisfy compactness. © 1997 Elsevier Science Inc.},
	language = {en},
	number = {1},
	urldate = {2022-06-14},
	journal = {International Journal of Approximate Reasoning},
	author = {Paris, J. and Vencovská, A.},
	month = jul,
	year = {1997},
	pages = {77--103},
}

@article{shore_axiomatic_1980,
	title = {Axiomatic derivation of the principle of maximum entropy and the principle of minimum cross-entropy},
	volume = {26},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1056144/},
	doi = {10.1109/TIT.1980.1056144},
	language = {en},
	number = {1},
	urldate = {2022-06-14},
	journal = {IEEE Transactions on Information Theory},
	author = {Shore, J. and Johnson, R.},
	month = jan,
	year = {1980},
	pages = {26--37},
}

@article{dufumier_rethinking_2022,
	title = {Rethinking {Positive} {Sampling} for {Contrastive} {Learning} with {Kernel}},
	doi = {10.48550/arXiv.2206.01646},
	abstract = {Data augmentation is a crucial component in unsupervised contrastive learning (CL). It determines how positive samples are defined and, ultimately, the quality of the representation. While efficient augmentations have been found for standard vision datasets, such as ImageNet, it is still an open problem in other applications, such as medical imaging, or in datasets with easy-to-learn but irrelevant imaging features. In this work, we propose a new way to define positive samples using kernel theory along with a novel loss called decoupled uniformity. We propose to integrate prior information, learnt from generative models or given as auxiliary attributes, into contrastive learning, to make it less dependent on data augmentation. We draw a connection between contrastive learning and the conditional mean embedding theory to derive tight bounds on the downstream classification loss. In an unsupervised setting, we empirically demonstrate that CL benefits from generative models, such as VAE and GAN, to less rely on data augmentations. We validate our framework on vision datasets including CIFAR10, CIFAR100, STL10 and ImageNet100 and a brain MRI dataset. In the weakly supervised setting, we demonstrate that our formulation provides state-of-the-art results.},
	journal = {ArXiv},
	author = {Dufumier, Benoit and Barbano, C. and Louiset, Robin and Duchesnay, E. and Gori, P.},
	year = {2022},
}

@article{tian_understanding_2022,
	title = {Understanding the {Role} of {Nonlinearity} in {Training} {Dynamics} of {Contrastive} {Learning}},
	doi = {10.48550/arXiv.2206.01342},
	abstract = {While the empirical success of self-supervised learning (SSL) heavily relies on the usage of deep nonlinear models, many theoretical works proposed to understand SSL still focus on linear ones. In this paper, we study the role of nonlinearity in the training dynamics of contrastive learning (CL) on one and two-layer nonlinear networks with homogeneous activation h(x) = h′(x)x. We theoretically demonstrate that (1) the presence of nonlinearity leads to many local optima even in 1-layer setting, each corresponding to certain patterns from the data distribution, while with linear activation, only one major pattern can be learned; and (2) nonlinearity leads to specialized weights into diverse patterns, a behavior that linear activation is proven not capable of. These findings suggest that models with lots of parameters can be regarded as a brute-force way to find these local optima induced by nonlinearity, a possible underlying reason why empirical observations such as the lottery ticket hypothesis hold. In addition, for 2-layer setting, we also discover global modulation: those local patterns discriminative from the perspective of global-level patterns are prioritized to learn, further characterizing the learning process. Simulation verifies our theoretical findings.},
	journal = {ArXiv},
	author = {Tian, Yuandong},
	year = {2022},
}

@inproceedings{tian_understanding_2022-1,
	title = {Understanding {Deep} {Contrastive} {Learning} via {Coordinate}-wise {Optimization}},
	abstract = {We show that Contrastive Learning (CL) under a broad family of loss functions (including InfoNCE) has a uniﬁed formulation of coordinate-wise optimization on the network parameter θ and pairwise importance α , where the max player θ learns representation for contrastiveness, and the min player α puts more weights on pairs of distinct samples that share similar representations. The resulting formulation, called α -CL , uniﬁes not only various existing contrastive losses, which differ by how sample-pair importance α is constructed, but also is able to extrapolate to give novel contrastive losses beyond popular ones, opening a new avenue of contrastive loss design. These novel losses yield comparable (or better) performance on CIFAR10 and STL-10 than classic InfoNCE. Furthermore, we also analyze the max player in detail: we prove that with ﬁxed α , max player is equivalent to Principal Component Analysis (PCA) for deep linear network, and almost all local minima are global and rank-1, recovering optimal PCA solutions. Finally, we extend our analysis on max player to 2-layer ReLU networks, showing that its ﬁxed points can have higher ranks.},
	author = {Tian, Yuandong},
	year = {2022},
}

@inproceedings{tian_understanding_2022-2,
	title = {Understanding {Deep} {Contrastive} {Learning} via {Coordinate}-wise {Optimization}},
	abstract = {We show that Contrastive Learning (CL) under a broad family of loss functions (including InfoNCE) has a uniﬁed formulation of coordinate-wise optimization on the network parameter θ and pairwise importance α , where the max player θ learns representation for contrastiveness, and the min player α puts more weights on pairs of distinct samples that share similar representations. The resulting formulation, called α -CL , uniﬁes not only various existing contrastive losses, which differ by how sample-pair importance α is constructed, but also is able to extrapolate to give novel contrastive losses beyond popular ones, opening a new avenue of contrastive loss design. These novel losses yield comparable (or better) performance on CIFAR10 and STL-10 than classic InfoNCE. Furthermore, we also analyze the max player in detail: we prove that with ﬁxed α , max player is equivalent to Principal Component Analysis (PCA) for deep linear network, and almost all local minima are global and rank-1, recovering optimal PCA solutions. Finally, we extend our analysis on max player to 2-layer ReLU networks, showing that its ﬁxed points can have higher ranks.},
	author = {Tian, Yuandong},
	year = {2022},
}

@article{franc_support_nodate,
	title = {Support {Vector} {Machines} as {Probabilistic} {Models}},
	abstract = {We show how the SVM can be viewed as a maximum likelihood estimate of a class of probabilistic models. This model class can be viewed as a reparametrization of the SVM in a similar vein to the ν-SVM reparametrizing the classical (C-)SVM. It is not discriminative, but has a non-uniform marginal. We illustrate the beneﬁts of this new view by rederiving and re-investigating two established SVM-related algorithms.},
	language = {en},
	author = {Franc, Vojtech and Zien, Alex and Schölkopf, Bernhard},
	pages = {8},
}

@book{murphy_probabilistic_2022,
	title = {Probabilistic {Machine} {Learning}: {An} introduction},
	url = {probml.ai},
	publisher = {MIT Press},
	author = {Murphy, Kevin P.},
	year = {2022},
}

@misc{noauthor_notitle_nodate,
	url = {https://probml.github.io/pml-book/book1.html},
	urldate = {2022-06-14},
}

@inproceedings{jaakkola_maximum_1999,
	title = {Maximum {Entropy} {Discrimination}},
	volume = {12},
	url = {https://papers.nips.cc/paper/1999/hash/4fa53be91b4933d536748a60458b9797-Abstract.html},
	abstract = {We present a general framework for discriminative estimation based  on the maximum entropy principle and its extensions.  All  calcula(cid:173) tions involve distributions over structures and/or parameters rather  than  specific  settings  and  reduce  to  relative  entropy  projections.  This holds  even  when  the data is  not  separable within  the chosen  parametric class,  in  the context of anomaly  detection  rather than  classification, or when the labels in the training set are uncertain or  incomplete.  Support  vector  machines  are  naturally subsumed  un(cid:173) der  this  class  and we  provide several extensions.  We  are also  able  to estimate exactly and efficiently discriminative distributions over  tree structures of class-conditional  models  within  this framework.  Preliminary experimental results  are  indicative of the potential in  these techniques.},
	urldate = {2022-06-13},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Jaakkola, Tommi and Meila, Marina and Jebara, Tony},
	year = {1999},
}

@techreport{allen-zhu_towards_2021,
	title = {Towards {Understanding} {Ensemble}, {Knowledge} {Distillation} and {Self}-{Distillation} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2012.09816},
	abstract = {We formally study how ensemble of deep learning models can improve test accuracy, and how the superior performance of ensemble can be distilled into a single model using knowledge distillation. We consider the challenging case where the ensemble is simply an average of the outputs of a few independently trained neural networks with the SAME architecture, trained using the SAME algorithm on the SAME data set, and they only differ by the random seeds used in the initialization. We empirically show that ensemble/knowledge distillation in deep learning works very differently from traditional learning theory, especially differently from ensemble of random feature mappings or the neural-tangent-kernel feature mappings, and is potentially out of the scope of existing theorems. Thus, to properly understand ensemble and knowledge distillation in deep learning, we develop a theory showing that when data has a structure we refer to as "multi-view", then ensemble of independently trained neural networks can provably improve test accuracy, and such superior test accuracy can also be provably distilled into a single model by training a single model to match the output of the ensemble instead of the true label. Our result sheds light on how ensemble works in deep learning in a way that is completely different from traditional theorems, and how the "dark knowledge" is hidden in the outputs of the ensemble -- that can be used in knowledge distillation -- comparing to the true data labels. In the end, we prove that self-distillation can also be viewed as implicitly combining ensemble and knowledge distillation to improve test accuracy.},
	number = {arXiv:2012.09816},
	urldate = {2022-06-13},
	institution = {arXiv},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	month = jul,
	year = {2021},
	doi = {10.48550/arXiv.2012.09816},
	note = {arXiv:2012.09816 [cs, math, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@techreport{pan_towards_2022,
	title = {Towards {Understanding} {Why} {Mask}-{Reconstruction} {Pretraining} {Helps} in {Downstream} {Tasks}},
	url = {http://arxiv.org/abs/2206.03826},
	abstract = {For unsupervised pretraining, mask-reconstruction pretraining (MRP) approaches randomly mask input patches and then reconstruct pixels or semantic features of these masked patches via an auto-encoder. Then for a downstream task, supervised fine-tuning the pretrained encoder remarkably surpasses the conventional supervised learning (SL) trained from scratch. However, it is still unclear 1) how MRP performs semantic learning in the pretraining phase and 2) why it helps in downstream tasks. To solve these problems, we theoretically show that on an auto-encoder of a two/one-layered convolution encoder/decoder, MRP can capture all discriminative semantics in the pretraining dataset, and accordingly show its provable improvement over SL on the classification downstream task. Specifically, we assume that pretraining dataset contains multi-view samples of ratio \$1-{\textbackslash}mu\$ and single-view samples of ratio \${\textbackslash}mu\$, where multi/single-view samples has multiple/single discriminative semantics. Then for pretraining, we prove that 1) the convolution kernels of the MRP encoder captures all discriminative semantics in the pretraining data; and 2) a convolution kernel captures at most one semantic. Accordingly, in the downstream supervised fine-tuning, most semantics would be captured and different semantics would not be fused together. This helps the downstream fine-tuned network to easily establish the relation between kernels and semantic class labels. In this way, the fine-tuned encoder in MRP provably achieves zero test error with high probability for both multi-view and single-view test data. In contrast, as proved by{\textasciitilde}[3], conventional SL can only obtain a test accuracy between around \$0.5{\textbackslash}mu\$ for single-view test data. These results together explain the benefits of MRP in downstream tasks. Experimental results testify to multi-view data assumptions and our theoretical implications.},
	number = {arXiv:2206.03826},
	urldate = {2022-06-12},
	institution = {arXiv},
	author = {Pan, Jiachun and Zhou, Pan and Yan, Shuicheng},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03826 [cs, stat]
version: 1
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{han_pre-trained_2021,
	title = {Pre-trained models: {Past}, present and future},
	volume = {2},
	issn = {2666-6510},
	shorttitle = {Pre-trained models},
	url = {https://www.sciencedirect.com/science/article/pii/S2666651021000231},
	doi = {10.1016/j.aiopen.2021.08.002},
	abstract = {Large-scale pre-trained models (PTMs) such as BERT and GPT have recently achieved great success and become a milestone in the field of artificial intelligence (AI). Owing to sophisticated pre-training objectives and huge model parameters, large-scale PTMs can effectively capture knowledge from massive labeled and unlabeled data. By storing knowledge into huge parameters and fine-tuning on specific tasks, the rich knowledge implicitly encoded in huge parameters can benefit a variety of downstream tasks, which has been extensively demonstrated via experimental verification and empirical analysis. It is now the consensus of the AI community to adopt PTMs as backbone for downstream tasks rather than learning models from scratch. In this paper, we take a deep look into the history of pre-training, especially its special relation with transfer learning and self-supervised learning, to reveal the crucial position of PTMs in the AI development spectrum. Further, we comprehensively review the latest breakthroughs of PTMs. These breakthroughs are driven by the surge of computational power and the increasing availability of data, towards four important directions: designing effective architectures, utilizing rich contexts, improving computational efficiency, and conducting interpretation and theoretical analysis. Finally, we discuss a series of open problems and research directions of PTMs, and hope our view can inspire and advance the future study of PTMs.},
	language = {en},
	urldate = {2022-06-12},
	journal = {AI Open},
	author = {Han, Xu and Zhang, Zhengyan and Ding, Ning and Gu, Yuxian and Liu, Xiao and Huo, Yuqi and Qiu, Jiezhong and Yao, Yuan and Zhang, Ao and Zhang, Liang and Han, Wentao and Huang, Minlie and Jin, Qin and Lan, Yanyan and Liu, Yang and Liu, Zhiyuan and Lu, Zhiwu and Qiu, Xipeng and Song, Ruihua and Tang, Jie and Wen, Ji-Rong and Yuan, Jinhui and Zhao, Wayne Xin and Zhu, Jun},
	month = jan,
	year = {2021},
	keywords = {Artificial intelligence, Language models, Multimodal processing, Natural language processing, Pre-trained models, Self-supervised learning, Transfer learning},
	pages = {225--250},
}

@article{lanckriet_robust_2003,
	title = {A robust minimax approach to classification},
	volume = {3},
	issn = {1532-4435},
	url = {https://doi.org/10.1162/153244303321897726},
	doi = {10.1162/153244303321897726},
	abstract = {When constructing a classifier, the probability of correct classification of future data points should be maximized. We consider a binary classification problem where the mean and covariance matrix of each class are assumed to be known. No further assumptions are made with respect to the class-conditional distributions. Misclassification probabilities are then controlled in a worst-case setting: that is, under all possible choices of class-conditional densities with given mean and covariance matrix, we minimize the worst-case (maximum) probability of misclassification of future data points. For a linear decision boundary, this desideratum is translated in a very direct way into a (convex) second order cone optimization problem, with complexity similar to a support vector machine problem. The minimax problem can be interpreted geometrically as minimizing the maximum of the Mahalanobis distances to the two classes. We address the issue of robustness with respect to estimation errors (in the means and covariances of the classes) via a simple modification of the input data. We also show how to exploit Mercer kernels in this setting to obtain nonlinear decision boundaries, yielding a classifier which proves to be competitive with current methods, including support vector machines. An important feature of this method is that a worst-case bound on the probability of misclassification of future data is always obtained explicitly.},
	number = {null},
	urldate = {2022-06-10},
	journal = {The Journal of Machine Learning Research},
	author = {Lanckriet, Gert R.G. and Ghaoui, Laurent El and Bhattacharyya, Chiranjib and Jordan, Michael I.},
	month = mar,
	year = {2003},
	keywords = {classification, convex optimization, kernel methods, second order cone programming},
	pages = {555--582},
}

@article{courty_optimal_2017,
	title = {Optimal {Transport} for {Domain} {Adaptation}},
	volume = {39},
	issn = {0162-8828, 2160-9292},
	url = {http://ieeexplore.ieee.org/document/7586038/},
	doi = {10.1109/TPAMI.2016.2615921},
	abstract = {Domain adaptation is one of the most challenging tasks of modern data analytics. If the adaptation is done correctly, models built on a speciﬁc data representation become more robust when confronted to data depicting the same classes, but described by another observation system. Among the many strategies proposed, ﬁnding domain-invariant representations has shown excellent properties, in particular since it allows to train a unique classiﬁer effective in all domains. In this paper, we propose a regularized unsupervised optimal transportation model to perform the alignment of the representations in the source and target domains. We learn a transportation plan matching both PDFs, which constrains labeled samples of the same class in the source domain to remain close during transport. This way, we exploit at the same time the labeled samples in the source and the distributions observed in both domains. Experiments on toy and challenging real visual adaptation examples show the interest of the method, that consistently outperforms state of the art approaches. In addition, numerical experiments show that our approach leads to better performances on domain invariant deep learning features and can be easily adapted to the semi-supervised case where few labeled samples are available in the target domain.},
	language = {en},
	number = {9},
	urldate = {2022-06-10},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Courty, Nicolas and Flamary, Remi and Tuia, Devis and Rakotomamonjy, Alain},
	month = sep,
	year = {2017},
	pages = {1853--1865},
}

@article{gao_distributionally_nodate,
	title = {Distributionally {Robust} {Stochastic} {Optimization} with {Wasserstein} {Distance}},
	language = {en},
	author = {Gao, Rui and Kleywegt, Anton J},
	pages = {59},
}

@article{gao_distributionally_nodate-1,
	title = {Distributionally {Robust} {Stochastic} {Optimization} with {Wasserstein} {Distance}},
	language = {en},
	author = {Gao, Rui and Kleywegt, Anton J},
	pages = {59},
}

@techreport{gao_distributionally_2022,
	title = {Distributionally {Robust} {Stochastic} {Optimization} with {Wasserstein} {Distance}},
	url = {http://arxiv.org/abs/1604.02199},
	abstract = {Distributionally robust stochastic optimization (DRSO) is an approach to optimization under uncertainty in which, instead of assuming that there is a known true underlying probability distribution, one hedges against a chosen set of distributions. In this paper we first point out that the set of distributions should be chosen to be appropriate for the application at hand, and that some of the choices that have been popular until recently are, for many applications, not good choices. We next consider sets of distributions that are within a chosen Wasserstein distance from a nominal distribution. Such a choice of sets has two advantages: (1) The resulting distributions hedged against are more reasonable than those resulting from other popular choices of sets. (2) The problem of determining the worst-case expectation over the resulting set of distributions has desirable tractability properties. We derive a strong duality reformulation of the corresponding DRSO problem and construct approximate worst-case distributions explicitly via the first-order optimality conditions of the dual problem. Our contributions are four-fold. (i) We identify necessary and sufficient conditions for the existence of a worst-case distribution, which are naturally related to the growth rate of the objective function. (ii) We show that the worst-case distributions resulting from an appropriate Wasserstein distance have a concise structure and a clear interpretation. (iii) Using this structure, we show that data-driven DRSO problems can be approximated to any accuracy by robust optimization problems, and thereby many DRSO problems become tractable by using tools from robust optimization. (iv) Our strong duality result holds in a very general setting. As examples, we show that it can be applied to infinite-dimensional process control and intensity estimation for point processes.},
	number = {arXiv:1604.02199},
	urldate = {2022-06-10},
	institution = {arXiv},
	author = {Gao, Rui and Kleywegt, Anton J.},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.1604.02199},
	note = {arXiv:1604.02199 [math]
type: article},
	keywords = {90C15, 90C46, Mathematics - Optimization and Control},
}

@inproceedings{liu_robust_2014,
	title = {Robust {Classification} {Under} {Sample} {Selection} {Bias}},
	volume = {27},
	url = {https://papers.nips.cc/paper/2014/hash/d67d8ab4f4c10bf22aa353e27879133c-Abstract.html},
	abstract = {In many important machine learning applications, the source distribution used to estimate a probabilistic classifier differs from the target distribution on which the classifier will be used to make predictions. Due to its asymptotic properties, sample-reweighted loss minimization is a commonly employed technique to deal with this difference. However, given finite amounts of labeled source data, this technique suffers from significant estimation errors in settings with large sample selection bias. We develop a framework for robustly learning a probabilistic classifier that adapts to different sample selection biases using a minimax estimation formulation. Our approach requires only accurate estimates of statistics under the source distribution and is otherwise as robust as possible to unknown properties of the conditional label distribution, except when explicit generalization assumptions are incorporated. We demonstrate the behavior and effectiveness of our approach on synthetic and UCI binary classification tasks.},
	urldate = {2022-06-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Liu, Anqi and Ziebart, Brian},
	year = {2014},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised {Learning}: {Generative} or {Contrastive}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Self-supervised {Learning}},
	url = {http://arxiv.org/abs/2006.08218},
	doi = {10.1109/TKDE.2021.3090866},
	abstract = {Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.},
	urldate = {2022-06-09},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Wang, Zhaoyu and Mian, Li and Zhang, Jing and Tang, Jie},
	year = {2021},
	note = {arXiv:2006.08218 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--1},
}

@inproceedings{zhao_masking_2020,
	address = {Online},
	title = {Masking as an {Efficient} {Alternative} to {Finetuning} for {Pretrained} {Language} {Models}},
	url = {https://aclanthology.org/2020.emnlp-main.174},
	doi = {10.18653/v1/2020.emnlp-main.174},
	abstract = {We present an efficient method of utilizing pretrained language models, where we learn selective binary masks for pretrained weights in lieu of modifying them through finetuning. Extensive evaluations of masking BERT, RoBERTa, and DistilBERT on eleven diverse NLP tasks show that our masking scheme yields performance comparable to finetuning, yet has a much smaller memory footprint when several tasks need to be inferred. Intrinsic evaluations show that representations computed by our binary masked language models encode information necessary for solving downstream tasks. Analyzing the loss landscape, we show that masking and finetuning produce models that reside in minima that can be connected by a line segment with nearly constant test accuracy. This confirms that masking can be utilized as an efficient alternative to finetuning.},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, Mengjie and Lin, Tao and Mi, Fei and Jaggi, Martin and Schütze, Hinrich},
	month = nov,
	year = {2020},
	pages = {2226--2241},
}

@techreport{bao_beit_2021,
	title = {{BEiT}: {BERT} {Pre}-{Training} of {Image} {Transformers}},
	shorttitle = {{BEiT}},
	url = {http://arxiv.org/abs/2106.08254},
	abstract = {We introduce a self-supervised vision representation model BEiT, which stands for Bidirectional Encoder representation from Image Transformers. Following BERT developed in the natural language processing area, we propose a masked image modeling task to pretrain vision Transformers. Specifically, each image has two views in our pre-training, i.e, image patches (such as 16x16 pixels), and visual tokens (i.e., discrete tokens). We first "tokenize" the original image into visual tokens. Then we randomly mask some image patches and fed them into the backbone Transformer. The pre-training objective is to recover the original visual tokens based on the corrupted image patches. After pre-training BEiT, we directly fine-tune the model parameters on downstream tasks by appending task layers upon the pretrained encoder. Experimental results on image classification and semantic segmentation show that our model achieves competitive results with previous pre-training methods. For example, base-size BEiT achieves 83.2\% top-1 accuracy on ImageNet-1K, significantly outperforming from-scratch DeiT training (81.8\%) with the same setup. Moreover, large-size BEiT obtains 86.3\% only using ImageNet-1K, even outperforming ViT-L with supervised pre-training on ImageNet-22K (85.2\%). The code and pretrained models are available at https://aka.ms/beit.},
	number = {arXiv:2106.08254},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Bao, Hangbo and Dong, Li and Wei, Furu},
	month = jun,
	year = {2021},
	note = {arXiv:2106.08254 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{touvron_training_2021,
	title = {Training data-efficient image transformers \& distillation through attention},
	url = {http://arxiv.org/abs/2012.12877},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1\% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2\% accuracy) and when transferring to other tasks. We share our code and models.},
	number = {arXiv:2012.12877},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},
	month = jan,
	year = {2021},
	note = {arXiv:2012.12877 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@techreport{hou_graphmae_2022,
	title = {{GraphMAE}: {Self}-{Supervised} {Masked} {Graph} {Autoencoders}},
	shorttitle = {{GraphMAE}},
	url = {http://arxiv.org/abs/2205.10803},
	abstract = {Self-supervised learning (SSL) has been extensively explored in recent years. Particularly, generative SSL has seen emerging success in natural language processing and other fields, such as the wide adoption of BERT and GPT. Despite this, contrastive learning--which heavily relies on structural data augmentation and complicated training strategies--has been the dominant approach in graph SSL, while the progress of generative SSL on graphs, especially graph autoencoders (GAEs), has thus far not reached the potential as promised in other fields. In this paper, we identify and examine the issues that negatively impact the development of GAEs, including their reconstruction objective, training robustness, and error metric. We present a masked graph autoencoder GraphMAE that mitigates these issues for generative self-supervised graph learning. Instead of reconstructing structures, we propose to focus on feature reconstruction with both a masking strategy and scaled cosine error that benefit the robust training of GraphMAE. We conduct extensive experiments on 21 public datasets for three different graph learning tasks. The results manifest that GraphMAE--a simple graph autoencoder with our careful designs--can consistently generate outperformance over both contrastive and generative state-of-the-art baselines. This study provides an understanding of graph autoencoders and demonstrates the potential of generative self-supervised learning on graphs.},
	number = {arXiv:2205.10803},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Hou, Zhenyu and Liu, Xiao and Cen, Yukuo and Dong, Yuxiao and Yang, Hongxia and Wang, Chunjie and Tang, Jie},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.10803},
	note = {arXiv:2205.10803 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
}

@article{tan_mgae_2022,
	title = {{MGAE}: {Masked} {Autoencoders} for {Self}-{Supervised} {Learning} on {Graphs}},
	shorttitle = {{MGAE}},
	abstract = {A novel masked graph autoencoder (MGAE) framework to perform effective learning on graph structure data that generally performs better than state-ofthe-art unsupervised learning competitors on link prediction and node classification. We introduce a novel masked graph autoencoder (MGAE) framework to perform effective learning on graph structure data. Taking insights from self-supervised learning, we randomly mask a large proportion of edges and try to reconstruct these missing edges during training. MGAE has two core designs. First, we find that masking a high ratio of the input graph structure, e.g., 70\%, yields a nontrivial and meaningful self-supervisory task that benefits downstream applications. Second, we employ a graph neural network (GNN) as an encoder to perform message propagation on the partially-masked graph. To reconstruct the large number of masked edges, a tailored cross-correlation decoder is proposed. It could capture the cross-correlation between the head and tail nodes of anchor edge in multigranularity. Coupling these two designs enables MGAE to be trained efficiently and effectively. Extensive experiments on multiple open datasets (Planetoid and OGB benchmarks) demonstrate that MGAE generally performs better than state-ofthe-art unsupervised learning competitors on link prediction and node classification.},
	journal = {ArXiv},
	author = {Tan, Qiaoyu and Liu, Ninghao and Huang, Xiao and Chen, Rui and Choi, Soo-Hyun and Hu, Xia},
	year = {2022},
}

@techreport{wettig_should_2022,
	title = {Should {You} {Mask} 15\% in {Masked} {Language} {Modeling}?},
	url = {http://arxiv.org/abs/2202.08005},
	abstract = {Masked language models conventionally use a masking rate of 15\% due to the belief that more masking would provide insufficient context to learn good representations, and less masking would make training too expensive. Surprisingly, we find that masking up to 40\% of input tokens can outperform the 15\% baseline, and even masking 80\% can preserve most of the performance, as measured by finetuning on downstream tasks. Increasing the masking rates has two distinct effects, which we investigate through careful ablations: (1) A larger proportion of input tokens are corrupted, reducing the context size and creating a harder task, and (2) models perform more predictions, which benefits training. We observe that larger models with more capacity to tackle harder tasks in particular favor higher masking rates. We also find that even more sophisticated masking schemes such as span masking or PMI masking can benefit from higher masking rates, albeit to a smaller extent. Our results contribute to a better understanding of masked language modeling and shed light on more efficient language pre-training.},
	number = {arXiv:2202.08005},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wettig, Alexander and Gao, Tianyu and Zhong, Zexuan and Chen, Danqi},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2202.08005},
	note = {arXiv:2202.08005 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{bao_unilmv2_2020,
	title = {{UniLMv2}: {Pseudo}-{Masked} {Language} {Models} for {Unified} {Language} {Model} {Pre}-{Training}},
	shorttitle = {{UniLMv2}},
	url = {https://proceedings.mlr.press/v119/bao20a.html},
	abstract = {We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of language understanding and generation tasks across several widely used benchmarks. The code and pre-trained models are available at https://github.com/microsoft/unilm.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Piao, Songhao and Zhou, Ming and Hon, Hsiao-Wuen},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {642--652},
}

@techreport{zhang_inductive_2021,
	title = {On the {Inductive} {Bias} of {Masked} {Language} {Modeling}: {From} {Statistical} to {Syntactic} {Dependencies}},
	shorttitle = {On the {Inductive} {Bias} of {Masked} {Language} {Modeling}},
	url = {http://arxiv.org/abs/2104.05694},
	abstract = {We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).},
	number = {arXiv:2104.05694},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Zhang, Tianyi and Hashimoto, Tatsunori},
	month = apr,
	year = {2021},
	doi = {10.48550/arXiv.2104.05694},
	note = {arXiv:2104.05694 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{goyal_exposing_2021,
	title = {Exposing the {Implicit} {Energy} {Networks} behind {Masked} {Language} {Models} via {Metropolis}--{Hastings}},
	url = {https://openreview.net/forum?id=6PvWo1kEvlT},
	abstract = {While recent work has shown that scores from models trained by the ubiquitous masked language modeling (MLM) objective effectively discriminate probable from improbable sequences, it is still an...},
	language = {en},
	urldate = {2022-06-09},
	author = {Goyal, Kartik and Dyer, Chris and Berg-Kirkpatrick, Taylor},
	month = sep,
	year = {2021},
}

@techreport{sinha_masked_2021,
	title = {Masked {Language} {Modeling} and the {Distributional} {Hypothesis}: {Order} {Word} {Matters} {Pre}-training for {Little}},
	shorttitle = {Masked {Language} {Modeling} and the {Distributional} {Hypothesis}},
	url = {http://arxiv.org/abs/2104.06644},
	abstract = {A possible explanation for the impressive performance of masked language model (MLM) pre-training is that such models have learned to represent the syntactic structures prevalent in classical NLP pipelines. In this paper, we propose a different explanation: MLMs succeed on downstream tasks almost entirely due to their ability to model higher-order word co-occurrence statistics. To demonstrate this, we pre-train MLMs on sentences with randomly shuffled word order, and show that these models still achieve high accuracy after fine-tuning on many downstream tasks -- including on tasks specifically designed to be challenging for models that ignore word order. Our models perform surprisingly well according to some parametric syntactic probes, indicating possible deficiencies in how we test representations for syntactic information. Overall, our results show that purely distributional information largely explains the success of pre-training, and underscore the importance of curating challenging evaluation datasets that require deeper linguistic knowledge.},
	number = {arXiv:2104.06644},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Sinha, Koustuv and Jia, Robin and Hupkes, Dieuwke and Pineau, Joelle and Williams, Adina and Kiela, Douwe},
	month = sep,
	year = {2021},
	doi = {10.48550/arXiv.2104.06644},
	note = {arXiv:2104.06644 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	number = {arXiv:1810.04805},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	doi = {10.48550/arXiv.1810.04805},
	note = {arXiv:1810.04805 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@techreport{rosin_time_2022,
	title = {Time {Masking} for {Temporal} {Language} {Models}},
	url = {http://arxiv.org/abs/2110.06366},
	abstract = {Our world is constantly evolving, and so is the content on the web. Consequently, our languages, often said to mirror the world, are dynamic in nature. However, most current contextual language models are static and cannot adapt to changes over time. In this work, we propose a temporal contextual language model called TempoBERT, which uses time as an additional context of texts. Our technique is based on modifying texts with temporal information and performing time masking - specific masking for the supplementary time information. We leverage our approach for the tasks of semantic change detection and sentence time prediction, experimenting on diverse datasets in terms of time, size, genre, and language. Our extensive evaluation shows that both tasks benefit from exploiting time masking.},
	number = {arXiv:2110.06366},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Rosin, Guy D. and Guy, Ido and Radinsky, Kira},
	month = jan,
	year = {2022},
	note = {arXiv:2110.06366 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@techreport{feichtenhofer_masked_2022,
	title = {Masked {Autoencoders} {As} {Spatiotemporal} {Learners}},
	url = {http://arxiv.org/abs/2205.09113},
	abstract = {This paper studies a conceptually simple extension of Masked Autoencoders (MAE) to spatiotemporal representation learning from videos. We randomly mask out spacetime patches in videos and learn an autoencoder to reconstruct them in pixels. Interestingly, we show that our MAE method can learn strong representations with almost no inductive bias on spacetime (only except for patch and positional embeddings), and spacetime-agnostic random masking performs the best. We observe that the optimal masking ratio is as high as 90\% (vs. 75\% on images), supporting the hypothesis that this ratio is related to information redundancy of the data. A high masking ratio leads to a large speedup, e.g., {\textgreater} 4x in wall-clock time or even more. We report competitive results on several challenging video datasets using vanilla Vision Transformers. We observe that MAE can outperform supervised pre-training by large margins. We further report encouraging results of training on real-world, uncurated Instagram data. Our study suggests that the general framework of masked autoencoding (BERT, MAE, etc.) can be a unified methodology for representation learning with minimal domain knowledge.},
	number = {arXiv:2205.09113},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Feichtenhofer, Christoph and Fan, Haoqi and Li, Yanghao and He, Kaiming},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2205.09113},
	note = {arXiv:2205.09113 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{chen_efficient_2022,
	title = {Efficient {Self}-supervised {Vision} {Pretraining} with {Local} {Masked} {Reconstruction}},
	url = {http://arxiv.org/abs/2206.00790},
	abstract = {Self-supervised learning for computer vision has achieved tremendous progress and improved many downstream vision tasks such as image classification, semantic segmentation, and object detection. Among these, generative self-supervised vision learning approaches such as MAE and BEiT show promising performance. However, their global masked reconstruction mechanism is computationally demanding. To address this issue, we propose local masked reconstruction (LoMaR), a simple yet effective approach that performs masked reconstruction within a small window of 7\${\textbackslash}times\$7 patches on a simple Transformer encoder, improving the trade-off between efficiency and accuracy compared to global masked reconstruction over the entire image. Extensive experiments show that LoMaR reaches 84.1\% top-1 accuracy on ImageNet-1K classification, outperforming MAE by 0.5\%. After finetuning the pretrained LoMaR on 384\${\textbackslash}times\$384 images, it can reach 85.4\% top-1 accuracy, surpassing MAE by 0.6\%. On MS COCO, LoMaR outperforms MAE by 0.5 \${\textbackslash}text\{AP\}{\textasciicircum}{\textbackslash}text\{box\}\$ on object detection and 0.5 \${\textbackslash}text\{AP\}{\textasciicircum}{\textbackslash}text\{mask\}\$ on instance segmentation. LoMaR is especially more computation-efficient on pretraining high-resolution images, e.g., it is 3.1\${\textbackslash}times\$ faster than MAE with 0.2\% higher classification accuracy on pretraining 448\${\textbackslash}times\$448 images. This local masked reconstruction learning mechanism can be easily integrated into any other generative self-supervised learning approach. Our code will be publicly available.},
	number = {arXiv:2206.00790},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Chen, Jun and Hu, Ming and Li, Boyang and Elhoseiny, Mohamed},
	month = jun,
	year = {2022},
	doi = {10.48550/arXiv.2206.00790},
	note = {arXiv:2206.00790 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zhao_self-supervised_2021,
	address = {Montreal, QC, Canada},
	title = {Self-{Supervised} {Visual} {Representations} {Learning} by {Contrastive} {Mask} {Prediction}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711044/},
	doi = {10.1109/ICCV48922.2021.01000},
	abstract = {Advanced self-supervised visual representation learning methods rely on the instance discrimination (ID) pretext task. We point out that the ID task has an implicit semantic consistency (SC) assumption, which may not hold in unconstrained datasets. In this paper, we propose a novel contrastive mask prediction (CMP) task for visual representation learning and design a mask contrast (MaskCo) framework to implement the idea. MaskCo contrasts region-level features instead of view-level features, which makes it possible to identify the positive sample without any assumptions. To solve the domain gap between masked and unmasked features, we design a dedicated mask prediction head in MaskCo. This module is shown to be the key to the success of the CMP. We evaluated MaskCo on training datasets beyond ImageNet and compare its performance with MoCo V2 [4]. Results show that MaskCo achieves comparable performance with MoCo V2 using ImageNet training dataset, but demonstrates a stronger performance across a range of downstream tasks when COCO or Conceptual Captions are used for training. MaskCo provides a promising alternative to the ID-based methods for self-supervised learning in the wild.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Zhao, Yucheng and Wang, Guangting and Luo, Chong and Zeng, Wenjun and Zha, Zheng-Jun},
	month = oct,
	year = {2021},
	pages = {10140--10149},
}

@techreport{xie_simmim_2022,
	title = {{SimMIM}: {A} {Simple} {Framework} for {Masked} {Image} {Modeling}},
	shorttitle = {{SimMIM}},
	url = {http://arxiv.org/abs/2111.09886},
	abstract = {This paper presents SimMIM, a simple framework for masked image modeling. We simplify recently proposed related approaches without special designs such as block-wise masking and tokenization via discrete VAE or clustering. To study what let the masked image modeling task learn good representations, we systematically study the major components in our framework, and find that simple designs of each component have revealed very strong representation learning performance: 1) random masking of the input image with a moderately large masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting raw pixels of RGB values by direct regression performs no worse than the patch classification approaches with complex designs; 3) the prediction head can be as light as a linear layer, with no worse performance than heavier ones. Using ViT-B, our approach achieves 83.8\% top-1 fine-tuning accuracy on ImageNet-1K by pre-training also on this dataset, surpassing previous best approach by +0.6\%. When applied on a larger model of about 650 million parameters, SwinV2-H, it achieves 87.1\% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We also leverage this approach to facilitate the training of a 3B model (SwinV2-G), that by \$40{\textbackslash}times\$ less data than that in previous practice, we achieve the state-of-the-art on four representative vision benchmarks. The code and models will be publicly available at https://github.com/microsoft/SimMIM.},
	number = {arXiv:2111.09886},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Xie, Zhenda and Zhang, Zheng and Cao, Yue and Lin, Yutong and Bao, Jianmin and Yao, Zhuliang and Dai, Qi and Hu, Han},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2111.09886},
	note = {arXiv:2111.09886 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@techreport{zha_time_2022,
	title = {Time {Series} {Generation} with {Masked} {Autoencoder}},
	url = {http://arxiv.org/abs/2201.07006},
	abstract = {This paper shows that masked autoencoder with extrapolator (ExtraMAE) is a scalable self-supervised model for time series generation. ExtraMAE randomly masks some patches of the original time series and learns temporal dynamics by recovering the masked patches. Our approach has two core designs. First, ExtraMAE is self-supervised. Supervision allows ExtraMAE to effectively and efficiently capture the temporal dynamics of the original time series. Second, ExtraMAE proposes an extrapolator to disentangle two jobs of the decoder: recovering latent representations and mapping them back into the feature space. These unique designs enable ExtraMAE to consistently and significantly outperform state-of-the-art (SoTA) benchmarks in time series generation. The lightweight architecture also makes ExtraMAE fast and scalable. ExtraMAE shows outstanding behavior in various downstream tasks such as time series classification, prediction, and imputation. As a self-supervised generative model, ExtraMAE allows explicit management of the synthetic data. We hope this paper will usher in a new era of time series generation with self-supervised models.},
	number = {arXiv:2201.07006},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Zha, Mengyue and Wong, SiuTim and Liu, Mengqi and Zhang, Tong and Chen, Kani},
	month = may,
	year = {2022},
	doi = {10.48550/arXiv.2201.07006},
	note = {arXiv:2201.07006 [cs]
type: article},
	keywords = {Computer Science - Machine Learning},
}

@techreport{wei_masked_2021,
	title = {Masked {Feature} {Prediction} for {Self}-{Supervised} {Visual} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2112.09133},
	abstract = {We present Masked Feature Prediction (MaskFeat) for self-supervised pre-training of video models. Our approach first randomly masks out a portion of the input sequence and then predicts the feature of the masked regions. We study five different types of features and find Histograms of Oriented Gradients (HOG), a hand-crafted feature descriptor, works particularly well in terms of both performance and efficiency. We observe that the local contrast normalization in HOG is essential for good results, which is in line with earlier work using HOG for visual recognition. Our approach can learn abundant visual knowledge and drive large-scale Transformer-based models. Without using extra model weights or supervision, MaskFeat pre-trained on unlabeled videos achieves unprecedented results of 86.7\% with MViT-L on Kinetics-400, 88.3\% on Kinetics-600, 80.4\% on Kinetics-700, 38.8 mAP on AVA, and 75.0\% on SSv2. MaskFeat further generalizes to image input, which can be interpreted as a video with a single frame and obtains competitive results on ImageNet.},
	number = {arXiv:2112.09133},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wei, Chen and Fan, Haoqi and Xie, Saining and Wu, Chao-Yuan and Yuille, Alan and Feichtenhofer, Christoph},
	month = dec,
	year = {2021},
	doi = {10.48550/arXiv.2112.09133},
	note = {arXiv:2112.09133 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{salazar_masked_2020,
	address = {Online},
	title = {Masked {Language} {Model} {Scoring}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.240},
	doi = {10.18653/v1/2020.acl-main.240},
	abstract = {Pretrained masked language models (MLMs) require ﬁnetuning for most NLP tasks. Instead, we evaluate MLMs out of the box via their pseudo-log-likelihood scores (PLLs), which are computed by masking tokens one by one. We show that PLLs outperform scores from autoregressive language models like GPT-2 in a variety of tasks. By rescoring ASR and NMT hypotheses, RoBERTa reduces an endto-end LibriSpeech model’s WER by 30\% relative and adds up to +1.7 BLEU on state-of-theart baselines for low-resource translation pairs, with further gains from domain adaptation. We attribute this success to PLL’s unsupervised expression of linguistic acceptability without a left-to-right bias, greatly improving on scores from GPT-2 (+10 points on island effects, NPI licensing in BLiMP). One can ﬁnetune MLMs to give scores without masking, enabling computation in a single inference pass. In all, PLLs and their associated pseudo-perplexities (PPPLs) enable plug-and-play use of the growing number of pretrained MLMs; e.g., we use a single cross-lingual model to rescore translations in multiple languages. We release our library for language model scoring at https: //github.com/awslabs/mlm-scoring.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Salazar, Julian and Liang, Davis and Nguyen, Toan Q. and Kirchhoff, Katrin},
	year = {2020},
	pages = {2699--2712},
}

@techreport{clark_electra_2020,
	title = {{ELECTRA}: {Pre}-training {Text} {Encoders} as {Discriminators} {Rather} {Than} {Generators}},
	shorttitle = {{ELECTRA}},
	url = {http://arxiv.org/abs/2003.10555},
	abstract = {Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the task is defined over all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by BERT given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where it performs comparably to RoBERTa and XLNet while using less than 1/4 of their compute and outperforms them when using the same amount of compute.},
	number = {arXiv:2003.10555},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Clark, Kevin and Luong, Minh-Thang and Le, Quoc V. and Manning, Christopher D.},
	month = mar,
	year = {2020},
	doi = {10.48550/arXiv.2003.10555},
	note = {arXiv:2003.10555 [cs]
type: article},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{meng_coco-lm_2021,
	title = {{COCO}-{LM}: {Correcting} and {Contrasting} {Text} {Sequences} for {Language} {Model} {Pretraining}},
	volume = {34},
	shorttitle = {{COCO}-{LM}},
	url = {https://proceedings.neurips.cc//paper/2021/hash/c2c2a04512b35d13102459f8784f1a2d-Abstract.html},
	abstract = {We present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50\% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.},
	urldate = {2022-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Meng, Yu and Xiong, Chenyan and Bajaj, Payal and tiwary, saurabh and Bennett, Paul and Han, Jiawei and SONG, XIA},
	year = {2021},
	pages = {23102--23114},
}

@techreport{chung_w2v-bert_2021,
	title = {W2v-{BERT}: {Combining} {Contrastive} {Learning} and {Masked} {Language} {Modeling} for {Self}-{Supervised} {Speech} {Pre}-{Training}},
	shorttitle = {W2v-{BERT}},
	url = {http://arxiv.org/abs/2108.06209},
	abstract = {Motivated by the success of masked language modeling{\textasciitilde}(MLM) in pre-training natural language processing models, we propose w2v-BERT that explores MLM for self-supervised speech representation learning. w2v-BERT is a framework that combines contrastive learning and MLM, where the former trains the model to discretize input continuous speech signals into a finite set of discriminative speech tokens, and the latter trains the model to learn contextualized speech representations via solving a masked prediction task consuming the discretized tokens. In contrast to existing MLM-based speech pre-training frameworks such as HuBERT, which relies on an iterative re-clustering and re-training process, or vq-wav2vec, which concatenates two separately trained modules, w2v-BERT can be optimized in an end-to-end fashion by solving the two self-supervised tasks{\textasciitilde}(the contrastive task and MLM) simultaneously. Our experiments show that w2v-BERT achieves competitive results compared to current state-of-the-art pre-trained models on the LibriSpeech benchmarks when using the Libri-Light{\textasciitilde}60k corpus as the unsupervised data. In particular, when compared to published models such as conformer-based wav2vec{\textasciitilde}2.0 and HuBERT, our model shows{\textasciitilde}5{\textbackslash}\% to{\textasciitilde}10{\textbackslash}\% relative WER reduction on the test-clean and test-other subsets. When applied to the Google's Voice Search traffic dataset, w2v-BERT outperforms our internal conformer-based wav2vec{\textasciitilde}2.0 by more than{\textasciitilde}30{\textbackslash}\% relatively.},
	number = {arXiv:2108.06209},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Chung, Yu-An and Zhang, Yu and Han, Wei and Chiu, Chung-Cheng and Qin, James and Pang, Ruoming and Wu, Yonghui},
	month = sep,
	year = {2021},
	doi = {10.48550/arXiv.2108.06209},
	note = {arXiv:2108.06209 [cs, eess]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@inproceedings{levine_pmi-masking_2020,
	title = {{PMI}-{Masking}: {Principled} masking of correlated spans},
	shorttitle = {{PMI}-{Masking}},
	url = {https://openreview.net/forum?id=3Aoft6NWFej},
	abstract = {Masking tokens uniformly at random constitutes a common flaw in the pretraining of Masked Language Models (MLMs) such as BERT. We show that such uniform masking allows an MLM to minimize its...},
	language = {en},
	urldate = {2022-06-09},
	author = {Levine, Yoav and Lenz, Barak and Lieber, Opher and Abend, Omri and Leyton-Brown, Kevin and Tennenholtz, Moshe and Shoham, Yoav},
	month = sep,
	year = {2020},
}

@inproceedings{li_mst_2021,
	title = {{MST}: {Masked} {Self}-{Supervised} {Transformer} for {Visual} {Representation}},
	volume = {34},
	shorttitle = {{MST}},
	url = {https://proceedings.neurips.cc/paper/2021/hash/6dbbe6abe5f14af882ff977fc3f35501-Abstract.html},
	abstract = {Transformer has been widely used for self-supervised pre-training in Natural Language Processing (NLP) and achieved great success. However, it has not been fully explored in visual self-supervised learning. Meanwhile, previous methods only consider the high-level feature and learning representation from a global perspective, which may fail to transfer to the downstream dense prediction tasks focusing on local features. In this paper, we present a novel Masked Self-supervised Transformer approach named MST, which can explicitly capture the local context of an image while preserving the global semantic information. Specifically, inspired by the Masked Language Modeling (MLM) in NLP, we propose a masked token strategy based on the multi-head self-attention map, which dynamically masks some tokens of local patches without damaging the crucial structure for self-supervised learning. More importantly, the masked tokens together with the remaining tokens are further recovered by a global image decoder, which preserves the spatial information of the image and is more friendly to the downstream dense prediction tasks. The experiments on multiple datasets demonstrate the effectiveness and generality of the proposed method. For instance, MST achieves Top-1 accuracy of 76.9\% with DeiT-S only using 300-epoch pre-training by linear evaluation, which outperforms supervised methods with the same epoch by 0.4\% and its comparable variant DINO by 1.0\%. For dense prediction tasks, MST also achieves 42.7\% mAP on MS COCO object detection and 74.04\% mIoU on Cityscapes segmentation only with 100-epoch pre-training.},
	urldate = {2022-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Li, Zhaowen and Chen, Zhiyang and Yang, Fan and Li, Wei and Zhu, Yousong and Zhao, Chaoyang and Deng, Rui and Wu, Liwei and Zhao, Rui and Tang, Ming and Wang, Jinqiao},
	year = {2021},
	pages = {13165--13176},
}

@techreport{kallidromitis_contrastive_2021,
	title = {Contrastive {Neural} {Processes} for {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2110.13623},
	abstract = {Recent contrastive methods show significant improvement in self-supervised learning in several domains. In particular, contrastive methods are most effective where data augmentation can be easily constructed e.g. in computer vision. However, they are less successful in domains without established data transformations such as time series data. In this paper, we propose a novel self-supervised learning framework that combines contrastive learning with neural processes. It relies on recent advances in neural processes to perform time series forecasting. This allows to generate augmented versions of data by employing a set of various sampling functions and, hence, avoid manually designed augmentations. We extend conventional neural processes and propose a new contrastive loss to learn times series representations in a self-supervised setup. Therefore, unlike previous self-supervised methods, our augmentation pipeline is task-agnostic, enabling our method to perform well across various applications. In particular, a ResNet with a linear classifier trained using our approach is able to outperform state-of-the-art techniques across industrial, medical and audio datasets improving accuracy over 10\% in ECG periodic data. We further demonstrate that our self-supervised representations are more efficient in the latent space, improving multiple clustering indexes and that fine-tuning our method on 10\% of labels achieves results competitive to fully-supervised learning.},
	number = {arXiv:2110.13623},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Kallidromitis, Konstantinos and Gudovskiy, Denis and Kozuka, Kazuki and Ohama, Iku and Rigazio, Luca},
	month = dec,
	year = {2021},
	doi = {10.48550/arXiv.2110.13623},
	note = {arXiv:2110.13623 [cs, eess]
type: article},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing},
}

@techreport{mathieu_contrastive_2021,
	title = {On {Contrastive} {Representations} of {Stochastic} {Processes}},
	url = {http://arxiv.org/abs/2106.10052},
	abstract = {Learning representations of stochastic processes is an emerging problem in machine learning with applications from meta-learning to physical object models to time series. Typical methods rely on exact reconstruction of observations, but this approach breaks down as observations become high-dimensional or noise distributions become complex. To address this, we propose a unifying framework for learning contrastive representations of stochastic processes (CReSP) that does away with exact reconstruction. We dissect potential use cases for stochastic process representations, and propose methods that accommodate each. Empirically, we show that our methods are effective for learning representations of periodic functions, 3D objects and dynamical processes. Our methods tolerate noisy high-dimensional observations better than traditional approaches, and the learned representations transfer to a range of downstream tasks.},
	number = {arXiv:2106.10052},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Mathieu, Emile and Foster, Adam and Teh, Yee Whye},
	month = oct,
	year = {2021},
	note = {arXiv:2106.10052 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{cao_how_2022,
	title = {How to {Understand} {Masked} {Autoencoders}},
	url = {http://arxiv.org/abs/2202.03670},
	abstract = {"Masked Autoencoders (MAE) Are Scalable Vision Learners" revolutionizes the self-supervised learning method in that it not only achieves the state-of-the-art for image pre-training, but is also a milestone that bridges the gap between visual and linguistic masked autoencoding (BERT-style) pre-trainings. However, to our knowledge, to date there are no theoretical perspectives to explain the powerful expressivity of MAE. In this paper, we, for the first time, propose a unified theoretical framework that provides a mathematical understanding for MAE. Specifically, we explain the patch-based attention approaches of MAE using an integral kernel under a non-overlapping domain decomposition setting. To help the research community to further comprehend the main reasons of the great success of MAE, based on our framework, we pose five questions and answer them with mathematical rigor using insights from operator theory.},
	number = {arXiv:2202.03670},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Cao, Shuhao and Xu, Peng and Clifton, David A.},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.03670},
	note = {arXiv:2202.03670 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{wei_why_2022,
	title = {Why {Do} {Pretrained} {Language} {Models} {Help} in {Downstream} {Tasks}? {An} {Analysis} of {Head} and {Prompt} {Tuning}},
	shorttitle = {Why {Do} {Pretrained} {Language} {Models} {Help} in {Downstream} {Tasks}?},
	url = {http://arxiv.org/abs/2106.09226},
	abstract = {Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.},
	number = {arXiv:2106.09226},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2106.09226},
	note = {arXiv:2106.09226 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{zhang_inductive_2021-1,
	title = {On the {Inductive} {Bias} of {Masked} {Language} {Modeling}: {From} {Statistical} to {Syntactic} {Dependencies}},
	shorttitle = {On the {Inductive} {Bias} of {Masked} {Language} {Modeling}},
	url = {http://arxiv.org/abs/2104.05694},
	abstract = {We study how masking and predicting tokens in an unsupervised fashion can give rise to linguistic structures and downstream performance gains. Recent theories have suggested that pretrained language models acquire useful inductive biases through masks that implicitly act as cloze reductions for downstream tasks. While appealing, we show that the success of the random masking strategy used in practice cannot be explained by such cloze-like masks alone. We construct cloze-like masks using task-specific lexicons for three different classification datasets and show that the majority of pretrained performance gains come from generic masks that are not associated with the lexicon. To explain the empirical success of these generic masks, we demonstrate a correspondence between the Masked Language Model (MLM) objective and existing methods for learning statistical dependencies in graphical models. Using this, we derive a method for extracting these learned statistical dependencies in MLMs and show that these dependencies encode useful inductive biases in the form of syntactic structures. In an unsupervised parsing evaluation, simply forming a minimum spanning tree on the implied statistical dependence structure outperforms a classic method for unsupervised parsing (58.74 vs. 55.91 UUAS).},
	number = {arXiv:2104.05694},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Zhang, Tianyi and Hashimoto, Tatsunori},
	month = apr,
	year = {2021},
	doi = {10.48550/arXiv.2104.05694},
	note = {arXiv:2104.05694 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{von_kugelgen_self-supervised_2022,
	title = {Self-{Supervised} {Learning} with {Data} {Augmentations} {Provably} {Isolates} {Content} from {Style}},
	url = {http://arxiv.org/abs/2106.04619},
	abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
	number = {arXiv:2106.04619},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
	month = jan,
	year = {2022},
	doi = {10.48550/arXiv.2106.04619},
	note = {arXiv:2106.04619 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} {We} {Know} {About} {How} {BERT} {Works}},
	volume = {8},
	shorttitle = {A {Primer} in {BERTology}},
	url = {https://aclanthology.org/2020.tacl-1.54},
	doi = {10.1162/tacl_a_00349},
	abstract = {Transformer-based models have pushed state of the art in many areas of NLP, but our understanding of what is behind their success is still limited. This paper is the first survey of over 150 studies of the popular BERT model. We review the current state of knowledge about how BERT works, what kind of information it learns and how it is represented, common modifications to its training objectives and architecture, the overparameterization issue, and approaches to compression. We then outline directions for future research.},
	urldate = {2022-06-09},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	year = {2020},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {842--866},
}

@inproceedings{mosbach_interplay_2020,
	address = {Online},
	title = {On the {Interplay} {Between} {Fine}-tuning and {Sentence}-{Level} {Probing} for {Linguistic} {Knowledge} in {Pre}-{Trained} {Transformers}},
	url = {https://aclanthology.org/2020.blackboxnlp-1.7},
	doi = {10.18653/v1/2020.blackboxnlp-1.7},
	abstract = {Fine-tuning pre-trained contextualized embedding models has become an integral part of the NLP pipeline. At the same time, probing has emerged as a way to investigate the linguistic knowledge captured by pre-trained models. Very little is, however, understood about how fine-tuning affects the representations of pre-trained models and thereby the linguistic knowledge they encode. This paper contributes towards closing this gap. We study three different pre-trained models: BERT, RoBERTa, and ALBERT, and investigate through sentence-level probing how fine-tuning affects their representations. We find that for some probing tasks fine-tuning leads to substantial changes in accuracy, possibly suggesting that fine-tuning introduces or even removes linguistic knowledge from a pre-trained model. These changes, however, vary greatly across different models, fine-tuning and probing tasks. Our analysis reveals that while fine-tuning indeed changes the representations of a pre-trained model and these changes are typically larger for higher layers, only in very few cases, fine-tuning has a positive effect on probing accuracy that is larger than just using the pre-trained model with a strong pooling method. Based on our findings, we argue that both positive and negative effects of fine-tuning on probing require a careful interpretation.},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Mosbach, Marius and Khokhlova, Anna and Hedderich, Michael A. and Klakow, Dietrich},
	month = nov,
	year = {2020},
	pages = {68--82},
}

@techreport{wu_deep_2020,
	title = {Deep {Transformer} {Models} for {Time} {Series} {Forecasting}: {The} {Influenza} {Prevalence} {Case}},
	shorttitle = {Deep {Transformer} {Models} for {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2001.08317},
	abstract = {In this paper, we present a new approach to time series forecasting. Time series data are prevalent in many scientific and engineering disciplines. Time series forecasting is a crucial task in modeling time series data, and is an important area of machine learning. In this work we developed a novel method that employs Transformer-based machine learning models to forecast time series data. This approach works by leveraging self-attention mechanisms to learn complex patterns and dynamics from time series data. Moreover, it is a generic framework and can be applied to univariate and multivariate time series data, as well as time series embeddings. Using influenza-like illness (ILI) forecasting as a case study, we show that the forecasting results produced by our approach are favorably comparable to the state-of-the-art.},
	number = {arXiv:2001.08317},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Wu, Neo and Green, Bradley and Ben, Xue and O'Banion, Shawn},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08317 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{merchant_what_2020,
	address = {Online},
	title = {What {Happens} {To} {BERT} {Embeddings} {During} {Fine}-tuning?},
	url = {https://aclanthology.org/2020.blackboxnlp-1.4},
	doi = {10.18653/v1/2020.blackboxnlp-1.4},
	abstract = {While much recent work has examined how linguistic information is encoded in pre-trained sentence representations, comparatively little is understood about how these models change when adapted to solve downstream tasks. Using a suite of analysis techniques—supervised probing, unsupervised similarity analysis, and layer-based ablations—we investigate how fine-tuning affects the representations of the BERT model. We find that while fine-tuning necessarily makes some significant changes, there is no catastrophic forgetting of linguistic phenomena. We instead find that fine-tuning is a conservative process that primarily affects the top layers of BERT, albeit with noteworthy variation across tasks. In particular, dependency parsing reconfigures most of the model, whereas SQuAD and MNLI involve much shallower processing. Finally, we also find that fine-tuning has a weaker effect on representations of out-of-domain sentences, suggesting room for improvement in model generalization.},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Merchant, Amil and Rahimtoroghi, Elahe and Pavlick, Ellie and Tenney, Ian},
	month = nov,
	year = {2020},
	pages = {33--44},
}

@article{kumar_fine-tuning_2022,
	title = {{FINE}-{TUNING} {CAN} {DISTORT} {PRETRAINED} {FEATURES} {AND} {UNDERPERFORM} {OUT}-{OF}-{DISTRIBUTION}},
	abstract = {When transferring a pretrained model to a downstream task, two popular methods are full ﬁne-tuning (updating all the model parameters) and linear probing (updating only the last linear layer—the “head”). It is well known that ﬁne-tuning leads to better accuracy in-distribution (ID). However, in this paper, we ﬁnd that ﬁnetuning can achieve worse accuracy than linear probing out-of-distribution (OOD) when the pretrained features are good and the distribution shift is large. On 10 distribution shift datasets (BREEDS-Living17, BREEDS-Entity30, DomainNet, CIFAR → STL, CIFAR-10.1, FMoW, ImageNetV2, ImageNet-R, ImageNet-A, ImageNet-Sketch), ﬁne-tuning obtains on average 2\% higher accuracy ID but 7\% lower accuracy OOD than linear probing. We show theoretically that this tradeoff between ID and OOD accuracy arises even in a simple setting: ﬁne-tuning overparameterized two-layer linear networks. Our analysis suggests that the easy two-step strategy of linear probing then full ﬁne-tuning (LP-FT), sometimes used as a ﬁne-tuning heuristic, combines the beneﬁts of both ﬁne-tuning and linear probing. Empirically, LP-FT outperforms both ﬁne-tuning and linear probing on the above datasets (1\% better ID, 10\% better OOD than full ﬁne-tuning).},
	language = {en},
	author = {Kumar, Ananya and Raghunathan, Aditi and Jones, Robbie and Ma, Tengyu and Liang, Percy},
	year = {2022},
	pages = {42},
}

@article{mosbach_stability_2021,
	title = {{ON} {THE} {STABILITY} {OF} {FINE}-{TUNING} {BERT}: {MISCON}- {CEPTIONS}, {EXPLANATIONS}, {AND} {STRONG} {BASELINES}},
	abstract = {Fine-tuning pre-trained transformer-based language models such as BERT has become a common practice dominating leaderboards across various NLP benchmarks. Despite the strong empirical performance of ﬁne-tuned models, ﬁne-tuning is an unstable process: training the same model with multiple random seeds can result in a large variance of the task performance. Previous literature (Devlin et al., 2019; Lee et al., 2020; Dodge et al., 2020) identiﬁed two potential reasons for the observed instability: catastrophic forgetting and small size of the ﬁne-tuning datasets. In this paper, we show that both hypotheses fail to explain the ﬁne-tuning instability. We analyze BERT, RoBERTa, and ALBERT, ﬁne-tuned on commonly used datasets from the GLUE benchmark, and show that the observed instability is caused by optimization difﬁculties that lead to vanishing gradients. Additionally, we show that the remaining variance of the downstream task performance can be attributed to differences in generalization where ﬁne-tuned models with the same training loss exhibit noticeably different test performance. Based on our analysis, we present a simple but strong baseline that makes ﬁne-tuning BERT-based models signiﬁcantly more stable than the previously proposed approaches. Code to reproduce our results is available online: https://github.com/uds-lsv/bert-stable-fine-tuning.},
	language = {en},
	author = {Mosbach, Marius and Andriushchenko, Maksym and Klakow, Dietrich},
	year = {2021},
	pages = {19},
}

@inproceedings{chen_empirical_2021,
	address = {Montreal, QC, Canada},
	title = {An {Empirical} {Study} of {Training} {Self}-{Supervised} {Vision} {Transformers}},
	isbn = {978-1-66542-812-5},
	url = {https://ieeexplore.ieee.org/document/9711302/},
	doi = {10.1109/ICCV48922.2021.00950},
	abstract = {This paper does not describe a novel method. Instead, it studies a straightforward, incremental, yet must-know baseline given the recent progress in computer vision: selfsupervised learning for Vision Transformers (ViT). While the training recipes for standard convolutional networks have been highly mature and robust, the recipes for ViT are yet to be built, especially in the self-supervised scenarios where training becomes more challenging. In this work, we go back to basics and investigate the effects of several fundamental components for training self-supervised ViT. We observe that instability is a major issue that degrades accuracy, and it can be hidden by apparently good results. We reveal that these results are indeed partial failure, and they can be improved when training is made more stable. We benchmark ViT results in MoCo v3 and several other selfsupervised frameworks, with ablations in various aspects. We discuss the currently positive evidence as well as challenges and open questions. We hope that this work will provide useful data points and experience for future research.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2021 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Chen, Xinlei and Xie, Saining and He, Kaiming},
	month = oct,
	year = {2021},
	pages = {9620--9629},
}

@inproceedings{reif_visualizing_2019,
	title = {Visualizing and {Measuring} the {Geometry} of {BERT}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html},
	abstract = {Transformer architectures show significant promise for natural language processing. Given that a single pretrained model can be fine-tuned to perform well on many different tasks, these networks appear to extract generally useful linguistic features. A natural question is how such networks represent this information internally. This paper describes qualitative and quantitative investigations of one particularly effective model, BERT. At a high level, linguistic features seem to be represented in separate semantic and syntactic subspaces. We find evidence of a fine-grained geometric representation of word senses. We also present empirical descriptions of syntactic representations in both attention matrices and individual word embeddings, as well as a mathematical argument to explain the geometry of these representations.},
	urldate = {2022-06-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viegas, Fernanda B and Coenen, Andy and Pearce, Adam and Kim, Been},
	year = {2019},
}

@techreport{song_mass_2019,
	title = {{MASS}: {Masked} {Sequence} to {Sequence} {Pre}-training for {Language} {Generation}},
	shorttitle = {{MASS}},
	url = {http://arxiv.org/abs/1905.02450},
	abstract = {Pre-training and fine-tuning, e.g., BERT, have achieved great success in language understanding by transferring knowledge from rich-resource pre-training task to the low/zero-resource downstream tasks. Inspired by the success of BERT, we propose MAsked Sequence to Sequence pre-training (MASS) for the encoder-decoder based language generation tasks. MASS adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence: its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input, and its decoder tries to predict this masked fragment. In this way, MASS can jointly train the encoder and decoder to develop the capability of representation extraction and language modeling. By further fine-tuning on a variety of zero/low-resource language generation tasks, including neural machine translation, text summarization and conversational response generation (3 tasks and totally 8 datasets), MASS achieves significant improvements over the baselines without pre-training or with other pre-training methods. Specially, we achieve the state-of-the-art accuracy (37.5 in terms of BLEU score) on the unsupervised English-French translation, even beating the early attention-based supervised model.},
	number = {arXiv:1905.02450},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Song, Kaitao and Tan, Xu and Qin, Tao and Lu, Jianfeng and Liu, Tie-Yan},
	month = jun,
	year = {2019},
	note = {arXiv:1905.02450 [cs]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{mahmood_masked_nodate,
	title = {Masked {Graph} {Modeling} for {Molecule} {Generation}},
	language = {en},
	author = {Mahmood, Omar and Mansimov, Elman and Bonneau, Richard and Cho, Kyunghyun},
	pages = {36},
}

@inproceedings{chen_generative_2020,
	title = {Generative {Pretraining} {From} {Pixels}},
	url = {https://proceedings.mlr.press/v119/chen20s.html},
	abstract = {Inspired by progress in unsupervised representation learning for natural language, we examine whether similar models can learn useful representations for images. We train a sequence Transformer to auto-regressively predict pixels, without incorporating knowledge of the 2D input structure. Despite training on low-resolution ImageNet without labels, we find that a GPT-2 scale model learns strong image representations as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10, we achieve 96.3\% accuracy with a linear probe, outperforming a supervised Wide ResNet, and 99.0\% accuracy with full fine-tuning, matching the top supervised pre-trained models. We are also competitive with self-supervised benchmarks on ImageNet when substituting pixels for a VQVAE encoding, achieving 69.0\% top-1 accuracy on a linear probe of our features.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun, Heewoo and Luan, David and Sutskever, Ilya},
	month = nov,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {1691--1703},
}

@inproceedings{vincent_extracting_2008,
	address = {Helsinki, Finland},
	title = {Extracting and composing robust features with denoising autoencoders},
	isbn = {978-1-60558-205-4},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390294},
	doi = {10.1145/1390156.1390294},
	abstract = {Previous work has shown that the diﬃculties in learning deep generative or discriminative models can be overcome by an initial unsupervised learning step that maps inputs to useful intermediate representations. We introduce and motivate a new training principle for unsupervised learning of a representation based on the idea of making the learned representations robust to partial corruption of the input pattern. This approach can be used to train autoencoders, and these denoising autoencoders can be stacked to initialize deep architectures. The algorithm can be motivated from a manifold learning and information theoretic perspective or from a generative model perspective. Comparative experiments clearly show the surprising advantage of corrupting the input of autoencoders on a pattern classiﬁcation benchmark suite.},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {Proceedings of the 25th international conference on {Machine} learning - {ICML} '08},
	publisher = {ACM Press},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	year = {2008},
	pages = {1096--1103},
}

@article{vincent_stacked_nodate,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
	language = {en},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	pages = {38},
}

@article{vincent_stacked_nodate-1,
	title = {Stacked {Denoising} {Autoencoders}: {Learning} {Useful} {Representations} in a {Deep} {Network} with a {Local} {Denoising} {Criterion}},
	abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classiﬁcation problems to yield signiﬁcantly lower classiﬁcation error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classiﬁers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
	language = {en},
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	pages = {38},
}

@techreport{gao_simcse_2022,
	title = {{SimCSE}: {Simple} {Contrastive} {Learning} of {Sentence} {Embeddings}},
	shorttitle = {{SimCSE}},
	url = {http://arxiv.org/abs/2104.08821},
	abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
	number = {arXiv:2104.08821},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
	month = may,
	year = {2022},
	note = {arXiv:2104.08821 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{gao_simcse_2022-1,
	title = {{SimCSE}: {Simple} {Contrastive} {Learning} of {Sentence} {Embeddings}},
	shorttitle = {{SimCSE}},
	url = {http://arxiv.org/abs/2104.08821},
	abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
	number = {arXiv:2104.08821},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
	month = may,
	year = {2022},
	note = {arXiv:2104.08821 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@techreport{rahimian_distributionally_2019,
	title = {Distributionally {Robust} {Optimization}: {A} {Review}},
	shorttitle = {Distributionally {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1908.05659},
	abstract = {The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. Statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. A modeling framework, called distributionally robust optimization (DRO), has recently received significant attention in both the operations research and statistical learning communities. This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.},
	number = {arXiv:1908.05659},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {Rahimian, Hamed and Mehrotra, Sanjay},
	month = aug,
	year = {2019},
	note = {arXiv:1908.05659 [cs, math, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{wang_robust_2021,
	address = {Melbourne, Australia},
	title = {Robust {Machine} {Learning} via {Privacy}/ {Rate}-{Distortion} {Theory}},
	isbn = {978-1-5386-8209-8},
	url = {https://ieeexplore.ieee.org/document/9517751/},
	doi = {10.1109/ISIT45174.2021.9517751},
	language = {en},
	urldate = {2022-06-09},
	booktitle = {2021 {IEEE} {International} {Symposium} on {Information} {Theory} ({ISIT})},
	publisher = {IEEE},
	author = {Wang, Ye and Aeron, Shuchin and Rakin, Adnan Siraj and Koike-Akino, Toshiaki and Moulin, Pierre},
	month = jul,
	year = {2021},
	pages = {1320--1325},
}

@techreport{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	number = {arXiv:2111.06377},
	urldate = {2022-06-09},
	institution = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]
version: 2
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{touvron_training_nodate,
	title = {Training data-efﬁcient image transformers \& distillation through attention},
	abstract = {Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classiﬁcation. These highperforming vision transformers are pre-trained with hundreds of millions of images using a large infrastructure, thereby limiting their adoption.},
	language = {en},
	author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jegou, Herve and Ai, Facebook},
	pages = {22},
}

@article{liu_kernel_nodate,
	title = {Kernel {Robust} {Bias}-{Aware} {Prediction} under {Covariate} {Shift}},
	abstract = {Under covariate shift, training (source) data and testing (target) data differ in input space distribution, but share the same conditional label distribution. This poses a challenging machine learning task. Robust Bias-Aware (RBA) prediction provides the conditional label distribution that is robust to the worstcase logarithmic loss for the target distribution while matching feature expectation constraints from the source distribution. However, employing RBA with insufﬁcient feature constraints may result in high certainty predictions for much of the source data, while leaving too much uncertainty for target data predictions. To overcome this issue, we extend the representer theorem to the RBA setting, enabling minimization of regularized expected target risk by a reweighted kernel expectation under the source distribution. By applying kernel methods, we establish consistency guarantees and demonstrate better performance of the RBA classiﬁer than competing methods on synthetically biased UCI datasets as well as datasets that have natural covariate shift.},
	language = {en},
	author = {Liu, Anqi and Fathony, Rizal and Ziebart, Brian D},
	pages = {10},
}

@article{ziebart_modeling_nodate,
	title = {Modeling {Interaction} via the {Principle} of {Maximum} {Causal} {Entropy}},
	abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and inﬂuence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
	language = {en},
	author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
	pages = {8},
}

@techreport{gao_simcse_2022-2,
	title = {{SimCSE}: {Simple} {Contrastive} {Learning} of {Sentence} {Embeddings}},
	shorttitle = {{SimCSE}},
	url = {http://arxiv.org/abs/2104.08821},
	abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
	number = {arXiv:2104.08821},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
	month = may,
	year = {2022},
	note = {arXiv:2104.08821 [cs]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@book{blanchet_doubly_2021,
	title = {Doubly {Robust} {Data}-{Driven} {Distributionally} {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1705.07168},
	abstract = {Data-driven Distributionally Robust Optimization (DD-DRO) via optimal transport has been shown to encompass a wide range of popular machine learning algorithms. The distributional uncertainty size is often shown to correspond to the regularization parameter. The type of regularization (e.g. the norm used to regularize) corresponds to the shape of the distributional uncertainty. We propose a data-driven robust optimization methodology to inform the transportation cost underlying the definition of the distributional uncertainty. We show empirically that this additional layer of robustification, which produces a method we called doubly robust data-driven distributionally robust optimization (DD-R-DRO), allows to enhance the generalization properties of regularized estimators while reducing testing error relative to state-of-the-art classifiers in a wide range of data sets.},
	urldate = {2022-06-08},
	author = {Blanchet, Jose and Kang, Yang and Zhang, Fan and He, Fei and Hu, Zhangyi},
	month = apr,
	year = {2021},
	doi = {10.1002/9781119821588.ch4},
	note = {arXiv:1705.07168 [stat]},
	keywords = {Statistics - Machine Learning},
}

@article{blanchet_robust_2019,
	title = {Robust {Wasserstein} {Profile} {Inference} and {Applications} to {Machine} {Learning}},
	volume = {56},
	issn = {0021-9002, 1475-6072},
	url = {http://arxiv.org/abs/1610.05627},
	doi = {10.1017/jpr.2019.49},
	abstract = {We show that several machine learning estimators, including square-root LASSO (Least Absolute Shrinkage and Selection) and regularized logistic regression can be represented as solutions to distributionally robust optimization (DRO) problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (Robust Wasserstein Profile Inference), a novel inference methodology which extends the use of methods inspired by Empirical Likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence, we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.},
	number = {3},
	urldate = {2022-06-08},
	journal = {Journal of Applied Probability},
	author = {Blanchet, Jose and Kang, Yang and Murthy, Karthyek},
	month = sep,
	year = {2019},
	note = {arXiv:1610.05627 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
	pages = {830--857},
}

@techreport{rahimian_distributionally_2019-1,
	title = {Distributionally {Robust} {Optimization}: {A} {Review}},
	shorttitle = {Distributionally {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1908.05659},
	abstract = {The concepts of risk-aversion, chance-constrained optimization, and robust optimization have developed significantly over the last decade. Statistical learning community has also witnessed a rapid theoretical and applied growth by relying on these concepts. A modeling framework, called distributionally robust optimization (DRO), has recently received significant attention in both the operations research and statistical learning communities. This paper surveys main concepts and contributions to DRO, and its relationships with robust optimization, risk-aversion, chance-constrained optimization, and function regularization.},
	number = {arXiv:1908.05659},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Rahimian, Hamed and Mehrotra, Sanjay},
	month = aug,
	year = {2019},
	note = {arXiv:1908.05659 [cs, math, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{delage_distributionally_2010,
	title = {Distributionally {Robust} {Optimization} {Under} {Moment} {Uncertainty} with {Application} to {Data}-{Driven} {Problems}},
	volume = {58},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1090.0741},
	doi = {10.1287/opre.1090.0741},
	language = {en},
	number = {3},
	urldate = {2022-06-08},
	journal = {Operations Research},
	author = {Delage, Erick and Ye, Yinyu},
	month = jun,
	year = {2010},
	pages = {595--612},
}

@techreport{bertsimas_data-driven_2014,
	title = {Data-{Driven} {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1401.0212},
	abstract = {The last decade witnessed an explosion in the availability of data for operations research applications. Motivated by this growing availability, we propose a novel schema for utilizing data to design uncertainty sets for robust optimization using statistical hypothesis tests. The approach is flexible and widely applicable, and robust optimization problems built from our new sets are computationally tractable, both theoretically and practically. Furthermore, optimal solutions to these problems enjoy a strong, finite-sample probabilistic guarantee. {\textbackslash}edit\{\vphantom{\}}We describe concrete procedures for choosing an appropriate set for a given application and applying our approach to multiple uncertain constraints. Computational evidence in portfolio management and queuing confirm that our data-driven sets significantly outperform traditional robust optimization techniques whenever data is available.},
	number = {arXiv:1401.0212},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Bertsimas, Dimitris and Gupta, Vishal and Kallus, Nathan},
	month = nov,
	year = {2014},
	doi = {10.48550/arXiv.1401.0212},
	note = {arXiv:1401.0212 [math]
type: article},
	keywords = {Mathematics - Optimization and Control},
}

@techreport{duchi_statistics_2018,
	title = {Statistics of {Robust} {Optimization}: {A} {Generalized} {Empirical} {Likelihood} {Approach}},
	shorttitle = {Statistics of {Robust} {Optimization}},
	url = {http://arxiv.org/abs/1610.03425},
	abstract = {We study statistical inference and distributionally robust solution methods for stochastic optimization problems, focusing on confidence intervals for optimal values and solutions that achieve exact coverage asymptotically. We develop a generalized empirical likelihood framework---based on distributional uncertainty sets constructed from nonparametric \$f\$-divergence balls---for Hadamard differentiable functionals, and in particular, stochastic optimization problems. As consequences of this theory, we provide a principled method for choosing the size of distributional uncertainty regions to provide one- and two-sided confidence intervals that achieve exact coverage. We also give an asymptotic expansion for our distributionally robust formulation, showing how robustification regularizes problems by their variance. Finally, we show that optimizers of the distributionally robust formulations we study enjoy (essentially) the same consistency properties as those in classical sample average approximations. Our general approach applies to quickly mixing stationary sequences, including geometrically ergodic Harris recurrent Markov chains.},
	number = {arXiv:1610.03425},
	urldate = {2022-06-08},
	institution = {arXiv},
	author = {Duchi, John and Glynn, Peter and Namkoong, Hongseok},
	month = jun,
	year = {2018},
	note = {arXiv:1610.03425 [stat]
type: article},
	keywords = {Statistics - Machine Learning},
}

@inproceedings{lafferty_additive_1999,
	address = {New York, NY, USA},
	series = {{COLT} '99},
	title = {Additive models, boosting, and inference for generalized divergences},
	isbn = {978-1-58113-167-3},
	url = {https://doi.org/10.1145/307400.307422},
	doi = {10.1145/307400.307422},
	urldate = {2022-06-07},
	booktitle = {Proceedings of the twelfth annual conference on {Computational} learning theory},
	publisher = {Association for Computing Machinery},
	author = {Lafferty, John},
	month = jul,
	year = {1999},
	pages = {125--133},
}

@article{koopman_distributions_1936,
	title = {On distributions admitting a sufficient statistic},
	volume = {39},
	number = {3},
	journal = {Transactions of the American Mathematical society},
	author = {Koopman, Bernard Osgood},
	year = {1936},
	note = {Publisher: JSTOR},
	pages = {399--409},
}

@article{darmois_sur_1935,
	title = {Sur les lois de probabilitéa estimation exhaustive},
	volume = {260},
	number = {1265},
	journal = {CR Acad. Sci. Paris},
	author = {Darmois, Georges},
	year = {1935},
	pages = {85},
}

@article{pitman_sufficient_1936,
	title = {Sufficient statistics and intrinsic accuracy},
	volume = {32},
	issn = {1469-8064, 0305-0041},
	url = {https://www.cambridge.org/core/journals/mathematical-proceedings-of-the-cambridge-philosophical-society/article/sufficient-statistics-and-intrinsic-accuracy/6A3E45FB1C423F3F684308F8910D6919},
	doi = {10.1017/S0305004100019307},
	abstract = {It is proved that if there exists a sufficient statistic for the estimation of an unknown parameter of a population, the frequency function of the population must be of a certain type.It is shown that some modification of previous theory of the intrinsic accuracy of statistics is necessary when the range of the population sampled is a function of the parameter to be estimated.Finally, the theory is extended to sufficient sets of statistics, i.e. sets of statistics which together contain all the information provided by a sample about an unknown parameter.},
	language = {en},
	number = {4},
	urldate = {2022-06-07},
	journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
	author = {Pitman, E. J. G.},
	month = dec,
	year = {1936},
	note = {Publisher: Cambridge University Press},
	pages = {567--579},
}

@article{csiszar_i-divergence_1975,
	title = {\${I}\$-{Divergence} {Geometry} of {Probability} {Distributions} and {Minimization} {Problems}},
	volume = {3},
	issn = {0091-1798},
	url = {https://www.jstor.org/stable/2959270},
	abstract = {Some geometric properties of PD's are established, Kullback's \$I\$-divergence playing the role of squared Euclidean distance. The minimum discrimination information problem is viewed as that of projecting a PD onto a convex set of PD's and useful existence theorems for and characterizations of the minimizing PD are arrived at. A natural generalization of known iterative algorithms converging to the minimizing PD in special situations is given; even for those special cases, our convergence proof is more generally valid than those previously published. As corollaries of independent interest, generalizations of known results on the existence of PD's or nonnegative matrices of a certain form are obtained. The Lagrange multiplier technique is not used.},
	number = {1},
	urldate = {2022-06-07},
	journal = {The Annals of Probability},
	author = {Csiszar, I.},
	year = {1975},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {146--158},
}

@techreport{you_large_2017,
	title = {Large {Batch} {Training} of {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1708.03888},
	abstract = {A common way to speed up training of large convolutional networks is to add computational units. Training is then performed using data-parallel synchronous Stochastic Gradient Descent (SGD) with mini-batch divided between computational units. With an increase in the number of nodes, the batch size grows. But training with large batch size often results in the lower model accuracy. We argue that the current recipe for large batch training (linear learning rate scaling with warm-up) is not general enough and training may diverge. To overcome this optimization difficulties we propose a new training algorithm based on Layer-wise Adaptive Rate Scaling (LARS). Using LARS, we scaled Alexnet up to a batch size of 8K, and Resnet-50 to a batch size of 32K without loss in accuracy.},
	number = {arXiv:1708.03888},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {You, Yang and Gitman, Igor and Ginsburg, Boris},
	month = sep,
	year = {2017},
	doi = {10.48550/arXiv.1708.03888},
	note = {arXiv:1708.03888 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@techreport{yun_small_2019,
	title = {Small {ReLU} networks are powerful memorizers: a tight analysis of memorization capacity},
	shorttitle = {Small {ReLU} networks are powerful memorizers},
	url = {http://arxiv.org/abs/1810.07770},
	abstract = {We study finite sample expressivity, i.e., memorization power of ReLU networks. Recent results require \$N\$ hidden nodes to memorize/interpolate arbitrary \$N\$ data points. In contrast, by exploiting depth, we show that 3-layer ReLU networks with \${\textbackslash}Omega({\textbackslash}sqrt\{N\})\$ hidden nodes can perfectly memorize most datasets with \$N\$ points. We also prove that width \${\textbackslash}Theta({\textbackslash}sqrt\{N\})\$ is necessary and sufficient for memorizing \$N\$ data points, proving tight bounds on memorization capacity. The sufficiency result can be extended to deeper networks; we show that an \$L\$-layer network with \$W\$ parameters in the hidden layers can memorize \$N\$ data points if \$W = {\textbackslash}Omega(N)\$. Combined with a recent upper bound \$O(WL{\textbackslash}log W)\$ on VC dimension, our construction is nearly tight for any fixed \$L\$. Subsequently, we analyze memorization capacity of residual networks under a general position assumption; we prove results that substantially reduce the known requirement of \$N\$ hidden nodes. Finally, we study the dynamics of stochastic gradient descent (SGD), and show that when initialized near a memorizing global minimum of the empirical risk, SGD quickly finds a nearby point with much smaller empirical risk.},
	number = {arXiv:1810.07770},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
	month = oct,
	year = {2019},
	note = {arXiv:1810.07770 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{bubeck_network_2020,
	title = {Network size and weights size for memorization with two-layers neural networks},
	url = {http://arxiv.org/abs/2006.02855},
	abstract = {In 1988, Eric B. Baum showed that two-layers neural networks with threshold activation function can perfectly memorize the binary labels of \$n\$ points in general position in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ using only \${\textbackslash}ulcorner n/d {\textbackslash}urcorner\$ neurons. We observe that with ReLU networks, using four times as many neurons one can fit arbitrary real labels. Moreover, for approximate memorization up to error \${\textbackslash}epsilon\$, the neural tangent kernel can also memorize with only \$O{\textbackslash}left({\textbackslash}frac\{n\}\{d\} {\textbackslash}cdot {\textbackslash}log(1/{\textbackslash}epsilon) {\textbackslash}right)\$ neurons (assuming that the data is well dispersed too). We show however that these constructions give rise to networks where the magnitude of the neurons' weights are far from optimal. In contrast we propose a new training procedure for ReLU networks, based on complex (as opposed to real) recombination of the neurons, for which we show approximate memorization with both \$O{\textbackslash}left({\textbackslash}frac\{n\}\{d\} {\textbackslash}cdot {\textbackslash}frac\{{\textbackslash}log(1/{\textbackslash}epsilon)\}\{{\textbackslash}epsilon\}{\textbackslash}right)\$ neurons, as well as nearly-optimal size of the weights.},
	number = {arXiv:2006.02855},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Bubeck, Sébastien and Eldan, Ronen and Lee, Yin Tat and Mikulincer, Dan},
	month = nov,
	year = {2020},
	note = {arXiv:2006.02855 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{vardi_optimal_2021,
	title = {On the {Optimal} {Memorization} {Power} of {ReLU} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2110.03187},
	abstract = {We study the memorization power of feedforward ReLU neural networks. We show that such networks can memorize any \$N\$ points that satisfy a mild separability assumption using \${\textbackslash}tilde\{O\}{\textbackslash}left({\textbackslash}sqrt\{N\}{\textbackslash}right)\$ parameters. Known VC-dimension upper bounds imply that memorizing \$N\$ samples requires \${\textbackslash}Omega({\textbackslash}sqrt\{N\})\$ parameters, and hence our construction is optimal up to logarithmic factors. We also give a generalized construction for networks with depth bounded by \$1 {\textbackslash}leq L {\textbackslash}leq {\textbackslash}sqrt\{N\}\$, for memorizing \$N\$ samples using \${\textbackslash}tilde\{O\}(N/L)\$ parameters. This bound is also optimal up to logarithmic factors. Our construction uses weights with large bit complexity. We prove that having such a large bit complexity is both necessary and sufficient for memorization with a sub-linear number of parameters.},
	number = {arXiv:2110.03187},
	urldate = {2022-05-26},
	institution = {arXiv},
	author = {Vardi, Gal and Yehudai, Gilad and Shamir, Ohad},
	month = oct,
	year = {2021},
	note = {arXiv:2110.03187 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{yun_small_2019-1,
	title = {Small {ReLU} networks are powerful memorizers: a tight analysis of memorization capacity},
	volume = {32},
	shorttitle = {Small {ReLU} networks are powerful memorizers},
	url = {https://proceedings.neurips.cc/paper/2019/hash/dbea3d0e2a17c170c412c74273778159-Abstract.html},
	urldate = {2022-05-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
	year = {2019},
}

@inproceedings{sontag_remarks_1990,
	title = {Remarks on {Interpolation} and {Recognition} {Using} {Neural} {Nets}},
	volume = {3},
	url = {https://papers.nips.cc/paper/1990/hash/2421fcb1263b9530df88f7f002e78ea5-Abstract.html},
	abstract = {We consider different  types  of single-hidden-Iayer feedforward  nets:  with  or  without  direct  input  to  output  connections,  and  using  either  thresh(cid:173) old  or  sigmoidal activation functions.  The  main results  show  that  direct  connections in  threshold nets  double  the  recognition  but not  the interpo(cid:173) lation power, while using sigmoids  rather than thresholds allows (at least)  doubling  both.  Various results are also given on VC dimension and  other  measures of recognition capabilities.},
	urldate = {2022-05-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Sontag, Eduardo},
	year = {1990},
}

@inproceedings{sakurai_n-h-1_1992,
	title = {n-h-1 networks store no less n*h+1 examples, but sometimes no more},
	volume = {3},
	doi = {10.1109/IJCNN.1992.227079},
	abstract = {The author shows that an n-h-1 artificial neural network with n real inputs, a single layer of h hidden units, and one binary output unit can store correctly at least n*h+1 examples in a general position. The proof is constructive so that weights are obtained deterministically from examples. The result is thought to be a generalization of the fact that one threshold gate can remember any n+1 examples in a general position. The number obtained is a good lower bound of the network capacity and is a great improvement on the previous best bound by S. Akaho and S. Amari (1990). It is also shown that the figure nh+1 is tight in a certain sense.{\textless}{\textgreater}},
	booktitle = {[{Proceedings} 1992] {IJCNN} {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Sakurai, A.},
	month = jun,
	year = {1992},
	keywords = {Artificial neural networks, Capacity planning, Circuits, Laboratories, Logic, Probability, Reservoirs, Upper bound, Vectors},
	pages = {936--941 vol.3},
}

@inproceedings{kowalczyk_dense_1997,
	address = {Nashville, Tennessee, United States},
	title = {Dense shattering and teaching dimensions for differentiable families (extended abstract)},
	isbn = {978-0-89791-891-6},
	url = {http://portal.acm.org/citation.cfm?doid=267460.267490},
	doi = {10.1145/267460.267490},
	language = {en},
	urldate = {2022-05-26},
	booktitle = {Proceedings of the tenth annual conference on {Computational} learning theory  - {COLT} '97},
	publisher = {ACM Press},
	author = {Kowalczyk, A.},
	year = {1997},
	pages = {143--151},
}

@article{gardner_space_1988,
	title = {The space of interactions in neural network models},
	volume = {21},
	issn = {0305-4470, 1361-6447},
	url = {https://iopscience.iop.org/article/10.1088/0305-4470/21/1/030},
	doi = {10.1088/0305-4470/21/1/030},
	abstract = {The typical fraction of the space of interactions between each pair of N Ising spins which solve the problem of storing a given set of p random patterns as N-bit spin configurations is considered. The volume is calculated explicitly as a function of the storage ratio, a = p / N , of the value K ( {\textgreater} O ) of the product of the spin and the magnetic field at each site and of the magnetisation, m. Here m may vary between 0 (no correlation) and 1 (completely correlated). The capacity increases with the correlation between patterns from a = 2 for correlated patterns with K = 0 and tends to infinity as m tends to 1. The calculations use a saddle-point method and the order parameters at the saddle point are assumed to be replica symmetric. This solution is shown to be locally stable. A local iterative learning algorithm for updating the interactions is given which will converge to a solution of given K provided such solutions exist.},
	language = {en},
	number = {1},
	urldate = {2022-05-26},
	journal = {Journal of Physics A: Mathematical and General},
	author = {Gardner, E},
	month = jan,
	year = {1988},
	pages = {257--270},
}

@techreport{caron_deep_2019,
	title = {Deep {Clustering} for {Unsupervised} {Learning} of {Visual} {Features}},
	url = {http://arxiv.org/abs/1807.05520},
	abstract = {Clustering is a class of unsupervised learning methods that has been extensively applied and studied in computer vision. Little work has been done to adapt it to the end-to-end training of visual features on large scale datasets. In this work, we present DeepCluster, a clustering method that jointly learns the parameters of a neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups the features with a standard clustering algorithm, k-means, and uses the subsequent assignments as supervision to update the weights of the network. We apply DeepCluster to the unsupervised training of convolutional neural networks on large datasets like ImageNet and YFCC100M. The resulting model outperforms the current state of the art by a significant margin on all the standard benchmarks.},
	number = {arXiv:1807.05520},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	month = mar,
	year = {2019},
	doi = {10.48550/arXiv.1807.05520},
	note = {arXiv:1807.05520 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@techreport{hjelm_learning_2019,
	title = {Learning deep representations by mutual information estimation and maximization},
	url = {http://arxiv.org/abs/1808.06670},
	abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
	number = {arXiv:1808.06670},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Hjelm, R. Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
	month = feb,
	year = {2019},
	doi = {10.48550/arXiv.1808.06670},
	note = {arXiv:1808.06670 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{hui_limitations_2022,
	title = {Limitations of {Neural} {Collapse} for {Understanding} {Generalization} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/2202.08384},
	abstract = {The recent work of Papyan, Han, \& Donoho (2020) presented an intriguing "Neural Collapse" phenomenon, showing a structural property of interpolating classifiers in the late stage of training. This opened a rich area of exploration studying this phenomenon. Our motivation is to study the upper limits of this research program: How far will understanding Neural Collapse take us in understanding deep learning? First, we investigate its role in generalization. We refine the Neural Collapse conjecture into two separate conjectures: collapse on the train set (an optimization property) and collapse on the test distribution (a generalization property). We find that while Neural Collapse often occurs on the train set, it does not occur on the test set. We thus conclude that Neural Collapse is primarily an optimization phenomenon, with as-yet-unclear connections to generalization. Second, we investigate the role of Neural Collapse in feature learning. We show simple, realistic experiments where training longer leads to worse last-layer features, as measured by transfer-performance on a downstream task. This suggests that neural collapse is not always desirable for representation learning, as previously claimed. Finally, we give preliminary evidence of a "cascading collapse" phenomenon, wherein some form of Neural Collapse occurs not only for the last layer, but in earlier layers as well. We hope our work encourages the community to continue the rich line of Neural Collapse research, while also considering its inherent limitations.},
	number = {arXiv:2202.08384},
	urldate = {2022-05-25},
	institution = {arXiv},
	author = {Hui, Like and Belkin, Mikhail and Nakkiran, Preetum},
	month = feb,
	year = {2022},
	doi = {10.48550/arXiv.2202.08384},
	note = {arXiv:2202.08384 [cs, stat]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{foster_improving_2021,
	title = {Improving {Transformation} {Invariance} in {Contrastive} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2010.09515},
	abstract = {We propose methods to strengthen the invariance properties of representations obtained by contrastive learning. While existing approaches implicitly induce a degree of invariance as representations are learned, we look to more directly enforce invariance in the encoding process. To this end, we first introduce a training objective for contrastive learning that uses a novel regularizer to control how the representation changes under transformation. We show that representations trained with this objective perform better on downstream tasks and are more robust to the introduction of nuisance transformations at test time. Second, we propose a change to how test time representations are generated by introducing a feature averaging approach that combines encodings from multiple transformations of the original input, finding that this leads to across the board performance gains. Finally, we introduce the novel Spirograph dataset to explore our ideas in the context of a differentiable generative process with multiple downstream tasks, showing that our techniques for learning invariance are highly beneficial.},
	number = {arXiv:2010.09515},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Foster, Adam and Pukdee, Rattana and Rainforth, Tom},
	month = mar,
	year = {2021},
	note = {arXiv:2010.09515 [cs, stat]
type: article},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{awasthi_more_2022,
	title = {Do {More} {Negative} {Samples} {Necessarily} {Hurt} in {Contrastive} {Learning}?},
	url = {http://arxiv.org/abs/2205.01789},
	abstract = {Recent investigations in noise contrastive estimation suggest, both empirically as well as theoretically, that while having more "negative samples" in the contrastive loss improves downstream classification performance initially, beyond a threshold, it hurts downstream performance due to a "collision-coverage" trade-off. But is such a phenomenon inherent in contrastive learning? We show in a simple theoretical setting, where positive pairs are generated by sampling from the underlying latent class (introduced by Saunshi et al. (ICML 2019)), that the downstream performance of the representation optimizing the (population) contrastive loss in fact does not degrade with the number of negative samples. Along the way, we give a structural characterization of the optimal representation in our framework, for noise contrastive estimation. We also provide empirical support for our theoretical results on CIFAR-10 and CIFAR-100 datasets.},
	number = {arXiv:2205.01789},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Awasthi, Pranjal and Dikkala, Nishanth and Kamath, Pritish},
	month = may,
	year = {2022},
	note = {arXiv:2205.01789 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{ash_investigating_2021,
	title = {Investigating the {Role} of {Negatives} in {Contrastive} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2106.09943},
	abstract = {Noise contrastive learning is a popular technique for unsupervised representation learning. In this approach, a representation is obtained via reduction to supervised learning, where given a notion of semantic similarity, the learner tries to distinguish a similar (positive) example from a collection of random (negative) examples. The success of modern contrastive learning pipelines relies on many parameters such as the choice of data augmentation, the number of negative examples, and the batch size; however, there is limited understanding as to how these parameters interact and affect downstream performance. We focus on disambiguating the role of one of these parameters: the number of negative examples. Theoretically, we show the existence of a collision-coverage trade-off suggesting that the optimal number of negative examples should scale with the number of underlying concepts in the data. Empirically, we scrutinize the role of the number of negatives in both NLP and vision tasks. In the NLP task, we find that the results broadly agree with our theory, while our vision experiments are murkier with performance sometimes even being insensitive to the number of negatives. We discuss plausible explanations for this behavior and suggest future directions to better align theory and practice.},
	number = {arXiv:2106.09943},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Ash, Jordan T. and Goel, Surbhi and Krishnamurthy, Akshay and Misra, Dipendra},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2106.09943},
	note = {arXiv:2106.09943 [cs, stat]
type: article},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@techreport{ericsson_why_2021,
	title = {Why {Do} {Self}-{Supervised} {Models} {Transfer}? {Investigating} the {Impact} of {Invariance} on {Downstream} {Tasks}},
	shorttitle = {Why {Do} {Self}-{Supervised} {Models} {Transfer}?},
	url = {http://arxiv.org/abs/2111.11398},
	abstract = {Self-supervised learning is a powerful paradigm for representation learning on unlabelled images. A wealth of effective new methods based on instance matching rely on data augmentation to drive learning, and these have reached a rough agreement on an augmentation scheme that optimises popular recognition benchmarks. However, there is strong reason to suspect that different tasks in computer vision require features to encode different (in)variances, and therefore likely require different augmentation strategies. In this paper, we measure the invariances learned by contrastive methods and confirm that they do learn invariance to the augmentations used and further show that this invariance largely transfers to related real-world changes in pose and lighting. We show that learned invariances strongly affect downstream task performance and confirm that different downstream tasks benefit from polar opposite (in)variances, leading to performance loss when the standard augmentation strategy is used. Finally, we demonstrate that a simple fusion of representations with complementary invariances ensures wide transferability to all the diverse downstream tasks considered.},
	number = {arXiv:2111.11398},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Ericsson, Linus and Gouk, Henry and Hospedales, Timothy M.},
	month = nov,
	year = {2021},
	note = {arXiv:2111.11398 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@techreport{richemond_byol_2020,
	title = {{BYOL} works even without batch statistics},
	url = {http://arxiv.org/abs/2010.10241},
	abstract = {Bootstrap Your Own Latent (BYOL) is a self-supervised learning approach for image representation. From an augmented view of an image, BYOL trains an online network to predict a target network representation of a different augmented view of the same image. Unlike contrastive methods, BYOL does not explicitly use a repulsion term built from negative pairs in its training objective. Yet, it avoids collapse to a trivial, constant representation. Thus, it has recently been hypothesized that batch normalization (BN) is critical to prevent collapse in BYOL. Indeed, BN flows gradients across batch elements, and could leak information about negative views in the batch, which could act as an implicit negative (contrastive) term. However, we experimentally show that replacing BN with a batch-independent normalization scheme (namely, a combination of group normalization and weight standardization) achieves performance comparable to vanilla BYOL (\$73.9{\textbackslash}\%\$ vs. \$74.3{\textbackslash}\%\$ top-1 accuracy under the linear evaluation protocol on ImageNet with ResNet-\$50\$). Our finding disproves the hypothesis that the use of batch statistics is a crucial ingredient for BYOL to learn useful representations.},
	number = {arXiv:2010.10241},
	urldate = {2022-05-24},
	institution = {arXiv},
	author = {Richemond, Pierre H. and Grill, Jean-Bastien and Altché, Florent and Tallec, Corentin and Strub, Florian and Brock, Andrew and Smith, Samuel and De, Soham and Pascanu, Razvan and Piot, Bilal and Valko, Michal},
	month = oct,
	year = {2020},
	doi = {10.48550/arXiv.2010.10241},
	note = {arXiv:2010.10241 [cs, stat]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sontag_vc_1998,
	title = {{VC} dimension of neural networks},
	volume = {168},
	journal = {NATO ASI Series F Computer and Systems Sciences},
	author = {Sontag, Eduardo D and {others}},
	year = {1998},
	note = {Publisher: Springer Verlag},
	pages = {69--96},
}

@book{zalinescu_convex_2002,
	title = {Convex {Analysis} in {General} {Vector} {Spaces}},
	isbn = {978-981-238-067-8},
	abstract = {The primary aim of this book is to present the conjugate and sub/differential calculus using the method of perturbation functions in order to obtain the most general results in this field. The secondary aim is to provide important applications of this calculus and of the properties of convex functions. Such applications are: the study of well-conditioned convex functions, uniformly convex and uniformly smooth convex functions, best approximation problems, characterizations of convexity, the study of the sets of weak sharp minima, well-behaved functions and the existence of global error bounds for convex inequalities, as well as the study of monotone multifunctions by using convex functions.},
	language = {en},
	publisher = {World Scientific},
	author = {Zalinescu, C.},
	month = jan,
	year = {2002},
	keywords = {Mathematics / Calculus, Mathematics / Functional Analysis, Mathematics / Vector Analysis, Science / Physics / General},
}

@book{feller_introduction_2008,
	title = {An introduction to probability theory and its applications, vol 2},
	publisher = {John Wiley \& Sons},
	author = {Feller, Willliam},
	year = {2008},
}

@article{cover_capacity_1968,
	title = {Capacity problems for linear machines},
	journal = {Pattern recognition},
	author = {Cover, Thomas M},
	year = {1968},
	note = {Publisher: New York},
	pages = {283--289},
}

@article{straszewicz_uber_1935,
	title = {Über exponierte punkte abgeschlossener punktmengen},
	volume = {24},
	journal = {Fundamenta Mathematicae},
	author = {Straszewicz, Stefan},
	year = {1935},
	note = {Publisher: Instytut Matematyczny Polskiej Akademii Nauk},
	pages = {139--143},
}

@incollection{rockafellar_convex_2015,
	title = {Convex analysis},
	booktitle = {Convex analysis},
	publisher = {Princeton university press},
	author = {Rockafellar, Ralph Tyrell},
	year = {2015},
}

@book{grunbaum_convex_1967,
	title = {Convex polytopes},
	volume = {16},
	publisher = {Springer},
	author = {Grünbaum, Branko and Klee, Victor and Perles, Micha A and Shephard, Geoffrey Colin},
	year = {1967},
}

@book{ziegler_lectures_2012,
	title = {Lectures on polytopes},
	volume = {152},
	publisher = {Springer Science \& Business Media},
	author = {Ziegler, Günter M},
	year = {2012},
}

@book{noauthor_lectures_nodate,
	title = {Lectures on {Polytopes}},
	url = {https://link.springer.com/book/10.1007/978-1-4613-8431-1},
	language = {en},
	urldate = {2022-05-23},
}

@book{brondsted_introduction_1983,
	address = {New York},
	series = {Graduate texts in mathematics},
	title = {An introduction to convex polytopes},
	isbn = {978-0-387-90722-2},
	language = {en},
	number = {90},
	publisher = {Springer-Verlag},
	author = {Brøndsted, Arne},
	year = {1983},
	keywords = {Convex polytopes},
}

@book{narici_topological_2010,
	address = {New York},
	edition = {2},
	title = {Topological {Vector} {Spaces}},
	isbn = {978-0-429-14789-0},
	abstract = {With many new concrete examples and historical notes, Topological Vector Spaces, Second Edition provides one of the most thorough and up-to-date treatments of the Hahn-Banach theorem. This edition explores the theorem's connection with the axiom of choice, discusses the uniqueness of Hahn-Banach extensions, and includes an entirely new chapter on v},
	publisher = {Chapman and Hall/CRC},
	author = {Narici, Lawrence and Beckenstein, Edward},
	month = jul,
	year = {2010},
	doi = {10.1201/9781584888673},
}

@book{boyd_convex_2004,
	address = {Cambridge, UK ; New York},
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	language = {en},
	publisher = {Cambridge University Press},
	author = {Boyd, Stephen P. and Vandenberghe, Lieven},
	year = {2004},
	keywords = {Convex functions, Mathematical optimization},
}

@book{noauthor_convex_nodate,
	title = {Convex {Polytopes}},
	url = {https://link.springer.com/book/10.1007/978-1-4613-0019-9},
	language = {en},
	urldate = {2022-05-22},
}

@inproceedings{mnih_learning_2013,
	title = {Learning word embeddings efficiently with noise-contrastive estimation},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/db2b4182156b2f1f817860ac9f409ad7-Abstract.html},
	abstract = {Continuous-valued word embeddings learned by neural language models have recently been shown to capture semantic and syntactic information about words very well, setting performance records on several word similarity tasks.  The best results are obtained by learning high-dimensional embeddings from very large quantities of data, which makes scalability of the training method a critical factor.  We propose a simple and scalable new approach to learning word embeddings based on training log-bilinear models with noise-contrastive estimation.  Our approach is simpler, faster, and produces better results than the current state-of-the art method of Mikolov et al. (2013a). We achieve results comparable to the best ones reported, which were obtained on a cluster, using four times less data and more than an order of magnitude less computing time. We also investigate several model types and find that the embeddings learned by the simpler models perform at least as well as those learned by the more complex ones.},
	urldate = {2022-05-19},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mnih, Andriy and Kavukcuoglu, Koray},
	year = {2013},
}

@techreport{belghazi_mine_2021,
	title = {{MINE}: {Mutual} {Information} {Neural} {Estimation}},
	shorttitle = {{MINE}},
	url = {http://arxiv.org/abs/1801.04062},
	abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
	number = {arXiv:1801.04062},
	urldate = {2022-05-19},
	institution = {arXiv},
	author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R. Devon},
	month = aug,
	year = {2021},
	doi = {10.48550/arXiv.1801.04062},
	note = {arXiv:1801.04062 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{burges_tutorial_1998,
	title = {A tutorial on support vector machines for pattern recognition},
	volume = {2},
	number = {2},
	journal = {Data mining and knowledge discovery},
	author = {Burges, Christopher JC},
	year = {1998},
	note = {Publisher: Springer},
	pages = {121--167},
}

@techreport{zarka_separation_2021,
	title = {Separation and {Concentration} in {Deep} {Networks}},
	url = {http://arxiv.org/abs/2012.10424},
	abstract = {Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and increasing the Fisher discriminant ratio. We explain this mechanism with two types of operators. We prove that a rectifier without biases applied to sign-invariant tight frames can separate class means and increase Fisher ratios. On the opposite, a soft-thresholding on tight frames can reduce within-class variabilities while preserving class means. Variance reduction bounds are proved for Gaussian mixture models. For image classification, we show that separation of class means can be achieved with rectified wavelet tight frames that are not learned. It defines a scattering transform. Learning \$1 {\textbackslash}times 1\$ convolutional tight frames along scattering channels and applying a soft-thresholding reduces within-class variabilities. The resulting scattering network reaches the classification accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no learned biases.},
	number = {arXiv:2012.10424},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {Zarka, John and Guth, Florentin and Mallat, Stéphane},
	month = mar,
	year = {2021},
	note = {arXiv:2012.10424 [cs]
type: article},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@techreport{ji_unconstrained_2022,
	title = {An {Unconstrained} {Layer}-{Peeled} {Perspective} on {Neural} {Collapse}},
	url = {http://arxiv.org/abs/2110.02796},
	abstract = {Neural collapse is a highly symmetric geometric pattern of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used.},
	number = {arXiv:2110.02796},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {Ji, Wenlong and Lu, Yiping and Zhang, Yiliang and Deng, Zhun and Su, Weijie J.},
	month = apr,
	year = {2022},
	doi = {10.48550/arXiv.2110.02796},
	note = {arXiv:2110.02796 [cs, stat]
type: article},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{fang_exploring_2021,
	title = {Exploring {Deep} {Neural} {Networks} via {Layer}-{Peeled} {Model}: {Minority} {Collapse} in {Imbalanced} {Training}},
	volume = {118},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Exploring {Deep} {Neural} {Networks} via {Layer}-{Peeled} {Model}},
	url = {http://arxiv.org/abs/2101.12699},
	doi = {10.1073/pnas.2103091118},
	abstract = {In this paper, we introduce the {\textbackslash}textit\{Layer-Peeled Model\}, a nonconvex yet analytically tractable optimization program, in a quest to better understand deep neural networks that are trained for a sufficiently long time. As the name suggests, this new model is derived by isolating the topmost layer from the remainder of the neural network, followed by imposing certain constraints separately on the two parts of the network. We demonstrate that the Layer-Peeled Model, albeit simple, inherits many characteristics of well-trained neural networks, thereby offering an effective tool for explaining and predicting common empirical patterns of deep learning training. First, when working on class-balanced datasets, we prove that any solution to this model forms a simplex equiangular tight frame, which in part explains the recently discovered phenomenon of neural collapse {\textbackslash}cite\{papyan2020prevalence\}. More importantly, when moving to the imbalanced case, our analysis of the Layer-Peeled Model reveals a hitherto unknown phenomenon that we term {\textbackslash}textit\{Minority Collapse\}, which fundamentally limits the performance of deep learning models on the minority classes. In addition, we use the Layer-Peeled Model to gain insights into how to mitigate Minority Collapse. Interestingly, this phenomenon is first predicted by the Layer-Peeled Model before being confirmed by our computational experiments.},
	number = {43},
	urldate = {2022-05-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
	month = oct,
	year = {2021},
	note = {arXiv:2101.12699 [cs, math, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
	pages = {e2103091118},
}

@techreport{e_emergence_2021,
	title = {On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers},
	url = {http://arxiv.org/abs/2012.05420},
	abstract = {A recent numerical study observed that neural network classifiers enjoy a large degree of symmetry in the penultimate layer. Namely, if \$h(x) = Af(x) +b\$ where \$A\$ is a linear map and \$f\$ is the output of the penultimate layer of the network (after activation), then all data points \$x\_\{i, 1\}, {\textbackslash}dots, x\_\{i, N\_i\}\$ in a class \$C\_i\$ are mapped to a single point \$y\_i\$ by \$f\$ and the points \$y\_i\$ are located at the vertices of a regular \$k-1\$-dimensional standard simplex in a high-dimensional Euclidean space. We explain this observation analytically in toy models for highly expressive deep neural networks. In complementary examples, we demonstrate rigorously that even the final output of the classifier \$h\$ is not uniform over data samples from a class \$C\_i\$ if \$h\$ is a shallow network (or if the deeper layers do not bring the data samples into a convenient geometric configuration).},
	number = {arXiv:2012.05420},
	urldate = {2022-05-17},
	institution = {arXiv},
	author = {E, Weinan and Wojtowytsch, Stephan},
	month = jun,
	year = {2021},
	doi = {10.48550/arXiv.2012.05420},
	note = {arXiv:2012.05420 [cs, stat]
type: article},
	keywords = {68T07, 62H30, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tsai_self-supervised_nodate,
	title = {{SELF}-{SUPERVISED} {LEARNING} {FROM} {A} {MULTI}-{VIEW} {PERSPECTIVE}},
	abstract = {As a subset of unsupervised representation learning, self-supervised representation learning adopts self-deﬁned signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Speciﬁcally, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework’s empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.},
	language = {en},
	author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	pages = {18},
}

@inproceedings{ruan_optimal_2021,
	title = {Optimal {Representations} for {Covariate} {Shift}},
	url = {https://openreview.net/forum?id=Rf58LPCwJj0},
	abstract = {Machine learning systems often experience a distribution shift between training and testing. In this paper, we introduce a simple variational objective whose optima are exactly the set of all...},
	language = {en},
	urldate = {2022-05-11},
	author = {Ruan, Yangjun and Dubois, Yann and Maddison, Chris J.},
	month = sep,
	year = {2021},
}

@inproceedings{devillers_does_2021,
	address = {Online},
	title = {Does language help generalization in vision models?},
	url = {https://aclanthology.org/2021.conll-1.13},
	doi = {10.18653/v1/2021.conll-1.13},
	abstract = {Vision models trained on multimodal datasets can benefit from the wide availability of large image-caption datasets. A recent model (CLIP) was found to generalize well in zero-shot and transfer learning settings. This could imply that linguistic or “semantic grounding” confers additional generalization abilities to the visual feature space. Here, we systematically evaluate various multimodal architectures and vision-only models in terms of unsupervised clustering, few-shot learning, transfer learning and adversarial robustness. In each setting, multimodal training produced no additional generalization capability compared to standard supervised visual training. We conclude that work is still required for semantic grounding to help improve vision models.},
	urldate = {2022-05-10},
	booktitle = {Proceedings of the 25th {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Devillers, Benjamin and Choksi, Bhavin and Bielawski, Romain and VanRullen, Rufin},
	month = nov,
	year = {2021},
	pages = {171--182},
}

@article{andreassen_evolution_2021,
	title = {The {Evolution} of {Out}-of-{Distribution} {Robustness} {Throughout} {Fine}-{Tuning}},
	url = {http://arxiv.org/abs/2106.15831},
	abstract = {Although machine learning models typically experience a drop in performance on out-of-distribution data, accuracies on in- versus out-of-distribution data are widely observed to follow a single linear trend when evaluated across a testbed of models. Models that are more accurate on the out-of-distribution data relative to this baseline exhibit "effective robustness" and are exceedingly rare. Identifying such models, and understanding their properties, is key to improving out-of-distribution performance. We conduct a thorough empirical investigation of effective robustness during fine-tuning and surprisingly find that models pre-trained on larger datasets exhibit effective robustness during training that vanishes at convergence. We study how properties of the data influence effective robustness, and we show that it increases with the larger size, more diversity, and higher example difficulty of the dataset. We also find that models that display effective robustness are able to correctly classify 10\% of the examples that no other current testbed model gets correct. Finally, we discuss several strategies for scaling effective robustness to the high-accuracy regime to improve the out-of-distribution accuracy of state-of-the-art models.},
	urldate = {2022-05-10},
	journal = {arXiv:2106.15831 [cs]},
	author = {Andreassen, Anders and Bahri, Yasaman and Neyshabur, Behnam and Roelofs, Rebecca},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.15831},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{fang_data_2022,
	title = {Data {Determines} {Distributional} {Robustness} in {Contrastive} {Language} {Image} {Pre}-training ({CLIP})},
	url = {http://arxiv.org/abs/2205.01397},
	abstract = {Contrastively trained image-text models such as CLIP, ALIGN, and BASIC have demonstrated unprecedented robustness to multiple challenging natural distribution shifts. Since these image-text models differ from previous training approaches in several ways, an important question is what causes the large robustness gains. We answer this question via a systematic experimental investigation. Concretely, we study five different possible causes for the robustness gains: (i) the training set size, (ii) the training distribution, (iii) language supervision at training time, (iv) language supervision at test time, and (v) the contrastive loss function. Our experiments show that the more diverse training distribution is the main cause for the robustness gains, with the other factors contributing little to no robustness. Beyond our experimental results, we also introduce ImageNet-Captions, a version of ImageNet with original text annotations from Flickr, to enable further controlled experiments of language-image training.},
	urldate = {2022-05-10},
	journal = {arXiv:2205.01397 [cs]},
	author = {Fang, Alex and Ilharco, Gabriel and Wortsman, Mitchell and Wan, Yuhao and Shankar, Vaishaal and Dave, Achal and Schmidt, Ludwig},
	month = may,
	year = {2022},
	note = {arXiv: 2205.01397},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{wu_unsupervised_2018,
	title = {Unsupervised {Feature} {Learning} via {Non}-{Parametric} {Instance}-level {Discrimination}},
	url = {http://arxiv.org/abs/1805.01978},
	abstract = {Neural net classifiers trained on data with annotated class labels can also capture apparent visual similarity among categories without being directed to do so. We study whether this observation can be extended beyond the conventional domain of supervised learning: Can we learn a good feature representation that captures apparent similarity among instances, instead of classes, by merely asking the feature to be discriminative of individual instances? We formulate this intuition as a non-parametric classification problem at the instance-level, and use noise-contrastive estimation to tackle the computational challenges imposed by the large number of instance classes. Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the state-of-the-art on ImageNet classification by a large margin. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.},
	urldate = {2022-05-09},
	journal = {arXiv:1805.01978 [cs]},
	author = {Wu, Zhirong and Xiong, Yuanjun and Yu, Stella and Lin, Dahua},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01978},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{chen_perfectly_2022,
	title = {Perfectly {Balanced}: {Improving} {Transfer} and {Robustness} of {Supervised} {Contrastive} {Learning}},
	shorttitle = {Perfectly {Balanced}},
	url = {http://arxiv.org/abs/2204.07596},
	abstract = {An ideal learned representation should display transferability and robustness. Supervised contrastive learning (SupCon) is a promising method for training accurate models, but produces representations that do not capture these properties due to class collapse -- when all points in a class map to the same representation. Recent work suggests that "spreading out" these representations improves them, but the precise mechanism is poorly understood. We argue that creating spread alone is insufficient for better representations, since spread is invariant to permutations within classes. Instead, both the correct degree of spread and a mechanism for breaking this invariance are necessary. We first prove that adding a weighted class-conditional InfoNCE loss to SupCon controls the degree of spread. Next, we study three mechanisms to break permutation invariance: using a constrained encoder, adding a class-conditional autoencoder, and using data augmentation. We show that the latter two encourage clustering of latent subclasses under more realistic conditions than the former. Using these insights, we show that adding a properly-weighted class-conditional InfoNCE loss and a class-conditional autoencoder to SupCon achieves 11.1 points of lift on coarse-to-fine transfer across 5 standard datasets and 4.7 points on worst-group robustness on 3 datasets, setting state-of-the-art on CelebA by 11.5 points.},
	urldate = {2022-04-20},
	journal = {arXiv:2204.07596 [cs, stat]},
	author = {Chen, Mayee F. and Fu, Daniel Y. and Narayan, Avanika and Zhang, Michael and Song, Zhao and Fatahalian, Kayvon and Ré, Christopher},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.07596},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{daneshmand_batch_2020,
	title = {Batch {Normalization} {Provably} {Avoids} {Rank} {Collapse} for {Randomly} {Initialised} {Deep} {Networks}},
	url = {http://arxiv.org/abs/2003.01652},
	abstract = {Randomly initialized neural networks are known to become harder to train with increasing depth, unless architectural enhancements like residual connections and batch normalization are used. We here investigate this phenomenon by revisiting the connection between random initialization in deep networks and spectral instabilities in products of random matrices. Given the rich literature on random matrices, it is not surprising to find that the rank of the intermediate representations in unnormalized networks collapses quickly with depth. In this work we highlight the fact that batch normalization is an effective strategy to avoid rank collapse for both linear and ReLU networks. Leveraging tools from Markov chain theory, we derive a meaningful lower rank bound in deep linear networks. Empirically, we also demonstrate that this rank robustness generalizes to ReLU nets. Finally, we conduct an extensive set of experiments on real-world data sets, which confirm that rank stability is indeed a crucial condition for training modern-day deep neural architectures.},
	urldate = {2022-04-17},
	journal = {arXiv:2003.01652 [cs, stat]},
	author = {Daneshmand, Hadi and Kohler, Jonas and Bach, Francis and Hofmann, Thomas and Lucchi, Aurelien},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.01652},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wang_understanding_2021,
	address = {Nashville, TN, USA},
	title = {Understanding the {Behaviour} of {Contrastive} {Loss}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9577669/},
	doi = {10.1109/CVPR46437.2021.00252},
	abstract = {Unsupervised contrastive learning has achieved outstanding success, while the mechanism of contrastive loss has been less studied. In this paper, we concentrate on the understanding of the behaviours of unsupervised contrastive loss. We will show that the contrastive loss is a hardness-aware loss function, and the temperature τ controls the strength of penalties on hard negative samples. The previous study has shown that uniformity is a key property of contrastive learning. We build relations between the uniformity and the temperature τ . We will show that uniformity helps the contrastive learning to learn separable features, however excessive pursuit to the uniformity makes the contrastive loss not tolerant to semantically similar samples, which may break the underlying semantic structure and be harmful to the formation of features useful for downstream tasks. This is caused by the inherent defect of the instance discrimination objective. Speciﬁcally, instance discrimination objective tries to push all different instances apart, ignoring the underlying relations between samples. Pushing semantically consistent samples apart has no positive effect for acquiring a prior informative to general downstream tasks. A well-designed contrastive loss should have some extents of tolerance to the closeness of semantically similar samples. Therefore, we ﬁnd that the contrastive loss meets a uniformity-tolerance dilemma, and a good choice of temperature can compromise these two properties properly to both learn separable features and tolerant to semantically similar samples, improving the feature qualities and the downstream performances.},
	language = {en},
	urldate = {2022-04-17},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Wang, Feng and Liu, Huaping},
	month = jun,
	year = {2021},
	pages = {2495--2504},
}

@inproceedings{lobacheva_periodic_2021,
	title = {On the {Periodic} {Behavior} of {Neural} {Network} {Training} with {Batch} {Normalization} and {Weight} {Decay}},
	url = {https://openreview.net/forum?id=B6uDDaDoW4a},
	abstract = {We study the periodic behavior of training dynamics caused by the interaction of batch normalization and weight decay.},
	language = {en},
	urldate = {2022-04-16},
	author = {Lobacheva, Ekaterina and Kodryan, Maxim and Chirkova, Nadezhda and Malinin, Andrey and Vetrov, Dmitry P.},
	month = may,
	year = {2021},
}

@article{fang_exploring_2021-1,
	title = {Exploring deep neural networks via layer-peeled model: {Minority} collapse in imbalanced training},
	volume = {118},
	shorttitle = {Exploring deep neural networks via layer-peeled model},
	url = {https://www.pnas.org/doi/10.1073/pnas.2103091118},
	doi = {10.1073/pnas.2103091118},
	number = {43},
	urldate = {2022-04-16},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fang, Cong and He, Hangfeng and Long, Qi and Su, Weijie J.},
	month = oct,
	year = {2021},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2103091118},
}

@article{oneill_classical_2021,
	title = {The {Classical} {Occupancy} {Distribution}: {Computation} and {Approximation}},
	volume = {75},
	issn = {0003-1305},
	shorttitle = {The {Classical} {Occupancy} {Distribution}},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/00031305.2019.1699445},
	doi = {10.1080/00031305.2019.1699445},
	abstract = {We examine the discrete distributional form that arises from the “classical occupancy problem,” which looks at the behavior of the number of occupied bins when we allocate a given number of balls uniformly at random to a given number of bins. We review the mass function and moments of the classical occupancy distribution and derive exact and asymptotic results for the mean, variance, skewness and kurtosis. We develop an algorithm to compute a cubic array of log-probabilities from the classical occupancy distribution. This algorithm allows the computation of large blocks of values while avoiding underflow problems in computation. Using this algorithm, we compute the classical occupancy distribution for a large block of values of balls and bins, and we measure the accuracy of its asymptotic approximation using the normal distribution. We analyze the accuracy of the normal approximation with respect to the variance, skewness and kurtosis of the distribution. Based on this analysis, we give some practical guidance on the feasibility of computing large blocks of values from the occupancy distribution, and when approximation is required.},
	number = {4},
	urldate = {2022-04-15},
	journal = {The American Statistician},
	author = {O’Neill, Ben},
	month = oct,
	year = {2021},
	note = {Publisher: Taylor \& Francis},
	keywords = {Approximation, Birthday problem, Classical occupancy distribution, Computation, Coupon collector problem, Distribution and moments},
	pages = {364--375},
}

@misc{noauthor_discussiontirage_2018,
	title = {Discussion:{Tirage} (mathématiques)},
	copyright = {Creative Commons Attribution-ShareAlike License},
	shorttitle = {Discussion},
	url = {https://fr.wikipedia.org/w/index.php?title=Discussion:Tirage_(math%C3%A9matiques)&oldid=144405701},
	language = {fr},
	urldate = {2022-04-15},
	journal = {Wikipédia},
	month = jan,
	year = {2018},
	note = {Page Version ID: 144405701},
}

@article{baldi_capacity_2019,
	title = {The capacity of feedforward neural networks},
	url = {http://arxiv.org/abs/1901.00434},
	abstract = {A long standing open problem in the theory of neural networks is the development of quantitative methods to estimate and compare the capabilities of different architectures. Here we define the capacity of an architecture by the binary logarithm of the number of functions it can compute, as the synaptic weights are varied. The capacity provides an upper bound on the number of bits that can be extracted from the training data and stored in the architecture during learning. We study the capacity of layered, fully-connected, architectures of linear threshold neurons with \$L\$ layers of size \$n\_1,n\_2, {\textbackslash}ldots, n\_L\$ and show that in essence the capacity is given by a cubic polynomial in the layer sizes: \$C(n\_1,{\textbackslash}ldots, n\_L)={\textbackslash}sum\_\{k=1\}{\textasciicircum}\{L-1\} {\textbackslash}min(n\_1,{\textbackslash}ldots,n\_k)n\_kn\_\{k+1\}\$, where layers that are smaller than all previous layers act as bottlenecks. In proving the main result, we also develop new techniques (multiplexing, enrichment, and stacking) as well as new bounds on the capacity of finite sets. We use the main result to identify architectures with maximal or minimal capacity under a number of natural constraints. This leads to the notion of structural regularization for deep architectures. While in general, everything else being equal, shallow networks compute more functions than deep networks, the functions computed by deep networks are more regular and "interesting".},
	urldate = {2022-04-13},
	journal = {arXiv:1901.00434 [cs, math, stat]},
	author = {Baldi, Pierre and Vershynin, Roman},
	month = mar,
	year = {2019},
	note = {arXiv: 1901.00434},
	keywords = {68Q32, 06E30, 92B20, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Combinatorics, Statistics - Machine Learning},
}

@article{mitchison_bounds_1989,
	title = {Bounds on the learning capacity of some multi-layer networks},
	volume = {60},
	issn = {0340-1200, 1432-0770},
	url = {https://link.springer.com/10.1007/BF00204772},
	doi = {10.1007/BF00204772},
	abstract = {We obtain bounds for the capacity of some multi-layer networks of linear threshold units. In the case of a network having n inputs, a single layer of h hidden units and an output layer of s units, where all the weights in the network are variable and s \_{\textless}h {\textless} n, the capacity m satisfies 2n{\textless}\_m{\textless}ntlogt, where t = 1 + h/s. We consider in more detail the case where there is a single output that is a fixed boolean function of the hidden units. In this case our upper bound is of order nhlogh but the argument which provided the lower bound of 2n no longer applies. However, by explicit computation in low dimensional cases we show that the capacity exceeds 2n but is substantially less than the upper bound. Finally, we describe a learning algorithm for multi-layer networks with a single output unit. This greatly outperforms back propagation at the task of learning random vectors and provides further empirical evidence that the lower bound of 2n can be exceeded.},
	language = {en},
	number = {5},
	urldate = {2022-04-13},
	journal = {Biological Cybernetics},
	author = {Mitchison, G. J. and Durbin, R. M.},
	month = mar,
	year = {1989},
	pages = {345--365},
}

@article{kowalczyk_estimates_1997,
	title = {Estimates of {Storage} {Capacity} of {Multilayer} {Perceptron} with {Threshold} {Logic} {Hidden} {Units}},
	volume = {10},
	issn = {0893-6080},
	url = {https://www.sciencedirect.com/science/article/pii/S0893608097000099},
	doi = {10.1016/S0893-6080(97)00009-9},
	abstract = {We estimate the storage capacity of multilayer perceptron with n inputs, h1 threshold logic units in the first hidden layer and 1 output. We show that if the network can memorize 50\% of all dichotomies of a randomly selected N-tuple of points of Rn with probability 1, then N≤2(nh1+1), while at 100\% memorization N≤nh1+1. Furthermore, if the bounds are reached, then the first hidden layer must be fully connected to the input. It is shown that such a network has memory capacity (in the sense of Cover) between nh1+1 and 2(nh1+1) input patterns and for the most efficient networks in this class between 1 and 2 input patterns per connection. Comparing these results with the recent estimates of VC-dimension we find that in contrast to a single neuron case, the VC-dimension exceeds the capacity for a sufficiently large n and h1. The results are based on the derivation of an explicit expression for the number of dichotomies which can be implemented by such a network for a special class of N-tuples of input patterns which has a positive probability of being randomly chosen.},
	language = {en},
	number = {8},
	urldate = {2022-04-13},
	journal = {Neural Networks},
	author = {Kowalczyk, Adam},
	month = nov,
	year = {1997},
	keywords = {Capacity, Counting function, Linear threshold unit, McCulloch–Dilts neuron, Multilayer perceptron, VC-dimension},
	pages = {1417--1433},
}

@book{ferguson_course_2017,
	address = {New York},
	title = {A {Course} in {Large} {Sample} {Theory}},
	isbn = {978-1-315-13628-8},
	abstract = {A Course in Large Sample Theory is presented in four parts. The first treats basic probabilistic notions, the second features the basic statistical tools for expanding the theory, the third contains special topics as applications of the general theory, and the fourth covers more standard statistical topics. Nearly all topics are covered in their multivariate setting.The book is intended as a first year graduate course in large sample theory for statisticians. It has been used by graduate students in statistics, biostatistics, mathematics, and related fields. Throughout the book there are many examples and exercises with solutions. It is an ideal text for self study.},
	publisher = {Routledge},
	author = {Ferguson, Thomas S.},
	month = sep,
	year = {2017},
	doi = {10.1201/9781315136288},
}

@article{weinan_emergence_2021,
	title = {On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers},
	url = {http://arxiv.org/abs/2012.05420},
	abstract = {A recent numerical study observed that neural network classifiers enjoy a large degree of symmetry in the penultimate layer. Namely, if \$h(x) = Af(x) +b\$ where \$A\$ is a linear map and \$f\$ is the output of the penultimate layer of the network (after activation), then all data points \$x\_\{i, 1\}, {\textbackslash}dots, x\_\{i, N\_i\}\$ in a class \$C\_i\$ are mapped to a single point \$y\_i\$ by \$f\$ and the points \$y\_i\$ are located at the vertices of a regular \$k-1\$-dimensional standard simplex in a high-dimensional Euclidean space. We explain this observation analytically in toy models for highly expressive deep neural networks. In complementary examples, we demonstrate rigorously that even the final output of the classifier \$h\$ is not uniform over data samples from a class \$C\_i\$ if \$h\$ is a shallow network (or if the deeper layers do not bring the data samples into a convenient geometric configuration).},
	urldate = {2022-04-11},
	journal = {arXiv:2012.05420 [cs, stat]},
	author = {Weinan, E and Wojtowytsch, Stephan},
	month = jun,
	year = {2021},
	note = {arXiv: 2012.05420},
	keywords = {68T07, 62H30, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ji_unconstrained_2021,
	title = {An {Unconstrained} {Layer}-{Peeled} {Perspective} on {Neural} {Collapse}},
	url = {http://arxiv.org/abs/2110.02796},
	abstract = {Neural collapse is a highly symmetric geometric pattern of neural networks that emerges during the terminal phase of training, with profound implications on the generalization performance and robustness of the trained networks. To understand how the last-layer features and classifiers exhibit this recently discovered implicit bias, in this paper, we introduce a surrogate model called the unconstrained layer-peeled model (ULPM). We prove that gradient flow on this model converges to critical points of a minimum-norm separation problem exhibiting neural collapse in its global minimizer. Moreover, we show that the ULPM with the cross-entropy loss has a benign global landscape for its loss function, which allows us to prove that all the critical points are strict saddle points except the global minimizers that exhibit the neural collapse phenomenon. Empirically, we show that our results also hold during the training of neural networks in real-world tasks when explicit regularization or weight decay is not used.},
	urldate = {2022-04-11},
	journal = {arXiv:2110.02796 [cs, stat]},
	author = {Ji, Wenlong and Lu, Yiping and Zhang, Yiliang and Deng, Zhun and Su, Weijie J.},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.02796},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kornblith_why_2021,
	title = {Why {Do} {Better} {Loss} {Functions} {Lead} to {Less} {Transferable} {Features}?},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper/2021/hash/f0bf4a2da952528910047c31b6c2e951-Abstract.html},
	abstract = {Previous work has proposed many new loss functions and regularizers that improve test accuracy on image classification tasks. However, it is not clear whether these loss functions learn better representations for downstream tasks. This paper studies how the choice of training objective affects the transferability of the hidden representations of convolutional neural networks trained on ImageNet. We show that many objectives lead to statistically significant improvements in ImageNet accuracy over vanilla softmax cross-entropy, but the resulting fixed feature extractors transfer substantially worse to downstream tasks, and the choice of loss has little effect when networks are fully fine-tuned on the new tasks. Using centered kernel alignment to measure similarity between hidden representations of networks, we find that differences among loss functions are apparent only in the last few layers of the network. We delve deeper into representations of the penultimate layer, finding that different objectives and hyperparameter combinations lead to dramatically different levels of class separation. Representations with higher class separation obtain higher accuracy on the original task, but their features are less useful for downstream tasks. Our results suggest there exists a trade-off between learning invariant features for the original task and features relevant for transfer tasks.},
	urldate = {2022-04-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Kornblith, Simon and Chen, Ting and Lee, Honglak and Norouzi, Mohammad},
	year = {2021},
	pages = {28648--28662},
}

@article{zarka_separation_2021-1,
	title = {Separation and {Concentration} in {Deep} {Networks}},
	url = {http://arxiv.org/abs/2012.10424},
	abstract = {Numerical experiments demonstrate that deep neural network classifiers progressively separate class distributions around their mean, achieving linear separability on the training set, and increasing the Fisher discriminant ratio. We explain this mechanism with two types of operators. We prove that a rectifier without biases applied to sign-invariant tight frames can separate class means and increase Fisher ratios. On the opposite, a soft-thresholding on tight frames can reduce within-class variabilities while preserving class means. Variance reduction bounds are proved for Gaussian mixture models. For image classification, we show that separation of class means can be achieved with rectified wavelet tight frames that are not learned. It defines a scattering transform. Learning \$1 {\textbackslash}times 1\$ convolutional tight frames along scattering channels and applying a soft-thresholding reduces within-class variabilities. The resulting scattering network reaches the classification accuracy of ResNet-18 on CIFAR-10 and ImageNet, with fewer layers and no learned biases.},
	urldate = {2022-04-11},
	journal = {arXiv:2012.10424 [cs]},
	author = {Zarka, John and Guth, Florentin and Mallat, Stéphane},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.10424},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{graf_dissecting_2021,
	title = {Dissecting {Supervised} {Contrastive} {Learning}},
	url = {https://proceedings.mlr.press/v139/graf21a.html},
	abstract = {Minimizing cross-entropy over the softmax scores of a linear map composed with a high-capacity encoder is arguably the most popular choice for training neural networks on supervised learning tasks. However, recent works show that one can directly optimize the encoder instead, to obtain equally (or even more) discriminative representations via a supervised variant of a contrastive objective. In this work, we address the question whether there are fundamental differences in the sought-for representation geometry in the output space of the encoder at minimal loss. Specifically, we prove, under mild assumptions, that both losses attain their minimum once the representations of each class collapse to the vertices of a regular simplex, inscribed in a hypersphere. We provide empirical evidence that this configuration is attained in practice and that reaching a close-to-optimal state typically indicates good generalization performance. Yet, the two losses show remarkably different optimization behavior. The number of iterations required to perfectly fit to data scales superlinearly with the amount of randomly flipped labels for the supervised contrastive loss. This is in contrast to the approximately linear scaling previously reported for networks trained with cross-entropy.},
	language = {en},
	urldate = {2022-04-11},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Graf, Florian and Hofer, Christoph and Niethammer, Marc and Kwitt, Roland},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3821--3830},
}

@article{lu_neural_2021,
	title = {Neural {Collapse} with {Cross}-{Entropy} {Loss}},
	url = {http://arxiv.org/abs/2012.08465},
	abstract = {We consider the variational problem of cross-entropy loss with \$n\$ feature vectors on a unit hypersphere in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$. We prove that when \$d {\textbackslash}geq n - 1\$, the global minimum is given by the simplex equiangular tight frame, which justifies the neural collapse behavior. We also prove that as \$n {\textbackslash}rightarrow {\textbackslash}infty\$ with fixed \$d\$, the minimizing points will distribute uniformly on the hypersphere and show a connection with the frame potential of Benedetto \& Fickus.},
	urldate = {2022-04-11},
	journal = {arXiv:2012.08465 [cs, math]},
	author = {Lu, Jianfeng and Steinerberger, Stefan},
	month = jan,
	year = {2021},
	note = {arXiv: 2012.08465},
	keywords = {Computer Science - Machine Learning, Mathematics - Classical Analysis and ODEs},
}

@article{mixon_neural_2020,
	title = {Neural collapse with unconstrained features},
	url = {http://arxiv.org/abs/2011.11619},
	abstract = {Neural collapse is an emergent phenomenon in deep learning that was recently discovered by Papyan, Han and Donoho. We propose a simple "unconstrained features model" in which neural collapse also emerges empirically. By studying this model, we provide some explanation for the emergence of neural collapse in terms of the landscape of empirical risk.},
	urldate = {2022-04-11},
	journal = {arXiv:2011.11619 [cs]},
	author = {Mixon, Dustin G. and Parshall, Hans and Pi, Jianzong},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.11619},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{ergen_revealing_2021,
	title = {Revealing the {Structure} of {Deep} {Neural} {Networks} via {Convex} {Duality}},
	url = {https://proceedings.mlr.press/v139/ergen21b.html},
	abstract = {We study regularized deep neural networks (DNNs) and introduce a convex analytic framework to characterize the structure of the hidden layers. We show that a set of optimal hidden layer weights for a norm regularized DNN training problem can be explicitly found as the extreme points of a convex set. For the special case of deep linear networks, we prove that each optimal weight matrix aligns with the previous layers via duality. More importantly, we apply the same characterization to deep ReLU networks with whitened data and prove the same weight alignment holds. As a corollary, we also prove that norm regularized deep ReLU networks yield spline interpolation for one-dimensional datasets which was previously known only for two-layer networks. Furthermore, we provide closed-form solutions for the optimal layer weights when data is rank-one or whitened. The same analysis also applies to architectures with batch normalization even for arbitrary data. Therefore, we obtain a complete explanation for a recent empirical observation termed Neural Collapse where class means collapse to the vertices of a simplex equiangular tight frame.},
	language = {en},
	urldate = {2022-04-11},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ergen, Tolga and Pilanci, Mert},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {3004--3014},
}

@article{rangamani_dynamics_nodate,
	title = {Dynamics and {Neural} {Collapse} in {Deep} {Classiﬁers} trained with the {Square} {Loss}},
	abstract = {Recent results suggest that square loss performs on par with cross-entropy loss in classiﬁcation tasks for deep networks. While the theoretical understanding of training deep networks with the cross-entropy loss has been growing, the study of square loss for classiﬁcation has been lacking. Here we study the dynamics of training under Gradient Descent techniques and show that we can expect convergence to minimum norm solutions when both Weight Decay (WD) and normalization techniques, like Batch Normalization (BN), are used. We perform numerical simulations that show approximate independence on initial conditions as suggested by our analysis, while in the absence of BN+WD we ﬁnd that good solutions can be achieved for small initializations. We prove that quasi-interpolating solutions obtained by gradient descent in the presence of WD are expected to show the recently discovered behavior of Neural Collapse and describe other predictions of the theory.},
	language = {en},
	author = {Rangamani, Akshay and Xu, Mengjia and Banburski, Andrzej and Liao, Qianli and Poggio, Tomaso},
	pages = {41},
}

@article{zhu_geometric_2021,
	title = {A {Geometric} {Analysis} of {Neural} {Collapse} with {Unconstrained} {Features}},
	url = {http://arxiv.org/abs/2105.02375},
	abstract = {We provide the first global optimization landscape analysis of \$Neural{\textbackslash};Collapse\$ -- an intriguing empirical phenomenon that arises in the last-layer classifiers and features of neural networks during the terminal phase of training. As recently reported by Papyan et al., this phenomenon implies that (\$i\$) the class means and the last-layer classifiers all collapse to the vertices of a Simplex Equiangular Tight Frame (ETF) up to scaling, and (\$ii\$) cross-example within-class variability of last-layer activations collapses to zero. We study the problem based on a simplified \$unconstrained{\textbackslash};feature{\textbackslash};model\$, which isolates the topmost layers from the classifier of the neural network. In this context, we show that the classical cross-entropy loss with weight decay has a benign global landscape, in the sense that the only global minimizers are the Simplex ETFs while all other critical points are strict saddles whose Hessian exhibit negative curvature directions. In contrast to existing landscape analysis for deep neural networks which is often disconnected from practice, our analysis of the simplified model not only does it explain what kind of features are learned in the last layer, but it also shows why they can be efficiently optimized in the simplified settings, matching the empirical observations in practical deep network architectures. These findings could have profound implications for optimization, generalization, and robustness of broad interests. For example, our experiments demonstrate that one may set the feature dimension equal to the number of classes and fix the last-layer classifier to be a Simplex ETF for network training, which reduces memory cost by over \$20{\textbackslash}\%\$ on ResNet18 without sacrificing the generalization performance.},
	urldate = {2022-04-11},
	journal = {arXiv:2105.02375 [cs, math, stat]},
	author = {Zhu, Zhihui and Ding, Tianyu and Zhou, Jinxin and Li, Xiao and You, Chong and Sulam, Jeremias and Qu, Qing},
	month = may,
	year = {2021},
	note = {arXiv: 2105.02375},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{papyan_prevalence_2020,
	title = {Prevalence of {Neural} {Collapse} during the terminal phase of deep learning training},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://arxiv.org/abs/2008.08186},
	doi = {10.1073/pnas.2015509117},
	abstract = {Modern practice for training classification deepnets involves a Terminal Phase of Training (TPT), which begins at the epoch where training error first vanishes; During TPT, the training error stays effectively zero while training loss is pushed towards zero. Direct measurements of TPT, for three prototypical deepnet architectures and across seven canonical classification datasets, expose a pervasive inductive bias we call Neural Collapse, involving four deeply interconnected phenomena: (NC1) Cross-example within-class variability of last-layer training activations collapses to zero, as the individual activations themselves collapse to their class-means; (NC2) The class-means collapse to the vertices of a Simplex Equiangular Tight Frame (ETF); (NC3) Up to rescaling, the last-layer classifiers collapse to the class-means, or in other words to the Simplex ETF, i.e. to a self-dual configuration; (NC4) For a given activation, the classifier's decision collapses to simply choosing whichever class has the closest train class-mean, i.e. the Nearest Class Center (NCC) decision rule. The symmetric and very simple geometry induced by the TPT confers important benefits, including better generalization performance, better robustness, and better interpretability.},
	number = {40},
	urldate = {2022-04-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Papyan, Vardan and Han, X. Y. and Donoho, David L.},
	month = oct,
	year = {2020},
	note = {arXiv: 2008.08186},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {24652--24663},
}

@article{dutta_mode_2010,
	title = {Mode estimation for discrete distributions},
	volume = {19},
	doi = {10.3103/S1066530710040046},
	abstract = {The problem of estimating the mode of a discrete distribution is considered. New characterizations of discrete unimodal and
multi-modal distributions are obtained. The proposed mode estimator is essentially the sample mode, modulo appropriate modifications
when the sample mode is not well defined. In the case of i.i.d. observations coming from a unimodal discrete distribution,
our proposed mode estimator is shown to possess a number of strong asymptotic properties. Many of these results extend to
the case of multi-modal discrete distributions as well. Our method also applies — and we have similar asymptotic results —
to the problem of mode estimation based on finitely many observations on a Markov chain whose equilibrium distribution is
the underlying unimodal distribution. For unimodal discrete distributions, we also propose a consistent large sample test
of mode based on the proposed statistic. Applications of mode estimation problem in Monte-Carlo optimization problem using
the Hastings Metropolis chain and in prediction problem using binary response variable, specially in the context of dose-response
experiments, are also illustrated.

Keywordsmode estimation and testing–discrete distribution–asymptotics–Hastings Metropolis chain–prediction problem},
	journal = {Mathematical Methods of Statistics},
	author = {Dutta, Santanu and Goswami, A.},
	month = dec,
	year = {2010},
	pages = {374--384},
}

@article{baum_capabilities_1988,
	title = {On the capabilities of multilayer perceptrons},
	volume = {4},
	issn = {0885064X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0885064X88900209},
	doi = {10.1016/0885-064X(88)90020-9},
	language = {en},
	number = {3},
	urldate = {2022-04-08},
	journal = {Journal of Complexity},
	author = {Baum, Eric B},
	month = sep,
	year = {1988},
	pages = {193--215},
}

@article{cover_geometrical_1965,
	title = {Geometrical and {Statistical} {Properties} of {Systems} of {Linear} {Inequalities} with {Applications} in {Pattern} {Recognition}},
	volume = {EC-14},
	issn = {0367-7508},
	url = {http://ieeexplore.ieee.org/document/4038449/},
	doi = {10.1109/PGEC.1965.264137},
	abstract = {This paper develops the separating capacities of families of nonlinear decision surfaces by a direct application of a theorem in classical combinatorial geometry. It is shown that a family of surfaces having d degrees of freedom has a natural separating capacity of 2d pattern vectors, thus extending and unifying results of Winder and others on the pattern-separating capacity of hyperplanes. Applying these ideas to the vertices of a binary n-cube yields bounds on the number of spherically, quadratically, and, in general, nonlinearly separable Boolean functions of n variables.},
	language = {en},
	number = {3},
	urldate = {2022-04-08},
	journal = {IEEE Transactions on Electronic Computers},
	author = {Cover, Thomas M.},
	month = jun,
	year = {1965},
	pages = {326--334},
}

@article{sompolinsky_introduction_nodate,
	title = {Introduction: {The} {Perceptron}},
	language = {en},
	author = {Sompolinsky, Haim},
	pages = {10},
}

@article{sontag_shattering_1997,
	title = {Shattering {All} {Sets} of \textit{‘k’} {Points} in “{General} {Position}” {Requires} ( \textit{k} — 1)/2 {Parameters}},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/9/2/337-348/6035},
	doi = {10.1162/neco.1997.9.2.337},
	abstract = {For classes of concepts defined by certain classes of analytic functions depending on n parameters, there are nonempty open sets of samples of length 2n + 2 that cannot be shattered. A slighly weaker result is also proved for piecewise-analytic functions. The special case of neural networks is discussed.},
	language = {en},
	number = {2},
	urldate = {2022-04-08},
	journal = {Neural Computation},
	author = {Sontag, Eduardo D.},
	month = feb,
	year = {1997},
	pages = {337--348},
}

@article{zhang_how_2022,
	title = {How {Does} {SimSiam} {Avoid} {Collapse} {Without} {Negative} {Samples}? {A} {Unified} {Understanding} with {Self}-supervised {Contrastive} {Learning}},
	shorttitle = {How {Does} {SimSiam} {Avoid} {Collapse} {Without} {Negative} {Samples}?},
	url = {http://arxiv.org/abs/2203.16262},
	abstract = {To avoid collapse in self-supervised learning (SSL), a contrastive loss is widely used but often requires a large number of negative samples. Without negative samples yet achieving competitive performance, a recent work has attracted significant attention for providing a minimalist simple Siamese (SimSiam) method to avoid collapse. However, the reason for how it avoids collapse without negative samples remains not fully clear and our investigation starts by revisiting the explanatory claims in the original SimSiam. After refuting their claims, we introduce vector decomposition for analyzing the collapse based on the gradient analysis of the \$l\_2\$-normalized representation vector. This yields a unified perspective on how negative samples and SimSiam alleviate collapse. Such a unified perspective comes timely for understanding the recent progress in SSL.},
	urldate = {2022-04-04},
	journal = {arXiv:2203.16262 [cs]},
	author = {Zhang, Chaoning and Zhang, Kang and Zhang, Chenshuang and Pham, Trung X. and Yoo, Chang D. and Kweon, In So},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.16262},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{wang_chaos_2022,
	title = {Chaos is a {Ladder}: {A} {New} {Theoretical} {Understanding} of {Contrastive} {Learning} via {Augmentation} {Overlap}},
	shorttitle = {Chaos is a {Ladder}},
	url = {http://arxiv.org/abs/2203.13457},
	abstract = {Recently, contrastive learning has risen to be a promising approach for large-scale self-supervised learning. However, theoretical understanding of how it works is still unclear. In this paper, we propose a new guarantee on the downstream performance without resorting to the conditional independence assumption that is widely adopted in previous work but hardly holds in practice. Our new theory hinges on the insight that the support of different intra-class samples will become more overlapped under aggressive data augmentations, thus simply aligning the positive samples (augmented views of the same sample) could make contrastive learning cluster intra-class samples together. Based on this augmentation overlap perspective, theoretically, we obtain asymptotically closed bounds for downstream performance under weaker assumptions, and empirically, we propose an unsupervised model selection metric ARC that aligns well with downstream accuracy. Our theory suggests an alternative understanding of contrastive learning: the role of aligning positive samples is more like a surrogate task than an ultimate goal, and the overlapped augmented views (i.e., the chaos) create a ladder for contrastive learning to gradually learn class-separated representations. The code for computing ARC is available at https://github.com/zhangq327/ARC.},
	urldate = {2022-04-04},
	journal = {arXiv:2203.13457 [cs, stat]},
	author = {Wang, Yifei and Zhang, Qi and Wang, Yisen and Yang, Jiansheng and Lin, Zhouchen},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.13457},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{shen_connect_2022,
	title = {Connect, {Not} {Collapse}: {Explaining} {Contrastive} {Learning} for {Unsupervised} {Domain} {Adaptation}},
	shorttitle = {Connect, {Not} {Collapse}},
	url = {http://arxiv.org/abs/2204.00570},
	abstract = {We consider unsupervised domain adaptation (UDA), where labeled data from a source domain (e.g., photographs) and unlabeled data from a target domain (e.g., sketches) are used to learn a classifier for the target domain. Conventional UDA methods (e.g., domain adversarial training) learn domain-invariant features to improve generalization to the target domain. In this paper, we show that contrastive pre-training, which learns features on unlabeled source and target data and then fine-tunes on labeled source data, is competitive with strong UDA methods. However, we find that contrastive pre-training does not learn domain-invariant features, diverging from conventional UDA intuitions. We show theoretically that contrastive pre-training can learn features that vary subtantially across domains but still generalize to the target domain, by disentangling domain and class information. Our results suggest that domain invariance is not necessary for UDA. We empirically validate our theory on benchmark vision datasets.},
	urldate = {2022-04-04},
	journal = {arXiv:2204.00570 [cs]},
	author = {Shen, Kendrick and Jones, Robbie and Kumar, Ananya and Xie, Sang Michael and HaoChen, Jeff Z. and Ma, Tengyu and Liang, Percy},
	month = apr,
	year = {2022},
	note = {arXiv: 2204.00570},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{vert_primer_nodate,
	title = {A primer on kernel methods},
	language = {en},
	author = {Vert, Jean-Philippe and Tsuda, Koji and Scholkopf, Bernhard},
	pages = {42},
}

@misc{noauthor_metric_nodate,
	title = {Metric {Learning}: {A} {Survey}},
	shorttitle = {Metric {Learning}},
	url = {https://ieeexplore.ieee.org/document/8186753?bkn=8186753},
	abstract = {The metric learning problem is concerned with learning a distance function tuned to a particular task, and has been shown to be useful when used in conjunction with nearest-neighbor methods and other techniques that rely on distances or similarities. Metric Learning: A Review presents an overview of existing research in this topic, including recent progress on scaling to high-dimensional feature spaces and to data sets with an extremely large number of data points. It presents as unified a framework as possible under which existing research on metric learning can be cast. The monograph starts out by focusing on linear metric learning approaches, and mainly concentrates on the class of Mahalanobis distance learning methods. It then discusses nonlinear metric learning approaches, focusing on the connections between the non-linear and linear approaches. Finally, it discusses extensions of metric learning, as well as applications to a variety of problems in computer vision, text analysis, program analysis, and multimedia. Metric Learning: A Review is an ideal reference for anyone interested in the metric learning problem. It synthesizes much of the recent work in the area and it is hoped that it will inspire new algorithms and applications.},
	language = {en-US},
	urldate = {2022-04-04},
}

@article{bellet_survey_2014,
	title = {A {Survey} on {Metric} {Learning} for {Feature} {Vectors} and {Structured} {Data}},
	url = {http://arxiv.org/abs/1306.6709},
	abstract = {The need for appropriate ways to measure the distance or similarity between data is ubiquitous in machine learning, pattern recognition and data mining, but handcrafting such good metrics for specific problems is generally difficult. This has led to the emergence of metric learning, which aims at automatically learning a metric from data and has attracted a lot of interest in machine learning and related fields for the past ten years. This survey paper proposes a systematic review of the metric learning literature, highlighting the pros and cons of each approach. We pay particular attention to Mahalanobis distance metric learning, a well-studied and successful framework, but additionally present a wide range of methods that have recently emerged as powerful alternatives, including nonlinear metric learning, similarity learning and local metric learning. Recent trends and extensions, such as semi-supervised metric learning, metric learning for histogram data and the derivation of generalization guarantees, are also covered. Finally, this survey addresses metric learning for structured data, in particular edit distance learning, and attempts to give an overview of the remaining challenges in metric learning for the years to come.},
	urldate = {2022-04-04},
	journal = {arXiv:1306.6709 [cs, stat]},
	author = {Bellet, Aurélien and Habrard, Amaury and Sebban, Marc},
	month = feb,
	year = {2014},
	note = {arXiv: 1306.6709},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{abbasnejad_survey_2012,
	title = {A survey of the state of the art in learning the kernels},
	abstract = {Abstract In recent years the machine learning community has witnessed a tremendous growth in the development of kernel-based learning algorithms. However, the performance of this class of algorithms greatly depends on the choice of the kernel function. Kernel function implicitly represents the inner product between a pair of points of a dataset in a higher dimensional space. This inner product amounts to the similarity between points and provide a solid foundation for nonlinear analysis in kernel-based learning algorithms. The most important challenge in kernel-based learning is the selection of an appropriate kernel for a given dataset. To remedy this problem, algorithms to learn the kernel have recently been proposed. These methods formulate a learning algorithm that finds an optimal kernel for a given dataset. In this paper, we present an overview of these algorithms and provide a comparison of various approaches to find an optimal kernel. Furthermore, a list of pivotal issues that lead to efficient design of such algorithms will be presented.},
	journal = {Knowl. Inf. Syst},
	author = {Abbasnejad, M. Ehsan and Ramachandram, Dhanesh and Mandava, · Rajeswari},
	year = {2012},
}

@article{suarez-diaz_tutorial_2020,
	title = {A {Tutorial} on {Distance} {Metric} {Learning}: {Mathematical} {Foundations}, {Algorithms}, {Experimental} {Analysis}, {Prospects} and {Challenges} (with {Appendices} on {Mathematical} {Background} and {Detailed} {Algorithms} {Explanation})},
	shorttitle = {A {Tutorial} on {Distance} {Metric} {Learning}},
	url = {http://arxiv.org/abs/1812.05944},
	abstract = {Distance metric learning is a branch of machine learning that aims to learn distances from the data, which enhances the performance of similarity-based algorithms. This tutorial provides a theoretical background and foundations on this topic and a comprehensive experimental analysis of the most-known algorithms. We start by describing the distance metric learning problem and its main mathematical foundations, divided into three main blocks: convex analysis, matrix analysis and information theory. Then, we will describe a representative set of the most popular distance metric learning methods used in classification. All the algorithms studied in this paper will be evaluated with exhaustive testing in order to analyze their capabilities in standard classification problems, particularly considering dimensionality reduction and kernelization. The results, verified by Bayesian statistical tests, highlight a set of outstanding algorithms. Finally, we will discuss several potential future prospects and challenges in this field. This tutorial will serve as a starting point in the domain of distance metric learning from both a theoretical and practical perspective.},
	urldate = {2022-04-04},
	journal = {arXiv:1812.05944 [cs, stat]},
	author = {Suárez-Díaz, Juan Luis and García, Salvador and Herrera, Francisco},
	month = aug,
	year = {2020},
	note = {arXiv: 1812.05944},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kaya_deep_2019,
	title = {Deep {Metric} {Learning}: {A} {Survey}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2073-8994},
	shorttitle = {Deep {Metric} {Learning}},
	url = {https://www.mdpi.com/2073-8994/11/9/1066},
	doi = {10.3390/sym11091066},
	abstract = {Metric learning aims to measure the similarity among samples while using an optimal distance metric for learning tasks. Metric learning methods, which generally use a linear projection, are limited in solving real-world problems demonstrating non-linear characteristics. Kernel approaches are utilized in metric learning to address this problem. In recent years, deep metric learning, which provides a better solution for nonlinear data through activation functions, has attracted researchers’ attention in many different areas. This article aims to reveal the importance of deep metric learning and the problems dealt with in this field in the light of recent studies. As far as the research conducted in this field are concerned, most existing studies that are inspired by Siamese and Triplet networks are commonly used to correlate among samples while using shared weights in deep metric learning. The success of these networks is based on their capacity to understand the similarity relationship among samples. Moreover, sampling strategy, appropriate distance metric, and the structure of the network are the challenging factors for researchers to improve the performance of the network model. This article is considered to be important, as it is the first comprehensive study in which these factors are systematically analyzed and evaluated as a whole and supported by comparing the quantitative results of the methods.},
	language = {en},
	number = {9},
	urldate = {2022-04-04},
	journal = {Symmetry},
	author = {Kaya, Mahmut and Bi̇lge, Hasan Şakir},
	month = sep,
	year = {2019},
	note = {Number: 9
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {deep metric learning, metric learning, siamese network, similarity, triplet network},
	pages = {1066},
}

@article{ghojogh_spectral_2022,
	title = {Spectral, {Probabilistic}, and {Deep} {Metric} {Learning}: {Tutorial} and {Survey}},
	shorttitle = {Spectral, {Probabilistic}, and {Deep} {Metric} {Learning}},
	url = {http://arxiv.org/abs/2201.09267},
	abstract = {This is a tutorial and survey paper on metric learning. Algorithms are divided into spectral, probabilistic, and deep metric learning. We first start with the definition of distance metric, Mahalanobis distance, and generalized Mahalanobis distance. In spectral methods, we start with methods using scatters of data, including the first spectral metric learning, relevant methods to Fisher discriminant analysis, Relevant Component Analysis (RCA), Discriminant Component Analysis (DCA), and the Fisher-HSIC method. Then, large-margin metric learning, imbalanced metric learning, locally linear metric adaptation, and adversarial metric learning are covered. We also explain several kernel spectral methods for metric learning in the feature space. We also introduce geometric metric learning methods on the Riemannian manifolds. In probabilistic methods, we start with collapsing classes in both input and feature spaces and then explain the neighborhood component analysis methods, Bayesian metric learning, information theoretic methods, and empirical risk minimization in metric learning. In deep learning methods, we first introduce reconstruction autoencoders and supervised loss functions for metric learning. Then, Siamese networks and its various loss functions, triplet mining, and triplet sampling are explained. Deep discriminant analysis methods, based on Fisher discriminant analysis, are also reviewed. Finally, we introduce multi-modal deep metric learning, geometric metric learning by neural networks, and few-shot metric learning.},
	urldate = {2022-04-04},
	journal = {arXiv:2201.09267 [cs, stat]},
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.09267},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_alighodsi_nodate,
	title = {{AliGhodsi} {Lec} 12, {Metric} {Learning} - {YouTube}},
	url = {https://www.youtube.com/watch?v=GhsHPY3-1zY&ab_channel=DataScienceCourses},
	urldate = {2022-04-04},
}

@misc{noauthor_deep_nodate,
	title = {Deep {Metric} {Learning}: a ({Long}) {Survey}},
	shorttitle = {Deep {Metric} {Learning}},
	url = {https://hav4ik.github.io/articles/deep-metric-learning-survey},
	abstract = {In this post, I’ll briefly go over the common approaches for Deep Metric Learning, as well as the new methods proposed in recent years.},
	urldate = {2022-04-04},
}

@article{xing_distance_nodate,
	title = {Distance metric learning, with application to clustering with side-information},
	abstract = {Many algorithms rely critically on being given a good metric over their inputs. For instance, data can often be clustered in many “plausible” ways, and if a clustering algorithm such as K-means initially fails to ﬁnd one that is meaningful to a user, the only recourse may be for the user to manually tweak the metric until sufﬁciently good clusters are found. For these and other applications requiring good metrics, it is desirable that we provide a more systematic way for users to indicate what they consider “similar.” For instance, we may ask them to provide examples. In this paper, we present an algorithm that, given examples of similar (and, if desired, dissimilar) pairs of points in ¢¤£ , learns a distance metric over ¢¥£ that respects these relationships. Our method is based on posing metric learning as a convex optimization problem, which allows us to give efﬁcient, local-optima-free algorithms. We also demonstrate empirically that the learned metrics can be used to signiﬁcantly improve clustering performance.},
	language = {en},
	author = {Xing, Eric P and Ng, Andrew Y and Jordan, Michael I and Russell, Stuart},
	pages = {8},
}

@article{noauthor_no_nodate,
	title = {[{No} title found]},
	issn = {22502459},
	abstract = {In this paper, we study some of the properties of fuzzy graph and prove some results on these. Fuzzy graph is the generalization of the ordinary graph and here fuzzy graph is a simple fuzzy graph. So we introduce a new structure of a fuzzy graph.},
	language = {en},
	journal = {International Journal of Emerging Technology and Advanced Engineering},
}

@article{pokle_contrasting_2022,
	title = {Contrasting the landscape of contrastive and non-contrastive learning},
	url = {http://arxiv.org/abs/2203.15702},
	abstract = {A lot of recent advances in unsupervised feature learning are based on designing features which are invariant under semantic data augmentations. A common way to do this is contrastive learning, which uses positive and negative samples. Some recent works however have shown promising results for non-contrastive learning, which does not require negative samples. However, the non-contrastive losses have obvious "collapsed" minima, in which the encoders output a constant feature embedding, independent of the input. A folk conjecture is that so long as these collapsed solutions are avoided, the produced feature representations should be good. In our paper, we cast doubt on this story: we show through theoretical results and controlled experiments that even on simple data models, non-contrastive losses have a preponderance of non-collapsed bad minima. Moreover, we show that the training process does not avoid these minima.},
	urldate = {2022-03-31},
	journal = {arXiv:2203.15702 [cs, stat]},
	author = {Pokle, Ashwini and Tian, Jinjin and Li, Yuchen and Risteski, Andrej},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.15702},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{battaglia_relational_2018,
	title = {Relational inductive biases, deep learning, and graph networks},
	url = {http://arxiv.org/abs/1806.01261},
	abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
	urldate = {2022-03-18},
	journal = {arXiv:1806.01261 [cs, stat]},
	author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
	month = oct,
	year = {2018},
	note = {arXiv: 1806.01261},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{abnar_transferring_2020,
	title = {Transferring {Inductive} {Biases} through {Knowledge} {Distillation}},
	url = {http://arxiv.org/abs/2006.00555},
	abstract = {Having the right inductive biases can be crucial in many tasks or scenarios where data or computing resources are a limiting factor, or where training data is not perfectly representative of the conditions at test time. However, defining, designing and efficiently adapting inductive biases is not necessarily straightforward. In this paper, we explore the power of knowledge distillation for transferring the effect of inductive biases from one model to another. We consider families of models with different inductive biases, LSTMs vs. Transformers and CNNs vs. MLPs, in the context of tasks and scenarios where having the right inductive biases is critical. We study the effect of inductive biases on the solutions the models converge to and investigate how and to what extent the effect of inductive biases is transferred through knowledge distillation, in terms of not only performance but also different aspects of converged solutions.},
	urldate = {2022-03-18},
	journal = {arXiv:2006.00555 [cs, stat]},
	author = {Abnar, Samira and Dehghani, Mostafa and Zuidema, Willem},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.00555},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{goyal_inductive_2021,
	title = {Inductive {Biases} for {Deep} {Learning} of {Higher}-{Level} {Cognition}},
	url = {http://arxiv.org/abs/2011.15091},
	abstract = {A fascinating hypothesis is that human and animal intelligence could be explained by a few principles (rather than an encyclopedic list of heuristics). If that hypothesis was correct, we could more easily both understand our own intelligence and build intelligent machines. Just like in physics, the principles themselves would not be sufficient to predict the behavior of complex systems like brains, and substantial computation might be needed to simulate human-like intelligence. This hypothesis would suggest that studying the kind of inductive biases that humans and animals exploit could help both clarify these principles and provide inspiration for AI research and neuroscience theories. Deep learning already exploits several key inductive biases, and this work considers a larger list, focusing on those which concern mostly higher-level and sequential conscious processing. The objective of clarifying these particular principles is that they could potentially help us build AI systems benefiting from humans' abilities in terms of flexible out-of-distribution and systematic generalization, which is currently an area where a large gap exists between state-of-the-art machine learning and human intelligence.},
	urldate = {2022-03-18},
	journal = {arXiv:2011.15091 [cs, stat]},
	author = {Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv: 2011.15091},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{wu_lime_2021,
	title = {{LIME}: {Learning} {Inductive} {Bias} for {Primitives} of {Mathematical} {Reasoning}},
	shorttitle = {{LIME}},
	url = {https://proceedings.mlr.press/v139/wu21c.html},
	abstract = {While designing inductive bias in neural architectures has been widely studied, we hypothesize that transformer networks are flexible enough to learn inductive bias from suitable generic tasks. Here, we replace architecture engineering by encoding inductive bias in the form of datasets. Inspired by Peirce’s view that deduction, induction, and abduction are the primitives of reasoning, we design three synthetic tasks that are intended to require the model to have these three abilities. We specifically design these tasks to be synthetic and devoid of mathematical knowledge to ensure that only the fundamental reasoning biases can be learned from these tasks. This defines a new pre-training methodology called "LIME" (Learning Inductive bias for Mathematical rEasoning). Models trained with LIME significantly outperform vanilla transformers on four very different large mathematical reasoning benchmarks. Unlike dominating the computation cost as traditional pre-training approaches, LIME requires only a small fraction of the computation cost of the typical downstream task. The code for generating LIME tasks is available at https://github.com/tonywu95/LIME.},
	language = {en},
	urldate = {2022-03-18},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Wu, Yuhuai and Rabe, Markus N. and Li, Wenda and Ba, Jimmy and Grosse, Roger B. and Szegedy, Christian},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11251--11262},
}

@article{warstadt_learning_2020,
	title = {Learning {Which} {Features} {Matter}: {RoBERTa} {Acquires} a {Preference} for {Linguistic} {Generalizations} ({Eventually})},
	shorttitle = {Learning {Which} {Features} {Matter}},
	url = {http://arxiv.org/abs/2010.05358},
	abstract = {One reason pretraining on self-supervised linguistic tasks is effective is that it teaches models features that are helpful for language understanding. However, we want pretrained models to learn not only to represent linguistic features, but also to use those features preferentially during fine-turning. With this goal in mind, we introduce a new English-language diagnostic set called MSGS (the Mixed Signals Generalization Set), which consists of 20 ambiguous binary classification tasks that we use to test whether a pretrained model prefers linguistic or surface generalizations during fine-tuning. We pretrain RoBERTa models from scratch on quantities of data ranging from 1M to 1B words and compare their performance on MSGS to the publicly available RoBERTa-base. We find that models can learn to represent linguistic features with little pretraining data, but require far more data to learn to prefer linguistic generalizations over surface ones. Eventually, with about 30B words of pretraining data, RoBERTa-base does demonstrate a linguistic bias with some regularity. We conclude that while self-supervised pretraining is an effective way to learn helpful inductive biases, there is likely room to improve the rate at which models learn which features matter.},
	urldate = {2022-03-18},
	journal = {arXiv:2010.05358 [cs]},
	author = {Warstadt, Alex and Zhang, Yian and Li, Haau-Sing and Liu, Haokun and Bowman, Samuel R.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.05358},
	keywords = {Computer Science - Computation and Language},
}

@inproceedings{daniely_optimal_2014,
	title = {Optimal learners for multiclass problems},
	booktitle = {Conference on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Daniely, Amit and Shalev-Shwartz, Shai},
	year = {2014},
	pages = {287--316},
}

@inproceedings{daniely_multiclass_2011,
	title = {Multiclass learnability and the erm principle},
	booktitle = {Proceedings of the 24th {Annual} {Conference} on {Learning} {Theory}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Daniely, Amit and Sabato, Sivan and Ben-David, Shai and Shalev-Shwartz, Shai},
	year = {2011},
	pages = {207--232},
}

@article{bendavid_characterizations_1995,
	title = {Characterizations of learnability for classes of \$\{\vphantom{\}}\$0,..., n)-valued functions},
	volume = {50},
	number = {1},
	journal = {Journal of Computer and System Sciences},
	author = {Bendavid, Shai and Cesabianchi, Nicolo and Haussler, David and Long, Philip M},
	year = {1995},
	note = {Publisher: Elsevier},
	pages = {74--86},
}

@misc{noauthor_zkf5051tmp_nodate,
	title = {zkf5051.tmp {\textbar} {Elsevier} {Enhanced} {Reader}},
	url = {https://reader.elsevier.com/reader/sd/pii/S0022000085710082?token=23A794C390BD4905D5ABB7A746256BC3BF2227B56C6377735108FD81B9CBA503EB0E58453D8106F3E8927EB5D7BCCDE3&originRegion=us-east-1&originCreation=20220317090847},
	language = {en},
	urldate = {2022-03-17},
	doi = {10.1006/jcss.1995.1008},
}

@misc{michael_mathematical_nodate,
	title = {Mathematical {Foundations} of {Supervised} {Learning}},
	author = {Michael, Wolf},
}

@article{hastie_classification_1998,
	title = {Classification by pairwise coupling},
	volume = {26},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-26/issue-2/Classification-by-pairwise-coupling/10.1214/aos/1028144844.full},
	doi = {10.1214/aos/1028144844},
	abstract = {We discuss a strategy for polychotomous classification that involves coupling the estimating class probabilities for each pair of classes, and estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated data sets. Classifiers used include linear discriminants, nearest neighbors, adaptive nonlinear methods and the support vector machine.},
	number = {2},
	urldate = {2022-03-10},
	journal = {The Annals of Statistics},
	author = {Hastie, Trevor and Tibshirani, Robert},
	month = apr,
	year = {1998},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62H30, 62J15, 68T10, Bradley-Terry model, Pairwise},
	pages = {451--471},
}

@article{jaiswal_survey_2021,
	title = {A {Survey} on {Contrastive} {Self}-supervised {Learning}},
	url = {http://arxiv.org/abs/2011.00362},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.},
	urldate = {2022-03-09},
	journal = {arXiv:2011.00362 [cs]},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = feb,
	year = {2021},
	note = {arXiv: 2011.00362},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jing_self-supervised_2019,
	title = {Self-supervised {Visual} {Feature} {Learning} with {Deep} {Neural} {Networks}: {A} {Survey}},
	shorttitle = {Self-supervised {Visual} {Feature} {Learning} with {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1902.06162},
	abstract = {Large-scale labeled data are generally required to train deep neural networks in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels. This paper provides an extensive review of deep learning-based self-supervised general visual feature learning methods from images or videos. First, the motivation, general pipeline, and terminologies of this field are described. Then the common deep neural network architectures that used for self-supervised learning are summarized. Next, the main components and evaluation metrics of self-supervised learning methods are reviewed followed by the commonly used image and video datasets and the existing self-supervised visual feature learning methods. Finally, quantitative performance comparisons of the reviewed methods on benchmark datasets are summarized and discussed for both image and video feature learning. At last, this paper is concluded and lists a set of promising future directions for self-supervised visual feature learning.},
	urldate = {2022-03-09},
	journal = {arXiv:1902.06162 [cs]},
	author = {Jing, Longlong and Tian, Yingli},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06162},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{jaiswal_survey_2021-1,
	title = {A {Survey} on {Contrastive} {Self}-supervised {Learning}},
	url = {http://arxiv.org/abs/2011.00362},
	abstract = {Self-supervised learning has gained popularity because of its ability to avoid the cost of annotating large-scale datasets. It is capable of adopting self-defined pseudo labels as supervision and use the learned representations for several downstream tasks. Specifically, contrastive learning has recently become a dominant component in self-supervised learning methods for computer vision, natural language processing (NLP), and other domains. It aims at embedding augmented versions of the same sample close to each other while trying to push away embeddings from different samples. This paper provides an extensive review of self-supervised methods that follow the contrastive approach. The work explains commonly used pretext tasks in a contrastive learning setup, followed by different architectures that have been proposed so far. Next, we have a performance comparison of different methods for multiple downstream tasks such as image classification, object detection, and action recognition. Finally, we conclude with the limitations of the current methods and the need for further techniques and future directions to make substantial progress.},
	urldate = {2022-03-09},
	journal = {arXiv:2011.00362 [cs]},
	author = {Jaiswal, Ashish and Babu, Ashwin Ramesh and Zadeh, Mohammad Zaki and Banerjee, Debapriya and Makedon, Fillia},
	month = feb,
	year = {2021},
	note = {arXiv: 2011.00362},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@book{hinton_unsupervised_1999,
	title = {Unsupervised learning: foundations of neural computation},
	publisher = {MIT press},
	author = {Hinton, Geoffrey and Sejnowski, Terrence J},
	year = {1999},
}

@book{rumelhart_parallel_1986,
	address = {Cambridge, MA, USA},
	title = {Parallel {Distributed} {Processing}: {Explorations} in the {Microstructure} of {Cognition}: {Foundations}},
	volume = {1},
	isbn = {978-0-262-18120-4},
	shorttitle = {Parallel {Distributed} {Processing}},
	language = {en},
	publisher = {A Bradford Book},
	author = {Rumelhart, David E. and McClelland, James L. and Group, PDP Research},
	month = jul,
	year = {1986},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	copyright = {2015 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2022-03-09},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	note = {Number: 7553
Publisher: Nature Publishing Group},
	keywords = {Computer science, Mathematics and computing},
	pages = {436--444},
}

@inproceedings{hinton_learning_1986,
	title = {Learning distributed representations of concepts},
	volume = {1},
	booktitle = {Proceedings of the eighth annual conference of the cognitive science society},
	publisher = {Amherst, MA},
	author = {Hinton, Geoffrey E and {others}},
	year = {1986},
	pages = {12},
}

@article{bengio_representation_2014,
	title = {Representation {Learning}: {A} {Review} and {New} {Perspectives}},
	shorttitle = {Representation {Learning}},
	url = {http://arxiv.org/abs/1206.5538},
	abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
	urldate = {2022-03-09},
	journal = {arXiv:1206.5538 [cs]},
	author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
	month = apr,
	year = {2014},
	note = {arXiv: 1206.5538},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{kolesnikov_revisiting_2019,
	address = {Long Beach, CA, USA},
	title = {Revisiting {Self}-{Supervised} {Visual} {Representation} {Learning}},
	isbn = {978-1-72813-293-8},
	url = {https://ieeexplore.ieee.org/document/8953672/},
	doi = {10.1109/CVPR.2019.00202},
	abstract = {Unsupervised visual representation learning remains a largely unsolved problem in computer vision research. Among a big body of recently proposed approaches for unsupervised learning of visual representations, a class of self-supervised techniques achieves superior performance on many challenging benchmarks. A large number of the pretext tasks for self-supervised learning have been studied, but other important aspects, such as the choice of convolutional neural networks (CNN), has not received equal attention. Therefore, we revisit numerous previously proposed self-supervised models, conduct a thorough large scale study and, as a result, uncover multiple crucial insights. We challenge a number of common practices in selfsupervised visual representation learning and observe that standard recipes for CNN design do not always translate to self-supervised representation learning. As part of our study, we drastically boost the performance of previously proposed techniques and outperform previously published state-of-the-art results by a large margin.},
	language = {en},
	urldate = {2022-03-09},
	booktitle = {2019 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Kolesnikov, Alexander and Zhai, Xiaohua and Beyer, Lucas},
	month = jun,
	year = {2019},
	pages = {1920--1929},
}

@article{goyal_self-supervised_2021,
	title = {Self-supervised {Pretraining} of {Visual} {Features} in the {Wild}},
	url = {http://arxiv.org/abs/2103.01988},
	abstract = {Recently, self-supervised learning methods like MoCo [22], SimCLR [8], BYOL [20] and SwAV [7] have reduced the gap with supervised methods. These results have been achieved in a control environment, that is the highly curated ImageNet dataset. However, the premise of self-supervised learning is that it can learn from any random image and from any unbounded dataset. In this work, we explore if self-supervision lives to its expectation by training large models on random, uncurated images with no supervision. Our ﬁnal SElf-supERvised (SEER) model, a RegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves 84.2\% top-1 accuracy, surpassing the best self-supervised pretrained model by 1\% and conﬁrming that self-supervised learning works in a real world setting. Interestingly, we also observe that selfsupervised models are good few-shot learners achieving 77.9\% top-1 with access to only 10\% of ImageNet.},
	language = {en},
	urldate = {2022-03-09},
	journal = {arXiv:2103.01988 [cs]},
	author = {Goyal, Priya and Caron, Mathilde and Lefaudeux, Benjamin and Xu, Min and Wang, Pengchao and Pai, Vivek and Singh, Mannat and Liptchinsky, Vitaliy and Misra, Ishan and Joulin, Armand and Bojanowski, Piotr},
	month = mar,
	year = {2021},
	note = {arXiv: 2103.01988},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
}

@article{bommasani_opportunities_2021,
	title = {On the {Opportunities} and {Risks} of {Foundation} {Models}},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2022-03-09},
	journal = {arXiv:2108.07258 [cs]},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.07258},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@article{zadeh_similarity_1971,
	title = {Similarity relations and fuzzy orderings},
	volume = {3},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025571800051},
	doi = {10.1016/S0020-0255(71)80005-1},
	abstract = {The notion of “similarity” as defined in this paper is essentially a generalization of the notion of equivalence. In the same vein, a fuzzy ordering is a generalization of the concept of ordering. For example, the relation x ≫ y (x is much larger than y) is a fuzzy linear ordering in the set of real numbers. More concretely, a similarity relation, S, is a fuzzy relation which is reflexive, symmetric, and transitive. Thus, let x, y be elements of a set X and μs(x,y) denote the grade of membership of the ordered pair (x,y) in S. Then S is a similarity relation in X if and only if, for all x, y, z in X, μs(x,x) = 1 (reflexivity), μs(x,y) = μs(y,x) (symmetry), and μs(x,z) ⩾ ∨ (μs(x,y) Å μs(y,z)) (transitivity), where ∀ and Å denote max and min, respectively. y A fuzzy ordering is a fuzzy relation which is transitive. In particular, a fuzzy partial ordering, P, is a fuzzy ordering which is reflexive and antisymmetric, that is, (μP(x,y) {\textgreater} 0 and x ≠ y) ⇒ μP(y,x) = 0. A fuzzy linear ordering is a fuzzy partial ordering in which x ≠ y ⇒ μs(x,y) {\textgreater} 0 or μs(y,x) {\textgreater} 0. A fuzzy preordering is a fuzzy ordering which is reflexive. A fuzzy weak ordering is a fuzzy preordering in which x ≠ y ⇒ μs(x,y) {\textgreater} 0 or μs(y,x) {\textgreater} 0. Various properties of similarity relations and fuzzy orderings are investigated and, as an illustration, an extended version of Szpilrajn's theorem is proved.},
	language = {en},
	number = {2},
	urldate = {2022-03-09},
	journal = {Information Sciences},
	author = {Zadeh, L. A.},
	month = apr,
	year = {1971},
	pages = {177--200},
}

@article{ciric_fuzzy_2007,
	title = {Fuzzy equivalence relations and their equivalence classes},
	volume = {158},
	issn = {0165-0114},
	url = {https://www.sciencedirect.com/science/article/pii/S0165011407000358},
	doi = {10.1016/j.fss.2007.01.010},
	abstract = {In this paper we investigate various properties of equivalence classes of fuzzy equivalence relations over a complete residuated lattice. We give certain characterizations of fuzzy semi-partitions and fuzzy partitions over a complete residuated lattice, as well as over a linearly ordered complete Heyting algebra. In the latter case, for a fuzzy equivalence relation over a linearly ordered complete Heyting algebra, we construct an algorithm for calculation of a minimal family of its equivalence classes which generates it. Most of the presented results are new, but some of them are generalizations of known results given in a way which simplifies and clarifies them.},
	language = {en},
	number = {12},
	urldate = {2022-03-09},
	journal = {Fuzzy Sets and Systems},
	author = {Ćirić, Miroslav and Ignjatović, Jelena and Bogdanović, Stojan},
	month = jun,
	year = {2007},
	keywords = {Complete residuated lattice, Fuzzy equivalence class, Fuzzy equivalence relation, Fuzzy partition, Fuzzy semi-partition, Linearly ordered Heyting algebra},
	pages = {1295--1313},
}

@misc{goyal_vissl_2021,
	title = {{VISSL}},
	url = {https://github.com/facebookresearch/vissl},
	author = {Goyal, Priya and Duval, Quentin and Reizenstein, Jeremy and Leavitt, Matthew and Xu, Min and Lefaudeux, Benjamin and Singh, Mannat and Reis, Vinicius and Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Misra, Ishan},
	year = {2021},
}

@article{le_tiny_2015,
	title = {Tiny {ImageNet} {Visual} {Recognition} {Challenge}},
	abstract = {In this work, we investigate the effect of convolutional network depth, receptive ﬁeld size, dropout layers, rectiﬁed activation unit type and dataset noise on its accuracy in Tiny-ImageNet Challenge settings. In order to make a thorough evaluation of the cause of the peformance improvement, we start with a basic 5 layer model with 5×5 convolutional receptive ﬁelds. We keep increasing network depth or reducing receptive ﬁeld size, and continue applying modern techniques, such as PReLu and dropout, to the model. Our model achieves excellent performance even compared to state-of-the-art results, with 0.444 ﬁnal error rate on the test set.},
	language = {en},
	author = {Le, Ya and Yang, Xuan},
	year = {2015},
	pages = {6},
}

@article{zagoruyko_wide_2017,
	title = {Wide {Residual} {Networks}},
	url = {http://arxiv.org/abs/1605.07146},
	abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
	urldate = {2022-03-08},
	journal = {arXiv:1605.07146 [cs]},
	author = {Zagoruyko, Sergey and Komodakis, Nikos},
	month = jun,
	year = {2017},
	note = {arXiv: 1605.07146},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{yuan_provable_2022,
	title = {Provable {Stochastic} {Optimization} for {Global} {Contrastive} {Learning}: {Small} {Batch} {Does} {Not} {Harm} {Performance}},
	shorttitle = {Provable {Stochastic} {Optimization} for {Global} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2202.12387},
	abstract = {In this paper, we study contrastive learning from an optimization perspective, aiming to analyze and address a fundamental issue of existing contrastive learning methods that either rely on a large batch size or a large dictionary. We consider a global objective for contrastive learning, which contrasts each positive pair with all negative pairs for an anchor point. From the optimization perspective, we explain why existing methods such as SimCLR requires a large batch size in order to achieve a satisfactory result. In order to remove such requirement, we propose a memory-efficient Stochastic Optimization algorithm for solving the Global objective of Contrastive Learning of Representations, named SogCLR. We show that its optimization error is negligible under a reasonable condition after a sufficient number of iterations or is diminishing for a slightly different global contrastive objective. Empirically, we demonstrate that on ImageNet with a batch size 256, SogCLR achieves a performance of 69.4\% for top-1 linear evaluation accuracy using ResNet-50, which is on par with SimCLR (69.3\%) with a large batch size 8,192. We also attempt to show that the proposed optimization technique is generic and can be applied to solving other contrastive losses, e.g., two-way contrastive losses for bimodal contrastive learning.},
	urldate = {2022-03-08},
	journal = {arXiv:2202.12387 [cs, math, stat]},
	author = {Yuan, Zhuoning and Wu, Yuexin and Qiu, Zihao and Du, Xianzhi and Zhang, Lijun and Zhou, Denny and Yang, Tianbao},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.12387},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@article{sinkhorn_concerning_1967,
	title = {Concerning nonnegative matrices and doubly stochastic matrices},
	volume = {21},
	number = {2},
	journal = {Pacific Journal of Mathematics},
	author = {Sinkhorn, Richard and Knopp, Paul},
	year = {1967},
	note = {Publisher: Mathematical Sciences Publishers},
	pages = {343--348},
}

@article{liu_self-supervised_2021,
	title = {Self-supervised {Learning}: {Generative} or {Contrastive}},
	issn = {1041-4347, 1558-2191, 2326-3865},
	shorttitle = {Self-supervised {Learning}},
	url = {http://arxiv.org/abs/2006.08218},
	doi = {10.1109/TKDE.2021.3090866},
	abstract = {Deep supervised learning has achieved great success in the last decade. However, its deficiencies of dependence on manual labels and vulnerability to attacks have driven people to explore a better solution. As an alternative, self-supervised learning attracts many researchers for its soaring performance on representation learning in the last several years. Self-supervised representation learning leverages input data itself as supervision and benefits almost all types of downstream tasks. In this survey, we take a look into new self-supervised learning methods for representation in computer vision, natural language processing, and graph learning. We comprehensively review the existing empirical methods and summarize them into three main categories according to their objectives: generative, contrastive, and generative-contrastive (adversarial). We further investigate related theoretical analysis work to provide deeper thoughts on how self-supervised learning works. Finally, we briefly discuss open problems and future directions for self-supervised learning. An outline slide for the survey is provided.},
	urldate = {2022-03-07},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Liu, Xiao and Zhang, Fanjin and Hou, Zhenyu and Wang, Zhaoyu and Mian, Li and Zhang, Jing and Tang, Jie},
	year = {2021},
	note = {arXiv: 2006.08218},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--1},
}

@inproceedings{dumoulin_adversarially_2017,
	title = {Adversarially {Learned} {Inference}},
	url = {https://openreview.net/forum?id=B1ElR4cgg},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Lamb, Alex and Arjovsky, Martín and Mastropietro, Olivier and Courville, Aaron C.},
	year = {2017},
}

@inproceedings{donahue_adversarial_2017,
	title = {Adversarial {Feature} {Learning}},
	url = {https://openreview.net/forum?id=BJtNZAFgg},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
	year = {2017},
}

@article{rezende_stochastic_2014,
	title = {Stochastic {Back}-propagation and {Variational} {Inference} in {Deep} {Latent} {Gaussian} {Models}},
	volume = {abs/1401.4082},
	url = {http://arxiv.org/abs/1401.4082},
	journal = {CoRR},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	year = {2014},
	note = {arXiv: 1401.4082},
}

@inproceedings{radford_unsupervised_2016,
	title = {Unsupervised {Representation} {Learning} with {Deep} {Convolutional} {Generative} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1511.06434},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
}

@inproceedings{vincent_extracting_2008,
	series = {{ACM} {International} {Conference} {Proceeding} {Series}},
	title = {Extracting and composing robust features with denoising autoencoders},
	volume = {307},
	url = {https://doi.org/10.1145/1390156.1390294},
	doi = {10.1145/1390156.1390294},
	booktitle = {Machine {Learning}, {Proceedings} of the {Twenty}-{Fifth} {International} {Conference} ({ICML} 2008), {Helsinki}, {Finland}, {June} 5-9, 2008},
	publisher = {ACM},
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Yoshua and Manzagol, Pierre-Antoine},
	editor = {Cohen, William W. and McCallum, Andrew and Roweis, Sam T.},
	year = {2008},
	pages = {1096--1103},
}

@inproceedings{grill_bootstrap_2020,
	title = {Bootstrap {Your} {Own} {Latent} - {A} {New} {Approach} to {Self}-{Supervised} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Grill, Jean-Bastien and Strub, Florian and Altché, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, Rémi and Valko, Michal},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@article{tian_understandingdual_2021,
	title = {Understanding self-supervised {Learning} {Dynamics} without {Contrastive} {Pairs}},
	url = {http://arxiv.org/abs/2102.06810},
	abstract = {While contrastive approaches of self-supervised learning (SSL) learn representations by minimizing the distance between two augmented views of the same data point (positive pairs) and maximizing views from different data points (negative pairs), recent {\textbackslash}emph\{non-contrastive\} SSL (e.g., BYOL and SimSiam) show remarkable performance \{{\textbackslash}it without\} negative pairs, with an extra learnable predictor and a stop-gradient operation. A fundamental question arises: why do these methods not collapse into trivial representations? We answer this question via a simple theoretical study and propose a novel approach, DirectPred, that {\textbackslash}emph\{directly\} sets the linear predictor based on the statistics of its inputs, without gradient training. On ImageNet, it performs comparably with more complex two-layer non-linear predictors that employ BatchNorm and outperforms a linear predictor by \$2.5{\textbackslash}\%\$ in 300-epoch training (and \$5{\textbackslash}\%\$ in 60-epoch). DirectPred is motivated by our theoretical study of the nonlinear learning dynamics of non-contrastive SSL in simple linear networks. Our study yields conceptual insights into how non-contrastive SSL methods learn, how they avoid representational collapse, and how multiple factors, like predictor networks, stop-gradients, exponential moving averages, and weight decay all come into play. Our simple theory recapitulates the results of real-world ablation studies in both STL-10 and ImageNet. Code is released https://github.com/facebookresearch/luckmatters/tree/master/ssl.},
	urldate = {2021-10-19},
	journal = {arXiv:2102.06810 [cs]},
	author = {Tian, Yuandong and Chen, Xinlei and Ganguli, Surya},
	month = oct,
	year = {2021},
	note = {arXiv: 2102.06810},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{asano_self-labelling_2020,
	title = {Self-labelling via simultaneous clustering and representation learning},
	url = {https://openreview.net/forum?id=Hyx-jyBFPr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year = {2020},
}

@inproceedings{chen_exploring_2021,
	address = {Nashville, TN, USA},
	title = {Exploring {Simple} {Siamese} {Representation} {Learning}},
	isbn = {978-1-66544-509-2},
	url = {https://ieeexplore.ieee.org/document/9578004/},
	doi = {10.1109/CVPR46437.2021.01549},
	language = {en},
	urldate = {2022-03-07},
	booktitle = {2021 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Chen, Xinlei and He, Kaiming},
	month = jun,
	year = {2021},
	pages = {15745--15753},
}

@article{li_momentum2_2021,
	title = {Momentum{\textasciicircum}2 {Teacher}: {Momentum} {Teacher} with {Momentum} {Statistics} for {Self}-{Supervised} {Learning}},
	shorttitle = {Momentum{\textasciicircum}2 {Teacher}},
	url = {http://arxiv.org/abs/2101.07525},
	abstract = {In this paper, we present a novel approach, Momentum\${\textasciicircum}2\$ Teacher, for student-teacher based self-supervised learning. The approach performs momentum update on both network weights and batch normalization (BN) statistics. The teacher's weight is a momentum update of the student, and the teacher's BN statistics is a momentum update of those in history. The Momentum\${\textasciicircum}2\$ Teacher is simple and efficient. It can achieve the state of the art results (74.5{\textbackslash}\%) under ImageNet linear evaluation protocol using small-batch size({\textbackslash}eg, 128), without requiring large-batch training on special hardware like TPU or inefficient across GPU operation ({\textbackslash}eg, shuffling BN, synced BN). Our implementation and pre-trained models will be given on GitHub{\textbackslash}footnote\{https://github.com/zengarden/momentum2-teacher\}.},
	urldate = {2022-03-07},
	journal = {arXiv:2101.07525 [cs]},
	author = {Li, Zeming and Liu, Songtao and Sun, Jian},
	month = jan,
	year = {2021},
	note = {arXiv: 2101.07525},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{tejankar_isd_2021,
	title = {{ISD}: {Self}-{Supervised} {Learning} by {Iterative} {Similarity} {Distillation}},
	shorttitle = {{ISD}},
	url = {http://arxiv.org/abs/2012.09259},
	abstract = {Recently, contrastive learning has achieved great results in self-supervised learning, where the main idea is to push two augmentations of an image (positive pairs) closer compared to other random images (negative pairs). We argue that not all random images are equal. Hence, we introduce a self supervised learning algorithm where we use a soft similarity for the negative images rather than a binary distinction between positive and negative pairs. We iteratively distill a slowly evolving teacher model to the student model by capturing the similarity of a query image to some random images and transferring that knowledge to the student. We argue that our method is less constrained compared to recent contrastive learning methods, so it can learn better features. Specifically, our method should handle unbalanced and unlabeled data better than existing contrastive learning methods, because the randomly chosen negative set might include many samples that are semantically similar to the query image. In this case, our method labels them as highly similar while standard contrastive methods label them as negative pairs. Our method achieves comparable results to the state-of-the-art models. We also show that our method performs better in the settings where the unlabeled data is unbalanced. Our code is available here: https://github.com/UMBCvision/ISD.},
	urldate = {2022-03-07},
	journal = {arXiv:2012.09259 [cs]},
	author = {Tejankar, Ajinkya and Koohpayegani, Soroush Abbasi and Pillai, Vipin and Favaro, Paolo and Pirsiavash, Hamed},
	month = sep,
	year = {2021},
	note = {arXiv: 2012.09259},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{fang_seed_2021,
	title = {{SEED}: {Self}-supervised {Distillation} {For} {Visual} {Representation}},
	url = {https://openreview.net/forum?id=AHm3dbp7D1D},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Fang, Zhiyuan and Wang, Jianfeng and Wang, Lijuan and Zhang, Lei and Yang, Yezhou and Liu, Zicheng},
	year = {2021},
}

@book{anthony_neural_2002,
	title = {Neural {Network} {Learning} - {Theoretical} {Foundations}},
	isbn = {978-0-521-57353-5},
	url = {http://www.cambridge.org/gb/knowledge/isbn/item1154061/?site\_locale=en\_GB},
	publisher = {Cambridge University Press},
	author = {Anthony, Martin and Bartlett, Peter L.},
	year = {2002},
}

@article{vapnik_uniform_1971,
	title = {On the {Uniform} {Convergence} of {Relative} {Frequencies} of {Events} to {Their} {Probabilities}},
	volume = {16},
	url = {https://doi.org/10.1137/1116025},
	doi = {10.1137/1116025},
	number = {2},
	journal = {Theory of Probability \& Its Applications},
	author = {Vapnik, V. N. and Chervonenkis, A. Ya.},
	year = {1971},
	note = {\_eprint: https://doi.org/10.1137/1116025},
	pages = {264--280},
}

@inproceedings{valiant_theory_1984,
	title = {A {Theory} of the {Learnable}},
	url = {https://doi.org/10.1145/800057.808710},
	doi = {10.1145/800057.808710},
	booktitle = {Proceedings of the 16th {Annual} {ACM} {Symposium} on {Theory} of {Computing}, {April} 30 - {May} 2, 1984, {Washington}, {DC}, {USA}},
	publisher = {ACM},
	author = {Valiant, Leslie G.},
	editor = {DeMillo, Richard A.},
	year = {1984},
	pages = {436--445},
}

@article{flajolet_birthday_1992,
	title = {Birthday paradox, coupon collectors, caching algorithms and self-organizing search},
	volume = {39},
	issn = {0166-218X},
	url = {https://www.sciencedirect.com/science/article/pii/0166218X9290177C},
	doi = {10.1016/0166-218X(92)90177-C},
	abstract = {This paper introduces a unified framework for the analysis of a class of random allocation processes that include: (i) the birthday paradox; (ii) the coupon collector problem; (iii) least-recently-used (LRU) caching in memory management systems under the independent reference model; (iv) the move-to-front heuristic of self-organizing search. All analyses are relative to general nonuniform probability distributions. Our approach to these problems comprises two stages. First, the probabilistic phenomena of interest are described by means of regular languages extended by addition of the shuffle product. Next, systematic translation mechanisms are used to derive integral representations for expectations and probability distributions.},
	language = {en},
	number = {3},
	urldate = {2022-03-07},
	journal = {Discrete Applied Mathematics},
	author = {Flajolet, Philippe and Gardy, Danièle and Thimonier, Loÿs},
	month = nov,
	year = {1992},
	pages = {207--229},
}

@inproceedings{leen_data_1994,
	title = {From {Data} {Distributions} to {Regularization} in {Invariant} {Learning}},
	volume = {7},
	url = {https://proceedings.neurips.cc/paper/1994/hash/7fa732b517cbed14a48843d74526c11a-Abstract.html},
	urldate = {2022-03-07},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Leen, Todd},
	year = {1994},
}

@article{jozefowicz_exploring_2016,
	title = {Exploring the {Limits} of {Language} {Modeling}},
	url = {http://arxiv.org/abs/1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	urldate = {2022-03-05},
	journal = {arXiv:1602.02410 [cs]},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02410},
	keywords = {Computer Science - Computation and Language},
}

@article{goyal_vision_2022,
	title = {Vision {Models} {Are} {More} {Robust} {And} {Fair} {When} {Pretrained} {On} {Uncurated} {Images} {Without} {Supervision}},
	url = {http://arxiv.org/abs/2202.08360},
	abstract = {Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet.},
	urldate = {2022-03-03},
	journal = {arXiv:2202.08360 [cs]},
	author = {Goyal, Priya and Duval, Quentin and Seessel, Isaac and Caron, Mathilde and Misra, Ishan and Sagun, Levent and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.08360},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society},
}

@article{baevski_data2vec_nodate,
	title = {data2vec: {A} {General} {Framework} for {Self}-supervised {Learning} in {Speech}, {Vision} and {Language}},
	abstract = {While the general idea of self-supervised learning is identical across modalities, the actual algorithms and objectives differ widely because they were developed with a single modality in mind. To get us closer to general self-supervised learning, we present data2vec, a framework that uses the same learning method for either speech, NLP or computer vision. The core idea is to predict latent representations of the full input data based on a masked view of the input in a selfdistillation setup using a standard Transformer architecture. Instead of predicting modality-specific targets such as words, visual tokens or units of human speech which are local in nature, data2vec predicts contextualized latent representations that contain information from the entire input. Experiments on the major benchmarks of speech recognition, image classification, and natural language understanding demonstrate a new state of the art or competitive performance to predominant approaches. Models and code are available at www.github.com/pytorch/fairseq/ tree/master/examples/data2vec.},
	language = {en},
	author = {Baevski, Alexei and Hsu, Wei-Ning and Xu, Qiantong and Babu, Arun and Gu, Jiatao and Auli, Michael},
	pages = {13},
}

@article{tomasev_pushing_2022,
	title = {Pushing the limits of self-supervised {ResNets}: {Can} we outperform supervised learning without labels on {ImageNet}?},
	shorttitle = {Pushing the limits of self-supervised {ResNets}},
	url = {http://arxiv.org/abs/2201.05119},
	abstract = {Despite recent progress made by self-supervised methods in representation learning with residual networks, they still underperform supervised learning on the ImageNet classification benchmark, limiting their applicability in performance-critical settings. Building on prior theoretical insights from Mitrovic et al., 2021, we propose ReLICv2 which combines an explicit invariance loss with a contrastive objective over a varied set of appropriately constructed data views. ReLICv2 achieves 77.1\% top-1 classification accuracy on ImageNet using linear evaluation with a ResNet50 architecture and 80.6\% with larger ResNet models, outperforming previous state-of-the-art self-supervised approaches by a wide margin. Most notably, ReLICv2 is the first representation learning method to consistently outperform the supervised baseline in a like-for-like comparison using a range of standard ResNet architectures. Finally we show that despite using ResNet encoders, ReLICv2 is comparable to state-of-the-art self-supervised vision transformers.},
	urldate = {2022-03-03},
	journal = {arXiv:2201.05119 [cs, stat]},
	author = {Tomasev, Nenad and Bica, Ioana and McWilliams, Brian and Buesing, Lars and Pascanu, Razvan and Blundell, Charles and Mitrovic, Jovana},
	month = jan,
	year = {2022},
	note = {arXiv: 2201.05119},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wang_deep_2017,
	title = {Deep {Variational} {Canonical} {Correlation} {Analysis}},
	url = {http://arxiv.org/abs/1610.03454},
	abstract = {We present deep variational canonical correlation analysis (VCCA), a deep multi-view learning model that extends the latent variable model interpretation of linear CCA to nonlinear observation models parameterized by deep neural networks. We derive variational lower bounds of the data likelihood by parameterizing the posterior probability of the latent variables from the view that is available at test time. We also propose a variant of VCCA called VCCA-private that can, in addition to the "common variables" underlying both views, extract the "private variables" within each view, and disentangles the shared and private information for multi-view data without hard supervision. Experimental results on real-world datasets show that our methods are competitive across domains.},
	urldate = {2022-03-03},
	journal = {arXiv:1610.03454 [cs]},
	author = {Wang, Weiran and Yan, Xinchen and Lee, Honglak and Livescu, Karen},
	month = feb,
	year = {2017},
	note = {arXiv: 1610.03454},
	keywords = {Computer Science - Machine Learning},
}

@article{akbari_vatt_2021,
	title = {{VATT}: {Transformers} for {Multimodal} {Self}-{Supervised} {Learning} from {Raw} {Video}, {Audio} and {Text}},
	shorttitle = {{VATT}},
	url = {http://arxiv.org/abs/2104.11178},
	abstract = {We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic, single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1\% on Kinetics-400, 83.6\% on Kinetics-600, 72.7\% on Kinetics-700, and 41.1\% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7\% top-1 accuracy on ImageNet compared to 64.7\% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4\% on AudioSet without any supervised pre-training. VATT's source code is publicly available.},
	urldate = {2022-03-03},
	journal = {arXiv:2104.11178 [cs, eess]},
	author = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
	month = dec,
	year = {2021},
	note = {arXiv: 2104.11178},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Multimedia, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{federici_learning_2020,
	title = {Learning {Robust} {Representations} via {Multi}-{View} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/2002.07017},
	abstract = {The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.},
	urldate = {2022-03-03},
	journal = {arXiv:2002.07017 [cs, stat]},
	author = {Federici, Marco and Dutta, Anjan and Forré, Patrick and Kushman, Nate and Akata, Zeynep},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.07017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lee_compressive_nodate,
	title = {Compressive {Visual} {Representations}},
	language = {en},
	author = {Lee, Kuang-Huei and Arnab, Anurag and Guadarrama, Sergio and Canny, John and Fischer, Ian},
	pages = {10},
}

@article{miyato_virtual_2018,
	title = {Virtual {Adversarial} {Training}: {A} {Regularization} {Method} for {Supervised} and {Semi}-{Supervised} {Learning}},
	shorttitle = {Virtual {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1704.03976},
	abstract = {We propose a new regularization method based on virtual adversarial loss: a new measure of local smoothness of the conditional label distribution given input. Virtual adversarial loss is defined as the robustness of the conditional label distribution around each input data point against local perturbation. Unlike adversarial training, our method defines the adversarial direction without label information and is hence applicable to semi-supervised learning. Because the directions in which we smooth the model are only "virtually" adversarial, we call our method virtual adversarial training (VAT). The computational cost of VAT is relatively low. For neural networks, the approximated gradient of virtual adversarial loss can be computed with no more than two pairs of forward- and back-propagations. In our experiments, we applied VAT to supervised and semi-supervised learning tasks on multiple benchmark datasets. With a simple enhancement of the algorithm based on the entropy minimization principle, our VAT achieves state-of-the-art performance for semi-supervised learning tasks on SVHN and CIFAR-10.},
	urldate = {2022-03-03},
	journal = {arXiv:1704.03976 [cs, stat]},
	author = {Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
	month = jun,
	year = {2018},
	note = {arXiv: 1704.03976},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xie_unsupervised_2020,
	title = {Unsupervised {Data} {Augmentation} for {Consistency} {Training}},
	url = {http://arxiv.org/abs/1904.12848},
	abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
	urldate = {2022-03-03},
	journal = {arXiv:1904.12848 [cs, stat]},
	author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
	month = nov,
	year = {2020},
	note = {arXiv: 1904.12848},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{sinha_consistency_2021,
	title = {Consistency regularization for variational auto-encoders},
	volume = {34},
	journal = {Advances in Neural Information Processing Systems},
	author = {Sinha, Samarth and Dieng, Adji Bousso},
	year = {2021},
}

@inproceedings{alayrac_self-supervised_2020,
	title = {Self-{Supervised} {MultiModal} {Versatile} {Networks}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/0060ef47b12160b9198302ebdb144dcf-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Alayrac, Jean-Baptiste and Recasens, Adrià and Schneider, Rosalia and Arandjelovic, Relja and Ramapuram, Jason and Fauw, Jeffrey De and Smaira, Lucas and Dieleman, Sander and Zisserman, Andrew},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@article{wang_multimodal_2021,
	title = {Multimodal {Self}-{Supervised} {Learning} of {General} {Audio} {Representations}},
	url = {http://arxiv.org/abs/2104.12807},
	abstract = {We present a multimodal framework to learn general audio representations from videos. Existing contrastive audio representation learning methods mainly focus on using the audio modality alone during training. In this work, we show that additional information contained in video can be utilized to greatly improve the learned features. First, we demonstrate that our contrastive framework does not require high resolution images to learn good audio features. This allows us to scale up the training batch size, while keeping the computational load incurred by the additional video modality to a reasonable level. Second, we use augmentations that mix together different samples. We show that this is effective to make the proxy task harder, which leads to substantial performance improvements when increasing the batch size. As a result, our audio model achieves a state-of-the-art of 42.4 mAP on the AudioSet classification downstream task, closing the gap between supervised and self-supervised methods trained on the same dataset. Moreover, we show that our method is advantageous on a broad range of non-semantic audio tasks, including speaker identification, keyword spotting, language identification, and music instrument classification.},
	urldate = {2022-03-03},
	journal = {arXiv:2104.12807 [cs, eess]},
	author = {Wang, Luyu and Luc, Pauline and Recasens, Adria and Alayrac, Jean-Baptiste and Oord, Aaron van den},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12807},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
}

@article{saunshi_understanding_2022,
	title = {Understanding {Contrastive} {Learning} {Requires} {Incorporating} {Inductive} {Biases}},
	url = {http://arxiv.org/abs/2202.14037},
	abstract = {Contrastive learning is a popular form of self-supervised learning that encourages augmentations (views) of the same input to have more similar representations compared to augmentations of different inputs. Recent attempts to theoretically explain the success of contrastive learning on downstream classification tasks prove guarantees depending on properties of \{{\textbackslash}em augmentations\} and the value of \{{\textbackslash}em contrastive loss\} of representations. We demonstrate that such analyses, that ignore \{{\textbackslash}em inductive biases\} of the function class and training algorithm, cannot adequately explain the success of contrastive learning, even \{{\textbackslash}em provably\} leading to vacuous guarantees in some settings. Extensive experiments on image and text domains highlight the ubiquity of this problem -- different function classes and algorithms behave very differently on downstream tasks, despite having the same augmentations and contrastive losses. Theoretical analysis is presented for the class of linear representations, where incorporating inductive biases of the function class allows contrastive learning to work with less stringent conditions compared to prior analyses.},
	urldate = {2022-03-02},
	journal = {arXiv:2202.14037 [cs]},
	author = {Saunshi, Nikunj and Ash, Jordan and Goel, Surbhi and Misra, Dipendra and Zhang, Cyril and Arora, Sanjeev and Kakade, Sham and Krishnamurthy, Akshay},
	month = feb,
	year = {2022},
	note = {arXiv: 2202.14037},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{caron_emerging_2021,
	title = {Emerging {Properties} in {Self}-{Supervised} {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2104.14294},
	abstract = {In this paper, we question if self-supervised learning provides new properties to Vision Transformer (ViT) that stand out compared to convolutional networks (convnets). Beyond the fact that adapting self-supervised methods to this architecture works particularly well, we make the following observations: first, self-supervised ViT features contain explicit information about the semantic segmentation of an image, which does not emerge as clearly with supervised ViTs, nor with convnets. Second, these features are also excellent k-NN classifiers, reaching 78.3\% top-1 on ImageNet with a small ViT. Our study also underlines the importance of momentum encoder, multi-crop training, and the use of small patches with ViTs. We implement our findings into a simple self-supervised method, called DINO, which we interpret as a form of self-distillation with no labels. We show the synergy between DINO and ViTs by achieving 80.1\% top-1 on ImageNet in linear evaluation with ViT-Base.},
	urldate = {2022-02-14},
	journal = {arXiv:2104.14294 [cs]},
	author = {Caron, Mathilde and Touvron, Hugo and Misra, Ishan and Jégou, Hervé and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
	month = may,
	year = {2021},
	note = {arXiv: 2104.14294},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{zbontar_barlow_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Barlow {Twins}: {Self}-{Supervised} {Learning} via {Redundancy} {Reduction}},
	volume = {139},
	shorttitle = {Barlow {Twins}},
	url = {http://proceedings.mlr.press/v139/zbontar21a.html},
	urldate = {2022-02-14},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}, {ICML} 2021, 18-24 {July} 2021, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Zbontar, Jure and Jing, Li and Misra, Ishan and LeCun, Yann and Deny, Stéphane},
	editor = {Meila, Marina and Zhang, Tong},
	year = {2021},
	pages = {12310--12320},
}

@inproceedings{misra_self-supervised_2020,
	title = {Self-{Supervised} {Learning} of {Pretext}-{Invariant} {Representations}},
	url = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Misra\_Self-Supervised\_Learning\_of\_Pretext-Invariant\_Representations\_CVPR\_2020\_paper.html},
	doi = {10.1109/CVPR42600.2020.00674},
	urldate = {2022-02-14},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2020, {Seattle}, {WA}, {USA}, {June} 13-19, 2020},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Misra, Ishan and Maaten, Laurens van der},
	year = {2020},
	pages = {6706--6716},
}

@inproceedings{yan_clusterfit_2020,
	title = {{ClusterFit}: {Improving} {Generalization} of {Visual} {Representations}},
	shorttitle = {{ClusterFit}},
	url = {https://openaccess.thecvf.com/content\_CVPR\_2020/html/Yan\_ClusterFit\_Improving\_Generalization\_of\_Visual\_Representations\_CVPR\_2020\_paper.html},
	doi = {10.1109/CVPR42600.2020.00654},
	urldate = {2022-02-14},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2020, {Seattle}, {WA}, {USA}, {June} 13-19, 2020},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Yan, Xueting and Misra, Ishan and Gupta, Abhinav and Ghadiyaram, Deepti and Mahajan, Dhruv},
	year = {2020},
	pages = {6508--6517},
}

@inproceedings{gidaris_unsupervised_2018,
	title = {Unsupervised {Representation} {Learning} by {Predicting} {Image} {Rotations}},
	url = {https://openreview.net/forum?id=S1v4N2l0-},
	urldate = {2022-02-14},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018, {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	year = {2018},
}

@inproceedings{noroozi_unsupervised_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Unsupervised {Learning} of {Visual} {Representations} by {Solving} {Jigsaw} {Puzzles}},
	volume = {9910},
	url = {https://doi.org/10.1007/978-3-319-46466-4\_5},
	doi = {10.1007/978-3-319-46466-4_5},
	urldate = {2022-02-14},
	booktitle = {Computer {Vision} - {ECCV} 2016 - 14th {European} {Conference}, {Amsterdam}, {The} {Netherlands}, {October} 11-14, 2016, {Proceedings}, {Part} {VI}},
	publisher = {Springer},
	author = {Noroozi, Mehdi and Favaro, Paolo},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {69--84},
}

@inproceedings{zhang_colorful_2016,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Colorful {Image} {Colorization}},
	volume = {9907},
	url = {https://doi.org/10.1007/978-3-319-46487-9\_40},
	doi = {10.1007/978-3-319-46487-9_40},
	urldate = {2022-02-14},
	booktitle = {Computer {Vision} - {ECCV} 2016 - 14th {European} {Conference}, {Amsterdam}, {The} {Netherlands}, {October} 11-14, 2016, {Proceedings}, {Part} {III}},
	publisher = {Springer},
	author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
	editor = {Leibe, Bastian and Matas, Jiri and Sebe, Nicu and Welling, Max},
	year = {2016},
	pages = {649--666},
}

@inproceedings{anonymous_fine-tuning_2021,
	title = {Fine-{Tuning} {Distorts} {Pretrained} {Features} and {Underperforms} {Out}-of-{Distribution}},
	url = {https://openreview.net/forum?id=UYneFzXSJWh},
	abstract = {When transferring a pretrained model to a downstream task, two popular methods are fine-tuning (updating all the model parameters) and linear probing (updating only the last linear layer). It is...},
	language = {en},
	urldate = {2022-01-18},
	author = {Anonymous},
	month = sep,
	year = {2021},
}

@article{xie_explanation_2021,
	title = {An {Explanation} of {In}-context {Learning} as {Implicit} {Bayesian} {Inference}},
	url = {http://arxiv.org/abs/2111.02080},
	abstract = {Large pretrained language models such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. Without being explicitly pretrained to do so, the language model learns from these examples during its forward pass without parameter updates on "out-of-distribution" prompts. Thus, it is unclear what mechanism enables in-context learning. In this paper, we study the role of the pretraining distribution on the emergence of in-context learning under a mathematical setting where the pretraining texts have long-range coherence. Here, language model pretraining requires inferring a latent document-level concept from the conditioning text to generate coherent next tokens. At test time, this mechanism enables in-context learning by inferring the shared latent concept between prompt examples and applying it to make a prediction on the test example. Concretely, we prove that in-context learning occurs implicitly via Bayesian inference of the latent concept when the pretraining distribution is a mixture of HMMs. This can occur despite the distribution mismatch between prompts and pretraining data. In contrast to messy large-scale pretraining datasets for in-context learning in natural language, we generate a family of small-scale synthetic datasets (GINC) where Transformer and LSTM language models both exhibit in-context learning. Beyond the theory which focuses on the effect of the pretraining distribution, we empirically find that scaling model size improves in-context accuracy even when the pretraining loss is the same.},
	urldate = {2022-01-18},
	journal = {arXiv:2111.02080 [cs]},
	author = {Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
	month = dec,
	year = {2021},
	note = {arXiv: 2111.02080},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{anonymous_how_2021,
	title = {How does {Contrastive} {Pre}-training {Connect} {Disparate} {Domains}?},
	url = {https://openreview.net/forum?id=vBn2OXZuQCF},
	abstract = {Pre-training on massive unlabeled datasets greatly improves accuracy under distribution shifts. As a first step toward understanding this, we study a popular pre-training method, contrastive...},
	language = {en},
	urldate = {2022-01-18},
	author = {Anonymous},
	month = sep,
	year = {2021},
}

@article{wei_why_2021,
	title = {Why {Do} {Pretrained} {Language} {Models} {Help} in {Downstream} {Tasks}? {An} {Analysis} of {Head} and {Prompt} {Tuning}},
	shorttitle = {Why {Do} {Pretrained} {Language} {Models} {Help} in {Downstream} {Tasks}?},
	url = {http://arxiv.org/abs/2106.09226},
	abstract = {Pretrained language models have achieved state-of-the-art performance when adapted to a downstream NLP task. However, theoretical analysis of these models is scarce and challenging since the pretraining and downstream tasks can be very different. We propose an analysis framework that links the pretraining and downstream tasks with an underlying latent variable generative model of text -- the downstream classifier must recover a function of the posterior distribution over the latent variables. We analyze head tuning (learning a classifier on top of the frozen pretrained model) and prompt tuning in this setting. The generative model in our analysis is either a Hidden Markov Model (HMM) or an HMM augmented with a latent memory component, motivated by long-term dependencies in natural language. We show that 1) under certain non-degeneracy conditions on the HMM, simple classification heads can solve the downstream task, 2) prompt tuning obtains downstream guarantees with weaker non-degeneracy conditions, and 3) our recovery guarantees for the memory-augmented HMM are stronger than for the vanilla HMM because task-relevant information is easier to recover from the long-term memory. Experiments on synthetically generated data from HMMs back our theoretical findings.},
	urldate = {2022-01-18},
	journal = {arXiv:2106.09226 [cs, stat]},
	author = {Wei, Colin and Xie, Sang Michael and Ma, Tengyu},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.09226},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xie_-n-out_2021,
	title = {In-{N}-{Out}: {Pre}-{Training} and {Self}-{Training} using {Auxiliary} {Information} for {Out}-of-{Distribution} {Robustness}},
	shorttitle = {In-{N}-{Out}},
	url = {http://arxiv.org/abs/2012.04550},
	abstract = {Consider a prediction setting with few in-distribution labeled examples and many unlabeled examples both in- and out-of-distribution (OOD). The goal is to learn a model which performs well both in-distribution and OOD. In these settings, auxiliary information is often cheaply available for every input. How should we best leverage this auxiliary information for the prediction task? Empirically across three image and time-series datasets, and theoretically in a multi-task linear regression setting, we show that (i) using auxiliary information as input features improves in-distribution error but can hurt OOD error; but (ii) using auxiliary information as outputs of auxiliary pre-training tasks improves OOD error. To get the best of both worlds, we introduce In-N-Out, which first trains a model with auxiliary inputs and uses it to pseudolabel all the in-distribution inputs, then pre-trains a model on OOD auxiliary outputs and fine-tunes this model with the pseudolabels (self-training). We show both theoretically and empirically that In-N-Out outperforms auxiliary inputs or outputs alone on both in-distribution and OOD error.},
	urldate = {2022-01-18},
	journal = {arXiv:2012.04550 [cs, stat]},
	author = {Xie, Sang Michael and Kumar, Ananya and Jones, Robbie and Khani, Fereshte and Ma, Tengyu and Liang, Percy},
	month = apr,
	year = {2021},
	note = {arXiv: 2012.04550},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{ross_first_2010,
	address = {Upper Saddle River, N.J},
	edition = {8th ed},
	title = {A first course in probability},
	isbn = {978-0-13-603313-4},
	language = {en},
	publisher = {Pearson Prentice Hall},
	author = {Ross, Sheldon M.},
	year = {2010},
	note = {OCLC: ocn237199460},
	keywords = {Probabilities, Textbooks},
}

@article{dhillon_baseline_2020,
	title = {A {Baseline} for {Few}-{Shot} {Image} {Classification}},
	url = {http://arxiv.org/abs/1909.02729},
	abstract = {Fine-tuning a deep network trained with the standard cross-entropy loss is a strong baseline for few-shot learning. When fine-tuned transductively, this outperforms the current state-of-the-art on standard datasets such as Mini-ImageNet, Tiered-ImageNet, CIFAR-FS and FC-100 with the same hyper-parameters. The simplicity of this approach enables us to demonstrate the first few-shot learning results on the ImageNet-21k dataset. We find that using a large number of meta-training classes results in high few-shot accuracies even for a large number of few-shot classes. We do not advocate our approach as the solution for few-shot learning, but simply use the results to highlight limitations of current benchmarks and few-shot protocols. We perform extensive studies on benchmark datasets to propose a metric that quantifies the "hardness" of a few-shot episode. This metric can be used to report the performance of few-shot algorithms in a more systematic way.},
	urldate = {2022-01-11},
	journal = {arXiv:1909.02729 [cs, stat]},
	author = {Dhillon, Guneet S. and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv: 1909.02729},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tian_rethinking_2020,
	title = {Rethinking {Few}-{Shot} {Image} {Classification}: a {Good} {Embedding} {Is} {All} {You} {Need}?},
	shorttitle = {Rethinking {Few}-{Shot} {Image} {Classification}},
	url = {http://arxiv.org/abs/2003.11539},
	abstract = {The focus of recent meta-learning research has been on the development of learning algorithms that can quickly adapt to test time tasks with limited data and low computational cost. Few-shot learning is widely used as one of the standard benchmarks in meta-learning. In this work, we show that a simple baseline: learning a supervised or self-supervised representation on the meta-training set, followed by training a linear classifier on top of this representation, outperforms state-of-the-art few-shot learning methods. An additional boost can be achieved through the use of self-distillation. This demonstrates that using a good learned embedding model can be more effective than sophisticated meta-learning algorithms. We believe that our findings motivate a rethinking of few-shot image classification benchmarks and the associated role of meta-learning algorithms. Code is available at: http://github.com/WangYueFt/rfs/.},
	urldate = {2022-01-11},
	journal = {arXiv:2003.11539 [cs]},
	author = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B. and Isola, Phillip},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.11539},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{galanti_role_2022,
	title = {On the {Role} of {Neural} {Collapse} in {Transfer} {Learning}},
	url = {http://arxiv.org/abs/2112.15121},
	abstract = {We study the ability of foundation models to learn representations for classification that are transferable to new, unseen classes. Recent results in the literature show that representations learned by a single classifier over many classes are competitive on few-shot learning problems with representations learned by special-purpose algorithms designed for such problems. In this paper we provide an explanation for this behavior based on the recently observed phenomenon that the features learned by overparameterized classification networks show an interesting clustering property, called neural collapse. We demonstrate both theoretically and empirically that neural collapse generalizes to new samples from the training classes, and -- more importantly -- to new classes as well, allowing foundation models to provide feature maps that work well in transfer learning and, specifically, in the few-shot setting.},
	urldate = {2022-01-11},
	journal = {arXiv:2112.15121 [cs]},
	author = {Galanti, Tomer and György, András and Hutter, Marcus},
	month = jan,
	year = {2022},
	note = {arXiv: 2112.15121},
	keywords = {Computer Science - Machine Learning},
}

@article{bartlett_nearly-tight_2017,
	title = {Nearly-tight {VC}-dimension and pseudodimension bounds for piecewise linear neural networks},
	url = {http://arxiv.org/abs/1703.02930},
	abstract = {We prove new upper and lower bounds on the VC-dimension of deep neural networks with the ReLU activation function. These bounds are tight for almost the entire range of parameters. Letting \$W\$ be the number of weights and \$L\$ be the number of layers, we prove that the VC-dimension is \$O(W L {\textbackslash}log(W))\$, and provide examples with VC-dimension \${\textbackslash}Omega( W L {\textbackslash}log(W/L) )\$. This improves both the previously known upper bounds and lower bounds. In terms of the number \$U\$ of non-linear units, we prove a tight bound \${\textbackslash}Theta(W U)\$ on the VC-dimension. All of these bounds generalize to arbitrary piecewise linear activation functions, and also hold for the pseudodimensions of these function classes. Combined with previous results, this gives an intriguing range of dependencies of the VC-dimension on depth for networks with different non-linearities: there is no dependence for piecewise-constant, linear dependence for piecewise-linear, and no more than quadratic dependence for general piecewise-polynomial.},
	urldate = {2022-01-08},
	journal = {arXiv:1703.02930 [cs]},
	author = {Bartlett, Peter L. and Harvey, Nick and Liaw, Chris and Mehrabian, Abbas},
	month = oct,
	year = {2017},
	note = {arXiv: 1703.02930},
	keywords = {Computer Science - Machine Learning},
}

@article{harutyunyan_estimating_2021,
	title = {Estimating informativeness of samples with {Smooth} {Unique} {Information}},
	url = {http://arxiv.org/abs/2101.06640},
	abstract = {We define a notion of information that an individual sample provides to the training of a neural network, and we specialize it to measure both how much a sample informs the final weights and how much it informs the function computed by the weights. Though related, we show that these quantities have a qualitatively different behavior. We give efficient approximations of these quantities using a linearized network and demonstrate empirically that the approximation is accurate for real-world architectures, such as pre-trained ResNets. We apply these measures to several problems, such as dataset summarization, analysis of under-sampled classes, comparison of informativeness of different data sources, and detection of adversarial and corrupted examples. Our work generalizes existing frameworks but enjoys better computational properties for heavily over-parametrized models, which makes it possible to apply it to real-world networks.},
	urldate = {2021-12-15},
	journal = {arXiv:2101.06640 [cs, stat]},
	author = {Harutyunyan, Hrayr and Achille, Alessandro and Paolini, Giovanni and Majumder, Orchid and Ravichandran, Avinash and Bhotika, Rahul and Soatto, Stefano},
	month = mar,
	year = {2021},
	note = {arXiv: 2101.06640},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{achille_where_2020,
	title = {Where is the {Information} in a {Deep} {Neural} {Network}?},
	url = {http://arxiv.org/abs/1905.12213},
	abstract = {Whatever information a deep neural network has gleaned from training data is encoded in its weights. How this information affects the response of the network to future data remains largely an open question. Indeed, even defining and measuring information entails some subtleties, since a trained network is a deterministic map, so standard information measures can be degenerate. We measure information in a neural network via the optimal trade-off between accuracy of the response and complexity of the weights, measured by their coding length. Depending on the choice of code, the definition can reduce to standard measures such as Shannon Mutual Information and Fisher Information. However, the more general definition allows us to relate information to generalization and invariance, through a novel notion of effective information in the activations of a deep network. We establish a novel relation between the information in the weights and the effective information in the activations, and use this result to show that models with low (information) complexity not only generalize better, but are bound to learn invariant representations of future inputs. These relations hinge not only on the architecture of the model, but also on how it is trained, highlighting the complex inter-dependency between the class of functions implemented by deep neural networks, the loss function used for training them from finite data, and the inductive bias implicit in the optimization.},
	urldate = {2021-12-15},
	journal = {arXiv:1905.12213 [cs, math, stat]},
	author = {Achille, Alessandro and Paolini, Giovanni and Soatto, Stefano},
	month = jun,
	year = {2020},
	note = {arXiv: 1905.12213},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_where_nodate,
	title = {‪{Where} is the information in a deep neural network?‬},
	shorttitle = {‪{Where} is the information in a deep neural network?},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LiwtZz4AAAAJ&cstart=20&pagesize=80&sortby=pubdate&citation_for_view=LiwtZz4AAAAJ:KlAtU1dfN6UC},
	abstract = {‪A Achille, G Paolini, S Soatto‬, ‪ArXiv Preprints, arXiv:1905.12213, 2019‬ - ‪Cited by 35‬},
	urldate = {2021-12-15},
}

@article{kearns_efficient_1994,
	title = {Efficient distribution-free learning of probabilistic concepts},
	volume = {48},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000005800625},
	doi = {10.1016/S0022-0000(05)80062-5},
	abstract = {In this paper we investigate a new formal model of machine learning in which the concept (Boolean function) to be learned may exhibit uncertain or probabilistic behavior—thus, the same input may sometimes be classified as a positive example and sometimes as a negative example. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. We adopt from the Valiant model of learining [28] the demands that learning algorithms be efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. In addition to giving many efficient algorithms for learning natural classes of p-concepts, we study and develop in detail an underlying theory of learning p-concepts.},
	language = {en},
	number = {3},
	urldate = {2021-12-15},
	journal = {Journal of Computer and System Sciences},
	author = {Kearns, Michael J. and Schapire, Robert E.},
	month = jun,
	year = {1994},
	pages = {464--497},
}

@article{topsoe_information-theoretical_1979,
	title = {Information-theoretical optimization techniques},
	volume = {15},
	number = {1},
	journal = {Kybernetika},
	author = {Topsøe, Flemming},
	year = {1979},
	note = {Publisher: Institute of Information Theory and Automation AS CR},
	pages = {8--27},
}

@article{xu_continuity_2020,
	title = {Continuity of {Generalized} {Entropy} and {Statistical} {Learning}},
	url = {http://arxiv.org/abs/2012.15829},
	abstract = {We study the continuity property of the generalized entropy as a functional of the underlying probability distribution, defined with an action space and a loss function, and use this property to answer the basic questions in statistical learning theory, the excess risk analyses for various learning methods. We first derive upper and lower bounds for the entropy difference of two distributions in terms of several commonly used \$f\$-divergences, the Wasserstein distance, and a distance that depends on the action space and the loss function. Examples are given along with the discussion of each general result, comparisons are made with the existing entropy difference bounds, and new mutual information upper bounds are derived based on the new results. We then apply the entropy difference bounds to the theory of statistical learning. It is shown that the excess risks in the two popular learning paradigms, the frequentist learning and the Bayesian learning, both can be studied with the continuity property of different forms of the generalized entropy. The analysis is then extended to the continuity of generalized conditional entropy. The extension provides performance bounds for Bayes decision making with mismatched distributions. It also leads to excess risk bounds for a third paradigm of learning, where the decision rule is optimally designed under the projection of the empirical distribution to a predefined family of distributions. We thus establish a unified method of excess risk analysis for the three major paradigms of statistical learning, through the continuity of generalized entropy.},
	urldate = {2021-12-13},
	journal = {arXiv:2012.15829 [cs, math, stat]},
	author = {Xu, Aolin},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.15829},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@article{halpern_decision_2013,
	title = {Decision {Theory} with {Resource}-{Bounded} {Agents}},
	url = {http://arxiv.org/abs/1308.3780},
	abstract = {There have been two major lines of research aimed at capturing resource-bounded players in game theory. The first, initiated by Rubinstein, charges an agent for doing costly computation; the second, initiated by Neyman, does not charge for computation, but limits the computation that agents can do, typically by modeling agents as finite automata. We review recent work on applying both approaches in the context of decision theory. For the first approach, we take the objects of choice in a decision problem to be Turing machines, and charge players for the ``complexity'' of the Turing machine chosen (e.g., its running time). This approach can be used to explain well-known phenomena like first-impression-matters biases (i.e., people tend to put more weight on evidence they hear early on) and belief polarization (two people with different prior beliefs, hearing the same evidence, can end up with diametrically opposed conclusions) as the outcomes of quite rational decisions. For the second approach, we model people as finite automata, and provide a simple algorithm that, on a problem that captures a number of settings of interest, provably performs optimally as the number of states in the automaton increases.},
	urldate = {2021-12-13},
	journal = {arXiv:1308.3780 [cs]},
	author = {Halpern, Joseph Y. and Pass, Rafael and Seeman, Lior},
	month = aug,
	year = {2013},
	note = {arXiv: 1308.3780},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory},
}

@article{seeman_im_2015,
	title = {I'{M} {Only} {Human}: {Game} {Theory} {With} {Computationally} {Bounded} {Agents}},
	author = {Seeman, Lior},
	year = {2015},
}

@article{simon_behavioral_1955,
	title = {A {Behavioral} {Model} of {Rational} {Choice}},
	volume = {69},
	issn = {00335533},
	url = {https://academic.oup.com/qje/article-lookup/doi/10.2307/1884852},
	doi = {10.2307/1884852},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {The Quarterly Journal of Economics},
	author = {Simon, Herbert A.},
	month = feb,
	year = {1955},
	pages = {99},
}

@article{sandhlom_coalitions_1997,
	title = {Coalitions among computationally bounded agents},
	volume = {94},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370297000301},
	doi = {10.1016/S0004-3702(97)00030-1},
	language = {en},
	number = {1-2},
	urldate = {2021-12-13},
	journal = {Artificial Intelligence},
	author = {Sandhlom, Tuomas W. and Lesser, Victor R.T},
	month = jul,
	year = {1997},
	pages = {99--137},
}

@article{simon_behavioral_1955-1,
	title = {A {Behavioral} {Model} of {Rational} {Choice}},
	volume = {69},
	issn = {00335533},
	url = {https://academic.oup.com/qje/article-lookup/doi/10.2307/1884852},
	doi = {10.2307/1884852},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {The Quarterly Journal of Economics},
	author = {Simon, Herbert A.},
	month = feb,
	year = {1955},
	pages = {99},
}

@book{degroot_optimal_2004,
	edition = {1},
	title = {Optimal {Statistical} {Decisions}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/0471729000},
	urldate = {2021-12-13},
	publisher = {John Wiley \& Sons, Ltd},
	author = {DeGroot, Morris H.},
	year = {2004},
	doi = {10.1002/0471729000},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471729000},
}

@article{neiswanger_h-entropy_2021,
	title = {H-{Entropy} {Search}: {Generalizing} {Bayesian} {Optimization} with a {Decision}-theoretic {Uncertainty} {Measure}},
	shorttitle = {H-{Entropy} {Search}},
	url = {https://openreview.net/forum?id=coQhmtxr5SN},
	abstract = {Bayesian optimization (BO) is a popular method for efficiently inferring optima of an expensive black-box function via a sequence of queries. Existing information-theoretic BO procedures aim to...},
	language = {en},
	urldate = {2021-12-13},
	author = {Neiswanger, Willie and Yu, Lantao and Zhao, Shengjia and Meng, Chenlin and Ermon, Stefano},
	month = sep,
	year = {2021},
}

@article{zhao_h-divergence_2020,
	title = {H-divergence: {A} {Decision}-{Theoretic} {Probability} {Discrepancy} {Measure}},
	shorttitle = {H-divergence},
	url = {https://openreview.net/forum?id=uBHs6zpY4in},
	abstract = {Measuring the discrepancy between two probability distributions is a fundamental problem in machine learning and statistics. Based on ideas from decision theory, we investigate a new class of...},
	language = {en},
	urldate = {2021-12-11},
	author = {Zhao, Shengjia and Sinha, Abhishek and He, Yutong and Perreault, Aidan and Song, Jiaming and Ermon, Stefano},
	month = sep,
	year = {2020},
}

@article{vidakovic_r-minimax_nodate,
	title = {r-{Minimax}: {A} {Paradigm} for {Conservative} {Robust} {Bayesians}},
	abstract = {In this chapter a tutorial overview of Gamma minimaxity (f-minimaxity) is provided. One of the assumptions of the robust Bayesian analysis is that prior distributions can seldom be quantified or elicited exactly. Instead, a family of priors, f, reflecting prior beliefs is elicited. The f-minimax decision-theoretic approach to statistical inference favors an action/rule which incorporates information specified via f and guards against the least favorable prior in f. This paradigm falls between Bayesian and minimax paradigms; it coincides with the former when prior information can be summarized in a single prior and with the latter when no prior information is available (or equivalently, possible priors belong to the class of all distributions).},
	language = {en},
	author = {Vidakovic, Brani},
	pages = {19},
}

@article{shwartz-ziv_opening_2017,
	title = {Opening the {Black} {Box} of {Deep} {Neural} {Networks} via {Information}},
	volume = {abs/1703.00810},
	url = {http://arxiv.org/abs/1703.00810},
	journal = {CoRR},
	author = {Shwartz-Ziv, Ravid and Tishby, Naftali},
	year = {2017},
	note = {arXiv: 1703.00810},
}

@article{scholkopf_towards_2021,
	title = {Towards {Causal} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.11107},
	abstract = {The two fields of machine learning and graphical causality arose and developed separately. However, there is now cross-pollination and increasing interest in both fields to benefit from the advances of the other. In the present paper, we review fundamental concepts of causal inference and relate them to crucial open problems of machine learning, including transfer and generalization, thereby assaying how causality can contribute to modern machine learning research. This also applies in the opposite direction: we note that most work in causality starts from the premise that the causal variables are given. A central problem for AI and causality is, thus, causal representation learning, the discovery of high-level causal variables from low-level observations. Finally, we delineate some implications of causality for machine learning and propose key research areas at the intersection of both communities.},
	urldate = {2021-12-05},
	journal = {arXiv:2102.11107 [cs]},
	author = {Schölkopf, Bernhard and Locatello, Francesco and Bauer, Stefan and Ke, Nan Rosemary and Kalchbrenner, Nal and Goyal, Anirudh and Bengio, Yoshua},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.11107},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{zhao_individual_2020,
	title = {Individual {Calibration} with {Randomized} {Forecasting}},
	url = {http://arxiv.org/abs/2006.10288},
	abstract = {Machine learning applications often require calibrated predictions, e.g. a 90{\textbackslash}\% credible interval should contain the true outcome 90{\textbackslash}\% of the times. However, typical definitions of calibration only require this to hold on average, and offer no guarantees on predictions made on individual samples. Thus, predictions can be systematically over or under confident on certain subgroups, leading to issues of fairness and potential vulnerabilities. We show that calibration for individual samples is possible in the regression setup if the predictions are randomized, i.e. outputting randomized credible intervals. Randomization removes systematic bias by trading off bias with variance. We design a training objective to enforce individual calibration and use it to train randomized regression functions. The resulting models are more calibrated for arbitrarily chosen subgroups of the data, and can achieve higher utility in decision making against adversaries that exploit miscalibrated predictions.},
	urldate = {2021-11-29},
	journal = {arXiv:2006.10288 [cs, stat]},
	author = {Zhao, Shengjia and Ma, Tengyu and Ermon, Stefano},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.10288},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhao_right_2021,
	title = {Right {Decisions} from {Wrong} {Predictions}: {A} {Mechanism} {Design} {Alternative} to {Individual} {Calibration}},
	shorttitle = {Right {Decisions} from {Wrong} {Predictions}},
	url = {http://arxiv.org/abs/2011.07476},
	abstract = {Decision makers often need to rely on imperfect probabilistic forecasts. While average performance metrics are typically available, it is difficult to assess the quality of individual forecasts and the corresponding utilities. To convey confidence about individual predictions to decision-makers, we propose a compensation mechanism ensuring that the forecasted utility matches the actually accrued utility. While a naive scheme to compensate decision-makers for prediction errors can be exploited and might not be sustainable in the long run, we propose a mechanism based on fair bets and online learning that provably cannot be exploited. We demonstrate an application showing how passengers could confidently optimize individual travel plans based on flight delay probabilities estimated by an airline.},
	urldate = {2021-11-29},
	journal = {arXiv:2011.07476 [cs, math, stat]},
	author = {Zhao, Shengjia and Ermon, Stefano},
	month = mar,
	year = {2021},
	note = {arXiv: 2011.07476},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Mathematics - Probability, Statistics - Applications, Statistics - Machine Learning},
}

@article{mazuelas_generalized_2021,
	title = {Generalized {Maximum} {Entropy} for {Supervised} {Classification}},
	url = {http://arxiv.org/abs/2007.05447},
	abstract = {The maximum entropy principle advocates to evaluate events' probabilities using a distribution that maximizes entropy among those that satisfy certain expectations' constraints. Such principle can be generalized for arbitrary decision problems where it corresponds to minimax approaches. This paper establishes a framework for supervised classification based on the generalized maximum entropy principle that leads to minimax risk classifiers (MRCs). We develop learning techniques that determine MRCs for general entropy functions and provide performance guarantees by means of convex optimization. In addition, we describe the relationship of the presented techniques with existing classification methods, and quantify MRCs performance in comparison with the proposed bounds and conventional methods.},
	urldate = {2021-11-29},
	journal = {arXiv:2007.05447 [cs, stat]},
	author = {Mazuelas, Santiago and Shen, Yuan and Pérez, Aritz},
	month = aug,
	year = {2021},
	note = {arXiv: 2007.05447},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wu_lecture_nodate,
	title = {Lecture notes on: {Information}-theoretic methods for high-dimensional statistics},
	language = {en},
	author = {Wu, Yihong},
	pages = {162},
}

@article{shui_beyond_2020,
	title = {Beyond \${\textbackslash}mathcal\{{H}\}\$-{Divergence}: {Domain} {Adaptation} {Theory} {With} {Jensen}-{Shannon} {Divergence}},
	shorttitle = {Beyond \${\textbackslash}mathcal\{{H}\}\$-{Divergence}},
	url = {http://arxiv.org/abs/2007.15567},
	abstract = {We reveal the incoherence between the widely-adopted empirical domain adversarial training and its generally-assumed theoretical counterpart based on \${\textbackslash}mathcal\{H\}\$-divergence. Concretely, we find that \${\textbackslash}mathcal\{H\}\$-divergence is not equivalent to Jensen-Shannon divergence, the optimization objective in domain adversarial training. To this end, we establish a new theoretical framework by directly proving the upper and lower target risk bounds based on joint distributional Jensen-Shannon divergence. We further derive bi-directional upper bounds for marginal and conditional shifts. Our framework exhibits inherent flexibilities for different transfer learning problems, which is usable for various scenarios where \${\textbackslash}mathcal\{H\}\$-divergence-based theory fails to adapt. From an algorithmic perspective, our theory enables a generic guideline unifying principles of semantic conditional matching, feature marginal matching, and label marginal shift correction. We employ algorithms for each principle and empirically validate the benefits of our framework on real datasets.},
	urldate = {2021-11-24},
	journal = {arXiv:2007.15567 [cs, stat]},
	author = {Shui, Changjian and Chen, Qi and Wen, Jun and Zhou, Fan and Gagné, Christian and Wang, Boyu},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.15567},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{blitzer_learning_2008,
	title = {Learning {Bounds} for {Domain} {Adaptation}},
	volume = {20},
	url = {https://papers.nips.cc/paper/2007/hash/42e77b63637ab381e8be5f8318cc28a2-Abstract.html},
	urldate = {2021-11-24},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Wortman, Jennifer},
	year = {2008},
}

@article{mansour_domain_2009,
	title = {Domain {Adaptation}: {Learning} {Bounds} and {Algorithms}},
	shorttitle = {Domain {Adaptation}},
	url = {http://arxiv.org/abs/0902.3430},
	abstract = {This paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data. Building on previous work by Ben-David et al. (2007), we introduce a novel distance between distributions, discrepancy distance, that is tailored to adaptation problems with arbitrary loss functions. We give Rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions. Using this distance, we derive novel generalization bounds for domain adaptation for a wide family of loss functions. We also present a series of novel adaptation bounds for large classes of regularization-based algorithms, including support vector machines and kernel ridge regression based on the empirical discrepancy. This motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give novel algorithms. We report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation.},
	urldate = {2021-11-24},
	journal = {arXiv:0902.3430 [cs]},
	author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
	month = feb,
	year = {2009},
	note = {arXiv: 0902.3430},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{dwork_outcome_2020,
	title = {Outcome {Indistinguishability}},
	url = {http://arxiv.org/abs/2011.13426},
	abstract = {Prediction algorithms assign numbers to individuals that are popularly understood as individual "probabilities" -- what is the probability of 5-year survival after cancer diagnosis? -- and which increasingly form the basis for life-altering decisions. Drawing on an understanding of computational indistinguishability developed in complexity theory and cryptography, we introduce Outcome Indistinguishability. Predictors that are Outcome Indistinguishable yield a generative model for outcomes that cannot be efficiently refuted on the basis of the real-life observations produced by Nature. We investigate a hierarchy of Outcome Indistinguishability definitions, whose stringency increases with the degree to which distinguishers may access the predictor in question. Our findings reveal that Outcome Indistinguishability behaves qualitatively differently than previously studied notions of indistinguishability. First, we provide constructions at all levels of the hierarchy. Then, leveraging recently-developed machinery for proving average-case fine-grained hardness, we obtain lower bounds on the complexity of the more stringent forms of Outcome Indistinguishability. This hardness result provides the first scientific grounds for the political argument that, when inspecting algorithmic risk prediction instruments, auditors should be granted oracle access to the algorithm, not simply historical predictions.},
	urldate = {2021-11-24},
	journal = {arXiv:2011.13426 [cs]},
	author = {Dwork, Cynthia and Kim, Michael P. and Reingold, Omer and Rothblum, Guy N. and Yona, Gal},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.13426},
	keywords = {Computer Science - Computational Complexity, Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning},
}

@article{levine_depth--width_2021,
	title = {The {Depth}-to-{Width} {Interplay} in {Self}-{Attention}},
	url = {http://arxiv.org/abs/2006.12467},
	abstract = {Self-attention architectures, which are rapidly pushing the frontier in natural language processing, demonstrate a surprising depth-inefficient behavior: previous works indicate that increasing the internal representation (network width) is just as useful as increasing the number of self-attention layers (network depth). We theoretically predict a width-dependent transition between depth-efficiency and depth-inefficiency in self-attention. We conduct systematic empirical ablations on networks of depths 6 to 48 that clearly reveal the theoretically predicted behaviors, and provide explicit quantitative suggestions regarding the optimal depth-to-width allocation for a given self-attention network size. The race towards beyond 1-Trillion parameter language models renders informed guidelines for increasing self-attention depth and width in tandem an essential ingredient. Our guidelines elucidate the depth-to-width trade-off in self-attention networks of sizes up to the scale of GPT3 (which we project to be too deep for its size), and beyond, marking an unprecedented width of 30K as optimal for a 1-Trillion parameter network.},
	urldate = {2021-11-19},
	journal = {arXiv:2006.12467 [cs, stat]},
	author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
	month = jan,
	year = {2021},
	note = {arXiv: 2006.12467},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{touati_learning_2021,
	title = {Learning {One} {Representation} to {Optimize} {All} {Rewards}},
	url = {http://arxiv.org/abs/2103.07945},
	abstract = {We introduce the forward-backward (FB) representation of the dynamics of a reward-free Markov decision process. It provides explicit near-optimal policies for any reward specified a posteriori. During an unsupervised phase, we use reward-free interactions with the environment to learn two representations via off-the-shelf deep learning methods and temporal difference (TD) learning. In the test phase, a reward representation is estimated either from observations or an explicit reward description (e.g., a target state). The optimal policy for that reward is directly obtained from these representations, with no planning. We assume access to an exploration scheme or replay buffer for the first phase. The corresponding unsupervised loss is well-principled: if training is perfect, the policies obtained are provably optimal for any reward function. With imperfect training, the sub-optimality is proportional to the unsupervised approximation error. The FB representation learns long-range relationships between states and actions, via a predictive occupancy map, without having to synthesize states as in model-based approaches. This is a step towards learning controllable agents in arbitrary black-box stochastic environments. This approach compares well to goal-oriented RL algorithms on discrete and continuous mazes, pixel-based MsPacman, and the FetchReach virtual robot arm. We also illustrate how the agent can immediately adapt to new tasks beyond goal-oriented RL.},
	urldate = {2021-11-18},
	journal = {arXiv:2103.07945 [cs, math]},
	author = {Touati, Ahmed and Ollivier, Yann},
	month = oct,
	year = {2021},
	note = {arXiv: 2103.07945},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@article{xu_learning_2021,
	title = {Learning {Representations} that {Support} {Robust} {Transfer} of {Predictors}},
	url = {http://arxiv.org/abs/2110.09940},
	abstract = {Ensuring generalization to unseen environments remains a challenge. Domain shift can lead to substantially degraded performance unless shifts are well-exercised within the available training environments. We introduce a simple robust estimation criterion -- transfer risk -- that is specifically geared towards optimizing transfer to new environments. Effectively, the criterion amounts to finding a representation that minimizes the risk of applying any optimal predictor trained on one environment to another. The transfer risk essentially decomposes into two terms, a direct transfer term and a weighted gradient-matching term arising from the optimality of per-environment predictors. Although inspired by IRM, we show that transfer risk serves as a better out-of-distribution generalization criterion, both theoretically and empirically. We further demonstrate the impact of optimizing such transfer risk on two controlled settings, each representing a different pattern of environment shift, as well as on two real-world datasets. Experimentally, the approach outperforms baselines across various out-of-distribution generalization tasks. Code is available at {\textbackslash}url\{https://github.com/Newbeeer/TRM\}.},
	urldate = {2021-11-17},
	journal = {arXiv:2110.09940 [cs]},
	author = {Xu, Yilun and Jaakkola, Tommi},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09940},
	keywords = {Computer Science - Machine Learning},
}

@article{haitner_inaccessible_2021,
	title = {Inaccessible {Entropy} {I}: {Inaccessible} {Entropy} {Generators} and {Statistically} {Hiding} {Commitments} from {One}-{Way} {Functions}},
	shorttitle = {Inaccessible {Entropy} {I}},
	url = {http://arxiv.org/abs/2010.05586},
	abstract = {We put forth a new computational notion of entropy, measuring the (in)feasibility of sampling high-entropy strings that are consistent with a given generator. Specifically, the i'th output block of a generator G has accessible entropy at most k if the following holds: when conditioning on its prior coin tosses, no polynomial-time strategy \${\textbackslash}widetilde\{G\}\$ can generate valid output for G's i'th output block with entropy greater than k. A generator has inaccessible entropy if the total accessible entropy (summed over the blocks) is noticeably smaller than the real entropy of G's output. As an application of the above notion, we improve upon the result of Haitner, Nguyen, Ong, Reingold, and Vadhan [Sicomp '09], presenting a much simpler and more efficient construction of statistically hiding commitment schemes from arbitrary one-way functions.},
	urldate = {2021-10-28},
	journal = {arXiv:2010.05586 [cs]},
	author = {Haitner, Iftach and Reingold, Omer and Vadhan, Salil and Wee, Hoeteck},
	month = aug,
	year = {2021},
	note = {arXiv: 2010.05586},
	keywords = {Computer Science - Cryptography and Security},
}

@incollection{goos_computational_2003,
	address = {Berlin, Heidelberg},
	title = {Computational {Analogues} of {Entropy}},
	volume = {2764},
	isbn = {978-3-540-40770-6 978-3-540-45198-3},
	url = {http://link.springer.com/10.1007/978-3-540-45198-3_18},
	abstract = {Min-entropy is a statistical measure of the amount of randomness that a particular distribution contains. In this paper we investigate the notion of computational min-entropy which is the computational analog of statistical min-entropy. We consider three possible deﬁnitions for this notion, and show equivalence and separation results for these deﬁnitions in various computational models.},
	language = {en},
	urldate = {2021-10-28},
	booktitle = {Approximation, {Randomization}, and {Combinatorial} {Optimization}.. {Algorithms} and {Techniques}},
	publisher = {Springer Berlin Heidelberg},
	author = {Barak, Boaz and Shaltiel, Ronen and Wigderson, Avi},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Arora, Sanjeev and Jansen, Klaus and Rolim, José D. P. and Sahai, Amit},
	year = {2003},
	doi = {10.1007/978-3-540-45198-3_18},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {200--215},
}

@incollection{boldyreva_unifying_2019,
	address = {Cham},
	title = {Unifying {Computational} {Entropies} via {Kullback}–{Leibler} {Divergence}},
	volume = {11693},
	isbn = {978-3-030-26950-0 978-3-030-26951-7},
	url = {http://link.springer.com/10.1007/978-3-030-26951-7_28},
	abstract = {We introduce KL-hardness, a new notion of hardness for search problems which on the one hand is satisﬁed by all one-way functions and on the other hand implies both next-block pseudoentropy and inaccessible-entropy, two forms of computational entropy used in recent constructions of pseudorandom generators and statistically hiding commitment schemes, respectively. Thus, KL-hardness uniﬁes the latter two notions of computational entropy and sheds light on the apparent “duality” between them. Additionally, it yields a more modular and illuminating proof that one-way functions imply next-block inaccessible entropy, similar in structure to the proof that one-way functions imply next-block pseudoentropy (Vadhan and Zheng, STOC ‘12).},
	language = {en},
	urldate = {2021-10-28},
	booktitle = {Advances in {Cryptology} – {CRYPTO} 2019},
	publisher = {Springer International Publishing},
	author = {Agrawal, Rohit and Chen, Yi-Hsiu and Horel, Thibaut and Vadhan, Salil},
	editor = {Boldyreva, Alexandra and Micciancio, Daniele},
	year = {2019},
	doi = {10.1007/978-3-030-26951-7_28},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {831--858},
}

@article{chen_simpler_2021,
	title = {Simpler, {Faster}, {Stronger}: {Breaking} {The} log-{K} {Curse} {On} {Contrastive} {Learners} {With} {FlatNCE}},
	shorttitle = {Simpler, {Faster}, {Stronger}},
	url = {http://arxiv.org/abs/2107.01152},
	abstract = {InfoNCE-based contrastive representation learners, such as SimCLR, have been tremendously successful in recent years. However, these contrastive schemes are notoriously resource demanding, as their effectiveness breaks down with small-batch training (i.e., the log-K curse, whereas K is the batch-size). In this work, we reveal mathematically why contrastive learners fail in the small-batch-size regime, and present a novel simple, non-trivial contrastive objective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no longer explicitly appeals to a discriminative classification goal for contrastive learning. Theoretically, we show FlatNCE is the mathematical dual formulation of InfoNCE, thus bridging the classical literature on energy modeling; and empirically, we demonstrate that, with minimal modification of code, FlatNCE enables immediate performance boost independent of the subject-matter engineering efforts. The significance of this work is furthered by the powerful generalization of contrastive learning techniques, and the introduction of new tools to monitor and diagnose contrastive training. We substantiate our claims with empirical evidence on CIFAR10, ImageNet, and other datasets, where FlatNCE consistently outperforms InfoNCE.},
	urldate = {2021-10-27},
	journal = {arXiv:2107.01152 [cs, math, stat]},
	author = {Chen, Junya and Gan, Zhe and Li, Xuan and Guo, Qing and Chen, Liqun and Gao, Shuyang and Chung, Tagyoung and Xu, Yi and Zeng, Belinda and Lu, Wenlian and Li, Fan and Carin, Lawrence and Tao, Chenyang},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.01152},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{nozawa_understanding_2021,
	title = {Understanding {Negative} {Samples} in {Instance} {Discriminative} {Self}-supervised {Representation} {Learning}},
	url = {http://arxiv.org/abs/2102.06866},
	abstract = {Instance discriminative self-supervised representation learning has been attracted attention thanks to its unsupervised nature and informative feature representation for downstream tasks. In practice, it commonly uses a larger number of negative samples than the number of supervised classes. However, there is an inconsistency in the existing analysis; theoretically, a large number of negative samples degrade classification performance on a downstream supervised task, while empirically, they improve the performance. We provide a novel framework to analyze this empirical result regarding negative samples using the coupon collector's problem. Our bound can implicitly incorporate the supervised loss of the downstream task in the self-supervised loss by increasing the number of negative samples. We confirm that our proposed analysis holds on real-world benchmark datasets.},
	urldate = {2021-10-27},
	journal = {arXiv:2102.06866 [cs, stat]},
	author = {Nozawa, Kento and Sato, Issei},
	month = oct,
	year = {2021},
	note = {arXiv: 2102.06866},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bao_sharp_2021,
	title = {Sharp {Learning} {Bounds} for {Contrastive} {Unsupervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2110.02501},
	abstract = {Contrastive unsupervised representation learning (CURL) encourages data representation to make semantically similar pairs closer than randomly drawn negative samples, which has been successful in various domains such as vision, language, and graphs. Although recent theoretical studies have attempted to explain its success by upper bounds of a downstream classification loss by the contrastive loss, they are still not sharp enough to explain an experimental fact: larger negative samples improve the classification performance. This study establishes a downstream classification loss bound with a tight intercept in the negative sample size. By regarding the contrastive loss as a downstream loss estimator, our theory not only improves the existing learning bounds substantially but also explains why downstream classification empirically improves with larger negative samples -- because the estimation variance of the downstream loss decays with larger negative samples. We verify that our theory is consistent with experiments on synthetic, vision, and language datasets.},
	urldate = {2021-10-27},
	journal = {arXiv:2110.02501 [cs]},
	author = {Bao, Han and Nagano, Yoshihiro and Nozawa, Kento},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.02501},
	keywords = {Computer Science - Machine Learning},
}

@article{austin_exchangeable_2015,
	title = {Exchangeable random measures},
	volume = {51},
	issn = {0246-0203},
	url = {http://arxiv.org/abs/1302.2116},
	doi = {10.1214/13-AIHP584},
	abstract = {Let A be a standard Borel space, and consider the space A{\textasciicircum}\{{\textbackslash}bbN{\textasciicircum}\{(k)\}\} of A-valued arrays indexed by all size-k subsets of {\textbackslash}bbN. This paper concerns random measures on such a space whose laws are invariant under the natural action of permutations of {\textbackslash}bbN. The main result is a representation theorem for such `exchangeable' random measures, obtained using the classical representation theorems for exchangeable arrays due to de Finetti, Hoover, Aldous and Kallenberg. After proving this representation, two applications of exchangeable random measures are given. The first is a short new proof of the Dovbysh-Sudakov Representation Theorem for exchangeable PSD matrices. The second is in the formulation of a natural class of limit objects for dilute mean-field spin glass models, retaining more information than just the limiting Gram-de Finetti matrix used in the study of the Sherrington-Kirkpatrick model.},
	number = {3},
	urldate = {2021-10-22},
	journal = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
	author = {Austin, Tim},
	month = aug,
	year = {2015},
	note = {arXiv: 1302.2116},
	keywords = {60G09 (primary), 60G57, 82B44, Mathematics - Probability},
}

@article{jing_understanding_2021,
	title = {Understanding {Dimensional} {Collapse} in {Contrastive} {Self}-supervised {Learning}},
	url = {http://arxiv.org/abs/2110.09348},
	abstract = {Self-supervised visual representation learning aims to learn useful representations without relying on human annotations. Joint embedding approach bases on maximizing the agreement between embedding vectors from different views of the same image. Various methods have been proposed to solve the collapsing problem where all embedding vectors collapse to a trivial constant solution. Among these methods, contrastive learning prevents collapse via negative sample pairs. It has been shown that non-contrastive methods suffer from a lesser collapse problem of a different nature: dimensional collapse, whereby the embedding vectors end up spanning a lower-dimensional subspace instead of the entire available embedding space. Here, we show that dimensional collapse also happens in contrastive learning. In this paper, we shed light on the dynamics at play in contrastive learning that leads to dimensional collapse. Inspired by our theory, we propose a novel contrastive learning method, called DirectCLR, which directly optimizes the representation space without relying on a trainable projector. Experiments show that DirectCLR outperforms SimCLR with a trainable linear projector on ImageNet.},
	urldate = {2021-10-19},
	journal = {arXiv:2110.09348 [cs]},
	author = {Jing, Li and Vincent, Pascal and LeCun, Yann and Tian, Yuandong},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.09348},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{hua_feature_2021,
	title = {On {Feature} {Decorrelation} in {Self}-{Supervised} {Learning}},
	url = {http://arxiv.org/abs/2105.00470},
	abstract = {In self-supervised representation learning, a common idea behind most of the state-of-the-art approaches is to enforce the robustness of the representations to predefined augmentations. A potential issue of this idea is the existence of completely collapsed solutions (i.e., constant features), which are typically avoided implicitly by carefully chosen implementation details. In this work, we study a relatively concise framework containing the most common components from recent approaches. We verify the existence of complete collapse and discover another reachable collapse pattern that is usually overlooked, namely dimensional collapse. We connect dimensional collapse with strong correlations between axes and consider such connection as a strong motivation for feature decorrelation (i.e., standardizing the covariance matrix). The gains from feature decorrelation are verified empirically to highlight the importance and the potential of this insight.},
	urldate = {2021-10-19},
	journal = {arXiv:2105.00470 [cs, stat]},
	author = {Hua, Tianyu and Wang, Wenxiao and Xue, Zihui and Ren, Sucheng and Wang, Yue and Zhao, Hang},
	month = aug,
	year = {2021},
	note = {arXiv: 2105.00470},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dawid_statistical_2015,
	title = {Statistical {Causality} from a {Decision}-{Theoretic} {Perspective}},
	volume = {2},
	issn = {2326-8298, 2326-831X},
	url = {http://arxiv.org/abs/1405.2292},
	doi = {10.1146/annurev-statistics-010814-020105},
	abstract = {We present an overview of the decision-theoretic framework of statistical causality, which is well-suited for formulating and solving problems of determining the effects of applied causes. The approach is described in detail, and is related to and contrasted with other current formulations, such as structural equation models and potential responses. Topics and applications covered include confounding, the effect of treatment on the treated, instrumental variables, and dynamic treatment strategies.},
	number = {1},
	urldate = {2021-10-18},
	journal = {Annual Review of Statistics and Its Application},
	author = {Dawid, A. Philip},
	month = apr,
	year = {2015},
	note = {arXiv: 1405.2292},
	keywords = {62A99, Mathematics - Statistics Theory, Statistics - Methodology},
	pages = {273--303},
}

@article{dawid_decision-theoretic_2021,
	title = {Decision-theoretic foundations for statistical causality},
	volume = {9},
	issn = {2193-3685},
	url = {https://www.degruyter.com/document/doi/10.1515/jci-2020-0008/html},
	doi = {10.1515/jci-2020-0008},
	abstract = {We develop a mathematical and interpretative foundation for the enterprise of decision-theoretic (DT) statistical causality, which is a straightforward way of representing and addressing causal questions. DT reframes causal inference as “assisted decision-making” and aims to understand when, and how, I can make use of external data, typically observational, to help me solve a decision problem by taking advantage of assumed relationships between the data and my problem. The relationships embodied in any representation of a causal problem require deeper justification, which is necessarily context-dependent. Here we clarify the considerations needed to support applications of the DT methodology. Exchangeability considerations are used to structure the required relationships, and a distinction drawn between intention to treat and intervention to treat forms the basis for the enabling condition of “ignorability.” We also show how the DT perspective unifies and sheds light on other popular formalisations of statistical causality, including potential responses and directed acyclic graphs.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Journal of Causal Inference},
	author = {Dawid, Philip},
	month = jan,
	year = {2021},
	note = {Publisher: De Gruyter},
	keywords = {directed acyclic graph, exchangeability, extended conditional independence, ignorability, potential outcome, single-world intervention graph},
	pages = {39--77},
}

@article{farrell_capacity_2021,
	title = {Capacity of {Group}-invariant {Linear} {Readouts} from {Equivariant} {Representations}: {How} {Many} {Objects} can be {Linearly} {Classified} {Under} {All} {Possible} {Views}?},
	shorttitle = {Capacity of {Group}-invariant {Linear} {Readouts} from {Equivariant} {Representations}},
	url = {http://arxiv.org/abs/2110.07472},
	abstract = {Equivariance has emerged as a desirable property of representations of objects subject to identity-preserving transformations that constitute a group, such as translations and rotations. However, the expressivity of a representation constrained by group equivariance is still not fully understood. We address this gap by providing a generalization of Cover's Function Counting Theorem that quantifies the number of linearly separable and group-invariant binary dichotomies that can be assigned to equivariant representations of objects. We find that the fraction of separable dichotomies is determined by the dimension of the space that is fixed by the group action. We show how this relation extends to operations such as convolutions, element-wise nonlinearities, and global and local pooling. While other operations do not change the fraction of separable dichotomies, local pooling decreases the fraction, despite being a highly nonlinear operation. Finally, we test our theory on intermediate representations of randomly initialized and fully trained convolutional neural networks and find perfect agreement.},
	urldate = {2021-10-18},
	journal = {arXiv:2110.07472 [cs, stat]},
	author = {Farrell, Matthew and Bordelon, Blake and Trivedi, Shubhendu and Pehlevan, Cengiz},
	month = oct,
	year = {2021},
	note = {arXiv: 2110.07472},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dawid_theory_2014,
	title = {Theory and applications of proper scoring rules},
	volume = {72},
	issn = {0026-1424, 2281-695X},
	url = {http://link.springer.com/10.1007/s40300-014-0039-y},
	doi = {10.1007/s40300-014-0039-y},
	abstract = {A scoring rule S(x; q) provides a way of judging the quality of a quoted probability density q for a random variable X in the light of its outcome x. It is called proper if honesty is your best policy, i.e., when you believe X has density p, your expected score is optimised by the choice q = p. The most celebrated proper scoring rule is the logarithmic score, S(x; q) = − log q(x): this is the only proper scoring rule that is local, in the sense of depending on the density function q only through its value at the observed value x. It is closely connected with likelihood inference, with communication theory, and with minimum description length model selection. However, every statistical decision problem induces a proper scoring rule, so there is a very wide variety of these. Many of them have additional interesting structure and properties. At a theoretical level, any proper scoring rule can be used as a foundational basis for the theory of subjective probability. At an applied level a proper scoring can be used to compare and improve probability forecasts, and, in a parametric setting, as an alternative tool for inference. In this article we give an overview of some uses of proper scoring rules in statistical inference, including frequentist estimation theory and Bayesian model selection with improper priors.},
	language = {en},
	number = {2},
	urldate = {2021-10-18},
	journal = {METRON},
	author = {Dawid, Alexander Philip and Musio, Monica},
	month = aug,
	year = {2014},
	pages = {169--183},
}

@inproceedings{farnia_minimax_2016,
	title = {A {Minimax} {Approach} to {Supervised} {Learning}},
	volume = {29},
	url = {https://proceedings.neurips.cc/paper/2016/hash/7b1ce3d73b70f1a7246e7b76a35fb552-Abstract.html},
	urldate = {2021-10-18},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Farnia, Farzan and Tse, David},
	year = {2016},
}

@article{dawid_minimum_2016,
	title = {Minimum {Scoring} {Rule} {Inference}},
	volume = {43},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12168},
	doi = {10.1111/sjos.12168},
	abstract = {Proper scoring rules are devices for encouraging honest assessment of probability distributions. Just like log-likelihood, which is a special case, a proper scoring rule can be applied to supply an unbiased estimating equation for any statistical model, and the theory of such equations can be applied to understand the properties of the associated estimator. In this paper, we discuss some novel applications of scoring rules to parametric inference. In particular, we focus on scoring rule test statistics, and we propose suitable adjustments to allow reference to the usual asymptotic chi-squared distribution. We further explore robustness and interval estimation properties, by both theory and simulations.},
	language = {en},
	number = {1},
	urldate = {2021-10-18},
	journal = {Scandinavian Journal of Statistics},
	author = {Dawid, A. Philip and Musio, Monica and Ventura, Laura},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/sjos.12168},
	keywords = {B-robustness, Bregman estimate, Godambe information, M-estimator, Tsallis score, composite score, pseudo-likelihood, unbiased estimating equation},
	pages = {123--138},
}

@article{dawid_coherent_1998,
	title = {Coherent measures of discrepancy, uncertainty and dependence, with applications to {Bayesian} predictive experimental design},
	volume = {139},
	journal = {Department of Statistical Science, University College London. http://www. ucl. ac. uk/Stats/research/abs94. html, Tech. Rep},
	author = {Dawid, A Philip},
	year = {1998},
}

@article{zhao_fundamental_2021,
	title = {Fundamental {Limits} and {Tradeoffs} in {Invariant} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2012.10713},
	abstract = {A wide range of machine learning applications such as privacy-preserving learning, algorithmic fairness, and domain adaptation/generalization among others, involve learning {\textbackslash}emph\{invariant representations\} of the data that aim to achieve two competing goals: (a) maximize information or accuracy with respect to a target response, and (b) maximize invariance or independence with respect to a set of protected features (e.g.{\textbackslash} for fairness, privacy, etc). Despite their wide applicability, theoretical understanding of the optimal tradeoffs -- with respect to accuracy, and invariance -- achievable by invariant representations is still severely lacking. In this paper, we provide precisely such an information-theoretic analysis of such tradeoffs under both classification and regression settings. We provide a geometric characterization of the accuracy and invariance achievable by any representation of the data; we term this feasible region the information plane. We provide a lower bound for this feasible region for the classification case, and an exact characterization for the regression case, which allows us to either bound or exactly characterize the Pareto optimal frontier between accuracy and invariance. Although our contributions are mainly theoretical, a key practical application of our results is in certifying the potential sub-optimality of any given representation learning algorithm for either classification or regression tasks. Our results shed new light on the fundamental interplay between accuracy and invariance, and may be useful in guiding the design of future representation learning algorithms.},
	urldate = {2021-10-05},
	journal = {arXiv:2012.10713 [cs, stat]},
	author = {Zhao, Han and Dan, Chen and Aragam, Bryon and Jaakkola, Tommi S. and Gordon, Geoffrey J. and Ravikumar, Pradeep},
	month = sep,
	year = {2021},
	note = {arXiv: 2012.10713},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{scholkopf_learning_2002,
	address = {Cambridge, Mass},
	series = {Adaptive computation and machine learning},
	title = {Learning with kernels: support vector machines, regularization, optimization, and beyond},
	isbn = {978-0-262-19475-4},
	shorttitle = {Learning with kernels},
	language = {en},
	publisher = {MIT Press},
	author = {Schölkopf, Bernhard and Smola, Alexander J.},
	year = {2002},
	keywords = {Kernel functions, Support vector machines},
}

@article{shawe-taylor_kernel_nodate,
	title = {Kernel {Methods} for {Pattern} {Analysis}},
	language = {en},
	author = {Shawe-Taylor, John and Cristianini, Nello},
	pages = {478},
}

@article{david_statistical_2016,
	title = {On statistical learning via the lens of compression},
	url = {http://arxiv.org/abs/1610.03592},
	abstract = {This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. The central theme of this work is establishing equivalences between learnability and compressibility, and utilizing these equivalences in the study of statistical learning theory. We begin with the setting of multiclass categorization (zero/one loss). We prove that in this case learnability is equivalent to compression of logarithmic sample size, and that uniform convergence implies compression of constant size. We then consider Vapnik's general learning setting: we show that in order to extend the compressibility-learnability equivalence to this case, it is necessary to consider an approximate variant of compression. Finally, we provide some applications of the compressibility-learnability equivalences: (i) Agnostic-case learnability and realizable-case learnability are equivalent in multiclass categorization problems (in terms of sample complexity). (ii) This equivalence between agnostic-case learnability and realizable-case learnability does not hold for general learning problems: There exists a learning problem whose loss function takes just three values, under which agnostic-case and realizable-case learnability are not equivalent. (iii) Uniform convergence implies compression of constant size in multiclass categorization problems. Part of the argument includes an analysis of the uniform convergence rate in terms of the graph dimension, in which we improve upon previous bounds. (iv) A dichotomy for sample compression in multiclass categorization problems: If a non-trivial compression exists then a compression of logarithmic size exists. (v) A compactness theorem for multiclass categorization problems.},
	urldate = {2021-09-23},
	journal = {arXiv:1610.03592 [cs, math]},
	author = {David, Ofir and Moran, Shay and Yehudayoff, Amir},
	month = dec,
	year = {2016},
	note = {arXiv: 1610.03592},
	keywords = {Computer Science - Discrete Mathematics, Computer Science - Logic in Computer Science, Computer Science - Machine Learning, Mathematics - Combinatorics, Mathematics - Logic},
}

@techreport{littlestone_relating_1986,
	title = {Relating {Data} {Compression} and {Learnability}},
	abstract = {We explore the learnability of two-valued functions from samples using  the paradigm of Data Compression. A rst algorithm (compression)  choses a small subset of the sample which is called the kernel. A second  algorithm predicts future values of the function from the kernel, i.e. the  algorithm acts as an hypothesis for the function to be learned. The second  algorithm must be able to reconstruct the correct function values when  given a point of the original sample. We demonstrate that the existence of  a suitable data compression scheme is sucient to ensure learnability. We  express the probability that the hypothesis predicts the function correctly  on a random sample point as a function of the sample and kernel sizes.  No assumptions are made on the probability distributions according to  which the sample points are generated.  This approach provides an alternative to that of [BEHW86], which  uses the Vapnik-Chervonenkis dimension to classify learnable geometric  concepts. Our bo...},
	author = {Littlestone, Nick and Warmuth, Manfred K.},
	year = {1986},
}

@article{shinohara_teachability_1991,
	title = {Teachability in computational learning},
	volume = {8},
	issn = {1882-7055},
	url = {https://doi.org/10.1007/BF03037091},
	doi = {10.1007/BF03037091},
	abstract = {This paper considers computational learning from the view-point of teaching. We introduce a notion of teachability with which we establish a relationship between the learnability and teachability. We also discuss the complexity issues of a teacher in relation to learning.},
	language = {en},
	number = {4},
	urldate = {2021-09-23},
	journal = {New Generation Computing},
	author = {Shinohara, Ayumi and Miyano, Satoru},
	month = feb,
	year = {1991},
	pages = {337--347},
}

@article{liu_teaching_nodate,
	title = {The {Teaching} {Dimension} of {Linear} {Learners}},
	abstract = {Teaching dimension is a learning theoretic quantity that speciﬁes the minimum training set size to teach a target model to a learner. Previous studies on teaching dimension focused on version-space learners which maintain all hypotheses consistent with the training data, and cannot be applied to modern machine learners which select a speciﬁc hypothesis via optimization. This paper presents the ﬁrst known teaching dimension for ridge regression, support vector machines, and logistic regression. We also exhibit optimal training sets that match these teaching dimensions. Our approach generalizes to other linear learners.},
	language = {en},
	author = {Liu, Ji and Zhu, Xiaojin},
	pages = {25},
}

@article{angluin_negative_1990,
	title = {Negative results for equivalence queries},
	volume = {5},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00116034},
	doi = {10.1007/BF00116034},
	abstract = {Weconsiderthe problemof exactidentificationof classesof conceptsusingonly equivalencequeries. Wedefinea combinatorialproperty,approximatefingerprints, of classes of conceptsand showthat no class with this property can be exactlyidentified in polynomial time using only equivalencequeries. As applications of this general theorem, we showthat there is no polynomialtime algorithmusing only equivalencequeries that exactly identifies deterministic or nondeterminlstic finite state acceptors, contextfree grammars, or disjunctive or conjunctivenormal formboolean formulas.},
	language = {en},
	number = {2},
	urldate = {2021-09-22},
	journal = {Machine Learning},
	author = {Angluin, Dana},
	month = jun,
	year = {1990},
	pages = {121--150},
}

@article{yarullin_equivalence_2020,
	title = {From equivalence queries to {PAC} learning: {The} case of implication theories},
	volume = {127},
	issn = {0888-613X},
	shorttitle = {From equivalence queries to {PAC} learning},
	url = {https://www.sciencedirect.com/science/article/pii/S0888613X20302176},
	doi = {10.1016/j.ijar.2020.08.011},
	abstract = {In Angluin's exact-learning framework, equivalence queries can be simulated by stochastic equivalence testing to achieve a probably approximately correct identification of an unknown concept. We present an analysis of the number of samples that need to be generated in the process leading to a theoretical improvement on an earlier approach. We apply this modification to a previously known probably approximately correct algorithm for computing implication bases with an implication oracle and evaluate its performance in terms of the number of queries to the oracle on artificial and real-world data.},
	language = {en},
	urldate = {2021-09-22},
	journal = {International Journal of Approximate Reasoning},
	author = {Yarullin, Ramil and Obiedkov, Sergei},
	month = dec,
	year = {2020},
	keywords = {Attribute exploration, Concept learning, Formal concept analysis, Implications, PAC learning, Query learning},
	pages = {1--16},
}

@article{angluin_queries_1988,
	title = {Queries and {Concept} {Learning}},
	volume = {2},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1022821128753},
	doi = {10.1023/A:1022821128753},
	abstract = {We consider the problem of using queries to learn an unknown concept. Several types of queries are described and studied: membership, equivalence, subset, superset, disjointness, and exhaustiveness queries. Examples are given of efficient learning methods using various subsets of these queries for formal domains, including the regular languages, restricted classes of context-free languages, the pattern languages, and restricted types of prepositional formulas. Some general lower bound techniques are given. Equivalence queries are compared with Valiant's criterion of probably approximately correct identification under random sampling.},
	language = {en},
	number = {4},
	urldate = {2021-09-22},
	journal = {Machine Learning},
	author = {Angluin, Dana},
	month = apr,
	year = {1988},
	pages = {319--342},
}

@article{maass_algorithms_1994,
	title = {Algorithms and lower bounds for on-line learning of geometrical concepts},
	volume = {14},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00993976},
	doi = {10.1007/BF00993976},
	abstract = {The complexityof on-line learning is investigated for the basic classes of geometricalobjects over a discrete("digitized")domain. In particular, upper and lowerboundsare derivedfor the complexityof learning algorithms for axis-parallel rectangles, rectangles in general position, balls, halfspaces, intersections of halfspaces, and semi-algebraicsets. The learning model consideredis the standardmodel for on-line learning from counterexamples.},
	language = {en},
	number = {3},
	urldate = {2021-09-22},
	journal = {Machine Learning},
	author = {Maass, Wolfgang and Tur�n, Gy�rgy},
	month = mar,
	year = {1994},
	pages = {251--269},
}

@article{goldman_learning_nodate,
	title = {Learning {Binary} {Relations} and {Total} {Orders}},
	abstract = {We study the problem of learning a binary relation between two sets of objects or between a set and itself. We represent a binary relation between a set of size n and a set of size m as an n m matrix of bits, whose (i j) entry is 1 if and only if the relation holds between the corresponding elements of the two sets. We present polynomial prediction algorithms for learning binary relations in an extended on-line learning model, where the examples are drawn by the learner, by a helpful teacher, by an adversary, or according to a uniform probability distribution on the instance space. In the rst part of this paper, we present results for the case that the matrix of the relation has at most k row types. We present upper and lower bounds on the number of prediction mistakes any prediction algorithm makes when learning such a matrix under the extended on-line learning model. Furthermore, we describe a technique that simpli es the proof of expected mistake bounds against a randomly chosen query sequence.},
	language = {en},
	author = {Goldman, Sally A and Rivest, Ronald L and Schapire, Robert E and Laboratories, T Bell and Hill, Murray},
	pages = {42},
}

@article{dehghani_teaching_nodate,
	title = {Teaching {Dimension} versus {VC} {Dimension}},
	language = {en},
	author = {Dehghani, Sina and Gupta, Neal and Sankararaman, Karthik Abinav},
	pages = {7},
}

@article{simon_open_nodate,
	title = {Open {Problem}: {Recursive} {Teaching} {Dimension} {Versus} {VC} {Dimension}},
	abstract = {The Recursive Teaching Dimension (RTD) of a concept class C is a complexity parameter referring to the worst-case number of labelled examples needed to learn any target concept in C from a teacher following the recursive teaching model. It is the ﬁrst teaching complexity notion for which interesting relationships to the VC dimension (VCD) have been established. In particular, for ﬁnite maximum classes of a given VCD d, the RTD equals d. To date, there is no concept class known for which the ratio of RTD over VCD exceeds 3/2. However, the only known upper bound on RTD in terms of VCD is exponential in the VCD and depends on the size of the concept class. We pose the following question: is the RTD upper-bounded by a function that grows only linearly in the VCD? Answering this question would further our understanding of the relationships between the complexity of teaching and the complexity of learning from randomly chosen examples. In addition, the answer to this question, whether positive or negative, is known to have implications on the study of the long-standing open sample compression conjecture, which claims that every concept class of VCD d has a sample compression scheme in which samples for concepts in the class are compressed to subsets of size no larger than d.},
	language = {en},
	author = {Simon, Hans and Zilles, Sandra},
	pages = {3},
}

@article{doliwa_recursive_2014,
	title = {Recursive {Teaching} {Dimension}, {VC}-{Dimension} and {Sample} {Compression}},
	volume = {15},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v15/doliwa14a.html},
	abstract = {This paper is concerned with various combinatorial parameters of classes that can be learned from a small set of examples. We show that the recursive teaching dimension, recently introduced by Zilles et al. (2008), is strongly connected to known complexity notions in machine learning, e.g., the self- directed learning complexity and the VC-dimension. To the best of our knowledge these are the first results unveiling such relations between teaching and query learning as well as between teaching and the VC-dimension. It will turn out that for many natural classes the RTD is upper-bounded by the VCD, e.g., classes of VC-dimension 1, intersection-closed classes and finite maximum classes. However, we will also show that there are certain (but rare) classes for which the recursive teaching dimension exceeds the VC-dimension. Moreover, for maximum classes, the combinatorial structure induced by the RTD, called teaching plan, is highly similar to the structure of sample compression schemes. Indeed one can transform any repetition-free teaching plan for a maximum class 

C
 into an unlabeled sample compression scheme for 

C
 and vice versa, while the latter is produced by (i) the corner- peeling algorithm of Rubinstein and Rubinstein (2012) and (ii) the tail matching algorithm of Kuzmin and Warmuth (2007).},
	number = {89},
	urldate = {2021-09-22},
	journal = {Journal of Machine Learning Research},
	author = {Doliwa, Thorsten and Fan, Gaojian and Simon, Hans Ulrich and Zilles, Sandra},
	year = {2014},
	pages = {3107--3131},
}

@article{moran_sample_2015,
	title = {Sample compression schemes for {VC} classes},
	url = {http://arxiv.org/abs/1503.06960},
	abstract = {Sample compression schemes were defined by Littlestone and Warmuth (1986) as an abstraction of the structure underlying many learning algorithms. Roughly speaking, a sample compression scheme of size \$k\$ means that given an arbitrary list of labeled examples, one can retain only \$k\$ of them in a way that allows to recover the labels of all other examples in the list. They showed that compression implies PAC learnability for binary-labeled classes, and asked whether the other direction holds. We answer their question and show that every concept class \$C\$ with VC dimension \$d\$ has a sample compression scheme of size exponential in \$d\$. The proof uses an approximate minimax phenomenon for binary matrices of low VC dimension, which may be of interest in the context of game theory.},
	urldate = {2021-09-22},
	journal = {arXiv:1503.06960 [cs]},
	author = {Moran, Shay and Yehudayoff, Amir},
	month = apr,
	year = {2015},
	note = {arXiv: 1503.06960},
	keywords = {Computer Science - Machine Learning},
}

@article{floyd_sample_1995,
	title = {Sample {Compression}, {Learnability}, and the {Vapnik}-{Chervonenkis} {Dimension}},
	volume = {21},
	issn = {0885-6125},
	url = {https://doi.org/10.1023/A:1022660318680},
	doi = {10.1023/A:1022660318680},
	abstract = {Within the framework of pac-learning, we explore the learnability of concepts from samples using the paradigm of sample compression schemes. A sample compression scheme of size k for a concept class C ⊆ 2X consists of a compression function and a reconstruction function. The compression function receives a finite sample set consistent with some concept in C and chooses a subset of k examples as the compression set. The reconstruction function forms a hypothesis on X from a compression set of k examples. For any sample set of a concept in C the compression set produced by the compression function must lead to a hypothesis consistent with the whole original sample set when it is fed to the reconstruction function. We demonstrate that the existence of a sample compression scheme of fixed-size for a class C is sufficient to ensure that the class C is pac-learnable.Previous work has shown that a class is pac-learnable if and only if the Vapnik-Chervonenkis (VC) dimension of the class is finite. In the second half of this paper we explore the relationship between sample compression schemes and the VC dimension. We define maximum and maximal classes of VC dimension d. For every maximum class of VC dimension d, there is a sample compression scheme of size d, and for sufficiently-large maximum classes there is no sample compression scheme of size less than d. We discuss briefly classes of VC dimension d that are maximal but not maximum. It is an open question whether every class of VC dimension d has a sample compression scheme of size O(d).},
	number = {3},
	journal = {Mach. Learn.},
	author = {Floyd, Sally and Warmuth, Manfred},
	month = dec,
	year = {1995},
	note = {Place: USA
Publisher: Kluwer Academic Publishers},
	keywords = {Sample compression, Vapnik-Chervonenkis dimension, pac-learning},
	pages = {269--304},
}

@article{hanneke_sample_2018,
	title = {Sample {Compression} for {Real}-{Valued} {Learners}},
	url = {http://arxiv.org/abs/1805.08254},
	abstract = {We give an algorithmically efficient version of the learner-to-compression scheme conversion in Moran and Yehudayoff (2016). In extending this technique to real-valued hypotheses, we also obtain an efficient regression-to-bounded sample compression converter. To our knowledge, this is the first general compressed regression result (regardless of efficiency or boundedness) guaranteeing uniform approximate reconstruction. Along the way, we develop a generic procedure for constructing weak real-valued learners out of abstract regressors; this may be of independent interest. In particular, this result sheds new light on an open question of H. Simon (1997). We show applications to two regression problems: learning Lipschitz and bounded-variation functions.},
	urldate = {2021-09-22},
	journal = {arXiv:1805.08254 [cs, stat]},
	author = {Hanneke, Steve and Kontorovich, Aryeh and Sadigurschi, Menachem},
	month = may,
	year = {2018},
	note = {arXiv: 1805.08254},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{darnstadt_order_nodate,
	title = {Order {Compression} {Schemes}},
	abstract = {Sample compression schemes are schemes for “encoding” a set of examples in a small subset of examples. The long-standing open sample compression conjecture states that, for any concept class C of VC-dimension d, there is a sample compression scheme in which samples for concepts in C are compressed to samples of size at most d.},
	language = {en},
	author = {Darnstadt, Malte and Doliwa, Thorsten and Simon, Hans Ulrich and Zilles, Sandra},
	pages = {15},
}

@inproceedings{samei_sample_2014,
	title = {Sample {Compression} for {Multi}-label {Concept} {Classes}},
	url = {https://proceedings.mlr.press/v35/samei14.html},
	abstract = {This paper studies labeled sample compression for multi-label concept classes. For a specific extension of the notion of VC-dimension to multi-label classes, we prove that every maximum multi-label class of dimension d has a sample compression scheme in which every sample is compressed to a subset of size at most d. We further show that every multi-label class of dimension 1 has a sample compression scheme using only sets of size at most 1. As opposed to the binary case, the latter result is not immediately implied by the former, since there are multi-label concept classes of dimension 1 that are not contained in maximum classes of dimension 1.},
	language = {en},
	urldate = {2021-09-22},
	booktitle = {Proceedings of {The} 27th {Conference} on {Learning} {Theory}},
	publisher = {PMLR},
	author = {Samei, Rahim and Semukhin, Pavel and Yang, Boting and Zilles, Sandra},
	month = may,
	year = {2014},
	note = {ISSN: 1938-7228},
	pages = {371--393},
}

@article{rubinstein_geometric_2012,
	title = {A {Geometric} {Approach} to {Sample} {Compression}},
	volume = {13},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v13/rubinstein12a.html},
	abstract = {The Sample Compression Conjecture of Littlestone \& Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer's Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a first negative result on the former, through a systematic investigation of finite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin \& Warmuth. A bijection between finite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in ℝd have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any finite maximum class, forming a peeling scheme as conjectured by Kuzmin \& Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d+k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to cubical complexes.},
	number = {42},
	urldate = {2021-09-22},
	journal = {Journal of Machine Learning Research},
	author = {Rubinstein, Benjamin I. P. and Rubinstein, J. Hyam},
	year = {2012},
	pages = {1221--1261},
}

@article{rubinstein_geometric_nodate,
	title = {A {Geometric} {Approach} to {Sample} {Compression}},
	abstract = {The Sample Compression Conjecture of Littlestone \& Warmuth has remained unsolved for a quarter century. While maximum classes (concept classes meeting Sauer’s Lemma with equality) can be compressed, the compression of general concept classes reduces to compressing maximal classes (classes that cannot be expanded without increasing VC dimension). Two promising ways forward are: embedding maximal classes into maximum classes with at most a polynomial increase to VC dimension, and compression via operating on geometric representations. This paper presents positive results on the latter approach and a ﬁrst negative result on the former, through a systematic investigation of ﬁnite maximum classes. Simple arrangements of hyperplanes in hyperbolic space are shown to represent maximum classes, generalizing the corresponding Euclidean result. We show that sweeping a generic hyperplane across such arrangements forms an unlabeled compression scheme of size VC dimension and corresponds to a special case of peeling the one-inclusion graph, resolving a recent conjecture of Kuzmin \& Warmuth. A bijection between ﬁnite maximum classes and certain arrangements of piecewise-linear (PL) hyperplanes in either a ball or Euclidean space is established. Finally we show that d-maximum classes corresponding to PL-hyperplane arrangements in Rd have cubical complexes homeomorphic to a d-ball, or equivalently complexes that are manifolds with boundary. A main result is that PL arrangements can be swept by a moving hyperplane to unlabeled d-compress any ﬁnite maximum class, forming a peeling scheme as conjectured by Kuzmin \& Warmuth. A corollary is that some d-maximal classes cannot be embedded into any maximum class of VC-dimension d + k, for any constant k. The construction of the PL sweeping involves Pachner moves on the one-inclusion graph, corresponding to moves of a hyperplane across the intersection of d other hyperplanes. This extends the well known Pachner moves for triangulations to cubical complexes.},
	language = {en},
	author = {Rubinstein, Benjamin I P and Rubinstein, J Hyam},
	pages = {41},
}

@article{zhu_overview_2018,
	title = {An {Overview} of {Machine} {Teaching}},
	url = {http://arxiv.org/abs/1801.05927},
	abstract = {In this paper we try to organize machine teaching as a coherent set of ideas. Each idea is presented as varying along a dimension. The collection of dimensions then form the problem space of machine teaching, such that existing teaching problems can be characterized in this space. We hope this organization allows us to gain deeper understanding of individual teaching problems, discover connections among them, and identify gaps in the field.},
	urldate = {2021-09-22},
	journal = {arXiv:1801.05927 [cs]},
	author = {Zhu, Xiaojin and Singla, Adish and Zilles, Sandra and Rafferty, Anna N.},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05927},
	keywords = {Computer Science - Machine Learning},
}

@article{zhu_machine_nodate,
	title = {Machine {Teaching}: an {Inverse} {Problem} to {Machine} {Learning} and an {Approach} {Toward} {Optimal} {Education}},
	abstract = {I draw the reader’s attention to machine teaching, the problem of ﬁnding an optimal training set given a machine learning algorithm and a target model. In addition to generating fascinating mathematical questions for computer scientists to ponder, machine teaching holds the promise of enhancing education and personnel training. The Socratic dialogue style aims to stimulate critical thinking.},
	language = {en},
	author = {Zhu, Xiaojin},
	pages = {5},
}

@article{goldman_complexity_1995,
	title = {On the {Complexity} of {Teaching}},
	volume = {50},
	issn = {0022-0000},
	url = {https://www.sciencedirect.com/science/article/pii/S0022000085710033},
	doi = {10.1006/jcss.1995.1003},
	abstract = {While most theoretical work in machine learning has focused on the complexity of learning, recently there has been increasing interest in formally studying the complexity of teaching. In this paper we study the complexity of teaching by considering a variant of the on-line learning model in which a helpful teacher selects the instances. We measure the complexity of teaching a concept from a given concept class by a combinatorial measure we call the teaching dimension, Informally, the teaching dimension of a concept class is the minimum number of instances a teacher must reveal to uniquely identify any target concept chosen from the class.},
	language = {en},
	number = {1},
	urldate = {2021-09-22},
	journal = {Journal of Computer and System Sciences},
	author = {Goldman, S. A. and Kearns, M. J.},
	month = feb,
	year = {1995},
	pages = {20--31},
}

@article{peltola_machine_2019,
	title = {Machine {Teaching} of {Active} {Sequential} {Learners}},
	url = {http://arxiv.org/abs/1809.02869},
	abstract = {Machine teaching addresses the problem of finding the best training data that can guide a learning algorithm to a target model with minimal effort. In conventional settings, a teacher provides data that are consistent with the true data distribution. However, for sequential learners which actively choose their queries, such as multi-armed bandits and active learners, the teacher can only provide responses to the learner's queries, not design the full data. In this setting, consistent teachers can be sub-optimal for finite horizons. We formulate this sequential teaching problem, which current techniques in machine teaching do not address, as a Markov decision process, with the dynamics nesting a model of the learner and the actions being the teacher's responses. Furthermore, we address the complementary problem of learning from a teacher that plans: to recognise the teaching intent of the responses, the learner is endowed with a model of the teacher. We test the formulation with multi-armed bandit learners in simulated experiments and a user study. The results show that learning is improved by (i) planning teaching and (ii) the learner having a model of the teacher. The approach gives tools to taking into account strategic (planning) behaviour of users of interactive intelligent systems, such as recommendation engines, by considering them as boundedly optimal teachers.},
	urldate = {2021-09-22},
	journal = {arXiv:1809.02869 [cs, stat]},
	author = {Peltola, Tomi and Çelikok, Mustafa Mert and Daee, Pedram and Kaski, Samuel},
	month = nov,
	year = {2019},
	note = {arXiv: 1809.02869},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{berenbrink_weighted_2009,
	address = {Berlin, Heidelberg},
	title = {The {Weighted} {Coupon} {Collector}'s {Problem} and {Applications}},
	isbn = {978-3-642-02882-3},
	abstract = {In the classical coupon collector's problem n coupons are given. In every step one of the n coupons is drawn uniformly at random (with replacement) and the goal is to obtain a copy of all the coupons. It is a well-known fact that in expectation \$n {\textbackslash}backslashsum\_\{k=1\}{\textasciicircum}n 1/k {\textbackslash}backslashapprox n {\textbackslash}backslashln n\$steps are needed to obtain all coupons.},
	booktitle = {Computing and {Combinatorics}},
	publisher = {Springer Berlin Heidelberg},
	author = {Berenbrink, Petra and Sauerwald, Thomas},
	editor = {Ngo, Hung Q.},
	year = {2009},
	pages = {449--458},
}

@inproceedings{mitrovic_representation_2021,
	title = {Representation {Learning} via {Invariant} {Causal} {Mechanisms}},
	url = {https://openreview.net/forum?id=9p2ekP904Rs},
	booktitle = {9th {International} {Conference} on {Learning} {Representations}, {ICLR} 2021, {Virtual} {Event}, {Austria}, {May} 3-7, 2021},
	publisher = {OpenReview.net},
	author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob C. and Buesing, Lars Holger and Blundell, Charles},
	year = {2021},
}

@article{patrick_multi-modal_2020,
	title = {Multi-modal {Self}-{Supervision} from {Generalized} {Data} {Transformations}},
	url = {http://arxiv.org/abs/2003.04298},
	abstract = {The recent success of self-supervised learning can be largely attributed to content-preserving transformations, which can be used to easily induce invariances. While transformations generate positive sample pairs in contrastive loss training, most recent work focuses on developing new objective formulations, and pays relatively little attention to the transformations themselves. In this paper, we introduce the framework of Generalized Data Transformations to (1) reduce several recent self-supervised learning objectives to a single formulation for ease of comparison, analysis, and extension, (2) allow a choice between being invariant or distinctive to data transformations, obtaining different supervisory signals, and (3) derive the conditions that combinations of transformations must obey in order to lead to well-posed learning objectives. This framework allows both invariance and distinctiveness to be injected into representations simultaneously, and lets us systematically explore novel contrastive objectives. We apply it to study multi-modal self-supervision for audio-visual representation learning from unlabelled videos, improving the state-of-the-art by a large margin, and even surpassing supervised pretraining. We demonstrate results on a variety of downstream video and audio classification and retrieval tasks, on datasets such as HMDB-51, UCF-101, DCASE2014, ESC-50 and VGG-Sound. In particular, we achieve new state-of-the-art accuracies of 72.8\% on HMDB-51 and 95.2\% on UCF-101.},
	urldate = {2021-08-18},
	journal = {arXiv:2003.04298 [cs]},
	author = {Patrick, Mandela and Asano, Yuki M. and Kuznetsova, Polina and Fong, Ruth and Henriques, João F. and Zweig, Geoffrey and Vedaldi, Andrea},
	month = jun,
	year = {2020},
	note = {arXiv: 2003.04298},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{mitrovic_representation_2020,
	title = {Representation {Learning} via {Invariant} {Causal} {Mechanisms}},
	url = {http://arxiv.org/abs/2010.07922},
	abstract = {Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on \$51\$ out of \$57\$ games.},
	urldate = {2021-08-18},
	journal = {arXiv:2010.07922 [cs, stat]},
	author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.07922},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{mitrovic_less_2020,
	title = {Less can be more in contrastive learning},
	url = {http://proceedings.mlr.press/v137/mitrovic20a.html},
	language = {en},
	urldate = {2021-08-18},
	publisher = {PMLR},
	author = {Mitrovic, Jovana and McWilliams, Brian and Rey, Melanie},
	month = feb,
	year = {2020},
	note = {ISSN: 2640-3498},
	pages = {70--75},
}

@article{muller_learning_2021,
	title = {Learning {Robust} {Models} {Using} {The} {Principle} of {Independent} {Causal} {Mechanisms}},
	url = {http://arxiv.org/abs/2010.07167},
	abstract = {Standard supervised learning breaks down under data distribution shift. However, the principle of independent causal mechanisms (ICM, Peters et al. (2017)) can turn this weakness into an opportunity: one can take advantage of distribution shift between different environments during training in order to obtain more robust models. We propose a new gradient-based learning framework whose objective function is derived from the ICM principle. We show theoretically and experimentally that neural networks trained in this framework focus on relations remaining invariant across environments and ignore unstable ones. Moreover, we prove that the recovered stable relations correspond to the true causal mechanisms under certain conditions. In both regression and classification, the resulting models generalize well to unseen scenarios where traditionally trained models fail.},
	urldate = {2021-08-18},
	journal = {arXiv:2010.07167 [cs, stat]},
	author = {Müller, Jens and Schmier, Robert and Ardizzone, Lynton and Rother, Carsten and Köthe, Ullrich},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.07167},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mansour_domain_2009-1,
	title = {Domain {Adaptation}: {Learning} {Bounds} and {Algorithms}},
	shorttitle = {Domain {Adaptation}},
	url = {http://arxiv.org/abs/0902.3430},
	abstract = {This paper addresses the general problem of domain adaptation which arises in a variety of applications where the distribution of the labeled sample available somewhat differs from that of the test data. Building on previous work by Ben-David et al. (2007), we introduce a novel distance between distributions, discrepancy distance, that is tailored to adaptation problems with arbitrary loss functions. We give Rademacher complexity bounds for estimating the discrepancy distance from finite samples for different loss functions. Using this distance, we derive novel generalization bounds for domain adaptation for a wide family of loss functions. We also present a series of novel adaptation bounds for large classes of regularization-based algorithms, including support vector machines and kernel ridge regression based on the empirical discrepancy. This motivates our analysis of the problem of minimizing the empirical discrepancy for various loss functions for which we also give novel algorithms. We report the results of preliminary experiments that demonstrate the benefits of our discrepancy minimization algorithms for domain adaptation.},
	urldate = {2021-07-25},
	journal = {arXiv:0902.3430 [cs]},
	author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
	month = feb,
	year = {2009},
	note = {arXiv: 0902.3430},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@incollection{li_deep_2018,
	address = {Cham},
	title = {Deep {Domain} {Generalization} via {Conditional} {Invariant} {Adversarial} {Networks}},
	volume = {11219},
	isbn = {978-3-030-01266-3 978-3-030-01267-0},
	url = {http://link.springer.com/10.1007/978-3-030-01267-0_38},
	abstract = {Domain generalization aims to learn a classiﬁcation model from multiple source domains and generalize it to unseen target domains. A critical problem in domain generalization involves learning domaininvariant representations. Let X and Y denote the features and the labels, respectively. Under the assumption that the conditional distribution P (Y {\textbar}X) remains unchanged across domains, earlier approaches to domain generalization learned the invariant representation T (X) by minimizing the discrepancy of the marginal distribution P (T (X)). However, such an assumption of stable P (Y {\textbar}X) does not necessarily hold in practice. In addition, the representation learning function T (X) is usually constrained to a simple linear transformation or shallow networks. To address the above two drawbacks, we propose an end-to-end conditional invariant deep domain generalization approach by leveraging deep neural networks for domain-invariant representation learning. The domain-invariance property is guaranteed through a conditional invariant adversarial network that can learn domain-invariant representations w.r.t. the joint distribution P (T (X), Y ) if the target domain data are not severely class unbalanced. We perform various experiments to demonstrate the eﬀectiveness of the proposed method.},
	language = {en},
	urldate = {2021-07-23},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
	year = {2018},
	doi = {10.1007/978-3-030-01267-0_38},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {647--663},
}

@article{gong_domain_2016,
	title = {Domain {Adaptation} with {Conditional} {Transferable} {Components}},
	abstract = {Domain adaptation arises in supervised learning when the training (source domain) and test (target domain) data have different distributions. Let X and Y denote the features and target, respectively, previous work on domain adaptation mainly considers the covariate shift situation where the distribution of the features P (X) changes across domains while the conditional distribution P (Y {\textbar}X) stays the same. To reduce domain discrepancy, recent methods try to ﬁnd invariant components T (X) that have similar P (T (X)) on different domains by explicitly minimizing a distribution discrepancy measure. However, it is not clear if P (Y {\textbar}T (X)) in different domains is also similar when P (Y {\textbar}X) changes. Furthermore, transferable components do not necessarily have to be invariant. If the change in some components is identiﬁable, we can make use of such components for prediction in the target domain. In this paper, we focus on the case where P (X{\textbar}Y ) and P (Y ) both change in a causal system in which Y is the cause for X. Under appropriate assumptions, we aim to extract conditional transferable components whose conditional distribution P (T (X){\textbar}Y ) is invariant after proper location-scale (LS) transformations, and identify how P (Y ) changes between domains simultaneously. We provide theoretical analysis and empirical evaluation on both synthetic and real-world data to show the effectiveness of our method.},
	language = {en},
	author = {Gong, Mingming and Zhang, Kun and Liu, Tongliang and Tao, Dacheng and Glymour, Clark and Scholkopf, Bernhard},
	year = {2016},
	pages = {10},
}

@article{wang_generalizing_2021,
	title = {Generalizing to {Unseen} {Domains}: {A} {Survey} on {Domain} {Generalization}},
	shorttitle = {Generalizing to {Unseen} {Domains}},
	url = {http://arxiv.org/abs/2103.03097},
	abstract = {Machine learning systems generally assume that the training and testing distributions are the same. To this end, a key requirement is to develop models that can generalize to unseen distributions. Domain generalization (DG), i.e., out-of-distribution generalization, has attracted increasing interests in recent years. Domain generalization deals with a challenging setting where one or several different but related domain(s) are given, and the goal is to learn a model that can generalize to an unseen test domain. Great progress has been made in the area of domain generalization for years. This paper presents the first review of recent advances in this area. First, we provide a formal definition of domain generalization and discuss several related fields. We then thoroughly review the theories related to domain generalization and carefully analyze the theory behind generalization. We categorize recent algorithms into three classes: data manipulation, representation learning, and learning strategy, and present several popular algorithms in detail for each category. Third, we introduce the commonly used datasets and applications. Finally, we summarize existing literature and present some potential research topics for the future.},
	urldate = {2021-07-25},
	journal = {arXiv:2103.03097 [cs]},
	author = {Wang, Jindong and Lan, Cuiling and Liu, Chang and Ouyang, Yidong and Zeng, Wenjun and Qin, Tao},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.03097},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{baktashmotlagh_unsupervised_2013,
	address = {Sydney, Australia},
	title = {Unsupervised {Domain} {Adaptation} by {Domain} {Invariant} {Projection}},
	isbn = {978-1-4799-2840-8},
	url = {http://ieeexplore.ieee.org/document/6751205/},
	doi = {10.1109/ICCV.2013.100},
	abstract = {Domain-invariant representations are key to addressing the domain shift problem where the training and test examples follow different distributions. Existing techniques that have attempted to match the distributions of the source and target domains typically compare these distributions in the original feature space. This space, however, may not be directly suitable for such a comparison, since some of the features may have been distorted by the domain shift, or may be domain speciﬁc. In this paper, we introduce a Domain Invariant Projection approach: An unsupervised domain adaptation method that overcomes this issue by extracting the information that is invariant across the source and target domains. More speciﬁcally, we learn a projection of the data to a low-dimensional latent space where the distance between the empirical distributions of the source and target examples is minimized. We demonstrate the effectiveness of our approach on the task of visual object recognition and show that it outperforms state-of-the-art methods on a standard domain adaptation benchmark dataset.},
	language = {en},
	urldate = {2021-07-25},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision}},
	publisher = {IEEE},
	author = {Baktashmotlagh, Mahsa and Harandi, Mehrtash T. and Lovell, Brian C. and Salzmann, Mathieu},
	month = dec,
	year = {2013},
	pages = {769--776},
}

@inproceedings{long_learning_2015,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Learning {Transferable} {Features} with {Deep} {Adaptation} {Networks}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/long15.html},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}, {ICML} 2015, {Lille}, {France}, 6-11 {July} 2015},
	publisher = {JMLR.org},
	author = {Long, Mingsheng and Cao, Yue and Wang, Jianmin and Jordan, Michael I.},
	editor = {Bach, Francis R. and Blei, David M.},
	year = {2015},
	pages = {97--105},
}

@inproceedings{long_transfer_2014,
	address = {Columbus, OH, USA},
	title = {Transfer {Joint} {Matching} for {Unsupervised} {Domain} {Adaptation}},
	isbn = {978-1-4799-5118-5},
	url = {https://ieeexplore.ieee.org/document/6909579},
	doi = {10.1109/CVPR.2014.183},
	abstract = {Visual domain adaptation, which learns an accurate classiﬁer for a new domain using labeled images from an old domain, has shown promising value in computer vision yet still been a challenging problem. Most prior works have explored two learning strategies independently for domain adaptation: feature matching and instance reweighting. In this paper, we show that both strategies are important and inevitable when the domain difference is substantially large. We therefore put forward a novel Transfer Joint Matching (TJM) approach to model them in a uniﬁed optimization problem. Speciﬁcally, TJM aims to reduce the domain difference by jointly matching the features and reweighting the instances across domains in a principled dimensionality reduction procedure, and construct new feature representation that is invariant to both the distribution difference and the irrelevant instances. Comprehensive experimental results verify that TJM can signiﬁcantly outperform competitive methods for cross-domain image recognition problems.},
	language = {en},
	urldate = {2021-07-25},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	publisher = {IEEE},
	author = {Long, Mingsheng and Wang, Jianmin and Ding, Guiguang and Sun, Jiaguang and Yu, Philip S.},
	month = jun,
	year = {2014},
	pages = {1410--1417},
}

@article{pan_domain_2011,
	title = {Domain {Adaptation} via {Transfer} {Component} {Analysis}},
	volume = {22},
	issn = {1941-0093},
	doi = {10.1109/TNN.2010.2091281},
	abstract = {Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.},
	number = {2},
	journal = {IEEE Transactions on Neural Networks},
	author = {Pan, Sinno Jialin and Tsang, Ivor W. and Kwok, James T. and Yang, Qiang},
	month = feb,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Neural Networks},
	keywords = {Dimensionality reduction, Feature extraction, Hilbert space, Hilbert space embedding of distributions, Kernel, Learning systems, Manifolds, Noise measurement, Optimization, domain adaptation, transfer learning},
	pages = {199--210},
}

@inproceedings{blanchard_generalizing_2011,
	title = {Generalizing from {Several} {Related} {Classification} {Tasks} to a {New} {Unlabeled} {Sample}},
	url = {https://proceedings.neurips.cc/paper/2011/hash/b571ecea16a9824023ee1af16897a582-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 24: 25th {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2011. {Proceedings} of a meeting held 12-14 {December} 2011, {Granada}, {Spain}},
	author = {Blanchard, Gilles and Lee, Gyemin and Scott, Clayton},
	editor = {Shawe-Taylor, John and Zemel, Richard S. and Bartlett, Peter L. and Pereira, Fernando C. N. and Weinberger, Kilian Q.},
	year = {2011},
	pages = {2178--2186},
}

@inproceedings{greenspan_combined_1992,
	title = {Combined {Neural} {Network} and {Rule}-{Based} {Framework} for {Probabilistic} {Pattern} {Recognition} and {Discovery}},
	volume = {4},
	url = {https://proceedings.neurips.cc/paper/1991/file/46922a0880a8f11f8f69cbb52b1396be-Paper.pdf},
	urldate = {2021-07-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {Greenspan, Hayit K. and Goodman, Rodney and Chellappa, Rama},
	editor = {Moody, J. and Hanson, S. and Lippmann, R. P.},
	year = {1992},
}

@article{muandet_domain_2013,
	title = {Domain {Generalization} via {Invariant} {Feature} {Representation}},
	abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classiﬁers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classiﬁer performance in practice.},
	language = {en},
	author = {Muandet, Krikamol and Balduzzi, David and Scholkopf, Bernhard},
	year = {2013},
	pages = {9},
}

@inproceedings{zhao_learning_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Learning} {Invariant} {Representations} for {Domain} {Adaptation}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/zhao19a.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Zhao, Han and Combes, Remi Tachet des and Zhang, Kun and Gordon, Geoffrey J.},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	pages = {7523--7532},
}

@article{moyer_invariant_2019,
	title = {Invariant {Representations} without {Adversarial} {Training}},
	url = {http://arxiv.org/abs/1805.09458},
	abstract = {Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.},
	urldate = {2021-07-16},
	journal = {arXiv:1805.09458 [cs, stat]},
	author = {Moyer, Daniel and Gao, Shuyang and Brekelmans, Rob and Steeg, Greg Ver and Galstyan, Aram},
	month = dec,
	year = {2019},
	note = {arXiv: 1805.09458},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cai_learning_2019,
	title = {Learning {Disentangled} {Semantic} {Representation} for {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2012.11807},
	doi = {10.24963/ijcai.2019/285},
	abstract = {Domain adaptation is an important but challenging task. Most of the existing domain adaptation methods struggle to extract the domain-invariant representation on the feature space with entangling domain information and semantic information. Different from previous efforts on the entangled feature space, we aim to extract the domain invariant semantic information in the latent disentangled semantic representation (DSR) of the data. In DSR, we assume the data generation process is controlled by two independent sets of variables, i.e., the semantic latent variables and the domain latent variables. Under the above assumption, we employ a variational auto-encoder to reconstruct the semantic latent variables and domain latent variables behind the data. We further devise a dual adversarial network to disentangle these two sets of reconstructed latent variables. The disentangled semantic latent variables are finally adapted across the domains. Experimental studies testify that our model yields state-of-the-art performance on several domain adaptation benchmark datasets.},
	urldate = {2021-07-16},
	journal = {Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence},
	author = {Cai, Ruichu and Li, Zijian and Wei, Pengfei and Qiao, Jie and Zhang, Kun and Hao, Zhifeng},
	month = aug,
	year = {2019},
	note = {arXiv: 2012.11807},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	pages = {2060--2066},
}

@inproceedings{weber_observer_2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Observer {Dependent} {Lossy} {Image} {Compression}},
	volume = {12544},
	url = {https://doi.org/10.1007/978-3-030-71278-5\_10},
	doi = {10.1007/978-3-030-71278-5_10},
	booktitle = {Pattern {Recognition} - 42nd {DAGM} {German} {Conference}, {DAGM} {GCPR} 2020, {Tübingen}, {Germany}, {September} 28 - {October} 1, 2020, {Proceedings}},
	publisher = {Springer},
	author = {Weber, Maurice and Renggli, Cédric and Grabner, Helmut and Zhang, Ce},
	editor = {Akata, Zeynep and Geiger, Andreas and Sattler, Torsten},
	year = {2020},
	pages = {130--144},
}

@article{liu_machine_2019,
	title = {Machine {Vision} {Guided} {3D} {Medical} {Image} {Compression} for {Efficient} {Transmission} and {Accurate} {Segmentation} in the {Clouds}},
	url = {http://arxiv.org/abs/1904.08487},
	abstract = {Cloud based medical image analysis has become popular recently due to the high computation complexities of various deep neural network (DNN) based frameworks and the increasingly large volume of medical images that need to be processed. It has been demonstrated that for medical images the transmission from local to clouds is much more expensive than the computation in the clouds itself. Towards this, 3D image compression techniques have been widely applied to reduce the data traffic. However, most of the existing image compression techniques are developed around human vision, i.e., they are designed to minimize distortions that can be perceived by human eyes. In this paper we will use deep learning based medical image segmentation as a vehicle and demonstrate that interestingly, machine and human view the compression quality differently. Medical images compressed with good quality w.r.t. human vision may result in inferior segmentation accuracy. We then design a machine vision oriented 3D image compression framework tailored for segmentation using DNNs. Our method automatically extracts and retains image features that are most important to the segmentation. Comprehensive experiments on widely adopted segmentation frameworks with HVSMR 2016 challenge dataset show that our method can achieve significantly higher segmentation accuracy at the same compression rate, or much better compression rate under the same segmentation accuracy, when compared with the existing JPEG 2000 method. To the best of the authors' knowledge, this is the first machine vision guided medical image compression framework for segmentation in the clouds.},
	urldate = {2021-07-02},
	journal = {arXiv:1904.08487 [cs, stat]},
	author = {Liu, Zihao and Xu, Xiaowei and Liu, Tao and Liu, Qi and Wang, Yanzhi and Shi, Yiyu and Wen, Wujie and Huang, Meiping and Yuan, Haiyun and Zhuang, Jian},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.08487},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{liu_deepn-jpeg_2018,
	title = {{DeepN}-{JPEG}: {A} {Deep} {Neural} {Network} {Favorable} {JPEG}-based {Image} {Compression} {Framework}},
	shorttitle = {{DeepN}-{JPEG}},
	url = {http://arxiv.org/abs/1803.05788},
	abstract = {As one of most fascinating machine learning techniques, deep neural network (DNN) has demonstrated excellent performance in various intelligent tasks such as image classification. DNN achieves such performance, to a large extent, by performing expensive training over huge volumes of training data. To reduce the data storage and transfer overhead in smart resource-limited Internet-of-Thing (IoT) systems, effective data compression is a "must-have" feature before transferring real-time produced dataset for training or classification. While there have been many well-known image compression approaches (such as JPEG), we for the first time find that a human-visual based image compression approach such as JPEG compression is not an optimized solution for DNN systems, especially with high compression ratios. To this end, we develop an image compression framework tailored for DNN applications, named "DeepN-JPEG", to embrace the nature of deep cascaded information process mechanism of DNN architecture. Extensive experiments, based on "ImageNet" dataset with various state-of-the-art DNNs, show that "DeepN-JPEG" can achieve {\textasciitilde}3.5x higher compression rate over the popular JPEG solution while maintaining the same accuracy level for image recognition, demonstrating its great potential of storage and power efficiency in DNN-based smart IoT system design.},
	urldate = {2021-07-02},
	journal = {arXiv:1803.05788 [cs]},
	author = {Liu, Zihao and Liu, Tao and Wen, Wujie and Jiang, Lei and Xu, Jie and Wang, Yanzhi and Quan, Gang},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.05788},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Computer Science - Performance},
}

@article{parascandolo_learning_2020,
	title = {Learning explanations that are hard to vary},
	url = {http://arxiv.org/abs/2009.00329},
	abstract = {In this paper, we investigate the principle that `good explanations are hard to vary' in the context of deep learning. We show that averaging gradients across examples -- akin to a logical OR of patterns -- can favor memorization and `patchwork' solutions that sew together different strategies, instead of identifying invariances. To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled. We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers.},
	urldate = {2021-07-01},
	journal = {arXiv:2009.00329 [cs, stat]},
	author = {Parascandolo, Giambattista and Neitz, Alexander and Orvieto, Antonio and Gresele, Luigi and Schölkopf, Bernhard},
	month = oct,
	year = {2020},
	note = {arXiv: 2009.00329},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dubois_lossy_2021,
	title = {Lossy {Compression} for {Lossless} {Prediction}},
	url = {http://arxiv.org/abs/2106.10800},
	abstract = {Most data is automatically collected and only ever "seen" by algorithms. Yet, data compressors preserve perceptual fidelity rather than just the information needed by algorithms performing downstream tasks. In this paper, we characterize the bit-rate required to ensure high performance on all predictive tasks that are invariant under a set of transformations, such as data augmentations. Based on our theory, we design unsupervised objectives for training neural compressors. Using these objectives, we train a generic image compressor that achieves substantial rate savings (more than \$1000{\textbackslash}times\$ on ImageNet) compared to JPEG on 8 datasets, without decreasing downstream classification performance.},
	urldate = {2021-06-26},
	journal = {arXiv:2106.10800 [cs, math, stat]},
	author = {Dubois, Yann and Bloem-Reddy, Benjamin and Ullrich, Karen and Maddison, Chris J.},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.10800},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{asoodeh_bottleneck_2020,
	title = {Bottleneck {Problems}: {Information} and {Estimation}-{Theoretic} {View}},
	volume = {22},
	issn = {1099-4300},
	shorttitle = {Bottleneck {Problems}},
	url = {http://arxiv.org/abs/2011.06208},
	doi = {10.3390/e22111325},
	abstract = {Information bottleneck (IB) and privacy funnel (PF) are two closely related optimization problems which have found applications in machine learning, design of privacy algorithms, capacity problems (e.g., Mrs. Gerber's Lemma), strong data processing inequalities, among others. In this work, we first investigate the functional properties of IB and PF through a unified theoretical framework. We then connect them to three information-theoretic coding problems, namely hypothesis testing against independence, noisy source coding and dependence dilution. Leveraging these connections, we prove a new cardinality bound for the auxiliary variable in IB, making its computation more tractable for discrete random variables. In the second part, we introduce a general family of optimization problems, termed as {\textbackslash}textit\{bottleneck problems\}, by replacing mutual information in IB and PF with other notions of mutual information, namely \$f\$-information and Arimoto's mutual information. We then argue that, unlike IB and PF, these problems lead to easily interpretable guarantee in a variety of inference tasks with statistical constraints on accuracy and privacy. Although the underlying optimization problems are non-convex, we develop a technique to evaluate bottleneck problems in closed form by equivalently expressing them in terms of lower convex or upper concave envelope of certain functions. By applying this technique to binary case, we derive closed form expressions for several bottleneck problems.},
	number = {11},
	urldate = {2021-06-26},
	journal = {Entropy},
	author = {Asoodeh, Shahab and Calmon, Flavio},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.06208},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory},
	pages = {1325},
}

@book{zgurovsky_big_2020,
	title = {Big {Data}: {Conceptual} {Analysis} and {Applications}},
	publisher = {Springer},
	author = {Zgurovsky, Michael Z and Zaychenko, Yuriy P},
	year = {2020},
}

@article{rolnick_tackling_2019,
	title = {Tackling {Climate} {Change} with {Machine} {Learning}},
	volume = {abs/1906.05433},
	url = {http://arxiv.org/abs/1906.05433},
	journal = {CoRR},
	author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and Milojevic-Dupont, Nikola and Jaques, Natasha and Waldman-Brown, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Körding, Konrad P. and Gomes, Carla P. and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer T. and Bengio, Yoshua},
	year = {2019},
	note = {\_eprint: 1906.05433},
}

@inproceedings{caesar_nuscenes_2020,
	title = {nuscenes: {A} multimodal dataset for autonomous driving},
	booktitle = {Proceedings of the {IEEE}/{CVF} conference on computer vision and pattern recognition},
	author = {Caesar, Holger and Bankiti, Varun and Lang, Alex H and Vora, Sourabh and Liong, Venice Erin and Xu, Qiang and Krishnan, Anush and Pan, Yu and Baldan, Giancarlo and Beijbom, Oscar},
	year = {2020},
	pages = {11621--11631},
}

@book{jacod_probability_2004,
	edition = {2nd},
	title = {Probability {Essentials}},
	publisher = {Springer-Verlag Berlin Heidelberg},
	author = {Jacod, Jean and Protter, Phillip},
	year = {2004},
}

@article{yeong_sensor_2021,
	title = {Sensor and sensor fusion technology in autonomous vehicles: a review},
	volume = {21},
	number = {6},
	journal = {Sensors},
	author = {Yeong, De Jong and Velasco-Hernandez, Gustavo and Barry, John and Walsh, Joseph and {others}},
	year = {2021},
	note = {Publisher: Multidisciplinary Digital Publishing Institute},
	pages = {2140},
}

@article{rosenthal_economics_2012,
	title = {The economics of long-term digital storage},
	journal = {Memory of the World in the Digital Age, Vancouver, BC},
	author = {Rosenthal, David SH and Rosenthal, Daniel C and Miller, Ethan L and Adams, Ian F and Storer, Mark W and Zadok, Erez},
	year = {2012},
}

@article{kalra_driving_2016,
	title = {Driving to safety: {How} many miles of driving would it take to demonstrate autonomous vehicle reliability?},
	volume = {94},
	journal = {Transportation Research Part A: Policy and Practice},
	author = {Kalra, Nidhi and Paddock, Susan M},
	year = {2016},
	note = {Publisher: Elsevier},
	pages = {182--193},
}

@inproceedings{ramamurthy_enabling_2017,
	series = {{EGU} {General} {Assembly} {Conference} {Abstracts}},
	title = {Enabling a new {Paradigm} to {Address} {Big} {Data} and {Open} {Science} {Challenges}},
	booktitle = {{EGU} {General} {Assembly} {Conference} {Abstracts}},
	author = {Ramamurthy, Mohan and Fisher, Ward},
	month = apr,
	year = {2017},
	pages = {17396},
}

@book{graphics_png_isoiec_2003,
	title = {{ISO}/{IEC} 15948},
	author = {Graphics (PNG), Portable Network},
	year = {2003},
	note = {Published: hhttp://www.libpng.org/pub/png/spec/iso/index-object.html},
}

@book{group_jpeg_itu-t_1992,
	title = {{ITU}-{T} {T}.81, {ITU}-{T} {T}.83, {ITU}-{T} {T}.84, {ITU}-{T} {T}.86, {ISO}/{IEC} 10918},
	author = {Group (JPEG), Joint Photographic Experts},
	year = {1992},
	note = {Published: www.jpeg.org/jpeg/},
}

@book{webp_google_2018,
	title = {Google},
	author = {{WebP}},
	year = {2018},
	note = {Published: developers.google.com/speed/webp},
}

@inproceedings{coates_analysis_2011,
	title = {An analysis of single-layer networks in unsupervised feature learning},
	booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Coates, Adam and Ng, Andrew and Lee, Honglak},
	year = {2011},
	pages = {215--223},
}

@techreport{sorscher_geometry_2021,
	type = {preprint},
	title = {The {Geometry} of {Concept} {Learning}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.03.21.436284},
	abstract = {Understanding the neural basis of our remarkable cognitive capacity to accurately learn novel highdimensional naturalistic concepts from just one or a few sensory experiences constitutes a fundamental problem. We propose a simple, biologically plausible, mathematically tractable, and computationally powerful neural mechanism for few-shot learning of naturalistic concepts. We posit that the concepts we can learn given few examples are deﬁned by tightly circumscribed manifolds in the neural ﬁring rate space of higher order sensory areas. We further posit that a single plastic downstream neuron can learn such concepts from few examples using a simple plasticity rule. We demonstrate the computational power of our simple proposal by showing it can achieve high few-shot learning accuracy on natural visual concepts using both macaque inferotemporal cortex representations and deep neural network models of these representations, and can even learn novel visual concepts speciﬁed only through language descriptions. Moreover, we develop a mathematical theory of few-shot learning that links neurophysiology to behavior by delineating several fundamental and measurable geometric properties of high-dimensional neural representations that can accurately predict the few-shot learning performance of naturalistic concepts across all our experiments. We discuss several implications of our theory for past and future studies in neuroscience, psychology and machine learning.},
	language = {en},
	urldate = {2021-06-18},
	institution = {Neuroscience},
	author = {Sorscher, Ben and Ganguli, Surya and Sompolinsky, Haim},
	month = mar,
	year = {2021},
	doi = {10.1101/2021.03.21.436284},
}

@article{von_kugelgen_self-supervised_2021,
	title = {Self-{Supervised} {Learning} with {Data} {Augmentations} {Provably} {Isolates} {Content} from {Style}},
	url = {http://arxiv.org/abs/2106.04619},
	abstract = {Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.},
	urldate = {2021-06-17},
	journal = {arXiv:2106.04619 [cs, stat]},
	author = {von Kügelgen, Julius and Sharma, Yash and Gresele, Luigi and Brendel, Wieland and Schölkopf, Bernhard and Besserve, Michel and Locatello, Francesco},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.04619},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{hadsell_dimensionality_2006,
	title = {Dimensionality {Reduction} by {Learning} an {Invariant} {Mapping}},
	volume = {2},
	doi = {10.1109/CVPR.2006.100},
	abstract = {Dimensionality reduction involves mapping a set of high dimensional input points onto a low dimensional manifold so that 'similar" points in input space are mapped to nearby points on the manifold. We present a method - called Dimensionality Reduction by Learning an Invariant Mapping (DrLIM) - for learning a globally coherent nonlinear function that maps the data evenly to the output manifold. The learning relies solely on neighborhood relationships and does not require any distancemeasure in the input space. The method can learn mappings that are invariant to certain transformations of the inputs, as is demonstrated with a number of experiments. Comparisons are made to other techniques, in particular LLE.},
	booktitle = {2006 {IEEE} {Computer} {Society} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR}'06)},
	author = {Hadsell, R. and Chopra, S. and LeCun, Y.},
	month = jun,
	year = {2006},
	note = {ISSN: 1063-6919},
	keywords = {Astronomy, Biology, Data visualization, Extraterrestrial measurements, Feature extraction, Geoscience, Image analysis, Image generation, Manufacturing industries, Service robots},
	pages = {1735--1742},
}

@article{li_self-supervised_2021,
	title = {Self-{Supervised} {Learning} with {Kernel} {Dependence} {Maximization}},
	url = {http://arxiv.org/abs/2106.08320},
	abstract = {We approach self-supervised learning of image representations from a statistical dependence perspective, proposing Self-Supervised Learning with the Hilbert-Schmidt Independence Criterion (SSL-HSIC). SSL-HSIC maximizes dependence between representations of transformed versions of an image and the image identity, while minimizing the kernelized variance of those features. This self-supervised learning framework yields a new understanding of InfoNCE, a variational lower bound on the mutual information (MI) between different transformations. While the MI itself is known to have pathologies which can result in meaningless representations being learned, its bound is much better behaved: we show that it implicitly approximates SSL-HSIC (with a slightly different regularizer). Our approach also gives us insight into BYOL, since SSL-HSIC similarly learns local neighborhoods of samples. SSL-HSIC allows us to directly optimize statistical dependence in time linear in the batch size, without restrictive data assumptions or indirect mutual information estimators. Trained with or without a target network, SSL-HSIC matches the current state-of-the-art for standard linear evaluation on ImageNet, semi-supervised learning and transfer to other classification and vision tasks such as semantic segmentation, depth estimation and object recognition.},
	urldate = {2021-06-17},
	journal = {arXiv:2106.08320 [cs, stat]},
	author = {Li, Yazhe and Pogodin, Roman and Sutherland, Danica J. and Gretton, Arthur},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.08320},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tsai_note_2021,
	title = {A {Note} on {Connecting} {Barlow} {Twins} with {Negative}-{Sample}-{Free} {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2104.13712},
	abstract = {In this report, we relate the algorithmic design of Barlow Twins' method to the Hilbert-Schmidt Independence Criterion (HSIC), thus establishing it as a contrastive learning approach that is free of negative samples. Through this perspective, we argue that Barlow Twins (and thus the class of negative-sample-free contrastive learning methods) suggests a possibility to bridge the two major families of self-supervised learning philosophies: non-contrastive and contrastive approaches. In particular, Barlow twins exemplified how we could combine the best practices of both worlds: avoiding the need of large training batch size and negative sample pairing (like non-contrastive methods) and avoiding symmetry-breaking network designs (like contrastive methods).},
	urldate = {2021-06-17},
	journal = {arXiv:2104.13712 [cs]},
	author = {Tsai, Yao-Hung Hubert and Bai, Shaojie and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.13712},
	keywords = {Computer Science - Machine Learning},
}

@article{haochen_provable_2021,
	title = {Provable {Guarantees} for {Self}-{Supervised} {Deep} {Learning} with {Spectral} {Contrastive} {Loss}},
	url = {http://arxiv.org/abs/2106.04156},
	abstract = {Recent works in self-supervised learning have advanced the state-of-the-art by relying on the contrastive learning paradigm, which learns representations by pushing positive pairs, or similar examples from the same class, closer together while keeping negative pairs far apart. Despite the empirical successes, theoretical foundations are limited -- prior analyses assume conditional independence of the positive pairs given the same class label, but recent empirical applications use heavily correlated positive pairs (i.e., data augmentations of the same image). Our work analyzes contrastive learning without assuming conditional independence of positive pairs using a novel concept of the augmentation graph on data. Edges in this graph connect augmentations of the same data, and ground-truth classes naturally form connected sub-graphs. We propose a loss that performs spectral decomposition on the population augmentation graph and can be succinctly written as a contrastive learning objective on neural net representations. Minimizing this objective leads to features with provable accuracy guarantees under linear probe evaluation. By standard generalization bounds, these accuracy guarantees also hold when minimizing the training contrastive loss. Empirically, the features learned by our objective can match or outperform several strong baselines on benchmark vision datasets. In all, this work provides the first provable analysis for contrastive learning where guarantees for linear probe evaluation can apply to realistic empirical settings.},
	urldate = {2021-06-10},
	journal = {arXiv:2106.04156 [cs, stat]},
	author = {HaoChen, Jeff Z. and Wei, Colin and Gaidon, Adrien and Ma, Tengyu},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.04156},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cai_are_2020,
	title = {Are all negatives created equal in contrastive instance discrimination?},
	url = {http://arxiv.org/abs/2010.06682},
	abstract = {Self-supervised learning has recently begun to rival supervised learning on computer vision tasks. Many of the recent approaches have been based on contrastive instance discrimination (CID), in which the network is trained to recognize two augmented versions of the same instance (a query and positive) while discriminating against a pool of other instances (negatives). The learned representation is then used on downstream tasks such as image classification. Using methodology from MoCo v2 (Chen et al., 2020), we divided negatives by their difficulty for a given query and studied which difficulty ranges were most important for learning useful representations. We found a minority of negatives -- the hardest 5\% -- were both necessary and sufficient for the downstream task to reach nearly full accuracy. Conversely, the easiest 95\% of negatives were unnecessary and insufficient. Moreover, the very hardest 0.1\% of negatives were unnecessary and sometimes detrimental. Finally, we studied the properties of negatives that affect their hardness, and found that hard negatives were more semantically similar to the query, and that some negatives were more consistently easy or hard than we would expect by chance. Together, our results indicate that negatives vary in importance and that CID may benefit from more intelligent negative treatment.},
	urldate = {2021-06-09},
	journal = {arXiv:2010.06682 [cs, eess]},
	author = {Cai, Tiffany Tianhui and Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.06682},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{wu_conditional_2020,
	title = {Conditional {Negative} {Sampling} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2010.02037},
	abstract = {Recent methods for learning unsupervised visual representations, dubbed contrastive learning, optimize the noise-contrastive estimation (NCE) bound on mutual information between two views of an image. NCE uses randomly sampled negative examples to normalize the objective. In this paper, we show that choosing difficult negatives, or those more similar to the current instance, can yield stronger representations. To do this, we introduce a family of mutual information estimators that sample negatives conditionally -- in a "ring" around each positive. We prove that these estimators lower-bound mutual information, with higher bias but lower variance than NCE. Experimentally, we find our approach, applied on top of existing models (IR, CMC, and MoCo) improves accuracy by 2-5\% points in each case, measured by linear evaluation on four standard image datasets. Moreover, we find continued benefits when transferring features to a variety of new image distributions from the Meta-Dataset collection and to a variety of downstream tasks such as object detection, instance segmentation, and keypoint detection.},
	urldate = {2021-06-09},
	journal = {arXiv:2010.02037 [cs, stat]},
	author = {Wu, Mike and Mosse, Milan and Zhuang, Chengxu and Yamins, Daniel and Goodman, Noah},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.02037},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wu_mutual_2020,
	title = {On {Mutual} {Information} in {Contrastive} {Learning} for {Visual} {Representations}},
	url = {http://arxiv.org/abs/2005.13149},
	abstract = {In recent years, several unsupervised, "contrastive" learning algorithms in vision have been shown to learn representations that perform remarkably well on transfer tasks. We show that this family of algorithms maximizes a lower bound on the mutual information between two or more "views" of an image where typical views come from a composition of image augmentations. Our bound generalizes the InfoNCE objective to support negative sampling from a restricted region of "difficult" contrasts. We find that the choice of negative samples and views are critical to the success of these algorithms. Reformulating previous learning objectives in terms of mutual information also simplifies and stabilizes them. In practice, our new objectives yield representations that outperform those learned with previous approaches for transfer to classification, bounding box detection, instance segmentation, and keypoint detection. \% experiments show that choosing more difficult negative samples results in a stronger representation, outperforming those learned with IR, LA, and CMC in classification, bounding box detection, instance segmentation, and keypoint detection. The mutual information framework provides a unifying comparison of approaches to contrastive learning and uncovers the choices that impact representation learning.},
	urldate = {2021-06-09},
	journal = {arXiv:2005.13149 [cs, stat]},
	author = {Wu, Mike and Zhuang, Chengxu and Mosse, Milan and Yamins, Daniel and Goodman, Noah},
	month = jun,
	year = {2020},
	note = {arXiv: 2005.13149},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jain_contrastive_2021,
	title = {Contrastive {Code} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2007.04973},
	abstract = {Recent work learns contextual representations of source code by reconstructing tokens from their context. For downstream semantic understanding tasks like summarizing code in English, these representations should ideally capture program functionality. However, we show that the popular reconstruction-based BERT model is sensitive to source code edits, even when the edits preserve semantics. We propose ContraCode: a contrastive pre-training task that learns code functionality, not form. ContraCode pre-trains a neural network to identify functionally similar variants of a program among many non-equivalent distractors. We scalably generate these variants using an automated source-to-source compiler as a form of data augmentation. Contrastive pre-training improves JavaScript summarization and TypeScript type inference accuracy by 2\% to 13\%. We also propose a new zero-shot JavaScript code clone detection dataset, showing that ContraCode is both more robust and semantically meaningful. On it, we outperform RoBERTa by 39\% AUROC in an adversarial setting and up to 5\% on natural code.},
	language = {en},
	urldate = {2021-06-09},
	journal = {arXiv:2007.04973 [cs, stat]},
	author = {Jain, Paras and Jain, Ajay and Zhang, Tianjun and Abbeel, Pieter and Gonzalez, Joseph E. and Stoica, Ion},
	month = apr,
	year = {2021},
	note = {arXiv: 2007.04973},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering, Statistics - Machine Learning},
}

@book{kakade_multi-view_nodate,
	title = {Multi-{View} {Regression} via {Canonical} {Correlation} {Analysis}},
	abstract = {Abstract. In the multi-view regression problem, we have a regression problem where the input variable (which is a real vector) can be partitioned into two different views, where it is assumed that either view of the input is sufficient to make accurate predictions — this is essentially (a significantly weaker version of) the co-training assumption for the regression problem. We provide a semi-supervised algorithm which first uses unlabeled data to learn a norm (or, equivalently, a kernel) and then uses labeled data in a ridge regression algorithm (with this induced norm) to provide the predictor. The unlabeled data is used via canonical correlation analysis (CCA, which is a closely related to PCA for two random variables) to derive an appropriate norm over functions. We are able to characterize the intrinsic dimensionality of the subsequent ridge regression problem (which uses this norm) by the correlation coefficients provided by CCA in a rather simple expression. Interestingly, the norm used by the ridge regression algorithm is derived from CCA, unlike in standard kernel methods where a special apriori norm is assumed (i.e. a Banach space is assumed). We discuss how this result shows that unlabeled data can decrease the sample complexity. 1},
	author = {Kakade, Sham M. and Foster, Dean P.},
}

@article{lee_predicting_2020,
	title = {Predicting {What} {You} {Already} {Know} {Helps}: {Provable} {Self}-{Supervised} {Learning}},
	shorttitle = {Predicting {What} {You} {Already} {Know} {Helps}},
	url = {http://arxiv.org/abs/2008.01064},
	abstract = {Self-supervised representation learning solves auxiliary prediction tasks (known as pretext tasks), that do not require labeled data, to learn semantic representations. These pretext tasks are created solely using the input features, such as predicting a missing image patch, recovering the color channels of an image from context, or predicting missing words, yet predicting this \$known{\textbackslash} \$information helps in learning representations effective for downstream prediction tasks. This paper posits a mechanism based on conditional independence to formalize how solving certain pretext tasks can learn representations that provably decreases the sample complexity of downstream supervised tasks. Formally, we quantify how approximate independence between the components of the pretext task (conditional on the label and latent variables) allows us to learn representations that can solve the downstream task with drastically reduced sample complexity by just training a linear layer on top of the learned representation.},
	urldate = {2021-06-09},
	journal = {arXiv:2008.01064 [cs, stat]},
	author = {Lee, Jason D. and Lei, Qi and Saunshi, Nikunj and Zhuo, Jiacheng},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.01064},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{federici_information-theoretic_2021,
	title = {An {Information}-theoretic {Approach} to {Distribution} {Shifts}},
	url = {http://arxiv.org/abs/2106.03783},
	abstract = {Safely deploying machine learning models to the real world is often a challenging process. Models trained with data obtained from a specific geographic location tend to fail when queried with data obtained elsewhere, agents trained in a simulation can struggle to adapt when deployed in the real world or novel environments, and neural networks that are fit to a subset of the population might carry some selection bias into their decision process. In this work, we describe the problem of data shift from a novel information-theoretic perspective by (i) identifying and describing the different sources of error, (ii) comparing some of the most promising objectives explored in the recent domain generalization, and fair classification literature. From our theoretical analysis and empirical evaluation, we conclude that the model selection procedure needs to be guided by careful considerations regarding the observed data, the factors used for correction, and the structure of the data-generating process.},
	urldate = {2021-06-08},
	journal = {arXiv:2106.03783 [cs, math]},
	author = {Federici, Marco and Tomioka, Ryota and Forré, Patrick},
	month = jun,
	year = {2021},
	note = {arXiv: 2106.03783},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning},
}

@inproceedings{blum_combining_1998,
	title = {Combining {Labeled} and {Unlabeled} {Data} with {Co}-{Training}},
	url = {https://doi.org/10.1145/279943.279962},
	doi = {10.1145/279943.279962},
	booktitle = {Proceedings of the {Eleventh} {Annual} {Conference} on {Computational} {Learning} {Theory}, {COLT} 1998, {Madison}, {Wisconsin}, {USA}, {July} 24-26, 1998},
	publisher = {ACM},
	author = {Blum, Avrim and Mitchell, Tom M.},
	editor = {Bartlett, Peter L. and Mansour, Yishay},
	year = {1998},
	pages = {92--100},
}

@article{wei_theoretical_2021,
	title = {{THEORETICAL} {ANALYSIS} {OF} {SELF}-{TRAINING} {WITH} {DEEP} {NETWORKS} {ON} {UNLABELED} {DATA}},
	abstract = {Self-training algorithms, which train a model to ﬁt pseudolabels predicted by another previously-learned model, have been very successful for learning with unlabeled data using neural networks. However, the current theoretical understanding of self-training only applies to linear models. This work provides a uniﬁed theoretical analysis of self-training with deep networks for semi-supervised learning, unsupervised domain adaptation, and unsupervised learning. At the core of our analysis is a simple but realistic “expansion” assumption, which states that a lowprobability subset of the data must expand to a neighborhood with large probability relative to the subset. We also assume that neighborhoods of examples in different classes have minimal overlap. We prove that under these assumptions, the minimizers of population objectives based on self-training and input-consistency regularization will achieve high accuracy with respect to ground-truth labels. By using off-the-shelf generalization bounds, we immediately convert this result to sample complexity guarantees for neural nets that are polynomial in the margin and Lipschitzness. Our results help explain the empirical successes of recently proposed self-training algorithms which use input consistency regularization.},
	language = {en},
	author = {Wei, Colin and Shen, Kendrick and Chen, Yining and Ma, Tengyu},
	year = {2021},
	pages = {30},
}

@article{khosla_supervised_2020,
	title = {Supervised {Contrastive} {Learning}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d89a66c7c80a29b1bdbab0f2a1a94af8-Abstract.html},
	language = {en},
	urldate = {2021-06-06},
	journal = {Advances in Neural Information Processing Systems},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	year = {2020},
	pages = {18661--18673},
}

@inproceedings{he_momentum_2020,
	address = {Seattle, WA, USA},
	title = {Momentum {Contrast} for {Unsupervised} {Visual} {Representation} {Learning}},
	isbn = {978-1-72817-168-5},
	url = {https://ieeexplore.ieee.org/document/9157636/},
	doi = {10.1109/CVPR42600.2020.00975},
	abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning [29] as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-ﬂy that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classiﬁcation. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
	language = {en},
	urldate = {2021-06-06},
	booktitle = {2020 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	month = jun,
	year = {2020},
	pages = {9726--9735},
}

@article{ermolov_whitening_2021,
	title = {Whitening for {Self}-{Supervised} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2007.06346},
	abstract = {Most of the current self-supervised representation learning (SSL) methods are based on the contrastive loss and the instance-discrimination task, where augmented versions of the same image instance ("positives") are contrasted with instances extracted from other images ("negatives"). For the learning to be effective, many negatives should be compared with a positive pair, which is computationally demanding. In this paper, we propose a different direction and a new loss function for SSL, which is based on the whitening of the latent-space features. The whitening operation has a "scattering" effect on the batch samples, avoiding degenerate solutions where all the sample representations collapse to a single point. Our solution does not require asymmetric networks and it is conceptually simple. Moreover, since negatives are not needed, we can extract multiple positive pairs from the same image instance. The source code of the method and of all the experiments is available at: https://github.com/htdt/self-supervised.},
	urldate = {2021-06-05},
	journal = {arXiv:2007.06346 [cs, stat]},
	author = {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu},
	month = may,
	year = {2021},
	note = {arXiv: 2007.06346},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bardes_vicreg_2021,
	title = {{VICReg}: {Variance}-{Invariance}-{Covariance} {Regularization} for {Self}-{Supervised} {Learning}},
	shorttitle = {{VICReg}},
	url = {http://arxiv.org/abs/2105.04906},
	abstract = {Recent self-supervised methods for image representation learning are based on maximizing the agreement between embedding vectors from different views of the same image. A trivial solution is obtained when the encoder outputs constant vectors. This collapse problem is often avoided through implicit biases in the learning architecture, that often lack a clear justification or interpretation. In this paper, we introduce VICReg (Variance-Invariance-Covariance Regularization), a method that explicitly avoids the collapse problem with a simple regularization term on the variance of the embeddings along each dimension individually. VICReg combines the variance term with a decorrelation mechanism based on redundancy reduction and covariance regularization, and achieves results on par with the state of the art on several downstream tasks. In addition, we show that incorporating our new variance term into other methods helps stabilize the training and leads to performance improvements.},
	urldate = {2021-06-05},
	journal = {arXiv:2105.04906 [cs]},
	author = {Bardes, Adrien and Ponce, Jean and LeCun, Yann},
	month = may,
	year = {2021},
	note = {arXiv: 2105.04906},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@book{noauthor_minimax_nodate,
	title = {Minimax {Optimal} {Density} {Estimation}},
}

@article{chen_intriguing_2020,
	title = {Intriguing {Properties} of {Contrastive} {Losses}},
	url = {http://arxiv.org/abs/2011.02803},
	abstract = {Contrastive loss and its variants have become very popular recently for learning visual representations without supervision. In this work, we first generalize the standard contrastive loss based on cross entropy to a broader family of losses that share an abstract form of \${\textbackslash}mathcal\{L\}\_\{{\textbackslash}text\{alignment\}\} + {\textbackslash}lambda {\textbackslash}mathcal\{L\}\_\{{\textbackslash}text\{distribution\}\}\$, where hidden representations are encouraged to (1) be aligned under some transformations/augmentations, and (2) match a prior distribution of high entropy. We show that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head, and the temperature scaling (\${\textbackslash}tau\$) widely used in the standard contrastive loss is (within a range) inversely related to the weighting (\${\textbackslash}lambda\$) between the two loss terms. We then study an intriguing phenomenon of feature suppression among competing features shared acros augmented views, such as "color distribution" vs "object class". We construct datasets with explicit and controllable competing features, and show that, for contrastive learning, a few bits of easy-to-learn shared features could suppress, and even fully prevent, the learning of other sets of competing features. Interestingly, this characteristic is much less detrimental in autoencoders based on a reconstruction loss. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features than others, while one may wish that a network would learn all competing features as much as its capacity allows.},
	urldate = {2021-06-05},
	journal = {arXiv:2011.02803 [cs, stat]},
	author = {Chen, Ting and Li, Lala},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.02803},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{wen_toward_2021,
	title = {Toward {Understanding} the {Feature} {Learning} {Process} of {Self}-supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2105.15134},
	abstract = {How can neural networks trained by contrastive learning extract features from the unlabeled data? Why does contrastive learning usually need much stronger data augmentations than supervised learning to ensure good representations? These questions involve both the optimization and statistical aspects of deep learning, but can hardly be answered by analyzing supervised learning, where the target functions are the highest pursuit. Indeed, in self-supervised learning, it is inevitable to relate to the optimization/generalization of neural networks to how they can encode the latent structures in the data, which we refer to as the {\textbackslash}textit\{feature learning process\}. In this work, we formally study how contrastive learning learns the feature representations for neural networks by analyzing its feature learning process. We consider the case where our data are comprised of two types of features: the more semantically aligned sparse features which we want to learn from, and the other dense features we want to avoid. Theoretically, we prove that contrastive learning using {\textbackslash}textbf\{ReLU\} networks provably learns the desired sparse features if proper augmentations are adopted. We present an underlying principle called {\textbackslash}textbf\{feature decoupling\} to explain the effects of augmentations, where we theoretically characterize how augmentations can reduce the correlations of dense features between positive samples while keeping the correlations of sparse features intact, thereby forcing the neural networks to learn from the self-supervision of sparse features. Empirically, we verified that the feature decoupling principle matches the underlying mechanism of contrastive learning in practice.},
	urldate = {2021-06-05},
	journal = {arXiv:2105.15134 [cs, stat]},
	author = {Wen, Zixin and Li, Yuanzhi},
	month = may,
	year = {2021},
	note = {arXiv: 2105.15134},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{du_few-shot_2021,
	title = {Few-{Shot} {Learning} via {Learning} the {Representation}, {Provably}},
	url = {http://arxiv.org/abs/2002.09434},
	abstract = {This paper studies few-shot learning via representation learning, where one uses \$T\$ source tasks with \$n\_1\$ data per task to learn a representation in order to reduce the sample complexity of a target task for which there is only \$n\_2 ({\textbackslash}ll n\_1)\$ data. Specifically, we focus on the setting where there exists a good {\textbackslash}emph\{common representation\} between source and target, and our goal is to understand how much of a sample size reduction is possible. First, we study the setting where this common representation is low-dimensional and provide a fast rate of \$O{\textbackslash}left({\textbackslash}frac\{{\textbackslash}mathcal\{C\}{\textbackslash}left({\textbackslash}Phi{\textbackslash}right)\}\{n\_1T\} + {\textbackslash}frac\{k\}\{n\_2\}{\textbackslash}right)\$; here, \${\textbackslash}Phi\$ is the representation function class, \${\textbackslash}mathcal\{C\}{\textbackslash}left({\textbackslash}Phi{\textbackslash}right)\$ is its complexity measure, and \$k\$ is the dimension of the representation. When specialized to linear representation functions, this rate becomes \$O{\textbackslash}left({\textbackslash}frac\{dk\}\{n\_1T\} + {\textbackslash}frac\{k\}\{n\_2\}{\textbackslash}right)\$ where \$d ({\textbackslash}gg k)\$ is the ambient input dimension, which is a substantial improvement over the rate without using representation learning, i.e. over the rate of \$O{\textbackslash}left({\textbackslash}frac\{d\}\{n\_2\}{\textbackslash}right)\$. This result bypasses the \${\textbackslash}Omega({\textbackslash}frac\{1\}\{T\})\$ barrier under the i.i.d. task assumption, and can capture the desired property that all \$n\_1T\$ samples from source tasks can be {\textbackslash}emph\{pooled\} together for representation learning. Next, we consider the setting where the common representation may be high-dimensional but is capacity-constrained (say in norm); here, we again demonstrate the advantage of representation learning in both high-dimensional linear regression and neural network learning. Our results demonstrate representation learning can fully utilize all \$n\_1T\$ samples from source tasks.},
	urldate = {2021-06-05},
	journal = {arXiv:2002.09434 [cs, math, stat]},
	author = {Du, Simon S. and Hu, Wei and Kakade, Sham M. and Lee, Jason D. and Lei, Qi},
	month = mar,
	year = {2021},
	note = {arXiv: 2002.09434},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{appalaraju_towards_2020,
	title = {Towards {Good} {Practices} in {Self}-supervised {Representation} {Learning}},
	url = {http://arxiv.org/abs/2012.00868},
	abstract = {Self-supervised representation learning has seen remarkable progress in the last few years. More recently, contrastive instance learning has shown impressive results compared to its supervised learning counterparts. However, even with the ever increased interest in contrastive instance learning, it is still largely unclear why these methods work so well. In this paper, we aim to unravel some of the mysteries behind their success, which are the good practices. Through an extensive empirical analysis, we hope to not only provide insights but also lay out a set of best practices that led to the success of recent work in self-supervised representation learning.},
	urldate = {2021-06-05},
	journal = {arXiv:2012.00868 [cs]},
	author = {Appalaraju, Srikar and Zhu, Yi and Xie, Yusheng and Fehérvári, István},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.00868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{purushwalkam_demystifying_2020,
	title = {Demystifying {Contrastive} {Self}-{Supervised} {Learning}: {Invariances}, {Augmentations} and {Dataset} {Biases}},
	volume = {33},
	shorttitle = {Demystifying {Contrastive} {Self}-{Supervised} {Learning}},
	url = {https://papers.nips.cc/paper/2020/hash/22f791da07b0d8a2504c2537c560001c-Abstract.html},
	language = {en},
	urldate = {2021-06-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Purushwalkam, Senthil and Gupta, Abhinav},
	year = {2020},
	pages = {3407--3418},
}

@inproceedings{chuang_debiased_2020,
	title = {Debiased {Contrastive} {Learning}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/63c3ddcc7b23daa1e42dc41f9a44a873-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Chuang, Ching-Yao and Robinson, Joshua and Lin, Yen-Chen and Torralba, Antonio and Jegelka, Stefanie},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@article{tian_what_2020,
	title = {What {Makes} for {Good} {Views} for {Contrastive} {Learning}?},
	volume = {33},
	url = {https://papers.nips.cc/paper/2020/hash/4c2e5eaae9152079b9e95845750bb9ab-Abstract.html},
	language = {en},
	urldate = {2021-06-05},
	journal = {Advances in Neural Information Processing Systems},
	author = {Tian, Yonglong and Sun, Chen and Poole, Ben and Krishnan, Dilip and Schmid, Cordelia and Isola, Phillip},
	year = {2020},
	pages = {6827--6839},
}

@inproceedings{asano_critical_2020,
	title = {A critical analysis of self-supervision, or what we can learn from a single image},
	url = {https://openreview.net/forum?id=B1esx6EYvr},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year = {2020},
}

@article{zimmermann_contrastive_2021,
	title = {Contrastive {Learning} {Inverts} the {Data} {Generating} {Process}},
	url = {http://arxiv.org/abs/2102.08850},
	abstract = {Contrastive learning has recently seen tremendous success in self-supervised learning. So far, however, it is largely unclear why the learned representations generalize so effectively to a large variety of downstream tasks. We here prove that feedforward models trained with objectives belonging to the commonly used InfoNCE family learn to implicitly invert the underlying generative model of the observed data. While the proofs make certain statistical assumptions about the generative model, we observe empirically that our findings hold even if these assumptions are severely violated. Our theory highlights a fundamental connection between contrastive learning, generative modeling, and nonlinear independent component analysis, thereby furthering our understanding of the learned representations as well as providing a theoretical foundation to derive more effective contrastive losses.},
	urldate = {2021-06-05},
	journal = {arXiv:2102.08850 [cs]},
	author = {Zimmermann, Roland S. and Sharma, Yash and Schneider, Steffen and Bethge, Matthias and Brendel, Wieland},
	month = may,
	year = {2021},
	note = {arXiv: 2102.08850},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{nozawa_pac-bayesian_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{PAC}-{Bayesian} {Contrastive} {Unsupervised} {Representation} {Learning}},
	volume = {124},
	url = {http://proceedings.mlr.press/v124/nozawa20a.html},
	booktitle = {Proceedings of the {Thirty}-{Sixth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}, {UAI} 2020, virtual online, {August} 3-6, 2020},
	publisher = {AUAI Press},
	author = {Nozawa, Kento and Germain, Pascal and Guedj, Benjamin},
	editor = {Adams, Ryan P. and Gogate, Vibhav},
	year = {2020},
	pages = {21--30},
}

@article{merad_about_2020,
	title = {About contrastive unsupervised representation learning for classification and its convergence},
	url = {http://arxiv.org/abs/2012.01064},
	abstract = {Contrastive representation learning has been recently proved to be very efficient for self-supervised training. These methods have been successfully used to train encoders which perform comparably to supervised training on downstream classification tasks. A few works have started to build a theoretical framework around contrastive learning in which guarantees for its performance can be proven. We provide extensions of these results to training with multiple negative samples and for multiway classification. Furthermore, we provide convergence guarantees for the minimization of the contrastive training error with gradient descent of an overparametrized deep neural encoder, and provide some numerical experiments that complement our theoretical findings},
	urldate = {2021-06-05},
	journal = {arXiv:2012.01064 [cs, stat]},
	author = {Merad, Ibrahim and Yu, Yiyang and Bacry, Emmanuel and Gaïffas, Stéphane},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.01064},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bansal_for_2020,
	title = {For self-supervised learning, {Rationality} implies generalization, provably},
	url = {http://arxiv.org/abs/2010.08508},
	abstract = {We prove a new upper bound on the generalization gap of classifiers that are obtained by first using self-supervision to learn a representation \$r\$ of the training data, and then fitting a simple (e.g., linear) classifier \$g\$ to the labels. Specifically, we show that (under the assumptions described below) the generalization gap of such classifiers tends to zero if \${\textbackslash}mathsf\{C\}(g) {\textbackslash}ll n\$, where \${\textbackslash}mathsf\{C\}(g)\$ is an appropriately-defined measure of the simple classifier \$g\$'s complexity, and \$n\$ is the number of training samples. We stress that our bound is independent of the complexity of the representation \$r\$. We do not make any structural or conditional-independence assumptions on the representation-learning task, which can use the same training dataset that is later used for classification. Rather, we assume that the training procedure satisfies certain natural noise-robustness (adding small amount of label noise causes small degradation in performance) and rationality (getting the wrong label is not better than getting no label at all) conditions that widely hold across many standard architectures. We show that our bound is non-vacuous for many popular representation-learning based classifiers on CIFAR-10 and ImageNet, including SimCLR, AMDIM and MoCo.},
	urldate = {2021-06-05},
	journal = {arXiv:2010.08508 [cs, stat]},
	author = {Bansal, Yamini and Kaplun, Gal and Barak, Boaz},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08508},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@inproceedings{huai_deep_2019,
	address = {Macao, China},
	title = {Deep {Metric} {Learning}: {The} {Generalization} {Analysis} and an {Adaptive} {Algorithm}},
	isbn = {978-0-9992411-4-1},
	shorttitle = {Deep {Metric} {Learning}},
	url = {https://www.ijcai.org/proceedings/2019/352},
	doi = {10.24963/ijcai.2019/352},
	abstract = {As an effective way to learn a distance metric between pairs of samples, deep metric learning (DML) has drawn signiﬁcant attention in recent years. The key idea of DML is to learn a set of hierarchical nonlinear mappings using deep neural networks, and then project the data samples into a new feature space for comparing or matching. Although DML has achieved practical success in many applications, there is no existing work that theoretically analyzes the generalization error bound for DML, which can measure how good a learned DML model is able to perform on unseen data. In this paper, we try to ﬁll up this research gap and derive the generalization error bound for DML. Additionally, based on the derived generalization bound, we propose a novel DML method (called ADroDML), which can adaptively learn the retention rates for the DML models with dropout in a theoretically justiﬁed way. Compared with existing DML works that require predeﬁned retention rates, ADroDML can learn the retention rates in an optimal way and achieve better performance. We also conduct experiments on realworld datasets to verify the ﬁndings derived from the generalization error bound and demonstrate the effectiveness of the proposed adaptive DML method.},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the {Twenty}-{Eighth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Huai, Mengdi and Xue, Hongfei and Miao, Chenglin and Yao, Liuyi and Su, Lu and Chen, Changyou and Zhang, Aidong},
	month = aug,
	year = {2019},
	pages = {2535--2541},
}

@inproceedings{wang_multitask_2019,
	title = {Multitask {Metric} {Learning}: {Theory} and {Algorithm}},
	shorttitle = {Multitask {Metric} {Learning}},
	url = {http://proceedings.mlr.press/v89/wang19f.html},
	abstract = {In this paper, we study the problem of multitask metric learning (mtML). We first examine the generalization bound of the regularized mtML formulation based on the notion of algorithmic stability, ...},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wang, Boyu and Zhang, Hejia and Liu, Peng and Shen, Zebang and Pineau, Joelle},
	month = apr,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {3362--3371},
}

@article{suarez_tutorial_2021,
	title = {A tutorial on distance metric learning: {Mathematical} foundations, algorithms, experimental analysis, prospects and challenges},
	volume = {425},
	url = {https://doi.org/10.1016/j.neucom.2020.08.017},
	doi = {10.1016/j.neucom.2020.08.017},
	journal = {Neurocomputing},
	author = {Suárez, Juan-Luis and García, Salvador and Herrera, Francisco},
	year = {2021},
	pages = {300--322},
}

@inproceedings{wang_understanding_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Understanding {Contrastive} {Representation} {Learning} through {Alignment} and {Uniformity} on the {Hypersphere}},
	volume = {119},
	url = {http://proceedings.mlr.press/v119/wang20k.html},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}, {ICML} 2020, 13-18 {July} 2020, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Wang, Tongzhou and Isola, Phillip},
	year = {2020},
	pages = {9929--9939},
}

@inproceedings{frosst_analyzing_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Analyzing and {Improving} {Representations} with the {Soft} {Nearest} {Neighbor} {Loss}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/frosst19a.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Frosst, Nicholas and Papernot, Nicolas and Hinton, Geoffrey E.},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	pages = {2012--2020},
}

@inproceedings{salakhutdinov_learning_2007,
	title = {Learning a {Nonlinear} {Embedding} by {Preserving} {Class} {Neighbourhood} {Structure}},
	url = {http://proceedings.mlr.press/v2/salakhutdinov07a.html},
	abstract = {We show how to pretrain and fine-tune a multilayer neural network to learn a nonlinear transformation from the input space to a lowdimensional feature space in which K-nearest neighbour classificat...},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Artificial {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Salakhutdinov, Ruslan and Hinton, Geoff},
	month = mar,
	year = {2007},
	note = {ISSN: 1938-7228},
	pages = {412--419},
}

@inproceedings{gutmann_noise-contrastive_2010,
	title = {Noise-contrastive estimation: {A} new estimation principle for unnormalized statistical models},
	shorttitle = {Noise-contrastive estimation},
	url = {http://proceedings.mlr.press/v9/gutmann10a.html},
	abstract = {We present a new estimation principle for parameterized statistical models. The idea is to perform nonlinear logistic regression to discriminate between the observed data and some artificially gene...},
	language = {en},
	urldate = {2021-06-05},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {JMLR Workshop and Conference Proceedings},
	author = {Gutmann, Michael and Hyvärinen, Aapo},
	month = mar,
	year = {2010},
	note = {ISSN: 1938-7228},
	pages = {297--304},
}

@article{weng_contrastive_2021,
	title = {Contrastive {Representation} {Learning}},
	url = {https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html},
	journal = {lilianweng.github.io/lil-log},
	author = {Weng, Lilian},
	year = {2021},
}

@article{weng_self-supervised_2019,
	title = {Self-{Supervised} {Representation} {Learning}},
	url = {https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html},
	journal = {lilianweng.github.io/lil-log},
	author = {Weng, Lilian},
	year = {2019},
}

@inproceedings{tosh_contrastive_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Contrastive learning, multi-view redundancy, and linear models},
	volume = {132},
	url = {http://proceedings.mlr.press/v132/tosh21a.html},
	booktitle = {Algorithmic {Learning} {Theory}, 16-19 {March} 2021, {Virtual} {Conference}, {Worldwide}},
	publisher = {PMLR},
	author = {Tosh, Christopher and Krishnamurthy, Akshay and Hsu, Daniel},
	editor = {Feldman, Vitaly and Ligett, Katrina and Sabato, Sivan},
	year = {2021},
	pages = {1179--1206},
}

@article{teng_can_2021,
	title = {Can {Pretext}-{Based} {Self}-{Supervised} {Learning} {Be} {Boosted} by {Downstream} {Data}? {A} {Theoretical} {Analysis}},
	shorttitle = {Can {Pretext}-{Based} {Self}-{Supervised} {Learning} {Be} {Boosted} by {Downstream} {Data}?},
	url = {http://arxiv.org/abs/2103.03568},
	abstract = {Pretext-based self-supervised learning aims to learn the semantic representation via a handcrafted pretext task over unlabeled data and then use the learned representation for downstream prediction tasks. It is proved that pretext-based self-supervised learning can effectively reduce the sample complexity of downstream tasks under Conditional Independence (CI) between the components of the pretext task conditional on the downstream label. However, the downstream sample complexity will get much worse if the CI condition does not hold. One interesting question is whether we can make the CI condition hold by using downstream data to refine the unlabeled data to boost self-supervised learning. At first glance, one might think that seeing downstream data in advance would always boost the downstream performance. However, we show that it is not intuitively true and point out that in some cases, it will hurt the final performance instead. In particular, we prove both model-free and model-dependent lower bounds of the number of downstream samples used for data refinement. Moreover, we conduct several experiments on both synthetic and real-world datasets to verify our theoretical results.},
	urldate = {2021-06-05},
	journal = {arXiv:2103.03568 [cs, stat]},
	author = {Teng, Jiaye and Huang, Weiran and He, Haowei},
	month = may,
	year = {2021},
	note = {arXiv: 2103.03568},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tsai_self-supervised_2021,
	title = {Self-supervised {Learning} from a {Multi}-view {Perspective}},
	url = {http://arxiv.org/abs/2006.05576},
	abstract = {As a subset of unsupervised representation learning, self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as object detection and image captioning. Many proposed approaches for self-supervised learning follow naturally a multi-view perspective, where the input (e.g., original images) and the self-supervised signals (e.g., augmented images) can be seen as two redundant views of the data. Building from this multi-view perspective, this paper provides an information-theoretical framework to better understand the properties that encourage successful self-supervised learning. Specifically, we demonstrate that self-supervised learned representations can extract task-relevant information and discard task-irrelevant information. Our theoretical framework paves the way to a larger space of self-supervised learning objective design. In particular, we propose a composite objective that bridges the gap between prior contrastive and predictive learning objectives, and introduce an additional objective term to discard task-irrelevant information. To verify our analysis, we conduct controlled experiments to evaluate the impact of the composite objectives. We also explore our framework's empirical generalization beyond the multi-view perspective, where the cross-view redundancy may not be clearly observed.},
	urldate = {2021-06-05},
	journal = {arXiv:2006.05576 [cs, stat]},
	author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
	month = mar,
	year = {2021},
	note = {arXiv: 2006.05576},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{saunshi_mathematical_2021,
	title = {A {Mathematical} {Exploration} of {Why} {Language} {Models} {Help} {Solve} {Downstream} {Tasks}},
	url = {http://arxiv.org/abs/2010.03648},
	abstract = {Autoregressive language models, pretrained using large text corpora to do well on next word prediction, have been successful at solving many downstream tasks, even with zero-shot usage. However, there is little theoretical understanding of this success. This paper initiates a mathematical study of this phenomenon for the downstream task of text classification by considering the following questions: (1) What is the intuitive connection between the pretraining task of next word prediction and text classification? (2) How can we mathematically formalize this connection and quantify the benefit of language modeling? For (1), we hypothesize, and verify empirically, that classification tasks of interest can be reformulated as sentence completion tasks, thus making language modeling a meaningful pretraining task. With a mathematical formalization of this hypothesis, we make progress towards (2) and show that language models that are \${\textbackslash}epsilon\$-optimal in cross-entropy (log-perplexity) learn features that can linearly solve such classification tasks with \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{{\textbackslash}epsilon\})\$ error, thus demonstrating that doing well on language modeling can be beneficial for downstream tasks. We experimentally verify various assumptions and theoretical findings, and also use insights from the analysis to design a new objective function that performs well on some classification tasks.},
	urldate = {2021-06-05},
	journal = {arXiv:2010.03648 [cs, stat]},
	author = {Saunshi, Nikunj and Malladi, Sadhika and Arora, Sanjeev},
	month = apr,
	year = {2021},
	note = {arXiv: 2010.03648},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{robinson_contrastive_2021,
	title = {Contrastive {Learning} with {Hard} {Negative} {Samples}},
	url = {http://arxiv.org/abs/2010.04592},
	abstract = {How can you sample good negative examples for contrastive learning? We argue that, as with metric learning, contrastive learning of representations benefits from hard negative samples (i.e., points that are difficult to distinguish from an anchor point). The key challenge toward using hard negatives is that contrastive methods must remain unsupervised, making it infeasible to adopt existing negative sampling strategies that use true similarity information. In response, we develop a new family of unsupervised sampling methods for selecting hard negative samples where the user can control the hardness. A limiting case of this sampling results in a representation that tightly clusters each class, and pushes different classes as far apart as possible. The proposed method improves downstream performance across multiple modalities, requires only few additional lines of code to implement, and introduces no computational overhead.},
	urldate = {2021-06-05},
	journal = {arXiv:2010.04592 [cs, stat]},
	author = {Robinson, Joshua and Chuang, Ching-Yao and Sra, Suvrit and Jegelka, Stefanie},
	month = jan,
	year = {2021},
	note = {arXiv: 2010.04592},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{tian_understanding_2021,
	title = {Understanding {Self}-supervised {Learning} with {Dual} {Deep} {Networks}},
	url = {http://arxiv.org/abs/2010.00578},
	abstract = {We propose a novel theoretical framework to understand contrastive self-supervised learning (SSL) methods that employ dual pairs of deep ReLU networks (e.g., SimCLR). First, we prove that in each SGD update of SimCLR with various loss functions, including simple contrastive loss, soft Triplet loss and InfoNCE loss, the weights at each layer are updated by a {\textbackslash}emph\{covariance operator\} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations. To further study what role the covariance operator plays and which features are learned in such a process, we model data generation and augmentation processes through a {\textbackslash}emph\{hierarchical latent tree model\} (HLTM) and prove that the hidden neurons of deep ReLU networks can learn the latent variables in HLTM, despite the fact that the network receives {\textbackslash}emph\{no direct supervision\} from these unobserved latent variables. This leads to a provable emergence of hierarchical features through the amplification of initially random selectivities through contrastive SSL. Extensive numerical studies justify our theoretical findings. Code is released in https://github.com/facebookresearch/luckmatters/tree/master/ssl.},
	urldate = {2021-06-05},
	journal = {arXiv:2010.00578 [cs, stat]},
	author = {Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
	month = feb,
	year = {2021},
	note = {arXiv: 2010.00578},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xu_multi-view_2015,
	title = {Multi-{View} {Intact} {Space} {Learning}},
	volume = {37},
	url = {https://doi.org/10.1109/TPAMI.2015.2417578},
	doi = {10.1109/TPAMI.2015.2417578},
	number = {12},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Xu, Chang and Tao, Dacheng and Xu, Chao},
	year = {2015},
	pages = {2531--2544},
}

@article{sun_pac-bayes_2017,
	title = {{PAC}-{Bayes} analysis of multi-view learning},
	volume = {35},
	url = {https://doi.org/10.1016/j.inffus.2016.09.008},
	doi = {10.1016/j.inffus.2016.09.008},
	journal = {Inf. Fusion},
	author = {Sun, Shiliang and Shawe-Taylor, John and Mao, Liang},
	year = {2017},
	pages = {117--131},
}

@article{harris_array_2020,
	title = {Array programming with {NumPy}},
	volume = {585},
	url = {https://doi.org/10.1038/s41586-020-2649-2},
	doi = {10.1038/s41586-020-2649-2},
	journal = {Nat.},
	author = {Harris, Charles R. and Millman, K. Jarrod and Walt, Stéfan van der and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and Kerkwijk, Marten H. van and Brett, Matthew and Haldane, Allan and Río, Jaime Fernández del and Wiebe, Mark and Peterson, Pearu and Gérard-Marchant, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
	year = {2020},
	pages = {357--362},
}

@article{takeuchi_characterizations_1975,
	title = {Characterizations of {Prediction} {Sufficiency} ({Adequacy}) in {Terms} of {Risk} {Functions}},
	volume = {3},
	issn = {0090-5364},
	url = {https://www.jstor.org/stable/3035531},
	abstract = {Prediction sufficiency (adequacy), as it is usually defined in terms of conditional expectations, does imply "real" prediction sufficiency; i.e. sufficiency in terms of risk functions. The converse holds provided we permit the loss to depend on the unknown parameter. This is no longer true if we insist on loss functions which do not involve the unknown parameter. Conditional independence still holds but ordinary sufficiency may fail. If, however, we require equivalence of risk functions, then ordinary sufficiency and, consequently, prediction sufficiency follows.},
	number = {4},
	urldate = {2021-06-04},
	journal = {The Annals of Statistics},
	author = {Takeuchi, Kei and Akahira, Masafumi},
	year = {1975},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {1018--1024},
}

@book{berger_rate_1971,
	series = {Prentice-{Hall} electrical engineering series},
	title = {Rate {Distortion} {Theory}: {A} {Mathematical} {Basis} for {Data} {Compression}},
	isbn = {978-0-13-753103-5},
	url = {https://books.google.ch/books?id=-HV1QgAACAAJ},
	publisher = {Prentice-Hall},
	author = {Berger, T.},
	year = {1971},
	lccn = {75148254},
}

@article{iri_fine_2019,
	title = {Fine {Asymptotics} for {Universal} {One}-to-{One} {Compression} of {Parametric} {Sources}},
	volume = {65},
	url = {https://doi.org/10.1109/TIT.2019.2898659},
	doi = {10.1109/TIT.2019.2898659},
	number = {4},
	journal = {IEEE Trans. Inf. Theory},
	author = {Iri, Nematollah and Kosut, Oliver},
	year = {2019},
	pages = {2442--2458},
}

@article{hayashi_minimum_2018,
	title = {Minimum {Rates} of {Approximate} {Sufficient} {Statistics}},
	volume = {64},
	url = {https://doi.org/10.1109/TIT.2017.2775612},
	doi = {10.1109/TIT.2017.2775612},
	number = {2},
	journal = {IEEE Trans. Inf. Theory},
	author = {Hayashi, Masahito and Tan, Vincent Y. F.},
	year = {2018},
	pages = {875--888},
}

@article{jiang_learning_2017,
	title = {{LEARNING} {SUMMARY} {STATISTIC} {FOR} {APPROXIMATE} {BAYESIAN} {COMPUTATION} {VIA} {DEEP} {NEURAL} {NETWORK}},
	volume = {27},
	issn = {1017-0405},
	url = {https://www.jstor.org/stable/26384090},
	abstract = {Approximate Bayesian Computation (ABC) methods are used to approximate posterior distributions in models with unknown or computationally intractable likelihoods. Both the accuracy and computational efficiency of ABC depend on the choice of summary statistic, but outside of special cases where the optimal summary statistics are known, it is unclear which guiding principles can be used to construct effective summary statistics. In this paper we explore the possibility of automating the process of constructing summary statistics by training deep neural networks to predict the parameters from artificially generated data: the resulting summary statistics are approximately posterior means of the parameters. With minimal model-specific tuning, our method constructs summary statistics for the Ising model and the moving-average model, which match or exceed theoretically-motivated summary statistics in terms of the accuracies of the resulting posteriors.},
	number = {4},
	urldate = {2021-06-03},
	journal = {Statistica Sinica},
	author = {Jiang, Bai and Wu, Tung-Yu and Zheng, Charles and Wong, Wing H.},
	year = {2017},
	note = {Publisher: Institute of Statistical Science, Academia Sinica},
	pages = {1595--1618},
}

@inproceedings{soatto_modeling_2016,
	title = {Modeling {Visual} {Representations}: {Defining} {Properties} and {Deep} {Approximations}},
	url = {http://arxiv.org/abs/1411.7676},
	booktitle = {4th {International} {Conference} on {Learning} {Representations}, {ICLR} 2016, {San} {Juan}, {Puerto} {Rico}, {May} 2-4, 2016, {Conference} {Track} {Proceedings}},
	author = {Soatto, Stefano and Chiuso, Alessandro},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2016},
}

@inproceedings{cvitkovic_minimal_2019,
	title = {Minimal {Achievable} {Sufficient} {Statistic} {Learning}},
	url = {http://proceedings.mlr.press/v97/cvitkovic19a.html},
	abstract = {We introduce Minimal Achievable Sufficient Statistic (MASS) Learning, a machine learning training objective for which the minima are minimal sufficient statistics with respect to a class of functio...},
	language = {en},
	urldate = {2021-06-03},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cvitkovic, Milan and Koliander, Günther},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498},
	pages = {1465--1474},
}

@article{skibinsky_adequate_1967,
	title = {Adequate {Subfields} and {Sufficiency}},
	volume = {38},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-38/issue-1/Adequate-Subfields-and-Sufficiency/10.1214/aoms/1177699065.full},
	doi = {10.1214/aoms/1177699065},
	abstract = {The Annals of Mathematical Statistics},
	number = {1},
	urldate = {2021-06-03},
	journal = {The Annals of Mathematical Statistics},
	author = {Skibinsky, Morris},
	month = feb,
	year = {1967},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {155--161},
}

@article{bahadur_sufficiency_1954,
	title = {Sufficiency and {Statistical} {Decision} {Functions}},
	volume = {25},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-25/issue-3/Sufficiency-and-Statistical-Decision-Functions/10.1214/aoms/1177728715.full},
	doi = {10.1214/aoms/1177728715},
	abstract = {This paper contains an account, in abstract terms, of sufficiency and of its role in statistical decision problems. The study of sufficiency in abstract terms was initiated by Halmos and Savage [1], and the present paper, although self-contained, is to be regarded as a continuation of their work. The main objects of the paper are to show that the justification for the use of sufficient statistics in statistical methodology which is sketched in the final section of [1] is valid under certain quite general conditions, and to extend this justification to the case of sequential experiments. The paper falls into two parts of which the first (Sections 2-7) is mainly expository and provides an account of the theory of sufficiency in the nonsequential case. The second part (Sections 8-11) then extends the theory to sequential experiments.},
	number = {3},
	urldate = {2021-06-03},
	journal = {The Annals of Mathematical Statistics},
	author = {Bahadur, R. R.},
	month = sep,
	year = {1954},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {423--462},
}

@article{halmos_application_1949,
	title = {Application of the {Radon}-{Nikodym} {Theorem} to the {Theory} of {Sufficient} {Statistics}},
	volume = {20},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-20/issue-2/Application-of-the-Radon-Nikodym-Theorem-to-the-Theory-of/10.1214/aoms/1177730032.full},
	doi = {10.1214/aoms/1177730032},
	abstract = {The body of this paper is written in terms of very general and abstract ideas which have been popular in pure mathematical work on the theory of probability for the last two or three decades. It seems to us that these ideas, so fruitful in pure mathematics, have something to contribute to mathematical statistics also, and this paper is an attempt to illustrate the sort of contribution we have in mind. The purpose of generality here is not to solve immediate practical problems, but rather to capture the logical essence of an important concept (sufficient statistic), and in particular to disentangle that concept from such ideas as Euclidean space, dimensionality, partial differentiation, and the distinction between continuous and discrete distributions, which seem to us extraneous. In accordance with these principles the center of the stage is occupied by a completely abstract sample space--that is a set \$X\$ of objects \$x\$, to be thought of as possible outcomes of an experimental program, distributed according to an unknown one of a certain set of probability measures. Perhaps the most familiar concrete example in statistics is the one in which \$X\$ is \$n\$ dimensional Cartesian space, the points of which represent \$n\$ independent observations of a normally distributed random variable with unknown parameters, and in which the probability measures considered are those induced by the various common normal distributions of the individual observations. A statistic is defined, as usual, to be a function \$T\$ of the outcome, whose values, however, are not necessarily real numbers but may themselves be abstract entities. Thus, in the concrete example, the entire set of \$n\$ observations, or, less trivially, the sequence of all sample moments about the origin are statistics with values in an \$n\$ dimensional and in an infinite dimensional space respectively. Another illuminating and very general example of a statistic may be obtained as follows. Suppose that the outcomes of two not necessarily statistically independent programs are thought of as one united outcome--then the outcome \$T\$ of the first program alone is a statistic relative to the united program. A technical measure theoretic result, known as the Radon-Nikodym theorem, is important in the study of statistics such as \$T\$. It is, for example, essential to the very definition of the basic concept of conditional probability of a subset \$E\$ of \$X\$ given a value \$y\$ of \$T\$. The statistic \$T\$ is called sufficient for the given set \${\textbackslash}mathcal\{M\}\$ of probability measures if (somewhat loosely speaking) the conditional probability of a subset \$E\$ of \$X\$ given a value \$y\$ of \$T\$ is the same for every probability measure in \${\textbackslash}mathcal\{M\}\$. It is, for instance, well known that the sample mean and variance together form a sufficient statistic for the measures described in the concrete example. The theory of sufficiency is in an especially satisfactory state for the case in which the set \${\textbackslash}mathcal\{M\}\$ of probability measures satisfies a certain condition described by the technical term dominated. A set \${\textbackslash}mathcal\{M\}\$ of probability measures is called dominated if each measure in the set may be expressed as the indefinite integral of a density function with respect to a fixed measure which is not itself necessarily in the set. It is easy to verify that both classical extremes, commonly referred to as the discrete and continuous cases, are dominated. One possible formulation of the principal result concerning sufficiency for dominated sets is a direct generalization to the abstract case of the well known Fisher-Neyman result: \$T\$ is sufficient if and only if the densities can be written as products of two factors, the first of which depends on the outcome through \$T\$ only and the second of which is independent of the unknown measure. Another way of phrasing this result is to say that \$T\$ is sufficient if and only if the likelihood ratio of every pair of measures in \${\textbackslash}mathcal\{M\}\$ depends on the outcome through \$T\$ only. The latter formulation makes sense even in the not necessarily dominated case but unfortunately it is not true in that case. The situation can be patched up somewhat by introducing a weaker notion called pairwise sufficiency. In ordinary statistical parlance one often speaks of a statistic sufficient for some of several parameters. The abstract results mentioned above can undoubtedly be extended to treat this concept.},
	number = {2},
	urldate = {2021-06-03},
	journal = {The Annals of Mathematical Statistics},
	author = {Halmos, Paul R. and Savage, L. J.},
	month = jun,
	year = {1949},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {225--241},
}

@article{lee_perceptual_2012,
	title = {Perceptual {Video} {Compression}: {A} {Survey}},
	volume = {6},
	issn = {1932-4553},
	shorttitle = {Perceptual {Video} {Compression}},
	url = {https://infoscience.epfl.ch/record/184275},
	doi = {10.1109/Jstsp.2012.2215006},
	abstract = {With the advances in understanding perceptual properties of the human visual system and constructing their computational models, efforts toward incorporating human perceptual mechanisms in video compression to achieve maximal perceptual quality have received great attention. This paper thoroughly reviews the recent advances of perceptual video compression mainly in terms of the three major components, namely, perceptual model definition, implementation of coding, and performance evaluation. Furthermore, open research issues and challenges are discussed in order to provide perspectives for future research trends.},
	language = {en},
	number = {6},
	urldate = {2021-05-26},
	journal = {Ieee Journal Of Selected Topics In Signal Processing},
	author = {Lee, Jong-Seok and Ebrahimi, Touradj},
	year = {2012},
	note = {Number: ARTICLE
Publisher: Ieee-Inst Electrical Electronics Engineers Inc},
	pages = {684},
}

@article{johnston_transform_1988,
	title = {Transform coding of audio signals using perceptual noise criteria},
	volume = {6},
	issn = {1558-0008},
	doi = {10.1109/49.608},
	abstract = {A 4-b/sample transform coder is designed using a psychoacoustically derived noise-making threshold that is based on the short-term spectrum of the signal. The coder has been tested in a formal subjective test involving a wide selection of monophonic audio inputs. The signals used in the test were of 15-kHz bandwidth, sampled at 32 kHz. The bit rate of the resulting coder was 128 kb/s. The subjective test shows that the coded signal could not be distinguished from the original at that bit rate. Subsequent informal work suggests that a bit rate of 96 kb/s may maintain transparency for the set of inputs used in the test.{\textless}{\textgreater}},
	number = {2},
	journal = {IEEE Journal on Selected Areas in Communications},
	author = {Johnston, J.D.},
	month = feb,
	year = {1988},
	note = {Conference Name: IEEE Journal on Selected Areas in Communications},
	keywords = {Bandwidth, Bit rate, Delay, Digital audio broadcasting, Dynamic range, Error analysis, Phase change materials, Satellite broadcasting, Testing, Transform coding},
	pages = {314--323},
}

@article{pan_digital_1993,
	title = {Digital {Audio} {Compression}},
	abstract = {Compared to most digital data types, with the exception of digital video, the data rates associ-ated with uncompressed digital audio are substan-tial. Digital audio compression enables more effi-cient storage and transmission of audio data. The many forms of audio compression techniques offer a range of encoder and decoder complexity, compressed audio quality, and differing amounts of data com-pression. The -law transformation and ADPCM coder are simple approaches with low-complexity, low-compression, and medium audio quality algo-rithms. The MPEG/audio standard is a high-complexity, high-compression, and high audio qual-ity algorithm. These techniques apply to general au-dio signals and are not specifically tuned for speech signals.},
	journal = {Digital Technical Journal},
	author = {Pan, Yen},
	year = {1993},
}

@article{lintott_galaxy_2008,
	title = {Galaxy {Zoo}: morphologies derived from visual inspection of galaxies from the {Sloan} {Digital} {Sky} {Survey}},
	volume = {389},
	issn = {0035-8711},
	shorttitle = {Galaxy {Zoo}},
	url = {http://adsabs.harvard.edu/abs/2008MNRAS.389.1179L},
	doi = {10.1111/j.1365-2966.2008.13689.x},
	abstract = {In order to understand the formation and subsequent evolution of 
galaxies one must first distinguish between the two main morphological
classes of massive systems: spirals and early-type systems. This paper
introduces a project, Galaxy Zoo, which provides visual morphological
classifications for nearly one million galaxies, extracted from the
Sloan Digital Sky Survey (SDSS). This achievement was made possible by
inviting the general public to visually inspect and classify these
galaxies via the internet. The project has obtained more than 4 ×
107 individual classifications made by {\textasciitilde}105
participants. We discuss the motivation and strategy for this project,
and detail how the classifications were performed and processed. We find
that Galaxy Zoo results are consistent with those for subsets of SDSS
galaxies classified by professional astronomers, thus demonstrating that
our data provide a robust morphological catalogue. Obtaining
morphologies by direct visual inspection avoids introducing biases
associated with proxies for morphology such as colour, concentration or
structural parameters. In addition, this catalogue can be used to
directly compare SDSS morphologies with older data sets. The
colour-magnitude diagrams for each morphological class are shown, and we
illustrate how these distributions differ from those inferred using
colour alone as a proxy for morphology.

This publication has been made possible by the participation of more
than 100000 volunteers in the Galaxy Zoo project. Their contributions
are individually acknowledged at
http://www.galaxyzoo.org/Volunteers.aspx

E-mail: cjl@astro.ox.ac.uk (CJL); kevins@astro.ox.ac.uk (KS)},
	urldate = {2021-05-24},
	journal = {Monthly Notices of the Royal Astronomical Society},
	author = {Lintott, Chris J. and Schawinski, Kevin and Slosar, Anže and Land, Kate and Bamford, Steven and Thomas, Daniel and Raddick, M. Jordan and Nichol, Robert C. and Szalay, Alex and Andreescu, Dan and Murray, Phil and Vandenberg, Jan},
	month = sep,
	year = {2008},
	keywords = {cD, galaxies: elliptical and lenticular, galaxies: general, galaxies: spiral, methods: data analysis},
	pages = {1179--1189},
}

@incollection{berger_rate-distortion_2003,
	title = {Rate-{Distortion} {Theory}},
	copyright = {Copyright © 2003 by John Wiley \& Sons, Inc. All rights reserved.},
	isbn = {978-0-471-21928-6},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/0471219282.eot142},
	abstract = {Rate-distortion theory is the branch of information theory that treats compressing the data produced by an information source down to a specified encoding rate that is strictly less than the source's entropy. This necessarily entails some lossiness, or distortion, between the original source data and the best approximation thereto that can be produced on the basis of the encoder's output bits. Rate-distortion theory was introduced in the seminal works written in 1948 and 1959 by C. E. Shannon, the founder of information theory. We describe Shannon's contribution and then trace its subsequent development worldwide. Heavier than usual emphasis is placed on the concept of “matching” a channel to a source in the rate-distortion sense, and also on the analogous matching of a source to a channel. Experimental evidence has been mounting in support of the hypothesis that living organisms often simultaneously achieve both of these matchings when processing their sensory inputs, thereby eliminating the need for the complex encoding and decoding operations that are needed in order to produce an information-theoretically optimum system in the absence of such double matching.},
	language = {en},
	urldate = {2021-05-23},
	booktitle = {Wiley {Encyclopedia} of {Telecommunications}},
	publisher = {American Cancer Society},
	author = {Berger, Toby},
	year = {2003},
	doi = {10.1002/0471219282.eot142},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/0471219282.eot142},
	keywords = {Shannon, bioinformation theory, distortion measure, joint source-channel coding, lossy source coding, rate-distortion},
}

@article{everett_generalized_1963,
	title = {Generalized {Lagrange} {Multiplier} {Method} for {Solving} {Problems} of {Optimum} {Allocation} of {Resources}},
	volume = {11},
	issn = {0030-364X},
	url = {https://pubsonline.informs.org/doi/abs/10.1287/opre.11.3.399},
	doi = {10.1287/opre.11.3.399},
	abstract = {The usefulness of Lagrange multipliers for optimization in the presence of constraints is not limited to differentiable functions. They can be applied to problems of maximizing an arbitrary real valued objective function over any set whatever, subject to bounds on the values of any other finite collection of real valued functions denned on the same set. While the use of the Lagrange multipliers does not guarantee that a solution will necessarily be found for all problems, it is “fail-safe” in the sense that any solution found by their use is a true solution. Since the method is so simple compared to other available methods it is often worth trying first, and succeeds in a surprising fraction of cases. They are particularly well suited to the solution of problems of allocating limited resources among a set of independent activities.},
	number = {3},
	urldate = {2021-05-23},
	journal = {Operations Research},
	author = {Everett, Hugh},
	month = jun,
	year = {1963},
	note = {Publisher: INFORMS},
	pages = {399--417},
}

@inproceedings{bottou_large-scale_2010,
	title = {Large-{Scale} {Machine} {Learning} with {Stochastic} {Gradient} {Descent}},
	url = {https://doi.org/10.1007/978-3-7908-2604-3_16},
	doi = {10.1007/978-3-7908-2604-3_16},
	booktitle = {19th {International} {Conference} on {Computational} {Statistics}, {COMPSTAT} 2010, {Paris}, {France}, {August} 22-27, 2010 - {Keynote}, {Invited} and {Contributed} {Papers}},
	publisher = {Physica-Verlag},
	author = {Bottou, Léon},
	editor = {Lechevallier, Yves and Saporta, Gilbert},
	year = {2010},
	pages = {177--186},
}

@inproceedings{xu_theory_2020,
	title = {A {Theory} of {Usable} {Information} under {Computational} {Constraints}},
	url = {https://openreview.net/forum?id=r1eBeyHFDH},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
	year = {2020},
}

@book{schechter_handbook_1996,
	title = {Handbook of {Analysis} and its {Foundations}},
	publisher = {Academic Press},
	author = {Schechter, Eric},
	year = {1996},
}

@techreport{pages_introduction_2014,
	title = {Introduction to optimal vector quantization and its applications for numerics},
	url = {https://hal.archives-ouvertes.fr/hal-01034196},
	abstract = {We present an introductory survey to optimal vector quantization and its first applications to Numerical Probability and, to a lesser extent to Information Theory and Data Mining. Both theoretical results on the quantization rate of a random vector taking values in R{\textasciicircum}d (equipped with the canonical Euclidean norm) and the learning procedures that allow to design optimal quantizers (CLVQ and Lloyd's I procedures) are presented. We also introduce and investigate the more recent notion of \{{\textbackslash}em greedy quantization\} which may be seen as a sequential optimal quantization. A rate optimal result is established. A brief comparison with Quasi-Monte Carlo method is also carried out.},
	urldate = {2021-05-21},
	author = {Pagès, Gilles},
	month = jul,
	year = {2014},
	keywords = {Competitive Learning Vector Quantization, Feynman-Kac's formula, Lloyd's I algorithm, Optimal vector quantization, Zador's Theorem, greedy quantization, learning algorithms, nearest neighbor search, optimal stopping, partial distance search, quantization tree, quasi-Monte Carlo method, stochastic gradient descent, variational inequality},
}

@book{mac_lane_algebra_1999,
	title = {Algebra},
	isbn = {978-0-8218-1646-2},
	abstract = {This book presents modern algebra from first principles and is accessible to undergraduates or graduates. It combines standard materials and necessary algebraic manipulations with general concepts that clarify meaning and importance. This conceptual approach to algebra starts with a description of algebraic structures by means of axioms chosen to suit the examples, for instance, axioms for groups, rings, fields, lattices, and vector spaces. This axiomatic approach - emphasized by Hilbert and developed in Germany by Noether, Artin, Van der Waerden, et al., in the 1920s - was popularized for the graduate level in the 1940s and 1950s to some degree by the authors' publication of A Survey of Modern Algebra. The present book presents the developments from that time to the first printing of this book. This third edition includes corrections made by the authors.},
	language = {en},
	publisher = {American Mathematical Soc.},
	author = {Mac Lane, Saunders and Birkhoff, Garrett},
	year = {1999},
	note = {Google-Books-ID: L6FENd8GHIUC},
	keywords = {Mathematics / Algebra / General},
}

@article{landauer_how_1986,
	title = {How much {Do} {People} {Remember}? {Some} {Estimates} of the {Quantity} of {Learned} {Information} in {Long}-term {Memory}},
	volume = {10},
	copyright = {© 1986 Cognitive Science Society, Inc.},
	issn = {1551-6709},
	shorttitle = {How much {Do} {People} {Remember}?},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1004_4},
	doi = {https://doi.org/10.1207/s15516709cog1004_4},
	abstract = {How much information from experience does a normal adult remember? The “functional information content” of human memory was estimated in several ways. The methods depend on measured rates of input and loss from very long- term memory and on analyses of the informational demands of human memory-based performance. Estimates ranged around 109 bits. It is speculated that the flexible and creative retrieval of facts by humans is a function of a large ratio of “hardware” capacity to functional storage requirements.},
	language = {en},
	number = {4},
	urldate = {2021-05-18},
	journal = {Cognitive Science},
	author = {Landauer, Thomas K.},
	year = {1986},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1004\_4},
	pages = {477--493},
}

@article{wu_capacity_2016,
	title = {The {Capacity} of {Cognitive} {Control} {Estimated} from a {Perceptual} {Decision} {Making} {Task}},
	volume = {6},
	issn = {2045-2322},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5034293/},
	doi = {10.1038/srep34025},
	abstract = {Cognitive control refers to the processes that permit selection and prioritization of information processing in different cognitive domains to reach the capacity-limited conscious mind. Although previous studies have suggested that the capacity of cognitive control itself is limited, a direct quantification of this capacity has not been attempted. In this behavioral study, we manipulated the information rate of cognitive control by parametrically varying both the uncertainty of stimul measured as information entropy and the exposure time of the stimuli. We used the relationship between the participants’ response accuracy and the information rate of cognitive control (in bits per second, bps) in the model fitting to estimate the capacity of cognitive control. We found that the capacity of cognitive control was approximately 3 to 4 bps, demonstrating that cognitive control as a higher-level function has a remarkably low capacity. This quantification of the capacity of cognitive control may have significant theoretical and clinical implications.},
	urldate = {2021-05-18},
	journal = {Scientific Reports},
	author = {Wu, Tingting and Dufford, Alexander J. and Mackie, Melissa-Ann and Egan, Laura J. and Fan, Jin},
	month = sep,
	year = {2016},
	pmid = {27659950},
	pmcid = {PMC5034293},
}

@article{moscoso_del_prado_martin_macroscopic_2011,
	title = {Macroscopic thermodynamics of reaction times},
	volume = {55},
	issn = {0022-2496},
	url = {https://www.sciencedirect.com/science/article/pii/S0022249611000344},
	doi = {10.1016/j.jmp.2011.04.001},
	abstract = {I present a new interpretation of reaction time (RT) data from behavioural experiments. From a physical perspective, the entropy of the RT distribution–the temporal entropy–provides a model-free estimate of the amount of processing performed by the cognitive system. This new measure shifts the focus from the conventional interpretation of RTs being either long or short, into their distribution being more or less complex in terms of entropy. I introduce the formulation of the theory, followed by an empirical test using a large database of human RTs in lexical processing tasks. Using the measure, I obtain estimates of the processing loads to individual stimuli (i.e., words), as well as estimates for the overall rate at which the system processes information in these tasks. The relation between the temporal entropy and the RTs can be captured by a simple linear equation. I argue that this equation constitutes the equivalent of a ‘phase diagram’ of a task, providing indications about the different mechanisms that are at play in it, and locating critical points signalling the transitions between these different mechanisms. The results suggest an adaptive system that adjusts its operational processing speed to the demands of each individual stimulus. This finding is in contradiction with a generalization of Hick’s Law positing a relatively constant processing speed within an experimental context.},
	language = {en},
	number = {4},
	urldate = {2021-05-18},
	journal = {Journal of Mathematical Psychology},
	author = {Moscoso del Prado Martín, Fermín},
	month = aug,
	year = {2011},
	keywords = {Cognitive processing speed, Entropy, Hick’s Law, Phase transition, Reaction time},
	pages = {302--319},
}

@article{hafez-kolahi_rate-distortion_2021,
	title = {Rate-{Distortion} {Analysis} of {Minimum} {Excess} {Risk} in {Bayesian} {Learning}},
	url = {http://arxiv.org/abs/2105.04180},
	abstract = {Minimum Excess Risk (MER) in Bayesian learning is defined as the difference between the minimum expected loss achievable when learning from data and the minimum expected loss that could be achieved if the underlying parameter \$W\$ was observed. In this paper, we build upon and extend the recent results of (Xu \& Raginsky, 2020) to analyze the MER in Bayesian learning and derive information-theoretic bounds on it. We formulate the problem as a (constrained) rate-distortion optimization and show how the solution can be bounded above and below by two other rate-distortion functions that are easier to study. The lower bound represents the minimum possible excess risk achievable by {\textbackslash}emph\{any\} process using \$R\$ bits of information from the parameter \$W\$. For the upper bound, the optimization is further constrained to use \$R\$ bits from the training set, a setting which relates MER to information-theoretic bounds on the generalization gap in frequentist learning. We derive information-theoretic bounds on the difference between these upper and lower bounds and show that they can provide order-wise tight rates for MER. This analysis gives more insight into the information-theoretic nature of Bayesian learning as well as providing novel bounds.},
	urldate = {2021-05-18},
	journal = {arXiv:2105.04180 [cs, math]},
	author = {Hafez-Kolahi, Hassan and Moniri, Behrad and Kasaei, Shohreh and Baghshah, Mahdieh Soleymani},
	month = may,
	year = {2021},
	note = {arXiv: 2105.04180},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning},
}

@article{monteleoni_climate_2013,
	title = {Climate {Informatics}: {Accelerating} {Discovering} in {Climate} {Science} with {Machine} {Learning}},
	volume = {15},
	issn = {1558-366X},
	shorttitle = {Climate {Informatics}},
	doi = {10.1109/MCSE.2013.50},
	abstract = {Given the impact of climate change, understanding the climate system is an international priority. The goal of climate informatics is to inspire collaboration between climate scientists and data scientists, in order to develop tools to analyze complex and ever-growing amounts of observed and simulated climate data, and thereby bridge the gap between data and understanding. Here, recent climate informatics work is presented, along with details of some of the remaining challenges.},
	number = {5},
	journal = {Computing in Science Engineering},
	author = {Monteleoni, Claire and Schmidt, Gavin A. and McQuade, Scott},
	month = sep,
	year = {2013},
	note = {Conference Name: Computing in Science Engineering},
	keywords = {Atmospheric measurements, Climate change, Informatics, Machine learning, Meteorology, climate informatics, climate science, data mining, machine learning, statistics},
	pages = {32--40},
}

@article{sebestyen_applicability_2021,
	title = {The {Applicability} of {Big} {Data} in {Climate} {Change} {Research}: {The} {Importance} of {System} of {Systems} {Thinking}},
	volume = {9},
	issn = {2296-665X},
	shorttitle = {The {Applicability} of {Big} {Data} in {Climate} {Change} {Research}},
	url = {https://www.frontiersin.org/articles/10.3389/fenvs.2021.619092/full#B192},
	doi = {10.3389/fenvs.2021.619092},
	abstract = {The aim of this paper is to provide an overview of the interrelationship between data science and climate studies, as well as describes how sustainability climate issues can be managed using the Big Data tools. Climate-related Big Data articles are analysed and categorized, which revealed the increasing number of applications of data-driven solutions in specific areas, however, broad integrative analyses are gaining less of a focus. Our major objective is to highlight the potential in the System of Systems (SoS) theorem, as the synergies between diverse disciplines and research ideas must be explored to gain a comprehensive overview of the issue. Data and systems science enables a large amount of heterogeneous data to be integrated and simulation models developed, while considering socio-environmental interrelations in parallel. The improved knowledge integration offered by the System of Systems thinking or climate computing has been demonstrated by analysing the possible inter-linkages of the latest Big Data application papers. The analysis highlights how data and models focusing on the specific areas of sustainability can be bridged to study the complex problems of climate change.},
	language = {English},
	urldate = {2021-05-18},
	journal = {Frontiers in Environmental Science},
	author = {Sebestyén, Viktor and Czvetkó, Tímea and Abonyi, János},
	year = {2021},
	note = {Publisher: Frontiers},
	keywords = {Climate Change, big data, climate computing, data science, modelling, systems of systems},
}

@article{ehteshami_bejnordi_diagnostic_2017,
	title = {Diagnostic {Assessment} of {Deep} {Learning} {Algorithms} for {Detection} of {Lymph} {Node} {Metastases} in {Women} {With} {Breast} {Cancer}},
	volume = {318},
	issn = {0098-7484},
	url = {https://doi.org/10.1001/jama.2017.14585},
	doi = {10.1001/jama.2017.14585},
	abstract = {Application of deep learning algorithms to whole-slide pathology images can potentially improve diagnostic accuracy and efficiency.Assess the performance of automated deep learning algorithms at detecting metastases in hematoxylin and eosin–stained tissue sections of lymph nodes of women with breast cancer and compare it with pathologists’ diagnoses in a diagnostic setting.Researcher challenge competition (CAMELYON16) to develop automated solutions for detecting lymph node metastases (November 2015-November 2016). A training data set of whole-slide images from 2 centers in the Netherlands with (n = 110) and without (n = 160) nodal metastases verified by immunohistochemical staining were provided to challenge participants to build algorithms. Algorithm performance was evaluated in an independent test set of 129 whole-slide images (49 with and 80 without metastases). The same test set of corresponding glass slides was also evaluated by a panel of 11 pathologists with time constraint (WTC) from the Netherlands to ascertain likelihood of nodal metastases for each slide in a flexible 2-hour session, simulating routine pathology workflow, and by 1 pathologist without time constraint (WOTC).Deep learning algorithms submitted as part of a challenge competition or pathologist interpretation.The presence of specific metastatic foci and the absence vs presence of lymph node metastasis in a slide or image using receiver operating characteristic curve analysis. The 11 pathologists participating in the simulation exercise rated their diagnostic confidence as definitely normal, probably normal, equivocal, probably tumor, or definitely tumor.The area under the receiver operating characteristic curve (AUC) for the algorithms ranged from 0.556 to 0.994. The top-performing algorithm achieved a lesion-level, true-positive fraction comparable with that of the pathologist WOTC (72.4\% [95\% CI, 64.3\%-80.4\%]) at a mean of 0.0125 false-positives per normal whole-slide image. For the whole-slide image classification task, the best algorithm (AUC, 0.994 [95\% CI, 0.983-0.999]) performed significantly better than the pathologists WTC in a diagnostic simulation (mean AUC, 0.810 [range, 0.738-0.884]; P \&lt; .001). The top 5 algorithms had a mean AUC that was comparable with the pathologist interpreting the slides in the absence of time constraints (mean AUC, 0.960 [range, 0.923-0.994] for the top 5 algorithms vs 0.966 [95\% CI, 0.927-0.998] for the pathologist WOTC).In the setting of a challenge competition, some deep learning algorithms achieved better diagnostic performance than a panel of 11 pathologists participating in a simulation exercise designed to mimic routine pathology workflow; algorithm performance was comparable with an expert pathologist interpreting whole-slide images without time constraints. Whether this approach has clinical utility will require evaluation in a clinical setting.},
	number = {22},
	journal = {JAMA},
	author = {Ehteshami Bejnordi, Babak and Veta, Mitko and Johannes van Diest, Paul and van Ginneken, Bram and Karssemeijer, Nico and Litjens, Geert and van der Laak, Jeroen A. W. M. and Consortium, {and} the CAMELYON16},
	year = {2017},
	pages = {2199--2210},
}

@article{srivastava_dropout_2014,
	title = {Dropout: a simple way to prevent neural networks from overfitting},
	volume = {15},
	url = {http://dl.acm.org/citation.cfm?id=2670313},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Srivastava, Nitish and Hinton, Geoffrey E. and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
	year = {2014},
	pages = {1929--1958},
}

@inproceedings{loshchilov_decoupled_2019,
	title = {Decoupled {Weight} {Decay} {Regularization}},
	url = {https://openreview.net/forum?id=Bkg6RiCqY7},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Loshchilov, Ilya and Hutter, Frank},
	year = {2019},
}

@inproceedings{nilsback_automated_2008,
	title = {Automated {Flower} {Classification} over a {Large} {Number} of {Classes}},
	url = {https://doi.org/10.1109/ICVGIP.2008.47},
	doi = {10.1109/ICVGIP.2008.47},
	booktitle = {Sixth {Indian} {Conference} on {Computer} {Vision}, {Graphics} \& {Image} {Processing}, {ICVGIP} 2008, {Bhubaneswar}, {India}, 16-19 {December} 2008},
	publisher = {IEEE Computer Society},
	author = {Nilsback, Maria-Elena and Zisserman, Andrew},
	year = {2008},
	pages = {722--729},
}

@article{li_learning_2007,
	title = {Learning generative visual models from few training examples: {An} incremental {Bayesian} approach tested on 101 object categories},
	volume = {106},
	url = {https://doi.org/10.1016/j.cviu.2005.09.012},
	doi = {10.1016/j.cviu.2005.09.012},
	number = {1},
	journal = {Comput. Vis. Image Underst.},
	author = {Li, Fei-Fei and Fergus, Robert and Perona, Pietro},
	year = {2007},
	pages = {59--70},
}

@inproceedings{parkhi_cats_2012,
	title = {Cats and {Dogs}},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Parkhi, Omkar M. and Vedaldi, Andrea and Zisserman, Andrew and Jawahar, C. V.},
	year = {2012},
}

@inproceedings{krause_3d_2013,
	title = {{3D} {Object} {Representations} for {Fine}-{Grained} {Categorization}},
	url = {https://doi.org/10.1109/ICCVW.2013.77},
	doi = {10.1109/ICCVW.2013.77},
	booktitle = {2013 {IEEE} {International} {Conference} on {Computer} {Vision} {Workshops}, {ICCV} {Workshops} 2013, {Sydney}, {Australia}, {December} 1-8, 2013},
	publisher = {IEEE Computer Society},
	author = {Krause, Jonathan and Stark, Michael and Deng, Jia and Fei-Fei, Li},
	year = {2013},
	pages = {554--561},
}

@article{runting_opportunities_2020,
	title = {Opportunities for big data in conservation and sustainability},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-15870-0},
	doi = {10.1038/s41467-020-15870-0},
	abstract = {Big data reveals new, stark pictures of the state of our environments. It also reveals ‘bright spots’ amongst the broad pattern of decline and—crucially—the key conditions for these cases. Big data analyses could benefit the planet if tightly coupled with ongoing sustainability efforts.},
	language = {en},
	number = {1},
	urldate = {2021-05-15},
	journal = {Nature Communications},
	author = {Runting, Rebecca K. and Phinn, Stuart and Xie, Zunyi and Venter, Oscar and Watson, James E. M.},
	month = apr,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {2003},
}

@techreport{krizhevsky_learning_2009,
	title = {Learning multiple layers of features from tiny images},
	author = {Krizhevsky, Alex},
	year = {2009},
}

@article{dosovitskiy_image_2020,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	volume = {abs/2010.11929},
	url = {https://arxiv.org/abs/2010.11929},
	journal = {CoRR},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	year = {2020},
	note = {\_eprint: 2010.11929},
}

@article{begaint_compressai_2020,
	title = {{CompressAI}: a {PyTorch} library and evaluation platform for end-to-end compression research},
	volume = {abs/2011.03029},
	url = {https://arxiv.org/abs/2011.03029},
	journal = {CoRR},
	author = {Bégaint, Jean and Racapé, Fabien and Feltman, Simon and Pushparaja, Akshay},
	year = {2020},
	note = {\_eprint: 2011.03029},
}

@inproceedings{sridharan_information_2008,
	title = {An {Information} {Theoretic} {Framework} for {Multi}-view {Learning}},
	url = {http://colt2008.cs.helsinki.fi/papers/94-Sridharan.pdf},
	booktitle = {21st {Annual} {Conference} on {Learning} {Theory} - {COLT} 2008, {Helsinki}, {Finland}, {July} 9-12, 2008},
	publisher = {Omnipress},
	author = {Sridharan, Karthik and Kakade, Sham M.},
	editor = {Servedio, Rocco A. and Zhang, Tong},
	year = {2008},
	pages = {403--414},
}

@inproceedings{tschannen_mutual_2020,
	title = {On {Mutual} {Information} {Maximization} for {Representation} {Learning}},
	url = {https://openreview.net/forum?id=rkxoh24FPH},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K. and Gelly, Sylvain and Lucic, Mario},
	year = {2020},
}

@inproceedings{tian_contrastive_2020,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Contrastive {Multiview} {Coding}},
	volume = {12356},
	url = {https://doi.org/10.1007/978-3-030-58621-8_45},
	doi = {10.1007/978-3-030-58621-8_45},
	booktitle = {Computer {Vision} - {ECCV} 2020 - 16th {European} {Conference}, {Glasgow}, {UK}, {August} 23-28, 2020, {Proceedings}, {Part} {XI}},
	publisher = {Springer},
	author = {Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
	editor = {Vedaldi, Andrea and Bischof, Horst and Brox, Thomas and Frahm, Jan-Michael},
	year = {2020},
	pages = {776--794},
}

@inproceedings{west_bottlesum_2019,
	title = {{BottleSum}: {Unsupervised} and {Self}-supervised {Sentence} {Summarization} using the {Information} {Bottleneck} {Principle}},
	url = {https://doi.org/10.18653/v1/D19-1389},
	doi = {10.18653/v1/D19-1389},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing}, {EMNLP}-{IJCNLP} 2019, {Hong} {Kong}, {China}, {November} 3-7, 2019},
	publisher = {Association for Computational Linguistics},
	author = {West, Peter and Holtzman, Ari and Buys, Jan and Choi, Yejin},
	editor = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
	year = {2019},
	pages = {3750--3759},
}

@inproceedings{bachman_learning_2019,
	title = {Learning {Representations} by {Maximizing} {Mutual} {Information} {Across} {Views}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/ddf354219aac374f1d40b7e760ee5bb7-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}},
	author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	pages = {15509--15519},
}

@inproceedings{ma_noise_2018,
	address = {Brussels, Belgium},
	title = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}: {Consistency} and {Statistical} {Efficiency}},
	shorttitle = {Noise {Contrastive} {Estimation} and {Negative} {Sampling} for {Conditional} {Models}},
	url = {https://www.aclweb.org/anthology/D18-1405},
	doi = {10.18653/v1/D18-1405},
	abstract = {Noise Contrastive Estimation (NCE) is a powerful parameter estimation method for log-linear models, which avoids calculation of the partition function or its derivatives at each training step, a computationally demanding step in many cases. It is closely related to negative sampling methods, now widely used in NLP. This paper considers NCE-based estimation of conditional models. Conditional models are frequently encountered in practice; however there has not been a rigorous theoretical analysis of NCE in this setting, and we will argue there are subtle but important questions when generalizing NCE to the conditional case. In particular, we analyze two variants of NCE for conditional models: one based on a classification objective, the other based on a ranking objective. We show that the ranking-based variant of NCE gives consistent parameter estimates under weaker assumptions than the classification-based method; we analyze the statistical efficiency of the ranking-based and classification-based variants of NCE; finally we describe experiments on synthetic data and language modeling showing the effectiveness and tradeoffs of both methods.},
	urldate = {2021-05-13},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ma, Zhuang and Collins, Michael},
	month = oct,
	year = {2018},
	pages = {3698--3707},
}

@article{rhodes_variational_2019,
	title = {Variational {Noise}-{Contrastive} {Estimation}},
	url = {http://arxiv.org/abs/1810.08010},
	abstract = {Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data.},
	urldate = {2021-05-13},
	journal = {arXiv:1810.08010 [cs, stat]},
	author = {Rhodes, Benjamin and Gutmann, Michael},
	month = feb,
	year = {2019},
	note = {arXiv: 1810.08010},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{minnen_joint_2018,
	title = {Joint {Autoregressive} and {Hierarchical} {Priors} for {Learned} {Image} {Compression}},
	url = {https://proceedings.neurips.cc/paper/2018/hash/53edebc543333dfbf7c5933af792c9c4-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2018, {NeurIPS} 2018, {December} 3-8, 2018, {Montréal}, {Canada}},
	author = {Minnen, David and Ballé, Johannes and Toderici, George},
	editor = {Bengio, Samy and Wallach, Hanna M. and Larochelle, Hugo and Grauman, Kristen and Cesa-Bianchi, Nicolò and Garnett, Roman},
	year = {2018},
	pages = {10794--10803},
}

@inproceedings{yang_variational_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Variational {Bayesian} {Quantization}},
	volume = {119},
	url = {http://proceedings.mlr.press/v119/yang20a.html},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}, {ICML} 2020, 13-18 {July} 2020, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Yang, Yibo and Bamler, Robert and Mandt, Stephan},
	year = {2020},
	pages = {10670--10680},
}

@inproceedings{yang_improving_2020,
	title = {Improving {Inference} for {Neural} {Image} {Compression}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/066f182b787111ed4cb65ed437f0855b-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Yang, Yibo and Bamler, Robert and Mandt, Stephan},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@inproceedings{mentzer_high-fidelity_2020,
	title = {High-{Fidelity} {Generative} {Image} {Compression}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/8a50bae297807da9e97722a0b3fd8f27-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Mentzer, Fabian and Toderici, George and Tschannen, Michael and Agustsson, Eirikur},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@book{cover_elements_2006,
	title = {Elements of information theory (2. ed.)},
	isbn = {978-0-471-24195-9},
	publisher = {Wiley},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	year = {2006},
}

@article{shannon_coding_1959,
	title = {Coding theorems for a discrete source with a fidelity criterion},
	volume = {4},
	number = {142-163},
	journal = {IRE Nat. Conv. Rec},
	author = {Shannon, Claude E},
	year = {1959},
	pages = {1},
}

@article{kolmogorov_shannon_1956,
	title = {On the {Shannon} theory of information transmission in the case of continuous signals},
	volume = {2},
	issn = {2168-2712},
	doi = {10.1109/TIT.1956.1056823},
	number = {4},
	journal = {IRE Transactions on Information Theory},
	author = {Kolmogorov, A.},
	month = dec,
	year = {1956},
	note = {Conference Name: IRE Transactions on Information Theory},
	keywords = {Bandwidth, Entropy, Gaussian distribution, Information theory, Mathematics, Particle measurements, Terminology},
	pages = {102--108},
}

@book{pinsker_information_1964,
	edition = {First edition},
	title = {Information and {Information} {Stability} of {Random} {Variables} and {Processes}},
	isbn = {978-0-8162-6804-7},
	abstract = {From the book sleeve: [...] information theory has important applications in the several branches of mathematics which could hardly have been anticipated at the time of its birth. We may mention the entropy variant in ergodic theory, which has settled several problems of long standing, the use of the entropy concepts in discussing various questions centered about Hilbert's thirteenth problem, Linnik's proof of the central limit theorem using the information functional, and a short proof of the equivalence-singularity dichotomy for Gaussian measures. In this book the author develops the concept of information stability, which underlies several of the above-mentioned applications of the theory (as well as the classical coding theorems), and then applies it with particular reference to stationary processes and Gaussian processes.[...]},
	language = {English},
	publisher = {Holden-Day, Inc.},
	author = {Pinsker, M. S.},
	editor = {Feinstein, Amiel},
	month = jan,
	year = {1964},
}

@article{koyama_out--distribution_2020,
	title = {Out-of-{Distribution} {Generalization} with {Maximal} {Invariant} {Predictor}},
	volume = {abs/2008.01883},
	url = {https://arxiv.org/abs/2008.01883},
	journal = {CoRR},
	author = {Koyama, Masanori and Yamaguchi, Shoichiro},
	year = {2020},
	note = {\_eprint: 2008.01883},
}

@inproceedings{minnen_channel-wise_2020,
	title = {Channel-{Wise} {Autoregressive} {Entropy} {Models} for {Learned} {Image} {Compression}},
	url = {https://doi.org/10.1109/ICIP40778.2020.9190935},
	doi = {10.1109/ICIP40778.2020.9190935},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing}, {ICIP} 2020, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {October} 25-28, 2020},
	publisher = {IEEE},
	author = {Minnen, David and Singh, Saurabh},
	year = {2020},
	pages = {3339--3343},
}

@inproceedings{agustsson_generative_2019,
	title = {Generative {Adversarial} {Networks} for {Extreme} {Learned} {Image} {Compression}},
	url = {https://doi.org/10.1109/ICCV.2019.00031},
	doi = {10.1109/ICCV.2019.00031},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision}, {ICCV} 2019, {Seoul}, {Korea} ({South}), {October} 27 - {November} 2, 2019},
	publisher = {IEEE},
	author = {Agustsson, Eirikur and Tschannen, Michael and Mentzer, Fabian and Timofte, Radu and Gool, Luc Van},
	year = {2019},
	pages = {221--231},
}

@article{chen_perceptually_2020,
	title = {Perceptually {Optimizing} {Deep} {Image} {Compression}},
	volume = {abs/2007.02711},
	url = {https://arxiv.org/abs/2007.02711},
	journal = {CoRR},
	author = {Chen, Li-Heng and Bampis, Christos G. and Li, Zhi and Norkin, Andrey and Bovik, Alan C.},
	year = {2020},
	note = {\_eprint: 2007.02711},
}

@article{johnston_computationally_2019,
	title = {Computationally {Efficient} {Neural} {Image} {Compression}},
	volume = {abs/1912.08771},
	url = {http://arxiv.org/abs/1912.08771},
	journal = {CoRR},
	author = {Johnston, Nick and Eban, Elad and Gordon, Ariel and Ballé, Johannes},
	year = {2019},
	note = {\_eprint: 1912.08771},
}

@inproceedings{amraee_compression_2011,
	title = {Compression of {3D} {MRI} images based on symmetry in prediction-error field},
	url = {https://doi.org/10.1109/ICME.2011.6011897},
	doi = {10.1109/ICME.2011.6011897},
	booktitle = {Proceedings of the 2011 {IEEE} {International} {Conference} on {Multimedia} and {Expo}, {ICME} 2011, 11-15 {July}, 2011, {Barcelona}, {Catalonia}, {Spain}},
	publisher = {IEEE Computer Society},
	author = {Amraee, Somaieh and Karimi, Nader and Samavi, Shadrokh and Shirani, Shahram},
	year = {2011},
	pages = {1--6},
}

@article{sanchez_symmetry-based_2009,
	title = {Symmetry-{Based} {Scalable} {Lossless} {Compression} of {3D} {Medical} {Image} {Data}},
	volume = {28},
	issn = {1558-254X},
	doi = {10.1109/TMI.2009.2012899},
	abstract = {We propose a novel symmetry-based technique for scalable lossless compression of 3D medical image data. The proposed method employs the 2D integer wavelet transform to decorrelate the data and an intraband prediction method to reduce the energy of the sub-bands by exploiting the anatomical symmetries typically present in structural medical images. A modified version of the embedded block coder with optimized truncation (EBCOT), tailored according to the characteristics of the data, encodes the residual data generated after prediction to provide resolution and quality scalability. Performance evaluations on a wide range of real 3D medical images show an average improvement of 15\% in lossless compression ratios when compared to other state-of-the art lossless compression methods that also provide resolution and quality scalability including 3D-JPEG2000, JPEG2000, and H.264/AVC intra-coding.},
	number = {7},
	journal = {IEEE Transactions on Medical Imaging},
	author = {Sanchez, V. and Abugharbieh, R. and Nasiopoulos, P.},
	month = jul,
	year = {2009},
	note = {Conference Name: IEEE Transactions on Medical Imaging},
	keywords = {Art, Biomedical imaging, Character generation, Decorrelation, Energy resolution, Image coding, Integer wavelet transform, Performance loss, Prediction methods, Scalability, Wavelet transforms, lossless compression, medical image compression, scalability, symmetry},
	pages = {1062--1072},
}

@article{bairagi_symmetry-based_2015,
	title = {Symmetry-{Based} {Biomedical} {Image} {Compression}},
	volume = {28},
	issn = {0897-1889},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4636716/},
	doi = {10.1007/s10278-015-9779-3},
	abstract = {Image compression techniques aim at reducing the amount of data needed to accurately represent an image, such that the image can be economically transmitted or archived. This paper deals with employing symmetry as a parameter for compression of biomedical images. The approach presented in this paper offers great potential in complete lossless compression of the biomedical image under consideration, with the reconstructed image being mathematically identical to the original image. The method comprises getting rid of the redundant data and encoding the non-redundant data for the purpose of regenerating the image at the receiver section without any observable change in the image data.},
	number = {6},
	urldate = {2021-05-04},
	journal = {Journal of Digital Imaging},
	author = {Bairagi, V. K.},
	month = dec,
	year = {2015},
	pmid = {25708892},
	pmcid = {PMC4636716},
	pages = {718--726},
}

@article{mitra_symmetry_2013,
	title = {Symmetry in {3D} {Geometry}: {Extraction} and {Applications}},
	volume = {32},
	url = {https://doi.org/10.1111/cgf.12010},
	doi = {10.1111/cgf.12010},
	number = {6},
	journal = {Comput. Graph. Forum},
	author = {Mitra, Niloy J. and Pauly, Mark and Wand, Michael and Ceylan, Duygu},
	year = {2013},
	pages = {1--23},
}

@inproceedings{gnutti_representation_2015,
	title = {Representation of signals by local symmetry decomposition},
	booktitle = {2015 23rd {European} {Signal} {Processing} {Conference} ({EUSIPCO})},
	publisher = {IEEE},
	author = {Gnutti, Alessandro and Guerrini, Fabrizio and Leonardi, Riccardo},
	year = {2015},
	pages = {983--987},
}

@inproceedings{cohen_group_2016,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Group {Equivariant} {Convolutional} {Networks}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/cohenc16.html},
	booktitle = {Proceedings of the 33nd {International} {Conference} on {Machine} {Learning}, {ICML} 2016, {New} {York} {City}, {NY}, {USA}, {June} 19-24, 2016},
	publisher = {JMLR.org},
	author = {Cohen, Taco and Welling, Max},
	editor = {Balcan, Maria-Florina and Weinberger, Kilian Q.},
	year = {2016},
	pages = {2990--2999},
}

@inproceedings{kondor_generalization_2018,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On the {Generalization} of {Equivariance} and {Convolution} in {Neural} {Networks} to the {Action} of {Compact} {Groups}},
	volume = {80},
	url = {http://proceedings.mlr.press/v80/kondor18a.html},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}, {ICML} 2018, {Stockholmsmässan}, {Stockholm}, {Sweden}, {July} 10-15, 2018},
	publisher = {PMLR},
	author = {Kondor, Risi and Trivedi, Shubhendu},
	editor = {Dy, Jennifer G. and Krause, Andreas},
	year = {2018},
	pages = {2752--2760},
}

@article{bruna_invariant_2013,
	title = {Invariant {Scattering} {Convolution} {Networks}},
	volume = {35},
	url = {https://doi.org/10.1109/TPAMI.2012.230},
	doi = {10.1109/TPAMI.2012.230},
	number = {8},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Bruna, Joan and Mallat, Stéphane},
	year = {2013},
	pages = {1872--1886},
}

@inproceedings{shawe-taylor_building_1989,
	title = {Building symmetries into feedforward networks},
	abstract = {One of the central tools developed by M. Minsky and S. Papert (1988) was the group invariance theorem. This theorem is concerned with choosing perceptron weights to recognise a predicate that is invariant under a group of permutations of the input. The theorem states that the weights can be chosen to be constant for equivalence classes of predicates under the action of the group. This paper presents this result in a graph theoretic light and then extends consideration to multilayer perceptrons. It is shown that, by choosing a multilayer network in such a way that the action of the group on the input nodes can be extended to the whole network, the invariance of the output under the action of the group can be guaranteed. This greatly reduces the number of degrees of freedom in the training of such a network. An example of using this technique to train a network to recognise isomorphism classes of graphs is given. This compares favourably with previous experiments using standard back-propagation. The connections between the group of symmetries and the network structure are explored and the relation to the problem of graph isomorphism is discussed.{\textless}{\textgreater}},
	booktitle = {1989 {First} {IEE} {International} {Conference} on {Artificial} {Neural} {Networks}, ({Conf}. {Publ}. {No}. 313)},
	author = {Shawe-Taylor, J.},
	month = oct,
	year = {1989},
	keywords = {Graph theory, Neural networks},
	pages = {158--162},
}

@inproceedings{zaheer_deep_2017,
	title = {Deep {Sets}},
	url = {https://proceedings.neurips.cc/paper/2017/hash/f22e4747da1aa27e363d86d40ff442fe-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2017, {December} 4-9, 2017, {Long} {Beach}, {CA}, {USA}},
	author = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Póczos, Barnabás and Salakhutdinov, Ruslan and Smola, Alexander J.},
	editor = {Guyon, Isabelle and Luxburg, Ulrike von and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
	year = {2017},
	pages = {3391--3401},
}

@article{bruna_learning_2013,
	title = {Learning {Stable} {Group} {Invariant} {Representations} with {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1301.3537},
	abstract = {Transformation groups, such as translations or rotations, effectively express part of the variability observed in many recognition problems. The group structure enables the construction of invariant signal representations with appealing mathematical properties, where convolutions, together with pooling operators, bring stability to additive and geometric perturbations of the input. Whereas physical transformation groups are ubiquitous in image and audio applications, they do not account for all the variability of complex signal classes. We show that the invariance properties built by deep convolutional networks can be cast as a form of stable group invariance. The network wiring architecture determines the invariance group, while the trainable filter coefficients characterize the group action. We give explanatory examples which illustrate how the network architecture controls the resulting invariance group. We also explore the principle by which additional convolutional layers induce a group factorization enabling more abstract, powerful invariant representations.},
	urldate = {2021-05-04},
	journal = {arXiv:1301.3537 [cs, math]},
	author = {Bruna, Joan and Szlam, Arthur and LeCun, Yann},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3537},
	keywords = {Computer Science - Artificial Intelligence, Mathematics - Numerical Analysis},
}

@article{wood_representation_1996,
	title = {Representation {Theory} and {Invariant} {Neural} {Networks}},
	volume = {69},
	url = {https://doi.org/10.1016/0166-218X(95)00075-3},
	doi = {10.1016/0166-218X(95)00075-3},
	number = {1-2},
	journal = {Discret. Appl. Math.},
	author = {Wood, Jeffrey and Shawe-Taylor, John},
	year = {1996},
	pages = {33--60},
}

@article{liu_recognizable_2016,
	title = {Recognizable or {Not}: {Towards} {Image} {Semantic} {Quality} {Assessment} for {Compression}},
	volume = {18},
	issn = {1557-2072},
	shorttitle = {Recognizable or {Not}},
	url = {https://doi.org/10.1007/s11220-016-0152-5},
	doi = {10.1007/s11220-016-0152-5},
	abstract = {Traditionally, image compression was optimized for the pixel-wise fidelity or the perceptual quality of the compressed images given a bit-rate budget. But recently, compressed images are more and more utilized for automatic semantic analysis tasks such as recognition and retrieval. For these tasks, we argue that the optimization target of compression is no longer perceptual quality, but the utility of the compressed images in the given automatic semantic analysis task. Accordingly, we propose to evaluate the quality of the compressed images neither at pixel level nor at perceptual level, but at semantic level. In this paper, we make preliminary efforts towards image semantic quality assessment (ISQA), focusing on the task of optical character recognition (OCR) from compressed images. We propose a full-reference ISQA measure by comparing the features extracted from text regions of original and compressed images. We then propose to integrate the ISQA measure into an image compression scheme. Experimental results show that our proposed ISQA measure is much better than PSNR and SSIM in evaluating the semantic quality of compressed images; accordingly, adopting our ISQA measure to optimize compression for OCR leads to significant bit-rate saving compared to using PSNR or SSIM. Moreover, we perform subjective test about text recognition from compressed images, and observe that our ISQA measure has high consistency with subjective recognizability. Our work explores new dimensions in image quality assessment, and demonstrates promising direction to achieve higher compression ratio for specific semantic analysis tasks.},
	language = {en},
	number = {1},
	urldate = {2021-05-04},
	journal = {Sensing and Imaging},
	author = {Liu, Dong and Wang, Dandan and Li, Houqiang},
	month = dec,
	year = {2016},
	pages = {1},
}

@inproceedings{fernandez_tropistic_1988,
	title = {On {Tropistic} {Processing} and {Its} {Applications}},
	url = {https://proceedings.neurips.cc/paper/1987/file/33e75ff09dd601bbe69f351039152189-Paper.pdf},
	urldate = {2021-05-04},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Fernández, Manuel},
	editor = {Anderson, D.},
	year = {1988},
}

@inproceedings{liu_classification-distortion-perception_2019,
	title = {On {The} {Classification}-{Distortion}-{Perception} {Tradeoff}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/6c29793a140a811d0c45ce03c1c93a28-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}},
	author = {Liu, Dong and Zhang, Haochen and Xiong, Zhiwei},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	pages = {1204--1213},
}

@inproceedings{torfason_towards_2018,
	title = {Towards {Image} {Understanding} from {Deep} {Compression} {Without} {Decoding}},
	url = {https://openreview.net/forum?id=HkXWCMbRW},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018, {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Torfason, Robert and Mentzer, Fabian and Agustsson, Eirikur and Tschannen, Michael and Timofte, Radu and Gool, Luc Van},
	year = {2018},
}

@inproceedings{luo_deepsic_2018,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{DeepSIC}: {Deep} {Semantic} {Image} {Compression}},
	volume = {11301},
	url = {https://doi.org/10.1007/978-3-030-04167-0_9},
	doi = {10.1007/978-3-030-04167-0_9},
	booktitle = {Neural {Information} {Processing} - 25th {International} {Conference}, {ICONIP} 2018, {Siem} {Reap}, {Cambodia}, {December} 13-16, 2018, {Proceedings}, {Part} {I}},
	publisher = {Springer},
	author = {Luo, Sihui and Yang, Yezhou and Yin, Yanling and Shen, Chengchao and Zhao, Ya and Song, Mingli},
	editor = {Cheng, Long and Leung, Andrew Chi-Sing and Ozawa, Seiichi},
	year = {2018},
	pages = {96--106},
}

@article{veeling_rotation_2018,
	title = {Rotation {Equivariant} {CNNs} for {Digital} {Pathology}},
	url = {http://arxiv.org/abs/1806.03962},
	abstract = {We propose a new model for digital pathology segmentation, based on the observation that histopathology images are inherently symmetric under rotation and reflection. Utilizing recent findings on rotation equivariant CNNs, the proposed model leverages these symmetries in a principled manner. We present a visual analysis showing improved stability on predictions, and demonstrate that exploiting rotation equivariance significantly improves tumor detection performance on a challenging lymph node metastases dataset. We further present a novel derived dataset to enable principled comparison of machine learning models, in combination with an initial benchmark. Through this dataset, the task of histopathology diagnosis becomes accessible as a challenging benchmark for fundamental machine learning research.},
	urldate = {2021-05-03},
	journal = {arXiv:1806.03962 [cs, stat]},
	author = {Veeling, Bastiaan S. and Linmans, Jasper and Winkens, Jim and Cohen, Taco and Welling, Max},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.03962},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{lin_microsoft_2015,
	title = {Microsoft {COCO}: {Common} {Objects} in {Context}},
	shorttitle = {Microsoft {COCO}},
	url = {http://arxiv.org/abs/1405.0312},
	abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
	urldate = {2021-05-03},
	journal = {arXiv:1405.0312 [cs]},
	author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Bourdev, Lubomir and Girshick, Ross and Hays, James and Perona, Pietro and Ramanan, Deva and Zitnick, C. Lawrence and Dollár, Piotr},
	month = feb,
	year = {2015},
	note = {arXiv: 1405.0312},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@misc{noauthor_linear_nodate,
	title = {Linear {Algebra} by {Georgi} {E}. {Shilov} and {Richard} {A}. {Silverman} {\textbar} {Technical} {Books} {Pdf}},
	url = {https://www.technicalbookspdf.com/linear-algebra-by-georgi-e-shilov-and-richard-a-silverman/},
	language = {en-US},
	urldate = {2021-05-01},
}

@misc{noauthor_canonical_2021,
	title = {Canonical form},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Canonical_form&oldid=1005451625},
	abstract = {In mathematics and computer science, a canonical, normal, or standard form of a mathematical object is a standard way of presenting that object as a mathematical expression. Often, it is one which provides the simplest representation of an object and which allows it to be identified in a unique way. The distinction between "canonical" and "normal" forms varies from subfield to subfield. In most fields, a canonical form specifies a unique representation for every object, while a normal form simply specifies its form, without the requirement of uniqueness.The canonical form of a positive integer in decimal representation is a finite sequence of digits that does not begin with zero. More generally, for a class of objects on which an equivalence relation is defined, a canonical form consists in the choice of a specific object in each class. For example:

Jordan normal form is a canonical form for matrix similarity.
The row echelon form is a canonical form, when one considers as equivalent a matrix and its left product by an invertible matrix.In computer science, and more specifically in computer algebra, when representing mathematical objects in a computer, there are usually many different ways to represent the same object. In this context, a canonical form is a representation such that every object has a unique representation (with canonicalization being the process through which a representation is put into its canonical form). Thus, the equality of two objects can easily be tested by testing the equality of their canonical forms. 
Despite this advantage, canonical forms frequently depend on arbitrary choices (like ordering the variables), which introduce difficulties for testing the equality of two objects resulting on independent computations. Therefore, in computer algebra, normal form is a weaker notion: A normal form is a representation such that zero is uniquely represented. This allows testing for equality by putting the difference of two objects in normal form.
Canonical form can also mean a differential form that is defined in a natural (canonical) way.},
	language = {en},
	urldate = {2021-05-01},
	journal = {Wikipedia},
	month = feb,
	year = {2021},
	note = {Page Version ID: 1005451625},
}

@article{finzi_practical_2021,
	title = {A {Practical} {Method} for {Constructing} {Equivariant} {Multilayer} {Perceptrons} for {Arbitrary} {Matrix} {Groups}},
	url = {http://arxiv.org/abs/2104.09459},
	abstract = {Symmetries and equivariance are fundamental to the generalization of neural networks on domains such as images, graphs, and point clouds. Existing work has primarily focused on a small number of groups, such as the translation, rotation, and permutation groups. In this work we provide a completely general algorithm for solving for the equivariant layers of matrix groups. In addition to recovering solutions from other works as special cases, we construct multilayer perceptrons equivariant to multiple groups that have never been tackled before, including \${\textbackslash}mathrm\{O\}(1,3)\$, \${\textbackslash}mathrm\{O\}(5)\$, \${\textbackslash}mathrm\{Sp\}(n)\$, and the Rubik's cube group. Our approach outperforms non-equivariant baselines, with applications to particle physics and dynamical systems. We release our software library to enable researchers to construct equivariant layers for arbitrary matrix groups.},
	urldate = {2021-04-29},
	journal = {arXiv:2104.09459 [cs, math, stat]},
	author = {Finzi, Marc and Welling, Max and Wilson, Andrew Gordon},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.09459},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Statistics - Machine Learning},
}

@inproceedings{blau_rethinking_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Rethinking {Lossy} {Compression}: {The} {Rate}-{Distortion}-{Perception} {Tradeoff}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/blau19a.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Blau, Yochai and Michaeli, Tomer},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	pages = {675--685},
}

@inproceedings{heeger_model_1995,
	title = {A model of perceptual image fidelity},
	url = {https://doi.org/10.1109/ICIP.1995.537485},
	doi = {10.1109/ICIP.1995.537485},
	booktitle = {Proceedings 1995 {International} {Conference} on {Image} {Processing}, {Washington}, {DC}, {USA}, {October} 23-26, 1995},
	publisher = {IEEE Computer Society},
	author = {Heeger, David J. and Teo, Patrick C.},
	year = {1995},
	keywords = {compression},
	pages = {343--345},
}

@article{heaton_ian_2018,
	title = {Ian {Goodfellow}, {Yoshua} {Bengio}, and {Aaron} {Courville}: {Deep} learning - {The} {MIT} {Press}, 2016, 800 pp, {ISBN}: 0262035618},
	volume = {19},
	url = {https://doi.org/10.1007/s10710-017-9314-z},
	doi = {10.1007/s10710-017-9314-z},
	number = {1-2},
	journal = {Genet. Program. Evolvable Mach.},
	author = {Heaton, Jeff},
	year = {2018},
	pages = {305--307},
}

@article{shorten_survey_2019,
	title = {A survey on {Image} {Data} {Augmentation} for {Deep} {Learning}},
	volume = {6},
	issn = {2196-1115},
	url = {https://doi.org/10.1186/s40537-019-0197-0},
	doi = {10.1186/s40537-019-0197-0},
	abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
	number = {1},
	urldate = {2021-04-28},
	journal = {Journal of Big Data},
	author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
	month = jul,
	year = {2019},
	keywords = {Big data, Data Augmentation, Deep Learning, GANs, Image data},
	pages = {60},
}

@article{beghdadi_survey_nodate,
	title = {A {Survey} of {Perceptual} {Image} {Processing} {Methods}},
	abstract = {Perceptual approaches have been widely used in many areas of visual information processing. This paper presents an overview of perceptual based approaches for image enhancement, segmentation and coding. The paper also provides a brief review of image quality assessment (IQA) methods, which are used to evaluate the performance of visual information processing techniques. The intent of this paper is not to review all the relevant works that have appeared in the literature, but rather to focus on few topics that have been extensively researched and developed over the past few decades. The goal is to present a perspective as broad as possible on this actively evolving domain due to relevant advances in vision research and signal processing. Therefore, for each topic, we identify the main contributions of perceptual approaches and their limitations, and outline how perceptual vision has in uenced current state-of-the-art techniques in image enhancement, segmentation, coding and visual information quality assessment.},
	language = {en},
	author = {Beghdadi, A},
	pages = {45},
}

@article{alemi_vib_2020,
	title = {{VIB} is {Half} {Bayes}},
	url = {http://arxiv.org/abs/2011.08711},
	abstract = {In discriminative settings such as regression and classification there are two random variables at play, the inputs X and the targets Y. Here, we demonstrate that the Variational Information Bottleneck can be viewed as a compromise between fully empirical and fully Bayesian objectives, attempting to minimize the risks due to finite sampling of Y only. We argue that this approach provides some of the benefits of Bayes while requiring only some of the work.},
	urldate = {2021-04-23},
	journal = {arXiv:2011.08711 [cs, stat]},
	author = {Alemi, Alexander A. and Morningstar, Warren R. and Poole, Ben and Fischer, Ian and Dillon, Joshua V.},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.08711},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{xu_minimum_2020,
	title = {Minimum {Excess} {Risk} in {Bayesian} {Learning}},
	url = {http://arxiv.org/abs/2012.14868},
	abstract = {We analyze the best achievable performance of Bayesian learning under generative models by defining and upper-bounding the minimum excess risk (MER): the gap between the minimum expected loss attainable by learning from data and the minimum expected loss that could be achieved if the model realization were known. The definition of MER provides a principled way to define different notions of uncertainties in Bayesian learning, including the aleatoric uncertainty and the minimum epistemic uncertainty. Two methods for deriving upper bounds for the MER are presented. The first method, generally suitable for Bayesian learning with a parametric generative model, upper-bounds the MER by the conditional mutual information between the model parameters and the quantity being predicted given the observed data. It allows us to quantify the rate at which the MER decays to zero as more data becomes available. The second method, particularly suitable for Bayesian learning with a parametric predictive model, relates the MER to the deviation of the posterior predictive distribution from the true predictive model, and further to the minimum estimation error of the model parameters from data. It explicitly shows how the uncertainty in model parameter estimation translates to the MER and to the final prediction uncertainty. We also extend the definition and analysis of MER to the setting with multiple parametric model families and the setting with nonparametric models. Along the discussions we draw some comparisons between the MER in Bayesian learning and the excess risk in frequentist learning.},
	urldate = {2021-04-22},
	journal = {arXiv:2012.14868 [cs, math, stat]},
	author = {Xu, Aolin and Raginsky, Maxim},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.14868},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{connor_representing_2019,
	title = {Representing {Closed} {Transformation} {Paths} in {Encoded} {Network} {Latent} {Space}},
	url = {http://arxiv.org/abs/1912.02644},
	abstract = {Deep generative networks have been widely used for learning mappings from a low-dimensional latent space to a high-dimensional data space. In many cases, data transformations are defined by linear paths in this latent space. However, the Euclidean structure of the latent space may be a poor match for the underlying latent structure in the data. In this work, we incorporate a generative manifold model into the latent space of an autoencoder in order to learn the low-dimensional manifold structure from the data and adapt the latent space to accommodate this structure. In particular, we focus on applications in which the data has closed transformation paths which extend from a starting point and return to nearly the same point. Through experiments on data with natural closed transformation paths, we show that this model introduces the ability to learn the latent dynamics of complex systems, generate transformation paths, and classify samples that belong on the same transformation path.},
	urldate = {2021-04-17},
	journal = {arXiv:1912.02644 [cs, stat]},
	author = {Connor, Marissa and Rozell, Christopher},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.02644},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{yang_groupifyvae_2021,
	title = {{GroupifyVAE}: from {Group}-based {Definition} to {VAE}-based {Unsupervised} {Representation} {Disentanglement}},
	shorttitle = {{GroupifyVAE}},
	url = {http://arxiv.org/abs/2102.10303},
	abstract = {The key idea of the state-of-the-art VAE-based unsupervised representation disentanglement methods is to minimize the total correlation of the latent variable distributions. However, it has been proved that VAE-based unsupervised disentanglement can not be achieved without introducing other inductive bias. In this paper, we address VAE-based unsupervised disentanglement by leveraging the constraints derived from the Group Theory based definition as the non-probabilistic inductive bias. More specifically, inspired by the nth dihedral group (the permutation group for regular polygons), we propose a specific form of the definition and prove its two equivalent conditions: isomorphism and "the constancy of permutations". We further provide an implementation of isomorphism based on two Group constraints: the Abel constraint for the exchangeability and Order constraint for the cyclicity. We then convert them into a self-supervised training loss that can be incorporated into VAE-based models to bridge their gaps from the Group Theory based definition. We train 1800 models covering the most prominent VAE-based models on five datasets to verify the effectiveness of our method. Compared to the original models, the Groupidied VAEs consistently achieve better mean performance with smaller variances, and make meaningful dimensions controllable.},
	urldate = {2021-04-17},
	journal = {arXiv:2102.10303 [cs]},
	author = {Yang, Tao and Ren, Xuanchi and Wang, Yuwang and Zeng, Wenjun and Zheng, Nanning and Ren, Pengju},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.10303},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{giannone_no_2020,
	title = {No {Representation} without {Transformation}},
	url = {http://arxiv.org/abs/1912.03845},
	abstract = {We extend the framework of variational autoencoders to represent transformations explicitly in the latent space. In the family of hierarchical graphical models that emerges, the latent space is populated by higher order objects that are inferred jointly with the latent representations they act on. To explicitly demonstrate the effect of these higher order objects, we show that the inferred latent transformations reflect interpretable properties in the observation space. Furthermore, the model is structured in such a way that in the absence of transformations, we can run inference and obtain generative capabilities comparable with standard variational autoencoders. Finally, utilizing the trained encoder, we outperform the baselines by a wide margin on a challenging out-of-distribution classification task.},
	urldate = {2021-04-17},
	journal = {arXiv:1912.03845 [cs, stat]},
	author = {Giannone, Giorgio and Saremi, Saeed and Masci, Jonathan and Osendorfer, Christian},
	month = apr,
	year = {2020},
	note = {arXiv: 1912.03845},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{dupont_equivariant_2020,
	title = {Equivariant {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2006.07630},
	abstract = {We propose a framework for learning neural scene representations directly from images, without 3D supervision. Our key insight is that 3D structure can be imposed by ensuring that the learned representation transforms like a real 3D scene. Specifically, we introduce a loss which enforces equivariance of the scene representation with respect to 3D transformations. Our formulation allows us to infer and render scenes in real time while achieving comparable results to models requiring minutes for inference. In addition, we introduce two challenging new datasets for scene representation and neural rendering, including scenes with complex lighting and backgrounds. Through experiments, we show that our model achieves compelling results on these datasets as well as on standard ShapeNet benchmarks.},
	urldate = {2021-04-17},
	journal = {arXiv:2006.07630 [cs, stat]},
	author = {Dupont, Emilien and Bautista, Miguel Angel and Colburn, Alex and Sankar, Aditya and Guestrin, Carlos and Susskind, Josh and Shan, Qi},
	month = dec,
	year = {2020},
	note = {arXiv: 2006.07630},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Statistics - Machine Learning},
}

@article{bouchacourt_addressing_2021,
	title = {Addressing the {Topological} {Defects} of {Disentanglement} via {Distributed} {Operators}},
	url = {http://arxiv.org/abs/2102.05623},
	abstract = {A core challenge in Machine Learning is to learn to disentangle natural factors of variation in data (e.g. object shape vs. pose). A popular approach to disentanglement consists in learning to map each of these factors to distinct subspaces of a model's latent representation. However, this approach has shown limited empirical success to date. Here, we show that, for a broad family of transformations acting on images--encompassing simple affine transformations such as rotations and translations--this approach to disentanglement introduces topological defects (i.e. discontinuities in the encoder). Motivated by classical results from group representation theory, we study an alternative, more flexible approach to disentanglement which relies on distributed latent operators, potentially acting on the entire latent space. We theoretically and empirically demonstrate the effectiveness of this approach to disentangle affine transformations. Our work lays a theoretical foundation for the recent success of a new generation of models using distributed operators for disentanglement.},
	urldate = {2021-04-17},
	journal = {arXiv:2102.05623 [cs]},
	author = {Bouchacourt, Diane and Ibrahim, Mark and Deny, Stéphane},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05623},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{cohen_transformation_2015,
	title = {Transformation {Properties} of {Learned} {Visual} {Representations}},
	url = {http://arxiv.org/abs/1412.7659},
	abstract = {When a three-dimensional object moves relative to an observer, a change occurs on the observer's image plane and in the visual representation computed by a learned model. Starting with the idea that a good visual representation is one that transforms linearly under scene motions, we show, using the theory of group representations, that any such representation is equivalent to a combination of the elementary irreducible representations. We derive a striking relationship between irreducibility and the statistical dependency structure of the representation, by showing that under restricted conditions, irreducible representations are decorrelated. Under partial observability, as induced by the perspective projection of a scene onto the image plane, the motion group does not have a linear action on the space of images, so that it becomes necessary to perform inference over a latent representation that does transform linearly. This idea is demonstrated in a model of rotating NORB objects that employs a latent representation of the non-commutative 3D rotation group SO(3).},
	urldate = {2021-04-17},
	journal = {arXiv:1412.7659 [cs]},
	author = {Cohen, Taco S. and Welling, Max},
	month = apr,
	year = {2015},
	note = {arXiv: 1412.7659},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{satorras_en_2021,
	title = {E(n) {Equivariant} {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2102.09844},
	abstract = {This paper introduces a new model to learn graph neural networks equivariant to rotations, translations, reflections and permutations called E(n)-Equivariant Graph Neural Networks (EGNNs). In contrast with existing methods, our work does not require computationally expensive higher-order representations in intermediate layers while it still achieves competitive or better performance. In addition, whereas existing methods are limited to equivariance on 3 dimensional spaces, our model is easily scaled to higher-dimensional spaces. We demonstrate the effectiveness of our method on dynamical systems modelling, representation learning in graph autoencoders and predicting molecular properties.},
	urldate = {2021-04-15},
	journal = {arXiv:2102.09844 [cs, stat]},
	author = {Satorras, Victor Garcia and Hoogeboom, Emiel and Welling, Max},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.09844},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2021-04-11},
	journal = {arXiv:2103.00020 [cs]},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv: 2103.00020},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{mouli_neural_2021,
	title = {{NEURAL} {NETWORKS} {FOR} {LEARNING} {COUNTERFAC}- {TUAL} {G}-{INVARIANCES} {FROM} {SINGLE} {ENVIRONMENTS}},
	abstract = {Despite —or maybe because of— their astonishing capacity to ﬁt data, neural networks are believed to have difﬁculties extrapolating beyond training data distribution. This work shows that, for extrapolations based on ﬁnite transformation groups, a model’s inability to extrapolate is unrelated to its capacity. Rather, the shortcoming is inherited from a learning hypothesis: Examples not explicitly observed with inﬁnitely many training examples have underspeciﬁed outcomes in the learner’s model. In order to endow neural networks with the ability to extrapolate over group transformations, we introduce a learning framework counterfactually-guided by the learning hypothesis that any group invariance to (known) transformation groups is mandatory even without evidence, unless the learner deems it inconsistent with the training data. Unlike existing invariance-driven methods for (counterfactual) extrapolations, this framework allows extrapolations from a single environment. Finally, we introduce sequence and image extrapolation tasks that validate our framework and showcase the shortcomings of traditional approaches.},
	language = {en},
	author = {Mouli, S Chandra and Ribeiro, Bruno},
	year = {2021},
	pages = {30},
}

@article{benton_learning_2020,
	title = {Learning {Invariances} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2010.11882},
	abstract = {Invariances to translations have imbued convolutional neural networks with powerful generalization properties. However, we often do not know a priori what invariances are present in the data, or to what extent a model should be invariant to a given symmetry group. We show how to {\textbackslash}emph\{learn\} invariances and equivariances by parameterizing a distribution over augmentations and optimizing the training loss simultaneously with respect to the network parameters and augmentation parameters. With this simple procedure we can recover the correct set and extent of invariances on image classification, regression, segmentation, and molecular property prediction from a large space of augmentations, on training data alone.},
	urldate = {2021-03-11},
	journal = {arXiv:2010.11882 [cs, stat]},
	author = {Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew Gordon},
	month = dec,
	year = {2020},
	note = {arXiv: 2010.11882},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{srinivas_curl_2020,
	title = {{CURL}: {Contrastive} {Unsupervised} {Representations} for {Reinforcement} {Learning}},
	shorttitle = {{CURL}},
	url = {http://arxiv.org/abs/2004.04136},
	abstract = {We present CURL: Contrastive Unsupervised Representations for Reinforcement Learning. CURL extracts high-level features from raw pixels using contrastive learning and performs off-policy control on top of the extracted features. CURL outperforms prior pixel-based methods, both model-based and model-free, on complex tasks in the DeepMind Control Suite and Atari Games showing 1.9x and 1.2x performance gains at the 100K environment and interaction steps benchmarks respectively. On the DeepMind Control Suite, CURL is the first image-based algorithm to nearly match the sample-efficiency of methods that use state-based features. Our code is open-sourced and available at https://github.com/MishaLaskin/curl.},
	urldate = {2021-03-11},
	journal = {arXiv:2004.04136 [cs, stat]},
	author = {Srinivas, Aravind and Laskin, Michael and Abbeel, Pieter},
	month = sep,
	year = {2020},
	note = {arXiv: 2004.04136},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{ferrari_zero-shot_2018,
	address = {Cham},
	title = {Zero-{Shot} {Deep} {Domain} {Adaptation}},
	volume = {11215},
	isbn = {978-3-030-01251-9 978-3-030-01252-6},
	url = {http://link.springer.com/10.1007/978-3-030-01252-6_47},
	abstract = {Domain adaptation is an important tool to transfer knowledge about a task (e.g. classiﬁcation) learned in a source domain to a second, or target domain. Current approaches assume that task-relevant target-domain data is available during training. We demonstrate how to perform domain adaptation when no such task-relevant target-domain data is available. To tackle this issue, we propose zero-shot deep domain adaptation (ZDDA), which uses privileged information from taskirrelevant dual-domain pairs. ZDDA learns a source-domain representation which is not only tailored for the task of interest but also close to the target-domain representation. Therefore, the source-domain task of interest solution (e.g. a classiﬁer for classiﬁcation tasks) which is jointly trained with the source-domain representation can be applicable to both the source and target representations. Using the MNIST, FashionMNIST, NIST, EMNIST, and SUN RGB-D datasets, we show that ZDDA can perform domain adaptation in classiﬁcation tasks without access to task-relevant target-domain training data. We also extend ZDDA to perform sensor fusion in the SUN RGB-D scene classiﬁcation task by simulating task-relevant target-domain representations with task-relevant source-domain data. To the best of our knowledge, ZDDA is the ﬁrst domain adaptation and sensor fusion method which requires no taskrelevant target-domain data. The underlying principle is not particular to computer vision data, but should be extensible to other domains.},
	language = {en},
	urldate = {2021-03-11},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Peng, Kuan-Chuan and Wu, Ziyan and Ernst, Jan},
	editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
	year = {2018},
	doi = {10.1007/978-3-030-01252-6_47},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {793--810},
}

@article{blitzer_zero-shot_nodate,
	title = {Zero-{Shot} {Domain} {Adaptation}: {A} {Multi}-{View} {Approach}},
	language = {en},
	author = {Blitzer, John},
	pages = {10},
}

@article{gulrajani_search_2020,
	title = {In {Search} of {Lost} {Domain} {Generalization}},
	url = {http://arxiv.org/abs/2007.01434},
	abstract = {The goal of domain generalization algorithms is to predict well on distributions different from those seen during training. While a myriad of domain generalization algorithms exist, inconsistencies in experimental conditions -- datasets, architectures, and model selection criteria -- render fair and realistic comparisons difficult. In this paper, we are interested in understanding how useful domain generalization algorithms are in realistic settings. As a first step, we realize that model selection is non-trivial for domain generalization tasks. Contrary to prior work, we argue that domain generalization algorithms without a model selection strategy should be regarded as incomplete. Next, we implement DomainBed, a testbed for domain generalization including seven multi-domain datasets, nine baseline algorithms, and three model selection criteria. We conduct extensive experiments using DomainBed and find that, when carefully implemented, empirical risk minimization shows state-of-the-art performance across all datasets. Looking forward, we hope that the release of DomainBed, along with contributions from fellow researchers, will streamline reproducible and rigorous research in domain generalization.},
	urldate = {2021-03-11},
	journal = {arXiv:2007.01434 [cs, stat]},
	author = {Gulrajani, Ishaan and Lopez-Paz, David},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.01434},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{rosenfeld_risks_nodate,
	title = {The {Risks} of {Invariant} {Risk} {Minimization}},
	abstract = {Invariant Causal Prediction [30] is a technique for out-of-distribution generalization which assumes that some aspects of the data distribution vary across the training set but that the underlying causal mechanisms remain constant. Recently, Arjovsky et al. [1] proposed Invariant Risk Minimization (IRM), an objective based on this idea for learning deep, invariant features of data which are a complex function of latent variables; many alternatives have subsequently been suggested. However, formal guarantees for all of these works are severely lacking. In this paper, we present the ﬁrst analysis of classiﬁcation under the IRM objective—as well as these recently proposed alternatives—under a fairly natural and general model. In the linear case, we show simple conditions under which the optimal solution succeeds or, more often, fails to recover the optimal invariant predictor. We furthermore present the very ﬁrst results in the non-linear regime: we demonstrate that IRM can fail catastrophically unless the test data are sufﬁciently similar to the training distribution—this is precisely the issue that it was intended to solve. Thus, in this setting we ﬁnd that IRM and its alternatives fundamentally do not improve over standard Empirical Risk Minimization.},
	language = {en},
	author = {Rosenfeld, Elan and Ravikumar, Pradeep and Risteski, Andrej},
	pages = {12},
}

@article{wu_representation_2020,
	title = {Representation {Bayesian} {Risk} {Decompositions} and {Multi}-{Source} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2004.10390},
	abstract = {We consider representation learning (hypothesis class \${\textbackslash}mathcal\{H\} = {\textbackslash}mathcal\{F\}{\textbackslash}circ{\textbackslash}mathcal\{G\}\$) where training and test distributions can be different. Recent studies provide hints and failure examples for domain invariant representation learning, a common approach for this problem, but the explanations provided are somewhat different and do not provide a unified picture. In this paper, we provide new decompositions of risk which give finer-grained explanations and clarify potential generalization issues. For Single-Source Domain Adaptation, we give an exact decomposition (an equality) of the target risk, via a natural hybrid argument, as sum of three factors: (1) source risk, (2) representation conditional label divergence, and (3) representation covariate shift. We derive a similar decomposition for the Multi-Source case. These decompositions reveal factors (2) and (3) as the precise reasons for failure to generalize. For example, we demonstrate that domain adversarial neural networks (DANN) attempt to regularize for (3) but miss (2), while a recent technique Invariant Risk Minimization (IRM) attempts to account for (2) but does not consider (3). We also verify our observations experimentally.},
	urldate = {2021-03-11},
	journal = {arXiv:2004.10390 [cs, stat]},
	author = {Wu, Xi and Guo, Yang and Chen, Jiefeng and Liang, Yingyu and Jha, Somesh and Chalasani, Prasad},
	month = jun,
	year = {2020},
	note = {arXiv: 2004.10390},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{schneider_generalized_nodate,
	title = {Generalized {Invariant} {Risk} {Minimization}: relating adaptation and invariant representation learning},
	abstract = {If faced with new domains or environments, a standard strategy is to adapt the parameters of a model trained on one domain such that it performs well on the new domain. Here we introduce Generalized Invariant Risk Minimization (G-IRM), a technique that takes a pre-speciﬁed adaptation mechanism and aims to ﬁnd invariant representations that (a) perform well across multiple different training environments and (b) cannot be improved through adaptation to individual environments. GIRM thereby generalizes ideas put forward by Invariant Risk Minimization (IRM) and allows us to directly compare the performance of invariant representations with adapted representations on an equal footing, i.e., with respect to the same adaptation mechanism. We propose a framework to test the hypotheses that (i) G-IRM outperforms IRM, (ii) G-IRM outperforms Empirical Risk Minimization (ERM) and (iii) that more powerful adaptation mechanisms lead to better G-IRM performance. Such a relationship would provide a novel and systematic way to design regularizers for invariant representation learning and has the potential to scale Invariant Risk Minimization towards real world datasets.},
	language = {en},
	author = {Schneider, Steffen and Krishna, Shubham and Eck, Luisa and Brendel, Wieland and Mathis, Mackenzie W and Bethge, Matthias},
	pages = {7},
}

@article{zhang_domain_nodate,
	title = {Domain {Adaptation} under {Target} and {Conditional} {Shift}},
	abstract = {Let X denote the feature and Y the target. We consider domain adaptation under three possible scenarios: (1) the marginal PY changes, while the conditional PX{\textbar}Y stays the same (target shift), (2) the marginal PY is ﬁxed, while the conditional PX{\textbar}Y changes with certain constraints (conditional shift), and (3) the marginal PY changes, and the conditional PX{\textbar}Y changes with constraints (generalized target shift). Using background knowledge, causal interpretations allow us to determine the correct situation for a problem at hand. We exploit importance reweighting or sample transformation to ﬁnd the learning machine that works well on test data, and propose to estimate the weights or transformations by reweighting or transforming training data to reproduce the covariate distribution on the test domain. Thanks to kernel embedding of conditional as well as marginal distributions, the proposed approaches avoid distribution estimation, and are applicable for high-dimensional problems. Numerical evaluations on synthetic and realworld data sets demonstrate the eﬀectiveness of the proposed framework.},
	language = {en},
	author = {Zhang, Kun and Scholkopf, Bernhard and Muandet, Krikamol and Wang, Zhikun},
	pages = {9},
}

@article{blanchard_domain_2021,
	title = {Domain {Generalization} by {Marginal} {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1711.07910},
	abstract = {In the problem of domain generalization (DG), there are labeled training data sets from several related prediction problems, and the goal is to make accurate predictions on future unlabeled data sets that are not known to the learner. This problem arises in several applications where data distributions fluctuate because of environmental, technical, or other sources of variation. We introduce a formal framework for DG, and argue that it can be viewed as a kind of supervised learning problem by augmenting the original feature space with the marginal distribution of feature vectors. While our framework has several connections to conventional analysis of supervised learning algorithms, several unique aspects of DG require new methods of analysis. This work lays the learning theoretic foundations of domain generalization, building on our earlier conference paper where the problem of DG was introduced (Blanchard et al., 2011). We present two formal models of data generation, corresponding notions of risk, and distribution-free generalization error analysis. By focusing our attention on kernel methods, we also provide more quantitative results and a universally consistent algorithm. An efficient implementation is provided for this algorithm, which is experimentally compared to a pooling strategy on one synthetic and three real-world data sets.},
	urldate = {2021-03-11},
	journal = {arXiv:1711.07910 [stat]},
	author = {Blanchard, Gilles and Deshmukh, Aniket Anand and Dogan, Urun and Lee, Gyemin and Scott, Clayton},
	month = jan,
	year = {2021},
	note = {arXiv: 1711.07910},
	keywords = {Statistics - Machine Learning},
}

@article{li_survey_2019,
	title = {A {Survey} of {Multi}-{View} {Representation} {Learning}},
	volume = {31},
	issn = {1041-4347, 1558-2191, 2326-3865},
	url = {http://arxiv.org/abs/1610.01206},
	doi = {10.1109/TKDE.2018.2872063},
	abstract = {Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then from the perspective of representation fusion we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.},
	number = {10},
	urldate = {2021-03-11},
	journal = {IEEE Transactions on Knowledge and Data Engineering},
	author = {Li, Yingming and Yang, Ming and Zhang, Zhongfei},
	month = oct,
	year = {2019},
	note = {arXiv: 1610.01206},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Machine Learning},
	pages = {1863--1883},
}

@article{sun_survey_nodate,
	title = {A survey of multi-view machine learning},
	abstract = {Multi-view learning or learning with multiple distinct feature sets is a rapidly growing direction in machine learning with well theoretical underpinnings and great practical success. This paper reviews theories developed to understand the properties and behaviors of multi-view learning, and gives a taxonomy of approaches according to the machine learning mechanisms involved and the fashions in which multiple views are exploited. This survey aims to provide an insightful organization of current developments in the ﬁeld of multi-view learning, identify their limitations, and give suggestions for further research. One feature of this survey is that we attempt to point out speciﬁc open problems which can hopefully be useful to promote the research of multi-view machine learning.},
	language = {en},
	author = {Sun, Shiliang},
	pages = {13},
}

@article{cortes_sample_2008,
	title = {Sample {Selection} {Bias} {Correction} {Theory}},
	url = {http://arxiv.org/abs/0805.2775},
	abstract = {This paper presents a theoretical analysis of sample selection bias correction. The sample bias correction technique commonly used in machine learning consists of reweighting the cost of an error on each training point of a biased sample to more closely reflect the unbiased distribution. This relies on weights derived by various estimation techniques based on finite samples. We analyze the effect of an error in that estimation on the accuracy of the hypothesis returned by the learning algorithm for two estimation techniques: a cluster-based estimation technique and kernel mean matching. We also report the results of sample bias correction experiments with several data sets using these techniques. Our analysis is based on the novel concept of distributional stability which generalizes the existing concept of point-based stability. Much of our work and proof techniques can be used to analyze other importance weighting techniques and their effect on accuracy when using a distributionally stable algorithm.},
	urldate = {2021-03-11},
	journal = {arXiv:0805.2775 [cs]},
	author = {Cortes, Corinna and Mohri, Mehryar and Riley, Michael and Rostamizadeh, Afshin},
	month = may,
	year = {2008},
	note = {arXiv: 0805.2775},
	keywords = {Computer Science - Machine Learning},
}

@article{chen_robust_nodate,
	title = {Robust {Covariate} {Shift} {Regression}},
	language = {en},
	author = {Chen, Xiangli and Monfort, Mathew and Liu, Anqi and Ziebart, Brian D},
	pages = {10},
}

@article{liu_robust_2017,
	title = {Robust {Covariate} {Shift} {Prediction} with {General} {Losses} and {Feature} {Views}},
	url = {http://arxiv.org/abs/1712.10043},
	abstract = {Covariate shift relaxes the widely-employed independent and identically distributed (IID) assumption by allowing different training and testing input distributions. Unfortunately, common methods for addressing covariate shift by trying to remove the bias between training and testing distributions using importance weighting often provide poor performance guarantees in theory and unreliable predictions with high variance in practice. Recently developed methods that construct a predictor that is inherently robust to the difficulties of learning under covariate shift are restricted to minimizing logloss and can be too conservative when faced with high-dimensional learning tasks. We address these limitations in two ways: by robustly minimizing various loss functions, including non-convex ones, under the testing distribution; and by separately shaping the influence of covariate shift according to different feature-based views of the relationship between input variables and example labels. These generalizations make robust covariate shift prediction applicable to more task scenarios. We demonstrate the benefits on classification under covariate shift tasks.},
	urldate = {2021-03-11},
	journal = {arXiv:1712.10043 [cs, stat]},
	author = {Liu, Anqi and Ziebart, Brian D.},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.10043},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zoroa_coupon_2017,
	title = {The coupon collector urn model with unequal probabilities in ecology and evolution},
	volume = {14},
	issn = {1742-5689},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5332560/},
	doi = {10.1098/rsif.2016.0643},
	abstract = {The sequential sampling of populations with unequal probabilities and with replacement in a closed population is a recurrent problem in ecology and evolution. Examples range from biodiversity sampling, epidemiology to the estimation of signal repertoire in animal communication. Many of these questions can be reformulated as urn problems, often as special cases of the coupon collector problem, most simply expressed as the number of coupons that must be collected to have a complete set. We aimed to apply the coupon collector model in a comprehensive manner to one example—hosts (balls) being searched (draws) and parasitized (ball colour change) by parasitic wasps—to evaluate the influence of differences in sampling probabilities between items on collection speed. Based on the model of a complete multinomial process over time, we define the distribution, distribution function, expectation and variance of the number of hosts parasitized after a given time, as well as the inverse problem, estimating the sampling effort. We develop the relationship between the risk distribution on the set of hosts and the speed of parasitization and propose a more elegant proof of the weak stochastic dominance among speeds of parasitization, using the concept of Schur convexity and the ‘Robin Hood transfer’ numerical operation. Numerical examples are provided and a conjecture about strong dominance—an ordering characteristic of random variables—is proposed. The speed at which new items are discovered is a function of the entire shape of the sampling probability distribution. The sole comparison of values of variances is not sufficient to compare speeds associated with different distributions, as generally assumed in ecological studies.},
	number = {127},
	urldate = {2021-03-10},
	journal = {Journal of the Royal Society Interface},
	author = {Zoroa, N. and Lesigne, E. and Fernández-Sáez, M. J. and Zoroa, P. and Casas, J.},
	month = feb,
	year = {2017},
	pmid = {28179550},
	pmcid = {PMC5332560},
}

@article{berger_maximum_1996,
	title = {A {Maximum} {Entropy} {Approach} to {Natural} {Language} {Processing}},
	volume = {22},
	url = {https://www.aclweb.org/anthology/J96-1002},
	number = {1},
	urldate = {2021-03-10},
	journal = {Computational Linguistics},
	author = {Berger, Adam L. and Della Pietra, Stephen A. and Della Pietra, Vincent J.},
	year = {1996},
	pages = {39--71},
}

@article{rigollet_optimal_2009,
	title = {Optimal rates for plug-in estimators of density level sets},
	volume = {15},
	issn = {1350-7265},
	url = {http://arxiv.org/abs/math/0611473},
	doi = {10.3150/09-BEJ184},
	abstract = {In the context of density level set estimation, we study the convergence of general plug-in methods under two main assumptions on the density for a given level \${\textbackslash}lambda\$. More precisely, it is assumed that the density (i) is smooth in a neighborhood of \${\textbackslash}lambda\$ and (ii) has \${\textbackslash}gamma\$-exponent at level \${\textbackslash}lambda\$. Condition (i) ensures that the density can be estimated at a standard nonparametric rate and condition (ii) is similar to Tsybakov's margin assumption which is stated for the classification framework. Under these assumptions, we derive optimal rates of convergence for plug-in estimators. Explicit convergence rates are given for plug-in estimators based on kernel density estimators when the underlying measure is the Lebesgue measure. Lower bounds proving optimality of the rates in a minimax sense when the density is H{\textbackslash}"older smooth are also provided.},
	number = {4},
	urldate = {2021-03-10},
	journal = {Bernoulli},
	author = {Rigollet, Philippe and Vert, Régis},
	month = nov,
	year = {2009},
	note = {arXiv: math/0611473},
	keywords = {Mathematics - Statistics Theory},
	pages = {1154--1178},
}

@article{deshmukh_generalization_2019,
	title = {A {Generalization} {Error} {Bound} for {Multi}-class {Domain} {Generalization}},
	url = {http://arxiv.org/abs/1905.10392},
	abstract = {Domain generalization is the problem of assigning labels to an unlabeled data set, given several similar data sets for which labels have been provided. Despite considerable interest in this problem over the last decade, there has been no theoretical analysis in the setting of multi-class classification. In this work, we study a kernel-based learning algorithm and establish a generalization error bound that scales logarithmically in the number of classes, matching state-of-the-art bounds for multi-class classification in the conventional learning setting. We also demonstrate empirically that the proposed algorithm achieves significant performance gains compared to a pooling strategy.},
	urldate = {2021-03-10},
	journal = {arXiv:1905.10392 [cs, stat]},
	author = {Deshmukh, Aniket Anand and Lei, Yunwen and Sharma, Srinagesh and Dogan, Urun and Cutler, James W. and Scott, Clayton},
	month = may,
	year = {2019},
	note = {arXiv: 1905.10392},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{koltchinskii_2004_2006,
	title = {2004 {IMS} {Medallion} {Lecture}: {Local} {Rademacher} complexities and oracle inequalities in risk minimization},
	volume = {34},
	issn = {0090-5364},
	shorttitle = {2004 {IMS} {Medallion} {Lecture}},
	url = {http://arxiv.org/abs/0708.0083},
	doi = {10.1214/009053606000001019},
	abstract = {Let \${\textbackslash}mathcal\{F\}\$ be a class of measurable functions \$f:S{\textbackslash}mapsto [0,1]\$ defined on a probability space \$(S,{\textbackslash}mathcal\{A\},P)\$. Given a sample (X\_1,...,X\_n) of i.i.d. random variables taking values in S with common distribution P, let P\_n denote the empirical measure based on (X\_1,...,X\_n). We study an empirical risk minimization problem \$P\_nf{\textbackslash}to {\textbackslash}min\$, \$f{\textbackslash}in {\textbackslash}mathcal\{F\}\$. Given a solution \${\textbackslash}hat\{f\}\_n\$ of this problem, the goal is to obtain very general upper bounds on its excess risk {\textbackslash}[{\textbackslash}mathcal\{E\}\_P({\textbackslash}hat\{f\}\_n):=P{\textbackslash}hat\{f\}\_n-{\textbackslash}inf\_\{f{\textbackslash}in {\textbackslash}mathcal\{F\}\}Pf,{\textbackslash}] expressed in terms of relevant geometric parameters of the class \${\textbackslash}mathcal\{F\}\$. Using concentration inequalities and other empirical processes tools, we obtain both distribution-dependent and data-dependent upper bounds on the excess risk that are of asymptotically correct order in many examples. The bounds involve localized sup-norms of empirical and Rademacher processes indexed by functions from the class. We use these bounds to develop model selection techniques in abstract risk minimization problems that can be applied to more specialized frameworks of regression and classification.},
	number = {6},
	urldate = {2021-03-10},
	journal = {The Annals of Statistics},
	author = {Koltchinskii, Vladimir},
	month = dec,
	year = {2006},
	note = {arXiv: 0708.0083},
	keywords = {62H30, 60B99, 68Q32 (Primary) 62G08, 68T05, 68T10 (Secondary), Mathematics - Statistics Theory},
	pages = {2593--2656},
}

@article{barron_approximation_1991,
	title = {Approximation of {Density} {Functions} by {Sequences} of {Exponential} {Families}},
	volume = {19},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-19/issue-3/Approximation-of-Density-Functions-by-Sequences-of-Exponential-Families/10.1214/aos/1176348252.full},
	doi = {10.1214/aos/1176348252},
	abstract = {Probability density functions are estimated by the method of maximum likelihood in sequences of regular exponential families. This method is also familiar as entropy maximization subject to empirical constraints. The approximating families of log-densities that we consider are polynomials, splines and trigonometric series. Bounds on the relative entropy (Kullback-Leibler distance) between the true density and the estimator are obtained and rates of convergence are established for log-density functions assumed to have square integrable derivatives.},
	number = {3},
	urldate = {2021-03-10},
	journal = {The Annals of Statistics},
	author = {Barron, Andrew R. and Sheu, Chyong-Hwa},
	month = sep,
	year = {1991},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {\$L\_2\$ approximation, 41A17, 62B10, 62F12, 62G05, Kullback-Leibler number, Log-density estimation, exponential families, minimum relative entropy estimation},
	pages = {1347--1369},
}

@article{li_learning_2020,
	title = {Learning {Invariant} {Representations} and {Risks} for {Semi}-supervised {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2010.04647},
	abstract = {The success of supervised learning hinges on the assumption that the training and test data come from the same underlying distribution, which is often not valid in practice due to potential distribution shift. In light of this, most existing methods for unsupervised domain adaptation focus on achieving domain-invariant representations and small source domain error. However, recent works have shown that this is not sufficient to guarantee good generalization on the target domain, and in fact, is provably detrimental under label distribution shift. Furthermore, in many real-world applications it is often feasible to obtain a small amount of labeled data from the target domain and use them to facilitate model training with source data. Inspired by the above observations, in this paper we propose the first method that aims to simultaneously learn invariant representations and risks under the setting of semi-supervised domain adaptation (Semi-DA). First, we provide a finite sample bound for both classification and regression problems under Semi-DA. The bound suggests a principled way to obtain target generalization, i.e. by aligning both the marginal and conditional distributions across domains in feature space. Motivated by this, we then introduce the LIRR algorithm for jointly {\textbackslash}textbf\{L\}earning {\textbackslash}textbf\{I\}nvariant {\textbackslash}textbf\{R\}epresentations and {\textbackslash}textbf\{R\}isks. Finally, extensive experiments are conducted on both classification and regression tasks, which demonstrates LIRR consistently achieves state-of-the-art performance and significant improvements compared with the methods that only learn invariant representations or invariant risks.},
	urldate = {2021-03-09},
	journal = {arXiv:2010.04647 [cs]},
	author = {Li, Bo and Wang, Yezhen and Zhang, Shanghang and Li, Dongsheng and Darrell, Trevor and Keutzer, Kurt and Zhao, Han},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.04647},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{sun_unsupervised_2019,
	title = {Unsupervised {Domain} {Adaptation} through {Self}-{Supervision}},
	url = {http://arxiv.org/abs/1909.11825},
	abstract = {This paper addresses unsupervised domain adaptation, the setting where labeled training data is available on a source domain, but the goal is to have good performance on a target domain with only unlabeled data. Like much of previous work, we seek to align the learned representations of the source and target domains while preserving discriminability. The way we accomplish alignment is by learning to perform auxiliary self-supervised task(s) on both domains simultaneously. Each self-supervised task brings the two domains closer together along the direction relevant to that task. Training this jointly with the main task classifier on the source domain is shown to successfully generalize to the unlabeled target domain. The presented objective is straightforward to implement and easy to optimize. We achieve state-of-the-art results on four out of seven standard benchmarks, and competitive results on segmentation adaptation. We also demonstrate that our method composes well with another popular pixel-level adaptation method.},
	urldate = {2021-03-09},
	journal = {arXiv:1909.11825 [cs, stat]},
	author = {Sun, Yu and Tzeng, Eric and Darrell, Trevor and Efros, Alexei A.},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.11825},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{feng_self-supervised_2019,
	address = {Seoul, Korea (South)},
	title = {Self-{Supervised} {Representation} {Learning} {From} {Multi}-{Domain} {Data}},
	isbn = {978-1-72814-803-8},
	url = {https://ieeexplore.ieee.org/document/9008251/},
	doi = {10.1109/ICCV.2019.00334},
	abstract = {We present an information-theoretically motivated constraint for self-supervised representation learning from multiple related domains. In contrast to previous selfsupervised learning methods, our approach learns from multiple domains, which has the beneﬁt of decreasing the build-in bias of individual domain, as well as leveraging information and allowing knowledge transfer across multiple domains. The proposed mutual information constraints encourage neural network to extract common invariant information across domains and to preserve peculiar information of each domain simultaneously. We adopt tractable upper and lower bounds of mutual information to make the proposed constraints solvable. The learned representation is more unbiased and robust toward the input images. Extensive experimental results on both multi-domain and large-scale datasets demonstrate the necessity and advantage of multi-domain self-supervised learning with mutual information constraints. Representations learned in our framework on state-of-the-art methods achieve improved performance than those learned on a single domain.},
	language = {en},
	urldate = {2021-03-09},
	booktitle = {2019 {IEEE}/{CVF} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	author = {Feng, Zeyu and Xu, Chang and Tao, Dacheng},
	month = oct,
	year = {2019},
	pages = {3244--3254},
}

@article{nguyen_domain_2021,
	title = {Domain {Invariant} {Representation} {Learning} with {Domain} {Density} {Transformations}},
	url = {http://arxiv.org/abs/2102.05082},
	abstract = {Domain generalization refers to the problem where we aim to train a model on data from a set of source domains so that the model can generalize to unseen target domains. Naively training a model on the aggregate set of data (pooled from all source domains) has been shown to perform suboptimally, since the information learned by that model might be domain-specific and generalize imperfectly to target domains. To tackle this problem, a predominant approach is to find and learn some domain-invariant information in order to use it for the prediction task. In this paper, we propose a theoretically grounded method to learn a domain-invariant representation by enforcing the representation network to be invariant under all transformation functions among domains. We also show how to use generative adversarial networks to learn such domain transformations to implement our method in practice. We demonstrate the effectiveness of our method on several widely used datasets for the domain generalization problem, on all of which we achieve competitive results with state-of-the-art models.},
	urldate = {2021-03-09},
	journal = {arXiv:2102.05082 [cs]},
	author = {Nguyen, A. Tuan and Tran, Toan and Gal, Yarin and Baydin, Atılım Güneş},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.05082},
	keywords = {Computer Science - Machine Learning},
}

@article{kazemi_unsupervised_nodate,
	title = {Unsupervised {Image}-to-{Image} {Translation} {Using} {Domain}-{Specific} {Variational} {Information} {Bound}},
	abstract = {Unsupervised image-to-image translation is a class of computer vision problems which aims at modeling conditional distribution of images in the target domain, given a set of unpaired images in the source and target domains. An image in the source domain might have multiple representations in the target domain. Therefore, ambiguity in modeling of the conditional distribution arises, specially when the images in the source and target domains come from different modalities. Current approaches mostly rely on simplifying assumptions to map both domains into a shared-latent space. Consequently, they are only able to model the domain-invariant information between the two modalities. These approaches usually fail to model domain-speciﬁc information which has no representation in the target domain. In this work, we propose an unsupervised image-to-image translation framework which maximizes a domain-speciﬁc variational information bound and learns the target domain-invariant representation of the two domain. The proposed framework makes it possible to map a single source image into multiple images in the target domain, utilizing several target domain-speciﬁc codes sampled randomly from the prior distribution, or extracted from reference images.},
	language = {en},
	author = {Kazemi, Hadi and Soleymani, Sobhan and Taherkhani, Fariborz and Iranmanesh, Seyed and Nasrabadi, Nasser},
	pages = {11},
}

@article{gholami_unsupervised_2018,
	title = {Unsupervised {Multi}-{Target} {Domain} {Adaptation}: {An} {Information} {Theoretic} {Approach}},
	shorttitle = {Unsupervised {Multi}-{Target} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1810.11547},
	abstract = {Unsupervised domain adaptation (uDA) models focus on pairwise adaptation settings where there is a single, labeled, source and a single target domain. However, in many real-world settings one seeks to adapt to multiple, but somewhat similar, target domains. Applying pairwise adaptation approaches to this setting may be suboptimal, as they fail to leverage shared information among multiple domains. In this work we propose an information theoretic approach for domain adaptation in the novel context of multiple target domains with unlabeled instances and one source domain with labeled instances. Our model aims to find a shared latent space common to all domains, while simultaneously accounting for the remaining private, domain-specific factors. Disentanglement of shared and private information is accomplished using a unified information-theoretic approach, which also serves to establish a stronger link between the latent representations and the observed data. The resulting model, accompanied by an efficient optimization algorithm, allows simultaneous adaptation from a single source to multiple target domains. We test our approach on three challenging publicly-available datasets, showing that it outperforms several popular domain adaptation methods.},
	urldate = {2021-03-09},
	journal = {arXiv:1810.11547 [cs]},
	author = {Gholami, Behnam and Sahu, Pritish and Rudovic, Ognjen and Bousmalis, Konstantinos and Pavlovic, Vladimir},
	month = oct,
	year = {2018},
	note = {arXiv: 1810.11547
version: 1},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{lemberger_primer_2020,
	title = {A {Primer} on {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2001.09994},
	abstract = {Standard supervised machine learning assumes that the distribution of the source samples used to train an algorithm is the same as the one of the target samples on which it is supposed to make predictions. However, as any data scientist will confirm, this is hardly ever the case in practice. The set of statistical and numerical methods that deal with such situations is known as domain adaptation, a field with a long and rich history. The myriad of methods available and the unfortunate lack of a clear and universally accepted terminology can however make the topic rather daunting for the newcomer. Therefore, rather than aiming at completeness, which leads to exhibiting a tedious catalog of methods, this pedagogical review aims at a coherent presentation of four important special cases: (1) prior shift, a situation in which training samples were selected according to their labels without any knowledge of their actual distribution in the target, (2) covariate shift which deals with a situation where training examples were picked according to their features but with some selection bias, (3) concept shift where the dependence of the labels on the features defers between the source and the target, and last but not least (4) subspace mapping which deals with a situation where features in the target have been subjected to an unknown distortion with respect to the source features. In each case we first build an intuition, next we provide the appropriate mathematical framework and eventually we describe a practical application.},
	urldate = {2021-03-09},
	journal = {arXiv:2001.09994 [cs, stat]},
	author = {Lemberger, Pirmin and Panico, Ivan},
	month = feb,
	year = {2020},
	note = {arXiv: 2001.09994
version: 2},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, stat.ML},
}

@article{tachet_domain_2020,
	title = {Domain {Adaptation} with {Conditional} {Distribution} {Matching} and {Generalized} {Label} {Shift}},
	url = {http://arxiv.org/abs/2003.04475},
	abstract = {Adversarial learning has demonstrated good performance in the unsupervised domain adaptation setting, by learning domain-invariant representations. However, recent work has shown limitations of this approach when label distributions differ between the source and target domains. In this paper, we propose a new assumption, generalized label shift (\$GLS\$), to improve robustness against mismatched label distributions. \$GLS\$ states that, conditioned on the label, there exists a representation of the input that is invariant between the source and target domains. Under \$GLS\$, we provide theoretical guarantees on the transfer performance of any classifier. We also devise necessary and sufficient conditions for \$GLS\$ to hold, by using an estimation of the relative class weights between domains and an appropriate reweighting of samples. Our weight estimation method could be straightforwardly and generically applied in existing domain adaptation (DA) algorithms that learn domain-invariant representations, with small computational overhead. In particular, we modify three DA algorithms, JAN, DANN and CDAN, and evaluate their performance on standard and artificial DA tasks. Our algorithms outperform the base versions, with vast improvements for large label distribution mismatches. Our code is available at https://tinyurl.com/y585xt6j.},
	urldate = {2021-03-09},
	journal = {arXiv:2003.04475 [cs, stat]},
	author = {Tachet, Remi and Zhao, Han and Wang, Yu-Xiang and Gordon, Geoff},
	month = dec,
	year = {2020},
	note = {arXiv: 2003.04475},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{you_universal_nodate,
	title = {Universal {Domain} {Adaptation}},
	language = {en},
	author = {You, Kaichao and Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I},
	pages = {10},
}

@article{lao_hypothesis_2020,
	title = {Hypothesis {Disparity} {Regularized} {Mutual} {Information} {Maximization}},
	url = {http://arxiv.org/abs/2012.08072},
	abstract = {We propose a hypothesis disparity regularized mutual information maximization{\textasciitilde}(HDMI) approach to tackle unsupervised hypothesis transfer -- as an effort towards unifying hypothesis transfer learning (HTL) and unsupervised domain adaptation (UDA) -- where the knowledge from a source domain is transferred solely through hypotheses and adapted to the target domain in an unsupervised manner. In contrast to the prevalent HTL and UDA approaches that typically use a single hypothesis, HDMI employs multiple hypotheses to leverage the underlying distributions of the source and target hypotheses. To better utilize the crucial relationship among different hypotheses -- as opposed to unconstrained optimization of each hypothesis independently -- while adapting to the unlabeled target domain through mutual information maximization, HDMI incorporates a hypothesis disparity regularization that coordinates the target hypotheses jointly learn better target representations while preserving more transferable source knowledge with better-calibrated prediction uncertainty. HDMI achieves state-of-the-art adaptation performance on benchmark datasets for UDA in the context of HTL, without the need to access the source data during the adaptation.},
	urldate = {2021-03-09},
	journal = {arXiv:2012.08072 [cs]},
	author = {Lao, Qicheng and Jiang, Xiang and Havaei, Mohammad},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.08072},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{zhang_bridging_nodate,
	title = {Bridging {Theory} and {Algorithm} for {Domain} {Adaptation}},
	abstract = {This paper addresses the problem of unsupervised domain adaption from theoretical and algorithmic perspectives. Existing domain adaptation theories naturally imply minimax optimization algorithms, which connect well with the domain adaptation methods based on adversarial learning. However, several disconnections still exist and form the gap between theory and algorithm. We extend previous theories (Mansour et al., 2009c; Ben-David et al., 2010) to multiclass classiﬁcation in domain adaptation, where classiﬁers based on the scoring functions and margin loss are standard choices in algorithm design. We introduce Margin Disparity Discrepancy, a novel measurement with rigorous generalization bounds, tailored to the distribution comparison with the asymmetric margin loss, and to the minimax optimization for easier training. Our theory can be seamlessly transformed into an adversarial learning algorithm for domain adaptation, successfully bridging the gap between theory and algorithm. A series of empirical studies show that our algorithm achieves the state of the art accuracies on challenging domain adaptation tasks.},
	language = {en},
	author = {Zhang, Yuchen and Liu, Tianle and Long, Mingsheng and Jordan, Michael I},
	pages = {10},
}

@article{le_deep_2019,
	title = {On {Deep} {Domain} {Adaptation}: {Some} {Theoretical} {Understandings}},
	shorttitle = {On {Deep} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/1811.06199},
	abstract = {Compared with shallow domain adaptation, recent progress in deep domain adaptation has shown that it can achieve higher predictive performance and stronger capacity to tackle structural data (e.g., image and sequential data). The underlying idea of deep domain adaptation is to bridge the gap between source and target domains in a joint space so that a supervised classifier trained on labeled source data can be nicely transferred to the target domain. This idea is certainly intuitive and powerful, however, limited theoretical understandings have been developed to support its underpinning principle. In this paper, we have provided a rigorous framework to explain why it is possible to close the gap of the target and source domains in the joint space. More specifically, we first study the loss incurred when performing transfer learning from the source to the target domain. This provides a theory that explains and generalizes existing work in deep domain adaptation which was mainly empirical. This enables us to further explain why closing the gap in the joint space can directly minimize the loss incurred for transfer learning between the two domains. To our knowledge, this offers the first theoretical result that characterizes a direct bound on the joint space and the gain of transfer learning via deep domain adaptation},
	urldate = {2021-03-09},
	journal = {arXiv:1811.06199 [cs, stat]},
	author = {Le, Trung and Nguyen, Khanh and Ho, Nhat and Bui, Hung and Phung, Dinh},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.06199},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhao_review_2020,
	title = {A {Review} of {Single}-{Source} {Deep} {Unsupervised} {Visual} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2009.00155},
	abstract = {Large-scale labeled training datasets have enabled deep neural networks to excel across a wide range of benchmark vision tasks. However, in many applications, it is prohibitively expensive and time-consuming to obtain large quantities of labeled data. To cope with limited labeled training data, many have attempted to directly apply models trained on a large-scale labeled source domain to another sparsely labeled or unlabeled target domain. Unfortunately, direct transfer across domains often performs poorly due to the presence of domain shift or dataset bias. Domain adaptation is a machine learning paradigm that aims to learn a model from a source domain that can perform well on a different (but related) target domain. In this paper, we review the latest single-source deep unsupervised domain adaptation methods focused on visual tasks and discuss new perspectives for future research. We begin with the definitions of different domain adaptation strategies and the descriptions of existing benchmark datasets. We then summarize and compare different categories of single-source unsupervised domain adaptation methods, including discrepancy-based methods, adversarial discriminative methods, adversarial generative methods, and self-supervision-based methods. Finally, we discuss future research directions with challenges and possible solutions.},
	urldate = {2021-03-09},
	journal = {arXiv:2009.00155 [cs, eess]},
	author = {Zhao, Sicheng and Yue, Xiangyu and Zhang, Shanghang and Li, Bo and Zhao, Han and Wu, Bichen and Krishna, Ravi and Gonzalez, Joseph E. and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A. and Keutzer, Kurt},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.00155},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{kouw_introduction_2019,
	title = {An introduction to domain adaptation and transfer learning},
	url = {http://arxiv.org/abs/1812.11806},
	abstract = {In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.},
	urldate = {2021-03-09},
	journal = {arXiv:1812.11806 [cs, stat]},
	author = {Kouw, Wouter M. and Loog, Marco},
	month = jan,
	year = {2019},
	note = {arXiv: 1812.11806
version: 2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{kouw_review_2019,
	title = {A review of domain adaptation without target labels},
	url = {http://arxiv.org/abs/1901.05335},
	abstract = {Domain adaptation has become a prominent problem setting in machine learning and related fields. This review asks the question: how can a classifier learn from a source domain and generalize to a target domain? We present a categorization of approaches, divided into, what we refer to as, sample-based, feature-based and inference-based methods. Sample-based methods focus on weighting individual observations during training based on their importance to the target domain. Feature-based methods revolve around on mapping, projecting and representing features such that a source classifier performs well on the target domain and inference-based methods incorporate adaptation into the parameter estimation procedure, for instance through constraints on the optimization procedure. Additionally, we review a number of conditions that allow for formulating bounds on the cross-domain generalization error. Our categorization highlights recurring ideas and raises questions important to further research.},
	urldate = {2021-03-09},
	journal = {arXiv:1901.05335 [cs, stat]},
	author = {Kouw, Wouter M. and Loog, Marco},
	month = jul,
	year = {2019},
	note = {arXiv: 1901.05335},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{zhang_transfer_2020,
	title = {Transfer {Adaptation} {Learning}: {A} {Decade} {Survey}},
	shorttitle = {Transfer {Adaptation} {Learning}},
	url = {http://arxiv.org/abs/1903.04687},
	abstract = {The world we see is ever-changing and it always changes with people, things, and the environment. Domain is referred to as the state of the world at a certain moment. A research problem is characterized as transfer adaptation learning (TAL) when it needs knowledge correspondence between different moments/domains. Conventional machine learning aims to find a model with the minimum expected risk on test data by minimizing the regularized empirical risk on the training data, which, however, supposes that the training and test data share similar joint probability distribution. TAL aims to build models that can perform tasks of target domain by learning knowledge from a semantic related but distribution different source domain. It is an energetic research filed of increasing influence and importance, which is presenting a blowout publication trend. This paper surveys the advances of TAL methodologies in the past decade, and the technical challenges and essential problems of TAL have been observed and discussed with deep insights and new perspectives. Broader solutions of transfer adaptation learning being created by researchers are identified, i.e., instance re-weighting adaptation, feature adaptation, classifier adaptation, deep network adaptation and adversarial adaptation, which are beyond the early semi-supervised and unsupervised split. The survey helps researchers rapidly but comprehensively understand and identify the research foundation, research status, theoretical limitations, future challenges and under-studied issues (universality, interpretability, and credibility) to be broken in the field toward universal representation and safe applications in open-world scenarios.},
	urldate = {2021-03-09},
	journal = {arXiv:1903.04687 [cs]},
	author = {Zhang, Lei and Gao, Xinbo},
	month = nov,
	year = {2020},
	note = {arXiv: 1903.04687},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{zhuang_comprehensive_2020,
	title = {A {Comprehensive} {Survey} on {Transfer} {Learning}},
	url = {http://arxiv.org/abs/1911.02685},
	abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
	urldate = {2021-03-09},
	journal = {arXiv:1911.02685 [cs, stat]},
	author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
	month = jun,
	year = {2020},
	note = {arXiv: 1911.02685},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{song_improving_2019,
	title = {Improving {Unsupervised} {Domain} {Adaptation} with {Variational} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/1911.09310},
	abstract = {Domain adaptation aims to leverage the supervision signal of source domain to obtain an accurate model for target domain, where the labels are not available. To leverage and adapt the label information from source domain, most existing methods employ a feature extracting function and match the marginal distributions of source and target domains in a shared feature space. In this paper, from the perspective of information theory, we show that representation matching is actually an insufficient constraint on the feature space for obtaining a model with good generalization performance in target domain. We then propose variational bottleneck domain adaptation (VBDA), a new domain adaptation method which improves feature transferability by explicitly enforcing the feature extractor to ignore the task-irrelevant factors and focus on the information that is essential to the task of interest for both source and target domains. Extensive experimental results demonstrate that VBDA significantly outperforms state-of-the-art methods across three domain adaptation benchmark datasets.},
	urldate = {2021-03-09},
	journal = {arXiv:1911.09310 [cs, stat]},
	author = {Song, Yuxuan and Yu, Lantao and Cao, Zhangjie and Zhou, Zhiming and Shen, Jian and Shao, Shuo and Zhang, Weinan and Yu, Yong},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.09310},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ben-david_impossibility_nodate,
	title = {Impossibility {Theorems} for {Domain} {Adaptation}},
	abstract = {The domain adaptation problem in machine learning occurs when the test data generating distribution diﬀers from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed. We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) similarity between the unlabeled distributions, (ii) existence of a classiﬁer in the hypothesis class with low error on both training and testing distributions, and (iii) the covariate shift assumption. I.e., the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions. We show that without either assumption (i) or (ii), the combination of the remaining assumptions is not suﬃcient to guarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm, as long as it does not have access to target labeled examples. In particular, we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions.},
	language = {en},
	author = {Ben-David, Shai and Luu, Teresa and Lu, Tyler and Pal, David},
	pages = {8},
}

@inproceedings{hsu_experimental_1988,
	title = {Experimental {Demonstrations} of {Optical} {Neural} {Computers}},
	url = {https://proceedings.neurips.cc/paper/1987/file/8f14e45fceea167a5a36dedd4bea2543-Paper.pdf},
	urldate = {2021-03-09},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Hsu, Ken and Brady, David and Psaltis, Demetri},
	editor = {Anderson, D.},
	year = {1988},
}

@article{ben-david_theory_2010,
	title = {A theory of learning from different domains},
	volume = {79},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-009-5152-4},
	doi = {10.1007/s10994-009-5152-4},
	language = {en},
	number = {1-2},
	urldate = {2021-03-09},
	journal = {Machine Learning},
	author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
	month = may,
	year = {2010},
	pages = {151--175},
}

@incollection{csurka_domain-adversarial_2017,
	address = {Cham},
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	isbn = {978-3-319-58346-4 978-3-319-58347-1},
	url = {http://link.springer.com/10.1007/978-3-319-58347-1_10},
	abstract = {We introduce a new representation learning approach for domain adaptation, in which data at training and test time come from similar but diﬀerent distributions. Our approach is directly inspired by the theory on domain adaptation suggesting that, for eﬀective domain transfer to be achieved, predictions must be made based on features that cannot discriminate between the training (source) and test (target) domains.},
	language = {en},
	urldate = {2021-03-09},
	booktitle = {Domain {Adaptation} in {Computer} {Vision} {Applications}},
	publisher = {Springer International Publishing},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
	editor = {Csurka, Gabriela},
	year = {2017},
	doi = {10.1007/978-3-319-58347-1_10},
	note = {Series Title: Advances in Computer Vision and Pattern Recognition},
	pages = {189--209},
}

@book{quinonero-candela_dataset_2009,
	address = {Cambridge, Mass},
	series = {Neural information processing series},
	title = {Dataset shift in machine learning},
	isbn = {978-0-262-17005-5},
	language = {en},
	publisher = {MIT Press},
	editor = {Quiñonero-Candela, Joaquin},
	year = {2009},
	note = {OCLC: ocn227205909},
	keywords = {Machine learning},
}

@article{kumar_understanding_2020,
	title = {Understanding {Self}-{Training} for {Gradual} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/2002.11361},
	abstract = {Machine learning systems must adapt to data distributions that evolve over time, in applications ranging from sensor networks and self-driving car perception modules to brain-machine interfaces. We consider gradual domain adaptation, where the goal is to adapt an initial classifier trained on a source domain given only unlabeled data that shifts gradually in distribution towards a target domain. We prove the first non-vacuous upper bound on the error of self-training with gradual shifts, under settings where directly adapting to the target domain can result in unbounded error. The theoretical analysis leads to algorithmic insights, highlighting that regularization and label sharpening are essential even when we have infinite data, and suggesting that self-training works particularly well for shifts with small Wasserstein-infinity distance. Leveraging the gradual shift structure leads to higher accuracies on a rotating MNIST dataset and a realistic Portraits dataset.},
	urldate = {2021-03-09},
	journal = {arXiv:2002.11361 [cs, stat]},
	author = {Kumar, Ananya and Ma, Tengyu and Liang, Percy},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.11361
version: 1},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bickel_discriminative_nodate,
	title = {Discriminative {Learning} {Under} {Covariate} {Shift}},
	abstract = {We address classiﬁcation problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution—problems also referred to as classiﬁcation under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classiﬁer for covariate shift. The optimization problem is convex under certain conditions; our ﬁndings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam ﬁltering, text classiﬁcation, and landmine detection.},
	language = {en},
	author = {Bickel, Steffen and De, Cs Uni-Potsdam and De, Cs Uni-Potsdam and De, Cs Uni-Potsdam},
	pages = {19},
}

@article{daume_iii_frustratingly_2009,
	title = {Frustratingly {Easy} {Domain} {Adaptation}},
	url = {http://arxiv.org/abs/0907.1815},
	abstract = {We describe an approach to domain adaptation that is appropriate exactly in the case when one has enough ``target'' data to do slightly better than just using only ``source'' data. Our approach is incredibly simple, easy to implement as a preprocessing step (10 lines of Perl!) and outperforms state-of-the-art approaches on a range of datasets. Moreover, it is trivially extended to a multi-domain adaptation problem, where one has data from a variety of different domains.},
	urldate = {2021-03-09},
	journal = {arXiv:0907.1815 [cs]},
	author = {Daumé III, Hal},
	month = jul,
	year = {2009},
	note = {arXiv: 0907.1815},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{acharya_estimating_2016,
	title = {Estimating {Renyi} {Entropy} of {Discrete} {Distributions}},
	url = {http://arxiv.org/abs/1408.1000},
	abstract = {It was recently shown that estimating the Shannon entropy H(p) of a discrete k-symbol distribution p requires Θ(k/ log k) samples, a number that grows near-linearly in the support size. In many applications H(p) can be replaced by the more general Re´nyi entropy of order α, Hα(p). We determine the number of samples needed to estimate Hα(p) for all α, showing that α {\textless} 1 requires a super-linear, roughly k1/α samples, noninteger α {\textgreater} 1 requires a near-linear k samples, but, perhaps surprisingly, integer α {\textgreater} 1 requires only Θ(k1−1/α) samples. Furthermore, developing on a recently established connection between polynomial approximation and estimation of additive functions of the form ∑x f (px), we reduce the sample complexity for noninteger values of α by a factor of log k compared to the empirical estimator. The estimators achieving these bounds are simple and run in time linear in the number of samples. Our lower bounds provide explicit constructions of distributions with different Re´nyi entropies that are hard to distinguish.},
	language = {en},
	urldate = {2021-03-09},
	journal = {arXiv:1408.1000 [cs, math]},
	author = {Acharya, Jayadev and Orlitsky, Alon and Suresh, Ananda Theertha and Tyagi, Himanshu},
	month = mar,
	year = {2016},
	note = {arXiv: 1408.1000},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Information Theory, Computer Science - Machine Learning},
}

@article{antos_convergence_2001,
	title = {Convergence properties of functional estimates for discrete distributions},
	volume = {19},
	copyright = {Copyright © 2001 John Wiley \& Sons, Inc.},
	issn = {1098-2418},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rsa.10019},
	doi = {https://doi.org/10.1002/rsa.10019},
	abstract = {Suppose P is an arbitrary discrete distribution on acountable alphabet 𝒳. Given an i.i.d. sample (X1,…,Xn) drawnfrom P, we consider the problem of estimating the entropy H(P) or some other functional F=F(P) of the unknown distribution P. We show that, for additive functionals satisfying mild conditions (including the cases of the mean, the entropy, and mutual information), the plug-in estimates of F are universally consistent. We also prove that, without further assumptions, no rate-of-convergence results can be obtained for any sequence of estimators. In the case of entropy estimation, under a variety of different assumptions, we get rate-of-convergence results for the plug-in estimate and for a nonparametric estimator based on match-lengths. The behavior of the variance and the expected error of the plug-in estimate is shown to be in sharp contrast to the finite-alphabet case. A number of other important examples of functionals are also treated in some detail. © 2001 John Wiley \& Sons, Inc. Random Struct. Alg., 19: 163–193, 2001},
	language = {en},
	number = {3-4},
	urldate = {2021-03-09},
	journal = {Random Structures \& Algorithms},
	author = {Antos, András and Kontoyiannis, Ioannis},
	year = {2001},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/rsa.10019},
	keywords = {entropy estimation, functional estimation, match lengths, rates of convergence},
	pages = {163--193},
}

@article{jiao_minimax_2015,
	title = {Minimax {Estimation} of {Functionals} of {Discrete} {Distributions}},
	url = {http://arxiv.org/abs/1406.6956},
	abstract = {We propose a general methodology for the construction and analysis of minimax estimators for a wide class of functionals of finite dimensional parameters, and elaborate on the case of discrete distributions, where the alphabet size \$S\$ is unknown and may be comparable with the number of observations \$n\$. We treat the respective regions where the functional is "nonsmooth" and "smooth" separately. In the "nonsmooth" regime, we apply an unbiased estimator for the best polynomial approximation of the functional whereas, in the "smooth" regime, we apply a bias-corrected Maximum Likelihood Estimator (MLE). We illustrate the merit of this approach by thoroughly analyzing two important cases: the entropy \$H(P) = {\textbackslash}sum\_\{i = 1\}{\textasciicircum}S -p\_i {\textbackslash}ln p\_i\$ and \$F\_{\textbackslash}alpha(P) = {\textbackslash}sum\_\{i = 1\}{\textasciicircum}S p\_i{\textasciicircum}{\textbackslash}alpha,{\textbackslash}alpha{\textgreater}0\$. We obtain the minimax \$L\_2\$ rates for estimating these functionals. In particular, we demonstrate that our estimator achieves the optimal sample complexity \$n {\textbackslash}asymp S/{\textbackslash}ln S\$ for entropy estimation. We also show that the sample complexity for estimating \$F\_{\textbackslash}alpha(P),0{\textless}{\textbackslash}alpha{\textless}1\$ is \$n{\textbackslash}asymp S{\textasciicircum}\{1/{\textbackslash}alpha\}/ {\textbackslash}ln S\$, which can be achieved by our estimator but not the MLE. For \$1{\textless}{\textbackslash}alpha{\textless}3/2\$, we show the minimax \$L\_2\$ rate for estimating \$F\_{\textbackslash}alpha(P)\$ is \$(n{\textbackslash}ln n){\textasciicircum}\{-2({\textbackslash}alpha-1)\}\$ regardless of the alphabet size, while the \$L\_2\$ rate for the MLE is \$n{\textasciicircum}\{-2({\textbackslash}alpha-1)\}\$. For all the above cases, the behavior of the minimax rate-optimal estimators with \$n\$ samples is essentially that of the MLE with \$n{\textbackslash}ln n\$ samples. We highlight the practical advantages of our schemes for entropy and mutual information estimation. We demonstrate that our approach reduces running time and boosts the accuracy compared to existing various approaches. Moreover, we show that the mutual information estimator induced by our methodology leads to significant performance boosts over the Chow--Liu algorithm in learning graphical models.},
	urldate = {2021-03-09},
	journal = {arXiv:1406.6956 [cs, math, stat]},
	author = {Jiao, Jiantao and Venkat, Kartik and Han, Yanjun and Weissman, Tsachy},
	month = mar,
	year = {2015},
	note = {arXiv: 1406.6956},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
}

@article{wu_minimax_2016,
	title = {Minimax rates of entropy estimation on large alphabets via best polynomial approximation},
	url = {http://arxiv.org/abs/1407.0381},
	abstract = {Consider the problem of estimating the Shannon entropy of a distribution over \$k\$ elements from \$n\$ independent samples. We show that the minimax mean-square error is within universal multiplicative constant factors of \$\${\textbackslash}Big({\textbackslash}frac\{k \}\{n {\textbackslash}log k\}{\textbackslash}Big){\textasciicircum}2 + {\textbackslash}frac\{{\textbackslash}log{\textasciicircum}2 k\}\{n\}\$\$ if \$n\$ exceeds a constant factor of \${\textbackslash}frac\{k\}\{{\textbackslash}log k\}\$; otherwise there exists no consistent estimator. This refines the recent result of Valiant-Valiant {\textbackslash}cite\{VV11\} that the minimal sample size for consistent entropy estimation scales according to \${\textbackslash}Theta({\textbackslash}frac\{k\}\{{\textbackslash}log k\})\$. The apparatus of best polynomial approximation plays a key role in both the construction of optimal estimators and, via a duality argument, the minimax lower bound.},
	urldate = {2021-03-09},
	journal = {arXiv:1407.0381 [cs, math, stat]},
	author = {Wu, Yihong and Yang, Pengkun},
	month = feb,
	year = {2016},
	note = {arXiv: 1407.0381},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
}

@inproceedings{valiant_estimating_2011,
	address = {San Jose, California, USA},
	title = {Estimating the unseen: an n/log(n)-sample estimator for entropy and support size, shown optimal via new {CLTs}},
	isbn = {978-1-4503-0691-1},
	shorttitle = {Estimating the unseen},
	url = {http://portal.acm.org/citation.cfm?doid=1993636.1993727},
	doi = {10.1145/1993636.1993727},
	abstract = {We introduce a new approach to characterizing the unobserved portion of a distribution, which provides sublinear–sample estimators achieving arbitrarily small additive constant error for a class of properties that includes entropy and distribution support size. Additionally, we show new matching lower bounds. Together, this settles the longstanding question of the sample complexities of these estimation problems, up to constant factors.},
	language = {en},
	urldate = {2021-03-09},
	booktitle = {Proceedings of the 43rd annual {ACM} symposium on {Theory} of computing - {STOC} '11},
	publisher = {ACM Press},
	author = {Valiant, Gregory and Valiant, Paul},
	year = {2011},
	pages = {685},
}

@article{han_minimax_nodate,
	title = {Minimax {Rate}-optimal {Estimation} of {KL} {Divergence} between {Discrete} {Distributions}},
	abstract = {We consider the problem of estimating the KL divergence between two discrete probability measures P and Q from empirical data in a non-asymptotic and possibly large alphabet setting. We construct minimax rate-optimal estimators for D(P Q) when the likelihood ratio is upper bounded by a constant which may depend on the support size, and show that the performance of the optimal estimator with n samples is essentially that of the Maximum Likelihood Estimator (MLE) with n ln n samples. Our estimator is adaptive in the sense that it does not require the knowledge of the support size nor the upper bound on the likelihood ratio. Our approach reﬁnes the Approximation methodology recently developed for the construction of near minimax estimators of functionals of highdimensional parameters, such as entropy, mutual information and 1 distance in large alphabet settings, and shows that the effective sample size enlargement phenomenon holds signiﬁcantly more widely than previously established.},
	language = {en},
	author = {Han, Yanjun and Jiao, Jiantao and Weissman, Tsachy},
	pages = {5},
}

@article{johansson_support_2019,
	title = {Support and {Invertibility} in {Domain}-{Invariant} {Representations}},
	url = {http://arxiv.org/abs/1903.03448},
	abstract = {Learning domain-invariant representations has become a popular approach to unsupervised domain adaptation and is often justified by invoking a particular suite of theoretical results. We argue that there are two significant flaws in such arguments. First, the results in question hold only for a fixed representation and do not account for information lost in non-invertible transformations. Second, domain invariance is often a far too strict requirement and does not always lead to consistent estimation, even under strong and favorable assumptions. In this work, we give generalization bounds for unsupervised domain adaptation that hold for any representation function by acknowledging the cost of non-invertibility. In addition, we show that penalizing distance between densities is often wasteful and propose a bound based on measuring the extent to which the support of the source domain covers the target domain. We perform experiments on well-known benchmarks that illustrate the short-comings of current standard practice.},
	urldate = {2021-03-09},
	journal = {arXiv:1903.03448 [cs, stat]},
	author = {Johansson, Fredrik D. and Sontag, David and Ranganath, Rajesh},
	month = jul,
	year = {2019},
	note = {arXiv: 1903.03448},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maurer_benet_2016,
	title = {The {Beneﬁt} of {Multitask} {Representation} {Learning}},
	abstract = {We discuss a general method to learn data representations from multiple tasks. We provide a justiﬁcation for this method in both settings of multitask learning and learning-to-learn. The method is illustrated in detail in the special case of linear feature learning. Conditions on the theoretical advantage oﬀered by multitask representation learning over independent task learning are established. In particular, focusing on the important example of half-space learning, we derive the regime in which multitask representation learning is beneﬁcial over independent task learning, as a function of the sample size, the number of tasks and the intrinsic data dimensionality. Other potential applications of our results include multitask feature learning in reproducing kernel Hilbert spaces and multilayer, deep networks.},
	language = {en},
	author = {Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
	year = {2016},
	pages = {32},
}

@article{ando_framework_2005,
	title = {A {Framework} for {Learning} {Predictive} {Structures} from {Multiple} {Tasks} and {Unlabeled} {Data}},
	abstract = {One of the most important issues in machine learning is whether one can improve the performance of a supervised learning algorithm by including unlabeled data. Methods that use both labeled and unlabeled data are generally referred to as semi-supervised learning. Although a number of such methods are proposed, at the current stage, we still don’t have a complete understanding of their eﬀectiveness. This paper investigates a closely related problem, which leads to a novel approach to semi-supervised learning. Speciﬁcally we consider learning predictive structures on hypothesis spaces (that is, what kind of classiﬁers have good predictive power) from multiple learning tasks. We present a general framework in which the structural learning problem can be formulated and analyzed theoretically, and relate it to learning with unlabeled data. Under this framework, algorithms for structural learning will be proposed, and computational issues will be investigated. Experiments will be given to demonstrate the eﬀectiveness of the proposed algorithms in the semi-supervised learning setting.},
	language = {en},
	author = {Ando, Rie Kubota and Zhang, Tong},
	year = {2005},
	pages = {37},
}

@article{baxter_model_2000,
	title = {A {Model} of {Inductive} {Bias} {Learning}},
	volume = {12},
	issn = {1076-9757},
	url = {http://arxiv.org/abs/1106.0245},
	doi = {10.1613/jair.731},
	abstract = {A major problem in machine learning is that of inductive bias: how to choose a learner's hypothesis space so that it is large enough to contain a solution to the problem being learnt, yet small enough to ensure reliable generalization from reasonably-sized training sets. Typically such bias is supplied by hand through the skill and insights of experts. In this paper a model for automatically learning bias is investigated. The central assumption of the model is that the learner is embedded within an environment of related learning tasks. Within such an environment the learner can sample from multiple tasks, and hence it can search for a hypothesis space that contains good solutions to many of the problems in the environment. Under certain restrictions on the set of all hypothesis spaces available to the learner, we show that a hypothesis space that performs well on a sufficiently large number of training tasks will also perform well when learning novel tasks in the same environment. Explicit bounds are also derived demonstrating that learning multiple tasks within an environment of related tasks can potentially give much better generalization than learning a single task.},
	urldate = {2021-03-08},
	journal = {Journal of Artificial Intelligence Research},
	author = {Baxter, J.},
	month = mar,
	year = {2000},
	note = {arXiv: 1106.0245},
	keywords = {Computer Science - Artificial Intelligence},
	pages = {149--198},
}

@article{mehta_minimax_2012,
	title = {Minimax {Multi}-{Task} {Learning} and a {Generalized} {Loss}-{Compositional} {Paradigm} for {MTL}},
	url = {http://arxiv.org/abs/1209.2784},
	abstract = {Since its inception, the modus operandi of multi-task learning (MTL) has been to minimize the task-wise mean of the empirical risks. We introduce a generalized loss-compositional paradigm for MTL that includes a spectrum of formulations as a subfamily. One endpoint of this spectrum is minimax MTL: a new MTL formulation that minimizes the maximum of the tasks' empirical risks. Via a certain relaxation of minimax MTL, we obtain a continuum of MTL formulations spanning minimax MTL and classical MTL. The full paradigm itself is loss-compositional, operating on the vector of empirical risks. It incorporates minimax MTL, its relaxations, and many new MTL formulations as special cases. We show theoretically that minimax MTL tends to avoid worst case outcomes on newly drawn test tasks in the learning to learn (LTL) test setting. The results of several MTL formulations on synthetic and real problems in the MTL and LTL test settings are encouraging.},
	urldate = {2021-03-08},
	journal = {arXiv:1209.2784 [cs, stat]},
	author = {Mehta, Nishant A. and Lee, Dongryeol and Gray, Alexander G.},
	month = sep,
	year = {2012},
	note = {arXiv: 1209.2784},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{deng_imagenet_2009,
	title = {{ImageNet}: {A} large-scale hierarchical image database},
	shorttitle = {{ImageNet}},
	doi = {10.1109/CVPR.2009.5206848},
	abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
	booktitle = {2009 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	month = jun,
	year = {2009},
	note = {ISSN: 1063-6919},
	keywords = {Explosions, Image databases, Image retrieval, ImageNet database, Information retrieval, Internet, Large-scale systems, Multimedia databases, Ontologies, Robustness, Spine, computer vision, image resolution, image retrieval, large-scale hierarchical image database, large-scale ontology, multimedia computing, multimedia data, ontologies (artificial intelligence), subtree, trees (mathematics), very large databases, visual databases, wordNet structure},
	pages = {248--255},
}

@article{rissanen_generalized_1976,
	title = {Generalized {Kraft} {Inequality} and {Arithmetic} {Coding}},
	volume = {20},
	issn = {0018-8646},
	doi = {10.1147/rd.203.0198},
	abstract = {Algorithms for encoding and decoding finite strings over a finite alphabet are described. The coding operations are arithmetic involving rational numbers li as parameters such that ∑i2−li≤2−ε. This coding technique requires no blocking, and the per-symbol length of the encoded string approaches the associated entropy within ε. The coding speed is comparable to that of conventional coding methods.},
	number = {3},
	journal = {IBM Journal of Research and Development},
	author = {Rissanen, Jorma J.},
	month = may,
	year = {1976},
	note = {Conference Name: IBM Journal of Research and Development},
	pages = {198--203},
}

@inproceedings{massey_entropy_1988,
	title = {On the entropy of integer-valued random variables},
	booktitle = {Int. {Workshop} on {Inf}. {Theory}},
	author = {Massey, James L.},
	year = {1988},
}

@book{lehmann_testing_2005,
	address = {New York},
	edition = {3rd ed},
	series = {Springer texts in statistics},
	title = {Testing statistical hypotheses},
	isbn = {978-0-387-98864-1},
	language = {en},
	publisher = {Springer},
	author = {Lehmann, Erich L. and Romano, Joseph P.},
	year = {2005},
	keywords = {Statistical hypothesis testing},
}

@article{rashevsky_life_1955,
	title = {Life, information theory, and topology},
	volume = {17},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02477860},
	doi = {10.1007/BF02477860},
	abstract = {The information content of an organism determines to a large extent its ability to perform the basic vital functions: selection of food, breaking up of the food molecules into appropriate parts, selection of those parts, and their assimilation. The information content needed is very large and requires a sufficiently large complexity of the organism. The information content of an organism is largely determined by the information content of the constituent organic molecules. The information content of the latter is in its turn determined by the number of physically distinguishable atoms or radicals of which the molecule is composed. The different arrangements of atoms in a molecule are represented by the structural formula, which is basically a graph. It is shown that the topology of this graph also determines to a large extent the information content. Different points of a graph may be physically indistinguishable; in general, however, they are different in regard to their topological properties. A study of the relations between the topological properties of graphs and their information content is suggested, and several theorems are demonstrated. A relation between topology and living processes is thus found also on the molecular level.},
	language = {en},
	number = {3},
	urldate = {2020-11-17},
	journal = {The bulletin of mathematical biophysics},
	author = {Rashevsky, Nicolas},
	month = sep,
	year = {1955},
	pages = {229--235},
}

@inproceedings{wallace_classification_1990,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Classification by {Minimum}-{Message}-{Length} {Inference}},
	volume = {468},
	url = {https://doi.org/10.1007/3-540-53504-7_63},
	doi = {10.1007/3-540-53504-7_63},
	booktitle = {Advances in {Computing} and {Information} - {ICCI}'90, {International} {Conference} on {Computing} and {Information}, {Niagara} {Falls}, {Canada}, {May} 23-26, 1990, {Proceedings}},
	publisher = {Springer},
	author = {Wallace, Chris S.},
	editor = {Akl, Selim G. and Fiala, Frantisek and Koczkodaj, Waldemar W.},
	year = {1990},
	pages = {72--81},
}

@article{tishby_information_2000,
	title = {The information bottleneck method},
	volume = {physics/0004057},
	url = {http://arxiv.org/abs/physics/0004057},
	journal = {CoRR},
	author = {Tishby, Naftali and Pereira, Fernando C. N. and Bialek, William},
	year = {2000},
}

@inproceedings{theis_lossy_2017,
	title = {Lossy {Image} {Compression} with {Compressive} {Autoencoders}},
	url = {https://openreview.net/forum?id=rJiNwv9gg},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
	year = {2017},
}

@inproceedings{poole_variational_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {On {Variational} {Bounds} of {Mutual} {Information}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/poole19a.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Poole, Ben and Ozair, Sherjil and Oord, Aäron van den and Alemi, Alex and Tucker, George},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	pages = {5171--5180},
}

@inproceedings{mcallester_formal_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Formal {Limitations} on the {Measurement} of {Mutual} {Information}},
	volume = {108},
	url = {http://proceedings.mlr.press/v108/mcallester20a.html},
	booktitle = {The 23rd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}, {AISTATS} 2020, 26-28 {August} 2020, {Online} [{Palermo}, {Sicily}, {Italy}]},
	publisher = {PMLR},
	author = {McAllester, David and Stratos, Karl},
	editor = {Chiappa, Silvia and Calandra, Roberto},
	year = {2020},
	pages = {875--884},
}

@inproceedings{lee_context-adaptive_2019,
	title = {Context-adaptive {Entropy} {Model} for {End}-to-end {Optimized} {Image} {Compression}},
	url = {https://openreview.net/forum?id=HyxKIiAqYQ},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Lee, Jooyoung and Cho, Seunghyun and Beack, Seung-Kwon},
	year = {2019},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	number = {11},
	journal = {Proceedings of the IEEE},
	author = {LeCun, Yann and Bottou, Léon and Bengio, Yoshua and Haffner, Patrick},
	year = {1998},
	note = {Publisher: Ieee},
	pages = {2278--2324},
}

@inproceedings{townsend_practical_2019,
	title = {Practical lossless compression with latent variables using bits back coding},
	url = {https://openreview.net/forum?id=ryE98iR5tm},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Townsend, James and Bird, Thomas and Barber, David},
	year = {2019},
}

@inproceedings{kingma_auto-encoding_2014,
	title = {Auto-{Encoding} {Variational} {Bayes}},
	url = {http://arxiv.org/abs/1312.6114},
	booktitle = {2nd {International} {Conference} on {Learning} {Representations}, {ICLR} 2014, {Banff}, {AB}, {Canada}, {April} 14-16, 2014, {Conference} {Track} {Proceedings}},
	author = {Kingma, Diederik P. and Welling, Max},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2014},
}

@inproceedings{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	url = {https://proceedings.neurips.cc/paper/2019/hash/bdbca288fee7f92f2bfa9f7012727740-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2019, {NeurIPS} 2019, {December} 8-14, 2019, {Vancouver}, {BC}, {Canada}},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché-Buc, Florence and Fox, Emily B. and Garnett, Roman},
	year = {2019},
	pages = {8024--8035},
}

@inproceedings{kingma_adam_2015,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	url = {http://arxiv.org/abs/1412.6980},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015, {San} {Diego}, {CA}, {USA}, {May} 7-9, 2015, {Conference} {Track} {Proceedings}},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	editor = {Bengio, Yoshua and LeCun, Yann},
	year = {2015},
}

@inproceedings{ioffe_batch_2015,
	series = {{JMLR} {Workshop} and {Conference} {Proceedings}},
	title = {Batch {Normalization}: {Accelerating} {Deep} {Network} {Training} by {Reducing} {Internal} {Covariate} {Shift}},
	volume = {37},
	url = {http://proceedings.mlr.press/v37/ioffe15.html},
	booktitle = {Proceedings of the 32nd {International} {Conference} on {Machine} {Learning}, {ICML} 2015, {Lille}, {France}, 6-11 {July} 2015},
	publisher = {JMLR.org},
	author = {Ioffe, Sergey and Szegedy, Christian},
	editor = {Bach, Francis R. and Blei, David M.},
	year = {2015},
	pages = {448--456},
}

@inproceedings{he_delving_2015,
	title = {Delving {Deep} into {Rectifiers}: {Surpassing} {Human}-{Level} {Performance} on {ImageNet} {Classification}},
	url = {https://doi.org/10.1109/ICCV.2015.123},
	doi = {10.1109/ICCV.2015.123},
	booktitle = {2015 {IEEE} {International} {Conference} on {Computer} {Vision}, {ICCV} 2015, {Santiago}, {Chile}, {December} 7-13, 2015},
	publisher = {IEEE Computer Society},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	year = {2015},
	pages = {1026--1034},
}

@inproceedings{flamich_compressing_2020,
	title = {Compressing {Images} by {Encoding} {Their} {Latent} {Representations} with {Relative} {Entropy} {Coding}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/ba053350fe56ed93e64b3e769062b680-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Flamich, Gergely and Havasi, Marton and Hernández-Lobato, José Miguel},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@article{wu_learnability_2019,
	title = {Learnability for the {Information} {Bottleneck}},
	volume = {21},
	url = {https://doi.org/10.3390/e21100924},
	doi = {10.3390/e21100924},
	number = {10},
	journal = {Entropy},
	author = {Wu, Tailin and Fischer, Ian S. and Chuang, Isaac L. and Tegmark, Max},
	year = {2019},
	pages = {924},
}

@article{fischer_conditional_2020,
	title = {The {Conditional} {Entropy} {Bottleneck}},
	volume = {22},
	url = {https://doi.org/10.3390/e22090999},
	doi = {10.3390/e22090999},
	number = {9},
	journal = {Entropy},
	author = {Fischer, Ian S.},
	year = {2020},
	pages = {999},
}

@article{duda_asymmetric_2009,
	title = {Asymmetric numeral systems},
	volume = {abs/0902.0271},
	url = {http://arxiv.org/abs/0902.0271},
	journal = {CoRR},
	author = {Duda, Jarek},
	year = {2009},
	note = {\_eprint: 0902.0271},
}

@inproceedings{dubois_learning_2020,
	title = {Learning {Optimal} {Representations} with the {Decodable} {Information} {Bottleneck}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/d8ea5f53c1b1eb087ac2e356253395d8-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Dubois, Yann and Kiela, Douwe and Schwab, David J. and Vedantam, Ramakrishna},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@inproceedings{chen_simple_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	volume = {119},
	url = {http://proceedings.mlr.press/v119/chen20j.html},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}, {ICML} 2020, 13-18 {July} 2020, {Virtual} {Event}},
	publisher = {PMLR},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey E.},
	year = {2020},
	pages = {1597--1607},
}

@article{lyle_benefits_2020,
	title = {On the {Benefits} of {Invariance} in {Neural} {Networks}},
	volume = {abs/2005.00178},
	url = {https://arxiv.org/abs/2005.00178},
	journal = {CoRR},
	author = {Lyle, Clare and Wilk, Mark van der and Kwiatkowska, Marta and Gal, Yarin and Bloem-Reddy, Benjamin},
	year = {2020},
	note = {\_eprint: 2005.00178},
}

@article{bloem-reddy_probabilistic_2020,
	title = {Probabilistic {Symmetries} and {Invariant} {Neural} {Networks}},
	volume = {21},
	url = {http://jmlr.org/papers/v21/19-322.html},
	journal = {J. Mach. Learn. Res.},
	author = {Bloem-Reddy, Benjamin and Teh, Yee Whye},
	year = {2020},
	pages = {90:1--90:61},
}

@inproceedings{singh_end--end_2020,
	title = {End-to-{End} {Learning} of {Compressible} {Features}},
	url = {https://doi.org/10.1109/ICIP40778.2020.9190860},
	doi = {10.1109/ICIP40778.2020.9190860},
	booktitle = {{IEEE} {International} {Conference} on {Image} {Processing}, {ICIP} 2020, {Abu} {Dhabi}, {United} {Arab} {Emirates}, {October} 25-28, 2020},
	publisher = {IEEE},
	author = {Singh, Saurabh and Abu-El-Haija, Sami and Johnston, Nick and Ballé, Johannes and Shrivastava, Abhinav and Toderici, George},
	year = {2020},
	pages = {3349--3353},
}

@inproceedings{balle_variational_2018,
	title = {Variational image compression with a scale hyperprior},
	url = {https://openreview.net/forum?id=rkcQFMZRb},
	booktitle = {6th {International} {Conference} on {Learning} {Representations}, {ICLR} 2018, {Vancouver}, {BC}, {Canada}, {April} 30 - {May} 3, 2018, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Ballé, Johannes and Minnen, David and Singh, Saurabh and Hwang, Sung Jin and Johnston, Nick},
	year = {2018},
}

@article{balle_nonlinear_2020,
	title = {Nonlinear {Transform} {Coding}},
	volume = {abs/2007.03034},
	url = {https://arxiv.org/abs/2007.03034},
	journal = {CoRR},
	author = {Ballé, Johannes and Chou, Philip A. and Minnen, David and Singh, Saurabh and Johnston, Nick and Agustsson, Eirikur and Hwang, Sung Jin and Toderici, George},
	year = {2020},
	note = {\_eprint: 2007.03034},
}

@inproceedings{balle_end--end_2017,
	title = {End-to-end {Optimized} {Image} {Compression}},
	url = {https://openreview.net/forum?id=rJxdQ3jeg},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
	year = {2017},
}

@inproceedings{alemi_deep_2017,
	title = {Deep {Variational} {Information} {Bottleneck}},
	url = {https://openreview.net/forum?id=HyxQzBceg},
	booktitle = {5th {International} {Conference} on {Learning} {Representations}, {ICLR} 2017, {Toulon}, {France}, {April} 24-26, 2017, {Conference} {Track} {Proceedings}},
	publisher = {OpenReview.net},
	author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
	year = {2017},
}

@inproceedings{agustsson_universally_2020,
	title = {Universally {Quantized} {Neural} {Compression}},
	url = {https://proceedings.neurips.cc/paper/2020/hash/92049debbe566ca5782a3045cf300a3c-Abstract.html},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 33: {Annual} {Conference} on {Neural} {Information} {Processing} {Systems} 2020, {NeurIPS} 2020, {December} 6-12, 2020, virtual},
	author = {Agustsson, Eirikur and Theis, Lucas},
	editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
	year = {2020},
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	urldate = {2021-02-26},
	journal = {arXiv:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{he_deep_2016,
	address = {Las Vegas, NV, USA},
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780459/},
	doi = {10.1109/CVPR.2016.90},
	urldate = {2021-02-25},
	booktitle = {2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = jun,
	year = {2016},
	pages = {770--778},
}

@inproceedings{wyatt_method_1988,
	title = {A {Method} for the {Design} of {Stable} {Lateral} {Inhibition} {Networks} that is {Robust} in the {Presence} of {Circuit} {Parasitics}},
	url = {https://proceedings.neurips.cc/paper/1987/file/a684eceee76fc522773286a895bc8436-Paper.pdf},
	urldate = {2021-02-25},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Wyatt, John and Standley, D.},
	editor = {Anderson, D.},
	year = {1988},
}

@article{lee_self-attention_2019,
	title = {Self-{Attention} {Graph} {Pooling}},
	url = {http://arxiv.org/abs/1904.08082},
	abstract = {Advanced methods of applying deep learning to structured data such as graphs have been proposed in recent years. In particular, studies have focused on generalizing convolutional neural networks to graph data, which includes redefining the convolution and the downsampling (pooling) operations for graphs. The method of generalizing the convolution operation to graphs has been proven to improve performance and is widely used. However, the method of applying downsampling to graphs is still difficult to perform and has room for improvement. In this paper, we propose a graph pooling method based on self-attention. Self-attention using graph convolution allows our pooling method to consider both node features and graph topology. To ensure a fair comparison, the same training procedures and model architectures were used for the existing pooling methods and our method. The experimental results demonstrate that our method achieves superior graph classification performance on the benchmark datasets using a reasonable number of parameters.},
	urldate = {2021-01-28},
	journal = {arXiv:1904.08082 [cs, stat]},
	author = {Lee, Junhyun and Lee, Inyeop and Kang, Jaewoo},
	month = jun,
	year = {2019},
	note = {arXiv: 1904.08082},
	keywords = {Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
}

@article{mesquita_rethinking_nodate,
	title = {Rethinking pooling in graph neural networks},
	abstract = {Graph pooling is a central component of a myriad of graph neural network (GNN) architectures. As an inheritance from traditional CNNs, most approaches formulate graph pooling as a cluster assignment problem, extending the idea of local patches in regular grids to graphs. Despite the wide adherence to this design choice, no work has rigorously evaluated its inﬂuence on the success of GNNs. In this paper, we build upon representative GNNs and introduce variants that challenge the need for locality-preserving representations, either using randomization or clustering on the complement graph. Strikingly, our experiments demonstrate that using these variants does not result in any decrease in performance. To understand this phenomenon, we study the interplay between convolutional layers and the subsequent pooling ones. We show that the convolutions play a leading role in the learned representations. In contrast to the common belief, local pooling is not responsible for the success of GNNs on relevant and widely-used benchmarks.},
	language = {en},
	author = {Mesquita, Diego and Souza, Amauri H and Kaski, Samuel},
	pages = {12},
}

@article{ying_hierarchical_2019,
	title = {Hierarchical {Graph} {Representation} {Learning} with {Differentiable} {Pooling}},
	url = {http://arxiv.org/abs/1806.08804},
	abstract = {Recently, graph neural networks (GNNs) have revolutionized the field of graph representation learning through effectively learned node embeddings, and achieved state-of-the-art results in tasks such as node classification and link prediction. However, current GNN methods are inherently flat and do not learn hierarchical representations of graphs---a limitation that is especially problematic for the task of graph classification, where the goal is to predict the label associated with an entire graph. Here we propose DiffPool, a differentiable graph pooling module that can generate hierarchical representations of graphs and can be combined with various graph neural network architectures in an end-to-end fashion. DiffPool learns a differentiable soft cluster assignment for nodes at each layer of a deep GNN, mapping nodes to a set of clusters, which then form the coarsened input for the next GNN layer. Our experimental results show that combining existing GNN methods with DiffPool yields an average improvement of 5-10\% accuracy on graph classification benchmarks, compared to all existing pooling approaches, achieving a new state-of-the-art on four out of five benchmark data sets.},
	urldate = {2021-01-20},
	journal = {arXiv:1806.08804 [cs, stat]},
	author = {Ying, Rex and You, Jiaxuan and Morris, Christopher and Ren, Xiang and Hamilton, William L. and Leskovec, Jure},
	month = feb,
	year = {2019},
	note = {arXiv: 1806.08804},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Social and Information Networks, Statistics - Machine Learning},
}

@inproceedings{atiya_learning_1988,
	title = {Learning on a {General} {Network}},
	url = {https://proceedings.neurips.cc/paper/1987/file/45c48cce2e2d7fbdea1afc51c7c6ad26-Paper.pdf},
	urldate = {2021-01-05},
	booktitle = {Neural {Information} {Processing} {Systems}},
	publisher = {American Institute of Physics},
	author = {Atiya, Amir},
	editor = {Anderson, D.},
	year = {1988},
	pages = {22--30},
}

@article{liese_divergences_2006,
	title = {On {Divergences} and {Informations} in {Statistics} and {Information} {Theory}},
	volume = {52},
	doi = {10.1109/TIT.2006.881731},
	abstract = {The paper deals with the f-divergences of Csiszar generalizing the discrimination information of Kullback, the total variation distance, the Hellinger divergence, and the Pearson divergence. All basic properties of f-divergences including relations to the decision errors are proved in a new manner replacing the classical Jensen inequality by a new generalized Taylor expansion of convex functions. Some new properties are proved too, e.g., relations to the statistical sufficiency and deficiency. The generalized Taylor expansion also shows very easily that all f-divergences are average statistical informations (differences between prior and posterior Bayes errors) mutually differing only in the weights imposed on various prior distributions. The statistical information introduced by De Groot and the classical information of Shannon are shown to be extremal cases corresponding to alpha=0 and alpha=1 in the class of the so-called Arimoto alpha-informations introduced in this paper for 0},
	journal = {Information Theory, IEEE Transactions on},
	author = {Liese, Friedrich and Vajda, Igor},
	month = nov,
	year = {2006},
	pages = {4394--4412},
}

@article{nielsen_elementary_2020,
	title = {An elementary introduction to information geometry},
	volume = {22},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1808.08271},
	doi = {10.3390/e22101100},
	abstract = {In this survey, we describe the fundamental differential-geometric structures of information manifolds, state the fundamental theorem of information geometry, and illustrate some use cases of these information manifolds in information sciences. The exposition is self-contained by concisely introducing the necessary concepts of differential geometry, but proofs are omitted for brevity.},
	number = {10},
	urldate = {2021-01-03},
	journal = {Entropy},
	author = {Nielsen, Frank},
	month = sep,
	year = {2020},
	note = {arXiv: 1808.08271},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1100},
}

@article{petersson_quantum_2020,
	title = {Quantum {Physics} without the {Physics}},
	url = {http://arxiv.org/abs/2012.03865},
	abstract = {This report explains the basic theory and common terminology of quantum physics without assuming any knowledge of physics. It was written by a group of applied mathematicians while they were reading up on the subject. The intended audience consists of applied mathematicians, computer scientists, or anyone else who wants to improve their understanding of quantum physics. We assume that the reader is familiar with fundamental concepts of linear algebra, differential equations, and to some extent the theory of Hilbert spaces. Most of the material can be found in the book by Nielsen and Chuang and in the lecture notes on open quantum systems by Lidar. Another excellent online source of information is Wikipedia, even though most of its articles on quantum physics assume a solid understanding of physics.},
	urldate = {2021-01-03},
	journal = {arXiv:2012.03865 [math-ph, physics:quant-ph]},
	author = {Petersson, N. Anders and Garcia, Fortino and Appelo, Daniel E. A. and Guenther, Stefanie and Choi, Younsoo and Vogt, Ryan},
	month = dec,
	year = {2020},
	note = {arXiv: 2012.03865},
	keywords = {65-00, 81-00, Mathematical Physics, Quantum Physics},
}

@article{sriperumbudur_injective_2008,
	title = {Injective {Hilbert} {Space} {Embeddings} of {Probability} {Measures}},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). The embedding function has been proven to be injective when the reproducing kernel is universal. In this case, the embedding induces a metric on the space of probability distributions deﬁned on compact metric spaces.},
	language = {en},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Lanckriet, Gert and Scholkopf, Bernhard},
	year = {2008},
	pages = {12},
}

@phdthesis{huszar_scoring_2013,
	title = {Scoring rules, {Divergences} and {Information} in {Bayesian} {Machine} {Learning}},
	language = {en},
	author = {Huszar, Ferenc},
	year = {2013},
	keywords = {generalized information},
}

@article{sriperumbudur_hilbert_nodate,
	title = {Hilbert {Space} {Embeddings} and {Metrics} on {Probability} {Measures}},
	abstract = {A Hilbert space embedding for probability measures has recently been proposed, with applications including dimensionality reduction, homogeneity testing, and independence testing. This embedding represents any probability measure as a mean element in a reproducing kernel Hilbert space (RKHS). A pseudometric on the space of probability measures can be deﬁned as the distance between distribution embeddings: we denote this as γk, indexed by the kernel function k that deﬁnes the inner product in the RKHS.},
	language = {en},
	author = {Sriperumbudur, Bharath K and Gretton, Arthur and Fukumizu, Kenji and Scholkopf, Bernhard and Lanckriet, Gert R G},
	pages = {45},
}

@article{eaton_predictive_1996,
	title = {A {Predictive} {Approach} to the {Bayesian} {Design} {Problem} with {Application} to {Normal} {Regression} {Models}},
	volume = {83},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2337436},
	abstract = {A predictive decision-theoretic approach is developed for the Bayesian design problem. The loss functions used are fair Bayes, or proper scoring rules, and are quadratic measures of distance between probability measures. Optimal Bayesian designs are those which minimise the preposterior risk for the decision problem. Such designs typically depend on both the prior distribution and the loss function. The results are applied to certain normal regression models where explicit optimal designs are constructed.},
	number = {1},
	urldate = {2021-01-02},
	journal = {Biometrika},
	author = {Eaton, Morris L. and Giovagnoli, Alessandra and Sebastiani, Paola},
	year = {1996},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {111--125},
}

@article{good_rational_1952,
	title = {Rational {Decisions}},
	volume = {14},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984087},
	abstract = {This paper deals first with the relationship between the theory of probability and the theory of rational behaviour. A method is then suggested for encouraging people to make accurate probability estimates, a connection with the theory of information being mentioned. Finally Wald's theory of statistical decision functions is summarised and generalised and its relation to the theory of rational behaviour is discussed.},
	number = {1},
	urldate = {2021-01-02},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Good, I. J.},
	year = {1952},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {107--114},
}

@article{good_comment_1971,
	title = {Comment on “{Measuring} information and uncertainty” by {Robert} {J}. {Buehler}},
	journal = {Foundations of Statistical Inference},
	author = {Good, IJ},
	year = {1971},
	pages = {337--339},
}

@article{brier_verification_1950,
	title = {{VERIFICATION} {OF} {FORECASTS} {EXPRESSED} {IN} {TERMS} {OF} {PROBABILITY}},
	volume = {78},
	issn = {1520-0493, 0027-0644},
	url = {https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml},
	doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
	abstract = {{\textless}section class="abstract"{\textgreater}{\textless}h2 class="abstractTitle text-title my-1" id="d568e2"{\textgreater}Abstract{\textless}/h2{\textgreater}{\textless}p{\textgreater}No Abstract Available.{\textless}/p{\textgreater}{\textless}/section{\textgreater}},
	language = {EN},
	number = {1},
	urldate = {2021-01-02},
	journal = {Monthly Weather Review},
	author = {Brier, Glenn W.},
	month = jan,
	year = {1950},
	note = {Publisher: American Meteorological Society
Section: Monthly Weather Review},
	pages = {1--3},
}

@article{csiszar_consistent_2006,
	title = {Consistent estimation of the basic neighborhood of {Markov} random fields},
	volume = {34},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1146576258},
	doi = {10.1214/009053605000000912},
	abstract = {For Markov random fields on ℤd with finite state space, we address the statistical estimation of the basic neighborhood, the smallest region that determines the conditional distribution at a site on the condition that the values at all other sites are given. A modification of the Bayesian Information Criterion, replacing likelihood by pseudo-likelihood, is proved to provide strongly consistent estimation from observing a realization of the field on increasing finite regions: the estimated basic neighborhood equals the true one eventually almost surely, not assuming any prior bound on the size of the latter. Stationarity of the Markov field is not required, and phase transition does not affect the results.},
	language = {EN},
	number = {1},
	urldate = {2021-01-02},
	journal = {Annals of Statistics},
	author = {Csiszár, Imre and Talata, Zsolt},
	month = feb,
	year = {2006},
	mrnumber = {MR2275237},
	zmnumber = {1102.62105},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Gibbs measure, Markov random field, information criterion, model selection, pseudo-likelihood, typicality},
	pages = {123--145},
}

@article{winkler_scoring_1996,
	title = {Scoring rules and the evaluation of probabilities},
	volume = {5},
	issn = {1863-8260},
	url = {https://doi.org/10.1007/BF02562681},
	doi = {10.1007/BF02562681},
	abstract = {In Bayesian inference and decision analysis, inferences and predictions are inherently probabilistic in nature. Scoring rules, which involve the computation of a score based on probability forecasts and what actually occurs, can be used to evaluate probabilities and to provide appropriate incentives for “good” probabilities. This paper review scoring rules and some related measures for evaluating probabilities, including decompositions of scoring rules and attributes of “goodness” of probabilites, comparability of scores, and the design of scoring rules for specific inferential and decision-making problems},
	language = {en},
	number = {1},
	urldate = {2021-01-02},
	journal = {Test},
	author = {Winkler, R. L. and Muñoz, Javier and Cervera, José L. and Bernardo, José M. and Blattenberger, Gail and Kadane, Joseph B. and Lindley, Dennis V. and Murphy, Allan H. and Oliver, Robert M. and Ríos-Insua, David},
	month = jun,
	year = {1996},
	pages = {1--60},
}

@article{americo_conditional_2020,
	title = {Conditional {Entropy} and {Data} {Processing}: {An} {Axiomatic} {Approach} {Based} on {Core}-{Concavity}},
	volume = {66},
	issn = {0018-9448, 1557-9654},
	shorttitle = {Conditional {Entropy} and {Data} {Processing}},
	url = {https://ieeexplore.ieee.org/document/9064819/},
	doi = {10.1109/TIT.2020.2987713},
	abstract = {This work presents an axiomatization for entropy based on an extension of concavity called core-concavity. We show that core-concavity characterizes the largest class of functions for which the data-processing inequality holds, under the assumption that conditional entropy is deﬁned as a generalized average. Also, under the same assumption, we show that data-processing and “conditioning reduces entropy” properties are equivalent. We prove several properties of core-concave functions, including generalization of perfect secrecy and of Fano’s inequality. We also show that deﬁnitions of conditional entropy based on worstcase can be retrieved as limit cases of generalized averages. A connection between statistical decision making and this axiomatic approach is also presented.},
	language = {en},
	number = {9},
	urldate = {2021-01-02},
	journal = {IEEE Transactions on Information Theory},
	author = {Americo, Arthur and Khouzani, Mhr. and Malacaria, Pasquale},
	month = sep,
	year = {2020},
	pages = {5537--5547},
}

@article{gneiting_making_2010,
	title = {Making and {Evaluating} {Point} {Forecasts}},
	url = {http://arxiv.org/abs/0912.0902},
	abstract = {Typically, point forecasting methods are compared and assessed by means of an error measure or scoring function, such as the absolute error or the squared error. The individual scores are then averaged over forecast cases, to result in a summary measure of the predictive performance, such as the mean absolute error or the (root) mean squared error. I demonstrate that this common practice can lead to grossly misguided inferences, unless the scoring function and the forecasting task are carefully matched. Effective point forecasting requires that the scoring function be specified ex ante, or that the forecaster receives a directive in the form of a statistical functional, such as the mean or a quantile of the predictive distribution. If the scoring function is specified ex ante, the forecaster can issue the optimal point forecast, namely, the Bayes rule. If the forecaster receives a directive in the form of a functional, it is critical that the scoring function be consistent for it, in the sense that the expected score is minimized when following the directive. A functional is elicitable if there exists a scoring function that is strictly consistent for it. Expectations, ratios of expectations and quantiles are elicitable. For example, a scoring function is consistent for the mean functional if and only if it is a Bregman function. It is consistent for a quantile if and only if it is generalized piecewise linear. Similar characterizations apply to ratios of expectations and to expectiles. Weighted scoring functions are consistent for functionals that adapt to the weighting in peculiar ways. Not all functionals are elicitable; for instance, conditional value-at-risk is not, despite its popularity in quantitative finance.},
	urldate = {2021-01-02},
	journal = {arXiv:0912.0902 [math, stat]},
	author = {Gneiting, Tilmann},
	month = mar,
	year = {2010},
	note = {arXiv: 0912.0902},
	keywords = {Mathematics - Probability, Mathematics - Statistics Theory, Statistics - Methodology},
}

@article{garcia-garcia_risk-based_nodate,
	title = {Risk-{Based} {Generalizations} of f-divergences},
	abstract = {We derive a generalized notion of f divergences, called (f, l)-divergences. We show that this generalization enjoys many of the nice properties of f -divergences, although it is a richer family. It also provides alternative deﬁnitions of standard divergences in terms of surrogate risks. As a ﬁrst practical application of this theory, we derive a new estimator for the Kulback-Leibler divergence that we use for clustering sets of vectors.},
	language = {en},
	author = {García-García, Darío},
	pages = {8},
}

@article{nguyen_surrogate_2009,
	title = {On surrogate loss functions and \$f\$-divergences},
	volume = {37},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/math/0510521},
	doi = {10.1214/08-AOS595},
	abstract = {The goal of binary classification is to estimate a discriminant function \${\textbackslash}gamma\$ from observations of covariate vectors and corresponding binary labels. We consider an elaboration of this problem in which the covariates are not available directly but are transformed by a dimensionality-reducing quantizer \$Q\$. We present conditions on loss functions such that empirical risk minimization yields Bayes consistency when both the discriminant function and the quantizer are estimated. These conditions are stated in terms of a general correspondence between loss functions and a class of functionals known as Ali-Silvey or \$f\$-divergence functionals. Whereas this correspondence was established by Blackwell [Proc. 2nd Berkeley Symp. Probab. Statist. 1 (1951) 93--102. Univ. California Press, Berkeley] for the 0--1 loss, we extend the correspondence to the broader class of surrogate loss functions that play a key role in the general theory of Bayes consistency for binary classification. Our result makes it possible to pick out the (strict) subset of surrogate loss functions that yield Bayes consistency for joint estimation of the discriminant function and the quantizer.},
	language = {en},
	number = {2},
	urldate = {2021-01-02},
	journal = {The Annals of Statistics},
	author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
	month = apr,
	year = {2009},
	note = {arXiv: math/0510521},
	keywords = {62G10, 68Q32, 62K05 (Primary), Computer Science - Information Theory, Mathematics - Statistics Theory},
	pages = {876--904},
}

@article{bartlett_convexity_2006,
	title = {Convexity, {Classification}, and {Risk} {Bounds}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214505000000907},
	doi = {10.1198/016214505000000907},
	language = {en},
	number = {473},
	urldate = {2021-01-02},
	journal = {Journal of the American Statistical Association},
	author = {Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
	month = mar,
	year = {2006},
	pages = {138--156},
}

@book{klebanov_robust_2009,
	title = {Robust and {Non}-{Robust} {Models} in {Statistics}},
	isbn = {978-1-60741-768-2},
	author = {Klebanov, Lev and Rachev, Svetlozar and Fabozzi, Frank},
	month = jan,
	year = {2009},
}

@article{buja_loss_nodate,
	title = {Loss {Functions} for {Binary} {Class} {Probability} {Estimation} and {Classiﬁcation}: {Structure} and {Applications}},
	abstract = {What are the natural loss functions or ﬁtting criteria for binary class probability estimation? This question has a simple answer: so-called “proper scoring rules”, that is, functions that score probability estimates in view of data in a Fisher-consistent manner. Proper scoring rules comprise most loss functions currently in use: log-loss, squared error loss, boosting loss, and as limiting cases cost-weighted misclassiﬁcation losses. Proper scoring rules have a rich structure: • Every proper scoring rules is a mixture (limit of sums) of cost-weighted misclassiﬁcation losses. The mixture is speciﬁed by a weight function (or measure) that describes which misclassiﬁcation cost weights are most emphasized by the proper scoring rule.},
	language = {en},
	author = {Buja, Andreas and Stuetzle, Werner and Shen, Yi},
	pages = {49},
}

@article{zamir_proof_1998,
	title = {A proof of the {Fisher} information inequality via a data processing argument},
	volume = {44},
	issn = {1557-9654},
	doi = {10.1109/18.669301},
	abstract = {The Fisher information J(X) of a random variable X under a translation parameter appears in information theory in the classical proof of the entropy-power inequality (EPI). It enters the proof of the EPI via the De-Bruijn identity, where it measures the variation of the differential entropy under a Gaussian perturbation, and via the convolution inequality J(X+Y)/sup -1//spl ges/J(X)/sup -1/+J(Y)/sup -1/ (for independent X and Y), known as the Fisher information inequality (FII). The FII is proved in the literature directly, in a rather involved way. We give an alternative derivation of the FII, as a simple consequence of a "data processing inequality" for the Cramer-Rao lower bound on parameter estimation.},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Zamir, R.},
	month = may,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Convolution, Covariance matrix, Cramer-Rao bounds, Cramer-Rao lower bound, Data processing, De-Bruijn identity, Density measurement, Entropy, Fisher information inequality, Gaussian perturbation, Gaussian processes, Information theory, Linear matrix inequalities, Mutual information, Random variables, convolution, convolution inequality, data-processing inequality, differential entropy variation, entropy, entropy-power inequality, information theory, parameter estimation, random processes, random variable, translation parameter},
	pages = {1246--1250},
}

@article{ziv_functionals_1973,
	title = {On functionals satisfying a data-processing theorem},
	volume = {19},
	issn = {1557-9654},
	doi = {10.1109/TIT.1973.1055015},
	abstract = {It is shown that the rate-distortion bound(R(d) {\textbackslash}leq C)remains true when-{\textbackslash}log xin the definition of mutual information is replaced by an arbitrary concave({\textbackslash}cup)nonincreasing function satisfying some technical conditions. Examples are given showing that for certain choices of the concave functions, the bounds obtained are better than the classical rate-distortion bounds.},
	number = {3},
	journal = {IEEE Transactions on Information Theory},
	author = {Ziv, J. and Zakai, M.},
	month = may,
	year = {1973},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Mutual information, Rate-distortion theory},
	pages = {275--283},
}

@article{basseville_divergence_nodate,
	title = {Divergence measures for statistical data processing},
	abstract = {This note provides a bibliography of investigations based on or related to divergence measures for theoretical and applied inference problems.},
	language = {en},
	author = {Basseville, Michèle},
	pages = {24},
}

@article{xu_converses_2015,
	title = {Converses for distributed estimation via strong data processing inequalities},
	url = {http://arxiv.org/abs/1504.06028},
	abstract = {We consider the problem of distributed estimation, where local processors observe independent samples conditioned on a common random parameter of interest, map the observations to a finite number of bits, and send these bits to a remote estimator over independent noisy channels. We derive converse results for this problem, such as lower bounds on Bayes risk. The main technical tools include a lower bound on the Bayes risk via mutual information and small ball probability, as well as strong data processing inequalities for the relative entropy. Our results can recover and improve some existing results on distributed estimation with noiseless channels, and also capture the effect of noisy channels on the estimation performance.},
	urldate = {2021-01-01},
	journal = {arXiv:1504.06028 [cs, math]},
	author = {Xu, Aolin and Raginsky, Maxim},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.06028},
	keywords = {Computer Science - Information Theory},
}

@inproceedings{jiao_information_2014,
	address = {Honolulu, HI, USA},
	title = {Information divergences and the curious case of the binary alphabet},
	isbn = {978-1-4799-5186-4},
	url = {https://ieeexplore.ieee.org/document/6874853},
	doi = {10.1109/ISIT.2014.6874853},
	abstract = {Four problems related to information divergence measures deﬁned on ﬁnite alphabets are considered. In three of the cases we consider, we illustrate a contrast which arises between the binary-alphabet and larger-alphabet settings. This is surprising in some instances, since characterizations for the larger-alphabet settings do not generalize their binary-alphabet counterparts. For example, we show that f -divergences are not the unique decomposable divergences on binary alphabets that satisfy the data processing inequality, despite contrary claims in the literature.},
	language = {en},
	urldate = {2021-01-01},
	booktitle = {2014 {IEEE} {International} {Symposium} on {Information} {Theory}},
	publisher = {IEEE},
	author = {Jiao, Jiantao and Courtade, Thomas and No, Albert and Venkat, Kartik and Weissman, Tsachy},
	month = jun,
	year = {2014},
	pages = {351--355},
}

@article{painsky_bregman_2020,
	title = {Bregman {Divergence} {Bounds} and {Universality} {Properties} of the {Logarithmic} {Loss}},
	url = {http://arxiv.org/abs/1810.07014},
	abstract = {A loss function measures the discrepancy between the true values and their estimated fits, for a given instance of data. In classification problems, a loss function is said to be proper if a minimizer of the expected loss is the true underlying probability. We show that for binary classification, the divergence associated with smooth, proper, and convex loss functions is upper bounded by the Kullback-Leibler (KL) divergence, to within a normalization constant. This implies that by minimizing the logarithmic loss associated with the KL divergence, we minimize an upper bound to any choice of loss from this set. As such the logarithmic loss is universal in the sense of providing performance guarantees with respect to a broad class of accuracy measures. Importantly, this notion of universality is not problem-specific, enabling its use in diverse applications, including predictive modeling, data clustering and sample complexity analysis. Generalizations to arbitrary finite alphabets are also developed. The derived inequalities extend several well-known \$f\$-divergence results.},
	urldate = {2021-01-01},
	journal = {arXiv:1810.07014 [cs, math]},
	author = {Painsky, Amichai and Wornell, Gregory W.},
	month = jan,
	year = {2020},
	note = {arXiv: 1810.07014},
	keywords = {Computer Science - Information Theory},
}

@article{brown_information_1990,
	title = {Information {Inequalities} for the {Bayes} {Risk}},
	volume = {18},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176347867},
	doi = {10.1214/aos/1176347867},
	abstract = {This paper presents lower bounds, derived from the information inequality, for the Bayes risk under scaled quadratic loss. Some numerical results are also presented which give some idea concerning the precision of these bounds. An appendix contains a proof of the information inequality without conditions on the estimator. This result is a direct extension of an earlier result of Fabian and Hannan.},
	language = {EN},
	number = {4},
	urldate = {2021-01-01},
	journal = {Annals of Statistics},
	author = {Brown, Lawrence D. and Gajek, Leslaw},
	month = dec,
	year = {1990},
	mrnumber = {MR1074424},
	zmnumber = {0722.62003},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Bayes risk, Information inequality (Cramer-Rao inequality), exponential family},
	pages = {1578--1594},
}

@article{chen_bayes_2016,
	title = {On {Bayes} {Risk} {Lower} {Bounds}},
	url = {http://arxiv.org/abs/1410.0503},
	abstract = {This paper provides a general technique for lower bounding the Bayes risk of statistical estimation, applicable to arbitrary loss functions and arbitrary prior distributions. A lower bound on the Bayes risk not only serves as a lower bound on the minimax risk, but also characterizes the fundamental limit of any estimator given the prior knowledge. Our bounds are based on the notion of \$f\$-informativity, which is a function of the underlying class of probability measures and the prior. Application of our bounds requires upper bounds on the \$f\$-informativity, thus we derive new upper bounds on \$f\$-informativity which often lead to tight Bayes risk lower bounds. Our technique leads to generalizations of a variety of classical minimax bounds (e.g., generalized Fano's inequality). Our Bayes risk lower bounds can be directly applied to several concrete estimation problems, including Gaussian location models, generalized linear models, and principal component analysis for spiked covariance models. To further demonstrate the applications of our Bayes risk lower bounds to machine learning problems, we present two new theoretical results: (1) a precise characterization of the minimax risk of learning spherical Gaussian mixture models under the smoothed analysis framework, and (2) lower bounds for the Bayes risk under a natural prior for both the prediction and estimation errors for high-dimensional sparse linear regression under an improper learning setting.},
	urldate = {2021-01-01},
	journal = {arXiv:1410.0503 [math, stat]},
	author = {Chen, Xi and Guntuboyina, Adityanand and Zhang, Yuchen},
	month = dec,
	year = {2016},
	note = {arXiv: 1410.0503},
	keywords = {Mathematics - Statistics Theory},
}

@article{yuksel_optimization_2012,
	title = {Optimization and {Convergence} of {Observation} {Channels} in {Stochastic} {Control}},
	url = {http://arxiv.org/abs/1009.3824},
	abstract = {This paper studies the optimization of observation channels (stochastic kernels) in partially observed stochastic control problems. In particular, existence and continuity properties are investigated mostly (but not exclusively) concentrating on the single-stage case. Continuity properties of the optimal cost in channels are explored under total variation, setwise convergence, and weak convergence. Sufficient conditions for compactness of a class of channels under total variation and setwise convergence are presented and applications to quantization are explored.},
	urldate = {2021-01-01},
	journal = {arXiv:1009.3824 [cs, math]},
	author = {Yüksel, Serdar and Linder, Tamás},
	month = feb,
	year = {2012},
	note = {arXiv: 1009.3824},
	keywords = {15A15, 15A09, 15A23, Computer Science - Information Theory, Mathematics - Optimization and Control},
}

@inproceedings{harremoes_information_2007,
	address = {Nice},
	title = {The {Information} {Bottleneck} {Revisited} or {How} to {Choose} a {Good} {Distortion} {Measure}},
	isbn = {978-1-4244-1397-3},
	url = {http://ieeexplore.ieee.org/document/4557285/},
	doi = {10.1109/ISIT.2007.4557285},
	abstract = {It is well-known that the information bottleneck method and rate distortion theory are related. Here it is described how the information bottleneck can be considered as rate distortion theory for a family of probability measures where information divergence is used as distortion measure. It is shown that the information bottleneck method has some properties that are not shared with rate distortion theory based on any other divergence measure. In this sense the information bottleneck method is unique.},
	language = {en},
	urldate = {2020-11-11},
	booktitle = {2007 {IEEE} {International} {Symposium} on {Information} {Theory}},
	publisher = {IEEE},
	author = {Harremoes, Peter and Tishby, Naftali},
	month = jun,
	year = {2007},
	pages = {566--570},
}

@article{li_strong_2018,
	title = {Strong {Functional} {Representation} {Lemma} and {Applications} to {Coding} {Theorems}},
	url = {http://arxiv.org/abs/1701.02827},
	abstract = {This paper shows that for any random variables \$X\$ and \$Y\$, it is possible to represent \$Y\$ as a function of \$(X,Z)\$ such that \$Z\$ is independent of \$X\$ and \$I(X;Z{\textbar}Y){\textbackslash}le{\textbackslash}log(I(X;Y)+1)+4\$ bits. We use this strong functional representation lemma (SFRL) to establish a bound on the rate needed for one-shot exact channel simulation for general (discrete or continuous) random variables, strengthening the results by Harsha et al. and Braverman and Garg, and to establish new and simple achievability results for one-shot variable-length lossy source coding, multiple description coding and Gray-Wyner system. We also show that the SFRL can be used to reduce the channel with state noncausally known at the encoder to a point-to-point channel, which provides a simple achievability proof of the Gelfand-Pinsker theorem.},
	urldate = {2020-10-26},
	journal = {arXiv:1701.02827 [cs, math]},
	author = {Li, Cheuk Ting and Gamal, Abbas El},
	month = jan,
	year = {2018},
	note = {arXiv: 1701.02827},
	keywords = {Computer Science - Information Theory, coding, information},
}

@article{grunwald_shannon_2004,
	title = {Shannon {Information} and {Kolmogorov} {Complexity}},
	volume = {cs.IT/0410002},
	url = {http://arxiv.org/abs/cs.IT/0410002},
	journal = {CoRR},
	author = {Grünwald, Peter and Vitányi, Paul M. B.},
	year = {2004},
	keywords = {algorithmic complexity, information, theory},
}

@article{zaidi_information_2020,
	title = {On the {Information} {Bottleneck} {Problems}: {Models}, {Connections}, {Applications} and {Information} {Theoretic} {Views}},
	volume = {22},
	issn = {1099-4300},
	shorttitle = {On the {Information} {Bottleneck} {Problems}},
	url = {http://arxiv.org/abs/2002.00008},
	doi = {10.3390/e22020151},
	abstract = {This tutorial paper focuses on the variants of the bottleneck problem taking an information theoretic perspective and discusses practical methods to solve it, as well as its connection to coding and learning aspects. The intimate connections of this setting to remote source-coding under logarithmic loss distortion measure, information combining, common reconstruction, the Wyner-Ahlswede-Korner problem, the efficiency of investment information, as well as, generalization, variational inference, representation learning, autoencoders, and others are highlighted. We discuss its extension to the distributed information bottleneck problem with emphasis on the Gaussian model and highlight the basic connections to the uplink Cloud Radio Access Networks (CRAN) with oblivious processing. For this model, the optimal trade-offs between relevance (i.e., information) and complexity (i.e., rates) in the discrete and vector Gaussian frameworks is determined. In the concluding outlook, some interesting problems are mentioned such as the characterization of the optimal inputs ("features") distributions under power limitations maximizing the "relevance" for the Gaussian information bottleneck, under "complexity" constraints.},
	number = {2},
	urldate = {2020-06-24},
	journal = {Entropy},
	author = {Zaidi, Abdellatif and Aguerri, Inaki Estella and Shamai, Shlomo},
	month = jan,
	year = {2020},
	note = {arXiv: 2002.00008},
	keywords = {Computer Science - Information Theory, Statistics - Machine Learning, ib, information},
	pages = {151},
}

@article{chechik_information_nodate,
	title = {Information {Bottleneck} for {Gaussian} {Variables}},
	abstract = {The problem of extracting the relevant aspects of data was previously addressed through the information bottleneck (IB) method, through (soft) clustering one variable while preserving information about another - relevance - variable. The current work extends these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters, for the special case of multivariate Gaussian variables. While the general continuous IB problem is difﬁcult to solve, we provide an analytic solution for the optimal representation and tradeoff between compression and relevance for the this important case. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized regression matrix Σx{\textbar}yΣ−x 1, which is also the basis obtained in canonical correlation analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector, through a cascade of structural phase transitions. This introduces a novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides a complete analytic expression of the preserved information as a function of the compression (the “information-curve”), in terms of the eigenvalue spectrum of the data. As in the discrete case, the information curve is concave and smooth, though it is made of different analytic segments for each optimal dimension. Finally, we show how the algorithmic theory developed in the IB framework provides an iterative algorithm for obtaining the optimal Gaussian projections.},
	language = {en},
	author = {Chechik, Gal and Globerson, Amir and Tishby, Naftali and Weiss, Yair},
	pages = {24},
}

@article{kraskov_estimating_2004,
	title = {Estimating {Mutual} {Information}},
	volume = {69},
	issn = {1539-3755, 1550-2376},
	url = {http://arxiv.org/abs/cond-mat/0305641},
	doi = {10.1103/PhysRevE.69.066138},
	abstract = {We present two classes of improved estimators for mutual information \$M(X,Y)\$, from samples of random points distributed according to some joint probability density \${\textbackslash}mu(x,y)\$. In contrast to conventional estimators based on binnings, they are based on entropy estimates from \$k\$-nearest neighbour distances. This means that they are data efficient (with \$k=1\$ we resolve structures down to the smallest possible scales), adaptive (the resolution is higher where data are more numerous), and have minimal bias. Indeed, the bias of the underlying entropy estimates is mainly due to non-uniformity of the density at the smallest resolved scale, giving typically systematic errors which scale as functions of \$k/N\$ for \$N\$ points. Numerically, we find that both families become \{{\textbackslash}it exact\} for independent distributions, i.e. the estimator \${\textbackslash}hat M(X,Y)\$ vanishes (up to statistical fluctuations) if \${\textbackslash}mu(x,y) = {\textbackslash}mu(x) {\textbackslash}mu(y)\$. This holds for all tested marginal distributions and for all dimensions of \$x\$ and \$y\$. In addition, we give estimators for redundancies between more than 2 random variables. We compare our algorithms in detail with existing algorithms. Finally, we demonstrate the usefulness of our estimators for assessing the actual independence of components obtained from independent component analysis (ICA), for improving ICA, and for estimating the reliability of blind source separation.},
	number = {6},
	urldate = {2020-12-27},
	journal = {Physical Review E},
	author = {Kraskov, Alexander and Stoegbauer, Harald and Grassberger, Peter},
	month = jun,
	year = {2004},
	note = {arXiv: cond-mat/0305641},
	keywords = {Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics},
	pages = {066138},
}

@book{guo_mutual_2004,
	title = {Mutual {Information} and {Minimum} {Mean}-square {Error} in {Gaussian} {Channels}},
	abstract = {This paper deals with arbitrarily distributed finite-power input signals observed through an additive Gaussian noise channel. It shows a new formula that connects the inputoutput mutual information and the minimum mean-square error (MMSE) achievable by optimal estimation of the input given the output. That is, the derivative of the mutual information (nats) with respect to the signal-to-noise ratio (SNR) is equal to half the MMSE, regardless of the input statistics. This relationship holds for both scalar and vector signals, as well as for discrete-time and continuous-time noncausal MMSE estimation. This fundamental information-theoretic result has an unexpected consequence in continuous-time nonlinear estimation: For any input signal with finite power, the causal filtering MMSE achieved at SNR is equal to the average value of the noncausal smoothing MMSE achieved with a channel whose signal-to-noise ratio is chosen uniformly distributed between 0 and SNR.},
	author = {Guo, Dongning and Shamai (Shitz), Shlomo and Verdú, Sergio},
	year = {2004},
}

@article{wu_functional_2012,
	title = {Functional {Properties} of {Minimum} {Mean}-{Square} {Error} and {Mutual} {Information}},
	volume = {58},
	issn = {0018-9448, 1557-9654},
	url = {http://ieeexplore.ieee.org/document/6084749/},
	doi = {10.1109/TIT.2011.2174959},
	abstract = {In addition to exploring its various regularity properties, we show that the minimum mean-square error (MMSE) is a concave functional of the input-output joint distribution. In the case of additive Gaussian noise, the MMSE is shown to be weakly continuous in the input distribution and Lipschitz continuous with respect to the quadratic Wasserstein distance for peak-limited inputs. Regularity properties of mutual information are also obtained. Several applications to information theory and the central limit theorem are discussed.},
	language = {en},
	number = {3},
	urldate = {2020-12-31},
	journal = {IEEE Transactions on Information Theory},
	author = {Wu, Yihong and Verdu, Sergio},
	month = mar,
	year = {2012},
	pages = {1289--1301},
}

@book{lehmann_theory_1998,
	address = {New York},
	edition = {2nd ed},
	series = {Springer texts in statistics},
	title = {Theory of point estimation},
	isbn = {978-0-387-98502-2},
	language = {en},
	publisher = {Springer},
	author = {Lehmann, E. L. and Casella, George},
	year = {1998},
	keywords = {Fix-point estimation},
}

@article{oord_representation_2019,
	title = {Representation {Learning} with {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/1807.03748},
	abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
	urldate = {2020-12-28},
	journal = {arXiv:1807.03748 [cs, stat]},
	author = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	month = jan,
	year = {2019},
	note = {arXiv: 1807.03748},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{toderici_variable_2016,
	title = {Variable {Rate} {Image} {Compression} with {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.06085},
	abstract = {A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32\${\textbackslash}times\$32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10\% or more.},
	urldate = {2020-12-28},
	journal = {arXiv:1511.06085 [cs]},
	author = {Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
	month = mar,
	year = {2016},
	note = {arXiv: 1511.06085},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
}

@article{rissanen_arithmetic_1979,
	title = {Arithmetic coding},
	volume = {23},
	issn = {0018-8646},
	url = {https://doi.org/10.1147/rd.232.0149},
	doi = {10.1147/rd.232.0149},
	abstract = {The earlier introduced arithmetic coding idea has been generalized to a very broad and flexible coding technique which includes virtually all known variable rate noiseless coding techniques as special cases. An outstanding feature of this technique is that alphabet extensions are not required. A complete decodability analysis is given. The relationship of arithmetic coding to other known nonblock codes is illuminated.},
	number = {2},
	urldate = {2020-12-28},
	journal = {IBM Journal of Research and Development},
	author = {Rissanen, J. and Langdon, G. G.},
	month = mar,
	year = {1979},
	pages = {149--162},
}

@article{paninski_estimation_2003,
	title = {Estimation of {Entropy} and {Mutual} {Information}},
	volume = {15},
	issn = {0899-7667, 1530-888X},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/089976603321780272},
	doi = {10.1162/089976603321780272},
	abstract = {We present some new results on the nonparametric estimation of entropy and mutual information. First, we use an exact local expansion of the entropy function to prove almost sure consistency and central limit theorems for three of the most commonly used discretized information estimators. The setup is related to Grenander's method of sieves and places no assumptions on the underlying probability measure generating the data. Second, we prove a converse to these consistency theorems, demonstrating that a misapplication of the most common estimation techniques leads to an arbitrarily poor estimate of the true information, even given unlimited data. This “inconsistency” theorem leads to an analytical approximation of the bias, valid in surprisingly small sample regimes and more accurate than the usual [Formula: see text] formula of Miller and Madow over a large region of parameter space. The two most practical implications of these results are negative: (1) information estimates in a certain data regime are likely contaminated by bias, even if “bias-corrected” estimators are used, and (2) confidence intervals calculated by standard techniques drastically underestimate the error of the most common estimation methods.
            Finally, we note a very useful connection between the bias of entropy estimators and a certain polynomial approximation problem. By casting bias calculation problems in this approximation theory framework, we obtain the best possible generalization of known asymptotic bias results. More interesting, this framework leads to an estimator with some nice properties: the estimator comes equipped with rigorous bounds on the maximum error over all possible underlying probability distributions, and this maximum error turns out to be surprisingly small. We demonstrate the application of this new estimator on both real and simulated data.},
	language = {en},
	number = {6},
	urldate = {2020-12-27},
	journal = {Neural Computation},
	author = {Paninski, Liam},
	month = jun,
	year = {2003},
	pages = {1191--1253},
}

@inproceedings{madiman_entropy_2008,
	address = {Porto, Portugal},
	title = {On the entropy of sums},
	isbn = {978-1-4244-2269-2},
	url = {http://ieeexplore.ieee.org/document/4578674/},
	doi = {10.1109/ITW.2008.4578674},
	abstract = {It is shown that the entropy of a sum of independent random vectors is a submodular set function, and upper bounds on the entropy of sums are obtained as a result in both discrete and continuous settings. These inequalities complement the lower bounds provided by the entropy power inequalities of Madiman and Barron (2007). As applications, new inequalities for the determinants of sums of positive-deﬁnite matrices are presented.},
	language = {en},
	urldate = {2020-12-26},
	booktitle = {2008 {IEEE} {Information} {Theory} {Workshop}},
	publisher = {IEEE},
	author = {Madiman, Mokshay},
	month = may,
	year = {2008},
	pages = {303--307},
}

@article{gressmann_probabilistic_2019,
	title = {Probabilistic supervised learning},
	url = {http://arxiv.org/abs/1801.00753},
	abstract = {Predictive modelling and supervised learning are central to modern data science. With predictions from an ever-expanding number of supervised black-box strategies - e.g., kernel methods, random forests, deep learning aka neural networks - being employed as a basis for decision making processes, it is crucial to understand the statistical uncertainty associated with these predictions. As a general means to approach the issue, we present an overarching framework for black-box prediction strategies that not only predict the target but also their own predictions' uncertainty. Moreover, the framework allows for fair assessment and comparison of disparate prediction strategies. For this, we formally consider strategies capable of predicting full distributions from feature variables, so-called probabilistic supervised learning strategies. Our work draws from prior work including Bayesian statistics, information theory, and modern supervised machine learning, and in a novel synthesis leads to (a) new theoretical insights such as a probabilistic bias-variance decomposition and an entropic formulation of prediction, as well as to (b) new algorithms and meta-algorithms, such as composite prediction strategies, probabilistic boosting and bagging, and a probabilistic predictive independence test. Our black-box formulation also leads (c) to a new modular interface view on probabilistic supervised learning and a modelling workflow API design, which we have implemented in the newly released skpro machine learning toolbox, extending the familiar modelling interface and meta-modelling functionality of sklearn. The skpro package provides interfaces for construction, composition, and tuning of probabilistic supervised learning strategies, together with orchestration features for validation and comparison of any such strategy - be it frequentist, Bayesian, or other.},
	urldate = {2020-12-24},
	journal = {arXiv:1801.00753 [cs, math, stat]},
	author = {Gressmann, Frithjof and Király, Franz J. and Mateen, Bilal and Oberhauser, Harald},
	month = may,
	year = {2019},
	note = {arXiv: 1801.00753},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{berger_lossy_1998,
	title = {Lossy source coding},
	volume = {44},
	number = {6},
	journal = {IEEE Transactions on Information Theory},
	author = {Berger, Toby and Gibson, Jerry D},
	year = {1998},
	note = {Publisher: IEEE},
	pages = {2693--2723},
}

@article{pinkston_encoding_1967,
	title = {Encoding independent sample information sources.},
	author = {Pinkston, John Turner},
	year = {1967},
	note = {Publisher: MIT Research Laboratory of Electronics},
}

@article{ishwar_note_2014,
	title = {A note on the sum-rate-distortion function of some lossy source coding problems involving infinite-valued distortion functions},
	url = {http://arxiv.org/abs/1408.0586},
	abstract = {For a number of lossy source coding problems it is shown that even if the usual single-letter sum-rate-distortion expressions may become invalid for non-infinite distortion functions, they can be approached, to any desired accuracy, via the usual valid expressions for appropriately truncated finite versions of the distortion functions.},
	urldate = {2020-12-21},
	journal = {arXiv:1408.0586 [cs, math]},
	author = {Ishwar, Prakash},
	month = aug,
	year = {2014},
	note = {arXiv: 1408.0586},
	keywords = {Computer Science - Information Theory},
}

@article{berger_rate_1968,
	title = {Rate distortion theory for sources with abstract alphabets and memory},
	volume = {13},
	issn = {00199958},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0019995868911236},
	doi = {10.1016/S0019-9958(68)91123-6},
	language = {en},
	number = {3},
	urldate = {2020-12-21},
	journal = {Information and Control},
	author = {Berger, Toby},
	month = sep,
	year = {1968},
	pages = {254--273},
}

@book{gallager_information_1968,
	address = {USA},
	title = {Information {Theory} and {Reliable} {Communication}},
	isbn = {978-0-471-29048-3},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Gallager, Robert G.},
	year = {1968},
}

@article{jaynes_information_1957,
	title = {Information {Theory} and {Statistical} {Mechanics}},
	volume = {106},
	url = {https://link.aps.org/doi/10.1103/PhysRev.106.620},
	doi = {10.1103/PhysRev.106.620},
	abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
	number = {4},
	urldate = {2020-12-21},
	journal = {Physical Review},
	author = {Jaynes, E. T.},
	month = may,
	year = {1957},
	note = {Publisher: American Physical Society},
	pages = {620--630},
}

@article{reinsel_data_2017,
	title = {Data age 2025: {The} evolution of data to life-critical},
	abstract = {Die globale Datensphäre wird von 23 Zettabyte im Jahr 2017 auf 175 Zettabyte im Jahr 2025 anwachsen. Fast 30 Prozent der weltweiten Daten werden in Echtzeit verarbeitet werden müssen. Ist Ihr Unternehmen dafür bereit?},
	language = {en},
	journal = {Don’t Focus on Big Data},
	author = {Reinsel, David and Gantz, John and Rydning, John},
	year = {2017},
}

@book{dijk_network_2020,
	title = {The {Network} {Society}},
	isbn = {978-1-5297-3632-8},
	abstract = {The Network Society is a clear, engaging guide to the past, consequences and future of digital communication, and forms a comprehensive introduction to how new media functions in contemporary society. Integrating both face-to-face and online communication, the fourth edition explores crucial new issues and challenges in today’s digital media ecology, in doing so exploring the centrality of power to understanding life in the network society. Featuring:  The rise of the ‘data economy’ The increasing importance of artificial intelligence. big data and robotics The growth of Internet platforms and how to regulate big tech. New coverage of disinformation and fake news, including deep fake videos Updates to the story of digital youth culture, as a foreshadow of future new media use  With examples, cases and real-world applications, this is the essential guide for digital and new media students seeking to understand a diverse, fast-moving field.},
	language = {en},
	publisher = {SAGE},
	author = {Dijk, Jan van},
	month = sep,
	year = {2020},
	note = {Google-Books-ID: C2r0DwAAQBAJ},
	keywords = {Language Arts \& Disciplines / Communication Studies, Social Science / Media Studies, Social Science / Sociology / General},
}

@article{karalias_erdos_2020,
	title = {Erdos {Goes} {Neural}: an {Unsupervised} {Learning} {Framework} for {Combinatorial} {Optimization} on {Graphs}},
	shorttitle = {Erdos {Goes} {Neural}},
	url = {http://arxiv.org/abs/2006.10643},
	abstract = {Combinatorial optimization problems are notoriously challenging for neural networks, especially in the absence of labeled instances. This work proposes an unsupervised learning framework for CO problems on graphs that can provide integral solutions of certified quality. Inspired by Erdos' probabilistic method, we use a neural network to parametrize a probability distribution over sets. Crucially, we show that when the network is optimized w.r.t. a suitably chosen loss, the learned distribution contains, with controlled probability, a low-cost integral solution that obeys the constraints of the combinatorial problem. The probabilistic proof of existence is then derandomized to decode the desired solutions. We demonstrate the efficacy of this approach to obtain valid solutions to the maximum clique problem and to perform local graph clustering. Our method achieves competitive results on both real datasets and synthetic hard instances.},
	urldate = {2020-12-17},
	journal = {arXiv:2006.10643 [cs, stat]},
	author = {Karalias, Nikolaos and Loukas, Andreas},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.10643},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{song_score-based_2020,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field (a.k.a., score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in diffusion probabilistic modeling and score-based generative modeling, and allows for new sampling procedures. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, which enables exact likelihood computation, and improved sampling efficiency. In addition, our framework enables conditional generation with an unconditional model, as we demonstrate with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 3.10 bits/dim, and demonstrate high fidelity generation of \$1024 {\textbackslash}times 1024\$ images for the first time from a score-based generative model.},
	urldate = {2020-12-07},
	journal = {arXiv:2011.13456 [cs, stat]},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = nov,
	year = {2020},
	note = {arXiv: 2011.13456},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{ravanbakhsh_equivariance_2017,
	title = {Equivariance {Through} {Parameter}-{Sharing}},
	url = {http://arxiv.org/abs/1702.08389},
	abstract = {We propose to study equivariance in deep neural networks through parameter symmetries. In particular, given a group \${\textbackslash}mathcal\{G\}\$ that acts discretely on the input and output of a standard neural network layer \${\textbackslash}phi\_\{W\}: {\textbackslash}Re{\textasciicircum}\{M\} {\textbackslash}to {\textbackslash}Re{\textasciicircum}\{N\}\$, we show that \${\textbackslash}phi\_\{W\}\$ is equivariant with respect to \${\textbackslash}mathcal\{G\}\$-action iff \${\textbackslash}mathcal\{G\}\$ explains the symmetries of the network parameters \$W\$. Inspired by this observation, we then propose two parameter-sharing schemes to induce the desirable symmetry on \$W\$. Our procedures for tying the parameters achieve \${\textbackslash}mathcal\{G\}\$-equivariance and, under some conditions on the action of \${\textbackslash}mathcal\{G\}\$, they guarantee sensitivity to all other permutation groups outside \${\textbackslash}mathcal\{G\}\$.},
	urldate = {2020-12-03},
	journal = {arXiv:1702.08389 [cs, stat]},
	author = {Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
	month = jun,
	year = {2017},
	note = {arXiv: 1702.08389},
	keywords = {Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning, symmetry},
}

@article{foster_improving_2020,
	title = {Improving {Transformation} {Invariance} in {Contrastive} {Representation} {Learning}},
	url = {http://arxiv.org/abs/2010.09515},
	abstract = {We propose methods to strengthen the invariance properties of representations obtained by contrastive learning. While existing approaches implicitly induce a degree of invariance as representations are learned, we look to more directly enforce invariance in the encoding process. To this end, we first introduce a training objective for contrastive learning that uses a novel regularizer to control how the representation changes under transformation. We show that representations trained with this objective perform better on downstream tasks and are more robust to the introduction of nuisance transformations at test time. Second, we propose a change to how test time representations are generated by introducing a feature averaging approach that combines encodings from multiple transformations of the original input, finding that this leads to across the board performance gains. Finally, we introduce the novel Spirograph dataset to explore our ideas in the context of a differentiable generative process with multiple downstream tasks, showing that our techniques for learning invariance are highly beneficial.},
	urldate = {2020-11-30},
	journal = {arXiv:2010.09515 [cs, stat]},
	author = {Foster, Adam and Pukdee, Rattana and Rainforth, Tom},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.09515},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{durkan_contrastive_2020,
	title = {On {Contrastive} {Learning} for {Likelihood}-free {Inference}},
	url = {http://arxiv.org/abs/2002.03712},
	abstract = {Likelihood-free methods perform parameter inference in stochastic simulator models where evaluating the likelihood is intractable but sampling synthetic data is possible. One class of methods for this likelihood-free problem uses a classifier to distinguish between pairs of parameter-observation samples generated using the simulator and pairs sampled from some reference distribution, which implicitly learns a density ratio proportional to the likelihood. Another popular class of methods fits a conditional distribution to the parameter posterior directly, and a particular recent variant allows for the use of flexible neural density estimators for this task. In this work, we show that both of these approaches can be unified under a general contrastive learning scheme, and clarify how they should be run and compared.},
	urldate = {2020-11-30},
	journal = {arXiv:2002.03712 [cs, stat]},
	author = {Durkan, Conor and Murray, Iain and Papamakarios, George},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.03712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{song_multi-label_2020,
	title = {Multi-label {Contrastive} {Predictive} {Coding}},
	url = {http://arxiv.org/abs/2007.09852},
	abstract = {Variational mutual information (MI) estimators are widely used in unsupervised representation learning methods such as contrastive predictive coding (CPC). A lower bound on MI can be obtained from a multi-class classification problem, where a critic attempts to distinguish a positive sample drawn from the underlying joint distribution from \$(m-1)\$ negative samples drawn from a suitable proposal distribution. Using this approach, MI estimates are bounded above by \${\textbackslash}log m\$, and could thus severely underestimate unless \$m\$ is very large. To overcome this limitation, we introduce a novel estimator based on a multi-label classification problem, where the critic needs to jointly identify multiple positive samples at the same time. We show that using the same amount of negative samples, multi-label CPC is able to exceed the \${\textbackslash}log m\$ bound, while still being a valid lower bound of mutual information. We demonstrate that the proposed approach is able to lead to better mutual information estimation, gain empirical improvements in unsupervised representation learning, and beat a current state-of-the-art knowledge distillation method over 10 out of 13 tasks.},
	urldate = {2020-11-27},
	journal = {arXiv:2007.09852 [cs, stat]},
	author = {Song, Jiaming and Ermon, Stefano},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.09852},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cranmer_discovering_2020,
	title = {Discovering {Symbolic} {Models} from {Deep} {Learning} with {Inductive} {Biases}},
	url = {http://arxiv.org/abs/2006.11287},
	abstract = {We develop a general approach to distill symbolic representations of a learned deep model by introducing strong inductive biases. We focus on Graph Neural Networks (GNNs). The technique works as follows: we first encourage sparse latent representations when we train a GNN in a supervised setting, then we apply symbolic regression to components of the learned model to extract explicit physical relations. We find the correct known equations, including force laws and Hamiltonians, can be extracted from the neural network. We then apply our method to a non-trivial cosmology example-a detailed dark matter simulation-and discover a new analytic formula which can predict the concentration of dark matter from the mass distribution of nearby cosmic structures. The symbolic expressions extracted from the GNN using our technique also generalized to out-of-distribution data better than the GNN itself. Our approach offers alternative directions for interpreting neural networks and discovering novel physical principles from the representations they learn.},
	urldate = {2020-11-20},
	journal = {arXiv:2006.11287 [astro-ph, physics:physics, stat]},
	author = {Cranmer, Miles and Sanchez-Gonzalez, Alvaro and Battaglia, Peter and Xu, Rui and Cranmer, Kyle and Spergel, David and Ho, Shirley},
	month = nov,
	year = {2020},
	note = {arXiv: 2006.11287},
	keywords = {Astrophysics - Cosmology and Nongalactic Astrophysics, Astrophysics - Instrumentation and Methods for Astrophysics, Computer Science - Machine Learning, Physics - Computational Physics, Statistics - Machine Learning},
}

@article{lee_learning_2019,
	title = {Learning finite-dimensional coding schemes with nonlinear reconstruction maps},
	url = {http://arxiv.org/abs/1812.09658},
	abstract = {This paper generalizes the Maurer--Pontil framework of finite-dimensional lossy coding schemes to the setting where a high-dimensional random vector is mapped to an element of a compact set of latent representations in a lower-dimensional Euclidean space, and the reconstruction map belongs to a given class of nonlinear maps. Under this setup, which encompasses a broad class of unsupervised representation learning problems, we establish a connection to approximate generative modeling under structural constraints using the tools from the theory of optimal transportation. Next, we consider problem of learning a coding scheme on the basis of a finite collection of training samples and present generalization bounds that hold with high probability. We then illustrate the general theory in the setting where the reconstruction maps are implemented by deep neural nets.},
	urldate = {2020-11-19},
	journal = {arXiv:1812.09658 [cs, stat]},
	author = {Lee, Jaeho and Raginsky, Maxim},
	month = mar,
	year = {2019},
	note = {arXiv: 1812.09658},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{li_why_2020,
	title = {Why {Are} {Convolutional} {Nets} {More} {Sample}-{Efficient} than {Fully}-{Connected} {Nets}?},
	url = {http://arxiv.org/abs/2010.08515},
	abstract = {Convolutional neural networks often dominate fully-connected counterparts in generalization performance, especially on image classification tasks. This is often explained in terms of 'better inductive bias'. However, this has not been made mathematically rigorous, and the hurdle is that the fully connected net can always simulate the convolutional net (for a fixed task). Thus the training algorithm plays a role. The current work describes a natural task on which a provable sample complexity gap can be shown, for standard training algorithms. We construct a single natural distribution on \${\textbackslash}mathbb\{R\}{\textasciicircum}d{\textbackslash}times{\textbackslash}\{{\textbackslash}pm 1{\textbackslash}\}\$ on which any orthogonal-invariant algorithm (i.e. fully-connected networks trained with most gradient-based methods from gaussian initialization) requires \${\textbackslash}Omega(d{\textasciicircum}2)\$ samples to generalize while \$O(1)\$ samples suffice for convolutional architectures. Furthermore, we demonstrate a single target function, learning which on all possible distributions leads to an \$O(1)\$ vs \${\textbackslash}Omega(d{\textasciicircum}2/{\textbackslash}varepsilon)\$ gap. The proof relies on the fact that SGD on fully-connected network is orthogonal equivariant. Similar results are achieved for \${\textbackslash}ell\_2\$ regression and adaptive training algorithms, e.g. Adam and AdaGrad, which are only permutation equivariant.},
	urldate = {2020-11-19},
	journal = {arXiv:2010.08515 [cs, stat]},
	author = {Li, Zhiyuan and Zhang, Yi and Arora, Sanjeev},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.08515},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, learning theory, symmetry},
}

@article{cohen_learning_2014,
	title = {Learning the {Irreducible} {Representations} of {Commutative} {Lie} {Groups}},
	url = {http://arxiv.org/abs/1402.4437},
	abstract = {We present a new probabilistic model of compact commutative Lie groups that produces invariant-equivariant and disentangled representations of data. To define the notion of disentangling, we borrow a fundamental principle from physics that is used to derive the elementary particles of a system from its symmetries. Our model employs a newfound Bayesian conjugacy relation that enables fully tractable probabilistic inference over compact commutative Lie groups -- a class that includes the groups that describe the rotation and cyclic translation of images. We train the model on pairs of transformed image patches, and show that the learned invariant representation is highly effective for classification.},
	urldate = {2020-11-19},
	journal = {arXiv:1402.4437 [cs]},
	author = {Cohen, Taco and Welling, Max},
	month = may,
	year = {2014},
	note = {arXiv: 1402.4437},
	keywords = {Computer Science - Machine Learning, symmetry},
}

@article{finzi_simplifying_2020,
	title = {Simplifying {Hamiltonian} and {Lagrangian} {Neural} {Networks} via {Explicit} {Constraints}},
	url = {http://arxiv.org/abs/2010.13581},
	abstract = {Reasoning about the physical world requires models that are endowed with the right inductive biases to learn the underlying dynamics. Recent works improve generalization for predicting trajectories by learning the Hamiltonian or Lagrangian of a system rather than the differential equations directly. While these methods encode the constraints of the systems using generalized coordinates, we show that embedding the system into Cartesian coordinates and enforcing the constraints explicitly with Lagrange multipliers dramatically simplifies the learning problem. We introduce a series of challenging chaotic and extended-body systems, including systems with N-pendulums, spring coupling, magnetic fields, rigid rotors, and gyroscopes, to push the limits of current approaches. Our experiments show that Cartesian coordinates with explicit constraints lead to a 100x improvement in accuracy and data efficiency.},
	urldate = {2020-11-19},
	journal = {arXiv:2010.13581 [physics, stat]},
	author = {Finzi, Marc and Wang, Ke Alexander and Wilson, Andrew Gordon},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.13581},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
}

@article{caron_unsupervised_2020,
	title = {Unsupervised {Learning} of {Visual} {Features} by {Contrasting} {Cluster} {Assignments}},
	url = {http://arxiv.org/abs/2006.09882},
	abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
	urldate = {2020-11-18},
	journal = {arXiv:2006.09882 [cs]},
	author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	month = oct,
	year = {2020},
	note = {arXiv: 2006.09882},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{choi_compression_2012,
	title = {Compression of {Graphical} {Structures}: {Fundamental} {Limits}, {Algorithms}, and {Experiments}},
	volume = {58},
	issn = {0018-9448, 1557-9654},
	shorttitle = {Compression of {Graphical} {Structures}},
	url = {http://ieeexplore.ieee.org/document/6145505/},
	doi = {10.1109/TIT.2011.2173710},
	language = {en},
	number = {2},
	urldate = {2020-11-17},
	journal = {IEEE Transactions on Information Theory},
	author = {Choi, Yongwook and Szpankowski, Wojciech},
	month = feb,
	year = {2012},
	pages = {620--638},
}

@book{harary_graphical_2014,
	title = {Graphical {Enumeration}},
	isbn = {978-1-4832-7378-5},
	abstract = {Graphical Enumeration deals with the enumeration of various kinds of graphs. Topics covered range from labeled enumeration and George Pólya's theorem to rooted and unrooted trees, graphs and digraphs, and power group enumeration. Superposition, blocks, and asymptotics are also discussed. A number of unsolved enumeration problems are presented.Comprised of 10 chapters, this book begins with an overview of labeled graphs, followed by a description of the basic enumeration theorem of Pólya. The next three chapters count an enormous variety of trees, graphs, and digraphs. The Power Group Enumeration Theorem is then described together with some of its applications, including the enumeration of self-complementary graphs and digraphs and finite automata. Two other chapters focus on the counting of superposition and blocks, while another chapter is devoted to asymptotic numbers that are developed for several different graphical structures. The book concludes with a comprehensive definitive list of unsolved graphical enumeration problems.This monograph will be of interest to both students and practitioners of mathematics.},
	language = {en},
	publisher = {Elsevier},
	author = {Harary, Frank and Palmer, Edgar M.},
	month = may,
	year = {2014},
	note = {Google-Books-ID: ZrvSBQAAQBAJ},
	keywords = {Mathematics / General},
}

@article{trucco_note_1956,
	title = {A note on the information content of graphs},
	volume = {18},
	issn = {1522-9602},
	url = {https://doi.org/10.1007/BF02477836},
	doi = {10.1007/BF02477836},
	abstract = {The role played by the group of a graph in determining sets of equivalent points is exposed and illustrated by a few simple examples.},
	language = {en},
	number = {2},
	urldate = {2020-11-17},
	journal = {The bulletin of mathematical biophysics},
	author = {Trucco, Ernesto},
	month = jun,
	year = {1956},
	pages = {129--135},
}

@article{alon_source_1996,
	title = {Source coding and graph entropies},
	doi = {10.1109/18.532875},
	abstract = {A sender wants to accurately convey information to a receiver who has some, possibly related, data. We study the expected number of bits the sender must transmit for one and for multiple instances in two communication scenarios and relate this number to the chromatic and Korner (1973) entropies of a naturally defined graph.},
	journal = {IEEE Trans. Inf. Theory},
	author = {Alon, N. and Orlitsky, A.},
	year = {1996},
}

@article{dehmer_information_2011,
	title = {Information {Theory} of {Networks}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2073-8994/3/4/767},
	doi = {10.3390/sym3040767},
	abstract = {The paper puts the emphasis on surveying information-theoretic network measures for analyzing the structure of networks. In order to apply the quantities interdisciplinarily, we also discuss some of their properties such as their structural interpretation and uniqueness.},
	language = {en},
	number = {4},
	urldate = {2020-11-17},
	journal = {Symmetry},
	author = {Dehmer, Matthias},
	month = dec,
	year = {2011},
	note = {Number: 4
Publisher: Molecular Diversity Preservation International},
	keywords = {information theory, networks, quantitative graph analysis},
	pages = {767--779},
}

@article{mowshowitz_entropy_1968,
	title = {Entropy and the complexity of graphs. {I}. {An} index of the relative complexity of a graph},
	volume = {30},
	issn = {0007-4985},
	doi = {10.1007/BF02476948},
	language = {eng},
	number = {1},
	journal = {The Bulletin of Mathematical Biophysics},
	author = {Mowshowitz, A.},
	month = mar,
	year = {1968},
	pmid = {5666816},
	keywords = {Mathematics},
	pages = {175--204},
}

@article{mowshowitz_entropy_2012,
	title = {Entropy and the {Complexity} of {Graphs} {Revisited}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/14/3/559},
	doi = {10.3390/e14030559},
	abstract = {This paper presents a taxonomy and overview of approaches to the measurement of graph and network complexity. The taxonomy distinguishes between deterministic (e.g., Kolmogorov complexity) and probabilistic approaches with a view to placing entropy-based probabilistic measurement in context. Entropy-based measurement is the main focus of the paper. Relationships between the different entropy functions used to measure complexity are examined; and intrinsic (e.g., classical measures) and extrinsic (e.g., Körner entropy) variants of entropy-based models are discussed in some detail.},
	language = {en},
	number = {3},
	urldate = {2020-11-17},
	journal = {Entropy},
	author = {Mowshowitz, Abbe and Dehmer, Matthias},
	month = mar,
	year = {2012},
	note = {Number: 3
Publisher: Molecular Diversity Preservation International},
	keywords = {Shannon entropy, complex networks, graph entropy},
	pages = {559--570},
}

@article{dehmer_history_2011,
	title = {A history of graph entropy measures},
	volume = {181},
	issn = {00200255},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0020025510004147},
	doi = {10.1016/j.ins.2010.08.041},
	abstract = {This survey seeks to describe methods for measuring the entropy of graphs and to demonstrate the wide applicability of entropy measures. Setting the scene with a review of classical measures for determining the structural information content of graphs, we discuss graph entropy measures which play an important role in a variety of problem areas, including biology, chemistry, and sociology. In addition, we examine relationships between selected entropy measures, illustrating differences quantitatively with concrete examples. Ó 2010 Elsevier Inc. All rights reserved.},
	language = {en},
	number = {1},
	urldate = {2020-11-17},
	journal = {Information Sciences},
	author = {Dehmer, Matthias and Mowshowitz, Abbe},
	month = jan,
	year = {2011},
	pages = {57--78},
}

@article{steinruecken_lossless_nodate,
	title = {Lossless {Data} {Compression}},
	abstract = {This thesis makes several contributions to the ﬁeld of data compression. Lossless data compression algorithms shorten the description of input objects, such as sequences of text, in a way that allows perfect recovery of the original object. Such algorithms exploit the fact that input objects are not uniformly distributed: by allocating shorter descriptions to more probable objects and longer descriptions to less probable objects, the expected length of the compressed output can be made shorter than the object’s original description. Compression algorithms can be designed to match almost any given probability distribution over input objects.},
	language = {en},
	author = {Steinruecken, Christian},
	pages = {230},
}

@article{bennett_entanglement-assisted_2002,
	title = {Entanglement-assisted capacity of a quantum channel and the reverse {Shannon} theorem},
	url = {http://arxiv.org/abs/quant-ph/0106052},
	abstract = {The entanglement-assisted classical capacity of a noisy quantum channel is the amount of information per channel use that can be sent over the channel in the limit of many uses of the channel, assuming that the sender and receiver have access to the resource of shared quantum entanglement, which may be used up by the communication protocol. We show that this capacity is given by an expression parallel to that for the capacity of a purely classical channel: i.e., the maximum, over channel inputs \${\textbackslash}rho\$, of the entropy of the channel input plus the entropy of the channel output minus their joint entropy, the latter being defined as the entropy of an entangled purification of \${\textbackslash}rho\$ after half of it has passed through the channel. We calculate entanglement-assisted capacities for two interesting quantum channels, the qubit amplitude damping channel and the bosonic channel with amplification/attenuation and Gaussian noise. We discuss how many independent parameters are required to completely characterize the asymptotic behavior of a general quantum channel, alone or in the presence of ancillary resources such as prior entanglement. In the classical analog of entanglement assisted communication--communication over a discrete memoryless channel (DMC) between parties who share prior random information--we show that one parameter is sufficient, i.e., that in the presence of prior shared random information, all DMC's of equal capacity can simulate one another with unit asymptotic efficiency.},
	urldate = {2020-11-17},
	journal = {arXiv:quant-ph/0106052},
	author = {Bennett, Charles H. and Shor, Peter W. and Smolin, John A. and Thapliyal, Ashish V.},
	month = may,
	year = {2002},
	note = {arXiv: quant-ph/0106052},
	keywords = {Quantum Physics},
}

@article{cuff_communication_2008,
	title = {Communication {Requirements} for {Generating} {Correlated} {Random} {Variables}},
	url = {http://arxiv.org/abs/0805.0065},
	abstract = {Two familiar notions of correlation are rediscovered as extreme operating points for simulating a discrete memoryless channel, in which a channel output is generated based only on a description of the channel input. Wyner's "common information" coincides with the minimum description rate needed. However, when common randomness independent of the input is available, the necessary description rate reduces to Shannon's mutual information. This work characterizes the optimal tradeoff between the amount of common randomness used and the required rate of description.},
	urldate = {2020-11-17},
	journal = {arXiv:0805.0065 [cs, math]},
	author = {Cuff, Paul},
	month = may,
	year = {2008},
	note = {arXiv: 0805.0065},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Information Theory, H.1.1, Mathematics - Probability},
}

@book{kallenberg_random_2017,
	series = {Probability {Theory} and {Stochastic} {Modelling}},
	title = {Random {Measures}, {Theory} and {Applications}},
	isbn = {978-3-319-41596-3},
	url = {https://www.springer.com/gp/book/9783319415963},
	abstract = {Offering the first comprehensive treatment of the theory of random measures, this book has a very broad scope, ranging from basic properties of Poisson and related processes to the modern theories of convergence, stationarity, Palm measures, conditioning, and compensation. The three large final chapters focus on applications within the areas of stochastic geometry, excursion theory, and branching processes. Although this theory plays a fundamental role in most areas of modern probability, much of it, including the most basic material, has previously been available only in scores of journal articles. The book is primarily directed towards researchers and advanced graduate students in stochastic processes and related areas.},
	language = {en},
	urldate = {2020-11-16},
	publisher = {Springer International Publishing},
	author = {Kallenberg, Olav},
	year = {2017},
	doi = {10.1007/978-3-319-41598-7},
}

@book{kallenberg_probabilistic_2005,
	address = {New York},
	series = {Probability and {Its} {Applications}},
	title = {Probabilistic {Symmetries} and {Invariance} {Principles}},
	isbn = {978-0-387-25115-8},
	url = {https://www.springer.com/gp/book/9780387251158},
	abstract = {This is the first comprehensive treatment of the three basic symmetries of probability theory—contractability, exchangeability, and rotatability—defined as invariance in distribution under contractions, permutations, and rotations. Originating with the pioneering work of de Finetti from the 1930's, the theory has evolved into a unique body of deep, beautiful, and often surprising results, comprising the basic representations and invariance properties in one and several dimensions, and exhibiting some unexpected links between the various symmetries as well as to many other areas of modern probability. Most chapters require only some basic, graduate level probability theory, and should be accessible to any serious researchers and graduate students in probability and statistics. Parts of the book may also be of interest to pure and applied mathematicians in other areas. The exposition is formally self-contained, with detailed references provided for any deeper facts from real analysis or probability used in the book. Olav Kallenberg received his Ph.D. in 1972 from Chalmers University in Gothenburg, Sweden. After teaching for many years at Swedish universities, he moved in 1985 to the US, where he is currently Professor of Mathematics at Auburn University. He is well known for his previous books Random Measures (4th edition, 1986) and Foundations of Modern Probability (2nd edition, 2002) and for numerous research papers in all areas of probability. In 1977, he was the second recipient ever of the prestigious Rollo Davidson Prize from Cambridge University. In 1991–94, he served as the Editor in Chief of Probability Theory and Related Fields. Professor Kallenberg is an elected fellow of the Institute of Mathematical Statistics.},
	language = {en},
	urldate = {2020-11-16},
	publisher = {Springer-Verlag},
	author = {Kallenberg, Olav},
	year = {2005},
	doi = {10.1007/0-387-28861-9},
}

@article{wijsman_existence_nodate,
	title = {{EXISTENCE} {OF} {LOCAL} {CROSS}-{SECTIONS} {IN} {LINEAR} {CARTAN} {G}-{SPACES} {UNDER} {THE} {ACTION} {OF} {NONCOMPACT} {GROUPS}},
	language = {en},
	author = {Wijsman, R A},
	pages = {7},
}

@inproceedings{wijsman_cross-sections_1967,
	title = {Cross-sections of orbits and their application to densities of maximal invariants},
	url = {https://projecteuclid.org/euclid.bsmsp/1200512999},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2020-11-15},
	publisher = {The Regents of the University of California},
	author = {Wijsman, R. A.},
	year = {1967},
	note = {ISSN: 0097-0433},
}

@article{koehn_global_1970,
	title = {Global {Cross} {Sections} and the {Densities} of {Maximal} {Invariants}},
	volume = {41},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177696704},
	doi = {10.1214/aoms/1177696704},
	abstract = {This paper generalizes some results of Wijsman concerning the calculation of the density of a maximal invariant. The idea of the technique is to represent the sample space as a product space, one factor ZZZ being a global cross section, i.e., essentially a set that intersects each orbit in a unique point, and the other factor being a coset space of the invariance group. Integration over the invariance group then gives the distribution of the identity function on ZZZ which is a maximal invariant. Part I of the paper gives sufficient conditions for the technique to be applicable, while Part II exhibits the technique along with an example. Part II is on a more elementary level than Part I and may be understood without a reading of Part I.},
	language = {EN},
	number = {6},
	urldate = {2020-11-15},
	journal = {Annals of Mathematical Statistics},
	author = {Koehn, Uwe},
	month = dec,
	year = {1970},
	mrnumber = {MR270478},
	zmnumber = {0215.26404},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {2045--2056},
}

@article{bondar_borel_1976,
	title = {Borel {Cross}-{Sections} and {Maximal} {Invariants}},
	volume = {4},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176343585},
	doi = {10.1214/aos/1176343585},
	abstract = {A measurable cross-section for orbits of a sample space under a free (exact) transformation group is shown to exist under topological regularity conditions. This is used to represent the sample space as essentially the product of a maximal invariant and an equivariant part, which implies Stein's representation for the density of the maximal invariant.},
	language = {EN},
	number = {5},
	urldate = {2020-11-15},
	journal = {Annals of Statistics},
	author = {Bondar, James V.},
	month = sep,
	year = {1976},
	mrnumber = {MR474589},
	zmnumber = {0345.62004},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Cross-section of orbits, disintegration of measure, maximal invariant},
	pages = {866--877},
}

@book{eaton_chapter_1989,
	title = {Chapter 2: {Group} actions and relatively invariant integrals},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 2},
	url = {https://projecteuclid.org/euclid.cbms/1462061032},
	abstract = {In this chapter, group actions are reviewed and are illustrated with examples of relevance for statistical applications. Relatively invariant integrals (measures) are defined and examples are given. An important result, due to Weil, gives necessary and sufficient conditions for the existence and uniqueness of relatively invariant integrals when the group action is transitive. A discussion of invariant and equivariant functions closes out the chapter.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 19-40
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-1,
	title = {Chapter 4: {Models} invariant under compact groups},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 4},
	url = {https://projecteuclid.org/euclid.cbms/1462061034},
	abstract = {Our goal here is to understand the structure of probability measures which are invariant under a compact group. In the first section, a basic representation theorem is proved and is interpreted in terms of random variables. Section 2 contains some basic examples while applications to some robustness problems are given in Section 3.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 55-67
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-2,
	title = {Chapter 6: {Invariant} decision problems},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 6},
	url = {https://projecteuclid.org/euclid.cbms/1462061036},
	abstract = {This rather lengthy chapter provides an introduction to invariant decision problems. After describing the basic ingredients in a decision problem, invariance is introduced and used to define an invariant decision problem. A main result in this chapter shows how to construct a best invariant rule when the group action is transitive on the parameter space and when the dominating measure is decomposable [That is, the integral J defined by the measure satisfies Equation (5.14) in Theorem 5.5.] Applications of this result to the construction of best invariant estimators are given.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 81-99
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-3,
	title = {Chapter 7: {Random} orthogonal matrices},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 7},
	url = {https://projecteuclid.org/euclid.cbms/1462061037},
	abstract = {Orthogonal matrices, both fixed and random, play an important role in much of statistics, especially in multivariate analysis. Connections between the orthogonal group 0" and the multivariate normal distribution are explored in James (1954) and in Wijsman (1957), as well as in many texts on multivariate analysis. In this chapter, invariance arguments are used to derive the density of a subblock of a uniformly distributed element of 0". This result is used to describe an upper bound on the rate at which one has convergence (as n {\textasciitilde} co) to the multivariate normal distribution.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 100-107
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-4,
	title = {Chapter 8: {Finite} de {Finetti} style theorems},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 8},
	url = {https://projecteuclid.org/euclid.cbms/1462061038},
	abstract = {The purpose of this chapter is to introduce the ideas surrounding the so called finite de Finetti style theorems. Four examples, one of which comes from the classical de Finetti theorem and three related to the normal distribution, are discussed here. These examples are introduced by first describing the "infinite version" of a result and then moving to the "finite version." In all of these examples, the infinite version came first, followed by a finite version. However, recent work on finite versions has suggested new infinite versions; some of these are discussed in the next chapter.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 108-120
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-5,
	title = {Chapter 9: {Finite} de {Finetti} style theorems for linear models},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 9},
	url = {https://projecteuclid.org/euclid.cbms/1462061039},
	abstract = {The implications of invariance assumptions for linear models are investigated here via a finite de Finetti style theorem. Before discussing linear models, we describe a general method for approximating projected measures. Of course, the origins of the method are in the four examples described in Chapter 8. All of the material in this chapter is from Diaconis, Eaton and Lauritzen (1987).},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 123-130
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-6,
	title = {Chapter 5: {Decomposable} measures},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 5},
	url = {https://projecteuclid.org/euclid.cbms/1462061035},
	abstract = {The main purpose of this chapter is to discuss the extent to which Theorem 4.3 can be generalized to arbitrary Radon measures (rather than probability measures) and to cases where G is not compact. Such generalizations can be used in the derivation of densities of maximal invariants as well as in other areas. The approach here is modelled after that described in Andersson (1982). Other possible approaches to this problem are described in Wijsman (1986) (the global cross section approach using some Lie group theory) and Farrell (1985) (a measure-theoretic cross section approach developed by Schwartz (1966), unpublished).},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 68-80
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-7,
	title = {Chapter 3: {Invariant} statistical models},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 3},
	url = {https://projecteuclid.org/euclid.cbms/1462061033},
	abstract = {In this lecture, invariant statistical models are introduced and a variety of examples is given. Invariant testing problems and equivariant estimators are introduced. Univariate and multivariate linear models provide a host of standard examples.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 41-54
Publication Title: Group invariance in applications in statistics},
}

@book{eaton_chapter_1989-8,
	title = {Chapter 1: {Integrals} and the {Haar} measure},
	isbn = {978-0-940600-15-7},
	shorttitle = {Chapter 1},
	url = {https://projecteuclid.org/euclid.cbms/1462061031},
	abstract = {In this and the next chapter, background material on integration, topological groups and group actions is presented. The concrete examples described below provide a direct connection between the rather abstract theory of Haar measure and its application to situations which are relevan't in statistical applications.},
	language = {EN},
	urldate = {2020-11-13},
	publisher = {IMS and ASA},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Journal Abbreviation: Group invariance in applications in statistics
Pages: 1-18
Publication Title: Group invariance in applications in statistics},
}

@inproceedings{berger_statistical_1988,
	title = {Statistical {Decision} {Theory} and {Bayesian} {Analysis}},
	doi = {10.2307/2288950},
	abstract = {An overview of statistical decision theory, which emphasizes the use and application of the philosophical ideas and mathematical structure of decision theory. The text assumes a knowledge of basic probability theory and some advanced calculus is also required.},
	author = {Berger, J. O.},
	year = {1988},
}

@book{wijsman_13_1990,
	title = {13. {Density} {Ratio} of a {Maximal} {Invariant}},
	isbn = {978-0-940600-19-5},
	url = {https://projecteuclid.org/euclid.lnms/1215540668},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	urldate = {2020-11-14},
	publisher = {Institute of Mathematical Statistics},
	author = {Wijsman, Robert A.},
	year = {1990},
	doi = {10.1214/lnms/1215540668},
	note = {Pages: 198-212
Publication Title: Invariant measures on groups and their use in statistics},
}

@book{giri_group_1996,
	address = {Singapore ; River Edge, NJ},
	title = {Group invariance in statistical inference},
	isbn = {978-981-02-1875-1},
	language = {en},
	publisher = {World Scientific},
	author = {Giri, Narayan C.},
	year = {1996},
	keywords = {Invariants, Multivariate analysis},
}

@article{eaton_group_1989,
	title = {Group {Invariance} {Applications} in {Statistics}},
	volume = {1},
	issn = {1935-5912},
	url = {https://www.jstor.org/stable/4153172},
	urldate = {2020-11-13},
	journal = {Regional Conference Series in Probability and Statistics},
	author = {Eaton, Morris L.},
	year = {1989},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {i--133},
}

@techreport{lauritzen_interrelationships_1974,
	title = {On the {Interrelationships} among {Sufficiency}, {Total} {Sufficiency} and {Some} {Related} {Concepts}},
	number = {8},
	institution = {Institute of Mathematical Statistics, University of Copenhagen.},
	author = {Lauritzen, Steffen},
	year = {1974},
}

@misc{schulman_sending_2020,
	title = {Sending {Samples} {Without} {Bits}-{Back}},
	url = {http://joschu.net/blog/sending-samples.html},
	urldate = {2020-11-12},
	author = {Schulman, John},
	year = {2020},
}

@inproceedings{frey_free_1996,
	title = {Free energy coding},
	doi = {10.1109/DCC.1996.488312},
	abstract = {We introduce a new approach to the problem of optimal compression when a source code produces multiple codewords for a given symbol. It may seem that the most sensible codeword to use in this case is the shortest one. However, in the proposed free energy approach, random codeword selection yields an effective codeword length that can be less than the shortest codeword length. If the random choices are Boltzmann distributed, the effective length is optimal for the given source code. The expectation-maximization parameter estimation algorithms minimize this effective codeword length. We illustrate the performance of free energy coding on a simple problem where a compression factor of two is gained by using the new method.},
	booktitle = {Proceedings of {Data} {Compression} {Conference} - {DCC} '96},
	author = {Frey, B. J. and Hinton, G. E.},
	month = mar,
	year = {1996},
	note = {ISSN: 1068-0314},
	keywords = {Boltzmann distribution, Data compression, Educational institutions, Parameter estimation, Source coding, codeword length, compression factor, data compression, encoding, expectation-maximization parameter estimation, free energy coding, multiple codewords, optimal compression, optimisation, parameter estimation, random codeword selection, source code},
	pages = {73--81},
}

@article{duda_asymmetric_2014,
	title = {Asymmetric numeral systems: entropy coding combining speed of {Huffman} coding with compression rate of arithmetic coding},
	shorttitle = {Asymmetric numeral systems},
	url = {http://arxiv.org/abs/1311.2540},
	abstract = {The modern data compression is mainly based on two approaches to entropy coding: Huffman (HC) and arithmetic/range coding (AC). The former is much faster, but approximates probabilities with powers of 2, usually leading to relatively low compression rates. The latter uses nearly exact probabilities - easily approaching theoretical compression rate limit (Shannon entropy), but at cost of much larger computational cost. Asymmetric numeral systems (ANS) is a new approach to accurate entropy coding, which allows to end this trade-off between speed and rate: the recent implementation [1] provides about \$50{\textbackslash}\%\$ faster decoding than HC for 256 size alphabet, with compression rate similar to provided by AC. This advantage is due to being simpler than AC: using single natural number as the state, instead of two to represent a range. Beside simplifying renormalization, it allows to put the entire behavior for given probability distribution into a relatively small table: defining entropy coding automaton. The memory cost of such table for 256 size alphabet is a few kilobytes. There is a large freedom while choosing a specific table - using pseudorandom number generator initialized with cryptographic key for this purpose allows to simultaneously encrypt the data. This article also introduces and discusses many other variants of this new entropy coding approach, which can provide direct alternatives for standard AC, for large alphabet range coding, or for approximated quasi arithmetic coding.},
	urldate = {2020-11-12},
	journal = {arXiv:1311.2540 [cs, math]},
	author = {Duda, Jarek},
	month = jan,
	year = {2014},
	note = {arXiv: 1311.2540},
	keywords = {Computer Science - Information Theory},
}

@inproceedings{hinton_keeping_1993,
	title = {Keeping the neural networks simple by minimizing the description length of the weights},
	doi = {10.1145/168304.168306},
	abstract = {Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-o between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed e ciently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights.},
	booktitle = {{COLT} '93},
	author = {Hinton, Geoffrey E. and Camp, D.},
	year = {1993},
}

@article{townsend_hilloc_2020,
	title = {{HILLOC}: {LOSSLESS} {IMAGE} {COMPRESSION} {WITH} {HI}- {ERARCHICAL} {LATENT} {VARIABLE} {MODELS}},
	abstract = {We make the following striking observation: fully convolutional VAE models trained on 32×32 ImageNet can generalize well, not just to 64×64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based ‘Bits-Back with ANS’ algorithm for lossless compression (Townsend et al., 2019) to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results1.},
	language = {en},
	author = {Townsend, James and Bird, Thomas and Kunze, Julius and Barber, David},
	year = {2020},
	pages = {14},
}

@book{csiszar_information_1982,
	address = {USA},
	title = {Information {Theory}: {Coding} {Theorems} for {Discrete} {Memoryless} {Systems}},
	isbn = {978-0-12-198450-2},
	shorttitle = {Information {Theory}},
	publisher = {Academic Press, Inc.},
	author = {Csiszar, Imre and Korner, Janos},
	year = {1982},
}

@article{rezende_variational_2016,
	title = {Variational {Inference} with {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/1505.05770},
	abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
	urldate = {2020-11-12},
	journal = {arXiv:1505.05770 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
	month = jun,
	year = {2016},
	note = {arXiv: 1505.05770},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@article{tomczak_vae_2018,
	title = {{VAE} with a {VampPrior}},
	url = {http://arxiv.org/abs/1705.07120},
	abstract = {Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.},
	urldate = {2020-11-12},
	journal = {arXiv:1705.07120 [cs, stat]},
	author = {Tomczak, Jakub M. and Welling, Max},
	month = feb,
	year = {2018},
	note = {arXiv: 1705.07120},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{brekelmans_exact_2019,
	title = {Exact {Rate}-{Distortion} in {Autoencoders} via {Echo} {Noise}},
	url = {http://arxiv.org/abs/1904.07199},
	abstract = {Compression is at the heart of effective representation learning. However, lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability, and the limitations this imposes on learning are largely unexplored. Further, the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general. We introduce a new noise channel, {\textbackslash}emph\{Echo noise\}, that admits a simple, exact expression for mutual information for arbitrary input distributions. The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions. With its complex encoding mechanism and exact rate regularization, Echo leads to improved bounds on log-likelihood and dominates \${\textbackslash}beta\$-VAEs across the achievable range of rate-distortion trade-offs. Further, we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations.},
	urldate = {2020-11-12},
	journal = {arXiv:1904.07199 [cs, math, stat]},
	author = {Brekelmans, Rob and Moyer, Daniel and Galstyan, Aram and Steeg, Greg Ver},
	month = nov,
	year = {2019},
	note = {arXiv: 1904.07199},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{turian_scalable_2007,
	title = {Scalable {Discriminative} {Learning} for {Natural} {Language} {Parsing} and {Translation}},
	volume = {19},
	url = {https://proceedings.neurips.cc/paper/2006/file/e8bf0f27d70d480d3ab793bb7619aaa5-Paper.pdf},
	urldate = {2020-11-12},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Turian, Joseph and Wellington, Benjamin and Melamed, I.},
	editor = {Schölkopf, B. and Platt, J. and Hoffman, T.},
	year = {2007},
	pages = {1409--1416},
}

@article{higgins_beta-vae_2017,
	title = {beta-{VAE}: {LEARNING} {BASIC} {VISUAL} {CONCEPTS} {WITH} {A} {CONSTRAINED} {VARIATIONAL} {FRAMEWORK}},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artiﬁcial intelligence that is able to learn and reason in the same way that humans do. We introduce β-VAE, a new state-of-the-art framework for automated discovery of interpretable factorised latent representations from raw image data in a completely unsupervised manner. Our approach is a modiﬁcation of the variational autoencoder (VAE) framework. We introduce an adjustable hyperparameter β that balances latent channel capacity and independence constraints with reconstruction accuracy. We demonstrate that β-VAE with appropriately tuned β {\textgreater} 1 qualitatively outperforms VAE (β = 1), as well as state of the art unsupervised (InfoGAN) and semi-supervised (DC-IGN) approaches to disentangled factor learning on a variety of datasets (celebA, faces and chairs). Furthermore, we devise a protocol to quantitatively compare the degree of disentanglement learnt by different models, and show that our approach also signiﬁcantly outperforms all baselines quantitatively. Unlike InfoGAN, β-VAE is stable to train, makes few assumptions about the data and relies on tuning a single hyperparameter β, which can be directly optimised through a hyperparameter search using weakly labelled data or through heuristic visual inspection for purely unsupervised data.},
	language = {en},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	year = {2017},
	pages = {22},
}

@techreport{globerson_optimality_2004,
	title = {On the optimality of the {Gaussian} information bottleneck curve},
	institution = {The Hebrew University of Jerusalem},
	author = {Globerson, Amir and Tishby, Naftali},
	year = {2004},
}

@article{kim_discovering_2017,
	title = {Discovering {Potential} {Correlations} via {Hypercontractivity}},
	volume = {19},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/1709.04024},
	doi = {10.3390/e19110586},
	abstract = {Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical examples of potential correlations.},
	number = {11},
	urldate = {2020-11-11},
	journal = {Entropy},
	author = {Kim, Hyeji and Gao, Weihao and Kannan, Sreeram and Oh, Sewoong and Viswanath, Pramod},
	month = nov,
	year = {2017},
	note = {arXiv: 1709.04024},
	keywords = {Statistics - Machine Learning},
	pages = {586},
}

@article{polyanskiy_strong_2016,
	title = {Strong data-processing inequalities for channels and {Bayesian} networks},
	url = {http://arxiv.org/abs/1508.06025},
	abstract = {The data-processing inequality, that is, \$I(U;Y) {\textbackslash}le I(U;X)\$ for a Markov chain \$U {\textbackslash}to X {\textbackslash}to Y\$, has been the method of choice for proving impossibility (converse) results in information theory and many other disciplines. Various channel-dependent improvements (called strong data-processing inequalities, or SDPIs) of this inequality have been proposed both classically and more recently. In this note we first survey known results relating various notions of contraction for a single channel. Then we consider the basic extension: given SDPI for each constituent channel in a Bayesian network, how to produce an end-to-end SDPI? Our approach is based on the (extract of the) Evans-Schulman method, which is demonstrated for three different kinds of SDPIs, namely, the usual Ahslwede-G{\textbackslash}'acs type contraction coefficients (mutual information), Dobrushin's contraction coefficients (total variation), and finally the \$F\_I\$-curve (the best possible non-linear SDPI for a given channel). Resulting bounds on the contraction coefficients are interpreted as probability of site percolation. As an example, we demonstrate how to obtain SDPI for an \$n\$-letter memoryless channel with feedback given an SDPI for \$n=1\$. Finally, we discuss a simple observation on the equivalence of a linear SDPI and comparison to an erasure channel (in the sense of "less noisy" order). This leads to a simple proof of a curious inequality of Samorodnitsky (2015), and sheds light on how information spreads in the subsets of inputs of a memoryless channel.},
	urldate = {2020-11-11},
	journal = {arXiv:1508.06025 [cs, math, stat]},
	author = {Polyanskiy, Yury and Wu, Yihong},
	month = jul,
	year = {2016},
	note = {arXiv: 1508.06025},
	keywords = {Computer Science - Information Theory, Mathematics - Statistics Theory},
}

@article{anantharam_maximal_2013,
	title = {On {Maximal} {Correlation}, {Hypercontractivity}, and the {Data} {Processing} {Inequality} studied by {Erkip} and {Cover}},
	url = {http://arxiv.org/abs/1304.6133},
	abstract = {In this paper we provide a new geometric characterization of the Hirschfeld-Gebelein-R{\textbackslash}'\{e\}nyi maximal correlation of a pair of random \$(X,Y)\$, as well as of the chordal slope of the nontrivial boundary of the hypercontractivity ribbon of \$(X,Y)\$ at infinity. The new characterizations lead to simple proofs for some of the known facts about these quantities. We also provide a counterexample to a data processing inequality claimed by Erkip and Cover, and find the correct tight constant for this kind of inequality.},
	urldate = {2020-11-11},
	journal = {arXiv:1304.6133 [cs, math]},
	author = {Anantharam, Venkat and Gohari, Amin and Kamath, Sudeep and Nair, Chandra},
	month = apr,
	year = {2013},
	note = {arXiv: 1304.6133},
	keywords = {Computer Science - Information Theory},
}

@article{strouse_deterministic_2016,
	title = {The deterministic information bottleneck},
	url = {http://arxiv.org/abs/1604.00268},
	abstract = {Lossy compression and clustering fundamentally involve a decision about what features are relevant and which are not. The information bottleneck method (IB) by Tishby, Pereira, and Bialek formalized this notion as an information-theoretic optimization problem and proposed an optimal tradeoff between throwing away as many bits as possible, and selectively keeping those that are most important. In the IB, compression is measure my mutual information. Here, we introduce an alternative formulation that replaces mutual information with entropy, which we call the deterministic information bottleneck (DIB), that we argue better captures this notion of compression. As suggested by its name, the solution to the DIB problem turns out to be a deterministic encoder, or hard clustering, as opposed to the stochastic encoder, or soft clustering, that is optimal under the IB. We compare the IB and DIB on synthetic data, showing that the IB and DIB perform similarly in terms of the IB cost function, but that the DIB significantly outperforms the IB in terms of the DIB cost function. We also empirically find that the DIB offers a considerable gain in computational efficiency over the IB, over a range of convergence parameters. Our derivation of the DIB also suggests a method for continuously interpolating between the soft clustering of the IB and the hard clustering of the DIB.},
	urldate = {2020-11-10},
	journal = {arXiv:1604.00268 [cond-mat, q-bio, stat]},
	author = {Strouse, D. J. and Schwab, David J.},
	month = dec,
	year = {2016},
	note = {arXiv: 1604.00268},
	keywords = {Computer Science - Information Theory, Condensed Matter - Statistical Mechanics, Quantitative Biology - Neurons and Cognition, Quantitative Biology - Quantitative Methods, Statistics - Machine Learning},
}

@article{dempster_maximum_1977,
	title = {Maximum {Likelihood} from {Incomplete} {Data} via the {EM} {Algorithm}},
	volume = {39},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2984875},
	abstract = {A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
	number = {1},
	urldate = {2020-11-10},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
	year = {1977},
	note = {Publisher: [Royal Statistical Society, Wiley]},
	pages = {1--38},
}

@article{csiszar_information_1984,
	title = {Information geometry and alternating minimization procedures},
	url = {https://scinapse.io/papers/1582051210},
	doi = {null},
	abstract = {Imre Csisz, Gábor Tusnády  {\textbar}},
	language = {en},
	urldate = {2020-11-10},
	author = {Csiszar, Imre and Tusnády, Gábor},
	month = jan,
	year = {1984},
}

@article{blahut_computation_1972,
	title = {Computation of channel capacity and rate-distortion functions},
	volume = {18},
	issn = {1557-9654},
	doi = {10.1109/TIT.1972.1054855},
	abstract = {By defining mutual information as a maximum over an appropriate space, channel capacities can be defined as double maxima and rate-distortion functions as double minima. This approach yields valuable new insights regarding the computation of channel capacities and rate-distortion functions. In particular, it suggests a simple algorithm for computing channel capacity that consists of a mapping from the set of channel input probability vectors into itself such that the sequence of probability vectors generated by successive applications of the mapping converges to the vector that achieves the capacity of the given channel. Analogous algorithms then are provided for computing rate-distortion functions and constrained channel capacities. The algorithms apply both to discrete and to continuous alphabet channels or sources. In addition, a formalization of the theory of channel capacity in the presence of constraints is included. Among the examples is the calculation of close upper and lower bounds to the rate-distortion function of a binary symmetric Markov source.},
	number = {4},
	journal = {IEEE Transactions on Information Theory},
	author = {Blahut, R.},
	month = jul,
	year = {1972},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Mutual information, Rate-distortion theory},
	pages = {460--473},
}

@article{arimoto_algorithm_1972,
	title = {An algorithm for computing the capacity of arbitrary discrete memoryless channels},
	volume = {18},
	issn = {1557-9654},
	doi = {10.1109/TIT.1972.1054753},
	abstract = {A systematic and iterative method of computing the capacity of arbitrary discrete memoryless channels is presented. The algorithm is very simple and involves only logarithms and exponentials in addition to elementary arithmetical operations. It has also the property of monotonic convergence to the capacity. In general, the approximation error is at least inversely proportional to the number of iterations; in certain circumstances, it is exponentially decreasing. Finally, a few inequalities that give upper and lower bounds on the capacity are derived.},
	number = {1},
	journal = {IEEE Transactions on Information Theory},
	author = {Arimoto, S.},
	month = jan,
	year = {1972},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Information rates, Memoryless channels},
	pages = {14--20},
}

@article{qian_multi-task_2020,
	title = {Multi-{Task} {Variational} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/2007.00339},
	abstract = {In this paper we propose a variational information bottleneck (VIB)-based framework for multi-task learning (MTL), where a more accurate latent representation can be obtained from the input data which also learn different tasks in parallel. Moreover, the task-dependent uncertainties are taken into account to learn the relative weights of task loss functions. The proposed method is examined with three publicly available data sets under different adversarial attacks. The overall classification performance of our model is promising. It can achieve comparable classification accuracies as the benchmarked models, and has shown a better robustness against adversarial attacks compared with other MTL models.},
	urldate = {2020-11-10},
	journal = {arXiv:2007.00339 [cs, stat]},
	author = {Qian, Weizhu and Chen, Bowei and Gechter, Franck},
	month = sep,
	year = {2020},
	note = {arXiv: 2007.00339},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vera_role_2018,
	title = {The {Role} of {Information} {Complexity} and {Randomization} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/1802.05355},
	abstract = {A grand challenge in representation learning is to learn the different explanatory factors of variation behind the high dimen- sional data. Encoder models are often determined to optimize performance on training data when the real objective is to generalize well to unseen data. Although there is enough numerical evidence suggesting that noise injection (during training) at the representation level might improve the generalization ability of encoders, an information-theoretic understanding of this principle remains elusive. This paper presents a sample-dependent bound on the generalization gap of the cross-entropy loss that scales with the information complexity (IC) of the representations, meaning the mutual information between inputs and their representations. The IC is empirically investigated for standard multi-layer neural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the gap and the IC appear to be in direct correlation, suggesting that SGD selects encoders to implicitly minimize the IC. We specialize the IC to study the role of Dropout on the generalization capacity of deep encoders which is shown to be directly related to the encoder capacity, being a measure of the distinguishability among samples from their representations. Our results support some recent regularization methods.},
	urldate = {2020-11-10},
	journal = {arXiv:1802.05355 [cs, stat]},
	author = {Vera, Matías and Piantanida, Pablo and Vega, Leonardo Rey},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.05355},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{bernado_bayesian_2009,
	title = {Bayesian theory},
	volume = {405},
	url = {https://www.wiley.com/en-us/Bayesian+Theory-p-9780471494645},
	urldate = {2020-11-10},
	publisher = {John Wiley \& Sons},
	author = {Bernado, Jose M. and Smith, Adrian F. M.},
	year = {2009},
}

@article{rezende_stochastic_2014,
	title = {Stochastic {Backpropagation} and {Approximate} {Inference} in {Deep} {Generative} {Models}},
	url = {http://arxiv.org/abs/1401.4082},
	abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent approximate posterior distributions, and that acts as a stochastic encoder of the data. We develop stochastic back-propagation -- rules for back-propagation through stochastic variables -- and use this to develop an algorithm that allows for joint optimisation of the parameters of both the generative and recognition model. We demonstrate on several real-world data sets that the model generates realistic samples, provides accurate imputations of missing data and is a useful tool for high-dimensional data visualisation.},
	urldate = {2020-11-10},
	journal = {arXiv:1401.4082 [cs, stat]},
	author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
	month = may,
	year = {2014},
	note = {arXiv: 1401.4082},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, Statistics - Methodology},
}

@article{austin_exchangeable_2015,
	title = {Exchangeable random measures},
	volume = {51},
	url = {http://www.numdam.org/item/AIHPB_2015__51_3_842_0/},
	doi = {10.1214/13-AIHP584},
	language = {en},
	number = {3},
	urldate = {2020-11-10},
	journal = {Annales de l'I.H.P. Probabilités et statistiques},
	author = {Austin, Tim},
	year = {2015},
	pages = {842--861},
}

@article{kallenberg_foundations_2002,
	title = {Foundations of {Modern} {Probability}},
	author = {Kallenberg, Olav},
	year = {2002},
	pages = {535},
}

@article{rezaei_entropy_2013,
	title = {Entropy and {Graphs}},
	url = {http://arxiv.org/abs/1311.5632},
	abstract = {The entropy of a graph is a functional depending both on the graph itself and on a probability distribution on its vertex set. This graph functional originated from the problem of source coding in information theory and was introduced by J. K{\textbackslash}"\{o\}rner in 1973. Although the notion of graph entropy has its roots in information theory, it was proved to be closely related to some classical and frequently studied graph theoretic concepts. For example, it provides an equivalent definition for a graph to be perfect and it can also be applied to obtain lower bounds in graph covering problems. In this thesis, we review and investigate three equivalent definitions of graph entropy and its basic properties. Minimum entropy colouring of a graph was proposed by N. Alon in 1996. We study minimum entropy colouring and its relation to graph entropy. We also discuss the relationship between the entropy and the fractional chromatic number of a graph which was already established in the literature. A graph \$G\$ is called {\textbackslash}emph\{symmetric with respect to a functional \$F\_G(P)\$\} defined on the set of all the probability distributions on its vertex set if the distribution \$P{\textasciicircum}*\$ maximizing \$F\_G(P)\$ is uniform on \$V(G)\$. Using the combinatorial definition of the entropy of a graph in terms of its vertex packing polytope and the relationship between the graph entropy and fractional chromatic number, we prove that vertex transitive graphs are symmetric with respect to graph entropy. Furthermore, we show that a bipartite graph is symmetric with respect to graph entropy if and only if it has a perfect matching. As a generalization of this result, we characterize some classes of symmetric perfect graphs with respect to graph entropy. Finally, we prove that the line graph of every bridgeless cubic graph is symmetric with respect to graph entropy.},
	urldate = {2020-11-06},
	journal = {arXiv:1311.5632 [math]},
	author = {Rezaei, Seyed Saeed Changiz},
	month = nov,
	year = {2013},
	note = {arXiv: 1311.5632},
	keywords = {Mathematics - Combinatorics},
}

@inproceedings{yongwook_choi_compression_2009,
	address = {Seoul, South Korea},
	title = {Compression of graphical structures},
	isbn = {978-1-4244-4312-3},
	url = {http://ieeexplore.ieee.org/document/5205736/},
	doi = {10.1109/ISIT.2009.5205736},
	abstract = {F. Brooks argues in [3] there is “no theory that gives us a metric for information embodied in structure”. Shannon himself alluded to it ﬁfty years earlier in his little known 1953 paper [14]. Indeed, in the past information theory dealt mostly with “conventional data”, be it textual data, image or video data. However, databases of various sorts have come into existence in recent years for storing “unconventional data” including biological data, web data, topographical maps, and medical data. In compressing such data structures, one must consider two types of information: the information conveyed by the structure itself, and then the information conveyed by the data labels implanted in the structure. In this paper, we attempt to address the former problem by studying information of graphical structures (i.e., unlabeled graphs). In particular, we consider Erdo¨s-Re´nyi graphs G(n, p) over n vertices in which edges are added randomly with probability p. We prove that the structural entropy of G(n, p) is `n2´h(p) − log n! + o(1) = `n2´h(p) − n log n + O(n), where h(p) = −p log p − (1 − p) log(1 − p) is the entropy rate of a conventional memoryless binary source. Then, we design a twostage encoding that optimally compress unlabeled graphs up to the ﬁrst two leading terms of the structural entropy.},
	language = {en},
	urldate = {2020-11-06},
	booktitle = {2009 {IEEE} {International} {Symposium} on {Information} {Theory}},
	publisher = {IEEE},
	author = {{Yongwook Choi} and Szpankowski, Wojciech},
	month = jun,
	year = {2009},
	pages = {364--368},
}

@inproceedings{varshney_universal_2007,
	title = {On {Universal} {Coding} of {Unordered} {Data}},
	doi = {10.1109/ITA.2007.4357578},
	abstract = {There are several applications in information transfer and storage where the order of source letters is irrelevant at the destination. For these source-destination pairs, multiset communication rather than the more difficult task of sequence communication may be performed. In this work, we study universal multiset communication. For classes of countable-alphabet sources that meet Kieffer's condition for sequence communication, we present a scheme that universally achieves a rate of n + o(n) bits per multiset letter for multiset communication. We also define redundancy measures that are normalized by the logarithm of the multiset size rather than per multiset letter and show that these redundancy measures cannot be driven to zero for the class of finite-alphabet memoryless multisets. This further implies that finite-alphabet memoryless multisets cannot be encoded universally with vanishing fractional redundancy.},
	booktitle = {2007 {Information} {Theory} and {Applications} {Workshop}},
	author = {Varshney, L. R. and Goyal, V. K.},
	month = jan,
	year = {2007},
	keywords = {Entropy, Humans, Information representation, Kieffer condition, Laboratories, Multimedia databases, Portable media players, Size measurement, Source coding, Telephony, Transaction databases, countable-alphabet sources, encoding, finite-alphabet memoryless multisets, fractional redundancy, information storage, information transfer, sequence communication, source-destination pairs, universal coding, universal multiset communication, unordered data},
	pages = {183--187},
}

@inproceedings{varshney_toward_2006,
	title = {Toward a source coding theory for sets},
	doi = {10.1109/DCC.2006.78},
	abstract = {The problem of communicating (unordered) sets, rather than (ordered) sequences is formulated. Elementary results in all major branches of source coding theory, including lossless coding, high-rate and low-rate quantization, and rate distortion theory are presented. In certain scenarios, rate savings of log n! bits for sets of size n are obtained. Asymptotically in the set size, the entropy rate is zero and for sources with an ordered parent alphabet, the (0,0) point is the rate distortion function.},
	booktitle = {Data {Compression} {Conference} ({DCC}'06)},
	author = {Varshney, L. R. and Goyal, V. K.},
	month = mar,
	year = {2006},
	note = {ISSN: 2375-0359},
	keywords = {Distortion measurement, Entropy, Information theory, Quantization, Random variables, Rate distortion theory, Size measurement, Source coding, Statistics, Uncertainty, communicating sets, entropy, entropy rate, high-rate quantization, lossless coding, low-rate quantization, ordered parent alphabet, rate distortion theory, set theory, source coding},
	pages = {13--22},
}

@inproceedings{varshney_ordered_2006,
	title = {Ordered and disordered source coding, in},
	abstract = {Abstract — The separation of the information in a vector into order (a permutation) and value (a multiset) is studied. We formulate rate-distortion problems in which order is irrelevant, for which we give conclusive results in cases where the set size approaches infinity. Conversely, we present a distortion measure on partial orders and source coding techniques based on partial orders. Multiple partial orders can be mutually refining, leading to a form of permutation coding for multiple descriptions. Permutation coding of a harmonic frame expansion gives lowcomplexity vector quantizers with low-rate performance superior to that of entropy-constrained scalar quantization. I.},
	booktitle = {Proc. {UCSD} {Workshop} {Inform}. {Theory} {Its} {Applications}, {La} {Jolla}, {CA}},
	author = {Varshney, Lav R. and Goyal, Vivek K.},
	year = {2006},
}

@article{varshney_benefiting_2007,
	title = {Benefiting from {Disorder}: {Source} {Coding} for {Unordered} {Data}},
	shorttitle = {Benefiting from {Disorder}},
	url = {http://arxiv.org/abs/0708.2310},
	abstract = {The order of letters is not always relevant in a communication task. This paper discusses the implications of order irrelevance on source coding, presenting results in several major branches of source coding theory: lossless coding, universal lossless coding, rate-distortion, high-rate quantization, and universal lossy coding. The main conclusions demonstrate that there is a significant rate savings when order is irrelevant. In particular, lossless coding of n letters from a finite alphabet requires Theta(log n) bits and universal lossless coding requires n + o(n) bits for many countable alphabet sources. However, there are no universal schemes that can drive a strong redundancy measure to zero. Results for lossy coding include distribution-free expressions for the rate savings from order irrelevance in various high-rate quantization schemes. Rate-distortion bounds are given, and it is shown that the analogue of the Shannon lower bound is loose at all finite rates.},
	urldate = {2020-11-06},
	journal = {arXiv:0708.2310 [cs, math]},
	author = {Varshney, Lav R. and Goyal, Vivek K.},
	month = aug,
	year = {2007},
	note = {arXiv: 0708.2310},
	keywords = {Computer Science - Information Theory},
}

@article{harremoes_maximum_2009,
	title = {Maximum {Entropy} on {Compact} {Groups}},
	volume = {11},
	issn = {1099-4300},
	url = {http://arxiv.org/abs/0901.0015},
	doi = {10.3390/e11020222},
	abstract = {On a compact group the Haar probability measure plays the role of uniform distribution. The entropy and rate distortion theory for this uniform distribution is studied. New results and simplified proofs on convergence of convolutions on compact groups are presented and they can be formulated as entropy increases to its maximum. Information theoretic techniques and Markov chains play a crucial role. The convergence results are also formulated via rate distortion functions. The rate of convergence is shown to be exponential.},
	number = {2},
	urldate = {2020-11-05},
	journal = {Entropy},
	author = {Harremoes, Peter},
	month = apr,
	year = {2009},
	note = {arXiv: 0901.0015},
	keywords = {Computer Science - Information Theory, Mathematics - Probability},
	pages = {222--237},
}

@article{rodrigues_machine_2020,
	title = {Machine {Learning} {Meets} {Computation} and {Communication} {Control} in {Evolving} {Edge} and {Cloud}: {Challenges} and {Future} {Perspective}},
	volume = {22},
	issn = {1553-877X},
	shorttitle = {Machine {Learning} {Meets} {Computation} and {Communication} {Control} in {Evolving} {Edge} and {Cloud}},
	doi = {10.1109/COMST.2019.2943405},
	abstract = {Mobile Edge Computing (MEC) is considered an essential future service for the implementation of 5G networks and the Internet of Things, as it is the best method of delivering computation and communication resources to mobile devices. It is based on the connection of the users to servers located on the edge of the network, which is especially relevant for real-time applications that demand minimal latency. In order to guarantee a resource-efficient MEC (which, for example, could mean improved Quality of Service for users or lower costs for service providers), it is important to consider certain aspects of the service model, such as where to offload the tasks generated by the devices, how many resources to allocate to each user (specially in the wired or wireless device-server communication) and how to handle inter-server communication. However, in the MEC scenarios with many and varied users, servers and applications, these problems are characterized by parameters with exceedingly high levels of dimensionality, resulting in too much data to be processed and complicating the task of finding efficient configurations. This will be particularly troublesome when 5G networks and Internet of Things roll out, with their massive amounts of devices. To address this concern, the best solution is to utilize Machine Learning (ML) algorithms, which enable the computer to draw conclusions and make predictions based on existing data without human supervision, leading to quick near-optimal solutions even in problems with high dimensionality. Indeed, in scenarios with too much data and too many parameters, ML algorithms are often the only feasible alternative. In this paper, a comprehensive survey on the use of ML in MEC systems is provided, offering an insight into the current progress of this research area. Furthermore, helpful guidance is supplied by pointing out which MEC challenges can be solved by ML solutions, what are the current trending algorithms in frontier ML research and how they could be used in MEC. These pieces of information should prove fundamental in encouraging future research that combines ML and MEC.},
	number = {1},
	journal = {IEEE Communications Surveys Tutorials},
	author = {Rodrigues, T. K. and Suto, K. and Nishiyama, H. and Liu, J. and Kato, N.},
	year = {2020},
	note = {Conference Name: IEEE Communications Surveys Tutorials},
	keywords = {5G mobile communication, 5G networks, Cloud computing, Edge computing, Internet of Things, MEC challenges, MEC scenarios, MEC systems, ML, ML algorithms, ML solutions, Machine learning, Mobile edge computing, Mobile handsets, Servers, Task analysis, artificial intelligence, cloud computing, cloudlet, communication control, communication resources, communication/computation resource management, current trending algorithms, demand minimal, efficient configurations, essential future service, frontier ML research, future perspective, inter-server communication, learning (artificial intelligence), mobile computing, mobile devices, mobile edge computing, near-optimal solutions, real-time applications, resource-efficient MEC, scalability, service model, service providers, wired device-server communication, wireless device-server communication},
	pages = {38--67},
}

@article{zhang_deep_2019,
	title = {Deep {Learning} in {Mobile} and {Wireless} {Networking}: {A} {Survey}},
	shorttitle = {Deep {Learning} in {Mobile} and {Wireless} {Networking}},
	url = {http://arxiv.org/abs/1803.04311},
	abstract = {The rapid uptake of mobile devices and the rising popularity of mobile applications and services pose unprecedented demands on mobile and wireless networking infrastructure. Upcoming 5G systems are evolving to support exploding mobile traffic volumes, agile management of network resource to maximize user experience, and extraction of fine-grained real-time analytics. Fulfilling these tasks is challenging, as mobile environments are increasingly complex, heterogeneous, and evolving. One potential solution is to resort to advanced machine learning techniques to help managing the rise in data volumes and algorithm-driven applications. The recent success of deep learning underpins new and powerful tools that tackle problems in this space. In this paper we bridge the gap between deep learning and mobile and wireless networking research, by presenting a comprehensive survey of the crossovers between the two areas. We first briefly introduce essential background and state-of-the-art in deep learning techniques with potential applications to networking. We then discuss several techniques and platforms that facilitate the efficient deployment of deep learning onto mobile systems. Subsequently, we provide an encyclopedic review of mobile and wireless networking research based on deep learning, which we categorize by different domains. Drawing from our experience, we discuss how to tailor deep learning to mobile environments. We complete this survey by pinpointing current challenges and open future directions for research.},
	urldate = {2020-11-05},
	journal = {arXiv:1803.04311 [cs]},
	author = {Zhang, Chaoyun and Patras, Paul and Haddadi, Hamed},
	month = jan,
	year = {2019},
	note = {arXiv: 1803.04311},
	keywords = {Computer Science - Machine Learning, Computer Science - Networking and Internet Architecture},
}

@article{hou_high_2019,
	title = {High {Resolution} {Medical} {Image} {Analysis} with {Spatial} {Partitioning}},
	url = {http://arxiv.org/abs/1909.03108},
	abstract = {Medical images such as 3D computerized tomography (CT) scans and pathology images, have hundreds of millions or billions of voxels/pixels. It is infeasible to train CNN models directly on such high resolution images, because neural activations of a single image do not fit in the memory of a single GPU/TPU, and naive data and model parallelism approaches do not work. Existing image analysis approaches alleviate this problem by cropping or down-sampling input images, which leads to complicated implementation and sub-optimal performance due to information loss. In this paper, we implement spatial partitioning, which internally distributes the input and output of convolutional layers across GPUs/TPUs. Our implementation is based on the Mesh-TensorFlow framework and the computation distribution is transparent to end users. With this technique, we train a 3D Unet on up to 512 by 512 by 512 resolution data. To the best of our knowledge, this is the first work for handling such high resolution images end-to-end.},
	urldate = {2020-11-05},
	journal = {arXiv:1909.03108 [cs, eess]},
	author = {Hou, Le and Cheng, Youlong and Shazeer, Noam and Parmar, Niki and Li, Yeqing and Korfiatis, Panagiotis and Drucker, Travis M. and Blezek, Daniel J. and Song, Xiaodan},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.03108},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing},
}

@article{steinruecken_compressing_2014,
	title = {Compressing {Sets} and {Multisets} of {Sequences}},
	url = {http://arxiv.org/abs/1401.6410},
	abstract = {This article describes lossless compression algorithms for multisets of sequences, taking advantage of the multiset's unordered structure. Multisets are a generalisation of sets where members are allowed to occur multiple times. A multiset can be encoded na{\textbackslash}"ively by simply storing its elements in some sequential order, but then information is wasted on the ordering. We propose a technique that transforms the multiset into an order-invariant tree representation, and derive an arithmetic code that optimally compresses the tree. Our method achieves compression even if the sequences in the multiset are individually incompressible (such as cryptographic hash sums). The algorithm is demonstrated practically by compressing collections of SHA-1 hash sums, and multisets of arbitrary, individually encodable objects.},
	urldate = {2020-11-05},
	journal = {arXiv:1401.6410 [cs, math, stat]},
	author = {Steinruecken, Christian},
	month = jan,
	year = {2014},
	note = {arXiv: 1401.6410},
	keywords = {Computer Science - Information Theory, Statistics - Applications},
}

@inproceedings{steinruecken_compressing_2016,
	address = {Snowbird, UT, USA},
	title = {Compressing {Combinatorial} {Objects}},
	isbn = {978-1-5090-1853-6},
	url = {http://ieeexplore.ieee.org/document/7786183/},
	doi = {10.1109/DCC.2016.77},
	abstract = {Most of the world’s digital data is currently encoded in a sequential form, and compression methods for sequences have been studied extensively. However, there are many types of nonsequential data for which good compression techniques are still largely unexplored. This paper contributes insights and concrete techniques for compressing various kinds of nonsequential data via arithmetic coding, and derives re-usable probabilistic data models from fairly generic structural assumptions. Near-optimal compression methods are described for certain types of permutations, combinations and multisets; and the conditions for optimality are made explicit for each method.},
	language = {en},
	urldate = {2020-11-05},
	booktitle = {2016 {Data} {Compression} {Conference} ({DCC})},
	publisher = {IEEE},
	author = {Steinruecken, Christian},
	month = mar,
	year = {2016},
	pages = {389--396},
}

@article{besta_survey_2019,
	title = {Survey and {Taxonomy} of {Lossless} {Graph} {Compression} and {Space}-{Efficient} {Graph} {Representations}},
	url = {http://arxiv.org/abs/1806.01799},
	abstract = {Various graphs such as web or social networks may contain up to trillions of edges. Compressing such datasets can accelerate graph processing by reducing the amount of I/O accesses and the pressure on the memory subsystem. Yet, selecting a proper compression method is challenging as there exist a plethora of techniques, algorithms, domains, and approaches in compressing graphs. To facilitate this, we present a survey and taxonomy on lossless graph compression that is the first, to the best of our knowledge, to exhaustively analyze this domain. Moreover, our survey does not only categorize existing schemes, but also explains key ideas, discusses formal underpinning in selected works, and describes the space of the existing compression schemes using three dimensions: areas of research (e.g., compressing web graphs), techniques (e.g., gap encoding), and features (e.g., whether or not a given scheme targets dynamic graphs). Our survey can be used as a guide to select the best lossless compression scheme in a given setting.},
	urldate = {2020-11-05},
	journal = {arXiv:1806.01799 [cs, math]},
	author = {Besta, Maciej and Hoefler, Torsten},
	month = apr,
	year = {2019},
	note = {arXiv: 1806.01799},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Discrete Mathematics, Computer Science - Information Theory},
}

@article{delgosha_universal_2019,
	title = {Universal {Lossless} {Compression} of {Graphical} {Data}},
	url = {http://arxiv.org/abs/1909.09844},
	abstract = {Graphical data is comprised of a graph with marks on its edges and vertices. The mark indicates the value of some attribute associated to the respective edge or vertex. Examples of such data arise in social networks, molecular and systems biology, and web graphs, as well as in several other application areas. Our goal is to design schemes that can efficiently compress such graphical data without making assumptions about its stochastic properties. Namely, we wish to develop a universal compression algorithm for graphical data sources. To formalize this goal, we employ the framework of local weak convergence, also called the objective method, which provides a technique to think of a marked graph as a kind of stationary stochastic processes, stationary with respect to movement between vertices of the graph. In recent work, we have generalized a notion of entropy for unmarked graphs in this framework, due to Bordenave and Caputo, to the case of marked graphs. We use this notion to evaluate the efficiency of a compression scheme. The lossless compression scheme we propose in this paper is then proved to be universally optimal in a precise technical sense. It is also capable of performing local data queries in the compressed form.},
	urldate = {2020-11-05},
	journal = {arXiv:1909.09844 [cs, math]},
	author = {Delgosha, Payam and Anantharam, Venkat},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.09844},
	keywords = {Computer Science - Information Theory, Mathematics - Probability},
}

@article{lundervold_overview_2019,
	series = {Special {Issue}: {Deep} {Learning} in {Medical} {Physics}},
	title = {An overview of deep learning in medical imaging focusing on {MRI}},
	volume = {29},
	issn = {0939-3889},
	url = {http://www.sciencedirect.com/science/article/pii/S0939388918301181},
	doi = {10.1016/j.zemedi.2018.11.002},
	abstract = {What has happened in machine learning lately, and what does it mean for the future of medical image analysis? Machine learning has witnessed a tremendous amount of attention over the last few years. The current boom started around 2009 when so-called deep artificial neural networks began outperforming other established models on a number of important benchmarks. Deep neural networks are now the state-of-the-art machine learning models across a variety of areas, from image analysis to natural language processing, and widely deployed in academia and industry. These developments have a huge potential for medical imaging technology, medical data analysis, medical diagnostics and healthcare in general, slowly being realized. We provide a short overview of recent advances and some associated challenges in machine learning applied to medical image processing and image analysis. As this has become a very broad and fast expanding field we will not survey the entire landscape of applications, but put particular focus on deep learning in MRI. Our aim is threefold: (i) give a brief introduction to deep learning with pointers to core references; (ii) indicate how deep learning has been applied to the entire MRI processing chain, from acquisition to image retrieval, from segmentation to disease prediction; (iii) provide a starting point for people interested in experimenting and perhaps contributing to the field of deep learning for medical imaging by pointing out good educational resources, state-of-the-art open-source code, and interesting sources of data and problems related medical imaging.},
	language = {en},
	number = {2},
	urldate = {2020-11-05},
	journal = {Zeitschrift für Medizinische Physik},
	author = {Lundervold, Alexander Selvikvåg and Lundervold, Arvid},
	month = may,
	year = {2019},
	keywords = {Deep learning, MRI, Machine learning, Medical imaging},
	pages = {102--127},
}

@article{pereira_machine_2009,
	title = {Machine learning classifiers and {fMRI}: a tutorial overview},
	volume = {45},
	issn = {1053-8119},
	shorttitle = {Machine learning classifiers and {fMRI}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2892746/},
	doi = {10.1016/j.neuroimage.2008.11.007},
	number = {1 Suppl},
	urldate = {2020-11-05},
	journal = {NeuroImage},
	author = {Pereira, Francisco and Mitchell, Tom and Botvinick, Matthew},
	month = mar,
	year = {2009},
	pmid = {19070668},
	pmcid = {PMC2892746},
	pages = {S199--S209},
}

@misc{pisano_using_2006,
	title = {Using satellite imagery to improve emergency relief},
	url = {https://odihpn.org/magazine/using-satellite-imagery-to-improve-emergency-relief/},
	abstract = {Mapping has always been indispensable to the progress of humankind. Ancient mapping consisted of drawing by hand the coastline of unknown lands as if observed from above. In 1583, the first Westerner admitted to the court of the Chinese emperor, Jesuit Mattia Ricci, used geographic information to convince the emperor to accept a cultural exchange... Read more »},
	language = {en-GB},
	urldate = {2020-11-05},
	journal = {Humanitarian Practice Network},
	author = {Pisano, Francesco},
	year = {2006},
}

@article{you_deep_nodate,
	title = {Deep {Gaussian} {Process} for {Crop} {Yield} {Prediction} {Based} on {Remote} {Sensing} {Data}},
	abstract = {Agricultural monitoring, in particular in developing countries, can help prevent famine and support humanitarian efforts. A central challenge is yield estimation, which is to predict crop yields before harvesting.},
	language = {en},
	author = {You, Jiaxuan and Li, Xiaocheng and Low, Melvin and Lobell, David and Ermon, Stefano},
	pages = {7},
}

@article{alburez-gutierrez_unhcr_2018,
	title = {The {UNHCR} {Demographic} {Projection} {Tool}: estimating the future size and composition of forcibly displaced populations},
	language = {en},
	author = {Alburez-Gutierrez, Diego and García, Carlota Segura},
	year = {2018},
	pages = {22},
}

@article{lacroix_automated_2018,
	title = {Automated {Shelter} {Recognition} in {Refugee} {Camps}},
	abstract = {In June 2018, more than 68.5 Million people across the globe were reported to be ﬂeeing war or persecution. Within the United Nations, UNOSAT is the organ in charge of collecting demographic information based on satellite images of refugee camps to ensure reliable UN operations providing shelter, food and medicines to refugees and internally displaced people. This work aims at assisting UNOSAT analysts by providing them with an automated shelter detector to increase their efﬁciency and quality of the analysis.},
	language = {en},
	author = {Lacroix, Nathan},
	year = {2018},
	pages = {45},
}

@article{yeh_using_2020,
	title = {Using publicly available satellite imagery and deep learning to understand economic well-being in {Africa}},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-020-16185-w},
	doi = {10.1038/s41467-020-16185-w},
	abstract = {Accurate and comprehensive measurements of economic well-being are fundamental inputs into both research and policy, but such measures are unavailable at a local level in many parts of the world. Here we train deep learning models to predict survey-based estimates of asset wealth across {\textasciitilde} 20,000 African villages from publicly-available multispectral satellite imagery. Models can explain 70\% of the variation in ground-measured village wealth in countries where the model was not trained, outperforming previous benchmarks from high-resolution imagery, and comparison with independent wealth measurements from censuses suggests that errors in satellite estimates are comparable to errors in existing ground data. Satellite-based estimates can also explain up to 50\% of the variation in district-aggregated changes in wealth over time, with daytime imagery particularly useful in this task. We demonstrate the utility of satellite-based estimates for research and policy, and demonstrate their scalability by creating a wealth map for Africa’s most populous country.},
	language = {en},
	number = {1},
	urldate = {2020-11-05},
	journal = {Nature Communications},
	author = {Yeh, Christopher and Perez, Anthony and Driscoll, Anne and Azzari, George and Tang, Zhongyi and Lobell, David and Ermon, Stefano and Burke, Marshall},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {2583},
}

@inproceedings{do-woo_kwon_efficient_2010,
	title = {Efficient digital hologram generation using reflection symmetry of principle fringe pattern},
	doi = {10.1109/ICTC.2010.5674655},
	abstract = {Recently a novel look-up table (N-LUT) method to dramatically reduce the number of pre-calculated interference patterns required for generation of digital holograms was proposed. In this method, principle fringe pattern (PFP) has reflection symmetry in geometry. Thus in this paper, we proposed the memory reduction method using this reflection symmetry of PFP. From some experimental result, the memory could be reduced by 50\%.},
	booktitle = {2010 {International} {Conference} on {Information} and {Communication} {Technology} {Convergence} ({ICTC})},
	author = {{Do-woo Kwon} and Kim, S. and Kim, E.},
	month = nov,
	year = {2010},
	note = {ISSN: 2162-1241},
	keywords = {3D display, Fresnel Zone Plate, Geometry, Holography, Image reconstruction, Image resolution, Novel Look-up Table, Pixel, Reflection, Reflection symmetry, Table lookup, holography},
	pages = {197--198},
}

@article{christandl_hs2010_2010,
	title = {{HS2010} {Symmetries} in {Quantum} {Information} {Theory} and {Quantum} {Computation}},
	abstract = {This course gives an introduction to Quantum Information Theory and Quantum Computation through the study of symmetries of physical systems. After an introduction to the concept of quantum information as the spin degree of a particle, the course develops this concept in two ways: First, the distillation of entanglement, one of the most fundamental tasks in quantum information theory, is explained as a measurement of the total spin of a bunch of particles. Second, computation is introduced as an exchange of particles, leading to the topological model of quantum computation.},
	language = {en},
	author = {Christandl, Matthias},
	year = {2010},
	pages = {54},
}

@misc{walter_symmetry_2018,
	title = {Symmetry and {Quantum} {Information}},
	author = {Walter, Michael},
	year = {2018},
}

@article{raj_local_2017,
	title = {Local {Group} {Invariant} {Representations} via {Orbit} {Embeddings}},
	url = {http://arxiv.org/abs/1612.01988},
	abstract = {Invariance to nuisance transformations is one of the desirable properties of effective representations. We consider transformations that form a {\textbackslash}emph\{group\} and propose an approach based on kernel methods to derive local group invariant representations. Locality is achieved by defining a suitable probability distribution over the group which in turn induces distributions in the input feature space. We learn a decision function over these distributions by appealing to the powerful framework of kernel methods and generate local invariant random feature maps via kernel approximations. We show uniform convergence bounds for kernel approximation and provide excess risk bounds for learning with these features. We evaluate our method on three real datasets, including Rotated MNIST and CIFAR-10, and observe that it outperforms competing kernel based approaches. The proposed method also outperforms deep CNN on Rotated-MNIST and performs comparably to the recently proposed group-equivariant CNN.},
	urldate = {2020-11-05},
	journal = {arXiv:1612.01988 [cs, stat]},
	author = {Raj, Anant and Kumar, Abhishek and Mroueh, Youssef and Fletcher, P. Thomas and Schölkopf, Bernhard},
	month = may,
	year = {2017},
	note = {arXiv: 1612.01988},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{haasdonk_invariant_2007,
	title = {Invariant kernel functions for pattern analysis and machine learning},
	volume = {68},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-007-5009-7},
	doi = {10.1007/s10994-007-5009-7},
	abstract = {In many learning problems prior knowledge about pattern variations can be formalized and beneﬁcially incorporated into the analysis system. The corresponding notion of invariance is commonly used in conceptionally diﬀerent ways. We propose a more distinguishing treatment in particular in the active ﬁeld of kernel methods for machine learning and pattern analysis. Additionally, the fundamental relation of invariant kernels and traditional invariant pattern analysis by means of invariant representations will be clariﬁed. After addressing these conceptional questions, we focus on practical aspects and present two generic approaches for constructing invariant kernels. The ﬁrst approach is based on a technique called invariant integration. The second approach builds on invariant distances. In principle, our approaches support general transformations in particular covering discrete and non-group or even an inﬁnite number of pattern-transformations. Additionally, both enable a smooth interpolation between invariant and non-invariant pattern analysis, i.e. they are a covering general framework. The wide applicability and various possible beneﬁts of invariant kernels are demonstrated in diﬀerent kernel methods.},
	language = {en},
	number = {1},
	urldate = {2020-11-05},
	journal = {Machine Learning},
	author = {Haasdonk, Bernard and Burkhardt, Hans},
	month = may,
	year = {2007},
	pages = {35--61},
}

@article{dao_kernel_2019,
	title = {A {Kernel} {Theory} of {Modern} {Data} {Augmentation}},
	abstract = {Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classiﬁcation. Next, we analyze more directly the effect of augmentation on kernel classiﬁers, showing that data augmentation can be approximated by ﬁrst-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workﬂows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.},
	language = {en},
	author = {Dao, Tri and Gu, Albert and Ratner, Alexander J and Smith, Virginia and Sa, Christopher De and Re, Christopher},
	year = {2019},
	keywords = {augmentation},
	pages = {10},
}

@article{mroueh_learning_2015,
	title = {Learning with {Group} {Invariant} {Features}: {A} {Kernel} {Perspective}},
	shorttitle = {Learning with {Group} {Invariant} {Features}},
	url = {http://arxiv.org/abs/1506.02544},
	abstract = {We analyze in this paper a random feature map based on a theory of invariance I-theory introduced recently. More specifically, a group invariant signal signature is obtained through cumulative distributions of group transformed random projections. Our analysis bridges invariant feature learning with kernel methods, as we show that this feature map defines an expected Haar integration kernel that is invariant to the specified group action. We show how this non-linear random feature map approximates this group invariant kernel uniformly on a set of \$N\$ points. Moreover, we show that it defines a function space that is dense in the equivalent Invariant Reproducing Kernel Hilbert Space. Finally, we quantify error rates of the convergence of the empirical risk minimization, as well as the reduction in the sample complexity of a learning algorithm using such an invariant representation for signal classification, in a classical supervised learning setting.},
	urldate = {2020-11-05},
	journal = {arXiv:1506.02544 [cs, stat]},
	author = {Mroueh, Youssef and Voinea, Stephen and Poggio, Tomaso},
	month = dec,
	year = {2015},
	note = {arXiv: 1506.02544},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@book{hayashi_group_2017,
	title = {A {Group} {Theoretic} {Approach} to {Quantum} {Information}},
	isbn = {978-3-319-45239-5},
	url = {https://www.springer.com/gp/book/9783319452395},
	abstract = {This book is the first one addressing quantum information from the viewpoint of group symmetry. Quantum systems have a group symmetrical structure. This structure enables to handle systematically quantum information processing. However, there is no other textbook focusing on group symmetry for quantum information although there exist many textbooks for group representation. After the mathematical preparation of quantum information, this book discusses quantum entanglement and its quantification by using group symmetry. Group symmetry drastically simplifies the calculation of several entanglement measures although their calculations are usually very difficult to handle. This book treats optimal information processes including quantum state estimation, quantum state cloning, estimation of group action and quantum channel etc. Usually it is very difficult to derive the optimal quantum information processes without asymptotic setting of these topics. However, group symmetry allows to derive these optimal solutions without assuming the asymptotic setting. Next, this book addresses the quantum error correcting code with the symmetric structure of Weyl-Heisenberg groups. This structure leads to understand the quantum error correcting code systematically. Finally, this book focuses on the quantum universal information protocols by using the group SU(d). This topic can be regarded as a quantum version of the Csiszar-Korner's universal coding theory with the type method. The required mathematical knowledge about group representation is summarized in the companion book, Group Representation for Quantum Theory.},
	language = {en},
	urldate = {2020-11-05},
	publisher = {Springer International Publishing},
	author = {Hayashi, Masahito},
	year = {2017},
	doi = {10.1007/978-3-319-45241-8},
}

@article{bendory_single-particle_2020,
	title = {Single-{Particle} {Cryo}-{Electron} {Microscopy}: {Mathematical} {Theory}, {Computational} {Challenges}, and {Opportunities}},
	volume = {37},
	issn = {1053-5888, 1558-0792},
	shorttitle = {Single-{Particle} {Cryo}-{Electron} {Microscopy}},
	url = {https://ieeexplore.ieee.org/document/9016106/},
	doi = {10.1109/MSP.2019.2957822},
	language = {en},
	number = {2},
	urldate = {2020-11-03},
	journal = {IEEE Signal Processing Magazine},
	author = {Bendory, Tamir and Bartesaghi, Alberto and Singer, Amit},
	month = mar,
	year = {2020},
	pages = {58--76},
}

@article{bandeira_estimation_2018,
	title = {Estimation under group actions: recovering orbits from invariants},
	shorttitle = {Estimation under group actions},
	url = {http://arxiv.org/abs/1712.10163},
	abstract = {Motivated by geometric problems in signal processing, computer vision, and structural biology, we study a class of orbit recovery problems where we observe very noisy copies of an unknown signal, each acted upon by a random element of some group (such as Z/p or SO(3)). The goal is to recover the orbit of the signal under the group action in the high-noise regime. This generalizes problems of interest such as multi-reference alignment (MRA) and the reconstruction problem in cryo-electron microscopy (cryo-EM). We obtain matching lower and upper bounds on the sample complexity of these problems in high generality, showing that the statistical difficulty is intricately determined by the invariant theory of the underlying symmetry group. In particular, we determine that for cryo-EM with noise variance \${\textbackslash}sigma{\textasciicircum}2\$ and uniform viewing directions, the number of samples required scales as \${\textbackslash}sigma{\textasciicircum}6\$. We match this bound with a novel algorithm for ab initio reconstruction in cryo-EM, based on invariant features of degree at most 3. We further discuss how to recover multiple molecular structures from heterogeneous cryo-EM samples.},
	urldate = {2020-11-03},
	journal = {arXiv:1712.10163 [cs, math, stat]},
	author = {Bandeira, Afonso S. and Blum-Smith, Ben and Kileel, Joe and Perry, Amelia and Weed, Jonathan and Wein, Alexander S.},
	month = jun,
	year = {2018},
	note = {arXiv: 1712.10163},
	keywords = {62F10, 92C55, 16W22, Computer Science - Data Structures and Algorithms, Computer Science - Information Theory, Mathematics - Commutative Algebra, Mathematics - Statistics Theory},
}

@article{hall_relationship_1965,
	title = {The {Relationship} {Between} {Sufficiency} and {Invariance} with {Applications} in {Sequential} {Analysis}},
	volume = {36},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177700169},
	doi = {10.1214/aoms/1177700169},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {2},
	urldate = {2020-11-03},
	journal = {Annals of Mathematical Statistics},
	author = {Hall, W. J. and Wijsman, R. A. and Ghosh, J. K.},
	month = apr,
	year = {1965},
	mrnumber = {MR178552},
	zmnumber = {0227.62007},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {575--614},
}

@article{dawid_invariance_1985,
	title = {Invariance and independence in multivariate distribution theory},
	volume = {17},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/0047259X85900867},
	doi = {10.1016/0047-259X(85)90086-7},
	abstract = {Several general results are presented whereby various properties of independence or conditional independence between certain random variables may be deduced from the symmetries enjoyed by their joint distributions. These are applied to the distributions of sample correlation and canonical correlation coefficients when the underlying data-distribution has suitable orthogonal invariance. A typical result is that, for a random sample of observations on three independent normal variables, r12, r13, and r23.1 are mutually independent.},
	language = {en},
	number = {3},
	urldate = {2020-11-02},
	journal = {Journal of Multivariate Analysis},
	author = {Dawid, A. P},
	month = dec,
	year = {1985},
	keywords = {invariance, spherical symmetric distributions},
	pages = {304--315},
}

@article{diaconis_sufficiency_1988,
	title = {Sufficiency as statistical symmetry},
	journal = {Proceedings of the AMS Centennial Symposium},
	author = {Diaconis, Persi},
	year = {1988},
}

@article{orbanz_exchangeability_nodate,
	title = {Exchangeability, symmetry and sufficiency},
	language = {en},
	author = {Orbanz, Peter},
	pages = {14},
}

@article{soatto_visual_2016,
	title = {Visual {Representations}: {Defining} {Properties} and {Deep} {Approximations}},
	shorttitle = {Visual {Representations}},
	url = {http://arxiv.org/abs/1411.7676},
	abstract = {Visual representations are defined in terms of minimal sufficient statistics of visual data, for a class of tasks, that are also invariant to nuisance variability. Minimal sufficiency guarantees that we can store a representation in lieu of raw data with smallest complexity and no performance loss on the task at hand. Invariance guarantees that the statistic is constant with respect to uninformative transformations of the data. We derive analytical expressions for such representations and show they are related to feature descriptors commonly used in computer vision, as well as to convolutional neural networks. This link highlights the assumptions and approximations tacitly assumed by these methods and explains empirical practices such as clamping, pooling and joint normalization.},
	urldate = {2020-11-02},
	journal = {arXiv:1411.7676 [cs]},
	author = {Soatto, Stefano and Chiuso, Alessandro},
	month = feb,
	year = {2016},
	note = {arXiv: 1411.7676},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{darvas_symmetry_nodate,
	title = {Symmetry, order, entropy and information},
	abstract = {Conditions of applicability of the laws established for thermodynamic entropy do not necessarily fit to the entropy defined for information. Therefore, one must handle carefully the informational conclusions derived by mathematical analogies from the laws that hold for thermodynamic entropy.},
	language = {en},
	author = {Darvas, György},
	pages = {14},
}

@inproceedings{collier_information_1996,
	title = {Information originates in symmetry breaking},
	abstract = {We find symmetry attractive. It interests us. Symmetry is often an indicator of the deep structure of things, whether they be natural phenomena, or the creations of artists. For example, the most fundamental conservation laws of physics are all based in symmetry. Similarly, the symmetries found in religious art throughout the world are intended to draw attention to deep spiritual truths. Not only do we find symmetry pleasing, but its discovery is often also surprising and illuminating as well. For these reasons, we are inclined to think that symmetries are informative, and that symmetries contain information. On the other hand, symmetries represent a kind of invariance under transformation. Such invariance implies that symmetrical things contain redundancies. Redundancy, in turn, implies that the information content of a symmetrical structure or configuration is less than that of a similar nonsymmetrical structure. Symmetry, then, entails a reduction in information content. These considerations present us with somewhat of a paradox: On the one hand, many symmetries that we find in the world are surprising, and surprise indicates informativeness. On the other hand, the surprise value of information arises because it presents us with the unexpected or improbable, but symmetries, far from creating the unexpected, ensure that the known can be extended through invariant transformations. How can this paradox be},
	booktitle = {Symmetry: {Culture} \& {Science}},
	author = {Collier, John},
	year = {1996},
	pages = {247--256},
}

@article{dixon_probabilistic_nodate,
	title = {Probabilistic {Group} {Theory}},
	abstract = {This survey discusses three aspects of the ways in which probability has been applied to the theory of ﬁnite groups: probabilistic statements about groups; construction of randomized algorithms in computational group theory; and application of probabilistic methods to prove deterministic theorems in group theory. It concludes with a brief summary of related results for inﬁnite groups.},
	language = {en},
	author = {Dixon, John D},
	pages = {18},
}

@article{parada-mayorga_algebraic_2020,
	title = {Algebraic {Neural} {Networks}: {Stability} to {Deformations}},
	shorttitle = {Algebraic {Neural} {Networks}},
	url = {http://arxiv.org/abs/2009.01433},
	abstract = {In this work we study the stability of algebraic neural networks (AlgNNs) with commutative algebras which unify CNNs and GNNs under the umbrella of algebraic signal processing. An AlgNN is a stacked layered structure where each layer is conformed by an algebra \${\textbackslash}mathcal\{A\}\$, a vector space \${\textbackslash}mathcal\{M\}\$ and a homomorphism \${\textbackslash}rho:{\textbackslash}mathcal\{A\}{\textbackslash}rightarrow{\textbackslash}text\{End\}({\textbackslash}mathcal\{M\})\$, where \${\textbackslash}text\{End\}({\textbackslash}mathcal\{M\})\$ is the set of endomorphims of \${\textbackslash}mathcal\{M\}\$. Signals in each layer are modeled as elements of \${\textbackslash}mathcal\{M\}\$ and are processed by elements of \${\textbackslash}text\{End\}({\textbackslash}mathcal\{M\})\$ defined according to the structure of \${\textbackslash}mathcal\{A\}\$ via \${\textbackslash}rho\$. This framework provides a general scenario that covers several types of neural network architectures where formal convolution operators are being used. We obtain stability conditions regarding to perturbations which are defined as distortions of \${\textbackslash}rho\$, reaching general results whose particular cases are consistent with recent findings in the literature for CNNs and GNNs. We consider conditions on the domain of the homomorphisms in the algebra that lead to stable operators. Interestingly, we found that these conditions are related to the uniform boundedness of the Fr{\textbackslash}'echet derivative of a function \$p:{\textbackslash}text\{End\}({\textbackslash}mathcal\{M\}){\textbackslash}rightarrow{\textbackslash}text\{End\}({\textbackslash}mathcal\{M\})\$ that maps the images of the generators of \${\textbackslash}mathcal\{A\}\$ on \${\textbackslash}text\{End\}({\textbackslash}mathcal\{M\})\$ into a power series representation that defines the filtering of elements in \${\textbackslash}mathcal\{M\}\$. Additionally, our results show that stability is universal to convolutional architectures whose algebraic signal model uses the same algebra.},
	urldate = {2020-10-30},
	journal = {arXiv:2009.01433 [cs, stat]},
	author = {Parada-Mayorga, Alejandro and Ribeiro, Alejandro},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.01433},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@incollection{chapelle_vicinal_2001,
	title = {Vicinal {Risk} {Minimization}},
	url = {http://papers.nips.cc/paper/1876-vicinal-risk-minimization.pdf},
	urldate = {2020-10-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 13},
	publisher = {MIT Press},
	author = {Chapelle, Olivier and Weston, Jason and Bottou, Léon and Vapnik, Vladimir},
	editor = {Leen, T. K. and Dietterich, T. G. and Tresp, V.},
	year = {2001},
	pages = {416--422},
}

@inproceedings{maaten_learning_2013,
	title = {Learning with {Marginalized} {Corrupted} {Features}},
	url = {http://proceedings.mlr.press/v28/vandermaaten13.html},
	abstract = {The goal of machine learning is to develop predictors that generalize well to test data. Ideally, this is achieved by training on very large (infinite) training data sets that capture all variation...},
	language = {en},
	urldate = {2020-10-30},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Maaten, Laurens and Chen, Minmin and Tyree, Stephen and Weinberger, Kilian},
	month = feb,
	year = {2013},
	note = {ISSN: 1938-7228},
	pages = {410--418},
}

@article{de_haan_natural_2020,
	title = {Natural {Graph} {Networks}},
	url = {http://arxiv.org/abs/2007.08349},
	abstract = {Conventional neural message passing algorithms are invariant under permutation of the messages and hence forget how the information flows through the network. Studying the local symmetries of graphs, we propose a more general algorithm that uses different kernels on different edges, making the network equivariant to local and global graph isomorphisms and hence more expressive. Using elementary category theory, we formalize many distinct equivariant neural networks as natural networks, and show that their kernels are 'just' a natural transformation between two functors. We give one practical instantiation of a natural network on graphs which uses a equivariant message network parameterization, yielding good performance on several benchmarks.},
	urldate = {2020-10-30},
	journal = {arXiv:2007.08349 [cs, stat]},
	author = {de Haan, Pim and Cohen, Taco and Welling, Max},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08349},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_what_nodate,
	title = {What is a {Natural} {Transformation}? {Definition} and {Examples}},
	shorttitle = {What is a {Natural} {Transformation}?},
	url = {https://www.math3ma.com/blog/what-is-a-natural-transformation},
	urldate = {2020-10-30},
}

@incollection{diekert_efficient_2007,
	address = {Berlin, Heidelberg},
	title = {Efficient {Computation} in {Groups} {Via} {Compression}},
	volume = {4649},
	isbn = {978-3-540-74509-9 978-3-540-74510-5},
	url = {http://link.springer.com/10.1007/978-3-540-74510-5_26},
	abstract = {A compressed variant of the word problem for ﬁnitely generated groups, where the input word is given by a context-free grammar that generates exactly one string (also called a straight-line program), is studied. It is shown that ﬁnite extensions and free products preserve the complexity of the compressed word problem and that the compressed word problem for a graph group can be solved in polynomial time. Using these results together with connections between the compressed word problem and the (classical) word problem allows to obtain new upper complexity bounds for certain automorphism groups and group extensions.},
	language = {en},
	urldate = {2020-10-30},
	booktitle = {Computer {Science} – {Theory} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lohrey, Markus and Schleimer, Saul},
	editor = {Diekert, Volker and Volkov, Mikhail V. and Voronkov, Andrei},
	year = {2007},
	doi = {10.1007/978-3-540-74510-5_26},
	note = {ISSN: 0302-9743, 1611-3349
Series Title: Lecture Notes in Computer Science},
	pages = {249--258},
}

@article{guerreiro_group_2016,
	title = {Group algebras and coding theory},
	volume = {10},
	issn = {1982-6907, 2316-9028},
	url = {http://link.springer.com/10.1007/s40863-016-0040-x},
	doi = {10.1007/s40863-016-0040-x},
	abstract = {In the ﬁrst part of this course, we present an introduction to the subject covering some of the important results that can be applied in this context, starting with the most basic facts. We begin with the famous theorem of Maschke and use Wedderburn’s Theorem to describe the structure of group algebras in the semisimple case and its relation to primitive idempotents. We consider splitting ﬁelds and a Theorem of R. Brauer then study the theorem of Berman and Witt that gives the number of simple componentes in the semisimple case. In the late sixties, S.D Berman [1] and F.J. MacWilliams [5], independently, introduced the idea of a group code, deﬁned as an ideal of a ﬁnite group algebra. In the second part, we construct idempotents for abelian codes, always using the structure of subgroups of the underlying group. In some cases, it is possible to compute the parameters of the codes, and bases, using the group algebra structure. The construction of idempotents may also be extended to some nonabelian codes deﬁned from dihedral and quaternion groups We ﬁnish mentioning some further developments on codes over rings.},
	language = {en},
	number = {2},
	urldate = {2020-10-30},
	journal = {São Paulo Journal of Mathematical Sciences},
	author = {Guerreiro, Marinês},
	month = dec,
	year = {2016},
	pages = {346--371},
}

@article{polcino_mllies_group_2019,
	title = {Group algebras and coding theory: a short survey},
	volume = {37},
	issn = {0120-419X},
	shorttitle = {Group algebras and coding theory},
	url = {http://www.scielo.org.co/scielo.php?script=sci_abstract&pid=S0120-419X2019000100153&lng=en&nrm=iso&tlng=en},
	doi = {10.18273/revint.v37n1-2019008.},
	abstract = {AbstractWe study codes constructed from ideals in group algebras and we are particularly interested in their dimensions and weights. First we introduced a special kind of idempotents and study the ideals they generate. We use this information to show that there exist abelian non-cyclic groups that give codes which are more convenient than the cyclic ones. Finally, we discuss briefly some facts about non-abelian codes.MSC2010: 16S34, 20C05, 94B15.Key words: code; Hamming distance; weight; group algebra; ideal; group code},
	number = {1},
	urldate = {2020-10-30},
	journal = {Revista Integración},
	author = {Polcino Mllies, César and Polcino Mllies, César},
	month = jun,
	year = {2019},
	note = {Publisher: Universidad Industrial de Santander},
	pages = {153--166},
}

@inproceedings{cibej_symmetry-compressible_2017,
	title = {Symmetry-{Compressible} {Graphs}},
	doi = {10.1109/DCC.2017.20},
	abstract = {This article describes an alternative representation of graphs, using symmetries. We define the class of graphs that are compressed using this representation as symmetry-compressible graphs. This class of graphs is extended into the class of near symmetry-compressible graphs, which includes many more graphs arising in practical applications. To demonstrate the practical potential of the proposed concepts, an empirical evaluation of two algorithms is given.},
	booktitle = {2017 {Data} {Compression} {Conference} ({DCC})},
	author = {Čibej, U. and Mihelič, J.},
	month = apr,
	year = {2017},
	note = {ISSN: 2375-0359},
	keywords = {Bipartite graph, Compression algorithms, Data compression, Image coding, Image reconstruction, Presses, data compression, graph automorphisms, graph compression, graph representation, graph theory, graphlet compression, symmetries, symmetry-compressible graphs},
	pages = {435--435},
}

@misc{perez-ramirez_application_2017,
	type = {Research {Article}},
	title = {Application of {Mathematical} {Symmetrical} {Group} {Theory} in the {Creation} {Process} of {Digital} {Holograms}},
	url = {https://www.hindawi.com/journals/mpe/2017/5612743/},
	abstract = {This work presents an algorithm to reduce the multiplicative computational complexity in the creation of digital holograms, where an object is considered as a set of point sources using mathematical symmetry properties of both the core in the Fresnel integral and the image. The image is modeled using group theory. This algorithm has multiplicative complexity equal to zero and an additive complexity for the case of sparse matrices or binary images, where is the number of pixels other than zero and is the total of points in the image.},
	language = {en},
	urldate = {2020-10-30},
	journal = {Mathematical Problems in Engineering},
	author = {Pérez-Ramírez, Agustín and Guerrero Juk, Julian and Sanchez-Lara, Rafael and Trejo-Sánchez, Joel Antonio and de la Cruz-May, Lelio},
	month = jul,
	year = {2017},
	doi = {https://doi.org/10.1155/2017/5612743},
	doi = {https://doi.org/10.1155/2017/5612743},
	note = {ISSN: 1024-123X
Pages: e5612743
Publisher: Hindawi
Volume: 2017},
}

@article{pei_representation_nodate,
	title = {{REPRESENTATION} {OF} {IMAGE} {BY} {LOCAL} {SYMMETRY} {DECOMPOSITION}},
	abstract = {We present a new technique for extracting local features from images, based on detecting and representing local symmetries. We introduce the even/odd symmetries. Then, we generalize the even/odd decomposition by concentrating the energy on either the even or the odd part by optimally placing the center of symmetry. Local symmetry intervals are thus located. An image is segmented into adjacent, variable-width, and variableheight where strong symmetry characteristics exist. Finally, we detect the even/odd symmetry in the natural image, and compute the Peak Signal-to-Noise Ratio (PSNR) and compression ratio between the original and compressed image.},
	language = {en},
	author = {Pei, Soo-Chang and Tasi, Hsin-Ying and Fuh, Chiou-Shann},
	pages = {8},
}

@article{gedeon_mathematical_2012,
	title = {The {Mathematical} {Structure} of {Information} {Bottleneck} {Methods}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/14/3/456},
	doi = {10.3390/e14030456},
	abstract = {Information Bottleneck-based methods use mutual information as a distortion function in order to extract relevant details about the structure of a complex system by compression. One of the approaches used to generate optimal compressed representations is by annealing a parameter. In this manuscript we present a common framework for the study of annealing in information distortion problems. We identify features that should be common to any annealing optimization problem. The main mathematical tools that we use come from the analysis of dynamical systems in the presence of symmetry (equivariant bifurcation theory). Through the compression problem, we make connections to the world of combinatorial optimization and pattern recognition. The two approaches use very different vocabularies and consider different problems to be “interesting”. We provide an initial link, through the Normalized Cut Problem, where the two disciplines can exchange tools and ideas.},
	language = {en},
	number = {3},
	urldate = {2020-10-30},
	journal = {Entropy},
	author = {Gedeon, Tomáš and Parker, Albert E. and Dimitrov, Alexander G.},
	month = mar,
	year = {2012},
	note = {Number: 3
Publisher: Molecular Diversity Preservation International},
	keywords = {bifurcations, information distortion, phase transition, spontaneous symmetry breaking},
	pages = {456--479},
}

@article{sanchez-garcia_exploiting_2020,
	title = {Exploiting symmetry in network analysis},
	volume = {3},
	copyright = {2020 The Author(s)},
	issn = {2399-3650},
	url = {https://www.nature.com/articles/s42005-020-0345-z},
	doi = {10.1038/s42005-020-0345-z},
	abstract = {Virtually all network analyses involve structural measures between pairs of vertices, or of the vertices themselves, and the large amount of symmetry present in real-world complex networks is inherited by such measures. This has practical consequences that have not yet been explored in full generality, nor systematically exploited by network practitioners. Here we study the effect of network symmetry on arbitrary network measures, and show how this can be exploited in practice in a number of ways, from redundancy compression, to computational reduction. We also uncover the spectral signatures of symmetry for an arbitrary network measure such as the graph Laplacian. Computing network symmetries is very efficient in practice, and we test real-world examples up to several million nodes. Since network models are ubiquitous in the Applied Sciences, and typically contain a large degree of structural redundancy, our results are not only significant, but widely applicable.},
	language = {en},
	number = {1},
	urldate = {2020-10-30},
	journal = {Communications Physics},
	author = {Sánchez-García, Rubén J.},
	month = may,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {group},
	pages = {1--15},
}

@article{kontoyiannis_compression_2020,
	title = {Compression and {Symmetry} of {Small}-{World} {Graphs} and {Structures}},
	url = {http://arxiv.org/abs/2007.15981},
	abstract = {For various purposes and, in particular, in the context of data compression, a graph can be examined at three levels. Its structure can be described as the unlabeled version of the graph; then the labeling of its structure can be added; and finally, given then structure and labeling, the contents of the labels can be described. Determining the amount of information present at each level and quantifying the degree of dependence between them, requires the study of symmetry, graph automorphism, entropy, and graph compressibility. In this paper, we focus on a class of small-world graphs. These are geometric random graphs where vertices are first connected to their nearest neighbors on a circle and then pairs of non-neighbors are connected according to a distance-dependent probability distribution. We establish the degree distribution of this model, and use it to prove the model's asymmetry in an appropriate range of parameters. Then we derive the relevant entropy and structural entropy of these random graphs, in connection with graph compression.},
	urldate = {2020-10-30},
	journal = {arXiv:2007.15981 [cs, math]},
	author = {Kontoyiannis, Ioannis and Lim, Yi Heng and Papakonstantinopoulou, Katia and Szpankowski, Wojtek},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.15981},
	keywords = {Computer Science - Information Theory, Mathematics - Combinatorics, Mathematics - Probability, compression, graph, group},
}

@article{chan_relation_2002,
	title = {On a relation between information inequalities and group theory},
	volume = {48},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1013138/},
	doi = {10.1109/TIT.2002.1013138},
	abstract = {In this paper, we establish a one-to-one correspondence between information inequalities and group inequalities. The major implication of our result is that we can prove information inequalities by proving the corresponding group inequalities, and vice versa. By giving a group-theoretic proof for all Shannon-type inequalities, we suggest that new inequalities could be discovered by making use of the rich set of tools in group theory. On the other hand, via a non-Shannon-type information inequality recently discovered by Zhang and Yeung, we obtain a new inequality in group theory whose meaning is yet to be understood.},
	language = {en},
	number = {7},
	urldate = {2020-10-30},
	journal = {IEEE Transactions on Information Theory},
	author = {Chan, T.H. and Yeung, R.W.},
	month = jul,
	year = {2002},
	pages = {1992--1995},
}

@article{lun_relationship_nodate,
	title = {A {Relationship} {Between} {Information} {Inequalities} and {Group} {Theory}},
	language = {en},
	author = {Lun, Desmond S},
	keywords = {group, information},
	pages = {8},
}

@article{chen_group-theoretic_2020,
	title = {A {Group}-{Theoretic} {Framework} for {Data} {Augmentation}},
	url = {http://arxiv.org/abs/1907.10905},
	abstract = {Data augmentation is a widely used trick when training deep neural networks: in addition to the original data, properly transformed data are also added to the training set. However, to the best of our knowledge, a clear mathematical framework to explain the performance benefits of data augmentation is not available. In this paper, we develop such a theoretical framework. We show data augmentation is equivalent to an averaging operation over the orbits of a certain group that keeps the data distribution approximately invariant. We prove that it leads to variance reduction. We study empirical risk minimization, and the examples of exponential families, linear regression, and certain two-layer neural networks. We also discuss how data augmentation could be used in problems with symmetry where other approaches are prevalent, such as in cryo-electron microscopy (cryo-EM).},
	urldate = {2020-10-30},
	journal = {arXiv:1907.10905 [cs, math, stat]},
	author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane H.},
	month = feb,
	year = {2020},
	note = {arXiv: 1907.10905},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, augmentation, symmetry},
}

@article{kusner_grammar_2017,
	title = {Grammar {Variational} {Autoencoder}},
	url = {http://arxiv.org/abs/1703.01925},
	abstract = {Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis.},
	urldate = {2020-10-29},
	journal = {arXiv:1703.01925 [stat]},
	author = {Kusner, Matt J. and Paige, Brooks and Hernández-Lobato, José Miguel},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.01925},
	keywords = {Statistics - Machine Learning, bayesian, discrete, variational},
}

@article{nokleby_rate-distortion_2017,
	title = {Rate-{Distortion} {Bounds} on {Bayes} {Risk} in {Supervised} {Learning}},
	url = {http://arxiv.org/abs/1605.02268},
	abstract = {We present an information-theoretic framework for bounding the number of labeled samples needed to train a classifier in a parametric Bayesian setting. We derive bounds on the average \$L\_p\$ distance between the learned classifier and the true maximum a posteriori classifier, which are well-established surrogates for the excess classification error due to imperfect learning. We provide lower and upper bounds on the rate-distortion function, using \$L\_p\$ loss as the distortion measure, of a maximum a priori classifier in terms of the differential entropy of the posterior distribution and a quantity called the interpolation dimension, which characterizes the complexity of the parametric distribution family. In addition to expressing the information content of a classifier in terms of lossy compression, the rate-distortion function also expresses the minimum number of bits a learning machine needs to extract from training data to learn a classifier to within a specified \$L\_p\$ tolerance. We use results from universal source coding to express the information content in the training data in terms of the Fisher information of the parametric family and the number of training samples available. The result is a framework for computing lower bounds on the Bayes \$L\_p\$ risk. This framework complements the well-known probably approximately correct (PAC) framework, which provides minimax risk bounds involving the Vapnik-Chervonenkis dimension or Rademacher complexity. Whereas the PAC framework provides upper bounds the risk for the worst-case data distribution, the proposed rate-distortion framework lower bounds the risk averaged over the data distribution. We evaluate the bounds for a variety of data models, including categorical, multinomial, and Gaussian models. In each case the bounds are provably tight orderwise, and in two cases we prove that the bounds are tight up to multiplicative constants.},
	urldate = {2020-10-29},
	journal = {arXiv:1605.02268 [cs, math, stat]},
	author = {Nokleby, Matthew and Beirami, Ahmad and Calderbank, Robert},
	month = nov,
	year = {2017},
	note = {arXiv: 1605.02268},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning, information},
}

@article{blier_description_2018,
	title = {The {Description} {Length} of {Deep} {Learning} {Models}},
	url = {http://arxiv.org/abs/1802.07044},
	abstract = {Solomonoff's general theory of inference and the Minimum Description Length principle formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks. Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff's approach.},
	urldate = {2020-10-26},
	journal = {arXiv:1802.07044 [cs]},
	author = {Blier, Léonard and Ollivier, Yann},
	month = nov,
	year = {2018},
	note = {arXiv: 1802.07044},
	keywords = {Computer Science - Machine Learning, MDL, coding, information},
}

@article{harsha_communication_nodate,
	title = {The {Communication} {Complexity} of {Correlation}},
	language = {en},
	author = {Harsha, Prahladh and Jain, Rahul and McAllester, David and Radhakrishnan, Jaikumar},
	keywords = {coding, communication complexity, information},
	pages = {12},
}

@article{kingma_bit-swap_2019,
	title = {Bit-{Swap}: {Recursive} {Bits}-{Back} {Coding} for {Lossless} {Compression} with {Hierarchical} {Latent} {Variables}},
	shorttitle = {Bit-{Swap}},
	url = {http://arxiv.org/abs/1905.06845},
	abstract = {The bits-back argument suggests that latent variable models can be turned into lossless compression schemes. Translating the bits-back argument into efficient and practical lossless compression schemes for general latent variable models, however, is still an open problem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), recently proposed by Townsend et al. (2019), makes bits-back coding practically feasible for latent variable models with one latent layer, but it is inefficient for hierarchical latent variable models. In this paper we propose Bit-Swap, a new compression scheme that generalizes BB-ANS and achieves strictly better compression rates for hierarchical latent variable models with Markov chain structure. Through experiments we verify that Bit-Swap results in lossless compression rates that are empirically superior to existing techniques. Our implementation is available at https://github.com/fhkingma/bitswap.},
	urldate = {2020-10-26},
	journal = {arXiv:1905.06845 [cs, math, stat]},
	author = {Kingma, Friso H. and Abbeel, Pieter and Ho, Jonathan},
	month = oct,
	year = {2019},
	note = {arXiv: 1905.06845},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning, coding, information},
}

@article{choi_neural_2019,
	title = {Neural {Joint} {Source}-{Channel} {Coding}},
	url = {http://arxiv.org/abs/1811.07557},
	abstract = {For reliable transmission across a noisy communication channel, classical results from information theory show that it is asymptotically optimal to separate out the source and channel coding processes. However, this decomposition can fall short in the finite bit-length regime, as it requires non-trivial tuning of hand-crafted codes and assumes infinite computational power for decoding. In this work, we propose to jointly learn the encoding and decoding processes using a new discrete variational autoencoder model. By adding noise into the latent codes to simulate the channel during training, we learn to both compress and error-correct given a fixed bit-length and computational budget. We obtain codes that are not only competitive against several separation schemes, but also learn useful robust representations of the data for downstream tasks such as classification. Finally, inference amortization yields an extremely fast neural decoder, almost an order of magnitude faster compared to standard decoding methods based on iterative belief propagation.},
	urldate = {2020-10-26},
	journal = {arXiv:1811.07557 [cs, math, stat]},
	author = {Choi, Kristy and Tatwawadi, Kedar and Grover, Aditya and Weissman, Tsachy and Ermon, Stefano},
	month = may,
	year = {2019},
	note = {arXiv: 1811.07557},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning, coding, generative, information},
}

@article{chen_variational_2017,
	title = {Variational {Lossy} {Autoencoder}},
	url = {http://arxiv.org/abs/1611.02731},
	abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x{\textbar}z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
	urldate = {2020-10-25},
	journal = {arXiv:1611.02731 [cs, stat]},
	author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.02731},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{havasi_minimal_2018,
	title = {Minimal {Random} {Code} {Learning}: {Getting} {Bits} {Back} from {Compressed} {Model} {Parameters}},
	shorttitle = {Minimal {Random} {Code} {Learning}},
	url = {http://arxiv.org/abs/1810.00440},
	abstract = {While deep neural networks are a highly successful model class, their large memory footprint puts considerable strain on energy consumption, communication bandwidth, and storage requirements. Consequently, model size reduction has become an utmost goal in deep learning. A typical approach is to train a set of deterministic weights, while applying certain techniques such as pruning and quantization, in order that the empirical weight distribution becomes amenable to Shannon-style coding schemes. However, as shown in this paper, relaxing weight determinism and using a full variational distribution over weights allows for more efficient coding schemes and consequently higher compression rates. In particular, following the classical bits-back argument, we encode the network weights using a random sample, requiring only a number of bits corresponding to the Kullback-Leibler divergence between the sampled variational distribution and the encoding distribution. By imposing a constraint on the Kullback-Leibler divergence, we are able to explicitly control the compression rate, while optimizing the expected loss on the training set. The employed encoding scheme can be shown to be close to the optimal information-theoretical lower bound, with respect to the employed variational family. Our method sets new state-of-the-art in neural network compression, as it strictly dominates previous approaches in a Pareto sense: On the benchmarks LeNet-5/MNIST and VGG-16/CIFAR-10, our approach yields the best test performance for a fixed memory budget, and vice versa, it achieves the highest compression rates for a fixed test performance.},
	urldate = {2020-10-24},
	journal = {arXiv:1810.00440 [cs, stat]},
	author = {Havasi, Marton and Peharz, Robert and Hernández-Lobato, José Miguel},
	month = sep,
	year = {2018},
	note = {arXiv: 1810.00440},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maddison_concrete_2017,
	title = {The {Concrete} {Distribution}: {A} {Continuous} {Relaxation} of {Discrete} {Random} {Variables}},
	shorttitle = {The {Concrete} {Distribution}},
	url = {http://arxiv.org/abs/1611.00712},
	abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
	urldate = {2020-10-07},
	journal = {arXiv:1611.00712 [cs, stat]},
	author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
	month = mar,
	year = {2017},
	note = {arXiv: 1611.00712},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{maddison_dual_2019,
	title = {Dual {Space} {Preconditioning} for {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1902.02257},
	abstract = {The conditions of relative smoothness and relative strong convexity were recently introduced for the analysis of Bregman gradient methods for convex optimization. We introduce a fully explicit descent scheme with relative smoothness in the dual space between the convex conjugate of the objective function and a designed dual reference function. For Legendre type convex functions under this dual relative smoothness, our scheme naturally remains in the interior of the domain, despite being fully explicit. We obtain linear convergence under dual relative strong convexity with a condition number that is invariant under horizontal translations. Our method is a non-linear preconditioning of gradient descent that can improve the conditioning of explicit first-order methods on problems with non-smooth or non-strongly convex structure. We show how this method can be applied to p-norm regression and exponential penalty function minimization.},
	urldate = {2020-09-09},
	journal = {arXiv:1902.02257 [math]},
	author = {Maddison, Chris J. and Paulin, Daniel and Teh, Yee Whye and Doucet, Arnaud},
	month = dec,
	year = {2019},
	note = {arXiv: 1902.02257},
	keywords = {Mathematics - Optimization and Control},
}

@article{maddison_hamiltonian_2018,
	title = {Hamiltonian {Descent} {Methods}},
	url = {http://arxiv.org/abs/1809.05042},
	abstract = {We propose a family of optimization methods that achieve linear convergence using first-order gradient information and constant step sizes on a class of convex functions much larger than the smooth and strongly convex ones. This larger class includes functions whose second derivatives may be singular or unbounded at their minima. Our methods are discretizations of conformal Hamiltonian dynamics, which generalize the classical momentum method to model the motion of a particle with non-standard kinetic energy exposed to a dissipative force and the gradient field of the function of interest. They are first-order in the sense that they require only gradient computation. Yet, crucially the kinetic gradient map can be designed to incorporate information about the convex conjugate in a fashion that allows for linear convergence on convex functions that may be non-smooth or non-strongly convex. We study in detail one implicit and two explicit methods. For one explicit method, we provide conditions under which it converges to stationary points of non-convex functions. For all, we provide conditions on the convex function and kinetic energy pair that guarantee linear convergence, and show that these conditions can be satisfied by functions with power growth. In sum, these methods expand the class of convex functions on which linear convergence is possible with first-order computation.},
	urldate = {2020-09-09},
	journal = {arXiv:1809.05042 [cs, math, stat]},
	author = {Maddison, Chris J. and Paulin, Daniel and Teh, Yee Whye and O'Donoghue, Brendan and Doucet, Arnaud},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.05042},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@article{maddison_filtering_2017,
	title = {Filtering {Variational} {Objectives}},
	url = {http://arxiv.org/abs/1705.09279},
	abstract = {When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.},
	urldate = {2020-09-09},
	journal = {arXiv:1705.09279 [cs, stat]},
	author = {Maddison, Chris J. and Lawson, Dieterich and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee Whye},
	month = nov,
	year = {2017},
	note = {arXiv: 1705.09279},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
}

@article{paulus_gradient_2020,
	title = {Gradient {Estimation} with {Stochastic} {Softmax} {Tricks}},
	url = {http://arxiv.org/abs/2006.08063},
	abstract = {The Gumbel-Max trick is the basis of many relaxed gradient estimators. These estimators are easy to implement and low variance, but the goal of scaling them comprehensively to large combinatorial distributions is still outstanding. Working within the perturbation model framework, we introduce stochastic softmax tricks, which generalize the Gumbel-Softmax trick to combinatorial spaces. Our framework is a unified perspective on existing relaxed estimators for perturbation models, and it contains many novel relaxations. We design structured relaxations for subset selection, spanning trees, arborescences, and others. When compared to less structured baselines, we find that stochastic softmax tricks can be used to train latent variable models that perform better and discover more latent structure.},
	urldate = {2020-08-26},
	journal = {arXiv:2006.08063 [cs, stat]},
	author = {Paulus, Max B. and Choi, Dami and Tarlow, Daniel and Krause, Andreas and Maddison, Chris J.},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.08063},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mathieu_riemannian_2020,
	title = {Riemannian {Continuous} {Normalizing} {Flows}},
	url = {http://arxiv.org/abs/2006.10605},
	abstract = {Normalizing flows have shown great promise for modelling flexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, torii, and hyperbolic spaces, most normalizing flows implicitly assume a flat geometry, making them either misspecified or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing flows, a model which admits the parametrization of flexible probability measures on smooth manifolds by defining flows as the solution to ordinary differential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard flows or previously introduced projected flows.},
	urldate = {2020-08-26},
	journal = {arXiv:2006.10605 [cs, stat]},
	author = {Mathieu, Emile and Nickel, Maximilian},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.10605},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{noauthor_dib_nodate,
	title = {{DIB} {Rebuttal}},
	url = {https://www.overleaf.com/project/5f31eb03fa693b000172757f},
	abstract = {An online LaTeX editor that's easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2020-08-13},
}

@article{mendelson_geometric_nodate,
	title = {A {Geometric} {Approach} to {Statistical} {Learning} {Theory}},
	language = {en},
	author = {Mendelson, Shahar},
	pages = {32},
}

@book{watanabe_algebraic_2009,
	address = {Cambridge},
	title = {Algebraic {Geometry} and {Statistical} {Learning} {Theory}},
	isbn = {978-0-511-80047-4},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511800474},
	abstract = {This article introduces the book, “algebraic geometry and statistical learning theory. ” A parametric model in statistics or a learning machine in information science is called singular if it is not identiﬁable or if its Fisher information matrix is not positive deﬁnite. Although a lot of statistical models and learning machines are singular, their statistical properties have been left unknown. In this book, an algebraic geometrical method is established on which we can construct new statistical theory for singular models.},
	language = {en},
	urldate = {2020-07-31},
	publisher = {Cambridge University Press},
	author = {Watanabe, Sumio},
	year = {2009},
	doi = {10.1017/CBO9780511800474},
}

@article{smith_bayesian_2018,
	title = {A {BAYESIAN} {PERSPECTIVE} {ON} {GENERALIZATION} {AND} {STOCHASTIC} {GRADIENT} {DESCENT}},
	language = {en},
	author = {Smith, Samuel L and Le, Quoc V},
	year = {2018},
	pages = {13},
}

@incollection{germain_pac-bayesian_2016,
	title = {{PAC}-{Bayesian} {Theory} {Meets} {Bayesian} {Inference}},
	url = {http://papers.nips.cc/paper/6569-pac-bayesian-theory-meets-bayesian-inference.pdf},
	urldate = {2020-07-30},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Germain, Pascal and Bach, Francis and Lacoste, Alexandre and Lacoste-Julien, Simon},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	pages = {1884--1892},
}

@article{nielsen_chernoff_2011,
	title = {Chernoff information of exponential families},
	url = {http://arxiv.org/abs/1102.2684},
	abstract = {Chernoff information upper bounds the probability of error of the optimal Bayesian decision rule for \$2\$-class classification problems. However, it turns out that in practice the Chernoff bound is hard to calculate or even approximate. In statistics, many usual distributions, such as Gaussians, Poissons or frequency histograms called multinomials, can be handled in the unified framework of exponential families. In this note, we prove that the Chernoff information for members of the same exponential family can be either derived analytically in closed form, or efficiently approximated using a simple geodesic bisection optimization technique based on an exact geometric characterization of the "Chernoff point" on the underlying statistical manifold.},
	urldate = {2020-07-30},
	journal = {arXiv:1102.2684 [cs, math]},
	author = {Nielsen, Frank},
	month = feb,
	year = {2011},
	note = {arXiv: 1102.2684},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Retrieval, Computer Science - Information Theory},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {1435-568X},
	url = {https://doi.org/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	number = {4},
	journal = {Mathematics of Control, Signals and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	pages = {303--314},
}

@article{jin_domain_2020,
	title = {Domain {Extrapolation} via {Regret} {Minimization}},
	url = {http://arxiv.org/abs/2006.03908},
	abstract = {Many real prediction tasks such as molecular property prediction require ability to extrapolate to unseen domains. The success in these tasks typically hinges on finding a good representation. In this paper, we extend invariant risk minimization (IRM) by recasting the simultaneous optimality condition in terms of regret, finding instead a representation that enables the predictor to be optimal against an oracle with hindsight access on held-out environments. The change refocuses the principle on generalization and doesn't collapse even with strong predictors that can perfectly fit all the training data. Our regret minimization (RGM) approach can be further combined with adaptive domain perturbations to handle combinatorially defined environments. We evaluate our method on two real-world applications: molecule property prediction and protein homology detection and show that RGM significantly outperforms previous state-of-the-art domain generalization techniques.},
	urldate = {2020-07-15},
	journal = {arXiv:2006.03908 [cs, stat]},
	author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.03908},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, invariance, representation},
}

@article{massey_causality_nodate,
	title = {{CAUSALITY}, {FEEDBACK} {AND} {DIRECTED} {INFORMATION}},
	abstract = {It is shown that the "usual definition" of a discrete memoryless channel (DMC) in fact prohibits the use of feedback. The difficulty stems from the confusion of causality and statistical dependence. An adequate definition of a DMC is given, as well as a definition of using a channel without feedback. A definition, closely based on an old idea of Marko, is given for the directed information flowing from one sequence to another. This directed information is used to give a simple proof of the well-known fact that the use of feedback cannot increase the capacity of a DMC. It is shown that, when feedback is present, directed information is a more useful quantity than the traditional mutual information.},
	language = {en},
	author = {Massey, James L},
	keywords = {causal, information},
	pages = {6},
}

@article{bissiri_general_2016,
	title = {A general framework for updating belief distributions},
	volume = {78},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12158},
	doi = {10.1111/rssb.12158},
	abstract = {We propose a framework for general Bayesian inference. We argue that a valid update of a prior belief distribution to a posterior can be made for parameters which are connected to observations through a loss function rather than the traditional likelihood function, which is recovered as a special case. Modern application areas make it increasingly challenging for Bayesians to attempt to model the true data-generating mechanism. For instance, when the object of interest is low dimensional, such as a mean or median, it is cumbersome to have to achieve this via a complete model for the whole data distribution. More importantly, there are settings where the parameter of interest does not directly index a family of density functions and thus the Bayesian approach to learning about such parameters is currently regarded as problematic. Our framework uses loss functions to connect information in the data to functionals of interest. The updating of beliefs then follows from a decision theoretic approach involving cumulative loss functions. Importantly, the procedure coincides with Bayesian updating when a true likelihood is known yet provides coherent subjective inference in much more general settings. Connections to other inference frameworks are highlighted.},
	language = {en},
	number = {5},
	urldate = {2020-07-10},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Bissiri, P. G. and Holmes, C. C. and Walker, S. G.},
	year = {2016},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12158},
	keywords = {Decision theory, General Bayesian updating, Generalized estimating equations, Gibbs posteriors, Information, Loss function, Maximum entropy, Provably approximately correct Bayes methods, Self-information loss function, bayes, theory},
	pages = {1103--1130},
}

@article{shen_algorithmic_nodate,
	title = {Algorithmic {Information} {Theory} and {Kolmogorov} {Complexity}},
	abstract = {This document contains lecture notes of an introductory course on Kolmogorov complexity. They cover basic notions of algorithmic information theory: Kolmogorov complexity (plain, conditional, preﬁx), notion of randomness (Martin-Lo¨f randomness, Mises–Church randomness), Solomonoff universal a priori probability and their properties (symmetry of information, connection between a priori probability and preﬁx complexity, criterion of randomness in terms of complexity) and applications (incompressibility method in computational complexity theory, incompleteness theorems).},
	language = {en},
	author = {Shen, Alexander},
	keywords = {algorithmic complexity, complexity, information},
	pages = {31},
}

@incollection{abbass_universal_2018,
	address = {Cham},
	title = {Universal {Artificial} {Intelligence}: {Practical} {Agents} and {Fundamental} {Challenges}},
	volume = {117},
	isbn = {978-3-319-64815-6 978-3-319-64816-3},
	shorttitle = {Universal {Artificial} {Intelligence}},
	url = {http://link.springer.com/10.1007/978-3-319-64816-3_2},
	abstract = {Motivation The dream of creating artiﬁcial devices that reach or outperform human intelligence is an old one, however a computationally eﬃcient theory of true intelligence has not been found yet, despite considerable eﬀorts in the last 50 years. Nowadays most research is more modest, focussing on solving more narrow, speciﬁc problems, associated with only some aspects of intelligence, like playing chess or natural language translation, either as a goal in itself or as a bottom-up approach. The dual, top down approach, is to ﬁnd a mathematical (not computational) deﬁnition of general intelligence. Note that the AI problem remains non-trivial even when ignoring computational aspects.},
	language = {en},
	urldate = {2020-07-10},
	booktitle = {Foundations of {Trusted} {Autonomy}},
	publisher = {Springer International Publishing},
	author = {Everitt, Tom and Hutter, Marcus},
	editor = {Abbass, Hussein A. and Scholz, Jason and Reid, Darryn J.},
	year = {2018},
	doi = {10.1007/978-3-319-64816-3_2},
	note = {Series Title: Studies in Systems, Decision and Control},
	keywords = {algorithmic complexity, information},
	pages = {15--46},
}

@article{maddox_rethinking_2020,
	title = {Rethinking {Parameter} {Counting} in {Deep} {Models}: {Effective} {Dimensionality} {Revisited}},
	shorttitle = {Rethinking {Parameter} {Counting} in {Deep} {Models}},
	url = {http://arxiv.org/abs/2003.02139},
	abstract = {Neural networks appear to have mysterious generalization properties when using parameter counting as a proxy for complexity. Indeed, neural networks often have many more parameters than there are data points, yet still provide good generalization performance. Moreover, when we measure generalization as a function of parameters, we see double descent behaviour, where the test error decreases, increases, and then again decreases. We show that many of these properties become understandable when viewed through the lens of effective dimensionality, which measures the dimensionality of the parameter space determined by the data. We relate effective dimensionality to posterior contraction in Bayesian deep learning, model selection, width-depth tradeoffs, double descent, and functional diversity in loss surfaces, leading to a richer understanding of the interplay between parameters and functions in deep models. We also show that effective dimensionality compares favourably to alternative norm- and flatness- based generalization measures.},
	urldate = {2020-07-10},
	journal = {arXiv:2003.02139 [cs, stat]},
	author = {Maddox, Wesley J. and Benton, Gregory and Wilson, Andrew Gordon},
	month = may,
	year = {2020},
	note = {arXiv: 2003.02139},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, bayes, complexity, deep theory, generalization},
}

@article{choe_empirical_2020,
	title = {An {Empirical} {Study} of {Invariant} {Risk} {Minimization}},
	url = {http://arxiv.org/abs/2004.05007},
	abstract = {Invariant risk minimization (IRM) (Arjovsky et al., 2019) is a recently proposed framework designed for learning predictors that are invariant to spurious correlations across different training environments. Yet, despite its theoretical justifications, IRM has not been extensively tested across various settings. In an attempt to gain a better understanding of the framework, we empirically investigate several research questions using IRMv1, which is the first practical algorithm proposed to approximately solve IRM. By extending the ColoredMNIST experiment in different ways, we find that IRMv1 (i) performs better as the spurious correlation varies more widely between training environments, (ii) learns an approximately invariant predictor when the underlying relationship is approximately invariant, and (iii) can be extended to an analogous setting for text classification.},
	urldate = {2020-07-10},
	journal = {arXiv:2004.05007 [cs, stat]},
	author = {Choe, Yo Joong and Ham, Jiyeon and Park, Kyubyong},
	month = jul,
	year = {2020},
	note = {arXiv: 2004.05007},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, causal, invariance},
}

@article{suter_robustly_2019,
	title = {Robustly {Disentangled} {Causal} {Mechanisms}: {Validating} {Deep} {Representations} for {Interventional} {Robustness}},
	shorttitle = {Robustly {Disentangled} {Causal} {Mechanisms}},
	url = {http://arxiv.org/abs/1811.00007},
	abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
	urldate = {2020-07-10},
	journal = {arXiv:1811.00007 [cs, stat]},
	author = {Suter, Raphael and Miladinović, Đorđe and Schölkopf, Bernhard and Bauer, Stefan},
	month = may,
	year = {2019},
	note = {arXiv: 1811.00007},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, causal, representation},
}

@article{peters_causal_2015,
	title = {Causal inference using invariant prediction: identification and confidence intervals},
	shorttitle = {Causal inference using invariant prediction},
	url = {http://arxiv.org/abs/1501.01332},
	abstract = {What is the difference of a prediction that is made with a causal model and a non-causal model? Suppose we intervene on the predictor variables or change the whole environment. The predictions from a causal model will in general work as well under interventions as for observational data. In contrast, predictions from a non-causal model can potentially be very wrong if we actively intervene on variables. Here, we propose to exploit this invariance of a prediction under a causal model for causal inference: given different experimental settings (for example various interventions) we collect all models that do show invariance in their predictive accuracy across settings and interventions. The causal model will be a member of this set of models with high probability. This approach yields valid confidence intervals for the causal relationships in quite general scenarios. We examine the example of structural equation models in more detail and provide sufficient assumptions under which the set of causal predictors becomes identifiable. We further investigate robustness properties of our approach under model misspecification and discuss possible extensions. The empirical properties are studied for various data sets, including large-scale gene perturbation experiments.},
	urldate = {2020-07-10},
	journal = {arXiv:1501.01332 [stat]},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	month = nov,
	year = {2015},
	note = {arXiv: 1501.01332},
	keywords = {Statistics - Methodology, causal, invariance},
}

@article{mohamed_statistical_nodate,
	title = {A {Statistical} {View} of {Deep} {Learning}},
	language = {en},
	author = {Mohamed, Shakir},
	pages = {31},
}

@article{ahuja_invariant_2020,
	title = {Invariant {Risk} {Minimization} {Games}},
	url = {http://arxiv.org/abs/2002.04692},
	abstract = {The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of Arjovsky et al. (2019). One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in Arjovsky et al. (2019). The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks.},
	urldate = {2020-07-10},
	journal = {arXiv:2002.04692 [cs, stat]},
	author = {Ahuja, Kartik and Shanmugam, Karthikeyan and Varshney, Kush R. and Dhurandhar, Amit},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.04692},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, causal, game, invariance},
}

@article{hlavackova-schindler_causality_2007,
	title = {Causality detection based on information-theoretic approaches in time series analysis},
	volume = {441},
	issn = {0370-1573},
	url = {http://www.sciencedirect.com/science/article/pii/S0370157307000403},
	doi = {10.1016/j.physrep.2006.12.004},
	abstract = {Synchronization, a basic nonlinear phenomenon, is widely observed in diverse complex systems studied in physical, biological and other natural sciences, as well as in social sciences, economy and finance. While studying such complex systems, it is important not only to detect synchronized states, but also to identify causal relationships (i.e. who drives whom) between concerned (sub) systems. The knowledge of information-theoretic measures (i.e. mutual information, conditional entropy) is essential for the analysis of information flow between two systems or between constituent subsystems of a complex system. However, the estimation of these measures from a set of finite samples is not trivial. The current extensive literatures on entropy and mutual information estimation provides a wide variety of approaches, from approximation-statistical, studying rate of convergence or consistency of an estimator for a general distribution, over learning algorithms operating on partitioned data space to heuristical approaches. The aim of this paper is to provide a detailed overview of information theoretic approaches for measuring causal influence in multivariate time series and to focus on diverse approaches to the entropy and mutual information estimation.},
	language = {en},
	number = {1},
	urldate = {2020-07-10},
	journal = {Physics Reports},
	author = {Hlaváčková-Schindler, Katerina and Paluš, Milan and Vejmelka, Martin and Bhattacharya, Joydeep},
	month = mar,
	year = {2007},
	keywords = {Causality, Entropy, Estimation, Mutual information, causal, information},
	pages = {1--46},
}

@article{arjovsky_invariant_2020,
	title = {Invariant {Risk} {Minimization}},
	url = {http://arxiv.org/abs/1907.02893},
	abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
	urldate = {2020-07-08},
	journal = {arXiv:1907.02893 [cs, stat]},
	author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
	month = mar,
	year = {2020},
	note = {arXiv: 1907.02893},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning, causal, invariance},
}

@article{wang_blessings_2019,
	title = {The {Blessings} of {Multiple} {Causes}},
	volume = {114},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2019.1686987},
	doi = {10.1080/01621459.2019.1686987},
	abstract = {Causal inference from observational data is a vital problem, but it comes with strong assumptions. Most methods assume that we observe all confounders, variables that affect both the causal variables and the outcome variables. This assumption is standard but it is also untestable. In this article, we develop the deconfounder, a way to do causal inference with weaker assumptions than the traditional methods require. The deconfounder is designed for problems of multiple causal inference: scientific studies that involve multiple causes whose effects are simultaneously of interest. Specifically, the deconfounder combines unsupervised machine learning and predictive model checking to use the dependencies among multiple causes as indirect evidence for some of the unobserved confounders. We develop the deconfounder algorithm, prove that it is unbiased, and show that it requires weaker assumptions than traditional causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder is an effective approach to estimating causal effects in problems of multiple causal inference. Supplementary materials for this article are available online.},
	number = {528},
	urldate = {2020-07-08},
	journal = {Journal of the American Statistical Association},
	author = {Wang, Yixin and Blei, David M.},
	month = oct,
	year = {2019},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/01621459.2019.1686987},
	keywords = {Causal inference, Machine learning, Probabilistic models, Unconfoundedness, Unobserved confounding, causal},
	pages = {1574--1596},
}

@article{janzing_quantifying_2013,
	title = {Quantifying causal influences},
	volume = {41},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/1203.6502},
	doi = {10.1214/13-AOS1145},
	abstract = {Many methods for causal inference generate directed acyclic graphs (DAGs) that formalize causal relations between \$n\$ variables. Given the joint distribution on all these variables, the DAG contains all information about how intervening on one variable changes the distribution of the other \$n-1\$ variables. However, quantifying the causal influence of one variable on another one remains a nontrivial question. Here we propose a set of natural, intuitive postulates that a measure of causal strength should satisfy. We then introduce a communication scenario, where edges in a DAG play the role of channels that can be locally corrupted by interventions. Causal strength is then the relative entropy distance between the old and the new distribution. Many other measures of causal strength have been proposed, including average causal effect, transfer entropy, directed information, and information flow. We explain how they fail to satisfy the postulates on simple DAGs of \${\textbackslash}leq3\$ nodes. Finally, we investigate the behavior of our measure on time-series, supporting our claims with experiments on simulated data.},
	number = {5},
	urldate = {2020-07-10},
	journal = {The Annals of Statistics},
	author = {Janzing, Dominik and Balduzzi, David and Grosse-Wentrup, Moritz and Schölkopf, Bernhard},
	month = oct,
	year = {2013},
	note = {arXiv: 1203.6502},
	keywords = {Mathematics - Statistics Theory, causal, information},
	pages = {2324--2358},
}

@incollection{mackay_bayesian_1992,
	title = {Bayesian {Model} {Comparison} and {Backprop} {Nets}},
	url = {http://papers.nips.cc/paper/488-bayesian-model-comparison-and-backprop-nets.pdf},
	urldate = {2020-07-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 4},
	publisher = {Morgan-Kaufmann},
	author = {MacKay, David J. C.},
	editor = {Moody, J. E. and Hanson, S. J. and Lippmann, R. P.},
	year = {1992},
	keywords = {bayes, complexity, generalization},
	pages = {839--846},
}

@inproceedings{zhang_aet_2019,
	title = {{AET} vs. {AED}: {Unsupervised} {Representation} {Learning} by {Auto}-{Encoding} {Transformations} {Rather} {Than} {Data}},
	url = {http://openaccess.thecvf.com/content_CVPR_2019/html/Zhang_AET_vs._AED_Unsupervised_Representation_Learning_by_Auto-Encoding_Transformations_Rather_CVPR_2019_paper.html},
	doi = {10.1109/CVPR.2019.00265},
	booktitle = {{IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}, {CVPR} 2019, {Long} {Beach}, {CA}, {USA}, {June} 16-20, 2019},
	publisher = {Computer Vision Foundation / IEEE},
	author = {Zhang, Liheng and Qi, Guo-Jun and Wang, Liqiang and Luo, Jiebo},
	year = {2019},
	keywords = {cv, empirical, selfsup},
}

@inproceedings{blondel_learning_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Learning {Classifiers} with {Fenchel}-{Young} {Losses}: {Generalized} {Entropies}, {Margins}, and {Algorithms}},
	volume = {89},
	shorttitle = {Learning {Classifiers} with {Fenchel}-{Young} {Losses}},
	url = {http://proceedings.mlr.press/v89/blondel19a.html},
	booktitle = {The 22nd {International} {Conference} on {Artificial} {Intelligence} and {Statistics}, {AISTATS} 2019, 16-18 {April} 2019, {Naha}, {Okinawa}, {Japan}},
	publisher = {PMLR},
	author = {Blondel, Mathieu and Martins, André F. T. and Niculae, Vlad},
	editor = {Chaudhuri, Kamalika and Sugiyama, Masashi},
	year = {2019},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {generalized information, losses},
	pages = {606--615},
}

@article{achille_emergence_2018,
	title = {Emergence of {Invariance} and {Disentanglement} in {Deep} {Representations}},
	volume = {19},
	url = {http://jmlr.org/papers/v19/17-646.html},
	abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
	journal = {J. Mach. Learn. Res.},
	author = {Achille, Alessandro and Soatto, Stefano},
	year = {2018},
	keywords = {generalization, ib, representation},
	pages = {50:1--50:34},
}

@article{merhav_universal_1998,
	title = {Universal prediction},
	volume = {44},
	issn = {1557-9654},
	doi = {10.1109/18.720534},
	abstract = {This paper consists of an overview on universal prediction from an information-theoretic perspective. Special attention is given to the notion of probability assignment under the self-information loss function, which is directly related to the theory of universal data compression. Both the probabilistic setting and the deterministic setting of the universal prediction problem are described with emphasis on the analogy and the differences between results in the two settings.},
	number = {6},
	journal = {IEEE Transactions on Information Theory},
	author = {Merhav, N. and Feder, M.},
	month = oct,
	year = {1998},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Control theory, Data compression, Entropy, Information theory, Machine learning, Natural languages, Operations research, Predictive models, Statistics, Stochastic processes, deterministic setting, information, information theory, overview, prediction theory, probabilistic setting, probability, probability assignment, reviews, self-information loss function, theory, universal data compression, universal prediction},
	pages = {2124--2147},
}

@book{shalev-shwartz_understanding_2014,
	address = {Cambridge},
	title = {Understanding {Machine} {Learning}: {From} {Theory} to {Algorithms}},
	isbn = {978-1-107-29801-9},
	shorttitle = {Understanding {Machine} {Learning}},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781107298019},
	language = {en},
	urldate = {2020-03-24},
	publisher = {Cambridge University Press},
	author = {Shalev-Shwartz, Shai and Ben-David, Shai},
	year = {2014},
	doi = {10.1017/CBO9781107298019},
	keywords = {theory},
}

@article{degroot_uncertainty_1962,
	title = {Uncertainty, {Information}, and {Sequential} {Experiments}},
	volume = {33},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177704567},
	doi = {10.1214/aoms/1177704567},
	abstract = {Consider a situation in which it is desired to gain knowledge about the true value of some parameter (or about the true state of the world) by means of experimentation. Let ΩΩ{\textbackslash}Omega denote the set of all possible values of the parameter θθ{\textbackslash}theta, and suppose that the experimenter's knowledge about the true value of θθ{\textbackslash}theta can be expressed, at each stage of experimentation, in terms of a probability distribution ξξ{\textbackslash}xi over ΩΩ{\textbackslash}Omega. Each distribution ξξ{\textbackslash}xi indicates a certain amount of uncertainty on the part of the experimenter about the true value of θθ{\textbackslash}theta, and it is assumed that for each ξξ{\textbackslash}xi this uncertainty can be characterized by a non-negative number. The information in an experiment is then defined as the expected difference between the uncertainty of the prior distribution over ΩΩ{\textbackslash}Omega and the uncertainty of the posterior distribution. In any particular situation, the selection of an appropriate uncertainty function would typically be based on the use to which the experimenter's knowledge about θθ{\textbackslash}theta is to be put. If, for example, the actions available to the experimenter and the losses associated with these actions can be specified as in a statistical decision problem, then presumably the uncertainty function would be determined from the loss function. In Section 2 some properties of uncertainty and information functions, and their relation to statistical decision problems and loss functions, are considered. In Section 3 the sequential sampling rule whereby experiments are performed until the uncertainty is reduced to a preassigned level is studied for various uncertainty functions and experiments. This rule has been previously studied by Lindley, [8], [9], in special cases where the uncertainty function is the Shannon entropy function. In Sections 4 and 5 the problem of optimally choosing the experiments to be performed sequentially from a class of available experiments is considered when the goal is either to minimize the expected uncertainty after a fixed number of experiments or to minimize the expected number of experiments needed to reduce the uncertainty to a fixed level. Particular problems of this nature have been treated by Bradt and Karlin [6]. The recent work of Chernoff [7] and Albert [1] on the sequential design of experiments is also of interest in relation to these problems.},
	language = {EN},
	number = {2},
	urldate = {2020-06-27},
	journal = {Annals of Mathematical Statistics},
	author = {DeGroot, M. H.},
	month = jun,
	year = {1962},
	mrnumber = {MR139242},
	zmnumber = {0151.22803},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {information, theory},
	pages = {404--419},
}

@inproceedings{globerson_minimum_2004,
	address = {Banff, Canada},
	series = {{UAI} '04},
	title = {The minimum information principle for discriminative learning},
	isbn = {978-0-9749039-0-3},
	abstract = {Exponential models of distributions are widely used in machine learning for classification and modelling. It is well known that they can be interpreted as maximum entropy models under empirical expectation constraints. In this work, we argue that for classification tasks, mutual information is a more suitable information theoretic measure to be optimized. We show how the principle of minimum mutual information generalizes that of maximum entropy, and provides a comprehensive framework for building discriminative classifiers. A game theoretic interpretation of our approach is then given, and several generalization bounds provided. We present iterative algorithms for solving the minimum information problem and its convex dual, and demonstrate their performance on various classification tasks. The results show that minimum information classifiers outperform the corresponding maximum entropy models.},
	urldate = {2020-07-05},
	booktitle = {Proceedings of the 20th conference on {Uncertainty} in artificial intelligence},
	publisher = {AUAI Press},
	author = {Globerson, Amir and Tishby, Naftali},
	month = jul,
	year = {2004},
	keywords = {information, robust},
	pages = {193--200},
}

@phdthesis{rodriguez_galvez_information_2019,
	title = {The {Information} {Bottleneck} : {Connections} to {Other} {Problems}, {Learning} and {Exploration} of the {IB} {Curve}},
	shorttitle = {The {Information} {Bottleneck}},
	url = {http://urn.kb.se/resolve?urn=urn:nbn:se:kth:diva-254421},
	abstract = {In this thesis we study the information bottleneck (IB) method. This is an informationtheoretic framework which addresses the question of what are the relevant factors of arandom variable X to expl ...},
	language = {eng},
	urldate = {2020-06-24},
	author = {Rodriguez Galvez, Borja},
	year = {2019},
	keywords = {ib},
}

@incollection{li_resource-bounded_2019,
	address = {Cham},
	title = {Resource-{Bounded} {Complexity}},
	isbn = {978-3-030-11297-4 978-3-030-11298-1},
	url = {http://link.springer.com/10.1007/978-3-030-11298-1_7},
	language = {en},
	urldate = {2019-12-19},
	booktitle = {An {Introduction} to {Kolmogorov} {Complexity} and {Its} {Applications}},
	publisher = {Springer International Publishing},
	author = {Li, Ming and Vitányi, Paul},
	collaborator = {Li, Ming and Vitányi, Paul},
	year = {2019},
	doi = {10.1007/978-3-030-11298-1_7},
	keywords = {algorithmic complexity, theory},
	pages = {547--622},
}

@article{dawid_geometry_2007,
	title = {The geometry of proper scoring rules},
	volume = {59},
	issn = {0020-3157, 1572-9052},
	url = {http://link.springer.com/10.1007/s10463-006-0099-8},
	doi = {10.1007/s10463-006-0099-8},
	abstract = {A decision problem is deﬁned in terms of an outcome space, an action space and a loss function. Starting from these simple ingredients, we can construct: Proper Scoring Rule; Entropy Function; Divergence Function; Riemannian Metric; and Unbiased Estimating Equation. From an abstract viewpoint, the loss function deﬁnes a duality between the outcome and action spaces, while the correspondence between a distribution and its Bayes act induces a self-duality. Together these determine a “decision geometry” for the family of distributions on outcome space. This allows generalisation of many standard statistical concepts and properties. In particular we deﬁne and study generalised exponential families. Several examples are analysed, including a general Bregman geometry.},
	language = {en},
	number = {1},
	urldate = {2019-12-20},
	journal = {Annals of the Institute of Statistical Mathematics},
	author = {Dawid, A. P.},
	month = feb,
	year = {2007},
	keywords = {information, theory},
	pages = {77--93},
}

@article{piran_dual_2020,
	title = {The {Dual} {Information} {Bottleneck}},
	url = {http://arxiv.org/abs/2006.04641},
	abstract = {The Information Bottleneck (IB) framework is a general characterization of optimal representations obtained using a principled approach for balancing accuracy and complexity. Here we present a new framework, the Dual Information Bottleneck (dualIB), which resolves some of the known drawbacks of the IB. We provide a theoretical analysis of the dualIB framework; (i) solving for the structure of its solutions (ii) unraveling its superiority in optimizing the mean prediction error exponent and (iii) demonstrating its ability to preserve exponential forms of the original distribution. To approach large scale problems, we present a novel variational formulation of the dualIB for Deep Neural Networks. In experiments on several data-sets, we compare it to a variational form of the IB. This exposes superior Information Plane properties of the dualIB and its potential in improvement of the error.},
	urldate = {2020-06-24},
	journal = {arXiv:2006.04641 [cs, math]},
	author = {Piran, Zoe and Shwartz-Ziv, Ravid and Tishby, Naftali},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.04641},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, ib, information},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	language = {en},
	number = {477},
	urldate = {2019-12-19},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	keywords = {generalized information, information},
	pages = {359--378},
}

@phdthesis{dziugaite_revisiting_nodate,
	title = {Revisiting {Generalization} for {Deep} {Learning}: {PAC}-{Bayes}, {Flat} {Minima}, and {Generative} {Models}},
	language = {en},
	author = {Dziugaite, Gintare Karolina},
	keywords = {deep theory, generalization},
}

@book{parmigiani_decision_2009,
	title = {Decision theory: {Principles} and approaches},
	volume = {812},
	language = {en},
	publisher = {John Wiley \& Sons},
	author = {Parmigiani, Giovanni and Inoue, Lurdes},
	year = {2009},
	keywords = {decision},
}

@article{bialek_predictability_2001,
	title = {Predictability, {Complexity}, and {Learning}},
	volume = {13},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976601753195969},
	doi = {10.1162/089976601753195969},
	language = {en},
	number = {11},
	urldate = {2019-12-07},
	journal = {Neural Computation},
	author = {Bialek, William and Nemenman, Ilya and Tishby, Naftali},
	month = nov,
	year = {2001},
	keywords = {information, time series},
	pages = {2409--2463},
}

@article{nguyen_optimal_2010,
	title = {Optimal feature selection for support vector machines},
	volume = {43},
	issn = {00313203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0031320309003409},
	doi = {10.1016/j.patcog.2009.09.003},
	abstract = {Selecting relevant features for support vector machine (SVM) classiﬁers is important for a variety of reasons such as generalization performance, computational efﬁciency, and feature interpretability. Traditional SVM approaches to feature selection typically extract features and learn SVM parameters independently. Independently performing these two steps might result in a loss of information related to the classiﬁcation process. This paper proposes a convex energy-based framework to jointly perform feature selection and SVM parameter learning for linear and non-linear kernels. Experiments on various databases show signiﬁcant reduction of features used while maintaining classiﬁcation performance.},
	language = {en},
	number = {3},
	urldate = {2020-06-29},
	journal = {Pattern Recognition},
	author = {Nguyen, Minh Hoai and de la Torre, Fernando},
	month = mar,
	year = {2010},
	keywords = {kernel, representation},
	pages = {584--591},
}

@article{saxe_information_2019,
	title = {On the information bottleneck theory of deep learning},
	volume = {2019},
	issn = {1742-5468},
	url = {https://iopscience.iop.org/article/10.1088/1742-5468/ab3985},
	doi = {10.1088/1742-5468/ab3985},
	abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three speciﬁc claims: ﬁrst, that deep networks undergo two distinct phases consisting of an initial ﬁtting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we ﬁnd that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB ﬁndings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the ﬁtting process rather than during a subsequent compression period.},
	language = {en},
	number = {12},
	urldate = {2020-06-24},
	journal = {Journal of Statistical Mechanics: Theory and Experiment},
	author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
	month = dec,
	year = {2019},
	keywords = {deep theory, ib, information},
	pages = {124020},
}

@article{jiao_justification_2015,
	title = {Justification of {Logarithmic} {Loss} via the {Benefit} of {Side} {Information}},
	volume = {61},
	issn = {0018-9448, 1557-9654},
	url = {http://arxiv.org/abs/1403.4679},
	doi = {10.1109/TIT.2015.2462848},
	abstract = {We consider a natural measure of relevance: the reduction in optimal prediction risk in the presence of side information. For any given loss function, this relevance measure captures the benefit of side information for performing inference on a random variable under this loss function. When such a measure satisfies a natural data processing property, and the random variable of interest has alphabet size greater than two, we show that it is uniquely characterized by the mutual information, and the corresponding loss function coincides with logarithmic loss. In doing so, our work provides a new characterization of mutual information, and justifies its use as a measure of relevance. When the alphabet is binary, we characterize the only admissible forms the measure of relevance can assume while obeying the specified data processing property. Our results naturally extend to measuring causal influence between stochastic processes, where we unify different causal-inference measures in the literature as instantiations of directed information.},
	number = {10},
	urldate = {2019-12-19},
	journal = {IEEE Transactions on Information Theory},
	author = {Jiao, Jiantao and Courtade, Thomas and Venkat, Kartik and Weissman, Tsachy},
	month = oct,
	year = {2015},
	note = {arXiv: 1403.4679},
	keywords = {Computer Science - Information Theory, information, losses, theory},
	pages = {5357--5365},
}

@article{anselmi_invariance_2015,
	title = {On {Invariance} and {Selectivity} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/1503.05938},
	abstract = {We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.},
	urldate = {2020-06-29},
	journal = {arXiv:1503.05938 [cs]},
	author = {Anselmi, Fabio and Rosasco, Lorenzo and Poggio, Tomaso},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.05938},
	keywords = {Computer Science - Machine Learning, representation, theory},
}

@article{kolchinsky_nonlinear_2019,
	title = {Nonlinear {Information} {Bottleneck}},
	volume = {21},
	url = {https://doi.org/10.3390/e21121181},
	doi = {10.3390/e21121181},
	abstract = {Information bottleneck (IB) is a technique for extracting information in one random variable \$X\$ that is relevant for predicting another random variable \$Y\$. IB works by encoding \$X\$ in a compressed "bottleneck" random variable \$M\$ from which \$Y\$ can be accurately decoded. However, finding the optimal bottleneck variable involves a difficult optimization problem, which until recently has been considered for only two limited cases: discrete \$X\$ and \$Y\$ with small state spaces, and continuous \$X\$ and \$Y\$ with a Gaussian joint distribution (in which case optimal encoding and decoding maps are linear). We propose a method for performing IB on arbitrarily-distributed discrete and/or continuous \$X\$ and \$Y\$, while allowing for nonlinear encoding and decoding maps. Our approach relies on a novel non-parametric upper bound for mutual information. We describe how to implement our method using neural networks. We then show that it achieves better performance than the recently-proposed "variational IB" method on several real-world datasets.},
	number = {12},
	journal = {Entropy},
	author = {Kolchinsky, Artemy and Tracey, Brendan D. and Wolpert, David H.},
	year = {2019},
	keywords = {ib, information},
	pages = {1181},
}

@article{shamir_learning_2010,
	title = {Learning and generalization with the information bottleneck},
	volume = {411},
	url = {https://doi.org/10.1016/j.tcs.2010.04.006},
	doi = {10.1016/j.tcs.2010.04.006},
	abstract = {The information bottleneck is an information theoretic framework, extending the classical notion of minimal sufﬁcient statistics, that ﬁnds concise representations for an ‘input’ random variable that are as relevant as possible for an ‘output’ variable. This framework has been used successfully in various supervised and unsupervised applications. However, its learning theoretic properties and justiﬁcation remained unclear as it differs from standard learning models in several crucial aspects, primarily its explicit reliance on the joint input-output distribution. In practice, an empirical plug-in estimate of the underlying distribution has been used, so far without any ﬁnite sample performance guarantees. In this paper we present several formal results that address these difﬁculties. We prove several non-uniform ﬁnite sample bounds that show that it can provide concise representations with good generalization based on smaller sample sizes than needed to estimate the underlying distribution. Based on these results, we can analyze the information bottleneck method as a learning algorithm in the familiar performance-complexity tradeoff framework. In addition, we formally describe the connection between the information bottleneck and minimal sufﬁcient statistics.},
	number = {29-30},
	journal = {Theor. Comput. Sci.},
	author = {Shamir, Ohad and Sabato, Sivan and Tishby, Naftali},
	year = {2010},
	keywords = {generalization, ib, information},
	pages = {2696--2711},
}

@inproceedings{bousquet_introduction_2003,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Introduction to {Statistical} {Learning} {Theory}},
	volume = {3176},
	url = {https://doi.org/10.1007/978-3-540-28650-9_8},
	doi = {10.1007/978-3-540-28650-9_8},
	booktitle = {Advanced {Lectures} on {Machine} {Learning}, {ML} {Summer} {Schools} 2003, {Canberra}, {Australia}, {February} 2-14, 2003, {Tübingen}, {Germany}, {August} 4-16, 2003, {Revised} {Lectures}},
	publisher = {Springer},
	author = {Bousquet, Olivier and Boucheron, Stéphane and Lugosi, Gábor},
	editor = {Bousquet, Olivier and Luxburg, Ulrike von and Rätsch, Gunnar},
	year = {2003},
	keywords = {generalization, statistical learning, theory},
	pages = {169--207},
}

@book{mackay_information_2003,
	title = {Information theory, inference, and learning algorithms},
	isbn = {978-0-521-64298-9},
	publisher = {Cambridge University Press},
	author = {MacKay, David J. C.},
	year = {2003},
}

@article{achille_information_2018,
	title = {Information {Dropout}: {Learning} {Optimal} {Representations} {Through} {Noisy} {Computation}},
	volume = {40},
	url = {https://doi.org/10.1109/TPAMI.2017.2784440},
	doi = {10.1109/TPAMI.2017.2784440},
	number = {12},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Achille, Alessandro and Soatto, Stefano},
	year = {2018},
	keywords = {ib, representation},
	pages = {2897--2905},
}

@article{hafez-kolahi_compressed_2020,
	title = {Do {Compressed} {Representations} {Generalize} {Better}?},
	url = {http://arxiv.org/abs/1909.09706},
	abstract = {One of the most studied problems in machine learning is finding reasonable constraints that guarantee the generalization of a learning algorithm. These constraints are usually expressed as some simplicity assumptions on the target. For instance, in the Vapnik-Chervonenkis (VC) theory the space of possible hypotheses is considered to have a limited VC dimension. In this paper, the constraint on the entropy \$H(X)\$ of the input variable \$X\$ is studied as a simplicity assumption. It is proven that the sample complexity to achieve an \${\textbackslash}epsilon\$-\${\textbackslash}delta\$ Probably Approximately Correct (PAC) hypothesis is bounded by \${\textbackslash}frac\{2{\textasciicircum}\{ {\textbackslash}left.6H(X){\textbackslash}middle/{\textbackslash}epsilon{\textbackslash}right.\}+{\textbackslash}log\{{\textbackslash}frac\{1\}\{{\textbackslash}delta\}\}\}\{{\textbackslash}epsilon{\textasciicircum}2\}\$ which is sharp up to the \${\textbackslash}frac\{1\}\{{\textbackslash}epsilon{\textasciicircum}2\}\$ factor. Morever, it is shown that if a feature learning process is employed to learn the compressed representation from the dataset, this bound no longer exists. These findings have important implications on the Information Bottleneck (IB) theory which had been utilized to explain the generalization power of Deep Neural Networks (DNNs), but its applicability for this purpose is currently under debate by researchers. In particular, this is a rigorous proof for the previous heuristic that compressed representations are exponentially easier to be learned. However, our analysis pinpoints two factors preventing the IB, in its current form, to be applicable in studying neural networks. Firstly, the exponential dependence of sample complexity on \${\textbackslash}frac\{1\}\{{\textbackslash}epsilon\}\$, which can lead to a dramatic effect on the bounds in practical applications when \${\textbackslash}epsilon\$ is small. Secondly, our analysis reveals that arguments based on input compression are inherently insufficient to explain generalization of methods like DNNs in which the features are also learned using available data.},
	urldate = {2020-06-24},
	journal = {arXiv:1909.09706 [cs, math, stat]},
	author = {Hafez-Kolahi, Hassan and Kasaei, Shohreh and Soleymani-Baghshah, Mahdiyeh},
	month = jan,
	year = {2020},
	note = {arXiv: 1909.09706},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning, generalization, ib, information, representation},
}

@article{hafez-kolahi_information_2019,
	title = {Information {Bottleneck} and its {Applications} in {Deep} {Learning}},
	url = {http://arxiv.org/abs/1904.03743},
	abstract = {Information Theory (IT) has been used in Machine Learning (ML) from early days of this field. In the last decade, advances in Deep Neural Networks (DNNs) have led to surprising improvements in many applications of ML. The result has been a paradigm shift in the community toward revisiting previous ideas and applications in this new framework. Ideas from IT are no exception. One of the ideas which is being revisited by many researchers in this new era, is Information Bottleneck (IB); a formulation of information extraction based on IT. The IB is promising in both analyzing and improving DNNs. The goal of this survey is to review the IB concept and demonstrate its applications in deep learning. The information theoretic nature of IB, makes it also a good candidate in showing the more general concept of how IT can be used in ML. Two important concepts are highlighted in this narrative on the subject, i) the concise and universal view that IT provides on seemingly unrelated methods of ML, demonstrated by explaining how IB relates to minimal sufficient statistics, stochastic gradient descent, and variational auto-encoders, and ii) the common technical mistakes and problems caused by applying ideas from IT, which is discussed by a careful study of some recent methods suffering from them.},
	urldate = {2020-06-24},
	journal = {arXiv:1904.03743 [cs, math, stat]},
	author = {Hafez-Kolahi, Hassan and Kasaei, Shohreh},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.03743},
	keywords = {Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning, ib, information},
}

@article{grunwald_game_2004,
	title = {Game theory, maximum entropy, minimum discrepancy and robust {Bayesian} decision theory},
	volume = {32},
	issn = {0090-5364},
	url = {http://arxiv.org/abs/math/0410076},
	doi = {10.1214/009053604000000553},
	abstract = {We describe and develop a close relationship between two problems that have customarily been regarded as distinct: that of maximizing entropy, and that of minimizing worst-case expected loss. Using a formulation grounded in the equilibrium theory of zero-sum games between Decision Maker and Nature, these two problems are shown to be dual to each other, the solution to each providing that to the other. Although Tops{\textbackslash}oe described this connection for the Shannon entropy over 20 years ago, it does not appear to be widely known even in that important special case. We here generalize this theory to apply to arbitrary decision problems and loss functions. We indicate how an appropriate generalized definition of entropy can be associated with such a problem, and we show that, subject to certain regularity conditions, the above-mentioned duality continues to apply in this extended context. This simultaneously provides a possible rationale for maximizing entropy and a tool for finding robust Bayes acts. We also describe the essential identity between the problem of maximizing entropy and that of minimizing a related discrepancy or divergence between distributions. This leads to an extension, to arbitrary discrepancies, of a well-known minimax theorem for the case of Kullback-Leibler divergence (the ``redundancy-capacity theorem'' of information theory). For the important case of families of distributions having certain mean values specified, we develop simple sufficient conditions and methods for identifying the desired solutions.},
	number = {4},
	urldate = {2019-12-19},
	journal = {The Annals of Statistics},
	author = {Grunwald, Peter D. and Dawid, A. Philip},
	month = aug,
	year = {2004},
	note = {arXiv: math/0410076},
	keywords = {62C20 (Primary) 94A17 (Secondary), Mathematics - Statistics Theory, information, robust, theory},
	pages = {1367--1433},
}

@inproceedings{jiang_fantastic_2020,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	url = {https://openreview.net/forum?id=SJgIPJBFvH},
	abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	year = {2020},
	keywords = {empirical, generalization},
}

@inproceedings{goldfeld_estimating_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Estimating {Information} {Flow} in {Deep} {Neural} {Networks}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/goldfeld19a.html},
	abstract = {We study the flow of information and the evolution of internal representations during deep neural network (DNN) training, aiming to demystify the compression aspect of the information bottleneck theory. The theory suggests that DNN training comprises a rapid fitting phase followed by a slower compression phase, in which the mutual information \$I(X;T)\$ between the input \$X\$ and internal representations \$T\$ decreases. Several papers observe compression of estimated mutual information on different DNN models, but the true \$I(X;T)\$ over these networks is provably either constant (discrete \$X\$) or infinite (continuous \$X\$). This work explains the discrepancy between theory and experiments, and clarifies what was actually measured by these past works. To this end, we introduce an auxiliary (noisy) DNN framework for which \$I(X;T)\$ is a meaningful quantity that depends on the network's parameters. This noisy framework is shown to be a good proxy for the original (deterministic) DNN both in terms of performance and the learned representations. We then develop a rigorous estimator for \$I(X;T)\$ in noisy DNNs and observe compression in various models. By relating \$I(X;T)\$ in the noisy DNN to an information-theoretic communication problem, we show that compression is driven by the progressive clustering of hidden representations of inputs from the same class. Several methods to directly monitor clustering of hidden representations, both in noisy and deterministic DNNs, are used to show that meaningful clusters form in the \$T\$ space. Finally, we return to the estimator of \$I(X;T)\$ employed in past works, and demonstrate that while it fails to capture the true (vacuous) mutual information, it does serve as a measure for clustering. This clarifies the past observations of compression and isolates the geometric clustering of hidden representations as the true phenomenon of interest.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Goldfeld, Ziv and Berg, Ewout van den and Greenewald, Kristjan H. and Melnyk, Igor and Nguyen, Nam and Kingsbury, Brian and Polyanskiy, Yury},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {ib},
	pages = {2299--2308},
}

@article{sagawa_distributionally_2020,
	title = {Distributionally {Robust} {Neural} {Networks} for {Group} {Shifts}: {On} the {Importance} of {Regularization} for {Worst}-{Case} {Generalization}},
	shorttitle = {Distributionally {Robust} {Neural} {Networks} for {Group} {Shifts}},
	url = {http://arxiv.org/abs/1911.08731},
	abstract = {Overparameterized neural networks can be highly accurate on average on an i.i.d. test set yet consistently fail on atypical groups of the data (e.g., by learning spurious correlations that hold on average but not in such groups). Distributionally robust optimization (DRO) allows us to learn models that instead minimize the worst-case training loss over a set of pre-deﬁned groups. However, we ﬁnd that naively applying group DRO to overparameterized neural networks fails: these models can perfectly ﬁt the training data, and any model with vanishing average training loss also already has vanishing worst-case training loss. Instead, the poor worst-case performance arises from poor generalization on some groups. By coupling group DRO models with increased regularization—a stronger-than-typical 2 penalty or early stopping—we achieve substantially higher worst-group accuracies, with 10–40 percentage point improvements on a natural language inference task and two image tasks, while maintaining high average accuracies. Our results suggest that regularization is important for worst-group generalization in the overparameterized regime, even if it is not needed for average generalization. Finally, we introduce a stochastic optimization algorithm, with convergence guarantees, to efﬁciently train group DRO models.},
	language = {en},
	urldate = {2020-04-21},
	journal = {arXiv:1911.08731 [cs, stat]},
	author = {Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
	month = apr,
	year = {2020},
	note = {arXiv: 1911.08731},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, robust},
}

@inproceedings{tishby_deep_2015,
	title = {Deep learning and the information bottleneck principle},
	url = {https://doi.org/10.1109/ITW.2015.7133169},
	doi = {10.1109/ITW.2015.7133169},
	booktitle = {2015 {IEEE} {Information} {Theory} {Workshop}, {ITW} 2015, {Jerusalem}, {Israel}, {April} 26 - {May} 1, 2015},
	publisher = {IEEE},
	author = {Tishby, Naftali and Zaslavsky, Noga},
	year = {2015},
	keywords = {deep theory, ib, information},
	pages = {1--5},
}

@inproceedings{gordon_convolutional_2020,
	title = {Convolutional {Conditional} {Neural} {Processes}},
	url = {https://openreview.net/forum?id=Skey4eBYPS},
	booktitle = {8th {International} {Conference} on {Learning} {Representations}, {ICLR} 2020, {Addis} {Ababa}, {Ethiopia}, {April} 26-30, 2020},
	publisher = {OpenReview.net},
	author = {Gordon, Jonathan and Bruinsma, Wessel P. and Foong, Andrew Y. K. and Requeima, James and Dubois, Yann and Turner, Richard E.},
	year = {2020},
	keywords = {NPF, metalearning, set},
}

@inproceedings{garnelo_conditional_2018,
	title = {Conditional {Neural} {Processes}},
	url = {http://proceedings.mlr.press/v80/garnelo18a.html},
	abstract = {Deep neural networks excel at function approximation, yet they are typically trained from scratch for each new function. On the other hand, Bayesian methods, such as Gaussian Processes (GPs), explo...},
	language = {en},
	urldate = {2020-04-25},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, S. M. Ali},
	month = jul,
	year = {2018},
	note = {ISSN: 1938-7228
Section: Machine Learning},
	keywords = {NPF, metalearning},
	pages = {1704--1713},
}

@inproceedings{hanin_complexity_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Complexity of {Linear} {Regions} in {Deep} {Networks}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/hanin19a.html},
	abstract = {It is well-known that the expressivity of a neural network depends on its architecture, with deeper networks expressing more complex functions. In the case of networks that compute piecewise linear functions, such as those with ReLU activation, the number of distinct linear regions is a natural measure of expressivity. It is possible to construct networks with merely a single region, or for which the number of linear regions grows exponentially with depth; it is not clear where within this range most networks fall in practice, either before or after training. In this paper, we provide a mathematical framework to count the number of linear regions of a piecewise linear network and measure the volume of the boundaries between these regions. In particular, we prove that for networks at initialization, the average number of regions along any one-dimensional subspace grows linearly in the total number of neurons, far below the exponential upper bound. We also find that the average distance to the nearest region boundary at initialization scales like the inverse of the number of neurons. Our theory suggests that, even after training, the number of linear regions is far below exponential, an intuition that matches our empirical observations. We conclude that the practical expressivity of neural networks is likely far below that of the theoretical maximum, and that this gap can be quantified.},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Hanin, Boris and Rolnick, David},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {complexity, deep theory},
	pages = {2596--2604},
}

@inproceedings{kolchinsky_caveats_2019,
	title = {Caveats for information bottleneck in deterministic scenarios},
	url = {https://openreview.net/forum?id=rke4HiAcY7},
	abstract = {Information bottleneck (IB) is a method for extracting information from one random variable \$X\$ that is relevant for predicting another random variable \$Y\$. To do so, IB identifies an intermediate "bottleneck" variable \$T\$ that has low mutual information \$I(X;T)\$ and high mutual information \$I(Y;T)\$. The "IB curve" characterizes the set of bottleneck variables that achieve maximal \$I(Y;T)\$ for a given \$I(X;T)\$, and is typically explored by maximizing the "IB Lagrangian", \$I(Y;T) - {\textbackslash}beta I(X;T)\$. In some cases, \$Y\$ is a deterministic function of \$X\$, including many classification problems in supervised learning where the output class \$Y\$ is a deterministic function of the input \$X\$. We demonstrate three caveats when using IB in any situation where \$Y\$ is a deterministic function of \$X\$: (1) the IB curve cannot be recovered by maximizing the IB Lagrangian for different values of \${\textbackslash}beta\$; (2) there are "uninteresting" trivial solutions at all points of the IB curve; and (3) for multi-layer classifiers that achieve low prediction error, different layers cannot exhibit a strict trade-off between compression and prediction, contrary to a recent proposal. We also show that when \$Y\$ is a small perturbation away from being a deterministic function of \$X\$, these three caveats arise in an approximate way. To address problem (1), we propose a functional that, unlike the IB Lagrangian, can recover the IB curve in all cases. We demonstrate the three caveats on the MNIST dataset.},
	booktitle = {7th {International} {Conference} on {Learning} {Representations}, {ICLR} 2019, {New} {Orleans}, {LA}, {USA}, {May} 6-9, 2019},
	publisher = {OpenReview.net},
	author = {Kolchinsky, Artemy and Tracey, Brendan D. and Kuyk, Steven Van},
	year = {2019},
	keywords = {ib, information},
}

@article{csiszar_axiomatic_2008,
	title = {Axiomatic {Characterizations} of {Information} {Measures}},
	volume = {10},
	issn = {1099-4300},
	url = {http://www.mdpi.com/1099-4300/10/3/261},
	doi = {10.3390/e10030261},
	abstract = {Axiomatic characterizations of Shannon entropy, Kullback I-divergence, and some generalized information measures are surveyed. Three directions are treated: (A) Characterization of functions of probability distributions suitable as information measures. (B) Characterization of set functions on the subsets of \{1, . . . , N \} representable by joint entropies of components of an N -dimensional random vector. (C) Axiomatic characterization of MaxEnt and related inference rules. The paper concludes with a brief discussion of the relevance of the axiomatic approach for information theory.},
	language = {en},
	number = {3},
	urldate = {2019-12-19},
	journal = {Entropy},
	author = {Csiszár, Imre},
	month = sep,
	year = {2008},
	keywords = {information, theory},
	pages = {261--273},
}

@article{vapnik_overview_1999,
	title = {An overview of statistical learning theory},
	volume = {10},
	issn = {10459227},
	url = {http://ieeexplore.ieee.org/document/788640/},
	doi = {10.1109/72.788640},
	abstract = {Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can ﬁnd detailed description of the theory (including proofs).},
	language = {en},
	number = {5},
	urldate = {2020-04-21},
	journal = {IEEE Transactions on Neural Networks},
	author = {Vapnik, V.N.},
	month = sep,
	year = {1999},
	keywords = {theory},
	pages = {988--999},
}

@inproceedings{saunshi_theoretical_2019,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {A {Theoretical} {Analysis} of {Contrastive} {Unsupervised} {Representation} {Learning}},
	volume = {97},
	url = {http://proceedings.mlr.press/v97/saunshi19a.html},
	booktitle = {Proceedings of the 36th {International} {Conference} on {Machine} {Learning}, {ICML} 2019, 9-15 {June} 2019, {Long} {Beach}, {California}, {USA}},
	publisher = {PMLR},
	author = {Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
	editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
	year = {2019},
	keywords = {contrastive, deep theory, selfsup, theory},
	pages = {5628--5637},
}

@article{grunwald_tutorial_2004,
	title = {A tutorial introduction to the minimum description length principle},
	url = {http://arxiv.org/abs/math/0406077},
	abstract = {This tutorial provides an overview of and introduction to Rissanen's Minimum Description Length (MDL) Principle. The first chapter provides a conceptual, entirely non-technical introduction to the subject. It serves as a basis for the technical introduction given in the second chapter, in which all the ideas of the first chapter are made mathematically precise. The main ideas are discussed in great conceptual and technical detail. This tutorial is an extended version of the first two chapters of the collection "Advances in Minimum Description Length: Theory and Application" (edited by P.Grunwald, I.J. Myung and M. Pitt, to be published by the MIT Press, Spring 2005).},
	urldate = {2020-01-08},
	journal = {arXiv:math/0406077},
	author = {Grunwald, Peter},
	month = jun,
	year = {2004},
	note = {arXiv: math/0406077},
	keywords = {6201, 6801, 68T05, 68T10, 9401, Computer Science - Information Theory, Computer Science - Machine Learning, Mathematics - Statistics Theory, information, theory},
}

@article{shannon_mathematical_1948,
	title = {A mathematical theory of communication},
	volume = {27},
	url = {https://doi.org/10.1002/j.1538-7305.1948.tb00917.x},
	doi = {10.1002/j.1538-7305.1948.tb00917.x},
	number = {4},
	journal = {Bell Syst. Tech. J.},
	author = {Shannon, Claude E.},
	year = {1948},
	keywords = {communication, information},
	pages = {623--656},
}

@article{amjad_learning_2019,
	title = {Learning {Representations} for {Neural} {Network}-{Based} {Classification} {Using} the {Information} {Bottleneck} {Principle}},
	issn = {0162-8828, 2160-9292, 1939-3539},
	url = {http://arxiv.org/abs/1802.09766},
	doi = {10.1109/TPAMI.2019.2909031},
	abstract = {In this theory paper, we investigate training deep neural networks (DNNs) for classification via minimizing the information bottleneck (IB) functional. We show that the resulting optimization problem suffers from two severe issues: First, for deterministic DNNs, either the IB functional is infinite for almost all values of network parameters, making the optimization problem ill-posed, or it is piecewise constant, hence not admitting gradient-based optimization methods. Second, the invariance of the IB functional under bijections prevents it from capturing properties of the learned representation that are desirable for classification, such as robustness and simplicity. We argue that these issues are partly resolved for stochastic DNNs, DNNs that include a (hard or soft) decision rule, or by replacing the IB functional with related, but more well-behaved cost functions. We conclude that recent successes reported about training DNNs using the IB framework must be attributed to such solutions. As a side effect, our results indicate limitations of the IB framework for the analysis of DNNs. We also note that rather than trying to repair the inherent problems in the IB functional, a better approach may be to design regularizers on latent representation enforcing the desired properties directly.},
	urldate = {2020-06-24},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Amjad, Rana Ali and Geiger, Bernhard C.},
	year = {2019},
	note = {arXiv: 1802.09766},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Information Theory, Computer Science - Machine Learning, information},
	pages = {1--1},
}
