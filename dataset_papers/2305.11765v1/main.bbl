\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{DKTZ20b}

\bibitem[ABL17]{awasthi2017power}
Pranjal Awasthi, Maria~Florina Balcan, and Philip~M Long.
\newblock The power of localization for efficiently learning linear separators
  with noise.
\newblock {\em Journal of the ACM (JACM)}, 63(6):1--27, 2017.

\bibitem[BH21]{balcan2021noise}
Maria-Florina Balcan and Nika Haghtalab.
\newblock Noise in classification.
\newblock {\em Beyond the Worst-Case Analysis of Algorithms}, page 361, 2021.

\bibitem[BK21]{bakshi2021list}
Ainesh Bakshi and Pravesh~K Kothari.
\newblock List-decodable subspace recovery: Dimension independent error in
  polynomial time.
\newblock In {\em Proceedings of the 2021 ACM-SIAM Symposium on Discrete
  Algorithms (SODA)}, pages 1279--1297. SIAM, 2021.

\bibitem[BZ17]{balcan2017sample}
Maria-Florina~F Balcan and Hongyang Zhang.
\newblock Sample and computationally efficient learning algorithms under
  s-concave distributions.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Dan15]{daniely2015ptas}
Amit Daniely.
\newblock A ptas for agnostically learning halfspaces.
\newblock In {\em Conference on Learning Theory}, pages 484--502. PMLR, 2015.

\bibitem[DKK{\etalchar{+}}22]{diakonikolas2022learning_general}
Ilias Diakonikolas, Daniel~M Kane, Vasilis Kontonis, Christos Tzamos, and Nikos
  Zarifis.
\newblock Learning general halfspaces with general massart noise under the
  gaussian distribution.
\newblock In {\em Proceedings of the 54th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 874--885, 2022.

\bibitem[DKK{\etalchar{+}}23]{diakonikolas2023efficient}
Ilias Diakonikolas, Daniel~M Kane, Vasilis Kontonis, Sihan Liu, and Nikos
  Zarifis.
\newblock Efficient testable learning of halfspaces with adversarial label
  noise.
\newblock {\em arXiv preprint arXiv:2303.05485}, 2023.

\bibitem[DKPZ21]{diakonikolas2021optimality}
Ilias Diakonikolas, Daniel~M Kane, Thanasis Pittas, and Nikos Zarifis.
\newblock The optimality of polynomial regression for agnostic learning under
  gaussian marginals in the sq model.
\newblock In {\em Conference on Learning Theory}, pages 1552--1584. PMLR, 2021.

\bibitem[DKR23]{diakonikolas2023near}
Ilias Diakonikolas, Daniel~M Kane, and Lisheng Ren.
\newblock Near-optimal cryptographic hardness of agnostically learning
  halfspaces and relu regression under gaussian marginals.
\newblock {\em arXiv preprint arXiv:2302.06512}, 2023.

\bibitem[DKS18]{diakonikolas2018learning}
Ilias Diakonikolas, Daniel~M Kane, and Alistair Stewart.
\newblock Learning geometric concepts with nasty noise.
\newblock In {\em Proceedings of the 50th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 1061--1073, 2018.

\bibitem[DKTZ20a]{diakonikolas2020learning}
Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis.
\newblock Learning halfspaces with massart noise under structured
  distributions.
\newblock In {\em Conference on Learning Theory}, pages 1486--1513. PMLR, 2020.

\bibitem[DKTZ20b]{diakonikolas2020non}
Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis.
\newblock Non-convex sgd learns halfspaces with adversarial label noise.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18540--18549, 2020.

\bibitem[DKTZ22]{diakonikolas2022learning_online}
Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos Zarifis.
\newblock Learning general halfspaces with adversarial label noise via online
  gradient descent.
\newblock In {\em International Conference on Machine Learning}, pages
  5118--5141. PMLR, 2022.

\bibitem[DKZ20]{diakonikolas2020near}
Ilias Diakonikolas, Daniel Kane, and Nikos Zarifis.
\newblock Near-optimal sq lower bounds for agnostically learning halfspaces and
  relus under gaussian marginals.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13586--13596, 2020.

\bibitem[FKP{\etalchar{+}}19]{fleming2019semialgebraic}
Noah Fleming, Pravesh Kothari, Toniann Pitassi, et~al.
\newblock Semialgebraic proofs and efficient algorithm design.
\newblock {\em Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 14(1-2):1--221, 2019.

\bibitem[GGK20]{goel2020statistical}
Surbhi Goel, Aravind Gollakota, and Adam Klivans.
\newblock Statistical-query lower bounds via functional gradients.
\newblock {\em Advances in Neural Information Processing Systems},
  33:2147--2158, 2020.

\bibitem[GKK23]{gollakota2022moment}
Aravind Gollakota, Adam~R Klivans, and Pravesh~K Kothari.
\newblock A moment-matching approach to testable learning and a new
  characterization of rademacher complexity.
\newblock {\em Proceedings of the fifty-fifth annual ACM Symposium on Theory of
  Computing}, 2023.
\newblock To appear.

\bibitem[GKSV23]{gollakota2023efficient}
Aravind Gollakota, Adam~R Klivans, Konstantinos Stavropoulos, and Arsen
  Vasilyan.
\newblock An efficient tester-learner for halfspaces.
\newblock {\em arXiv preprint arXiv:2302.14853}, 2023.

\bibitem[KKK19]{karmalkar2019list}
Sushrut Karmalkar, Adam Klivans, and Pravesh Kothari.
\newblock List-decodable linear regression.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem[KLS95]{kannan1995isoperimetric}
Ravi Kannan, L{\'a}szl{\'o} Lov{\'a}sz, and Mikl{\'o}s Simonovits.
\newblock Isoperimetric problems for convex bodies and a localization lemma.
\newblock {\em Discrete \& Computational Geometry}, 13:541--559, 1995.

\bibitem[KLS09]{klivans2009learning}
Adam~R Klivans, Philip~M Long, and Rocco~A Servedio.
\newblock Learning halfspaces with malicious noise.
\newblock {\em Journal of Machine Learning Research}, 10(12), 2009.

\bibitem[KS17]{kothari2017better}
Pravesh~K Kothari and Jacob Steinhardt.
\newblock Better agnostic clustering via relaxed tensor norms.
\newblock {\em arXiv preprint arXiv:1711.07465}, 2017.

\bibitem[Las01]{lasserre2001new}
Jean~B Lasserre.
\newblock New positive semidefinite relaxations for nonconvex quadratic
  programs.
\newblock {\em Advances in Convex Analysis and Global Optimization: Honoring
  the Memory of C. Caratheodory (1873--1950)}, pages 319--331, 2001.

\bibitem[LV07]{lovasz2007geometry}
L{\'a}szl{\'o} Lov{\'a}sz and Santosh Vempala.
\newblock The geometry of logconcave functions and sampling algorithms.
\newblock {\em Random Structures \& Algorithms}, 30(3):307--358, 2007.

\bibitem[LV18]{lee2018kannan}
Yin~Tat Lee and Santosh~S Vempala.
\newblock The {Kannan-Lov\'asz-Simonovits conjecture}.
\newblock {\em arXiv preprint arXiv:1807.03465}, 2018.

\bibitem[Nes00]{nesterov2000squared}
Yurii Nesterov.
\newblock Squared functional systems and optimization problems.
\newblock {\em High performance optimization}, pages 405--440, 2000.

\bibitem[Par00]{parrilo2000structured}
Pablo~A Parrilo.
\newblock {\em Structured semidefinite programs and semialgebraic geometry
  methods in robustness and optimization}.
\newblock California Institute of Technology, 2000.

\bibitem[RV23]{rubinfeld2022testing}
Ronitt Rubinfeld and Arsen Vasilyan.
\newblock Testing distributional assumptions of learning algorithms.
\newblock {\em Proceedings of the fifty-fifth annual ACM Symposium on Theory of
  Computing}, 2023.
\newblock To appear.

\bibitem[RY20]{raghavendra2020list}
Prasad Raghavendra and Morris Yau.
\newblock List decodable learning via sum of squares.
\newblock In {\em Proceedings of the Fourteenth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 161--180. SIAM, 2020.

\bibitem[Sho87]{shor1987quadratic}
N.Z. Shor.
\newblock Quadratic optimization problems.
\newblock {\em Izv. Akad. Nauk SSSR Tekhn. Kibernet.}, 1987(1):128--139, 222,
  1987.

\bibitem[SW14]{saumard2014log}
Adrien Saumard and Jon~A Wellner.
\newblock Log-concavity and strong log-concavity: a review.
\newblock {\em Statistics surveys}, 8:45, 2014.

\bibitem[YZ17]{yan2017revisiting}
Songbai Yan and Chicheng Zhang.
\newblock Revisiting perceptron: Efficient and label-optimal learning of
  halfspaces.
\newblock {\em Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Zha18]{zhang2018efficient}
Chicheng Zhang.
\newblock Efficient active learning of sparse halfspaces.
\newblock In {\em Conference on Learning Theory}, pages 1856--1880. PMLR, 2018.

\bibitem[ZL21]{zhang2021improved}
Chicheng Zhang and Yinan Li.
\newblock Improved algorithms for efficient active learning halfspaces with
  massart and tsybakov noise.
\newblock In {\em Conference on Learning Theory}, pages 4526--4527. PMLR, 2021.

\bibitem[ZSA20]{zhang2020efficient}
Chicheng Zhang, Jie Shen, and Pranjal Awasthi.
\newblock Efficient active learning of sparse halfspaces with arbitrary bounded
  noise.
\newblock {\em Advances in Neural Information Processing Systems},
  33:7184--7197, 2020.

\end{thebibliography}
