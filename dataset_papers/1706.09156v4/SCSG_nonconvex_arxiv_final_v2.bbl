\begin{thebibliography}{10}

\bibitem{Agarwal14}
Alekh Agarwal and Leon Bottou.
\newblock A lower bound for the optimization of finite sums.
\newblock {\em ArXiv e-prints abs/1410.0723}, 2014.

\bibitem{agarwal16}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima for nonconvex optimization in linear
  time.
\newblock {\em arXiv preprint arXiv:1611.01146}, 2016.

\bibitem{natasha}
Zeyuan Allen-Zhu.
\newblock Natasha: Faster stochastic non-convex optimization via strongly
  non-convex parameter.
\newblock {\em arXiv preprint arXiv:1702.00763}, 2017.

\bibitem{Zhu16}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Variance reduction for faster non-convex optimization.
\newblock {\em ArXiv e-prints abs/1603.05643}, 2016.

\bibitem{zhu14}
Zeyuan Allen-Zhu and Lorenzo Orecchia.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock {\em arXiv preprint arXiv:1407.1537}, 2014.

\bibitem{Zhu15}
Zeyuan Allen-Zhu and Yang Yuan.
\newblock Improved {SVRG} for non-strongly-convex or sum-of-non-convex
  objectives.
\newblock {\em ArXiv e-prints, abs/1506.01972}, 2015.

\bibitem{bertsekas97}
Dimitri~P Bertsekas.
\newblock A new class of incremental gradient methods for least squares
  problems.
\newblock {\em SIAM Journal on Optimization}, 7(4):913--926, 1997.

\bibitem{carmon16}
Yair Carmon, John~C Duchi, Oliver Hinder, and Aaron Sidford.
\newblock Accelerated methods for non-convex optimization.
\newblock {\em arXiv preprint arXiv:1611.00756}, 2016.

\bibitem{carmon17}
Yair Carmon, Oliver Hinder, John~C Duchi, and Aaron Sidford.
\newblock Convex until proven guilty: Dimension-free acceleration of gradient
  descent on non-convex functions.
\newblock {\em arXiv preprint arXiv:1705.02766}, 2017.

\bibitem{Adagrad}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock {\em Journal of Machine Learning Research}, 12(Jul):2121--2159, 2011.

\bibitem{gaivoronski94}
Alexei~A Gaivoronski.
\newblock Convergence properties of backpropagation for neural nets via theory
  of stochastic gradient methods. {P}art 1.
\newblock {\em Optimization methods and Software}, 4(2):117--134, 1994.

\bibitem{ghadimi13}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{ghadimi16acc}
Saeed Ghadimi and Guanghui Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock {\em Mathematical Programming}, 156(1-2):59--99, 2016.

\bibitem{PSVRG}
Reza Harikandeh, Mohamed~Osama Ahmed, Alim Virani, Mark Schmidt, Jakub
  Kone{\v{c}}n{\`y}, and Scott Sallinen.
\newblock Stop wasting my gradients: Practical {SVRG}.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2242--2250, 2015.

\bibitem{karimi16}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{L}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{lecun15}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{SCSG}
Lihua Lei and Michael~I Jordan.
\newblock Less than a single pass: Stochastically controlled stochastic
  gradient method.
\newblock {\em arXiv preprint arXiv:1609.03261}, 2016.

\bibitem{GLM}
Peter McCullagh and John~A Nelder.
\newblock {\em Generalized {L}inear {M}odels}.
\newblock CRC Press, 1989.

\bibitem{Nemirovsky09}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on Optimization}, 19(4):1574--1609, 2009.

\bibitem{nesterov04}
Yurii Nesterov.
\newblock {\em Introductory Lectures on Convex Optimization: A Basic Course}.
\newblock Kluwer Academic Publishers, Massachusetts, 2004.

\bibitem{polyak63}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock {\em Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3(4):643--653, 1963.

\bibitem{reddi16svrg}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1603.06160}, 2016.

\bibitem{reddi16saga}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Fast incremental method for nonconvex optimization.
\newblock {\em arXiv preprint arXiv:1603.06159}, 2016.

\bibitem{momentum}
Ilya Sutskever, James Martens, George~E Dahl, and Geoffrey~E Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock {\em ICML (3)}, 28:1139--1147, 2013.

\bibitem{tseng98}
Paul Tseng.
\newblock An incremental gradient (-projection) method with momentum term and
  adaptive stepsize rule.
\newblock {\em SIAM Journal on Optimization}, 8(2):506--531, 1998.

\end{thebibliography}
