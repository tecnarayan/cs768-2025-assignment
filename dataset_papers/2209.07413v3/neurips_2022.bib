@misc{lee2019snip,
      title={SNIP: Single-shot Network Pruning based on Connection Sensitivity}, 
      author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip H. S. Torr},
      year={2019},
      eprint={1810.02340},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{https://doi.org/10.48550/arxiv.2110.08616,
  doi = {10.48550/ARXIV.2110.08616},
  
  url = {https://arxiv.org/abs/2110.08616},
  
  author = {Zhang, Zhihao and Jia, Zhihao},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {GradSign: Model Performance Inference with Theoretical Insights},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@article{Dong_2021,
	doi = {10.1109/tpami.2021.3054824},
  
	url = {https://doi.org/10.11092Ftpami.2021.3054824},
  
	year = 2021,
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	pages = {1--1},
  
	author = {Xuanyi Dong and Lu Liu and Katarzyna Musial and Bogdan Gabrys},
  
	title = {{NATS}-Bench: Benchmarking {NAS} Algorithms for Architecture Topology and Size},
  
	journal = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence}
}

@inproceedings{Lones2003EnzymeGP,
  title={Enzyme genetic programming : modelling biological evolvability in genetic programming},
  author={M. Lones},
  year={2003}
}
@article{DEAP_JMLR2012,
    author    = " F\'elix-Antoine Fortin and Fran\c{c}ois-Michel {De Rainville} and Marc-Andr\'e Gardner and Marc Parizeau and Christian Gagn\'e ",
    title     = { {DEAP}: Evolutionary Algorithms Made Easy },
    pages     = { 2171--2175 },
    volume    = { 13 },
    month     = { jul },
    year      = { 2012 },
    journal   = { Journal of Machine Learning Research }
}

@misc{real2019regularized,
      title={Regularized Evolution for Image Classifier Architecture Search}, 
      author={Esteban Real and Alok Aggarwal and Yanping Huang and Quoc V Le},
      year={2019},
      eprint={1802.01548},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{luo2019neural,
      title={Neural Architecture Optimization}, 
      author={Renqian Luo and Fei Tian and Tao Qin and Enhong Chen and Tie-Yan Liu},
      year={2019},
      eprint={1808.07233},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{liu2019darts,
      title={DARTS: Differentiable Architecture Search}, 
      author={Hanxiao Liu and Karen Simonyan and Yiming Yang},
      year={2019},
      eprint={1806.09055},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{yang2020cars,
      title={CARS: Continuous Evolution for Efficient Neural Architecture Search}, 
      author={Zhaohui Yang and Yunhe Wang and Xinghao Chen and Boxin Shi and Chao Xu and Chunjing Xu and Qi Tian and Chang Xu},
      year={2020},
      eprint={1909.04977},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{xie2020snas,
      title={SNAS: Stochastic Neural Architecture Search}, 
      author={Sirui Xie and Hehui Zheng and Chunxiao Liu and Liang Lin},
      year={2020},
      eprint={1812.09926},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{cai2019proxylessnas,
      title={ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware}, 
      author={Han Cai and Ligeng Zhu and Song Han},
      year={2019},
      eprint={1812.00332},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{cai2020onceforall,
      title={Once-for-All: Train One Network and Specialize it for Efficient Deployment}, 
      author={Han Cai and Chuang Gan and Tianzhe Wang and Zhekai Zhang and Song Han},
      year={2020},
      eprint={1908.09791},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hu2020dsnas,
      title={DSNAS: Direct Neural Architecture Search without Parameter Retraining}, 
      author={Shoukang Hu and Sirui Xie and Hehui Zheng and Chunxiao Liu and Jianping Shi and Xunying Liu and Dahua Lin},
      year={2020},
      eprint={2002.09128},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{ying2019nasbench101,
      title={NAS-Bench-101: Towards Reproducible Neural Architecture Search}, 
      author={Chris Ying and Aaron Klein and Esteban Real and Eric Christiansen and Kevin Murphy and Frank Hutter},
      year={2019},
      eprint={1902.09635},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}@misc{yu2019evaluating,
      title={Evaluating the Search Phase of Neural Architecture Search}, 
      author={Kaicheng Yu and Christian Sciuto and Martin Jaggi and Claudiu Musat and Mathieu Salzmann},
      year={2019},
      eprint={1902.08142},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{kao2020confuciux,
      title={ConfuciuX: Autonomous Hardware Resource Assignment for DNN Accelerators using Reinforcement Learning}, 
      author={Sheng-Chun Kao and Geonhwa Jeong and Tushar Krishna},
      year={2020},
      eprint={2009.02010},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}
@misc{akhauri2021rhnas,
      title={RHNAS: Realizable Hardware and Neural Architecture Search}, 
      author={Yash Akhauri and Adithya Niranjan and J. Pablo Munoz and Suvadeep Banerjee and Abhijit Davare and Pasquale Cocchini and Anton A. Sorokin and Ravi Iyer and Nilesh Jain},
      year={2021},
      eprint={2106.09180},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{choi2021dance,
      title={DANCE: Differentiable Accelerator/Network Co-Exploration}, 
      author={Kanghyun Choi and Deokki Hong and Hojae Yoon and Joonsang Yu and Youngsok Kim and Jinho Lee},
      year={2021},
      eprint={2009.06237},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{zhang2020dna,
      title={DNA: Differentiable Network-Accelerator Co-Search}, 
      author={Yongan Zhang and Yonggan Fu and Weiwen Jiang and Chaojian Li and Haoran You and Meng Li and Vikas Chandra and Yingyan Lin},
      year={2020},
      eprint={2010.14778},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{mellor2021neural,
      title={Neural Architecture Search without Training}, 
      author={Joseph Mellor and Jack Turner and Amos Storkey and Elliot J. Crowley},
      year={2021},
      eprint={2006.04647},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{xiang2021zerocost,
      title={Zero-Cost Proxies Meet Differentiable Architecture Search}, 
      author={Lichuan Xiang and Łukasz Dudziak and Mohamed S. Abdelfattah and Thomas Chau and Nicholas D. Lane and Hongkai Wen},
      year={2021},
      eprint={2106.06799},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{abdelfattah2021zerocost,
      title={Zero-Cost Proxies for Lightweight NAS}, 
      author={Mohamed S. Abdelfattah and Abhinav Mehrotra and Łukasz Dudziak and Nicholas D. Lane},
      year={2021},
      eprint={2101.08134},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{chen2021neural,
      title={Neural Architecture Search on ImageNet in Four GPU Hours: A Theoretically Inspired Perspective}, 
      author={Wuyang Chen and Xinyu Gong and Zhangyang Wang},
      year={2021},
      eprint={2102.11535},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{lin2021zennas,
      title={Zen-NAS: A Zero-Shot NAS for High-Performance Deep Image Recognition}, 
      author={Ming Lin and Pichao Wang and Zhenhong Sun and Hesen Chen and Xiuyu Sun and Qi Qian and Hao Li and Rong Jin},
      year={2021},
      eprint={2102.01063},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{timit,
  added-at = {2008-02-26T11:58:58.000+0100},
  author = {Garofolo, J. S. and Lamel, L. F. and Fisher, W. M. and Fiscus, J. G. and Pallett, D. S. and Dahlgren, N. L.},
  biburl = {https://www.bibsonomy.org/bibtex/2f03c554589b58a88b798d35211645b6e/schaul},
  citeulike-article-id = {2382101},
  description = {idsia},
  interhash = {2aeb4ac73650922f64feb31c9e2b35f3},
  intrahash = {f03c554589b58a88b798d35211645b6e},
  keywords = {juergen},
  priority = {2},
  publisher = {NIST},
  timestamp = {2008-02-26T11:59:12.000+0100},
  title = {DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus CDROM},
  year = 1993
}

@misc{pham2018efficient,
      title={Efficient Neural Architecture Search via Parameter Sharing}, 
      author={Hieu Pham and Melody Y. Guan and Barret Zoph and Quoc V. Le and Jeff Dean},
      year={2018},
      eprint={1802.03268},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@misc{hu2020anglebased,
      title={Angle-based Search Space Shrinking for Neural Architecture Search}, 
      author={Yiming Hu and Yuding Liang and Zichao Guo and Ruosi Wan and Xiangyu Zhang and Yichen Wei and Qingyi Gu and Jian Sun},
      year={2020},
      eprint={2004.13431},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@misc{white2021study,
      title={A Study on Encodings for Neural Architecture Search}, 
      author={Colin White and Willie Neiswanger and Sam Nolen and Yash Savani},
      year={2021},
      eprint={2007.04965},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@InProceedings{parisotto2017neuro-symbolic,
author = {Parisotto, Emilio and Mohamed, Abdelrahman and Singh, Rishabh and Li, Lihong and Zhou, Denny and Kohli, Pushmeet},
title = {Neuro-Symbolic Program Synthesis},
booktitle = {5th International Conference on Learning Representations (ICLR 2017)},
year = {2017},
month = {February},
abstract = {Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the RecursiveReverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training.},
url = {https://www.microsoft.com/en-us/research/publication/neuro-symbolic-program-synthesis-2/},
edition = {5th International Conference on Learning Representations (ICLR 2017)},
}
@misc{andrychowicz2016learning,
      title={Learning to learn by gradient descent by gradient descent}, 
      author={Marcin Andrychowicz and Misha Denil and Sergio Gomez and Matthew W. Hoffman and David Pfau and Tom Schaul and Brendan Shillingford and Nando de Freitas},
      year={2016},
      eprint={1606.04474},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}
@misc{tanaka2020pruning,
      title={Pruning neural networks without any data by iteratively conserving synaptic flow}, 
      author={Hidenori Tanaka and Daniel Kunin and Daniel L. K. Yamins and Surya Ganguli},
      year={2020},
      eprint={2006.05467},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@Inbook{Vanschoren2019,
author="Vanschoren, Joaquin",
editor="Hutter, Frank
and Kotthoff, Lars
and Vanschoren, Joaquin",
title="Meta-Learning",
bookTitle="Automated Machine Learning: Methods, Systems, Challenges",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="35--61",
abstract="Meta-learning, or learning to learn, is the science of systematically observing how different machine learning approaches perform on a wide range of learning tasks, and then learning from this experience, or meta-data, to learn new tasks much faster than otherwise possible. Not only does this dramatically speed up and improve the design of machine learning pipelines or neural architectures, it also allows us to replace hand-engineered algorithms with novel approaches learned in a data-driven way. In this chapter, we provide an overview of the state of the art in this fascinating and continuously evolving field.",
isbn="978-3-030-05318-5",
doi="10.1007/978-3-030-05318-5_2",
url="https://doi.org/10.1007/978-3-030-05318-5_2"
}
@misc{real2020automlzero,
      title={AutoML-Zero: Evolving Machine Learning Algorithms From Scratch}, 
      author={Esteban Real and Chen Liang and David R. So and Quoc V. Le},
      year={2020},
      eprint={2003.03384},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{349932,
  author={Bengio, S. and Bengio, Y. and Cloutier, J.},
  booktitle={Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence}, 
  title={Use of genetic programming for the search of a new learning rule for neural networks}, 
  year={1994},
  volume={},
  number={},
  pages={324-327 vol.1},
  doi={10.1109/ICEC.1994.349932}}
  
@misc{dong2020nasbench201,
      title={NAS-Bench-201: Extending the Scope of Reproducible Neural Architecture Search}, 
      author={Xuanyi Dong and Yi Yang},
      year={2020},
      eprint={2001.00326},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{radosavovic2019network,
      title={On Network Design Spaces for Visual Recognition}, 
      author={Ilija Radosavovic and Justin Johnson and Saining Xie and Wan-Yen Lo and Piotr Dollár},
      year={2019},
      eprint={1905.13214},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{liu2018progressive,
      title={Progressive Neural Architecture Search}, 
      author={Chenxi Liu and Barret Zoph and Maxim Neumann and Jonathon Shlens and Wei Hua and Li-Jia Li and Li Fei-Fei and Alan Yuille and Jonathan Huang and Kevin Murphy},
      year={2018},
      eprint={1712.00559},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{tan2019mnasnet,
      title={MnasNet: Platform-Aware Neural Architecture Search for Mobile}, 
      author={Mingxing Tan and Bo Chen and Ruoming Pang and Vijay Vasudevan and Mark Sandler and Andrew Howard and Quoc V. Le},
      year={2019},
      eprint={1807.11626},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{zoph2018learning,
      title={Learning Transferable Architectures for Scalable Image Recognition}, 
      author={Barret Zoph and Vijay Vasudevan and Jonathon Shlens and Quoc V. Le},
      year={2018},
      eprint={1707.07012},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{
mehrotra2021nasbenchasr,
title={{\{}NAS{\}}-Bench-{\{}ASR{\}}: Reproducible Neural Architecture Search for Speech Recognition},
author={Abhinav Mehrotra and Alberto Gil C. P. Ramos and Sourav Bhattacharya and {\L}ukasz Dudziak and Ravichander Vipperla and Thomas Chau and Mohamed S Abdelfattah and Samin Ishtiaq and Nicholas Donald Lane},
booktitle={International Conference on Learning Representations (ICLR)},
year={2021}
}

@misc{zhou2020econas,
      title={EcoNAS: Finding Proxies for Economical Neural Architecture Search}, 
      author={Dongzhan Zhou and Xinchi Zhou and Wenwei Zhang and Chen Change Loy and Shuai Yi and Xuesen Zhang and Wanli Ouyang},
      year={2020},
      eprint={2001.01233},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@misc{lannelongue2020green,
      title={Green Algorithms: Quantifying the carbon footprint of computation}, 
      author={Loïc Lannelongue and Jason Grealey and Michael Inouye},
      year={2020},
      eprint={2007.07610},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}
@article{bootstrapNAS,
  author    = {J. Pablo Mu{\~{n}}oz and
               Nikolay Lyalyushkin and
               Yash Akhauri and
               Anastasia Senina and
               Alexander Kozlov and
               Nilesh Jain},
  title     = {Enabling {NAS} with Automated Super-Network Generation},
  journal   = {CoRR},
  volume    = {abs/2112.10878},
  year      = {2021},
  url       = {https://arxiv.org/abs/2112.10878},
  eprinttype = {arXiv},
  eprint    = {2112.10878},
  timestamp = {Tue, 04 Jan 2022 15:59:27 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2112-10878.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

