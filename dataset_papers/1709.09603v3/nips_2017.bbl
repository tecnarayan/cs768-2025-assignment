\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proceedings of The 32nd International Conference on Machine
  Learning}, pages 448--456, 2015.

\bibitem[Zagoruyko and Komodakis(2016)]{Zagoruyko2016WRN}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In \emph{BMVC}, 2016.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2818--2826, 2016.

\bibitem[Edelman et~al.(1998)Edelman, Arias, and Smith]{edelman1998geometry}
Alan Edelman, Tom{\'a}s~A Arias, and Steven~T Smith.
\newblock The geometry of algorithms with orthogonality constraints.
\newblock \emph{SIAM journal on Matrix Analysis and Applications}, 20\penalty0
  (2):\penalty0 303--353, 1998.

\bibitem[Absil et~al.(2009)Absil, Mahony, and Sepulchre]{absil2009optimization}
P-A Absil, Robert Mahony, and Rodolphe Sepulchre.
\newblock \emph{Optimization algorithms on matrix manifolds}.
\newblock Princeton University Press, 2009.

\bibitem[Salimans and Kingma(2016)]{salimans2016weight}
Tim Salimans and Diederik~P Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 29}, pages
  901--909, 2016.

\bibitem[Arpit et~al.(2016)Arpit, Zhou, Kota, and
  Govindaraju]{arpit2016normalization}
Devansh Arpit, Yingbo Zhou, Bhargava Kota, and Venu Govindaraju.
\newblock Normalization propagation: A parametric technique for removing
  internal covariate shift in deep networks.
\newblock In \emph{Proceedings of The 33rd International Conference on Machine
  Learning}, pages 1168--1176, 2016.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Kingma and Ba(2014)]{adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Absil et~al.(2004)Absil, Mahony, and Sepulchre]{absil2004riemannian}
P-A Absil, Robert Mahony, and Rodolphe Sepulchre.
\newblock Riemannian geometry of grassmann manifolds with a view on algorithmic
  computation.
\newblock \emph{Acta Applicandae Mathematicae}, 80\penalty0 (2):\penalty0
  199--220, 2004.

\bibitem[do~Carmo(1976)]{do1976differential}
M.P. do~Carmo.
\newblock \emph{Differential Geometry of Curves and Surfaces}.
\newblock Prentice-Hall, 1976.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{williams1986learning}
David~E. Rumelhart, Geoffrey~E. Hinton, and Ronald~J. Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 10 1986.

\bibitem[Graves(2011)]{graves2011practical}
Alex Graves.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems 24}, pages
  2348--2356, 2011.

\bibitem[Ghahramani et~al.(1996)Ghahramani, Hinton,
  et~al.]{ghahramani1996algorithm}
Zoubin Ghahramani, Geoffrey~E Hinton, et~al.
\newblock The {EM} algorithm for mixtures of factor analyzers.
\newblock Technical report, Technical Report CRG-TR-96-1, University of
  Toronto, 1996.

\bibitem[Hamm and Lee(2009)]{hamm2009extended}
Jihun Hamm and Daniel~D Lee.
\newblock Extended grassmann kernels for subspace-based learning.
\newblock In \emph{Advances in Neural Information Processing Systems 21}, pages
  601--608, 2009.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, Department of Computer Science, University of
  Toronto}, 2009.

\bibitem[Netzer et~al.(2011)Netzer, Wang, Coates, Bissacco, Wu, and
  Ng]{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In \emph{NIPS workshop on deep learning and unsupervised feature
  learning}, 2011.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[He et~al.(2016{\natexlab{a}})He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016{\natexlab{a}}.

\bibitem[He et~al.(2016{\natexlab{b}})He, Zhang, Ren, and Sun]{he2016identity}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In \emph{European Conference on Computer Vision}, pages 630--645.
  Springer, 2016{\natexlab{b}}.

\bibitem[Ioffe(2017)]{ioffe2017batch}
Sergey Ioffe.
\newblock Batch renormalization: Towards reducing minibatch dependence in
  batch-normalized models.
\newblock \emph{arXiv preprint arXiv:1702.03275}, 2017.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning}, pages 1139--1147, 2013.

\bibitem[Clevert et~al.(2015)Clevert, Unterthiner, and
  Hochreiter]{clevert2015fast}
Djork-Arn{\'e} Clevert, Thomas Unterthiner, and Sepp Hochreiter.
\newblock Fast and accurate deep network learning by exponential linear units
  ({ELUs}).
\newblock \emph{arXiv preprint arXiv:1511.07289}, 2015.

\bibitem[Snoek et~al.(2015)Snoek, Rippel, Swersky, Kiros, Satish, Sundaram,
  Patwary, Prabhat, and Adams]{snoek2015scalable}
Jasper Snoek, Oren Rippel, Kevin Swersky, Ryan Kiros, Nadathur Satish,
  Narayanan Sundaram, Mostofa Patwary, Mr~Prabhat, and Ryan Adams.
\newblock Scalable bayesian optimization using deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  2171--2180, 2015.

\bibitem[Lee et~al.(2016)Lee, Gallagher, and Tu]{lee2016generalizing}
Chen-Yu Lee, Patrick~W Gallagher, and Zhuowen Tu.
\newblock Generalizing pooling functions in convolutional neural networks:
  Mixed, gated, and tree.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, 2016.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European Conference on Computer Vision}, pages 646--661.
  Springer, 2016.

\bibitem[Amari(1998)]{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Pascanu and Bengio(2014)]{pascanu2013revisiting}
Razvan Pascanu and Yoshua Bengio.
\newblock Revisiting natural gradient for deep networks.
\newblock \emph{In International Conference on Learning Representations}, 2014.

\bibitem[Martens and Grosse(2015)]{martens2015optimizing}
James Martens and Roger~B Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock In \emph{Proceedings of The 32nd International Conference on Machine
  Learning}, pages 2408--2417, 2015.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
Behnam Neyshabur, Ruslan~R Salakhutdinov, and Nati Srebro.
\newblock Path-sgd: Path-normalized optimization in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2015.

\bibitem[Badrinarayanan et~al.(2015)Badrinarayanan, Mishra, and
  Cipolla]{badrinarayanan2015understanding}
Vijay Badrinarayanan, Bamdev Mishra, and Roberto Cipolla.
\newblock Understanding symmetries in deep networks.
\newblock \emph{arXiv preprint arXiv:1511.01029}, 2015.

\bibitem[Sherman and Morrison(1950)]{sherman1950adjustment}
Jack Sherman and Winifred~J Morrison.
\newblock Adjustment of an inverse matrix corresponding to a change in one
  element of a given matrix.
\newblock \emph{The Annals of Mathematical Statistics}, 21\penalty0
  (1):\penalty0 124--127, 1950.

\bibitem[Ding and Yao(2007)]{ding2007eigenvalue}
Jiu Ding and Guangming Yao.
\newblock The eigenvalue problem of a specially updated matrix.
\newblock \emph{Applied mathematics and computation}, 185\penalty0
  (1):\penalty0 415--420, 2007.

\end{thebibliography}
