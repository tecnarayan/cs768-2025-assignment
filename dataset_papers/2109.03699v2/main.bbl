\begin{thebibliography}{10}

\bibitem{Yanmaz2017}
E.~Yanmaz, M.~Quaritsch, S.~Yahyanejad, B.~Rinner, H.~Hellwagner, and
  C.~Bettstetter, ``Communication and coordination for drone networks,'' in
  {\em Proc. International Conference on Ad Hoc Networks}, pp.~79--91, 2017.

\bibitem{chalaki2020hysteretic}
B.~Chalaki and A.~A. Malikopoulos, ``A hysteretic q-learning coordination
  framework for emerging mobility systems in smart cities,'' {\em
  ArXiv:2011.03137}, 2020.

\bibitem{venturini2021distributed}
F.~Venturini, F.~Mason, F.~Pase, F.~Chiariotti, A.~Testolin, A.~Zanella, and
  M.~Zorzi, ``Distributed reinforcement learning for flexible and efficient uav
  swarm control,'' {\em ArXiv:2103.04666}, 2021.

\bibitem{Yan2013}
Z.~Yan, N.~Jouandeau, and A.~A. Cherif, ``A survey and analysis of multi-robot
  coordination,'' {\em International Journal of Advanced Robotic Systems},
  vol.~10, no.~12, p.~399, 2013.

\bibitem{Krishnamurthy2008}
V.~Krishnamurthy, M.~Maskery, and G.~Yin, ``Decentralized adaptive filtering
  algorithms for sensor activation in an unattended ground sensor network,''
  {\em IEEE Transactions on Signal Processing}, vol.~56, no.~12,
  pp.~6086--6101, 2008.

\bibitem{yuan2020towards}
M.~Yuan, Q.~Cao, M.-o. Pun, and Y.~Chen, ``Towards user scheduling for 6g: A
  fairness-oriented scheduler using multi-agent reinforcement learning,'' {\em
  ArXiv:2012.15081}, 2020.

\bibitem{zhang2021intelligent}
W.~Zhang, H.~Liu, F.~Wang, T.~Xu, H.~Xin, D.~Dou, and H.~Xiong, ``Intelligent
  electric vehicle charging recommendation based on multi-agent reinforcement
  learning,'' {\em ArXiv:2102.07359}, 2021.

\bibitem{Sutton-PG}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour, ``Policy gradient
  methods for reinforcement learning with function approximation,'' in {\em
  Proc. Advances in Neural Information Processing Systems (NeurIPS)}, vol.~12,
  2000.

\bibitem{konda2000actor}
V.~R. Konda and J.~N. Tsitsiklis, ``Actor-critic algorithms,'' in {\em Proc.
  Advances in Neural Information Processing Systems (NeurIPS)}, pp.~1008--1014,
  2000.

\bibitem{peters2008natural}
J.~Peters and S.~Schaal, ``Natural actor-critic,'' {\em Neurocomputing},
  vol.~71, no.~7-9, pp.~1180--1190, 2008.

\bibitem{bhatnagar2009natural}
S.~Bhatnagar, R.~S. Sutton, M.~Ghavamzadeh, and M.~Lee, ``Natural actor--critic
  algorithms,'' {\em Automatica}, vol.~45, no.~11, pp.~2471--2482, 2009.

\bibitem{zhang2018fully}
K.~Zhang, Z.~Yang, H.~Liu, T.~Zhang, and T.~Basar, ``Fully decentralized
  multi-agent reinforcement learning with networked agents,'' in {\em Proc.
  International Conference on Machine Learning (ICML)}, pp.~5872--5881, 2018.

\bibitem{zhang2018networked}
K.~Zhang, Z.~Yang, and T.~Basar, ``Networked multi-agent reinforcement learning
  in continuous spaces,'' in {\em Proc. 2018 IEEE Conference on Decision and
  Control (CDC)}, pp.~2771--2776, IEEE, 2018.

\bibitem{bono2018cooperative}
G.~Bono, J.~S. Dibangoye, L.~Matignon, F.~Pereyron, and O.~Simonin,
  ``Cooperative multi-agent policy gradient,'' in {\em Proc. Joint European
  Conference on Machine Learning and Knowledge Discovery in Databases (ECML
  PKDD)}, pp.~459--476, 2018.

\bibitem{perolat2018actor}
J.~Perolat, B.~Piot, and O.~Pietquin, ``Actor-critic fictitious play in
  simultaneous move multistage games,'' in {\em Proc. International Conference
  on Artificial Intelligence and Statistics}, pp.~919--928, 2018.

\bibitem{zhang2019distributed}
Y.~Zhang and M.~M. Zavlanos, ``Distributed off-policy actor-critic
  reinforcement learning with policy consensus,'' in {\em Proc, Conference on
  Decision and Control (CDC)}, pp.~4674--4679, 2019.

\bibitem{lin2019communication}
Y.~Lin, K.~Zhang, Z.~Yang, Z.~Wang, T.~Ba{\c{s}}ar, R.~Sandhu, and J.~Liu, ``A
  communication-efficient multi-agent actor-critic algorithm for distributed
  reinforcement learning,'' in {\em 2019 IEEE 58th Conference on Decision and
  Control (CDC)}, pp.~5562--5567, 2019.

\bibitem{heredia2019distributed}
P.~C. Heredia and S.~Mou, ``Distributed multi-agent reinforcement learning by
  actor-critic method,'' {\em IFAC-PapersOnLine}, vol.~52, no.~20,
  pp.~363--368, 2019.

\bibitem{lin2019asynchronous}
Y.~Lin, Y.~Luo, K.~Zhang, Z.~Yang, Z.~Wang, T.~Basar, R.~Sandhu, and J.~Liu,
  ``An asynchronous multi-agent actor-critic algorithm for distributed
  reinforcement learning,'' in {\em NeurIPS Optimization Foundations for
  Reinforcement Learning Workshop}, 2019.

\bibitem{chen2020delay}
B.~Chen, M.~Xu, Z.~Liu, L.~Li, and D.~Zhao, ``Delay-aware multi-agent
  reinforcement learning,'' {\em ArXiv:2005.05441}, 2020.

\bibitem{foerster2018counterfactual}
J.~Foerster, G.~Farquhar, T.~Afouras, N.~Nardelli, and S.~Whiteson,
  ``Counterfactual multi-agent policy gradients,'' in {\em Proc. Association
  for the Advancement of Artificial Intelligence (AAAI)}, vol.~32, 2018.

\bibitem{ma2021modeling}
X.~Ma, Y.~Yang, C.~Li, Y.~Lu, Q.~Zhao, and Y.~Jun, ``Modeling the interaction
  between agents in cooperative multi-agent reinforcement learning,'' {\em
  ArXiv:2102.06042}, 2021.

\bibitem{lyu2021contrasting}
X.~Lyu, Y.~Xiao, B.~Daley, and C.~Amato, ``Contrasting centralized and
  decentralized critics in multi-agent reinforcement learning,'' {\em
  ArXiv:2102.04402}, 2021.

\bibitem{qiu2019finite}
S.~Qiu, Z.~Yang, J.~Ye, and Z.~Wang, ``On the finite-time convergence of
  actor-critic algorithm,'' in {\em NeurIPS Optimization Foundations for
  Reinforcement Learning Workshop}, 2019.

\bibitem{kumar2019sample}
H.~Kumar, A.~Koppel, and A.~Ribeiro, ``On the sample complexity of actor-critic
  method for reinforcement learning with function approximation,'' {\em
  ArXiv:1910.08412}, 2019.

\bibitem{xu2020non}
T.~Xu, Z.~Wang, and Y.~Liang, ``Non-asymptotic convergence analysis of two
  time-scale (natural) actor-critic algorithms,'' {\em ArXiv:2005.03557}, 2020.

\bibitem{wu2020finite}
Y.~F. Wu, W.~ZHANG, P.~Xu, and Q.~Gu, ``A finite-time analysis of two
  time-scale actor-critic methods,'' in {\em Proc. Advances in Neural
  Information Processing Systems (NeurIPS)}, vol.~33, pp.~17617--17628, 2020.

\bibitem{xu2020improving}
T.~Xu, Z.~Wang, and Y.~Liang, ``Improving sample complexity bounds for
  (natural) actor-critic algorithms,'' in {\em Proc. Advances in Neural
  Information Processing Systems (NeurIPS)}, vol.~33, 2020.

\bibitem{suttle2019multi}
W.~Suttle, Z.~Yang, K.~Zhang, Z.~Wang, T.~Basar, and J.~Liu, ``A multi-agent
  off-policy actor-critic algorithm for distributed reinforcement learning,''
  {\em ArXiv:1903.06372}, 2019.

\bibitem{konda2002actor}
V.~Konda, ``Actor-critic algorithms (ph.d. thesis),'' {\em Department of
  Electrical Engineering and Computer Science, Massachusetts Institute of
  Technology}, 2002.

\bibitem{bhatnagar2010actor}
S.~Bhatnagar, ``An actor--critic algorithm with function approximation for
  discounted cost constrained markov decision processes,'' {\em Systems \&
  Control Letters}, vol.~59, no.~12, pp.~760--766, 2010.

\bibitem{kakade2001natural}
S.~M. Kakade, ``A natural policy gradient,'' in {\em Proc. Advances in Neural
  Information Processing Systems (NeurIPS)}, vol.~14, 2001.

\bibitem{bhatnagar2007incremental}
S.~Bhatnagar, M.~Ghavamzadeh, M.~Lee, and R.~S. Sutton, ``Incremental natural
  actor-critic algorithms,'' in {\em Proc. Advances in Neural Information
  Processing Systems (NeurIPS)}, vol.~20, pp.~105--112, 2007.

\bibitem{wang2019neural}
L.~Wang, Q.~Cai, Z.~Yang, and Z.~Wang, ``Neural policy gradient methods: Global
  optimality and rates of convergence,'' {\em ArXiv:1909.01150}, 2019.

\bibitem{Wai2018}
H.-T. Wai, Z.~Yang, Z.~Wang, and M.~Hong, ``Multi-agent reinforcement learning
  via double averaging primal-dual optimization,'' in {\em Proc. Advances in
  Neural Information Processing Systems (NeurIPS)}, pp.~9672--9683, 2018.

\bibitem{Doan19a}
T.~Doan, S.~Maguluri, and J.~Romberg, ``Finite-time analysis of distributed
  {TD}(0) with linear function approximation on multi-agent reinforcement
  learning,'' in {\em Proc. International Conference on Machine Learning
  (ICML)}, vol.~97, pp.~1626--1635, 09--15 Jun 2019.

\bibitem{wang2020decentralized}
G.~Wang, S.~Lu, G.~Giannakis, G.~Tesauro, and J.~Sun, ``Decentralized td
  tracking with linear function approximation and its finite-time analysis,''
  in {\em Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  vol.~33, 2020.

\bibitem{liu2021distributed}
R.~Liu and A.~Olshevsky, ``Distributed td (0) with almost no communication,''
  {\em ArXiv:2104.07855}, 2021.

\bibitem{sun2020finite}
J.~Sun, G.~Wang, G.~B. Giannakis, Q.~Yang, and Z.~Yang, ``Finite-sample
  analysis of decentralized temporal-difference learning with linear function
  approximation,'' in {\em Proc. International Conference on Artificial
  Intelligence and Statistics (AISTATS)}, pp.~4485--4495, 2020.

\bibitem{macua2014distributed}
S.~V. Macua, J.~Chen, S.~Zazo, and A.~H. Sayed, ``Distributed policy evaluation
  under multiple behavior strategies,'' {\em IEEE Transactions on Automatic
  Control}, vol.~60, no.~5, pp.~1260--1274, 2014.

\bibitem{stankovic2016multi}
M.~S. Stankovi{\'c} and S.~S. Stankovi{\'c}, ``Multi-agent temporal-difference
  learning with linear function approximation: Weak convergence under
  time-varying network topologies,'' in {\em Proc. American Control Conference
  (ACC)}, pp.~167--172, 2016.

\bibitem{cassano2020multi}
L.~Cassano, K.~Yuan, and A.~H. Sayed, ``Multi-agent fully decentralized value
  function learning with linear convergence rates,'' {\em IEEE Transactions on
  Automatic Control}, 2020.

\bibitem{DTDC}
Z.~Chen, Y.~Zhou, and R.~Chen, ``Multi-agent off-policy td learning:
  Finite-time analysis with near-optimal sample complexity and communication
  complexity,'' {\em ArXiv:2103.13147}, 2021.

\bibitem{srinivasan2018actor}
S.~Srinivasan, M.~Lanctot, V.~Zambaldi, J.~P{\'e}rolat, K.~Tuyls, R.~Munos, and
  M.~Bowling, ``Actor-critic policy optimization in partially observable
  multiagent environments,'' {\em ArXiv:1810.09026}, 2018.

\bibitem{hennes2020neural}
D.~Hennes, D.~Morrill, S.~Omidshafiei, R.~Munos, J.~Perolat, M.~Lanctot,
  A.~Gruslys, J.-B. Lespiau, P.~Parmas, E.~Du{\'e}{\~n}ez-Guzm{\'a}n, {\em
  et~al.}, ``Neural replicator dynamics: Multiagent learning via hedging policy
  gradients,'' in {\em Proc. International Conference on Autonomous Agents and
  MultiAgent Systems (AAMAS)}, pp.~492--501, 2020.

\bibitem{xiao2021shaping}
B.~Xiao, B.~Ramasubramanian, and R.~Poovendran, ``Shaping advice in deep
  multi-agent reinforcement learning,'' {\em ArXiv:2103.15941}, 2021.

\bibitem{lowe2017multi}
R.~Lowe, Y.~Wu, A.~Tamar, J.~Harb, P.~Abbeel, and I.~Mordatch, ``Multi-agent
  actor-critic for mixed cooperative-competitive environments,'' {\em
  ArXiv:1706.02275}, 2017.

\bibitem{zhang2016data}
H.~Zhang, H.~Jiang, Y.~Luo, and G.~Xiao, ``Data-driven optimal consensus
  control for discrete-time multi-agent systems with unknown dynamics using
  reinforcement learning method,'' {\em IEEE Transactions on Industrial
  Electronics}, vol.~64, no.~5, pp.~4091--4100, 2016.

\bibitem{luo2019natural}
Y.~Luo, Z.~Yang, Z.~Wang, and M.~Kolar, ``Natural actor-critic converges
  globally for hierarchical linear quadratic regulator,'' {\em
  ArXiv:1912.06875}, 2019.

\bibitem{wang2019achieving}
W.~Wang, J.~Hao, Y.~Wang, and M.~Taylor, ``Achieving cooperation through deep
  multiagent reinforcement learning in sequential prisoner's dilemmas,'' in
  {\em Proc. of International Conference on Distributed Artificial Intelligence
  (DAI)}, pp.~1--7, 2019.

\bibitem{agarwal2019theory}
A.~Agarwal, S.~M. Kakade, J.~D. Lee, and G.~Mahajan, ``On the theory of policy
  gradient methods: Optimality, approximation, and distribution shift,'' {\em
  ArXiv:1908.00261}, 2019.

\bibitem{bai2021joint}
Q.~Bai, M.~Agarwal, and V.~Aggarwal, ``Joint optimization of multi-objective
  reinforcement learning with policy gradient based algorithm,'' {\em
  ArXiv:2105.14125}, 2021.

\bibitem{daskalakis2021independent}
C.~Daskalakis, D.~J. Foster, and N.~Golowich, ``Independent policy gradient
  methods for competitive reinforcement learning,'' {\em ArXiv:2101.04233},
  2021.

\bibitem{zhao2021provably}
Y.~Zhao, Y.~Tian, J.~D. Lee, and S.~S. Du, ``Provably efficient policy gradient
  methods for two-player zero-sum markov games,'' {\em ArXiv:2102.08903}, 2021.

\bibitem{alfano2021dimension}
C.~Alfano and P.~Rebeschini, ``Dimension-free rates for natural policy gradient
  in multi-agent reinforcement learning,'' {\em ArXiv:2109.11692}, 2021.

\bibitem{bhandari2018finite}
J.~Bhandari, D.~Russo, and R.~Singal, ``A finite time analysis of temporal
  difference learning with linear function approximation,'' in {\em Proc.
  Conference on Learning Theory (COLT)}, vol.~75, pp.~1691--1692, 2018.

\bibitem{xu2019two}
T.~Xu, S.~Zou, and Y.~Liang, ``Two time-scale off-policy td learning:
  Non-asymptotic analysis over markovian samples,'' in {\em Proc. Advances in
  Neural Information Processing Systems (NeurIPS)}, pp.~10634--10644, 2019.

\bibitem{xu2020sample}
T.~Xu and Y.~Liang, ``Sample complexity bounds for two timescale value-based
  reinforcement learning algorithms,'' {\em ArXiv:2011.05053}, 2020.

\bibitem{ma20a}
M.~Shaocong, Z.~Yi, and Z.~Shaofeng, ``Variance-reduced off-policy tdc
  learning: Non-asymptotic convergence analysis,'' in {\em Proc. Advances in
  Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{yang2020sample}
L.~Yang, Q.~Zheng, and G.~Pan, ``Sample complexity of policy gradient finding
  second-order stationary points,'' {\em ArXiv:2012.01491}, 2020.

\bibitem{ghosh2020model}
A.~Ghosh and V.~Aggarwal, ``Model free reinforcement learning algorithm for
  stationary mean field equilibrium for multiple types of agents,'' {\em
  ArXiv:2012.15377}, 2020.

\bibitem{singh2020squarm}
N.~Singh, D.~Data, J.~George, and S.~Diggavi, ``Squarm-sgd:
  Communication-efficient momentum sgd for decentralized optimization,'' {\em
  ArXiv:2005.07041}, 2020.

\bibitem{saha2020decentralized}
R.~Saha, S.~Rini, M.~Rao, and A.~Goldsmith, ``Decentralized optimization over
  noisy, rate-constrained networks: How to agree by talking about how we
  disagree,'' {\em ArXiv:2010.11292}, 2020.

\bibitem{Liu2020An}
Y.~Liu, K.~Zhang, T.~Basar, and W.~Yin, ``An improved analysis of
  (variance-reduced) policy gradient and natural policy gradient methods,'' in
  {\em Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  vol.~33, pp.~7624--7636, 2020.

\bibitem{xu2021doubly}
T.~Xu, Z.~Yang, Z.~Wang, and Y.~Liang, ``Doubly robust off-policy actor-critic:
  Convergence and optimality,'' {\em ArXiv:2102.11866}, 2021.

\bibitem{xu2020primal}
T.~Xu, Y.~Liang, and G.~Lan, ``A primal approach to constrained policy
  optimization: Global optimality and finite-time analysis,'' {\em
  ArXiv:2011.05869}, 2020.

\bibitem{sutton1999policy}
R.~S. Sutton, D.~A. McAllester, S.~P. Singh, Y.~Mansour, {\em et~al.}, ``Policy
  gradient methods for reinforcement learning with function approximation.,''
  in {\em Proc. Advances in Neural Information Processing Systems (NeurIPS)},
  vol.~99, pp.~1057--1063, 1999.

\bibitem{qiu2021rmix}
W.~Qiu, X.~Wang, R.~Yu, R.~Wang, X.~He, B.~An, S.~Obraztsova, and
  Z.~Rabinovich, ``Rmix: Learning risk-sensitive policies forcooperative
  reinforcement learning agents,'' in {\em Proc. Advances in Neural Information
  Processing Systems (NeurIPS)}, 2021.

\bibitem{sutton2018reinforcement}
R.~S. Sutton and A.~G. Barto, {\em Reinforcement learning: An introduction
  (Second Edition)}.
\newblock 2018.

\end{thebibliography}
