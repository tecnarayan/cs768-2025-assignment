\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2018)Aljundi, Babiloni, Elhoseiny, Rohrbach, and
  Tuytelaars]{MAS}
Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and Tuytelaars, T.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Bach et~al.(2004)Bach, Lanckriet, and Jordan]{bach2004multiple}
Bach, F.~R., Lanckriet, G.~R., and Jordan, M.~I.
\newblock Multiple kernel learning, conic duality, and the {SMO} algorithm.
\newblock In \emph{International Conference on Machine Learning}, 2004.

\bibitem[Carratino et~al.(2018)Carratino, Rudi, and
  Rosasco]{carratino2018learning}
Carratino, L., Rudi, A., and Rosasco, L.
\newblock Learning with sgd and random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Chaudhry et~al.(2018)Chaudhry, Dokania, Ajanthan, and
  Torr]{Chaudhry2018RiemannianWF}
Chaudhry, A., Dokania, P.~K., Ajanthan, T., and Torr, P. H.~S.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock In \emph{European Conference on Computer Vision}, 2018.

\bibitem[Chaudhry et~al.(2019{\natexlab{a}})Chaudhry, Ranzato, Rohrbach, and
  Elhoseiny]{AGEM}
Chaudhry, A., Ranzato, M., Rohrbach, M., and Elhoseiny, M.
\newblock Efficient lifelong learning with {A}-{GEM}.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Chaudhry et~al.(2019{\natexlab{b}})Chaudhry, Rohrbach, Elhoseiny,
  Ajanthan, Dokania, Torr, and Ranzato]{Chaudhry2019OnTE}
Chaudhry, A., Rohrbach, M., Elhoseiny, M., Ajanthan, T., Dokania, P.~K., Torr,
  P. H.~S., and Ranzato, M.
\newblock On tiny episodic memories in continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Chaudhry et~al.(2020)Chaudhry, Khan, Dokania, and
  Torr]{chaudhry2020continual}
Chaudhry, A., Khan, N., Dokania, P.~K., and Torr, P.~H.
\newblock Continual learning in low-rank orthogonal subspaces.
\newblock In \emph{Advances in Neural Information Processing System}, 2020.

\bibitem[Cristianini et~al.(2000)Cristianini, Shawe-Taylor,
  et~al.]{cristianini2000introduction}
Cristianini, N., Shawe-Taylor, J., et~al.
\newblock \emph{An introduction to support vector machines and other
  kernel-based learning methods}.
\newblock Cambridge university press, 2000.

\bibitem[Diehl \& Cauwenberghs(2003)Diehl and Cauwenberghs]{diehl2003svm}
Diehl, C.~P. and Cauwenberghs, G.
\newblock Svm incremental learning, adaptation and optimization.
\newblock In \emph{Proceedings of the International Joint Conference on Neural
  Networks}, 2003.

\bibitem[Ebrahimi et~al.(2020)Ebrahimi, Elhoseiny, Darrell, and
  Rohrbach]{ebrahimi2019uncertainty}
Ebrahimi, S., Elhoseiny, M., Darrell, T., and Rohrbach, M.
\newblock Uncertainty-guided continual learning with bayesian neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{Goodfellow2013AnEI}
Goodfellow, I.~J., Mirza, M., Xiao, D., Courville, A., and Bengio, Y.
\newblock An empirical investigation of catastrophic forgeting in gradientbased
  neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Gordon et~al.(2019)Gordon, Bronskill, Bauer, Nowozin, and
  Turner]{gordon2018meta}
Gordon, J., Bronskill, J., Bauer, M., Nowozin, S., and Turner, R.~E.
\newblock Meta-learning probabilistic inference for prediction.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hadsell et~al.(2020)Hadsell, Rao, Rusu, and
  Pascanu]{hadsell2020embracing}
Hadsell, R., Rao, D., Rusu, A.~A., and Pascanu, R.
\newblock Embracing change: Continual learning in deep neural networks.
\newblock \emph{Trends in Cognitive Sciences}, 2020.

\bibitem[Jerfel et~al.(2019)Jerfel, Grant, Griffiths, and
  Heller]{Jerfel2018ReconcilingMA}
Jerfel, G., Grant, E., Griffiths, T.~L., and Heller, K.~A.
\newblock Reconciling meta-learning and continual learning with online mixtures
  of tasks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and Hadsell]{kirkpatrick2017overcoming}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., Hassabis, D.,
  Clopath, C., Kumaran, D., and Hadsell, R.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Kolouri et~al.(2019)Kolouri, Ketz, Zou, Krichmar, and
  Pilly]{kolouri2019attention}
Kolouri, S., Ketz, N., Zou, X., Krichmar, J., and Pilly, P.
\newblock Attention-based structural-plasticity.
\newblock \emph{arXiv preprint arXiv:1903.06070}, 2019.

\bibitem[Lange et~al.(2019)Lange, Aljundi, Masana, Parisot, Jia, Leonardis,
  Slabaugh, and Tuytelaars]{Lange2019ContinualLA}
Lange, M., Aljundi, R., Masana, M., Parisot, S., Jia, X., Leonardis, A.,
  Slabaugh, G.~G., and Tuytelaars, T.
\newblock Continual learning: A comparative study on how to defy forgetting in
  classification tasks.
\newblock \emph{arXiv preprint arXiv:1909.08383}, 2019.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
LeCun, Y., Bengio, Y., and Hinton, G.
\newblock Deep learning.
\newblock \emph{Nature}, 2015.

\bibitem[Lee et~al.(2017)Lee, Kim, Jun, Ha, and Zhang]{lee2017overcoming}
Lee, S.-W., Kim, J.-H., Jun, J., Ha, J.-W., and Zhang, B.-T.
\newblock Overcoming catastrophic forgetting by incremental moment matching.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Li et~al.(2019)Li, Zhou, Wu, Socher, and Xiong]{li2019learn}
Li, X., Zhou, Y., Wu, T., Socher, R., and Xiong, C.
\newblock Learn to grow: A continual structure learning framework for
  overcoming catastrophic forgetting.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Lopez-Paz \& Ranzato(2017)Lopez-Paz and Ranzato]{lopez2017gradient}
Lopez-Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Mallya \& Lazebnik(2018)Mallya and Lazebnik]{PackNet}
Mallya, A. and Lazebnik, S.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2018.

\bibitem[Masse et~al.(2018)Masse, Grant, and Freedman]{Gating}
Masse, N.~Y., Grant, G.~D., and Freedman, D.~J.
\newblock Alleviating catastrophic forgetting using context-dependent gating
  and synaptic stabilization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (44), 2018.

\bibitem[McCloskey \& Cohen(1989)McCloskey and
  Cohen]{McCloskey1989CatastrophicII}
McCloskey, M. and Cohen, N.~J.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock \emph{Academic Press}, 1989.

\bibitem[Mirzadeh et~al.(2020)Mirzadeh, Farajtabar, Pascanu, and
  Ghasemzadeh]{mirzadeh2020understanding}
Mirzadeh, S.~I., Farajtabar, M., Pascanu, R., and Ghasemzadeh, H.
\newblock Understanding the role of training regimes in continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Nguyen et~al.(2018)Nguyen, Li, Bui, and Turner]{nguyen2017variational}
Nguyen, C.~V., Li, Y., Bui, T.~D., and Turner, R.~E.
\newblock Variational continual learning.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Parisi et~al.(2018)Parisi, Kemker, Part, Kanan, and
  Wermter]{Parisi2018ContinualLL}
Parisi, G., Kemker, R., Part, J.~L., Kanan, C., and Wermter, S.
\newblock Continual lifelong learning with neural networks: A review.
\newblock \emph{Neural Networks}, 2018.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, and et. al.]{Pytorch}
Paszke, A., Gross, S., Massa, F., and et. al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock 2019.

\bibitem[Patacchiola et~al.(2020)Patacchiola, Turner, Crowley, O'Boyle, and
  Storkey]{patacchiola2020bayesian}
Patacchiola, M., Turner, J., Crowley, E.~J., O'Boyle, M., and Storkey, A.
\newblock Bayesian meta-learning for the few-shot setting via deep kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Pentina \& Ben-David(2015)Pentina and Ben-David]{pentina2015multi}
Pentina, A. and Ben-David, S.
\newblock Multi-task and lifelong learning of kernels.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  2015.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{rahimi2007random}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2007.

\bibitem[Ramasesh et~al.(2021)Ramasesh, Dyer, and Raghu]{ramasesh2020anatomy}
Ramasesh, V.~V., Dyer, E., and Raghu, M.
\newblock Anatomy of catastrophic forgetting: Hidden representations and task
  semantics.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Kolesnikov, Sperl, and
  Lampert]{Rebuffi2016iCaRLIC}
Rebuffi, S.-A., Kolesnikov, A.~I., Sperl, G., and Lampert, C.~H.
\newblock {iCaRL}: Incremental classifier and representation learning.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  2017.

\bibitem[Riemer et~al.(2019)Riemer, Cases, Ajemian, Liu, Rish, Tu, and
  Tesauro]{riemer2018learning}
Riemer, M., Cases, I., Ajemian, R., Liu, M., Rish, I., Tu, Y., and Tesauro, G.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Ring(1998)]{ring1998child}
Ring, M.~B.
\newblock Child: A first step towards continual learning.
\newblock \emph{Learning to learn}, 1998.

\bibitem[Rios \& Itti(2018)Rios and Itti]{rios2018closed}
Rios, A. and Itti, L.
\newblock Closed-loop gan for continual learning.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2018.

\bibitem[Ritter et~al.(2018)Ritter, Botev, and Barber]{ritter2018online}
Ritter, H., Botev, A., and Barber, D.
\newblock Online structured laplace approximations for overcoming catastrophic
  forgetting.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Rudin(1962)]{rudin1962fourier}
Rudin, W.
\newblock \emph{Fourier analysis on groups}.
\newblock Wiley Online Library, 1962.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 2015.

\bibitem[Rusu et~al.(2016)Rusu, Rabinowitz, Desjardins, Soyer, Kirkpatrick,
  Kavukcuoglu, Pascanu, and Hadsell]{rusu2016progressive}
Rusu, A.~A., Rabinowitz, N.~C., Desjardins, G., Soyer, H., Kirkpatrick, J.,
  Kavukcuoglu, K., Pascanu, R., and Hadsell, R.
\newblock Progressive neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Schmidhuber(2015)]{schmidhuber2015deep}
Schmidhuber, J.
\newblock Deep learning in neural networks: An overview.
\newblock \emph{Neural Networks}, 2015.

\bibitem[Sch{\"o}lkopf \& Smola(2002)Sch{\"o}lkopf and
  Smola]{smola1998learning}
Sch{\"o}lkopf, B. and Smola, A.~J.
\newblock \emph{Learning with kernels}.
\newblock MIT Press, 2002.

\bibitem[Sch{\"o}lkopf et~al.(2001)Sch{\"o}lkopf, Herbrich, and
  Smola]{scholkopf2001generalized}
Sch{\"o}lkopf, B., Herbrich, R., and Smola, A.~J.
\newblock A generalized representer theorem.
\newblock In \emph{International Conference on Computational Learning Theory},
  2001.

\bibitem[Schwarz et~al.(2018)Schwarz, Czarnecki, Luketina, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{schwarz2018progress}
Schwarz, J., Czarnecki, W., Luketina, J., Grabska-Barwinska, A., Teh, Y.~W.,
  Pascanu, R., and Hadsell, R.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Shin et~al.(2017)Shin, Lee, Kim, and Kim]{shin2017continual}
Shin, H., Lee, J.~K., Kim, J., and Kim, J.
\newblock Continual learning with deep generative replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Sinha \& Duchi(2016)Sinha and Duchi]{sinha2016learning}
Sinha, A. and Duchi, J.~C.
\newblock Learning kernels with random features.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Smola \& Sch{\"o}lkopf(2004)Smola and
  Sch{\"o}lkopf]{smola2004tutorial}
Smola, A.~J. and Sch{\"o}lkopf, B.
\newblock A tutorial on support vector regression.
\newblock \emph{Statistics and computing}, 2004.

\bibitem[Titsias et~al.(2020)Titsias, Schwarz, Matthews, Pascanu, and
  Teh]{titsias2019functional}
Titsias, M.~K., Schwarz, J., Matthews, A. G. d.~G., Pascanu, R., and Teh, Y.~W.
\newblock Functional regularisation for continual learning using gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Tossou et~al.(2019)Tossou, Dura, Laviolette, Marchand, and
  Lacoste]{tossou2019adaptive}
Tossou, P., Dura, B., Laviolette, F., Marchand, M., and Lacoste, A.
\newblock Adaptive deep kernel learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Kavukcuoglu, and
  Wierstra]{vinyals2016matching}
Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D.
\newblock Matching networks for one shot learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2016.

\bibitem[Wilson et~al.(2016{\natexlab{a}})Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Deep kernel learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2016{\natexlab{a}}.

\bibitem[Wilson et~al.(2016{\natexlab{b}})Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016stochastic}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Stochastic variational deep kernel learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2016{\natexlab{b}}.

\bibitem[Wortsman et~al.(2020)Wortsman, Ramanujan, Liu, Kembhavi, Rastegari,
  Yosinski, and Farhadi]{wortsman2020supermasks}
Wortsman, M., Ramanujan, V., Liu, R., Kembhavi, A., Rastegari, M., Yosinski,
  J., and Farhadi, A.
\newblock Supermasks in superposition.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Yoon et~al.(2018)Yoon, Yang, Lee, and Hwang]{yoon2018lifelong}
Yoon, J., Yang, E., Lee, J., and Hwang, S.~J.
\newblock Lifelong learning with dynamically expandable networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{zenke2017continual}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Wang, Lim, and Feng]{zhang2019prototype}
Zhang, M., Wang, T., Lim, J.~H., and Feng, J.
\newblock Prototype reminding for continual learning.
\newblock \emph{arXiv preprint arXiv:1905.09447}, 2019.

\bibitem[Zhen et~al.(2020)Zhen, Sun, Du, Xu, Yin, Shao, and
  Snoek]{zhen2020learning}
Zhen, X., Sun, H., Du, Y., Xu, J., Yin, Y., Shao, L., and Snoek, C.
\newblock Learning to learn kernels with variational random features.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\end{thebibliography}
