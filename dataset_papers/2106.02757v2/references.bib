@inproceedings{kakade2002approximately,
  title={Approximately optimal approximate reinforcement learning},
  author={Kakade, Sham and Langford, John},
  booktitle={ICML},
  year={2002}
}


@article{hart1968formal,
  title={A formal basis for the heuristic determination of minimum cost paths},
  author={Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal={IEEE Transactions on Systems Science and Cybernetics},
  volume={4},
  number={2},
  pages={100--107},
  year={1968},
  publisher={IEEE}
}


@inproceedings{pearl1981,
  title={Heuristic Search Theory: Survey of Recent Results},
  author={Judea Pearl},
  booktitle={IJCAI},
  year={1981}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{florensa2017reverse,
  title={Reverse curriculum generation for reinforcement learning},
  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
  booktitle={CoRL},
  year={2017}
}

@inproceedings{asai2020,
author={Masataro Asai and Christian Muise},
title={Learning Neural-Symbolic Descriptive Planning Models via Cube-Space Priors: The Voyage Home (to STRIPS)},
booktitle={IJCAI},
year={2020}
}

@book{russell2020,
author={Stuart J. Russell and Peter Norvig},
title={Artificial Intelligence: A Modern Approach},
publisher={Pearson},
year={2020},
edition={4th}
}

@article{kolobov2012,
title={Planning with {M}arkov decision processes: An {AI} perspective},
author={Mausam and Andrey Kolobov},
journal={Synthesis Lectures on Artificial Intelligence and Machine Learning},
year={2012},
volume={6},
pages={1--210}
}

@article{browne2012survey,
  title={A survey of monte carlo tree search methods},
  author={Browne, Cameron B and Powley, Edward and Whitehouse, Daniel and Lucas, Simon M and Cowling, Peter I and Rohlfshagen, Philipp and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  journal={IEEE Transactions on Computational Intelligence and AI in games},
  volume={4},
  number={1},
  pages={1--43},
  year={2012},
  publisher={IEEE}
}

@article{russell2002artificial,
  title={Artificial intelligence: a modern approach},
  author={Russell, Stuart and Norvig, Peter},
  year={2002}
}

@inproceedings{sidford2018near,
  title={Near-optimal time and sample complexities for solving {M}arkov decision processes with a generative model},
  author={Sidford, Aaron and Wang, Mengdi and Wu, Xian and Yang, Lin F and Ye, Yinyu},
  booktitle={NeurIPS},
  year={2018}
}

@article{jin2020pessimism,
  title={Is Pessimism Provably Efficient for Offline {RL}?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2012.15085},
  year={2020}
}

@inproceedings{jin2018q,
  title={Is {Q}-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{ng1999policy,
  title={Policy invariance under reward transformations: Theory and application to reward shaping},
  author={Ng, Andrew Y and Harada, Daishi and Russell, Stuart},
  booktitle={ICML},
  year={1999}
}

@inproceedings{hester2018deep,
  title={Deep {Q}-learning from demonstrations},
  author={Hester, Todd and Vecerik, Matej and Pietquin, Olivier and Lanctot, Marc and Schaul, Tom and Piot, Bilal and Horgan, Dan and Quan, John and Sendonaris, Andrew and Osband, Ian and others},
  booktitle={AAAI},
  year={2018}
}

@inproceedings{hu2020learning,
  title={Learning to Utilize Shaping Rewards: A New Approach of Reward Shaping},
  author={Hu, Yujing and Wang, Weixun and Jia, Hangtian and Wang, Yixiang and Chen, Yingfeng and Hao, Jianye and Wu, Feng and Fan, Changjie},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{amit2020discount,
  title={Discount Factor as a Regularizer in Reinforcement Learning},
  author={Amit, Ron and Meir, Ron and Ciosek, Kamil},
  booktitle={ICML},
  year={2020}
}

@inproceedings{liu2020provably,
  title={Provably Good Batch Off-Policy Reinforcement Learning Without Great Exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{romoff2019separating,
  title={Separating value functions across time-scales},
  author={Romoff, Joshua and Henderson, Peter and Touati, Ahmed and Brunskill, Emma and Pineau, Joelle and Ollivier, Yann},
  booktitle={ICML},
  year={2019}
}

@article{gulcehre2021regularized,
  title={Regularized Behavior Value Estimation},
  author={Gulcehre, Caglar and Colmenarejo, Sergio G{\'o}mez and Wang, Ziyu and Sygnowski, Jakub and Paine, Thomas and Zolna, Konrad and Chen, Yutian and Hoffman, Matthew and Pascanu, Razvan and de Freitas, Nando},
  journal={arXiv preprint arXiv:2103.09575},
  year={2021}
}

@inproceedings{lu2018non,
  title={Non-delusional Q-learning and value iteration},
  author={Lu, Tyler and Schuurmans, Dale and Boutilier, Craig},
  booktitle={NeurIPS},
  year={2018}
}

@inproceedings{dann2015sample,
  title={Sample complexity of episodic fixed-horizon reinforcement learning},
  author={Dann, Christoph and Brunskill, Emma},
  booktitle={NIPS},
  year={2015}
}

@inproceedings{pmlr-v119-foster20a,
  title = 	 {Beyond {UCB}: Optimal and Efficient Contextual Bandits with Regression Oracles},
  author =       {Foster, Dylan and Rakhlin, Alexander},
  booktitle = 	 {ICML},
  year = 	 {2020}
}

@article{nair2020accelerating,
  title={Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in {S}tarCraft {II} using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

@inproceedings{farahmand2016truncated,
  title={Truncated approximate dynamic programming with task-dependent terminal value},
  author={Farahmand, Amir-massoud and Nikovski, Daniel N and Igarashi, Yuji and Konaka, Hiroki},
  booktitle={AAAI},
  year={2016}
}

@article{bertsekas2005dynamic,
  title={Dynamic programming and suboptimal control: A survey from ADP to MPC},
  author={Bertsekas, Dimitri P},
  journal={European Journal of Control},
  volume={11},
  number={4-5},
  pages={310--334},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{bhardwaj2020blending,
  title={Blending MPC \& Value Function Approximation for Efficient Reinforcement Learning},
  author={Bhardwaj, Mohak and Choudhury, Sanjiban and Boots, Byron},
  booktitle={ICLR},
  year={2021}
}

@inproceedings{hoeller2020deep,
  title={Deep value model predictive control},
  author={Hoeller, David and Farshidian, Farbod and Hutter, Marco},
  booktitle={CoRL},
  year={2020}
}

@inproceedings{zhong2013value,
  title={Value function approximation and model predictive control},
  author={Zhong, Mingyuan and Johnson, Mikala and Tassa, Yuval and Erez, Tom and Todorov, Emanuel},
  booktitle={IEEE International Symposium on Adaptive Dynamic Programming and Reinforcement Learning},
  pages={100--107},
  year={2013}
}

@article{richalet1978model,
  title={Model predictive heuristic control},
  author={Richalet, Jacques and Rault, Andr{\'e} and Testud, JL and Papon, J},
  journal={Automatica},
  volume={14},
  number={5},
  pages={413--428},
  year={1978},
  publisher={Pergamon Press, Inc. Elmsford, NY, USA}
}

@inproceedings{bejjani2018planning,
  title={Planning with a receding horizon for manipulation in clutter using a learned value function},
  author={Bejjani, Wissam and Papallas, Rafael and Leonetti, Matteo and Dogar, Mehmet R},
  booktitle={Humanoids},
  pages={1--9},
  year={2018}
}

@inproceedings{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{jiang2015dependence,
  title={The Dependence of Effective Planning Horizon on Model Accuracy},
  author={Jiang, Nan and Kulesza, Alex and Singh, Satinder and Lewis, Richard},
  booktitle={AAMAS},
  year={2015}
}

@article{bianchi2013heuristically,
  title={Heuristically-accelerated multiagent reinforcement learning},
  author={Bianchi, Reinaldo AC and Martins, Murilo F and Ribeiro, Carlos HC and Costa, Anna HR},
  journal={IEEE transactions on cybernetics},
  volume={44},
  number={2},
  pages={252--265},
  year={2013},
  publisher={IEEE}
}

@article{wei2021non,
  title={Non-stationary Reinforcement Learning without Prior Knowledge: An Optimal Black-box Approach},
  author={Wei, Chen-Yu and Luo, Haipeng},
  journal={arXiv preprint arXiv:2102.05406},
  year={2021}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  year={2018}
}

@inproceedings{chen2018improving,
  title={Improving Offline Value-Function Approximations for POMDPs by Reducing Discount Factors},
  author={Chen, Yi-Chun and Kochenderfer, Mykel J and Spaan, Matthijs TJ},
  booktitle={IROS},
  year={2018}
}

@inproceedings{efroni2018beyond,
  title={Beyond the one-step greedy approach in reinforcement learning},
  author={Efroni, Yonathan and Dalal, Gal and Scherrer, Bruno and Mannor, Shie},
  booktitle={ICML},
  year={2018}
}

@inproceedings{jiang2016structural,
  title={On structural properties of MDPs that bound loss due to shallow planning},
  author={Jiang, Nan and Singh, Satinder and Tewari, Ambuj},
  booktitle={IJCAI},
  year={2016}
}

@inproceedings{petrik2008biasing,
  title={Biasing approximate dynamic programming with a lower discount factor},
  author={Petrik, Marek and Scherrer, Bruno},
  booktitle={NIPS},
  year={2008}
}


@InProceedings{pmlr-v80-liang18b,
  title = 	 {{RL}lib: Abstractions for Distributed Reinforcement Learning},
  author =       {Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle = 	 {ICML},
  year = 	 {2018}
}

@article{Hoffmann_2001,
   title={The {FF} Planning System: Fast Plan Generation Through Heuristic Search},
   volume={14},
   journal={Journal of Artificial Intelligence Research},
   author={Hoffmann, J. and Nebel, B.},
   year={2001},
   pages={253–302}
}

@article{Richter_2010,
   title={The {LAMA} Planner: Guiding Cost-Based Anytime Planning with Landmarks},
   volume={39},
   journal={Journal of Artificial Intelligence Research},
   author={Richter, S. and Westphal, M.},
   year={2010},
   pages={127–177}
}

@InProceedings{akolobov-aaai10,
title={Classical Planning in {MDP} Heuristics: With a Little Help from Generalization},
author={Andrey Kolobov and Mausam and Daniel S. Weld},
booktitle={AAAI},
year={2010}
}


@InProceedings{pmlr-v80-espeholt18a,
  title = 	 {{IMPALA}: Scalable Distributed Deep-{RL} with Importance Weighted Actor-Learner Architectures},
  author =       {Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and Legg, Shane and Kavukcuoglu, Koray},
  booktitle = 	 {ICML},
  year = 	 {2018}
}


@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{schulman2015high,
  title={High-dimensional continuous control using generalized advantage estimation},
  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},
  booktitle={ICLR},
  year={2016}
}

@inproceedings{seijen2014true,
  title={True online TD (lambda)},
  author={Seijen, Harm and Sutton, Rich},
  booktitle={ICML},
  year={2014}
}

@inproceedings{sherstan2020gamma,
  title={Gamma-nets: Generalizing value estimation over timescale},
  author={Sherstan, Craig and Dohare, Shibhansh and MacGlashan, James and G{\"u}nther, Johannes and Pilarski, Patrick M},
  booktitle={AAAI},
  year={2020}
}

@article{tessler2020maximizing,
  title={Maximizing the total reward via reward tweaking},
  author={Tessler, Chen and Mannor, Shie},
  journal={arXiv preprint arXiv:2002.03327},
  year={2020}
}

@inproceedings{todorov2012mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={IROS},
  year={2012}
}


@article{procgen,
      title={Measuring Sample Efficiency and Generalization in Reinforcement Learning Benchmarks: Neur{IPS} 2020 {P}rocgen Benchmark}, 
      author={Sharada Mohanty and Jyotish Poonganam and Adrien Gaidon and Andrey Kolobov and Blake Wulfe and Dipam Chakraborty and Gražvydas Šemetulskis and João Schapke and Jonas Kubilius and Jurgis Pašukonis and Linas Klimas and Matthew Hausknecht and Patrick MacAlpine and Quang Nhat Tran and Thomas Tumiel and Xiaocheng Tang and Xinwei Chen and Christopher Hesse and Jacob Hilton and William Hebgen Guss and Sahika Genc and John Schulman and Karl Cobbe},
      year={2021},
      journal={arXiv preprint arXiv:2103.15332}
}


@inproceedings{neurips2019_eba237ec,
 author = {Van Seijen, Harm and Fatemi, Mehdi and Tavakoli, Arash},
 booktitle = {NeurIPS},
 title = {Using a Logarithmic Mapping to Enable Lower Discount Factors in Reinforcement Learning},
 year = {2019}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015}
}

@article{blackwell1962discrete,
  title={Discrete dynamic programming},
  author={Blackwell, David},
  journal={The Annals of Mathematical Statistics},
  pages={719--726},
  year={1962},
  publisher={JSTOR}
}

@misc{garage,
 author = {The garage contributors},
 title = {Garage: A toolkit for reproducible reinforcement learning research},
 year = {2019},
 publisher = {GitHub},
 journal = {GitHub repository},
 howpublished = {{https://github.com/rlworkgroup/garage}}
}

@inproceedings{cheng2020policy,
  title={Policy Improvement via Imitation of Multiple Oracles},
  author={Cheng, Ching-An and Kolobov, Andrey and Agarwal, Alekh},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{cobbe2020leveraging,
      title={Leveraging Procedural Generation to Benchmark Reinforcement Learning}, 
      author={Karl Cobbe and Christopher Hesse and Jacob Hilton and John Schulman},
      year={2020},
      booktitle={ICML}
}

@inproceedings{sun2017deeply,
  title={Deeply aggrevated: Differentiable imitation learning for sequential prediction},
  author={Sun, Wen and Venkatraman, Arun and Gordon, Geoffrey J and Boots, Byron and Bagnell, J Andrew},
  booktitle={ICML},
  year={2017}
}

@inproceedings{sun2018truncated,
  title={TRUNCATED HORIZON POLICY SEARCH: COMBINING REINFORCEMENT LEARNING \& IMITATION LEARNING},
  author={Sun, Wen and Bagnell, J Andrew and Boots, Byron},
  booktitle={ICLR},
  year={2018}
}