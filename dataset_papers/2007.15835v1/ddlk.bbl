\begin{thebibliography}{24}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bishop(1994)]{bishop:1994:mixture-density-networks}
Christopher~M Bishop.
\newblock Mixture density networks.
\newblock Technical report, Aston University, Birmingham, UK, 1994.

\bibitem[{Candes} et~al.(2016){Candes}, {Fan}, {Janson}, and
  {Lv}]{ModelX_Knockoffs}
E.~{Candes}, Y.~{Fan}, L.~{Janson}, and J.~{Lv}.
\newblock {Panning for Gold: Model-X Knockoffs for High-dimensional Controlled
  Variable Selection}.
\newblock \emph{ArXiv e-prints}, October 2016.

\bibitem[Candes et~al.(2018)Candes, Fan, Janson, and Lv]{candes2018panning}
Emmanuel Candes, Yingying Fan, Lucas Janson, and Jinchi Lv.
\newblock Panning for gold:‘model-x’knockoffs for high dimensional
  controlled variable selection.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 80\penalty0 (3):\penalty0 551--577, 2018.

\bibitem[Falcon(2019)]{falcon2019pytorch}
WA~Falcon.
\newblock Pytorch lightning.
\newblock \emph{GitHub. Note: https://github.
  com/williamFalcon/pytorch-lightning Cited by}, 3, 2019.

\bibitem[Figurnov et~al.(2018)Figurnov, Mohamed, and
  Mnih]{figurnov2018implicit}
Mikhail Figurnov, Shakir Mohamed, and Andriy Mnih.
\newblock Implicit reparameterization gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  441--452, 2018.

\bibitem[Friedman(2002)]{friedman2002stochastic}
Jerome~H Friedman.
\newblock Stochastic gradient boosting.
\newblock \emph{Computational statistics \& data analysis}, 38\penalty0
  (4):\penalty0 367--378, 2002.

\bibitem[Germain et~al.(2015)Germain, Gregor, Murray, and
  Larochelle]{germain2015made}
Mathieu Germain, Karol Gregor, Iain Murray, and Hugo Larochelle.
\newblock {MADE:} masked autoencoder for distribution estimation.
\newblock \emph{CoRR}, abs/1502.03509, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03509}.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{wgan-train}
Ishaan Gulrajani, Faruk Ahmed, Mart{\'{\i}}n Arjovsky, Vincent Dumoulin, and
  Aaron~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock \emph{CoRR}, abs/1704.00028, 2017.
\newblock URL \url{http://arxiv.org/abs/1704.00028}.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{gumbel-jang}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{CoRR}, abs/1611.01144, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.01144}.

\bibitem[Jordon et~al.(2019)Jordon, Yoon, and van~der Schaar]{knockoffgan}
James Jordon, Jinsung Yoon, and Mihaela van~der Schaar.
\newblock Knockoff{GAN}: Generating knockoffs for feature selection using
  generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=ByeZ5jC5YQ}.

\bibitem[Ke et~al.(2017)Ke, Meng, Finley, Wang, Chen, Ma, Ye, and
  Liu]{ke2017lightgbm}
Guolin Ke, Qi~Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma, Qiwei
  Ye, and Tie-Yan Liu.
\newblock Lightgbm: A highly efficient gradient boosting decision tree.
\newblock In \emph{Advances in neural information processing systems}, pages
  3146--3154, 2017.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma et~al.(2015)Kingma, Salimans, and
  Welling]{kingma2015variational}
Durk~P Kingma, Tim Salimans, and Max Welling.
\newblock Variational dropout and the local reparameterization trick.
\newblock In \emph{Advances in neural information processing systems}, pages
  2575--2583, 2015.

\bibitem[Liang et~al.(2018)Liang, Li, and Zhou]{liang2018bayesian}
Faming Liang, Qizhai Li, and Lei Zhou.
\newblock Bayesian neural networks for selection of drug sensitive genes.
\newblock \emph{Journal of the American Statistical Association}, 113\penalty0
  (523):\penalty0 955--972, 2018.

\bibitem[Liu and Zheng(2018)]{liu2018auto}
Ying Liu and Cheng Zheng.
\newblock Auto-encoding knockoff generator for fdr controlled variable
  selection.
\newblock \emph{arXiv preprint arXiv:1809.10765}, 2018.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{gumbel-maddison}
Chris~J. Maddison, Andriy Mnih, and Yee~Whye Teh.
\newblock The concrete distribution: {A} continuous relaxation of discrete
  random variables.
\newblock \emph{CoRR}, abs/1611.00712, 2016.
\newblock URL \url{http://arxiv.org/abs/1611.00712}.

\bibitem[Mescheder et~al.(2017)Mescheder, Nowozin, and
  Geiger]{mescheder2017numerics}
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
\newblock The numerics of gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1825--1835, 2017.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018training}
Lars Mescheder, Andreas Geiger, and Sebastian Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock \emph{arXiv preprint arXiv:1801.04406}, 2018.

\bibitem[Ramdas et~al.(2015)Ramdas, Reddi, P{\'o}czos, Singh, and
  Wasserman]{ramdas2015decreasing}
Aaditya Ramdas, Sashank~Jakkam Reddi, Barnab{\'a}s P{\'o}czos, Aarti Singh, and
  Larry Wasserman.
\newblock On the decreasing power of kernel and distance based nonparametric
  hypothesis tests in high dimensions.
\newblock In \emph{Twenty-Ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Romano et~al.(2018)Romano, Sesia, and Candès]{DeepKnockoffs}
Yaniv Romano, Matteo Sesia, and Emmanuel~J. Candès.
\newblock Deep knockoffs, 2018.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in neural information processing systems}, pages
  2234--2242, 2016.

\bibitem[Sesia et~al.(2017)Sesia, Sabatti, and Cand{\`e}s]{sesia2017gene}
Matteo Sesia, Chiara Sabatti, and Emmanuel~J Cand{\`e}s.
\newblock Gene hunting with knockoffs for hidden markov models.
\newblock \emph{arXiv preprint arXiv:1706.04677}, 2017.

\bibitem[Tansey et~al.(2018)Tansey, Veitch, Zhang, Rabadan, and
  Blei]{tansey2018holdout}
Wesley Tansey, Victor Veitch, Haoran Zhang, Raul Rabadan, and David~M Blei.
\newblock The holdout randomization test: Principled and easy black box feature
  selection.
\newblock \emph{arXiv preprint arXiv:1811.00645}, 2018.

\bibitem[Yang et~al.(2012)Yang, Soares, Greninger, Edelman, Lightfoot, Forbes,
  Bindal, Beare, Smith, Thompson, et~al.]{yang2012genomics}
Wanjuan Yang, Jorge Soares, Patricia Greninger, Elena~J Edelman, Howard
  Lightfoot, Simon Forbes, Nidhi Bindal, Dave Beare, James~A Smith, I~Richard
  Thompson, et~al.
\newblock Genomics of drug sensitivity in cancer (gdsc): a resource for
  therapeutic biomarker discovery in cancer cells.
\newblock \emph{Nucleic acids research}, 41\penalty0 (D1):\penalty0 D955--D961,
  2012.

\end{thebibliography}
