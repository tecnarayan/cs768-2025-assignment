\begin{thebibliography}{10}

\bibitem{radford2019language}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners, 2019.

\bibitem{brown2020language}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,
  Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens Winter,
  Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,
  Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,
  Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners, 2020.

\bibitem{schick2020exploiting}
Timo Schick and Hinrich Schütze.
\newblock Exploiting cloze questions for few-shot text classification and
  natural language inference.
\newblock {\em Computing Research Repository}, arXiv:2001.07676, 2020.

\bibitem{jiang-etal-2020-know}
Zhengbao Jiang, Frank~F. Xu, Jun Araki, and Graham Neubig.
\newblock How can we know what language models know?
\newblock {\em TACL}, 8:423--438, 2020.

\bibitem{gao2020making}
Tianyu Gao, Adam Fisch, and Danqi Chen.
\newblock Making pre-trained language models better few-shot learners.
\newblock {\em arXiv preprint arXiv:2012.15723}, 2020.

\bibitem{zhao2021calibrate}
Tony~Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models, 2021.

\bibitem{lu2021fantastically}
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.
\newblock Fantastically ordered prompts and where to find them: Overcoming
  few-shot prompt order sensitivity, 2021.

\bibitem{liu2021what}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, L.~Carin, and W.~Chen.
\newblock What makes good in-context examples for gpt-3?
\newblock {\em arXiv}, abs/2101.06804, 2021.

\bibitem{schick2020small}
Timo Schick and Hinrich Schütze.
\newblock It's not just size that matters: Small language models are also
  few-shot learners.
\newblock {\em Computing Research Repository}, arXiv:2009.07118, 2020.

\bibitem{perez2021rissanen}
Ethan Perez, Douwe Kiela, and Kyunghyun Cho.
\newblock Rissanen data analysis: Examining dataset characteristics via
  description length.
\newblock In {\em ICML}, 2021.

\bibitem{schick2020few}
Timo Schick and H.~Schutze.
\newblock Few-shot text generation with pattern-exploiting training.
\newblock {\em arXiv}, abs/2012.11926, 2020.

\bibitem{tam2021improving}
Derek Tam, Rakesh~R Menon, Mohit Bansal, Shashank Srivastava, and Colin Raffel.
\newblock Improving and simplifying pattern exploiting training.
\newblock {\em arxiv preprint arXiv:2103.11955}, 2021.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  Gretchen Krueger, and Ilya Sutskever.
\newblock Learning transferable visual models from natural language
  supervision, 2021.

\bibitem{wang2021entailment}
Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma.
\newblock Entailment as few-shot learner, 2021.

\bibitem{scao2021data}
Teven~Le Scao and Alexander~M. Rush.
\newblock How many data points is a prompt worth?, 2021.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv}, abs/1910.01108, 2019.

\bibitem{petroni-etal-2019-language}
Fabio Petroni, Tim Rockt{\"a}schel, Sebastian Riedel, Patrick Lewis, Anton
  Bakhtin, Yuxiang Wu, and Alexander Miller.
\newblock Language models as knowledge bases?
\newblock In {\em EMNLP}, pages 2463--2473, Hong Kong, China, November 2019.
  ACL.

\bibitem{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan
  Wierstra.
\newblock Matching networks for one shot learning.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em NeuRIPS}, volume~29. Curran Associates, Inc., 2016.

\bibitem{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, {\em NeuRIPS}, volume~30. Curran
  Associates, Inc., 2017.

\bibitem{ravi2017optimization}
S.~Ravi and H.~Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In {\em ICLR}, 2017.

\bibitem{li2017learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock {\em arXiv}, abs/1606.01885, 2017.

\bibitem{allend1974relationship}
David~M. Allen.
\newblock The relationship between variable selection and data agumentation and
  a method for prediction.
\newblock {\em Technometrics}, 16(1):125--127, 1974.

\bibitem{stone1974cross}
M.~Stone.
\newblock Cross‐validatory choice and assessment of statistical predictions.
\newblock {\em Journal of the Royal Statistical Society. Series A
  (Methodological)}, 36:111--133, 1974.

\bibitem{geisser1975predictive}
Seymour Geisser.
\newblock The predictive sample reuse method with applications.
\newblock {\em Journal of the American Statistical Association},
  70(350):320--328, 1975.

\bibitem{hastie2001statistical}
Trevor Hastie, Robert Tibshirani, and Jerome Friedman.
\newblock {\em The Elements of Statistical Learning}.
\newblock Springer Series in Statistics. Springer New York Inc., New York, NY,
  USA, 2001.

\bibitem{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em ICML}, volume~70 of {\em ICML’17}, pages 1126--1135.
  JMLR.org, 2017.

\bibitem{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em NeurIPS}, volume~32.
  Curran Associates, Inc., 2019.

\bibitem{rissanen1978modeling}
J.~Rissanen.
\newblock Modeling by shortest data description.
\newblock {\em Automatica}, 14(5):465 -- 471, 1978.

\bibitem{rissanen1984universal}
J.~{Rissanen}.
\newblock Universal coding, information, prediction, and estimation.
\newblock {\em IEEE Transactions on Information Theory}, 30(4):629--636, 1984.

\bibitem{dawid1984present}
A.~P. Dawid.
\newblock Present position and potential developments: Some personal views:
  Statistical theory: The prequential approach.
\newblock {\em Journal of the Royal Statistical Society. Series A (General)},
  147(2):278--292, 1984.

\bibitem{grunwald2004tutorial}
Peter Grünwald.
\newblock A tutorial introduction to the minimum description length principle.
\newblock {\em CoRR}, math.ST/0406077, 06 2004.

\bibitem{blier2018description}
L\'{e}onard Blier and Yann Ollivier.
\newblock The description length of deep learning models.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em NeuRIPS}, volume~31, pages 2216--2226. Curran
  Associates, Inc., 2018.

\bibitem{blumer1987occam}
Alselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred~K. Warmuth.
\newblock Occam's razor.
\newblock {\em Inf. Process. Lett.}, 24(6):377–380, April 1987.

\bibitem{sinha2021masked}
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle Pineau, Adina Williams, and
  Douwe Kiela.
\newblock Masked language modeling and the distributional hypothesis: Order
  word matters pre-training for little.
\newblock {\em CoRR}, abs/2104.06644, 2021.

\bibitem{phillips2000feret}
P.~J. {Phillips}, {Hyeonjoon Moon}, S.~A. {Rizvi}, and P.~J. {Rauss}.
\newblock The feret evaluation methodology for face-recognition algorithms.
\newblock {\em TPAMI}, 22(10):1090--1104, 2000.

\bibitem{buolamwini2018gender}
Joy Buolamwini and Timnit Gebru.
\newblock Gender shades: Intersectional accuracy disparities in commercial
  gender classification.
\newblock In Sorelle~A. Friedler and Christo Wilson, editors, {\em Fairness,
  Accountability and Transparency}, volume~81 of {\em PMLR}, pages 77--91, New
  York, NY, USA, 23--24 Feb 2018. PMLR.

\bibitem{henderson2018ethical}
Peter Henderson, Koustuv Sinha, Nicolas Angelard-Gontier, Nan~Rosemary Ke,
  Genevieve Fried, Ryan Lowe, and Joelle Pineau.
\newblock Ethical challenges in data-driven dialogue systems.
\newblock In {\em AAAI/ACM Conference on AI, Ethics, and Society}, AIES '18,
  page 123–129, New York, NY, USA, 2018. Association for Computing Machinery.

\bibitem{khatri2018advancing}
Chandra Khatri, Behnam Hedayatnia, Anu Venkatesh, Jeff Nunn, Yi~Pan, Qing Liu,
  Han Song, Anna Gottardi, Sanjeev Kwatra, Sanju Pancholi, Ming Cheng, Qinglang
  Chen, Lauren Stubel, Karthik Gopalakrishnan, Kate Bland, Raefer Gabriel,
  Arindam Mandal, Dilek Hakkani{-}T{\"{u}}r, Gene Hwang, Nate Michel, Eric
  King, and Rohit Prasad.
\newblock Advancing the state of the art in open domain dialog systems through
  the alexa prize.
\newblock {\em CoRR}, abs/1812.10757, 2018.

\bibitem{garcia2015comprehensive}
Javier Garc{{\'i}}a, Fern, and o~Fern{{\'a}}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock {\em JMLR}, 16(42):1437--1480, 2015.

\bibitem{amodei2016concrete}
Dario Amodei, Chris Olah, Jacob Steinhardt, Paul~F. Christiano, John Schulman,
  and Dan Man{\'{e}}.
\newblock Concrete problems in {AI} safety.
\newblock {\em CoRR}, abs/1606.06565, 2016.

\bibitem{watanabe2010asymptotic}
Sumio Watanabe.
\newblock Asymptotic equivalence of bayes cross validation and widely
  applicable information criterion in singular learning theory.
\newblock {\em JMLR}, 11(116):3571--3594, 2010.

\bibitem{akaike1974new}
H.~Akaike.
\newblock A new look at the statistical model identification.
\newblock {\em TACON}, 19(6):716--723, 1974.

\bibitem{mallows1973some}
C.~L. Mallows.
\newblock Some comments on cp.
\newblock {\em Technometrics}, 15(4):661--675, 1973.

\bibitem{hutter2011sequential}
Frank Hutter, Holger~H. Hoos, and Kevin Leyton-Brown.
\newblock Sequential model-based optimization for general algorithm
  configuration.
\newblock In Carlos A.~Coello Coello, editor, {\em Learning and Intelligent
  Optimization}, pages 507--523, Berlin, Heidelberg, 2011. Springer Berlin
  Heidelberg.

\bibitem{bergstra2011algorithms}
James Bergstra, R\'{e}mi Bardenet, Yoshua Bengio, and Bal\'{a}zs K\'{e}gl.
\newblock Algorithms for hyper-parameter optimization.
\newblock In J.~Shawe-Taylor, R.~Zemel, P.~Bartlett, F.~Pereira, and K.~Q.
  Weinberger, editors, {\em NeuRIPS}, volume~24. Curran Associates, Inc., 2011.

\bibitem{snoek2012practical}
Jasper Snoek, Hugo Larochelle, and Ryan~P Adams.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In F.~Pereira, C.~J.~C. Burges, L.~Bottou, and K.~Q. Weinberger,
  editors, {\em NeuRIPS}, volume~25. Curran Associates, Inc., 2012.

\bibitem{miikkulainen2019evolving}
Risto Miikkulainen, Jason Liang, Elliot Meyerson, Aditya Rawal, Daniel Fink,
  Olivier Francon, Bala Raju, Hormoz Shahrzad, Arshak Navruzyan, Nigel Duffy,
  and Babak Hodjat.
\newblock Chapter 15 - evolving deep neural networks.
\newblock In Robert Kozma, Cesare Alippi, Yoonsuck Choe, and Francesco~Carlo
  Morabito, editors, {\em Artificial Intelligence in the Age of Neural Networks
  and Brain Computing}, pages 293--312. Academic Press, 2019.

\bibitem{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V. Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock {\em AAAI}, 33(01):4780--4789, Jul. 2019.

\bibitem{zoph2017neural}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock In {\em ICLR}. OpenReview.net, 2017.

\bibitem{larsen1996design}
J.~Larsen, L.K. Hansen, C.~Svarer, and M.~Ohlsson.
\newblock Design and regularization of neural networks: the optimal use of a
  validation set.
\newblock In {\em Neural Networks for Signal Processing VI. IEEE Signal
  Processing Society Workshop}, pages 62--71, 1996.

\bibitem{bengio2000gradient}
Yoshua Bengio.
\newblock {Gradient-Based Optimization of Hyperparameters}.
\newblock {\em Neural Computation}, 12(8):1889--1900, 08 2000.

\bibitem{chapelle2004choosing}
O.~Chapelle, V.~Vapnik, O.~Bousquet, and S.~Mukherjee.
\newblock Choosing multiple parameters for support vector machines.
\newblock {\em Machine Learning}, 46:131--159, 2004.

\bibitem{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock {DARTS}: Differentiable architecture search.
\newblock In {\em ICLR}, 2019.

\bibitem{shin-etal-2020-autoprompt}
Taylor Shin, Yasaman Razeghi, Robert~L. Logan~IV, Eric Wallace, and Sameer
  Singh.
\newblock {A}uto{P}rompt: {E}liciting {K}nowledge from {L}anguage {M}odels with
  {A}utomatically {G}enerated {P}rompts.
\newblock In {\em EMNLP}, pages 4222--4235, Online, November 2020. ACL.

\bibitem{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and
  Jie Tang.
\newblock Gpt understands, too, 2021.

\bibitem{zhong2021factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen.
\newblock Factual probing is {[MASK]:} learning vs. learning to recall.
\newblock {\em CoRR}, abs/2104.05240, 2021.

\bibitem{poerner-etal-2020-e}
Nina Poerner, Ulli Waltinger, and Hinrich Sch{\"u}tze.
\newblock {E}-{BERT}: Efficient-yet-effective entity embeddings for {BERT}.
\newblock In {\em Findings of EMNLP}, pages 803--818, Online, November 2020.
  ACL.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em EMNLP: System Demonstrations}, pages 38--45, Online, October
  2020. ACL.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em NeuRIPS}, pages
  8024--8035. Curran Associates, Inc., 2019.

\bibitem{voita-titov-2020-information}
Elena Voita and Ivan Titov.
\newblock Information-theoretic probing with minimum description length.
\newblock In {\em EMNLP}, pages 183--196, Online, November 2020. ACL.

\bibitem{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em NeuRIPS}, volume~32.
  Curran Associates, Inc., 2019.

\bibitem{lan2020albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock Albert: A lite bert for self-supervised learning of language
  representations.
\newblock In {\em ICLR}, 2020.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em NAACL}, pages 4171--4186, Minneapolis, Minnesota, June 2019.
  ACL.

\bibitem{triantafillou2020metadataset}
Eleni Triantafillou, Tyler Zhu, Vincent Dumoulin, Pascal Lamblin, Utku Evci,
  Kelvin Xu, Ross Goroshin, Carles Gelada, Kevin Swersky, Pierre-Antoine
  Manzagol, and Hugo Larochelle.
\newblock Meta-dataset: A dataset of datasets for learning to learn from few
  examples.
\newblock In {\em ICLR}, 2020.

\bibitem{ye2021crossfit}
Qinyuan Ye, Bill~Yuchen Lin, and Xiang Ren.
\newblock Crossfit: {A} few-shot learning challenge for cross-task
  generalization in {NLP}.
\newblock {\em CoRR}, abs/2104.08835, 2021.

\bibitem{caruana1995learning}
Rich Caruana.
\newblock Learning many related tasks at the same time with backpropagation.
\newblock In G.~Tesauro, D.~Touretzky, and T.~Leen, editors, {\em NeuRIPS},
  volume~7. MIT Press, 1995.

\bibitem{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock {\em Machine Learning}, 28(1):41–75, July 1997.

\bibitem{phang2018stilts}
Jason Phang, Thibault F\'evry, and Samuel~R. Bowman.
\newblock Sentence encoders on stilts: Supplementary training on intermediate
  labeled-data tasks.
\newblock {\em CoRR}, abs/1811.01088, 2018.

\bibitem{liu-etal-2019-multi}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock In {\em ACL}, pages 4487--4496, Florence, Italy, July 2019. ACL.

\bibitem{kocijan-etal-2019-surprisingly}
Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas
  Lukasiewicz.
\newblock A surprisingly robust trick for the {W}inograd schema challenge.
\newblock In {\em ACL}, pages 4837--4842, Florence, Italy, July 2019. ACL.

\bibitem{xie2020unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le.
\newblock Unsupervised data augmentation for consistency training.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em NeuRIPS}, volume~33, pages 6256--6268. Curran Associates, Inc.,
  2020.

\bibitem{chen-etal-2020-mixtext}
Jiaao Chen, Zichao Yang, and Diyi Yang.
\newblock {M}ix{T}ext: Linguistically-informed interpolation of hidden space
  for semi-supervised text classification.
\newblock In {\em ACL}, pages 2147--2157, Online, July 2020. ACL.

\bibitem{yang-etal-2020-generative}
Yiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan
  Le~Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey.
\newblock Generative data augmentation for commonsense reasoning.
\newblock In {\em Findings of EMNLP}, pages 1008--1025, Online, November 2020.
  ACL.

\bibitem{artetxe2018unsupervised-neural}
Mikel Artetxe, Gorka Labaka, Eneko Agirre, and Kyunghyun Cho.
\newblock Unsupervised neural machine translation.
\newblock In {\em ICLR}, 2018.

\bibitem{lample2018unsupervised}
Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and Marc'Aurelio Ranzato.
\newblock Unsupervised machine translation using monolingual corpora only.
\newblock In {\em ICLR}, 2018.

\bibitem{perez-etal-2020-unsupervised}
Ethan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun Cho, and Douwe Kiela.
\newblock Unsupervised question decomposition for question answering.
\newblock In {\em EMNLP}, pages 8864--8880, Online, November 2020. ACL.

\bibitem{oliver2018realistic}
Avital Oliver, Augustus Odena, Colin Raffel, Ekin~D. Cubuk, and Ian~J.
  Goodfellow.
\newblock Realistic evaluation of deep semi-supervised learning algorithms.
\newblock {\em CoRR}, abs/1804.09170, 2018.

\bibitem{dodge-etal-2019-show}
Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, and Noah~A. Smith.
\newblock Show your work: Improved reporting of experimental results.
\newblock In {\em EMNLP}, pages 2185--2194, Hong Kong, China, November 2019.
  ACL.

\bibitem{ettinger-2020-bert}
Allyson Ettinger.
\newblock What {BERT} is not: Lessons from a new suite of psycholinguistic
  diagnostics for language models.
\newblock {\em TACL}, 8:34--48, 2020.

\bibitem{vehtari2017practical}
Aki Vehtari, Andrew Gelman, and Jonah Gabry.
\newblock Practical bayesian model evaluation using leave-one-out
  cross-validation and waic.
\newblock {\em Statistics and Computing}, 27(5):1413–1432, September 2017.

\bibitem{dodge2020finetuning}
Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi,
  and Noah~A. Smith.
\newblock Fine-tuning pretrained language models: Weight initializations, data
  orders, and early stopping.
\newblock {\em CoRR}, abs/2002.06305, 2020.

\bibitem{clark-etal-2019-boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock {B}ool{Q}: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock In {\em NAACL}, pages 2924--2936, Minneapolis, Minnesota, June 2019.
  ACL.

\bibitem{de2019commitment}
Marie-Catherine de~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock The commitmentbank: Investigating projection in naturally occurring
  discourse.
\newblock {\em Sinn und Bedeutung}, 23(2):107--124, July 2019.

\bibitem{dagan2006pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The pascal recognising textual entailment challenge.
\newblock In Joaquin Qui{\~{n}}onero-Candela, Ido Dagan, Bernardo Magnini, and
  Florence d'Alch{\'e} Buc, editors, {\em Machine Learning Challenges.
  Evaluating Predictive Uncertainty, Visual Object Classification, and
  Recognising Tectual Entailment}, pages 177--190, Berlin, Heidelberg, 2006.
  Springer Berlin Heidelberg.

\bibitem{bar2006second}
Roy Bar~Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo
  Magnini, and Idan Szpektor.
\newblock The second {PASCAL} recognising textual entailment challenge.
\newblock In {\em Second {PASCAL} Challenges Workshop on Recognising Textual
  Entailment}, 2006.

\bibitem{giampiccolo2007pascal}
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan.
\newblock The third pascal recognizing textual entailment challenge.
\newblock In {\em ACL-PASCAL Workshop on Textual Entailment and Paraphrasing},
  RTE '07, page 1–9, USA, 2007. ACL.

\bibitem{bentivogli2009fifth}
Luisa Bentivogli, Ido Dagan, Hoa~Trang Dang, Danilo Giampiccolo, and Bernardo
  Magnini.
\newblock The fifth pascal recognizing textual entailment challenge.
\newblock In {\em TAC}, 2009.

\bibitem{pilehvar2018wic}
Mohammad~Taher Pilehvar and Jose Camacho-Collados.
\newblock {WiC}: The word-in-context dataset for evaluating context-sensitive
  meaning representations.
\newblock In {\em NAACL}. ACL, 2019.

\bibitem{levesque2012winograd}
Hector~J. Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The winograd schema challenge.
\newblock In {\em KR}, KR'12, page 552–561. AAAI Press, 2012.

\bibitem{khashabi2018looking}
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan
  Roth.
\newblock Looking beyond the surface: A challenge set for reading comprehension
  over multiple sentences.
\newblock In {\em NAACL}. ACL, 2018.

\bibitem{zhang2018record}
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin
  Van~Durme.
\newblock {R}e{C}o{RD}: Bridging the gap between human and machine commonsense
  reading comprehension.
\newblock {\em arXiv preprint 1810.12885}, 2018.

\end{thebibliography}
