\begin{thebibliography}{}

\bibitem[\protect\astroncite{A.~Angelova}{2012}]{angelova2012moments}
A.~Angelova, J.\leavevmode\nopagebreak\newline 2012.
\newblock On moments of sample mean and variance.
\newblock {\em International Journal of Pure and Applied Mathematics}, 79.

\bibitem[\protect\astroncite{Agakov and Barber}{2004}]{agakov2004auxiliary}
Agakov, F.~V. and D.~Barber\leavevmode\nopagebreak\newline 2004.
\newblock An auxiliary variational method.
\newblock In {\em International Conference on Neural Information Processing},
  Pp.~ 561--566. Springer.

\bibitem[\protect\astroncite{Atanov et~al.}{2019}]{atanov2018the}
Atanov, A., A.~Ashukha, K.~Struminsky, D.~Vetrov, and
  M.~Welling\leavevmode\nopagebreak\newline 2019.
\newblock The deep weight prior.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\astroncite{Burda et~al.}{2015}]{burda2015importance}
Burda, Y., R.~Grosse, and R.~Salakhutdinov\leavevmode\nopagebreak\newline 2015.
\newblock Importance weighted autoencoders.
\newblock {\em arXiv preprint arXiv:1509.00519}.

\bibitem[\protect\astroncite{Chen et~al.}{2016}]{chen2016variational}
Chen, X., D.~P. Kingma, T.~Salimans, Y.~Duan, P.~Dhariwal, J.~Schulman,
  I.~Sutskever, and P.~Abbeel\leavevmode\nopagebreak\newline 2016.
\newblock Variational lossy autoencoder.
\newblock {\em arXiv preprint arXiv:1611.02731}.

\bibitem[\protect\astroncite{Dieng et~al.}{2017}]{dieng2017variational}
Dieng, A.~B., D.~Tran, R.~Ranganath, J.~Paisley, and
  D.~Blei\leavevmode\nopagebreak\newline 2017.
\newblock Variational inference via $\chi$ upper bound minimization.
\newblock In {\em Advances in Neural Information Processing Systems}, Pp.~
  2732--2741.

\bibitem[\protect\astroncite{Dillon et~al.}{2017}]{dillon2017tensorflow}
Dillon, J.~V., I.~Langmore, D.~Tran, E.~Brevdo, S.~Vasudevan, D.~Moore,
  B.~Patton, A.~Alemi, M.~Hoffman, and R.~A.
  Saurous\leavevmode\nopagebreak\newline 2017.
\newblock Tensorflow distributions.
\newblock {\em arXiv preprint arXiv:1711.10604}.

\bibitem[\protect\astroncite{Dinh et~al.}{2016}]{DBLP:journals/corr/DinhSB16}
Dinh, L., J.~Sohl{-}Dickstein, and S.~Bengio\leavevmode\nopagebreak\newline
  2016.
\newblock Density estimation using real {NVP}.
\newblock {\em CoRR}, abs/1605.08803.

\bibitem[\protect\astroncite{Domke and Sheldon}{2018}]{domke2018importance}
Domke, J. and D.~R. Sheldon\leavevmode\nopagebreak\newline 2018.
\newblock Importance weighting and variational inference.
\newblock In {\em Advances in Neural Information Processing Systems 31},
  S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and
  R.~Garnett, eds., Pp.~ 4471--4480.
\newblock Curran Associates, Inc.

\bibitem[\protect\astroncite{Goodfellow
  et~al.}{2014}]{goodfellow2014generative}
Goodfellow, I., J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio\leavevmode\nopagebreak\newline 2014.
\newblock Generative adversarial nets.
\newblock In {\em Advances in neural information processing systems}, Pp.~
  2672--2680.

\bibitem[\protect\astroncite{Grosse et~al.}{2015}]{Grosse2015SandwichingTM}
Grosse, R.~B., Z.~Ghahramani, and R.~P. Adams\leavevmode\nopagebreak\newline
  2015.
\newblock Sandwiching the marginal likelihood using bidirectional monte carlo.
\newblock {\em CoRR}, abs/1511.02543.

\bibitem[\protect\astroncite{Guo et~al.}{2016}]{guo2016boosting}
Guo, F., X.~Wang, K.~Fan, T.~Broderick, and D.~B.
  Dunson\leavevmode\nopagebreak\newline 2016.
\newblock Boosting variational inference.
\newblock {\em arXiv preprint arXiv:1611.05559}.

\bibitem[\protect\astroncite{Hinton and van
  Camp}{1993}]{Hinton:1993:KNN:168304.168306}
Hinton, G.~E. and D.~van Camp\leavevmode\nopagebreak\newline 1993.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In {\em Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, COLT '93, Pp.~ 5--13, New York, NY, USA. ACM.

\bibitem[\protect\astroncite{Husz{\'a}r}{2017}]{huszar2017variational}
Husz{\'a}r, F.\leavevmode\nopagebreak\newline 2017.
\newblock Variational inference using implicit distributions.
\newblock {\em arXiv preprint arXiv:1702.08235}.

\bibitem[\protect\astroncite{Jebara and Pentland}{2001}]{jebara2001reversing}
Jebara, T. and A.~Pentland\leavevmode\nopagebreak\newline 2001.
\newblock On reversing jensen's inequality.
\newblock In {\em Advances in Neural Information Processing Systems}, Pp.~
  231--237.

\bibitem[\protect\astroncite{Kim et~al.}{2018}]{pmlr-v80-kim18e}
Kim, Y., S.~Wiseman, A.~Miller, D.~Sontag, and
  A.~Rush\leavevmode\nopagebreak\newline 2018.
\newblock Semi-amortized variational autoencoders.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, J.~Dy and A.~Krause, eds., volume~80 of {\em Proceedings of
  Machine Learning Research}, Pp.~ 2678--2687, Stockholmsmässan, Stockholm
  Sweden. PMLR.

\bibitem[\protect\astroncite{Kingma and
  Ba}{2014}]{DBLP:journals/corr/KingmaB14}
Kingma, D.~P. and J.~Ba\leavevmode\nopagebreak\newline 2014.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980.

\bibitem[\protect\astroncite{Kingma and Dhariwal}{2018}]{kingma2018glow}
Kingma, D.~P. and P.~Dhariwal\leavevmode\nopagebreak\newline 2018.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock In {\em Advances in Neural Information Processing Systems 31},
  S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi, and
  R.~Garnett, eds., Pp.~ 10235--10244.
\newblock Curran Associates, Inc.

\bibitem[\protect\astroncite{Kingma et~al.}{2016}]{NIPS2016_6581}
Kingma, D.~P., T.~Salimans, R.~Jozefowicz, X.~Chen, I.~Sutskever, and
  M.~Welling\leavevmode\nopagebreak\newline 2016.
\newblock Improved variational inference with inverse autoregressive flow.
\newblock In {\em Advances in Neural Information Processing Systems 29}, D.~D.
  Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett, eds., Pp.~
  4743--4751.
\newblock Curran Associates, Inc.

\bibitem[\protect\astroncite{Kingma and Welling}{2013}]{kingma2013auto}
Kingma, D.~P. and M.~Welling\leavevmode\nopagebreak\newline 2013.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}.

\bibitem[\protect\astroncite{Kuleshov and Ermon}{2017}]{kuleshov2017neural}
Kuleshov, V. and S.~Ermon\leavevmode\nopagebreak\newline 2017.
\newblock Neural variational inference and learning in undirected graphical
  models.
\newblock In {\em Advances in Neural Information Processing Systems}, Pp.~
  6734--6743.

\bibitem[\protect\astroncite{Lake et~al.}{2015}]{lake2015human}
Lake, B.~M., R.~Salakhutdinov, and J.~B.
  Tenenbaum\leavevmode\nopagebreak\newline 2015.
\newblock Human-level concept learning through probabilistic program induction.
\newblock {\em Science}, 350(6266):1332--1338.

\bibitem[\protect\astroncite{LeCun et~al.}{1998}]{lecun1998gradient}
LeCun, Y., L.~Bottou, Y.~Bengio, and P.~Haffner\leavevmode\nopagebreak\newline
  1998.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324.

\bibitem[\protect\astroncite{Louizos et~al.}{2017}]{NIPS2017_6921}
Louizos, C., K.~Ullrich, and M.~Welling\leavevmode\nopagebreak\newline 2017.
\newblock Bayesian compression for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems 30},
  I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan,
  and R.~Garnett, eds., Pp.~ 3288--3298.
\newblock Curran Associates, Inc.

\bibitem[\protect\astroncite{Maaløe et~al.}{2016}]{pmlr-v48-maaloe16}
Maaløe, L., C.~K. Sønderby, S.~K. Sønderby, and
  O.~Winther\leavevmode\nopagebreak\newline 2016.
\newblock Auxiliary deep generative models.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning}, M.~F. Balcan and K.~Q. Weinberger, eds., volume~48 of {\em
  Proceedings of Machine Learning Research}, Pp.~ 1445--1453, New York, New
  York, USA. PMLR.

\bibitem[\protect\astroncite{MacKay}{1995}]{MACKAY199573}
MacKay, D.~J.\leavevmode\nopagebreak\newline 1995.
\newblock Bayesian neural networks and density networks.
\newblock {\em Nuclear Instruments and Methods in Physics Research Section A:
  Accelerators, Spectrometers, Detectors and Associated Equipment}, 354(1):73
  -- 80.
\newblock Proceedings of the Third Workshop on Neutron Scattering Data
  Analysis.

\bibitem[\protect\astroncite{Mescheder et~al.}{2017}]{mescheder2017adversarial}
Mescheder, L., S.~Nowozin, and A.~Geiger\leavevmode\nopagebreak\newline 2017.
\newblock Adversarial variational bayes: Unifying variational autoencoders and
  generative adversarial networks.
\newblock In {\em International Conference on Machine Learning (ICML)}.

\bibitem[\protect\astroncite{Mohamed and
  Lakshminarayanan}{2016}]{mohamed2016learning}
Mohamed, S. and B.~Lakshminarayanan\leavevmode\nopagebreak\newline 2016.
\newblock Learning in implicit generative models.
\newblock {\em arXiv preprint arXiv:1610.03483}.

\bibitem[\protect\astroncite{Molchanov et~al.}{2018}]{molchanov2018doubly}
Molchanov, D., V.~Kharitonov, A.~Sobolev, and
  D.~Vetrov\leavevmode\nopagebreak\newline 2018.
\newblock Doubly semi-implicit variational inference.
\newblock {\em arXiv preprint arXiv:1810.02789}.

\bibitem[\protect\astroncite{Neal}{2001}]{neal2001annealed}
Neal, R.~M.\leavevmode\nopagebreak\newline 2001.
\newblock Annealed importance sampling.
\newblock {\em Statistics and computing}, 11(2):125--139.

\bibitem[\protect\astroncite{Nowozin}{2018}]{nowozin2018debiasing}
Nowozin, S.\leavevmode\nopagebreak\newline 2018.
\newblock Debiasing evidence approximations: On importance-weighted
  autoencoders and jackknife variational inference.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\astroncite{Nowozin et~al.}{2016}]{NIPS2016_6066}
Nowozin, S., B.~Cseke, and R.~Tomioka\leavevmode\nopagebreak\newline 2016.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In {\em Advances in Neural Information Processing Systems 29}, D.~D.
  Lee, M.~Sugiyama, U.~V. Luxburg, I.~Guyon, and R.~Garnett, eds., Pp.~
  271--279.
\newblock Curran Associates, Inc.

\bibitem[\protect\astroncite{Papamakarios
  et~al.}{2017}]{DBLP:conf/nips/PapamakariosMP17}
Papamakarios, G., I.~Murray, and T.~Pavlakou\leavevmode\nopagebreak\newline
  2017.
\newblock Masked autoregressive flow for density estimation.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, 4-9 December 2017,
  Long Beach, CA, {USA}}, Pp.~ 2335--2344.

\bibitem[\protect\astroncite{Rainforth et~al.}{2018}]{Rainforth2018TighterVB}
Rainforth, T., A.~R. Kosiorek, T.~A. Le, C.~J. Maddison, M.~Igl, F.~Wood, and
  Y.~W. Teh\leavevmode\nopagebreak\newline 2018.
\newblock Tighter variational bounds are not necessarily better.
\newblock In {\em ICML}.

\bibitem[\protect\astroncite{Ranganath
  et~al.}{2016}]{ranganath2016hierarchical}
Ranganath, R., D.~Tran, and D.~Blei\leavevmode\nopagebreak\newline 2016.
\newblock Hierarchical variational models.
\newblock In {\em International Conference on Machine Learning}, Pp.~ 324--333.

\bibitem[\protect\astroncite{Reddi et~al.}{2018}]{j.2018on}
Reddi, S.~J., S.~Kale, and S.~Kumar\leavevmode\nopagebreak\newline 2018.
\newblock On the convergence of adam and beyond.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\astroncite{Rezende and
  Mohamed}{2015}]{rezende2015variational}
Rezende, D.~J. and S.~Mohamed\leavevmode\nopagebreak\newline 2015.
\newblock Variational inference with normalizing flows.
\newblock {\em arXiv preprint arXiv:1505.05770}.

\bibitem[\protect\astroncite{Rezende et~al.}{2014}]{pmlr-v32-rezende14}
Rezende, D.~J., S.~Mohamed, and D.~Wierstra\leavevmode\nopagebreak\newline
  2014.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning}, E.~P. Xing and T.~Jebara, eds., volume~32 of {\em Proceedings of
  Machine Learning Research}, Pp.~ 1278--1286, Bejing, China. PMLR.

\bibitem[\protect\astroncite{Salimans et~al.}{2015}]{salimans2015markov}
Salimans, T., D.~Kingma, and M.~Welling\leavevmode\nopagebreak\newline 2015.
\newblock Markov chain monte carlo and variational inference: Bridging the gap.
\newblock In {\em International Conference on Machine Learning}, Pp.~
  1218--1226.

\bibitem[\protect\astroncite{Sharot}{1976}]{sharot1976generalized}
Sharot, T.\leavevmode\nopagebreak\newline 1976.
\newblock The generalized jackknife: finite samples and subsample sizes.
\newblock {\em Journal of the American Statistical Association},
  71(354):451--454.

\bibitem[\protect\astroncite{Shi et~al.}{2017}]{shi2017kernel}
Shi, J., S.~Sun, and J.~Zhu\leavevmode\nopagebreak\newline 2017.
\newblock Kernel implicit variational inference.
\newblock {\em arXiv preprint arXiv:1705.10119}.

\bibitem[\protect\astroncite{Titsias and Ruiz}{2018}]{titsias2018unbiased}
Titsias, M.~K. and F.~J. Ruiz\leavevmode\nopagebreak\newline 2018.
\newblock Unbiased implicit variational inference.
\newblock {\em arXiv preprint arXiv:1808.02078}.

\bibitem[\protect\astroncite{Tran et~al.}{2015}]{tran2015copula}
Tran, D., D.~Blei, and E.~M. Airoldi\leavevmode\nopagebreak\newline 2015.
\newblock Copula variational inference.
\newblock In {\em Advances in Neural Information Processing Systems}, Pp.~
  3564--3572.

\bibitem[\protect\astroncite{Tucker et~al.}{2019}]{tucker2018doubly}
Tucker, G., D.~Lawson, S.~Gu, and C.~J. Maddison\leavevmode\nopagebreak\newline
  2019.
\newblock Doubly reparameterized gradient estimators for monte carlo
  objectives.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[\protect\astroncite{Uehara et~al.}{2016}]{uehara2016generative}
Uehara, M., I.~Sato, M.~Suzuki, K.~Nakayama, and
  Y.~Matsuo\leavevmode\nopagebreak\newline 2016.
\newblock Generative adversarial nets from a density ratio estimation
  perspective.
\newblock {\em arXiv preprint arXiv:1610.02920}.

\bibitem[\protect\astroncite{Wainwright et~al.}{2008}]{wainwright2008graphical}
Wainwright, M.~J., M.~I. Jordan, et~al.\leavevmode\nopagebreak\newline 2008.
\newblock Graphical models, exponential families, and variational inference.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  1(1--2):1--305.

\bibitem[\protect\astroncite{Waterhouse et~al.}{1996}]{waterhouse1996bayesian}
Waterhouse, S.~R., D.~MacKay, and A.~J. Robinson\leavevmode\nopagebreak\newline
  1996.
\newblock Bayesian methods for mixtures of experts.
\newblock In {\em Advances in neural information processing systems}, Pp.~
  351--357.

\bibitem[\protect\astroncite{Williams}{1992}]{Williams92simplestatistical}
Williams, R.~J.\leavevmode\nopagebreak\newline 1992.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock In {\em Machine Learning}, Pp.~ 229--256.

\bibitem[\protect\astroncite{Yin and Zhou}{2018}]{yin2018semi}
Yin, M. and M.~Zhou\leavevmode\nopagebreak\newline 2018.
\newblock Semi-implicit variational inference.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of {\em Proceedings of Machine Learning Research}, Pp.~
  5660--5669. PMLR.

\end{thebibliography}
