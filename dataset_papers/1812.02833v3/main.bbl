\begin{thebibliography}{59}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille and Soatto(2019)]{achille19emergence}
Alessandro Achille and Stefano Soatto.
\newblock Emergence of invariance and disentanglement in deep representations.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0 (50), 2019.

\bibitem[Alemi et~al.(2018)Alemi, Poole, Fischer, Dillon, Saurous, and
  Murphy]{alemi2018fixing}
Alexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon, Rif~A Saurous, and
  Kevin Murphy.
\newblock Fixing a broken {ELBO}.
\newblock In \emph{International Conference on Machine Learning}, pages
  159--168, 2018.

\bibitem[Alemi et~al.(2017)Alemi, Fischer, Dillon, and Murphy]{alemi2016deep}
Alexander~A Alemi, Ian Fischer, Joshua~V Dillon, and Kevin Murphy.
\newblock Deep variational information bottleneck.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Ansari and Soh(2019)]{Ansari:2018tf}
Abdul~Fatir Ansari and Harold Soh.
\newblock Hyperprior induced unsupervised disentanglement of latent
  representations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2019.

\bibitem[Bengio et~al.(2013)Bengio, Courville, and Vincent]{Bengio2013}
Yoshua Bengio, Aaron Courville, and Pascal Vincent.
\newblock Representation learning: A review and new perspectives.
\newblock \emph{IEEE Trans. Pattern Anal. Mach. Intell.}, 35\penalty0
  (8):\penalty0 1798--1828, August 2013.
\newblock ISSN 0162-8828.

\bibitem[Bouchacourt et~al.(2018)Bouchacourt, Tomioka, and
  Nowozin]{BouchacourtTN18}
Diane Bouchacourt, Ryota Tomioka, and Sebastian Nowozin.
\newblock Multi-level variational autoencoder: Learning disentangled
  representations from grouped observations.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2018.

\bibitem[Burgess et~al.(2018)Burgess, Higgins, Pal, Matthey, Watters,
  Desjardins, and Lerchner]{DBLP:journals/corr/abs-1804-03599}
Christopher~P. Burgess, Irina Higgins, Arka Pal, Lo{\"{\i}}c Matthey, Nick
  Watters, Guillaume Desjardins, and Alexander Lerchner.
\newblock Understanding disentangling in {\(\beta\)}-vae.
\newblock \emph{CoRR}, abs/1804.03599, 2018.

\bibitem[Chen et~al.(2018)Chen, Li, Grosse, and Duvenaud]{chen2018isolating}
Ricky T.~Q. Chen, Xuechen Li, Roger Grosse, and David Duvenaud.
\newblock Isolating sources of disentanglement in variational autoencoders.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Chen et~al.(2016)Chen, Duan, Houthooft, Schulman, Sutskever, and
  Abbeel]{chen2016infogan}
Xi~Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Infogan: Interpretable representation learning by information
  maximizing generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2172--2180, 2016.

\bibitem[Chen et~al.(2017)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{Chen2016wm}
Xi~Chen, Diederik~P Kingma, Tim Salimans, Yan Duan, Prafulla Dhariwal, John
  Schulman, Ilya Sutskever, and Pieter Abbeel.
\newblock {Variational Lossy Autoencoder}.
\newblock 2017.

\bibitem[Cheung et~al.(2014)Cheung, Livezey, Bansal, and
  Olshausen]{cheung2014discovering}
Brian Cheung, Jesse~A Livezey, Arjun~K Bansal, and Bruno~A Olshausen.
\newblock Discovering hidden factors of variation in deep networks.
\newblock \emph{arXiv preprint arXiv:1412.6583}, 2014.

\bibitem[Coates and Ng(2011)]{conf/icml/CoatesN11}
Adam Coates and Andrew~Y. Ng.
\newblock The importance of encoding versus training with sparse coding and
  vector quantization.
\newblock In Lise Getoor and Tobias Scheffer, editors, \emph{ICML}, pages
  921--928. Omnipress, 2011.

\bibitem[Dilokthanakul et~al.(2019)Dilokthanakul, Pawlowski, and
  Shanahan]{dilokthanakul2019explicit}
Nat Dilokthanakul, Nick Pawlowski, and Murray Shanahan.
\newblock Explicit information placement on latent variables using auxiliary
  generative modelling task, 2019.
\newblock URL \url{https://openreview.net/forum?id=H1l-SjA5t7}.

\bibitem[Domke and Sheldon(2018)]{domke2018importance}
Justin Domke and Daniel Sheldon.
\newblock Importance weighting and varational inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4471--â€“4480, 2018.

\bibitem[Eastwood and Williams(2018)]{eastwood2018a}
Cian Eastwood and Christopher K.~I. Williams.
\newblock A framework for the quantitative evaluation of disentangled
  representations.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Esmaeili et~al.(2019)Esmaeili, Wu, Jain, Siddharth, Paige, and van~de
  Meent]{Esmaeili2018up}
Babak Esmaeili, Hao Wu, Sarthak Jain, N~Siddharth, Brooks Paige, and Jan-Willem
  van~de Meent.
\newblock {Hierarchical Disentangled Representations}.
\newblock \emph{Artificial Intelligence and Statistics}, 2019.

\bibitem[Higgins et~al.(2016)Higgins, Matthey, Pal, Burgess, Glorot, Botvinick,
  Mohamed, and Lerchner]{higgins2016beta}
Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot,
  Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner.
\newblock beta-{VAE}: {L}earning basic visual concepts with a constrained
  variational framework.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2016.

\bibitem[Higgins et~al.(2018)Higgins, Amos, Pfau, Racaniere, Matthey, Rezende,
  and Lerchner]{higgins2018towards}
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey,
  Danilo Rezende, and Alexander Lerchner.
\newblock Towards a definition of disentangled representations.
\newblock \emph{arXiv preprint arXiv:1812.02230}, 2018.

\bibitem[Hjelm et~al.(2019)Hjelm, Fedorov, Lavoie-Marchildon, Grewal, Bachman,
  Trischler, and Bengio]{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Hoffman and Johnson(2016)]{Hoffman2016vz}
Matthew~D Hoffman and Matthew~J Johnson.
\newblock {ELBO surgery: yet another way to carve up the variational evidence
  lower bound}.
\newblock In \emph{Workshop on Advances in Approximate Bayesian Inference,
  NIPS}, pages 1--4, 2016.

\bibitem[Hoffman et~al.(2017)Hoffman, Riquelme, and Johnson]{Hoffman2017uq}
Matthew~D Hoffman, Carlos Riquelme, and Matthew~J Johnson.
\newblock {The $\beta$-VAE{\textquoteright}s Implicit Prior}.
\newblock In \emph{Workshop on Bayesian Deep Learning, NIPS}, pages 1--5, 2017.

\bibitem[Hurley and Rickard(2008)]{Hurley2008ComparingMO}
Niall~P. Hurley and Scott~T. Rickard.
\newblock Comparing measures of sparsity.
\newblock \emph{IEEE Transactions on Information Theory}, 55:\penalty0
  4723--4741, 2008.

\bibitem[Hyv{\"a}rinen and Oja(2000)]{hyvarinen2000independent}
Aapo Hyv{\"a}rinen and Erkki Oja.
\newblock Independent component analysis: algorithms and applications.
\newblock \emph{Neural networks}, 13\penalty0 (4-5):\penalty0 411--430, 2000.

\bibitem[Johnson et~al.(2016)Johnson, Duvenaud, Wiltschko, Adams, and
  Datta]{Johnson:2016ud}
Matthew Johnson, David~K Duvenaud, Alex Wiltschko, Ryan~P Adams, and Sandeep~R
  Datta.
\newblock Composing graphical models with neural networks for structured
  representations and fast inference.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2946--2954. 2016.

\bibitem[Kim and Mnih(2018)]{Hyunjik2018}
Hyunjik Kim and Andriy Mnih.
\newblock Disentangling by factorising.
\newblock In \emph{International Conference on Machine Learning}, 2018.

\bibitem[Kingma and Ba(2015)]{Kingma:2014us}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kingma and Welling(2014)]{KingmaW13}
Diederik~P. Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock In \emph{International Conference on Learning Representations}, 2014.

\bibitem[Kingma et~al.(2014)Kingma, Mohamed, Rezende, and
  Welling]{kingma2014semi}
Diederik~P Kingma, Shakir Mohamed, Danilo~Jimenez Rezende, and Max Welling.
\newblock Semi-supervised learning with deep generative models.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2014.

\bibitem[Kolouri et~al.(2019)Kolouri, Pope, Martin, and Rohde]{Kolouri:2018vo}
Soheil Kolouri, Phillip~E. Pope, Charles~E. Martin, and Gustavo~K. Rohde.
\newblock Sliced wasserstein auto-encoders.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Kumar et~al.(2017)Kumar, Sattigeri, and Balakrishnan]{Kumar:2017vs}
Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan.
\newblock {Variational Inference of Disentangled Latent Concepts from Unlabeled
  Observations}.
\newblock \emph{arXiv.org}, November 2017.

\bibitem[Larochelle and Bengio(2008)]{Larochelle:2008:CUD:1390156.1390224}
Hugo Larochelle and Yoshua Bengio.
\newblock Classification using discriminative restricted boltzmann machines.
\newblock In \emph{International Conference on Machine Learning}, pages
  536--543, New York, NY, USA, 2008. ACM.
\newblock ISBN 978-1-60558-205-4.

\bibitem[Lee et~al.(2007)Lee, Battle, Raina, and Ng]{SparseCoding}
Honglak Lee, Alexis Battle, Rajat Raina, and Andrew~Y. Ng.
\newblock Efficient sparse coding algorithms.
\newblock In B.~Sch\"{o}lkopf, J.~C. Platt, and T.~Hoffman, editors,
  \emph{Advances in Neural Information Processing Systems}, pages 801--808. MIT
  Press, 2007.

\bibitem[Lipton(2016)]{lipton2016mythos}
Zachary~C Lipton.
\newblock The mythos of model interpretability.
\newblock \emph{arXiv preprint arXiv:1606.03490}, 2016.

\bibitem[Locatello et~al.(2019)Locatello, Bauer, Lucic, Gelly, Sch{\"o}lkopf,
  and Bachem]{locatello2018challenging}
Francesco Locatello, Stefan Bauer, Mario Lucic, Sylvain Gelly, Bernhard
  Sch{\"o}lkopf, and Olivier Bachem.
\newblock Challenging common assumptions in the unsupervised learning of
  disentangled representations.
\newblock \emph{International Conference on Machine Learning}, 2019.

\bibitem[Maddison et~al.(2017)Maddison, Lawson, Tucker, Heess, Norouzi, Mnih,
  Doucet, and Teh]{maddison2017filtering}
Chris~J Maddison, John Lawson, George Tucker, Nicolas Heess, Mohammad Norouzi,
  Andriy Mnih, Arnaud Doucet, and Yee Teh.
\newblock Filtering variational objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6573--6583, 2017.

\bibitem[Makhzani et~al.(2015)Makhzani, Shlens, Jaitly, Goodfellow, and
  Frey]{makhzani2015adversarial}
Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, and Brendan
  Frey.
\newblock Adversarial autoencoders.
\newblock \emph{arXiv preprint arXiv:1511.05644}, 2015.

\bibitem[Mathieu et~al.(2016)Mathieu, Zhao, Zhao, Ramesh, Sprechmann, and
  LeCun]{mathieu2016disentangling}
Michael~F Mathieu, Junbo~Jake Zhao, Junbo Zhao, Aditya Ramesh, Pablo
  Sprechmann, and Yann LeCun.
\newblock Disentangling factors of variation in deep representation using
  adversarial training.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5040--5048, 2016.

\bibitem[Matthey et~al.(2017)Matthey, Higgins, Hassabis, and
  Lerchner]{dsprites17}
Loic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner.
\newblock dsprites: Disentanglement testing sprites dataset.
\newblock https://github.com/deepmind/dsprites-dataset/, 2017.

\bibitem[Olshausen and Field(1996)]{olshausen96}
B.~Olshausen and D.~Field.
\newblock Emergence of simple-cell receptive field properties by learning a
  sparse code for natural images.
\newblock \emph{Nature}, 381:\penalty0 607--609, 1996.

\bibitem[Petersen et~al.(2008)Petersen, Pedersen, et~al.]{petersen2008matrix}
Kaare~Brandt Petersen, Michael~Syskind Pedersen, et~al.
\newblock The matrix cookbook.
\newblock \emph{Technical University of Denmark}, 7\penalty0 (15):\penalty0
  510, 2008.

\bibitem[Phuong et~al.(2018)Phuong, Welling, Kushman, Tomioka, and
  Nowozin]{phuong2018the}
Mary Phuong, Max Welling, Nate Kushman, Ryota Tomioka, and Sebastian Nowozin.
\newblock The mutual autoencoder: Controlling information in latent code
  representations, 2018.
\newblock URL \url{https://openreview.net/forum?id=HkbmWqxCZ}.

\bibitem[Radford et~al.(2016)Radford, Metz, and
  Chintala]{DBLP:journals/corr/RadfordMC15}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Rainforth et~al.(2018{\natexlab{a}})Rainforth, Cornish, Yang,
  Warrington, and Wood]{rainforth2018nesting}
Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, and Frank
  Wood.
\newblock On nesting monte carlo estimators.
\newblock In \emph{International Conference on Machine Learning}, pages
  4264--4273, 2018{\natexlab{a}}.

\bibitem[Rainforth et~al.(2018{\natexlab{b}})Rainforth, Kosiorek, Le, Maddison,
  Igl, Wood, and Teh]{rainforth2018tighter}
Tom Rainforth, Adam~R. Kosiorek, Tuan~Anh Le, Chris~J. Maddison, Maximilian
  Igl, Frank Wood, and Yee~Whye Teh.
\newblock Tighter variational bounds are not necessarily better.
\newblock \emph{International Conference on Machine Learning},
  2018{\natexlab{b}}.

\bibitem[Ranzato et~al.(2007)Ranzato, Poultney, Chopra, and Cun]{razanto2007}
Marc Ranzato, Christopher Poultney, Sumit Chopra, and Yann~L. Cun.
\newblock Efficient learning of sparse representations with an energy-based
  model.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1137--1144. MIT Press, 2007.

\bibitem[Reddi et~al.(2018)Reddi, Kale, and Kumar]{Reddi:2018wc}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Reed et~al.(2014)Reed, Sohn, Zhang, and Lee]{reed2014learning}
Scott Reed, Kihyuk Sohn, Yuting Zhang, and Honglak Lee.
\newblock Learning to disentangle factors of variation with manifold
  interaction.
\newblock In \emph{International Conference on Machine Learning}, pages
  1431--1439, 2014.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Danilo~Jimenez Rezende, Shakir Mohamed, and Daan Wierstra.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock 2014.

\bibitem[Rolinek et~al.(2018)Rolinek, Zietlow, and
  Martius]{rolinek2018variational}
Michal Rolinek, Dominik Zietlow, and Georg Martius.
\newblock {V}ariational {A}utoencoders {P}ursue {PCA} {D}irections (by
  {A}ccident).
\newblock \emph{arXiv preprint arXiv:1812.06775}, 2018.

\bibitem[Schmidhuber(1992)]{schmidhuber1992learning}
J{\"u}rgen Schmidhuber.
\newblock Learning factorial codes by predictability minimization.
\newblock \emph{Neural Computation}, 4\penalty0 (6):\penalty0 863--879, 1992.

\bibitem[Siddharth et~al.(2017)Siddharth, Paige, Van~de Meent, Desmaison,
  Goodman, Kohli, Wood, and Torr]{siddharth2017learning}
N.~Siddharth, T~Brooks Paige, Jan-Willem Van~de Meent, Alban Desmaison, Noah
  Goodman, Pushmeet Kohli, Frank Wood, and Philip Torr.
\newblock Learning disentangled representations with semi-supervised deep
  generative models.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5925--5935, 2017.

\bibitem[StÃ¼hmer et~al.(2019)StÃ¼hmer, Turner, and Nowozin]{stuhmer2019isavae}
Jan StÃ¼hmer, Richard Turner, and Sebastian Nowozin.
\newblock {ISA}-{VAE}: Independent subspace analysis with variational
  autoencoders, 2019.
\newblock URL \url{https://openreview.net/forum?id=rJl_NhR9K7}.

\bibitem[Tolstikhin et~al.(2018)Tolstikhin, Bousquet, Gelly, and
  Schoelkopf]{Tolstikhin:2017wy}
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf.
\newblock Wasserstein auto-encoders.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Tonolini et~al.(2019)Tonolini, Jensen, and
  Murray-Smith]{tonolini2019variational}
Francesco Tonolini, Bjorn~Sand Jensen, and Roderick Murray-Smith.
\newblock Variational sparse coding, 2019.
\newblock URL \url{https://openreview.net/forum?id=SkeJ6iR9Km}.

\bibitem[Turner and Sahani(2011)]{turner2011two}
Richard~E. Turner and Maneesh Sahani.
\newblock Two problems with variational expectation maximisation for
  time-series models.
\newblock \emph{D. Barber, T. Cemgil, and S. Chiappa (eds.), Bayesian Time
  series models, chapter 5}, page 109â€“130, 2011.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017/online}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem[Xu and Durrett(2018)]{Xu:2018tl}
Jiacheng Xu and Greg Durrett.
\newblock {Spherical Latent Spaces for Stable Variational Autoencoders}.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2018.

\bibitem[Yang and Amari(1997)]{yang1997adaptive}
Howard~Hua Yang and Shun-ichi Amari.
\newblock Adaptive online learning algorithms for blind separation: maximum
  entropy and minimum mutual information.
\newblock \emph{Neural computation}, 9\penalty0 (7):\penalty0 1457--1482, 1997.

\bibitem[Zhao et~al.(2017)Zhao, Song, and Ermon]{ZhaoSE17b}
Shengjia Zhao, Jiaming Song, and Stefano Ermon.
\newblock Infovae: Information maximizing variational autoencoders.
\newblock \emph{CoRR}, abs/1706.02262, 2017.
\newblock URL \url{http://arxiv.org/abs/1706.02262}.

\end{thebibliography}
