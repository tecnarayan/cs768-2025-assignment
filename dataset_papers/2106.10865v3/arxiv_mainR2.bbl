\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{CWG{\etalchar{+}}19}

\bibitem[AGL21]{abramovich2021multiclass}
Felix Abramovich, Vadim Grinshtein, and Tomer Levy.
\newblock Multiclass classification by sparse multinomial logistic regression.
\newblock {\em IEEE Transactions on Information Theory}, 67(7):4637--4646,
  2021.

\bibitem[AKLZ20]{aubin2020generalization}
Benjamin Aubin, Florent Krzakala, Yue Lu, and Lenka Zdeborov\'{a}.
\newblock Generalization error in high-dimensional perceptrons: {A}pproaching
  {B}ayes error with convex optimization.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 12199--12210. Curran Associates, Inc., 2020.

\bibitem[ASH21]{ardeshir2021support}
Navid Ardeshir, Clayton Sanford, and Daniel~J Hsu.
\newblock Support vector machines and linear regression coincide with very
  high-dimensional features.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[ASS01]{allwein2000reducing}
Erin~L. Allwein, Robert~E. Schapire, and Yoram Singer.
\newblock Reducing multiclass to binary: A unifying approach for margin
  classifiers.
\newblock {\em Journal of Machine Learning Research}, 1:113–141, September
  2001.

\bibitem[BB99]{bredensteiner1999multicategory}
Erin~J Bredensteiner and Kristin~P Bennett.
\newblock Multicategory classification by support vector machines.
\newblock In {\em Computational Optimization}, pages 53--79. Springer, 1999.

\bibitem[BEH20]{bosman2020visualising}
Anna~Sergeevna Bosman, Andries Engelbrecht, and Mardé Helbig.
\newblock Visualising basins of attraction for the cross-entropy and the
  squared error neural network loss functions.
\newblock {\em Neurocomputing}, 400:113--136, 2020.

\bibitem[Ber09]{bernstein2009matrix}
Dennis~S Bernstein.
\newblock {\em Matrix mathematics: theory, facts, and formulas}.
\newblock Princeton university press, 2009.

\bibitem[BG01]{buhot2001robust}
Arnaud Buhot and Mirta~B Gordon.
\newblock Robust learning and generalization with support vector machines.
\newblock {\em Journal of Physics A: Mathematical and General},
  34(21):4377--4388, May 2001.

\bibitem[BHMM19]{belkin2019reconciling}
Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal.
\newblock Reconciling modern machine-learning practice and the classical
  bias{\textendash}variance trade-off.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(32):15849--15854, 2019.

\bibitem[BHX20]{belkin2020two}
Mikhail Belkin, Daniel Hsu, and Ji~Xu.
\newblock Two models of double descent for weak features.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(4):1167--1180,
  2020.

\bibitem[BLLT20]{bartlett2020benign}
Peter~L. Bartlett, Philip~M. Long, G{\'a}bor Lugosi, and Alexander Tsigler.
\newblock Benign overfitting in linear regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(48):30063--30070, 2020.

\bibitem[BM94]{bennett1994multicategory}
Kristin~P. Bennett and O.L. Mangasarian.
\newblock Multicategory discrimination via linear programming.
\newblock {\em Optimization Methods and Software}, 3(1-3):27--39, 1994.

\bibitem[BM03]{bartlett2002rademacher}
Peter~L. Bartlett and Shahar Mendelson.
\newblock Rademacher and {G}aussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3:463–482, March 2003.

\bibitem[CGB21]{cao2021risk}
Yuan Cao, Quanquan Gu, and Mikhail Belkin.
\newblock Risk bounds for over-parameterized maximum margin classification on
  sub-{G}aussian mixtures.
\newblock {\em arXiv preprint arXiv:2104.13628}, 2021.

\bibitem[CKMY16]{cortes2016structured}
Corinna Cortes, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang.
\newblock Structured prediction theory based on factor graph complexity.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.

\bibitem[CL21]{chatterji2020finite}
Niladri~S Chatterji and Philip~M Long.
\newblock Finite-sample analysis of interpolating linear classifiers in the
  overparameterized regime.
\newblock {\em Journal of Machine Learning Research}, 22(129):1--30, 2021.

\bibitem[CLRS09]{cormen2009introduction}
Thomas~H. Cormen, Charles~E. Leiserson, Ronald~L. Rivest, and Clifford Stein.
\newblock {\em Introduction to Algorithms, Third Edition}.
\newblock The MIT Press, 3rd edition, 2009.

\bibitem[CS02]{crammer2001algorithmic}
Koby Crammer and Yoram Singer.
\newblock On the algorithmic implementation of multiclass kernel-based vector
  machines.
\newblock {\em Journal of Machine Learning Research}, 2:265–292, March 2002.

\bibitem[CWG{\etalchar{+}}19]{TengyuMa}
Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1567--1578, 2019.

\bibitem[DB95]{dietterich1994solving}
Thomas~G. Dietterich and Ghulum Bakiri.
\newblock Solving multiclass learning problems via error-correcting output
  codes.
\newblock {\em Journal of Artificial Intelligence Research}, 2(1):263–286,
  January 1995.

\bibitem[DCO20]{demirkaya2020exploring}
Ahmet Demirkaya, Jiasi Chen, and Samet Oymak.
\newblock Exploring the role of loss functions in multiclass classification.
\newblock In {\em 2020 54th Annual Conference on Information Sciences and
  Systems (CISS)}, pages 1--5, 2020.

\bibitem[DET05]{donoho2005stable}
David~L Donoho, Michael Elad, and Vladimir~N Temlyakov.
\newblock Stable recovery of sparse overcomplete representations in the
  presence of noise.
\newblock {\em IEEE Transactions on information theory}, 52(1):6--18, 2005.

\bibitem[DKT21]{deng2019model}
Zeyu Deng, Abla Kammoun, and Christos Thrampoulidis.
\newblock {A model of double descent for high-dimensional binary linear
  classification}.
\newblock {\em Information and Inference: A Journal of the IMA}, April 2021.

\bibitem[DL20]{dhifallah2020precise}
Oussama Dhifallah and Yue~M Lu.
\newblock A precise performance analysis of learning with random features.
\newblock {\em arXiv preprint arXiv:2008.11904}, 2020.

\bibitem[DOS99]{dietrich1999statistical}
Rainer Dietrich, Manfred Opper, and Haim Sompolinsky.
\newblock Statistical mechanics of support vector networks.
\newblock {\em Physical Review Letters}, 82:2975--2978, Apr 1999.

\bibitem[DR17]{dziugaite2017computing}
Gintare~Karolina Dziugaite and Daniel~M Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock {\em arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[EHN96]{engl1996regularization}
Heinz~Werner Engl, Martin Hanke, and Andreas Neubauer.
\newblock {\em Regularization of inverse problems}, volume 375.
\newblock Springer Science \& Business Media, 1996.

\bibitem[F\"02]{furnkranz2002round}
Johannes F\"{u}rnkranz.
\newblock Round robin classification.
\newblock {\em Journal of Machine Learning Research}, 2:721–747, March 2002.

\bibitem[FHLS21a]{NC5}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority
  collapse in imbalanced training.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(43), 2021.

\bibitem[FHLS21b]{NC6}
Cong Fang, Hangfeng He, Qi~Long, and Weijie~J Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority
  collapse in imbalanced training.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(43), 2021.

\bibitem[GCOZ17]{gajowniczek2017generalized}
Krzysztof Gajowniczek, Leszek~J. Chmielewski, Arkadiusz Or{\l}owski, and Tomasz
  Z{\k{a}}bkowski.
\newblock Generalized entropy cost function in neural networks.
\newblock In Alessandra Lintas, Stefano Rovetta, Paul~F.M.J. Verschure, and
  Alessandro~E.P. Villa, editors, {\em Artificial Neural Networks and Machine
  Learning -- ICANN 2017}, pages 128--136, Cham, 2017. Springer International
  Publishing.

\bibitem[GHNK21]{NC8}
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt.
\newblock Dissecting supervised constrastive learning.
\newblock In {\em International Conference on Machine Learning}, pages
  3821--3830. PMLR, 2021.

\bibitem[GHST05]{graepel2005pac}
Thore Graepel, Ralf Herbrich, and John Shawe-Taylor.
\newblock Pac-bayesian compression bounds on the prediction error of learning
  algorithms for classification.
\newblock {\em Machine Learning}, 59(1-2):55--76, 2005.

\bibitem[GJS{\etalchar{+}}20]{geiger2020scaling}
Mario Geiger, Arthur Jacot, Stefano Spigler, Franck Gabriel, Levent Sagun,
  St{\'{e}}phane d'Ascoli, Giulio Biroli, Cl{\'{e}}ment Hongler, and Matthieu
  Wyart.
\newblock Scaling description of generalization with number of parameters in
  deep learning.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2020(2):023401, February 2020.

\bibitem[GLL{\etalchar{+}}11]{germain2011pac}
Pascal Germain, Alexandre Lacoste, Fran{\c{c}}ois Laviolette, Mario Marchand,
  and Sara Shanian.
\newblock A pac-bayes sample-compression approach to kernel methods.
\newblock In {\em ICML}, 2011.

\bibitem[HB20]{hui2020evaluation}
Like Hui and Mikhail Belkin.
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock {\em arXiv preprint arXiv:2006.07322}, 2020.

\bibitem[HJ12]{horn2012matrix}
Roger~A. Horn and Charles~R. Johnson.
\newblock {\em Matrix Analysis}.
\newblock Cambridge University Press, USA, 2nd edition, 2012.

\bibitem[HMRT19]{hastie2019surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em arXiv preprint arXiv:1903.08560}, 2019.

\bibitem[HMX21]{hsu2020proliferation}
Daniel Hsu, Vidya Muthukumar, and Ji~Xu.
\newblock On the proliferation of support vectors in high dimensions.
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, {\em Proceedings of
  The 24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of {\em Proceedings of Machine Learning Research}, pages 91--99.
  PMLR, 13--15 Apr 2021.

\bibitem[HPD21]{NC3}
XY~Han, Vardan Papyan, and David~L Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the
  central path.
\newblock {\em arXiv preprint arXiv:2106.02073}, 2021.

\bibitem[Hua17]{huang2017asymptotic}
Hanwen Huang.
\newblock Asymptotic behavior of support vector machine for spiked population
  model.
\newblock {\em Journal of Machine Learning Research}, 18(45):1--21, 2017.

\bibitem[HYS16]{hou2016squared}
Le~Hou, Chen-Ping Yu, and Dimitris Samaras.
\newblock Squared earth mover's distance-based loss for training deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1611.05916}, 2016.

\bibitem[IMSV19]{masnadi2012cost}
Arya Iranmehr, Hamed Masnadi-Shirazi, and Nuno Vasconcelos.
\newblock Cost-sensitive support vector machines.
\newblock {\em Neurocomputing}, 343:50--64, 2019.

\bibitem[JT19]{ji2019implicit}
Ziwei Ji and Matus Telgarsky.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In Alina Beygelzimer and Daniel Hsu, editors, {\em Proceedings of the
  Thirty-Second Conference on Learning Theory}, volume~99 of {\em Proceedings
  of Machine Learning Research}, pages 1772--1798, Phoenix, USA, 25--28 Jun
  2019. PMLR.

\bibitem[KA21]{svm_abla}
Abla Kammoun and Mohamed-Slim Alouini.
\newblock On the precise error analysis of support vector machines.
\newblock {\em IEEE Open Journal of Signal Processing}, 2:99--118, 2021.

\bibitem[KLS20]{kobak2020optimal}
Dmitry Kobak, Jonathan Lomond, and Benoit Sanchez.
\newblock The optimal ridge penalty for real-world high-dimensional data can be
  zero or negative due to the implicit ridge regularization.
\newblock {\em Journal of Machine Learning Research}, 21(169):1--16, 2020.

\bibitem[KP02]{koltchinskii2002empirical}
V.~Koltchinskii and D.~Panchenko.
\newblock {Empirical Margin Distributions and Bounding the Generalization Error
  of Combined Classifiers}.
\newblock {\em The Annals of Statistics}, 30(1):1 -- 50, 2002.

\bibitem[KPOT21]{VSloss}
Ganesh~Ramachandra Kini, Orestis Paraskevas, Samet Oymak, and Christos
  Thrampoulidis.
\newblock Label-imbalanced and group-sensitive classification under
  overparameterization.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[KS18]{kumar2018robust}
Himanshu Kumar and P.~S. Sastry.
\newblock Robust loss functions for learning multi-class classifiers.
\newblock In {\em 2018 IEEE International Conference on Systems, Man, and
  Cybernetics (SMC)}, pages 687--692, 2018.

\bibitem[KT21]{kini2021phase}
Ganesh~Ramachandra Kini and Christos Thrampoulidis.
\newblock Phase transitions for one-vs-one and one-vs-all linear separability
  in multiclass gaussian mixtures.
\newblock In {\em ICASSP 2021 - 2021 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 4020--4024, 2021.

\bibitem[KTW{\etalchar{+}}20]{khosla2020supervised}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:18661--18673, 2020.

\bibitem[LDBK15]{lei2015multi}
Yunwen Lei, Urun Dogan, Alexander Binder, and Marius Kloft.
\newblock Multi-class svms: From tighter data-dependent generalization bounds
  to novel algorithms.
\newblock In C.~Cortes, N.~Lawrence, D.~Lee, M.~Sugiyama, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems}, volume~28.
  Curran Associates, Inc., 2015.

\bibitem[LDZK19]{lei2019data}
Yunwen Lei, Ürün Dogan, Ding-Xuan Zhou, and Marius Kloft.
\newblock Data-dependent generalization bounds for multi-class classification.
\newblock {\em IEEE Transactions on Information Theory}, 65(5):2995--3021,
  2019.

\bibitem[LLW04]{lee2004multicategory}
Yoonkyung Lee, Yi~Lin, and Grace Wahba.
\newblock Multicategory support vector machines.
\newblock {\em Journal of the American Statistical Association},
  99(465):67--81, 2004.

\bibitem[Lol20]{lolas2020regularization}
Panagiotis Lolas.
\newblock Regularization in high-dimensional regression and classification via
  random matrix theory.
\newblock {\em arXiv preprint arXiv:2003.13723}, 2020.

\bibitem[LR21]{liang2021interpolating}
Tengyuan Liang and Benjamin Recht.
\newblock Interpolating classifiers make few mistakes.
\newblock {\em arXiv preprint arXiv:2101.11815}, 2021.

\bibitem[LS20]{liang2020precise}
Tengyuan Liang and Pragya Sur.
\newblock A precise high-dimensional asymptotic theory for boosting and
  min-l1-norm interpolated classifiers.
\newblock {\em arXiv preprint arXiv:2002.01586}, 2020.

\bibitem[LS22]{NC4}
Jianfeng Lu and Stefan Steinerberger.
\newblock Neural collapse under cross-entropy loss.
\newblock {\em Applied and Computational Harmonic Analysis}, 2022.

\bibitem[Mau16]{maurer2016vector}
Andreas Maurer.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock In Ronald Ortner, Hans~Ulrich Simon, and Sandra Zilles, editors, {\em
  Algorithmic Learning Theory}, pages 3--17, Cham, 2016. Springer International
  Publishing.

\bibitem[MJR{\etalchar{+}}20]{Menon}
Aditya~Krishna Menon, Sadeep Jayasumana, Ankit~Singh Rawat, Himanshu Jain,
  Andreas Veit, and Sanjiv Kumar.
\newblock Long-tail learning via logit adjustment.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem[MLC19]{mai2019large}
Xiaoyi Mai, Zhenyu Liao, and Romain Couillet.
\newblock A large scale analysis of logistic regression: Asymptotic performance
  and new insights.
\newblock In {\em ICASSP 2019 - 2019 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 3357--3361, 2019.

\bibitem[MNS{\etalchar{+}}21]{muthukumar2021classification}
Vidya Muthukumar, Adhyyan Narang, Vignesh Subramanian, Mikhail Belkin, Daniel
  Hsu, and Anant Sahai.
\newblock Classification vs regression in overparameterized regimes: Does the
  loss function matter?
\newblock {\em Journal of Machine Learning Research}, 22(222):1--69, 2021.

\bibitem[MO05]{malzahn2005statistical}
Dörthe Malzahn and Manfred Opper.
\newblock A statistical physics approach for the analysis of machine learning
  algorithms on real data.
\newblock {\em Journal of Statistical Mechanics: Theory and Experiment},
  2005(11):P11001--P11001, nov 2005.

\bibitem[MPP20]{NC2}
Dustin~G Mixon, Hans Parshall, and Jianzong Pi.
\newblock Neural collapse with unconstrained features.
\newblock {\em arXiv preprint arXiv:2011.11619}, 2020.

\bibitem[MR16]{maximov2016tight}
Yu~Maximov and Daria Reshetova.
\newblock Tight risk bounds for multi-class margin classifiers.
\newblock {\em Pattern Recognition and Image Analysis}, 26:673--680, 2016.

\bibitem[MRSY19]{montanari2019generalization}
Andrea Montanari, Feng Ruan, Youngtak Sohn, and Jun Yan.
\newblock The generalization error of max-margin linear classifiers:
  High-dimensional asymptotics in the overparametrized regime.
\newblock {\em arXiv preprint arXiv:1911.01544}, 2019.

\bibitem[MVSS20]{muthukumar2020harmless}
Vidya Muthukumar, Kailas Vodrahalli, Vignesh Subramanian, and Anant Sahai.
\newblock Harmless interpolation of noisy data in regression.
\newblock {\em IEEE Journal on Selected Areas in Information Theory}, 2020.

\bibitem[PGS13]{pires2013cost}
Bernardo~\'{A}vila Pires, Mohammad Ghavamzadeh, and Csaba Szepesv\'{a}ri.
\newblock Cost-sensitive multiclass classification risk bounds.
\newblock In {\em Proceedings of the 30th International Conference on
  International Conference on Machine Learning - Volume 28}, ICML'13, page
  III–1391–III–1399. JMLR.org, 2013.

\bibitem[PHD20]{papyan2020prevalence}
Vardan Papyan, X.~Y. Han, and David~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(40):24652--24663, 2020.

\bibitem[PL20a]{poggio2020explicit}
Tomaso Poggio and Qianli Liao.
\newblock Explicit regularization and implicit bias in deep network classifiers
  trained with the square loss.
\newblock {\em arXiv preprint arXiv:2101.00072}, 2020.

\bibitem[PL20b]{NC7}
Tomaso Poggio and Qianli Liao.
\newblock Explicit regularization and implicit bias in deep network classifiers
  trained with the square loss.
\newblock {\em arXiv preprint arXiv:2101.00072}, 2020.

\bibitem[PS16]{pires2016multiclass}
Bernardo~{\'A}vila Pires and Csaba Szepesv{\'a}ri.
\newblock Multiclass classification calibration functions.
\newblock {\em arXiv preprint arXiv:1609.06385}, 2016.

\bibitem[RDS{\etalchar{+}}15]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em International journal of computer vision}, 115(3):211--252,
  2015.

\bibitem[Rif02]{rifkin2002everything}
Ryan~Michael Rifkin.
\newblock {\em Everything old is new again: a fresh look at historical
  approaches in machine learning}.
\newblock PhD thesis, MaSSachuSettS InStitute of Technology, 2002.

\bibitem[RK04]{rifkin2004defense}
Ryan Rifkin and Aldebaro Klautau.
\newblock In defense of one-vs-all classification.
\newblock {\em Journal of Machine Learning Research}, 5:101--141, 2004.

\bibitem[RV{\etalchar{+}}13]{rudelson2013hanson}
Mark Rudelson, Roman Vershynin, et~al.
\newblock Hanson-wright inequality and sub-gaussian concentration.
\newblock {\em Electronic Communications in Probability}, 18, 2013.

\bibitem[SAH19]{salehi2019impact}
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi.
\newblock The impact of regularization on high-dimensional logistic regression.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc., 2019.

\bibitem[SAH20]{salehi2020performance}
Fariborz Salehi, Ehsan Abbasi, and Babak Hassibi.
\newblock The performance analysis of generalized margin maximizers on
  separable data.
\newblock In {\em International Conference on Machine Learning}, pages
  8417--8426. PMLR, 2020.

\bibitem[SAS22]{subramaniangeneralization}
Vignesh Subramanian, Rahul Arya, and Anant Sahai.
\newblock Generalization for multiclass classification with overparameterized
  linear models.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem[SC19]{sur2019modern}
Pragya Sur and Emmanuel~J Cand{\`e}s.
\newblock A modern maximum-likelihood theory for high-dimensional logistic
  regression.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(29):14516--14525, 2019.

\bibitem[SFBL98]{schapire1998boosting}
Robert~E Schapire, Yoav Freund, Peter Bartlett, and Wee~Sun Lee.
\newblock Boosting the margin: A new explanation for the effectiveness of
  voting methods.
\newblock {\em The Annals of Statistics}, 26(5):1651--1686, 1998.

\bibitem[SHN{\etalchar{+}}18]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em Journal of Machine Learning Research}, 19(1):2822--2878, 2018.

\bibitem[TB07]{tewari2007consistency}
Ambuj Tewari and Peter~L Bartlett.
\newblock On the consistency of multiclass classification methods.
\newblock {\em Journal of Machine Learning Research}, 8(36):1007--1025, 2007.

\bibitem[TB20]{tsigler2020benign}
Alexander Tsigler and Peter~L Bartlett.
\newblock Benign overfitting in ridge regression.
\newblock {\em arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[TOS20]{thrampoulidis2020theoretical}
Christos Thrampoulidis, Samet Oymak, and Mahdi Soltanolkotabi.
\newblock Theoretical insights into multiclass classification: A
  high-dimensional asymptotic view.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 8907--8920. Curran Associates, Inc., 2020.

\bibitem[TPT20]{taheri2020sharp}
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis.
\newblock Sharp asymptotics and optimal performance for inference in binary
  models.
\newblock In Silvia Chiappa and Roberto Calandra, editors, {\em Proceedings of
  the Twenty Third International Conference on Artificial Intelligence and
  Statistics}, volume 108 of {\em Proceedings of Machine Learning Research},
  pages 3739--3749. PMLR, 26--28 Aug 2020.

\bibitem[TPT21]{taheri2020fundamental}
Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis.
\newblock Fundamental limits of ridge-regularized empirical risk minimization
  in high dimensions.
\newblock In Arindam Banerjee and Kenji Fukumizu, editors, {\em Proceedings of
  The 24th International Conference on Artificial Intelligence and Statistics},
  volume 130 of {\em Proceedings of Machine Learning Research}, pages
  2773--2781. PMLR, 13--15 Apr 2021.

\bibitem[Tro06]{tropp2006just}
Joel~A Tropp.
\newblock Just relax: Convex programming methods for identifying sparse signals
  in noise.
\newblock {\em IEEE transactions on information theory}, 52(3):1030--1051,
  2006.

\bibitem[Vap13]{vapnik2013nature}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem[Wai19]{wainwright2019high}
Martin~J Wainwright.
\newblock {\em High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wel74]{welch1974lower}
Lloyd Welch.
\newblock Lower bounds on the maximum cross correlation of signals (corresp.).
\newblock {\em IEEE Transactions on Information theory}, 20(3):397--399, 1974.

\bibitem[WS23]{wu2023precise}
David~X Wu and Anant Sahai.
\newblock Precise asymptotic generalization for multiclass classification with
  overparameterized linear models.
\newblock {\em arXiv preprint arXiv:2306.13255}, 2023.

\bibitem[WT21]{wang2020benign}
Ke~Wang and Christos Thrampoulidis.
\newblock Binary classification of gaussian mixtures: Abundance of support
  vectors, benign overfitting and regularization.
\newblock {\em arXiv preprint arXiv:2011.09148}, 2021.

\bibitem[WW98]{weston1998multi}
Jason Weston and Chris Watkins.
\newblock Multi-class support vector machines.
\newblock Technical report, Citeseer, 1998.

\bibitem[ZBH{\etalchar{+}}17]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem[ZDZ{\etalchar{+}}21]{NC1}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and
  Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zha04]{zhang2004statistical}
Tong Zhang.
\newblock Statistical behavior and consistency of classification methods based
  on convex risk minimization.
\newblock {\em The Annals of Statistics}, 32(1):56--85, 2004.

\end{thebibliography}
