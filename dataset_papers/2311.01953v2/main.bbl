\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos, Heess, and Riedmiller]{abdolmaleki2018maximum}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Munos, R., Heess, N., and Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock \emph{arXiv preprint arXiv:1806.06920}, 2018.

\bibitem[Bernstein et~al.(2002)Bernstein, Givan, Immerman, and Zilberstein]{bernstein2002complexity}
Bernstein, D.~S., Givan, R., Immerman, N., and Zilberstein, S.
\newblock The complexity of decentralized control of markov decision processes.
\newblock \emph{Mathematics of operations research}, 27\penalty0 (4):\penalty0 819--840, 2002.

\bibitem[Busoniu et~al.(2008)Busoniu, Babuska, and De~Schutter]{busoniu2008comprehensive}
Busoniu, L., Babuska, R., and De~Schutter, B.
\newblock A comprehensive survey of multiagent reinforcement learning.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, 38\penalty0 (2):\penalty0 156--172, 2008.

\bibitem[Carroll et~al.(2019)Carroll, Shah, Ho, Griffiths, Seshia, Abbeel, and Dragan]{carroll2019utility}
Carroll, M., Shah, R., Ho, M.~K., Griffiths, T., Seshia, S., Abbeel, P., and Dragan, A.
\newblock On the utility of learning about humans for human-ai coordination.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Chapelle \& Li(2011)Chapelle and Li]{chapelle2011empirical}
Chapelle, O. and Li, L.
\newblock An empirical evaluation of thompson sampling.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Claus \& Boutilier(1998)Claus and Boutilier]{claus1998dynamics}
Claus, C. and Boutilier, C.
\newblock The dynamics of reinforcement learning in cooperative multiagent systems.
\newblock \emph{AAAI/IAAI}, 1998\penalty0 (746-752):\penalty0 2, 1998.

\bibitem[De~Witt et~al.(2020)De~Witt, Gupta, Makoviichuk, Makoviychuk, Torr, Sun, and Whiteson]{de2020independent}
De~Witt, C.~S., Gupta, T., Makoviichuk, D., Makoviychuk, V., Torr, P.~H., Sun, M., and Whiteson, S.
\newblock Is independent learning all you need in the starcraft multi-agent challenge?
\newblock \emph{arXiv preprint arXiv:2011.09533}, 2020.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and Whiteson]{foerster2018counterfactual}
Foerster, J., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~32, 2018.

\bibitem[Ghosh et~al.(2020)Ghosh, C~Machado, and Le~Roux]{ghosh2020operator}
Ghosh, D., C~Machado, M., and Le~Roux, N.
\newblock An operator view of policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 3397--3406, 2020.

\bibitem[H{\"a}m{\"a}l{\"a}inen et~al.(2020)H{\"a}m{\"a}l{\"a}inen, Babadi, Ma, and Lehtinen]{hamalainen2020ppo}
H{\"a}m{\"a}l{\"a}inen, P., Babadi, A., Ma, X., and Lehtinen, J.
\newblock Ppo-cma: Proximal policy optimization with covariance matrix adaptation.
\newblock In \emph{2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP)}, pp.\  1--6. IEEE, 2020.

\bibitem[Hu et~al.(2023)Hu, Zhang, Hegde, and Schmidt]{hu2023optimistic}
Hu, B., Zhang, T.~H., Hegde, N., and Schmidt, M.
\newblock Optimistic thompson sampling-based algorithms for episodic reinforcement learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  890--899. PMLR, 2023.

\bibitem[Hu et~al.(2021)Hu, Hu, and Liao]{hu2021policy}
Hu, J., Hu, S., and Liao, S.-w.
\newblock Policy regularization via noisy advantage values for cooperative multi-agent actor-critic methods.
\newblock \emph{arXiv preprint arXiv:2106.14334}, 2021.

\bibitem[Imagawa et~al.(2019)Imagawa, Hiraoka, and Tsuruoka]{imagawa2019optimistic}
Imagawa, T., Hiraoka, T., and Tsuruoka, Y.
\newblock Optimistic proximal policy optimization.
\newblock \emph{arXiv preprint arXiv:1906.11075}, 2019.

\bibitem[Jaques et~al.(2019)Jaques, Lazaridou, Hughes, Gulcehre, Ortega, Strouse, Leibo, and De~Freitas]{jaques2019social}
Jaques, N., Lazaridou, A., Hughes, E., Gulcehre, C., Ortega, P., Strouse, D., Leibo, J.~Z., and De~Freitas, N.
\newblock Social influence as intrinsic motivation for multi-agent deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\  3040--3049. PMLR, 2019.

\bibitem[Kapetanakis \& Kudenko(2002)Kapetanakis and Kudenko]{kapetanakis2002reinforcement}
Kapetanakis, S. and Kudenko, D.
\newblock Reinforcement learning of coordination in cooperative multi-agent systems.
\newblock \emph{AAAI/IAAI}, 2002:\penalty0 326--331, 2002.

\bibitem[Kuba et~al.(2021)Kuba, Chen, Wen, Wen, Sun, Wang, and Yang]{kuba2021trust}
Kuba, J.~G., Chen, R., Wen, M., Wen, Y., Sun, F., Wang, J., and Yang, Y.
\newblock Trust region policy optimisation in multi-agent reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.11251}, 2021.

\bibitem[Lauer \& Riedmiller(2000)Lauer and Riedmiller]{lauer2000algorithm}
Lauer, M. and Riedmiller, M.
\newblock An algorithm for distributed reinforcement learning in cooperative multi-agent systems.
\newblock In \emph{In Proceedings of the Seventeenth International Conference on Machine Learning}. Citeseer, 2000.

\bibitem[Li et~al.(2022)Li, Tang, Yang, Hao, Sang, Zheng, Hao, Taylor, Tao, Wang, et~al.]{li2022pmic}
Li, P., Tang, H., Yang, T., Hao, X., Sang, T., Zheng, Y., Hao, J., Taylor, M.~E., Tao, W., Wang, Z., et~al.
\newblock Pmic: Improving multi-agent reinforcement learning with progressive mutual information collaboration.
\newblock \emph{arXiv preprint arXiv:2203.08553}, 2022.

\bibitem[Liu et~al.(2021)Liu, Jain, Yeh, and Schwing]{liu2021cooperative}
Liu, I.-J., Jain, U., Yeh, R.~A., and Schwing, A.
\newblock Cooperative exploration for multi-agent deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\  6826--6836. PMLR, 2021.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Pieter~Abbeel, and Mordatch]{lowe2017multi}
Lowe, R., Wu, Y.~I., Tamar, A., Harb, J., Pieter~Abbeel, O., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive environments.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Maas et~al.(2013)Maas, Hannun, Ng, et~al.]{maas2013rectifier}
Maas, A.~L., Hannun, A.~Y., Ng, A.~Y., et~al.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In \emph{Proc. icml}, volume~30. Atlanta, Georgia, USA, 2013.

\bibitem[Mahajan et~al.(2019)Mahajan, Rashid, Samvelyan, and Whiteson]{mahajan2019maven}
Mahajan, A., Rashid, T., Samvelyan, M., and Whiteson, S.
\newblock Maven: Multi-agent variational exploration.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Matignon et~al.(2007)Matignon, Laurent, and Le~Fort-Piat]{matignon2007hysteretic}
Matignon, L., Laurent, G.~J., and Le~Fort-Piat, N.
\newblock Hysteretic q-learning: an algorithm for decentralized reinforcement learning in cooperative multi-agent teams.
\newblock In \emph{2007 IEEE/RSJ International Conference on Intelligent Robots and Systems}, pp.\  64--69. IEEE, 2007.

\bibitem[Matignon et~al.(2012)Matignon, Laurent, and Le~Fort-Piat]{matignon2012independent}
Matignon, L., Laurent, G.~J., and Le~Fort-Piat, N.
\newblock Independent reinforcement learners in cooperative markov games: a survey regarding coordination problems.
\newblock \emph{The Knowledge Engineering Review}, 27\penalty0 (1):\penalty0 1--31, 2012.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley, Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T., Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\  1928--1937. PMLR, 2016.

\bibitem[Munos et~al.(2014)]{munos2014bandits}
Munos, R. et~al.
\newblock From bandits to monte-carlo tree search: The optimistic principle applied to optimization and planning.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 7\penalty0 (1):\penalty0 1--129, 2014.

\bibitem[Oh et~al.(2018)Oh, Guo, Singh, and Lee]{oh2018self}
Oh, J., Guo, Y., Singh, S., and Lee, H.
\newblock Self-imitation learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  3878--3887. PMLR, 2018.

\bibitem[Omidshafiei et~al.(2017)Omidshafiei, Pazis, Amato, How, and Vian]{omidshafiei2017deep}
Omidshafiei, S., Pazis, J., Amato, C., How, J.~P., and Vian, J.
\newblock Deep decentralized multi-task multi-agent reinforcement learning under partial observability.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2681--2690. PMLR, 2017.

\bibitem[Palmer et~al.(2017)Palmer, Tuyls, Bloembergen, and Savani]{palmer2017lenient}
Palmer, G., Tuyls, K., Bloembergen, D., and Savani, R.
\newblock Lenient multi-agent deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1707.04402}, 2017.

\bibitem[Panait et~al.(2006)Panait, Sullivan, and Luke]{panait2006lenient}
Panait, L., Sullivan, K., and Luke, S.
\newblock Lenient learners in cooperative multiagent systems.
\newblock In \emph{Proceedings of the fifth international joint conference on Autonomous agents and multiagent systems}, pp.\  801--803, 2006.

\bibitem[Papoudakis et~al.(2020)Papoudakis, Christianos, Sch{\"a}fer, and Albrecht]{papoudakis2020benchmarking}
Papoudakis, G., Christianos, F., Sch{\"a}fer, L., and Albrecht, S.~V.
\newblock Benchmarking multi-agent deep reinforcement learning algorithms in cooperative tasks.
\newblock \emph{arXiv preprint arXiv:2006.07869}, 2020.

\bibitem[Peng et~al.(2021)Peng, Rashid, Schroeder~de Witt, Kamienny, Torr, B{\"o}hmer, and Whiteson]{peng2021facmac}
Peng, B., Rashid, T., Schroeder~de Witt, C., Kamienny, P.-A., Torr, P., B{\"o}hmer, W., and Whiteson, S.
\newblock Facmac: Factored multi-agent centralised policy gradients.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 12208--12221, 2021.

\bibitem[Peters et~al.(2010)Peters, Mulling, and Altun]{peters2010relative}
Peters, J., Mulling, K., and Altun, Y.
\newblock Relative entropy policy search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~24, pp.\  1607--1612, 2010.

\bibitem[Rashid et~al.(2020{\natexlab{a}})Rashid, Farquhar, Peng, and Whiteson]{rashid2020weighted}
Rashid, T., Farquhar, G., Peng, B., and Whiteson, S.
\newblock Weighted qmix: Expanding monotonic value function factorisation for deep multi-agent reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 10199--10210, 2020{\natexlab{a}}.

\bibitem[Rashid et~al.(2020{\natexlab{b}})Rashid, Samvelyan, De~Witt, Farquhar, Foerster, and Whiteson]{rashid2020monotonic}
Rashid, T., Samvelyan, M., De~Witt, C.~S., Farquhar, G., Foerster, J., and Whiteson, S.
\newblock Monotonic value function factorisation for deep multi-agent reinforcement learning.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 7234--7284, 2020{\natexlab{b}}.

\bibitem[Russo et~al.(2018)Russo, Van~Roy, Kazerouni, Osband, Wen, et~al.]{russo2018tutorial}
Russo, D.~J., Van~Roy, B., Kazerouni, A., Osband, I., Wen, Z., et~al.
\newblock A tutorial on thompson sampling.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 11\penalty0 (1):\penalty0 1--96, 2018.

\bibitem[Schulman et~al.(2015)Schulman, Moritz, Levine, Jordan, and Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sun et~al.(2022)Sun, Devlin, Beck, Hofmann, and Whiteson]{sun2022monotonic}
Sun, M., Devlin, S., Beck, J., Hofmann, K., and Whiteson, S.
\newblock Monotonic improvement guarantees under non-stationarity for decentralized ppo.
\newblock \emph{arXiv preprint arXiv:2202.00082}, 2022.

\bibitem[Sunehag et~al.(2017)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi, Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, et~al.]{sunehag2017value}
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V., Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., et~al.
\newblock Value-decomposition networks for cooperative multi-agent learning.
\newblock \emph{arXiv preprint arXiv:1706.05296}, 2017.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{Proceedings of the AAAI conference on artificial intelligence}, volume~30, 2016.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik, Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung, J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2023)Wang, Tian, Wan, Wen, Wang, and Zhang]{wang2023order}
Wang, X., Tian, Z., Wan, Z., Wen, Y., Wang, J., and Zhang, W.
\newblock Order matters: Agent-by-agent policy optimization.
\newblock \emph{arXiv preprint arXiv:2302.06205}, 2023.

\bibitem[Wei \& Luke(2016)Wei and Luke]{wei2016lenient}
Wei, E. and Luke, S.
\newblock Lenient learning in independent-learner stochastic cooperative games.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0 (1):\penalty0 2914--2955, 2016.

\bibitem[Wiegand(2004)]{wiegand2004analysis}
Wiegand, R.~P.
\newblock \emph{An analysis of cooperative coevolutionary algorithms}.
\newblock George Mason University, 2004.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist reinforcement learning.
\newblock \emph{Reinforcement learning}, pp.\  5--32, 1992.

\bibitem[Yu et~al.(2022)Yu, Velu, Vinitsky, Gao, Wang, Bayen, and Wu]{yu2022surprising}
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., and Wu, Y.
\newblock The surprising effectiveness of ppo in cooperative multi-agent games.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 24611--24624, 2022.

\bibitem[Yu et~al.(2023)Yu, Gao, Liu, Xu, Tang, Yang, Wang, and Wu]{yu2023learning}
Yu, C., Gao, J., Liu, W., Xu, B., Tang, H., Yang, J., Wang, Y., and Wu, Y.
\newblock Learning zero-shot cooperation with humans, assuming humans are biased.
\newblock \emph{arXiv preprint arXiv:2302.01605}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Pan, Xiao, Chandar, and Rajendran]{zhao2023conditionally}
Zhao, X., Pan, Y., Xiao, C., Chandar, S., and Rajendran, J.
\newblock Conditionally optimistic exploration for cooperative deep multi-agent reinforcement learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  2529--2540. PMLR, 2023.

\end{thebibliography}
