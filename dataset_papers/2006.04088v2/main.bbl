\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Balakrishnan et~al.(2017)Balakrishnan, Wainwright, and
  Yu]{balakrishnan2017statistical}
Sivaraman Balakrishnan, Martin~J Wainwright, and Bin Yu.
\newblock Statistical guarantees for the {EM} algorithm: From population to
  sample-based analysis.
\newblock \emph{The Annals of Statistics}, 45\penalty0 (1):\penalty0 77--120,
  2017.

\bibitem[Caldas et~al.(2018)Caldas, Wu, Li, Kone{\v{c}}n{\`y}, McMahan, Smith,
  and Talwalkar]{caldas2018leaf}
Sebastian Caldas, Peter Wu, Tian Li, Jakub Kone{\v{c}}n{\`y}, H~Brendan
  McMahan, Virginia Smith, and Ameet Talwalkar.
\newblock Leaf: A benchmark for federated settings.
\newblock \emph{arXiv preprint arXiv:1812.01097}, 2018.

\bibitem[Caruana(1997)]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock \emph{Machine learning}, 28\penalty0 (1):\penalty0 41--75, 1997.

\bibitem[Chen et~al.(2018)Chen, Luo, Dong, Li, and He]{chen2018federated}
Fei Chen, Mi~Luo, Zhenhua Dong, Zhenguo Li, and Xiuqiang He.
\newblock Federated meta-learning with fast convergence and efficient
  communication.
\newblock \emph{arXiv preprint arXiv:1802.07876}, 2018.

\bibitem[Daskalakis et~al.(2016)Daskalakis, Tzamos, and
  Zampetakis]{daskalakis2016ten}
Constantinos Daskalakis, Christos Tzamos, and Manolis Zampetakis.
\newblock Ten steps of {EM} suffice for mixtures of two gaussians.
\newblock \emph{arXiv preprint arXiv:1609.00368}, 2016.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
Ofer Dekel, Ran Gilad-Bachrach, Ohad Shamir, and Lin Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jan):\penalty0 165--202, 2012.

\bibitem[DeSarbo and Cron(1988)]{desarbo1988maximum}
Wayne~S DeSarbo and William~L Cron.
\newblock A maximum likelihood methodology for clusterwise linear regression.
\newblock \emph{Journal of Classification}, 5\penalty0 (2):\penalty0 249--282,
  1988.

\bibitem[Duan et~al.(2020)Duan, Liu, Ji, Liu, Liang, Chen, and
  Tan]{duan2020fedgroup}
Moming Duan, Duo Liu, Xinyuan Ji, Renping Liu, Liang Liang, Xianzhang Chen, and
  Yujuan Tan.
\newblock Fed{G}roup: Accurate federated learning via decomposed
  similarity-based clustering.
\newblock \emph{arXiv preprint arXiv:2010.06870}, 2020.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020personalized}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Personalized federated learning: A meta-learning approach.
\newblock \emph{arXiv preprint arXiv:2002.07948}, 2020.

\bibitem[Fienup(1982)]{fienup1982phase}
James~R Fienup.
\newblock Phase retrieval algorithms: a comparison.
\newblock \emph{Applied optics}, 21\penalty0 (15):\penalty0 2758--2769, 1982.

\bibitem[Ghosh and Ramchandran(2020)]{ghosh2020alternating}
Avishek Ghosh and Kannan Ramchandran.
\newblock Alternating minimization converges super-linearly for mixed linear
  regression.
\newblock \emph{arXiv preprint arXiv:2004.10914}, 2020.

\bibitem[Ghosh et~al.(2020)Ghosh, Maity, and Mazumdar]{ghosh2020distributed}
Avishek Ghosh, Raj~Kumar Maity, and Arya Mazumdar.
\newblock Distributed newton can communicate less and resist byzantine workers.
\newblock In \emph{Advances in Neural Information Processing Systems 2020},
  2020.

\bibitem[Goodfellow et~al.(2013)Goodfellow, Mirza, Xiao, Courville, and
  Bengio]{goodfellow2013empirical}
Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6211}, 2013.

\bibitem[Hard et~al.(2018)Hard, Rao, Mathews, Ramaswamy, Beaufays, Augenstein,
  Eichner, Kiddon, and Ramage]{hard2018federated}
Andrew Hard, Kanishka Rao, Rajiv Mathews, Swaroop Ramaswamy, Fran{\c{c}}oise
  Beaufays, Sean Augenstein, Hubert Eichner, Chlo{\'e} Kiddon, and Daniel
  Ramage.
\newblock Federated learning for mobile keyboard prediction.
\newblock \emph{arXiv preprint arXiv:1811.03604}, 2018.

\bibitem[Jain et~al.(2013)Jain, Netrapalli, and Sanghavi]{jain2013low}
Prateek Jain, Praneeth Netrapalli, and Sujay Sanghavi.
\newblock Low-rank matrix completion using alternating minimization.
\newblock In \emph{Proceedings of the forty-fifth annual ACM symposium on
  Theory of computing}, pages 665--674, 2013.

\bibitem[Jiang et~al.(2019)Jiang, Kone{\v{c}}n{\`y}, Rush, and
  Kannan]{jiang2019improving}
Yihan Jiang, Jakub Kone{\v{c}}n{\`y}, Keith Rush, and Sreeram Kannan.
\newblock Improving federated learning personalization via model agnostic meta
  learning.
\newblock \emph{arXiv preprint arXiv:1909.12488}, 2019.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and Hadsell]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Kone{\v{c}}n{\`y} et~al.(2016)Kone{\v{c}}n{\`y}, McMahan, Ramage, and
  Richt{\'a}rik]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: distributed machine learning for on-device
  intelligence.
\newblock \emph{arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[Krizhevsky and Hinton(2009)]{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical Report}, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee(2005)]{lee2005effective}
Dar-Shyang Lee.
\newblock Effective {G}aussian mixture learning for video background
  subtraction.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 27\penalty0 (5):\penalty0 827--832, 2005.

\bibitem[Li et~al.(2018)Li, Xu, Chen, Giannakis, and Ling]{li2018rsa}
Liping Li, Wei Xu, Tianyi Chen, Georgios~B Giannakis, and Qing Ling.
\newblock Rsa: {B}yzantine-robust stochastic aggregation methods for
  distributed learning from heterogeneous datasets.
\newblock \emph{arXiv preprint arXiv:1811.03761}, 2018.

\bibitem[Li et~al.(2014)Li, Andersen, Park, Smola, Ahmed, Josifovski, Long,
  Shekita, and Su]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In \emph{11th $\{$USENIX$\}$ Symposium on Operating Systems Design
  and Implementation ($\{$OSDI$\}$ 14)}, pages 583--598, 2014.

\bibitem[Li and Kim(2003)]{li2003clustering}
Qing Li and Byeong~Man Kim.
\newblock Clustering approach for hybrid recommender system.
\newblock In \emph{Proceedings IEEE/WIC International Conference on Web
  Intelligence (WI 2003)}, pages 33--38. IEEE, 2003.

\bibitem[Li et~al.(2019)Li, Huang, Yang, Wang, and Zhang]{li2019convergence}
Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang.
\newblock On the convergence of fedavg on non-iid data.
\newblock \emph{arXiv preprint arXiv:1907.02189}, 2019.

\bibitem[Lloyd(1982)]{lloyd1982least}
Stuart Lloyd.
\newblock Least squares quantization in {PCM}.
\newblock \emph{IEEE Transactions on Information Theory}, 28\penalty0
  (2):\penalty0 129--137, 1982.

\bibitem[Lopez-Paz and Ranzato(2017)]{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem[Mansour et~al.(2020)Mansour, Mohri, Ro, and Suresh]{mansour2020three}
Yishay Mansour, Mehryar Mohri, Jae Ro, and Ananda~Theertha Suresh.
\newblock Three approaches for personalization with applications to federated
  learning.
\newblock \emph{arXiv preprint arXiv:2002.10619}, 2020.

\bibitem[McMahan and Ramage(2017)]{mcmahan2017federated}
Brendan McMahan and Daniel Ramage.
\newblock Federated learning: Collaborative machine learning without
  centralized training data.
\newblock
  \url{https://research.googleblog.com/2017/04/federated-learning-collaborative.html},
  2017.

\bibitem[McMahan et~al.(2016)McMahan, Moore, Ramage, Hampson, and Aguera~y
  Arcas]{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
  Aguera~y Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock \emph{arXiv preprint arXiv:1602.05629}, 2016.

\bibitem[Millane(1990)]{millane1990phase}
Rick~P Millane.
\newblock Phase retrieval in crystallography and optics.
\newblock \emph{JOSA A}, 7\penalty0 (3):\penalty0 394--411, 1990.

\bibitem[Mohri et~al.(2019)Mohri, Sivek, and Suresh]{mohri2019agnostic}
Mehryar Mohri, Gary Sivek, and Ananda~Theertha Suresh.
\newblock Agnostic federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4615--4625. PMLR, 2019.

\bibitem[Netrapalli et~al.(2013)Netrapalli, Jain, and
  Sanghavi]{netrapalli2013phase}
Praneeth Netrapalli, Prateek Jain, and Sujay Sanghavi.
\newblock Phase retrieval using alternating minimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2796--2804, 2013.

\bibitem[Recht et~al.(2011)Recht, Re, Wright, and Niu]{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  693--701, 2011.

\bibitem[Rudelson and Vershynin(2013)]{rudelson2013hanson}
Mark Rudelson and Roman Vershynin.
\newblock Hanson-{W}right inequality and sub-{G}aussian concentration.
\newblock \emph{Electronic Communications in Probability}, 18, 2013.

\bibitem[Sahu et~al.(2018)Sahu, Li, Sanjabi, Zaheer, Talwalkar, and
  Smith]{sahu-heterogeneous}
Anit~Kumar Sahu, Tian Li, Maziar Sanjabi, Manzil Zaheer, Ameet Talwalkar, and
  Virginia Smith.
\newblock On the convergence of federated optimization in heterogeneous
  networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 3, 2018.

\bibitem[Sarwar et~al.(2002)Sarwar, Karypis, Konstan, and
  Riedl]{sarwar2002recommender}
Badrul~M Sarwar, George Karypis, Joseph Konstan, and John Riedl.
\newblock Recommender systems for large-scale e-commerce: Scalable neighborhood
  formation using clustering.
\newblock In \emph{Proceedings of the Fifth International Conference on
  Computer and Information Technology}, volume~1, pages 291--324, 2002.

\bibitem[Sattler et~al.(2019{\natexlab{a}})Sattler, M{\"u}ller, and
  Samek]{sattler2019clustered}
Felix Sattler, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Clustered federated learning: Model-agnostic distributed multi-task
  optimization under privacy constraints.
\newblock \emph{arXiv preprint arXiv:1910.01991}, 2019{\natexlab{a}}.

\bibitem[Sattler et~al.(2019{\natexlab{b}})Sattler, Wiedemann, M{\"u}ller, and
  Samek]{sattler-non-iid}
Felix Sattler, Simon Wiedemann, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Robust and communication-efficient federated learning from non-iid
  data.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  31\penalty0 (9):\penalty0 3400--3413, 2019{\natexlab{b}}.

\bibitem[Smith et~al.(2017)Smith, Chiang, Sanjabi, and
  Talwalkar]{smith2017federated}
Virginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet~S Talwalkar.
\newblock Federated multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4424--4434, 2017.

\bibitem[Sun et~al.(2014)Sun, Ioannidis, and Montanari]{sun2014learning}
Yuekai Sun, Stratis Ioannidis, and Andrea Montanari.
\newblock Learning mixtures of linear classifiers.
\newblock In \emph{ICML}, pages 721--729, 2014.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Waldspurger(2018)]{waldspurger2018phase}
Ir{\`e}ne Waldspurger.
\newblock Phase retrieval with random gaussian sensing vectors by alternating
  projections.
\newblock \emph{IEEE Transactions on Information Theory}, 64\penalty0
  (5):\penalty0 3301--3312, 2018.

\bibitem[Wang et~al.(2018)Wang, Roosta-Khorasani, Xu, and
  Mahoney]{wang2018giant}
Shusen Wang, Farbod Roosta-Khorasani, Peng Xu, and Michael~W. Mahoney.
\newblock {GIANT}: Globally improved approximate {N}ewton method for
  distributed optimization.
\newblock In \emph{Thirty-Second Conference on Neural Information Processing
  Systems}, 2018.

\bibitem[Wedel and Kamakura(2000)]{wedel2000mixture}
Michel Wedel and Wagner~A Kamakura.
\newblock Mixture regression models.
\newblock In \emph{Market segmentation}, pages 101--124. Springer, 2000.

\bibitem[Xu and Jordan(1996)]{xu1996convergence}
Lei Xu and Michael~I Jordan.
\newblock On convergence properties of the {EM} algorithm for {G}aussian
  mixtures.
\newblock \emph{Neural Computation}, 8\penalty0 (1):\penalty0 129--151, 1996.

\bibitem[Yan et~al.(2017)Yan, Yin, and Sarkar]{yan2017convergence}
Bowei Yan, Mingzhang Yin, and Purnamrita Sarkar.
\newblock Convergence of gradient {EM} on multi-component mixture of gaussians.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6956--6966, 2017.

\bibitem[Yi et~al.(2014)Yi, Caramanis, and Sanghavi]{yi2014alternating}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Alternating minimization for mixed linear regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  613--621, 2014.

\bibitem[Yi et~al.(2016)Yi, Caramanis, and Sanghavi]{yi2016solving}
Xinyang Yi, Constantine Caramanis, and Sujay Sanghavi.
\newblock Solving a mixture of many random linear equations by tensor
  decomposition and alternating minimization.
\newblock \emph{arXiv preprint arXiv:1608.05749}, 2016.

\bibitem[Yin et~al.(2018)Yin, Pedarsani, Chen, and
  Ramchandran]{yin2018learning}
Dong Yin, Ramtin Pedarsani, Yudong Chen, and Kannan Ramchandran.
\newblock Learning mixtures of sparse linear regressions using sparse graph
  codes.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (3):\penalty0 1430--1451, 2018.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and
  Chandra]{zhao2018federated}
Yue Zhao, Meng Li, Liangzhen Lai, Naveen Suda, Damon Civin, and Vikas Chandra.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Li, and
  Smola]{zinkevich2010parallelized}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2595--2603, 2010.

\end{thebibliography}
