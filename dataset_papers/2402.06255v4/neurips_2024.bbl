\begin{thebibliography}{10}

\bibitem{openai2023gpt4}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock In {\em arXiv}, 2023.

\bibitem{alon2023detecting}
Gabriel Alon and Michael Kamfonas.
\newblock Detecting language model attacks with perplexity.
\newblock In {\em arXiv}, 2023.

\bibitem{bai2023query}
Yang Bai, Yisen Wang, Yuyuan Zeng, Yong Jiang, and Shu-Tao Xia.
\newblock Query efficient black-box adversarial attack on deep neural networks.
\newblock {\em Pattern Recognition}, 133:109037, 2023.

\bibitem{bai2022training}
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Tristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.
\newblock Training a helpful and harmless assistant with reinforcement learning from human feedback.
\newblock {\em arXiv preprint arXiv:2204.05862}, 2022.

\bibitem{bai2022constitutional}
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et~al.
\newblock Constitutional ai: Harmlessness from ai feedback.
\newblock In {\em arXiv}, 2022.

\bibitem{ms-marco}
Payal Bajaj, Daniel Campos, Nick Craswell, Li~Deng, Jianfeng Gao, Xiaodong Liu, Rangan Majumder, Andrew McNamara, Bhaskar Mitra, Tri Nguyen, Mir Rosenberg, Xia Song, Alina Stoica, Saurabh Tiwary, and Tong Wang.
\newblock Ms marco: A human generated machine reading comprehension dataset.
\newblock In {\em arXiv}, 2018.

\bibitem{bhardwaj2023red}
Rishabh Bhardwaj and Soujanya Poria.
\newblock Red-teaming large language models using chain of utterances for safety-alignment.
\newblock In {\em arXiv}, 2023.

\bibitem{bianchi2023safety}
Federico Bianchi, Mirac Suzgun, Giuseppe Attanasio, Paul R{\"o}ttger, Dan Jurafsky, Tatsunori Hashimoto, and James Zou.
\newblock Safety-tuned llamas: Lessons from improving the safety of large language models that follow instructions.
\newblock In {\em arXiv}, 2023.

\bibitem{burgess2023hacking}
Matt Burgess.
\newblock The hacking of {ChatGPT} is just getting started.
\newblock \url{https://www.wired.com/story/chatgpt-jailbreak-generative-ai-hacking/}, 2023.

\bibitem{cao2023defending}
Bochuan Cao, Yuanpu Cao, Lu~Lin, and Jinghui Chen.
\newblock Defending against alignment-breaking attacks via robustly aligned llm.
\newblock In {\em arXiv}, 2023.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em S$\&$P}, 2017.

\bibitem{chao2023jailbreaking}
Patrick Chao, Alexander Robey, Edgar Dobriban, Hamed Hassani, George~J Pappas, and Eric Wong.
\newblock Jailbreaking black box large language models in twenty queries.
\newblock In {\em arXiv}, 2023.

\bibitem{christian2023jailbreak}
Jon Christian.
\newblock Amazing "jailbreak" bypasses {ChatGPT}'s ethics safeguards.
\newblock \url{https://futurism.com/amazing-jailbreak-chatgpt}, 2023.

\bibitem{deng2023attack}
Boyi Deng, Wenjie Wang, Fuli Feng, Yang Deng, Qifan Wang, and Xiangnan He.
\newblock Attack prompt generation for red teaming and defending large language models.
\newblock In {\em arXiv}, 2023.

\bibitem{deng2023multilingual}
Yue Deng, Wenxuan Zhang, Sinno~Jialin Pan, and Lidong Bing.
\newblock Multilingual jailbreak challenges in large language models.
\newblock In {\em ICLR}, 2024.

\bibitem{dong2022survey}
Qingxiu Dong, Lei Li, Damai Dai, Ce~Zheng, Zhiyong Wu, Baobao Chang, Xu~Sun, Jingjing Xu, and Zhifang Sui.
\newblock A survey on in-context learning.
\newblock In {\em arXiv}, 2022.

\bibitem{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et~al.
\newblock The llama 3 herd of models.
\newblock {\em arXiv preprint arXiv:2407.21783}, 2024.

\bibitem{cold-attack}
Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, and Bin Hu.
\newblock Cold-attack: Jailbreaking llms with stealthiness and controllability.
\newblock In {\em ICML}, 2024.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In {\em ICLR}, 2021.

\bibitem{huang2021unlearnable}
Hanxun Huang, Xingjun Ma, Sarah~Monazam Erfani, James Bailey, and Yisen Wang.
\newblock Unlearnable examples: Making personal data unexploitable.
\newblock In {\em ICLR}, 2021.

\bibitem{imani2023mathprompter}
Shima Imani, Liang Du, and Harsh Shrivastava.
\newblock Mathprompter: Mathematical reasoning using large language models.
\newblock In {\em SIGIR}, 2023.

\bibitem{jain2023baseline}
Neel Jain, Avi Schwarzschild, Yuxin Wen, Gowthami Somepalli, John Kirchenbauer, Ping yeh Chiang, Micah Goldblum, Aniruddha Saha, Jonas Geiping, and Tom Goldstein.
\newblock Baseline defenses for adversarial attacks against aligned language models.
\newblock In {\em arXiv}, 2023.

\bibitem{jia2024improved}
Xiaojun Jia, Tianyu Pang, Chao Du, Yihao Huang, Jindong Gu, Yang Liu, Xiaochun Cao, and Min Lin.
\newblock Improved techniques for optimization-based jailbreaking on large language models.
\newblock In {\em arXiv}, 2024.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{kanepajs2024towards}
Art{\=u}rs Kanepajs, Vladimir Ivanov, and Richard Moulange.
\newblock Towards safe multilingual frontier ai.
\newblock {\em arXiv preprint arXiv:2409.13708}, 2024.

\bibitem{kumar2023certifying}
Aounon Kumar, Chirag Agarwal, Suraj Srinivas, Aaron~Jiaxun Li, Soheil Feizi, and Himabindu Lakkaraju.
\newblock Certifying llm safety against adversarial prompting.
\newblock In {\em arXiv}, 2023.

\bibitem{li2023multistep}
Haoran Li, Dadi Guo, Wei Fan, Mingshi Xu, Jie Huang, Fanpu Meng, and Yangqiu Song.
\newblock Multi-step jailbreaking privacy attacks on chatgpt.
\newblock In {\em EMNLP}, 2023.

\bibitem{li2024cross}
Jie Li, Yi~Liu, Chongyang Liu, Ling Shi, Xiaoning Ren, Yaowen Zheng, Yang Liu, and Yinxing Xue.
\newblock A cross-language investigation into jailbreak attacks in large language models.
\newblock In {\em arXiv}, 2024.

\bibitem{liu2024your}
Jiawei Liu, Chunqiu~Steven Xia, Yuyao Wang, and Lingming Zhang.
\newblock Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.
\newblock In {\em NeurIPS}, 2024.

\bibitem{liu2023improving}
Yixin Liu, Avi Singh, C~Daniel Freeman, John~D Co-Reyes, and Peter~J Liu.
\newblock Improving large language model fine-tuning for solving math problems.
\newblock In {\em arXiv}, 2023.

\bibitem{ma2021finding}
Chen Ma, Xiangyu Guo, Li~Chen, Jun-Hai Yong, and Yisen Wang.
\newblock Finding optimal tangent points for reducing distortions of hard-label attacks.
\newblock In {\em NeurIPS}, 2021.

\bibitem{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em arXiv}, 2017.

\bibitem{mehrotra2023tree}
Anay Mehrotra, Manolis Zampetakis, Paul Kassianik, Blaine Nelson, Hyrum Anderson, Yaron Singer, and Amin Karbasi.
\newblock Tree of attacks: Jailbreaking black-box llms automatically.
\newblock In {\em arXiv}, 2023.

\bibitem{mo2022adversarial}
Yichuan Mo, Dongxian Wu, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock When adversarial training meets vision transformers: Recipes from training to architecture.
\newblock In {\em NeurIPS}, 2022.

\bibitem{ouyang2022training}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock In {\em NeurIPS}, 2022.

\bibitem{perez2022ignore}
F{\'a}bio Perez and Ian Ribeiro.
\newblock Ignore previous prompt: Attack techniques for language models.
\newblock In {\em arXiv}, 2022.

\bibitem{phute2023llm}
Mansi Phute, Alec Helbling, Matthew~Daniel Hull, ShengYun Peng, Sebastian Szyller, Cory Cornelius, and Duen~Horng Chau.
\newblock Llm self defense: By self examination, llms know they are being tricked.
\newblock In {\em The Second Tiny Papers Track at ICLR}, 2024.

\bibitem{rafailov2024direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher~D Manning, Stefano Ermon, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a reward model.
\newblock In {\em NeurIPS}, 2024.

\bibitem{robey2023smoothllm}
Alexander Robey, Eric Wong, Hamed Hassani, and George Pappas.
\newblock Smoothllm: Defending large language models against jailbreaking attacks.
\newblock In {\em NeurIPS Workshop R0-FoMo}, 2023.

\bibitem{shanahan2023role}
Murray Shanahan, Kyle McDonell, and Laria Reynolds.
\newblock Role play with large language models.
\newblock {\em Nature}, 623(7987):493--498, 2023.

\bibitem{shayegani2023survey}
Erfan Shayegani, Md~Abdullah~Al Mamun, Yu~Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh.
\newblock Survey of vulnerabilities in large language models revealed by adversarial attacks.
\newblock In {\em arXiv}, 2023.

\bibitem{shen2023anything}
Xinyue Shen, Zeyuan Chen, Michael Backes, Yun Shen, and Yang Zhang.
\newblock " do anything now": Characterizing and evaluating in-the-wild jailbreak prompts on large language models.
\newblock In {\em CCS}, 2024.

\bibitem{walker2022dan}
Walker Spider.
\newblock Dan is my new friend.
\newblock \url{https://www.reddit.com/r/ChatGPT/comments/zlcyr9/dan_is_my_new_friend/}, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian~Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
  Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock In {\em arXiv}, 2023.

\bibitem{wang2022self}
Hongjun Wang and Yisen Wang.
\newblock Self-ensemble adversarial training for improved robustness.
\newblock In {\em ICLR}, 2022.

\bibitem{wang2023simple}
Hongjun Wang and Yisen Wang.
\newblock Generalist: Decoupling natural and robust generalization.
\newblock In {\em CVPR}, 2023.

\bibitem{wang2024theoretical}
Yifei Wang, Yuyang Wu, Zeming Wei, Stefanie Jegelka, and Yisen Wang.
\newblock A theoretical understanding of self-correction through in-context alignment.
\newblock In {\em arXiv}, 2024.

\bibitem{wang2019dynamic}
Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu.
\newblock On the convergence and robustness of adversarial training.
\newblock In {\em ICML}, 2019.

\bibitem{wang2024adversarial}
Yisen Wang, Yichuan Mo, Dongxian Wu, Mingjie Li, Xingjun Ma, and Zhouchen Lin.
\newblock On the adversarial transferability of generalized" skip connections".
\newblock In {\em arXiv}, 2024.

\bibitem{wang2020improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified examples.
\newblock In {\em ICLR}, 2020.

\bibitem{wang2023rolellm}
Zekun~Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, et~al.
\newblock Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models.
\newblock In {\em arXiv}, 2023.

\bibitem{wei2023jailbroken}
Alexander Wei, Nika Haghtalab, and Jacob Steinhardt.
\newblock Jailbroken: How does llm safety training fail?
\newblock In {\em NeurIPS}, 2023.

\bibitem{wei2023cfa}
Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang.
\newblock Cfa: Class-wise calibrated fair adversarial training.
\newblock In {\em CVPR}, 2023.

\bibitem{wei_jailbreak_2023}
Zeming Wei, Yifei Wang, and Yisen Wang.
\newblock Jailbreak and guard aligned language models with only few in-context demonstrations.
\newblock In {\em arXiv}, 2023.

\bibitem{wu2020skip}
Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma.
\newblock Skip connections matter: On the transferability of adversarial examples generated with resnets.
\newblock In {\em ICLR}, 2020.

\bibitem{wu2020adversarial}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In {\em NeurIPS}, 2020.

\bibitem{wu2023brief}
Tianyu Wu, Shizhu He, Jingping Liu, Siqi Sun, Kang Liu, Qing-Long Han, and Yang Tang.
\newblock A brief overview of chatgpt: The history, status quo and potential future development.
\newblock {\em IEEE/CAA Journal of Automatica Sinica}, 10(5):1122--1136, 2023.

\bibitem{xie2023defending}
Yueqi Xie, Jingwei Yi, Jiawei Shao, Justin Curl, Lingjuan Lyu, Qifeng Chen, Xing Xie, and Fangzhao Wu.
\newblock Defending chatgpt against jailbreak attack via self-reminders.
\newblock {\em Nature Machine Intelligence}, 5(12):1486--1496, 2023.

\bibitem{xu2024safedecoding}
Zhangchen Xu, Fengqing Jiang, Luyao Niu, Jinyuan Jia, Bill~Yuchen Lin, and Radha Poovendran.
\newblock Safedecoding: Defending against jailbreak attacks via safety-aware decoding.
\newblock In {\em ACL}, 2024.

\bibitem{yao2024survey}
Yifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun, and Yue Zhang.
\newblock A survey on large language model (llm) security and privacy: The good, the bad, and the ugly.
\newblock {\em High-Confidence Computing}, page 100211, 2024.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em ICML}, 2019.

\bibitem{zhang2023planning}
Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua~B Tenenbaum, and Chuang Gan.
\newblock Planning with large language models for code generation.
\newblock In {\em ICLR}, 2023.

\bibitem{zhang2024duality}
Yihao Zhang, Hangzhou He, Jingyu Zhu, Huanran Chen, Yifei Wang, and Zeming Wei.
\newblock On the duality between sharpness-aware minimization and adversarial training.
\newblock In {\em ICML 2024}, 2024.

\bibitem{zhang2024towards}
Yihao Zhang, Zeming Wei, Jun Sun, and Meng Sun.
\newblock Towards general conceptual model editing via adversarial representation engineering.
\newblock In {\em NeurIPS}, 2024.

\bibitem{zhang2024safe}
Zhexin Zhang, Junxiao Yang, Pei Ke, Shiyao Cui, Chujie Zheng, Hongning Wang, and Minlie Huang.
\newblock Safe unlearning: A surprisingly effective and generalizable solution to defend against jailbreak attacks.
\newblock In {\em arXiv}, 2024.

\bibitem{zheng2024prompt}
Chujie Zheng, Fan Yin, Hao Zhou, Fandong Meng, Jie Zhou, Kai-Wei Chang, Minlie Huang, and Nanyun Peng.
\newblock On prompt-driven safeguarding for large language models.
\newblock In {\em ICML}, 2024.

\bibitem{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock In {\em NeurIPS}, 2024.

\bibitem{zhou2024virtual}
Yuqi Zhou, Lin Lu, Hanchi Sun, Pan Zhou, and Lichao Sun.
\newblock Virtual context: Enhancing jailbreak attacks with special token injection.
\newblock In {\em arXiv}, 2024.

\bibitem{autodan}
Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Zichao Wang, Furong Huang, Ani Nenkova, and Tong Sun.
\newblock Autodan: Interpretable gradient-based adversarial attacks on large language models.
\newblock In {\em COLM}, 2024.

\bibitem{GCG}
Andy Zou, Zifan Wang, J.~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language models.
\newblock In {\em arXiv}, 2023.

\bibitem{zou2024system}
Xiaotian Zou, Yongkang Chen, and Ke~Li.
\newblock Is the system message really important to jailbreaks in large language models?
\newblock In {\em arXiv}, 2024.

\end{thebibliography}
