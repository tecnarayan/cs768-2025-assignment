\begin{thebibliography}{10}

\bibitem{Maas1997NetworksOS}
Wofgang Maass.
\newblock Networks of spiking neurons: the third generation of neural network models.
\newblock {\em Neural Networks}, 14:1659--1671, 1997.

\bibitem{Fang2021DeepRL}
Wei Fang, Zhaofei Yu, Yanqing Chen, Tiejun Huang, Timoth{\'e}e Masquelier, and Yonghong Tian.
\newblock Deep residual learning in spiking neural networks.
\newblock In {\em Neural Information Processing Systems}, 2021.

\bibitem{Ding2021OptimalAC}
Jianhao Ding, Zhaofei Yu, Yonghong Tian, and Tiejun Huang.
\newblock Optimal {ANN-SNN} conversion for fast and accurate inference in deep spiking neural networks.
\newblock In Zhi{-}Hua Zhou, editor, {\em Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI} 2021}, pages 2328--2336, 2021.

\bibitem{Zhou2022SpikformerWS}
Zhaokun Zhou, Yuesheng Zhu, Chao He, Yaowei Wang, Shuicheng Yan, Yonghong Tian, and Li~Yuan.
\newblock Spikformer: When spiking neural network meets transformer.
\newblock In {\em The Eleventh International Conference on Learning Representations, {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023}, 2023.

\bibitem{yao2023spike}
Man Yao, JiaKui Hu, Zhaokun Zhou, Li~Yuan, Yonghong Tian, XU~Bo, and Guoqi Li.
\newblock Spike-driven transformer.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems}, 2023.

\bibitem{yao2024spikedriven}
Man Yao, JiaKui Hu, Tianxiang Hu, Yifan Xu, Zhaokun Zhou, Yonghong Tian, Bo~XU, and Guoqi Li.
\newblock Spike-driven transformer v2: Meta spiking neural network architecture inspiring the design of next-generation neuromorphic chips.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{Fang2020IncorporatingLM}
Wei Fang, Zhaofei Yu, Yanqing Chen, Timoth{\'e}e Masquelier, Tiejun Huang, and Yonghong Tian.
\newblock Incorporating learnable membrane time constant to enhance learning of spiking neural networks.
\newblock {\em 2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, pages 2641--2651, 2020.

\bibitem{lv2023spiking}
Changze Lv, Jianhan Xu, and Xiaoqing Zheng.
\newblock Spiking convolutional neural networks for text classification.
\newblock In {\em The Eleventh International Conference on Learning Representations (ICLR)}, 2023.

\bibitem{li2024seenn}
Yuhang Li, Tamar Geller, Youngeun Kim, and Priyadarshini Panda.
\newblock Seenn: Towards temporal spiking early exit neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{wu2018spatio}
Yujie Wu, Lei Deng, Guoqi Li, and Luping Shi.
\newblock Spatio-temporal backpropagation for training high-performance spiking neural networks.
\newblock {\em Frontiers in neuroscience}, 12:323875, 2018.

\bibitem{zheng2021going}
Hanle Zheng, Yujie Wu, Lei Deng, Yifan Hu, and Guoqi Li.
\newblock Going deeper with directly-trained larger spiking neural networks.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11062--11070, 2021.

\bibitem{duan2022temporal}
Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, and Tiejun Huang.
\newblock Temporal effective batch normalization in spiking neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 35:34377--34390, 2022.

\bibitem{zhou2023spikingformer}
Chenlin Zhou, Liutao Yu, Zhaokun Zhou, Han Zhang, Zhengyu Ma, Huihui Zhou, and Yonghong Tian.
\newblock Spikingformer: Spike-driven residual learning for transformer-based spiking neural network.
\newblock {\em arXiv preprint arXiv:2304.11954}, 2023.

\bibitem{gu2020hippo}
Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Hippo: Recurrent memory with optimal polynomial projections.
\newblock {\em Advances in neural information processing systems}, 33:1474--1487, 2020.

\bibitem{Vaswani2017AttentionIA}
Ashish Vaswani, Noam~M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em ArXiv}, abs/1706.03762, 2017.

\bibitem{zador2019critique}
Anthony~M Zador.
\newblock A critique of pure learning and what artificial neural networks can learn from animal brains.
\newblock {\em Nature communications}, 10(1):3770, 2019.

\bibitem{mitchell2021ai}
Melanie Mitchell.
\newblock Why ai is harder than we think.
\newblock In {\em Proceedings of the Genetic and Evolutionary Computation Conference}, pages 3--3, 2021.

\bibitem{marder2001central}
Eve Marder and Dirk Bucher.
\newblock Central pattern generators and the control of rhythmic movements.
\newblock {\em Current biology}, 11(23):R986--R996, 2001.

\bibitem{marder1996principles}
Eve Marder and Ronald~L Calabrese.
\newblock Principles of rhythmic motor pattern generation.
\newblock {\em Physiological reviews}, 76(3):687--717, 1996.

\bibitem{grillner2006biological}
Sten Grillner.
\newblock Biological pattern generation: the cellular and computational logic of networks in motion.
\newblock {\em Neuron}, 52(5):751--766, 2006.

\bibitem{kiehn2016decoding}
Ole Kiehn.
\newblock Decoding the organization of spinal circuits that control locomotion.
\newblock {\em Nature Reviews Neuroscience}, 17(4):224--238, 2016.

\bibitem{liu2022petr}
Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.
\newblock Petr: Position embedding transformation for multi-view 3d object detection.
\newblock In {\em European Conference on Computer Vision}, pages 531--548. Springer, 2022.

\bibitem{dosovitskiy2021an}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{SpikingJelly}
Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Yonghong Tian, and other contributors.
\newblock Spikingjelly, 2020.

\bibitem{Werbos1990BackpropagationTT}
Paul~J. Werbos.
\newblock Backpropagation through time: What it does and how to do it.
\newblock {\em Proc. IEEE}, 78:1550--1560, 1990.

\bibitem{shaw-etal-2018-self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In {\em Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)}, pages 464--468. Association for Computational Linguistics, 2018.

\bibitem{shiv2019novel}
Vighnesh Shiv and Chris Quirk.
\newblock Novel positional encodings to enable tree-based transformers.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{raffel2020exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{Lv2023SpikeBERTAL}
Changze Lv, Tianlong Li, Jianhan Xu, Chenxi Gu, Zixuan Ling, Cenyuan Zhang, Xiaoqing Zheng, and Xuanjing Huang.
\newblock Spikebert: A language spikformer learned from bert with knowledge distillation.
\newblock 2023.

\bibitem{bal2024spikingbert}
Malyaban Bal and Abhronil Sengupta.
\newblock Spikingbert: Distilling bert to train spiking language models using implicit differentiation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 10998--11006, 2024.

\bibitem{zhu2023spikegpt}
Rui-Jie Zhu, Qihang Zhao, and Jason~K Eshraghian.
\newblock Spikegpt: Generative pre-trained language model with spiking neural networks.
\newblock {\em arXiv preprint arXiv:2302.13939}, 2023.

\bibitem{lv2024efficient}
Changze Lv, Yansen Wang, Dongqi Han, Xiaoqing Zheng, Xuanjing Huang, and Dongsheng Li.
\newblock Efficient and effective time-series forecasting with spiking neural networks.
\newblock In {\em Forty-first International Conference on Machine Learning (ICML)}, 2024.

\bibitem{li2017diffusion}
Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu.
\newblock Diffusion convolutional recurrent neural network: Data-driven traffic forecasting.
\newblock {\em arXiv preprint arXiv:1707.01926}, 2017.

\bibitem{lai2018modeling}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In {\em The 41st International ACM SIGIR Conference on Research \& Development in Information Retrieval}, pages 95--104, 2018.

\bibitem{Pang2005SeeingSE}
Bo~Pang and Lillian Lee.
\newblock Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.
\newblock In {\em ACL}, 2005.

\bibitem{Socher2013RecursiveDM}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning, A.~Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In {\em EMNLP}, 2013.

\bibitem{Li2017CIFAR10DVSAE}
Hongmin Li, Hanchao Liu, Xiangyang Ji, Guoqi Li, and Luping Shi.
\newblock Cifar10-dvs: An event-stream dataset for object classification.
\newblock {\em Frontiers in Neuroscience}, 11, 2017.

\bibitem{bai2018empirical}
Shaojie Bai, J~Zico Kolter, and Vladlen Koltun.
\newblock An empirical evaluation of generic convolutional and recurrent networks for sequence modeling.
\newblock {\em arXiv preprint arXiv:1803.01271}, 2018.

\bibitem{Devlin2019BERTPO}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em North American Chapter of the Association for Computational Linguistics}, 2019.

\bibitem{zhou2024spikformer}
Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, and Li~Yuan.
\newblock Spikformer v2: Join the high accuracy club on imagenet with an snn ticket.
\newblock {\em arXiv preprint arXiv:2401.02020}, 2024.

\bibitem{kim2022rate}
Youngeun Kim, Hyoungseob Park, Abhishek Moitra, Abhiroop Bhattacharjee, Yeshwanth Venkatesha, and Priyadarshini Panda.
\newblock Rate coding or direct coding: Which one is better for accurate, robust, and energy-efficient spiking neural networks?
\newblock In {\em ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, pages 71--75. IEEE, 2022.

\bibitem{han2020deep}
Bing Han and Kaushik Roy.
\newblock Deep spiking neural network: Energy efficiency through time based coding.
\newblock In {\em European conference on computer vision}, pages 388--404. Springer, 2020.

\bibitem{comsa2022temporal}
IM~Comsa, K~Potempa, L~Versari, T~Fischbacher, A~Gesmundo, and J~Alakuijala.
\newblock Temporal coding in spiking neural networks with alpha synaptic function: Learning with backpropagation.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems}, 33(10):5939--5952, 2022.

\bibitem{yoon2016lif}
Young~C Yoon.
\newblock Lif and simplified srm neurons encode signals into spikes via a form of asynchronous pulse sigma--delta modulation.
\newblock {\em IEEE transactions on neural networks and learning systems}, 28(5):1192--1205, 2016.

\bibitem{zhao2024dynamic}
Han Zhao, Xu~Yang, Cheng Deng, and Junchi Yan.
\newblock Dynamic reactive spiking graph neural network.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 16970--16978, 2024.

\bibitem{yuste2005cortex}
Rafael Yuste, Jason~N MacLean, Jeffrey Smith, and Anders Lansner.
\newblock The cortex as a central pattern generator.
\newblock {\em Nature Reviews Neuroscience}, 6(6):477--483, 2005.

\bibitem{marblestone2016toward}
Adam~H Marblestone, Greg Wayne, and Konrad~P Kording.
\newblock Toward an integration of deep learning and neuroscience.
\newblock {\em Frontiers in computational neuroscience}, 10:94, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{loshchilov2018decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\end{thebibliography}
