\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
A.~Agarwal, S.~Kakade, A.~Krishnamurthy, and W.~Sun.
\newblock {FLAMBE}: Structural complexity and representation learning of low
  rank {MDP}s.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Auer and Ortner(2007)]{auer06logarithmic}
P.~Auer and R.~Ortner.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In B.~Sch\"{o}lkopf, J.~Platt, and T.~Hoffman, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~19, pages
  49--56. MIT Press, 2007.

\bibitem[Awerbuch and Kleinberg(2004)]{AK04}
B.~Awerbuch and R.~D. Kleinberg.
\newblock Adaptive routing with end-to-end feedback: distributed learning and
  geometric approaches.
\newblock In \emph{STOC 2004}, pages 45--53, 2004.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{AOM17}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 263--272, 2017.

\bibitem[Bas-Serrano et~al.(2021)Bas-Serrano, Curi, Krause, and Neu]{qreps}
J.~Bas-Serrano, S.~Curi, A.~Krause, and G.~Neu.
\newblock Logistic {Q}-learning.
\newblock In \emph{AI \& Statistics}, pages 3610--3618, 2021.

\bibitem[Burnetas and Katehakis(1997)]{BK97}
A.~N. Burnetas and M.~N. Katehakis.
\newblock Optimal adaptive policies for {M}arkov {D}ecision {P}rocesses.
\newblock \emph{Mathematics of Operations Research}, 22\penalty0 (1):\penalty0
  222--255, 1997.

\bibitem[{Cai} et~al.(2019){Cai}, {Yang}, {Jin}, and
  {Wang}]{2019arXiv191205830C}
Q.~{Cai}, Z.~{Yang}, C.~{Jin}, and Z.~{Wang}.
\newblock Provably efficient exploration in policy optimization.
\newblock \emph{arXiv e-prints}, art. arXiv:1912.05830, Dec. 2019.

\bibitem[Dani et~al.(2008)Dani, Kakade, and Hayes]{NIPS2007_3371}
V.~Dani, S.~M. Kakade, and T.~P. Hayes.
\newblock The price of bandit information for online optimization.
\newblock In \emph{Advances in Neural Information Processing Systems 20}, pages
  345--352. 2008.

\bibitem[Dann and Brunskill(2015)]{DB15}
C.~Dann and E.~Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{DLB17}
C.~Dann, T.~Lattimore, and E.~Brunskill.
\newblock Unifying {PAC} and regret: Uniform {PAC} bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30}, pages
  5713--5723. 2017.

\bibitem[Dick et~al.(2014)Dick, Gy\"orgy, and Szepesv\'ari]{DGySz14}
T.~Dick, A.~Gy\"orgy, and {\relax{Cs}}.~Szepesv\'ari.
\newblock Online learning in {Markov} decision processes with changing cost
  sequences.
\newblock In \emph{International Conference on Machine Learning}, pages
  512--520, 2014.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and
  Mansour]{10.1287/moor.1090.0396}
E.~Even-Dar, S.~M. Kakade, and Y.~Mansour.
\newblock Online {Markov} decision processes.
\newblock \emph{Math. Oper. Res.}, 34\penalty0 (3):\penalty0 726â€“736, 2009.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and Ortner]{FPL18}
R.~Fruit, M.~Pirotta, A.~Lazaric, and R.~Ortner.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1573--1581, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch10ucrl}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 99:\penalty0 1563--1600,
  August 2010.
\newblock ISSN 1532-4435.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{JAZBJ18}
C.~Jin, Z.~Allen-Zhu, S.~Bubeck, and M.~I. Jordan.
\newblock Is {Q}-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and Yu]{JJLSY20}
C.~Jin, T.~Jin, H.~Luo, S.~Sra, and T.~Yu.
\newblock Learning adversarial {MDPs} with bandit feedback and unknown
  transition.
\newblock In \emph{International Conference on Machine Learning},
  2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{2019arXiv190705388J}
C.~Jin, Z.~Yang, Z.~Wang, and M.~I. Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Proceedings of the 33rd Annual Conference on Learning Theory
  (COLT 2020)}, pages 2137--2143, 2020{\natexlab{b}}.

\bibitem[McMahan and Blum(2004)]{MB04}
H.~B. McMahan and A.~Blum.
\newblock Online geometric optimization in the bandit setting against an
  adaptive adversary.
\newblock In \emph{COLT 2004}, pages 109--123, 2004.

\bibitem[Neu and Bart\'ok(2013)]{NeuBartok13}
G.~Neu and G.~Bart\'ok.
\newblock An efficient algorithm for learning with semi-bandit feedback.
\newblock In \emph{Proceedings of the 24th International Conference on
  Algorithmic Learning Theory (ALT 2013)}, pages 234--248, 2013.

\bibitem[Neu and Bart\'ok(2016)]{NB16}
G.~Neu and G.~Bart\'ok.
\newblock Importance weighting without importance weights: An efficient
  algorithm for combinatorial semi-bandits.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0 1--21,
  2016.

\bibitem[{Neu} and {Olkhovskaya}(2020)]{2020NO}
G.~{Neu} and J.~{Olkhovskaya}.
\newblock Efficient and robust algorithms for adversarial linear contextual
  bandits.
\newblock In \emph{Proceedings of the 33rd Annual Conference on Learning Theory
  (COLT 2020)}, pages 3049--3068, 2020.

\bibitem[Neu and Pike-Burke(2020)]{NPB20}
G.~Neu and C.~Pike-Burke.
\newblock A unifying view of optimism in episodic reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Neu et~al.(2010)Neu, Gy\"{o}rgy, and Szepesv\'ari]{NGS10}
G.~Neu, A.~Gy\"{o}rgy, and {\text Cs}.~Szepesv\'ari.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{Proceedings of the 23rd Annual Conference on Learning Theory
  (COLT 2010)}, pages 231--243, 2010.

\bibitem[Neu et~al.(2012)Neu, Gy\"orgy, and Szepesv\'ari]{pmlr-v22-neu12}
G.~Neu, A.~Gy\"orgy, and {\text Cs}.~Szepesv\'ari.
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \emph{Proceedings of the Fifteenth International Conference on
  Artificial Intelligence and Statistics}, pages 805--813, 2012.

\bibitem[Neu et~al.(2013)Neu, Gy\"{o}rgy, Szepesv\'ari, and Antos]{NGS13}
G.~Neu, A.~Gy\"{o}rgy, {\text Cs}.~Szepesv\'ari, and A.~Antos.
\newblock Online {M}arkov decision processes under bandit feedback.
\newblock volume~59, pages 1804--1812, 01 2013.
\newblock \doi{10.1109/TAC.2013.2292137}.

\bibitem[Orabona(2019)]{Ora19}
F.~Orabona.
\newblock A modern introduction to online learning.
\newblock \emph{arXiv preprint arXiv:1912.13213}, 2019.

\bibitem[Pacchiano et~al.(2021)Pacchiano, Lee, Bartlett, and Nachum]{PLBN21}
A.~Pacchiano, J.~Lee, P.~Bartlett, and O.~Nachum.
\newblock Near optimal policy optimization via {REPS}.
\newblock \emph{arXiv preprint arXiv:2103.09756}, 2021.

\bibitem[Peters et~al.(2010)Peters, M{\"u}lling, and Altun]{PMA10}
J.~Peters, K.~M{\"u}lling, and Y.~Altun.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI 2010}, pages 1607--1612, 2010.
\newblock ISBN 978-1-57735-463-5.

\bibitem[Pires and Szepesv{\'a}ri(2016)]{PS16}
B.~{\'A}. Pires and {\text Cs}.~Szepesv{\'a}ri.
\newblock Policy error bounds for model-based reinforcement learning with
  factored linear models.
\newblock In \emph{Conference on Learning Theory}, pages 121--151, 2016.

\bibitem[Rosenberg and Mansour(2019)]{pmlrv97rosenberg19a}
A.~Rosenberg and Y.~Mansour.
\newblock Online convex optimization in adversarial {M}arkov decision
  processes.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pages 5478--5486, 2019.

\bibitem[Tewari and Bartlett()]{tewari08optimistic}
A.~Tewari and P.~L. Bartlett.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible {MDPs}.
\newblock In \emph{Advances in Neural Information Processing Systems 20}, pages
  1505--1512.

\bibitem[Wei et~al.(2021)Wei, Jafarnia~Jahromi, Luo, and Jain]{wei2020learning}
C.-Y. Wei, M.~Jafarnia~Jahromi, H.~Luo, and R.~Jain.
\newblock Learning infinite-horizon average-reward {MDPs} with linear function
  approximation.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics}, pages 3007--3015, 2021.

\bibitem[Yang and Wang(2020)]{YW19}
L.~F. Yang and M.~Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, 2020.

\bibitem[Yao et~al.(2014)Yao, Szepesv\'ari, Pires, and Zhang]{factlin}
H.~Yao, {\text Cs}.~Szepesv\'ari, B.~Pires, and X.~Zhang.
\newblock Pseudo-{MDPs} and factored linear action models.
\newblock 10 2014.
\newblock \doi{10.1109/ADPRL.2014.7010633}.

\bibitem[Zimin and Neu(2013)]{NIPS20134974}
A.~Zimin and G.~Neu.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  26}, pages 1583--1591. Curran Associates, Inc., 2013.

\end{thebibliography}
