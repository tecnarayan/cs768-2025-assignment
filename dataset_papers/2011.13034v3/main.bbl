\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abels et~al.(2018)Abels, Roijers, Lenaerts, Now{\'e}, and
  Steckelmacher]{abels2018dynamic}
Axel Abels, Diederik~M Roijers, Tom Lenaerts, Ann Now{\'e}, and Denis
  Steckelmacher.
\newblock Dynamic weights in multi-objective deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1809.07803}, 2018.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Joshua Achiam, David Held, Aviv Tamar, and Pieter Abbeel.
\newblock Constrained policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 22--31.
  PMLR, 2017.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Brantley et~al.(2020)Brantley, Dudik, Lykouris, Miryoosefi,
  Simchowitz, Slivkins, and Sun]{brantley2020constrained}
Kiant{\'e} Brantley, Miroslav Dudik, Thodoris Lykouris, Sobhan Miryoosefi, Max
  Simchowitz, Aleksandrs Slivkins, and Wen Sun.
\newblock Constrained episodic reinforcement learning in concave-convex and
  knapsack settings.
\newblock \emph{arXiv preprint arXiv:2006.05051}, 2020.

\bibitem[Cheung(2019)]{cheung2019exploration}
Wang~Chi Cheung.
\newblock Exploration-exploitation trade-off in reinforcement learning on
  online markov decision processes with global concave rewards.
\newblock \emph{arXiv preprint arXiv:1905.06466}, 2019.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1703.07710}, 2017.

\bibitem[Efroni et~al.(2020)Efroni, Mannor, and Pirotta]{efroni2020exploration}
Yonathan Efroni, Shie Mannor, and Matteo Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem[El~Chamie et~al.(2018)El~Chamie, Yu, A{\c{c}}{\i}kme{\c{s}}e, and
  Ono]{el2018controlled}
Mahmoud El~Chamie, Yue Yu, Beh{\c{c}}et A{\c{c}}{\i}kme{\c{s}}e, and Masahiro
  Ono.
\newblock Controlled markov processes with safety state constraints.
\newblock \emph{IEEE Transactions on Automatic Control}, 64\penalty0
  (3):\penalty0 1003--1018, 2018.

\bibitem[Fisac et~al.(2018)Fisac, Akametalu, Zeilinger, Kaynama, Gillula, and
  Tomlin]{fisac2018general}
Jaime~F Fisac, Anayo~K Akametalu, Melanie~N Zeilinger, Shahab Kaynama, Jeremy
  Gillula, and Claire~J Tomlin.
\newblock A general safety framework for learning-based control in uncertain
  robotic systems.
\newblock \emph{IEEE Transactions on Automatic Control}, 64\penalty0
  (7):\penalty0 2737--2752, 2018.

\bibitem[Garc{\i}a and Fern{\'a}ndez(2015)]{garcia2015comprehensive}
Javier Garc{\i}a and Fernando Fern{\'a}ndez.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4863--4873, 2018.

\bibitem[Jin et~al.(2019)Jin, Jin, Luo, Sra, and Yu]{jin2019learning}
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu.
\newblock Learning adversarial mdps with bandit feedback and unknown
  transition, 2019.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and Yu]{jin2020reward}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.02794}, 2020.

\bibitem[Johnson and Lindenstrauss(1984)]{johnson1984extensions}
William~B Johnson and Joram Lindenstrauss.
\newblock Extensions of lipschitz mappings into a hilbert space.
\newblock \emph{Contemporary mathematics}, 26\penalty0 (189-206):\penalty0 1,
  1984.

\bibitem[Kaufmann et~al.(2020)Kaufmann, M{\'e}nard, Domingues, Jonsson,
  Leurent, and Valko]{kaufmann2020adaptive}
Emilie Kaufmann, Pierre M{\'e}nard, Omar~Darwiche Domingues, Anders Jonsson,
  Edouard Leurent, and Michal Valko.
\newblock Adaptive reward-free exploration.
\newblock \emph{arXiv preprint arXiv:2006.06294}, 2020.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock \emph{arXiv preprint arXiv:0907.3740}, 2009.

\bibitem[M{\'e}nard et~al.(2020)M{\'e}nard, Domingues, Jonsson, Kaufmann,
  Leurent, and Valko]{menard2020fast}
Pierre M{\'e}nard, Omar~Darwiche Domingues, Anders Jonsson, Emilie Kaufmann,
  Edouard Leurent, and Michal Valko.
\newblock Fast active learning for pure exploration in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.13442}, 2020.

\bibitem[Mossalam et~al.(2016)Mossalam, Assael, Roijers, and
  Whiteson]{mossalam2016multi}
Hossam Mossalam, Yannis~M Assael, Diederik~M Roijers, and Shimon Whiteson.
\newblock Multi-objective deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1610.02707}, 2016.

\bibitem[Natarajan and Tadepalli(2005)]{natarajan2005dynamic}
Sriraam Natarajan and Prasad Tadepalli.
\newblock Dynamic preferences in multi-criteria reinforcement learning.
\newblock In \emph{Proceedings of the 22nd international conference on Machine
  learning}, pages 601--608, 2005.

\bibitem[Neu et~al.(2012)Neu, Gyorgy, and Szepesv{\'a}ri]{neu2012adversarial}
Gergely Neu, Andras Gyorgy, and Csaba Szepesv{\'a}ri.
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 805--813,
  2012.

\bibitem[Roijers et~al.(2013)Roijers, Vamplew, Whiteson, and
  Dazeley]{roijers2013survey}
Diederik~M Roijers, Peter Vamplew, Shimon Whiteson, and Richard Dazeley.
\newblock A survey of multi-objective sequential decision-making.
\newblock \emph{Journal of Artificial Intelligence Research}, 48:\penalty0
  67--113, 2013.

\bibitem[Rosenberg and Mansour(2019)]{rosenberg2019online}
Aviv Rosenberg and Yishay Mansour.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock \emph{arXiv preprint arXiv:1905.07773}, 2019.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Wachi and Sui(2020)]{wachi2020safe}
Akifumi Wachi and Yanan Sui.
\newblock Safe reinforcement learning in constrained markov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  9797--9806. PMLR, 2020.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Salakhutdinov]{wang2020reward}
Ruosong Wang, Simon~S Du, Lin~F Yang, and Ruslan Salakhutdinov.
\newblock On reward-free reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2006.11274}, 2020.

\bibitem[Yang et~al.(2019)Yang, Sun, and Narasimhan]{yang2019generalized}
Runzhe Yang, Xingyuan Sun, and Karthik Narasimhan.
\newblock A generalized algorithm for multi-objective reinforcement learning
  and policy adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14610--14621, 2019.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning
  without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages
  7304--7312. PMLR, 2019.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Ma, and Singla]{zhang2020task}
Xuezhou Zhang, Yuzhe Ma, and Adish Singla.
\newblock Task-agnostic exploration in reinforcement learning,
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Du, and Ji]{zhang2020nearly}
Zihan Zhang, Simon~S Du, and Xiangyang Ji.
\newblock Nearly minimax optimal reward-free reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05901}, 2020{\natexlab{b}}.

\end{thebibliography}
