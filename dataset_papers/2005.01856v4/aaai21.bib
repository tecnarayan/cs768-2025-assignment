
@article{motiian_unified_2017,
	title = {Unified {Deep} {Supervised} {Domain} {Adaptation} and {Generalization}},
	journal = {arXiv},
	author = {Motiian, Saeid and Piccirilli, Marco and Adjeroh, Donald A. and Doretto, Gianfranco},
	year = {2017},
}

@article{shankar_generalizing_2018,
	title = {Generalizing {Across} {Domains} via {Cross}-{Gradient} {Training}},
	journal = {arXiv},
	author = {Shankar, Shiv and Piratla, Vihari and Chakrabarti, Soumen and Chaudhuri, Siddhartha and Jyothi, Preethi and Sarawagi, Sunita},
	year = {2018},
}

@inproceedings{ghifary_domain_2015,
	title = {Domain {Generalization} for {Object} {Recognition} with {Multi}-task {Autoencoders}},
	booktitle = {ICCV},
	author = {Ghifary, Muhammad and Kleijn, W. Bastiaan and Zhang, Mengjie and Balduzzi, David},
	year = {2015},
}

@article{carlucci_agnostic_2018,
	title = {{Hallucinating Agnostic Images to Generalize Across Domains}},
	journal = {arXiv},
	author = {Carlucci, Fabio M. and Russo, Paolo and Tommasi, Tatiana and Caputo, Barbara},
	year = {2018}
}

@article{ding_deep_2018,
	title = {Deep {Domain} {Generalization} {With} {Structured} {Low}-{Rank} {Constraint}},
	volume = {27},
	number = {1},
	journal = {IEEE Transactions on Image Processing},
	author = {Ding, Zhengming and Fu, Yun},
	year = {2018},
	pages = {304--313},
}

@article{arjovsky_invariant_2019,
	title = {Invariant {Risk} {Minimization}},
	journal = {arXiv},
	author = {Arjovsky, Martin and Bottou, Léon and Gulrajani, Ishaan and Lopez-Paz, David},
	year = {2019},
}

@article{magliacane_domain_nodate,
	title = {Domain {Adaptation} by {Using} {Causal} {Inference} to {Predict} {Invariant} {Conditional} {Distributions}},
	author = {Magliacane, Sara},
}

@article{mhasawade_population-aware_2019,
	title = {Population-aware {Hierarchical} {Bayesian} {Domain} {Adaptation} via {Multiple}-component {Invariant} {Learning}},
	journal = {arXiv},
	author = {Mhasawade, Vishwali and Rehman, Nabeel Abdur and Chunara, Rumi},
	year = {2019},
}

@article{van_ommen_robust_nodate,
	title = {Robust {Causal} {Domain} {Adaptation} in a {Simple} {Diagnostic} {Setting}},
	author = {van Ommen, Thijs and Vanommen, T and Nl, Uu},
}

@article{singh_fair_2019,
	title = {Fair {Predictors} under {Distribution} {Shift}},
	journal = {arXiv},
	author = {Singh, Harvineet and Singh, Rina and Mhasawade, Vishwali and Chunara, Rumi},
	year = {2019},
}

@article{suter_robustly_2019,
	title = {Robustly {Disentangled} {Causal} {Mechanisms}: {Validating} {Deep} {Representations} for {Interventional} {Robustness}},
	journal = {arXiv},
	author = {Suter, Raphael and Miladinovi\'c, Đorđe and Schölkopf, Bernhard and Bauer, Stefan},
	year = {2019},
}

@article{long_conditional_2018,
	title = {Conditional {Adversarial} {Domain} {Adaptation}},
	journal = {arXiv},
	author = {Long, Mingsheng and Cao, Zhangjie and Wang, Jianmin and Jordan, Michael I.},
	year = {2018},
}

@article{heinze-deml_conditional_2019,
	title = {Conditional {Variance} {Penalties} and {Domain} {Shift} {Robustness}},
	journal = {arXiv},
	author = {Heinze-Deml, Christina and Meinshausen, Nicolai},
	year = {2019},
}

@article{gowal_achieving_2019,
	title = {Achieving {Robustness} in the {Wild} via {Adversarial} {Mixing} with {Disentangled} {Representations}},
	journal = {arXiv},
	author = {Gowal, Sven and Qin, Chongli and Huang, Po-Sen and Cemgil, Taylan and Dvijotham, Krishnamurthy and Mann, Timothy and Kohli, Pushmeet},
	year = {2019},
}

@article{johansson_support_2019,
	title = {Support and {Invertibility} in {Domain}-{Invariant} {Representations}},
	journal = {arXiv},
	author = {Johansson, Fredrik D. and Sontag, David and Ranganath, Rajesh},
	year = {2019},
}

@article{zhao_learning_2019,
	title = {On {Learning} {Invariant} {Representation} for {Domain} {Adaptation}},
	journal = {arXiv},
	author = {Zhao, Han and Combes, Remi Tachet des and Zhang, Kun and Gordon, Geoffrey J.},
	year = {2019},
}

@article{tzeng_deep_2014,
	title = {Deep {Domain} {Confusion}: {Maximizing} for {Domain} {Invariance}},
	journal = {arXiv},
	author = {Tzeng, Eric and Hoffman, Judy and Zhang, Ning and Saenko, Kate and Darrell, Trevor},
	year = {2014},
}

@article{blom_upper_2018,
	title = {An {Upper} {Bound} for {Random} {Measurement} {Error} in {Causal} {Discovery}},
	journal = {UAI},
	author = {Blom, Tineke and Klimovskaia, Anna and Magliacane, Sara and Mooij, Joris M.},
	year = {2018},
}

@article{tobin_domain_2017,
	title = {Domain {Randomization} for {Transferring} {Deep} {Neural} {Networks} from {Simulation} to the {Real} {World}},
	journal = {arXiv},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	year = {2017},
}

@article{shrivastava_learning_2017,
	title = {Learning from {Simulated} and {Unsupervised} {Images} through {Adversarial} {Training}},
	journal = {arXiv},
	author = {Shrivastava, Ashish and Pfister, Tomas and Tuzel, Oncel and Susskind, Josh and Wang, Wenda and Webb, Russ},
	year = {2017},
}

@article{zhang_mixup_2018,
	title = {mixup: {Beyond} {Empirical} {Risk} {Minimization}},
	journal = {arXiv},
	author = {Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N. and Lopez-Paz, David},
	year = {2018},
}

@article{carlucci_domain_2019,
	title = {Domain {Generalization} by {Solving} {Jigsaw} {Puzzles}},
	journal = {arXiv},
	author = {Carlucci, Fabio Maria and D'Innocente, Antonio and Bucci, Silvia and Caputo, Barbara and Tommasi, Tatiana},
	year = {2019},
}

@incollection{balaji_metareg_2018,
	title = {{MetaReg}: {Towards} {Domain} {Generalization} using {Meta}-{Regularization}},
	booktitle = {NeurIPS},
	author = {Balaji, Yogesh and Sankaranarayanan, Swami and Chellappa, Rama},
	year = {2018}
}

@article{mancini_best_2018,
	title = {Best sources forward: domain generalization through source-specific nets},
	journal = {arXiv},
	author = {Mancini, Massimiliano and Bulò, Samuel Rota and Caputo, Barbara and Ricci, Elisa},
	year = {2018},
}

@inproceedings{wang_learning_2018,
	title = {Learning {Robust} {Representations} by {Projecting} {Superficial} {Statistics} {Out}},
	author = {Wang, Haohan and He, Zexue and Lipton, Zachary C. and Xing, Eric P.},
	booktitle={ICLR},
	year = {2018}
}

@article{li_learning_2017,
	title = {Learning to {Generalize}: {Meta}-{Learning} for {Domain} {Generalization}},
	journal = {arXiv},
	author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
	year = {2017},
}

@article{ilse_diva_2019,
	title = {{DIVA}: {Domain} {Invariant} {Variational} {Autoencoders}},
	journal = {arXiv},
	author = {Ilse, Maximilian and Tomczak, Jakub M. and Louizos, Christos and Welling, Max},
	year = {2019},
}

@article{tellez_quantifying_2019,
	title = {Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology},
	journal = {arXiv},
	author = {Tellez, David and Litjens, Geert and Bandi, Peter and Bulten, Wouter and Bokhorst, John-Melle and Ciompi, Francesco and van der Laak, Jeroen},
	year = {2019},
}

@article{tobin_domain_2017-1,
	title = {Domain {Randomization} for {Transferring} {Deep} {Neural} {Networks} from {Simulation} to the {Real} {World}},
	journal = {arXiv},
	author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
	year = {2017},
}

@article{peters_causal_2016,
	title = {Causal inference by using invariant prediction: identification and confidence intervals},
	volume = {78},
	number = {5},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Peters, Jonas and Bühlmann, Peter and Meinshausen, Nicolai},
	year = {2016},
	pages = {947--1012},
}

@article{perez_effectiveness_2017,
	title = {The {Effectiveness} of {Data} {Augmentation} in {Image} {Classification} using {Deep} {Learning}},
	journal = {arXiv},
	author = {Perez, Luis and Wang, Jason},
	year = {2017},
}

@inproceedings{cubuk_autoaugment_2019,
	title = {{AutoAugment}: {Learning} {Augmentation} {Strategies} {From} {Data}},
	booktitle = {CVPR},
	author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
	year = {2019},
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {NIPS},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@article{ganin_domain-adversarial_2016,
	title = {Domain-{Adversarial} {Training} of {Neural} {Networks}},
	journal = {arXiv},
	author = {Ganin, Yaroslav and Ustinova, Evgeniya and Ajakan, Hana and Germain, Pascal and Larochelle, Hugo and Laviolette, François and Marchand, Mario and Lempitsky, Victor},
	year = {2016},
}

@incollection{hutchison_undoing_2012,
	address = {Berlin, Heidelberg},
	title = {Undoing the {Damage} of {Dataset} {Bias}},
	booktitle = {ECCV},
	author = {Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A. and Torralba, Antonio},
	year = {2012},
}

@article{pearl_causal_2009,
	title = {Causal inference in statistics: {An} overview},
	volume = {3},
	journal = {Statistics Surveys},
	author = {Pearl, Judea},
	year = {2009},
	pages = {96--146}
}

@article{azulay_why_2019,
	title = {Why do deep convolutional networks generalize so poorly to small image transformations?},
	journal = {arXiv},
	author = {Azulay, Aharon and Weiss, Yair},
	year = {2019},
}

@incollection{krizhevsky_imagenet_2012-1,
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	booktitle = {NIPS},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = {2012},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-{Based} {Learning} {Applied} to {Document} {Recognition}},
	author = {LeCun, Yann and Bottou, Leon and Bengio, Yoshua and Ha, Patrick},
	year = {1998},
}

@inproceedings{li_deeper_2017,
	title = {Deeper, {Broader} and {Artier} {Domain} {Generalization}},
	booktitle = {ICCV},
	author = {Li, Da and Yang, Yongxin and Song, Yi-Zhe and Hospedales, Timothy M.},
	month = oct,
	year = {2017},
	file = {Li et al. - 2017 - Deeper, Broader and Artier Domain Generalization.pdf:/Users/mx/Zotero/storage/PFIRYV24/Li et al. - 2017 - Deeper, Broader and Artier Domain Generalization.pdf:application/pdf}
}

@article{goodfellow_generative_2014,
	title = {Generative {Adversarial} {Networks}},
    journal = {arXiv},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = {2014},
}

@article{castro_causality_2019,
	title = {Causality matters in medical imaging},
    journal = {arXiv},
	author = {Castro, Daniel C. and Walker, Ian and Glocker, Ben},
	year = {2019},
}


@article{muandet_domain_2013,
	title = {Domain {Generalization} via {Invariant} {Feature} {Representation}},
	url = {http://arxiv.org/abs/1301.2115},
	abstract = {This paper investigates domain generalization: How to take knowledge acquired from an arbitrary number of related domains and apply it to previously unseen domains? We propose Domain-Invariant Component Analysis (DICA), a kernel-based optimization algorithm that learns an invariant transformation by minimizing the dissimilarity across domains, whilst preserving the functional relationship between input and output variables. A learning-theoretic analysis shows that reducing dissimilarity improves the expected generalization ability of classifiers on new domains, motivating the proposed algorithm. Experimental results on synthetic and real-world datasets demonstrate that DICA successfully learns invariant features and improves classifier performance in practice.},
	urldate = {2020-02-20},
	journal = {arXiv:1301.2115 [cs, stat]},
	author = {Muandet, Krikamol and Balduzzi, David and Schölkopf, Bernhard},
	month = jan,
	year = {2013},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: The 30th International Conference on Machine Learning (ICML 2013)},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/CBFW7GPE/Muandet et al. - 2013 - Domain Generalization via Invariant Feature Repres.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/ZXEHHRE5/1301.html:text/html}
}

@incollection{ferrari_deep_2018,
	title = {Deep {Domain} {Generalization} via {Conditional} {Invariant} {Adversarial} {Networks}},
	booktitle = {ECCV},
	author = {Li, Ya and Tian, Xinmei and Gong, Mingming and Liu, Yajing and Liu, Tongliang and Zhang, Kun and Tao, Dacheng},
	year = {2018},
}

@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={60},
  year={2019},
  publisher={Springer}
}

@incollection{vapnik_principles_1992,
	title = {Principles of {Risk} {Minimization} for {Learning} {Theory}},
	booktitle = {NIPS},
	author = {Vapnik, V.},
	year = {1992},
}

@book{Peters2017,
author = {Peters, J. and Janzing, D. and Sch\"olkopf, B.},
title = {Elements of Causal Inference: Foundations and Learning Algorithms},
address = {Cambridge, MA, USA},
publisher = {MIT Press},
year = {2017}
}

@inproceedings{de2019causal,
  title={Causal confusion in imitation learning},
  author={de Haan, Pim and Jayaraman, Dinesh and Levine, Sergey},
  booktitle={NeurIPS},
  pages={11693--11704},
  year={2019}
}


@misc{li_automating_2020,
	title = {Automating {Data} {Augmentation}: {Practice}, {Theory} and {New} {Direction}},
	shorttitle = {Automating {Data} {Augmentation}},
	url = {http://ai.stanford.edu/blog/data-augmentation/},
	abstract = {Data augmentation is a de facto technique used in nearly every state-of-the-art machine learning model in applications such as image and text classification. Heuristic data augmentation schemes are often tuned manually by human experts with extensive domain knowledge, and may result in suboptimal augmentation policies. In this blog post, we provide a broad overview of recent efforts in this exciting research area, which resulted in new algorithms for automating the search process of transformation functions, new theoretical insights that improve the understanding of various augmentation techniques commonly used in practice, and a new framework for exploiting data augmentation to patch a flawed model and improve performance on crucial subpopulation of data.},
	urldate = {2020-05-01},
	journal = {SAIL Blog},
	author = {Li, Sharon Y.},
	month = apr,
	year = {2020},
	note = {Library Catalog: ai.stanford.edu},
	file = {Snapshot:/Users/mx/Zotero/storage/D6S4EJ3Y/data-augmentation.html:text/html}
}


@article{krueger_out--distribution_2020,
	title = {Out-of-{Distribution} {Generalization} via {Risk} {Extrapolation} ({REx})},
	url = {http://arxiv.org/abs/2003.00688},
	abstract = {Generalizing outside of the training distribution is an open challenge for current machine learning systems. A weak form of out-of-distribution (OoD) generalization is the ability to successfully interpolate between multiple observed distributions. One way to achieve this is through robust optimization, which seeks to minimize the worst-case risk over convex combinations of the training distributions. However, a much stronger form of OoD generalization is the ability of models to extrapolate beyond the distributions observed during training. In pursuit of strong OoD generalization, we introduce the principle of Risk Extrapolation (REx). REx can be viewed as encouraging robustness over affine combinations of training risks, by encouraging strict equality between training risks. We show conceptually how this principle enables extrapolation, and demonstrate the effectiveness and scalability of instantiations of REx on various OoD generalization tasks. Our code can be found at https://github.com/capybaralet/REx\_code\_release.},
	urldate = {2020-05-01},
	journal = {arXiv:2003.00688 [cs, stat]},
	author = {Krueger, David and Caballero, Ethan and Jacobsen, Joern-Henrik and Zhang, Amy and Binas, Jonathan and Priol, Remi Le and Courville, Aaron},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.00688},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/PWNJPSUJ/Krueger et al. - 2020 - Out-of-Distribution Generalization via Risk Extrap.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/PJ7XCP9Y/2003.html:text/html}
}
	
	
@article{gontijo-lopes_affinity_2020,
	title = {{Affinity} and {Diversity}: {Quantifying} {Mechanisms} of {Data} {Augmentation}},
	shorttitle = {Affinity and {Diversity}},
	url = {http://arxiv.org/abs/2002.08973},
	urldate = {2020-05-26},
	journal = {arXiv:2002.08973 [cs, stat]},
	author = {Gontijo-Lopes, Raphael and Smullin, Sylvia J. and Cubuk, Ekin D. and Dyer, Ethan},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.08973},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning}
}

@article{lyle_benefits_2020,
	title = {On the {Benefits} of {Invariance} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2005.00178},
	abstract = {Many real world data analysis problems exhibit invariant structure, and models that take advantage of this structure have shown impressive empirical performance, particularly in deep learning. While the literature contains a variety of methods to incorporate invariance into models, theoretical understanding is poor and there is no way to assess when one method should be preferred over another. In this work, we analyze the benefits and limitations of two widely used approaches in deep learning in the presence of invariance: data augmentation and feature averaging. We prove that training with data augmentation leads to better estimates of risk and gradients thereof, and we provide a PAC-Bayes generalization bound for models trained with data augmentation. We also show that compared to data augmentation, feature averaging reduces generalization error when used with convex losses, and tightens PAC-Bayes bounds. We provide empirical support of these theoretical results, including a demonstration of why generalization may not improve by training with data augmentation: the `learned invariance' fails outside of the training distribution.},
	urldate = {2020-05-26},
	journal = {arXiv:2005.00178 [cs, stat]},
	author = {Lyle, Clare and van der Wilk, Mark and Kwiatkowska, Marta and Gal, Yarin and Bloem-Reddy, Benjamin},
	month = apr,
	year = {2020},
	note = {arXiv: 2005.00178},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/494KI46A/Lyle et al. - 2020 - On the Benefits of Invariance in Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/SMNQWD4S/2005.html:text/html}
}


@article{cranmer_lagrangian_2020,
	title = {Lagrangian {Neural} {Networks}},
	url = {http://arxiv.org/abs/2003.04630},
	abstract = {Accurate models of the world are built upon notions of its underlying symmetries. In physics, these symmetries correspond to conservation laws, such as for energy and momentum. Yet even though neural network models see increasing use in the physical sciences, they struggle to learn these symmetries. In this paper, we propose Lagrangian Neural Networks (LNNs), which can parameterize arbitrary Lagrangians using neural networks. In contrast to models that learn Hamiltonians, LNNs do not require canonical coordinates, and thus perform well in situations where canonical momenta are unknown or difficult to compute. Unlike previous approaches, our method does not restrict the functional form of learned energies and will produce energy-conserving models for a variety of tasks. We test our approach on a double pendulum and a relativistic particle, demonstrating energy conservation where a baseline approach incurs dissipation and modeling relativity without canonical coordinates where a Hamiltonian approach fails. Finally, we show how this model can be applied to graphs and continuous systems using a Lagrangian Graph Network, and demonstrate it on the 1D wave equation.},
	urldate = {2020-05-29},
	journal = {arXiv:2003.04630 [physics, stat]},
	author = {Cranmer, Miles and Greydanus, Sam and Hoyer, Stephan and Battaglia, Peter and Spergel, David and Ho, Shirley},
	month = mar,
	year = {2020},
	note = {arXiv: 2003.04630},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Physics - Computational Physics, Physics - Data Analysis, Statistics and Probability, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/D896JJFC/Cranmer et al. - 2020 - Lagrangian Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/BFDUX5WD/2003.html:text/html}
}

@article{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1602.07576},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
	urldate = {2020-05-29},
	journal = {arXiv:1602.07576 [cs, stat]},
	author = {Cohen, Taco S. and Welling, Max},
	month = jun,
	year = {2016},
	note = {arXiv: 1602.07576},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/XL2GD9D8/Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/RV38NZ5S/1602.html:text/html}
}

@article{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2020-05-29},
	journal = {arXiv:1609.02907 [cs, stat]},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv: 1609.02907},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/UJPBWS2I/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/E8BUXKRQ/1609.html:text/html}
	}
	


@article{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2020-08-18},
	journal = {arXiv:1912.01703 [cs, stat]},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv: 1912.01703},
	keywords = {Computer Science - Machine Learning, Computer Science - Mathematical Software, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/VG98A3II/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/NXIJ47S7/1912.html:text/html}
}


@article{subbaswamy_development_2019,
	title = {From development to deployment: dataset shift, causality, and shift-stable models in health {AI}},
	issn = {1465-4644, 1468-4357},
	shorttitle = {From development to deployment},
	url = {https://academic.oup.com/biostatistics/advance-article/doi/10.1093/biostatistics/kxz041/5631850},
	doi = {10.1093/biostatistics/kxz041},
	language = {en},
	urldate = {2020-01-06},
	journal = {Biostatistics},
	author = {Subbaswamy, Adarsh and Saria, Suchi},
	month = nov,
	year = {2019},
	pages = {kxz041},
	file = {Subbaswamy and Saria - 2019 - From development to deployment dataset shift, cau.pdf:/Users/mx/Zotero/storage/4AS54HEW/Subbaswamy and Saria - 2019 - From development to deployment dataset shift, cau.pdf:application/pdf}
}

@article{mooij_joint_2019,
	title = {Joint {Causal} {Inference} from {Multiple} {Contexts}},
	url = {http://arxiv.org/abs/1611.10351},
	abstract = {The gold standard for discovering causal relations is by means of experimentation. Over the last decades, alternative methods have been proposed that can infer causal relations between variables from certain statistical patterns in purely observational data. We introduce Joint Causal Inference (JCI), a novel approach to causal discovery from multiple data sets that elegantly uniﬁes both approaches. JCI is a causal modeling approach rather than a speciﬁc algorithm, and it can be used in combination with any causal discovery algorithm that can take into account certain background knowledge. The main idea is to reduce causal discovery from multiple datasets originating from diﬀerent contexts (e.g., diﬀerent experimental conditions) to causal discovery from a single pooled dataset by adding auxiliary context variables and incorporating applicable background knowledge on the causal relationships involving the context variables. We propose diﬀerent ﬂavours of JCI that diﬀer in the amount of background knowledge that is assumed. JCI can deal with several diﬀerent types of interventions in a uniﬁed fashion, does not require knowledge on intervention targets or types in case of interventional data, and allows one to fully exploit all the information in the joint distribution on system and context variables. We explain how some well-known causal discovery algorithms can be seen as implementations of the JCI framework, but we also propose novel implementations that are simple adaptations of existing causal discovery methods for purely observational data to the JCI setting. We evaluate diﬀerent implementations of the JCI approach on synthetic data and on ﬂow cytometry protein expression data and conclude that JCI implementations can outperform state-of-the-art causal discovery algorithms.},
	language = {en},
	urldate = {2020-01-06},
	journal = {arXiv:1611.10351 [cs, stat]},
	author = {Mooij, Joris M. and Magliacane, Sara and Claassen, Tom},
	month = apr,
	year = {2019},
	note = {arXiv: 1611.10351},
	keywords = {Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Mooij et al. - 2019 - Joint Causal Inference from Multiple Contexts.pdf:/Users/mx/Zotero/storage/CW7WV5QX/Mooij et al. - 2019 - Joint Causal Inference from Multiple Contexts.pdf:application/pdf}
}

@inproceedings{pearl_transportability_2011,
	address = {Vancouver, BC, Canada},
	title = {Transportability of {Causal} and {Statistical} {Relations}: {A} {Formal} {Approach}},
	isbn = {978-1-4673-0005-6 978-0-7695-4409-0},
	shorttitle = {Transportability of {Causal} and {Statistical} {Relations}},
	url = {http://ieeexplore.ieee.org/document/6137426/},
	doi = {10.1109/ICDMW.2011.169},
	abstract = {We address the problem of transferring information learned from experiments to a different environment, in which only passive observations can be collected. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between environments and, using this representation, we derive procedures for deciding whether effects in the target environment can be inferred from experiments conducted elsewhere. When the answer is afﬁrmative, the procedures identify the set of experiments and observations that need be conducted to license the transport. We further discuss how transportability analysis can guide the transfer of knowledge in non-experimental learning to minimize re-measurement cost and improve prediction power.},
	language = {en},
	urldate = {2020-08-20},
	booktitle = {2011 {IEEE} 11th {International} {Conference} on {Data} {Mining} {Workshops}},
	publisher = {IEEE},
	author = {Pearl, Judea and Bareinboim, Elias},
	month = dec,
	year = {2011},
	pages = {540--547},
	file = {Pearl and Bareinboim - 2011 - Transportability of Causal and Statistical Relatio.pdf:/Users/mx/Zotero/storage/52A7T37A/Pearl and Bareinboim - 2011 - Transportability of Causal and Statistical Relatio.pdf:application/pdf}
}

@article{subbaswamy_counterfactual_2018,
	title = {Counterfactual {Normalization}: {Proactively} {Addressing} {Dataset} {Shift} and {Improving} {Reliability} {Using} {Causal} {Mechanisms}},
	shorttitle = {Counterfactual {Normalization}},
	url = {http://arxiv.org/abs/1808.03253},
	abstract = {Predictive models can fail to generalize from training to deployment environments because of dataset shift, posing a threat to model reliability and the safety of downstream decisions made in practice. Instead of using samples from the target distribution to reactively correct dataset shift, we use graphical knowledge of the causal mechanisms relating variables in a prediction problem to proactively remove relationships that do not generalize across environments, even when these relationships may depend on unobserved variables (violations of the "no unobserved confounders" assumption). To accomplish this, we identify variables with unstable paths of statistical influence and remove them from the model. We also augment the causal graph with latent counterfactual variables that isolate unstable paths of statistical influence, allowing us to retain stable paths that would otherwise be removed. Our experiments demonstrate that models that remove vulnerable variables and use estimates of the latent variables transfer better, often outperforming in the target domain despite some accuracy loss in the training domain.},
	urldate = {2020-08-20},
	journal = {arXiv:1808.03253 [cs, stat]},
	author = {Subbaswamy, Adarsh and Saria, Suchi},
	month = aug,
	year = {2018},
	note = {arXiv: 1808.03253},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/GQ5RL3GD/Subbaswamy and Saria - 2018 - Counterfactual Normalization Proactively Addressi.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/8TWXJJ44/1808.html:text/html}
}

@article{subbaswamy_preventing_2019,
	title = {Preventing {Failures} {Due} to {Dataset} {Shift}: {Learning} {Predictive} {Models} {That} {Transport}},
	shorttitle = {Preventing {Failures} {Due} to {Dataset} {Shift}},
	url = {http://arxiv.org/abs/1812.04597},
	abstract = {Classical supervised learning produces unreliable models when training and target distributions differ, with most existing solutions requiring samples from the target domain. We propose a proactive approach which learns a relationship in the training domain that will generalize to the target domain by incorporating prior knowledge of aspects of the data generating process that are expected to differ as expressed in a causal selection diagram. Specifically, we remove variables generated by unstable mechanisms from the joint factorization to yield the Surgery Estimator---an interventional distribution that is invariant to the differences across environments. We prove that the surgery estimator finds stable relationships in strictly more scenarios than previous approaches which only consider conditional relationships, and demonstrate this in simulated experiments. We also evaluate on real world data for which the true causal diagram is unknown, performing competitively against entirely data-driven approaches.},
	urldate = {2020-08-20},
	journal = {arXiv:1812.04597 [cs, stat]},
	author = {Subbaswamy, Adarsh and Schulam, Peter and Saria, Suchi},
	month = feb,
	year = {2019},
	note = {arXiv: 1812.04597},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mx/Zotero/storage/I72S7TSY/Subbaswamy et al. - 2019 - Preventing Failures Due to Dataset Shift Learning.pdf:application/pdf;arXiv.org Snapshot:/Users/mx/Zotero/storage/LNA2JQKW/1812.html:text/html}
}

@article{rojas-carulla_invariant_nodate,
	title = {Invariant {Models} for {Causal} {Transfer} {Learning}},
	abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the ﬁeld of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are suﬃciently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
	language = {en},
	author = {Rojas-Carulla, Mateo and Scholkopf, Bernhard and Turner, Richard and Peters, Jonas},
	pages = {34},
	year = {2018},
	file = {Rojas-Carulla et al. - Invariant Models for Causal Transfer Learning.pdf:/Users/mx/Zotero/storage/EKZHGHUQ/Rojas-Carulla et al. - Invariant Models for Causal Transfer Learning.pdf:application/pdf}
}

@incollection{magliacane_domain_2018,
	title = {Domain {Adaptation} by {Using} {Causal} {Inference} to {Predict} {Invariant} {Conditional} {Distributions}},
	url = {http://papers.nips.cc/paper/8282-domain-adaptation-by-using-causal-inference-to-predict-invariant-conditional-distributions.pdf},
	urldate = {2020-08-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Magliacane, Sara and van Ommen, Thijs and Claassen, Tom and Bongers, Stephan and Versteeg, Philip and Mooij, Joris M},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	pages = {10846--10856},
	file = {NIPS Full Text PDF:/Users/mx/Zotero/storage/BR5XD5XI/Magliacane et al. - 2018 - Domain Adaptation by Using Causal Inference to Pre.pdf:application/pdf;NIPS Snapshot:/Users/mx/Zotero/storage/Y5EHKXBH/8282-domain-adaptation-by-using-causal-inference-to-predict-invariant-conditional-distributions.html:text/html}
}

@article{bareinboim_causal_2016,
	title = {Causal inference and the data-fusion problem},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1510507113},
	doi = {10.1073/pnas.1510507113},
	abstract = {Result 1 (Identification in Policy Evaluation). The analysis of policy evaluation problems has reached a fairly satisfactory state of maturity. We now possess a complete solution to the problem of identification whenever assumptions are expressible in DAG form. This entails (i) graphical and algorithmic criteria for deciding identifiability of policy questions, (ii) automated procedures for extracting each and every identifiable estimand, and (iii) extensions to models invoking sequential dynamic decisions with unmeasured confounders. These results were developed in several stages over the past 20 years (14, 16, 18, 24, 26).},
	language = {en},
	number = {27},
	urldate = {2020-08-20},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bareinboim, Elias and Pearl, Judea},
	month = jul,
	year = {2016},
	pages = {7345--7352},
	file = {Bareinboim and Pearl - 2016 - Causal inference and the data-fusion problem.pdf:/Users/mx/Zotero/storage/FH37GLBA/Bareinboim and Pearl - 2016 - Causal inference and the data-fusion problem.pdf:application/pdf}
}

@article{pearl_external_2014,
	title = {External {Validity}: {From} {Do}-{Calculus} to {Transportability} {Across} {Populations}},
	volume = {29},
	issn = {0883-4237},
	shorttitle = {External {Validity}},
	url = {http://projecteuclid.org/euclid.ss/1421330548},
	doi = {10.1214/14-STS486},
	abstract = {The generalizability of empirical ﬁndings to new environments, settings or populations, often called “external validity,” is essential in most scientiﬁc explorations. This paper treats a particular problem of generalizability, called “transportability,” deﬁned as a license to transfer causal effects learned in experimental studies to a new population, in which only observational studies can be conducted. We introduce a formal representation called “selection diagrams” for expressing knowledge about differences and commonalities between populations of interest and, using this representation, we reduce questions of transportability to symbolic derivations in the docalculus. This reduction yields graph-based procedures for deciding, prior to observing any data, whether causal effects in the target population can be inferred from experimental ﬁndings in the study population. When the answer is afﬁrmative, the procedures identify what experimental and observational ﬁndings need be obtained from the two populations, and how they can be combined to ensure bias-free transport.},
	language = {en},
	number = {4},
	urldate = {2020-08-20},
	journal = {Statistical Science},
	author = {Pearl, Judea and Bareinboim, Elias},
	month = nov,
	year = {2014},
	pages = {579--595},
	file = {Pearl and Bareinboim - 2014 - External Validity From Do-Calculus to Transportab.pdf:/Users/mx/Zotero/storage/FM7IPQ4L/Pearl and Bareinboim - 2014 - External Validity From Do-Calculus to Transportab.pdf:application/pdf}
}



}
