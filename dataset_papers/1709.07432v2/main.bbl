\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aharoni et~al.(2017)Aharoni, Rattner, and Permuter]{aharoni2017}
Z.~Aharoni, G.~Rattner, and H.~Permuter.
\newblock Gradual learning of deep recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1708.08863}, 2017.

\bibitem[Bellegarda(2004)]{bellegarda2004adaptation}
J.~R. Bellegarda.
\newblock Statistical language model adaptation: review and perspectives.
\newblock \emph{Speech Communication}, 42\penalty0 (1):\penalty0 93--108, 2004.

\bibitem[Chung et~al.(2017)Chung, Ahn, and Bengio]{chung2017}
J.~Chung, S.~Ahn, and Y.~Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock \emph{ICLR}, 2017.

\bibitem[Cooijmans et~al.(2017)Cooijmans, Ballas, Laurent, and
  Courville]{cooijmans2017}
T.~Cooijmans, N.~Ballas, C.~Laurent, and A.~Courville.
\newblock Recurrent batch normalization.
\newblock \emph{ICLR}, 2017.

\bibitem[Fortunato et~al.(2017)Fortunato, Blundell, and Vinyals]{fortunato2017}
M.~Fortunato, C.~Blundell, and O.~Vinyals.
\newblock Bayesian recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1704.02798}, 2017.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016}
Y.~Gal and Z.~Ghahramani.
\newblock A theoretically grounded application of dropout in recurrent neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1019--1027, 2016.

\bibitem[Grave et~al.(2017)Grave, Joulin, and Usunier]{grave2017}
E.~Grave, A.~Joulin, and N.~Usunier.
\newblock Improving neural language models with a continuous cache.
\newblock \emph{ICLR}, 2017.

\bibitem[Graves(2013)]{Graves-2013}
A.~Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1308.0850}, 2013.

\bibitem[Ha et~al.(2017)Ha, Dai, and Lee]{Ha2017}
D.~Ha, A.~Dai, and Q.~Lee.
\newblock Hypernetworks.
\newblock \emph{ICLR}, 2017.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter-1997}
S.~Hochreiter and J.~Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 9:\penalty0 1735--1780, 1997.

\bibitem[Hutter(2006)]{Hutter-2006}
M.~Hutter.
\newblock The human knowledge compression prize.
\newblock \emph{URL http://prize.hutter1.net}, 2006.

\bibitem[Inan et~al.(2017)Inan, Khosravi, and Socher]{inan2017}
H.~Inan, K.~Khosravi, and R.~Socher.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock \emph{ICLR}, 2017.

\bibitem[Jelinek et~al.(1991)Jelinek, Merialdo, Roukos, and
  Strauss]{jelinek1991}
F.~Jelinek, B.~Merialdo, S.~Roukos, and M.~Strauss.
\newblock A dynamic language model for speech recognition.
\newblock In \emph{HLT}, volume~91, pp.\  293--295, 1991.

\bibitem[Kalchbrenner et~al.(2016)Kalchbrenner, Espeholt, Simonyan, Oord,
  Graves, and Kavukcuoglu]{Kalchbrenner2016}
N.~Kalchbrenner, L.~Espeholt, K.~Simonyan, A.~Oord, A.~Graves, and
  K.~Kavukcuoglu.
\newblock Neural machine translation in linear time.
\newblock \emph{arXiv preprint arXiv:1610.10099}, 2016.

\bibitem[Kim et~al.(2016)Kim, Jernite, Sontag, and Rush]{Kim2015}
Y.~Kim, Y.~Jernite, D.~Sontag, and A.~M. Rush.
\newblock Character-aware neural language models.
\newblock In \emph{Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Koehn(2005)]{koehn2005}
P.~Koehn.
\newblock Europarl: A parallel corpus for statistical machine translation.
\newblock In \emph{MT {Summit}}, volume~5, pp.\  79--86, 2005.

\bibitem[Krause et~al.(2016)Krause, Lu, Murray, and Renals]{krause2016}
B.~Krause, L.~Lu, I.~Murray, and S.~Renals.
\newblock Multiplicative {LSTM} for sequence modelling.
\newblock \emph{arXiv preprint arXiv:1609.07959}, 2016.

\bibitem[Krause et~al.(2017)Krause, Murray, Renals, and Lu]{krause2017}
B.~Krause, I.~Murray, S.~Renals, and L.~Lu.
\newblock Multiplicative {LSTM} for sequence modelling.
\newblock \emph{ICLR Workshop track}, 2017.
\newblock URL \url{https://openreview.net/forum?id=SJCS5rXFl}.

\bibitem[Kuhn(1988)]{kuhn1988}
R.~Kuhn.
\newblock Speech recognition and the frequency of recently used words: A
  modified {M}arkov model for natural language.
\newblock In \emph{Proceedings of the 12th conference on Computational
  linguistics-Volume 1}, pp.\  348--350. Association for Computational
  Linguistics, 1988.

\bibitem[Marcus et~al.(1993)Marcus, Marcinkiewicz, and Santorini]{marcus1993}
M.~P. Marcus, M.~A. Marcinkiewicz, and B.~Santorini.
\newblock Building a large annotated corpus of {English}: The {Penn Treebank}.
\newblock \emph{Computational linguistics}, 19\penalty0 (2):\penalty0 313--330,
  1993.

\bibitem[Melis et~al.(2017)Melis, Dyer, and Blunsom]{melis2017}
G.~Melis, C.~Dyer, and P.~Blunsom.
\newblock On the state of the art of evaluation in neural language models.
\newblock \emph{arXiv preprint arXiv:1707.05589}, 2017.

\bibitem[Merity et~al.(2017{\natexlab{a}})Merity, Keskar, and
  Socher]{merity2017}
S.~Merity, N.~S. Keskar, and R.~Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock \emph{arXiv preprint arXiv:1708.02182}, 2017{\natexlab{a}}.

\bibitem[Merity et~al.(2017{\natexlab{b}})Merity, Xiong, Bradbury, and
  Socher]{Merity2016}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{ICLR}, 2017{\natexlab{b}}.

\bibitem[Mikolov \& Zweig(2012)Mikolov and Zweig]{Mikolov2012b}
T.~Mikolov and G.~Zweig.
\newblock Context dependent recurrent neural network language model.
\newblock \emph{SLT}, 12:\penalty0 234--239, 2012.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\`y}, and
  Khudanpur]{mikolov2010}
T.~Mikolov, M.~Karafi{\'a}t, L.~Burget, J.~Cernock{\`y}, and S.~Khudanpur.
\newblock Recurrent neural network based language model.
\newblock In \emph{Interspeech}, volume~2, pp.\ ~3, 2010.

\bibitem[Mikolov et~al.(2012)Mikolov, Sutskever, Deoras, Le, Kombrink, and
  Cernocky]{mikolov2012subword}
T.~Mikolov, I.~Sutskever, A.~Deoras, H.~Le, S.~Kombrink, and J.~Cernocky.
\newblock Subword language modeling with neural networks.
\newblock \emph{preprint (http://www. fit. vutbr. cz/imikolov/rnnlm/char.
  pdf)}, 2012.

\bibitem[Mujika et~al.(2017)Mujika, Meier, and Steger]{mujika2017}
A.~Mujika, F.~Meier, and A.~Steger.
\newblock Fast-slow recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1705.08639}, 2017.

\bibitem[Ororbia~II et~al.(2017)Ororbia~II, Mikolov, and Reitter]{ororbia2017}
A.~G. Ororbia~II, T.~Mikolov, and D.~Reitter.
\newblock Learning simpler language models with the differential state
  framework.
\newblock \emph{Neural Computation}, 2017.

\bibitem[Polyak \& Juditsky(1992)Polyak and Juditsky]{polyak1992}
B.~T. Polyak and A.~B. Juditsky.
\newblock Acceleration of stochastic approximation by averaging.
\newblock \emph{SIAM Journal on Control and Optimization}, 30\penalty0
  (4):\penalty0 838--855, 1992.

\bibitem[Press \& Wolf(2017)Press and Wolf]{press2017}
O.~Press and L.~Wolf.
\newblock Using the output embedding to improve language models.
\newblock \emph{EACL 2017}, pp.\  157, 2017.

\bibitem[Salimans \& Kingma(2016)Salimans and Kingma]{salimans2016}
T.~Salimans and D.~P. Kingma.
\newblock Weight normalization: A simple reparameterization to accelerate
  training of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  901--909, 2016.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{tieleman2012}
T.~Tieleman and G.~E. Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural Networks for Machine Learning}, 4\penalty0
  (2), 2012.

\bibitem[Wan et~al.(2013)Wan, Zeiler, Zhang, Cun, and Fergus]{wan2013}
L.~Wan, M.~Zeiler, S.~Zhang, Yann~L. Cun, and R.~Fergus.
\newblock Regularization of neural networks using dropconnect.
\newblock In \emph{Proceedings of the 30th international conference on machine
  learning (ICML-13)}, pp.\  1058--1066, 2013.

\bibitem[Werbos(1990)]{werbos-1990}
P.~J. Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock \emph{Proceedings of the IEEE}, 78:\penalty0 1550--1560, 1990.

\bibitem[Wu et~al.(2016)Wu, Zhang, Zhang, Bengio, and Salakhutdinov]{wu2016}
Y.~Wu, S.~Zhang, Y.~Zhang, Y.~Bengio, and R.~Salakhutdinov.
\newblock On multiplicative integration with recurrent neural networks.
\newblock In \emph{NIPS}, 2016.

\bibitem[Zaremba et~al.(2014)Zaremba, Sutskever, and Vinyals]{Zaremba-2014}
W.~Zaremba, I.~Sutskever, and O.~Vinyals.
\newblock Recurrent neural network regularization.
\newblock \emph{arXiv preprint arXiv:1409.2329}, 2014.

\bibitem[Zilly et~al.(2017)Zilly, Srivastava, Koutn{\'\i}k, and
  Schmidhuber]{zilly2017}
J.~G. Zilly, R.~K. Srivastava, J.~Koutn{\'\i}k, and J.~Schmidhuber.
\newblock Recurrent highway networks.
\newblock \emph{ICLR}, 2017.

\bibitem[Zoph \& Le(2017)Zoph and Le]{zoph2017}
B.~Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{ICLR}, 2017.

\end{thebibliography}
