@inproceedings{Louizos2017LearningSN,
  title={Learning Sparse Neural Networks through L0 Regularization},
  author={Christos Louizos and Max Welling and Diederik P. Kingma},
  booktitle={ICLR},
  year={2017},
}

@inproceedings{Zhu2018ToPO,
  title={To prune, or not to prune: exploring the efficacy of pruning for model compression},
  author={Michael Zhu and Suyog Gupta},
  booktitle={ICLR},
  year={2018},
}

@inproceedings{Han2015LearningBW,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Song Han and Jeff Pool and John Tran and William J. Dally},
  booktitle={NIPS},
  year={2015}
}

@inproceedings{Guo2016DynamicNS,
  title={Dynamic Network Surgery for Efficient DNNs},
  author={Yiwen Guo and Anbang Yao and Yurong Chen},
  booktitle={NIPS},
  year={2016}
}

@inproceedings{Ramanujan2019WhatsHI,
  title={What's Hidden in a Randomly Weighted Neural Network?},
  author={Vivek Ramanujan and Mitchell Wortsman and Aniruddha Kembhavi and Ali Farhadi and Mohammad Rastegari},
  booktitle={CVPR},
  year={2020},
}

@article{Mallya2018PiggybackAM,
  title={Piggyback: Adding Multiple Tasks to a Single, Fixed Network by Learning to Mask},
  author={Arun Mallya and Svetlana Lazebnik},
  journal={ArXiv},
  year={2018},
  volume={abs/1801.06519}
}

@article{Bengio2013EstimatingOP,
  title={Estimating or Propagating Gradients Through Stochastic Neurons for Conditional Computation},
  author={Yoshua Bengio and Nicholas L{\'e}onard and Aaron C. Courville},
  journal={ArXiv},
  year={2013},
  volume={abs/1308.3432}
}

@inproceedings{Jang2016CategoricalRW,
  title={Categorical Reparameterization with Gumbel-Softmax},
  author={Eric Jang and Shixiang Gu and Ben Poole},
  booktitle={NIPS},
  year={2016}
}

@article{Gale2019TheSO,
  title={The State of Sparsity in Deep Neural Networks},
  author={Trevor Gale and Erich Elsen and Sara Hooker},
  journal={ArXiv},
  year={2019},
  volume={abs/1902.09574}
}

@inproceedings{Rajpurkar2016SQuAD10,
  title={SQuAD: 100, 000+ Questions for Machine Comprehension of Text},
  author={Pranav Rajpurkar and Jian Zhang and Konstantin Lopyrev and Percy Liang},
  booktitle={EMNLP},
  year={2016}
}

@inproceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "NAACL",
  year = "2018",
}

@online{WinNT,
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},
  title = {First Quora Dataset Release: Question Pairs},
  year = 2017,
  url = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},
  urldate = {2019-04-03}
}

@inproceedings{conneau2018xnli,
  author = "Conneau, Alexis
                 and Rinott, Ruty
                 and Lample, Guillaume
                 and Williams, Adina
                 and Bowman, Samuel R.
                 and Schwenk, Holger
                 and Stoyanov, Veselin",
  title = "XNLI: Evaluating Cross-lingual Sentence Representations",
  booktitle = "EMNLP",
  year = "2018",
}

@article{Wolf2019HuggingFacesTS,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R'emi Louf and Morgan Funtowicz and Jamie Brew},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.03771}
}

@inproceedings{ruder-etal-2019-transfer,
    title = "Transfer Learning in Natural Language Processing",
    author = "Ruder, Sebastian  and
      Peters, Matthew E.  and
      Swayamdipta, Swabha  and
      Wolf, Thomas",
    booktitle = "NAACL",
    year = "2019",
}

@inproceedings{devlin-etal-2019-bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL},
  year={2019}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@article{liu2019roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@inproceedings{He2018AMCAF,
  title={AMC: AutoML for Model Compression and Acceleration on Mobile Devices},
  author={Yihui He and Ji Lin and Zhijian Liu and Hanrui Wang and Li-Jia Li and Song Han},
  booktitle={ECCV},
  year={2018}
}

@article{Jiao2019TinyBERTDB,
  title={TinyBERT: Distilling BERT for Natural Language Understanding},
  author={Xiaoqi Jiao and Y. Yin and Lifeng Shang and Xin Jiang and Xusong Chen and Linlin Li and Fang Wang and Qun Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.10351}
}

@inproceedings{Sanh2019DistilBERTAD,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  booktitle={NeurIPS EMC2 Workshop},
  year={2019}
}

@inproceedings{Bucila2006ModelC,
  title={Model compression},
  author={Cristian Bucila and Rich Caruana and Alexandru Niculescu-Mizil},
  booktitle={KDD},
  year={2006}
}

@inproceedings{Hinton2015DistillingTK,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  booktitle={NIPS},
  year={2014}
}

@inproceedings{Fan2019ReducingTD,
  title={Reducing Transformer Depth on Demand with Structured Dropout},
  author={Angela Fan and Edouard Grave and Armand Joulin},
  booktitle={ICLR},
  year={2020},
}

@article{Guo2019ReweightedPP,
  title={Reweighted Proximal Pruning for Large-Scale Language Representation},
  author={Fu-Ming Guo and Sijia Liu and Finlay S. Mungall and Xue Lian Lin and Yanzhi Wang},
  journal={ArXiv},
  year={2019},
  volume={abs/1909.12486}
}

@inproceedings{Courbariaux2015BinaryConnectTD,
  title={BinaryConnect: Training Deep Neural Networks with binary weights during propagations},
  author={Matthieu Courbariaux and Yoshua Bengio and Jean-Pierre David},
  booktitle={NIPS},
  year={2015},
}

@article{Courbariaux2016BinarizedNN,
  title={Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1},
  author={Matthieu Courbariaux and Itay Hubara and Daniel Soudry and Ran El-Yaniv and Yoshua Bengio},
  journal={arXiv: Learning},
  year={2016}
}

@article{Cands2007EnhancingSB,
  title={Enhancing Sparsity by Reweighted â„“1 Minimization},
  author={Emmanuel J. Cand{\`e}s and Michael B. Wakin and S. Boyd},
  journal={Journal of Fourier Analysis and Applications},
  year={2007},
  volume={14},
  pages={877-905}
}

@article{Parikh2014ProximalA,
  title={Proximal Algorithms},
  author={Neal Parikh and Stephen P. Boyd},
  journal={Found. Trends Optim.},
  year={2014},
  volume={1},
  pages={127-239}
}

@inproceedings{Yang2017DesigningEC,
  title={Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning},
  author={Tien-Ju Yang and Yu-Hsin Chen and Vivienne Sze},
  booktitle={CVPR},
  year={2017},
}

@article{fan2020training,
  title={Training with Quantization Noise for Extreme Model Compression},
  author={Angela Fan and Pierre Stock and Benjamin Graham and Edouard Grave and R{\'e}mi Gribonval and Herv{\'e} J{\'e}gou and Armand Joulin},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.07320}
}

@inproceedings{Zafrir2019Q8BERTQ8,
  title={Q8BERT: Quantized 8Bit BERT},
  author={Ofir Zafrir and Guy Boudoukh and Peter Izsak and Moshe Wasserblat},
  booktitle={NeurIPS EMC2 Workshop},
  year={2019}
}

@inproceedings{Stock2019AndTB,
  title={And the Bit Goes Down: Revisiting the Quantization of Neural Networks},
  author={Pierre Stock and Armand Joulin and R{\'e}mi Gribonval and Benjamin Graham and Herv{\'e} J{\'e}gou},
  booktitle={ICLR},
  year={2020},
}

@article{Gong2014CompressingDC,
  title={Compressing Deep Convolutional Networks using Vector Quantization},
  author={Yunchao Gong and Liu Liu and Ming Yang and Lubomir D. Bourdev},
  journal={ArXiv},
  year={2014},
  volume={abs/1412.6115}
}

@inproceedings{Li2020TrainLT,
  title={Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers},
  author={Zhuohan Li and Eric Wallace and Sheng Shen and Kevin Lin and Kurt Keutzer and Dan Klein and Joseph E. Gonzalez},
  booktitle={ICML},
  year={2020},
}

@inproceedings{Han2016DeepCC,
  title={Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  author={Song Han and Huizi Mao and William J. Dally},
  booktitle={ICLR},
  year={2016},
}

@article{Tang2019DistillingTK,
  title={Distilling Task-Specific Knowledge from BERT into Simple Neural Networks},
  author={Raphael Tang and Yao Lu and Linqing Liu and Lili Mou and Olga Vechtomova and Jimmy Lin},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.12136}
}

@inproceedings{Michel2019AreSH,
  title={Are Sixteen Heads Really Better than One?},
  author={Paul Michel and Omer Levy and Graham Neubig},
  booktitle={NeurIPS},
  year={2019},
}

@inproceedings{Voita2019AnalyzingMS,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Elena Voita and David Talbot and F. Moiseev and Rico Sennrich and Ivan Titov},
  booktitle={ACL},
  year={2019}
}

@inproceedings{Yu2019PlayingTL,
  title={Playing the lottery with rewards and multiple languages: lottery tickets in RL and NLP},
  author={Haonan Yu and Sergey Edunov and Yuandong Tian and Ari S. Morcos},
  booktitle={ICLR},
  year={2020}
}

@article{BachSparsity2011,
author = {Bach, Francis and Jenatton, Rodolphe and Mairal, Julien and Obozinski, Guillaume},
year = {2011},
month = {09},
pages = {},
title = {Structured Sparsity through Convex Optimization},
volume = {27},
journal = {Statistical Science},
doi = {10.1214/12-STS394}
}

@article{Raffel2019ExploringTL,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.10683}
}

@inproceedings{Strubell2019EnergyAP,
  title={Energy and Policy Considerations for Deep Learning in NLP},
  author={Emma Strubell and Ananya Ganesh and Andrew McCallum},
  booktitle={ACL},
  year={2019}
}

@article{Frankle2019TheLT,
  title={The Lottery Ticket Hypothesis at Scale},
  author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel M. Roy and Michael Carbin},
  journal={ArXiv},
  year={2019},
  volume={abs/1903.01611}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NIPS},
  year={2017}
}

@article{Turc2019WellReadSL,
  title={Well-Read Students Learn Better: The Impact of Student Initialization on Knowledge Distillation},
  author={Iulia Turc and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  journal={ArXiv},
  year={2019},
  volume={abs/1908.08962}
}

@inproceedings{Ding2019GlobalSM,
  title={Global Sparse Momentum SGD for Pruning Very Deep Neural Networks},
  author={Xiaohan Ding and Guiguang Ding and Xiangxin Zhou and Yuchen Guo and Ji Liu and Jungong Han},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{LeCun1989OptimalBD,
  title={Optimal Brain Damage},
  author={Yann LeCun and John S. Denker and Sara A. Solla},
  booktitle={NIPS},
  year={1989}
}

@inproceedings{Hassibi1993OptimalBS,
  title={Optimal Brain Surgeon: Extensions and performance comparisons},
  author={Babak Hassibi and David G. Stork and Gregory J. Wolff},
  booktitle={NIPS},
  year={1993}
}

@article{Theis2018FasterGP,
  title={Faster gaze prediction with dense networks and Fisher pruning},
  author={Lucas Theis and Iryna Korshunova and Alykhan Tejani and Ferenc Husz{\'a}r},
  journal={ArXiv},
  year={2018},
  volume={abs/1801.05787}
}

@article{Lu2018GenderBI,
  title={Gender Bias in Neural Natural Language Processing},
  author={Kaiji Lu and Piotr Mardziel and Fangjing Wu and Preetam Amancharla and Anupam Datta},
  journal={ArXiv},
  year={2018},
  volume={abs/1807.11714}
}

@article{Vig2020CausalMA,
  title={Causal Mediation Analysis for Interpreting Neural NLP: The Case of Gender Bias},
  author={Jesse Vig and Sebastian Gehrmann and Yonatan Belinkov and Sharon Qian and Daniel Nevo and Yaron Singer and Stuart M. Shieber},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.12265}
}

@inproceedings{Wallace2019UniversalAT,
  title={Universal Adversarial Triggers for Attacking and Analyzing NLP},
  author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
  booktitle={EMNLP/IJCNLP},
  year={2019}
}

@inproceedings{Lee2019SNIPSN,
  title={SNIP: Single-shot Network Pruning based on Connection Sensitivity},
  author={Namhoon Lee and Thalaiyasingam Ajanthan and Philip H. S. Torr},
  booktitle={ICLR},
  year={2019},
}

@article{Sajjad2020PoorMB,
  title={Poor Man's BERT: Smaller and Faster Transformer Models},
  author={Hassan Sajjad and Fahim Dalvi and Nadir Durrani and Preslav Nakov},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.03844}
}

@article{Wang2019StructuredPO,
  title={Structured Pruning of Large Language Models},
  author={Ziheng Wang and Jeremy Wohlwend and Tao Lei},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.04732}
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
    year={2020},
    volume={abs/2005.14165},
    journal={arXiv},
}

@inproceedings{Han2016EIEEI,
  title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author={Song Han and Xingyu Liu and Huizi Mao and Jing Pu and Ardavan Pedram and Mark Horowitz and William J. Dally},
  booktitle={ISCA},
  year={2016},
}

@INPROCEEDINGS{6757323,
  author={M. {Horowitz}},
  booktitle={ISSCC}, 
  title={1.1 Computing's energy problem (and what we can do about it)}, 
  year={2014},
}

@article{Dettmers2019SparseNF,
  title={Sparse Networks from Scratch: Faster Training without Losing Performance},
  author={Tim Dettmers and L. Zettlemoyer},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.04840}
}

@inproceedings{Gordon2020CompressingBS,
  title={Compressing BERT: Studying the Effects of Weight Pruning on Transfer Learning},
  author={Mitchell A. Gordon and Kevin Duh and Nicholas Andrews},
  booktitle={RepL4NLP@ACL},
  year={2020}
}

@article{McCarley2019PruningAB,
  title={Pruning a BERT-based Question Answering Model},
  author={J. Scott McCarley},
  journal={ArXiv},
  year={2019},
  volume={abs/1910.06360}
}

@inproceedings{Frankle2019LinearMC,
  title={Linear Mode Connectivity and the Lottery Ticket Hypothesis},
  author={Jonathan Frankle and G. Dziugaite and D. M. Roy and Michael Carbin},
  booktitle={ICML},
  year={2020}
}