\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2022)Abbasi-Yadkori, Gyorgy, and
  Lazic]{abbasi2022new}
Yasin Abbasi-Yadkori, Andras Gyorgy, and Nevena Lazic.
\newblock A new look at dynamic regret for non-stationary stochastic bandits.
\newblock \emph{arXiv preprint arXiv:2201.06532}, 2022.

\bibitem[Auer et~al.(2019)Auer, Gajane, and Ortner]{auer2019adaptively}
Peter Auer, Pratik Gajane, and Ronald Ortner.
\newblock Adaptively tracking the best bandit arm with an unknown number of
  distribution changes.
\newblock In \emph{Conference on Learning Theory}, pages 138--158. PMLR, 2019.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Chen and Luo(2021)]{chen2021finding}
Liyu Chen and Haipeng Luo.
\newblock Finding the stochastic shortest path with low regret: The adversarial
  cost and unknown transition case.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Jafarnia-Jahromi, Jain, and
  Luo]{chen2021implicit}
Liyu Chen, Mehdi Jafarnia-Jahromi, Rahul Jain, and Haipeng Luo.
\newblock Implicit finite-horizon approximation and efficient optimal
  algorithms for stochastic shortest path.
\newblock \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Luo, and Wei]{chen2021minimax}
Liyu Chen, Haipeng Luo, and Chen-Yu Wei.
\newblock Minimax regret for stochastic shortest path with adversarial costs
  and known transition.
\newblock In \emph{Conference on Learning Theory}, pages 1180--1215. PMLR,
  2021{\natexlab{b}}.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Jain, and Luo]{chen2021improved}
Liyu Chen, Rahul Jain, and Haipeng Luo.
\newblock Improved no-regret algorithms for stochastic shortest path with
  linear {MDP}.
\newblock In \emph{International Conference on Machine Learning},
  2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Luo, and
  Rosenberg]{chen2022policy}
Liyu Chen, Haipeng Luo, and Aviv Rosenberg.
\newblock Policy optimization for stochastic shortest path.
\newblock \emph{Conference on Learning Theory}, 2022{\natexlab{b}}.

\bibitem[Chen et~al.(2021{\natexlab{c}})Chen, Wang, Zhao, and
  Zheng]{chen2021combinatorial}
Wei Chen, Liwei Wang, Haoyu Zhao, and Kai Zheng.
\newblock Combinatorial semi-bandit in the non-stationary environment.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 865--875.
  PMLR, 2021{\natexlab{c}}.

\bibitem[Chen et~al.(2019)Chen, Lee, Luo, and Wei]{chen2019new}
Yifang Chen, Chung-Wei Lee, Haipeng Luo, and Chen-Yu Wei.
\newblock A new algorithm for non-stationary contextual bandits: Efficient,
  optimal and parameter-free.
\newblock In \emph{Conference on Learning Theory}, pages 696--726. PMLR, 2019.

\bibitem[Cheung et~al.(2020)Cheung, Simchi-Levi, and
  Zhu]{cheung2020reinforcement}
Wang~Chi Cheung, David Simchi-Levi, and Ruihao Zhu.
\newblock Reinforcement learning for non-stationary {M}arkov decision
  processes: The blessing of (more) optimism.
\newblock In \emph{International Conference on Machine Learning}, pages
  1843--1854. PMLR, 2020.

\bibitem[Cohen et~al.(2020)Cohen, Kaplan, Mansour, and
  Rosenberg]{cohen2020near}
Alon Cohen, Haim Kaplan, Yishay Mansour, and Aviv Rosenberg.
\newblock Near-optimal regret bounds for stochastic shortest path.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, volume 119, pages 8210--8219. PMLR, 2020.

\bibitem[Cohen et~al.(2021)Cohen, Efroni, Mansour, and
  Rosenberg]{cohen2021minimax}
Alon Cohen, Yonathan Efroni, Yishay Mansour, and Aviv Rosenberg.
\newblock Minimax regret for stochastic shortest path.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Ding and Lavaei(2022)]{ding2022provably}
Yuhao Ding and Javad Lavaei.
\newblock Provably efficient primal-dual reinforcement learning for {CMDP}s
  with non-stationary objectives and constraints.
\newblock \emph{arXiv preprint arXiv:2201.11965}, 2022.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Pirotta, Kaufmann, and
  Valko]{domingues2021kernel}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Matteo Pirotta, Emilie Kaufmann,
  and Michal Valko.
\newblock A kernel-based approach to non-stationary reinforcement learning in
  metric spaces.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3538--3546. PMLR, 2021.

\bibitem[Faury et~al.(2021)Faury, Russac, Abeille, and
  Calauz{\`e}nes]{faury2021regret}
Louis Faury, Yoan Russac, Marc Abeille, and Cl{\'e}ment Calauz{\`e}nes.
\newblock Regret bounds for generalized linear bandits under parameter drift.
\newblock \emph{arXiv preprint arXiv:2103.05750}, 2021.

\bibitem[Fei et~al.(2020)Fei, Yang, Wang, and Xie]{fei2020dynamic}
Yingjie Fei, Zhuoran Yang, Zhaoran Wang, and Qiaomin Xie.
\newblock Dynamic regret of policy optimization in non-stationary environments.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6743--6754, 2020.

\bibitem[Gajane et~al.(2018)Gajane, Ortner, and Auer]{gajane2018sliding}
Pratik Gajane, Ronald Ortner, and Peter Auer.
\newblock A sliding-window algorithm for {M}arkov decision processes with
  arbitrarily changing rewards and transitions.
\newblock \emph{arXiv preprint arXiv:1805.10066}, 2018.

\bibitem[Gerchinovitz and Lattimore(2016)]{gerchinovitz2016refined}
S{\'e}bastien Gerchinovitz and Tor Lattimore.
\newblock Refined lower bounds for adversarial bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Jafarnia-Jahromi et~al.(2021)Jafarnia-Jahromi, Chen, Jain, and
  Luo]{jafarnia2021online}
Mehdi Jafarnia-Jahromi, Liyu Chen, Rahul Jain, and Haipeng Luo.
\newblock Online learning for stochastic shortest path model via posterior
  sampling.
\newblock \emph{arXiv preprint arXiv:2106.05335}, 2021.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lykouris et~al.(2021)Lykouris, Simchowitz, Slivkins, and
  Sun]{lykouris2021corruption}
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun.
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In \emph{Conference on Learning Theory}, pages 3242--3245. PMLR,
  2021.

\bibitem[Mao et~al.(2021)Mao, Zhang, Zhu, Simchi-Levi, and Basar]{mao2020model}
Weichao Mao, Kaiqing Zhang, Ruihao Zhu, David Simchi-Levi, and Tamer Basar.
\newblock Near-optimal model-free reinforcement learning in non-stationary
  episodic mdps.
\newblock In \emph{International Conference on Machine Learning}, pages
  7447--7458. PMLR, 2021.

\bibitem[Min et~al.(2021)Min, He, Wang, and Gu]{min2021learning}
Yifei Min, Jiafan He, Tianhao Wang, and Quanquan Gu.
\newblock Learning stochastic shortest path with linear function approximation.
\newblock \emph{arXiv preprint arXiv:2110.12727}, 2021.

\bibitem[Ortner et~al.(2020)Ortner, Gajane, and Auer]{pmlr-v115-ortner20a}
Ronald Ortner, Pratik Gajane, and Peter Auer.
\newblock Variational regret bounds for reinforcement learning.
\newblock In Ryan~P. Adams and Vibhav Gogate, editors, \emph{Proceedings of The
  35th Uncertainty in Artificial Intelligence Conference}, volume 115 of
  \emph{Proceedings of Machine Learning Research}, pages 81--90. PMLR, 22--25
  Jul 2020.

\bibitem[Rosenberg and Mansour(2021)]{rosenberg2020adversarial}
Aviv Rosenberg and Yishay Mansour.
\newblock Stochastic shortest path with adversarially changing costs.
\newblock In Zhi{-}Hua Zhou, editor, \emph{Proceedings of the Thirtieth
  International Joint Conference on Artificial Intelligence, {IJCAI} 2021,
  Virtual Event / Montreal, Canada, 19-27 August 2021}, 2021.

\bibitem[Russac et~al.(2020)Russac, Capp{\'e}, and
  Garivier]{russac2020algorithms}
Yoan Russac, Olivier Capp{\'e}, and Aur{\'e}lien Garivier.
\newblock Algorithms for non-stationary generalized linear bandits.
\newblock \emph{arXiv preprint arXiv:2003.10113}, 2020.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{efroni2020optimistic}
Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 8604--8613, 2020.

\bibitem[Suk and Kpotufe(2021)]{suk2021tracking}
Joe Suk and Samory Kpotufe.
\newblock Tracking most severe arm changes in bandits.
\newblock \emph{arXiv preprint arXiv:2112.13838}, 2021.

\bibitem[Tarbouriech et~al.(2020)Tarbouriech, Garcelon, Valko, Pirotta, and
  Lazaric]{tarbouriech2020no}
Jean Tarbouriech, Evrard Garcelon, Michal Valko, Matteo Pirotta, and Alessandro
  Lazaric.
\newblock No-regret exploration in goal-oriented reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  9428--9437. PMLR, 2020.

\bibitem[Tarbouriech et~al.(2021)Tarbouriech, Zhou, Du, Pirotta, Valko, and
  Lazaric]{tarbouriech2021stochastic}
Jean Tarbouriech, Runlong Zhou, Simon~S Du, Matteo Pirotta, Michal Valko, and
  Alessandro Lazaric.
\newblock Stochastic shortest path: Minimax, parameter-free and towards
  horizon-free regret.
\newblock \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Touati and Vincent(2020)]{touati2020efficient}
Ahmed Touati and Pascal Vincent.
\newblock Efficient learning in non-stationary linear {M}arkov decision
  processes.
\newblock \emph{arXiv preprint arXiv:2010.12870}, 2020.

\bibitem[Vial et~al.(2021)Vial, Parulekar, Shakkottai, and
  Srikant]{vial2021regret}
Daniel Vial, Advait Parulekar, Sanjay Shakkottai, and R~Srikant.
\newblock Regret bounds for stochastic shortest path problems with linear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2105.01593}, 2021.

\bibitem[Wei and Luo(2021)]{wei2021non}
Chen-Yu Wei and Haipeng Luo.
\newblock Non-stationary reinforcement learning without prior knowledge: An
  optimal black-box approach.
\newblock In \emph{Conference on Learning Theory}, pages 4300--4354. PMLR,
  2021.

\bibitem[Wei et~al.(2022)Wei, Dann, and Zimmert]{wei2022model}
Chen-Yu Wei, Christoph Dann, and Julian Zimmert.
\newblock A model selection approach for corruption robust reinforcement
  learning.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 1043--1096. PMLR, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Ji, and Du]{zhang2020reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon~S Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal
  algorithm escaping the curse of horizon.
\newblock In \emph{Conference On Learning Theory}, 2020.

\bibitem[Zhou et~al.(2020)Zhou, Chen, Varshney, and
  Jagmohan]{zhou2020nonstationary}
Huozhi Zhou, Jinglin Chen, Lav~R Varshney, and Ashish Jagmohan.
\newblock Nonstationary reinforcement learning with linear function
  approximation.
\newblock \emph{arXiv preprint arXiv:2010.04244}, 2020.

\end{thebibliography}
