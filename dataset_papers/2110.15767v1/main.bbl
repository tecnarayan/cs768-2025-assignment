\begin{thebibliography}{100}

\bibitem{esteva2019guide}
Andre Esteva, Alexandre Robicquet, Bharath Ramsundar, Volodymyr Kuleshov, Mark
  DePristo, Katherine Chou, Claire Cui, Greg Corrado, Sebastian Thrun, and Jeff
  Dean.
\newblock A guide to deep learning in healthcare.
\newblock {\em Nature medicine}, 25(1):24--29, 2019.

\bibitem{yao2019strong}
Li~Yao, Jordan Prosky, Ben Covington, and Kevin Lyman.
\newblock A strong baseline for domain adaptation and generalization in medical
  imaging.
\newblock {\em arXiv preprint arXiv:1904.01638}, 2019.

\bibitem{li2020domain}
Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex~C Kot.
\newblock Domain generalization for medical imaging classification with
  linear-dependency regularization.
\newblock {\em arXiv preprint arXiv:2009.12829}, 2020.

\bibitem{bashyam2020medical}
Vishnu~M Bashyam, Jimit Doshi, Guray Erus, Dhivya Srinivasan, Ahmed Abdulkadir,
  Mohamad Habes, Yong Fan, Colin~L Masters, Paul Maruff, Chuanjun Zhuo, et~al.
\newblock Medical image harmonization using deep learning based canonical
  mapping: Toward robust and generalizable learning in imaging.
\newblock {\em arXiv preprint arXiv:2010.05355}, 2020.

\bibitem{zhang2018review}
Dongxia Zhang, Xiaoqing Han, and Chunyu Deng.
\newblock Review on the research and practice of deep learning and
  reinforcement learning in smart grids.
\newblock {\em CSEE Journal of Power and Energy Systems}, 4(3):362--370, 2018.

\bibitem{karimipour2019deep}
Hadis Karimipour, Ali Dehghantanha, Reza~M Parizi, Kim-Kwang~Raymond Choo, and
  Henry Leung.
\newblock A deep and scalable unsupervised machine learning system for
  cyber-attack detection in large-scale smart grids.
\newblock {\em IEEE Access}, 7:80778--80788, 2019.

\bibitem{samad2017controls}
Tariq Samad and Anuradha~M Annaswamy.
\newblock Controls for smart grids: Architectures and applications.
\newblock {\em Proceedings of the IEEE}, 105(11):2244--2261, 2017.

\bibitem{julian2020never}
Ryan Julian, Benjamin Swanson, Gaurav~S Sukhatme, Sergey Levine, Chelsea Finn,
  and Karol Hausman.
\newblock Never stop learning: The effectiveness of fine-tuning in robotic
  reinforcement learning.
\newblock {\em arXiv e-prints}, pages arXiv--2004, 2020.

\bibitem{kober2013reinforcement}
Jens Kober, J~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock {\em The International Journal of Robotics Research},
  32(11):1238--1274, 2013.

\bibitem{sunderhauf2018limits}
Niko S{\"u}nderhauf, Oliver Brock, Walter Scheirer, Raia Hadsell, Dieter Fox,
  J{\"u}rgen Leitner, Ben Upcroft, Pieter Abbeel, Wolfram Burgard, Michael
  Milford, et~al.
\newblock The limits and potentials of deep learning for robotics.
\newblock {\em The International Journal of Robotics Research},
  37(4-5):405--420, 2018.

\bibitem{biggio2013evasion}
Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim
  {\v{S}}rndi{\'c}, Pavel Laskov, Giorgio Giacinto, and Fabio Roli.
\newblock Evasion attacks against machine learning at test time.
\newblock In {\em Joint European conference on machine learning and knowledge
  discovery in databases}, pages 387--402. Springer, 2013.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em 2017 ieee symposium on security and privacy (sp)}, pages
  39--57. IEEE, 2017.

\bibitem{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}, 2019.

\bibitem{djolonga2020robustness}
Josip Djolonga, Jessica Yung, Michael Tschannen, Rob Romijnders, Lucas Beyer,
  Alexander Kolesnikov, Joan Puigcerver, Matthias Minderer, Alexander D'Amour,
  Dan Moldovan, et~al.
\newblock On robustness and transferability of convolutional neural networks.
\newblock {\em arXiv preprint arXiv:2007.08558}, 2020.

\bibitem{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{hendrycks2020many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock {\em arXiv preprint arXiv:2006.16241}, 2020.

\bibitem{torralba2011unbiased}
Antonio Torralba and Alexei~A Efros.
\newblock Unbiased look at dataset bias.
\newblock In {\em CVPR 2011}, pages 1521--1528. IEEE, 2011.

\bibitem{datta2014automated}
Amit Datta, Michael~Carl Tschantz, and Anupam Datta.
\newblock Automated experiments on ad privacy settings: A tale of opacity,
  choice, and discrimination.
\newblock {\em arXiv preprint arXiv:1408.6491}, 2014.

\bibitem{kay2015unequal}
Matthew Kay, Cynthia Matuszek, and Sean~A Munson.
\newblock Unequal representation and gender stereotypes in image search results
  for occupations.
\newblock In {\em Proceedings of the 33rd Annual ACM Conference on Human
  Factors in Computing Systems}, pages 3819--3828, 2015.

\bibitem{angwin2016machine}
Julia Angwin, Jeff Larson, Surya Mattu, and Lauren Kirchner.
\newblock Machine bias.
\newblock {\em ProPublica, May}, 23(2016):139--159, 2016.

\bibitem{sonar2020invariant}
Anoopkumar Sonar, Vincent Pacelli, and Anirudha Majumdar.
\newblock Invariant policy optimization: Towards stronger generalization in
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2006.01096}, 2020.

\bibitem{vinitsky2020robust}
Eugene Vinitsky, Yuqing Du, Kanaad Parvate, Kathy Jang, Pieter Abbeel, and
  Alexandre Bayen.
\newblock Robust reinforcement learning using adversarial populations.
\newblock {\em arXiv preprint arXiv:2008.01825}, 2020.

\bibitem{tukey1960survey}
John~W Tukey.
\newblock A survey of sampling from contaminated distributions.
\newblock {\em Contributions to probability and statistics}, pages 448--485,
  1960.

\bibitem{huber1992robust}
Peter~J Huber.
\newblock Robust estimation of a location parameter.
\newblock In {\em Breakthroughs in statistics}, pages 492--518. Springer, 1992.

\bibitem{huber2004robust}
Peter~J Huber.
\newblock {\em Robust statistics}, volume 523.
\newblock John Wiley \& Sons, 2004.

\bibitem{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}, 2014.

\bibitem{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}, 2017.

\bibitem{wong2017provable}
Eric Wong and J~Zico Kolter.
\newblock Provable {D}efenses {A}gainst {A}dversarial {E}xamples {V}ia the
  {C}onvex {O}uter {A}dversarial {P}olytope.
\newblock {\em arXiv preprint arXiv:1711.00851}, 2017.

\bibitem{huang2017adversarial}
Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel.
\newblock Adversarial attacks on neural network policies.
\newblock {\em arXiv preprint arXiv:1702.02284}, 2017.

\bibitem{sinha2018gradient}
Ayan Sinha, Zhao Chen, Vijay Badrinarayanan, and Andrew Rabinovich.
\newblock Gradient adversarial training of neural networks.
\newblock {\em arXiv preprint arXiv:1806.08028}, 2018.

\bibitem{shaham2018understanding}
Uri Shaham, Yutaro Yamada, and Sahand Negahban.
\newblock Understanding adversarial training: Increasing local stability of
  supervised models through robust optimization.
\newblock {\em Neurocomputing}, 307:195--204, 2018.

\bibitem{carlini2019evaluating}
Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
  Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey
  Kurakin.
\newblock On evaluating adversarial robustness.
\newblock {\em arXiv preprint arXiv:1902.06705}, 2019.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock In {\em International Conference on Machine Learning}, pages
  274--283. PMLR, 2018.

\bibitem{li2019implicit}
Yan Li, Ethan~X Fang, Huan Xu, and Tuo Zhao.
\newblock Implicit bias of gradient descent based adversarial training on
  separable data.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{soltanolkotabi2018theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 65(2):742--769, 2018.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{arpit2017closer}
Devansh Arpit, Stanis{\l}aw Jastrz{\k{e}}bski, Nicolas Ballas, David Krueger,
  Emmanuel Bengio, Maxinder~S Kanwal, Tegan Maharaj, Asja Fischer, Aaron
  Courville, Yoshua Bengio, et~al.
\newblock A closer look at memorization in deep networks.
\newblock In {\em International Conference on Machine Learning}, pages
  233--242. PMLR, 2017.

\bibitem{ge2017learning}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock {\em arXiv preprint arXiv:1711.00501}, 2017.

\bibitem{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In {\em International conference on machine learning}, pages
  605--614. PMLR, 2017.

\bibitem{wu2020adversarial}
Dongxian Wu, Shu-tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock {\em arXiv preprint arXiv:2004.05884}, 2020.

\bibitem{cheng2020cat}
Minhao Cheng, Qi~Lei, Pin-Yu Chen, Inderjit Dhillon, and Cho-Jui Hsieh.
\newblock Cat: Customized adversarial training for improved robustness.
\newblock {\em arXiv preprint arXiv:2002.06789}, 2020.

\bibitem{kannan2018adversarial}
Harini Kannan, Alexey Kurakin, and Ian Goodfellow.
\newblock Adversarial logit pairing.
\newblock {\em arXiv preprint arXiv:1803.06373}, 2018.

\bibitem{guo2017countering}
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der~Maaten.
\newblock Countering adversarial images using input transformations.
\newblock {\em arXiv preprint arXiv:1711.00117}, 2017.

\bibitem{shaham2018defending}
Uri Shaham, James Garritano, Yutaro Yamada, Ethan Weinberger, Alex Cloninger,
  Xiuyuan Cheng, Kelly Stanton, and Yuval Kluger.
\newblock Defending against adversarial images using basis functions
  transformations.
\newblock {\em arXiv preprint arXiv:1803.10840}, 2018.

\bibitem{dhillon2018stochastic}
Guneet~S Dhillon, Kamyar Azizzadenesheli, Zachary~C Lipton, Jeremy Bernstein,
  Jean Kossaifi, Aran Khanna, and Anima Anandkumar.
\newblock Stochastic activation pruning for robust adversarial defense.
\newblock {\em arXiv preprint arXiv:1803.01442}, 2018.

\bibitem{carmon2019unlabeled}
Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy Liang, and John~C Duchi.
\newblock Unlabeled data improves adversarial robustness.
\newblock {\em arXiv preprint arXiv:1905.13736}, 2019.

\bibitem{bai2019hilbert}
Yang Bai, Yan Feng, Yisen Wang, Tao Dai, Shu-Tao Xia, and Yong Jiang.
\newblock Hilbert-based generative defense for adversarial examples.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 4784--4793, 2019.

\bibitem{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Zheng Xu, John Dickerson, Christoph
  Studer, Larry~S Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock {\em arXiv preprint arXiv:1904.12843}, 2019.

\bibitem{papernot2016distillation}
Nicolas Papernot, Patrick McDaniel, Xi~Wu, Somesh Jha, and Ananthram Swami.
\newblock Distillation as a defense to adversarial perturbations against deep
  neural networks.
\newblock In {\em 2016 IEEE symposium on security and privacy (SP)}, pages
  582--597. IEEE, 2016.

\bibitem{dobriban2020provable}
Edgar Dobriban, Hamed Hassani, David Hong, and Alexander Robey.
\newblock Provable tradeoffs in adversarially robust classification.
\newblock {\em arXiv preprint arXiv:2006.05161}, 2020.

\bibitem{javanmard2020precise}
Adel Javanmard, Mahdi Soltanolkotabi, and Hamed Hassani.
\newblock Precise tradeoffs in adversarial training for linear regression.
\newblock In {\em Conference on Learning Theory}, pages 2034--2078. PMLR, 2020.

\bibitem{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock {\em arXiv preprint arXiv:1805.12152}, 2018.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In {\em International Conference on Machine Learning}, pages
  7472--7482. PMLR, 2019.

\bibitem{wang2019improving}
Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu.
\newblock Improving adversarial robustness requires revisiting misclassified
  examples.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{zheng2016improving}
Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow.
\newblock Improving the robustness of deep neural networks via stability
  training.
\newblock In {\em Proceedings of the ieee conference on computer vision and
  pattern recognition}, pages 4480--4488, 2016.

\bibitem{ding2018mma}
Gavin~Weiguang Ding, Yash Sharma, Kry Yik~Chau Lui, and Ruitong Huang.
\newblock Mma training: Direct input space margin maximization through
  adversarial training.
\newblock {\em arXiv preprint arXiv:1812.02637}, 2018.

\bibitem{vapnik2013nature}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer science \& business media, 2013.

\bibitem{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David.
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem{paternain2019constrained}
Santiago Paternain, Luiz~FO Chamon, Miguel Calvo-Fullana, and Alejandro
  Ribeiro.
\newblock Constrained reinforcement learning has zero duality gap.
\newblock {\em arXiv preprint arXiv:1910.13393}, 2019.

\bibitem{chamon2020probably}
Luiz Chamon and Alejandro Ribeiro.
\newblock Probably approximately correct constrained learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{tramer2020fundamental}
Florian Tram{\`e}r, Jens Behrmann, Nicholas Carlini, Nicolas Papernot, and
  J{\"o}rn-Henrik Jacobsen.
\newblock Fundamental tradeoffs between invariance and sensitivity to
  adversarial perturbations.
\newblock In {\em International Conference on Machine Learning}, pages
  9561--9571. PMLR, 2020.

\bibitem{robey2020model}
Alexander Robey, Hamed Hassani, and George~J Pappas.
\newblock Model-based robust deep learning.
\newblock {\em arXiv preprint arXiv:2005.10247}, 2020.

\bibitem{robey2021model}
Alexander Robey, George~J Pappas, and Hamed Hassani.
\newblock Model-based domain generalization.
\newblock {\em arXiv preprint arXiv:2102.11436}, 2021.

\bibitem{goodfellow2009measuring}
Ian Goodfellow, Honglak Lee, Quoc Le, Andrew Saxe, and Andrew Ng.
\newblock Measuring invariances in deep networks.
\newblock {\em Advances in neural information processing systems}, 22:646--654,
  2009.

\bibitem{wong2020learning}
Eric Wong and J~Zico Kolter.
\newblock Learning perturbation sets for robust machine learning.
\newblock {\em arXiv preprint arXiv:2007.08450}, 2020.

\bibitem{gowal2020achieving}
Sven Gowal, Chongli Qin, Po-Sen Huang, Taylan Cemgil, Krishnamurthy Dvijotham,
  Timothy Mann, and Pushmeet Kohli.
\newblock Achieving robustness in the wild via adversarial mixing with
  disentangled representations.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 1211--1220, 2020.

\bibitem{awasthi2020adversarial}
Pranjal Awasthi, Natalie Frank, and Mehryar Mohri.
\newblock Adversarial learning guarantees for linear hypotheses and neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  431--441. PMLR, 2020.

\bibitem{yin2019rademacher}
Dong Yin, Ramchandran Kannan, and Peter Bartlett.
\newblock Rademacher complexity for adversarially robust generalization.
\newblock In {\em International Conference on Machine Learning}, pages
  7085--7094. PMLR, 2019.

\bibitem{cullina2018pac}
Daniel Cullina, Arjun~Nitin Bhagoji, and Prateek Mittal.
\newblock Pac-learning in the presence of evasion adversaries.
\newblock {\em arXiv preprint arXiv:1806.01471}, 2018.

\bibitem{montasser2020efficiently}
Omar Montasser, Surbhi Goel, Ilias Diakonikolas, and Nathan Srebro.
\newblock Efficiently learning adversarially robust halfspaces with noise.
\newblock {\em arXiv preprint arXiv:2005.07652}, 2020.

\bibitem{montasser2019vc}
Omar Montasser, Steve Hanneke, and Nathan Srebro.
\newblock Vc classes are adversarially robustly learnable, but only improperly.
\newblock {\em arXiv preprint arXiv:1902.04217}, 2019.

\bibitem{shaham2015understanding}
Uri Shaham, Yutaro Yamada, and Sahand Negahban.
\newblock Understanding adversarial training: Increasing local stability of
  neural nets through robust optimization.
\newblock {\em arXiv preprint arXiv:1511.05432}, 2015.

\bibitem{wong2020fast}
Eric Wong, Leslie Rice, and J~Zico Kolter.
\newblock Fast is better than free: Revisiting adversarial training.
\newblock {\em arXiv preprint arXiv:2001.03994}, 2020.

\bibitem{shi2020adaptive}
Yucheng Shi, Yahong Han, Quanxin Zhang, and Xiaohui Kuang.
\newblock Adaptive iterative attack towards explainable adversarial robustness.
\newblock {\em Pattern Recognition}, 105:107309, 2020.

\bibitem{raghunathan2020understanding}
Aditi Raghunathan, Sang~Michael Xie, Fanny Yang, John Duchi, and Percy Liang.
\newblock Understanding and mitigating the tradeoff between robustness and
  accuracy.
\newblock {\em arXiv preprint arXiv:2002.10716}, 2020.

\bibitem{yang2020closer}
Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Ruslan Salakhutdinov, and
  Kamalika Chaudhuri.
\newblock A closer look at accuracy vs. robustness.
\newblock {\em arXiv preprint arXiv:2003.02460}, 2020.

\bibitem{chamon2020empirical}
Luiz~FO Chamon, Santiago Paternain, Miguel Calvo-Fullana, and Alejandro
  Ribeiro.
\newblock The empirical duality gap of constrained statistical learning.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 8374--8378. IEEE, 2020.

\bibitem{bartlett2017spectrally}
Peter Bartlett, Dylan~J Foster, and Matus Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock {\em arXiv preprint arXiv:1706.08498}, 2017.

\bibitem{khim2018adversarial}
Justin Khim and Po-Ling Loh.
\newblock Adversarial risk bounds via function transformation.
\newblock {\em arXiv preprint arXiv:1810.09519}, 2018.

\bibitem{tu2019theoretical}
Zhuozhuo Tu, Jingwei Zhang, and Dacheng Tao.
\newblock Theoretical analysis of adversarial learning: A minimax approach.
\newblock 2019.

\bibitem{bubeck2015finite}
Sebastien Bubeck, Ronen Eldan, and Joseph Lehec.
\newblock Finite-time analysis of projected langevin monte carlo.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1243--1251. Citeseer, 2015.

\bibitem{nishimura2020discontinuous}
Akihiko Nishimura, David~B Dunson, and Jianfeng Lu.
\newblock Discontinuous hamiltonian monte carlo for discrete parameters and
  discontinuous likelihoods.
\newblock {\em Biometrika}, 107(2):365--380, 2020.

\bibitem{holmstrom1992using}
Lasse Holmstrom, Petri Koistinen, et~al.
\newblock Using additive noise in back-propagation training.
\newblock {\em IEEE transactions on neural networks}, 3(1):24--38, 1992.

\bibitem{lopes2019improving}
Raphael~Gontijo Lopes, Dong Yin, Ben Poole, Justin Gilmer, and Ekin~D Cubuk.
\newblock Improving robustness without sacrificing accuracy with patch gaussian
  augmentation.
\newblock {\em arXiv preprint arXiv:1906.02611}, 2019.

\bibitem{rusak2020simple}
Evgenia Rusak, Lukas Schott, Roland~S Zimmermann, Julian Bitterwolf, Oliver
  Bringmann, Matthias Bethge, and Wieland Brendel.
\newblock A simple way to make neural networks robust against diverse image
  corruptions.
\newblock In {\em European Conference on Computer Vision}, pages 53--69.
  Springer, 2020.

\bibitem{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock {\em arXiv preprint arXiv:1708.04552}, 2017.

\bibitem{zhong2020random}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi~Yang.
\newblock Random erasing data augmentation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 13001--13008, 2020.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6023--6032, 2019.

\bibitem{takahashi2019data}
Ryo Takahashi, Takashi Matsubara, and Kuniaki Uehara.
\newblock Data augmentation using random image cropping and patching for deep
  cnns.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology},
  30(9):2917--2931, 2019.

\bibitem{jalal2017robust}
Ajil Jalal, Andrew Ilyas, Constantinos Daskalakis, and Alexandros~G Dimakis.
\newblock The robust manifold defense: Adversarial training using generative
  models.
\newblock {\em arXiv preprint arXiv:1712.09196}, 2017.

\bibitem{xiao2018generating}
Chaowei Xiao, Bo~Li, Jun-Yan Zhu, Warren He, Mingyan Liu, and Dawn Song.
\newblock Generating adversarial examples with adversarial networks.
\newblock {\em arXiv preprint arXiv:1801.02610}, 2018.

\bibitem{samangouei2018defense}
Pouya Samangouei, Maya Kabkab, and Rama Chellappa.
\newblock Defense-gan: Protecting classifiers against adversarial attacks using
  generative models.
\newblock {\em arXiv preprint arXiv:1805.06605}, 2018.

\bibitem{chen2020group}
Shuxiao Chen, Edgar Dobriban, and Jane~H Lee.
\newblock A group-theoretic framework for data augmentation.
\newblock {\em Journal of Machine Learning Research}, 21(245):1--71, 2020.

\bibitem{neal2011mcmc}
Radford~M Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{croce2020robustbench}
Francesco Croce, Maksym Andriushchenko, Vikash Sehwag, Nicolas Flammarion, Mung
  Chiang, Prateek Mittal, and Matthias Hein.
\newblock Robustbench: a standardized adversarial robustness benchmark.
\newblock {\em arXiv preprint arXiv:2010.09670}, 2020.

\bibitem{bertsekas2009convex}
Dimitri~P Bertsekas.
\newblock {\em Convex optimization theory}.
\newblock Athena Scientific Belmont, 2009.

\bibitem{rockafellar2009variational}
R~Tyrrell Rockafellar and Roger J-B Wets.
\newblock {\em Variational analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem{ruszczynski2011nonlinear}
Andrzej Ruszczynski.
\newblock {\em Nonlinear optimization}.
\newblock Princeton university press, 2011.

\bibitem{bonnans2013perturbation}
J~Fr{\'e}d{\'e}ric Bonnans and Alexander Shapiro.
\newblock {\em Perturbation analysis of optimization problems}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{betancourt2017conceptual}
Michael Betancourt.
\newblock A conceptual introduction to hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:1701.02434}, 2017.

\bibitem{bishop2006pattern}
Christopher~M Bishop.
\newblock Pattern recognition.
\newblock {\em Machine learning}, 128(9), 2006.

\bibitem{MNISTWebPage}
The {MNIST} database of handwritten digits {Home Page}.
\newblock \url{http://yann.lecun.com/exdb/mnist/}.

\bibitem{zeiler2012adadelta}
Matthew~D Zeiler.
\newblock Adadelta: an adaptive learning rate method.
\newblock {\em arXiv preprint arXiv:1212.5701}, 2012.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{bubeck2014convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em arXiv preprint arXiv:1405.4980}, 2014.

\bibitem{bonnans2019convex}
J~Fr{\'e}d{\'e}ric Bonnans.
\newblock {\em Convex and Stochastic Optimization}.
\newblock Springer, 2019.

\bibitem{bertsekas2000gradient}
Dimitri~P Bertsekas and John~N Tsitsiklis.
\newblock Gradient convergence in gradient methods with errors.
\newblock {\em SIAM Journal on Optimization}, 10(3):627--642, 2000.

\bibitem{ajalloeian2020analysis}
Ahmad Ajalloeian and Sebastian~U Stich.
\newblock Analysis of sgd with biased gradient estimators.
\newblock {\em arXiv preprint arXiv:2008.00051}, 2020.

\bibitem{sinha2017certifying}
Aman Sinha, Hongseok Namkoong, Riccardo Volpi, and John Duchi.
\newblock Certifying some distributional robustness with principled adversarial
  training.
\newblock {\em arXiv preprint arXiv:1710.10571}, 2017.

\bibitem{gao2017wasserstein}
Rui Gao, Xi~Chen, and Anton~J Kleywegt.
\newblock Wasserstein distributional robustness and regularization in
  statistical learning.
\newblock {\em arXiv e-prints}, pages arXiv--1712, 2017.

\bibitem{ben2009robust}
Aharon Ben-Tal, Laurent El~Ghaoui, and Arkadi Nemirovski.
\newblock {\em Robust optimization}.
\newblock Princeton university press, 2009.

\bibitem{salman2019provably}
Hadi Salman, Greg Yang, Jerry Li, Pengchuan Zhang, Huan Zhang, Ilya
  Razenshteyn, and Sebastien Bubeck.
\newblock Provably robust deep learning via adversarially trained smoothed
  classifiers.
\newblock {\em arXiv preprint arXiv:1906.04584}, 2019.

\bibitem{cohen2019certified}
Jeremy Cohen, Elan Rosenfeld, and Zico Kolter.
\newblock Certified adversarial robustness via randomized smoothing.
\newblock In {\em International Conference on Machine Learning}, pages
  1310--1320. PMLR, 2019.

\bibitem{kumar2020curse}
Aounon Kumar, Alexander Levine, Tom Goldstein, and Soheil Feizi.
\newblock Curse of dimensionality on randomized smoothing for certifiable
  robustness.
\newblock In {\em International Conference on Machine Learning}, pages
  5458--5467. PMLR, 2020.

\bibitem{donti2021dc3}
Priya~L Donti, David Rolnick, and J~Zico Kolter.
\newblock Dc3: A learning method for optimization with hard constraints.
\newblock {\em arXiv preprint arXiv:2104.12225}, 2021.

\bibitem{pathak2015constrained}
Deepak Pathak, Philipp Krahenbuhl, and Trevor Darrell.
\newblock Constrained convolutional neural networks for weakly supervised
  segmentation.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1796--1804, 2015.

\bibitem{chen2018approximating}
Steven Chen, Kelsey Saulnier, Nikolay Atanasov, Daniel~D Lee, Vijay Kumar,
  George~J Pappas, and Manfred Morari.
\newblock Approximating explicit model predictive control using constrained
  neural networks.
\newblock In {\em 2018 Annual American control conference (ACC)}, pages
  1520--1527. IEEE, 2018.

\bibitem{frerix2020homogeneous}
Thomas Frerix, Matthias Nie{\ss}ner, and Daniel Cremers.
\newblock Homogeneous linear inequality constraints for neural network
  activations.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition Workshops}, pages 748--749, 2020.

\bibitem{amos2017optnet}
Brandon Amos and J~Zico Kolter.
\newblock Optnet: Differentiable optimization as a layer in neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  136--145. PMLR, 2017.

\bibitem{ravi2018constrained}
Sathya~N Ravi, Tuan Dinh, Vishnu Lokhande, and Vikas Singh.
\newblock Constrained deep learning using conditional gradient and applications
  in computer vision.
\newblock {\em arXiv preprint arXiv:1803.06453}, 2018.

\bibitem{agrawal2019differentiable}
Akshay Agrawal, Brandon Amos, Shane Barratt, Stephen Boyd, Steven Diamond, and
  Zico Kolter.
\newblock Differentiable convex optimization layers.
\newblock {\em arXiv preprint arXiv:1910.12430}, 2019.

\bibitem{karras1995efficient}
Dimitris~A Karras and Stavros~J Perantonis.
\newblock An efficient constrained training algorithm for feedforward networks.
\newblock {\em IEEE Transactions on Neural Networks}, 6(6):1420--1434, 1995.

\end{thebibliography}
