\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2017)Agarwal, Luo, Neyshabur, and
  Schapire]{agarwal2017corralling}
Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert~E Schapire.
\newblock Corralling a band of bandit algorithms.
\newblock In \emph{Conference on Learning Theory}, pages 12--38. PMLR, 2017.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20095--20107, 2020.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Baird(1995)]{baird1995residual}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Machine Learning Proceedings 1995}, pages 30--37. Elsevier,
  1995.

\bibitem[Bartlett(2008)]{bartlett2008fast}
Peter~L Bartlett.
\newblock Fast rates for estimation error and oracle inequalities for model
  selection.
\newblock \emph{Econometric Theory}, pages 545--552, 2008.

\bibitem[Bartlett et~al.(2002)Bartlett, Boucheron, and
  Lugosi]{bartlett2002model}
Peter~L Bartlett, St{\'e}phane Boucheron, and G{\'a}bor Lugosi.
\newblock Model selection and error estimation.
\newblock \emph{Machine Learning}, 48\penalty0 (1):\penalty0 85--113, 2002.

\bibitem[Bartlett et~al.(2019)Bartlett, Harvey, Liaw, and
  Mehrabian]{bartlett2019nearly}
Peter~L Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 2285--2301, 2019.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym (2016).
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Chang et~al.(2022)Chang, Wang, Kallus, and Sun]{chang2022learning}
Jonathan Chang, Kaiwen Wang, Nathan Kallus, and Wen Sun.
\newblock Learning bellman complete representations for offline policy
  evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  2938--2971. PMLR, 2022.

\bibitem[Chatterji et~al.(2020)Chatterji, Muthukumar, and
  Bartlett]{chatterji2020osom}
Niladri Chatterji, Vidya Muthukumar, and Peter Bartlett.
\newblock Osom: A simultaneously optimal algorithm for multi-armed and linear
  contextual bandits.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1844--1854, 2020.

\bibitem[Chen and Jiang(2019)]{chen2019information}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1042--1051. PMLR, 2019.

\bibitem[Dai et~al.(2020)Dai, Nachum, Chow, Li, Szepesv{\'a}ri, and
  Schuurmans]{dai2020coindice}
Bo~Dai, Ofir Nachum, Yinlam Chow, Lihong Li, Csaba Szepesv{\'a}ri, and Dale
  Schuurmans.
\newblock Coindice: Off-policy confidence interval estimation.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 9398--9411, 2020.

\bibitem[Duan et~al.(2020)Duan, Jia, and Wang]{duan2020minimax}
Yaqi Duan, Zeyu Jia, and Mengdi Wang.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  2701--2709. PMLR, 2020.

\bibitem[Duan et~al.(2021)Duan, Jin, and Li]{duan2021risk}
Yaqi Duan, Chi Jin, and Zhiyuan Li.
\newblock Risk bounds and rademacher complexity in batch reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2892--2902. PMLR, 2021.

\bibitem[Farahmand and Szepesv{\'a}ri(2011)]{farahmand2011model}
Amir-massoud Farahmand and Csaba Szepesv{\'a}ri.
\newblock Model selection in reinforcement learning.
\newblock \emph{Machine learning}, 85\penalty0 (3):\penalty0 299--332, 2011.

\bibitem[Foster et~al.(2019)Foster, Krishnamurthy, and Luo]{foster2019model}
Dylan Foster, Akshay Krishnamurthy, and Haipeng Luo.
\newblock Model selection for contextual bandits.
\newblock \emph{arXiv preprint arXiv:1906.00531}, 2019.

\bibitem[Foster et~al.(2021)Foster, Krishnamurthy, Simchi-Levi, and
  Xu]{foster2021offline}
Dylan~J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu.
\newblock Offline reinforcement learning: Fundamental barriers for value
  function approximation.
\newblock \emph{arXiv preprint arXiv:2111.10919}, 2021.

\bibitem[Hallak et~al.(2013)Hallak, Di-Castro, and Mannor]{hallak2013model}
Assaf Hallak, Dotan Di-Castro, and Shie Mannor.
\newblock Model selection in markovian processes.
\newblock In \emph{Proceedings of the 19th ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 374--382, 2013.

\bibitem[Jiang and Li(2016)]{jiang2016doubly}
Nan Jiang and Lihong Li.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  652--661. PMLR, 2016.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, and Singh]{jiang2015abstraction}
Nan Jiang, Alex Kulesza, and Satinder Singh.
\newblock Abstraction selection in model-based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  179--188. PMLR, 2015.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Ying Jin, Zhuoran Yang, and Zhaoran Wang.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pages
  5084--5096. PMLR, 2021.

\bibitem[Kumar et~al.(2021)Kumar, Singh, Tian, Finn, and
  Levine]{kumar2021workflow}
Aviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine.
\newblock A workflow for offline model-free robotic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2109.10813}, 2021.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Sascha Lange, Thomas Gabel, and Martin Riedmiller.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pages 45--73. Springer, 2012.

\bibitem[Lee et~al.(2021{\natexlab{a}})Lee, Pacchiano, Muthukumar, Kong, and
  Brunskill]{lee2021online}
Jonathan Lee, Aldo Pacchiano, Vidya Muthukumar, Weihao Kong, and Emma
  Brunskill.
\newblock Online model selection for reinforcement learning with function
  approximation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3340--3348. PMLR, 2021{\natexlab{a}}.

\bibitem[Lee et~al.(2021{\natexlab{b}})Lee, Tucker, Nachum, and
  Dai]{lee2021model}
Jonathan~N Lee, George Tucker, Ofir Nachum, and Bo~Dai.
\newblock Model selection in batch policy optimization.
\newblock \emph{arXiv preprint arXiv:2112.12320}, 2021{\natexlab{b}}.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liu et~al.(2020)Liu, Swaminathan, Agarwal, and
  Brunskill]{liu2020provably}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Provably good batch off-policy reinforcement learning without great
  exploration.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1264--1274, 2020.

\bibitem[Lugosi and Nobel(1999)]{lugosi1999adaptive}
G{\'a}bor Lugosi and Andrew~B Nobel.
\newblock Adaptive model selection using empirical complexities.
\newblock \emph{The Annals of Statistics}, 27\penalty0 (6):\penalty0
  1830--1864, 1999.

\bibitem[Mandlekar et~al.(2021)Mandlekar, Xu, Wong, Nasiriany, Wang, Kulkarni,
  Fei-Fei, Savarese, Zhu, and Mart{\'\i}n-Mart{\'\i}n]{mandlekar2021matters}
Ajay Mandlekar, Danfei Xu, Josiah Wong, Soroush Nasiriany, Chen Wang, Rohun
  Kulkarni, Li~Fei-Fei, Silvio Savarese, Yuke Zhu, and Roberto
  Mart{\'\i}n-Mart{\'\i}n.
\newblock What matters in learning from offline human demonstrations for robot
  manipulation.
\newblock \emph{arXiv preprint arXiv:2108.03298}, 2021.

\bibitem[Massart(2007)]{massart2007concentration}
Pascal Massart.
\newblock \emph{Concentration inequalities and model selection: Ecole d'Et{\'e}
  de Probabilit{\'e}s de Saint-Flour XXXIII-2003}.
\newblock Springer, 2007.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2010--2020. PMLR, 2020.

\bibitem[Modi et~al.(2021)Modi, Chen, Krishnamurthy, Jiang, and
  Agarwal]{modi2021model}
Aditya Modi, Jinglin Chen, Akshay Krishnamurthy, Nan Jiang, and Alekh Agarwal.
\newblock Model-free representation learning and exploration in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2102.07035}, 2021.

\bibitem[Munos and Szepesv{\'a}ri(2008)]{munos2008finite}
R{\'e}mi Munos and Csaba Szepesv{\'a}ri.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Muthukumar and Krishnamurthy(2021)]{muthukumar2021universal}
Vidya Muthukumar and Akshay Krishnamurthy.
\newblock Universal and data-adaptive algorithms for model selection in linear
  contextual bandits.
\newblock \emph{arXiv preprint arXiv:2111.04688}, 2021.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{nachum2019dualdice}
Ofir Nachum, Yinlam Chow, Bo~Dai, and Lihong Li.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary
  distribution corrections.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Pacchiano et~al.(2020)Pacchiano, Phan, Abbasi~Yadkori, Rao, Zimmert,
  Lattimore, and Szepesvari]{pacchiano2020model}
Aldo Pacchiano, My~Phan, Yasin Abbasi~Yadkori, Anup Rao, Julian Zimmert, Tor
  Lattimore, and Csaba Szepesvari.
\newblock Model selection in contextual stochastic bandit problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 10328--10337, 2020.

\bibitem[Paine et~al.(2020)Paine, Paduraru, Michi, Gulcehre, Zolna, Novikov,
  Wang, and de~Freitas]{paine2020hyperparameter}
Tom~Le Paine, Cosmin Paduraru, Andrea Michi, Caglar Gulcehre, Konrad Zolna,
  Alexander Novikov, Ziyu Wang, and Nando de~Freitas.
\newblock Hyperparameter selection for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2007.09055}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Precup(2000)]{precup2000eligibility}
Doina Precup.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock \emph{Computer Science Department Faculty Publication Series},
  page~80, 2000.

\bibitem[Seno and Imai(2021)]{seno2021d3rlpy}
Takuma Seno and Michita Imai.
\newblock d3rlpy: An offline deep reinforcement learning library.
\newblock \emph{arXiv preprint arXiv:2111.03788}, 2021.

\bibitem[Simchi-Levi and Xu(2021)]{simchi2021bypassing}
David Simchi-Levi and Yunzong Xu.
\newblock Bypassing the monster: A faster and simpler optimal algorithm for
  contextual bandits under realizability.
\newblock \emph{Mathematics of Operations Research}, 2021.

\bibitem[Su et~al.(2020)Su, Srinath, and Krishnamurthy]{su2020adaptive}
Yi~Su, Pavithra Srinath, and Akshay Krishnamurthy.
\newblock Adaptive estimator selection for off-policy evaluation.
\newblock In \emph{International Conference on Machine Learning}, pages
  9196--9205. PMLR, 2020.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tang and Wiens(2021)]{tang2021model}
Shengpu Tang and Jenna Wiens.
\newblock Model selection for offline reinforcement learning: Practical
  considerations for healthcare settings.
\newblock In \emph{Machine Learning for Healthcare Conference}, pages 2--35.
  PMLR, 2021.

\bibitem[Thomas and Brunskill(2016)]{thomas2016data}
Philip Thomas and Emma Brunskill.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2139--2148. PMLR, 2016.

\bibitem[Uehara and Sun(2021)]{uehara2021pessimistic}
Masatoshi Uehara and Wen Sun.
\newblock Pessimistic model-based offline reinforcement learning under partial
  coverage.
\newblock \emph{arXiv preprint arXiv:2107.06226}, 2021.

\bibitem[Vershynin(2018)]{vershynin2018high}
Roman Vershynin.
\newblock \emph{High-dimensional probability: An introduction with applications
  in data science}, volume~47.
\newblock Cambridge university press, 2018.

\bibitem[Wang et~al.(2020)Wang, Foster, and Kakade]{wang2020statistical}
Ruosong Wang, Dean~P Foster, and Sham~M Kakade.
\newblock What are the statistical limits of offline rl with linear function
  approximation?
\newblock \emph{arXiv preprint arXiv:2010.11895}, 2020.

\bibitem[Xie and Jiang(2021)]{xie2021batch}
Tengyang Xie and Nan Jiang.
\newblock Batch value-function approximation with only realizability.
\newblock In \emph{International Conference on Machine Learning}, pages
  11404--11413. PMLR, 2021.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{xie2019towards}
Tengyang Xie, Yifei Ma, and Yu-Xiang Wang.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Xie et~al.(2021)Xie, Cheng, Jiang, Mineiro, and
  Agarwal]{xie2021bellman}
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal.
\newblock Bellman-consistent pessimism for offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 6683--6694, 2021.

\bibitem[Zanette(2021)]{zanette2021exponential}
Andrea Zanette.
\newblock Exponential lower bounds for batch reinforcement learning: Batch rl
  can be exponentially harder than online rl.
\newblock In \emph{International Conference on Machine Learning}, pages
  12287--12297. PMLR, 2021.

\bibitem[Zhan et~al.(2022)Zhan, Huang, Huang, Jiang, and Lee]{zhan2022offline}
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason~D Lee.
\newblock Offline reinforcement learning with realizability and single-policy
  concentrability.
\newblock \emph{arXiv preprint arXiv:2202.04634}, 2022.

\bibitem[Zhang and Jiang(2021)]{zhang2021towards}
Siyuan Zhang and Nan Jiang.
\newblock Towards hyperparameter-free policy selection for offline
  reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\end{thebibliography}
