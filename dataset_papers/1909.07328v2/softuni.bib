% Encoding: UTF-8

@Book{russell2016artificial,
  author    = {Russell, Stuart and Norvig, Peter},
  publisher = {Addison Wesley},
  title     = {Artificial Intelligence: A Modern Approach, Global Edition},
  year      = {2018},
  isbn      = {1292153962},
  date      = {2018-11-28},
  ean       = {9781292153964},
  url       = {https://www.ebook.de/de/product/25939961/stuart_russell_peter_norvig_artificial_intelligence_a_modern_approach_global_edition.html},
}

@inproceedings{sochersentiment,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@article{invariantriskmin,
  title={Invariant risk minimization},
  author={Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and Lopez-Paz, David},
  journal={arXiv preprint arXiv:1907.02893},
  year={2019}
}

@article{invariantcausalpred,
  title={Invariant causal prediction for nonlinear models},
  author={Heinze-Deml, Christina and Peters, Jonas and Meinshausen, Nicolai},
  journal={Journal of Causal Inference},
  volume={6},
  number={2},
  year={2018},
  publisher={De Gruyter}
}

@InProceedings{numberbatch,
  author       = {Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  title        = {{ConceptNet} 5.5: An Open Multilingual Graph of General Knowledge},
  year         = {2017},
  pages        = {4444--4451},
  abstract     = {Machine learning about language can be improved by supplying it with specific knowledge and sources of external information. We present here a new version of the linked open data resource ConceptNet that is particularly well suited to be used with modern NLP techniques such as word embeddings. ConceptNet is a knowledge graph that connects words and phrases of natural language with labeled edges. Its knowledge is collected from many sources that include expert-created resources, crowd-sourcing, and games with a purpose. It is designed to represent the general knowledge involved in understanding language, improving natural language applications by allowing the application to better understand the meanings behind the words people use. When ConceptNet is combined with word embeddings acquired from distributional semantics (such as word2vec), it provides applications with understanding that they would not acquire from distributional semantics alone, nor from narrower resources such as WordNet or DBPedia. We demonstrate this with state-of-the-art results on intrinsic evaluations of word relatedness that translate into improvements on applications of word vectors, including solving SAT-style analogies.},
  eprint       = {1612.03975},
  eprintclass  = {cs.CL},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1612.03975v2:PDF},
  journaltitle = {AAAI 31 (2017) 4444-4451},
  keywords     = {cs.CL, I.2.7},
}

@Article{leastcommongeneraliser,
  author    = {Reynolds, John C.},
  title     = {Transformational systems and algebraic structure of atomic formulas},
  journal   = {Machine intelligence},
  year      = {1970},
  volume    = {5},
  pages     = {135--151},
  publisher = {Edinburgh University Press},
}

@Book{lcgfromfacts,
  title     = {Inductive inference of theories from facts},
  publisher = {Yale University, Department of Computer Science},
  year      = {1981},
  author    = {Shapiro, Ehud Y.},
}

@Article{harnessinglogicregularisation,
  author    = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
  title     = {Harnessing Deep Neural Networks with Logic Rules},
  journal   = {ACL},
  year      = {2016},
  booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  doi       = {10.18653/v1/p16-1228},
  eprint    = {http://arxiv.org/abs/1603.06318v5},
  publisher = {Association for Computational Linguistics},
}

@Article{brainaiqa,
  author    = {Idiart, Marco A. P. and Villavicencio, Aline and Katz, Boris and Renn{\'{o}}-Costa, C{\'{e}}sar and Lisman, John},
  title     = {How the Brain Represents Language and Answers Questions? Using an {AI} System to Understand the Underlying Neurobiological Mechanisms},
  journal   = {Frontiers in Computational Neuroscience},
  year      = {2019},
  volume    = {13},
  month     = mar,
  doi       = {10.3389/fncom.2019.00012},
  publisher = {Frontiers Media {SA}},
}

@InProceedings{gn2n,
  author      = {Liu, Fei and Perez, Julien},
  booktitle   = {Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers},
  title       = {Gated End-to-End Memory Networks},
  year        = {2017},
  pages       = {1--10},
  publisher   = {Association for Computational Linguistics},
  abstract    = {Machine reading using differentiable reasoning models has recently shown remarkable progress. In this context, End-to-End trainable Memory Networks, MemN2N, have demonstrated promising performance on simple natural language based reasoning tasks such as factual reasoning and basic deduction. However, other tasks, namely multi-fact question-answering, positional reasoning or dialog related tasks, remain challenging particularly due to the necessity of more complex interactions between the memory and controller modules composing this family of models. In this paper, we introduce a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Concretely, we develop a Gated End-to-End trainable Memory Network architecture, GMemN2N. From the machine learning perspective, this new capability is learned in an end-to-end fashion without the use of any additional supervision signal which is, as far as our knowledge goes, the first of its kind. Our experiments show significant improvements on the most challenging tasks in the 20 bAbI dataset, without the use of any domain knowledge. Then, we show improvements on the dialog bAbI tasks including the real human-bot conversion-based Dialog State Tracking Challenge (DSTC-2) dataset. On these two datasets, our model sets the new state of the art.},
  doi         = {10.18653/v1/e17-1001},
  eprint      = {1610.04211},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1610.04211v2:PDF},
  keywords    = {cs.CL, stat.ML},
}

@Article{entnet,
  author       = {Henaff, Mikael and Weston, Jason and Szlam, Arthur and Bordes, Antoine and LeCun, Yann},
  title        = {Tracking the World State with Recurrent Entity Networks},
  year         = {2017},
  abstract     = {We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass.},
  eprint       = {1612.03969},
  eprintclass  = {cs.CL},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1612.03969v3:PDF},
  journaltitle = {ICLR 2017},
  keywords     = {cs.CL},
}

@Article{qrn,
  author      = {Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh},
  journal     = {ICLR},
  title       = {Query-Reduction Networks for Question Answering},
  year        = {2017},
  abstract    = {In this paper, we study the problem of question answering when reasoning over multiple facts is required. We propose Query-Reduction Network (QRN), a variant of Recurrent Neural Network (RNN) that effectively handles both short-term (local) and long-term (global) sequential dependencies to reason over multiple facts. QRN considers the context sentences as a sequence of state-changing triggers, and reduces the original query to a more informed query as it observes each trigger (context sentence) through time. Our experiments show that QRN produces the state-of-the-art results in bAbI QA and dialog tasks, and in a real goal-oriented dialog dataset. In addition, QRN formulation allows parallelization on RNN's time axis, saving an order of magnitude in time complexity for training and inference.},
  eprint      = {1606.04582},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1606.04582v6:PDF},
  keywords    = {cs.CL, cs.NE},
}

@Article{dnc,
  author    = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwi{\'{n}}ska, Agnieszka and Colmenarejo, Sergio G{\'{o}}mez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
  title     = {Hybrid computing using a neural network with dynamic external memory},
  journal   = {Nature},
  year      = {2016},
  volume    = {538},
  number    = {7626},
  pages     = {471--476},
  month     = oct,
  doi       = {10.1038/nature20101},
  publisher = {Springer Nature},
}

@Article{bacwardchaining,
  author    = {Apt, Krzysztof R. and van Emden, M. H.},
  title     = {Contributions to the Theory of Logic Programming},
  journal   = {Journal of the {ACM}},
  year      = {1982},
  volume    = {29},
  number    = {3},
  pages     = {841--862},
  month     = jul,
  doi       = {10.1145/322326.322339},
  publisher = {Association for Computing Machinery ({ACM})},
}

@InCollection{nbf,
  author    = {Clark, Keith L.},
  title     = {Negation as Failure},
  booktitle = {Logic and Data Bases},
  publisher = {Springer {US}},
  year      = {1978},
  pages     = {293--322},
  doi       = {10.1007/978-1-4684-3384-5_11},
}

@Article{ilp,
  author    = {Muggleton, Stephen and de Raedt, Luc},
  title     = {Inductive Logic Programming: Theory and methods},
  journal   = {The Journal of Logic Programming},
  year      = {1994},
  volume    = {19-20},
  pages     = {629--679},
  month     = may,
  doi       = {10.1016/0743-1066(94)90035-3},
  publisher = {Elsevier {BV}},
}

@Book{prolog,
  title     = {Programming in Prolog},
  publisher = {Springer Berlin Heidelberg},
  year      = {2003},
  author    = {Clocksin, William F. and Mellish, Christopher S.},
  doi       = {10.1007/978-3-642-55481-0},
}

@InProceedings{treelstm,
  author        = {Tai, Kai Sheng and Socher, Richard and Manning, Christopher D.},
  title         = {Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks},
  booktitle     = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year          = {2015},
  month         = feb,
  publisher     = {Association for Computational Linguistics},
  abstract      = {Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank).},
  archiveprefix = {arXiv},
  doi           = {10.3115/v1/p15-1150},
  eprint        = {http://arxiv.org/abs/1503.00075v3},
  file          = {1503.00075v3\:PDF:\:http/\://arxiv.org/pdf/1503.00075v3\:PDF:PDF},
  journal       = {arXiv preprint arXiv:1503.00075},
  keywords      = {cs.CL, cs.AI, cs.LG},
  primaryclass  = {cs.CL},
}

@Article{graphnetworks,
  author      = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and others},
  title       = {Relational inductive biases, deep learning, and graph networks},
  journal     = {arXiv preprint arXiv:1806.01261},
  year        = {2018},
  abstract    = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  date        = {2018-06-04},
  eprint      = {http://arxiv.org/abs/1806.01261v3},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1806.01261v3:PDF},
  keywords    = {cs.LG, cs.AI, stat.ML},
}

@inproceedings{stablemodel,
  title={The stable model semantics for logic programming.},
  author={Gelfond, Michael and Lifschitz, Vladimir},
  booktitle={ICLP/SLP},
  volume={88},
  pages={1070--1080},
  year={1988}
}

@InProceedings{gru,
  author    = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  title     = {On the Properties of Neural Machine Translation: Encoder{\textendash}Decoder Approaches},
  booktitle = {Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
  year      = {2014},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/w14-4012},
}

@Article{adam,
  author       = {Kingma, Diederik P. and Ba, Jimmy},
  title        = {Adam: A Method for Stochastic Optimization},
  year         = {2015},
  abstract     = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  eprint       = {1412.6980},
  eprintclass  = {cs.LG},
  eprinttype   = {arXiv},
  file         = {:http\://arxiv.org/pdf/1412.6980v9:PDF},
  journaltitle = {ICLR},
  keywords     = {cs.LG},
}

@Book{neurosymbolic,
  title     = {Neural-Symbolic Learning Systems},
  publisher = {Springer London},
  year      = {2002},
  author    = {Broda, Krysia B. and Garcez, Artur S. D'Avila and Gabbay, Dov M.},
  isbn      = {1852335122},
  date      = {2002-08-06},
  ean       = {9781852335120},
  pagetotal = {288},
}

@Article{grueval,
  author      = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  title       = {Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling},
  journal     = {NIPS Deep Learning and Representation Learning Workshop},
  year        = {2014},
  abstract    = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  date        = {2014-12-11},
  eprint      = {http://arxiv.org/abs/1412.3555v1},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1412.3555v1:PDF},
  keywords    = {cs.NE, cs.LG},
}

@InProceedings{nlentailment,
  author      = {Bowman, Samuel R. and Angeli, Gabor and Potts, Christopher and Manning, Christopher D.},
  booktitle   = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  title       = {A large annotated corpus for learning natural language inference},
  year        = {2015},
  publisher   = {Association for Computational Linguistics},
  abstract    = {Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.},
  date        = {2015-08-21},
  doi         = {10.18653/v1/d15-1075},
  eprint      = {1508.05326},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1508.05326v1:PDF},
  keywords    = {cs.CL},
}

@InProceedings{asp,
  author    = {Lifschitz, Vladimir},
  booktitle = {AAAI},
  title     = {What Is Answer Set Programming?.},
  number    = {2008},
  pages     = {1594--1597},
  volume    = {8},
  date      = {2008},
}

@InCollection{NIPS2013_5028,
  author    = {Socher, Richard and Chen, Danqi and Manning, Christopher D. and Ng, Andrew},
  booktitle = {Advances in Neural Information Processing Systems 26},
  publisher = {Curran Associates, Inc.},
  title     = {Reasoning With Neural Tensor Networks for Knowledge Base Completion},
  editor    = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
  pages     = {926--934},
  date      = {2013},
  url       = {http://papers.nips.cc/paper/5028-reasoning-with-neural-tensor-networks-for-knowledge-base-completion.pdf},
}

@Article{tensorlog,
  author      = {Cohen, William W.},
  title       = {TensorLog: A Differentiable Deductive Database},
  journal     = {arXiv:1605.06523},
  year        = {2016},
  abstract    = {Large knowledge bases (KBs) are useful in many tasks, but it is unclear how to integrate this sort of knowledge into "deep" gradient-based learning systems. To address this problem, we describe a probabilistic deductive database, called TensorLog, in which reasoning uses a differentiable process. In TensorLog, each clause in a logical theory is first converted into certain type of factor graph. Then, for each type of query to the factor graph, the message-passing steps required to perform belief propagation (BP) are "unrolled" into a function, which is differentiable. We show that these functions can be composed recursively to perform inference in non-trivial logical theories containing multiple interrelated clauses and predicates. Both compilation and inference in TensorLog are efficient: compilation is linear in theory size and proof depth, and inference is linear in database size and the number of message-passing steps used in BP. We also present experimental results with TensorLog and discuss its relationship to other first-order probabilistic logics.},
  date        = {2016-05-20},
  eprint      = {http://arxiv.org/abs/1605.06523v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1605.06523v2:PDF},
  keywords    = {cs.AI, cs.DB, cs.LG},
}

@Article{logictensor,
  author      = {Serafini, Luciano and d'Avila Garcez, Artur},
  title       = {Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge},
  journal     = {arXiv:1606.04422},
  year        = {2016},
  abstract    = {We propose Logic Tensor Networks: a uniform framework for integrating automatic learning and reasoning. A logic formalism called Real Logic is defined on a first-order language whereby formulas have truth-value in the interval [0,1] and semantics defined concretely on the domain of real numbers. Logical constants are interpreted as feature vectors of real numbers. Real Logic promotes a well-founded integration of deductive reasoning on a knowledge-base and efficient data-driven relational machine learning. We show how Real Logic can be implemented in deep Tensor Neural Networks with the use of Google's tensorflow primitives. The paper concludes with experiments applying Logic Tensor Networks on a simple but representative example of knowledge completion.},
  date        = {2016-06-14},
  eprint      = {http://arxiv.org/abs/1606.04422v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1606.04422v2:PDF},
  keywords    = {cs.AI, cs.LG, cs.LO, cs.NE},
}

@Article{neuralunification,
  author    = {Komendantskaya, Ekaterina},
  title     = {Unification neural networks: unification by error-correction learning},
  journal   = {Logic Journal of the IGPL},
  year      = {2011},
  volume    = {19},
  number    = {6},
  pages     = {821--847},
  publisher = {OUP},
}

@Article{interpretability,
  author      = {Lipton, Zachary C.},
  journal     = {Communications of the {ACM}},
  title       = {The Mythos of Model Interpretability},
  year        = {2018},
  number      = {10},
  pages       = {36--43},
  volume      = {61},
  abstract    = {Supervised machine learning models boast remarkable predictive capabilities. But can you trust your model? Will it work in deployment? What else can it tell you about the world? We want models to be not only good, but interpretable. And yet the task of interpretation appears underspecified. Papers provide diverse and sometimes non-overlapping motivations for interpretability, and offer myriad notions of what attributes render models interpretable. Despite this ambiguity, many papers proclaim interpretability axiomatically, absent further explanation. In this paper, we seek to refine the discourse on interpretability. First, we examine the motivations underlying interest in interpretability, finding them to be diverse and occasionally discordant. Then, we address model properties and techniques thought to confer interpretability, identifying transparency to humans and post-hoc explanations as competing notions. Throughout, we discuss the feasibility and desirability of different notions, and question the oft-made assertions that linear models are interpretable and that deep neural networks are not.},
  doi         = {10.1145/3233231},
  eprint      = {1606.03490},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1606.03490v3:PDF},
  keywords    = {cs.LG, cs.AI, cs.CV, cs.NE, stat.ML},
  publisher   = {Association for Computing Machinery ({ACM})},
}

@Article{xaisurvey,
  author    = {Adadi, Amina and Berrada, Mohammed},
  title     = {Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence ({XAI})},
  journal   = {{IEEE} Access},
  year      = {2018},
  volume    = {6},
  pages     = {52138--52160},
  doi       = {10.1109/access.2018.2870052},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
}

@InProceedings{gradcam,
  author    = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  title     = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
  booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
  year      = {2017},
  pages     = {618--626},
  month     = oct,
  publisher = {{IEEE}},
  doi       = {10.1109/iccv.2017.74},
}

@Article{layerwiserelevance,
  author    = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'{e}}goire and Klauschen, Frederick and Müller, Klaus-Robert and Samek, Wojciech},
  title     = {On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation},
  journal   = {{PLOS} {ONE}},
  year      = {2015},
  volume    = {10},
  number    = {7},
  pages     = {e0130140},
  month     = jul,
  doi       = {10.1371/journal.pone.0130140},
  editor    = {Oscar Deniz Suarez},
  publisher = {Public Library of Science ({PLoS})},
}

@Article{counterfactualthinking,
  author    = {Roese, Neal J.},
  title     = {Counterfactual thinking.},
  journal   = {Psychological Bulletin},
  year      = {1997},
  volume    = {121},
  number    = {1},
  pages     = {133--148},
  doi       = {10.1037/0033-2909.121.1.133},
  publisher = {American Psychological Association ({APA})},
}

@Article{pearlcausalinference,
  author    = {Pearl, Judea},
  title     = {The Seven Tools of Causal Inference with Reflections on Machine Learning},
  journal   = {Communications of the {ACM}},
  year      = {2019},
  volume    = {62},
  number    = {3},
  pages     = {54--60},
  month     = feb,
  doi       = {10.1145/3241036},
  publisher = {Association for Computing Machinery ({ACM})},
  school    = {Technical Report, Communications of Association for Computing Machinery},
}

@article{pearlcausationcounterfactual,
  title={Probabilities of causation: three counterfactual interpretations and their identification},
  author={Pearl, Judea},
  journal={Synthese},
  volume={121},
  number={1-2},
  pages={93--149},
  year={1999},
  publisher={Springer}
}

@inproceedings{chainer,
  author       = "Tokui, Seiya and Oono, Kenta and Hido, Shohei and Clayton, Justin",
  title        = "Chainer: a Next-Generation Open Source Framework for Deep Learning",
  booktitle    = "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS)",
  year         = "2015",
  url          = "http://learningsys.org/papers/LearningSys_2015_paper_33.pdf"
}

@Article{statisticscausalinference,
  author    = {Holland, Paul W.},
  title     = {Statistics and Causal Inference},
  journal   = {Journal of the American Statistical Association},
  year      = {1986},
  volume    = {81},
  number    = {396},
  pages     = {945--960},
  month     = dec,
  doi       = {10.1080/01621459.1986.10478354},
  publisher = {Informa {UK} Limited},
}

@Book{counterfactualscausalinference,
  title     = {Counterfactuals and Causal Inference: Methods and Principles for Social Research (Analytical Methods for Social Research)},
  publisher = {Cambridge University Press},
  year      = {2007},
  author    = {Morgan, Stephen L. and Winship, Christopher},
  isbn      = {0521671930},
}

@InProceedings{gradcamplus,
  author       = {Chattopadhay, Aditya and Sarkar, Anirban and Howlader, Prantik and Balasubramanian, Vineeth N.},
  title        = {Grad-{CAM}++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks},
  booktitle    = {2018 {IEEE} Winter Conference on Applications of Computer Vision ({WACV})},
  year         = {2018},
  pages        = {839--847},
  month        = mar,
  organization = {IEEE},
  publisher    = {{IEEE}},
  doi          = {10.1109/wacv.2018.00097},
}

@Article{xaisocialsci,
  author      = {Miller, Tim},
  journal     = {Artificial Intelligence},
  title       = {Explanation in Artificial Intelligence: Insights from the Social Sciences},
  year        = {2019},
  pages       = {1--38},
  volume      = {267},
  abstract    = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to make their algorithms more understandable. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exists vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations towards the explanation process. This paper argues that the field of explainable artificial intelligence should build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  doi         = {10.1016/j.artint.2018.07.007},
  eprint      = {1706.07269},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1706.07269v3:PDF},
  keywords    = {cs.AI},
  publisher   = {Elsevier {BV}},
}

@InProceedings{neuralprolog,
  author       = {Ding, Liya},
  title        = {Neural prolog-the concepts, construction and mechanism},
  booktitle    = {Systems, Man and Cybernetics, 1995. Intelligent Systems for the 21\textsuperscript{st} Century., IEEE International Conference on},
  year         = {1995},
  volume       = {4},
  pages        = {3603--3608},
  organization = {IEEE},
}

@Article{holographicembed,
  author      = {Nickel, Maximilian and Rosasco, Lorenzo and Poggio, Tomaso},
  title       = {Holographic Embeddings of Knowledge Graphs},
  journal     = {AAAI},
  year        = {2016},
  pages       = {1955--1961},
  abstract    = {Learning embeddings of entities and relations is an efficient and versatile method to perform machine learning on relational data such as knowledge graphs. In this work, we propose holographic embeddings (HolE) to learn compositional vector space representations of entire knowledge graphs. The proposed method is related to holographic models of associative memory in that it employs circular correlation to create compositional representations. By using correlation as the compositional operator HolE can capture rich interactions but simultaneously remains efficient to compute, easy to train, and scalable to very large datasets. In extensive experiments we show that holographic embeddings are able to outperform state-of-the-art methods for link prediction in knowledge graphs and relational learning benchmark datasets.},
  booktitle   = {AAAI},
  date        = {2015-10-16},
  eprint      = {http://arxiv.org/abs/1510.04935v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1510.04935v2:PDF},
  keywords    = {cs.AI, cs.LG, stat.ML, I.2.6; I.2.4},
}

@Article{liftedneuralnetworks,
  author      = {Sourek, Gustav and Aschenbrenner, Vojtech and Zelezny, Filip and Kuzelka, Ondrej},
  title       = {Lifted Relational Neural Networks},
  year        = {2015},
  abstract    = {We propose a method combining relational-logic representations with neural network learning. A general lifted architecture, possibly reflecting some background domain knowledge, is described through relational rules which may be handcrafted or learned. The relational rule-set serves as a template for unfolding possibly deep neural networks whose structures also reflect the structures of given training or testing relational examples. Different networks corresponding to different examples share their weights, which co-evolve during training by stochastic gradient descent algorithm. The framework allows for hierarchical relational modeling constructs and learning of latent relational concepts through shared hidden layers weights corresponding to the rules. Discovery of notable relational concepts and experiments on 78 relational learning benchmarks demonstrate favorable performance of the method.},
  date        = {2015-08-20},
  eprint      = {1508.05128},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1508.05128v2:PDF},
  keywords    = {cs.AI, cs.LG, cs.NE},
}

@Article{neuralproginterpreters,
  author      = {Reed, Scott and de Freitas, Nando},
  title       = {Neural Programmer-Interpreters},
  journal     = {arXiv:1511.06279},
  year        = {2015},
  abstract    = {We propose the neural programmer-interpreter (NPI): a recurrent and compositional neural network that learns to represent and execute programs. NPI has three learnable components: a task-agnostic recurrent core, a persistent key-value program memory, and domain-specific encoders that enable a single NPI to operate in multiple perceptually diverse environments with distinct affordances. By learning to compose lower-level programs to express higher-level programs, NPI reduces sample complexity and increases generalization ability compared to sequence-to-sequence LSTMs. The program memory allows efficient learning of additional tasks by building on existing programs. NPI can also harness the environment (e.g. a scratch pad with read-write pointers) to cache intermediate results of computation, lessening the long-term memory burden on recurrent hidden units. In this work we train the NPI with fully-supervised execution traces; each program has example sequences of calls to the immediate subprograms conditioned on the input. Rather than training on a huge number of relatively weak labels, NPI learns from a small number of rich examples. We demonstrate the capability of our model to learn several types of compositional programs: addition, sorting, and canonicalizing 3D models. Furthermore, a single NPI learns to execute these programs and all 21 associated subprograms.},
  date        = {2015-11-19},
  eprint      = {http://arxiv.org/abs/1511.06279v4},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1511.06279v4:PDF},
  keywords    = {cs.LG, cs.NE},
}

@Article{mac,
  author      = {Hudson, Drew A. and Manning, Christopher D.},
  title       = {Compositional Attention Networks for Machine Reasoning},
  journal     = {ICLR},
  year        = {2018},
  abstract    = {We present the MAC network, a novel fully differentiable neural network architecture, designed to facilitate explicit and expressive reasoning. MAC moves away from monolithic black-box neural architectures towards a design that encourages both transparency and versatility. The model approaches problems by decomposing them into a series of attention-based reasoning steps, each performed by a novel recurrent Memory, Attention, and Composition (MAC) cell that maintains a separation between control and memory. By stringing the cells together and imposing structural constraints that regulate their interaction, MAC effectively learns to perform iterative reasoning processes that are directly inferred from the data in an end-to-end approach. We demonstrate the model's strength, robustness and interpretability on the challenging CLEVR dataset for visual reasoning, achieving a new state-of-the-art 98.9% accuracy, halving the error rate of the previous best model. More importantly, we show that the model is computationally-efficient and data-efficient, in particular requiring 5x less data than existing models to achieve strong results.},
  date        = {2018-03-08},
  eprint      = {http://arxiv.org/abs/1803.03067v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1803.03067v2:PDF},
  keywords    = {cs.AI},
}

@InProceedings{curriculum,
  author       = {Bengio, Yoshua and Louradour, J{\'{e}}r{\^{o}}me and Collobert, Ronan and Weston, Jason},
  title        = {Curriculum learning},
  booktitle    = {Proceedings of the 26th Annual International Conference on Machine Learning - {ICML} {\textquotesingle}09},
  year         = {2009},
  pages        = {41--48},
  organization = {ACM},
  publisher    = {{ACM} Press},
  doi          = {10.1145/1553374.1553380},
}

@InProceedings{timlowdim,
  author    = {Rocktäschel, Tim and Bo{\v{s}}njak, Matko and Singh, Sameer and Riedel, Sebastian},
  title     = {Low-Dimensional Embeddings of Logic},
  booktitle = {Proceedings of the {ACL} 2014 Workshop on Semantic Parsing},
  year      = {2014},
  pages     = {45--49},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/w14-2409},
}

@Article{neuralsymbolicmachines,
  author      = {Liang, Chen and Berant, Jonathan and Le, Quoc and Forbus, Kenneth D. and Lao, Ni},
  title       = {Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision},
  journal     = {ACL},
  year        = {2017},
  abstract    = {Harnessing the statistical power of neural networks to perform language understanding and symbolic reasoning is difficult, when it requires executing efficient discrete operations against a large knowledge-base. In this work, we introduce a Neural Symbolic Machine, which contains (a) a neural "programmer", i.e., a sequence-to-sequence model that maps language utterances to programs and utilizes a key-variable memory to handle compositionality (b) a symbolic "computer", i.e., a Lisp interpreter that performs program execution, and helps find good programs by pruning the search space. We apply REINFORCE to directly optimize the task reward of this structured prediction problem. To train with weak supervision and improve the stability of REINFORCE, we augment it with an iterative maximum-likelihood training process. NSM outperforms the state-of-the-art on the WebQuestionsSP dataset when trained from question-answer pairs only, without requiring any feature engineering or domain-specific knowledge.},
  date        = {2016-10-31},
  eprint      = {http://arxiv.org/abs/1611.00020v4},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1611.00020v4:PDF},
  keywords    = {cs.CL, cs.AI, cs.LG},
}

@InProceedings{glove,
  author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  title     = {Glove: Global Vectors for Word Representation},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
  year      = {2014},
  pages     = {1532--1543},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/d14-1162},
}

@InProceedings{word2vec,
  author      = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  title       = {Distributed Representations of Words and Phrases and their Compositionality},
  booktitle   = {NIPS},
  year        = {2013},
  pages       = {3111--3119},
  abstract    = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
  date        = {2013-10-16},
  eprint      = {http://arxiv.org/abs/1310.4546v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1310.4546v1:PDF},
  keywords    = {cs.CL, cs.LG, stat.ML},
}

@Article{wordembedssurvey,
  author    = {Camacho-Collados, Jose and Pilehvar, Mohammad Taher},
  title     = {From Word To Sense Embeddings: A Survey on Vector Representations of Meaning},
  journal   = {Journal of Artificial Intelligence Research},
  year      = {2018},
  volume    = {63},
  pages     = {743--788},
  month     = dec,
  doi       = {10.1613/jair.1.11259},
  eprint    = {http://arxiv.org/abs/1805.04032v2},
  publisher = {{AI} Access Foundation},
}

@InProceedings{relationnetworks,
  author      = {Santoro, Adam and Raposo, David and Barrett, David G. and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Tim},
  booktitle   = {NIPS},
  title       = {A simple neural network module for relational reasoning},
  year        = {2017},
  pages       = {4974--4983},
  abstract    = {Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Our work shows how a deep learning architecture equipped with an RN module can implicitly discover and learn to reason about entities and their relations.},
  date        = {2017-06-05},
  eprint      = {1706.01427},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1706.01427v1:PDF},
  keywords    = {cs.CL, cs.LG},
}

@InProceedings{singh2015towards,
  author    = {Singh, Sameer and Rocktäschel, Tim and Riedel, Sebastian},
  title     = {Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction},
  booktitle = {Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing},
  year      = {2015},
  pages     = {135--142},
  publisher = {Association for Computational Linguistics},
  doi       = {10.3115/v1/w15-1519},
}

@Article{neuralstack,
  author      = {Joulin, Armand and Mikolov, Tomas},
  title       = {Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets},
  journal     = {NIPS},
  year        = {2015},
  pages       = {190--198},
  abstract    = {Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.},
  date        = {2015-03-03},
  eprint      = {http://arxiv.org/abs/1503.01007v4},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1503.01007v4:PDF},
  keywords    = {cs.NE, cs.LG},
}

@Article{neuralturing,
  author      = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  title       = {Neural Turing Machines},
  journal     = {arXiv:1410.5401},
  year        = {2014},
  abstract    = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  date        = {2014-10-20},
  eprint      = {http://arxiv.org/abs/1410.5401v2},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1410.5401v2:PDF},
  keywords    = {cs.NE},
}

@InProceedings{learnsimplealgo,
  author      = {Zaremba, Wojciech and Mikolov, Tomas and Joulin, Armand and Fergus, Rob},
  title       = {Learning Simple Algorithms from Examples},
  booktitle   = {ICML},
  year        = {2016},
  pages       = {421--429},
  abstract    = {We present an approach for learning simple algorithms such as copying, multi-digit addition and single digit multiplication directly from examples. Our framework consists of a set of interfaces, accessed by a controller. Typical interfaces are 1-D tapes or 2-D grids that hold the input and output data. For the controller, we explore a range of neural network-based models which vary in their ability to abstract the underlying algorithm from training instances and generalize to test examples with many thousands of digits. The controller is trained using $Q$-learning with several enhancements and we show that the bottleneck is in the capabilities of the controller rather than in the search incurred by $Q$-learning.},
  date        = {2015-11-23},
  eprint      = {http://arxiv.org/abs/1511.07275v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1511.07275v2:PDF},
  keywords    = {cs.AI, cs.LG},
}

@Article{evans2018learning,
  author      = {Evans, Richard and Grefenstette, Edward},
  journal     = {Journal of Artificial Intelligence Research},
  title       = {Learning Explanatory Rules from Noisy Data},
  year        = {2018},
  pages       = {1--64},
  volume      = {61},
  abstract    = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
  doi         = {10.1613/jair.5714},
  eprint      = {1711.04574},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1711.04574v2:PDF},
  keywords    = {cs.NE, math.LO},
  publisher   = {{AI} Access Foundation},
}

@Article{babi,
  author      = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
  journal     = {ICLR},
  title       = {Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks},
  year        = {2016},
  abstract    = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
  eprint      = {1502.05698},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1502.05698v10:PDF},
  keywords    = {cs.AI, cs.CL, stat.ML},
}

@InProceedings{dmn,
  author      = {Kumar, Ankit and Irsoy, Ozan and Ondruska, Peter and Iyyer, Mohit and Bradbury, James and Gulrajani, Ishaan and Zhong, Victor and Paulus, Romain and Socher, Richard},
  booktitle   = {ICML},
  title       = {Ask Me Anything: Dynamic Memory Networks for Natural Language Processing},
  year        = {2016},
  pages       = {1378--1387},
  abstract    = {Most tasks in natural language processing can be cast into question answering (QA) problems over language input. We introduce the dynamic memory network (DMN), a neural network architecture which processes input sequences and questions, forms episodic memories, and generates relevant answers. Questions trigger an iterative attention process which allows the model to condition its attention on the inputs and the result of previous iterations. These results are then reasoned over in a hierarchical recurrent sequence model to generate answers. The DMN can be trained end-to-end and obtains state-of-the-art results on several types of tasks and datasets: question answering (Facebook's bAbI dataset), text classification for sentiment analysis (Stanford Sentiment Treebank) and sequence modeling for part-of-speech tagging (WSJ-PTB). The training for these different tasks relies exclusively on trained word vector representations and input-question-answer triplets.},
  eprint      = {1506.07285},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1506.07285v5:PDF},
  keywords    = {cs.CL, cs.LG, cs.NE},
}

@InProceedings{memn2n,
  author      = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
  title       = {End-To-End Memory Networks},
  booktitle   = {NIPS},
  year        = {2015},
  pages       = {2440--2448},
  abstract    = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
  date        = {2015-03-31},
  eprint      = {1503.08895},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1503.08895v5:PDF},
  keywords    = {cs.NE, cs.CL},
}

@Article{timntp,
  author      = {Rocktäschel, Tim and Riedel, Sebastian},
  journal     = {NIPS},
  title       = {End-to-End Differentiable Proving},
  year        = {2017},
  pages       = {3791--3803},
  abstract    = {We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules.},
  booktitle   = {NIPS},
  date        = {2017-05-31},
  eprint      = {1705.11040},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1705.11040v2:PDF},
  keywords    = {cs.NE, cs.AI, cs.LG, cs.LO},
}

@Article{logicprogramming,
  author    = {Apt, Krzysztof R. and Bol, Roland N.},
  title     = {Logic programming and negation: A survey},
  journal   = {The Journal of Logic Programming},
  year      = {1994},
  volume    = {19-20},
  pages     = {9--71},
  month     = may,
  doi       = {10.1016/0743-1066(94)90024-8},
  publisher = {Elsevier {BV}},
}

@Article{memnn,
  author      = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
  journal     = {ICLR},
  title       = {Memory Networks},
  year        = {2015},
  abstract    = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
  eprint      = {1410.3916},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1410.3916v11:PDF},
  keywords    = {cs.AI, cs.CL, stat.ML},
}

@Article{dmnvisual,
  author      = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  journal     = {ICML},
  title       = {Dynamic Memory Networks for Visual and Textual Question Answering},
  year        = {2016},
  pages       = {2397--2406},
  abstract    = {Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown whether the architecture achieves strong results for question answering when supporting facts are not marked during training or whether it could be applied to other modalities such as images. Based on an analysis of the DMN, we propose several improvements to its memory and input modules. Together with these changes we introduce a novel input module for images in order to be able to answer visual questions. Our new DMN+ model improves the state of the art on both the Visual Question Answering dataset and the \babi-10k text question-answering dataset without supporting fact supervision.},
  booktitle   = {ICML},
  date        = {2016-03-04},
  eprint      = {1603.01417},
  eprintclass = {cs.NE},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1603.01417v1:PDF},
  keywords    = {cs.NE, cs.CL, cs.CV},
}

@Article{lstm,
  author    = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  title     = {Long Short-Term Memory},
  journal   = {Neural Computation},
  year      = {1997},
  volume    = {9},
  number    = {8},
  pages     = {1735--1780},
  month     = nov,
  doi       = {10.1162/neco.1997.9.8.1735},
  publisher = {{MIT} Press - Journals},
}

@Article{possibleworldnet,
  author        = {Evans, Richard and Saxton, David and Amos, David and Kohli, Pushmeet and Grefenstette, Edward},
  title         = {Can Neural Networks Understand Logical Entailment?},
  journal       = {ICLR},
  year          = {2018},
  month         = feb,
  abstract      = {We introduce a new dataset of logical entailments for the purpose of measuring models' ability to capture and exploit the structure of logical expressions against an entailment prediction task. We use this task to compare a series of architectures which are ubiquitous in the sequence-processing literature, in addition to a new model class---PossibleWorldNets---which computes entailment as a "convolution over possible worlds". Results show that convolutional networks present the wrong inductive bias for this class of problems relative to LSTM RNNs, tree-structured neural networks outperform LSTM RNNs due to their enhanced ability to exploit the syntax of logic, and PossibleWorldNets outperform all benchmarks.},
  archiveprefix = {arXiv},
  date          = {2018-02-23},
  eprint        = {http://arxiv.org/abs/1802.08535v1},
  eprintclass   = {cs.NE},
  eprinttype    = {arXiv},
  file          = {:http\://arxiv.org/pdf/1802.08535v1:PDF;1802.08535v1\:PDF:\:http/\://arxiv.org/pdf/1802.08535v1\:PDF:PDF},
  keywords      = {cs.NE, cs.AI},
  primaryclass  = {cs.NE},
}

@Book{deeplearningbook,
  title     = {Deep Learning},
  publisher = {The MIT Press},
  year      = {2017},
  author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  month     = jan,
  isbn      = {0262035618},
  date      = {2017-01-03},
  doi       = {10.1016/b978-0-12-804291-5.00010-6},
  ean       = {9780262035613},
  pagetotal = {800},
}

@Article{neurosymsurvey,
  author      = {Besold, Tarek R. and d'Avila Garcez, Artur and Bader, Sebastian and Bowman, Howard and Domingos, Pedro and Hitzler, Pascal and Kuehnberger, Kai-Uwe and Lamb, Luis C. and Lowd, Daniel and Lima, Priscila Machado Vieira and de Penning, Leo and Pinkas, Gadi and Poon, Hoifung and Zaverucha, Gerson},
  title       = {Neural-Symbolic Learning and Reasoning: A Survey and Interpretation},
  abstract    = {The study and understanding of human behaviour is relevant to computer science, artificial intelligence, neural computation, cognitive science, philosophy, psychology, and several other areas. Presupposing cognition as basis of behaviour, among the most prominent tools in the modelling of behaviour are computational-logic systems, connectionist models of cognition, and models of uncertainty. Recent studies in cognitive science, artificial intelligence, and psychology have produced a number of cognitive models of reasoning, learning, and language that are underpinned by computation. In addition, efforts in computer science research have led to the development of cognitive computational systems integrating machine learning and automated reasoning. Such systems have shown promise in a range of applications, including computational biology, fault diagnosis, training and assessment in simulators, and software verification. This joint survey reviews the personal ideas and views of several researchers on neural-symbolic learning and reasoning. The article is organised in three parts: Firstly, we frame the scope and goals of neural-symbolic computation and have a look at the theoretical foundations. We then proceed to describe the realisations of neural-symbolic computation, systems, and applications. Finally we present the challenges facing the area and avenues for further research.},
  date        = {2017-11-10},
  eprint      = {1711.03902},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1711.03902v1:PDF},
  keywords    = {cs.AI},
}

@Article{deeplogic,
  author        = {Cingillioglu, Nuri and Russo, Alessandra},
  journal       = {AAAI-MAKE},
  title         = {DeepLogic: Towards End-to-End Differentiable Logical Reasoning},
  year          = {2019},
  abstract      = {Combining machine learning with logic-based expert systems in order to get the best of both worlds are becoming increasingly popular. However, to what extent machine learning can already learn to reason over rule-based knowledge is still an open problem. In this paper, we explore how symbolic logic, defined as logic programs at a character level, is learned to be represented in a high-dimensional vector space using RNN-based iterative neural networks to perform reasoning. We create a new dataset that defines 12 classes of logic programs exemplifying increased level of complexity of logical reasoning and train the networks in an end-to-end fashion to learn whether a logic program entails a given query. We analyse how learning the inference algorithm gives rise to representations of atoms, literals and rules within logic programs and evaluate against increasing lengths of predicate and constant symbols as well as increasing steps of multi-hop reasoning.},
  archiveprefix = {arXiv},
  eprint        = {1805.07433},
  eprintclass   = {cs.NE},
  eprinttype    = {arXiv},
  file          = {:http\://arxiv.org/pdf/1805.07433v3:PDF},
  keywords      = {cs.NE, cs.AI, cs.LO},
}

@Article{chineseroom,
  author    = {Searle, John R.},
  title     = {Minds, brains, and programs},
  journal   = {Behavioral and brain sciences},
  year      = {1980},
  volume    = {3},
  number    = {3},
  pages     = {417--424},
  doi       = {10.1016/b978-1-4832-1446-7.50007-8},
  publisher = {Cambridge University Press},
}

@Book{languageofthought,
  title     = {The Language of Thought.},
  publisher = {Philosophy Documentation Center},
  year      = {1978},
  author    = {Morton, Adam and Fodor, Jerry A.},
  volume    = {75},
  month     = mar,
  doi       = {10.2307/2025426},
  journal   = {The Journal of Philosophy},
  pages     = {161},
}

@Book{coevoflangandbrain,
  title     = {The Symbolic Species: The Co-Evolution of Language and the Brain},
  publisher = {Wiley},
  year      = {1998},
  author    = {Unger, J. Marshall and Deacon, Terrence W.},
  volume    = {82},
  doi       = {10.2307/329984},
  journal   = {The Modern Language Journal},
  pages     = {437},
}

@Article{physicalsymbolsystem,
  author    = {Newell, Allen and Simon, Herbert A.},
  title     = {Computer science as empirical inquiry: symbols and search},
  journal   = {Communications of the {ACM}},
  year      = {1976},
  volume    = {19},
  number    = {3},
  pages     = {113--126},
  month     = mar,
  doi       = {10.1145/360018.360022},
  publisher = {Association for Computing Machinery ({ACM})},
}

@Book{mathcognition,
  title     = {The Handbook of Mathematical Cognition},
  publisher = {Psychology Press},
  year      = {2004},
  author    = {Campbell, Jamie I. D.},
  isbn      = {1841694118},
}

@Book{piagetsymthought,
  author    = {Piaget, Jean},
  publisher = {Routledge},
  title     = {The Psychology of Intelligence},
  year      = {2001},
  isbn      = {0415254019},
  date      = {2001-05-17},
  ean       = {9780415254014},
  pagetotal = {216},
}

@InCollection{turingmachinethink,
  author    = {Turing, Alan M.},
  title     = {Computing Machinery and Intelligence},
  booktitle = {Parsing the Turing Test},
  publisher = {Springer},
  year      = {2009},
  pages     = {23--65},
  doi       = {10.1007/978-1-4020-6710-5_3},
}

@Article{symandsubsyminteg,
  author    = {Kelley, Troy D.},
  title     = {Symbolic and Sub-Symbolic Representations in Computational Models of Human Cognition},
  journal   = {Theory {\&} Psychology},
  year      = {2003},
  volume    = {13},
  number    = {6},
  pages     = {847--860},
  month     = dec,
  doi       = {10.1177/0959354303136005},
  publisher = {{SAGE} Publications},
}

@Article{cogmodelsymbolic,
  author    = {Lewis, Richard L.},
  title     = {Cognitive modeling, symbolic},
  journal   = {The MIT encyclopedia of the cognitive sciences},
  year      = {1999},
  pages     = {525--527},
  publisher = {MIT Press},
}

@Article{senseandreference,
  author    = {Frege, Gottlob},
  title     = {Sense and Reference},
  journal   = {The Philosophical Review},
  year      = {1948},
  volume    = {57},
  number    = {3},
  pages     = {209},
  month     = may,
  doi       = {10.2307/2181485},
  publisher = {{JSTOR}},
}

@Article{attsurvey,
  author      = {Chaudhari, Sneha and Polatkan, Gungor and Ramanath, Rohan and Mithal, Varun},
  journal     = {IJCAI},
  title       = {An Attentive Survey of Attention Models},
  year        = {2019},
  abstract    = {Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy which groups existing techniques into coherent categories. We review the different neural architectures in which attention has been incorporated, and also show how attention improves interpretability of neural models. Finally, we discuss some applications in which modeling attention has a significant impact. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.},
  date        = {2019-04-05},
  eprint      = {1904.02874},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1904.02874v1:PDF},
  keywords    = {cs.LG, stat.ML},
}

@Article{bahdanauatt,
  author      = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal     = {ICLR},
  title       = {Neural Machine Translation by Jointly Learning to Align and Translate},
  abstract    = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  date        = {2014-09-01},
  eprint      = {1409.0473},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1409.0473v7:PDF},
  keywords    = {cs.CL, cs.LG, cs.NE, stat.ML},
}

@InProceedings{nmtatt,
  author      = {Luong, Thang and Pham, Hieu and Manning, Christopher D.},
  booktitle   = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  title       = {Effective Approaches to Attention-based Neural Machine Translation},
  year        = {2015},
  publisher   = {Association for Computational Linguistics},
  abstract    = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  date        = {2015-08-17},
  doi         = {10.18653/v1/d15-1166},
  eprint      = {1508.04025},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1508.04025v5:PDF},
  keywords    = {cs.CL},
}

@Article{gumbelsoftmax,
  author      = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  journal     = {ICLR},
  title       = {Categorical Reparameterization with Gumbel-Softmax},
  year        = {2017},
  abstract    = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  date        = {2016-11-03},
  eprint      = {http://arxiv.org/abs/1611.01144v5},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1611.01144v5:PDF},
  groups      = {[Nuri:]},
  keywords    = {stat.ML, cs.LG},
}

@Book{sticksword,
  title     = {The Developmental Benefits Of Playgrounds},
  publisher = {Association for Childhood Education International},
  year      = {2004},
  author    = {Frost, Joe L. and Brown, Pei-San and Sutterby, John A. and Thornton, Candra D.},
  isbn      = {0871731649},
}

@inproceedings{sanitychecksforsaliencymaps,
  title={Sanity checks for saliency maps},
  author={Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
  booktitle={Advances in Neural Information Processing Systems},
  pages={9505--9515},
  year={2018}
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Markings\;2\;1\;\;\;\;;
2 StaticGroup:[Nuri:]\;2\;1\;\;\;\;;
}
