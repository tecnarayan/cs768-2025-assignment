\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2023)Ahn, Cheng, Song, Yun, Jadbabaie, and
  Sra]{ahn2023linear}
Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit
  Sra.
\newblock Linear attention is (maybe) all you need (to understand transformer
  optimization).
\newblock \emph{arXiv preprint arXiv:2310.01082}, 2023.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and
  Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Allen-Zhu and Li(2023)]{allen2023physics}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Physics of language models: Part 1, context-free grammar.
\newblock \emph{arXiv preprint arXiv:2305.13673}, 2023.

\bibitem[Alon and Spencer(2016)]{alon2016probabilistic}
Noga Alon and Joel~H Spencer.
\newblock \emph{The probabilistic method}.
\newblock John Wiley \& Sons, 2016.

\bibitem[Black et~al.(2022)Black, Biderman, Hallahan, Anthony, Gao, Golding,
  He, Leahy, McDonell, Phang, et~al.]{black2022gpt}
Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence
  Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et~al.
\newblock Gpt-neox-20b: An open-source autoregressive language model.
\newblock \emph{Proceedings of BigScience -- Workshop on Challenges {\&}
  Perspectives in Creating Large Language Models}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Neural Information Processing Systems}, 2020.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1}, 2019.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and
  Zhang]{edelman2022inductive}
Benjamin~L Edelman, Surbhi Goel, Sham Kakade, and Cyril Zhang.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2022.

\bibitem[Elhage et~al.(2021)Elhage, Nanda, Olsson, Henighan, Joseph, Mann,
  Askell, Bai, Chen, Conerly, DasSarma, Drain, Ganguli, Hatfield-Dodds,
  Hernandez, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{elhage2021mathematical}
Nelson Elhage, Neel Nanda, Catherine Olsson, Tom Henighan, Nicholas Joseph, Ben
  Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn
  Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Andy Jones, Jackson
  Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark,
  Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock A mathematical framework for transformer circuits.
\newblock \emph{Transformer Circuits Thread}, 2021.
\newblock https://transformer-circuits.pub/2021/framework/index.html.

\bibitem[Erdogdu et~al.(2016)Erdogdu, Dicker, and Bayati]{erdogdu2016scaled}
Murat~A Erdogdu, Lee~H Dicker, and Mohsen Bayati.
\newblock Scaled least squares estimator for glms in large-scale problems.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Shivam Garg, Dimitris Tsipras, Percy~S Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? a case study of simple
  function classes.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30583--30598, 2022.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and
  Papailiopoulos]{giannou2023looped}
Angeliki Giannou, Shashank Rajput, Jy-yong Sohn, Kangwook Lee, Jason~D Lee, and
  Dimitris Papailiopoulos.
\newblock Looped transformers as programmable computers.
\newblock \emph{arXiv preprint arXiv:2301.13196}, 2023.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{graves2014neural}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines.
\newblock \emph{arXiv preprint arXiv:1410.5401}, 2014.

\bibitem[Hochreiter and Schmidhuber(1997)]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 1997.

\bibitem[Jastrzebski et~al.(2018)Jastrzebski, Arpit, Ballas, Verma, Che, and
  Bengio]{jastrzebski2018residual}
Stanis≈Çaw Jastrzebski, Devansh Arpit, Nicolas Ballas, Vikas Verma, Tong Che,
  and Yoshua Bengio.
\newblock Residual connections encourage iterative inference.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=SJa9iHgAZ}.

\bibitem[Li and Malik(2017)]{li2016learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Lieber et~al.(2021)Lieber, Sharir, Lenz, and
  Shoham]{lieber2021jurassic}
Opher Lieber, Or~Sharir, Barak Lenz, and Yoav Shoham.
\newblock Jurassic-1: Technical details and evaluation.
\newblock \emph{White Paper. AI21 Labs}, 2021.

\bibitem[Mahankali et~al.(2023)Mahankali, Hashimoto, and Ma]{mahankali2023one}
Arvind Mahankali, Tatsunori~B Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context
  learner with one layer of linear self-attention.
\newblock \emph{arXiv preprint arXiv:2307.03576}, 2023.

\bibitem[Min et~al.(2021)Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min2021metaicl}
Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi.
\newblock Metaicl: Learning to learn in context.
\newblock \emph{Proceedings of the Conference of the North American Chapter of
  the Association for Computational Linguistics: Human Language Technologies},
  2021.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan,
  Mann, Askell, Bai, Chen, Conerly, Drain, Ganguli, Hatfield-Dodds, Hernandez,
  Johnston, Jones, Kernion, Lovitt, Ndousse, Amodei, Brown, Clark, Kaplan,
  McCandlish, and Olah]{olsson2022context}
Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma,
  Tom Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly,
  Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott
  Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario
  Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, and Chris Olah.
\newblock In-context learning and induction heads.
\newblock \emph{Transformer Circuits Thread}, 2022.

\bibitem[P{\'e}rez et~al.(2021)P{\'e}rez, Barcel{\'o}, and
  Marinkovic]{perez2021attention}
Jorge P{\'e}rez, Pablo Barcel{\'o}, and Javier Marinkovic.
\newblock Attention is turing complete.
\newblock \emph{The Journal of Machine Learning Research}, 2021.

\bibitem[Rae et~al.(2021)Rae, Borgeaud, Cai, Millican, Hoffmann, Song,
  Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Jack~W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann,
  Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young,
  et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Imanol Schlag, Kazuki Irie, and J{\"u}rgen Schmidhuber.
\newblock Linear transformers are secretly fast weight programmers.
\newblock In \emph{International Conference on Machine Learning}, pages
  9355--9366. PMLR, 2021.

\bibitem[Siegelmann and Sontag(1992)]{siegelmann1992computational}
Hava~T Siegelmann and Eduardo~D Sontag.
\newblock On the computational power of neural nets.
\newblock In \emph{Proceedings of Workshop on Computational learning theory},
  1992.

\bibitem[Vapnik(1999)]{vapnik1999nature}
Vladimir Vapnik.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer science \& business media, 1999.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 2017.

\bibitem[von Oswald et~al.(2023)von Oswald, Niklasson, Randazzo, Sacramento,
  Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~a}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  35151--35174. PMLR, 2023.

\bibitem[Wei et~al.(2022)Wei, Chen, and Ma]{wei2022statistically}
Colin Wei, Yining Chen, and Tengyu Ma.
\newblock Statistically meaningful approximation: a case study on approximating
  turing machines with transformers.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 12071--12083, 2022.

\bibitem[Wortsman et~al.(2023)Wortsman, Lee, Gilmer, and
  Kornblith]{wortsman2023replacing}
Mitchell Wortsman, Jaehoon Lee, Justin Gilmer, and Simon Kornblith.
\newblock Replacing softmax with relu in vision transformers.
\newblock \emph{arXiv preprint arXiv:2309.08586}, 2023.

\bibitem[Xie et~al.(2021)Xie, Raghunathan, Liang, and Ma]{xie2021explanation}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Frei, and Bartlett]{zhang2023trained}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock \emph{arXiv preprint arXiv:2306.09927}, 2023.

\bibitem[Zhao et~al.(2023)Zhao, Panigrahi, Ge, and Arora]{zhao2023transformers}
Haoyu Zhao, Abhishek Panigrahi, Rong Ge, and Sanjeev Arora.
\newblock Do transformers parse while predicting the masked word?
\newblock \emph{arXiv preprint arXiv:2303.08117}, 2023.

\end{thebibliography}
