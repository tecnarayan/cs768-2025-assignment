\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{Ba2016LN}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization, 2016.

\bibitem[Benz et~al.(2021)Benz, Zhang, Karjauv, and Kweon]{2021_WACV_Benz}
Philipp Benz, Chaoning Zhang, Adil Karjauv, and In~So Kweon.
\newblock Revisiting batch normalization for improving corruption robustness.
\newblock In \emph{WACV}, 2021.

\bibitem[Bhardwaj et~al.(2021)Bhardwaj, Majumder, Poria, and
  Hovy]{Bhardwaj2021TextClsTrans}
Rishabh Bhardwaj, Navonil Majumder, Soujanya Poria, and Eduard Hovy.
\newblock More identifiable yet equally performant transformers for text
  classification.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association
  for Computational Linguistics and the 11th International Joint Conference on
  Natural Language Processing (Volume 1: Long Papers)}, pages 1172--1182.
  Association for Computational Linguistics, 2021.

\bibitem[Bjorck et~al.(2018)Bjorck, Gomes, Selman, and
  Weinberger]{Bjorck2018BNLargeLr}
Nils Bjorck, Carla~P Gomes, Bart Selman, and Kilian~Q Weinberger.
\newblock Understanding batch normalization.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{Brown2020GPT3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon
  Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris
  Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess,
  Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
  and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem[Chiley et~al.(2019)Chiley, Sharapov, Kosson, Koster, Reece, Samaniego
  de~la Fuente, Subbiah, and James]{2019_NeurIPS_Chiley}
Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs Koster, Ryan Reece, Sofia
  Samaniego de~la Fuente, Vishal Subbiah, and Michael James.
\newblock Online normalization for training neural networks.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{Dai2019TransXL}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 2978--2988. Association for
  Computational Linguistics, 2019.

\bibitem[Daneshmand et~al.(2020)Daneshmand, Kohler, Bach, Hofmann, and
  Lucchi]{Dane2020BNRank1}
Hadi Daneshmand, Jonas~Moritz Kohler, Francis Bach, Thomas Hofmann, and
  Aurélien Lucchi.
\newblock Batch normalization provably avoids ranks collapse for randomly
  initialised deep networks.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Daneshmand et~al.(2021)Daneshmand, Joudaki, and Bach]{Dane2021BNRank2}
Hadi Daneshmand, Amir Joudaki, and Francis Bach.
\newblock Batch normalization orthogonalizes representations in deep random
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{Devlin2019Bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186.
  Association for Computational Linguistics, 2019.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{Hardt2016TrainFG}
Moritz Hardt, Benjamin Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{ArXiv}, abs/1509.01240, 2016.

\bibitem[Huang et~al.(2019)Huang, Zhou, Zhu, Liu, and Shao]{Huang2019IterNorm}
Lei Huang, Yi~Zhou, Fan Zhu, Li~Liu, and Ling Shao.
\newblock Iterative normalization: Beyond standardization towards efficient
  whitening.
\newblock \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 4869--4878, 2019.

\bibitem[Huang et~al.(2020)Huang, Qin, Liu, Zhu, and Shao]{Huang2020BNDynamic}
Lei Huang, Jie Qin, Li~Liu, Fan Zhu, and Ling Shao.
\newblock Layer-wise conditioning analysis in exploring the learning dynamics
  of dnns.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm,
  editors, \emph{Computer Vision -- ECCV 2020}, pages 384--401. Springer
  International Publishing, 2020.
\newblock ISBN 978-3-030-58536-5.

\bibitem[Huang et~al.(2022)Huang, Zhou, Wang, Luo, and Liu]{2022_CVPR_Huang}
Lei Huang, Yi~Zhou, Tian Wang, Jie Luo, and Xianglong Liu.
\newblock Delving into the estimation shift of batch normalization in a
  network.
\newblock \emph{arXiv preprint arXiv:2203.10778}, 2022.

\bibitem[Huang et~al.(2015)Huang, Xu, and Yu]{Huang2015CRF}
Zhiheng Huang, Wei Xu, and Kai Yu.
\newblock Bidirectional lstm-crf models for sequence tagging.
\newblock \emph{ArXiv}, abs/1508.01991, 2015.

\bibitem[Ioffe(2017)]{2017_NIPS_Ioffe}
Sergey Ioffe.
\newblock Batch renormalization: Towards reducing minibatch dependence in
  batch-normalized models.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Ioffe and Szegedy(2015)]{Ioffe2015BN}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, volume~37 of \emph{Proceedings of Machine Learning Research},
  pages 448--456. PMLR, 2015.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{2018_UAI_Izmailov}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock \emph{arXiv preprint arXiv:1803.05407}, 2018.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009CIFAR10}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{Kri2012AlexNet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{NIPS}, pages 1106--1114, 2012.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{Lecun2015DL}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Li et~al.(2019)Li, Chen, Hu, and Yang]{Li2019BNDropout}
Xiang Li, Shuo Chen, Xiaolin Hu, and Jian Yang.
\newblock Understanding the disharmony between dropout and batch normalization
  by variance shift.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2019.

\bibitem[Li et~al.(2016)Li, Wang, Shi, Liu, and Hou]{2017_arxiv_Li}
Yanghao Li, Naiyan Wang, Jianping Shi, Jiaying Liu, and Xiaodi Hou.
\newblock Revisiting batch normalization for practical domain adaptation.
\newblock \emph{arXiv preprint arXiv:1603.04779}, 2016.

\bibitem[Luo et~al.(2019{\natexlab{a}})Luo, Ren, Peng, Zhang, and
  Li]{2019_ICLR_Luo}
Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, and Jingyu Li.
\newblock Differentiable learning-to-normalize via switchable normalization.
\newblock In \emph{ICLR}, 2019{\natexlab{a}}.

\bibitem[Luo et~al.(2019{\natexlab{b}})Luo, Wang, Shao, and Peng]{Luo2018BNReg}
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng.
\newblock Towards understanding regularization in batch normalization.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Ma et~al.(2019)Ma, Zhang, Zhang, Duan, Hou, Zhou, and
  Song]{Ma2019TensorTrans}
Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Ming Zhou, and
  Dawei Song.
\newblock A tensorized transformer for language modeling.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and Potts]{Maas2011IMDB}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages 142--150.
  Association for Computational Linguistics, 2011.

\bibitem[Martens and Grosse(2015)]{Marten2015KFAC}
James Martens and Roger~B. Grosse.
\newblock Optimizing neural networks with kronecker-factored approximate
  curvature.
\newblock \emph{CoRR}, abs/1503.05671, 2015.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{MerityX2016WikiText}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{CoRR}, abs/1609.07843, 2016.

\bibitem[Mikolov et~al.(2011)Mikolov, Deoras, Kombrink, Burget, and
  Cernocky]{Mikolov2011PTB}
Tomas Mikolov, Anoop Deoras, Stefan Kombrink, Lukas Burget, and Jan~"Honza"
  Cernocky.
\newblock Empirical evaluation and combination of advanced language modeling
  techniques.
\newblock In \emph{Interspeech}. ISCA, August 2011.

\bibitem[Nado et~al.(2020)Nado, Padhy, Sculley, D'Amour, Lakshminarayanan, and
  Snoek]{2020_arxiv_Nado}
Zachary Nado, Shreyas Padhy, D~Sculley, Alexander D'Amour, Balaji
  Lakshminarayanan, and Jasper Snoek.
\newblock Evaluating prediction-time batch normalization for robustness under
  covariate shift.
\newblock \emph{arXiv preprint arXiv:2006.10963}, 2020.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{Ott2019Fairseq}
Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng,
  David Grangier, and Michael Auli.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics (Demonstrations)},
  pages 48--53. Association for Computational Linguistics, 2019.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and Zhu]{Pap2002Bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: A method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th Annual Meeting on Association for
  Computational Linguistics}, ACL '02, page 311–318. Association for
  Computational Linguistics, 2002.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{Radford2019GPT2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock 2019.

\bibitem[Sang and Meulder(2003)]{Sang2003CoNLL}
Erik Tjong~Kim Sang and Fien~De Meulder.
\newblock Introduction to the conll-2003 shared task: Language-independent
  named entity recognition.
\newblock In \emph{CoNLL}, 2003.

\bibitem[Santurkar et~al.(2018)Santurkar, Tsipras, Ilyas, and
  Madry]{San2018BnHelpOpt}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31, 2018.

\bibitem[Schneider et~al.(2020)Schneider, Rusak, Eck, Bringmann, Brendel, and
  Bethge]{2020_NIPS_Schneider}
Steffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel,
  and Matthias Bethge.
\newblock Improving robustness against common corruptions by covariate shift
  adaptation.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Shen et~al.(2020)Shen, Yao, Gholami, Mahoney, and
  Keutzer]{Shen2020Powernorm}
Sheng Shen, Zhewei Yao, Amir Gholami, Michael Mahoney, and Kurt Keutzer.
\newblock Powernorm: Rethinking batch normalization in transformers.
\newblock In \emph{ICML}, 2020.

\bibitem[Singh and Shrivastava(2019)]{2019_ICCV_Singh}
Saurabh Singh and Abhinav Shrivastava.
\newblock Evalnorm: Estimating batch normalization statistics for evaluation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Summers and Dinneen(2020)]{2020_ICLR_Summers}
Cecilia Summers and Michael~J. Dinneen.
\newblock Four things everyone should know to improve batch normalization.
\newblock In \emph{ICLR}, 2020.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{Vaswani2017Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Wu and He(2018)]{2018_ECCV_Wu}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{ECCV}, 2018.

\bibitem[Wu and Johnson(2021)]{Wu2021RevistBN}
Yuxin Wu and Justin Johnson.
\newblock Rethinking "batch" in batchnorm.
\newblock \emph{CoRR}, abs/2105.07576, 2021.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{Xiong2020NoWarmUp}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing,
  Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tie-Yan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{ICML 2020}, 2020.

\bibitem[Xu et~al.(2019)Xu, Sun, Zhang, Zhao, and Lin]{Xu2019UnderstandLN}
Jingjing Xu, Xu~Sun, Zhiyuan Zhang, Guangxiang Zhao, and Junyang Lin.
\newblock Understanding and improving layer normalization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Yan et~al.(2019)Yan, Deng, Li, and Qiu]{Yan2019TENER}
Hang Yan, Bocao Deng, Xiaonan Li, and Xipeng Qiu.
\newblock Tener: Adapting transformer encoder for named entity recognition,
  2019.

\bibitem[Yan et~al.(2020)Yan, Wan, Zhang, Zhang, Wei, and Sun]{2020_ICLR_Yan}
Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun.
\newblock Towards stabilizing batch statistics in backward propagation of batch
  normalization.
\newblock In \emph{ICLR}, 2020.

\bibitem[Yao et~al.(2021{\natexlab{a}})Yao, Cao, Lin, Liu, Zhang, and
  Hu]{Yao2021LeveragingBN}
Zhuliang Yao, Yue Cao, Yutong Lin, Ze~Liu, Zheng Zhang, and Han Hu.
\newblock Leveraging batch normalization for vision transformers.
\newblock \emph{2021 IEEE/CVF International Conference on Computer Vision
  Workshops (ICCVW)}, pages 413--422, 2021{\natexlab{a}}.

\bibitem[Yao et~al.(2021{\natexlab{b}})Yao, Cao, Zheng, Huang, and
  Lin]{2021_CVPR_Yao}
Zhuliang Yao, Yue Cao, Shuxin Zheng, Gao Huang, and Stephen Lin.
\newblock Cross-iteration batch normalization.
\newblock In \emph{CVPR}, 2021{\natexlab{b}}.

\bibitem[Yong et~al.(2020)Yong, Huang, Hua, and Zhang]{2020_ECCV_Yong}
Hongwei Yong, Jianqiang Huang, Xiansheng Hua, and Lei Zhang.
\newblock Gradient centralization: A new optimization technique for deep neural
  networks.
\newblock In \emph{ECCV}, 2020.

\bibitem[Zhang and Sennrich(2019)]{Zhang2019RMSNorm}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem[Zhang and Yang(2018)]{Zhang2018Resume}
Yue Zhang and Jie Yang.
\newblock Chinese ner using lattice lstm.
\newblock In \emph{ACL}, 2018.

\end{thebibliography}
