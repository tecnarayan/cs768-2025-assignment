\begin{thebibliography}{10}

\bibitem{shepard2008step}
R.~N. Shepard, ``The step to rationality: The efficacy of thought experiments
  in science, ethics, and free will,'' {\em Cognitive Science}, vol.~32, no.~1,
  pp.~3--35, 2008.

\bibitem{dennett2013intuition}
D.~C. Dennett, {\em Intuition pumps and other tools for thinking}.
\newblock WW Norton \& Company, 2013.

\bibitem{brown2020language}
T.~Brown, B.~Mann, N.~Ryder, M.~Subbiah, J.~D. Kaplan, P.~Dhariwal,
  A.~Neelakantan, P.~Shyam, G.~Sastry, A.~Askell, {\em et~al.}, ``Language
  models are few-shot learners,'' {\em Advances in neural information
  processing systems}, vol.~33, pp.~1877--1901, 2020.

\bibitem{chowdhery2022palm}
A.~Chowdhery, S.~Narang, J.~Devlin, M.~Bosma, G.~Mishra, A.~Roberts, P.~Barham,
  H.~W. Chung, C.~Sutton, S.~Gehrmann, {\em et~al.}, ``Palm: Scaling language
  modeling with pathways,'' {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{touvron2023llama}
H.~Touvron, T.~Lavril, G.~Izacard, X.~Martinet, M.-A. Lachaux, T.~Lacroix,
  B.~Rozi{\`e}re, N.~Goyal, E.~Hambro, F.~Azhar, {\em et~al.}, ``Llama: Open
  and efficient foundation language models,'' {\em arXiv preprint
  arXiv:2302.13971}, 2023.

\bibitem{srivastava2023beyond}
B.~bench authors, ``Beyond the imitation game: Quantifying and extrapolating
  the capabilities of language models,'' {\em Transactions on Machine Learning
  Research}, 2023.

\bibitem{cobbe2021training}
K.~Cobbe, V.~Kosaraju, M.~Bavarian, M.~Chen, H.~Jun, L.~Kaiser, M.~Plappert,
  J.~Tworek, J.~Hilton, R.~Nakano, {\em et~al.}, ``Training verifiers to solve
  math word problems,'' {\em arXiv preprint arXiv:2110.14168}, 2021.

\bibitem{nye2021show}
M.~Nye, A.~J. Andreassen, G.~Gur-Ari, H.~Michalewski, J.~Austin, D.~Bieber,
  D.~Dohan, A.~Lewkowycz, M.~Bosma, D.~Luan, {\em et~al.}, ``Show your work:
  Scratchpads for intermediate computation with language models,'' {\em arXiv
  preprint arXiv:2112.00114}, 2021.

\bibitem{kojima2022large}
T.~Kojima, S.~S. Gu, M.~Reid, Y.~Matsuo, and Y.~Iwasawa, ``Large language
  models are zero-shot reasoners,'' {\em arXiv preprint arXiv:2205.11916},
  2022.

\bibitem{wei2022chain}
J.~Wei, X.~Wang, D.~Schuurmans, M.~Bosma, brian ichter, F.~Xia, E.~H. Chi,
  Q.~V. Le, and D.~Zhou, ``Chain of thought prompting elicits reasoning in
  large language models,'' in {\em Advances in Neural Information Processing
  Systems} (A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, eds.), 2022.

\bibitem{dohan2022language}
D.~Dohan, W.~Xu, A.~Lewkowycz, J.~Austin, D.~Bieber, R.~G. Lopes, Y.~Wu,
  H.~Michalewski, R.~A. Saurous, J.~Sohl-dickstein, {\em et~al.}, ``Language
  model cascades,'' {\em arXiv preprint arXiv:2207.10342}, 2022.

\bibitem{lampinen2022can}
A.~K. Lampinen, I.~Dasgupta, S.~C. Chan, K.~Matthewson, M.~H. Tessler,
  A.~Creswell, J.~L. McClelland, J.~X. Wang, and F.~Hill, ``Can language models
  learn from explanations in context?,'' {\em arXiv preprint arXiv:2204.02329},
  2022.

\bibitem{zelikman2022star}
E.~Zelikman, Y.~Wu, J.~Mu, and N.~Goodman, ``{ST}a{R}: Bootstrapping reasoning
  with reasoning,'' in {\em Advances in Neural Information Processing Systems}
  (A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, eds.), 2022.

\bibitem{wang2022self}
X.~Wang, J.~Wei, D.~Schuurmans, Q.~V. Le, E.~H. Chi, S.~Narang, A.~Chowdhery,
  and D.~Zhou, ``Self-consistency improves chain of thought reasoning in
  language models,'' in {\em The Eleventh International Conference on Learning
  Representations}, 2022.

\bibitem{zhou2022least}
D.~Zhou, N.~Sch{\"a}rli, L.~Hou, J.~Wei, N.~Scales, X.~Wang, D.~Schuurmans,
  C.~Cui, O.~Bousquet, Q.~Le, {\em et~al.}, ``Least-to-most prompting enables
  complex reasoning in large language models,'' {\em arXiv preprint
  arXiv:2205.10625}, 2022.

\bibitem{yao2023tree}
S.~Yao, D.~Yu, J.~Zhao, I.~Shafran, T.~L. Griffiths, Y.~Cao, and K.~Narasimhan,
  ``Tree of thoughts: Deliberate problem solving with large language models,''
  {\em arXiv preprint arXiv:2305.10601}, 2023.

\bibitem{blei2003latent}
D.~M. Blei, A.~Y. Ng, and M.~I. Jordan, ``Latent dirichlet allocation,'' {\em
  Journal of machine Learning research}, vol.~3, no.~Jan, pp.~993--1022, 2003.

\bibitem{blei2007correlated}
D.~M. Blei and J.~D. Lafferty, ``{A correlated topic model of Science},'' {\em
  The Annals of Applied Statistics}, vol.~1, no.~1, pp.~17 -- 35, 2007.

\bibitem{chan2022data}
S.~C. Chan, A.~Santoro, A.~K. Lampinen, J.~X. Wang, A.~K. Singh, P.~H.
  Richemond, J.~McClelland, and F.~Hill, ``Data distributional properties drive
  emergent in-context learning in transformers,'' in {\em Advances in Neural
  Information Processing Systems} (A.~H. Oh, A.~Agarwal, D.~Belgrave, and
  K.~Cho, eds.), 2022.

\bibitem{radford2019language}
A.~Radford, J.~Wu, R.~Child, D.~Luan, D.~Amodei, and I.~Sutskever, ``Language
  models are unsupervised multitask learners,'' 2019.

\bibitem{wolf2020transformers}
T.~Wolf, L.~Debut, V.~Sanh, J.~Chaumond, C.~Delangue, A.~Moi, P.~Cistac,
  T.~Rault, R.~Louf, M.~Funtowicz, {\em et~al.}, ``Transformers:
  State-of-the-art natural language processing,'' in {\em Proceedings of the
  2020 conference on empirical methods in natural language processing: system
  demonstrations}, pp.~38--45, 2020.

\bibitem{sennrich2016neural}
R.~Sennrich, B.~Haddow, and A.~Birch, ``Neural machine translation of rare
  words with subword units,'' in {\em Proceedings of the 54th Annual Meeting of
  the Association for Computational Linguistics (Volume 1: Long Papers)},
  pp.~1715--1725, 2016.

\bibitem{kingma2015adam}
D.~P. Kingma and J.~Ba, ``Adam: A method for stochastic optimization,'' in {\em
  Proceedings of the 3rd International Conference on Learning Representations
  (ICLR)}, 2015.

\end{thebibliography}
