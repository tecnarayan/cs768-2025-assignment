\begin{thebibliography}{10}

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in neural information processing systems}, 35:23716--23736, 2022.

\bibitem{alon2021on}
Uri Alon and Eran Yahav.
\newblock On the bottleneck of graph neural networks and its practical implications.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{barbero2024localityaware}
Federico Barbero, Ameya Velingker, Amin Saberi, Michael~M. Bronstein, and Francesco~Di Giovanni.
\newblock Locality-aware graph rewiring in {GNN}s.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{barbero2024round}
Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, and Petar Veli{\v{c}}kovi{\'c}.
\newblock Round and round we go! what makes rotary positional encodings useful?
\newblock {\em arXiv preprint arXiv:2410.06205}, 2024.

\bibitem{bengio94}
Y.~Bengio, P.~Simard, and P.~Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE Transactions on Neural Networks}, 5(2):157--166, 1994.

\bibitem{dehghani2019universal}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Łukasz Kaiser.
\newblock Universal transformers, 2019.

\bibitem{deletang2023neural}
Grégoire Delétang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li~Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro~A. Ortega.
\newblock Neural networks and the chomsky hierarchy, 2023.

\bibitem{di2023over}
Francesco Di~Giovanni, Lorenzo Giusti, Federico Barbero, Giulia Luise, Pietro Lio, and Michael~M Bronstein.
\newblock On over-squashing in message passing neural networks: The impact of width, depth, and topology.
\newblock In {\em International Conference on Machine Learning}, pages 7865--7885. PMLR, 2023.

\bibitem{dudzik2024asynchronous}
Andrew~Joseph Dudzik, Tamara von Glehn, Razvan Pascanu, and Petar Veli{\v{c}}kovi{\'c}.
\newblock Asynchronous algorithmic alignment with cocycles.
\newblock In {\em Learning on Graphs Conference}, pages 3--1. PMLR, 2024.

\bibitem{team2023gemini}
Team Gemini.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{giovanni2024how}
Francesco~Di Giovanni, T.~Konstantin Rusch, Michael Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, and Petar Veli{\v{c}}kovi{\'c}.
\newblock How does over-squashing affect the power of {GNN}s?
\newblock {\em Transactions on Machine Learning Research}, 2024.

\bibitem{hahn2020theoretical}
Michael Hahn.
\newblock Theoretical limitations of self-attention in neural sequence models.
\newblock {\em Transactions of the Association for Computational Linguistics}, 8:156--171, 2020.

\bibitem{herranzcelotti2024stabilizing}
Luca Herranz-Celotti and Jean Rouat.
\newblock Stabilizing rnn gradients through pre-training, 2024.

\bibitem{HochSchm97}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural Computation}, 9(8):1735--1780, 1997.

\bibitem{kazemnejad2024impact}
Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan~Ramamurthy, Payel Das, and Siva Reddy.
\newblock The impact of positional encoding on length generalization in transformers.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{li2022competition}
Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R{\'e}mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal~Lago, et~al.
\newblock Competition-level code generation with alphacode.
\newblock {\em Science}, 378(6624):1092--1097, 2022.

\bibitem{liu2024lost}
Nelson~F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang.
\newblock Lost in the middle: How language models use long contexts.
\newblock {\em Transactions of the Association for Computational Linguistics}, 12:157--173, 2024.

\bibitem{merrill2023expresssive}
William Merrill and Ashish Sabharwal.
\newblock The expresssive power of transformers with chain of thought.
\newblock {\em arXiv preprint arXiv:2310.07923}, 2023.

\bibitem{openai2023gpt}
R~OpenAI.
\newblock Gpt-4 technical report. arxiv 2303.08774.
\newblock {\em View in Article}, 2(5), 2023.

\bibitem{pascanu13}
Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.
\newblock On the difficulty of training recurrent neural networks.
\newblock In Sanjoy Dasgupta and David McAllester, editors, {\em Proceedings of the 30th International Conference on Machine Learning}, volume~28 of {\em Proceedings of Machine Learning Research}, pages 1310--1318, Atlanta, Georgia, USA, 17--19 Jun 2013. PMLR.

\bibitem{peng2024limitations}
Binghui Peng, Srini Narayanan, and Christos Papadimitriou.
\newblock On limitations of the transformer architecture, 2024.

\bibitem{perez2021attention}
Jorge P{\'e}rez, Pablo Barcel{\'o}, and Javier Marinkovic.
\newblock Attention is turing-complete.
\newblock {\em Journal of Machine Learning Research}, 22(75):1--35, 2021.

\bibitem{pillai2005perron}
S~Unnikrishna Pillai, Torsten Suel, and Seunghun Cha.
\newblock The perron-frobenius theorem: some of its applications.
\newblock {\em IEEE Signal Processing Magazine}, 22(2):62--75, 2005.

\bibitem{press2021train}
Ofir Press, Noah~A Smith, and Mike Lewis.
\newblock Train short, test long: Attention with linear biases enables input length extrapolation.
\newblock {\em arXiv preprint arXiv:2108.12409}, 2021.

\bibitem{schick2024toolformer}
Timo Schick, Jane Dwivedi-Yu, Roberto Dess{\`\i}, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom.
\newblock Toolformer: Language models can teach themselves to use tools.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{su2024roformer}
Jianlin Su, Murtadha Ahmed, Yu~Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding.
\newblock {\em Neurocomputing}, 568:127063, 2024.

\bibitem{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale, Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}, 2024.

\bibitem{team2024gemma2}
Gemma Team, Morgane Riviere, Shreya Pathak, Pier~Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, L{\'e}onard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ram{\'e}, et~al.
\newblock Gemma 2: Improving open language models at a practical size.
\newblock {\em arXiv preprint arXiv:2408.00118}, 2024.

\bibitem{topping2021understanding}
Jake Topping, Francesco Di~Giovanni, Benjamin~Paul Chamberlain, Xiaowen Dong, and Michael~M Bronstein.
\newblock Understanding over-squashing and bottlenecks on graphs via curvature.
\newblock 2022.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{velivckovic2024softmax}
Petar Veli{\v{c}}kovi{\'c}, Christos Perivolaropoulos, Federico Barbero, and Razvan Pascanu.
\newblock softmax is not enough (for sharp out-of-distribution).
\newblock {\em arXiv preprint arXiv:2410.01104}, 2024.

\bibitem{wei2022emergent}
Jason Wei, Yi~Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et~al.
\newblock Emergent abilities of large language models.
\newblock {\em arXiv preprint arXiv:2206.07682}, 2022.

\bibitem{weiss2021thinking}
Gail Weiss, Yoav Goldberg, and Eran Yahav.
\newblock Thinking like transformers, 2021.

\bibitem{xiong2020layer}
Ruibin Xiong, Yunchang Yang, Di~He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu.
\newblock On layer normalization in the transformer architecture.
\newblock In {\em International Conference on Machine Learning}, pages 10524--10533. PMLR, 2020.

\bibitem{zhai2023stabilizing}
Shuangfei Zhai, Tatiana Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, and Joshua~M Susskind.
\newblock Stabilizing transformer training by preventing attention entropy collapse.
\newblock In {\em International Conference on Machine Learning}, pages 40770--40803. PMLR, 2023.

\end{thebibliography}
