\begin{thebibliography}{10}

\bibitem{abbas2023semdedup}
{\sc Abbas, A., Tirumala, K., Simig, D., Ganguli, S., and Morcos, A.~S.}
\newblock Semdedup: Data-efficient learning at web-scale through semantic deduplication, 2023.

\bibitem{aharoni-goldberg-2020-unsupervised}
{\sc Aharoni, R., and Goldberg, Y.}
\newblock Unsupervised domain clusters in pretrained language models.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\/} (Online, July 2020), D.~Jurafsky, J.~Chai, N.~Schluter, and J.~Tetreault, Eds., Association for Computational Linguistics, pp.~7747--7763.

\bibitem{bommasani2021foundation}
{\sc Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.}
\newblock On the opportunities and risks of foundation models.
\newblock {\em arXiv preprint arXiv:2108.07258\/} (2021).

\bibitem{bukharin2024datadiversitymattersrobust}
{\sc Bukharin, A., and Zhao, T.}
\newblock Data diversity matters for robust instruction tuning, 2024.

\bibitem{cer2018universal}
{\sc Cer, D., Yang, Y., yi~Kong, S., Hua, N., Limtiaco, N., John, R.~S., Constant, N., Guajardo-Cespedes, M., Yuan, S., Tar, C., Sung, Y.-H., Strope, B., and Kurzweil, R.}
\newblock Universal sentence encoder, 2018.

\bibitem{palm}
{\sc Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B., Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S., Dev, S., Michalewski, H., Garcia, X., Misra, V., Robinson, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai, A.~M., Pillai, T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-Hellstern, K., Eck, D., Dean, J., Petrov, S., and Fiedel, N.}
\newblock Palm: scaling language modeling with pathways.
\newblock {\em J. Mach. Learn. Res. 24}, 1 (mar 2024).

\bibitem{clark-etal-2020-tydi}
{\sc Clark, J.~H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J.}
\newblock {T}y{D}i {QA}: A benchmark for information-seeking question answering in typologically diverse languages.
\newblock {\em Transactions of the Association for Computational Linguistics 8\/} (2020), 454--470.

\bibitem{DatabricksBlog2023DollyV2}
{\sc Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.}
\newblock Free dolly: Introducing the world's first truly open instruction-tuned llm, 2023.

\bibitem{cuturi2013sinkhorn}
{\sc Cuturi, M.}
\newblock Sinkhorn distances: Lightspeed computation of optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2013), C.~Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Weinberger, Eds., vol.~26, Curran Associates, Inc.

\bibitem{devlin-etal-2019-bert}
{\sc Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.}
\newblock {BERT}: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)\/} (Minneapolis, Minnesota, June 2019), Association for Computational Linguistics, pp.~4171--4186.

\bibitem{elazar2024whats}
{\sc Elazar, Y., Bhagia, A., Magnusson, I.~H., Ravichander, A., Schwenk, D., Suhr, A., Walsh, E.~P., Groeneveld, D., Soldaini, L., Singh, S., Hajishirzi, H., Smith, N.~A., and Dodge, J.}
\newblock What's in my big data?
\newblock In {\em The Twelfth International Conference on Learning Representations\/} (2024).

\bibitem{engstrom2024dsdm}
{\sc Engstrom, L., Feldmann, A., and Madry, A.}
\newblock Dsdm: Model-aware dataset selection with datamodels, 2024.

\bibitem{copycat}
{\sc Fr\"{o}be, M., Bevendorff, J., Gienapp, L., V\"{o}lske, M., Stein, B., Potthast, M., and Hagen, M.}
\newblock Copycat: Near-duplicates within and between the clueweb and the common crawl.
\newblock In {\em Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval\/} (New York, NY, USA, 2021), SIGIR '21, Association for Computing Machinery, p.~2398–2404.

\bibitem{gao2020pile}
{\sc Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., Presser, S., and Leahy, C.}
\newblock The pile: An 800gb dataset of diverse text for language modeling, 2020.

\bibitem{gionis1999similarity-lsh}
{\sc Gionis, A., Indyk, P., Motwani, R., et~al.}
\newblock Similarity search in high dimensions via hashing.
\newblock In {\em Vldb\/} (1999), vol.~99, pp.~518--529.

\bibitem{guo2020accelerating}
{\sc Guo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., and Kumar, S.}
\newblock Accelerating large-scale inference with anisotropic vector quantization.
\newblock In {\em International Conference on Machine Learning\/} (2020), PMLR, pp.~3887--3896.

\bibitem{gururangan-etal-2020-dont}
{\sc Gururangan, S., Marasovi{\'c}, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., and Smith, N.~A.}
\newblock Don{'}t stop pretraining: Adapt language models to domains and tasks.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\/} (Online, July 2020), Association for Computational Linguistics, pp.~8342--8360.

\bibitem{hendrycks2021measuring-mmlu}
{\sc Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.}
\newblock Measuring massive multitask language understanding.
\newblock In {\em International Conference on Learning Representations\/} (2021).

\bibitem{hernandez2022scaling}
{\sc Hernandez, D., Brown, T., Conerly, T., DasSarma, N., Drain, D., El-Showk, S., Elhage, N., Hatfield-Dodds, Z., Henighan, T., Hume, T., Johnston, S., Mann, B., Olah, C., Olsson, C., Amodei, D., Joseph, N., Kaplan, J., and McCandlish, S.}
\newblock Scaling laws and interpretability of learning from repeated data, 2022.

\bibitem{hu2022lora}
{\sc Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.}
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In {\em International Conference on Learning Representations\/} (2022).

\bibitem{datamodels-ilyas22a}
{\sc Ilyas, A., Park, S.~M., Engstrom, L., Leclerc, G., and Madry, A.}
\newblock Datamodels: Understanding predictions with data and data with predictions.
\newblock In {\em Proceedings of the 39th International Conference on Machine Learning\/} (17--23 Jul 2022), K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and S.~Sabato, Eds., vol.~162 of {\em Proceedings of Machine Learning Research}, PMLR, pp.~9525--9587.

\bibitem{jiang2023mistral}
{\sc Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., de~las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L.~R., Lachaux, M.-A., Stock, P., Scao, T.~L., Lavril, T., Wang, T., Lacroix, T., and Sayed, W.~E.}
\newblock Mistral 7b, 2023.

\bibitem{johnson2019-faiss}
{\sc Johnson, J., Douze, M., and J{\'e}gou, H.}
\newblock Billion-scale similarity search with {GPUs}.
\newblock {\em IEEE Transactions on Big Data 7}, 3 (2019), 535--547.

\bibitem{influence}
{\sc Koh, P.~W., and Liang, P.}
\newblock Understanding black-box predictions via influence functions.
\newblock In {\em Proceedings of the 34th International Conference on Machine Learning - Volume 70\/} (2017), ICML'17, JMLR.org, p.~1885–1894.

\bibitem{kringelum2016chemprot}
{\sc Kringelum, J., Kjaerulff, S.~K., Brunak, S., Lund, O., Oprea, T.~I., and Taboureau, O.}
\newblock Chemprot-3.0: a global chemical biology diseases mapping.
\newblock {\em Database 2016\/} (2016), bav123.

\bibitem{köpf2023openassistant}
{\sc Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.~M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A.}
\newblock Openassistant conversations -- democratizing large language model alignment, 2023.

\bibitem{albert}
{\sc Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R.}
\newblock {ALBERT:} {A} lite {BERT} for self-supervised learning of language representations.
\newblock {\em CoRR abs/1909.11942\/} (2019).

\bibitem{lee-etal-2022-deduplicating}
{\sc Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N.}
\newblock Deduplicating training data makes language models better.
\newblock In {\em Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\/} (Dublin, Ireland, May 2022), Association for Computational Linguistics, pp.~8424--8445.

\bibitem{liu2024what}
{\sc Liu, W., Zeng, W., He, K., Jiang, Y., and He, J.}
\newblock What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
\newblock In {\em The Twelfth International Conference on Learning Representations\/} (2024).

\bibitem{DBLP:journals/corr/roberta}
{\sc Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.}
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock {\em CoRR abs/1907.11692\/} (2019).

\bibitem{longpre2023flan}
{\sc Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.~W., Tay, Y., Zhou, D., Le, Q.~V., Zoph, B., Wei, J., et~al.}
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock {\em arXiv preprint arXiv:2301.13688\/} (2023).

\bibitem{luan-etal-2018-multi}
{\sc Luan, Y., He, L., Ostendorf, M., and Hajishirzi, H.}
\newblock Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction.
\newblock In {\em Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\/} (Brussels, Belgium, Oct.-Nov. 2018), E.~Riloff, D.~Chiang, J.~Hockenmaier, and J.~Tsujii, Eds., Association for Computational Linguistics, pp.~3219--3232.

\bibitem{maas-etal-2011-imdb}
{\sc Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A.~Y., and Potts, C.}
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies\/} (Portland, Oregon, USA, June 2011), D.~Lin, Y.~Matsumoto, and R.~Mihalcea, Eds., Association for Computational Linguistics, pp.~142--150.

\bibitem{malkov2018hnsw}
{\sc Malkov, Y.~A., and Yashunin, D.~A.}
\newblock Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence 42}, 4 (2018), 824--836.

\bibitem{moore-lewis-2010-intelligent}
{\sc Moore, R.~C., and Lewis, W.}
\newblock Intelligent selection of language model training data.
\newblock In {\em Proceedings of the {ACL} 2010 Conference Short Papers\/} (Uppsala, Sweden, July 2010), J.~Haji{\v{c}}, S.~Carberry, S.~Clark, and J.~Nivre, Eds., Association for Computational Linguistics, pp.~220--224.

\bibitem{parzen1962kde}
{\sc Parzen, E.}
\newblock On estimation of a probability density function and mode.
\newblock {\em The annals of mathematical statistics 33}, 3 (1962), 1065--1076.

\bibitem{rae2022scaling}
{\sc Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F., Aslanides, J., Henderson, S., Ring, R., Young, S., Rutherford, E., Hennigan, T., Menick, J., Cassirer, A., Powell, R., van~den Driessche, G., Hendricks, L.~A., Rauh, M., Huang, P.-S., Glaese, A., Welbl, J., Dathathri, S., Huang, S., Uesato, J., Mellor, J., Higgins, I., Creswell, A., McAleese, N., Wu, A., Elsen, E., Jayakumar, S., Buchatskaya, E., Budden, D., Sutherland, E., Simonyan, K., Paganini, M., Sifre, L., Martens, L., Li, X.~L., Kuncoro, A., Nematzadeh, A., Gribovskaya, E., Donato, D., Lazaridou, A., Mensch, A., Lespiau, J.-B., Tsimpoukelli, M., Grigorev, N., Fritz, D., Sottiaux, T., Pajarskas, M., Pohlen, T., Gong, Z., Toyama, D., de~Masson~d'Autume, C., Li, Y., Terzi, T., Mikulik, V., Babuschkin, I., Clark, A., de~Las~Casas, D., Guy, A., Jones, C., Bradbury, J., Johnson, M., Hechtman, B., Weidinger, L., Gabriel, I., Isaac, W., Lockhart, E., Osindero, S., Rimell, L., Dyer, C., Vinyals, O., Ayoub, K., Stanway, J., Bennett,
  L., Hassabis, D., Kavukcuoglu, K., and Irving, G.}
\newblock Scaling language models: Methods, analysis \& insights from training gopher, 2022.

\bibitem{ruder-plank-2017-learning-bayes}
{\sc Ruder, S., and Plank, B.}
\newblock Learning to select data for transfer learning with {B}ayesian optimization.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\/} (Copenhagen, Denmark, Sept. 2017), Association for Computational Linguistics, pp.~372--382.

\bibitem{ruder-plank-2017-learning}
{\sc Ruder, S., and Plank, B.}
\newblock Learning to select data for transfer learning with {B}ayesian optimization.
\newblock In {\em Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing\/} (Copenhagen, Denmark, Sept. 2017), M.~Palmer, R.~Hwa, and S.~Riedel, Eds., Association for Computational Linguistics, pp.~372--382.

\bibitem{finetune-theory}
{\sc Shachaf, G., Brutzkus, A., and Globerson, A.}
\newblock A theoretical analysis of fine-tuning with linear teachers.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2021), M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, Eds., vol.~34, Curran Associates, Inc., pp.~15382--15394.

\bibitem{suzgun2022challenging-bbh}
{\sc Suzgun, M., Scales, N., Sch{\"a}rli, N., Gehrmann, S., Tay, Y., Chung, H.~W., Chowdhery, A., Le, Q.~V., Chi, E.~H., Zhou, D., , and Wei, J.}
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock {\em arXiv preprint arXiv:2210.09261\/} (2022).

\bibitem{tirumala2023d4}
{\sc Tirumala, K., Simig, D., Aghajanyan, A., and Morcos, A.~S.}
\newblock D4: Improving llm pretraining via document de-duplication and diversification, 2023.

\bibitem{touvron2023llama}
{\sc Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozière, B., Goyal, N., Hambro, E., Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lample, G.}
\newblock Llama: Open and efficient foundation language models, 2023.

\bibitem{wang2024diversitymeasurementsubsetselection}
{\sc Wang, P., Shen, Y., Guo, Z., Stallone, M., Kim, Y., Golland, P., and Panda, R.}
\newblock Diversity measurement and subset selection for instruction tuning datasets, 2024.

\bibitem{wei2022chain-cot}
{\sc Wei, J., Wang, X., Schuurmans, D., Bosma, M., brian ichter, Xia, F., Chi, E.~H., Le, Q.~V., and Zhou, D.}
\newblock Chain of thought prompting elicits reasoning in large language models.
\newblock In {\em Advances in Neural Information Processing Systems\/} (2022), A.~H. Oh, A.~Agarwal, D.~Belgrave, and K.~Cho, Eds.

\bibitem{wenzek-etal-2020-ccnet}
{\sc Wenzek, G., Lachaux, M.-A., Conneau, A., Chaudhary, V., Guzm{\'a}n, F., Joulin, A., and Grave, E.}
\newblock {CCN}et: Extracting high quality monolingual datasets from web crawl data.
\newblock In {\em Proceedings of the Twelfth Language Resources and Evaluation Conference\/} (Marseille, France, May 2020), N.~Calzolari, F.~B{\'e}chet, P.~Blache, K.~Choukri, C.~Cieri, T.~Declerck, S.~Goggi, H.~Isahara, B.~Maegaard, J.~Mariani, H.~Mazo, A.~Moreno, J.~Odijk, and S.~Piperidis, Eds., European Language Resources Association, pp.~4003--4012.

\bibitem{xia2024less}
{\sc Xia, M., Malladi, S., Gururangan, S., Arora, S., and Chen, D.}
\newblock Less: Selecting influential data for instruction tuning.

\bibitem{xie2023dsir}
{\sc Xie, S.~M., Santurkar, S., Ma, T., and Liang, P.}
\newblock Data selection for language models via importance resampling.
\newblock {\em arXiv preprint arXiv:2302.03169\/} (2023).

\bibitem{DBLP:journals/corr/yao-nlp-scratch}
{\sc Yao, X., Zheng, Y., Yang, X., and Yang, Z.}
\newblock {NLP} from scratch without large-scale pretraining: {A} simple and efficient framework.
\newblock {\em CoRR abs/2111.04130\/} (2021).

\bibitem{pmlr-v162-yao22c-from-scratch}
{\sc Yao, X., Zheng, Y., Yang, X., and Yang, Z.}
\newblock {NLP} from scratch without large-scale pretraining: A simple and efficient framework.
\newblock In {\em Proceedings of the 39th International Conference on Machine Learning\/} (17--23 Jul 2022), K.~Chaudhuri, S.~Jegelka, L.~Song, C.~Szepesvari, G.~Niu, and S.~Sabato, Eds., vol.~162 of {\em Proceedings of Machine Learning Research}, PMLR, pp.~25438--25451.

\bibitem{zhang2024instruction}
{\sc Zhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S., Li, J., Hu, R., Zhang, T., Wu, F., and Wang, G.}
\newblock Instruction tuning for large language models: A survey, 2024.

\bibitem{DBLP:journals/corr/ZhangZL15}
{\sc Zhang, X., Zhao, J.~J., and LeCun, Y.}
\newblock Character-level convolutional networks for text classification.
\newblock {\em CoRR abs/1509.01626\/} (2015).

\end{thebibliography}
