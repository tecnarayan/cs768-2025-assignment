\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abnar \& Zuidema(2020)Abnar and
  Zuidema]{abnar-zuidema-2020-quantifying}
Abnar, S. and Zuidema, W.
\newblock Quantifying attention flow in transformers.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, 2020.

\bibitem[Baevski \& Auli(2018)Baevski and Auli]{baevski2018adaptive}
Baevski, A. and Auli, M.
\newblock Adaptive input representations for neural language modeling.
\newblock \emph{arXiv preprint arXiv:1809.10853}, 2018.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Bahdanau, D., Cho, K., and Bengio, Y.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Bapna et~al.(2018)Bapna, Chen, Firat, Cao, and Wu]{bapna2018training}
Bapna, A., Chen, M.~X., Firat, O., Cao, Y., and Wu, Y.
\newblock Training deeper neural machine translation models with transparent
  attention.
\newblock \emph{arXiv preprint arXiv:1808.07561}, 2018.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and
  Zagoruyko]{carion2020end}
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., and Zagoruyko,
  S.
\newblock End-to-end object detection with transformers.
\newblock \emph{arXiv preprint arXiv:2005.12872}, 2020.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{chelba2013one}
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and
  Robinson, T.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock \emph{arXiv preprint arXiv:1312.3005}, 2013.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and
  Sutskever]{chen2020generative}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I.
\newblock Generative pretraining from pixels.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1691--1703. PMLR, 2020.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
  Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dehghani et~al.(2018)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{dehghani2018universal}
Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, {\L}.
\newblock Universal transformers.
\newblock \emph{arXiv preprint arXiv:1807.03819}, 2018.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[He et~al.(2020)He, Ravula, Kanagal, and Ainslie]{he2020realformer}
He, R., Ravula, A., Kanagal, B., and Ainslie, J.
\newblock Realformer: Transformer likes residual attention.
\newblock \emph{arXiv e-prints}, pp.\  arXiv--2012, 2020.

\bibitem[He et~al.(2018)He, Tan, Xia, He, Qin, Chen, and Liu]{he2018layer}
He, T., Tan, X., Xia, Y., He, D., Qin, T., Chen, Z., and Liu, T.-Y.
\newblock Layer-wise coordination between encoder and decoder for neural
  machine translation.
\newblock In \emph{Proceedings of the 32Nd International Conference on Neural
  Information Processing Systems}, pp.\  7955--7965, 2018.

\bibitem[Heek et~al.(2020)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van {Z}ee, M.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2020.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Huang, G., Liu, Z., Van Der~Maaten, L., and Weinberger, K.~Q.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  4700--4708, 2017.

\bibitem[Kolesnikov et~al.(2019)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2019big}
Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and
  Houlsby, N.
\newblock Big transfer (bit): General visual representation learning.
\newblock \emph{arXiv preprint arXiv:1912.11370}, 2019.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo2018sentencepiece}
Kudo, T. and Richardson, J.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}, 2018.

\bibitem[Kumar et~al.(2021)Kumar, Weissenborn, and
  Kalchbrenner]{kumar2021colorization}
Kumar, M., Weissenborn, D., and Kalchbrenner, N.
\newblock Colorization transformer.
\newblock \emph{arXiv preprint arXiv:2102.04432}, 2021.

\bibitem[Langley(2000)]{langley00}
Langley, P.
\newblock Crafting papers on machine learning.
\newblock In Langley, P. (ed.), \emph{Proceedings of the 17th International
  Conference on Machine Learning (ICML 2000)}, pp.\  1207--1216, Stanford, CA,
  2000. Morgan Kaufmann.

\bibitem[Liu et~al.(2020)Liu, Duh, Liu, and Gao]{liu2020very}
Liu, X., Duh, K., Liu, L., and Gao, J.
\newblock Very deep transformers for neural machine translation.
\newblock \emph{arXiv preprint arXiv:2008.07772}, 2020.

\bibitem[Nilsback \& Zisserman(2008)Nilsback and
  Zisserman]{nilsback2008automated}
Nilsback, M.-E. and Zisserman, A.
\newblock Automated flower classification over a large number of classes.
\newblock In \emph{2008 Sixth Indian Conference on Computer Vision, Graphics \&
  Image Processing}, pp.\  722--729. IEEE, 2008.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and
  Auli]{ott-etal-2018-scaling}
Ott, M., Edunov, S., Grangier, D., and Auli, M.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation:
  Research Papers}, pp.\  1--9, Brussels, Belgium, October 2018. Association
  for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6301}.
\newblock URL \url{https://www.aclweb.org/anthology/W18-6301}.

\bibitem[Parikh et~al.(2016)Parikh, T{\"a}ckstr{\"o}m, Das, and
  Uszkoreit]{parikh2016decomposable}
Parikh, A.~P., T{\"a}ckstr{\"o}m, O., Das, D., and Uszkoreit, J.
\newblock A decomposable attention model for natural language inference.
\newblock \emph{arXiv preprint arXiv:1606.01933}, 2016.

\bibitem[Parkhi et~al.(2012)Parkhi, Vedaldi, Zisserman, and
  Jawahar]{parkhi2012cats}
Parkhi, O.~M., Vedaldi, A., Zisserman, A., and Jawahar, C.
\newblock Cats and dogs.
\newblock In \emph{2012 IEEE conference on computer vision and pattern
  recognition}, pp.\  3498--3505. IEEE, 2012.

\bibitem[Post(2018)]{post2018call}
Post, M.
\newblock A call for clarity in reporting bleu scores.
\newblock \emph{arXiv preprint arXiv:1804.08771}, 2018.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[So et~al.(2019)So, Liang, and Le]{so2019evolved}
So, D.~R., Liang, C., and Le, Q.~V.
\newblock The evolved transformer.
\newblock \emph{arXiv preprint arXiv:1901.11117}, 2019.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015highway}
Srivastava, R.~K., Greff, K., and Schmidhuber, J.
\newblock Highway networks.
\newblock \emph{arXiv preprint arXiv:1505.00387}, 2015.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017revisiting}
Sun, C., Shrivastava, A., Singh, S., and Gupta, A.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, 2017.

\bibitem[Tay et~al.(2018)Tay, Tuan, Hui, and Su]{tay2018densely}
Tay, Y., Tuan, L.~A., Hui, S.~C., and Su, J.
\newblock Densely connected attention propagation for reading comprehension.
\newblock \emph{arXiv preprint arXiv:1811.04210}, 2018.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{tay2020long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{arXiv preprint arXiv:2011.04006}, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Dehghani, Bahri, and
  Metzler]{tay2020efficient}
Tay, Y., Dehghani, M., Bahri, D., and Metzler, D.
\newblock Efficient transformers: A survey.
\newblock \emph{arXiv preprint arXiv:2009.06732}, 2020{\natexlab{b}}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Zaheer, M., Guruganesh, G., Dubey, A., Ainslie, J., Alberti, C., Ontanon, S.,
  Pham, P., Ravula, A., Wang, Q., Yang, L., et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock \emph{arXiv preprint arXiv:2007.14062}, 2020.

\end{thebibliography}
