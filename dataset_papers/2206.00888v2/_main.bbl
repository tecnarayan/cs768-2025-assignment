\begin{thebibliography}{10}

\bibitem{baevski2022data2vec}
Alexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael
  Auli.
\newblock Data2vec: A general framework for self-supervised learning in speech,
  vision and language.
\newblock {\em arXiv preprint arXiv:2202.03555}, 2022.

\bibitem{baevski2020wav2vec}
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.
\newblock Wav2vec 2.0: A framework for self-supervised learning of speech
  representations.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12449--12460, 2020.

\bibitem{brock2021high}
Andy Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In {\em International Conference on Machine Learning}, pages
  1059--1071. PMLR, 2021.

\bibitem{burchi2021efficient}
Maxime Burchi and Valentin Vielzeuf.
\newblock {Efficient Conformer}: Progressive downsampling and grouped attention
  for automatic speech recognition.
\newblock {\em arXiv preprint arXiv:2109.01163}, 2021.

\bibitem{chang2020end}
Xuankai Chang, Aswin~Shanmugam Subramanian, Pengcheng Guo, Shinji Watanabe,
  Yuya Fujita, and Motoi Omachi.
\newblock {End-to-end ASR} with adaptive span self-attention.
\newblock In {\em INTERSPEECH}, pages 3595--3599, 2020.

\bibitem{chen2021wavlm}
Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu~Wu, Shujie Liu, Zhuo Chen, Jinyu
  Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, et~al.
\newblock {WavLM}: Large-scale self-supervised pre-training for full stack
  speech processing.
\newblock {\em arXiv preprint arXiv:2110.13900}, 2021.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional {Transformers} for
  language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{dollar2021fast}
Piotr Doll{\'a}r, Mannat Singh, and Ross Girshick.
\newblock Fast and accurate model scaling.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 924--932, 2021.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: {Transformers} for image recognition
  at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fan2021multiscale}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock {Multiscale Vision Transformers}.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 6824--6835, 2021.

\bibitem{garofolo1993timit}
John~S Garofolo.
\newblock Timit acoustic phonetic continuous speech corpus.
\newblock {\em Linguistic Data Consortium, 1993}, 1993.

\bibitem{geng2018hardware2}
Xue Geng, Jie Lin, Bin Zhao, Anmin Kong, Mohamed M~Sabry Aly, and Vijay
  Chandrasekhar.
\newblock Hardware-aware softmax approximation for deep neural networks.
\newblock In {\em Asian Conference on Computer Vision}, pages 107--122.
  Springer, 2018.

\bibitem{geng2018hardware1}
Xue Geng, Jie Lin, Bin Zhao, Zhe Wang, Mohamed M~Sabry Aly, and Vijay
  Chandrasekhar.
\newblock Hardware-aware exponential approximation for deep neural network.
\newblock 2018.

\bibitem{graves2012sequence}
Alex Graves.
\newblock Sequence transduction with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1211.3711}, 2012.

\bibitem{graves2006connectionist}
Alex Graves, Santiago Fern{\'a}ndez, Faustino Gomez, and J{\"u}rgen
  Schmidhuber.
\newblock Connectionist temporal classification: labelling unsegmented sequence
  data with recurrent neural networks.
\newblock In {\em Proceedings of the 23rd international conference on Machine
  learning}, pages 369--376, 2006.

\bibitem{gulati2020conformer}
Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu~Zhang, Jiahui Yu,
  Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang.
\newblock Conformer: Convolution-augmented {Transformer} for speech
  recognition.
\newblock {\em arXiv preprint arXiv:2005.08100}, 2020.

\bibitem{guo2021recent}
Pengcheng Guo et~al.
\newblock Recent developments on {ESPNet} toolkit boosted by {Conformer}.
\newblock In {\em ICASSP 2021-2021 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 5874--5878. IEEE, 2021.

\bibitem{han2019state}
Kyu~J Han, Ramon Prieto, and Tao Ma.
\newblock State-of-the-art speech recognition using multi-stream self-attention
  with dilated 1d convolutions.
\newblock In {\em 2019 IEEE Automatic Speech Recognition and Understanding
  Workshop (ASRU)}, pages 54--61. IEEE, 2019.

\bibitem{han2020contextnet}
Wei Han, Zhengdong Zhang, Yu~Zhang, Jiahui Yu, Chung-Cheng Chiu, James Qin,
  Anmol Gulati, Ruoming Pang, and Yonghui Wu.
\newblock {ContextNet}: Improving convolutional neural networks for automatic
  speech recognition with global context.
\newblock {\em arXiv preprint arXiv:2005.03191}, 2020.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock {MobileNets}: Efficient convolutional neural networks for mobile
  vision applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{hsu2021hubert}
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung~Hubert Tsai, Kushal Lakhotia, Ruslan
  Salakhutdinov, and Abdelrahman Mohamed.
\newblock {HuBERT}: Self-supervised speech representation learning by masked
  prediction of hidden units.
\newblock {\em IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29:3451--3460, 2021.

\bibitem{hu2018squeeze}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-{Excitation} networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141, 2018.

\bibitem{karita2019comparative}
Shigeki Karita et~al.
\newblock A comparative study on {Transformer vs RNN} in speech applications.
\newblock In {\em 2019 IEEE Automatic Speech Recognition and Understanding
  Workshop (ASRU)}, pages 449--456. IEEE, 2019.

\bibitem{squezeformer_github}
Sehoon Kim.
\newblock https://github.com/kssteven418/squeezeformer.

\bibitem{kim2021bert}
Sehoon Kim, Amir Gholami, Zhewei Yao, Michael~W Mahoney, and Kurt Keutzer.
\newblock {I-BERT}: Integer-only bert quantization.
\newblock In {\em International conference on machine learning}, pages
  5506--5518. PMLR, 2021.

\bibitem{kriman2020quartznet}
Samuel Kriman, Stanislav Beliaev, Boris Ginsburg, Jocelyn Huang, Oleksii
  Kuchaiev, Vitaly Lavrukhin, Ryan Leary, Jason Li, and Yang Zhang.
\newblock {QuartzNet}: Deep automatic speech recognition with 1d time-channel
  separable convolutions.
\newblock In {\em ICASSP}, pages 6124--6128. IEEE, 2020.

\bibitem{kudo2018sentencepiece}
Taku Kudo and John Richardson.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock {\em arXiv preprint arXiv:1808.06226}, 2018.

\bibitem{li2019jasper}
Jason Li, Vitaly Lavrukhin, Boris Ginsburg, Ryan Leary, Oleksii Kuchaiev,
  Jonathan~M Cohen, Huyen Nguyen, and Ravi~Teja Gadde.
\newblock Jasper: An end-to-end convolutional neural acoustic model.
\newblock {\em arXiv preprint arXiv:1904.03288}, 2019.

\bibitem{li2021improved}
Yanghao Li, Chao-Yuan Wu, Haoqi Fan, Karttikeya Mangalam, Bo~Xiong, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Improved {Multiscale Vision Transformers} for classification and
  detection.
\newblock {\em arXiv preprint arXiv:2112.01526}, 2021.

\bibitem{likhomanenko2020rethinking}
Tatiana Likhomanenko, Qiantong Xu, Vineel Pratap, Paden Tomasello, Jacob Kahn,
  Gilad Avidov, Ronan Collobert, and Gabriel Synnaeve.
\newblock Rethinking evaluation in {ASR}: {Are} our models robust enough?
\newblock {\em arXiv preprint arXiv:2010.11745}, 2020.

\bibitem{liu2021improving}
Chunxi Liu, Frank Zhang, Duc Le, Suyoun Kim, Yatharth Saraf, and Geoffrey
  Zweig.
\newblock Improving {RNN Transducer} based {ASR} with auxiliary tasks.
\newblock In {\em 2021 IEEE Spoken Language Technology Workshop (SLT)}, pages
  172--179. IEEE, 2021.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{lu2019understanding}
Yiping Lu, Zhuohan Li, Di~He, Zhiqing Sun, Bin Dong, Tao Qin, Liwei Wang, and
  Tie-Yan Liu.
\newblock Understanding and improving {Transformer} from a multi-particle
  dynamic system point of view.
\newblock {\em arXiv preprint arXiv:1906.02762}, 2019.

\bibitem{luscher2019rwth}
Christoph L{\"u}scher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel,
  Albert Zeyer, Ralf Schl{\"u}ter, and Hermann Ney.
\newblock {RWTH ASR Systems for LibriSpeech}: Hybrid vs attention--w/o data
  augmentation.
\newblock {\em arXiv preprint arXiv:1905.03072}, 2019.

\bibitem{majumdar2021citrinet}
Somshubra Majumdar, Jagadeesh Balam, Oleksii Hrinchuk, Vitaly Lavrukhin, Vahid
  Noroozi, and Boris Ginsburg.
\newblock Citrinet: Closing the gap between non-autoregressive and
  autoregressive end-to-end models for automatic speech recognition.
\newblock {\em arXiv preprint arXiv:2104.01721}, 2021.

\bibitem{ng2021pushing}
Edwin~G Ng, Chung-Cheng Chiu, Yu~Zhang, and William Chan.
\newblock Pushing the limits of non-autoregressive speech recognition.
\newblock {\em arXiv preprint arXiv:2104.03416}, 2021.

\bibitem{nemo}
{NVDIA Nemo}.
\newblock https://github.com/nvidia/nemo.

\bibitem{nvdla_primer}
{NVDLA Primer}.
\newblock http://nvdla.org/primer.html, 2021.

\bibitem{pan2020asapp}
Jing Pan, Joshua Shapiro, Jeremy Wohlwend, Kyu~J Han, Tao Lei, and Tao Ma.
\newblock {ASAPP-ASR: Multistream CNN and self-attentive SRU for SOTA speech
  recognition}.
\newblock {\em arXiv preprint arXiv:2005.10469}, 2020.

\bibitem{panayotov2015librispeech}
Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur.
\newblock {Librispeech: an ASR corpus based on public domain audio books}.
\newblock In {\em 2015 IEEE international conference on acoustics, speech and
  signal processing (ICASSP)}, pages 5206--5210. IEEE, 2015.

\bibitem{park2019specaugment}
Daniel~S Park, William Chan, Yu~Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin~D
  Cubuk, and Quoc~V Le.
\newblock Specaugment: {A} simple data augmentation method for automatic speech
  recognition.
\newblock {\em arXiv preprint arXiv:1904.08779}, 2019.

\bibitem{perslev2019u}
Mathias Perslev, Michael Jensen, Sune Darkner, Poul~J{\o}rgen Jennum, and
  Christian Igel.
\newblock {U-Time}: A fully convolutional network for time series segmentation
  applied to sleep staging.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{radford2018improving}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training.
\newblock 2018.

\bibitem{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-{Net}: Convolutional networks for biomedical image segmentation.
\newblock In {\em International Conference on Medical image computing and
  computer-assisted intervention}, pages 234--241. Springer, 2015.

\bibitem{shim2021understanding}
Kyuhong Shim, Jungwook Choi, and Wonyong Sung.
\newblock Understanding the role of self attention for efficient speech
  recognition.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{sifre2014rigid}
Laurent Sifre and St{\'e}phane Mallat.
\newblock Rigid-motion scattering for texture classification.
\newblock {\em arXiv preprint arXiv:1403.1687}, 2014.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image {Transformers} \& distillation through
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  10347--10357. PMLR, 2021.

\bibitem{dwsconv}
Vincent Vanhoucke.
\newblock Learning visual representations at scale.
\newblock 2014.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wang2021unispeech}
Chengyi Wang, Yu~Wu, Yao Qian, Kenichi Kumatani, Shujie Liu, Furu Wei, Michael
  Zeng, and Xuedong Huang.
\newblock Unispeech: Unified speech representation learning with labeled and
  unlabeled data.
\newblock In {\em International Conference on Machine Learning}, pages
  10937--10947. PMLR, 2021.

\bibitem{wang2022deepnet}
Hongyu Wang, Shuming Ma, Li~Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei.
\newblock {DeepNet: Scaling Transformers} to 1,000 layers.
\newblock {\em arXiv preprint arXiv:2203.00555}, 2022.

\bibitem{wang2020transformer}
Yongqiang Wang et~al.
\newblock Transformer-based acoustic modeling for hybrid speech recognition.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 6874--6878. IEEE, 2020.

\bibitem{yu2021nn}
Joonsang Yu, Junki Park, Seongmin Park, Minsoo Kim, Sihwa Lee, Dong~Hyun Lee,
  and Jungwook Choi.
\newblock {NN-LUT}: Neural approximation of non-linear operations for efficient
  {Transformer} inference.
\newblock {\em arXiv preprint arXiv:2112.02191}, 2021.

\bibitem{zhang2020faster}
Frank Zhang, Yongqiang Wang, Xiaohui Zhang, Chunxi Liu, Yatharth Saraf, and
  Geoffrey Zweig.
\newblock Faster, simpler and more accurate hybrid asr systems using
  wordpieces.
\newblock {\em arXiv preprint arXiv:2005.09150}, 2020.

\bibitem{zhang2020transformer}
Qian Zhang, Han Lu, Hasim Sak, Anshuman Tripathi, Erik McDermott, Stephen Koo,
  and Shankar Kumar.
\newblock Transformer transducer: A streamable speech recognition model with
  {Transformer} encoders and {RNN-T} loss.
\newblock In {\em ICASSP 2020-2020 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 7829--7833. IEEE, 2020.

\bibitem{zhang2020stochastic}
Shucong Zhang, Erfan Loweimi, Peter Bell, and Steve Renals.
\newblock Stochastic attention head removal: A simple and effective method for
  improving {Transformer} based {ASR} models.
\newblock {\em arXiv preprint arXiv:2011.04004}, 2020.

\bibitem{zhang2021usefulness}
Shucong Zhang, Erfan Loweimi, Peter Bell, and Steve Renals.
\newblock On the usefulness of self-attention for automatic speech recognition
  with {Transformers}.
\newblock In {\em 2021 IEEE Spoken Language Technology Workshop (SLT)}, pages
  89--96. IEEE, 2021.

\bibitem{zhang2021benchmarking}
Xiaohui Zhang, Frank Zhang, Chunxi Liu, Kjell Schubert, Julian Chan, Pradyot
  Prakash, Jun Liu, Ching-Feng Yeh, Fuchun Peng, Yatharth Saraf, et~al.
\newblock Benchmarking lf-mmi, ctc and rnn-t criteria for streaming asr.
\newblock In {\em 2021 IEEE Spoken Language Technology Workshop (SLT)}, pages
  46--51. IEEE, 2021.

\bibitem{zhang2017towards}
Ying Zhang, Mohammad Pezeshki, Phil{\'e}mon Brakel, Saizheng Zhang, Cesar
  Laurent~Yoshua Bengio, and Aaron Courville.
\newblock Towards end-to-end speech recognition with deep convolutional neural
  networks.
\newblock {\em arXiv preprint arXiv:1701.02720}, 2017.

\bibitem{zhang2020pushing}
Yu~Zhang, James Qin, Daniel~S Park, Wei Han, Chung-Cheng Chiu, Ruoming Pang,
  Quoc~V Le, and Yonghui Wu.
\newblock Pushing the limits of semi-supervised learning for automatic speech
  recognition.
\newblock {\em arXiv preprint arXiv:2010.10504}, 2020.

\end{thebibliography}
