\begin{thebibliography}{10}

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{gupta2015model}
Suyog Gupta, Wei Zhang, and Josh Milthorpe.
\newblock Model accuracy and runtime tradeoff in distributed deep learning.
\newblock {\em arXiv preprint arXiv:1509.04210}, 2015.

\bibitem{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1223--1231, 2012.

\bibitem{zhang2015deep}
Sixin Zhang, Anna~E Choromanska, and Yann LeCun.
\newblock Deep learning with elastic averaging sgd.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  685--693, 2015.

\bibitem{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock {\em arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{blot2016gossip}
Michael Blot, David Picard, Matthieu Cord, and Nicolas Thome.
\newblock Gossip training for deep learning.
\newblock {\em arXiv preprint arXiv:1611.09726}, 2016.

\bibitem{jin2016scale}
Peter~H Jin, Qiaochu Yuan, Forrest Iandola, and Kurt Keutzer.
\newblock How to scale distributed deep learning?
\newblock {\em arXiv preprint arXiv:1611.04581}, 2016.

\bibitem{MRWG11}
Kushal Mukherjee, Asok Ray, Thomas Wettergren, Shalabh Gupta, and Shashi Phoha.
\newblock Real-time adaptation of decision thresholds in sensor networks for
  detection of moving targets.
\newblock {\em Automatica}, 47(1):185 -- 191, 2011.

\bibitem{LGLPS17}
Chao Liu, Yongqiang Gong, Simon Laflamme, Brent Phares, and Soumik Sarkar.
\newblock Bridge damage detection using spatiotemporal patterns extracted from
  dense sensor network.
\newblock {\em Measurement Science and Technology}, 28(1):014011, 2017.

\bibitem{CH10}
H.-L. Choi and J.~P. How.
\newblock Continuous trajectory planning of mobile sensors for informative
  forecasting.
\newblock {\em Automatica}, 46(8):1266--1275, 2010.

\bibitem{JCSR16}
D.~K. Jha, P.~Chattopadhyay, S.~Sarkar, and A.~Ray.
\newblock Path planning in gps-denied environments with collective intelligence
  of distributed sensor networks.
\newblock {\em International Journal of Control}, 89, 2016.

\bibitem{chilimbi2014project}
Trishul~M Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman.
\newblock Project adam: Building an efficient and scalable deep learning
  training system.
\newblock In {\em OSDI}, volume~14, pages 571--582, 2014.

\bibitem{strom2015scalable}
Nikko Strom.
\newblock Scalable distributed dnn training using commodity gpu cloud
  computing.
\newblock In {\em INTERSPEECH}, volume~7, page~10, 2015.

\bibitem{su2015experiments}
Hang Su and Haoyu Chen.
\newblock Experiments on parallel training of deep neural network using model
  averaging.
\newblock {\em arXiv preprint arXiv:1507.01239}, 2015.

\bibitem{zhang2015staleness}
Wei Zhang, Suyog Gupta, Xiangru Lian, and Ji~Liu.
\newblock Staleness-aware async-sgd for distributed deep learning.
\newblock {\em arXiv preprint arXiv:1511.05950}, 2015.

\bibitem{de2016efficient}
Soham De and Tom Goldstein.
\newblock Efficient distributed sgd with variance reduction.
\newblock In {\em Data Mining (ICDM), 2016 IEEE 16th International Conference
  on}, pages 111--120. IEEE, 2016.

\bibitem{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock {\em arXiv preprint arXiv:1702.08704}, 2017.

\bibitem{lan2017communication}
Guanghui Lan, Soomin Lee, and Yi~Zhou.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock {\em arXiv preprint arXiv:1701.03961}, 2017.

\bibitem{hajinezhadzenith}
Davood Hajinezhad, Mingyi Hong, and Alfredo Garcia.
\newblock Zenith: A zeroth-order distributed algorithm for multi-agent
  nonconvex optimization.

\bibitem{nedic2009distributed}
Angelia Nedic and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48--61, 2009.

\bibitem{zeng2016nonconvex}
Jinshan Zeng and Wotao Yin.
\newblock On nonconvex decentralized gradient descent.
\newblock {\em arXiv preprint arXiv:1608.05766}, 2016.

\bibitem{NO16}
Angelia Nedić and Alex Olshevsky.
\newblock Stochastic gradient-push for strongly convex functions on
  time-varying directed graphs.
\newblock {\em IEEE Transactions on Automatic Control 61.12}, pages 3936--3947,
  2016.

\bibitem{polyak1964some}
Boris~T Polyak.
\newblock Some methods of speeding up the convergence of iteration methods.
\newblock {\em USSR Computational Mathematics and Mathematical Physics},
  4(5):1--17, 1964.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{bottou2016optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em arXiv preprint arXiv:1606.04838}, 2016.

\bibitem{yuan2013convergence}
Kun Yuan, Qing Ling, and Wotao Yin.
\newblock On the convergence of decentralized gradient descent.
\newblock {\em arXiv preprint arXiv:1310.7063}, 2013.

\bibitem{chollet2015keras}
Fran\c{c}ois Chollet.
\newblock Keras.
\newblock \url{https://github.com/fchollet/keras}, 2015.

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen,
  Craig Citro, Greg~S Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, et~al.
\newblock Tensorflow: Large-scale machine learning on heterogeneous distributed
  systems.
\newblock {\em arXiv preprint arXiv:1603.04467}, 2016.

\bibitem{ZBHR16}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em CoRR}, abs/1611.03530, 2016.

\bibitem{nedic2015distributed}
Angelia Nedi{\'c} and Alex Olshevsky.
\newblock Distributed optimization over time-varying directed graphs.
\newblock {\em IEEE Transactions on Automatic Control}, 60(3):601--615, 2015.

\bibitem{ram2012}
S.~Ram, A.~Nedic, and V.~Veeravalli.
\newblock A new class of distributed optimization algorithms: application to
  regression of distributed data.
\newblock {\em Optimization Methods and Software}, 27(1):71– 88, 2012.

\end{thebibliography}
