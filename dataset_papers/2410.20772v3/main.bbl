\begin{thebibliography}{10}

\bibitem{RELU}
Abien~Fred Agarap.
\newblock Deep learning using rectified linear units (relu).
\newblock {\em arXiv preprint arXiv:1803.08375}, 2018.

\bibitem{ahmed2010empirical}
Nesreen~K Ahmed, Amir~F Atiya, Neamat~El Gayar, and Hisham El-Shishiny.
\newblock An empirical comparison of machine learning models for time series forecasting.
\newblock {\em Econometric reviews}, 29(5-6):594--621, 2010.

\bibitem{bloomfield2004fourier}
Peter Bloomfield.
\newblock {\em Fourier analysis of time series: an introduction}.
\newblock John Wiley \& Sons, 2004.

\bibitem{boser1992training}
Bernhard~E Boser, Isabelle~M Guyon, and Vladimir~N Vapnik.
\newblock A training algorithm for optimal margin classifiers.
\newblock In {\em Proceedings of the fifth annual workshop on Computational learning theory}, pages 144--152, 1992.

\bibitem{box1968some}
George~EP Box and Gwilym~M Jenkins.
\newblock Some recent advances in forecasting and control.
\newblock {\em Journal of the Royal Statistical Society. Series C (Applied Statistics)}, 17(2):91--109, 1968.

\bibitem{EnergyData}
Luis~M Candanedo, V{\'e}ronique Feldheim, and Dominique Deramaix.
\newblock Data driven prediction models of energy use of appliances in a low-energy house.
\newblock {\em Energy and buildings}, 140:81--97, 2017.

\bibitem{TiDE}
Abhimanyu Das, Weihao Kong, Andrew Leach, Shaan Mathur, Rajat Sen, and Rose Yu.
\newblock Long-term forecasting with tide: Time-series dense encoder.
\newblock {\em arXiv preprint arXiv:2304.08424}, 2023.

\bibitem{TSMixer}
Vijay Ekambaram, Arindam Jati, Nam Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock Tsmixer: Lightweight mlp-mixer model for multivariate time series forecasting.
\newblock In {\em Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining}, pages 459--469, 2023.

\bibitem{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock {\em Annals of statistics}, pages 1189--1232, 2001.

\bibitem{GaussianProcess}
Agathe Girard, Carl Rasmussen, Joaquin~Q Candela, and Roderick Murray-Smith.
\newblock Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting.
\newblock {\em Advances in neural information processing systems}, 15, 2002.

\bibitem{GELU}
Dan Hendrycks and Kevin Gimpel.
\newblock Gaussian error linear units (gelus).
\newblock {\em arXiv preprint arXiv:1606.08415}, 2016.

\bibitem{hewage2020temporal}
Pradeep Hewage, Ardhendu Behera, Marcello Trovati, Ella Pereira, Morteza Ghahremani, Francesco Palmieri, and Yonghuai Liu.
\newblock Temporal convolutional neural (tcn) network for an effective weather forecasting using time-series data from the local weather station.
\newblock {\em Soft Computing}, 24:16453--16482, 2020.

\bibitem{hinton2006fast}
Geoffrey~E Hinton, Simon Osindero, and Yee-Whye Teh.
\newblock A fast learning algorithm for deep belief nets.
\newblock {\em Neural computation}, 18(7):1527--1554, 2006.

\bibitem{ho1995random}
Tin~Kam Ho.
\newblock Random decision forests.
\newblock In {\em Proceedings of 3rd international conference on document analysis and recognition}, volume~1, pages 278--282. IEEE, 1995.

\bibitem{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem{hsu2019measuring}
Tzu-Ming~Harry Hsu, Hang Qi, and Matthew Brown.
\newblock Measuring the effects of non-identical data distribution for federated visual classification.
\newblock {\em arXiv preprint arXiv:1909.06335}, 2019.

\bibitem{hu1964adaptive}
MJC Hu and Halbert~E Root.
\newblock An adaptive data processing system for weather forecasting.
\newblock {\em Journal of Applied Meteorology and Climatology}, 3(5):513--523, 1964.

\bibitem{hyndman2018forecasting}
Rob~J Hyndman and George Athanasopoulos.
\newblock {\em Forecasting: principles and practice}.
\newblock OTexts, 2018.

\bibitem{9770652}
A.~Kahraman, G.~Yang, and P.~Hou.
\newblock Wind power forecasting using lstm incorporating fourier transformation based denoising technique.
\newblock In {\em 20th International Workshop on Large-Scale Integration of Wind Power into Power Systems as well as on Transmission Networks for Offshore Wind Power Plants (WIW 2021)}, volume 2021, pages 94--98, 2021.

\bibitem{SVM}
Kyoung-jae Kim.
\newblock Financial time series forecasting using support vector machines.
\newblock {\em Neurocomputing}, 55(1-2):307--319, 2003.

\bibitem{RevIN}
Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi, and Jaegul Choo.
\newblock Reversible instance normalization for accurate time-series forecasting against distribution shift.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{Adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{Reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{krapf2018power}
Diego Krapf, Enzo Marinari, Ralf Metzler, Gleb Oshanin, Xinran Xu, and Alessio Squarcini.
\newblock Power spectral density of a single brownian trajectory: what one can and cannot learn from it.
\newblock {\em New Journal of Physics}, 20(2):023029, 2018.

\bibitem{lai2018modeling}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In {\em The 41st international ACM SIGIR conference on research \& development in information retrieval}, pages 95--104, 2018.

\bibitem{LSTNet}
Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
\newblock Modeling long-and short-term temporal patterns with deep neural networks.
\newblock In {\em The 41st international ACM SIGIR conference on research \& development in information retrieval}, pages 95--104, 2018.

\bibitem{LogSparseTransformer}
Shiyang Li, Xiaoyong Jin, Yao Xuan, Xiyou Zhou, Wenhu Chen, Yu-Xiang Wang, and Xifeng Yan.
\newblock Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{RLinear}
Zhe Li, Shiyi Qi, Yiduo Li, and Zenglin Xu.
\newblock Revisiting long-term time series forecasting: An investigation on linear mapping.
\newblock {\em arXiv preprint arXiv:2305.10721}, 2023.

\bibitem{SCINet}
Minhao Liu, Ailing Zeng, Muxi Chen, Zhijian Xu, Qiuxia Lai, Lingna Ma, and Qiang Xu.
\newblock Scinet: Time series modeling and forecasting with sample convolution and interaction.
\newblock {\em Advances in Neural Information Processing Systems}, 35:5816--5828, 2022.

\bibitem{liu2020dstp}
Yeqi Liu, Chuanyang Gong, Ling Yang, and Yingyi Chen.
\newblock Dstp-rnn: A dual-stage two-phase attention-based recurrent neural network for long-term and multivariate time series prediction.
\newblock {\em Expert Systems with Applications}, 143:113082, 2020.

\bibitem{iTransformer}
Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, and Mingsheng Long.
\newblock itransformer: Inverted transformers are effective for time series forecasting.
\newblock {\em arXiv preprint arXiv:2310.06625}, 2023.

\bibitem{Stationary}
Yong Liu, Haixu Wu, Jianmin Wang, and Mingsheng Long.
\newblock Non-stationary transformers: Rethinking the stationarity in time series forecasting.
\newblock {\em NeurIPS}, 2022.

\bibitem{wiki:BrownianNoise}
MickOhrberg.
\newblock Brownian noise --- {Wikipedia}{,} the free encyclopedia, 2024.
\newblock [Online; accessed 9-April-2024].

\bibitem{moreno2023deep}
Fernando Moreno-Pino, Pablo~M Olmos, and Antonio Art{\'e}s-Rodr{\'\i}guez.
\newblock Deep autoregressive models with spectral attention.
\newblock {\em Pattern Recognition}, 133:109014, 2023.

\bibitem{arima}
Brian~K Nelson.
\newblock Time series analysis using autoregressive integrated moving average (arima) models.
\newblock {\em Academic emergency medicine}, 5(7):739--744, 1998.

\bibitem{PatchTST}
Yuqi Nie, Nam~H Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam.
\newblock A time series is worth 64 words: Long-term forecasting with transformers.
\newblock {\em arXiv preprint arXiv:2211.14730}, 2022.

\bibitem{pandiyan2020analysis}
Vigneashwara Pandiyan, Rita Drissi-Daoudi, Sergey Shevchik, Giulio Masinelli, Roland Log{\'e}, and Kilian Wasmer.
\newblock Analysis of time, frequency and time-frequency domain features from acoustic emissions during laser powder-bed fusion process.
\newblock {\em Procedia CIRP}, 94:392--397, 2020.

\bibitem{PyTorch}
Adam Paszke, S.~Gross, Francisco Massa, A.~Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, Alban Desmaison, Andreas K{\"o}pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em NeurIPS}, 2019.

\bibitem{rezaei2021stock}
Hadi Rezaei, Hamidreza Faaljou, and Gholamreza Mansourfar.
\newblock Stock price prediction using deep learning and frequency decomposition.
\newblock {\em Expert Systems with Applications}, 169:114332, 2021.

\bibitem{rosenblatt1957perceptron}
Frank Rosenblatt.
\newblock {\em The perceptron, a perceiving and recognizing automaton Project Para}.
\newblock Cornell Aeronautical Laboratory, 1957.

\bibitem{sagheer2019time}
Alaa Sagheer and Mostafa Kotb.
\newblock Time series forecasting of petroleum production using deep lstm recurrent networks.
\newblock {\em Neurocomputing}, 323:203--213, 2019.

\bibitem{9446858}
Donghwan Song, Adrian~Matias Chung~Baek, and Namhun Kim.
\newblock Forecasting stock market indices using padding-based fourier transform denoising and time series deep learning models.
\newblock {\em IEEE Access}, 9:83786--83796, 2021.

\bibitem{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock {\em Advances in neural information processing systems}, 27, 2014.

\bibitem{Transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{wan2019multivariate}
Renzhuo Wan, Shuping Mei, Jun Wang, Min Liu, and Fan Yang.
\newblock Multivariate temporal convolutional network: A deep neural networks approach for multivariate time series forecasting.
\newblock {\em Electronics}, 8(8):876, 2019.

\bibitem{wen2022transformers}
Qingsong Wen, Tian Zhou, Chaoli Zhang, Weiqi Chen, Ziqing Ma, Junchi Yan, and Liang Sun.
\newblock Transformers in time series: A survey.
\newblock {\em arXiv preprint arXiv:2202.07125}, 2022.

\bibitem{bptt}
Paul~J Werbos.
\newblock Backpropagation through time: what it does and how to do it.
\newblock {\em Proceedings of the IEEE}, 78(10):1550--1560, 1990.

\bibitem{Timesnet}
Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng Long.
\newblock Timesnet: Temporal 2d-variation modeling for general time series analysis.
\newblock {\em ICLR}, 2023.

\bibitem{Autoformer}
Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long.
\newblock Autoformer: Decomposition transformers with auto-correlation for long-term series forecasting.
\newblock {\em Advances in neural information processing systems}, 34:22419--22430, 2021.

\bibitem{FITS}
Zhijian Xu, Ailing Zeng, and Qiang Xu.
\newblock {FITS}: Modeling time series with \$10k\$ parameters.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2024.

\bibitem{WaveForM}
Fuhao Yang, Xin Li, Min Wang, Hongyu Zang, Wei Pang, and Mingzhong Wang.
\newblock Waveform: Graph enhanced wavelet learning for long sequence forecasting of multivariate time series.
\newblock In {\em Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 10754--10761, 2023.

\bibitem{FreTS}
Kun Yi, Qi~Zhang, Wei Fan, Shoujin Wang, Pengyang Wang, Hui He, Ning An, Defu Lian, Longbing Cao, and Zhendong Niu.
\newblock Frequency-domain mlps are more effective learners in time series forecasting.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{DLinear}
Ailing Zeng, Muxi Chen, Lei Zhang, and Qiang Xu.
\newblock Are transformers effective for time series forecasting?
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~37, pages 11121--11128, 2023.

\bibitem{Crossformer}
Yunhao Zhang and Junchi Yan.
\newblock Crossformer: Transformer utilizing cross-dimension dependency for multivariate time series forecasting.
\newblock In {\em The eleventh international conference on learning representations}, 2022.

\bibitem{Informer}
Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang.
\newblock Informer: Beyond efficient transformer for long sequence time-series forecasting.
\newblock In {\em Proceedings of the AAAI conference on artificial intelligence}, volume~35, pages 11106--11115, 2021.

\bibitem{Fedformer}
Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.
\newblock Fedformer: Frequency enhanced decomposed transformer for long-term series forecasting.
\newblock In {\em International conference on machine learning}, pages 27268--27286. PMLR, 2022.

\end{thebibliography}
