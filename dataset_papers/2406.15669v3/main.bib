@article{van_kempen_fast_2023,
	title = {Fast and accurate protein structure search with {Foldseek}},
	volume = {42},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.02.07.479398},
	abstract = {As structure prediction methods are generating millions of publicly available protein structures, searching these databases is becoming a bottleneck. Foldseek aligns the structure of a query protein against a database by describing the amino acid backbone of proteins as sequences over a structural alphabet. Foldseek decreases computation times by four to five orders of magnitude with 86\%, 88\% and 133\% of the sensitivities of DALI, TM-align and CE, respectively.},
	language = {en},
	urldate = {2023-05-09},
	journal = {Nature Biotechnology},
	author = {Van Kempen, Michel and Kim, Stephanie S. and Tumescheit, Charlotte and Mirdita, Milot and Lee, Jeongjae and Gilchrist, Cameron L.M. and Söding, Johannes and Steinegger, Martin},
	year = {2023},
	doi = {10.1101/2022.02.07.479398},
	keywords = {Computational, ML Proteins, notion},
	pages = {243--246},
	file = {Van Kempen et al. - 2022 - Fast and accurate protein structure search with Fo.pdf:/Users/jason/Zotero/storage/BVHZCSJY/Van Kempen et al. - 2022 - Fast and accurate protein structure search with Fo.pdf:application/pdf},
}

@article{merchant_semantic_nodate,
	title = {Semantic mining of functional de novo genes from a genomic language model},
	abstract = {Generative genomics models can design increasingly complex biological systems. However, effectively controlling these models to generate novel sequences with desired functions remains a major challenge. Here, we show that Evo, a 7-billion parameter genomic language model, can perform function-guided design that generalizes beyond natural sequences. By learning semantic relationships across multiple genes, Evo enables a genomic “autocomplete” in which a DNA prompt encoding a desired function instructs the model to generate novel DNA sequences that can be mined for similar functions. We term this process “semantic mining,” which, unlike traditional genome mining, can access a sequence landscape unconstrained by discovered evolutionary innovation. We validate this approach by experimentally testing the activity of generated anti-CRISPR proteins and toxin-antitoxin systems, including de novo genes with no significant homology to any natural protein. Strikingly, in-context protein design with Evo achieves potent activity and high experimental success rates even in the absence of structural hypotheses, known evolutionary conservation, or task-specific fine-tuning. We then use Evo to autocomplete millions of prompts to produce SynGenome, a first-of-its-kind database containing over 120 billion base pairs of AI-generated genomic sequences that enables semantic mining across many possible functions. The semantic mining paradigm enables functional exploration that ventures beyond the observed evolutionary universe.},
	language = {en},
	author = {Merchant, Aditi T and King, Samuel H and Nguyen, Eric and Hie, Brian L},
	keywords = {Annotation, Dataset, DNA, Generative, notion, Protein Language Model},
    journal = {bioRxiv},
    year = {2024},
	file = {Merchant et al. - Semantic mining of functional de novo genes from a.pdf:/Users/jason/Zotero/storage/S5PP5KQK/Merchant et al. - Semantic mining of functional de novo genes from a.pdf:application/pdf},
}

@article{liu_enzymecage_2024,
	title = {{EnzymeCAGE}: {A} {Geometric} {Foundation} {Model} for {Enzyme} {Retrieval} with {Evolutionary} {Insights}},
	language = {en},
	urldate = {2024-12-17},
	journal = {bioRxiv},
	author = {Liu, Yong and Hua, Chenqing and Zeng, Tao and Rao, Jiahua and Zhang, Zhongyue and Wu, Ruibo and Coley, Connor W and Zheng, Shuangjia},
	year = {2024},
	keywords = {Annotation, Enzyme},
	file = {Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf:/Users/jason/Zotero/storage/BG79BRX4/Sarrafzadeh - 1990 - Department of electrical engineering and computer .pdf:application/pdf},
}

@article{rost,
    author = {Rost, Burkhard},
    title = "{Twilight zone of protein sequence alignments}",
    journal = {Protein Engineering, Design and Selection},
    volume = {12},
    number = {2},
    pages = {85-94},
    year = {1999},
    month = {02},
    abstract = "{Sequence alignments unambiguously distinguish between protein pairs of similar and non-similar structure when the pairwise sequence identity is high (\&gt;40\% for long alignments). The signal gets blurred in the twilight zone of 20–35\% sequence identity. Here, more than a million sequence alignments were analysed between protein pairs of known structures to re-define a line distinguishing between true and false positives for low levels of similarity. Four results stood out. (i) The transition from the safe zone of sequence alignment into the twilight zone is described by an explosion of false negatives. More than 95\% of all pairs detected in the twilight zone had different structures. More precisely, above a cut-off roughly corresponding to 30\% sequence identity, 90\% of the pairs were homologous; below 25\% less than 10\% were. (ii) Whether or not sequence homology implied structural identity depended crucially on the alignment length. For example, if 10 residues were similar in an alignment of length 16 (\&gt;60\%), structural similarity could not be inferred. (iii) The `more similar than identical' rule (discarding all pairs for which percentage similarity was lower than percentage identity) reduced false positives significantly. (iv) Using intermediate sequences for finding links between more distant families was almost as successful: pairs were predicted to be homologous when the respective sequence families had proteins in common. All findings are applicable to automatic database searches.}",
    issn = {1741-0126},
    doi = {10.1093/protein/12.2.85},
    url = {https://doi.org/10.1093/protein/12.2.85},
    eprint = {https://academic.oup.com/peds/article-pdf/12/2/85/18542824/120085.pdf},
}




@article{yang_conditional_2024,
	title = {Conditional {Enzyme} {Generation} {Using} {Protein} {Language} {Models} with {Adapters}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2410.03634},
	abstract = {The conditional generation of proteins with desired functions and/or properties is a key goal for generative models. Existing methods based on prompting of language models can generate proteins conditioned on a target functionality, such as a desired enzyme family. However, these methods are limited to simple, tokenized conditioning and have not been shown to generalize to unseen functions. In this study, we propose ProCALM (Protein Conditionally Adapted Language Model), an approach for the conditional generation of proteins using adapters to protein language models. Our specific implementation of ProCALM involves finetuning ProGen2 to incorporate conditioning representations of enzyme function and taxonomy. ProCALM matches existing methods at conditionally generating sequences from target enzyme families. Impressively, it can also generate within the joint distribution of enzymatic function and taxonomy, and it can generalize to rare and unseen enzyme families and taxonomies. Overall, ProCALM is a flexible and computationally efficient approach, and we expect that it can be extended to a wide range of generative language models.},
	language = {en},
	urldate = {2024-10-07},
	journal = {arXiv},
	author = {Yang, Jason and Bhatnagar, Aadyot and Ruffolo, Jeffrey A. and Madani, Ali},
	month = oct,
	year = {2024},
	note = {arXiv:2410.03634 null},
	keywords = {notion},
	file = {Yang et al. - 2024 - Conditional Enzyme Generation Using Protein Langua.pdf:/Users/jason/Zotero/storage/C6UUSCRZ/Yang et al. - 2024 - Conditional Enzyme Generation Using Protein Langua.pdf:application/pdf},
}

@article{paton_generation_2024,
	title = {Generation of connections between protein sequence space and chemical space to enable a predictive model for biocatalysis},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://chemrxiv.org/engage/chemrxiv/article-details/670c192f51558a15eff5c275},
	doi = {10.26434/chemrxiv-2024-w4dtr},
	abstract = {The application of biocatalysis in synthesis has the potential to offer dramatically streamlined routes toward target molecules, exquisite and tunable catalyst-controlled selectivity, as well as more sustainable processes. Despite these advantages, biocatalytic synthetic strategies can be high risk to implement. Successful execution of these approaches requires identifying an enzyme capable of performing chemistry on a specific intermediate in a synthesis which often calls for extensive screening of enzymes and protein engineering. Strategies for predicting which enzyme is most likely to be compatible with a given small molecule have been hindered by the lack of well-studied biocatalytic reactions. The under exploration of connections between chemical and protein sequence spaces constrains navigation between these two landscapes. Herein, this longstanding challenge is overcome in a two-phase effort relying on high throughput experimentation to populate connections between substrate chemical space and biocatalyst sequence space, and the subsequent development of machine learning models that enable the navigation between these two landscapes. Using a curated library of α-ketoglutarate-dependent non-heme iron (NHI) enzymes, the BioCatSet1 dataset was generated to capture the reactivity of each biocatalyst with {\textgreater}100 substrates. In addition to the discovery of novel chemistry, BioCatSet1 was leveraged to develop a predictive workflow that provides a ranked list of enzymes that have the greatest compatibility with a given substrate. To make this tool accessible to the community, we built CATNIP, an open access web interface to our predictive workflows. We anticipate our approach can be readily expanded to additional enzyme and transformation classes, and will derisk the application of biocatalysis in chemical synthesis.},
	language = {en},
	urldate = {2024-10-19},
	journal = {chemRxiv},
	author = {Paton, Alexandra and Boiko, Daniil and Perkins, Jonathan and Cemalovic, Nicholas and Reschützegger, Thiago and Gomes, Gabe and Narayan, Alison},
	month = oct,
	year = {2024},
	keywords = {ML Chemistry, ML Proteins, notion, Substrate},
	file = {Paton et al. - 2024 - Generation of connections between protein sequence.pdf:/Users/jason/Zotero/storage/Q22W4QQ8/Paton et al. - 2024 - Generation of connections between protein sequence.pdf:application/pdf},
}

@article{char_protnote_nodate,
	title = {{ProtNote}: a multimodal method for protein-function annotation},
	abstract = {Understanding the protein sequence-function relationship is essential for advancing protein biology and engineering. However, fewer than 1\% of known protein sequences have human-verified functions. While deep learning methods have demonstrated promise for protein function prediction, current models are limited to predicting only those functions on which they were trained. Here, we introduce ProtNote, a multimodal deep learning model that leverages free-form text to enable both supervised and zero-shot protein function prediction. ProtNote not only maintains near state-of-the-art performance for annotations in its train set, but also generalizes to unseen and novel functions in zero-shot test settings. We envision that ProtNote will enhance protein function discovery by enabling scientists to use free text inputs, without restriction to predefined labels – a necessary capability for navigating the dynamic landscape of protein biology.},
	language = {en},
	author = {Char, Samir and Corley, Nathaniel and Alamdari, Sarah and Yang, Kevin K and Amini, Ava P},
	keywords = {Annotation, notion},
    year = {2024},
    journal = {bioRxiv},
	file = {Char et al. - ProtNote a multimodal method for protein-function.pdf:/Users/jason/Zotero/storage/SY5SPGBF/Char et al. - ProtNote a multimodal method for protein-function.pdf:application/pdf},
}

@article{hamamsy_protein_2023,
	title = {Protein remote homology detection and structural alignment using deep learning},
	issn = {1087-0156, 1546-1696},
	url = {https://www.nature.com/articles/s41587-023-01917-2},
	doi = {10.1038/s41587-023-01917-2},
	abstract = {Abstract
            Exploiting sequence–structure–function relationships in biotechnology requires improved methods for aligning proteins that have low sequence similarity to previously annotated proteins. We develop two deep learning methods to address this gap, TM-Vec and DeepBLAST. TM-Vec allows searching for structure–structure similarities in large sequence databases. It is trained to accurately predict TM-scores as a metric of structural similarity directly from sequence pairs without the need for intermediate computation or solution of structures. Once structurally similar proteins have been identified, DeepBLAST can structurally align proteins using only sequence information by identifying structurally homologous regions between proteins. It outperforms traditional sequence alignment methods and performs similarly to structure-based alignment methods. We show the merits of TM-Vec and DeepBLAST on a variety of datasets, including better identification of remotely homologous proteins compared with state-of-the-art sequence alignment and structure prediction methods.},
	language = {en},
	urldate = {2023-09-07},
	journal = {Nature Biotechnology},
	author = {Hamamsy, Tymor and Morton, James T. and Blackwell, Robert and Berenberg, Daniel and Carriero, Nicholas and Gligorijevic, Vladimir and Strauss, Charlie E. M. and Leman, Julia Koehler and Cho, Kyunghyun and Bonneau, Richard},
	month = sep,
	year = {2023},
	keywords = {notion, ML Proteins, Structure},
	file = {Hamamsy et al. - 2023 - Protein remote homology detection and structural a.pdf:/Users/jason/Zotero/storage/L4XUSYDJ/Hamamsy et al. - 2023 - Protein remote homology detection and structural a.pdf:application/pdf},
}

@article{de_crecy-lagard_limitations_2024,
	title = {Limitations of {Current} {Machine}-{Learning} {Models} in {Predicting} {Enzymatic} {Functions} for {Uncharacterized} {Proteins}},
	copyright = {https://www.biorxiv.org/about/FAQ\#license},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.07.01.601547},
	doi = {10.1101/2024.07.01.601547},
	abstract = {Thirty to seventy percent of proteins in any given genome have no assigned function and have been labeled as the protein unknownme. This large knowledge gap prevents the biological community from fully leveraging the plethora of genomic data that is now available. Machine-learning approaches are showing some promise in propagating functional knowledge from experimentally characterized proteins to the correct set of isofunctional orthologs. However, they largely fail to predict enzymatic functions unseen in the training set, as shown by dissecting the predictions made for 450 enzymes of unknown function from the model bacteria Escherichia coli using the DeepECTransformer platform. Lessons from these failures can help the community develop machine-learning methods that assist domain experts in making testable functional predictions for more members of the uncharacterized proteome.},
	language = {en},
	urldate = {2024-10-16},
	journal = {bioRxiv},
	author = {De Crecy-Lagard, Valerie and Dias, Raquel and Friedberg, Iddo and Yuan, Yifeng and Swairjo, Manal},
	month = jul,
	year = {2024},
	keywords = {Annotation, notion},
	file = {De Crecy-Lagard et al. - 2024 - Limitations of Current Machine-Learning Models in .pdf:/Users/jason/Zotero/storage/D7CCPNAZ/De Crecy-Lagard et al. - 2024 - Limitations of Current Machine-Learning Models in .pdf:application/pdf},
}

@article{zhuang_instructbiomol_2024,
	title = {{InstructBioMol}: {Advancing} {Biomolecule} {Understanding} and {Design} {Following} {Human} {Instructions}},
	shorttitle = {{InstructBioMol}},
	url = {http://arxiv.org/abs/2410.07919},
	abstract = {Understanding and designing biomolecules, such as proteins and small molecules, is central to advancing drug discovery, synthetic biology, and enzyme engineering. Recent breakthroughs in Artificial Intelligence (AI) have revolutionized biomolecular research, achieving remarkable accuracy in biomolecular prediction and design. However, a critical gap remains between AI’s computational power and researchers’ intuition, using natural language to align molecular complexity with human intentions. Large Language Models (LLMs) have shown potential to interpret human intentions, yet their application to biomolecular research remains nascent due to challenges including specialized knowledge requirements, multimodal data integration, and semantic alignment between natural language and biomolecules. To address these limitations, we present InstructBioMol, a novel LLM designed to bridge natural language and biomolecules through a comprehensive any-to-any alignment of natural language, molecules, and proteins. This model can integrate multimodal biomolecules as input, and enable researchers to articulate design goals in natural language, providing biomolecular outputs that meet precise biological needs. Experimental results demonstrate InstructBioMol can understand and design biomolecules following human instructions. Notably, it can generate drug molecules with a 10\% improvement in binding affinity and design enzymes that achieve an ESP Score of 70.4, making it the only method to surpass the enzymesubstrate interaction threshold of 60.0 recommended by the ESP developer. This highlights its potential to transform real-world biomolecular research.},
	language = {en},
	urldate = {2024-10-13},
	journal = {bioRxiv},
	author = {Zhuang, Xiang and Ding, Keyan and Lyu, Tianwen and Jiang, Yinuo and Li, Xiaotong and Xiang, Zhuoyi and Wang, Zeyuan and Qin, Ming and Feng, Kehua and Wang, Jike and Zhang, Qiang and Chen, Huajun},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07919 [cs, q-bio]},
	keywords = {Multimodal, notion},
	file = {Zhuang et al. - 2024 - InstructBioMol Advancing Biomolecule Understandin.pdf:/Users/jason/Zotero/storage/2M97UHYR/Zhuang et al. - 2024 - InstructBioMol Advancing Biomolecule Understandin.pdf:application/pdf},
}

@article{song_accurately_2024,
	title = {Accurately predicting enzyme functions through geometric graph learning on {ESMFold}-predicted structures},
	volume = {15},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-52533-w},
	doi = {10.1038/s41467-024-52533-w},
	language = {en},
	number = {1},
	urldate = {2024-09-20},
	journal = {Nature Communications},
	author = {Song, Yidong and Yuan, Qianmu and Chen, Sheng and Zeng, Yuansong and Zhao, Huiying and Yang, Yuedong},
	month = sep,
	year = {2024},
	keywords = {notion, Annotation},
	pages = {8180},
	file = {Song et al. - 2024 - Accurately predicting enzyme functions through geo.pdf:/Users/jason/Zotero/storage/4TYA5EJ4/Song et al. - 2024 - Accurately predicting enzyme functions through geo.pdf:application/pdf},
}

@article{xiao_proteingpt_2024,
	title = {{ProteinGPT}: {Multimodal} {LLM} for {Protein} {Property} {Prediction} and {Structure} {Understanding}},
	shorttitle = {{ProteinGPT}},
	url = {http://arxiv.org/abs/2408.11363},
	abstract = {Understanding biological processes, drug development, and biotechnological advancements requires detailed analysis of protein structures and sequences, a task in protein research that is inherently complex and time-consuming when performed manually. To streamline this process, we introduce ProteinGPT, a state-of-the-art multi-modal protein chat system, that allows users to upload protein sequences and/or structures for comprehensive protein analysis and responsive inquiries. ProteinGPT seamlessly integrates protein sequence and structure encoders with linear projection layers for precise representation adaptation, coupled with a large language model (LLM) to generate accurate and contextually relevant responses. To train ProteinGPT, we construct a large-scale dataset of 132,092 proteins with annotations, and optimize the instruction-tuning process using GPT-4o. This innovative system ensures accurate alignment between the user-uploaded data and prompts, simplifying protein analysis. Experiments show that ProteinGPT can produce promising responses to proteins and their corresponding questions.},
	language = {en},
	urldate = {2024-08-22},
	journal = {arXiv},
	author = {Xiao, Yijia and Sun, Edward and Jin, Yiqiao and Wang, Qifan and Wang, Wei},
	month = aug,
	year = {2024},
	note = {arXiv:2408.11363 [cs, q-bio]},
	keywords = {Language Model, notion},
	file = {Xiao et al. - 2024 - ProteinGPT Multimodal LLM for Protein Property Pr.pdf:/Users/jason/Zotero/storage/PGEI4S8D/Xiao et al. - 2024 - ProteinGPT Multimodal LLM for Protein Property Pr.pdf:application/pdf},
}

@article{huo_multi-modal_2024,
	title = {Multi-{Modal} {Large} {Language} {Model} {Enables} {Protein} {Function} {Prediction}},
	language = {en},
	author = {Huo, Mingjia and Guo, Han and Cheng, Xingyi and Singh, Digvijay and Rahmani, Hamidreza and Li, Shen and Gerlof, Philipp and Ideker, Trey and Grotjahn, Danielle A and Villa, Elizabeth and Song, Le and Xie, Pengtao},
	year = {2024},
    journal = {arXiv},
	keywords = {notion, Multimodal, Language Model},
	file = {Multi-Modal Large Language Model Enables Protein F.pdf:/Users/jason/Zotero/storage/5NG9Q7G2/Multi-Modal Large Language Model Enables Protein F.pdf:application/pdf},
}

@article{zheng_fedkea_2024,
	title = {{FEDKEA}: {Enzyme} function prediction with a large pretrained protein language model and distance-weighted k-nearest neighbor},
	language = {en},
    journal = {bioRxiv},
	author = {Zheng, Lei and Li, Bowen and Xu, Siqi and Chen, Junnan and Liang, Guanxiang},
	year = {2024},
	keywords = {Annotation, notion},
	file = {FEDKEA Enzyme function prediction with a large pr.pdf:/Users/jason/Zotero/storage/HSXGRBUS/FEDKEA Enzyme function prediction with a large pr.pdf:application/pdf},
}
@article{reisenbauer_catalyzing_2024,
	title = {Catalyzing the future: recent advances in chemical synthesis using enzymes},
	volume = {83},
	issn = {13675931},
	shorttitle = {Catalyzing the future},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1367593124001121},
	doi = {10.1016/j.cbpa.2024.102536},
	language = {en},
	urldate = {2024-10-06},
	journal = {Current Opinion in Chemical Biology},
	author = {Reisenbauer, Julia C. and Sicinski, Kathleen M. and Arnold, Frances H.},
	month = dec,
	year = {2024},
	keywords = {notion},
	pages = {102536},
	file = {Reisenbauer et al. - 2024 - Catalyzing the future recent advances in chemical.pdf:/Users/jason/Zotero/storage/GZPFI58N/Reisenbauer et al. - 2024 - Catalyzing the future recent advances in chemical.pdf:application/pdf},
}

@article{hua_reactzyme_2024,
	title = {Reactzyme: {A} {Benchmark} for {Enzyme}-{Reaction} {Prediction}},
	shorttitle = {Reactzyme},
    journal = {NeurIPS},
	abstract = {Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies. Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes. Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation.},
	language = {en},
	urldate = {2024-08-30},
	author = {Hua, Chenqing and Zhong, Bozitao and Luan, Sitao and Hong, Liang and Wolf, Guy and Precup, Doina and Zheng, Shuangjia},
	month = dec,
	year = {2024},
	keywords = {Benchmark, Dataset, notion},
	file = {Hua et al. - 2024 - Reactzyme A Benchmark for Enzyme-Reaction Predict.pdf:/Users/jason/Zotero/storage/LS25AAYK/Hua et al. - 2024 - Reactzyme A Benchmark for Enzyme-Reaction Predict.pdf:application/pdf},
}

@article{levin_merging_2022,
	title = {Merging enzymatic and synthetic chemistry with computational synthesis planning},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-35422-y},
	doi = {10.1038/s41467-022-35422-y},
	abstract = {Abstract
            
              Synthesis planning programs trained on chemical reaction data can design efficient routes to new molecules of interest, but are limited in their ability to leverage rare chemical transformations. This challenge is acute for enzymatic reactions, which are valuable due to their selectivity and sustainability but are few in number. We report a retrosynthetic search algorithm using two neural network models for retrosynthesis–one covering 7984 enzymatic transformations and one 163,723 synthetic transformations–that balances the exploration of enzymatic and synthetic reactions to identify hybrid synthesis plans. This approach extends the space of retrosynthetic moves by thousands of uniquely enzymatic one-step transformations, discovers routes to molecules for which synthetic or enzymatic searches find none, and designs shorter routes for others. Application to (-)-Δ
              9
              tetrahydrocannabinol (THC) (dronabinol) and R,R-formoterol (arformoterol) illustrates how our strategy facilitates the replacement of metal catalysis, high step counts, or costly enantiomeric resolution with more elegant hybrid proposals.},
	language = {en},
	number = {1},
	urldate = {2024-10-01},
	journal = {Nature Communications},
	author = {Levin, Itai and Liu, Mengjie and Voigt, Christopher A. and Coley, Connor W.},
	month = dec,
	year = {2022},
	keywords = {Dataset, notion},
	pages = {7747},
	file = {Levin et al. - 2022 - Merging enzymatic and synthetic chemistry with com.pdf:/Users/jason/Zotero/storage/CDN4WDZE/Levin et al. - 2022 - Merging enzymatic and synthetic chemistry with com.pdf:application/pdf},
}


@article{bernett_guiding_2024,
	title = {Guiding questions to avoid data leakage in biological machine learning applications},
	volume = {21},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/s41592-024-02362-y},
	doi = {10.1038/s41592-024-02362-y},
	language = {en},
	number = {8},
	urldate = {2024-08-12},
	journal = {Nature Methods},
	author = {Bernett, Judith and Blumenthal, David B. and Grimm, Dominik G. and Haselbeck, Florian and Joeres, Roman and Kalinina, Olga V. and List, Markus},
	month = aug,
	year = {2024},
	keywords = {notion},
	pages = {1444--1453},
	file = {Bernett et al. - 2024 - Guiding questions to avoid data leakage in biologi.pdf:/Users/jason/Zotero/storage/ID9MQZAF/Bernett et al. - 2024 - Guiding questions to avoid data leakage in biologi.pdf:application/pdf},
}

@article{gilchrist_multiple_2024,
	title = {Multiple {Protein} {Structure} {Alignment} at {Scale} with {FoldMason}},
	language = {en},
	author = {Gilchrist, Cameron L M and Mirdita, Milot and Steinegger, Martin},
	year = {2024},
	keywords = {Bioinformatics, Metagenomics, notion},
	file = {Gilchrist et al. - Multiple Protein Structure Alignment at Scale with.pdf:/Users/jason/Zotero/storage/QNC76IQR/Gilchrist et al. - Multiple Protein Structure Alignment at Scale with.pdf:application/pdf},
}

@article{campbell_viper_2024,
	title = {{VIPER}: {A} {General} {Model} for {Prediction} of {Enzyme} {Substrates}},
	shorttitle = {{VIPER}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.06.21.599972},
	doi = {10.1101/2024.06.21.599972},
	abstract = {Enzymes, nature's catalysts, possess remarkable properties such as high stereo-, regio-, and chemo-speciﬁcity. These properties allow enzymes to greatly simplify complex synthetic processes, resulting in improved yields and reduced manufacturing costs compared to traditional chemical methods. However, the lack of experimental characterization of enzyme substrates, with only a few thousand out of tens of millions of known enzymes in Uniprot having annotated substrates, severely limits the ability of chemists to repurpose enzymes for industrial applications. Previous machine learning models aimed at predicting enzyme substrates have been hampered by poor generalization to new substrates. Here, we introduce VIPER (Virtual Interaction Predictor for Enzyme Reactivity), a model that achieves an average 34\% improvement over the previous stateof-the-art model (ProSmith) in reaction prediction for unseen substrates. Furthermore, we present a novel benchmarking methodology for assessing the outof-distribution generalization capabilities of enzyme-substrate prediction models. VIPER represents a signiﬁcant advance towards the in silico prediction of enzymesubstrate compatibility, paving the way for the discovery of novel biocatalytic routes for the sustainable synthesis of high-value chemicals.},
	language = {en},
	urldate = {2024-06-29},
	author = {Campbell, Max James},
	month = jun,
	year = {2024},
	keywords = {Annotation, notion, Substrate},
	file = {Campbell - 2024 - VIPER A General Model for Prediction of Enzyme Su.pdf:/Users/jason/Zotero/storage/UH7A7WYY/Campbell - 2024 - VIPER A General Model for Prediction of Enzyme Su.pdf:application/pdf},
}


@article{baker_protein_2024,
	title = {Protein design meets biosecurity},
	volume = {383},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.ado1671},
	doi = {10.1126/science.ado1671},
	abstract = {The power and accuracy of computational protein design have been increasing rapidly with the incorporation of artificial intelligence (AI) approaches. This promises to transform biotechnology, enabling advances across sustainability and medicine. DNA synthesis plays a critical role in materializing designed proteins. However, as with all major revolutionary changes, this technology is vulnerable to misuse and the production of dangerous biological agents. To enable the full benefits of this revolution while mitigating risks that may emerge, all synthetic gene sequence and synthesis data should be collected and stored in repositories that are only queried in emergencies to ensure that protein design proceeds in a safe, secure, and trustworthy manner.},
	language = {en},
	number = {6681},
	urldate = {2024-01-28},
	journal = {Science},
	author = {Baker, David and Church, George},
	month = jan,
	year = {2024},
	keywords = {notion},
	pages = {349--349},
	file = {Baker and Church - 2024 - Protein design meets biosecurity.pdf:/Users/jason/Zotero/storage/6JB99V7W/Baker and Church - 2024 - Protein design meets biosecurity.pdf:application/pdf},
}


@article{hamamsy_learning_2023,
	title = {Learning sequence, structure, and function representations of proteins with language models},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.11.26.568742},
	abstract = {The sequence-structure-function relationships that ultimately generate the diversity of extant observed proteins is complex, as proteins bridge the gap between multiple informational and physical scales involved in nearly all cellular processes. One limitation of existing protein annotation databases such as UniProt is that less than 1\% of proteins have experimentally verified functions, and computational methods are needed to fill in the missing information. Here, we demonstrate that a multi-aspect framework based on protein language models can learn sequence-structure-function representations of amino acid sequences, and can provide the foundation for sensitive sequence-structure-function aware protein sequence search and annotation. Based on this model, we introduce a multi-aspect information retrieval system for proteins, Protein-Vec, cover ing sequence, structure, and function aspects, that enables computational protein annotation and function prediction at tree-of-life scales.},
	language = {en},
	urldate = {2023-11-27},
	author = {Hamamsy, Tymor and Barot, Meet and Morton, James T. and Steinegger, Martin and Bonneau, Richard and Cho, Kyunghyun},
	month = nov,
	year = {2023},
	doi = {10.1101/2023.11.26.568742},
	keywords = {Annotation, ML Proteins, notion},
	file = {Hamamsy et al. - 2023 - Learning sequence, structure, and function represe.pdf:/Users/jason/Zotero/storage/8EVU2SYI/Hamamsy et al. - 2023 - Learning sequence, structure, and function represe.pdf:application/pdf},
}

@misc{wang_instructprotein_2023,
	title = {{InstructProtein}: {Aligning} {Human} and {Protein} {Language} via {Knowledge} {Instruction}},
	shorttitle = {{InstructProtein}},
	url = {http://arxiv.org/abs/2310.03269},
	abstract = {Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing annotation imbalance and instruction deficits in existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by large margins. Moreover, InstructProtein serves as a pioneering step towards text-based protein function prediction and sequence design, effectively bridging the gap between protein and human language understanding.},
	language = {en},
	urldate = {2024-06-17},
	publisher = {arXiv},
	author = {Wang, Zeyuan and Zhang, Qiang and Ding, Keyan and Qin, Ming and Zhuang, Xiang and Li, Xiaotong and Chen, Huajun},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03269 [cs, q-bio]},
	keywords = {Generative, Language Model, notion},
	file = {Wang et al. - 2023 - InstructProtein Aligning Human and Protein Langua.pdf:/Users/jason/Zotero/storage/99JQGR27/Wang et al. - 2023 - InstructProtein Aligning Human and Protein Langua.pdf:application/pdf},
}

@article{boger_functional_2024,
	title = {Functional protein mining with conformal guarantees},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.06.27.601042},
	doi = {10.1101/2024.06.27.601042},
	abstract = {Molecular structure prediction and homology detection provide a promising path to discovering new protein function and evolutionary relationships. However, current approaches lack statistical reliability assurances, limiting their practical utility for selecting proteins for further experimental and in-silico characterization. To address this challenge, we introduce a novel approach to protein search leveraging principles from conformal prediction, offering a framework that ensures statistical guarantees with user-specified risk on outputs from any protein search model. Our method (1) lets users select any loss metric (i.e. false discovery rate) and assigns reliable functional probabilities for annotating genes of unknown function; (2) achieves state-of-the-art performance in enzyme classification without training new models; and (3) robustly and rapidly pre-filters proteins for computationally intensive structural alignment algorithms. Our framework enhances the reliability of protein homology detection and enables the discovery of new proteins with likely desirable functional properties.},
	language = {en},
	urldate = {2024-06-29},
	author = {Boger, Ron S. and Chithrananda, Seyone and Angelopoulos, Anastasios N. and Yoon, Peter Hyungjun and Jordan, Michael I and Doudna, Jennifer A.},
	month = jun,
	year = {2024},
	keywords = {Annotation, notion, Uncertainty Quantification},
	file = {Boger et al. - 2024 - Functional protein mining with conformal guarantee.pdf:/Users/jason/Zotero/storage/GITPJUJ9/Boger et al. - 2024 - Functional protein mining with conformal guarantee.pdf:application/pdf},
}

@misc{ramos_review_2024,
	title = {A {Review} of {Large} {Language} {Models} and {Autonomous} {Agents} in {Chemistry}},
	url = {http://arxiv.org/abs/2407.01603},
	abstract = {Large language models (LLMs) are emerging as a powerful tool in chemistry across multiple domains. In chemistry, LLMs are able to accurately predict properties, design new molecules, optimize synthesis pathways, and accelerate drug and material discovery. A core emerging idea is combining LLMs with chemistry-specific tools like synthesis planners and databases, leading to so-called “agents.” This review covers LLMs’ recent history, current capabilities, design, challenges specific to chemistry, and future directions. Particular attention is given to agents and their emergence as a cross-chemistry paradigm. Agents have proven effective in diverse domains of chemistry, but challenges remain. It is unclear if creating domain-specific versus generalist agents and developing autonomous pipelines versus "co-pilot" systems will accelerate chemistry. An emerging direction is the development of multi-agent systems using a human-in-the-loop approach. Due to the incredibly fast development of this field, a repository has been built to keep track of the latest studies: https: //github.com/ur-whitelab/LLMs-in-science.},
	language = {en},
	urldate = {2024-07-03},
	publisher = {arXiv},
	author = {Ramos, Mayk Caldas and Collison, Christopher J. and White, Andrew D.},
	month = jun,
	year = {2024},
	note = {arXiv:2407.01603 [physics]},
	keywords = {notion, Review},
	file = {Ramos et al. - 2024 - A Review of Large Language Models and Autonomous A.pdf:/Users/jason/Zotero/storage/8LTMW2I5/Ramos et al. - 2024 - A Review of Large Language Models and Autonomous A.pdf:application/pdf},
}

@article{de_crecy-lagard_limitations_2024,
	title = {Limitations of {Current} {Machine}-{Learning} {Models} in {Predicting} {Enzymatic} {Functions} for {Uncharacterized} {Proteins}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.07.01.601547},
	doi = {10.1101/2024.07.01.601547},
	abstract = {Abstract
          
            Thirty to seventy percent of proteins in any given genome have no assigned function and have been labeled as the protein “unknownme”. This large knowledge gap prevents the biological community from fully leveraging the plethora of genomic data that is now available. Machine-learning approaches are showing some promise in propagating functional knowledge from experimentally characterized proteins to the correct set of isofunctional orthologs. However, they largely fail to predict enzymatic functions unseen in the training set, as shown by dissecting the predictions made for 450 enzymes of unknown function from the model bacteria
            Escherichia coli
            using the DeepECTransformer platform. Lessons from these failures can help the community develop machine-learning methods that assist domain experts in making testable functional predictions for more members of the uncharacterized proteome.},
	language = {en},
	urldate = {2024-07-08},
	author = {De Crécy-Lagard, Valérie and Dias, Raquel and Friedberg, Iddo and Yuan, Yifeng and Swairjo, Manal A.},
	month = jul,
	year = {2024},
	keywords = {Annotation, notion},
	file = {De Crécy-Lagard et al. - 2024 - Limitations of Current Machine-Learning Models in .pdf:/Users/jason/Zotero/storage/BSUHDXQA/De Crécy-Lagard et al. - 2024 - Limitations of Current Machine-Learning Models in .pdf:application/pdf},
}

@article{duan_predicting_2024,
	title = {Predicting {Enzyme} {Functions} {Using} {Contrastive} {Learning} with {Hierarchical} {Enzyme} {Structure} {Information}},
	abstract = {Enzyme functional annotation is a fundamental challenge in biology, and many computational tools have been developed. Accurate function prediction of enzymes relies heavily on sequence and structural information, providing critical insights into enzyme activity and specificity. However, for less studied proteins or proteins with previously uncharacterized functions or multiple activities, most of these tools cannot accurately predict functional annotations, such as enzyme commission (EC) numbers. At the same time, functional hierarchical information between enzyme species categorized based on EC numbers has not been sufficiently investigated. To address these challenges, we propose a machine learning algorithm named EnzHier, which assigns EC numbers to enzymes with better accuracy and reliability compared to state-of-the-art tools. EnzHier cleverly learns the functional hierarchy of enzymes by optimizing triplet loss, enabling it to annotate understudied enzymes confidently and identify confounding enzymes with two or more EC numbers. By incorporating both sequence and structural information, EnzHier enhances its predictive capabilities. We experimentally demonstrate its excellent performance. We anticipate that this tool will be widely used to predict the function of uncharacterized enzymes, thereby advancing many fields such as drug design and discovery and medical diagnostics.},
	language = {en},
	author = {Duan, Hongyu and Li, Ziyan and Wu, Yixuan and Chen, Wen and Xia, Li C},
	year = {2024},
	keywords = {Annotation, notion, Structure},
	file = {2407.11942v1.pdf:/Users/jason/Zotero/storage/DFCGKJH8/2407.11942v1.pdf:application/pdf;Duan et al. - Predicting Enzyme Functions Using Contrastive Lear.pdf:/Users/jason/Zotero/storage/PKH22YKK/Duan et al. - Predicting Enzyme Functions Using Contrastive Lear.pdf:application/pdf},
}

@article{duan_boosting_2024,
	title = {Boosting the {Predictive} {Power} of {Protein} {Representations} with a {Corpus} of {Text} {Annotations}},
	abstract = {Protein language models are trained to predict amino acid sequences from vast protein databases, while learning to represent proteins as feature vectors. These vector representations have enabled impressive applications, from predicting mutation effects to protein folding. One of the reasons offered for the success of these models is that conserved sequence motifs tend to be important for protein fitness. Yet, the relationship between sequence conservation and fitness can be confounded by the evolutionary and environmental context. Should we therefore look to other data sources that may contain more direct functional information? In this work, we conduct a comprehensive study examining the effects of training protein models to predict nineteen types of text annotations from UniProt. Our results show that finetuning protein models on a subset of these annotations enhances the models’ predictive capabilities on a variety of function prediction tasks. Notably, our model outperforms the search algorithm BLAST, which none of the pre-trained protein models accomplished in our evaluation. Our results suggest that a much wider array of data modalities, such as text annotations, may be tapped to improve protein language models.},
	language = {en},
	author = {Duan, Haonan and Skreta, Marta and Cotta, Leonardo and Rajaonson, Ella Miray and Dhawan, Nikita and Aspuru-Guzik, Alán and Maddison, Chris J},
	year = {2024},
	keywords = {Annotation, Language Model, notion},
	file = {Duan et al. - Boosting the Predictive Power of Protein Represent.pdf:/Users/jason/Zotero/storage/22QHAZ9T/Duan et al. - Boosting the Predictive Power of Protein Represent.pdf:application/pdf},
}

@article{ribeiro_ezmechanism_2023,
	title = {{EzMechanism}: an automated tool to propose catalytic mechanisms of enzyme reactions},
	volume = {20},
	issn = {1548-7091, 1548-7105},
	shorttitle = {{EzMechanism}},
	url = {https://www.nature.com/articles/s41592-023-02006-7},
	doi = {10.1038/s41592-023-02006-7},
	abstract = {Abstract
            Over the years, hundreds of enzyme reaction mechanisms have been studied using experimental and simulation methods. This rich literature on biological catalysis is now ripe for use as the foundation of new knowledge-based approaches to investigate enzyme mechanisms. Here, we present a tool able to automatically infer mechanistic paths for a given three-dimensional active site and enzyme reaction, based on a set of catalytic rules compiled from the Mechanism and Catalytic Site Atlas, a database of enzyme mechanisms. EzMechanism (pronounced as ‘Easy’ Mechanism) is available to everyone through a web user interface. When studying a mechanism, EzMechanism facilitates and improves the generation of hypotheses, by making sure that relevant information is considered, as derived from the literature on both related and unrelated enzymes. We validated EzMechanism on a set of 62 enzymes and have identified paths for further improvement, including the need for additional and more generic catalytic rules.},
	language = {en},
	number = {10},
	urldate = {2024-08-03},
	journal = {Nature Methods},
	author = {Ribeiro, Antonio J. M. and Riziotis, Ioannis G. and Tyzack, Jonathan D. and Borkakoti, Neera and Thornton, Janet M.},
	month = oct,
	year = {2023},
	keywords = {Dataset, notion},
	pages = {1516--1522},
	file = {Ribeiro et al. - 2023 - EzMechanism an automated tool to propose catalyti.pdf:/Users/jason/Zotero/storage/IXFJ6E9N/Ribeiro et al. - 2023 - EzMechanism an automated tool to propose catalyti.pdf:application/pdf},
}

@article{jang_accurate_2024,
	title = {Accurate prediction of protein function using statistics-informed graph networks},
	volume = {15},
	language = {en},
	journal = {Nature Communications},
	author = {Jang, Yaan J},
	year = {2024},
	keywords = {Annotation, Bioinformatics, notion},
	pages = {6601},
	file = {Jang - 2024 - Accurate prediction of protein function using stat.pdf:/Users/jason/Zotero/storage/W32JHJJF/Jang - 2024 - Accurate prediction of protein function using stat.pdf:application/pdf},
}

@misc{laurent_lab-bench_2024,
	title = {{LAB}-{Bench}: {Measuring} {Capabilities} of {Language} {Models} for {Biology} {Research}},
	shorttitle = {{LAB}-{Bench}},
	url = {http://arxiv.org/abs/2407.10362},
	abstract = {There is widespread optimism that frontier Large Language Models (LLMs) and LLM-augmented systems have the potential to rapidly accelerate scientific discovery across disciplines. Today, many benchmarks exist to measure LLM knowledge and reasoning on “textbook”-style science questions, but few if any benchmarks are designed to evaluate language model performance on practical tasks required for scientific research, such as literature search, protocol planning, and data analysis. As a step toward building such benchmarks, we introduce the Language Agent Biology Benchmark (LAB-Bench), a broad dataset of over 2,400 multiple choice questions for evaluating AI systems on a range of practical biology research capabilities, including recall and reasoning over literature, interpretation of figures, access and navigation of databases, and comprehension and manipulation of DNA and protein sequences. Importantly, in contrast to previous scientific benchmarks, we expect that an AI system that can achieve consistently high scores on the more difficult LAB-Bench tasks would serve as a useful assistant for researchers in areas such as literature search and molecular cloning. As an initial assessment of the emergent scientific task capabilities of frontier language models, we measure performance of several against our benchmark and report results compared to human expert biology researchers. We will continue to update and expand LAB-Bench over time, and expect it to serve as a useful tool in the development of automated research systems going forward. A public subset of LAB-Bench is available for use at the following url: https://huggingface.co/datasets/futurehouse/lab-bench.},
	language = {en},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Laurent, Jon M. and Janizek, Joseph D. and Ruzo, Michael and Hinks, Michaela M. and Hammerling, Michael J. and Narayanan, Siddharth and Ponnapati, Manvitha and White, Andrew D. and Rodriques, Samuel G.},
	month = jul,
	year = {2024},
	note = {arXiv:2407.10362 [cs]},
	keywords = {notion},
	annote = {Comment: 40 pages, 5 main figures, 1 main table, 2 supplemental figures, 4 supplemental tables. Submitted to NeurIPS 2024 Datasets and Benchmarks track (in review)},
	file = {Laurent et al. - 2024 - LAB-Bench Measuring Capabilities of Language Mode.pdf:/Users/jason/Zotero/storage/5FBQNBRH/Laurent et al. - 2024 - LAB-Bench Measuring Capabilities of Language Mode.pdf:application/pdf},
}

@article{hong_fast_2024,
	title = {Fast, sensitive detection of protein homologs using deep dense retrieval},
	issn = {1087-0156, 1546-1696},
	url = {https://www.nature.com/articles/s41587-024-02353-6},
	doi = {10.1038/s41587-024-02353-6},
	language = {en},
	urldate = {2024-08-11},
	journal = {Nature Biotechnology},
	author = {Hong, Liang and Hu, Zhihang and Sun, Siqi and Tang, Xiangru and Wang, Jiuming and Tan, Qingxiong and Zheng, Liangzhen and Wang, Sheng and Xu, Sheng and King, Irwin and Gerstein, Mark and Li, Yu},
	month = aug,
	year = {2024},
	keywords = {notion},
	file = {Hong et al. - 2024 - Fast, sensitive detection of protein homologs usin.pdf:/Users/jason/Zotero/storage/LA685I23/Hong et al. - 2024 - Fast, sensitive detection of protein homologs usin.pdf:application/pdf},
}

@article{radivojac_large_scale_2013,
	title = {A large-scale evaluation of computational protein function prediction},
	volume = {10},
	copyright = {http://www.springer.com/tdm},
	issn = {1548-7091, 1548-7105},
	url = {https://www.nature.com/articles/nmeth.2340},
	doi = {10.1038/nmeth.2340},
	language = {en},
	number = {3},
	urldate = {2024-06-13},
	journal = {Nature Methods},
	author = {Radivojac, Predrag and Clark, Wyatt T and Oron, Tal Ronnen and Schnoes, Alexandra M and Wittkop, Tobias and Sokolov, Artem and Graim, Kiley and Funk, Christopher and Verspoor, Karin and Ben-Hur, Asa and Pandey, Gaurav and Yunes, Jeffrey M and Talwalkar, Ameet S and Repo, Susanna and Souza, Michael L and Piovesan, Damiano and Casadio, Rita and Wang, Zheng and Cheng, Jianlin and Fang, Hai and Gough, Julian and Koskinen, Patrik and Törönen, Petri and Nokso-Koivisto, Jussi and Holm, Liisa and Cozzetto, Domenico and Buchan, Daniel W A and Bryson, Kevin and Jones, David T and Limaye, Bhakti and Inamdar, Harshal and Datta, Avik and Manjari, Sunitha K and Joshi, Rajendra and Chitale, Meghana and Kihara, Daisuke and Lisewski, Andreas M and Erdin, Serkan and Venner, Eric and Lichtarge, Olivier and Rentzsch, Robert and Yang, Haixuan and Romero, Alfonso E and Bhat, Prajwal and Paccanaro, Alberto and Hamp, Tobias and Kaßner, Rebecca and Seemayer, Stefan and Vicedo, Esmeralda and Schaefer, Christian and Achten, Dominik and Auer, Florian and Boehm, Ariane and Braun, Tatjana and Hecht, Maximilian and Heron, Mark and Hönigschmid, Peter and Hopf, Thomas A and Kaufmann, Stefanie and Kiening, Michael and Krompass, Denis and Landerer, Cedric and Mahlich, Yannick and Roos, Manfred and Björne, Jari and Salakoski, Tapio and Wong, Andrew and Shatkay, Hagit and Gatzmann, Fanny and Sommer, Ingolf and Wass, Mark N and Sternberg, Michael J E and Škunca, Nives and Supek, Fran and Bošnjak, Matko and Panov, Panče and Džeroski, Sašo and Šmuc, Tomislav and Kourmpetis, Yiannis A I and Van Dijk, Aalt D J and Braak, Cajo J F Ter and Zhou, Yuanpeng and Gong, Qingtian and Dong, Xinran and Tian, Weidong and Falda, Marco and Fontana, Paolo and Lavezzo, Enrico and Di Camillo, Barbara and Toppo, Stefano and Lan, Liang and Djuric, Nemanja and Guo, Yuhong and Vucetic, Slobodan and Bairoch, Amos and Linial, Michal and Babbitt, Patricia C and Brenner, Steven E and Orengo, Christine and Rost, Burkhard and Mooney, Sean D and Friedberg, Iddo},
	month = mar,
	year = {2013},
	keywords = {Annotation, notion},
	pages = {221--227},
	file = {Radivojac et al. - 2013 - A large-scale evaluation of computational protein .pdf:/Users/jason/Zotero/storage/KSTD9NW6/Radivojac et al. - 2013 - A large-scale evaluation of computational protein .pdf:application/pdf},
}

@misc{reaxys, title={Reaxys An expert-curated chemical database Elsevier}, url={https://www.elsevier.com/products/reaxys}, abstractNote={Find chemical patents, bioactivity data and chemistry data. Chemists get up to speed quickly with an intuitive interface that follows their workflows.}, journal={www.elsevier.com}, language={en-US} }


@article{Kanehisa_Furumichi_Sato_Kawashima_Ishiguro_2023, title={KEGG for taxonomy-based analysis of pathways and genomes}, volume={51}, ISSN={0305-1048}, DOI={10.1093/nar/gkac963}, abstractNote={KEGG (https://www.kegg.jp) is a manually curated database resource integrating various biological objects categorized into systems, genomic, chemical and health information. Each object (database entry) is identified by the KEGG identifier (kid), which generally takes the form of a prefix followed by a five-digit number, and can be retrieved by appending /entry/kid in the URL. The KEGG pathway map viewer, the Brite hierarchy viewer and the newly released KEGG genome browser can be launched by appending /pathway/kid, /brite/kid and /genome/kid, respectively, in the URL. Together with an improved annotation procedure for KO (KEGG Orthology) assignment, an increasing number of eukaryotic genomes have been included in KEGG for better representation of organisms in the taxonomic tree. Multiple taxonomy files are generated for classification of KEGG organisms and viruses, and the Brite hierarchy viewer is used for taxonomy mapping, a variant of Brite mapping in the new KEGG Mapper suite. The taxonomy mapping enables analysis of, for example, how functional links of genes in the pathway and physical links of genes on the chromosome are conserved among organism groups.}, number={D1}, journal={Nucleic Acids Research}, author={Kanehisa, Minoru and Furumichi, Miho and Sato, Yoko and Kawashima, Masayuki and Ishiguro-Watanabe, Mari}, year={2023}, month=jan, pages={D587–D592} }


@misc{johnston_machine_2023,
	title = {Machine {Learning} for {Protein} {Engineering}},
	url = {http://arxiv.org/abs/2305.16634},
	doi = {10.48550/arXiv.2305.16634},
	abstract = {Directed evolution of proteins has been the most effective method for protein engineering. However, a new paradigm is emerging, fusing the library generation and screening approaches of traditional directed evolution with computation through the training of machine learning models on protein sequence fitness data. This chapter highlights successful applications of machine learning to protein engineering and directed evolution, organized by the improvements that have been made with respect to each step of the directed evolution cycle. Additionally, we provide an outlook for the future based on the current direction of the field, namely in the development of calibrated models and in incorporating other modalities, such as protein structure.},
	urldate = {2023-10-13},
	publisher = {arXiv},
	author = {Johnston, Kadina E. and Fannjiang, Clara and Wittmann, Bruce J. and Hie, Brian L. and Yang, Kevin K. and Wu, Zachary},
	month = may,
	year = {2023},
	note = {arXiv:2305.16634 [q-bio]},
	keywords = {notion},
	file = {arXiv Fulltext PDF:/Users/jason/Zotero/storage/SPKI5W5Y/Johnston et al. - 2023 - Machine Learning for Protein Engineering.pdf:application/pdf;arXiv.org Snapshot:/Users/jason/Zotero/storage/T4XHA4HM/2305.html:text/html},
}

@article{kouba_machine_2023,
	title = {Machine {Learning}-{Guided} {Protein} {Engineering}},
	volume = {13},
	issn = {2155-5435, 2155-5435},
	url = {https://pubs.acs.org/doi/10.1021/acscatal.3c02743},
	doi = {10.1021/acscatal.3c02743},
	abstract = {Recent progress in engineering highly promising biocatalysts has increasingly involved machine learning methods. These methods leverage existing experimental and simulation data to aid in the discovery and annotation of promising enzymes, as well as in suggesting beneficial mutations for improving known targets. The field of machine learning for protein engineering is gathering steam, driven by recent success stories and notable progress in other areas. It already encompasses ambitious tasks such as understanding and predicting protein structure and function, catalytic efficiency, enantioselectivity, protein dynamics, stability, solubility, aggregation, and more. Nonetheless, the field is still evolving, with many challenges to overcome and questions to address. In this Perspective, we provide an overview of ongoing trends in this domain, highlight recent case studies, and examine the current limitations of machine learning-based methods. We emphasize the crucial importance of thorough experimental validation of emerging models before their use for rational protein design. We present our opinions on the fundamental problems and outline the potential directions for future research.},
	language = {en},
	number = {21},
	urldate = {2023-10-17},
	journal = {ACS Catalysis},
	author = {Kouba, Petr and Kohout, Pavel and Haddadi, Faraneh and Bushuiev, Anton and Samusevich, Raman and Sedlar, Jiri and Damborsky, Jiri and Pluskal, Tomas and Sivic, Josef and Mazurenko, Stanislav},
	month = oct,
	year = {2023},
	keywords = {Skimmed, notion, ML Proteins, Review},
	pages = {13863--13895},
	file = {Kouba et al. - 2023 - Machine Learning-Guided Protein Engineering.pdf:/Users/jason/Zotero/storage/IME8SXXX/Kouba et al. - 2023 - Machine Learning-Guided Protein Engineering.pdf:application/pdf},
}

@article{shaw_protex_nodate,
	title = {{ProtEx}: {A} {Retrieval}-{Augmented} {Approach} for {Protein} {Function} {Prediction}},
	abstract = {Mapping a protein sequence to its underlying biological function is a critical problem of increasing importance in biology. In this work, we propose ProtEx, a retrieval-augmented approach for protein function prediction that leverages exemplars from a database to improve accuracy and robustness and enable generalization to unseen classes. Our approach relies on a novel multi-sequence pretraining task, and a ﬁne-tuning strategy that effectively conditions predictions on retrieved exemplars. Our method achieves state-of-the-art results across multiple datasets and settings for predicting Enzyme Commission (EC) numbers, Gene Ontology (GO) terms, and Pfam families. Our ablations and analysis highlight the impact of conditioning predictions on exemplar sequences, especially for classes and sequences less well represented in the training data.},
	language = {en},
	author = {Shaw, Peter and Gurram, Bhaskar and Belanger, David and Gane, Andreea and Bileschi, Maxwell L and Colwell, Lucy J and Toutanova, Kristina and Parikh, Ankur P},
	keywords = {notion},
	file = {Shaw et al. - ProtEx A Retrieval-Augmented Approach for Protein.pdf:/Users/jason/Zotero/storage/VZITZWUY/Shaw et al. - ProtEx A Retrieval-Augmented Approach for Protein.pdf:application/pdf},
    year = {2024},
}


@article{su_protrek_nodate,
	title = {{ProTrek}: {Navigating} the {Protein} {Universe} through {Tri}-{Modal} {Contrastive} {Learning}},
	abstract = {ProTrek, a tri-modal protein language model, enables contrastive learning of protein sequence, structure, and function (SSF). Through its natural language search interface, users can navigate the vast protein universe in seconds, accessing nine distinct search tasks that cover all possible pairwise combinations of SSF. Additionally, ProTrek serves as a general-purpose protein representation model, excelling in various downstream prediction tasks through supervised transfer learning, thereby providing extensive support for protein research and analysis.},
	language = {en},
	author = {Su, Jin and Zhou, Xibin and Zhang, Xuting and Yuan, Fajie},
	file = {Su et al. - ProTrek Navigating the Protein Universe through T.pdf:/Users/jason/Zotero/storage/DCJVVBYX/Su et al. - ProTrek Navigating the Protein Universe through T.pdf:application/pdf},
    year = {2024},
}

@article{jain_new_2024,
	title = {A {New} {Age} of {Biocatalysis} {Enabled} by {Generic} {Activation} {Modes}},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	issn = {2691-3704, 2691-3704},
	url = {https://pubs.acs.org/doi/10.1021/jacsau.4c00247},
	doi = {10.1021/jacsau.4c00247},
	abstract = {Biocatalysis is currently undergoing a profound transformation. The field moves from relying on nature’s chemical logic to a discipline that exploits generic activation modes, allowing for novel biocatalytic reactions and, in many instances, entirely new chemistry. Generic activation modes enable a wide range of reaction types and played a pivotal role in advancing the fields of organo- and photocatalysis. This perspective aims to summarize the principal activation modes harnessed in enzymes to develop new biocatalysts. Although extensively researched in the past, the highlighted activation modes, when applied within enzyme active sites, facilitate chemical transformations that have largely eluded efficient and selective catalysis. This advance is attributed to multiple tunable interactions in the substrate binding pocket that precisely control competing reaction pathways and transition states. We will highlight cases of new synthetic methodologies achieved by engineered enzymes and will provide insights into potential future developments in this rapidly evolving field.},
	language = {en},
	urldate = {2024-06-02},
	journal = {JACS Au},
	author = {Jain, Shubhanshu and Ospina, Felipe and Hammer, Stephan C.},
	month = may,
	year = {2024},
	keywords = {notion},
	pages = {jacsau.4c00247},
	file = {Jain et al. - 2024 - A New Age of Biocatalysis Enabled by Generic Activ.pdf:/Users/jason/Zotero/storage/GN97Q7GG/Jain et al. - 2024 - A New Age of Biocatalysis Enabled by Generic Activ.pdf:application/pdf},
}

@article{chen_simple_2020,
	title = {A {Simple} {Framework} for {Contrastive} {Learning} of {Visual} {Representations}},
	url = {http://arxiv.org/abs/2002.05709},
	abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
	urldate = {2021-10-04},
	journal = {arXiv:2002.05709 [cs, stat]},
	author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	month = jun,
	year = {2020},
	note = {arXiv: 2002.05709},
	keywords = {ML, notion},
	file = {arXiv Fulltext PDF:/Users/jason/Zotero/storage/BG7L54SV/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf:application/pdf;arXiv.org Snapshot:/Users/jason/Zotero/storage/5UU4NKAX/2002.html:text/html},
}

@article{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2021-10-04},
	journal = {arXiv:1810.04805 [cs]},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv: 1810.04805},
	keywords = {ML, notion},
	file = {arXiv Fulltext PDF:/Users/jason/Zotero/storage/D55ETMJG/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/Users/jason/Zotero/storage/P3G4KBFR/1810.html:text/html},
}

@article{arnold_directed_2018,
	title = {Directed {Evolution}: {Bringing} {New} {Chemistry} to {Life}},
	volume = {57},
	issn = {1433-7851, 1521-3773},
	shorttitle = {Directed {Evolution}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/anie.201708408},
	doi = {10.1002/anie.201708408},
	language = {en},
	number = {16},
	urldate = {2021-10-06},
	journal = {Angewandte Chemie International Edition},
	author = {Arnold, Frances H.},
	month = apr,
	year = {2018},
	keywords = {notion, Protein Engineering},
	pages = {4143--4148},
	file = {Arnold - 2018 - Directed Evolution Bringing New Chemistry to Life.pdf:/Users/jason/Zotero/storage/CK4RI74W/Arnold - 2018 - Directed Evolution Bringing New Chemistry to Life.pdf:application/pdf},
}

@article{tawfik_enzyme_2010,
	title = {Enzyme {Promiscuity}: {A} {Mechanistic} and {Evolutionary} {Perspective}},
	volume = {79},
	issn = {0066-4154, 1545-4509},
	shorttitle = {Enzyme {Promiscuity}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-biochem-030409-143718},
	doi = {10.1146/annurev-biochem-030409-143718},
	abstract = {Many, if not most, enzymes can promiscuously catalyze reactions, or act on substrates, other than those for which they evolved. Here, we discuss the structural, mechanistic, and evolutionary implications of this manifestation of inﬁdelity of molecular recognition. We deﬁne promiscuity and related phenomena and also address their generality and physiological implications. We discuss the mechanistic enzymology of promiscuity—how enzymes, which generally exert exquisite speciﬁcity, catalyze other, and sometimes barely related, reactions. Finally, we address the hypothesis that promiscuous enzymatic activities serve as evolutionary starting points and highlight the unique evolutionary features of promiscuous enzyme functions.},
	language = {en},
	number = {1},
	urldate = {2021-10-06},
	journal = {Annual Review of Biochemistry},
	author = {Tawfik, Olga Khersonsky {and} Dan S.},
	month = jun,
	year = {2010},
	keywords = {notion, Protein Engineering},
	pages = {471--505},
	file = {Tawfik - 2010 - Enzyme Promiscuity A Mechanistic and Evolutionary.pdf:/Users/jason/Zotero/storage/QUV4NQ4S/Tawfik - 2010 - Enzyme Promiscuity A Mechanistic and Evolutionary.pdf:application/pdf},
}

@article{rives_biological_2021,
	title = {Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences},
	volume = {118},
	copyright = {Copyright © 2021 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/118/15/e2016239118},
	doi = {10.1073/pnas.2016239118},
	abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Protein language modeling at the scale of evolution is a logical step toward predictive and generative artificial intelligence for biology. To this end, we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone. The learned representation space has a multiscale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure and improving state-of-the-art features for long-range contact prediction.},
	language = {en},
	number = {15},
	urldate = {2021-12-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Liu, Jason and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
	month = apr,
	year = {2021},
	pmid = {33876751},
	keywords = {ML Proteins, notion, Representations, Skimmed},
	file = {Full Text PDF:/Users/jason/Zotero/storage/BSM3FPMT/Rives et al. - 2021 - Biological structure and function emerge from scal.pdf:application/pdf;Snapshot:/Users/jason/Zotero/storage/7V8R24XN/e2016239118.html:text/html},
}

@article{gligorijevic_structure-based_2021,
	title = {Structure-based protein function prediction using graph convolutional networks},
	volume = {12},
	copyright = {2021 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-021-23303-9},
	doi = {10.1038/s41467-021-23303-9},
	abstract = {The rapid increase in the number of proteins in sequence databases and the diversity of their functions challenge computational approaches for automated function prediction. Here, the authors introduce DeepFRI, a Graph Convolutional Network for predicting protein functions by leveraging sequence features extracted from a protein language model and protein structures.},
	language = {en},
	number = {1},
	urldate = {2022-01-04},
	journal = {Nature Communications},
	author = {Gligorijević, Vladimir and Renfrew, P. Douglas and Kosciolek, Tomasz and Leman, Julia Koehler and Berenberg, Daniel and Vatanen, Tommi and Chandler, Chris and Taylor, Bryn C. and Fisk, Ian M. and Vlamakis, Hera and Xavier, Ramnik J. and Knight, Rob and Cho, Kyunghyun and Bonneau, Richard},
	month = may,
	year = {2021},
	keywords = {Read, notion, ML Proteins, Graphs/Structures},
	pages = {1--14},
	file = {Full Text PDF:/Users/jason/Zotero/storage/C8FBQXVV/Gligorijević et al. - 2021 - Structure-based protein function prediction using .pdf:application/pdf;Snapshot:/Users/jason/Zotero/storage/Q7QGCGT3/s41467-021-23303-9.html:text/html},
}

@article{rao_evaluating_2019,
	title = {Evaluating {Protein} {Transfer} {Learning} with {TAPE}},
	url = {https://arxiv.org/abs/1906.08230v1},
	abstract = {Protein modeling is an increasingly popular area of machine learning research. Semi-supervised learning has emerged as an important paradigm in protein modeling due to the high cost of acquiring supervised protein labels, but the current literature is fragmented when it comes to datasets and standardized evaluation techniques. To facilitate progress in this field, we introduce the Tasks Assessing Protein Embeddings (TAPE), a set of five biologically relevant semi-supervised learning tasks spread across different domains of protein biology. We curate tasks into specific training, validation, and test splits to ensure that each task tests biologically relevant generalization that transfers to real-life scenarios. We benchmark a range of approaches to semi-supervised protein representation learning, which span recent work as well as canonical sequence learning techniques. We find that self-supervised pretraining is helpful for almost all models on all tasks, more than doubling performance in some cases. Despite this increase, in several cases features learned by self-supervised pretraining still lag behind features extracted by state-of-the-art non-neural techniques. This gap in performance suggests a huge opportunity for innovative architecture design and improved modeling paradigms that better capture the signal in biological sequences. TAPE will help the machine learning community focus effort on scientifically relevant problems. Toward this end, all data and code used to run these experiments are available at https://github.com/songlab-cal/tape.},
	language = {en},
	urldate = {2022-01-06},
	author = {Rao, Roshan and Bhattacharya, Nicholas and Thomas, Neil and Duan, Yan and Chen, Xi and Canny, John and Abbeel, Pieter and Song, Yun S.},
	month = jun,
	year = {2019},
	keywords = {ML Proteins, notion, Representations, Skimmed},
	file = {Rao et al. - 2019 - Evaluating Protein Transfer Learning with TAPE.pdf:/Users/jason/Zotero/storage/QJ28ZJTL/Rao et al. - 2019 - Evaluating Protein Transfer Learning with TAPE.pdf:application/pdf;Snapshot:/Users/jason/Zotero/storage/PLT4CC3X/1906.html:text/html},
}

@article{romero_exploring_2009,
	title = {Exploring protein fitness landscapes by directed evolution},
	volume = {10},
	doi = {10.1038/nrm2805},
	author = {Romero, Philip A and Arnold, Frances H},
	year = {2009},
	keywords = {notion, Protein Engineering},
	journal = {Nature Reviews Molecular Cell Biology},
	pages = {866--876},
	file = {PDF:/Users/jason/Zotero/storage/DB3BCLQ4/nrm2805.pdf:application/pdf;Romero and Arnold - 2009 - Exploring protein fitness landscapes by directed e.pdf:/Users/jason/Zotero/storage/M7ASCVAC/Romero and Arnold - 2009 - Exploring protein fitness landscapes by directed e.pdf:application/pdf},
}

@techreport{dallago_flip_2021,
	title = {{FLIP}: {Benchmark} tasks in fitness landscape inference for proteins},
	shorttitle = {{FLIP}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.11.09.467890},
	abstract = {Machine learning could enable an unprecedented level of control in protein engineering for therapeutic and industrial applications. Critical to its use in designing proteins with desired properties, machine learning models must capture the protein sequence-function relationship, often termed ﬁtness landscape. Existing benchmarks like CASP or CAFA assess structure and function predictions of proteins, respectively, yet they do not target metrics relevant for protein engineering. In this work, we introduce Fitness Landscape Inference for Proteins (FLIP), a benchmark for function prediction to encourage rapid scoring of representation learning for protein engineering. Our curated tasks, baselines, and metrics probe model generalization in settings relevant for protein engineering, e.g. low-resource and extrapolative. Currently, FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families. In order to enable ease of use and future expansion to new tasks, all data are presented in a standard format. FLIP scripts and data are freely accessible at https://benchmark.protein.properties.},
	language = {en},
	urldate = {2022-03-12},
	author = {Dallago, Christian and Mou, Jody and Johnston, Kadina E. and Wittmann, Bruce J. and Bhattacharya, Nicholas and Goldman, Samuel and Madani, Ali and Yang, Kevin K.},
	month = nov,
	year = {2021},
	doi = {10.1101/2021.11.09.467890},
	keywords = {ML Proteins, notion, Read},
	file = {Dallago et al. - 2021 - FLIP Benchmark tasks in fitness landscape inferen.pdf:/Users/jason/Zotero/storage/4R7UA6F5/Dallago et al. - 2021 - FLIP Benchmark tasks in fitness landscape inferen.pdf:application/pdf},
}

@article{steinegger_mmseqs2_2017,
	title = {{MMseqs2} enables sensitive protein sequence searching for the analysis of massive data sets},
	volume = {35},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1696},
	url = {https://www.nature.com/articles/nbt.3988},
	doi = {10.1038/nbt.3988},
	language = {en},
	number = {11},
	urldate = {2022-03-15},
	journal = {Nature Biotechnology},
	author = {Steinegger, Martin and Söding, Johannes},
	month = nov,
	year = {2017},
	note = {Number: 11
Publisher: Nature Publishing Group},
	keywords = {Computational, notion, Skimmed},
	pages = {1026--1028},
	file = {Full Text PDF:/Users/jason/Zotero/storage/TBQ45G8E/Steinegger and Söding - 2017 - MMseqs2 enables sensitive protein sequence searchi.pdf:application/pdf;Snapshot:/Users/jason/Zotero/storage/PRLV5HLJ/nbt.html:text/html},
}

@article{kreutter_predicting_2021,
	title = {Predicting enzymatic reactions with a molecular transformer},
	volume = {12},
	issn = {2041-6520, 2041-6539},
	url = {http://xlink.rsc.org/?DOI=D1SC02362D},
	doi = {10.1039/D1SC02362D},
	abstract = {The enzymatic transformer was trained with a combination of patent reactions and biotransformations and predicts the structure and stereochemistry of enzyme-catalyzed reaction products with remarkable accuracy.
          , 
            The use of enzymes for organic synthesis allows for simplified, more economical and selective synthetic routes not accessible to conventional reagents. However, predicting whether a particular molecule might undergo a specific enzyme transformation is very difficult. Here we used multi-task transfer learning to train the molecular transformer, a sequence-to-sequence machine learning model, with one million reactions from the US Patent Office (USPTO) database combined with 32 181 enzymatic transformations annotated with a text description of the enzyme. The resulting enzymatic transformer model predicts the structure and stereochemistry of enzyme-catalyzed reaction products with remarkable accuracy. One of the key novelties is that we combined the reaction SMILES language of only 405 atomic tokens with thousands of human language tokens describing the enzymes, such that our enzymatic transformer not only learned to interpret SMILES, but also the natural language as used by human experts to describe enzymes and their mutations.},
	language = {en},
	number = {25},
	urldate = {2022-04-26},
	journal = {Chemical Science},
	author = {Kreutter, David and Schwaller, Philippe and Reymond, Jean-Louis},
	year = {2021},
	keywords = {Read, notion, ML Proteins},
	pages = {8648--8659},
	file = {Kreutter et al. - 2021 - Predicting enzymatic reactions with a molecular tr.pdf:/Users/jason/Zotero/storage/3ZLCCVDP/Kreutter et al. - 2021 - Predicting enzymatic reactions with a molecular tr.pdf:application/pdf},
}

@article{elnaggar_prottrans_2021,
	title = {{ProtTrans}: {Towards} {Cracking} the {Language} of {Life}’s {Code} {Through} {Self}-{Supervised} {Learning}},
	volume = {14},
	abstract = {Computational biology and bioinformatics provide vast data gold-mines from protein sequences, ideal for Language Models taken from NLP. These LMs reach for new prediction frontiers at low inference costs. Here, we trained two auto-regressive models (Transformer-XL, XLNet) and four auto-encoder models (BERT, Albert, Electra, T5) on data from UniRef and BFD containing up to 393 billion amino acids. The LMs were trained on the Summit supercomputer using 5616 GPUs and TPU Pod up-to 1024 cores. Dimensionality reduction revealed that the raw protein LM-embeddings from unlabeled data captured some biophysical features of protein sequences. We validated the advantage of using the embeddings as exclusive input for several subsequent tasks. The ﬁrst was a per-residue prediction of protein secondary structure (3-state accuracy Q3=81\%-87\%); the second were per-protein predictions of protein sub-cellular localization (ten-state accuracy: Q10=81\%) and membrane vs. water-soluble (2-state accuracy Q2=91\%). For the per-residue predictions the transfer of the most informative embeddings (ProtT5) for the ﬁrst time outperformed the state-of-the-art without using evolutionary information thereby bypassing expensive database searches. Taken together, the results implied that protein LMs learned some of the grammar of the language of life. To facilitate future work, we released our models at https://github.com/agemagician/ProtTrans.},
	language = {en},
	number = {8},
	author = {Elnaggar, Ahmed and Heinzinger, Michael and Dallago, Christian and Rehawi, Ghalia and Wang, Yu and Jones, Llion and Gibbs, Tom and Feher, Tamas and Angerer, Christoph and Steinegger, Martin and Bhowmik, Debsindhu and Rost, Burkhard},
	year = {2021},
	keywords = {ML Proteins, notion, Representations},
	pages = {29},
	file = {Elnaggar et al. - 2021 - ProtTrans Towards Cracking the Language of Life’s.pdf:/Users/jason/Zotero/storage/YPNP6SWU/Elnaggar et al. - 2021 - ProtTrans Towards Cracking the Language of Life’s.pdf:application/pdf},
}

@article{madani_large_2023,
	title = {Large language models generate functional protein sequences across diverse families},
	issn = {1087-0156, 1546-1696},
	url = {https://www.nature.com/articles/s41587-022-01618-2},
	doi = {10.1038/s41587-022-01618-2},
	language = {en},
	urldate = {2023-01-26},
	journal = {Nature Biotechnology},
	author = {Madani, Ali and Krause, Ben and Greene, Eric R. and Subramanian, Subu and Mohr, Benjamin P. and Holton, James M. and Olmos, Jose Luis and Xiong, Caiming and Sun, Zachary Z. and Socher, Richard and Fraser, James S. and Naik, Nikhil},
	month = jan,
	year = {2023},
	keywords = {Generative, ML Proteins, notion, Representations},
	file = {Madani et al. - 2023 - Large language models generate functional protein .pdf:/Users/jason/Zotero/storage/8XLUL8E6/Madani et al. - 2023 - Large language models generate functional protein .pdf:application/pdf},
}

@article{yu_machine_2023,
	title = {Machine learning-enabled retrobiosynthesis of molecules},
	volume = {6},
	issn = {2520-1158},
	url = {https://www.nature.com/articles/s41929-022-00909-w},
	doi = {10.1038/s41929-022-00909-w},
	language = {en},
	number = {2},
	urldate = {2023-02-20},
	journal = {Nature Catalysis},
	author = {Yu, Tianhao and Boob, Aashutosh Girish and Volk, Michael J. and Liu, Xuan and Cui, Haiyang and Zhao, Huimin},
	month = feb,
	year = {2023},
	keywords = {ML Proteins, notion, Read, Review},
	pages = {137--151},
	file = {Yu et al. - 2023 - Machine learning-enabled retrobiosynthesis of mole.pdf:/Users/jason/Zotero/storage/Q4KTUYKW/Yu et al. - 2023 - Machine learning-enabled retrobiosynthesis of mole.pdf:application/pdf},
}

@article{lin_evolutionary-scale_2023,
	title = {Evolutionary-scale prediction of atomic-level protein structure with a language model},
	volume = {379},
	doi = {https://doi.org/10.1126/science.ade2574},
	language = {en},
	number = {6637},
	author = {Lin, Zeming and Akin, Halil and Rao, Roshan and Hie, Brian and Zhu, Zhongkai and Lu, Wenting and Smetanin, Nikita and Verkuil, Robert and Kabeli, Ori and Shmueli, Yaniv and Fazel-Zarandi, Maryam and Sercu, Tom and Candido, Salvatore and Rives, Alexander},
	year = {2023},
    journal = {Science},
	keywords = {Dataset, ML Proteins, notion, Read, Representations, Skimmed, Structure},
	pages = {1123--1130},
	file = {Lin et al. - 2023 - Evolutionary-scale prediction of atomic-level prot.pdf:/Users/jason/Zotero/storage/CNHU6M6Q/Lin et al. - 2023 - Evolutionary-scale prediction of atomic-level prot.pdf:application/pdf;Lin et al. - Evolutionary-scale prediction of atomic level prot.pdf:/Users/jason/Zotero/storage/2SG5NGAJ/Lin et al. - Evolutionary-scale prediction of atomic level prot.pdf:application/pdf},
}

@article{yu_enzyme_2023,
	title = {Enzyme function prediction using contrastive learning},
	volume = {379},
	doi = {10.1126/science.adf2465},
	language = {en},
	number = {6639},
	author = {Yu, Tianhao and Cui, Haiyang and Li, Jianan Canal and Luo, Yunan and Jiang, Guangde and Zhao, Huimin},
	year = {2023},
	keywords = {ML Proteins, notion, Skimmed},
	pages = {1358--1363},
	file = {science.adf2465_sm.pdf:/Users/jason/Zotero/storage/76SAXTMR/science.adf2465_sm.pdf:application/pdf;Yu et al. - 2023 - Enzyme function prediction using contrastive learn.pdf:/Users/jason/Zotero/storage/3CWDIERH/Yu et al. - 2023 - Enzyme function prediction using contrastive learn.pdf:application/pdf},
}

@article{kroll_general_2023,
	title = {A general model to predict small molecule substrates of enzymes based on machine and deep learning},
	volume = {14},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-023-38347-2},
	doi = {10.1038/s41467-023-38347-2},
	abstract = {Abstract
            For most proteins annotated as enzymes, it is unknown which primary and/or secondary reactions they catalyze. Experimental characterizations of potential substrates are time-consuming and costly. Machine learning predictions could provide an efficient alternative, but are hampered by a lack of information regarding enzyme non-substrates, as available training data comprises mainly positive examples. Here, we present ESP, a general machine-learning model for the prediction of enzyme-substrate pairs with an accuracy of over 91\% on independent and diverse test data. ESP can be applied successfully across widely different enzymes and a broad range of metabolites included in the training data, outperforming models designed for individual, well-studied enzyme families. ESP represents enzymes through a modified transformer model, and is trained on data augmented with randomly sampled small molecules assigned as non-substrates. By facilitating easy in silico testing of potential substrates, the ESP web server may support both basic and applied science.},
	language = {en},
	number = {1},
	urldate = {2023-05-23},
	journal = {Nature Communications},
	author = {Kroll, Alexander and Ranjan, Sahasra and Engqvist, Martin K. M. and Lercher, Martin J.},
	month = may,
	year = {2023},
	keywords = {Skimmed, notion, ML Proteins, Substrate},
	pages = {2787},
	file = {Kroll et al. - 2023 - A general model to predict small molecule substrat.pdf:/Users/jason/Zotero/storage/9CSN4LBR/Kroll et al. - 2023 - A general model to predict small molecule substrat.pdf:application/pdf},
}

@article{alamdari_protein_nodate,
	title = {Protein generation with evolutionary diffusion: sequence is all you need},
	abstract = {Deep generative models are increasingly powerful tools for the in silico design of novel proteins. Recently, a family of generative models called diffusion models has demonstrated the ability to generate biologically plausible proteins that are dissimilar to any actual proteins seen in nature, enabling unprecedented capability and control in de novo protein design. However, current state-of-the-art models generate protein structures, which limits the scope of their training data and restricts generations to a small and biased subset of protein design space. Here, we introduce a general-purpose diffusion framework, EvoDiff, that combines evolutionary-scale data with the distinct conditioning capabilities of diffusion models for controllable protein generation in sequence space. EvoDiff generates high-ﬁdelity, diverse, and structurally-plausible proteins that cover natural sequence and functional space. Critically, EvoDiff can generate proteins inaccessible to structure-based models, such as those with disordered regions, while maintaining the ability to design scaffolds for functional structural motifs, demonstrating the universality of our sequence-based formulation. We envision that EvoDiff will expand capabilities in protein engineering beyond the structure-function paradigm toward programmable, sequence-ﬁrst design.},
	language = {en},
	author = {Alamdari, Sarah and Thakkar, Nitya and Lu, Alex X and Fusi, Nicolo and Amini, Ava P and Yang, Kevin K},
    year = {2023},
    keywords = {ML Proteins, notion},
	file = {Alamdari et al. - Protein generation with evolutionary diffusion se.pdf:/Users/jason/Zotero/storage/FLZBLM5C/Alamdari et al. - Protein generation with evolutionary diffusion se.pdf:application/pdf},
}

@article{schwaller_molecular_2019,
	title = {Molecular {Transformer}: {A} {Model} for {Uncertainty}-{Calibrated} {Chemical} {Reaction} {Prediction}},
	volume = {5},
	issn = {2374-7943, 2374-7951},
	shorttitle = {Molecular {Transformer}},
	url = {https://pubs.acs.org/doi/10.1021/acscentsci.9b00576},
	doi = {10.1021/acscentsci.9b00576},
	abstract = {Organic synthesis is one of the key stumbling blocks in medicinal chemistry. A necessary yet unsolved step in planning synthesis is solving the forward problem: Given reactants and reagents, predict the products. Similar to other work, we treat reaction prediction as a machine translation problem between simpliﬁed molecular-input line-entry system (SMILES) strings (a text-based representation) of reactants, reagents, and the products. We show that a multihead attention Molecular Transformer model outperforms all algorithms in the literature, achieving a top-1 accuracy above 90\% on a common benchmark data set. Molecular Transformer makes predictions by inferring the correlations between the presence and absence of chemical motifs in the reactant, reagent, and product present in the data set. Our model requires no handcrafted rules and accurately predicts subtle chemical transformations. Crucially, our model can accurately estimate its own uncertainty, with an uncertainty score that is 89\% accurate in terms of classifying whether a prediction is correct. Furthermore, we show that the model is able to handle inputs without a reactant−reagent split and including stereochemistry, which makes our method universally applicable.},
	language = {en},
	number = {9},
	urldate = {2021-09-29},
	journal = {ACS Central Science},
	author = {Schwaller, Philippe and Laino, Teodoro and Gaudin, Théophile and Bolgar, Peter and Hunter, Christopher A. and Bekas, Costas and Lee, Alpha A.},
	month = sep,
	year = {2019},
	keywords = {notion},
	pages = {1572--1583},
	file = {Schwaller et al. - 2019 - Molecular Transformer A Model for Uncertainty-Cal.pdf:/Users/jason/Zotero/storage/JD4535DF/Schwaller et al. - 2019 - Molecular Transformer A Model for Uncertainty-Cal.pdf:application/pdf},
}

@article{pavlopoulos_unraveling_2023,
	title = {Unraveling the functional dark matter through global metagenomics},
	volume = {622},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-023-06583-7},
	doi = {10.1038/s41586-023-06583-7},
	abstract = {Abstract
            
              Metagenomes encode an enormous diversity of proteins, reflecting a multiplicity of functions and activities
              1,2
              . Exploration of this vast sequence space has been limited to a comparative analysis against reference microbial genomes and protein families derived from those genomes. Here, to examine the scale of yet untapped functional diversity beyond what is currently possible through the lens of reference genomes, we develop a computational approach to generate reference-free protein families from the sequence space in metagenomes. We analyse 26,931 metagenomes and identify 1.17 billion protein sequences longer than 35 amino acids with no similarity to any sequences from 102,491 reference genomes or the Pfam database
              3
              . Using massively parallel graph-based clustering, we group these proteins into 106,198 novel sequence clusters with more than 100 members, doubling the number of protein families obtained from the reference genomes clustered using the same approach. We annotate these families on the basis of their taxonomic, habitat, geographical and gene neighbourhood distributions and, where sufficient sequence diversity is available, predict protein three-dimensional models, revealing novel structures. Overall, our results uncover an enormously diverse functional space, highlighting the importance of further exploring the microbial functional dark matter.},
	language = {en},
	number = {7983},
	urldate = {2023-10-12},
	journal = {Nature},
	author = {Pavlopoulos, Georgios A. and Baltoumas, Fotis A. and Liu, Sirui and Selvitopi, Oguz and Camargo, Antonio Pedro and Nayfach, Stephen and Azad, Ariful and Roux, Simon and Call, Lee and Ivanova, Natalia N. and Chen, I. Min and Paez-Espino, David and Karatzas, Evangelos and {Novel Metagenome Protein Families Consortium} and Acinas, Silvia G. and Ahlgren, Nathan and Attwood, Graeme and Baldrian, Petr and Berry, Timothy and Bhatnagar, Jennifer M. and Bhaya, Devaki and Bidle, Kay D. and Blanchard, Jeffrey L. and Boyd, Eric S. and Bowen, Jennifer L. and Bowman, Jeff and Brawley, Susan H. and Brodie, Eoin L. and Brune, Andreas and Bryant, Donald A. and Buchan, Alison and Cadillo-Quiroz, Hinsby and Campbell, Barbara J. and Cavicchioli, Ricardo and Chuckran, Peter F. and Coleman, Maureen and Crowe, Sean and Colman, Daniel R. and Currie, Cameron R. and Dangl, Jeff and Delherbe, Nathalie and Denef, Vincent J. and Dijkstra, Paul and Distel, Daniel D. and Eloe-Fadrosh, Emiley and Fisher, Kirsten and Francis, Christopher and Garoutte, Aaron and Gaudin, Amelie and Gerwick, Lena and Godoy-Vitorino, Filipa and Guerra, Peter and Guo, Jiarong and Habteselassie, Mussie Y. and Hallam, Steven J. and Hatzenpichler, Roland and Hentschel, Ute and Hess, Matthias and Hirsch, Ann M. and Hug, Laura A. and Hultman, Jenni and Hunt, Dana E. and Huntemann, Marcel and Inskeep, William P. and James, Timothy Y. and Jansson, Janet and Johnston, Eric R. and Kalyuzhnaya, Marina and Kelly, Charlene N. and Kelly, Robert M. and Klassen, Jonathan L. and Nüsslein, Klaus and Kostka, Joel E. and Lindow, Steven and Lilleskov, Erik and Lynes, Mackenzie and Mackelprang, Rachel and Martin, Francis M. and Mason, Olivia U. and McKay, R. Michael and McMahon, Katherine and Mead, David A. and Medina, Monica and Meredith, Laura K. and Mock, Thomas and Mohn, William W. and Moran, Mary Ann and Murray, Alison and Neufeld, Josh D. and Neumann, Rebecca and Norton, Jeanette M. and Partida-Martinez, Laila P. and Pietrasiak, Nicole and Pelletier, Dale and Reddy, T. B. K. and Reese, Brandi Kiel and Reichart, Nicholas J. and Reiss, Rebecca and Saito, Mak A. and Schachtman, Daniel P. and Seshadri, Rekha and Shade, Ashley and Sherman, David and Simister, Rachel and Simon, Holly and Stegen, James and Stepanauskas, Ramunas and Sullivan, Matthew and Sumner, Dawn Y. and Teeling, Hanno and Thamatrakoln, Kimberlee and Treseder, Kathleen and Tringe, Susannah and Vaishampayan, Parag and Valentine, David L. and Waldo, Nicholas B. and Waldrop, Mark P. and Walsh, David A. and Ward, David M. and Wilkins, Michael and Whitman, Thea and Woolet, Jamie and Woyke, Tanja and Iliopoulos, Ioannis and Konstantinidis, Konstantinos and Tiedje, James M. and Pett-Ridge, Jennifer and Baker, David and Visel, Axel and Ouzounis, Christos A. and Ovchinnikov, Sergey and Buluç, Aydin and Kyrpides, Nikos C.},
	month = oct,
	year = {2023},
	keywords = {Annotation, ML Proteins, notion},
	pages = {594--602},
	file = {41586_2023_6583_MOESM1_ESM.docx:/Users/jason/Zotero/storage/M9AGDIRJ/41586_2023_6583_MOESM1_ESM.docx:application/vnd.openxmlformats-officedocument.wordprocessingml.document;Pavlopoulos et al. - 2023 - Unraveling the functional dark matter through glob.pdf:/Users/jason/Zotero/storage/D5NUKGQA/Pavlopoulos et al. - 2023 - Unraveling the functional dark matter through glob.pdf:application/pdf},
}

@article{ferruz_protgpt2_2022,
	title = {{ProtGPT2} is a deep unsupervised language model for protein design},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-32007-7},
	doi = {10.1038/s41467-022-32007-7},
	abstract = {Protein design aims to build novel proteins customized for specific purposes, thereby holding the potential to tackle many environmental and biomedical problems. Recent progress in Transformer-based architectures has enabled the implementation of language models capable of generating text with human-like capabilities. Here, motivated by this success, we describe ProtGPT2, a language model trained on the protein space that generates de novo protein sequences following the principles of natural ones. The generated proteins display natural amino acid propensities, while disorder predictions indicate that 88\% of ProtGPT2-generated proteins are globular, in line with natural sequences. Sensitive sequence searches in protein databases show that ProtGPT2 sequences are distantly related to natural ones, and similarity networks further demonstrate that ProtGPT2 is sampling unexplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences yields well-folded non-idealized structures with embodiments and large loops and reveals topologies not captured in current structure databases. ProtGPT2 generates sequences in a matter of seconds and is freely available.},
	language = {en},
	number = {1},
	urldate = {2023-10-13},
	journal = {Nature Communications},
	author = {Ferruz, Noelia and Schmidt, Steffen and Höcker, Birte},
	month = jul,
	year = {2022},
	keywords = {notion},
	pages = {4348},
	file = {Full Text PDF:/Users/jason/Zotero/storage/54SA6W4B/Ferruz et al. - 2022 - ProtGPT2 is a deep unsupervised language model for.pdf:application/pdf},
}

@article{varadi_alphafold_2022,
	title = {{AlphaFold} {Protein} {Structure} {Database}: massively expanding the structural coverage of protein-sequence space with high-accuracy models},
	volume = {50},
	issn = {0305-1048},
	shorttitle = {{AlphaFold} {Protein} {Structure} {Database}},
	url = {https://doi.org/10.1093/nar/gkab1061},
	doi = {10.1093/nar/gkab1061},
	abstract = {The AlphaFold Protein Structure Database (AlphaFold DB, https://alphafold.ebi.ac.uk) is an openly accessible, extensive database of high-accuracy protein-structure predictions. Powered by AlphaFold v2.0 of DeepMind, it has enabled an unprecedented expansion of the structural coverage of the known protein-sequence space. AlphaFold DB provides programmatic access to and interactive visualization of predicted atomic coordinates, per-residue and pairwise model-confidence estimates and predicted aligned errors. The initial release of AlphaFold DB contains over 360,000 predicted structures across 21 model-organism proteomes, which will soon be expanded to cover most of the (over 100 million) representative sequences from the UniRef90 data set.The AlphaFold Protein Structure Database (AlphaFold DB, https://alphafold.ebi.ac.uk) is an extensive, public database of highly accurate protein structure models. The models are the products of AlphaFold2, an Artificial Intelligence algorithm developed by DeepMind. AlphaFold enabled scientists to investigate an unprecedented number of protein structures. The database we describe here provides access to these predicted models and information on their accuracy. The first version of AlphaFold DB contains over 360,000 models of 21 biologically essential species.},
	number = {D1},
	urldate = {2023-10-17},
	journal = {Nucleic Acids Research},
	author = {Varadi, Mihaly and Anyango, Stephen and Deshpande, Mandar and Nair, Sreenath and Natassia, Cindy and Yordanova, Galabina and Yuan, David and Stroe, Oana and Wood, Gemma and Laydon, Agata and Žídek, Augustin and Green, Tim and Tunyasuvunakool, Kathryn and Petersen, Stig and Jumper, John and Clancy, Ellen and Green, Richard and Vora, Ankur and Lutfi, Mira and Figurnov, Michael and Cowie, Andrew and Hobbs, Nicole and Kohli, Pushmeet and Kleywegt, Gerard and Birney, Ewan and Hassabis, Demis and Velankar, Sameer},
	month = jan,
	year = {2022},
	keywords = {notion},
	pages = {D439--D444},
	file = {Full Text PDF:/Users/jason/Zotero/storage/FPGI6KY9/Varadi et al. - 2022 - AlphaFold Protein Structure Database massively ex.pdf:application/pdf;Snapshot:/Users/jason/Zotero/storage/ZTFLHWLM/6430488.html:text/html},
}

@article{buller_nature_2023,
	title = {From nature to industry: {Harnessing} enzymes for biocatalysis},
	volume = {382},
	issn = {0036-8075, 1095-9203},
	shorttitle = {From nature to industry},
	url = {https://www.science.org/doi/10.1126/science.adh8615},
	doi = {10.1126/science.adh8615},
	abstract = {Biocatalysis harnesses enzymes to make valuable products. This green technology is used in countless applications from bench scale to industrial production and allows practitioners to access complex organic molecules, often with fewer synthetic steps and reduced waste. The last decade has seen an explosion in the development of experimental and computational tools to tailor enzymatic properties, equipping enzyme engineers with the ability to create biocatalysts that perform reactions not present in nature. By using (chemo)-enzymatic synthesis routes or orchestrating intricate enzyme cascades, scientists can synthesize elaborate targets ranging from DNA and complex pharmaceuticals to starch made in vitro from CO
              2
              -derived methanol. In addition, new chemistries have emerged through the combination of biocatalysis with transition metal catalysis, photocatalysis, and electrocatalysis. This review highlights recent key developments, identifies current limitations, and provides a future prospect for this rapidly developing technology.
            
          , 
            Editor’s summary
            
              As nature’s catalysts, enzymes are adaptable, specific, and interoperable. There have been important advances in the identification of transformations catalyzed by enzymes, and new methods have been developed for identifying, engineering, or creating de novo suitable enzymes for a wide range of applications. In a review, Buller
              et al
              . collect this recent progress in biocatalysis research and look ahead toward increasing the use of computational tools, accelerating design test cycles, and expanding chemistry beyond what is familiar in nature. Engineered enzymes provide a green chemistry solution to the synthesis of complex organic molecules and can be used in the degradation of waste plastic and chemicals. —Michael A. Funk
            
          , 
            Engineered enzymes provide a green chemistry solution for synthesis of complex molecules.
          , 
            
              BACKGROUND
              Biocatalysis is an approach to synthetic chemistry in which enzymes carry out chemical reactions. Historically, enzymes from natural sources have been used to break down oils and proteins in laundry detergents, produce semisynthetic antibiotics, and create simple chiral precursors for the pharmaceutical industry. In the past 5 years, the number of available protein sequences has increased by a staggering 20-fold, accelerating the discovery of enzymes with useful activities and properties. Directed evolution, the cornerstone of our ability to tailor enzymatic properties, is allowing researchers to tailor enzymes for the synthesis of complex molecules, the modification of biological therapeutics, and the breakdown of plastic waste. Machine learning–driven protein-structure prediction, coupled with advances in automation and high-throughput screening, is further advancing our ability to create enzymes with desired function. The ability to add nonbiological catalytic elements to enzymes means that enzyme engineers no longer have to rely solely on natural catalytic machinery. Scientists have thus gained the capacity to redesign, reimagine, and repurpose enzymes. Illustrative applications include enzyme cascades to manufacture the antivirals islatravir and molnupiravir, biocatalysts that can generate and control radicals, and enzymes that exploit photocatalysis to affect stereocontrolled C–C couplings, hydroaminations, or Diels-Alder reactions.
            
            
              ADVANCES
              
                The past 5 years have witnessed a surge in the development of data-driven tools enabling the accelerated discovery, engineering, and deployment of enzymes for applications in chemistry, medicine, and food technology. By leveraging enzymes’ ability to control the environment of a chemical reaction, scientists have developed successful platforms for the construction of complex small molecules from suitable starting materials or even CO
                2
                . In addition to their use in (chemo)-enzymatic cascade reactions, precisely tailored biocatalysts can manufacture new therapeutic modalities, such as antisense oligonucleotide therapeutics, and bioconjugates. DNA synthesis, which still relies on phosphoramidite chemistry, is being reinvented with template-independent deoxynucleotidyl transferases to make the process faster and cheaper. Machine learning, already dominating protein-structure prediction and design, is finding applications in enzyme engineering, including improvement of functions such as enantioselectivity, activity, and stability.
              
            
            
              OUTLOOK
              In the decade ahead, biocatalysis research and applications will continue to profit from advances in data mining, machine learning, and DNA reading and writing. The combinatorial design of enzymes and the ease with which new enzyme variants can be experimentally generated lends itself to train data-intense machine-learning algorithms. Ideally, the sequence-function data of variants screened in a directed-evolution campaign could be used to predict which variants to evaluate next. Machine learning works best with clean and reproducible data, mandating the standardization and reporting of data and the further development of experimental techniques, including molecular biology methods, automation, and high-throughput screening assays.
              The toolbox of drug discovery is expanding beyond traditional small molecules (molecular weights {\textless}500 g/mol) to include RNA therapeutics, protein degraders, cyclopeptides, antibody drug conjugates, and gene therapy. Consequently, the synthetic complexity of lead molecules and clinical candidates is increasing with a growing percentage of chiral and beyond-rule-of-5 molecules. Enzymatic synthesis will play a key role beyond its present impact in small-molecule drug discovery and development.
              The discovery of new enzyme families and the rational design of new enzyme functions will expand the toolbox of available biocatalysts. Development and deployment of retrosynthetic tools containing enzymatic reactions will be an important step toward democratizing biocatalysis and making it available to the nonexpert. Fueled by these innovations, nature’s catalysts will be profitably used to address current challenges, including the fight against diseases, provision of affordable clean energy, and reduction of industrial and consumer waste.
              
                
                  
                    The accelerating development of biocatalysis.
                    In silico and experimental tools developed in the last decade allow the fast creation of tailored enzymes. In addition to the concise synthesis of complex small molecules in elegant (chemo)-enzymatic cascades, newly accessible applications include enzymatic DNA synthesis, the generation of therapeutic oligonucleotides, and up- and recycling of plastic waste. [Figure created with Canva]},
	language = {en},
	number = {6673},
	urldate = {2023-11-27},
	journal = {Science},
	author = {Buller, R. and Lutz, S. and Kazlauskas, R. J. and Snajdrova, R. and Moore, J. C. and Bornscheuer, U. T.},
	month = nov,
	year = {2023},
	keywords = {ML Proteins, notion, Protein Engineering, Read, Review},
	pages = {eadh8615},
	file = {Buller et al. - 2023 - From nature to industry Harnessing enzymes for bi.pdf:/Users/jason/Zotero/storage/X39RMA7Y/Buller et al. - 2023 - From nature to industry Harnessing enzymes for bi.pdf:application/pdf},
}

@article{chen_engineering_2020,
	title = {Engineering new catalytic activities in enzymes},
	volume = {3},
	issn = {2520-1158},
	url = {https://www.nature.com/articles/s41929-019-0385-5},
	doi = {10.1038/s41929-019-0385-5},
	language = {en},
	number = {3},
	urldate = {2023-12-12},
	journal = {Nature Catalysis},
	author = {Chen, Kai and Arnold, Frances H.},
	month = jan,
	year = {2020},
	keywords = {notion, Review},
	pages = {203--213},
	file = {Chen and Arnold - 2020 - Engineering new catalytic activities in enzymes.pdf:/Users/jason/Zotero/storage/CGVU62ZT/Chen and Arnold - 2020 - Engineering new catalytic activities in enzymes.pdf:application/pdf},
}

@article{liu_multi-modal_2023,
	title = {Multi-modal molecule structure–text model for text-based retrieval and editing},
	volume = {5},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-023-00759-6},
	doi = {10.1038/s42256-023-00759-6},
	language = {en},
	number = {12},
	urldate = {2023-12-19},
	journal = {Nature Machine Intelligence},
	author = {Liu, Shengchao and Nie, Weili and Wang, Chengpeng and Lu, Jiarui and Qiao, Zhuoran and Liu, Ling and Tang, Jian and Xiao, Chaowei and Anandkumar, Animashree},
	month = dec,
	year = {2023},
	keywords = {notion, Representations, Annotation, Multimodal},
	pages = {1447--1457},
	file = {42256_2023_759_MOESM1_ESM-2.pdf:/Users/jason/Zotero/storage/3KK9ST6L/42256_2023_759_MOESM1_ESM-2.pdf:application/pdf;Liu et al. - 2023 - Multi-modal molecule structure–text model for text.pdf:/Users/jason/Zotero/storage/AGWTG3C3/Liu et al. - 2023 - Multi-modal molecule structure–text model for text.pdf:application/pdf},
}

@article{finnigan_retrobiocat_2023,
	title = {{RetroBioCat} {Database}: {A} {Platform} for {Collaborative} {Curation} and {Automated} {Meta}-{Analysis} of {Biocatalysis} {Data}},
	volume = {13},
	issn = {2155-5435, 2155-5435},
	shorttitle = {{RetroBioCat} {Database}},
	url = {https://pubs.acs.org/doi/10.1021/acscatal.3c01418},
	doi = {10.1021/acscatal.3c01418},
	abstract = {Despite the increasing use of biocatalysis for organic synthesis, there are currently no databases that adequately capture synthetic biotransformations. The lack of a biocatalysis database prevents accelerating biocatalyst characterization efforts from being leveraged to quickly identify candidate enzymes for reactions or cascades, slowing their development. The RetroBioCat Database (available at retrobiocat.com) addresses this gap by capturing information on synthetic biotransformations and providing an analysis platform that allows biocatalysis data to be searched and explored through a range of highly interactive data visualization tools. This database makes it simple to explore available enzymes, their substrate scopes, and how characterized enzymes are related to each other and the wider sequence space. Data entry is facilitated through an openly accessible curation platform, featuring automated tools to accelerate the process. The RetroBioCat Database democratizes biocatalysis knowledge and has the potential to accelerate biocatalytic reaction development, making it a valuable resource for the community.},
	language = {en},
	number = {17},
	urldate = {2024-01-20},
	journal = {ACS Catalysis},
	author = {Finnigan, William and Lubberink, Max and Hepworth, Lorna J. and Citoler, Joan and Mattey, Ashley P. and Ford, Grayson J. and Sangster, Jack and Cosgrove, Sebastian C. and Da Costa, Bruna Zucoloto and Heath, Rachel S. and Thorpe, Thomas W. and Yu, Yuqi and Flitsch, Sabine L. and Turner, Nicholas J.},
	month = sep,
	year = {2023},
	keywords = {notion, Dataset},
	pages = {11771--11780},
	file = {Finnigan et al. - 2023 - RetroBioCat Database A Platform for Collaborative.pdf:/Users/jason/Zotero/storage/XKXWCICA/Finnigan et al. - 2023 - RetroBioCat Database A Platform for Collaborative.pdf:application/pdf},
}

@misc{liu_text-guided_2023,
	title = {A Text-guided Protein Design Framework},
	url = {http://arxiv.org/abs/2302.04611},
	abstract = {Current AI-assisted protein design mainly utilizes protein sequential and structural information. Meanwhile, there exists tremendous knowledge curated by humans in the text format describing proteins’ high-level functionalities. Yet, whether the incorporation of such text data can help protein design tasks has not been explored. To bridge this gap, we propose ProteinDT, a multi-modal framework that leverages textual descriptions for protein design. ProteinDT consists of three subsequent steps: ProteinCLAP which aligns the representation of two modalities, a facilitator that generates the protein representation from the text modality, and a decoder that creates the protein sequences from the representation. To train ProteinDT, we construct a large dataset, SwissProtCLAP, with 441K text and protein pairs. We quantitatively verify the effectiveness of ProteinDT on three challenging tasks: (1) over 90\% accuracy for text-guided protein generation; (2) best hit ratio on 10 zero-shot text-guided protein editing tasks; (3) superior performance on four out of six protein property prediction benchmarks.},
	language = {en},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {Liu, Shengchao and Li, Yanjing and Li, Zhuoxinran and Gitter, Anthony and Zhu, Yutao and Lu, Jiarui and Xu, Zhao and Nie, Weili and Ramanathan, Arvind and Xiao, Chaowei and Tang, Jian and Guo, Hongyu and Anandkumar, Anima},
	month = dec,
	year = {2023},
	note = {arXiv preprint arXiv:2302.04611},
	keywords = {notion, Annotation, Multimodal},
	file = {Liu et al. - 2023 - A Text-guided Protein Design Framework.pdf:/Users/jason/Zotero/storage/YA9T77DT/Liu et al. - 2023 - A Text-guided Protein Design Framework.pdf:application/pdf},
}

@article{samusevich_discovery_nodate,
	title = {Discovery and {Characterization} of {Terpene} {Synthases} {Powered} by {Machine} {Learning}},
	abstract = {Terpene synthases (TPSs) generate the scaffolds of the largest class of natural products, including several first-line medicines. The amount of available protein sequences is increasing exponentially, but computational characterization of their function remains an unsolved challenge. We assembled a curated dataset of one thousand characterized TPS reactions and developed a method to devise highly accurate machine-learning models for functional annotation in a low-data regime. Our models significantly outperform existing methods for TPS detection and substrate prediction. By applying the models to large protein sequence databases, we discovered seven TPS enzymes previously undetected by state-of-the-art computational tools and experimentally confirmed their activity. Furthermore, we discovered a new TPS structural domain and distinct subtypes of previously known domains. This work demonstrates the potential of machine learning to speed up the discovery and characterization of novel TPSs.},
	language = {en},
	author = {Samusevich, Raman and Hebra, Téo and Bushuiev, Roman and Bushuiev, Anton and Kulhánek, Jonáš and Čalounová, Tereza and Perković, Milana and Tajovská, Adéla and Sivic, Josef and Pluskal, Tomáš},
    year = {2024},
	keywords = {notion, Representations, Annotation},
	file = {Samusevich et al. - Discovery and Characterization of Terpene Synthase.pdf:/Users/jason/Zotero/storage/NQJLBXBH/Samusevich et al. - Discovery and Characterization of Terpene Synthase.pdf:application/pdf},
}

@misc{zhang_scientific_2024,
	title = {Scientific {Large} {Language} {Models}: {A} {Survey} on {Biological} \& {Chemical} {Domains}},
	shorttitle = {Scientific {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2401.14656},
	abstract = {Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of "scientific language", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs.},
	language = {en},
	urldate = {2024-02-01},
	publisher = {arXiv},
	author = {Zhang, Qiang and Ding, Keyang and Lyv, Tianwen and Wang, Xinda and Yin, Qingyu and Zhang, Yiwen and Yu, Jing and Wang, Yuhao and Li, Xiaotong and Xiang, Zhuoyi and Zhuang, Xiang and Wang, Zeyuan and Qin, Ming and Zhang, Mengyao and Zhang, Jinlu and Cui, Jiyu and Xu, Renjun and Chen, Hongyang and Fan, Xiaohui and Xing, Huabin and Chen, Huajun},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14656 [cs]},
	keywords = {notion},
	file = {Zhang et al. - 2024 - Scientific Large Language Models A Survey on Biol.pdf:/Users/jason/Zotero/storage/MEM92TWN/Zhang et al. - 2024 - Scientific Large Language Models A Survey on Biol.pdf:application/pdf},
}

@article{yang_opportunities_2024,
	title = {Opportunities and {Challenges} for {Machine} {Learning}-{Assisted} {Enzyme} {Engineering}},
	copyright = {All rights reserved},
	issn = {2374-7943, 2374-7951},
	url = {https://pubs.acs.org/doi/10.1021/acscentsci.3c01275},
	doi = {10.1021/acscentsci.3c01275},
	abstract = {Enzymes can be engineered at the level of their amino acid sequences to optimize key properties such as expression, stability, substrate range, and catalytic efficiency�or even to unlock new catalytic activities not found in nature. Because the search space of possible proteins is vast, enzyme engineering usually involves discovering an enzyme starting point that has some level of the desired activity followed by directed evolution to improve its “fitness” for a desired application. Recently, machine learning (ML) has emerged as a powerful tool to complement this empirical process. ML models can contribute to (1) starting point discovery by functional annotation of known protein sequences or generating novel protein sequences with desired functions and (2) navigating protein fitness landscapes for fitness optimization by learning mappings between protein sequences and their associated fitness values. In this Outlook, we explain how ML complements enzyme engineering and discuss its future potential to unlock improved engineering outcomes.},
	language = {en},
	urldate = {2024-02-07},
	journal = {ACS Central Science},
	author = {Yang, Jason and Li, Francesca-Zhoufan and Arnold, Frances H.},
	month = feb,
	year = {2024},
	keywords = {notion},
	pages = {acscentsci.3c01275},
	file = {Yang et al. - 2024 - Opportunities and Challenges for Machine Learning-.pdf:/Users/jason/Zotero/storage/XFHRYXWZ/Yang et al. - 2024 - Opportunities and Challenges for Machine Learning-.pdf:application/pdf},
}

@misc{wang_diffusion_2024,
	title = {Diffusion {Language} {Models} {Are} {Versatile} {Protein} {Learners}},
	url = {http://arxiv.org/abs/2402.18567},
	abstract = {This paper introduces diffusion protein language model (DPLM), a versatile protein language model that demonstrates strong generative and predictive capabilities for protein sequences. We first pre-train scalable DPLMs from evolutionaryscale protein sequences within a generative selfsupervised discrete diffusion probabilistic framework, which generalizes language modeling for proteins in a principled way. After pre-training, DPLM exhibits the ability to generate structurally plausible, novel and diverse protein sequences for unconditional generation. We further demonstrate the proposed diffusion generative pre-training make DPLM possess a better understanding of proteins, making it a superior representation learner, which can be fine-tuned for various predictive tasks, comparing favorably to ESM2 (Lin et al., 2022). Moreover, DPLM can be tailored for various needs, which showcases its prowess of conditional generation in several ways: (1) conditioning on partial peptide sequences, e.g., generating scaffolds for functional motifs with high success rate; (2) incorporating other modalities as conditioner, e.g., structure-conditioned generation for inverse folding; and (3) steering sequence generation towards desired properties, e.g., satisfying specified secondary structures, through a plug-and-play classifier guidance.},
	language = {en},
	urldate = {2024-03-05},
	publisher = {arXiv},
	author = {Wang, Xinyou and Zheng, Zaixiang and Ye, Fei and Xue, Dongyu and Huang, Shujian and Gu, Quanquan},
	month = feb,
	year = {2024},
	note = {arXiv:2402.18567 [cs, q-bio]},
	keywords = {notion, ML Proteins, Representations, Diffusion},
	file = {Wang et al. - 2024 - Diffusion Language Models Are Versatile Protein Le.pdf:/Users/jason/Zotero/storage/SVMQG7WX/Wang et al. - 2024 - Diffusion Language Models Are Versatile Protein Le.pdf:application/pdf},
}

@misc{tavakoli_rxn_2022,
	title = {Rxn {Hypergraph}: a {Hypergraph} {Attention} {Model} for {Chemical} {Reaction} {Representation}},
	shorttitle = {Rxn {Hypergraph}},
	url = {http://arxiv.org/abs/2201.01196},
	abstract = {It is fundamental for science and technology to be able to predict chemical reactions and their properties. To achieve such skills, it is important to develop good representations of chemical reactions, or good deep learning architectures that can learn such representations automatically from the data. There is currently no universal and widely adopted method for robustly representing chemical reactions. Most existing methods suffer from one or more drawbacks, such as: (1) lacking universality; (2) lacking robustness; (3) lacking interpretability; or (4) requiring excessive manual pre-processing. Here we exploit graph-based representations of molecular structures to develop and test a hypergraph attention neural network approach to solve at once the reaction representation and property-prediction problems, alleviating the aforementioned drawbacks. We evaluate this hypergraph representation in three experiments using three independent data sets of chemical reactions. In all experiments, the hypergraphbased approach matches or outperforms other representations and their corresponding models of chemical reactions while yielding interpretable multi-level representations.},
	language = {en},
	urldate = {2024-03-07},
	publisher = {arXiv},
	author = {Tavakoli, Mohammadamin and Shmakov, Alexander and Ceccarelli, Francesco and Baldi, Pierre},
	month = jan,
	year = {2022},
	note = {arXiv:2201.01196 [physics]},
	keywords = {notion, ML Chemistry, Representations},
	file = {Tavakoli et al. - 2022 - Rxn Hypergraph a Hypergraph Attention Model for C.pdf:/Users/jason/Zotero/storage/468I3KMF/Tavakoli et al. - 2022 - Rxn Hypergraph a Hypergraph Attention Model for C.pdf:application/pdf},
}

@misc{khosla_supervised_2021,
	title = {Supervised {Contrastive} {Learning}},
	url = {http://arxiv.org/abs/2004.11362},
	abstract = {Contrastive learning applied to self-supervised representation learning has seen a resurgence in recent years, leading to state of the art performance in the unsupervised training of deep image models. Modern batch contrastive approaches subsume or signiﬁcantly outperform traditional contrastive losses such as triplet, max-margin and the N-pairs loss. In this work, we extend the self-supervised batch contrastive approach to the fully-supervised setting, allowing us to effectively leverage label information. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. We analyze two possible versions of the supervised contrastive (SupCon) loss, identifying the best-performing formulation of the loss. On ResNet-200, we achieve top-1 accuracy of 81.4\% on the ImageNet dataset, which is 0.8\% above the best number reported for this architecture. We show consistent outperformance over cross-entropy on other datasets and two ResNet variants. The loss shows beneﬁts for robustness to natural corruptions, and is more stable to hyperparameter settings such as optimizers and data augmentations. Our loss function is simple to implement and reference TensorFlow code is released at https://t.ly/supcon 1.},
	language = {en},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
	month = mar,
	year = {2021},
	note = {arXiv:2004.11362 [cs, stat]},
	keywords = {notion, ML},
	file = {Khosla et al. - 2021 - Supervised Contrastive Learning.pdf:/Users/jason/Zotero/storage/6P8KGXZC/Khosla et al. - 2021 - Supervised Contrastive Learning.pdf:application/pdf},
}

@article{schwaller_mapping_2021,
	title = {Mapping the space of chemical reactions using attention-based neural networks},
	volume = {3},
	issn = {2522-5839},
	url = {https://www.nature.com/articles/s42256-020-00284-w},
	doi = {10.1038/s42256-020-00284-w},
	language = {en},
	number = {2},
	urldate = {2024-03-10},
	journal = {Nature Machine Intelligence},
	author = {Schwaller, Philippe and Probst, Daniel and Vaucher, Alain C. and Nair, Vishnu H. and Kreutter, David and Laino, Teodoro and Reymond, Jean-Louis},
	month = jan,
	year = {2021},
	keywords = {notion, ML Chemistry},
	pages = {144--152},
	file = {Schwaller et al. - 2021 - Mapping the space of chemical reactions using atte.pdf:/Users/jason/Zotero/storage/8V7JJBNH/Schwaller et al. - 2021 - Mapping the space of chemical reactions using atte.pdf:application/pdf},
}

@article{probst_reaction_2022,
	title = {Reaction classification and yield prediction using the differential reaction fingerprint {DRFP}},
	volume = {1},
	issn = {2635-098X},
	url = {http://xlink.rsc.org/?DOI=D1DD00006C},
	doi = {10.1039/D1DD00006C},
	abstract = {Differential Reaction Fingerprint DRFP is a chemical reaction fingerprint enabling simple machine learning models running on standard hardware to reach DFT- and deep learning-based accuracies in reaction yield prediction and reaction classification.
          , 
            
              Predicting the nature and outcome of reactions using computational methods is a crucial tool to accelerate chemical research. The recent application of deep learning-based learned fingerprints to reaction classification and reaction yield prediction has shown an impressive increase in performance compared to previous methods such as DFT- and structure-based fingerprints. However, learned fingerprints require large training data sets, are inherently biased, and are based on complex deep learning architectures. Here we present the differential reaction fingerprint
              DRFP
              . The
              DRFP
              algorithm takes a reaction SMILES as an input and creates a binary fingerprint based on the symmetric difference of two sets containing the circular molecular
              n
              -grams generated from the molecules listed left and right from the reaction arrow, respectively, without the need for distinguishing between reactants and reagents. We show that
              DRFP
              performs better than DFT-based fingerprints in reaction yield prediction and other structure-based fingerprints in reaction classification, reaching the performance of state-of-the-art learned fingerprints in both tasks while being data-independent.},
	language = {en},
	number = {2},
	urldate = {2024-03-11},
	journal = {Digital Discovery},
	author = {Probst, Daniel and Schwaller, Philippe and Reymond, Jean-Louis},
	year = {2022},
	keywords = {notion, ML Chemistry},
	pages = {91--97},
	file = {Probst et al. - 2022 - Reaction classification and yield prediction using.pdf:/Users/jason/Zotero/storage/KHJ5WE45/Probst et al. - 2022 - Reaction classification and yield prediction using.pdf:application/pdf},
}

@misc{bran_chemcrow_2023,
	title = {{ChemCrow}: {Augmenting} large-language models with chemistry tools},
	shorttitle = {{ChemCrow}},
	url = {http://arxiv.org/abs/2304.05376},
	abstract = {Over the last decades, excellent computational chemistry tools have been developed. Integrating them into a single platform with enhanced accessibility could help reaching their full potential by overcoming steep learning curves. Recently, large-language models (LLMs) have shown strong performance in tasks across domains, but struggle with chemistry-related problems. Moreover, these models lack access to external knowledge sources, limiting their usefulness in scientific applications. In this study, we introduce ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design. By integrating 18 expert-designed tools, ChemCrow augments the LLM performance in chemistry, and new capabilities emerge. Our agent autonomously planned and executed the syntheses of an insect repellent, three organocatalysts, and guided the discovery of a novel chromophore. Our evaluation, including both LLM and expert assessments, demonstrates ChemCrow’s effectiveness in automating a diverse set of chemical tasks. Surprisingly, we find that GPT-4 as an evaluator cannot distinguish between clearly wrong GPT-4 completions and Chemcrow’s performance. Our work not only aids expert chemists and lowers barriers for non-experts, but also fosters scientific advancement by bridging the gap between experimental and computational chemistry. Publicly available code can be found at https://github.com/ur-whitelab/chemcrow-public.},
	language = {en},
	urldate = {2024-03-12},
	publisher = {arXiv},
	author = {Bran, Andres M. and Cox, Sam and Schilter, Oliver and Baldassari, Carlo and White, Andrew D. and Schwaller, Philippe},
	month = oct,
	year = {2023},
	note = {arXiv:2304.05376 [physics, stat]},
	keywords = {notion},
	file = {Bran et al. - 2023 - ChemCrow Augmenting large-language models with ch.pdf:/Users/jason/Zotero/storage/IDZ4RB94/Bran et al. - 2023 - ChemCrow Augmenting large-language models with ch.pdf:application/pdf},
}

@misc{xiao_molbind_2024,
	title = {{MolBind}: {Multimodal} {Alignment} of {Language}, {Molecules}, and {Proteins}},
	shorttitle = {{MolBind}},
	url = {http://arxiv.org/abs/2403.08167},
	abstract = {Recent advancements in biology and chemistry have leveraged multi-modal learning, integrating molecules and their natural language descriptions to enhance drug discovery. However, current pre-training frameworks are limited to two modalities, and designing a unified network to process different modalities (e.g., natural language, 2D molecular graphs, 3D molecular conformations, and 3D proteins) remains challenging due to inherent gaps among them. In this work, we propose MOLBIND, a framework that trains encoders for multiple modalities through contrastive learning, mapping all modalities to a shared feature space for multi-modal semantic alignment. To facilitate effective pre-training of MOLBIND on multiple modalities, we also build and collect a high-quality dataset with four modalities, MolBind-M4, including graph-language, conformation-language, graph-conformation, and conformation-protein paired data. MOLBIND shows superior zero-shot learning performance across a wide range of tasks, demonstrating its strong capability of capturing the underlying semantics of multiple modalities. Our code and data can be found at MolBind.},
	language = {en},
	urldate = {2024-03-14},
	publisher = {arXiv},
	author = {Xiao, Teng and Cui, Chao and Zhu, Huaisheng and Honavar, Vasant G.},
	month = mar,
	year = {2024},
	note = {arXiv:2403.08167 [cs, q-bio]},
	keywords = {notion, Multimodal},
	file = {Xiao et al. - 2024 - MolBind Multimodal Alignment of Language, Molecul.pdf:/Users/jason/Zotero/storage/UPKLK4DU/Xiao et al. - 2024 - MolBind Multimodal Alignment of Language, Molecul.pdf:application/pdf},
}

@article{boorla_catpred_nodate,
	title = {{CatPred}: {A} comprehensive framework for deep learning in vitro enzyme kinetic parameters kcat, {Km} and {Ki}},
	language = {en},
	author = {Boorla, Veda Sheersh},
	keywords = {notion, Dataset},
	file = {Boorla - CatPred A comprehensive framework for deep learni.pdf:/Users/jason/Zotero/storage/APHX7RVM/Boorla - CatPred A comprehensive framework for deep learni.pdf:application/pdf},
}

@techreport{boadu_improving_2024,
	type = {preprint},
	title = {Improving protein function prediction by learning and integrating representations of protein sequences and function labels},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.03.11.584495},
	abstract = {Motivation: As fewer than 1\% of proteins have protein function information determined experimentally, computationally predicting the function of proteins is critical for obtaining functional information for most proteins and has been a major challenge in protein bioinformatics. Despite the significant progress made in protein function prediction by the community in the last decade, the general accuracy of protein function prediction is still not high, particularly for rare function terms associated with few proteins in the protein function annotation database such as the UniProt.},
	language = {en},
	urldate = {2024-03-15},
	institution = {Bioinformatics},
	author = {Boadu, Frimpong and Cheng, Jianlin},
	month = mar,
	year = {2024},
	doi = {10.1101/2024.03.11.584495},
	keywords = {notion, Multimodal},
	file = {Boadu and Cheng - 2024 - Improving protein function prediction by learning .pdf:/Users/jason/Zotero/storage/EYEJGWK7/Boadu and Cheng - 2024 - Improving protein function prediction by learning .pdf:application/pdf},
}

@misc{zhang_survey_2023,
	title = {A {Survey} of {Controllable} {Text} {Generation} using {Transformer}-based {Pre}-trained {Language} {Models}},
	url = {http://arxiv.org/abs/2201.05337},
	abstract = {Controllable Text Generation (CTG) is emerging area in the field of natural language generation (NLG). It is regarded as crucial for the development of advanced text generation technologies that better meet the specific constraints in practical applications. In recent years, methods using large-scale pre-trained language models (PLMs), in particular the widely used transformer-based PLMs, have become a new paradigm of NLG, allowing generation of more diverse and fluent text. However, due to the limited level of interpretability of deep neural networks, the controllability of these methods need to be guaranteed. To this end, controllable text generation using transformer-based PLMs has become a rapidly growing yet challenging new research hotspot. A diverse range of approaches have emerged in the recent 3-4 years, targeting different CTG tasks that require different types of controlled constraints. In this paper, we present a systematic critical review on the common tasks, main approaches, and evaluation methods in this area. Finally, we discuss the challenges that the field is facing, and put forward various promising future directions. To the best of our knowledge, this is the first survey paper to summarize the state-of-the-art CTG techniques from the perspective of Transformer-based PLMs. We hope it can help researchers and practitioners in the related fields to quickly track the academic and technological frontier, providing them with a landscape of the area and a roadmap for future research. CCS Concepts: • General and reference → Surveys and overviews; • Computing methodologies → Natural language processing.},
	language = {en},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
	month = aug,
	year = {2023},
	note = {arXiv:2201.05337 [cs]},
	keywords = {notion},
	file = {Zhang et al. - 2023 - A Survey of Controllable Text Generation using Tra.pdf:/Users/jason/Zotero/storage/8IZ9X4YU/Zhang et al. - 2023 - A Survey of Controllable Text Generation using Tra.pdf:application/pdf},
}

@inproceedings{ribeiro_structural_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Structural {Adapters} in {Pretrained} {Language} {Models} for {AMR}-to-{Text} {Generation}},
	url = {https://aclanthology.org/2021.emnlp-main.351},
	doi = {10.18653/v1/2021.emnlp-main.351},
	language = {en},
	urldate = {2024-03-17},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ribeiro, Leonardo F. R. and Zhang, Yue and Gurevych, Iryna},
	year = {2021},
	keywords = {ML, notion},
	pages = {4269--4282},
	file = {Ribeiro et al. - 2021 - Structural Adapters in Pretrained Language Models .pdf:/Users/jason/Zotero/storage/5UQP54QF/Ribeiro et al. - 2021 - Structural Adapters in Pretrained Language Models .pdf:application/pdf},
}

@article{schneider_development_2015,
	title = {Development of a {Novel} {Fingerprint} for {Chemical} {Reactions} and {Its} {Application} to {Large}-{Scale} {Reaction} {Classification} and {Similarity}},
	volume = {55},
	issn = {1549-9596, 1549-960X},
	url = {https://pubs.acs.org/doi/10.1021/ci5006614},
	doi = {10.1021/ci5006614},
	abstract = {Fingerprint methods applied to molecules have proven to be useful for similarity determination and as inputs to machine-learning models. Here, we present the development of a new ﬁngerprint for chemical reactions and validate its usefulness in building machine-learning models and in similarity assessment. Our ﬁnal ﬁngerprint is constructed as the diﬀerence of the atompair ﬁngerprints of products and reactants and includes agents via calculated physicochemical properties. We validated the ﬁngerprints on a large data set of reactions text-mined from granted United States patents from the last 40 years that have been classiﬁed using a substructure-based expert system. We applied machine learning to build a 50-class predictive model for reactiontype classiﬁcation that correctly predicts 97\% of the reactions in an external test set. Impressive accuracies were also observed when applying the classiﬁer to reactions from an in-house electronic laboratory notebook. The performance of the novel ﬁngerprint for assessing reaction similarity was evaluated by a cluster analysis that recovered 48 out of 50 of the reaction classes with a median F-score of 0.63 for the clusters. The data sets used for training and primary validation as well as all python scripts required to reproduce the analysis are provided in the Supporting Information.},
	language = {en},
	number = {1},
	urldate = {2024-03-19},
	journal = {Journal of Chemical Information and Modeling},
	author = {Schneider, Nadine and Lowe, Daniel M. and Sayle, Roger A. and Landrum, Gregory A.},
	month = jan,
	year = {2015},
	keywords = {notion, ML Chemistry, Representations},
	pages = {39--53},
	file = {Schneider et al. - 2015 - Development of a Novel Fingerprint for Chemical Re.pdf:/Users/jason/Zotero/storage/HBVIHHNL/Schneider et al. - 2015 - Development of a Novel Fingerprint for Chemical Re.pdf:application/pdf},
}

@misc{gao_simcse_2022,
	title = {{SimCSE}: {Simple} {Contrastive} {Learning} of {Sentence} {Embeddings}},
	shorttitle = {{SimCSE}},
	url = {http://arxiv.org/abs/2104.08821},
	abstract = {This paper presents SimCSE, a simple contrastive learning framework that greatly advances state-of-the-art sentence embeddings. We first describe an unsupervised approach, which takes an input sentence and predicts itself in a contrastive objective, with only standard dropout used as noise. This simple method works surprisingly well, performing on par with previous supervised counterparts. We find that dropout acts as minimal data augmentation, and removing it leads to a representation collapse. Then, we propose a supervised approach, which incorporates annotated pairs from natural language inference datasets into our contrastive learning framework by using "entailment" pairs as positives and "contradiction" pairs as hard negatives. We evaluate SimCSE on standard semantic textual similarity (STS) tasks, and our unsupervised and supervised models using BERT base achieve an average of 76.3\% and 81.6\% Spearman's correlation respectively, a 4.2\% and 2.2\% improvement compared to the previous best results. We also show -- both theoretically and empirically -- that the contrastive learning objective regularizes pre-trained embeddings' anisotropic space to be more uniform, and it better aligns positive pairs when supervised signals are available.},
	language = {en},
	urldate = {2024-03-20},
	publisher = {arXiv},
	author = {Gao, Tianyu and Yao, Xingcheng and Chen, Danqi},
	month = may,
	year = {2022},
	note = {arXiv:2104.08821 [cs]},
	keywords = {notion, ML},
	file = {Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf:/Users/jason/Zotero/storage/CLEULM6L/Gao et al. - 2022 - SimCSE Simple Contrastive Learning of Sentence Em.pdf:application/pdf},
}

@techreport{robinson_contrasting_2023,
	type = {preprint},
	title = {Contrasting {Sequence} with {Structure}: {Pre}-training {Graph} {Representations} with {PLMs}},
	shorttitle = {Contrasting {Sequence} with {Structure}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.12.01.569611},
	abstract = {Understanding protein function is vital for drug discovery, disease diagnosis, and protein engineering. While Protein Language Models (PLMs) pre-trained on vast protein sequence datasets have achieved remarkable success, equivalent Protein Structure Models (PSMs) remain underrepresented. We attribute this to the relative lack of high-confidence structural data and suitable pre-training objectives. In this context, we introduce BioCLIP, a contrastive learning framework that pre-trains PSMs by leveraging PLMs, generating meaningful per-residue and per-chain structural representations. When evaluated on tasks such as protein-protein interaction, Gene Ontology annotation, and Enzyme Commission number prediction, BioCLIPtrained PSMs consistently outperform models trained from scratch and further enhance performance when merged with sequence embeddings. Notably, BioCLIP approaches, or exceeds, specialized methods across all benchmarks using its singular pre-trained design. Our work addresses the challenges of obtaining quality structural data and designing self-supervised objectives, setting the stage for more comprehensive models of protein function. Source code is publicly available2.},
	language = {en},
	urldate = {2024-03-23},
	institution = {Bioinformatics},
	author = {Robinson, Louis Callum Butler and Atkinson, Timothy and Copoiu, Liviu and Bordes, Patrick and Pierrot, Thomas and Barrett, Thomas},
	month = dec,
	year = {2023},
	doi = {10.1101/2023.12.01.569611},
	keywords = {notion, ML Proteins, Structure},
	file = {Robinson et al. - 2023 - Contrasting Sequence with Structure Pre-training .pdf:/Users/jason/Zotero/storage/35DHGXB6/Robinson et al. - 2023 - Contrasting Sequence with Structure Pre-training .pdf:application/pdf},
}

@article{notin_proteingym_nodate,
	title = {{ProteinGym}: {Large}-{Scale} {Benchmarks} for {Protein} {Fitness} {Prediction} and {Design}},
	abstract = {Predicting the effects of mutations in proteins is critical to many applications, from understanding genetic disease to designing novel proteins that can address our most pressing challenges in climate, agriculture and healthcare. Despite a surge in machine learning-based protein models to tackle these questions, an assessment of their respective benefits is challenging due to the use of distinct, often contrived, experimental datasets, and the variable performance of models across different protein families. Addressing these challenges requires scale. To that end we introduce ProteinGym, a large-scale and holistic set of benchmarks specifically designed for protein fitness prediction and design. It encompasses both a broad collection of over 250 standardized deep mutational scanning assays, spanning millions of mutated sequences, as well as curated clinical datasets providing highquality expert annotations about mutation effects. We devise a robust evaluation framework that combines metrics for both fitness prediction and design, factors in known limitations of the underlying experimental methods, and covers both zero-shot and supervised settings. We report the performance of a diverse set of over 70 high-performing models from various subfields (eg., alignment-based, inverse folding) into a unified benchmark suite. We open source the corresponding codebase, datasets, MSAs, structures, model predictions and develop a user-friendly website that facilitates data access and analysis.},
	language = {en},
    year = {2023},
	author = {Notin, Pascal and Kollasch, Aaron W and Ritter, Daniel and van Niekerk, Lood and Paul, Steffanie and Spinner, Hansen and Rollins, Nathan and Shaw, Ada and Weitzman, Ruben and Frazer, Jonathan and Dias, Mafalda and Franceschi, Dinko and Orenbuch, Rose and Gal, Yarin and Marks, Debora S},
	keywords = {Dataset, notion},
	file = {Notin et al. - ProteinGym Large-Scale Benchmarks for Protein Fit.pdf:/Users/jason/Zotero/storage/EWHFKK2T/Notin et al. - ProteinGym Large-Scale Benchmarks for Protein Fit.pdf:application/pdf},
}

@article{probst_biocatalysed_2022,
	title = {Biocatalysed synthesis planning using data-driven learning},
	volume = {13},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-28536-w},
	doi = {10.1038/s41467-022-28536-w},
	abstract = {Abstract
            Enzyme catalysts are an integral part of green chemistry strategies towards a more sustainable and resource-efficient chemical synthesis. However, the use of biocatalysed reactions in retrosynthetic planning clashes with the difficulties in predicting the enzymatic activity on unreported substrates and enzyme-specific stereo- and regioselectivity. As of now, only rule-based systems support retrosynthetic planning using biocatalysis, while initial data-driven approaches are limited to forward predictions. Here, we extend the data-driven forward reaction as well as retrosynthetic pathway prediction models based on the Molecular Transformer architecture to biocatalysis. The enzymatic knowledge is learned from an extensive data set of publicly available biochemical reactions with the aid of a new class token scheme based on the enzyme commission classification number, which captures catalysis patterns among different enzymes belonging to the same hierarchy. The forward reaction prediction model (top-1 accuracy of 49.6\%), the retrosynthetic pathway (top-1 single-step round-trip accuracy of 39.6\%) and the curated data set are made publicly available to facilitate the adoption of enzymatic catalysis in the design of greener chemistry processes.},
	language = {en},
	number = {1},
	urldate = {2024-04-05},
	journal = {Nature Communications},
	author = {Probst, Daniel and Manica, Matteo and Nana Teukam, Yves Gaetan and Castrogiovanni, Alessandro and Paratore, Federico and Laino, Teodoro},
	month = feb,
	year = {2022},
	keywords = {notion, Dataset, Annotation},
	pages = {964},
	file = {Probst et al. - 2022 - Biocatalysed synthesis planning using data-driven .pdf:/Users/jason/Zotero/storage/QDJDXDZY/Probst et al. - 2022 - Biocatalysed synthesis planning using data-driven .pdf:application/pdf},
}

@article{finnigan_retrobiocat_2021,
	title = {{RetroBioCat} as a computer-aided synthesis planning tool for biocatalytic reactions and cascades},
	volume = {4},
	issn = {2520-1158},
	url = {https://www.nature.com/articles/s41929-020-00556-z},
	doi = {10.1038/s41929-020-00556-z},
	language = {en},
	number = {2},
	urldate = {2024-04-05},
	journal = {Nature Catalysis},
	author = {Finnigan, William and Hepworth, Lorna J. and Flitsch, Sabine L. and Turner, Nicholas J.},
	month = jan,
	year = {2021},
	keywords = {notion, Dataset},
	pages = {98--104},
	file = {Finnigan et al. - 2021 - RetroBioCat as a computer-aided synthesis planning.pdf:/Users/jason/Zotero/storage/N9DV4PAI/Finnigan et al. - 2021 - RetroBioCat as a computer-aided synthesis planning.pdf:application/pdf},
}

@misc{harding-larsen_protein_2024,
	title = {Protein {Representations}: {Encoding} {Biological} {Information} for {Machine} {Learning} in {Biocatalysis}},
	copyright = {https://creativecommons.org/licenses/by-nc-nd/4.0/},
	shorttitle = {Protein {Representations}},
	url = {https://chemrxiv.org/engage/chemrxiv/article-details/660bf8839138d23161f4abbe},
	doi = {10.26434/chemrxiv-2024-7hwf7},
	abstract = {Enzymes offer a more environmentally friendly and low-impact solution to conventional chemistry, but they often require additional engineering for industrial settings, an endeavor that is challenging and laborious. To address this issue, the power of machine learning can be harnessed to produce predictive models that facilitate in silico study and engineering of novel enzymatic properties. However, the conversion from the biological domain to the computational realm requires special attention to ensure the training of accurate and precise models. In this review, we examine the critical step of encoding protein information to numeric representations for use in machine learning. We selected the most important approaches for encoding the three distinct biological protein representations — primary sequence, 3D structure, and dynamics — to explore their requirements for employment and inherent biases. Combined representations of proteins and substrates are also introduced as emergent tools in biocatalysis. We propose the division of fixed representations, a collection of rule-based encoding strategies, and learned representations extracted from the latent spaces of large neural networks. To select the most suitable protein representation, we propose two main factors governing this choice. The first one is the model setup, being influenced by the size of the training dataset and the choice of architecture. The second factor is the model objectives, concerning the assayed property, the difference between wild-type models and mutant predictors, and requirements for explainability. This review is aimed at serving as a source of information and guidance for properly representing enzymes in future machine learning models for biocatalysis.},
	language = {en},
	urldate = {2024-04-05},
	author = {Harding-Larsen, David and Funk, Jonathan and Madsen, Niklas Gesmar and Gharabli, Hani and Acevedo-Rocha, Carlos G. and Mazurenko, Stanislav and Welner, Ditte Hededam},
	month = apr,
	year = {2024},
	keywords = {notion, Review},
	file = {Harding-Larsen et al. - 2024 - Protein Representations Encoding Biological Infor.pdf:/Users/jason/Zotero/storage/YNT7LE3R/Harding-Larsen et al. - 2024 - Protein Representations Encoding Biological Infor.pdf:application/pdf},
}

@article{carbonell_selenzyme_2018,
	title = {Selenzyme: enzyme selection tool for pathway design},
	volume = {34},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {1367-4803, 1367-4811},
	shorttitle = {Selenzyme},
	url = {https://academic.oup.com/bioinformatics/article/34/12/2153/4841713},
	doi = {10.1093/bioinformatics/bty065},
	abstract = {Summary: Synthetic biology applies the principles of engineering to biology in order to create biological functionalities not seen before in nature. One of the most exciting applications of synthetic biology is the design of new organisms with the ability to produce valuable chemicals including pharmaceuticals and biomaterials in a greener; sustainable fashion. Selecting the right enzymes to catalyze each reaction step in order to produce a desired target compound is, however, not trivial. Here, we present Selenzyme, a free online enzyme selection tool for metabolic pathway design. The user is guided through several decision steps in order to shortlist the best candidates for a given pathway step. The tool graphically presents key information about enzymes based on existing databases and tools such as: similarity of sequences and of catalyzed reactions; phylogenetic distance between source organism and intended host species; multiple alignment highlighting conserved regions, predicted catalytic site, and active regions and relevant properties such as predicted solubility and transmembrane regions. Selenzyme provides bespoke sequence selection for automated workﬂows in biofoundries.},
	language = {en},
	number = {12},
	urldate = {2024-04-07},
	journal = {Bioinformatics},
	author = {Carbonell, Pablo and Wong, Jerry and Swainston, Neil and Takano, Eriko and Turner, Nicholas J and Scrutton, Nigel S and Kell, Douglas B and Breitling, Rainer and Faulon, Jean-Loup},
	editor = {Stegle, Oliver},
	month = jun,
	year = {2018},
	keywords = {Read, Annotation},
	pages = {2153--2154},
	file = {Carbonell et al. - 2018 - Selenzyme enzyme selection tool for pathway desig.pdf:/Users/jason/Zotero/storage/KMIZGTH8/Carbonell et al. - 2018 - Selenzyme enzyme selection tool for pathway desig.pdf:application/pdf},
}

@misc{beltagy_scibert_2019,
	title = {{SciBERT}: {A} {Pretrained} {Language} {Model} for {Scientific} {Text}},
	shorttitle = {{SciBERT}},
	url = {http://arxiv.org/abs/1903.10676},
	abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
	language = {en},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
	month = sep,
	year = {2019},
	note = {arXiv:1903.10676 [cs]},
	file = {Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf:/Users/jason/Zotero/storage/VGXAU3MA/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf:application/pdf},
}

@article{richman_curation_2022,
	title = {Curation of a list of chemicals in biosolids from {EPA} {National} {Sewage} {Sludge} {Surveys} \& {Biennial} {Review} {Reports}},
	volume = {9},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01267-9},
	doi = {10.1038/s41597-022-01267-9},
	abstract = {Abstract
            Section 405(d) of the Clean Water Act requires the US Environmental Protection Agency to review sewage sludge regulations every two years to identify any additional pollutants that may occur in biosolids and to set regulations for pollutants identified in biosolids if sufficient scientific evidence shows they may harm human health or the environment. To date, EPA has conducted eight biennial reviews to identify chemical and microbial pollutants and three national sewage sludge surveys to identify pollutants and obtain concentration data for chemicals found in biosolids. Prior to 2021, there was inconsistent reporting of chemicals identified and EPA did not cumulatively track chemicals in biosolids. Through the efforts presented here, EPA produced a list of 726 chemicals and structure-based classes found in biosolids based on biennial reviews and national sewage sludge surveys. Summary statistics of concentration data are also reported for the 484 chemicals found in the three national sewage sludge surveys. The creation of the Biosolids List supports EPA in assessing the potential risk of chemical pollutants found in biosolids.},
	language = {en},
	number = {1},
	urldate = {2024-04-10},
	journal = {Scientific Data},
	author = {Richman, Tess and Arnold, Elyssa and Williams, Antony J.},
	month = apr,
	year = {2022},
	keywords = {Dataset},
	pages = {180},
	file = {Richman et al. - 2022 - Curation of a list of chemicals in biosolids from .pdf:/Users/jason/Zotero/storage/K9A7V4HN/Richman et al. - 2022 - Curation of a list of chemicals in biosolids from .pdf:application/pdf},
}

@misc{ramakrishnan_functional_2023,
	title = {Functional profiling of the sequence stockpile: a review and assessment of in silico prediction tools},
	shorttitle = {Functional profiling of the sequence stockpile},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.07.12.548726},
	doi = {10.1101/2023.07.12.548726},
	abstract = {In silico functional annotation of proteins is crucial to narrowing the sequencing-accelerated gap in our understanding of protein activities. Numerous function annotation methods exist, and their ranks have been growing, particularly so with the recent deep learning-based developments. However, it is unclear if these tools are truly predictive. As we are not aware of any methods that can identify new terms in functional ontologies, we ask if they can, at least, identify molecular functions of new protein sequences that are non-homologous to or far-removed from known protein families. Here, we explore the potential and limitations of the existing methods in predicting molecular functions of thousands of such orphan proteins. Lacking the ground truth functional annotations, we transformed the assessment of function prediction into evaluation of functional similarity of orphan siblings, i.e. pairs of proteins that likely share function, but that are unlike any of the currently functionally annotated sequences. Notably, our approach transcends the limitations of functional annotation vocabularies and provides a platform to compare different methods without the need for mapping terms across ontologies. We find that most existing methods are limited to identifying functional similarity of homologous sequences and are thus descriptive, rather than predictive of function. Curiously, despite their seemingly unlimited by-homology scope, novel deep learning methods also remain far from capturing functional signal encoded in protein sequence. We believe that our work will inspire the development of a new generation of methods that push our knowledge boundaries and promote exploration and discovery in the molecular function domain.},
	language = {en},
	urldate = {2024-04-13},
	author = {Ramakrishnan, Prabakaran and Bromberg, Yana},
	month = jul,
	year = {2023},
	keywords = {notion, Annotation},
	file = {Ramakrishnan and Bromberg - 2023 - Functional profiling of the sequence stockpile a .pdf:/Users/jason/Zotero/storage/BUWHIKJZ/Ramakrishnan and Bromberg - 2023 - Functional profiling of the sequence stockpile a .pdf:application/pdf},
}

@article{chang_deepp450_2024,
	title = {{DeepP450}: {Predicting} {Human} {P450} {Activities} of {Small} {Molecules} by {Integrating} {Pretrained} {Protein} {Language} {Model} and {Molecular} {Representation}},
	copyright = {https://doi.org/10.15223/policy-029},
	issn = {1549-9596, 1549-960X},
	shorttitle = {{DeepP450}},
	url = {https://pubs.acs.org/doi/10.1021/acs.jcim.4c00115},
	doi = {10.1021/acs.jcim.4c00115},
	abstract = {Cytochrome P450 enzymes (CYPs) play a crucial role in Phase I drug metabolism in the human body, and CYP activity toward compounds can significantly affect druggability, making early prediction of CYP activity and substrate identification essential for therapeutic development. Here, we established a deep learning model for assessing potential CYP substrates, DeepP450, by fine-tuning protein and molecule pretrained models through feature integration with cross-attention and self-attention layers. This model exhibited high prediction accuracy (0.92) on the test set, with area under the receiver operating characteristic curve (AUROC) values ranging from 0.89 to 0.98 in substrate/nonsubstrate predictions across the nine major human CYPs, surpassing current benchmarks for CYP activity prediction. Notably, DeepP450 uses only one model to predict substrates/nonsubstrates for any of the nine CYPs and exhibits certain generalizability on novel compounds and different categories of human CYPs, which could greatly facilitate early stage drug design by avoiding CYP-reactive compounds.},
	language = {en},
	urldate = {2024-04-14},
	journal = {Journal of Chemical Information and Modeling},
	author = {Chang, Jiamin and Fan, Xiaoyu and Tian, Boxue},
	month = apr,
	year = {2024},
	keywords = {notion, ML Chemistry, ML Proteins},
	pages = {acs.jcim.4c00115},
	file = {Chang et al. - 2024 - DeepP450 Predicting Human P450 Activities of Smal.pdf:/Users/jason/Zotero/storage/W2D57RYG/Chang et al. - 2024 - DeepP450 Predicting Human P450 Activities of Smal.pdf:application/pdf},
}

@misc{ma_retrieved_2023,
	title = {Retrieved {Sequence} {Augmentation} for {Protein} {Representation} {Learning}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.02.22.529597},
	doi = {10.1101/2023.02.22.529597},
	abstract = {The advancement of protein representation learning has been significantly influenced by the remarkable progress in language models. Accordingly, protein language models perform inference from individual sequences, thereby limiting their capacity to incorporate evolutionary knowledge present in sequence variations. Existing solutions, which rely on Multiple Sequence Alignments (MSA), suffer from substantial computational overhead and suboptimal generalization performance for de novo proteins. In light of these problems, we introduce a novel paradigm called Retrieved Sequence Augmentation (RSA) that enhances protein representation learning without necessitating additional alignment or preprocessing. RSA associates query protein sequences with a collection of structurally or functionally similar sequences in the database and integrates them for subsequent predictions. We demonstrate that protein language models benefit from retrieval enhancement in both structural and property prediction tasks, achieving a 5\% improvement over MSA Transformer on average while being 373 times faster. Furthermore, our model exhibits superior transferability to new protein domains and outperforms MSA Transformer in de novo protein prediction. This study fills a much-encountered gap in protein prediction and brings us a step closer to demystifying the domain knowledge needed to understand protein sequences. Code is available at https://github.com/HKUNLP/RSA.},
	language = {en},
	urldate = {2024-04-17},
	author = {Ma, Chang and Zhao, Haiteng and Zheng, Lin and Xin, Jiayi and Li, Qintong and Wu, Lijun and Deng, Zhihong and Lu, Yang and Liu, Qi and Kong, Lingpeng},
	month = feb,
	year = {2023},
	keywords = {notion, Representations},
	file = {Ma et al. - 2023 - Retrieved Sequence Augmentation for Protein Repres.pdf:/Users/jason/Zotero/storage/447PUQGY/Ma et al. - 2023 - Retrieved Sequence Augmentation for Protein Repres.pdf:application/pdf},
}

@misc{samusevich_highly_2024,
	title = {Highly accurate discovery of terpene synthases powered by machine learning reveals functional terpene cyclization in {Archaea}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2024.01.29.577750},
	doi = {10.1101/2024.01.29.577750},
	abstract = {Terpene synthases (TPSs) generate the scaffolds of the largest class of natural products, including several first-line medicines. The amount of available protein sequences is increasing exponentially, and accurate computational characterization of their function remains an unsolved challenge. We assembled a curated dataset of one thousand characterized TPS reactions and developed a method to devise highly accurate machine-learning models for functional annotation in a low-data regime. Our models significantly outperform existing methods for TPS detection and substrate prediction. By applying the models to large protein sequence databases, we discovered seven TPS enzymes previously undetected by state-of-the-art protein signatures and experimentally confirmed their activity, including the first reported TPSs in the major domain of life Archaea. Furthermore, we discovered a new TPS structural domain and distinct subtypes of previously known domains. This work demonstrates the potential of machine learning to speed up the discovery and characterization of novel TPSs.},
	language = {en},
	urldate = {2024-04-26},
	author = {Samusevich, Raman and Hebra, Teo and Bushuiev, Roman and Bushuiev, Anton and Čalounová, Tereza and Smrčková, Helena and Chatpatanasiri, Ratthachat and Kulhánek, Jonáš and Perković, Milana and Engst, Martin and Tajovská, Adéla and Sivic, Josef and Pluskal, Tomáš},
	month = jan,
	year = {2024},
	keywords = {notion, ML Proteins},
	file = {Samusevich et al. - 2024 - Highly accurate discovery of terpene synthases pow.pdf:/Users/jason/Zotero/storage/9XPX2B68/Samusevich et al. - 2024 - Highly accurate discovery of terpene synthases pow.pdf:application/pdf},
}

@article{ryu_deep_2019,
	title = {Deep learning enables high-quality and high-throughput prediction of enzyme commission numbers},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/full/10.1073/pnas.1821905116},
	doi = {10.1073/pnas.1821905116},
	abstract = {Significance
            Identification of enzyme commission (EC) numbers is essential for accurately understanding enzyme functions. Although several EC number prediction tools are available, they have room for further improvement with respect to computation time, precision, coverage, and the total size of the files needed for EC number prediction. Here, we present DeepEC, a deep learning-based computational framework that predicts EC numbers with high precision in a high-throughput manner. DeepEC shows much improved prediction performance when compared with the 5 representative EC number prediction tools that are currently available. DeepEC will be useful in studying enzyme functions by implementing them independently or as part of a third-party software program.
          , 
            High-quality and high-throughput prediction of enzyme commission (EC) numbers is essential for accurate understanding of enzyme functions, which have many implications in pathologies and industrial biotechnology. Several EC number prediction tools are currently available, but their prediction performance needs to be further improved to precisely and efficiently process an ever-increasing volume of protein sequence data. Here, we report DeepEC, a deep learning-based computational framework that predicts EC numbers for protein sequences with high precision and in a high-throughput manner. DeepEC takes a protein sequence as input and predicts EC numbers as output. DeepEC uses 3 convolutional neural networks (CNNs) as a major engine for the prediction of EC numbers, and also implements homology analysis for EC numbers that cannot be classified by the CNNs. Comparative analyses against 5 representative EC number prediction tools show that DeepEC allows the most precise prediction of EC numbers, and is the fastest and the lightest in terms of the disk space required. Furthermore, DeepEC is the most sensitive in detecting the effects of mutated domains/binding site residues of protein sequences. DeepEC can be used as an independent tool, and also as a third-party software component in combination with other computational platforms that examine metabolic reactions.},
	language = {en},
	number = {28},
	urldate = {2024-04-26},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ryu, Jae Yong and Kim, Hyun Uk and Lee, Sang Yup},
	month = jul,
	year = {2019},
	keywords = {notion},
	pages = {13996--14001},
	file = {Ryu et al. - 2019 - Deep learning enables high-quality and high-throug.pdf:/Users/jason/Zotero/storage/P4VKUUYF/Ryu et al. - 2019 - Deep learning enables high-quality and high-throug.pdf:application/pdf},
}

@article{price_mutant_2018,
	title = {Mutant phenotypes for thousands of bacterial genes of unknown function},
	volume = {557},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-018-0124-0},
	doi = {10.1038/s41586-018-0124-0},
	language = {en},
	number = {7706},
	urldate = {2024-04-26},
	journal = {Nature},
	author = {Price, Morgan N. and Wetmore, Kelly M. and Waters, R. Jordan and Callaghan, Mark and Ray, Jayashree and Liu, Hualan and Kuehl, Jennifer V. and Melnyk, Ryan A. and Lamson, Jacob S. and Suh, Yumi and Carlson, Hans K. and Esquivel, Zuelma and Sadeeshkumar, Harini and Chakraborty, Romy and Zane, Grant M. and Rubin, Benjamin E. and Wall, Judy D. and Visel, Axel and Bristow, James and Blow, Matthew J. and Arkin, Adam P. and Deutschbauer, Adam M.},
	month = may,
	year = {2018},
	keywords = {Dataset, notion},
	pages = {503--509},
	file = {Price et al. - 2018 - Mutant phenotypes for thousands of bacterial genes.pdf:/Users/jason/Zotero/storage/965GYG96/Price et al. - 2018 - Mutant phenotypes for thousands of bacterial genes.pdf:application/pdf},
}

@article{sanderson_proteinfer_2023,
	title = {{ProteInfer}, deep neural networks for protein functional inference},
	volume = {12},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/80942},
	doi = {10.7554/eLife.80942},
	abstract = {Predicting the function of a protein from its amino acid sequence is a long-­standing challenge in bioinformatics. Traditional approaches use sequence alignment to compare a query sequence either to thousands of models of protein families or to large databases of individual protein sequences. Here we introduce ProteInfer, which instead employs deep convolutional neural networks to directly predict a variety of protein functions – Enzyme Commission (EC) numbers and Gene Ontology (GO) terms – directly from an unaligned amino acid sequence. This approach provides precise predictions which complement alignment-­based methods, and the computational efficiency of a single neural network permits novel and lightweight software interfaces, which we demonstrate with an in-b­ rowser graphical interface for protein function prediction in which all computation is performed on the user’s personal computer with no data uploaded to remote servers. Moreover, these models place full-­length amino acid sequences into a generalised functional space, facilitating downstream analysis and interpretation. To read the interactive version of this paper, please visit https://google-research.github.io/proteinfer/.},
	language = {en},
	urldate = {2024-04-27},
	journal = {eLife},
	author = {Sanderson, Theo and Bileschi, Maxwell L and Belanger, David and Colwell, Lucy J},
	month = feb,
	year = {2023},
	keywords = {notion},
	pages = {e80942},
	file = {Sanderson et al. - 2023 - ProteInfer, deep neural networks for protein funct.pdf:/Users/jason/Zotero/storage/AAQFKTAF/Sanderson et al. - 2023 - ProteInfer, deep neural networks for protein funct.pdf:application/pdf},
}

@article{heid_enzymemap_2023,
	title = {{EnzymeMap}: curation, validation and data-driven prediction of enzymatic reactions},
	volume = {14},
	issn = {2041-6520, 2041-6539},
	shorttitle = {{EnzymeMap}},
	url = {https://xlink.rsc.org/?DOI=D3SC02048G},
	doi = {10.1039/D3SC02048G},
	abstract = {New curation and atom-mapping routine leading to large database of enzymatic reactions boosts performance of deep learning models.
          , 
            Enzymatic reactions are an ecofriendly, selective, and versatile addition, sometimes even alternative to organic reactions for the synthesis of chemical compounds such as pharmaceuticals or fine chemicals. To identify suitable reactions, computational models to predict the activity of enzymes on non-native substrates, to perform retrosynthetic pathway searches, or to predict the outcomes of reactions including regio- and stereoselectivity are becoming increasingly important. However, current approaches are substantially hindered by the limited amount of available data, especially if balanced and atom mapped reactions are needed and if the models feature machine learning components. We therefore constructed a high-quality dataset (EnzymeMap) by developing a large set of correction and validation algorithms for recorded reactions in the literature and showcase its significant positive impact on machine learning models of retrosynthesis, forward prediction, and regioselectivity prediction, outperforming previous approaches by a large margin. Our dataset allows for deep learning models of enzymatic reactions with unprecedented accuracy, and is freely available online.},
	language = {en},
	number = {48},
	urldate = {2024-05-01},
	journal = {Chemical Science},
	author = {Heid, Esther and Probst, Daniel and Green, William H. and Madsen, Georg K. H.},
	year = {2023},
	keywords = {notion, Dataset},
	pages = {14229--14242},
	file = {Heid et al. - 2023 - EnzymeMap curation, validation and data-driven pr.pdf:/Users/jason/Zotero/storage/R4TSJGTW/Heid et al. - 2023 - EnzymeMap curation, validation and data-driven pr.pdf:application/pdf},
}

@article{zhao_statistical_2009,
	title = {A statistical framework to evaluate virtual screening},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/2.0},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-10-225},
	doi = {10.1186/1471-2105-10-225},
	abstract = {Background: Receiver operating characteristic (ROC) curve is widely used to evaluate virtual screening (VS) studies. However, the method fails to address the "early recognition" problem specific to VS. Although many other metrics, such as RIE, BEDROC, and pROC that emphasize "early recognition" have been proposed, there are no rigorous statistical guidelines for determining the thresholds and performing significance tests. Also no comparisons have been made between these metrics under a statistical framework to better understand their performances.
Results: We have proposed a statistical framework to evaluate VS studies by which the threshold to determine whether a ranking method is better than random ranking can be derived by bootstrap simulations and 2 ranking methods can be compared by permutation test. We found that different metrics emphasize "early recognition" differently. BEDROC and RIE are 2 statistically equivalent metrics. Our newly proposed metric SLR is superior to pROC. Through extensive simulations, we observed a "seesaw effect" – overemphasizing early recognition reduces the statistical power of a metric to detect true early recognitions.
Conclusion: The statistical framework developed and tested by us is applicable to any other metric as well, even if their exact distribution is unknown. Under this framework, a threshold can be easily selected according to a pre-specified type I error rate and statistical comparisons between 2 ranking methods becomes possible. The theoretical null distribution of SLR metric is available so that the threshold of SLR can be exactly determined without resorting to bootstrap simulations, which makes it easy to use in practical virtual screening studies.},
	language = {en},
	number = {1},
	urldate = {2024-05-01},
	journal = {BMC Bioinformatics},
	author = {Zhao, Wei and Hevener, Kirk E and White, Stephen W and Lee, Richard E and Boyett, James M},
	month = dec,
	year = {2009},
	keywords = {notion},
	pages = {225},
	file = {Zhao et al. - 2009 - A statistical framework to evaluate virtual screen.pdf:/Users/jason/Zotero/storage/BUU3ARFG/Zhao et al. - 2009 - A statistical framework to evaluate virtual screen.pdf:application/pdf},
}

@misc{mikhael_clipzyme_2024,
	title = {{CLIPZyme}: {Reaction}-{Conditioned} {Virtual} {Screening} of {Enzymes}},
	shorttitle = {{CLIPZyme}},
	url = {http://arxiv.org/abs/2402.06748},
	abstract = {Computational screening of naturally occurring proteins has the potential to identify efficient catalysts among the hundreds of millions of sequences that remain uncharacterized. Current experimental methods remain time, cost and labor intensive, limiting the number of enzymes they can reasonably screen. In this work, we propose a computational framework for in-silico enzyme screening. Through a contrastive objective, we train CLIPZyme to encode and align representations of enzyme structures and reaction pairs. With no standard computational baseline, we compare CLIPZyme to existing EC (enzyme commission) predictors applied to virtual enzyme screening and show improved performance in scenarios where limited information on the reaction is available (BEDROC85 of 44.69\%). Additionally, we evaluate combining EC predictors with CLIPZyme and show its generalization capacity on both unseen reactions and protein clusters.},
	language = {en},
	urldate = {2024-05-01},
	publisher = {arXiv},
	author = {Mikhael, Peter G. and Chinn, Itamar and Barzilay, Regina},
	month = feb,
	year = {2024},
	note = {arXiv:2402.06748 [q-bio]},
	file = {Mikhael et al. - 2024 - CLIPZyme Reaction-Conditioned Virtual Screening o.pdf:/Users/jason/Zotero/storage/E95KXQ6R/Mikhael et al. - 2024 - CLIPZyme Reaction-Conditioned Virtual Screening o.pdf:application/pdf},
}

@misc{carrami_pqa_2024,
	title = {{PQA}: {Zero}-shot {Protein} {Question} {Answering} for {Free}-form {Scientific} {Enquiry} with {Large} {Language} {Models}},
	shorttitle = {{PQA}},
	url = {http://arxiv.org/abs/2402.13653},
	abstract = {We introduce the novel task of zero-shot Protein Question Answering (PQA) for free-form scientific enquiry. Given a previously unseen protein sequence and a natural language question, the task is to deliver a scientifically accurate answer. This task not only supports future biological research, but could also provide a test bed for assessing the scientific precision of large language models (LLMs). We contribute the first specialized dataset for PQA model training, containing 257K protein sequences annotated with 1.97M scientific question-answer pairs. Additionally, we propose and study several novel biologically relevant benchmarks for scientific PQA. Employing two robust multi-modal architectures, we establish an initial state-of-the-art performance for PQA and reveal key performance factors through ablation studies. Our comprehensive PQA framework, named Pika, including dataset, code, model checkpoints, and a user-friendly demo, is openly accessible on github.com/EMCarrami/Pika, promoting wider research and application in the field.},
	language = {en},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Carrami, Eli M. and Sharifzadeh, Sahand},
	month = feb,
	year = {2024},
	note = {arXiv:2402.13653 [cs]},
	keywords = {Language Model, Multimodal, notion},
	file = {Carrami and Sharifzadeh - 2024 - PQA Zero-shot Protein Question Answering for Free.pdf:/Users/jason/Zotero/storage/N8Q6I5UA/Carrami and Sharifzadeh - 2024 - PQA Zero-shot Protein Question Answering for Free.pdf:application/pdf},
}

@article{munsamy_conditional_nodate,
	title = {Conditional language models enable the efficient design of proficient enzymes},
	abstract = {The design of functional enzymes holds promise for transformative solutions across various domains but presents significant challenges. Inspired by the success of language models in generating nature-like proteins, we explored the potential of an enzyme-specific language model in designing catalytically active artificial enzymes. Here, we introduce ZymCTRL ('enzyme control'), a conditional language model trained on the enzyme sequence space, capable of generating enzymes based on user-defined specifications. Experimental validation at diverse data regimes and for different enzyme families demonstrated ZymCTRL's ability to generate active enzymes across various sequence identity ranges. Specifically, we describe the design of carbonic anhydrases and lactate dehydrogenases in zero-shot, demonstrating activity at sequence identities below 40\% compared to natural proteins without requiring further training of the model. Biophysical analysis confirmed the globularity and well-folded nature of the generated sequences. Furthermore, fine-tuning the model enabled the generation of lactate dehydrogenases more likely to pass in silico filters and with activity comparable to their natural counterparts. Two of the artificial lactate dehydrogenases were scaled up and successfully lyophilised, maintaining activity, and demonstrating preliminary conversion in one-pot enzymatic cascades under extreme conditions. Our findings open a new door towards the rapid and cost-effective design of artificial proficient enzymes. The model and training data are freely available to the community.},
	language = {en},
    year={2024},
	author = {Munsamy, Geraldene and Illanes-Vicioso, Ramiro and Funcillo, Silvia and Lindner, Sebastian and Ayres, Gavin and Sheehan, Lesley S and Moss, Steven and Eckhard, Ulrich and Lorenz, Philipp and Ferruz, Noelia},
	keywords = {ML Proteins, notion},
	file = {Munsamy et al. - Conditional language models enable the efficient d.pdf:/Users/jason/Zotero/storage/J4IHD4JN/Munsamy et al. - Conditional language models enable the efficient d.pdf:application/pdf},
}

@article{ayres_hifi-nn_nodate,
	title = {{HiFi}-{NN} annotates the microbial dark matter with {Enzyme} {Commission} numbers},
	abstract = {The accurate computational annotation of protein sequences with enzymatic function, especially those that are part of the functional and taxonomic dark matter, remains a fundamental challenge in bioinformatics. Here, we present HiFi-NN, (Hierarchically-Finetuned Nearest Neighbor search) which annotates protein sequences to the 4th level of EC (enzyme commission) number with greater precision and recall than all existing deep learning methods. HiFi-NN is a hierarchicallyfinetuned deep learning method based on a combination of semi-supervised representation learning and a nearest neighbours classifier. Furthermore, we show that this method can correctly identify the EC number of a given sequence to identities below 40\%, where the current state of the art annotation tool, BLASTp, cannot. We proceed to improve the representations learned by increasing the diversity of the training set, not just in sequence space but also in terms of the environment the sequences have been sampled from. Finally, we use HiFi-NN to annotate a portion of microbial dark matter sequences in the MGnify database.},
	language = {en},
	author = {Ayres, Gavin and Munsamy, Geraldene and Heinzinger, Michael and Ferruz, Noelia and Yang, Kevin and Lorenz, Philipp},
    year = {2024},
	keywords = {Annotation, notion},
	file = {Ayres et al. - HiFi-NN annotates the microbial dark matter with E.pdf:/Users/jason/Zotero/storage/85MSBN62/Ayres et al. - HiFi-NN annotates the microbial dark matter with E.pdf:application/pdf},
}

@article{mikhael_graph-based_2024,
	title = {Graph-Based Forward Synthesis Prediction of Biocatalyzed Reactions},
	abstract = {The identification of biocatalyzed reaction products plays a critical role in enzyme function prediction, drug discovery, and metabolic engineering. Uncovering the products of biocatalyzed reactions experimentally is both time-consuming and costly, which underscores the urgent need for computational methods. Previous machine learning methods have largely focused on spontaneous, non-biocatalyzed reactions but do not perform well when applied to biocatalyzed reactions specifically. We present a novel approach that harnesses graph-based deep learning to predict the primary products of enzyme-catalyzed reactions, considering both the protein sequence and substrates involved. On the recently published dataset EnzymeMap, we find that our method based on graph-editing outperforms existing transformer-based approaches.},
	language = {en},
	author = {Mikhael, Peter G and Chinn, Itamar and Barzilay, Regina},
	year = {2024},
	keywords = {notion},
	file = {Mikhael et al. - 2024 - GRAPH-BASED FORWARD SYNTHESIS PREDICTION OF BIOCAT.pdf:/Users/jason/Zotero/storage/MYSKWP74/Mikhael et al. - 2024 - GRAPH-BASED FORWARD SYNTHESIS PREDICTION OF BIOCAT.pdf:application/pdf},
}

@article{singh_contrastive_2023,
	title = {Contrastive learning in protein language space predicts interactions between drugs and protein targets},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2220778120},
	doi = {10.1073/pnas.2220778120},
	abstract = {Sequence-based prediction of drug–target interactions has the potential to accelerate drug discovery by complementing experimental screens. Such computational prediction needs to be generalizable and scalable while remaining sensitive to subtle variations in the inputs. However, current computational techniques fail to simultaneously meet these goals, often sacrificing performance of one to achieve the others. We develop a deep learning model, ConPLex, successfully leveraging the advances in pretrained protein language models (“PLex”) and employing a protein-anchored contrastive coembedding (“Con”) to outperform state-of-the-art approaches. ConPLex achieves high accuracy, broad adaptivity to unseen data, and specificity against decoy compounds. It makes predictions of binding based on the distance between learned representations, enabling predictions at the scale of massive compound libraries and the human proteome. Experimental testing of 19 kinase-drug interaction predictions validated 12 interactions, including four with subnanomolar affinity, plus a strongly binding EPHB1 inhibitor (
              K
              
                D
              
              = 1.3 nM). Furthermore, ConPLex embeddings are interpretable, which enables us to visualize the drug–target embedding space and use embeddings to characterize the function of human cell-surface proteins. We anticipate that ConPLex will facilitate efficient drug discovery by making highly sensitive in silico drug screening feasible at the genome scale. ConPLex is available open source at
              https://ConPLex.csail.mit.edu
              .},
	language = {en},
	number = {24},
	urldate = {2024-05-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Singh, Rohit and Sledzieski, Samuel and Bryson, Bryan and Cowen, Lenore and Berger, Bonnie},
	month = jun,
	year = {2023},
	keywords = {notion},
	pages = {e2220778120},
	file = {Singh et al. - 2023 - Contrastive learning in protein language space pre.pdf:/Users/jason/Zotero/storage/IR2LD23F/Singh et al. - 2023 - Contrastive learning in protein language space pre.pdf:application/pdf},
}

@article{hirota_deepes_nodate,
	title = {{DeepES}: {Deep} learning-based enzyme screening to identify orphan enzyme genes},
	abstract = {Motivation Progress in sequencing technology has led to determination of large numbers of protein sequences, and large enzyme databases are now available. Although many computational tools for enzyme annotation were developed, sequence information is unavailable for many enzymes, known as orphan enzymes. These orphan enzymes hinder sequence similarity-based functional annotation, leading gaps in understanding the association between sequences and enzymatic reactions.},
	language = {en},
	author = {Hirota, Keisuke and Salim, Felix and Yamada, Takuji},
	keywords = {notion},
    year = {2024},
	file = {Hirota et al. - DeepES Deep learning-based enzyme screening to id.pdf:/Users/jason/Zotero/storage/WJC5TXLC/Hirota et al. - DeepES Deep learning-based enzyme screening to id.pdf:application/pdf},
}

@article{bileschi_using_2022,
	title = {Using deep learning to annotate the protein universe},
	volume = {40},
	issn = {1087-0156, 1546-1696},
	url = {https://www.nature.com/articles/s41587-021-01179-w},
	doi = {10.1038/s41587-021-01179-w},
	language = {en},
	number = {6},
	urldate = {2024-05-16},
	journal = {Nature Biotechnology},
	author = {Bileschi, Maxwell L. and Belanger, David and Bryant, Drew H. and Sanderson, Theo and Carter, Brandon and Sculley, D. and Bateman, Alex and DePristo, Mark A. and Colwell, Lucy J.},
	month = jun,
	year = {2022},
	pages = {932--937},
	file = {Bileschi et al. - 2022 - Using deep learning to annotate the protein univer.pdf:/Users/jason/Zotero/storage/8AHX5Z3M/Bileschi et al. - 2022 - Using deep learning to annotate the protein univer.pdf:application/pdf},
}

@article{liu_group_2023,
	title = {A {Group} {Symmetric} {Stochastic} {Differential} {Equation} {Model} for {Molecule} {Multi}-modal {Pretraining}},
	abstract = {Molecule pretraining has quickly become the go-to schema to boost the performance of AIbased drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multimodal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous downstream tasks than the previous work. By comparing with 17 pretraining baselines, we empirically verify that MoleculeSDE can learn an expressive representation with state-of-the-art performance on 26 out of 32 downstream tasks. The source codes are available in this repository.},
	language = {en},
	author = {Liu, Shengchao and Du, Weitao and Ma, Zhiming and Guo, Hongyu and Tang, Jian},
	file = {Liu et al. - A Group Symmetric Stochastic Differential Equation.pdf:/Users/jason/Zotero/storage/IM8BRQ8A/Liu et al. - A Group Symmetric Stochastic Differential Equation.pdf:application/pdf},
	year = {2023},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, ﬁnding that the latter are computationally more efﬁcient and produce higher-quality samples.},
	language = {en},
	urldate = {2024-05-18},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {notion},
	file = {Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:/Users/jason/Zotero/storage/JLQ8LRAH/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf},
}

@article{wu_proteinclip_nodate,
	title = {{ProteinCLIP}: enhancing protein language models with natural language},
	language = {en},
	author = {Wu, Kevin E and Chang, Howard and Zou, James},
	keywords = {notion},
    year = {2024},
	file = {Wu et al. - ProteinCLIP enhancing protein language models wit.pdf:/Users/jason/Zotero/storage/X2VMMFY9/Wu et al. - ProteinCLIP enhancing protein language models wit.pdf:application/pdf},
}

@article{altschup_basic_nodate,
	title = {Basic Local Alignment Search Tool},
    year = {1990},
	language = {en},
    journal = {Journal of Molecular Biology},
	author = {AltschuP, Stephen F and Gish, Warren and Miller, Webb and Myers, Eugene W and Lipman, David J},
	file = {AltschuP et al. - Basic Local Alignment Search Tool.pdf:/Users/jason/Zotero/storage/G9EZNNQK/AltschuP et al. - Basic Local Alignment Search Tool.pdf:application/pdf},
}

@article{chang_brenda_2021,
	title = {{BRENDA}, the {ELIXIR} core data resource in 2021: new developments and updates},
	volume = {49},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{BRENDA}, the {ELIXIR} core data resource in 2021},
	url = {https://academic.oup.com/nar/article/49/D1/D498/5992283},
	doi = {10.1093/nar/gkaa1025},
	abstract = {The BRENDA enzyme database (https://www.brendaenzymes.org), established in 1987, has evolved into the main collection of functional enzyme and metabolism data. In 2018, BRENDA was selected as an ELIXIR Core Data Resource. BRENDA provides reliable data, continuous curation and updates of classiﬁed enzymes, and the integration of newly discovered enzymes. The main part contains {\textgreater}5 million data for ∼90 000 enzymes from ∼13 000 organisms, manually extracted from ∼157 000 primary literature references, combined with information of text and data mining, data integration, and prediction algorithms. Supplements comprise disease-related data, protein sequences, 3D structures, genome annotations, ligand information, taxonomic, bibliographic, and kinetic data. BRENDA offers an easy access to enzyme information from quick to advanced searches, text- and structured-based queries for enzyme-ligand interactions, word maps, and visualization of enzyme data. The BRENDA Pathway Maps are completely revised and updated for an enhanced interactive and intuitive usability. The new design of the Enzyme Summary Page provides an improved access to each individual enzyme. A new protein structure 3D viewer was integrated. The prediction of the intracellular localization of eukaryotic enzymes has been implemented. The new EnzymeDetector combines BRENDA enzyme annotations with protein and genome databases for the detection of eukaryotic and prokaryotic enzymes.},
	language = {en},
	number = {D1},
	urldate = {2024-05-26},
	journal = {Nucleic Acids Research},
	author = {Chang, Antje and Jeske, Lisa and Ulbrich, Sandra and Hofmann, Julia and Koblitz, Julia and Schomburg, Ida and Neumann-Schaal, Meina and Jahn, Dieter and Schomburg, Dietmar},
	month = jan,
	year = {2021},
	pages = {D498--D508},
	file = {Chang et al. - 2021 - BRENDA, the ELIXIR core data resource in 2021 new.pdf:/Users/jason/Zotero/storage/U73GHFLB/Chang et al. - 2021 - BRENDA, the ELIXIR core data resource in 2021 new.pdf:application/pdf},
}

@article{bansal_rhea_2022,
	title = {Rhea, the reaction knowledgebase in 2022},
	volume = {50},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	url = {https://academic.oup.com/nar/article/50/D1/D693/6424769},
	doi = {10.1093/nar/gkab1016},
	abstract = {Rhea (https://www.rhea-db.org) is an expert-curated knowledgebase of biochemical reactions based on the chemical ontology ChEBI (Chemical Entities of Biological Interest) (https://www.ebi.ac.uk/chebi). In this paper, we describe a number of key developments in Rhea since our last report in the database issue of Nucleic Acids Research in 2019. These include improved reaction coverage in Rhea, the adoption of Rhea as the reference vocabulary for enzyme annotation in the UniProt knowledgebase UniProtKB (https://www.uniprot.org), the development of a new Rhea website, and the designation of Rhea as an ELIXIR Core Data Resource. We hope that these and other developments will enhance the utility of Rhea as a reference resource to study and engineer enzymes and the metabolic systems in which they function.},
	language = {en},
	number = {D1},
	urldate = {2024-05-26},
	journal = {Nucleic Acids Research},
	author = {Bansal, Parit and Morgat, Anne and Axelsen, Kristian B and Muthukrishnan, Venkatesh and Coudert, Elisabeth and Aimo, Lucila and Hyka-Nouspikel, Nevila and Gasteiger, Elisabeth and Kerhornou, Arnaud and Neto, Teresa Batista and Pozzato, Monica and Blatter, Marie-Claude and Ignatchenko, Alex and Redaschi, Nicole and Bridge, Alan},
	month = jan,
	year = {2022},
	pages = {D693--D700},
	file = {Bansal et al. - 2022 - Rhea, the reaction knowledgebase in 2022.pdf:/Users/jason/Zotero/storage/AINC4SQE/Bansal et al. - 2022 - Rhea, the reaction knowledgebase in 2022.pdf:application/pdf},
}

@article{the_uniprot_consortium_uniprot_2023,
	title = {{UniProt}: the {Universal} {Protein} {Knowledgebase} in 2023},
	volume = {51},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{UniProt}},
	url = {https://academic.oup.com/nar/article/51/D1/D523/6835362},
	doi = {10.1093/nar/gkac1052},
	abstract = {The aim of the UniProt Knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. In this publication we describe enhancements made to our data processing pipeline and to our website to adapt to an everincreasing information content. The number of sequences in UniProtKB has risen to over 227 million and we are working towards including a reference proteome for each taxonomic group. We continue to extract detailed annotations from the literature to update or create reviewed entries, while unreviewed entries are supplemented with annotations provided by automated systems using a variety of machine-learning techniques. In addition, the scientiﬁc community continues their contributions of publications and annotations to UniProt entries of their interest. Finally, we describe our new website (https://www.uniprot.org/), designed to enhance our users’ experience and make our data easily accessible to the research community. This interface includes access to AlphaFold structures for more than 85\% of all entries as well as improved visualisations for subcellular localisation of proteins.},
	language = {en},
	number = {D1},
	urldate = {2024-05-26},
	journal = {Nucleic Acids Research},
	author = {{The UniProt Consortium} and Bateman, Alex and Martin, Maria-Jesus and Orchard, Sandra and Magrane, Michele and Ahmad, Shadab and Alpi, Emanuele and Bowler-Barnett, Emily H and Britto, Ramona and Bye-A-Jee, Hema and Cukura, Austra and Denny, Paul and Dogan, Tunca and Ebenezer, ThankGod and Fan, Jun and Garmiri, Penelope and Da Costa Gonzales, Leonardo Jose and Hatton-Ellis, Emma and Hussein, Abdulrahman and Ignatchenko, Alexandr and Insana, Giuseppe and Ishtiaq, Rizwan and Joshi, Vishal and Jyothi, Dushyanth and Kandasaamy, Swaathi and Lock, Antonia and Luciani, Aurelien and Lugaric, Marija and Luo, Jie and Lussi, Yvonne and MacDougall, Alistair and Madeira, Fabio and Mahmoudy, Mahdi and Mishra, Alok and Moulang, Katie and Nightingale, Andrew and Pundir, Sangya and Qi, Guoying and Raj, Shriya and Raposo, Pedro and Rice, Daniel L and Saidi, Rabie and Santos, Rafael and Speretta, Elena and Stephenson, James and Totoo, Prabhat and Turner, Edward and Tyagi, Nidhi and Vasudev, Preethi and Warner, Kate and Watkins, Xavier and Zaru, Rossana and Zellner, Hermann and Bridge, Alan J and Aimo, Lucila and Argoud-Puy, Ghislaine and Auchincloss, Andrea H and Axelsen, Kristian B and Bansal, Parit and Baratin, Delphine and Batista Neto, Teresa M and Blatter, Marie-Claude and Bolleman, Jerven T and Boutet, Emmanuel and Breuza, Lionel and Gil, Blanca Cabrera and Casals-Casas, Cristina and Echioukh, Kamal Chikh and Coudert, Elisabeth and Cuche, Beatrice and De Castro, Edouard and Estreicher, Anne and Famiglietti, Maria L and Feuermann, Marc and Gasteiger, Elisabeth and Gaudet, Pascale and Gehant, Sebastien and Gerritsen, Vivienne and Gos, Arnaud and Gruaz, Nadine and Hulo, Chantal and Hyka-Nouspikel, Nevila and Jungo, Florence and Kerhornou, Arnaud and Le Mercier, Philippe and Lieberherr, Damien and Masson, Patrick and Morgat, Anne and Muthukrishnan, Venkatesh and Paesano, Salvo and Pedruzzi, Ivo and Pilbout, Sandrine and Pourcel, Lucille and Poux, Sylvain and Pozzato, Monica and Pruess, Manuela and Redaschi, Nicole and Rivoire, Catherine and Sigrist, Christian J A and Sonesson, Karin and Sundaram, Shyamala and Wu, Cathy H and Arighi, Cecilia N and Arminski, Leslie and Chen, Chuming and Chen, Yongxing and Huang, Hongzhan and Laiho, Kati and McGarvey, Peter and Natale, Darren A and Ross, Karen and Vinayaka, C R and Wang, Qinghua and Wang, Yuqi and Zhang, Jian},
	month = jan,
	year = {2023},
	pages = {D523--D531},
	file = {The UniProt Consortium et al. - 2023 - UniProt the Universal Protein Knowledgebase in 20.pdf:/Users/jason/Zotero/storage/UI26CZ2G/The UniProt Consortium et al. - 2023 - UniProt the Universal Protein Knowledgebase in 20.pdf:application/pdf},
}

@article{mistry_pfam_2021,
	title = {Pfam: {The} protein families database in 2021},
	volume = {49},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {Pfam},
	url = {https://academic.oup.com/nar/article/49/D1/D412/5943818},
	doi = {10.1093/nar/gkaa913},
	abstract = {The Pfam database is a widely used resource for classifying protein sequences into families and domains. Since Pfam was last described in this journal, over 350 new families have been added in Pfam 33.1 and numerous improvements have been made to existing entries. To facilitate research on COVID-19, we have revised the Pfam entries that cover the SARS-CoV-2 proteome, and built new entries for regions that were not covered by Pfam. We have reintroduced Pfam-B which provides an automatically generated supplement to Pfam and contains 136 730 novel clusters of sequences that are not yet matched by a Pfam family. The new Pfam-B is based on a clustering by the MMseqs2 software. We have compared all of the regions in the RepeatsDB to those in Pfam and have started to use the results to build and reﬁne Pfam repeat families. Pfam is freely available for browsing and download at http://pfam.xfam.org/.},
	language = {en},
	number = {D1},
	urldate = {2024-05-26},
	journal = {Nucleic Acids Research},
	author = {Mistry, Jaina and Chuguransky, Sara and Williams, Lowri and Qureshi, Matloob and Salazar, Gustavo A and Sonnhammer, Erik L L and Tosatto, Silvio C E and Paladin, Lisanna and Raj, Shriya and Richardson, Lorna J and Finn, Robert D and Bateman, Alex},
	month = jan,
	year = {2021},
	pages = {D412--D419},
	file = {Mistry et al. - 2021 - Pfam The protein families database in 2021.pdf:/Users/jason/Zotero/storage/8SYJT8KF/Mistry et al. - 2021 - Pfam The protein families database in 2021.pdf:application/pdf},
}

@article{finn_hmmer_2015,
	title = {{HMMER} web server: 2015 update},
	volume = {43},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{HMMER} web server},
	url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv397},
	doi = {10.1093/nar/gkv397},
	abstract = {The HMMER website, available at http://www.ebi.ac. uk/Tools/hmmer/, provides access to the protein homology search algorithms found in the HMMER software suite. Since the ﬁrst release of the website in 2011, the search repertoire has been expanded to include the iterative search algorithm, jackhmmer. The continued growth of the target sequence databases means that traditional tabular representations of signiﬁcant sequence hits can be overwhelming to the user. Consequently, additional ways of presenting homology search results have been developed, allowing them to be summarised according to taxonomic distribution or domain architecture. The taxonomy and domain architecture representations can be used in combination to ﬁlter the results according to the needs of a user. Searches can also be restricted prior to submission using a new taxonomic ﬁlter, which not only ensures that the results are speciﬁc to the requested taxonomic group, but also improves search performance. The repertoire of proﬁle hidden Markov model libraries, which are used for annotation of query sequences with protein families and domains, has been expanded to include the libraries from CATH-Gene3D, PIRSF, Superfamily and TIGRFAMs. Finally, we discuss the relocation of the HMMER webserver to the European Bioinformatics Institute and the potential impact that this will have.},
	language = {en},
	number = {W1},
	urldate = {2024-05-26},
	journal = {Nucleic Acids Research},
	author = {Finn, Robert D. and Clements, Jody and Arndt, William and Miller, Benjamin L. and Wheeler, Travis J. and Schreiber, Fabian and Bateman, Alex and Eddy, Sean R.},
	month = jul,
	year = {2015},
	pages = {W30--W38},
	file = {Finn et al. - 2015 - HMMER web server 2015 update.pdf:/Users/jason/Zotero/storage/BHYH8M9F/Finn et al. - 2015 - HMMER web server 2015 update.pdf:application/pdf},
}

@article{ashburner_gene_2000,
	title = {Gene {Ontology}: tool for the unification of biology},
	volume = {25},
	copyright = {http://www.springer.com/tdm},
	issn = {1061-4036, 1546-1718},
	shorttitle = {Gene {Ontology}},
	url = {https://www.nature.com/articles/ng0500_25},
	doi = {10.1038/75556},
	language = {en},
	number = {1},
	urldate = {2024-05-26},
	journal = {Nature Genetics},
	author = {Ashburner, Michael and Ball, Catherine A. and Blake, Judith A. and Botstein, David and Butler, Heather and Cherry, J. Michael and Davis, Allan P. and Dolinski, Kara and Dwight, Selina S. and Eppig, Janan T. and Harris, Midori A. and Hill, David P. and Issel-Tarver, Laurie and Kasarskis, Andrew and Lewis, Suzanna and Matese, John C. and Richardson, Joel E. and Ringwald, Martin and Rubin, Gerald M. and Sherlock, Gavin},
	month = may,
	year = {2000},
	pages = {25--29},
	file = {Ashburner et al. - 2000 - Gene Ontology tool for the unification of biology.pdf:/Users/jason/Zotero/storage/62G8UHE2/Ashburner et al. - 2000 - Gene Ontology tool for the unification of biology.pdf:application/pdf},
}

@article{abramson_accurate_2024,
	title = {Accurate structure prediction of biomolecular interactions with {AlphaFold} 3},
	issn = {0028-0836, 1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07487-w},
	doi = {10.1038/s41586-024-07487-w},
	language = {en},
	urldate = {2024-05-26},
	journal = {Nature},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O’Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and Žemgulytė, Akvilė and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and Žídek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	month = may,
	year = {2024},
	file = {Abramson et al. - 2024 - Accurate structure prediction of biomolecular inte.pdf:/Users/jason/Zotero/storage/ETA377EN/Abramson et al. - 2024 - Accurate structure prediction of biomolecular inte.pdf:application/pdf},
}

@article{varadi_alphafold_2024,
	title = {{AlphaFold} {Protein} {Structure} {Database} in 2024: providing structure coverage for over 214 million protein sequences},
	volume = {52},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{AlphaFold} {Protein} {Structure} {Database} in 2024},
	url = {https://academic.oup.com/nar/article/52/D1/D368/7337620},
	doi = {10.1093/nar/gkad1011},
	abstract = {The AlphaFold Database Protein Structure Database (AlphaFold DB, https://alphafold.ebi.ac.uk) has significantly impacted structural biology by amassing over 214 million predicted protein structures, expanding from the initial 300k structures released in 2021. Enabled by the groundbreaking AlphaFold2 artificial intelligence (AI) system, the predictions archived in AlphaFold DB have been integrated into primary data resources such as PDB, UniProt, Ensembl, InterPro and MobiDB. Our manuscript details subsequent enhancements in data archiving, covering successive releases encompassing model organisms, global health proteomes, Swiss-Prot integration, and a host of curated protein datasets. We detail the data access mechanisms of AlphaFold DB, from direct file access via FTP to advanced queries using Google Cloud Public Datasets and the programmatic access endpoints of the database. We also discuss the improvements and services added since its initial release, including enhancements to the Predicted Aligned Error viewer, customisation options for the 3D viewer, and improvements in the search engine of AlphaFold DB.},
	language = {en},
	number = {D1},
	urldate = {2024-05-26},
	journal = {Nucleic Acids Research},
	author = {Varadi, Mihaly and Bertoni, Damian and Magana, Paulyna and Paramval, Urmila and Pidruchna, Ivanna and Radhakrishnan, Malarvizhi and Tsenkov, Maxim and Nair, Sreenath and Mirdita, Milot and Yeo, Jingi and Kovalevskiy, Oleg and Tunyasuvunakool, Kathryn and Laydon, Agata and Žídek, Augustin and Tomlinson, Hamish and Hariharan, Dhavanthi and Abrahamson, Josh and Green, Tim and Jumper, John and Birney, Ewan and Steinegger, Martin and Hassabis, Demis and Velankar, Sameer},
	month = jan,
	year = {2024},
	pages = {D368--D375},
	file = {Varadi et al. - 2024 - AlphaFold Protein Structure Database in 2024 prov.pdf:/Users/jason/Zotero/storage/4ELC82C3/Varadi et al. - 2024 - AlphaFold Protein Structure Database in 2024 prov.pdf:application/pdf},
}


@inproceedings{liu2022pretraining,
    title={Pre-training Molecular Graph Representation with 3D Geometry},
    author={Shengchao Liu and Hanchen Wang and Weiyang Liu and Joan Lasenby and Hongyu Guo and Jian Tang},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=xQUe1pOKPam}
}


@article{oord2018representation,
	author       = {Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
	title        = {Representation learning with contrastive predictive coding},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1807.03748},
	keywords     = {main}
}