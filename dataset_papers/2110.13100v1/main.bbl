\begin{thebibliography}{123}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, et~al.]{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock \emph{International journal of computer vision}, 115\penalty0
  (3):\penalty0 211--252, 2015.

\bibitem[Ruder(2016)]{ruder2016overview}
Sebastian Ruder.
\newblock An overview of gradient descent optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1609.04747}, 2016.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Huang et~al.(2016)Huang, Sun, Liu, Sedra, and
  Weinberger]{huang2016deep}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In \emph{European conference on computer vision}, pages 646--661.
  Springer, 2016.

\bibitem[Brock et~al.(2017{\natexlab{a}})Brock, Lim, Ritchie, and
  Weston]{brock2017freezeout}
Andrew Brock, Theodore Lim, James~M Ritchie, and Nick Weston.
\newblock Freezeout: Accelerate training by progressively freezing layers.
\newblock \emph{arXiv preprint arXiv:1706.04983}, 2017{\natexlab{a}}.

\bibitem[Choi et~al.(2019)Choi, Passos, Shallue, and Dahl]{choi2019faster}
Dami Choi, Alexandre Passos, Christopher~J Shallue, and George~E Dahl.
\newblock Faster neural network training with data echoing.
\newblock \emph{arXiv preprint arXiv:1907.05550}, 2019.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[NVIDIA()]{nvidia}
NVIDIA.
\newblock Nvidia data center deep learning product performance.
\newblock URL
  \url{https://developer.nvidia.com/deep-learning-performance-training-inference}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Strubell et~al.(2019)Strubell, Ganesh, and
  McCallum]{strubell2019energy}
Emma Strubell, Ananya Ganesh, and Andrew McCallum.
\newblock Energy and policy considerations for deep learning in nlp.
\newblock \emph{arXiv preprint arXiv:1906.02243}, 2019.

\bibitem[Cai et~al.(2019)Cai, Gan, Wang, Zhang, and Han]{cai2019onceforall}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once-for-all: Train one network and specialize it for efficient
  deployment, 2019.

\bibitem[Thompson et~al.(2020{\natexlab{a}})Thompson, Greenewald, Lee, and
  Manso]{thompson2020computational}
Neil~C Thompson, Kristjan Greenewald, Keeheon Lee, and Gabriel~F Manso.
\newblock The computational limits of deep learning.
\newblock \emph{arXiv preprint arXiv:2007.05558}, 2020{\natexlab{a}}.

\bibitem[Ha et~al.(2016)Ha, Dai, and Le]{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock \emph{arXiv preprint arXiv:1609.09106}, 2016.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Hospedales et~al.(2020)Hospedales, Antoniou, Micaelli, and
  Storkey]{hospedales2020meta}
Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey.
\newblock Meta-learning in neural networks: A survey.
\newblock \emph{arXiv preprint arXiv:2004.05439}, 2020.

\bibitem[Andrychowicz et~al.(2016)Andrychowicz, Denil, Gomez, Hoffman, Pfau,
  Schaul, Shillingford, and De~Freitas]{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In \emph{Advances in neural information processing systems}, pages
  3981--3989, 2016.

\bibitem[Ravi and Larochelle(2016)]{ravi2016optimization}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock 2016.

\bibitem[Liu et~al.(2018{\natexlab{a}})Liu, Simonyan, and Yang]{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock Darts: Differentiable architecture search.
\newblock \emph{arXiv preprint arXiv:1806.09055}, 2018{\natexlab{a}}.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Kipf and Welling(2017)]{KipfW17}
Thomas~N. Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=SJU4ayYgl}.

\bibitem[Veličković et~al.(2018)Veličković, Cucurull, Casanova, Romero,
  Liò, and Bengio]{velickovic2018graph}
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro
  Liò, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=rJXMpikCZ}.

\bibitem[Dwivedi et~al.(2020)Dwivedi, Joshi, Laurent, Bengio, and
  Bresson]{dwivedi2020benchmarking}
Vijay~Prakash Dwivedi, Chaitanya~K Joshi, Thomas Laurent, Yoshua Bengio, and
  Xavier Bresson.
\newblock Benchmarking graph neural networks.
\newblock \emph{arXiv preprint arXiv:2003.00982}, 2020.

\bibitem[Zhang et~al.(2018)Zhang, Ren, and Urtasun]{zhang2018graph}
Chris Zhang, Mengye Ren, and Raquel Urtasun.
\newblock Graph hypernetworks for neural architecture search.
\newblock \emph{arXiv preprint arXiv:1810.05749}, 2018.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Xie et~al.(2017)Xie, Girshick, Doll{\'a}r, Tu, and
  He]{xie2017aggregated}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 1492--1500, 2017.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Zoph and Le(2016)]{zoph2016neural}
Barret Zoph and Quoc~V Le.
\newblock Neural architecture search with reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1611.01578}, 2016.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 8697--8710, 2018.

\bibitem[Liu et~al.(2018{\natexlab{b}})Liu, Zoph, Neumann, Shlens, Hua, Li,
  Fei-Fei, Yuille, Huang, and Murphy]{liu2018progressive}
Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,
  Li~Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy.
\newblock Progressive neural architecture search.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 19--34, 2018{\natexlab{b}}.

\bibitem[Real et~al.(2019)Real, Aggarwal, Huang, and Le]{real2019regularized}
Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc~V Le.
\newblock Regularized evolution for image classifier architecture search.
\newblock In \emph{Proceedings of the aaai conference on artificial
  intelligence}, volume~33, pages 4780--4789, 2019.

\bibitem[Chen et~al.(2019)Chen, Xie, Wu, and Tian]{chen2019progressive}
Xin Chen, Lingxi Xie, Jun Wu, and Qi~Tian.
\newblock Progressive darts: Bridging the optimization gap for nas in the wild.
\newblock \emph{arXiv preprint arXiv:1912.10952}, 2019.

\bibitem[Howard et~al.(2019)Howard, Sandler, Chu, Chen, Chen, Tan, Wang, Zhu,
  Pang, Vasudevan, et~al.]{howard2019searching}
Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo~Chen, Mingxing
  Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et~al.
\newblock Searching for mobilenetv3.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1314--1324, 2019.

\bibitem[Li et~al.(2015)Li, Tarlow, Brockschmidt, and Zemel]{li2015gated}
Yujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel.
\newblock Gated graph sequence neural networks.
\newblock \emph{arXiv preprint arXiv:1511.05493}, 2015.

\bibitem[Cho et~al.(2014)Cho, Van~Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Kyunghyun Cho, Bart Van~Merri{\"e}nboer, Caglar Gulcehre, Dzmitry Bahdanau,
  Fethi Bougares, Holger Schwenk, and Yoshua Bengio.
\newblock Learning phrase representations using rnn encoder-decoder for
  statistical machine translation.
\newblock \emph{arXiv preprint arXiv:1406.1078}, 2014.

\bibitem[Brock et~al.(2021{\natexlab{a}})Brock, De, and
  Smith]{brock2021characterizing}
Andrew Brock, Soham De, and Samuel~L Smith.
\newblock Characterizing signal propagation to close the performance gap in
  unnormalized resnets.
\newblock \emph{arXiv preprint arXiv:2101.08692}, 2021{\natexlab{a}}.

\bibitem[Brock et~al.(2021{\natexlab{b}})Brock, De, Smith, and
  Simonyan]{brock2021high}
Andrew Brock, Soham De, Samuel~L Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock \emph{arXiv preprint arXiv:2102.06171}, 2021{\natexlab{b}}.

\bibitem[Sifre and Mallat(2014)]{sifre2014rigid}
Laurent Sifre and St{\'e}phane Mallat.
\newblock Rigid-motion scattering for texture classification.
\newblock \emph{arXiv preprint arXiv:1403.1687}, 2014.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Hu et~al.(2018)Hu, Shen, and Sun]{hu2018squeeze}
Jie Hu, Li~Shen, and Gang Sun.
\newblock Squeeze-and-excitation networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7132--7141, 2018.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock \emph{arXiv preprint arXiv:1706.03762}, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Kuhn(1955)]{kuhn1955hungarian}
Harold~W Kuhn.
\newblock The hungarian method for the assignment problem.
\newblock \emph{Naval research logistics quarterly}, 2\penalty0 (1-2):\penalty0
  83--97, 1955.

\bibitem[Hagberg et~al.(2008)Hagberg, Swart, and S~Chult]{hagberg2008exploring}
Aric Hagberg, Pieter Swart, and Daniel S~Chult.
\newblock Exploring network structure, dynamics, and function using networkx.
\newblock Technical report, Los Alamos National Lab.(LANL), Los Alamos, NM
  (United States), 2008.

\bibitem[Barrat et~al.(2004)Barrat, Barthelemy, Pastor-Satorras, and
  Vespignani]{barrat2004architecture}
Alain Barrat, Marc Barthelemy, Romualdo Pastor-Satorras, and Alessandro
  Vespignani.
\newblock The architecture of complex weighted networks.
\newblock \emph{Proceedings of the national academy of sciences}, 101\penalty0
  (11):\penalty0 3747--3752, 2004.

\bibitem[You et~al.(2020{\natexlab{a}})You, Leskovec, He, and
  Xie]{you2020graph}
Jiaxuan You, Jure Leskovec, Kaiming He, and Saining Xie.
\newblock Graph structure of neural networks, 2020{\natexlab{a}}.

\bibitem[Golubeva et~al.(2020)Golubeva, Neyshabur, and
  Gur-Ari]{golubeva2020wider}
Anna Golubeva, Behnam Neyshabur, and Guy Gur-Ari.
\newblock Are wider nets better given the same number of parameters?
\newblock \emph{arXiv preprint arXiv:2010.14495}, 2020.

\bibitem[Zagoruyko and Komodakis(2016)]{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock \emph{arXiv preprint arXiv:1605.07146}, 2016.

\bibitem[Nguyen and Hein(2017)]{nguyen2017loss}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{International conference on machine learning}, pages
  2603--2612. PMLR, 2017.

\bibitem[Srivastava et~al.(2015)Srivastava, Greff, and
  Schmidhuber]{srivastava2015training}
Rupesh~Kumar Srivastava, Klaus Greff, and J{\"u}rgen Schmidhuber.
\newblock Training very deep networks.
\newblock \emph{arXiv preprint arXiv:1507.06228}, 2015.

\bibitem[Hooker(2020)]{hooker2020hardware}
Sara Hooker.
\newblock The hardware lottery, 2020.

\bibitem[Galloway et~al.(2019)Galloway, Golubeva, Tanay, Moussa, and
  Taylor]{galloway2019batch}
Angus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham~W
  Taylor.
\newblock Batch normalization is a cause of adversarial vulnerability.
\newblock \emph{arXiv preprint arXiv:1905.02161}, 2019.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock \emph{arXiv preprint arXiv:1903.12261}, 2019.

\bibitem[Wu and He(2018)]{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem[Qiao et~al.(2019)Qiao, Wang, Liu, Shen, and Yuille]{qiao2019micro}
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan Yuille.
\newblock Micro-batch training with batch-channel normalization and weight
  standardization.
\newblock \emph{arXiv preprint arXiv:1903.10520}, 2019.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Dauphin, and
  Ma]{zhang2019fixup}
Hongyi Zhang, Yann~N Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock \emph{arXiv preprint arXiv:1901.09321}, 2019{\natexlab{a}}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pages 249--256, 2010.

\bibitem[Chang et~al.(2019)Chang, Flokas, and Lipson]{chang2019principled}
Oscar Chang, Lampros Flokas, and Hod Lipson.
\newblock Principled weight initialization for hypernetworks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Alon and Yahav(2020)]{alon2020bottleneck}
Uri Alon and Eran Yahav.
\newblock On the bottleneck of graph neural networks and its practical
  implications.
\newblock \emph{arXiv preprint arXiv:2006.05205}, 2020.

\bibitem[El~Hihi and Bengio(1996)]{el1996hierarchical}
Salah El~Hihi and Yoshua Bengio.
\newblock Hierarchical recurrent neural networks for long-term dependencies.
\newblock In \emph{Advances in neural information processing systems}, pages
  493--499, 1996.

\bibitem[Liu et~al.(2020)Liu, Wang, and Ji]{liu2020non}
Meng Liu, Zhengyang Wang, and Shuiwang Ji.
\newblock Non-local graph neural networks.
\newblock \emph{arXiv preprint arXiv:2005.14612}, 2020.

\bibitem[Pei et~al.(2020)Pei, Wei, Chang, Lei, and Yang]{pei2020geom}
Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu~Lei, and Bo~Yang.
\newblock Geom-gcn: Geometric graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:2002.05287}, 2020.

\bibitem[You et~al.(2019)You, Ying, and Leskovec]{you2019position}
Jiaxuan You, Rex Ying, and Jure Leskovec.
\newblock Position-aware graph neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  7134--7143. PMLR, 2019.

\bibitem[Yang et~al.(2021)Yang, Wang, Song, Yuan, and Tao]{yang2021spagan}
Yiding Yang, Xinchao Wang, Mingli Song, Junsong Yuan, and Dacheng Tao.
\newblock Spagan: Shortest path graph attention network.
\newblock \emph{arXiv preprint arXiv:2101.03464}, 2021.

\bibitem[Radiuk(2017)]{radiuk2017impact}
Pavlo~M Radiuk.
\newblock Impact of training set batch size on the performance of convolutional
  neural networks for diverse datasets.
\newblock \emph{Information Technology and Management Science}, 20\penalty0
  (1):\penalty0 20--24, 2017.

\bibitem[Yang et~al.(2020)Yang, Wang, Chen, Shi, Xu, Xu, Tian, and
  Xu]{yang2020cars}
Zhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu,
  Qi~Tian, and Chang Xu.
\newblock Cars: Continuous evolution for efficient neural architecture search.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1829--1838, 2020.

\bibitem[He et~al.(2020)He, Ye, Shen, and Zhang]{he2020milenas}
Chaoyang He, Haishan Ye, Li~Shen, and Tong Zhang.
\newblock Milenas: Efficient neural architecture search via mixed-level
  reformulation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11993--12002, 2020.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Qian, Delgadillo, Muller, Thabet, and
  Ghanem]{li2020sgas}
Guohao Li, Guocheng Qian, Itzel~C Delgadillo, Matthias Muller, Ali Thabet, and
  Bernard Ghanem.
\newblock Sgas: Sequential greedy architecture search.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 1620--1630, 2020{\natexlab{a}}.

\bibitem[Wichrowska et~al.(2017)Wichrowska, Maheswaranathan, Hoffman,
  Colmenarejo, Denil, Freitas, and Sohl-Dickstein]{wichrowska2017learned}
Olga Wichrowska, Niru Maheswaranathan, Matthew~W Hoffman, Sergio~Gomez
  Colmenarejo, Misha Denil, Nando Freitas, and Jascha Sohl-Dickstein.
\newblock Learned optimizers that scale and generalize.
\newblock In \emph{International Conference on Machine Learning}, pages
  3751--3760. PMLR, 2017.

\bibitem[Metz et~al.(2020)Metz, Maheswaranathan, Freeman, Poole, and
  Sohl-Dickstein]{metz2020tasks}
Luke Metz, Niru Maheswaranathan, C~Daniel Freeman, Ben Poole, and Jascha
  Sohl-Dickstein.
\newblock Tasks, stability, architecture, and compute: Training more effective
  learned optimizers, and using them to train themselves.
\newblock \emph{arXiv preprint arXiv:2009.11243}, 2020.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Praneeth~Karimireddy, Veit,
  Kim, Reddi, Kumar, and Sra]{zhang2019adam}
Jingzhao Zhang, Sai Praneeth~Karimireddy, Andreas Veit, Seungyeon Kim,
  Sashank~J Reddi, Sanjiv Kumar, and Suvrit Sra.
\newblock Why adam beats sgd for attention models.
\newblock \emph{arXiv e-prints}, pages arXiv--1912, 2019{\natexlab{b}}.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Gong, and Zhu]{li2020neural}
Wei Li, Shaogang Gong, and Xiatian Zhu.
\newblock Neural graph embedding for neural architecture search.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 4707--4714, 2020{\natexlab{b}}.

\bibitem[Wen et~al.(2019)Wen, Liu, Li, Chen, Bender, and
  Kindermans]{wen2019neural}
Wei Wen, Hanxiao Liu, Hai Li, Yiran Chen, Gabriel Bender, and Pieter-Jan
  Kindermans.
\newblock Neural predictor for neural architecture search.
\newblock \emph{arXiv preprint arXiv:1912.00848}, 2019.

\bibitem[Jin et~al.(2019)Jin, Song, and Hu]{jin2019auto}
Haifeng Jin, Qingquan Song, and Xia Hu.
\newblock Auto-keras: An efficient neural architecture search system.
\newblock In \emph{Proceedings of the 25th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 1946--1956, 2019.

\bibitem[Kriege et~al.(2020)Kriege, Johansson, and Morris]{kriege2020survey}
Nils~M Kriege, Fredrik~D Johansson, and Christopher Morris.
\newblock A survey on graph kernels.
\newblock \emph{Applied Network Science}, 5\penalty0 (1):\penalty0 1--42, 2020.

\bibitem[Makarov et~al.(2021)Makarov, Kiselev, Nikitinsky, and
  Subelj]{makarov2021survey}
Ilya Makarov, Dmitrii Kiselev, Nikita Nikitinsky, and Lovro Subelj.
\newblock Survey on graph embeddings and their applications to machine learning
  problems on graphs.
\newblock \emph{PeerJ Computer Science}, 7, 2021.

\bibitem[Wen et~al.(2020)Wen, Liu, Chen, Li, Bender, and
  Kindermans]{wen2020neural}
Wei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, and Pieter-Jan
  Kindermans.
\newblock Neural predictor for neural architecture search.
\newblock In \emph{European Conference on Computer Vision}, pages 660--676.
  Springer, 2020.

\bibitem[Lukasik et~al.(2020)Lukasik, Friede, Stuckenschmidt, and
  Keuper]{lukasik2020neural}
Jovita Lukasik, David Friede, Heiner Stuckenschmidt, and Margret Keuper.
\newblock Neural architecture performance prediction using graph neural
  networks.
\newblock \emph{arXiv preprint arXiv:2010.10024}, 2020.

\bibitem[Baker et~al.(2017)Baker, Gupta, Raskar, and
  Naik]{baker2017accelerating}
Bowen Baker, Otkrist Gupta, Ramesh Raskar, and Nikhil Naik.
\newblock Accelerating neural architecture search using performance prediction.
\newblock \emph{arXiv preprint arXiv:1705.10823}, 2017.

\bibitem[Kornblith et~al.(2019)Kornblith, Shlens, and Le]{kornblith2019better}
Simon Kornblith, Jonathon Shlens, and Quoc~V Le.
\newblock Do better imagenet models transfer better?
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2661--2671, 2019.

\bibitem[Huh et~al.(2016)Huh, Agrawal, and Efros]{huh2016makes}
Minyoung Huh, Pulkit Agrawal, and Alexei~A Efros.
\newblock What makes imagenet good for transfer learning?
\newblock \emph{arXiv preprint arXiv:1608.08614}, 2016.

\bibitem[Neyshabur et~al.(2020)Neyshabur, Sedghi, and
  Zhang]{neyshabur2020being}
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
\newblock What is being transferred in transfer learning?
\newblock \emph{arXiv preprint arXiv:2008.11687}, 2020.

\bibitem[Raghu et~al.(2019)Raghu, Zhang, Kleinberg, and
  Bengio]{raghu2019transfusion}
Maithra Raghu, Chiyuan Zhang, Jon Kleinberg, and Samy Bengio.
\newblock Transfusion: Understanding transfer learning for medical imaging.
\newblock \emph{arXiv preprint arXiv:1902.07208}, 2019.

\bibitem[Zhai et~al.(2019)Zhai, Puigcerver, Kolesnikov, Ruyssen, Riquelme,
  Lucic, Djolonga, Pinto, Neumann, Dosovitskiy, et~al.]{zhai2019large}
Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos
  Riquelme, Mario Lucic, Josip Djolonga, Andre~Susano Pinto, Maxim Neumann,
  Alexey Dosovitskiy, et~al.
\newblock A large-scale study of representation learning with the visual task
  adaptation benchmark.
\newblock \emph{arXiv preprint arXiv:1910.04867}, 2019.

\bibitem[PyTorch()]{pytorchdetection}
PyTorch.
\newblock Pytorch object detection finetuning tutorial.
\newblock URL
  \url{https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html}.

\bibitem[Wang et~al.(2007)Wang, Shi, Song, and Shen]{wang2007object}
Liming Wang, Jianbo Shi, Gang Song, and I-fan Shen.
\newblock Object detection combining recognition and segmentation.
\newblock In \emph{Asian conference on computer vision}, pages 189--199.
  Springer, 2007.

\bibitem[Schmidhuber and Blog()]{schmidhubermetalearning}
J{\"u}rgen Schmidhuber and AI~Blog.
\newblock Metalearning machines learn to learn (1987-).

\bibitem[Kirsch and Schmidhuber(2020)]{kirsch2020meta}
Louis Kirsch and J{\"u}rgen Schmidhuber.
\newblock Meta learning backpropagation and improving it.
\newblock \emph{arXiv preprint arXiv:2012.14905}, 2020.

\bibitem[Gomes et~al.(2021)Gomes, L{\'e}ger, and Gagn{\'e}]{gomes2021meta}
Hugo~Siqueira Gomes, Benjamin L{\'e}ger, and Christian Gagn{\'e}.
\newblock Meta learning black-box population-based optimizers.
\newblock \emph{arXiv preprint arXiv:2103.03526}, 2021.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1126--1135. JMLR. org, 2017.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard~S Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{arXiv preprint arXiv:1703.05175}, 2017.

\bibitem[Romero et~al.(2016)Romero, Carrier, Erraqabi, Sylvain, Auvolat,
  Dejoie, Legault, Dub{\'e}, Hussin, and Bengio]{romero2016diet}
Adriana Romero, Pierre~Luc Carrier, Akram Erraqabi, Tristan Sylvain, Alex
  Auvolat, Etienne Dejoie, Marc-Andr{\'e} Legault, Marie-Pierre Dub{\'e},
  Julie~G Hussin, and Yoshua Bengio.
\newblock Diet networks: thin parameters for fat genomics.
\newblock \emph{arXiv preprint arXiv:1611.09340}, 2016.

\bibitem[Requeima et~al.(2019)Requeima, Gordon, Bronskill, Nowozin, and
  Turner]{requeima2019fast}
James Requeima, Jonathan Gordon, John Bronskill, Sebastian Nowozin, and
  Richard~E Turner.
\newblock Fast and flexible multi-task classification using conditional neural
  adaptive processes.
\newblock \emph{arXiv preprint arXiv:1906.07697}, 2019.

\bibitem[Li et~al.(2019)Li, Dong, Mei, Ma, Huang, and Hu]{li2019lgm}
Huaiyu Li, Weiming Dong, Xing Mei, Chongyang Ma, Feiyue Huang, and Bao-Gang Hu.
\newblock Lgm-net: Learning to generate matching networks for few-shot
  learning.
\newblock In \emph{International conference on machine learning}, pages
  3825--3834. PMLR, 2019.

\bibitem[Bertinetto et~al.(2016)Bertinetto, Henriques, Valmadre, Torr, and
  Vedaldi]{bertinetto2016learning}
Luca Bertinetto, Jo{\~a}o~F Henriques, Jack Valmadre, Philip~HS Torr, and
  Andrea Vedaldi.
\newblock Learning feed-forward one-shot learners.
\newblock \emph{arXiv preprint arXiv:1606.05233}, 2016.

\bibitem[Lian et~al.(2020)Lian, Zheng, Xu, Lu, Lin, Zhao, Huang, and
  Gao]{lian2020towards}
Dongze Lian, Yin Zheng, Yintao Xu, Yanxiong Lu, Leyu Lin, Peilin Zhao, Junzhou
  Huang, and Shenghua Gao.
\newblock Towards fast adaptation of neural architectures with meta learning.
\newblock In \emph{ICLR}. JMLR. org, 2020.

\bibitem[Elsken et~al.(2020)Elsken, Staffler, Metzen, and
  Hutter]{elsken2020meta}
Thomas Elsken, Benedikt Staffler, Jan~Hendrik Metzen, and Frank Hutter.
\newblock Meta-learning of neural architectures for few-shot learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 12365--12375, 2020.

\bibitem[Chen et~al.(2020)Chen, Duan, Chen, Xu, Chen, Liang, Zhang, and
  Li]{chen2020catch}
Xin Chen, Yawen Duan, Zewei Chen, Hang Xu, Zihao Chen, Xiaodan Liang, Tong
  Zhang, and Zhenguo Li.
\newblock Catch: Context-based meta reinforcement learning for transferrable
  architecture search.
\newblock In \emph{European Conference on Computer Vision}, pages 185--202.
  Springer, 2020.

\bibitem[Yu et~al.(2020)Yu, Jin, Liu, Bender, Kindermans, Tan, Huang, Song,
  Pang, and Le]{yu2020bignas}
Jiahui Yu, Pengchong Jin, Hanxiao Liu, Gabriel Bender, Pieter-Jan Kindermans,
  Mingxing Tan, Thomas Huang, Xiaodan Song, Ruoming Pang, and Quoc Le.
\newblock Bignas: Scaling up neural architecture search with big single-stage
  models.
\newblock \emph{arXiv preprint arXiv:2003.11142}, 2020.

\bibitem[He et~al.(2021)He, Zhao, and Chu]{he2021automl}
Xin He, Kaiyong Zhao, and Xiaowen Chu.
\newblock Automl: A survey of the state-of-the-art.
\newblock \emph{Knowledge-Based Systems}, 212:\penalty0 106622, 2021.

\bibitem[Brock et~al.(2017{\natexlab{b}})Brock, Lim, Ritchie, and
  Weston]{brock2017smash}
Andrew Brock, Theodore Lim, James~M Ritchie, and Nick Weston.
\newblock Smash: one-shot model architecture search through hypernetworks.
\newblock \emph{arXiv preprint arXiv:1708.05344}, 2017{\natexlab{b}}.

\bibitem[Bender et~al.(2018)Bender, Kindermans, Zoph, Vasudevan, and
  Le]{bender2018understanding}
Gabriel Bender, Pieter-Jan Kindermans, Barret Zoph, Vijay Vasudevan, and Quoc
  Le.
\newblock Understanding and simplifying one-shot architecture search.
\newblock In \emph{International Conference on Machine Learning}, pages
  550--559, 2018.

\bibitem[Ma et~al.(2021)Ma, Ahmed, Willke, and Philip]{ma2021deep}
Guixiang Ma, Nesreen~K Ahmed, Theodore~L Willke, and S~Yu Philip.
\newblock Deep graph similarity learning: A survey.
\newblock \emph{Data Mining and Knowledge Discovery}, pages 1--38, 2021.

\bibitem[Bai et~al.(2019)Bai, Ding, Bian, Chen, Sun, and Wang]{bai2019simgnn}
Yunsheng Bai, Hao Ding, Song Bian, Ting Chen, Yizhou Sun, and Wei Wang.
\newblock Simgnn: A neural network approach to fast graph similarity
  computation.
\newblock In \emph{Proceedings of the Twelfth ACM International Conference on
  Web Search and Data Mining}, pages 384--392, 2019.

\bibitem[Dowson and Landau(1982)]{dowson1982frechet}
DC~Dowson and BV~Landau.
\newblock The fr{\'e}chet distance between multivariate normal distributions.
\newblock \emph{Journal of multivariate analysis}, 12\penalty0 (3):\penalty0
  450--455, 1982.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Chan, Luk, and
  Borealis]{liu2019auto2}
Chia-Cheng Liu, Harris Chan, Kevin Luk, and AI~Borealis.
\newblock Auto-regressive graph generation modeling with improved evaluation
  methods.
\newblock In \emph{33rd Conference on Neural Information Processing Systems.
  Vancouver, Canada}, 2019{\natexlab{a}}.

\bibitem[Zilly et~al.(2019)Zilly, Zilly, Richter, Wattenhofer, Censi, and
  Frazzoli]{zilly2019frechet}
Julian Zilly, Hannes Zilly, Oliver Richter, Roger Wattenhofer, Andrea Censi,
  and Emilio Frazzoli.
\newblock The frechet distance of training and test distribution predicts the
  generalization gap.
\newblock 2019.

\bibitem[Thompson et~al.(2020{\natexlab{b}})Thompson, Ghalebi, DeVries, and
  Taylor]{thompson2020building}
Rylee Thompson, Elahe Ghalebi, Terrance DeVries, and Graham~W Taylor.
\newblock Building lego using deep generative models of graphs.
\newblock \emph{arXiv preprint arXiv:2012.11543}, 2020{\natexlab{b}}.

\bibitem[Yanardag and Vishwanathan(2015)]{yanardag2015deep}
Pinar Yanardag and SVN Vishwanathan.
\newblock Deep graph kernels.
\newblock In \emph{Proceedings of the 21th ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1365--1374, 2015.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, Ranzato, and
  de~Freitas]{Denil2013-la}
Misha Denil, Babak Shakibi, Laurent Dinh, Marc'Aurelio Ranzato, and Nando
  de~Freitas.
\newblock Predicting parameters in deep learning.
\newblock In C~J~C Burges, L~Bottou, M~Welling, Z~Ghahramani, and K~Q
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  26}, pages 2148--2156. Curran Associates, Inc., 2013.

\bibitem[Ratzlaff and Fuxin(2019)]{ratzlaff2019hypergan}
Neale Ratzlaff and Li~Fuxin.
\newblock Hypergan: A generative model for diverse, performant neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5361--5369. PMLR, 2019.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Peng, and
  Schwing]{liu2019knowledge}
Iou-Jen Liu, Jian Peng, and Alexander~G Schwing.
\newblock Knowledge flow: Improve upon your teachers.
\newblock \emph{arXiv preprint arXiv:1904.05878}, 2019{\natexlab{b}}.

\bibitem[Cheng et~al.(2017)Cheng, Wang, Zhou, and Zhang]{cheng2017survey}
Yu~Cheng, Duo Wang, Pan Zhou, and Tao Zhang.
\newblock A survey of model compression and acceleration for deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:1710.09282}, 2017.

\bibitem[Dauphin and Schoenholz(2019)]{dauphin2019metainit}
Yann Dauphin and Samuel~S Schoenholz.
\newblock Metainit: Initializing learning by learning to initialize.
\newblock 2019.

\bibitem[Zhu et~al.(2021)Zhu, Ni, Xu, Kong, Huang, and
  Goldstein]{zhu2021gradinit}
Chen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W~Ronny Huang, and Tom Goldstein.
\newblock Gradinit: Learning to initialize neural networks for stable and
  efficient training.
\newblock \emph{arXiv preprint arXiv:2102.08098}, 2021.

\bibitem[Das et~al.(2021)Das, Bhalgat, and Porikli]{das2021data}
Debasmit Das, Yash Bhalgat, and Fatih Porikli.
\newblock Data-driven weight initialization with sylvester solvers.
\newblock \emph{arXiv preprint arXiv:2105.10335}, 2021.

\bibitem[Yu et~al.(2019)Yu, Chen, Gao, and Yu]{yu2019dag}
Yue Yu, Jie Chen, Tian Gao, and Mo~Yu.
\newblock Dag-gnn: Dag structure learning with graph neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  7154--7163. PMLR, 2019.

\bibitem[Guo and Zhao(2020)]{guo2020systematic}
Xiaojie Guo and Liang Zhao.
\newblock A systematic survey on deep generative models for graph generation.
\newblock \emph{arXiv preprint arXiv:2007.06686}, 2020.

\bibitem[Radosavovic et~al.(2020)Radosavovic, Kosaraju, Girshick, He, and
  Doll{\'a}r]{radosavovic2020designing}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10428--10436, 2020.

\bibitem[You et~al.(2020{\natexlab{b}})You, Ying, and Leskovec]{you2020design}
Jiaxuan You, Zhitao Ying, and Jure Leskovec.
\newblock Design space for graph neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\end{thebibliography}
