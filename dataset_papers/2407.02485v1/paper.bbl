\begin{thebibliography}{101}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adlakha et~al.(2022)Adlakha, Dhuliawala, Suleman, de~Vries, and Reddy]{adlakha2022topiocqa}
Adlakha, V., Dhuliawala, S., Suleman, K., de~Vries, H., and Reddy, S.
\newblock Topiocqa: Open-domain conversational question answering with topic switching.
\newblock \emph{TACL}, 2022.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm}
Anil, R., Dai, A.~M., Firat, O., Johnson, M., Lepikhin, D., Passos, A., Shakeri, S., Taropa, E., Bailey, P., Chen, Z., et~al.
\newblock Palm 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Anthropic(2023)]{claude2}
Anthropic.
\newblock Model card and evaluations for claude models.
\newblock 2023.

\bibitem[Asai et~al.(2024{\natexlab{a}})Asai, Wu, Wang, Sil, and Hajishirzi]{asai2024selfrag}
Asai, A., Wu, Z., Wang, Y., Sil, A., and Hajishirzi, H.
\newblock Self-{RAG}: Learning to retrieve, generate, and critique through self-reflection.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Asai et~al.(2024{\natexlab{b}})Asai, Zhong, Chen, Koh, Zettlemoyer, Hajishirzi, and Yih]{asai2024reliable}
Asai, A., Zhong, Z., Chen, D., Koh, P.~W., Zettlemoyer, L., Hajishirzi, H., and Yih, W.-t.
\newblock Reliable, adaptable, and attributable language models with retrieval.
\newblock \emph{arXiv preprint arXiv:2403.03187}, 2024{\natexlab{b}}.

\bibitem[Bajaj et~al.(2016)Bajaj, Campos, Craswell, Deng, Gao, Liu, Majumder, McNamara, Mitra, Nguyen, et~al.]{msmarco}
Bajaj, P., Campos, D., Craswell, N., Deng, L., Gao, J., Liu, X., Majumder, R., McNamara, A., Mitra, B., Nguyen, T., et~al.
\newblock Ms marco: A human generated machine reading comprehension dataset.
\newblock \emph{arXiv preprint arXiv:1611.09268}, 2016.

\bibitem[Berant et~al.(2013)Berant, Chou, Frostig, and Liang]{webq}
Berant, J., Chou, A., Frostig, R., and Liang, P.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In \emph{EMNLP}, 2013.

\bibitem[Borgeaud et~al.(2022)Borgeaud, Mensch, Hoffmann, Cai, Rutherford, Millican, Van Den~Driessche, Lespiau, Damoc, Clark, et~al.]{borgeaud2022improving}
Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., Van Den~Driessche, G.~B., Lespiau, J.-B., Damoc, B., Clark, A., et~al.
\newblock Improving language models by retrieving from trillions of tokens.
\newblock In \emph{ICML}. PMLR, 2022.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Xiao, Zhang, Luo, Lian, and Liu]{bge_m3}
Chen, J., Xiao, S., Zhang, P., Luo, K., Lian, D., and Liu, Z.
\newblock Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Cano, Romanou, Bonnet, Matoba, Salvi, Pagliardini, Fan, K{\"o}pf, Mohtashami, et~al.]{chen2023meditron}
Chen, Z., Cano, A.~H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., Pagliardini, M., Fan, S., K{\"o}pf, A., Mohtashami, A., et~al.
\newblock Meditron-70b: Scaling medical pretraining for large language models.
\newblock \emph{arXiv preprint arXiv:2311.16079}, 2023{\natexlab{b}}.

\bibitem[Chung et~al.(2024)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2024scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, Y., Wang, X., Dehghani, M., Brahma, S., et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{JMLR}, 25\penalty0 (70), 2024.

\bibitem[Conover et~al.(2023)Conover, Hayes, Mathur, Xie, Wan, Shah, Ghodsi, Wendell, Zaharia, and Xin]{DatabricksBlog2023DollyV2}
Conover, M., Hayes, M., Mathur, A., Xie, J., Wan, J., Shah, S., Ghodsi, A., Wendell, P., Zaharia, M., and Xin, R.
\newblock Free {D}olly: Introducing the world's first truly open instruction-tuned llm, 2023.

\bibitem[Dasigi et~al.(2019)Dasigi, Liu, Marasovi{\'c}, Smith, and Gardner]{dasigi2019quoref}
Dasigi, P., Liu, N.~F., Marasovi{\'c}, A., Smith, N.~A., and Gardner, M.
\newblock Quoref: A reading comprehension dataset with questions requiring coreferential reasoning.
\newblock In \emph{EMNLP}, 2019.

\bibitem[DeepSeek(2024)]{deepseekai2024deepseekv2}
DeepSeek.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model, 2024.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu, Firat, et~al.]{du2022glam}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M., Zhou, Y., Yu, A.~W., Firat, O., et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{ICML}, 2022.

\bibitem[Dua et~al.(2019)Dua, Wang, Dasigi, Stanovsky, Singh, and Gardner]{dua2019drop}
Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.
\newblock Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs.
\newblock In \emph{NAACL}, 2019.

\bibitem[Fan et~al.(2019)Fan, Jernite, Perez, Grangier, Weston, and Auli]{fan2019eli5}
Fan, A., Jernite, Y., Perez, E., Grangier, D., Weston, J., and Auli, M.
\newblock Eli5: Long form question answering.
\newblock In \emph{ACL}, 2019.

\bibitem[Feng et~al.(2020)Feng, Wan, Gunasekara, Patel, Joshi, and Lastras]{feng2020doc2dial}
Feng, S., Wan, H., Gunasekara, C., Patel, S., Joshi, S., and Lastras, L.
\newblock doc2dial: A goal-oriented document-grounded dialogue dataset.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Glass et~al.(2022)Glass, Rossiello, Chowdhury, Naik, Cai, and Gliozzo]{glass-etal-2022-re2g}
Glass, M., Rossiello, G., Chowdhury, M. F.~M., Naik, A., Cai, P., and Gliozzo, A.
\newblock {R}e2{G}: Retrieve, rerank, generate.
\newblock In \emph{NAACL}, 2022.

\bibitem[Guu et~al.(2020)Guu, Lee, Tung, Pasupat, and Chang]{guu2020retrieval}
Guu, K., Lee, K., Tung, Z., Pasupat, P., and Chang, M.
\newblock Retrieval augmented language model pre-training.
\newblock In \emph{ICML}, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{mmlu}
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{ICLR}, 2021.

\bibitem[Ho et~al.(2020)Ho, Nguyen, Sugawara, and Aizawa]{2wikimqa}
Ho, X., Nguyen, A.-K.~D., Sugawara, S., and Aizawa, A.
\newblock Constructing a multi-hop qa dataset for comprehensive evaluation of reasoning steps.
\newblock In \emph{COLING}, 2020.

\bibitem[Honovich et~al.(2023)Honovich, Scialom, Levy, and Schick]{honovich2022unnatural}
Honovich, O., Scialom, T., Levy, O., and Schick, T.
\newblock Unnatural instructions: Tuning language models with (almost) no human labor.
\newblock In \emph{ACL}, 2023.

\bibitem[Huang et~al.(2023)Huang, Ping, Xu, Shoeybi, Chang, and Catanzaro]{huang2023raven}
Huang, J., Ping, W., Xu, P., Shoeybi, M., Chang, K. C.-C., and Catanzaro, B.
\newblock Raven: In-context learning with retrieval augmented encoder-decoder language models.
\newblock \emph{arXiv preprint arXiv:2308.07922}, 2023.

\bibitem[Izacard \& Grave(2021)Izacard and Grave]{fid}
Izacard, G. and Grave, E.
\newblock Leveraging passage retrieval with generative models for open domain question answering.
\newblock In \emph{EACL}, 2021.

\bibitem[Izacard et~al.(2022)Izacard, Caron, Hosseini, Riedel, Bojanowski, Joulin, and Grave]{izacard2021contriever}
Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E.
\newblock Unsupervised dense information retrieval with contrastive learning.
\newblock \emph{TMLR}, 2022.

\bibitem[Izacard et~al.(2023)Izacard, Lewis, Lomeli, Hosseini, Petroni, Schick, Dwivedi-Yu, Joulin, Riedel, and Grave]{atlas}
Izacard, G., Lewis, P., Lomeli, M., Hosseini, L., Petroni, F., Schick, T., Dwivedi-Yu, J., Joulin, A., Riedel, S., and Grave, E.
\newblock Atlas: Few-shot learning with retrieval augmented language models.
\newblock \emph{JMLR}, 24\penalty0 (251):\penalty0 1--43, 2023.

\bibitem[Jeong et~al.(2024)Jeong, Baek, Cho, Hwang, and Park]{jeong2024adaptive}
Jeong, S., Baek, J., Cho, S., Hwang, S.~J., and Park, J.~C.
\newblock Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity.
\newblock In \emph{NAACL}, 2024.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, et~al.]{jiang2024mixtral}
Jiang, A.~Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Hanna, E.~B., et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Jiang et~al.(2023)Jiang, Xu, Gao, Sun, Liu, Dwivedi-Yu, Yang, Callan, and Neubig]{activerag}
Jiang, Z., Xu, F.~F., Gao, L., Sun, Z., Liu, Q., Dwivedi-Yu, J., Yang, Y., Callan, J., and Neubig, G.
\newblock Active retrieval augmented generation.
\newblock In \emph{EMNLP}, 2023.

\bibitem[Jin et~al.(2021)Jin, Pan, Oufattole, Weng, Fang, and Szolovits]{medqa}
Jin, D., Pan, E., Oufattole, N., Weng, W.-H., Fang, H., and Szolovits, P.
\newblock What disease does this patient have? a large-scale open domain question answering dataset from medical exams.
\newblock \emph{Applied Sciences}, 11\penalty0 (14):\penalty0 6421, 2021.

\bibitem[Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu]{jin2019pubmedqa}
Jin, Q., Dhingra, B., Liu, Z., Cohen, W., and Lu, X.
\newblock Pubmedqa: A dataset for biomedical research question answering.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Jin et~al.(2023)Jin, Kim, Chen, Comeau, Yeganova, Wilbur, and Lu]{jin2023medcpt}
Jin, Q., Kim, W., Chen, Q., Comeau, D.~C., Yeganova, L., Wilbur, W.~J., and Lu, Z.
\newblock Medcpt: Contrastive pre-trained transformers with large-scale pubmed search logs for zero-shot biomedical information retrieval.
\newblock \emph{Bioinformatics}, 39\penalty0 (11), 2023.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and Zettlemoyer]{triviaqa}
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset for reading comprehension.
\newblock In \emph{ACL}, 2017.

\bibitem[Karpukhin et~al.(2020)Karpukhin, Oguz, Min, Lewis, Wu, Edunov, Chen, and Yih]{dpr}
Karpukhin, V., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov, S., Chen, D., and Yih, W.-t.
\newblock Dense passage retrieval for open-domain question answering.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Kasai et~al.(2023)Kasai, Sakaguchi, yoichi takahashi, Bras, Asai, Yu, Radev, Smith, Choi, and Inui]{kasai2023realtime}
Kasai, J., Sakaguchi, K., yoichi takahashi, Bras, R.~L., Asai, A., Yu, X.~V., Radev, D., Smith, N.~A., Choi, Y., and Inui, K.
\newblock Realtime {QA}: What's the answer right now?
\newblock In \emph{NeurIPS}, 2023.

\bibitem[Khalifa et~al.(2023)Khalifa, Logeswaran, Lee, Lee, and Wang]{khalifa-etal-2023-rerank}
Khalifa, M., Logeswaran, L., Lee, M., Lee, H., and Wang, L.
\newblock Few-shot reranking for multi-hop {QA} via language model prompting.
\newblock In \emph{ACL}, 2023.

\bibitem[Khattab et~al.(2022)Khattab, Santhanam, Li, Hall, Liang, Potts, and Zaharia]{khattab2022demonstrate}
Khattab, O., Santhanam, K., Li, X.~L., Hall, D., Liang, P., Potts, C., and Zaharia, M.
\newblock Demonstrate-search-predict: Composing retrieval and language models for knowledge-intensive nlp.
\newblock \emph{arXiv preprint arXiv:2212.14024}, 2022.

\bibitem[Kim et~al.(2023)Kim, Hessel, Jiang, Lu, Yu, Zhou, Bras, Alikhani, Kim, Sap, et~al.]{kim2022soda}
Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, P., Bras, R.~L., Alikhani, M., Kim, G., Sap, M., et~al.
\newblock Soda: Million-scale dialogue distillation with social commonsense contextualization.
\newblock In \emph{EMNLP}, 2023.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Ko{\v{c}}isk{\`y} et~al.(2018)Ko{\v{c}}isk{\`y}, Schwarz, Blunsom, Dyer, Hermann, Melis, and Grefenstette]{kovcisky2018narrativeqa}
Ko{\v{c}}isk{\`y}, T., Schwarz, J., Blunsom, P., Dyer, C., Hermann, K.~M., Melis, G., and Grefenstette, E.
\newblock The narrativeqa reading comprehension challenge.
\newblock \emph{TACL}, 2018.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins, Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee, et~al.]{nq}
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{TACL}, 2019.

\bibitem[Köpf et~al.(2023)Köpf, Kilcher, von Rütte, Anagnostidis, Tam, Stevens, Barhoum, Duc, Stanley, Nagyfi, ES, Suri, Glushkov, Dantuluri, Maguire, Schuhmann, Nguyen, and Mattick]{köpf2023openassistant}
Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N.~M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A.
\newblock Openassistant conversations - democratizing large language model alignment.
\newblock \emph{arXiv preprint arXiv: 2304.07327}, 2023.

\bibitem[Lazaridou et~al.(2022)Lazaridou, Gribovskaya, Stokowiec, and Grigorev]{lazaridou2022internet}
Lazaridou, A., Gribovskaya, E., Stokowiec, W., and Grigorev, N.
\newblock Internet-augmented language models through few-shot prompting for open-domain question answering.
\newblock \emph{arXiv preprint arXiv:2203.05115}, 2022.

\bibitem[Lee et~al.(2024)Lee, Roy, Xu, Raiman, Shoeybi, Catanzaro, and Ping]{lee2024nv}
Lee, C., Roy, R., Xu, M., Raiman, J., Shoeybi, M., Catanzaro, B., and Ping, W.
\newblock Nv-embed: Improved techniques for training llms as generalist embedding models.
\newblock \emph{arXiv preprint arXiv:2405.17428}, 2024.

\bibitem[Lewis et~al.(2020)Lewis, Perez, Piktus, Petroni, Karpukhin, Goyal, K{\"u}ttler, Lewis, Yih, Rockt{\"a}schel, et~al.]{lewis2020retrieval}
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K{\"u}ttler, H., Lewis, M., Yih, W.-t., Rockt{\"a}schel, T., et~al.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock \emph{NeurIPS}, 33, 2020.

\bibitem[Lin et~al.(2019)Lin, Tafjord, Clark, and Gardner]{lin2019reasoning}
Lin, K., Tafjord, O., Clark, P., and Gardner, M.
\newblock Reasoning over paragraph effects in situations.
\newblock In \emph{Workshop on Machine Reading for Question Answering}, 2019.

\bibitem[Lin et~al.(2023)Lin, Asai, Li, Oguz, Lin, Mehdad, Yih, and Chen]{dragon}
Lin, S.-C., Asai, A., Li, M., Oguz, B., Lin, J., Mehdad, Y., Yih, W.-t., and Chen, X.
\newblock How to train your dragon: Diverse augmentation towards generalizable dense retrieval.
\newblock In \emph{Findings of EMNLP}, 2023.

\bibitem[Lin et~al.(2024)Lin, Chen, Chen, Shi, Lomeli, James, Rodriguez, Kahn, Szilvasy, Lewis, Zettlemoyer, and tau Yih]{lin2024radit}
Lin, X.~V., Chen, X., Chen, M., Shi, W., Lomeli, M., James, R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M., Zettlemoyer, L., and tau Yih, W.
\newblock {RA}-{DIT}: Retrieval-augmented dual instruction tuning.
\newblock In \emph{ICLR}, 2024.

\bibitem[Liu et~al.(2024)Liu, Ping, Roy, Xu, Shoeybi, and Catanzaro]{liu2024chatqa}
Liu, Z., Ping, W., Roy, R., Xu, P., Shoeybi, M., and Catanzaro, B.
\newblock Chatqa: Surpassing gpt-4 on conversational qa and rag.
\newblock \emph{arXiv preprint arXiv:2401.10225}, 2024.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, et~al.]{longpre2023flan}
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.~W., Tay, Y., Zhou, D., Le, Q.~V., et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock In \emph{ICML}, 2023.

\bibitem[Luan et~al.(2021)Luan, Eisenstein, Toutanova, and Collins]{luan2021sparse}
Luan, Y., Eisenstein, J., Toutanova, K., and Collins, M.
\newblock Sparse, dense, and attentional representations for text retrieval.
\newblock \emph{TACL}, 2021.

\bibitem[Luo et~al.(2023)Luo, Chuang, Gong, Zhang, Kim, Wu, Fox, Meng, and Glass]{luo2023sail}
Luo, H., Chuang, Y.-S., Gong, Y., Zhang, T., Kim, Y., Wu, X., Fox, D., Meng, H., and Glass, J.
\newblock Sail: Search-augmented instruction learning.
\newblock \emph{arXiv preprint arXiv:2305.15225}, 2023.

\bibitem[Ma et~al.(2023)Ma, Wang, Yang, Wei, and Lin]{repllama}
Ma, X., Wang, L., Yang, N., Wei, F., and Lin, J.
\newblock Fine-tuning llama for multi-stage text retrieval.
\newblock \emph{arXiv preprint arXiv:2310.08319}, 2023.

\bibitem[Mallen et~al.(2023)Mallen, Asai, Zhong, Das, Khashabi, and Hajishirzi]{popqa}
Mallen, A., Asai, A., Zhong, V., Das, R., Khashabi, D., and Hajishirzi, H.
\newblock When not to trust language models: Investigating effectiveness of parametric and non-parametric memories.
\newblock In \emph{ACL}, 2023.

\bibitem[Menon et~al.(2022)Menon, Jayasumana, Rawat, Kim, Reddi, and Kumar]{menon2022defense}
Menon, A., Jayasumana, S., Rawat, A.~S., Kim, S., Reddi, S., and Kumar, S.
\newblock In defense of dual-encoders for neural ranking.
\newblock In \emph{ICML}, 2022.

\bibitem[Meta-AI(2024)]{llama3}
Meta-AI.
\newblock Llama 3 model card.
\newblock 2024.

\bibitem[Mistral(2024)]{Mixtral8x22B}
Mistral.
\newblock Mixtral 8x22b.
\newblock 2024.
\newblock URL \url{https://mistral.ai/news/mixtral-8x22b/}.

\bibitem[Mitra et~al.(2018)Mitra, Craswell, et~al.]{mitra2018introduction}
Mitra, B., Craswell, N., et~al.
\newblock An introduction to neural information retrieval.
\newblock \emph{Foundations and Trends{\textregistered} in Information Retrieval}, 2018.

\bibitem[Muennighoff et~al.(2024)Muennighoff, Su, Wang, Yang, Wei, Yu, Singh, and Kiela]{muennighoff2024generative}
Muennighoff, N., Su, H., Wang, L., Yang, N., Wei, F., Yu, T., Singh, A., and Kiela, D.
\newblock Generative representational instruction tuning.
\newblock \emph{arXiv preprint arXiv:2402.09906}, 2024.

\bibitem[Nogueira et~al.(2020)Nogueira, Jiang, Pradeep, and Lin]{monot5}
Nogueira, R., Jiang, Z., Pradeep, R., and Lin, J.
\newblock Document ranking with a pretrained sequence-to-sequence model.
\newblock In \emph{Findings of EMNLP}, 2020.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock Introducing {ChatGPT}, 2022.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock {GPT-4}, 2023.

\bibitem[Oren et~al.(2024)Oren, Meister, Chatterji, Ladhak, and Hashimoto]{oren2024proving}
Oren, Y., Meister, N., Chatterji, N.~S., Ladhak, F., and Hashimoto, T.
\newblock Proving test set contamination in black-box language models.
\newblock In \emph{ICLR}, 2024.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{NeurIPS}, 35, 2022.

\bibitem[Pal et~al.(2022)Pal, Umapathi, and Sankarasubbu]{pal2022medmcqa}
Pal, A., Umapathi, L.~K., and Sankarasubbu, M.
\newblock Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering.
\newblock In \emph{CHIL}, 2022.

\bibitem[Petroni et~al.(2021)Petroni, Piktus, Fan, Lewis, Yazdani, De~Cao, Thorne, Jernite, Karpukhin, Maillard, Plachouras, Rockt{\"a}schel, and Riedel]{petroni-etal-2021-kilt}
Petroni, F., Piktus, A., Fan, A., Lewis, P., Yazdani, M., De~Cao, N., Thorne, J., Jernite, Y., Karpukhin, V., Maillard, J., Plachouras, V., Rockt{\"a}schel, T., and Riedel, S.
\newblock {KILT}: a benchmark for knowledge intensive language tasks.
\newblock In \emph{NAACL}, 2021.

\bibitem[Qin et~al.(2024)Qin, Jagerman, Hui, Zhuang, Wu, Shen, Liu, Liu, Metzler, Wang, et~al.]{qin2023large}
Qin, Z., Jagerman, R., Hui, K., Zhuang, H., Wu, J., Shen, J., Liu, T., Liu, J., Metzler, D., Wang, X., et~al.
\newblock Large language models are effective text rankers with pairwise ranking prompting.
\newblock In \emph{Findings of NAACL}, 2024.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{rajpurkar2016squad}
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.
\newblock Squad: 100,000+ questions for machine comprehension of text.
\newblock In \emph{EMNLP}, 2016.

\bibitem[Ram et~al.(2023)Ram, Levine, Dalmedigos, Muhlgay, Shashua, Leyton-Brown, and Shoham]{ram2023context}
Ram, O., Levine, Y., Dalmedigos, I., Muhlgay, D., Shashua, A., Leyton-Brown, K., and Shoham, Y.
\newblock In-context retrieval-augmented language models.
\newblock \emph{TACL}, 2023.

\bibitem[Robertson et~al.(2004)Robertson, Zaragoza, and Taylor]{robertson2004simple}
Robertson, S., Zaragoza, H., and Taylor, M.
\newblock Simple bm25 extension to multiple weighted fields.
\newblock In \emph{CIKM}, 2004.

\bibitem[Sachan et~al.(2021)Sachan, Reddy, Hamilton, Dyer, and Yogatama]{sachan2021endtoend}
Sachan, D.~S., Reddy, S., Hamilton, W.~L., Dyer, C., and Yogatama, D.
\newblock End-to-end training of multi-document reader and retriever for open-domain question answering.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Shao et~al.(2023)Shao, Gong, Shen, Huang, Duan, and Chen]{shao2023enhancing}
Shao, Z., Gong, Y., Shen, Y., Huang, M., Duan, N., and Chen, W.
\newblock Enhancing retrieval-augmented large language models with iterative retrieval-generation synergy.
\newblock In \emph{Findings of EMNLP}, 2023.

\bibitem[Shi et~al.(2024)Shi, Min, Yasunaga, Seo, James, Lewis, Zettlemoyer, and Yih]{shi2023replug}
Shi, W., Min, S., Yasunaga, M., Seo, M., James, R., Lewis, M., Zettlemoyer, L., and Yih, W.-t.
\newblock Replug: Retrieval-augmented black-box language models.
\newblock In \emph{NAACL}, 2024.

\bibitem[Sun et~al.(2023)Sun, Yan, Ma, Wang, Ren, Chen, Yin, and Ren]{sun-etal-2023-chatgpt}
Sun, W., Yan, L., Ma, X., Wang, S., Ren, P., Chen, Z., Yin, D., and Ren, Z.
\newblock Is {C}hat{GPT} good at search? investigating large language models as re-ranking agents.
\newblock In \emph{EMNLP}, 2023.

\bibitem[Thakur et~al.(2021)Thakur, Reimers, R{\"u}ckl{\'e}, Srivastava, and Gurevych]{thakur2021beir}
Thakur, N., Reimers, N., R{\"u}ckl{\'e}, A., Srivastava, A., and Gurevych, I.
\newblock Beir: A heterogeneous benchmark for zero-shot evaluation of information retrieval models.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Thorne et~al.(2018)Thorne, Vlachos, Christodoulopoulos, and Mittal]{thorne2018fever}
Thorne, J., Vlachos, A., Christodoulopoulos, C., and Mittal, A.
\newblock Fever: A large-scale dataset for fact extraction and verification.
\newblock In \emph{NAACL}, 2018.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Trischler et~al.(2017)Trischler, Wang, Yuan, Harris, Sordoni, Bachman, and Suleman]{newsqa}
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Bachman, P., and Suleman, K.
\newblock Newsqa: A machine comprehension dataset.
\newblock In \emph{RepL4NLP Workshop at ACL}, 2017.

\bibitem[Trivedi et~al.(2023)Trivedi, Balasubramanian, Khot, and Sabharwal]{trivedi2023interleaving}
Trivedi, H., Balasubramanian, N., Khot, T., and Sabharwal, A.
\newblock Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.
\newblock In \emph{ACL}, 2023.

\bibitem[Tsatsaronis et~al.(2015)Tsatsaronis, Balikas, Malakasiotis, Partalas, Zschunke, Alvers, Weissenborn, Krithara, Petridis, Polychronopoulos, et~al.]{bioasq}
Tsatsaronis, G., Balikas, G., Malakasiotis, P., Partalas, I., Zschunke, M., Alvers, M.~R., Weissenborn, D., Krithara, A., Petridis, S., Polychronopoulos, D., et~al.
\newblock An overview of the bioasq large-scale biomedical semantic indexing and question answering competition.
\newblock \emph{BMC bioinformatics}, 2015.

\bibitem[Wang et~al.(2024)Wang, Ping, McAfee, Xu, Li, Shoeybi, and Catanzaro]{wang2023instructretro}
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi, M., and Catanzaro, B.
\newblock Instructretro: Instruction tuning post retrieval-augmented pretraining.
\newblock In \emph{ICML}, 2024.

\bibitem[Wang et~al.(2022)Wang, Yang, Huang, Jiao, Yang, Jiang, Majumder, and Wei]{wang2022text}
Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Jiang, D., Majumder, R., and Wei, F.
\newblock Text embeddings by weakly-supervised contrastive pre-training.
\newblock \emph{arXiv preprint arXiv:2212.03533}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Yang, Huang, Yang, Majumder, and Wei]{wang2023improving}
Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R., and Wei, F.
\newblock Improving text embeddings with large language models.
\newblock \emph{arXiv preprint arXiv:2401.00368}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{wang2023self}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language models with self-generated instructions.
\newblock In \emph{ACL}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Araki, Jiang, Parvez, and Neubig]{wang2023learning}
Wang, Z., Araki, J., Jiang, Z., Parvez, M.~R., and Neubig, G.
\newblock Learning to filter context for retrieval-augmented generation.
\newblock \emph{arXiv preprint arXiv:2311.08377}, 2023{\natexlab{c}}.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai, A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{ICLR}, 2022.

\bibitem[Wu et~al.(2024)Wu, Lin, Zhang, Zhang, Xie, and Wang]{pmcllama}
Wu, C., Lin, W., Zhang, X., Zhang, Y., Xie, W., and Wang, Y.
\newblock Pmc-llama: toward building open-source language models for medicine.
\newblock \emph{JAMIA}, 2024.

\bibitem[Wu et~al.(2023)Wu, Parish, Cheng, Min, Ammanabrolu, Ostendorf, and Hajishirzi]{wu2023inscit}
Wu, Z., Parish, R., Cheng, H., Min, S., Ammanabrolu, P., Ostendorf, M., and Hajishirzi, H.
\newblock Inscit: Information-seeking conversations with mixed-initiative interactions.
\newblock \emph{TACL}, 2023.

\bibitem[Xiong et~al.(2024)Xiong, Jin, Lu, and Zhang]{xiong2024benchmarking}
Xiong, G., Jin, Q., Lu, Z., and Zhang, A.
\newblock Benchmarking retrieval-augmented generation for medicine.
\newblock \emph{arXiv preprint arXiv:2402.13178}, 2024.

\bibitem[Xu et~al.(2024{\natexlab{a}})Xu, Shi, and Choi]{xu2024recomp}
Xu, F., Shi, W., and Choi, E.
\newblock {RECOMP}: Improving retrieval-augmented {LM}s with context compression and selective augmentation.
\newblock In \emph{ICLR}, 2024{\natexlab{a}}.

\bibitem[Xu et~al.(2024{\natexlab{b}})Xu, Ping, Wu, McAfee, Zhu, Liu, Subramanian, Bakhturina, Shoeybi, and Catanzaro]{xu2023retrieval}
Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B.
\newblock Retrieval meets long context large language models.
\newblock In \emph{ICLR}, 2024{\natexlab{b}}.

\bibitem[Yang et~al.(2018)Yang, Qi, Zhang, Bengio, Cohen, Salakhutdinov, and Manning]{yang2018hotpotqa}
Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W.~W., Salakhutdinov, R., and Manning, C.~D.
\newblock {HotpotQA}: A dataset for diverse, explainable multi-hop question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Yoran et~al.(2024)Yoran, Wolfson, Ram, and Berant]{robustlm}
Yoran, O., Wolfson, T., Ram, O., and Berant, J.
\newblock Making retrieval-augmented language models robust to irrelevant context.
\newblock In \emph{ICLR}, 2024.

\bibitem[Yu et~al.(2023{\natexlab{a}})Yu, Iter, Wang, Xu, Ju, Sanyal, Zhu, Zeng, and Jiang]{genread}
Yu, W., Iter, D., Wang, S., Xu, Y., Ju, M., Sanyal, S., Zhu, C., Zeng, M., and Jiang, M.
\newblock Generate rather than retrieve: Large language models are strong context generators.
\newblock In \emph{ICLR}, 2023{\natexlab{a}}.

\bibitem[Yu et~al.(2023{\natexlab{b}})Yu, Zhang, Pan, Ma, Wang, and Yu]{chain_of_note}
Yu, W., Zhang, H., Pan, X., Ma, K., Wang, H., and Yu, D.
\newblock Chain-of-note: Enhancing robustness in retrieval-augmented language models.
\newblock \emph{arXiv preprint arXiv:2311.09210}, 2023{\natexlab{b}}.

\bibitem[Yu et~al.(2024)Yu, Zhang, Liang, Jiang, and Sabharwal]{refeed}
Yu, W., Zhang, Z., Liang, Z., Jiang, M., and Sabharwal, A.
\newblock Improving language models via plug-and-play retrieval feedback, 2024.

\bibitem[Yu et~al.(2022)Yu, Xiong, Sun, Zhang, and Overwijk]{yu2022coco}
Yu, Y., Xiong, C., Sun, S., Zhang, C., and Overwijk, A.
\newblock Coco-dr: Combating distribution shift in zero-shot dense retrieval with contrastive and distributionally robust learning.
\newblock In \emph{EMNLP}, 2022.

\bibitem[Zhang et~al.(2024)Zhang, Patil, Jain, Shen, Zaharia, Stoica, and Gonzalez]{zhang2024raft}
Zhang, T., Patil, S.~G., Jain, N., Shen, S., Zaharia, M., Stoica, I., and Gonzalez, J.~E.
\newblock Raft: Adapting language model to domain specific rag.
\newblock \emph{arXiv preprint arXiv:2403.10131}, 2024.

\bibitem[Zhu et~al.(2021)Zhu, Lei, Huang, Wang, Zhang, Lv, Feng, and Chua]{zhu2021tat}
Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J., Feng, F., and Chua, T.-S.
\newblock Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.
\newblock In \emph{ACL}, 2021.

\bibitem[Zhu et~al.(2024)Zhu, Zhang, Zhang, Chen, Xie, Dou, Liu, and Wen]{zhu2024inters}
Zhu, Y., Zhang, P., Zhang, C., Chen, Y., Xie, B., Dou, Z., Liu, Z., and Wen, J.-R.
\newblock Inters: Unlocking the power of large language models in search with instruction tuning.
\newblock In \emph{ACL}, 2024.

\end{thebibliography}
