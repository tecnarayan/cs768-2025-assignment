\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2018)Arora, Cohen, and Hazan]{arora2018optimization}
Arora, S., Cohen, N., and Hazan, E.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  244--253, 2018.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  7413--7424, 2019.

\bibitem[Azulay et~al.(2021)Azulay, Moroshko, Nacson, Woodworth, Srebro,
  Globerson, and Soudry]{azulay2021implicit}
Azulay, S., Moroshko, E., Nacson, M.~S., Woodworth, B., Srebro, N., Globerson,
  A., and Soudry, D.
\newblock On the implicit bias of initialization shape: Beyond infinitesimal
  mirror descent.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Bah et~al.(2022)Bah, Rauhut, Terstiege, and
  Westdickenberg]{bah2022learning}
Bah, B., Rauhut, H., Terstiege, U., and Westdickenberg, M.
\newblock {Learning deep linear neural networks: Riemannian gradient flows and
  convergence to global minimizers}.
\newblock \emph{Information and Inference: A Journal of the IMA}, 11\penalty0
  (1):\penalty0 307--353, 2022.

\bibitem[Balda et~al.(2018)Balda, Behboodi, and Mathar]{balda2018tensor}
Balda, E.~R., Behboodi, A., and Mathar, R.
\newblock A tensor analysis on dense connectivity via convolutional arithmetic
  circuits.
\newblock \emph{Preprint}, 2018.

\bibitem[Bartlett et~al.(2018)Bartlett, Helmbold, and
  Long]{bartlett2018gradient}
Bartlett, P., Helmbold, D., and Long, P.
\newblock Gradient descent with identity initialization efficiently learns
  positive definite linear transformations.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  520--529, 2018.

\bibitem[Beylkin \& Mohlenkamp(2002)Beylkin and
  Mohlenkamp]{beylkin2002numerical}
Beylkin, G. and Mohlenkamp, M.~J.
\newblock Numerical operator calculus in higher dimensions.
\newblock \emph{Proceedings of the National Academy of Sciences}, 99\penalty0
  (16):\penalty0 10246--10251, 2002.

\bibitem[Beylkin et~al.(2009)Beylkin, Garcke, and
  Mohlenkamp]{beylkin2009multivariate}
Beylkin, G., Garcke, J., and Mohlenkamp, M.~J.
\newblock Multivariate regression and machine learning with sums of separable
  functions.
\newblock \emph{SIAM Journal on Scientific Computing}, 31\penalty0
  (3):\penalty0 1840--1857, 2009.

\bibitem[Blanc et~al.(2020)Blanc, Gupta, Valiant, and
  Valiant]{blanc2020implicit}
Blanc, G., Gupta, N., Valiant, G., and Valiant, P.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2020.

\bibitem[Chizat \& Bach(2020)Chizat and Bach]{chizat2020implicit}
Chizat, L. and Bach, F.
\newblock Implicit bias of gradient descent for wide two-layer neural networks
  trained with the logistic loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  1305--1338,
  2020.

\bibitem[Chou et~al.(2020)Chou, Gieshoff, Maly, and Rauhut]{chou2020gradient}
Chou, H.-H., Gieshoff, C., Maly, J., and Rauhut, H.
\newblock Gradient descent for deep matrix factorization: Dynamics and implicit
  bias towards low rank.
\newblock \emph{arXiv preprint arXiv:2011.13772}, 2020.

\bibitem[Chou et~al.(2021)Chou, Maly, and Rauhut]{chou2021more}
Chou, H.-H., Maly, J., and Rauhut, H.
\newblock More is less: Inducing sparsity via overparameterization.
\newblock \emph{arXiv preprint arXiv:2112.11027}, 2021.

\bibitem[Cohen \& Shashua(2014)Cohen and Shashua]{cohen2014simnets}
Cohen, N. and Shashua, A.
\newblock Simnets: A generalization of convolutional networks.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS),
  Deep Learning Workshop}, 2014.

\bibitem[Cohen \& Shashua(2016)Cohen and Shashua]{cohen2016convolutional}
Cohen, N. and Shashua, A.
\newblock Convolutional rectifier networks as generalized tensor
  decompositions.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Cohen \& Shashua(2017)Cohen and Shashua]{cohen2017inductive}
Cohen, N. and Shashua, A.
\newblock Inductive bias of deep convolutional networks through pooling
  geometry.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\bibitem[Cohen et~al.(2016{\natexlab{a}})Cohen, Sharir, and
  Shashua]{cohen2016deep}
Cohen, N., Sharir, O., and Shashua, A.
\newblock Deep simnets.
\newblock \emph{IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2016{\natexlab{a}}.

\bibitem[Cohen et~al.(2016{\natexlab{b}})Cohen, Sharir, and
  Shashua]{cohen2016expressive}
Cohen, N., Sharir, O., and Shashua, A.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock \emph{Conference On Learning Theory (COLT)}, 2016{\natexlab{b}}.

\bibitem[Cohen et~al.(2017)Cohen, Sharir, Levine, Tamari, Yakira, and
  Shashua]{cohen2017analysis}
Cohen, N., Sharir, O., Levine, Y., Tamari, R., Yakira, D., and Shashua, A.
\newblock Analysis and design of convolutional networks via hierarchical tensor
  decompositions.
\newblock \emph{Intel Collaborative Research Institute for Computational
  Intelligence (ICRI-CI) Special Issue on Deep Learning Theory}, 2017.

\bibitem[Cohen et~al.(2018)Cohen, Tamari, and Shashua]{cohen2018boosting}
Cohen, N., Tamari, R., and Shashua, A.
\newblock Boosting dilated convolutional networks with mixed tensor
  decompositions.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Da~Silva \& Herrmann(2015)Da~Silva and Herrmann]{da2015optimization}
Da~Silva, C. and Herrmann, F.~J.
\newblock Optimization on the hierarchical tucker manifold--applications to
  tensor completion.
\newblock \emph{Linear Algebra and its Applications}, 481:\penalty0 131--173,
  2015.

\bibitem[Du et~al.(2018)Du, Hu, and Lee]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  384--395, 2018.

\bibitem[Eftekhari \& Zygalakis(2021)Eftekhari and
  Zygalakis]{eftekhari2021implicit}
Eftekhari, A. and Zygalakis, K.
\newblock Limitations of implicit bias in matrix sensing: Initialization rank
  matters.
\newblock \emph{arXiv preprint arXiv:2008.12091}, 2021.

\bibitem[Elkabetz \& Cohen(2021)Elkabetz and Cohen]{elkabetz2021continuous}
Elkabetz, O. and Cohen, N.
\newblock Continuous vs. discrete optimization of deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Ergen \& Pilanci(2021)Ergen and Pilanci]{ergen2021implicit}
Ergen, T. and Pilanci, M.
\newblock Implicit convex regularizers of cnn architectures: Convex
  optimization of two-and three-layer networks in polynomial time.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Felser et~al.(2021)Felser, Trenti, Sestini, Gianelle, Zuliani,
  Lucchesi, and Montangero]{felser2021quantum}
Felser, T., Trenti, M., Sestini, L., Gianelle, A., Zuliani, D., Lucchesi, D.,
  and Montangero, S.
\newblock Quantum-inspired machine learning on high-energy physics data.
\newblock \emph{npj Quantum Information}, 7\penalty0 (1):\penalty0 1--8, 2021.

\bibitem[Ge et~al.(2021)Ge, Ren, Wang, and Zhou]{ge2021understanding}
Ge, R., Ren, Y., Wang, X., and Zhou, M.
\newblock Understanding deflation process in over-parametrized tensor
  decomposition.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gidel, G., Bach, F., and Lacoste-Julien, S.
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  3196--3206, 2019.

\bibitem[Gissin et~al.(2020)Gissin, Shalev-Shwartz, and
  Daniely]{gissin2020implicit}
Gissin, D., Shalev-Shwartz, S., and Daniely, A.
\newblock The implicit bias of depth: How incremental learning drives
  generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Grant et~al.(2018)Grant, Benedetti, Cao, Hallam, Lockhart, Stojevic,
  Green, and Severini]{grant2018hierarchical}
Grant, E., Benedetti, M., Cao, S., Hallam, A., Lockhart, J., Stojevic, V.,
  Green, A.~G., and Severini, S.
\newblock Hierarchical quantum classifiers.
\newblock \emph{npj Quantum Information}, 4\penalty0 (1):\penalty0 1--8, 2018.

\bibitem[Grasedyck(2010)]{grasedyck2010hierarchical}
Grasedyck, L.
\newblock Hierarchical singular value decomposition of tensors.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 31\penalty0
  (4):\penalty0 2029--2054, 2010.

\bibitem[Grasedyck et~al.(2013)Grasedyck, Kressner, and
  Tobler]{grasedyck2013literature}
Grasedyck, L., Kressner, D., and Tobler, C.
\newblock A literature survey of low-rank tensor approximation techniques.
\newblock \emph{GAMM-Mitteilungen}, 36\penalty0 (1):\penalty0 53--78, 2013.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  9461--9471, 2018.

\bibitem[Hackbusch(2006)]{hackbusch2006efficient}
Hackbusch, W.
\newblock On the efficient evaluation of coalescence integrals in population
  balance models.
\newblock \emph{Computing}, 78\penalty0 (2):\penalty0 145--159, 2006.

\bibitem[Hackbusch(2012)]{hackbusch2012tensor}
Hackbusch, W.
\newblock \emph{Tensor spaces and numerical tensor calculus}, volume~42.
\newblock Springer, 2012.

\bibitem[Hackbusch \& K{\"u}hn(2009)Hackbusch and K{\"u}hn]{hackbusch2009new}
Hackbusch, W. and K{\"u}hn, S.
\newblock A new scheme for the tensor representation.
\newblock \emph{Journal of Fourier analysis and applications}, 15\penalty0
  (5):\penalty0 706--722, 2009.

\bibitem[HaoChen et~al.(2021)HaoChen, Wei, Lee, and Ma]{haochen2021shape}
HaoChen, J.~Z., Wei, C., Lee, J., and Ma, T.
\newblock Shape matters: Understanding the implicit bias of the noise
  covariance.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2021.

\bibitem[Harrison et~al.(2003)Harrison, Fann, Yanai, and
  Beylkin]{harrison2003multiresolution}
Harrison, R.~J., Fann, G.~I., Yanai, T., and Beylkin, G.
\newblock Multiresolution quantum chemistry in multiwavelet bases.
\newblock In \emph{International Conference on Computational Science}, pp.\
  103--110. Springer, 2003.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition (CVPR)}, pp.\  770--778, 2016.

\bibitem[Hitchcock(1927)]{hitchcock1927expression}
Hitchcock, F.~L.
\newblock The expression of a tensor or a polyadic as a sum of products.
\newblock \emph{Journal of Mathematics and Physics}, 6\penalty0 (1-4):\penalty0
  164--189, 1927.

\bibitem[Hong et~al.(2020)Hong, Gao, Yao, Zhang, Plaza, and
  Chanussot]{hong2020graph}
Hong, D., Gao, L., Yao, J., Zhang, B., Plaza, A., and Chanussot, J.
\newblock Graph convolutional networks for hyperspectral image classification.
\newblock \emph{IEEE Transactions on Geoscience and Remote Sensing}, 2020.

\bibitem[Hu et~al.(2020)Hu, Xiao, Adlam, and Pennington]{hu2020surprising}
Hu, W., Xiao, L., Adlam, B., and Pennington, J.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Jagadeesan et~al.(2021)Jagadeesan, Razenshteyn, and
  Gunasekar]{jagadeesan2021inductive}
Jagadeesan, M., Razenshteyn, I., and Gunasekar, S.
\newblock Inductive bias of multi-channel linear convolutional networks with
  bounded weight norm.
\newblock \emph{arXiv preprint arXiv:2102.12238}, 2021.

\bibitem[Ji \& Telgarsky(2019{\natexlab{a}})Ji and Telgarsky]{ji2019gradient}
Ji, Z. and Telgarsky, M.
\newblock Gradient descent aligns the layers of deep linear networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019{\natexlab{a}}.

\bibitem[Ji \& Telgarsky(2019{\natexlab{b}})Ji and Telgarsky]{ji2019implicit}
Ji, Z. and Telgarsky, M.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  1772--1798,
  2019{\natexlab{b}}.

\bibitem[Ji \& Telgarsky(2020)Ji and Telgarsky]{ji2020directional}
Ji, Z. and Telgarsky, M.
\newblock Directional convergence and alignment in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Kargas \& Sidiropoulos(2020)Kargas and
  Sidiropoulos]{kargas2020nonlinear}
Kargas, N. and Sidiropoulos, N.~D.
\newblock Nonlinear system identification via tensor completion.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pp.\  4420--4427, 2020.

\bibitem[Kargas \& Sidiropoulos(2021)Kargas and
  Sidiropoulos]{kargas2021supervised}
Kargas, N. and Sidiropoulos, N.~D.
\newblock Supervised learning and canonical decomposition of multivariate
  functions.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0
  1097--1107, 2021.

\bibitem[Khrulkov et~al.(2018)Khrulkov, Novikov, and
  Oseledets]{khrulkov2018expressive}
Khrulkov, V., Novikov, A., and Oseledets, I.
\newblock Expressive power of recurrent neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Khrulkov et~al.(2019)Khrulkov, Hrinchuk, and
  Oseledets]{khrulkov2019generalized}
Khrulkov, V., Hrinchuk, O., and Oseledets, I.
\newblock Generalized tensor models for recurrent neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Kim et~al.(2020)Kim, Linsley, Thakkar, and
  Serre]{kim2020disentangling}
Kim, J., Linsley, D., Thakkar, K., and Serre, T.
\newblock Disentangling neural mechanisms for perceptual grouping.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Kohn et~al.(2021)Kohn, Merkh, Mont{\'u}far, and
  Trager]{kohn2021geometry}
Kohn, K., Merkh, T., Mont{\'u}far, G., and Trager, M.
\newblock Geometry of linear convolutional networks.
\newblock \emph{arXiv preprint arXiv:2108.01538}, 2021.

\bibitem[Kolda(2006)]{kolda2006multilinear}
Kolda, T.~G.
\newblock Multilinear operators for higher-order decompositions.
\newblock Technical report, 2006.

\bibitem[Kolda \& Bader(2009)Kolda and Bader]{kolda2009tensor}
Kolda, T.~G. and Bader, B.~W.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM review}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[Lampinen \& Ganguli(2019)Lampinen and Ganguli]{lampinen2019analytic}
Lampinen, A.~K. and Ganguli, S.
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2019.

\bibitem[Levine et~al.(2018{\natexlab{a}})Levine, Sharir, and
  Shashua]{levine2018benefits}
Levine, Y., Sharir, O., and Shashua, A.
\newblock Benefits of depth for long-term memory of recurrent networks.
\newblock \emph{International Conference on Learning Representations (ICLR)
  Workshop}, 2018{\natexlab{a}}.

\bibitem[Levine et~al.(2018{\natexlab{b}})Levine, Yakira, Cohen, and
  Shashua]{levine2018deep}
Levine, Y., Yakira, D., Cohen, N., and Shashua, A.
\newblock Deep learning and quantum entanglement: Fundamental connections with
  implications to network design.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018{\natexlab{b}}.

\bibitem[Levine et~al.(2019)Levine, Sharir, Cohen, and
  Shashua]{levine2019quantum}
Levine, Y., Sharir, O., Cohen, N., and Shashua, A.
\newblock Quantum entanglement in deep learning architectures.
\newblock \emph{To appear in Physical Review Letters}, 2019.

\bibitem[Levine et~al.(2020)Levine, Wies, Sharir, Bata, and
  Shashua]{levine2020limits}
Levine, Y., Wies, N., Sharir, O., Bata, H., and Shashua, A.
\newblock Limits to depth efficiencies of self-attention.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Levine et~al.(2022)Levine, Wies, Jannai, Navon, Hoshen, and
  Shashua]{levine2022inductive}
Levine, Y., Wies, N., Jannai, D., Navon, D., Hoshen, Y., and Shashua, A.
\newblock The inductive bias of in-context learning: Rethinking pretraining
  example design.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2022.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Li, Y., Ma, T., and Zhang, H.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Proceedings of the 31st Conference On Learning Theory
  (COLT)}, pp.\  2--47, 2018.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Luo, and Lyu]{li2021towards}
Li, Z., Luo, Y., and Lyu, K.
\newblock Towards resolving the implicit bias of gradient descent for matrix
  factorization: Greedy low-rank learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Wang, and Arora]{li2021happens}
Li, Z., Wang, T., and Arora, S.
\newblock What happens after sgd reaches zero loss?--a mathematical framework.
\newblock \emph{arXiv preprint arXiv:2110.06914}, 2021{\natexlab{b}}.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, Windolf, and
  Serre]{linsley2018learning}
Linsley, D., Kim, J., Veerabadran, V., Windolf, C., and Serre, T.
\newblock Learning long-range spatial dependencies with horizontal gated
  recurrent units.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Lyu \& Li(2020)Lyu and Li]{lyu2020gradient}
Lyu, K. and Li, J.
\newblock Gradient descent maximizes the margin of homogeneous neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Lyu et~al.(2021)Lyu, Li, Wang, and Arora]{lyu2021gradient}
Lyu, K., Li, Z., Wang, R., and Arora, S.
\newblock Gradient descent on two-layer nets: Margin maximization and
  simplicity bias.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Milanesi et~al.(2021)Milanesi, Kadri, Ayache, and
  Arti{\`e}res]{milanesi2021implicit}
Milanesi, P., Kadri, H., Ayache, S., and Arti{\`e}res, T.
\newblock Implicit regularization in deep tensor factorization.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  2021.

\bibitem[Min et~al.(2021)Min, Tarmoun, Vidal, and Mallada]{min2021explicit}
Min, H., Tarmoun, S., Vidal, R., and Mallada, E.
\newblock On the explicit role of initialization on the convergence and
  implicit bias of overparametrized linear networks.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Mlynarski et~al.(2019)Mlynarski, Delingette, Criminisi, and
  Ayache]{mlynarski2019convolutional}
Mlynarski, P., Delingette, H., Criminisi, A., and Ayache, N.
\newblock 3d convolutional neural networks for tumor segmentation using
  long-range 2d context.
\newblock \emph{Computerized Medical Imaging and Graphics}, 73:\penalty0
  60--72, 2019.

\bibitem[Moroshko et~al.(2020)Moroshko, Gunasekar, Woodworth, Lee, Srebro, and
  Soudry]{moroshko2020implicit}
Moroshko, E., Gunasekar, S., Woodworth, B., Lee, J.~D., Srebro, N., and Soudry,
  D.
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Mulayoff \& Michaeli(2020)Mulayoff and Michaeli]{mulayoff2020unique}
Mulayoff, R. and Michaeli, T.
\newblock Unique properties of wide minima in deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Mulayoff et~al.(2021)Mulayoff, Michaeli, and
  Soudry]{mulayoff2021implicit}
Mulayoff, R., Michaeli, T., and Soudry, D.
\newblock The implicit bias of minima stability: A view from function space.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Nacson et~al.(2019{\natexlab{a}})Nacson, Gunasekar, Lee, Srebro, and
  Soudry]{nacson2019lexicographic}
Nacson, M.~S., Gunasekar, S., Lee, J., Srebro, N., and Soudry, D.
\newblock Lexicographic and depth-sensitive margins in homogeneous and
  non-homogeneous deep models.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2019{\natexlab{a}}.

\bibitem[Nacson et~al.(2019{\natexlab{b}})Nacson, Lee, Gunasekar, Savarese,
  Srebro, and Soudry]{nacson2019convergence}
Nacson, M.~S., Lee, J., Gunasekar, S., Savarese, P. H.~P., Srebro, N., and
  Soudry, D.
\newblock Convergence of gradient descent on separable data.
\newblock In \emph{Proceedings of the Twenty-Second International Conference on
  Artificial Intelligence and Statistics}, 2019{\natexlab{b}}.

\bibitem[Neyshabur(2017)]{neyshabur2017implicit}
Neyshabur, B.
\newblock Implicit regularization in deep learning.
\newblock \emph{PhD thesis}, 2017.

\bibitem[Oymak \& Soltanolkotabi(2019)Oymak and
  Soltanolkotabi]{oymak2019overparameterized}
Oymak, S. and Soltanolkotabi, M.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4951--4960, 2019.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock Automatic differentiation in pytorch.
\newblock In \emph{NIPS-W}, 2017.

\bibitem[Pesme et~al.(2021)Pesme, Pillaud-Vivien, and
  Flammarion]{pesme2021implicit}
Pesme, S., Pillaud-Vivien, L., and Flammarion, N.
\newblock Implicit bias of sgd for diagonal linear networks: a provable benefit
  of stochasticity.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Rauhut et~al.(2017)Rauhut, Schneider, and Stojanac]{rauhut2017low}
Rauhut, H., Schneider, R., and Stojanac, {\v{Z}}.
\newblock Low rank tensor recovery via iterative hard thresholding.
\newblock \emph{Linear Algebra and its Applications}, 523:\penalty0 220--262,
  2017.

\bibitem[Razin \& Cohen(2020)Razin and Cohen]{razin2020implicit}
Razin, N. and Cohen, N.
\newblock Implicit regularization in deep learning may not be explainable by
  norms.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2020.

\bibitem[Razin et~al.(2021)Razin, Maman, and Cohen]{razin2021implicit}
Razin, N., Maman, A., and Cohen, N.
\newblock Implicit regularization in tensor factorization.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Sarussi et~al.(2021)Sarussi, Brutzkus, and
  Globerson]{sarussi2021towards}
Sarussi, R., Brutzkus, A., and Globerson, A.
\newblock Towards understanding learning in neural networks with linear
  teachers.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Saxe et~al.(2014)Saxe, McClelland, and Ganguli]{saxe2014exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2014.

\bibitem[Shachaf et~al.(2021)Shachaf, Brutzkus, and
  Globerson]{shachaf2021theoretical}
Shachaf, G., Brutzkus, A., and Globerson, A.
\newblock A theoretical analysis of fine-tuning with linear teachers.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[Sharir \& Shashua(2018)Sharir and Shashua]{sharir2018expressive}
Sharir, O. and Shashua, A.
\newblock On the expressive power of overlapping architectures of deep
  learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Sharir et~al.(2016)Sharir, Tamari, Cohen, and
  Shashua]{sharir2016tensorial}
Sharir, O., Tamari, R., Cohen, N., and Shashua, A.
\newblock Tensorial mixture models.
\newblock \emph{arXiv preprint}, 2016.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Steinlechner(2016)]{steinlechner2016riemannian}
Steinlechner, M.
\newblock Riemannian optimization for high-dimensional tensor completion.
\newblock \emph{SIAM Journal on Scientific Computing}, 38\penalty0
  (5):\penalty0 S461--S484, 2016.

\bibitem[Stoudenmire(2018)]{stoudenmire2018learning}
Stoudenmire, E.~M.
\newblock Learning relevant features of data with multi-scale tensor networks.
\newblock \emph{Quantum Science and Technology}, 3\penalty0 (3):\penalty0
  034003, 2018.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2021long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Teschl(2012)]{teschl2012ordinary}
Teschl, G.
\newblock \emph{Ordinary differential equations and dynamical systems}, volume
  140.
\newblock American Mathematical Soc., 2012.

\bibitem[Vardi \& Shamir(2021)Vardi and Shamir]{vardi2021implicit}
Vardi, G. and Shamir, O.
\newblock Implicit regularization in relu networks with the square loss.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2021.

\bibitem[Vardi et~al.(2021)Vardi, Shamir, and Srebro]{vardi2021margin}
Vardi, G., Shamir, O., and Srebro, N.
\newblock On margin maximization in linear and relu networks.
\newblock \emph{arXiv preprint arXiv:2110.02732}, 2021.

\bibitem[Wang et~al.(2016)Wang, Xiong, Wang, Qiao, Lin, Tang, and
  Van~Gool]{wang2016temporal}
Wang, L., Xiong, Y., Wang, Z., Qiao, Y., Lin, D., Tang, X., and Van~Gool, L.
\newblock Temporal segment networks: Towards good practices for deep action
  recognition.
\newblock In \emph{European conference on computer vision}, pp.\  20--36.
  Springer, 2016.

\bibitem[Wies et~al.(2021)Wies, Levine, Jannai, and
  Shashua]{wies2021transformer}
Wies, N., Levine, Y., Jannai, D., and Shashua, A.
\newblock Which transformer architecture fits my data? a vocabulary bottleneck
  in self-attention.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
Woodworth, B., Gunasekar, S., Lee, J.~D., Moroshko, E., Savarese, P., Golan,
  I., Soudry, D., and Srebro, N.
\newblock Kernel and rich regimes in overparametrized models.
\newblock In \emph{Conference on Learning Theory (COLT)}, pp.\  3635--3673,
  2020.

\bibitem[Yun et~al.(2021)Yun, Krishnan, and Mobahi]{yun2021unifying}
Yun, C., Krishnan, S., and Mobahi, H.
\newblock A unifying view on implicit bias in training linear neural networks.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2021.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2017.

\end{thebibliography}
