\begin{thebibliography}{10}

\bibitem{bauschke:21:gmo}
H.~H. Bauschke, W.~M. Moursi, and X.~Wang.
\newblock Generalized monotone operators and their averaged resolvents.
\newblock {\em {Mathematical Programming}}, 189(1):{55--74}, 2021.

\bibitem{beck:09:afi}
A.~Beck and M.~Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em {SIAM J. Imaging Sci.}}, 2(1):{183--202}, 2009.

\bibitem{combettes:04:pmf}
P.~L. Combettes and T.~Pennanen.
\newblock Proximal methods for cohypomonotone operators.
\newblock {\em {SIAM J. Control Optim.}}, 43(2):{731--42}, 2004.

\bibitem{dang:15:otc}
C.~D. Dang and G.~Lan.
\newblock On the convergence properties of non-{Euclidean} extragradient
  methods for variational inequalities with generalized monotone operators.
\newblock {\em Computational Optimization and Applications}, 60(2):{277--310},
  2015.

\bibitem{devolder:11:sfo}
O.~Devolder.
\newblock Stochastic first order methods in smooth convex optimization, 2011.

\bibitem{diakonikolas:20:hif}
J.~Diakonikolas.
\newblock Halpern iteration for near-optimal and parameter-free monotone
  inclusion and strong solutions to variational inequalities.
\newblock In {\em Conference on Learning Theory}, pages 1428--1451. PMLR, 2020.

\bibitem{diakonikolas:21:emf}
J.~Diakonikolas, C.~Daskalakis, and M.~Jordan.
\newblock Efficient methods for structured nonconvex-nonconcave min-max
  optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2746--2754. PMLR, 2021.

\bibitem{ghadimi:16:agm}
S.~Ghadimi and G.~Lan.
\newblock Accelerated gradient methods for nonconvex nonlinear and stochastic
  programming.
\newblock {\em {Mathematical Programming}}, 156(1):{59--99}, March 2016.

\bibitem{golowich:20:lii}
N.~Golowich, S.~Pattathil, C.~Daskalakis, and A.~Ozdaglar.
\newblock Last iterate is slower than averaged iterate in smooth convex-concave
  saddle point problems.
\newblock In {\em {Conference on Learning Theory (COLT)}}, 2020.

\bibitem{goodfellow:14:gan}
I.~J. Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial networks.
\newblock In {\em {Neural Info. Proc. Sys.}}, 2014.

\bibitem{grimmer:20:tlo}
B.~Grimmer, H.~Lu, P.~Worah, and V.~Mirrokni.
\newblock The landscape of the proximal point method for nonconvex-nonconcave
  minimax optimization, 2020.

\bibitem{halpern:67:fpo}
B.~Halpern.
\newblock Fixed points of nonexpanding maps.
\newblock {\em Bull. Amer. Math. Soc}, 73:{957--961}, 1967.

\bibitem{hsieh:19:otc}
Y.-G. Hsieh, F.~Iutzeler, J.~Malick, and P.~Mertikopoulos.
\newblock On the convergence of single-call stochastic extra-gradient methods.
\newblock In {\em {Neural Info. Proc. Sys.}}, 2019.

\bibitem{hsieh:20:eau}
Y.-G. Hsieh, F.~Iutzeler, J.~Malick, and P.~Mertikopoulos.
\newblock Explore aggressively, update conservatively: Stochastic extragradient
  methods with variable stepsize scaling.
\newblock In {\em {Neural Info. Proc. Sys.}}, 2020.

\bibitem{hsieh:21:tlo}
Y.-P. Hsieh, P.~Mertikopoulos, and V.~Cevher.
\newblock The limits of min-max optimization algorithms: convergence to
  spurious non-critical sets.
\newblock In {\em {Proc. Intl. Conf. Mach. Learn}}, 2021.

\bibitem{juditsky:11:svi}
A.~Juditsky, A.~Nemirovski, and C.~Tauvel.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm.
\newblock {\em Stochastic Systems}, 1(1):17--58, 2011.

\bibitem{kim2021accelerated}
D.~Kim.
\newblock Accelerated proximal point method for maximally monotone operators.
\newblock {\em Mathematical Programming}, 190(1):57--87, 2021.

\bibitem{kohlenbach:21:otp}
U.~Kohlenbach.
\newblock On the proximal point algorithm and its {Halpern}-type variant for
  generalized monotone operators in {Hilbert} space.
\newblock {\em {Optimization Letters}}, 2021.

\bibitem{korpelevich:76:aem}
G.~M. Korpelevich.
\newblock An extragradient method for finding saddle points and other problems.
\newblock {\em {Ekonomika i Mateaticheskie Metody}}, 12(4):{747--56}, 1976.

\bibitem{letcher:21:oti}
A.~Letcher.
\newblock On the impossibility of global convergence in multi-loss
  optimization.
\newblock In {\em {Proc. Intl. Conf. on Learning Representations}}, 2021.

\bibitem{lieder:21:otc}
F.~Lieder.
\newblock On the convergence rate of the {Halpern}-iteration.
\newblock {\em {Optimization Letters}}, 15(2):{405--418}, 2021.

\bibitem{liu2020towards}
M.~Liu, Y.~Mroueh, J.~Ross, W.~Zhang, X.~Cui, P.~Das, and T.~Yang.
\newblock Towards better understanding of adaptive gradient algorithms in
  generative adversarial nets.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{liu:21:foc}
M.~Liu, H.~Rafique, Q.~Lin, and T.~Yang.
\newblock First-order convergence theory for weakly-convex-weakly-concave
  min-max problems.
\newblock {\em {J. Mach. Learning Res.}}, 22(169):{1--34}, 2021.

\bibitem{malitsky:20:gro}
Y.~Malitsky.
\newblock Golden ratio algorithms for variational inequalities.
\newblock {\em {Mathematical Programming}}, 184(1):{383--410}, 2020.

\bibitem{malitsky2018first}
Y.~Malitsky and T.~Pock.
\newblock A first-order primal-dual algorithm with linesearch.
\newblock {\em SIAM Journal on Optimization}, 28(1):411--432, 2018.

\bibitem{mertikopoulos:19:omd}
P.~Mertikopoulos, B.~Lecouat, H.~Zenati, C.-S. Foo, V.~Chandrasekhar, and
  G.~Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: going the extra
  (gradient) mile.
\newblock In {\em {Proc. Intl. Conf. on Learning Representations}}, 2019.

\bibitem{madry:18:tdl}
A.~M\k{a}dry, A.~Makelov, L.~Schmdit, D.~Tsipras, and A.~Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em {Proc. Intl. Conf. on Learning Representations}}, 2018.

\bibitem{mokhtari:20:aua}
A.~Mokhtari, A.~Ozdaglar, and S.~Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: {Proximal} point approach.
\newblock In {\em {Proc. Intl. Conf. Artificial Intelligence and Stat.
  (AISTATS)}}, 2020.

\bibitem{monteiro:10:otc}
R.~D.~C. Monteiro and B.~F. Svaiter.
\newblock On the complexity of the hybrid proximal extragradient method for the
  iterates and the ergodic mean.
\newblock {\em SIAM J. Optim.}, 20(6):{2755--87}, 2010.

\bibitem{monteiro2011complexity}
R.~D.~C. Monteiro and B.~F. Svaiter.
\newblock Complexity of variants of {Tseng's} modified {FB} splitting and
  {Korpelevich's} methods for hemivariational inequalities with applications to
  saddle-point and convex optimization problems.
\newblock {\em SIAM Journal on Optimization}, 21(4):1688--1720, 2011.

\bibitem{mukkamala2020convex}
M.~C. Mukkamala, P.~Ochs, T.~Pock, and S.~Sabach.
\newblock Convex-concave backtracking for inertial bregman proximal gradient
  algorithms in nonconvex optimization.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(3):658--682,
  2020.

\bibitem{nemirovski:04:pmw}
A.~Nemirovski.
\newblock Prox-method with rate of convergence {$O(1/t)$} for variational
  inequalities with {Lipschitz} continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM J. Optim.}, 15(1):{229--51}, 2004.

\bibitem{nemirovski:09:rsa}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em {SIAM J. Optim.}}, 19(4):{1574--609}, 2009.

\bibitem{nemirovski:83}
A.~Nemirovski and D.~Yudin.
\newblock {\em Problem complexity and method efficiency in optimization}.
\newblock wiley, 1983.

\bibitem{nesterov:83:amf}
Y.~Nesterov.
\newblock A method for unconstrained convex minimization problem with the rate
  of convergence {$O(1/k^2)$}.
\newblock {\em {Dokl. Akad. Nauk. USSR}}, 269(3):{543--7}, 1983.

\bibitem{nesterov:05:smo}
Y.~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em {Mathematical Programming}}, 103(1):{127--52}, May 2005.

\bibitem{nesterov:07:dea}
Y.~Nesterov.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock {\em {Mathematical Programming}}, 109(2-3):{319--44}, March 2007.

\bibitem{ouyang:21:lcb}
Y.~Ouyang and Y.~Xu.
\newblock Lower complexity bounds of first-order methods for convex-concave
  bilinear saddle-point problems.
\newblock {\em {Mathematical Programming}}, 185({1}):{1--35}, 2021.

\bibitem{popov:80:amo}
L.~D. Popov.
\newblock A modification of the {Arrow-Hurwicz} method for search of saddle
  points.
\newblock {\em {Mathematical notes of the Academy of Sciences of the USSR}},
  28(5):{845--8}, November 1980.

\bibitem{ryu:19:oao}
E.~K. Ryu, K.~Yuan, and W.~Yin.
\newblock {ODE} analysis of stochastic gradient methods with optimism and
  anchoring for minimax problems and {GAN}s, 2019.
\newblock arxiv 1905.10899.

\bibitem{solodov:99:aha}
M.~V. Solodov and B.~F. Svaiter.
\newblock A hybrid approximate extragradient--proximal point algorithm using
  the enlargement of a maximal monotone operator.
\newblock {\em Set-Valued Analysis}, 7(4):323--345, 1999.

\bibitem{song:20:ode}
C.~Song, Z.~Zhou, Y.~Zhou, Y.~Jiang, and Y.~Ma.
\newblock Optimistic dual extrapolation for coherent non-monotone variational
  inequalities.
\newblock In {\em {Neural Info. Proc. Sys.}}, 2020.

\bibitem{yoon:21:aaf}
T.~Yoon and E.~K. Ryu.
\newblock Accelerated algorithms for smooth convex-concave minimax problems
  with $\mathcal{O}(1/k^2)$ rate on squared gradient norm.
\newblock In {\em {Proc. Intl. Conf. Mach. Learn}}, 2021.

\bibitem{zhou:17:smd}
Z.~Zhou, P.~Mertikopoulos, N.~Bambos, S.~Boyd, and P.~Glynn.
\newblock Stochastic mirror descent in variationally coherent optimization
  problems.
\newblock In {\em {Neural Info. Proc. Sys.}}, 2017.

\end{thebibliography}
