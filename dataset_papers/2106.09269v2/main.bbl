\begin{thebibliography}{10}

\bibitem{pytorch2021nninit}
torch.nn.init -- {PyTorch} 1.8.1 documentation.
\newblock \url{https://pytorch.org/docs/stable/nn.init.html}.
\newblock Accessed: 2021-10-21.

\bibitem{bellec2018deep}
Guillaume Bellec, David Kappel, Wolfgang Maass, and Robert Legenstein.
\newblock Deep rewiring: Training very sparse deep networks.
\newblock {\em International Conference on Learning Representations}, 2018.

\bibitem{diffenderfer2021multiprize}
James Diffenderfer and Bhavya Kailkhura.
\newblock Multi-prize lottery ticket hypothesis: Finding accurate binary neural
  networks by pruning a randomly weighted network.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning}, pages
  2943--2952. PMLR, 2020.

\bibitem{openlth}
Facebook.
\newblock facebookresearch/open\_lth.
\newblock \url{https://github.com/facebookresearch/open_lth}.
\newblock Accessed: 2021-10-21.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{han2017dsd}
Song Han, Jeff Pool, Sharan Narang, Huizi Mao, Enhao Gong, Shijian Tang, Erich
  Elsen, Peter Vajda, Manohar Paluri, John Tran, et~al.
\newblock Dsd: Dense-sparse-dense training for deep neural networks.
\newblock {\em International Conference on Learning Representations}, 2017.

\bibitem{han2015learning}
Song Han, Jeff Pool, John Tran, and William~J Dally.
\newblock Learning both weights and connections for efficient neural networks.
\newblock In {\em Proceedings of the 28th International Conference on Neural
  Information Processing Systems-Volume 1}, pages 1135--1143, 2015.

\bibitem{hanson1988comparing}
Stephen Hanson and Lorien Pratt.
\newblock Comparing biases for minimal network construction with
  back-propagation.
\newblock {\em Advances in Neural Information Processing Systems}, 1:177--185,
  1988.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em 2015 IEEE International Conference on Computer Vision
  (ICCV)}, pages 1026--1034. IEEE Computer Society, 2015.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em 2016 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, pages 770--778. IEEE Computer Society, 2016.

\bibitem{hoefler2021sparsity}
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock {\em arXiv preprint arXiv:2102.00554}, 2021.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  598--605, 1990.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{loshchilov2017sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock In {\em International Conference on Learning Representations}, 2017.

\bibitem{louizos2018learning}
Christos Louizos, Max Welling, and Diederik~P Kingma.
\newblock Learning sparse neural networks through l\_0 regularization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{maas2011learning}
Andrew~L. Maas, Raymond~E. Daly, Peter~T. Pham, Dan Huang, Andrew~Y. Ng, and
  Christopher Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In {\em Proceedings of the 49th Annual Meeting of the Association for
  Computational Linguistics: Human Language Technologies}, pages 142--150,
  Portland, Oregon, USA, June 2011. Association for Computational Linguistics.

\bibitem{malach2020proving}
Eran Malach, Gilad Yehudai, Shai Shalev-Schwartz, and Ohad Shamir.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock In {\em International Conference on Machine Learning}, pages
  6682--6691. PMLR, 2020.

\bibitem{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature communications}, 9(1):1--12, 2018.

\bibitem{molchanov2017variational}
Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov.
\newblock Variational dropout sparsifies deep neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  2498--2507. PMLR, 2017.

\bibitem{morcos2019one}
Ari Morcos, Haonan Yu, Michela Paganini, and Yuandong Tian.
\newblock One ticket to win them all: generalizing lottery ticket
  initializations across datasets and optimizers.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  4646--4655. PMLR, 2019.

\bibitem{orseau2020logarithmic}
Laurent Orseau, Marcus Hutter, and Omar Rivasplata.
\newblock Logarithmic pruning is all you need.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 2925--2934. Curran Associates, Inc., 2020.

\bibitem{pensia2020optimal}
Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, and Dimitris
  Papailiopoulos.
\newblock Optimal lottery tickets via subset sum: Logarithmic
  over-parameterization is sufficient.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 2599--2610. Curran Associates, Inc., 2020.

\bibitem{ramanujan2020what}
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and
  Mohammad Rastegari.
\newblock What's hidden in a randomly weighted neural network?
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11893--11902, 2020.

\bibitem{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536, 1986.

\bibitem{russakovsky2015imagenet}
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma,
  Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein,
  Alexander~C. Berg, and Li~Fei-Fei.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock {\em International Journal of Computer Vision (IJCV)},
  115(3):211--252, 2015.

\bibitem{tanaka2020pruning}
Hidenori Tanaka, Daniel Kunin, Daniel~L Yamins, and Surya Ganguli.
\newblock Pruning neural networks without any data by iteratively conserving
  synaptic flow.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, {\em Advances in Neural Information Processing Systems}, volume~33,
  pages 6377--6389. Curran Associates, Inc., 2020.

\bibitem{bentrevett2021pytorch}
Ben Trevett.
\newblock bentrevett/pytorch-sentiment-analysis.
\newblock \url{https://github.com/bentrevett/pytorch-sentiment-analysis}.
\newblock Accessed: 2021-10-21.

\bibitem{wang2020Picking}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wang2020pruning}
Yulong Wang, Xiaolu Zhang, Lingxi Xie, Jun Zhou, Hang Su, Bo~Zhang, and Xiaolin
  Hu.
\newblock Pruning from scratch.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 12273--12280, 2020.

\bibitem{zagoruyko2016wide}
Sergey Zagoruyko and Nikos Komodakis.
\newblock Wide residual networks.
\newblock In Edwin R.~Hancock Richard C.~Wilson and William A.~P. Smith,
  editors, {\em Proceedings of the British Machine Vision Conference (BMVC)},
  pages 87.1--87.12. BMVA Press, September 2016.

\bibitem{zhou2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\end{thebibliography}
