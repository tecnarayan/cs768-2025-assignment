\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Abbasi-Yadkori, Y., Bartlett, P., Bhatia, K., Lazic, N., Szepesvari, C., and
  Weisz, G.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3692--3702, 2019.

\bibitem[Abounadi et~al.(2001)Abounadi, Bertsekas, and
  Borkar]{abounadi2001learning}
Abounadi, J., Bertsekas, D., and Borkar, V.~S.
\newblock Learning algorithms for markov decision processes with average cost.
\newblock \emph{SIAM Journal on Control and Optimization}, 40\penalty0
  (3):\penalty0 681--698, 2001.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and
  Abbeel]{achiam2017constrained}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pp.\  22--31. JMLR. org, 2017.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2020optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock Optimality and approximation with policy gradient methods in markov
  decision processes.
\newblock In \emph{Conference on Learning Theory}, pp.\  64--66. PMLR, 2020.

\bibitem[Altman(1999)]{altman1999constrained}
Altman, E.
\newblock \emph{Constrained Markov decision processes}, volume~7.
\newblock CRC Press, 1999.

\bibitem[Amit et~al.(2020)Amit, Meir, and Ciosek]{amit2020discount}
Amit, R., Meir, R., and Ciosek, K.
\newblock Discount factor as a regularizer in reinforcement learning.
\newblock In \emph{International conference on machine learning}, 2020.

\bibitem[Amodei et~al.(2016)Amodei, Olah, Steinhardt, Christiano, Schulman, and
  Man{\'e}]{amodei2016concrete}
Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., and
  Man{\'e}, D.
\newblock Concrete problems in ai safety.
\newblock \emph{arXiv preprint arXiv:1606.06565}, 2016.

\bibitem[Andrychowicz et~al.(2020)Andrychowicz, Raichuk, Sta{\'n}czyk, Orsini,
  Girgin, Marinier, Hussenot, Geist, Pietquin, Michalski,
  et~al.]{andrychowicz2020matters}
Andrychowicz, M., Raichuk, A., Sta{\'n}czyk, P., Orsini, M., Girgin, S.,
  Marinier, R., Hussenot, L., Geist, M., Pietquin, O., Michalski, M., et~al.
\newblock What matters in on-policy reinforcement learning? a large-scale
  empirical study.
\newblock \emph{arXiv preprint arXiv:2006.05990}, 2020.

\bibitem[Baxter \& Bartlett(2001)Baxter and Bartlett]{baxter2001infinite}
Baxter, J. and Bartlett, P.~L.
\newblock Infinite-horizon policy-gradient estimation.
\newblock \emph{Journal of Artificial Intelligence Research}, 15:\penalty0
  319--350, 2001.

\bibitem[Bertsekas et~al.(1995)Bertsekas, Bertsekas, Bertsekas, and
  Bertsekas]{bertsekas1995dynamic}
Bertsekas, D.~P., Bertsekas, D.~P., Bertsekas, D.~P., and Bertsekas, D.~P.
\newblock \emph{Dynamic programming and optimal control}, volume 1,2.
\newblock Athena scientific Belmont, MA, 1995.

\bibitem[Blackwell(1962)]{blackwell1962discrete}
Blackwell, D.
\newblock Discrete dynamic programming.
\newblock \emph{The Annals of Mathematical Statistics}, pp.\  719--726, 1962.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Br{\'e}maud(2020)]{bremaud2020markov}
Br{\'e}maud, P.
\newblock \emph{Markov Chains Gibbs Fields, Monte Carlo Simulation and Queues}.
\newblock Springer, 2 edition, 2020.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym, 2016.

\bibitem[Cho \& Meyer(2001)Cho and Meyer]{cho2001comparison}
Cho, G.~E. and Meyer, C.~D.
\newblock Comparison of perturbation bounds for the stationary distribution of
  a markov chain.
\newblock \emph{Linear Algebra and its Applications}, 335\penalty0
  (1-3):\penalty0 137--150, 2001.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Grinstead \& Snell(2012)Grinstead and
  Snell]{grinstead2012introduction}
Grinstead, C.~M. and Snell, J.~L.
\newblock \emph{Introduction to probability}.
\newblock American Mathematical Soc., 2012.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Horn \& Johnson(2012)Horn and Johnson]{horn2012matrix}
Horn, R.~A. and Johnson, C.~R.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Howard(1960)]{howard1960dynamic}
Howard, R.~A.
\newblock \emph{Dynamic programming and markov processes.}
\newblock John Wiley, 1960.

\bibitem[Hunter(2005)]{hunter2005stationary}
Hunter, J.~J.
\newblock Stationary distributions and mean first passage times of perturbed
  markov chains.
\newblock \emph{Linear Algebra and its Applications}, 410:\penalty0 217--243,
  2005.

\bibitem[Jiang et~al.(2015)Jiang, Kulesza, Singh, and
  Lewis]{jiang2015dependence}
Jiang, N., Kulesza, A., Singh, S., and Lewis, R.
\newblock The dependence of effective planning horizon on model accuracy.
\newblock In \emph{Proceedings of the 2015 International Conference on
  Autonomous Agents and Multiagent Systems}, pp.\  1181--1189. Citeseer, 2015.

\bibitem[Jiang et~al.(2016)Jiang, Singh, and Tewari]{jiang2016structural}
Jiang, N., Singh, S.~P., and Tewari, A.
\newblock On structural properties of mdps that bound loss due to shallow
  planning.
\newblock In \emph{IJCAI}, pp.\  1640--1647, 2016.

\bibitem[Kakade(2001{\natexlab{a}})]{kakade2001optimizing}
Kakade, S.
\newblock Optimizing average reward using discounted rewards.
\newblock In \emph{International Conference on Computational Learning Theory},
  pp.\  605--615. Springer, 2001{\natexlab{a}}.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, volume~2,
  pp.\  267--274, 2002.

\bibitem[Kakade(2001{\natexlab{b}})]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14,
  2001{\natexlab{b}}.

\bibitem[Kallenberg(1983)]{kallenberg1983linear}
Kallenberg, L.
\newblock \emph{Linear Programming and Finite Markovian Control Problems}.
\newblock Centrum Voor Wiskunde en Informatica, 1983.

\bibitem[Kemeny \& Snell(1960)Kemeny and Snell]{kemeny1960finite}
Kemeny, J. and Snell, I.
\newblock \emph{Finite {M}arkov {C}hains}.
\newblock Van Nostrand, New Jersey, 1960.

\bibitem[Lehmann \& Casella(2006)Lehmann and Casella]{lehmann2006theory}
Lehmann, E.~L. and Casella, G.
\newblock \emph{Theory of point estimation}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Lehnert et~al.(2018)Lehnert, Laroche, and van
  Seijen]{lehnert2018value}
Lehnert, L., Laroche, R., and van Seijen, H.
\newblock On value function representation of long horizon problems.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Levin \& Peres(2017)Levin and Peres]{levin2017markov}
Levin, D.~A. and Peres, Y.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2016continuous}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[Mahadevan(1996)]{mahadevan1996average}
Mahadevan, S.
\newblock Average reward reinforcement learning: Foundations, algorithms, and
  empirical results.
\newblock \emph{Machine learning}, 22\penalty0 (1-3):\penalty0 159--195, 1996.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{NIPS Deep Learning Workshop}, 2013.

\bibitem[Naik et~al.(2019)Naik, Shariff, Yasui, and Sutton]{naik2019discounted}
Naik, A., Shariff, R., Yasui, N., and Sutton, R.~S.
\newblock Discounted reinforcement learning is not an optimization problem.
\newblock \emph{NeurIPS Optimization Foundations for Reinforcement Learning
  Workshop}, 2019.

\bibitem[Neu et~al.(2010)Neu, Antos, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{neu2010online}
Neu, G., Antos, A., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C.
\newblock Online markov decision processes under bandit feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1804--1812, 2010.

\bibitem[Peters \& Schaal(2008)Peters and Schaal]{peters2008reinforcement}
Peters, J. and Schaal, S.
\newblock Reinforcement learning of motor skills with policy gradients.
\newblock \emph{Neural networks}, 21\penalty0 (4):\penalty0 682--697, 2008.

\bibitem[Petrik \& Scherrer(2008)Petrik and Scherrer]{petrik2008biasing}
Petrik, M. and Scherrer, B.
\newblock Biasing approximate dynamic programming with a lower discount factor.
\newblock In \emph{Twenty-Second Annual Conference on Neural Information
  Processing Systems-NIPS 2008}, 2008.

\bibitem[Pirotta et~al.(2013)Pirotta, Restelli, Pecorino, and
  Calandriello]{pirotta2013safe}
Pirotta, M., Restelli, M., Pecorino, A., and Calandriello, D.
\newblock Safe policy iteration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  307--315, 2013.

\bibitem[Ross(1985)]{ross1985constrained}
Ross, K.~W.
\newblock Constrained markov decision processes with queueing applications.
\newblock \emph{Dissertation Abstracts International Part B: Science and
  Engineering[DISS. ABST. INT. PT. B- SCI. \& ENG.],}, 46\penalty0 (4), 1985.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1889--1897, 2015.

\bibitem[Schulman et~al.(2016)Schulman, Moritz, Levine, Jordan, and
  Abbeel]{schulman2016high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2016.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Schwartz(1993)]{schwartz1993reinforcement}
Schwartz, A.
\newblock A reinforcement learning method for maximizing undiscounted rewards.
\newblock In \emph{Proceedings of the tenth international conference on machine
  learning}, volume 298, pp.\  298--305, 1993.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
Silver, D., Huang, A., Maddison, C.~J., Guez, A., Sifre, L., Van Den~Driessche,
  G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M.,
  et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Silver et~al.(2018)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2018general}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock A general reinforcement learning algorithm that masters chess, shogi,
  and go through self-play.
\newblock \emph{Science}, 362\penalty0 (6419):\penalty0 1140--1144, 2018.

\bibitem[Song et~al.(2020)Song, Abdolmaleki, Springenberg, Clark, Soyer, Rae,
  Noury, Ahuja, Liu, Tirumala, et~al.]{song2020v}
Song, H.~F., Abdolmaleki, A., Springenberg, J.~T., Clark, A., Soyer, H., Rae,
  J.~W., Noury, S., Ahuja, A., Liu, S., Tirumala, D., et~al.
\newblock V-mpo: on-policy maximum a posteriori policy optimization for
  discrete and continuous control.
\newblock \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1057--1063, 2000.

\bibitem[Tadepalli \& Ok(1994)Tadepalli and Ok]{tadepalli1994h}
Tadepalli, P. and Ok, D.
\newblock H-learning: A reinforcement learning method to optimize undiscounted
  average reward.
\newblock Technical Report 94-30-01, Oregon State University, 1994.

\bibitem[Tessler et~al.(2019)Tessler, Mankowitz, and Mannor]{tessler2018reward}
Tessler, C., Mankowitz, D.~J., and Mannor, S.
\newblock Reward constrained policy optimization.
\newblock \emph{International Conference on Learning Representation (ICLR)},
  2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Tsybakov(2008)]{tsybakov2008introduction}
Tsybakov, A.~B.
\newblock \emph{Introduction to nonparametric estimation}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Veinott(1966)]{veinott1966finding}
Veinott, A.~F.
\newblock On finding optimal policies in discrete dynamic programming with no
  discounting.
\newblock \emph{The Annals of Mathematical Statistics}, 37\penalty0
  (5):\penalty0 1284--1294, 1966.

\bibitem[Vuong et~al.(2019)Vuong, Zhang, and Ross]{vuong2019supervised}
Vuong, Q., Zhang, Y., and Ross, K.~W.
\newblock Supervised policy update for deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representation (ICLR)},
  2019.

\bibitem[Wan et~al.(2020)Wan, Naik, and Sutton]{wan2020learning}
Wan, Y., Naik, A., and Sutton, R.~S.
\newblock Learning and planning in average-reward markov decision processes.
\newblock \emph{arXiv preprint arXiv:2006.16318}, 2020.

\bibitem[Wei et~al.(2020)Wei, Jafarnia-Jahromi, Luo, Sharma, and
  Jain]{wei2020model}
Wei, C.-Y., Jafarnia-Jahromi, M., Luo, H., Sharma, H., and Jain, R.
\newblock Model-free reinforcement learning in infinite-horizon average-reward
  markov decision processes.
\newblock In \emph{International conference on machine learning}, 2020.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Grosse, Liao, and Ba]{wu2017scalable}
Wu, Y., Mansimov, E., Grosse, R.~B., Liao, S., and Ba, J.
\newblock Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  pp.\  5285--5294, 2017.

\bibitem[Yang et~al.(2016)Yang, Gao, An, Wang, and Chen]{yang2016efficient}
Yang, S., Gao, Y., An, B., Wang, H., and Chen, X.
\newblock Efficient average reward reinforcement learning using constant
  shifting values.
\newblock In \emph{AAAI}, pp.\  2258--2264, 2016.

\bibitem[Yang et~al.(2020)Yang, Rosca, Narasimhan, and
  Ramadge]{yang2020projection}
Yang, T.-Y., Rosca, J., Narasimhan, K., and Ramadge, P.~J.
\newblock Projection-based constrained policy optimization.
\newblock In \emph{International Conference on Learning Representation (ICLR)},
  2020.

\bibitem[Zhang et~al.(2020)Zhang, Vuong, and Ross]{zhang2020first}
Zhang, Y., Vuong, Q., and Ross, K.
\newblock First order constrained optimization in policy space.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Zhao et~al.(2011)Zhao, Hachiya, Niu, and Sugiyama]{zhao2011analysis}
Zhao, T., Hachiya, H., Niu, G., and Sugiyama, M.
\newblock Analysis and improvement of policy gradient estimation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  262--270, 2011.

\end{thebibliography}
