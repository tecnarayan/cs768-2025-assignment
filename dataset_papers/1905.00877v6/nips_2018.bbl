\begin{thebibliography}{10}

\bibitem{askari2018lifted}
Armin Askari, Geoffrey Negiar, Rajiv Sambharya, and Laurent~El Ghaoui.
\newblock Lifted neural networks.
\newblock {\em arXiv preprint arXiv:1805.01532}, 2018.

\bibitem{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock {\em arXiv preprint arXiv:1802.00420}, 2018.

\bibitem{boltyanskii1960theory}
Vladimir~Grigor'evich Boltyanskii, Revaz~Valer'yanovich Gamkrelidze, and
  Lev~Semenovich Pontryagin.
\newblock The theory of optimal processes. i. the maximum principle.
\newblock Technical report, TRW SPACE TECHNOLOGY LABS LOS ANGELES CALIF, 1960.

\bibitem{carlini2017towards}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In {\em 2017 IEEE Symposium on Security and Privacy (SP)}, pages
  39--57. IEEE, 2017.

\bibitem{chen2018neural}
Tian~Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David~K Duvenaud.
\newblock Neural ordinary differential equations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6572--6583, 2018.

\bibitem{cisse2017parseval}
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas
  Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 854--863. JMLR. org, 2017.

\bibitem{evans2005introduction}
Lawrence~C Evans.
\newblock An introduction to mathematical optimal control theory.
\newblock {\em Lecture Notes, University of California, Department of
  Mathematics, Berkeley}, 2005.

\bibitem{goodfellow2016deep}
Ian Goodfellow, Yoshua Bengio, and Aaron Courville.
\newblock {\em Deep learning}.
\newblock MIT press, 2016.

\bibitem{gu2018fenchel}
Fangda Gu, Armin Askari, and Laurent~El Ghaoui.
\newblock Fenchel lifted networks: A lagrange relaxation of neural network
  training.
\newblock {\em arXiv preprint arXiv:1811.08039}, 2018.

\bibitem{haber2017stable}
Eldad Haber and Lars Ruthotto.
\newblock Stable architectures for deep neural networks.
\newblock {\em Inverse Problems}, 34(1):014004, 2017.

\bibitem{huo2018decoupled}
Zhouyuan Huo, Bin Gu, Qian Yang, and Heng Huang.
\newblock Decoupled parallel backpropagation with convergence guarantee.
\newblock {\em arXiv preprint arXiv:1804.10574}, 2018.

\bibitem{ilyas2017robust}
Andrew Ilyas, Ajil Jalal, Eirini Asteri, Constantinos Daskalakis, and
  Alexandros~G Dimakis.
\newblock The robust manifold defense: Adversarial training using generative
  models.
\newblock {\em arXiv preprint arXiv:1712.09196}, 2017.

\bibitem{jaderberg2017decoupled}
Max Jaderberg, Wojciech~Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex
  Graves, David Silver, and Koray Kavukcuoglu.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1627--1635. JMLR. org, 2017.

\bibitem{jakubovitz2018improving}
Daniel Jakubovitz and Raja Giryes.
\newblock Improving dnn robustness to adversarial attacks using jacobian
  regularization.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 514--529, 2018.

\bibitem{kurakin2016adversarial}
Alexey Kurakin, Ian Goodfellow, and Samy Bengio.
\newblock Adversarial machine learning at scale.
\newblock {\em arXiv preprint arXiv:1611.01236}, 2016.

\bibitem{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em nature}, 521(7553):436, 2015.

\bibitem{lecun1988theoretical}
Yann LeCun, D~Touresky, G~Hinton, and T~Sejnowski.
\newblock A theoretical framework for back-propagation.
\newblock In {\em Proceedings of the 1988 connectionist models summer school},
  volume~1, pages 21--28. CMU, Pittsburgh, Pa: Morgan Kaufmann, 1988.

\bibitem{li2018lifted}
Jia Li, Cong Fang, and Zhouchen Lin.
\newblock Lifted proximal operator machines.
\newblock {\em arXiv preprint arXiv:1811.01501}, 2018.

\bibitem{li2017maximum}
Qianxiao Li, Long Chen, Cheng Tai, and E~Weinan.
\newblock Maximum principle based algorithms for deep learning.
\newblock {\em The Journal of Machine Learning Research}, 18(1):5998--6026,
  2017.

\bibitem{pmlr-v80-li18b}
Qianxiao Li and Shuji Hao.
\newblock An optimal control approach to deep learning and applications to
  discrete-weight neural networks.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of {\em
  Proceedings of Machine Learning Research}, pages 2985--2994,
  Stockholmsm√§ssan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem{lin2018defensive}
Ji~Lin, Chuang Gan, and Song Han.
\newblock Defensive quantization: When efficiency meets robustness.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{lu2017beyond}
Yiping Lu, Aoxiao Zhong, Quanzheng Li, and Bin Dong.
\newblock Beyond finite layer neural networks: Bridging deep architectures and
  numerical differential equations.
\newblock {\em arXiv preprint arXiv:1710.10121}, 2017.

\bibitem{luo2019random}
Tiange Luo, Tianle Cai, Mengxiao Zhang, Siyu Chen, and Liwei Wang.
\newblock {RANDOM} {MASK}: Towards robust convolutional neural networks, 2019.

\bibitem{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{moosavi2016deepfool}
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard.
\newblock Deepfool: a simple and accurate method to fool deep neural networks.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 2574--2582, 2016.

\bibitem{pontryagin1987mathematical}
Lev~Semenovich Pontryagin.
\newblock {\em Mathematical theory of optimal processes}.
\newblock CRC, 1987.

\bibitem{qian2018l2}
Haifeng Qian and Mark~N Wegman.
\newblock L2-nonexpansive neural networks.
\newblock {\em arXiv preprint arXiv:1802.07896}, 2018.

\bibitem{shafahi2019adversarial}
Ali Shafahi, Mahyar Najibi, Amin Ghiasi, Xu~Zeng, John Dickerson, Christoph
  Studer, Larry S.~Davis, Gavin Taylor, and Tom Goldstein.
\newblock Adversarial training for free!
\newblock {\em arXiv preprint arXiv:1904.12843}, 2019.

\bibitem{song2017pixeldefend}
Yang Song, Taesup Kim, Sebastian Nowozin, Stefano Ermon, and Nate Kushman.
\newblock Pixeldefend: Leveraging generative models to understand and defend
  against adversarial examples.
\newblock {\em arXiv preprint arXiv:1710.10766}, 2017.

\bibitem{sonoda2019transport}
Sho Sonoda and Noboru Murata.
\newblock Transport analysis of infinitely deep neural network.
\newblock {\em The Journal of Machine Learning Research}, 20(1):31--82, 2019.

\bibitem{sun2019enhancing}
Ke~Sun, Zhanxing Zhu, and Zhouchen Lin.
\newblock Enhancing the robustness of deep neural networks by boundary
  conditional gan.
\newblock {\em arXiv preprint arXiv:1902.11029}, 2019.

\bibitem{svoboda2018peernets}
Jan Svoboda, Jonathan Masci, Federico Monti, Michael Bronstein, and Leonidas
  Guibas.
\newblock Peernets: Exploiting peer wisdom against adversarial attacks.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}, 2013.

\bibitem{taylor2016training}
Gavin Taylor, Ryan Burmeister, Zheng Xu, Bharat Singh, Ankit Patel, and Tom
  Goldstein.
\newblock Training neural networks without gradients: A scalable admm approach.
\newblock In {\em International conference on machine learning}, pages
  2722--2731, 2016.

\bibitem{thorpe2018deep}
Matthew Thorpe and Yves van Gennip.
\newblock Deep limits of residual neural networks.
\newblock {\em arXiv preprint arXiv:1810.11741}, 2018.

\bibitem{wald1939contributions}
Abraham Wald.
\newblock Contributions to the theory of statistical estimation and testing
  hypotheses.
\newblock {\em The Annals of Mathematical Statistics}, 10(4):299--326, 1939.

\bibitem{wang2018enresnet}
Bao Wang, Binjie Yuan, Zuoqiang Shi, and Stanley~J Osher.
\newblock Enresnet: Resnet ensemble via the feynman-kac formalism.
\newblock {\em arXiv preprint arXiv:1811.10745}, 2018.

\bibitem{weinan2017proposal}
E~Weinan.
\newblock A proposal on machine learning via dynamical systems.
\newblock {\em Communications in Mathematics and Statistics}, 5(1):1--11, 2017.

\bibitem{weinan2019mean}
E~Weinan, Jiequn Han, and Qianxiao Li.
\newblock A mean-field optimal control formulation of deep learning.
\newblock {\em Research in the Mathematical Sciences}, 6(1):10, 2019.

\bibitem{xie2018feature}
Cihang Xie, Yuxin Wu, Laurens van~der Maaten, Alan Yuille, and Kaiming He.
\newblock Feature denoising for improving adversarial robustness.
\newblock {\em arXiv preprint arXiv:1812.03411}, 2018.

\bibitem{xu2017feature}
Weilin Xu, David Evans, and Yanjun Qi.
\newblock Feature squeezing: Detecting adversarial examples in deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1704.01155}, 2017.

\bibitem{ye2018bayesian}
Nanyang Ye and Zhanxing Zhu.
\newblock Bayesian adversarial learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6892--6901, 2018.

\bibitem{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric~P Xing, Laurent~El Ghaoui, and
  Michael~I Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock {\em arXiv preprint arXiv:1901.08573}, 2019.

\bibitem{zhang2019towards}
Jingfeng Zhang, Bo~Han, Laura Wynter, Kian~Hsiang Low, and Mohan Kankanhalli.
\newblock Towards robust resnet: A small step but a giant leap.
\newblock {\em arXiv preprint arXiv:1902.10887}, 2019.

\bibitem{zhang2018dynamically}
Xiaoshuai Zhang, Yiping Lu, Jiaying Liu, and Bin Dong.
\newblock Dynamically unfolding recurrent restorer: A moving endpoint control
  method for image restoration.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{zugner2018adversarial}
Daniel Z{\"u}gner, Amir Akbarnejad, and Stephan G{\"u}nnemann.
\newblock Adversarial attacks on neural networks for graph data.
\newblock In {\em Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 2847--2856. ACM, 2018.

\end{thebibliography}
