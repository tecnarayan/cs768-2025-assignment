\begin{thebibliography}{10}

\bibitem{10.1016/S0022-0000(03)00025-4}
Dimitris Achlioptas.
\newblock Database-friendly random projections: Johnson-lindenstrauss with
  binary coins.
\newblock {\em J. Comput. Syst. Sci.}, 66(4):671â€“687, June 2003.

\bibitem{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}, 2018.

\bibitem{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  322--332, 2019.

\bibitem{bell15minc}
Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala.
\newblock Material recognition in the wild with the materials in context
  database.
\newblock {\em Computer Vision and Pattern Recognition (CVPR)}, 2015.

\bibitem{bingham2001random}
Ella Bingham and Heikki Mannila.
\newblock Random projection in dimensionality reduction: applications to image
  and text data.
\newblock In {\em Proceedings of the seventh ACM SIGKDD international
  conference on Knowledge discovery and data mining}, pages 245--250, 2001.

\bibitem{DBLP:journals/corr/abs-1710-10174}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev{-}Shwartz.
\newblock {SGD} learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock {\em CoRR}, abs/1710.10174, 2017.

\bibitem{DBLP:journals/corr/abs-1710-11029}
Pratik Chaudhari and Stefano Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock {\em CoRR}, abs/1710.11029, 2017.

\bibitem{chaudhari2018stochastic}
Pratik Chaudhari and Stefano Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{pmlr-v97-du19c}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  1675--1685, Long Beach, California, USA, 09--15 Jun 2019. PMLR.

\bibitem{du2018gradient_theory_1}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{goldblum2019truth}
Micah Goldblum, Jonas Geiping, Avi Schwarzschild, Michael Moeller, and Tom
  Goldstein.
\newblock Truth or backpropaganda? an empirical investigation of deep learning
  theory, 2019.

\bibitem{hayou2019mean}
Soufiane Hayou, Arnaud Doucet, and Judith Rousseau.
\newblock Mean-field behaviour of neural tangent kernel for deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1905.13654}, 2019.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem{DBLP:journals/corr/abs-1811-11880}
Daniel Justus, John Brennan, Stephen Bonner, and Andrew~Stephen McGough.
\newblock Predicting the computational cost of deep learning models.
\newblock {\em CoRR}, abs/1811.11880, 2018.

\bibitem{DBLP:journals/corr/KeskarMNST16}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em CoRR}, abs/1609.04836, 2016.

\bibitem{KrauseStarkDengFei-Fei_3DRR2013}
Jonathan Krause, Michael Stark, Jia Deng, and Li~Fei-Fei.
\newblock 3d object representations for fine-grained categorization.
\newblock In {\em 4th International IEEE Workshop on 3D Representation and
  Recognition (3dRR-13)}, Sydney, Australia, 2013.

\bibitem{krizhevsky:2009}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, Computer Science Department, University of Toronto,
  2009.

\bibitem{lee2017deep}
Jaehoon Lee, Yasaman Bahri, Roman Novak, Samuel~S Schoenholz, Jeffrey
  Pennington, and Jascha Sohl-Dickstein.
\newblock Deep neural networks as gaussian processes.
\newblock {\em arXiv preprint arXiv:1711.00165}, 2017.

\bibitem{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S. Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent, 2019.

\bibitem{li2020rethinking}
Hao Li, Pratik Chaudhari, Hao Yang, Michael Lam, Avinash Ravichandran, Rahul
  Bhotika, and Stefano Soatto.
\newblock Rethinking the hyperparameters for fine-tuning, 2020.

\bibitem{DBLP:journals/corr/LiY17c}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock {\em CoRR}, abs/1705.09886, 2017.

\bibitem{maji13fine-grained}
S.~Maji, J.~Kannala, E.~Rahtu, M.~Blaschko, and A.~Vedaldi.
\newblock Fine-grained visual classification of aircraft.
\newblock Technical report, 2013.

\bibitem{mu2020gradients}
Fangzhou Mu, Yingyu Liang, and Yin Li.
\newblock Gradients as features for deep representation learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{conf/cvpr/QuattoniT09}
Ariadna Quattoni and Antonio Torralba.
\newblock Recognizing indoor scenes.
\newblock In {\em CVPR}, pages 413--420. IEEE Computer Society, 2009.

\bibitem{saxe2013exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}, 2013.

\bibitem{shawe2005eigenspectrum}
J.~{Shawe-Taylor}, C.~K.~I. {Williams}, N.~{Cristianini}, and J.~{Kandola}.
\newblock On the eigenspectrum of the gram matrix and the generalization error
  of kernel-pca.
\newblock {\em IEEE Transactions on Information Theory}, 51(7):2510--2522,
  2005.

\bibitem{Smith2018ABP}
Samuel~L. Smith and Quoc~V. Le.
\newblock A bayesian perspective on generalization and stochastic gradient
  descent.
\newblock {\em ArXiv}, abs/1710.06451, 2018.

\bibitem{WelinderEtal2010}
P.~Welinder, S.~Branson, T.~Mita, C.~Wah, F.~Schroff, S.~Belongie, and
  P.~Perona.
\newblock {Caltech-UCSD Birds 200}.
\newblock Technical Report CNS-TR-2010-001, California Institute of Technology,
  2010.

\bibitem{DBLP:conf/iclr/ZhangBHRV17}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.

\bibitem{DBLP:journals/corr/abs-1811-08888}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock {\em CoRR}, abs/1811.08888, 2018.

\end{thebibliography}
