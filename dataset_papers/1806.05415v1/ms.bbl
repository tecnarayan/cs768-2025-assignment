\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2016)Abbasi-Yadkori, Bartlett, and
  Wright]{abbasi2016fast}
Abbasi-Yadkori, Y., Bartlett, P.~L., and Wright, S.~J.
\newblock A fast and reliable policy improvement algorithm.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1338--1346,
  2016.

\bibitem[Achiam et~al.(2017)Achiam, Held, Tamar, and Abbeel]{achiam2017cpo}
Achiam, J., Held, D., Tamar, A., and Abbeel, P.
\newblock Constrained policy optimization.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of \emph{ICML'17}, pp.\  22--31, 2017.

\bibitem[Bowerman(1974)]{bowerman1974nonstationary}
Bowerman, B.~L.
\newblock \emph{Nonstationary Markov decision processes and related topics in
  nonstationary Markov chains}.
\newblock PhD thesis, Iowa State University, 1974.

\bibitem[Bueno et~al.(2017)Bueno, Mau{\'a}, Barros, and
  Cozman]{bueno2017modeling}
Bueno, T.~P., Mau{\'a}, D.~D., Barros, L.~N., and Cozman, F.~G.
\newblock Modeling markov decision processes with imprecise probabilities using
  probabilistic logic programming.
\newblock In \emph{Proceedings of the Tenth International Symposium on
  Imprecise Probability: Theories and Applications}, pp.\  49--60, 2017.

\bibitem[Cheevaprawatdomrong et~al.(2007)Cheevaprawatdomrong, Schochetman,
  Smith, and Garcia]{cheevaprawatdomrong2007solution}
Cheevaprawatdomrong, T., Schochetman, I.~E., Smith, R.~L., and Garcia, A.
\newblock Solution and forecast horizons for infinite-horizon nonhomogeneous
  markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 32\penalty0 (1):\penalty0
  51--72, 2007.

\bibitem[Ciosek \& Whiteson(2017)Ciosek and Whiteson]{ciosek2017offer}
Ciosek, K.~A. and Whiteson, S.
\newblock Offer: Off-environment reinforcement learning.
\newblock In \emph{AAAI}, pp.\  1819--1825, 2017.

\bibitem[Delgado et~al.(2009)Delgado, de~Barros, Cozman, and
  Shirota]{delgado2009representing}
Delgado, K.~V., de~Barros, L.~N., Cozman, F.~G., and Shirota, R.
\newblock Representing and solving factored markov decision processes with
  imprecise probabilities.
\newblock \emph{Proceedings ISIPTA, Durham, United Kingdom}, pp.\  169--178,
  2009.

\bibitem[Deza \& Deza(2009)Deza and Deza]{deza2009encyclopedia}
Deza, M.~M. and Deza, E.
\newblock Encyclopedia of distances.
\newblock In \emph{Encyclopedia of Distances}, pp.\  1--583. Springer, 2009.

\bibitem[Florensa et~al.(2017)Florensa, Held, Wulfmeier, Zhang, and
  Abbeel]{florensa2017reversecurriculum}
Florensa, C., Held, D., Wulfmeier, M., Zhang, M., and Abbeel, P.
\newblock Reverse curriculum generation for reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pp.\  482--495, 2017.

\bibitem[Garcia \& Smith(2000)Garcia and Smith]{garcia2000solving}
Garcia, A. and Smith, R.~L.
\newblock Solving nonstationary infinite horizon dynamic optimization problems.
\newblock \emph{Journal of Mathematical Analysis and Applications},
  244\penalty0 (2):\penalty0 304--317, 2000.

\bibitem[Ghate \& Smith(2013)Ghate and Smith]{ghate2013linear}
Ghate, A. and Smith, R.~L.
\newblock A linear programming approach to nonstationary infinite-horizon
  markov decision processes.
\newblock \emph{Operations Research}, 61\penalty0 (2):\penalty0 413--425, 2013.

\bibitem[Ghavamzadeh et~al.(2016)Ghavamzadeh, Petrik, and
  Chow]{ghavamzadeh2016safe}
Ghavamzadeh, M., Petrik, M., and Chow, Y.
\newblock Safe policy improvement by minimizing robust baseline regret.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2298--2306, 2016.

\bibitem[Givan et~al.(1997)Givan, Leach, and Dean]{givan1997boundedmdp}
Givan, R., Leach, S., and Dean, T.
\newblock Bounded parameter markov decision processes.
\newblock In Steel, S. and Alami, R. (eds.), \emph{Recent Advances in AI
  Planning}, pp.\  234--246. Springer Berlin Heidelberg, 1997.

\bibitem[Haviv \& Van~der Heyden(1984)Haviv and Van~der
  Heyden]{haviv1984perturbation}
Haviv, M. and Van~der Heyden, L.
\newblock Perturbation bounds for the stationary probabilities of a finite
  markov chain.
\newblock \emph{Advances in Applied Probability}, 16\penalty0 (4):\penalty0
  804--818, 1984.

\bibitem[Holland \& Goldberg(1989)Holland and Goldberg]{holland1989genetic}
Holland, J. and Goldberg, D.
\newblock Genetic algorithms in search, optimization and machine learning.
\newblock \emph{Massachusetts: Addison-Wesley}, 1989.

\bibitem[Hopp et~al.(1987)Hopp, Bean, and Smith]{hopp1987new}
Hopp, W.~J., Bean, J.~C., and Smith, R.~L.
\newblock A new optimality criterion for nonhomogeneous markov decision
  processes.
\newblock \emph{Operations Research}, 35\penalty0 (6):\penalty0 875--883, 1987.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{Proceedings of the 19th International Conference on Machine
  Learning}, volume~2, pp.\  267--274, 2002.

\bibitem[Kakade et~al.(2003)]{kakade2003sample}
Kakade, S.~M. et~al.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock PhD thesis, University of London London, England, 2003.

\bibitem[Ni \& Liu(2008)Ni and Liu]{ni2008boundedpomdp}
Ni, Y. and Liu, Z.-Q.
\newblock Bounded-parameter partially observable markov decision processes.
\newblock In \emph{Proceedings of the Eighteenth International Conference on
  International Conference on Automated Planning and Scheduling}, pp.\
  240--247. AAAI Press, 2008.

\bibitem[Osogami(2015)]{osogami2015robustpomdp}
Osogami, T.
\newblock Robust partially observable markov decision process.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, volume~37 of \emph{ICML'15}, pp.\  106--115, 2015.

\bibitem[Papini et~al.(2017)Papini, Pirotta, and Restelli]{papini2017adaptive}
Papini, M., Pirotta, M., and Restelli, M.
\newblock Adaptive batch size for safe policy gradients.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  3594--3603. Curran Associates,
  Inc., 2017.

\bibitem[Peters et~al.(2010)Peters, M{\"u}lling, and Altun]{peters2010relative}
Peters, J., M{\"u}lling, K., and Altun, Y.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI}, pp.\  1607--1612. Atlanta, 2010.

\bibitem[Pirotta et~al.(2013{\natexlab{a}})Pirotta, Restelli, and
  Bascetta]{pirotta2013adaptive}
Pirotta, M., Restelli, M., and Bascetta, L.
\newblock Adaptive step-size for policy gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1394--1402, 2013{\natexlab{a}}.

\bibitem[Pirotta et~al.(2013{\natexlab{b}})Pirotta, Restelli, Pecorino, and
  Calandriello]{pirotta2013safe}
Pirotta, M., Restelli, M., Pecorino, A., and Calandriello, D.
\newblock Safe policy iteration.
\newblock In \emph{Proceedings of the 30th International Conference on
  International Conference on Machine Learning}, volume~28 of \emph{ICML'13},
  pp.\  307--315, 2013{\natexlab{b}}.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rafferty et~al.(2011)Rafferty, Brunskill, Griffiths, and
  Shafto]{rafferty2011faster}
Rafferty, A.~N., Brunskill, E., Griffiths, T.~L., and Shafto, P.
\newblock Faster teaching by pomdp planning.
\newblock In \emph{AIED}, pp.\  280--287. Springer, 2011.

\bibitem[Rubinstein(1999)]{rubinstein1999cross}
Rubinstein, R.
\newblock The cross-entropy method for combinatorial and continuous
  optimization.
\newblock \emph{Methodology and computing in applied probability}, 1\penalty0
  (2):\penalty0 127--190, 1999.

\bibitem[Satia \& Lave~Jr(1973)Satia and Lave~Jr]{satia1973markovian}
Satia, J.~K. and Lave~Jr, R.~E.
\newblock Markovian decision processes with uncertain transition probabilities.
\newblock \emph{Operations Research}, 21\penalty0 (3):\penalty0 728--740, 1973.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, ICML'15, pp.\  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  1057--1063, 2000.

\bibitem[White~III \& Eldeib(1994)White~III and Eldeib]{white1994markov}
White~III, C.~C. and Eldeib, H.~K.
\newblock Markov decision processes with imprecise transition probabilities.
\newblock \emph{Operations Research}, 42\penalty0 (4):\penalty0 739--749, 1994.

\end{thebibliography}
