\begin{thebibliography}{10}

\bibitem{HWconvex}
Radoslaw Adamczak.
\newblock {A note on the Hanson-Wright inequality for random vectors with
  dependencies}.
\newblock {\em Electronic Communications in Probability}, 20:1--13, 2015.

\bibitem{hammer}
Radoslaw Adamczak, Alexander~E Litvak, Alain Pajor, and Nicole
  Tomczak-Jaegermann.
\newblock Restricted isometry property of matrices with independent columns and
  neighborly polytopes by random sampling.
\newblock {\em Constructive Approximation}, 34(1):61--88, 2011.

\bibitem{adlam2019random}
Ben Adlam, Jake Levinson, and Jeffrey Pennington.
\newblock A random matrix perspective on mixtures of nonlinearities for deep
  learning, 2019.
\newblock \texttt{arXiv:1912.00827}.

\bibitem{adlam2020neural}
Ben Adlam and Jeffrey Pennington.
\newblock The neural tangent kernel in high dimensions: Triple descent and a
  multi-scale theory of generalization.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem{AllenZhuEtal2018}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{AroraEtal2019}
Sanjeev Arora, Simon~S. Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Rousong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem{Auer96}
P.~Auer, M.~Herbster, and M.~Warmuth.
\newblock Exponentially many local minima for single neurons.
\newblock In {\em Neural Information Processing Systems (NIPS)}, 1996.

\bibitem{avron2017random}
Haim Avron, Michael Kapralov, Cameron Musco, Christopher Musco, Ameya
  Velingker, and Amir Zandieh.
\newblock Random fourier features for kernel ridge regression: Approximation
  bounds and statistical guarantees.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{bartlett2019nearly}
Peter Bartlett, Nick Harvey, Christopher Liaw, and Abbas Mehrabian.
\newblock Nearly-tight vc-dimension and pseudodimension bounds for piecewise
  linear neural networks.
\newblock {\em Journal of Machine Learning Research (JMLR)}, 20(63):1--17,
  2019.

\bibitem{bartlett2021deep}
Peter~L Bartlett, Andrea Montanari, and Alexander Rakhlin.
\newblock Deep learning: a statistical viewpoint.
\newblock {\em Acta numerica}, 30:87--201, 2021.

\bibitem{Baum1988}
Eric~B. Baum.
\newblock On the capabilities of multilayer perceptrons.
\newblock {\em Journal of Complexity}, 4, 1988.

\bibitem{benigni2019eigenvalue}
Lucas Benigni and Sandrine P{\'e}ch{\'e}.
\newblock {Eigenvalue distribution of some nonlinear models of random
  matrices}.
\newblock {\em Electronic Journal of Probability}, 26:1 -- 37, 2021.

\bibitem{bubeck2020network}
S{\'e}bastien Bubeck, Ronen Eldan, Yin~Tat Lee, and Dan Mikulincer.
\newblock Network size and weights size for memorization with two-layers neural
  networks.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{buchanan2021deep}
Sam Buchanan, Dar Gilboa, and John Wright.
\newblock Deep networks and the multiple manifold problem.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{chen2020much}
Zixiang Chen, Yuan Cao, Difan Zou, and Quanquan Gu.
\newblock How much over-parameterization is sufficient to learn deep relu
  networks?
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2020.

\bibitem{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach.
\newblock On lazy training in differentiable programming.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, volume~32,
  2019.

\bibitem{cover1965geometrical}
Thomas~M. Cover.
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock {\em IEEE Transactions on Electronic Computers}, (3):326--334, 1965.

\bibitem{daniely2020memorizing}
Amit Daniely.
\newblock Memorizing gaussians with no over-parameterizaion via gradient decent
  on neural networks, 2020.
\newblock \texttt{arXiv:2003.12895}.

\bibitem{daniely2020neural}
Amit Daniely.
\newblock Neural networks learning and memorization with (almost) no
  over-parameterization.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{DuEtal2019}
Simon~S. Du, Jason~D. Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{DuEtal2018_ICLR}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{fan2020spectra}
Zhou Fan and Zhichao Wang.
\newblock Spectra of the conjugate kernel and neural tangent kernel for
  linear-width neural networks.
\newblock In {\em Neural information processing systems (NeurIPS)}, 2020.

\bibitem{ge2019mildly}
Rong Ge, Runzhe Wang, and Haoyu Zhao.
\newblock Mildly overparametrized neural nets can memorize training data
  efficiently, 2019.
\newblock \texttt{arXiv:1909.11837}.

\bibitem{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock When do neural networks outperform kernel methods?
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{ghorbani2021linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em The Annals of Statistics}, 49(2):1029--1054, 2021.

\bibitem{XavierBengio2010}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2010.

\bibitem{han2017deep}
Dongyoon Han, Jiwhan Kim, and Junmo Kim.
\newblock Deep pyramidal residual networks.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pages 5927--5935, 2017.

\bibitem{hastie2022surprises}
Trevor Hastie, Andrea Montanari, Saharon Rosset, and Ryan~J Tibshirani.
\newblock Surprises in high-dimensional ridgeless least squares interpolation.
\newblock {\em The Annals of Statistics}, 50(2):949--986, 2022.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015.

\bibitem{hu2020surprising}
Wei Hu, Lechao Xiao, Ben Adlam, and Jeffrey Pennington.
\newblock The surprising simplicity of the early-time learning dynamics of
  neural networks.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{huang2021fl}
Baihe Huang, Xiaoxiao Li, Zhao Song, and Xin Yang.
\newblock Fl-ntk: A neural tangent kernel-based framework for federated
  learning analysis.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{JacotEtc2018}
Arthur Jacot, Franck Gabriel, and Cl√©ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem{ji2019polylogarithmic}
Ziwei Ji and Matus Telgarsky.
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{kowalczyk1994counting}
Adam Kowalczyk.
\newblock Counting function theorem for multi-layer networks.
\newblock In {\em Neural Information Processing Systems (NIPS)}, 1994.

\bibitem{lecun2012efficient}
Yann LeCun, L{\'e}on Bottou, Genevieve Orr, and Klaus-Robert M{\"u}ller.
\newblock Efficient backprop.
\newblock In {\em Neural networks: Tricks of the trade}, pages 9--48. Springer,
  2012.

\bibitem{liao2018spectrum}
Zhenyu Liao and Romain Couillet.
\newblock On the spectrum of random features maps of high dimensional data.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{liao2020random}
Zhenyu Liao, Romain Couillet, and Michael~W. Mahoney.
\newblock {A random matrix analysis of random Fourier features: beyond the
  Gaussian kernel, a precise phase transition, and the corresponding double
  descent}.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{louart2018random}
Cosme Louart, Zhenyu Liao, and Romain Couillet.
\newblock A random matrix approach to neural networks.
\newblock {\em The Annals of Applied Probability}, 28(2):1190--1248, 2018.

\bibitem{mei2021generalization}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Generalization error of random feature and kernel methods:
  hypercontractivity and kernel matrix concentration.
\newblock {\em Applied and Computational Harmonic Analysis}, 2021.

\bibitem{mei2022generalization}
Song Mei and Andrea Montanari.
\newblock The generalization error of random features regression: Precise
  asymptotics and the double descent curve.
\newblock {\em Communications on Pure and Applied Mathematics}, 75(4):667--766,
  2022.

\bibitem{Andrea2020}
Andrea Montanari and Yiqiao Zhong.
\newblock The interpolation phase transition in neural networks: Memorization
  and generalization under lazy training, 2020.
\newblock \texttt{arXiv:2007.12826}.

\bibitem{nguyen2021proof}
Quynh Nguyen.
\newblock On the proof of global convergence of gradient descent for deep relu
  networks with linear widths.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{nguyen2021solutions}
Quynh Nguyen, Pierre Br{\'e}chet, and Marco Mondelli.
\newblock When are solutions connected in deep networks?
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{QuynhICML2017}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{QuynhICML2018}
Quynh Nguyen and Matthias Hein.
\newblock Optimization landscape and expressivity of deep {CNN}s.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{QuynhMarco2020}
Quynh Nguyen and Marco Mondelli.
\newblock Global convergence of deep networks with one wide layer followed by
  pyramidal topology.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem{tightbounds}
Quynh Nguyen, Marco Mondelli, and Guido Montufar.
\newblock Tight bounds on the smallest eigenvalue of the neural tangent kernel
  for deep {ReLU} networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{oymak2019overparameterized}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In {\em International Conference on Machine Learning (ICML)}, 2019.

\bibitem{OymakMahdi2019}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Toward moderate overparameterization: Global convergence guarantees
  for training shallow neural networks.
\newblock {\em IEEE Journal on Selected Areas in Information Theory},
  1(1):84--105, 2020.

\bibitem{peche2019note}
Sandrine P{\'e}ch{\'e}.
\newblock A note on the {Pennington-Worah} distribution.
\newblock {\em Electronic Communications in Probability}, 24:1--7, 2019.

\bibitem{pennington2017geometry}
Jeffrey Pennington and Yasaman Bahri.
\newblock Geometry of neural network loss surfaces via random matrix theory.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2017.

\bibitem{pennington2018emergence}
Jeffrey Pennington, Samuel Schoenholz, and Surya Ganguli.
\newblock The emergence of spectral universality in deep networks.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2018.

\bibitem{pennington2017nonlinear}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2017.

\bibitem{pennington2018spectrum}
Jeffrey Pennington and Pratik Worah.
\newblock The spectrum of the fisher information matrix of a
  single-hidden-layer neural network.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2018.

\bibitem{piccolo2021analysis}
Vanessa Piccolo and Dominik Schr{\"o}der.
\newblock Analysis of one-hidden-layer neural networks via the resolvent
  method.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{SafranShamir2018}
Itay Safran and Ohad Shamir.
\newblock Spurious local minima are common in two-layer {ReLU} neural networks.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2018.

\bibitem{sakurai1992nh}
Akito Sakurai.
\newblock nh-1 networks store no less n* h+ 1 examples, but sometimes no more.
\newblock In {\em IJCNN International Joint Conference on Neural Networks},
  1992.

\bibitem{seddik2020random}
Mohamed El~Amine Seddik, Cosme Louart, Mohamed Tamaazousti, and Romain
  Couillet.
\newblock Random matrix theory proves that deep learning representations of
  {GAN-}data behave as gaussian mixtures.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2020.

\bibitem{VGG}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2015.

\bibitem{theoreticalinsghts}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 65(2):742--769, 2018.

\bibitem{song2021subquadratic}
Chaehwan Song, Ali Ramezani-Kebrya, Thomas Pethick, Armin Eftekhari, and Volkan
  Cevher.
\newblock Subquadratic overparameterization for shallow neural networks.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2021.

\bibitem{SongYang2020}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound, 2020.
\newblock \texttt{arXiv:1906.03593}.

\bibitem{vershrandmat}
Roman Vershynin.
\newblock {\em Introduction to the non-asymptotic analysis of random matrices},
  page 210‚Äì268.
\newblock Cambridge University Press, 2012.

\bibitem{vershynin2018high}
Roman Vershynin.
\newblock {\em High-dimensional probability: An introduction with applications
  in data science}.
\newblock Cambridge university press, 2018.

\bibitem{vershynin2020memory}
Roman Vershynin.
\newblock Memory capacity of neural networks with threshold and rectified
  linear unit activations.
\newblock {\em SIAM Journal on Mathematics of Data Science}, 2(4):1004--1033,
  2020.

\bibitem{wang2021modular}
Jun-Kun Wang, Chi-Heng Lin, and Jacob~D Abernethy.
\newblock A modular analysis of provable acceleration via {P}olyak‚Äôs
  momentum: Training a wide {ReLU} network and a deep linear network.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{wang2021deformed}
Zhichao Wang and Yizhe Zhu.
\newblock Deformed semicircle law and concentration of nonlinear random
  matrices for ultra-wide neural networks, 2021.
\newblock \texttt{arXiv:2109.09304}.

\bibitem{wu2019global}
Xiaoxia Wu, Simon~S. Du, and Rachel Ward.
\newblock Global convergence of adaptive gradient methods for an
  over-parameterized neural network, 2019.
\newblock \texttt{arXiv:1902.07111}.

\bibitem{Yun2019}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small nonlinearities in activation functions create bad local minima
  in neural networks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{yun2019small}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Small {ReLU} networks are powerful memorizers: a tight analysis of
  memorization capacity.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2019.

\bibitem{Zhang2017}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires re-thinking generalization.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109(3):467--492, 2020.

\bibitem{ZouGu2019}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2019.

\end{thebibliography}
