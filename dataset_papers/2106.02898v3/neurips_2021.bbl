\begin{thebibliography}{10}

\bibitem{simplelinedrawing}
D.B.Walther, B.Chai, E.Caddigan, D.M.Beck, and L.Fei-Fei.
\newblock Simple line drawings suffice for functional mri decoding of natural
  scene categories.
\newblock In {\em PNAS}, 2011.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR}, pages 248--255. Ieee, 2009.

\bibitem{sgd-pruning}
Xiaohan Ding, Guiguang Ding, Yuchen Guo, and Jungong Han.
\newblock Centripetal {SGD} for pruning very deep convolutional networks with
  complicated structure.
\newblock In {\em {CVPR} 2019, Long Beach, CA, USA, June 16-20, 2019}, 2019.

\bibitem{multi_pruning}
Jinyang Guo, Wanli Ouyang, and Dong Xu.
\newblock Multi-dimensional pruning: {A} unified framework for model
  compression.
\newblock In {\em {CVPR} 2020}, 2020.

\bibitem{ghostnet}
Kai Han, Yunhe Wang, Qi~Tian, Jianyuan Guo, Chunjing Xu, and Chang Xu.
\newblock Ghostnet: More features from cheap operations.
\newblock In {\em CVPR}, 2020.

\bibitem{resnet}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{Distill}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem{mobilenet}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock {\em arXiv preprint arXiv:1704.04861}, 2017.

\bibitem{huang2018}
Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van~der Maaten, and
  Kilian~Q Weinberger.
\newblock Multi-scale dense networks for resource efficient image
  classification.
\newblock 2018.

\bibitem{huang2018data}
Zehao Huang and Naiyan Wang.
\newblock Data-driven sparse structure selection for deep neural networks.
\newblock In {\em ECCV}, pages 304--320, 2018.

\bibitem{bnn}
Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua
  Bengio.
\newblock Binarized neural networks.
\newblock In {\em NeurIPS}, pages 4107--4115, 2016.

\bibitem{bn}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In {\em CVPR}, pages 2704--2713, 2018.

\bibitem{gumbel_softmax}
Eric Jang, Shixiang Gu, and Ben Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock In {\em {ICLR} 2017}, 2017.

\bibitem{l1-pruning}
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans~Peter Graf.
\newblock Pruning filters for efficient convnets.
\newblock In {\em ICLR}, 2017.

\bibitem{PFP}
Lucas Liebenwein, Cenk Baykal, Harry Lang, Dan Feldman, and Daniela Rus.
\newblock Provable filter pruning for efficient neural networks.
\newblock In {\em {ICLR} 2020,}. OpenReview.net, 2020.

\bibitem{coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em ECCV}. Springer, 2014.

\bibitem{thinet}
Jian-Hao Luo, Jianxin Wu, and Weiyao Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In {\em ICCV}, pages 5058--5066, 2017.

\bibitem{asampling}
Chris~J. Maddison, Daniel Tarlow, and Tom Minka.
\newblock A* sampling.
\newblock In {\em Advances in Neural Information Processing Systems 27: Annual
  Conference on Neural Information Processing Systems 2014, December 8-13 2014,
  Montreal, Quebec, Canada}, pages 3086--3094, 2014.

\bibitem{mullapudi2018hydranets}
Ravi~Teja Mullapudi, William~R Mark, Noam Shazeer, and Kayvon Fatahalian.
\newblock Hydranets: Specialized dynamic architectures for efficient inference.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8080--8089, 2018.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em NeurIPS}, 2019.

\bibitem{fasterrcnn}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster {R-CNN}: Towards real-time object detection with region
  proposal networks.
\newblock In {\em NeurIPS}, 2015.

\bibitem{mobilev2}
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh
  Chen.
\newblock Mobilenetv2: Inverted residuals and linear bottlenecks.
\newblock In {\em CVPR}, pages 4510--4520, 2018.

\bibitem{efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em ICML}, 2019.

\bibitem{tang2021manifold}
Yehui Tang, Yunhe Wang, Yixing Xu, Yiping Deng, Chao Xu, Dacheng Tao, and Chang
  Xu.
\newblock Manifold regularized dynamic network pruning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5018--5028, 2021.

\bibitem{train_test_discrepancy}
Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv{\'e} J{\'e}gou.
\newblock Fixing the train-test resolution discrepancy.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2019.

\bibitem{uzkent2020learning}
Burak Uzkent and Stefano Ermon.
\newblock Learning when and where to zoom with deep reinforcement learning.
\newblock In {\em CVPR}, pages 12345--12354, 2020.

\bibitem{aig2018}
Andreas Veit and Serge Belongie.
\newblock Convolutional networks with adaptive inference graphs.
\newblock 2018.

\bibitem{verelst2020dynamic}
Thomas Verelst and Tinne Tuytelaars.
\newblock Dynamic convolutions: Exploiting spatial sparsity for faster
  inference.
\newblock In {\em CVPR}, pages 2320--2329, 2020.

\bibitem{wang2019elastic}
Huiyu Wang, Aniruddha Kembhavi, Ali Farhadi, Alan~L Yuille, and Mohammad
  Rastegari.
\newblock Elastic: Improving cnns with dynamic scaling policies.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 2258--2267, 2019.

\bibitem{2020Accelerate}
W.~Wang, M.~Chen, S.~Zhao, L.~Chen, J.~Hu, H.~Liu, D.~Cai, X.~He, and W.~Liu.
\newblock Accelerate cnns from three dimensions: A comprehensive pruning
  framework.
\newblock 2020.

\bibitem{wang2020resolution}
Yikai Wang, Fuchun Sun, Duo Li, and Anbang Yao.
\newblock Resolution switchable networks for runtime efficient image
  recognition.
\newblock In {\em European Conference on Computer Vision}, pages 533--549.
  Springer, 2020.

\bibitem{GFnet}
Yulin Wang, Kangchen Lv, Rui Huang, Shiji Song, Le~Yang, and Gao Huang.
\newblock Glance and focus: a dynamic approach to reducing spatial redundancy
  in image classification.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{versatile}
Yunhe Wang, Chang Xu, Chunjing XU, Chao Xu, and Dacheng Tao.
\newblock Learning versatile filters for efficient convolutional neural
  networks.
\newblock In {\em NeurIPS}, 2018.

\bibitem{blockdrop}
Zuxuan Wu, Tushar Nagarajan, Abhishek Kumar, Steven Rennie, Larry~S Davis,
  Kristen Grauman, and Rogerio Feris.
\newblock Blockdrop: Dynamic inference paths in residual networks.
\newblock In {\em CVPR}, 2018.

\bibitem{resnext}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em CVPR}, pages 1492--1500, 2017.

\bibitem{xu2021renas}
Yixing Xu, Yunhe Wang, Kai Han, Yehui Tang, Shangling Jui, Chunjing Xu, and
  Chang Xu.
\newblock Renas: Relativistic evaluation of neural architecture search.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 4411--4420, 2021.

\bibitem{xu2020kernel}
Yixing Xu, Chang Xu, Xinghao Chen, Wei Zhang, Chunjing Xu, and Yunhe Wang.
\newblock Kernel based progressive distillation for adder neural networks.
\newblock {\em arXiv preprint arXiv:2009.13044}, 2020.

\bibitem{yang2020resolution}
Le~Yang, Yizeng Han, Xi~Chen, Shiji Song, Jifeng Dai, and Gao Huang.
\newblock Resolution adaptive networks for efficient inference.
\newblock In {\em CVPR}, 2020.

\bibitem{yang2020distilling}
Yiding Yang, Jiayan Qiu, Mingli Song, Dacheng Tao, and Xinchao Wang.
\newblock Distilling knowledge from graph convolutional networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7074--7083, 2020.

\bibitem{zhang2020spconv}
Qiulin Zhang, Zhuqing Jiang, Qishuo Lu, Jia'nan Han, Zhengxin Zeng, Shanghua
  Gao, and Aidong Men.
\newblock Split to be slim: An overlooked redundancy in vanilla convolution.
\newblock In {\em {IJCAI-20}}, pages 3195--3201, 2020.

\bibitem{shufflenet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock {\em CVPR}, 2018.

\end{thebibliography}
