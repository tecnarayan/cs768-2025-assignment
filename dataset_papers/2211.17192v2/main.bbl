\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bapna et~al.(2020)Bapna, Arivazhagan, and
  Firat]{Controlling_Computation_versus_Quality_Sequence_Models}
Bapna, A., Arivazhagan, N., and Firat, O.
\newblock Controlling computation versus quality for neural sequence models.
\newblock \emph{ArXiv}, abs/2002.07106, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{gpt3}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, NIPS'20, Red Hook, NY, USA, 2020. Curran
  Associates Inc.
\newblock ISBN 9781713829546.

\bibitem[Burton(1985)]{speculative_computation}
Burton, F.~W.
\newblock Speculative computation, parallelism, and functional programming.
\newblock \emph{IEEE Transactions on Computers}, C-34\penalty0 (12):\penalty0
  1190--1193, 1985.
\newblock \doi{10.1109/TC.1985.6312218}.

\bibitem[Chelba et~al.(2013)Chelba, Mikolov, Schuster, Ge, Brants, Koehn, and
  Robinson]{lm1b}
Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P.~T., and
  Robinson, T.
\newblock One billion word benchmark for measuring progress in statistical
  language modeling.
\newblock In \emph{Interspeech}, 2013.

\bibitem[Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and
  Jumper]{chen2023accelerating}
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper,
  J.~M.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{ArXiv}, abs/2302.01318, 2023.

\bibitem[Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez,
  Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury,
  Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski,
  Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph,
  Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat,
  Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat,
  Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel]{palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K.,
  Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N.~M.,
  Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.~C., Pope, R., Bradbury, J.,
  Austin, J., Isard, M., Gur-Ari, G., Yin, P., Duke, T., Levskaya, A.,
  Ghemawat, S., Dev, S., Michalewski, H., Garc{\'i}a, X., Misra, V., Robinson,
  K., Fedus, L., Zhou, D., Ippolito, D., Luan, D., Lim, H., Zoph, B.,
  Spiridonov, A., Sepassi, R., Dohan, D., Agrawal, S., Omernick, M., Dai,
  A.~M., Pillai, T.~S., Pellat, M., Lewkowycz, A., Moreira, E., Child, R.,
  Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta, B., D{\'i}az, M., Firat, O.,
  Catasta, M., Wei, J., Meier-Hellstern, K.~S., Eck, D., Dean, J., Petrov, S.,
  and Fiedel, N.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{ArXiv}, abs/2204.02311, 2022.

\bibitem[Dehghani et~al.(2021)Dehghani, Arnab, Beyer, Vaswani, and
  Tay]{The_Efficiency_Misnomer}
Dehghani, M., Arnab, A., Beyer, L., Vaswani, A., and Tay, Y.
\newblock The efficiency misnomer.
\newblock \emph{ArXiv}, abs/2110.12894, 2021.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{ArXiv}, abs/1810.04805, 2019.

\bibitem[Elbayad et~al.(2019)Elbayad, Gu, Grave, and
  Auli]{depth_adaptive_transformer}
Elbayad, M., Gu, J., Grave, E., and Auli, M.
\newblock Depth-adaptive transformer.
\newblock \emph{ArXiv}, abs/1910.10073, 2019.

\bibitem[Han et~al.(2021)Han, Huang, Song, Yang, Wang, and
  Wang]{Dynamic_Neural_Networks_Survey}
Han, Y., Huang, G., Song, S., Yang, L., Wang, H., and Wang, Y.
\newblock Dynamic neural networks: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 44:\penalty0 7436--7456, 2021.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{gelu}
Hendrycks, D. and Gimpel, K.
\newblock Bridging nonlinearities and stochastic regularizers with gaussian
  error linear units.
\newblock \emph{ArXiv}, abs/1606.08415, 2016.

\bibitem[Hennessy \& Patterson(2012)Hennessy and
  Patterson]{HennessyPatterson12}
Hennessy, J.~L. and Patterson, D.~A.
\newblock \emph{Computer Architecture: A Quantitative Approach}.
\newblock Morgan Kaufmann, Amsterdam, 5 edition, 2012.
\newblock ISBN 978-0-12-383872-8.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and
  Dean]{Distilling_the_Knowledge_in_a_Neural_Network}
Hinton, G.~E., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{ArXiv}, abs/1503.02531, 2015.

\bibitem[Hubara et~al.(2016)Hubara, Courbariaux, Soudry, El-Yaniv, and
  Bengio]{Quantized_Neural_Networks}
Hubara, I., Courbariaux, M., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Quantized neural networks: Training neural networks with low
  precision weights and activations.
\newblock \emph{ArXiv}, abs/1609.07061, 2016.

\bibitem[Jaszczur et~al.(2021)Jaszczur, Chowdhery, Mohiuddin, Kaiser, Gajewski,
  Michalewski, and Kanerva]{Sparse_is_Enough_in_Scaling_Transformers}
Jaszczur, S., Chowdhery, A., Mohiuddin, A., Kaiser, L., Gajewski, W.,
  Michalewski, H., and Kanerva, J.
\newblock Sparse is enough in scaling transformers.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5485--5551, 2020.

\bibitem[Roberts et~al.(2022)Roberts, Chung, Levskaya, Mishra, Bradbury, Andor,
  Narang, Lester, Gaffney, Mohiuddin, Hawthorne, Lewkowycz, Salcianu, van Zee,
  Austin, Goodman, Soares, Hu, Tsvyashchenko, Chowdhery, Bastings, Bulian,
  Garc{\'i}a, Ni, Chen, Kenealy, Clark, Lee, Garrette, Lee-Thorp, Raffel,
  Shazeer, Ritter, Bosma, Passos, Maitin-Shepard, Fiedel, Omernick, Saeta,
  Sepassi, Spiridonov, Newlan, and Gesmundo]{t5x}
Roberts, A., Chung, H.~W., Levskaya, A., Mishra, G., Bradbury, J., Andor, D.,
  Narang, S., Lester, B., Gaffney, C., Mohiuddin, A., Hawthorne, C., Lewkowycz,
  A., Salcianu, A., van Zee, M., Austin, J., Goodman, S., Soares, L.~B., Hu,
  H., Tsvyashchenko, S., Chowdhery, A., Bastings, J., Bulian, J., Garc{\'i}a,
  X., Ni, J., Chen, A., Kenealy, K., Clark, J., Lee, S., Garrette, D.~H.,
  Lee-Thorp, J., Raffel, C., Shazeer, N.~M., Ritter, M., Bosma, M., Passos, A.,
  Maitin-Shepard, J.~B., Fiedel, N., Omernick, M., Saeta, B., Sepassi, R.,
  Spiridonov, A., Newlan, J., and Gesmundo, A.
\newblock Scaling up models and data with t5x and seqio.
\newblock \emph{ArXiv}, abs/2203.17189, 2022.

\bibitem[Scardapane et~al.(2020)Scardapane, Scarpiniti, Baccarelli, and
  Uncini]{early_exits}
Scardapane, S., Scarpiniti, M., Baccarelli, E., and Uncini, A.
\newblock Why should we add early exits to neural networks?
\newblock \emph{Cognitive Computation}, 12\penalty0 (5):\penalty0 954--966,
  2020.

\bibitem[Schuster et~al.(2021)Schuster, Fisch, Jaakkola, and
  Barzilay]{Confident_Adaptive_Transformers}
Schuster, T., Fisch, A., Jaakkola, T., and Barzilay, R.
\newblock Consistent accelerated inference via confident adaptive transformers.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, 2021.

\bibitem[Schwartz et~al.(2020)Schwartz, Stanovsky, Swayamdipta, Dodge, and
  Smith]{Matching_Model_and_Instance_Complexities}
Schwartz, R., Stanovsky, G., Swayamdipta, S., Dodge, J., and Smith, N.~A.
\newblock The right tool for the job: Matching model and instance complexities.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2020.

\bibitem[Shazeer(2019)]{multi-query-attn}
Shazeer, N.~M.
\newblock Fast transformer decoding: One write-head is all you need.
\newblock \emph{ArXiv}, abs/1911.02150, 2019.

\bibitem[So et~al.(2021)So, Ma'nke, Liu, Dai, Shazeer, and Le]{Primer}
So, D.~R., Ma'nke, W., Liu, H., Dai, Z., Shazeer, N.~M., and Le, Q.~V.
\newblock Primer: Searching for efficient transformers for language modeling.
\newblock \emph{ArXiv}, abs/2109.08668, 2021.

\bibitem[Stern et~al.(2018)Stern, Shazeer, and
  Uszkoreit]{Blockwise_Parallel_Decoding}
Stern, M., Shazeer, N., and Uszkoreit, J.
\newblock Blockwise parallel decoding for deep autoregressive models.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Sukhbaatar et~al.(2019)Sukhbaatar, Grave, Bojanowski, and
  Joulin]{Adaptive_Attention_Span_in_Transformers}
Sukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.
\newblock Adaptive attention span in transformers.
\newblock In \emph{Annual Meeting of the Association for Computational
  Linguistics}, 2019.

\bibitem[Sun et~al.(2021)Sun, Ge, Wei, and Wang]{sad}
Sun, X., Ge, T., Wei, F., and Wang, H.
\newblock Instantaneous grammatical error correction with shallow aggressive
  decoding.
\newblock \emph{ArXiv}, abs/2106.04970, 2021.

\bibitem[Thoppilan et~al.(2022)Thoppilan, Freitas, Hall, Shazeer, Kulshreshtha,
  Cheng, Jin, Bos, Baker, Du, Li, Lee, Zheng, Ghafouri, Menegali, Huang,
  Krikun, Lepikhin, Qin, Chen, Xu, Chen, Roberts, Bosma, Zhou, Chang, Krivokon,
  Rusch, Pickett, Meier-Hellstern, Morris, Doshi, Santos, Duke, S{\o}raker,
  Zevenbergen, Prabhakaran, D{\'i}az, Hutchinson, Olson, Molina, Hoffman-John,
  Lee, Aroyo, Rajakumar, Butryna, Lamm, Kuzmina, Fenton, Cohen, Bernstein,
  Kurzweil, Aguera-Arcas, Cui, Croak, hsin Chi, and Le]{lamda}
Thoppilan, R., Freitas, D.~D., Hall, J., Shazeer, N.~M., Kulshreshtha, A.,
  Cheng, H.-T., Jin, A., Bos, T., Baker, L., Du, Y., Li, Y., Lee, H., Zheng,
  H., Ghafouri, A., Menegali, M., Huang, Y., Krikun, M., Lepikhin, D., Qin, J.,
  Chen, D., Xu, Y., Chen, Z., Roberts, A., Bosma, M., Zhou, Y., Chang, C.-C.,
  Krivokon, I.~A., Rusch, W.~J., Pickett, M., Meier-Hellstern, K.~S., Morris,
  M.~R., Doshi, T., Santos, R.~D., Duke, T., S{\o}raker, J.~H., Zevenbergen,
  B., Prabhakaran, V., D{\'i}az, M., Hutchinson, B., Olson, K., Molina, A.,
  Hoffman-John, E., Lee, J., Aroyo, L., Rajakumar, R., Butryna, A., Lamm, M.,
  Kuzmina, V.~O., Fenton, J., Cohen, A., Bernstein, R., Kurzweil, R.,
  Aguera-Arcas, B., Cui, C., Croak, M., hsin Chi, E.~H., and Le, Q.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{ArXiv}, abs/2201.08239, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{transformer}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Yu et~al.(2022)Yu, Xu, Koh, Luong, Baid, Wang, Vasudevan, Ku, Yang,
  Ayan, Hutchinson, Han, Parekh, Li, Zhang, Baldridge, and Wu]{parti}
Yu, J., Xu, Y., Koh, J.~Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku,
  A., Yang, Y., Ayan, B.~K., Hutchinson, B.~C., Han, W., Parekh, Z., Li, X.,
  Zhang, H., Baldridge, J., and Wu, Y.
\newblock Scaling autoregressive models for content-rich text-to-image
  generation.
\newblock \emph{ArXiv}, abs/2206.10789, 2022.

\end{thebibliography}
