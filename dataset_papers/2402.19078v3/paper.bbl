\begin{thebibliography}{97}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu \& Hazan(2016)Allen-Zhu and Hazan]{allen2016optimal}
Allen-Zhu, Z. and Hazan, E.
\newblock Optimal black-box reductions between optimization objectives.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Amir \& Hasegawa(1989)Amir and Hasegawa]{amir1989nonlinear}
Amir, H.~M. and Hasegawa, T.
\newblock Nonlinear mixed-discrete structural optimization.
\newblock \emph{Journal of Structural Engineering}, 115\penalty0 (3):\penalty0 626--646, 1989.

\bibitem[Beck \& Teboulle(2012)Beck and Teboulle]{beck2012smoothing}
Beck, A. and Teboulle, M.
\newblock Smoothing and first order methods: A unified framework.
\newblock \emph{SIAM Journal on Optimization}, 22\penalty0 (2):\penalty0 557--580, 2012.

\bibitem[Bowman(1976)]{bowman1976relationship}
Bowman, V.~J.
\newblock On the relationship of the tchebycheff norm and the efficient frontier of multiple-criteria objectives.
\newblock In \emph{Multiple Criteria Decision Making}, pp.\  76--86. Springer, 1976.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex Optimization}.
\newblock Cambridge University Press, 2004.

\bibitem[Chen et~al.(2023)Chen, Fernando, Ying, and Chen]{chen2023three}
Chen, L., Fernando, H., Ying, Y., and Chen, T.
\newblock Three-way trade-off in multi-objective learning: Optimization, generalization and conflict-avoidance.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Chen \& Kwok(2022)Chen and Kwok]{chen2022multi}
Chen, W. and Kwok, J.
\newblock Multi-objective deep learning with adaptive reference vectors.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 35:\penalty0 32723--32735, 2022.

\bibitem[Chen(2012)]{chen2012smoothing}
Chen, X.
\newblock Smoothing methods for nonsmooth, nonconvex minimization.
\newblock \emph{Mathematical Programming}, 134:\penalty0 71--99, 2012.

\bibitem[Chen et~al.(2018)Chen, Badrinarayanan, Lee, and Rabinovich]{chen2018grad}
Chen, Z., Badrinarayanan, V., Lee, C.-Y., and Rabinovich, A.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning}, pp.\  794--803, 2018.

\bibitem[Chen et~al.(2020)Chen, Ngiam, Huang, Luong, Kretzschmar, Chai, and Anguelov]{chen2020just}
Chen, Z., Ngiam, J., Huang, Y., Luong, T., Kretzschmar, H., Chai, Y., and Anguelov, D.
\newblock Just pick a sign: Optimizing deep multitask models with gradient sign dropout.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 2039--2050, 2020.

\bibitem[Cheng \& Li(1999)Cheng and Li]{cheng1999generalized}
Cheng, F. and Li, X.
\newblock Generalized center method for multiobjective engineering optimization.
\newblock \emph{Engineering Optimization}, 31\penalty0 (5):\penalty0 641--661, 1999.

\bibitem[Chennupati et~al.(2019)Chennupati, Sistu, Yogamani, and A~Rawashdeh]{chennupati2019multinet++}
Chennupati, S., Sistu, G., Yogamani, S., and A~Rawashdeh, S.
\newblock Multinet++: Multi-stream feature aggregation and geometric loss strategy for multi-task learning.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)}, 2019.

\bibitem[Choo \& Atkins(1983)Choo and Atkins]{choo1983proper}
Choo, E.~U. and Atkins, D.
\newblock Proper efficiency in nonconvex multicriteria programming.
\newblock \emph{Mathematics of Operations Research}, 8\penalty0 (3):\penalty0 467--470, 1983.

\bibitem[Dai et~al.(2023)Dai, Fei, and Lu]{dai2023improvable}
Dai, Y., Fei, N., and Lu, Z.
\newblock Improvable gap balancing for multi-task learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  496--506. PMLR, 2023.

\bibitem[{Das} \& {Dennis}(1997){Das} and {Dennis}]{das1997a}
{Das}, I. and {Dennis}, J.
\newblock A closer look at drawbacks of minimizing weighted sums of objectives for pareto set generation in multicriteria optimization problems.
\newblock \emph{Structural Optimization}, 14\penalty0 (1):\penalty0 63--69, 1997.

\bibitem[Deb \& Srinivasan(2006)Deb and Srinivasan]{deb2006innovization}
Deb, K. and Srinivasan, A.
\newblock Innovization: Innovating design principles through optimization.
\newblock In \emph{Genetic and Evolutionary Computation Conference (GECCO)}, pp.\  1629--1636, 2006.

\bibitem[D{\'e}sid{\'e}ri(2012)]{desideri2012mutiple}
D{\'e}sid{\'e}ri, J.-A.
\newblock Mutiple-gradient descent algorithm for multiobjective optimization.
\newblock In \emph{European Congress on Computational Methods in Applied Sciences and Engineering (ECCOMAS 2012)}, 2012.

\bibitem[Dosovitskiy \& Djolonga(2019)Dosovitskiy and Djolonga]{dosovitskiy2019you}
Dosovitskiy, A. and Djolonga, J.
\newblock You only train once: Loss-conditional training of deep networks.
\newblock \emph{International Conference on Learning Representations (ICLR)}, 2019.

\bibitem[Dunlavy \& O’Leary(2005)Dunlavy and O’Leary]{dunlavy2005homotopy}
Dunlavy, D.~M. and O’Leary, D.~P.
\newblock Homotopy optimization methods for global optimization.
\newblock In \emph{Technical Report}, 2005.

\bibitem[Ehrgott(2005)]{ehrgott2005multicriteria}
Ehrgott, M.
\newblock \emph{Multicriteria Optimization}, volume 491.
\newblock Springer Science \& Business Media, 2005.

\bibitem[Fernando et~al.(2023)Fernando, Shen, Liu, Chaudhury, Murugesan, and Chen]{fernando2022mitigating}
Fernando, H.~D., Shen, H., Liu, M., Chaudhury, S., Murugesan, K., and Chen, T.
\newblock Mitigating gradient bias in multi-objective learning: A provably convergent approach.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2023.

\bibitem[Fliege \& Svaiter(2000)Fliege and Svaiter]{fliege2000steepest}
Fliege, J. and Svaiter, B.~F.
\newblock Steepest descent methods for multicriteria optimization.
\newblock \emph{Mathematical Methods of Operations Research}, 51\penalty0 (3):\penalty0 479--494, 2000.

\bibitem[Fliege \& Vaz(2016)Fliege and Vaz]{fliege2016method}
Fliege, J. and Vaz, A. I.~F.
\newblock A method for constrained multiobjective optimization based on sqp techniques.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (4):\penalty0 2091--2119, 2016.

\bibitem[Fliege et~al.(2019)Fliege, Vaz, and Vicente]{fliege2019complexity}
Fliege, J., Vaz, A. I.~F., and Vicente, L.~N.
\newblock Complexity of gradient descent for multiobjective optimization.
\newblock \emph{Optimization Methods and Software}, 34\penalty0 (5):\penalty0 949--959, 2019.

\bibitem[Geoffrion(1967)]{geoffrion1967}
Geoffrion, A.~M.
\newblock Solving bicriterion mathematical programs.
\newblock \emph{Operations Research}, 15\penalty0 (1):\penalty0 39–54, 1967.

\bibitem[Gidel et~al.(2017)Gidel, Jebara, and Lacoste-Julien]{gidel2017frank}
Gidel, G., Jebara, T., and Lacoste-Julien, S.
\newblock Frank-wolfe algorithms for saddle point problems.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  362--371. PMLR, 2017.

\bibitem[Goffin(1977)]{goffin1977convergence}
Goffin, J.-L.
\newblock On convergence rates of subgradient optimization methods.
\newblock \emph{Mathematical Programming}, 13:\penalty0 329--347, 1977.

\bibitem[Goh \& Yang(1998)Goh and Yang]{goh1998convexification}
Goh, C. and Yang, X.
\newblock Convexification of a noninferior frontier.
\newblock \emph{Journal of Optimization Theory and Applications}, 97:\penalty0 759--768, 1998.

\bibitem[Hazan et~al.(2016)Hazan, Levy, and Shalev-Shwartz]{hazan2016graduated}
Hazan, E., Levy, K.~Y., and Shalev-Shwartz, S.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[He et~al.(2022)He, Feng, Cheng, Ji, Guo, and Caverlee]{he2022metabalance}
He, Y., Feng, X., Cheng, C., Ji, G., Guo, Y., and Caverlee, J.
\newblock Metabalance: improving multi-task recommendations via adapting gradient magnitudes of auxiliary tasks.
\newblock In \emph{Proceedings of the ACM Web Conference 2022}, pp.\  2205--2215, 2022.

\bibitem[Hillermeier(2001)]{hillermeier2001generalized}
Hillermeier, C.
\newblock Generalized homotopy approach to multiobjective optimization.
\newblock \emph{Journal of Optimization Theory and Applications}, 110\penalty0 (3):\penalty0 557--583, 2001.

\bibitem[Hu et~al.(2023)Hu, Xian, Wu, Fan, Yin, and Zhao]{hu2023revisiting}
Hu, Y., Xian, R., Wu, Q., Fan, Q., Yin, L., and Zhao, H.
\newblock Revisiting scalarization in multi-task learning: A theoretical perspective.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 36, 2023.

\bibitem[Jain et~al.(2023)Jain, Raparthy, Hern{\'a}ndez-Garc{\'\i}a, Rector-Brooks, Bengio, Miret, and Bengio]{jain2023multi}
Jain, M., Raparthy, S.~C., Hern{\'a}ndez-Garc{\'\i}a, A., Rector-Brooks, J., Bengio, Y., Miret, S., and Bengio, E.
\newblock Multi-objective {GF}low{N}ets.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  14631--14653. PMLR, 2023.

\bibitem[Kendall et~al.(2018)Kendall, Gal, and Cipolla]{kendall2017multi}
Kendall, A., Gal, Y., and Cipolla, R.
\newblock Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition ({CVPR})}, 2018.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2015adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2015.

\bibitem[Kurin et~al.(2022)Kurin, De~Palma, Kostrikov, Whiteson, and Mudigonda]{kurin2022defense}
Kurin, V., De~Palma, A., Kostrikov, I., Whiteson, S., and Mudigonda, P.~K.
\newblock In defense of the unitary scalarization for deep multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 12169--12183, 2022.

\bibitem[Letcher et~al.(2019)Letcher, Balduzzi, Racaniere, Martens, Foerster, Tuyls, and Graepel]{letcher2019differentiable}
Letcher, A., Balduzzi, D., Racaniere, S., Martens, J., Foerster, J., Tuyls, K., and Graepel, T.
\newblock Differentiable game mechanics.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 20\penalty0 (84):\penalty0 1--40, 2019.

\bibitem[Li(1996)]{li1996convexification}
Li, D.
\newblock Convexification of a noninferior frontier.
\newblock \emph{Journal of Optimization Theory and Applications}, 88:\penalty0 177--196, 1996.

\bibitem[Lin \& Zhang(2023)Lin and Zhang]{lin2023libmtl}
Lin, B. and Zhang, Y.
\newblock Libmtl: A python library for deep multi-task learning.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (1-7):\penalty0 18, 2023.

\bibitem[Lin et~al.(2022{\natexlab{a}})Lin, Feiyang, Zhang, and Tsang]{lin2022reasonable}
Lin, B., Feiyang, Y., Zhang, Y., and Tsang, I.
\newblock Reasonable effectiveness of random weighting: A litmus test for multi-task learning.
\newblock \emph{Transactions on Machine Learning Research}, 2022{\natexlab{a}}.

\bibitem[Lin et~al.(2023{\natexlab{a}})Lin, Jiang, Ye, Zhang, Chen, Chen, Liu, and Kwok]{lin2023dualbalancing}
Lin, B., Jiang, W., Ye, F., Zhang, Y., Chen, P., Chen, Y.-C., Liu, S., and Kwok, J.~T.
\newblock Dual-balancing for multi-task learning.
\newblock \emph{arXiv preprint arXiv:2308.12029}, 2023{\natexlab{a}}.

\bibitem[Lin et~al.(2020{\natexlab{a}})Lin, Jin, and Jordan]{lin2020gradient}
Lin, T., Jin, C., and Jordan, M.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  6083--6093. PMLR, 2020{\natexlab{a}}.

\bibitem[Lin et~al.(2019)Lin, Zhen, Li, Zhang, and Kwong]{lin2019pareto}
Lin, X., Zhen, H.-L., Li, Z., Zhang, Q., and Kwong, S.
\newblock Pareto multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  12060--12070, 2019.

\bibitem[Lin et~al.(2020{\natexlab{b}})Lin, Yang, Zhang, and Kwong]{lin2020controllable}
Lin, X., Yang, Z., Zhang, Q., and Kwong, S.
\newblock Controllable pareto multi-task learning.
\newblock \emph{arXiv preprint arXiv:2010.06313}, 2020{\natexlab{b}}.

\bibitem[Lin et~al.(2022{\natexlab{b}})Lin, Yang, and Zhang]{lin2022pareto_combinatorial}
Lin, X., Yang, Z., and Zhang, Q.
\newblock Pareto set learning for neural multi-objective combinatorial optimization.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2022{\natexlab{b}}.

\bibitem[Lin et~al.(2022{\natexlab{c}})Lin, Yang, Zhang, and Zhang]{lin2022pareto_expensive}
Lin, X., Yang, Z., Zhang, X., and Zhang, Q.
\newblock Pareto set learning for expensive multiobjective optimization.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022{\natexlab{c}}.

\bibitem[Lin et~al.(2023{\natexlab{b}})Lin, Yang, Zhang, and Zhang]{lin2023continuation}
Lin, X., Yang, Z., Zhang, X., and Zhang, Q.
\newblock Continuation path learning for homotopy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  21288--21311, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Liu, Jin, Stone, and Liu]{liu2021conflict}
Liu, B., Liu, X., Jin, X., Stone, P., and Liu, Q.
\newblock Conflict-averse gradient descent for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 34:\penalty0 18878--18890, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Li, Kuang, Xue, Chen, Yang, Liao, and Zhang]{liu2021towards}
Liu, L., Li, Y., Kuang, Z., Xue, J., Chen, Y., Yang, W., Liao, Q., and Zhang, W.
\newblock Towards impartial multi-task learning.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021{\natexlab{b}}.

\bibitem[Liu \& Vicente(2021)Liu and Vicente]{liu2021stochastic}
Liu, S. and Vicente, L.~N.
\newblock The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning.
\newblock \emph{Annals of Operations Research}, pp.\  1--30, 2021.

\bibitem[{Liu} et~al.(2019){Liu}, {Johns}, and {Davison}]{liu2019end}
{Liu}, S., {Johns}, E., and {Davison}, A.~J.
\newblock End-to-end multi-task learning with attention.
\newblock In \emph{2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  1871--1880, 2019.

\bibitem[Liu et~al.(2022)Liu, James, Davison, and Johns]{liu2022auto}
Liu, S., James, S., Davison, A., and Johns, E.
\newblock Auto-lambda: Disentangling dynamic task relationships.
\newblock \emph{Transactions on Machine Learning Research}, 2022.

\bibitem[Ma et~al.(2020)Ma, Du, and Matusik]{ma2020efficient}
Ma, P., Du, T., and Matusik, W.
\newblock Efficient continuous pareto exploration in multi-task learning.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Mahapatra \& Rajan(2020)Mahapatra and Rajan]{mahapatramulti2020multi}
Mahapatra, D. and Rajan, V.
\newblock Multi-task learning with user preferences: Gradient descent with controlled ascent in pareto optimization.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2020.

\bibitem[Malkiel \& Wolf(2021)Malkiel and Wolf]{malkiel2021mtadam}
Malkiel, I. and Wolf, L.
\newblock Mtadam: Automatic balancing of multiple training loss terms.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing}, pp.\  10713--10729, 2021.

\bibitem[Maninis et~al.(2019)Maninis, Radosavovic, and Kokkinos]{maninis2019attentive}
Maninis, K.-K., Radosavovic, I., and Kokkinos, I.
\newblock Attentive single-tasking of multiple tasks.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  1851--1860, 2019.

\bibitem[Martinez et~al.(2020)Martinez, Bertran, and Sapiro]{martinez2020minimax}
Martinez, N., Bertran, M., and Sapiro, G.
\newblock Minimax pareto fairness: A multi objective perspective.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  6755--6764. PMLR, 2020.

\bibitem[Mertikopoulos et~al.(2019)Mertikopoulos, Lecouat, Zenati, Foo, Chandrasekhar, and Piliouras]{mertikopoulos2019optimistic}
Mertikopoulos, P., Lecouat, B., Zenati, H., Foo, C.-S., Chandrasekhar, V., and Piliouras, G.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, pp.\  1--23, 2019.

\bibitem[Miettinen(1999)]{miettinen1999nonlinear}
Miettinen, K.
\newblock \emph{Nonlinear Multiobjective Optimization}, volume~12.
\newblock Springer Science \& Business Media, 1999.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Ozdaglar, and Pattathil]{mokhtari2020unified}
Mokhtari, A., Ozdaglar, A., and Pattathil, S.
\newblock A unified analysis of extra-gradient and optimistic gradient methods for saddle point problems: Proximal point approach.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics (AISTATS)}, pp.\  1497--1507. PMLR, 2020.

\bibitem[Momma et~al.(2022)Momma, Dong, and Liu]{momma2022multi}
Momma, M., Dong, C., and Liu, J.
\newblock A multi-objective/multi-task learning framework induced by pareto stationarity.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  15895--15907. PMLR, 2022.

\bibitem[Monteiro \& Svaiter(2010)Monteiro and Svaiter]{monteiro2010complexity}
Monteiro, R.~D. and Svaiter, B.~F.
\newblock On the complexity of the hybrid proximal extragradient method for the iterates and the ergodic mean.
\newblock \emph{SIAM Journal on Optimization}, 20\penalty0 (6):\penalty0 2755--2787, 2010.

\bibitem[Navon et~al.(2021)Navon, Shamsian, Chechik, and Fetaya]{navon2020learning}
Navon, A., Shamsian, A., Chechik, G., and Fetaya, E.
\newblock Learning the pareto front with hypernetworks.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Navon et~al.(2022)Navon, Shamsian, Achituve, Maron, Kawaguchi, Chechik, and Fetaya]{navon2022multi}
Navon, A., Shamsian, A., Achituve, I., Maron, H., Kawaguchi, K., Chechik, G., and Fetaya, E.
\newblock Multi-task learning as a bargaining game.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\  16428--16446. PMLR, 2022.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Nemirovski, A.
\newblock Prox-method with rate of convergence o (1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0 229--251, 2004.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Nesterov, Y.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical Programming}, 103:\penalty0 127--152, 2005.

\bibitem[Nesterov(2007)]{nesterov2007dual}
Nesterov, Y.
\newblock Dual extrapolation and its applications to solving variational inequalities and related problems.
\newblock \emph{Mathematical Programming}, 109\penalty0 (2):\penalty0 319--344, 2007.

\bibitem[Nocedal \& Wright(1999)Nocedal and Wright]{nocedal1999numerical}
Nocedal, J. and Wright, S.~J.
\newblock \emph{Numerical Optimization}.
\newblock Springer, 1999.

\bibitem[Palaniappan \& Bach(2016)Palaniappan and Bach]{palaniappan2016stochastic}
Palaniappan, B. and Bach, F.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 29, 2016.

\bibitem[Parisi et~al.(2014)Parisi, Pirotta, Smacchia, Bascetta, and Restelli]{parisi2014policy}
Parisi, S., Pirotta, M., Smacchia, N., Bascetta, L., and Restelli, M.
\newblock Policy gradient approaches for multi-objective sequential deb.
\newblock In \emph{International Joint Conference on Neural Networks}, 2014.

\bibitem[Ramakrishnan et~al.(2014)Ramakrishnan, Dral, Rupp, and Von~Lilienfeld]{ramakrishnan2014quantum}
Ramakrishnan, R., Dral, P.~O., Rupp, M., and Von~Lilienfeld, O.~A.
\newblock Quantum chemistry structures and properties of 134 kilo molecules.
\newblock \emph{Scientific Data}, 1\penalty0 (1):\penalty0 1--7, 2014.

\bibitem[Ray \& Liew(2002)Ray and Liew]{ray2002swarm}
Ray, T. and Liew, K.
\newblock A swarm metaphor for multiobjective design optimization.
\newblock \emph{Engineering Optimization}, 34\penalty0 (2):\penalty0 141--153, 2002.

\bibitem[Royer et~al.(2023)Royer, Blankevoort, and Ehteshami~Bejnordi]{royer2023scalarization}
Royer, A., Blankevoort, T., and Ehteshami~Bejnordi, B.
\newblock Scalarization for multi-task and multi-domain learning at scale.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 36, 2023.

\bibitem[Ruchte \& Grabocka(2021)Ruchte and Grabocka]{ruchte2021scalable}
Ruchte, M. and Grabocka, J.
\newblock Scalable pareto front approximation for deep multi-objective learning.
\newblock In \emph{IEEE International Conference on Data Mining (ICDM)}, 2021.

\bibitem[Saenko et~al.(2010)Saenko, Kulis, Fritz, and Darrell]{saenko2010adapting}
Saenko, K., Kulis, B., Fritz, M., and Darrell, T.
\newblock Adapting visual category models to new domains.
\newblock In \emph{Computer Vision--ECCV 2010: 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part IV 11}, pp.\  213--226. Springer, 2010.

\bibitem[Sch{\"a}ffler et~al.(2002)Sch{\"a}ffler, Schultz, and Weinzierl]{schaffler2002stochastic}
Sch{\"a}ffler, S., Schultz, R., and Weinzierl, K.
\newblock Stochastic method for the solution of unconstrained vector optimization problems.
\newblock \emph{Journal of Optimization Theory and Applications}, 114:\penalty0 209--222, 2002.

\bibitem[Sener \& Koltun(2018)Sener and Koltun]{sener2018multi}
Sener, O. and Koltun, V.
\newblock Multi-task learning as multi-objective optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\  525--536, 2018.

\bibitem[Senushkin et~al.(2023)Senushkin, Patakin, Kuznetsov, and Konushin]{senushkin2023independent}
Senushkin, D., Patakin, N., Kuznetsov, A., and Konushin, A.
\newblock Independent component alignment for multi-task learning.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  20083--20093, 2023.

\bibitem[Silberman et~al.(2012)Silberman, Hoiem, Kohli, and Fergus]{silberman2012indoor}
Silberman, N., Hoiem, D., Kohli, P., and Fergus, R.
\newblock Indoor segmentation and support inference from rgbd images.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, pp.\  746--760. Springer, 2012.

\bibitem[Steuer \& Choo(1983)Steuer and Choo]{steuer1983interactive}
Steuer, R.~E. and Choo, E.-U.
\newblock An interactive weighted tchebycheff procedure for multiple objective programming.
\newblock \emph{Mathematical Programming}, 26\penalty0 (3):\penalty0 326--344, 1983.

\bibitem[Tanabe \& Ishibuchi(2020)Tanabe and Ishibuchi]{tanabe2020easy}
Tanabe, R. and Ishibuchi, H.
\newblock An easy-to-use real-world multi-objective optimization problem suite.
\newblock \emph{Applied Soft Computing}, 89:\penalty0 106078, 2020.

\bibitem[Vaidyanathan et~al.(2003)Vaidyanathan, Tucker, Papila, and Shyy]{vaidyanathan2003cfd}
Vaidyanathan, R., Tucker, K., Papila, N., and Shyy, W.
\newblock Cfd-based design optimization for single element rocket injector.
\newblock In \emph{41st Aerospace Sciences Meeting and Exhibit}, pp.\  296, 2003.

\bibitem[Vandenhende et~al.(2021)Vandenhende, Georgoulis, Van~Gansbeke, Proesmans, Dai, and Van~Gool]{vandenhende2021multi}
Vandenhende, S., Georgoulis, S., Van~Gansbeke, W., Proesmans, M., Dai, D., and Van~Gool, L.
\newblock Multi-task learning for dense prediction tasks: A survey.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)}, 44\penalty0 (7):\penalty0 3614--3633, 2021.

\bibitem[Wang \& Zhang(2023)Wang and Zhang]{wang2023stochastic}
Wang, R. and Zhang, C.
\newblock Stochastic smoothing accelerated gradient method for nonsmooth convex composite optimization.
\newblock \emph{arXiv preprint arXiv:2308.01252}, 2023.

\bibitem[Wang \& Tsvetkov(2021)Wang and Tsvetkov]{wang2021gradient}
Wang, Z. and Tsvetkov, Y.
\newblock Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Xiao et~al.(2023)Xiao, Ban, and Ji]{xiao2023direction}
Xiao, P., Ban, H., and Ji, K.
\newblock Direction-oriented multi-objective learning: Simple and provable stochastic algorithms.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Xie et~al.(2021)Xie, Shi, Zhou, Yang, Zhang, Yu, and Li]{xie2020mars}
Xie, Y., Shi, C., Zhou, H., Yang, Y., Zhang, W., Yu, Y., and Li, L.
\newblock Mars: Markov molecular sampling for multi-objective drug discovery.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2021.

\bibitem[Xin et~al.(2022)Xin, Ghorbani, Gilmer, Garg, and Firat]{xin2022current}
Xin, D., Ghorbani, B., Gilmer, J., Garg, A., and Firat, O.
\newblock Do current multi-task optimization methods in deep learning even help?
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 13597--13609, 2022.

\bibitem[Xu et~al.(2020)Xu, Tian, Ma, Rus, Sueda, and Matusik]{xu2020prediction}
Xu, J., Tian, Y., Ma, P., Rus, D., Sueda, S., and Matusik, W.
\newblock Prediction-guided multi-objective reinforcement learning for continuous robot control.
\newblock In \emph{International Conference on Machine Learning}, pp.\  10607--10616. PMLR, 2020.

\bibitem[Xu et~al.(2016)Xu, Yan, Lin, and Yang]{xu2016homotopy}
Xu, Y., Yan, Y., Lin, Q., and Yang, T.
\newblock Homotopy smoothing for non-smooth problems with lower complexity than $o(1/\epsilon)$.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 29, 2016.

\bibitem[Xu et~al.(2023)Xu, Zhang, Xu, and Lan]{xu2023unified}
Xu, Z., Zhang, H., Xu, Y., and Lan, G.
\newblock A unified single-loop alternating gradient projection algorithm for nonconvex--concave and convex--nonconcave minimax problems.
\newblock \emph{Mathematical Programming}, 201\penalty0 (1):\penalty0 635--706, 2023.

\bibitem[Yu et~al.(2020)Yu, Kumar, Gupta, Levine, Hausman, and Finn]{yu2020gradient}
Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn, C.
\newblock Gradient surgery for multi-task learning.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:\penalty0 5824--5836, 2020.

\bibitem[Zhang et~al.(2020)Zhang, Xiao, Sun, and Luo]{zhang2020single}
Zhang, J., Xiao, P., Sun, R., and Luo, Z.
\newblock A single-loop smoothed gradient descent-ascent algorithm for nonconvex-concave min-max problems.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 33:\penalty0 7377--7389, 2020.

\bibitem[Zhang \& Li(2007)Zhang and Li]{zhang2007moea}
Zhang, Q. and Li, H.
\newblock {MOEA/D}: A multiobjective evolutionary algorithm based on decomposition.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 11\penalty0 (6):\penalty0 712--731, 2007.

\bibitem[Zhang et~al.(2023)Zhang, Lin, Xue, Chen, and Zhang]{zhang2023hypervolume}
Zhang, X., Lin, X., Xue, B., Chen, Y., and Zhang, Q.
\newblock Hypervolume maximization: A geometric view of pareto set learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2023.

\bibitem[Zhou et~al.(2022)Zhou, Zhang, Jiang, Zhong, Gu, and Zhu]{zhou2022convergence}
Zhou, S., Zhang, W., Jiang, J., Zhong, W., Gu, J., and Zhu, W.
\newblock On the convergence of stochastic multi-objective gradient manipulation and beyond.
\newblock In \emph{Advances in Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Zitzler et~al.(2007)Zitzler, Brockhoff, and Thiele]{zitzler2007hypervolume}
Zitzler, E., Brockhoff, D., and Thiele, L.
\newblock The hypervolume indicator revisited: On the design of pareto-compliant indicators via weighted integration.
\newblock In \emph{International Conference on Evolutionary Multi-Criterion Optimization (EMO)}, 2007.

\end{thebibliography}
