\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Aky{\"u}rek, E., Schuurmans, D., Andreas, J., Ma, T., and Zhou, D.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\bibitem[Alain \& Bengio(2016)Alain and Bengio]{alain2016understanding}
Alain, G. and Bengio, Y.
\newblock Understanding intermediate layers using linear classifier probes.
\newblock \emph{arXiv preprint arXiv:1610.01644}, 2016.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds, M., et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Bai et~al.(2023)Bai, Chen, Wang, Xiong, and Mei]{bai2023transformers}
Bai, Y., Chen, F., Wang, H., Xiong, C., and Mei, S.
\newblock Transformers as statisticians: Provable in-context learning with in-context algorithm selection.
\newblock \emph{arXiv preprint arXiv:2306.04637}, 2023.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff, et~al.]{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q.~G., Bradley, H., O’Brien, K., Hallahan, E., Khan, M.~A., Purohit, S., Prashanth, U.~S., Raff, E., et~al.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2397--2430. PMLR, 2023.

\bibitem[Bietti et~al.(2023)Bietti, Cabannes, Bouchacourt, Jegou, and Bottou]{bietti2023birth}
Bietti, A., Cabannes, V., Bouchacourt, D., Jegou, H., and Bottou, L.
\newblock Birth of a transformer: A memory viewpoint.
\newblock \emph{arXiv preprint arXiv:2306.00802}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Brunner et~al.(2019)Brunner, Liu, Pascual, Richter, Ciaramita, and Wattenhofer]{brunner2019identifiability}
Brunner, G., Liu, Y., Pascual, D., Richter, O., Ciaramita, M., and Wattenhofer, R.
\newblock On identifiability in transformers.
\newblock \emph{arXiv preprint arXiv:1908.04211}, 2019.

\bibitem[Chan et~al.(2022{\natexlab{a}})Chan, Santoro, Lampinen, Wang, Singh, Richemond, McClelland, and Hill]{chan2022data}
Chan, S., Santoro, A., Lampinen, A., Wang, J., Singh, A., Richemond, P., McClelland, J., and Hill, F.
\newblock Data distributional properties drive emergent in-context learning in transformers.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 18878--18891, 2022{\natexlab{a}}.

\bibitem[Chan et~al.(2022{\natexlab{b}})Chan, Dasgupta, Kim, Kumaran, Lampinen, and Hill]{chan2022transformers}
Chan, S.~C., Dasgupta, I., Kim, J., Kumaran, D., Lampinen, A.~K., and Hill, F.
\newblock Transformers generalize differently from information stored in context vs in weights.
\newblock \emph{arXiv preprint arXiv:2210.05675}, 2022{\natexlab{b}}.

\bibitem[Dai et~al.(2023)Dai, Sun, Dong, Hao, Ma, Sui, and Wei]{dai2023can}
Dai, D., Sun, Y., Dong, L., Hao, Y., Ma, S., Sui, Z., and Wei, F.
\newblock Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers.
\newblock In \emph{ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models}, 2023.

\bibitem[Deora et~al.(2023)Deora, Ghaderi, Taheri, and Thrampoulidis]{deora2023optimization}
Deora, P., Ghaderi, R., Taheri, H., and Thrampoulidis, C.
\newblock On the optimization and generalization of multi-head attention.
\newblock \emph{arXiv preprint arXiv:2310.12680}, 2023.

\bibitem[Dong et~al.(2022)Dong, Li, Dai, Zheng, Wu, Chang, Sun, Xu, and Sui]{dong2022survey}
Dong, Q., Li, L., Dai, D., Zheng, C., Wu, Z., Chang, B., Sun, X., Xu, J., and Sui, Z.
\newblock A survey for in-context learning.
\newblock \emph{arXiv preprint arXiv:2301.00234}, 2022.

\bibitem[Edelman et~al.(2022)Edelman, Goel, Kakade, and Zhang]{edelman2022inductive}
Edelman, B.~L., Goel, S., Kakade, S., and Zhang, C.
\newblock Inductive biases and variable creation in self-attention mechanisms.
\newblock In \emph{International Conference on Machine Learning}, pp.\  5793--5831. PMLR, 2022.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang, He, Thite, Nabeshima, et~al.]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Garg et~al.(2022)Garg, Tsipras, Liang, and Valiant]{garg2022can}
Garg, S., Tsipras, D., Liang, P.~S., and Valiant, G.
\newblock What can transformers learn in-context? a case study of simple function classes.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 30583--30598, 2022.

\bibitem[Giannou et~al.(2023)Giannou, Rajput, Sohn, Lee, Lee, and Papailiopoulos]{giannou2023looped}
Giannou, A., Rajput, S., Sohn, J.-y., Lee, K., Lee, J.~D., and Papailiopoulos, D.
\newblock Looped transformers as programmable computers.
\newblock \emph{arXiv preprint arXiv:2301.13196}, 2023.

\bibitem[Huang et~al.(2023)Huang, Cheng, and Liang]{huang2023context}
Huang, Y., Cheng, Y., and Liang, Y.
\newblock In-context convergence of transformers.
\newblock \emph{arXiv preprint arXiv:2310.05249}, 2023.

\bibitem[Kim \& Mnih(2018)Kim and Mnih]{kim2018disentangling}
Kim, H. and Mnih, A.
\newblock Disentangling by factorising.
\newblock In \emph{International Conference on Machine Learning}, pp.\  2649--2658. PMLR, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kirsch et~al.(2022)Kirsch, Harrison, Sohl-Dickstein, and Metz]{kirsch2022general}
Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L.
\newblock General-purpose in-context learning by meta-learning transformers.
\newblock \emph{arXiv preprint arXiv:2212.04458}, 2022.

\bibitem[Li et~al.(2022)Li, Hopkins, Bau, Vi{\'e}gas, Pfister, and Wattenberg]{li2022emergent}
Li, K., Hopkins, A.~K., Bau, D., Vi{\'e}gas, F., Pfister, H., and Wattenberg, M.
\newblock Emergent world representations: Exploring a sequence model trained on a synthetic task.
\newblock \emph{arXiv preprint arXiv:2210.13382}, 2022.

\bibitem[Li et~al.(2023)Li, Ildiz, Papailiopoulos, and Oymak]{li2023transformers}
Li, Y., Ildiz, M.~E., Papailiopoulos, D., and Oymak, S.
\newblock Transformers as algorithms: Generalization and stability in in-context learning.
\newblock \emph{International Conference on Machine Learning}, 2023.

\bibitem[Liu et~al.(2022)Liu, Ash, Goel, Krishnamurthy, and Zhang]{liu2022transformers}
Liu, B., Ash, J.~T., Goel, S., Krishnamurthy, A., and Zhang, C.
\newblock Transformers learn shortcuts to automata.
\newblock \emph{arXiv preprint arXiv:2210.10749}, 2022.

\bibitem[Lu et~al.(2023)Lu, Bigoulaeva, Sachdeva, Madabushi, and Gurevych]{lu2023emergent}
Lu, S., Bigoulaeva, I., Sachdeva, R., Madabushi, H.~T., and Gurevych, I.
\newblock Are emergent abilities in large language models just in-context learning?
\newblock \emph{arXiv preprint arXiv:2309.01809}, 2023.

\bibitem[Michaud et~al.(2023)Michaud, Liu, Girit, and Tegmark]{michaud2023quantization}
Michaud, E.~J., Liu, Z., Girit, U., and Tegmark, M.
\newblock The quantization model of neural scaling.
\newblock \emph{arXiv preprint arXiv:2303.13506}, 2023.

\bibitem[Min et~al.(2021)Min, Lewis, Zettlemoyer, and Hajishirzi]{min2021metaicl}
Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H.
\newblock Metaicl: Learning to learn in context.
\newblock \emph{arXiv preprint arXiv:2110.15943}, 2021.

\bibitem[Min et~al.(2022)Min, Lyu, Holtzman, Artetxe, Lewis, Hajishirzi, and Zettlemoyer]{min2022rethinking}
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M., Hajishirzi, H., and Zettlemoyer, L.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?
\newblock \emph{arXiv preprint arXiv:2202.12837}, 2022.

\bibitem[Nanda et~al.(2023)Nanda, Chan, Lieberum, Smith, and Steinhardt]{nanda2023progress}
Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J.
\newblock Progress measures for grokking via mechanistic interpretability.
\newblock \emph{arXiv preprint arXiv:2301.05217}, 2023.

\bibitem[Olsson et~al.(2022)Olsson, Elhage, Nanda, Joseph, DasSarma, Henighan, Mann, Askell, Bai, Chen, et~al.]{olsson2022context}
Olsson, C., Elhage, N., Nanda, N., Joseph, N., DasSarma, N., Henighan, T., Mann, B., Askell, A., Bai, Y., Chen, A., et~al.
\newblock In-context learning and induction heads.
\newblock \emph{arXiv preprint arXiv:2209.11895}, 2022.

\bibitem[Power et~al.(2022)Power, Burda, Edwards, Babuschkin, and Misra]{power2022grokking}
Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V.
\newblock Grokking: Generalization beyond overfitting on small algorithmic datasets.
\newblock \emph{arXiv preprint arXiv:2201.02177}, 2022.

\bibitem[Press et~al.(2019)Press, Smith, and Levy]{press2019improving}
Press, O., Smith, N.~A., and Levy, O.
\newblock Improving transformer models by reordering their sublayers.
\newblock \emph{arXiv preprint arXiv:1911.03864}, 2019.

\bibitem[Reddy(2023)]{reddy2023mechanistic}
Reddy, G.
\newblock The mechanistic basis of data dependence and abrupt learning in an in-context classification task.
\newblock \emph{arXiv preprint arXiv:2312.03002}, 2023.

\bibitem[Richter \& Wattenhofer(2020)Richter and Wattenhofer]{richter2020normalized}
Richter, O. and Wattenhofer, R.
\newblock Normalized attention without probability cage.
\newblock \emph{arXiv preprint arXiv:2005.09561}, 2020.

\bibitem[Schouten et~al.(2022)Schouten, Bloem, and Vossen]{schouten2022probing}
Schouten, S., Bloem, P., and Vossen, P.
\newblock Probing the representations of named entities in transformer-based language models.
\newblock In \emph{Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP}, pp.\  384--393, 2022.

\bibitem[Shin et~al.(2022)Shin, Lee, Ahn, Kim, Kim, Kim, Cho, Lee, Park, Ha, et~al.]{shin2022effect}
Shin, S., Lee, S.-W., Ahn, H., Kim, S., Kim, H., Kim, B., Cho, K., Lee, G., Park, W., Ha, J.-W., et~al.
\newblock On the effect of pretraining corpora on in-context learning by a large-scale language model.
\newblock \emph{arXiv preprint arXiv:2204.13509}, 2022.

\bibitem[Singh et~al.(2023)Singh, Chan, Moskovitz, Grant, Saxe, and Hill]{singh2023transient}
Singh, A.~K., Chan, S.~C., Moskovitz, T., Grant, E., Saxe, A.~M., and Hill, F.
\newblock The transient nature of emergent in-context learning in transformers.
\newblock \emph{arXiv preprint arXiv:2311.08360}, 2023.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and Potts]{socher2013recursive}
Socher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and Potts, C.
\newblock Recursive deep models for semantic compositionality over a sentiment treebank.
\newblock In \emph{Proceedings of the 2013 conference on empirical methods in natural language processing}, pp.\  1631--1642, 2013.

\bibitem[Tian et~al.(2023)Tian, Wang, Chen, and Du]{tian2023scan}
Tian, Y., Wang, Y., Chen, B., and Du, S.
\newblock Scan and snap: Understanding training dynamics and token composition in 1-layer transformer.
\newblock \emph{arXiv preprint arXiv:2305.16380}, 2023.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Voita \& Titov(2020)Voita and Titov]{voita2020information}
Voita, E. and Titov, I.
\newblock Information-theoretic probing with minimum description length.
\newblock \emph{arXiv preprint arXiv:2003.12298}, 2020.

\bibitem[Voita et~al.(2019)Voita, Sennrich, and Titov]{voita2019bottom}
Voita, E., Sennrich, R., and Titov, I.
\newblock The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives.
\newblock \emph{arXiv preprint arXiv:1909.01380}, 2019.

\bibitem[von Oswald et~al.(2022)von Oswald, Niklasson, Randazzo, Sacramento, Mordvintsev, Zhmoginov, and Vladymyrov]{von2022transformers}
von Oswald, J., Niklasson, E., Randazzo, E., Sacramento, J., Mordvintsev, A., Zhmoginov, A., and Vladymyrov, M.
\newblock Transformers learn in-context by gradient descent.
\newblock \emph{arXiv preprint arXiv:2212.07677}, 2022.

\bibitem[Wei et~al.(2022)Wei, Tay, Bommasani, Raffel, Zoph, Borgeaud, Yogatama, Bosma, Zhou, Metzler, et~al.]{wei2022emergent}
Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., Yogatama, D., Bosma, M., Zhou, D., Metzler, D., et~al.
\newblock Emergent abilities of large language models.
\newblock \emph{arXiv preprint arXiv:2206.07682}, 2022.

\bibitem[Wei et~al.(2023)Wei, Wei, Tay, Tran, Webson, Lu, Chen, Liu, Huang, Zhou, et~al.]{wei2023larger}
Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., Chen, X., Liu, H., Huang, D., Zhou, D., et~al.
\newblock Larger language models do in-context learning differently.
\newblock \emph{arXiv preprint arXiv:2303.03846}, 2023.

\bibitem[Wiegreffe \& Pinter(2019)Wiegreffe and Pinter]{wiegreffe2019attention}
Wiegreffe, S. and Pinter, Y.
\newblock Attention is not not explanation.
\newblock \emph{arXiv preprint arXiv:1908.04626}, 2019.

\end{thebibliography}
