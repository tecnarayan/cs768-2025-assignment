\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2016)Abadi, Chu, Goodfellow, McMahan, Mironov, Talwar,
  and Zhang]{abadi2016deep}
Abadi, M., Chu, A., Goodfellow, I., McMahan, H.~B., Mironov, I., Talwar, K.,
  and Zhang, L.
\newblock Deep learning with differential privacy.
\newblock In \emph{Proceedings of the 2016 ACM SIGSAC conference on computer
  and communications security}, pp.\  308--318, 2016.

\bibitem[Andriushchenko \& Flammarion(2022)Andriushchenko and
  Flammarion]{andriushchenko2022towards}
Andriushchenko, M. and Flammarion, N.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  639--668. PMLR, 2022.

\bibitem[Arpit et~al.(2017)Arpit, Jastrzebski, Ballas, Krueger, Bengio, Kanwal,
  Maharaj, Fischer, Courville, Bengio, et~al.]{arpit2017closer}
Arpit, D., Jastrzebski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal, M.~S.,
  Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al.
\newblock A closer look at memorization in deep networks.
\newblock In \emph{International conference on machine learning}, pp.\
  233--242. PMLR, 2017.

\bibitem[Bae et~al.(2022)Bae, Ng, Lo, Ghassemi, and Grosse]{bae2022if}
Bae, J., Ng, N., Lo, A., Ghassemi, M., and Grosse, R.~B.
\newblock If influence functions are the answer, then what is the question?
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 17953--17967, 2022.

\bibitem[Basu et~al.(2021)Basu, Pope, and Feizi]{basu2021influence}
Basu, S., Pope, P., and Feizi, S.
\newblock Influence functions in deep learning are fragile.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=xHKVVHGDOEk}.

\bibitem[Bousquet \& Elisseeff(2002)Bousquet and
  Elisseeff]{bousquet2002stability}
Bousquet, O. and Elisseeff, A.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Brown et~al.(2021)Brown, Bun, Feldman, Smith, and
  Talwar]{brown2021memorization}
Brown, G., Bun, M., Feldman, V., Smith, A., and Talwar, K.
\newblock When is memorization of irrelevant training data necessary for
  high-accuracy learning?
\newblock In \emph{Proceedings of the 53rd annual ACM SIGACT symposium on
  theory of computing}, pp.\  123--132, 2021.

\bibitem[Carlini et~al.(2019)Carlini, Erlingsson, and
  Papernot]{carlini2019distribution}
Carlini, N., Erlingsson, U., and Papernot, N.
\newblock Distribution density, tails, and outliers in machine learning:
  Metrics and applications.
\newblock \emph{arXiv preprint arXiv:1910.13427}, 2019.

\bibitem[Carlini et~al.(2022)Carlini, Chien, Nasr, Song, Terzis, and
  Tramer]{carlini2022membership}
Carlini, N., Chien, S., Nasr, M., Song, S., Terzis, A., and Tramer, F.
\newblock Membership inference attacks from first principles.
\newblock In \emph{2022 IEEE Symposium on Security and Privacy (SP)}, pp.\
  1897--1914. IEEE, 2022.

\bibitem[Dwork et~al.(2006)Dwork, McSherry, Nissim, and
  Smith]{dwork2006calibrating}
Dwork, C., McSherry, F., Nissim, K., and Smith, A.
\newblock Calibrating noise to sensitivity in private data analysis.
\newblock In \emph{Theory of Cryptography: Third Theory of Cryptography
  Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3},
  pp.\  265--284. Springer, 2006.

\bibitem[Fawzi et~al.(2018)Fawzi, Moosavi-Dezfooli, Frossard, and
  Soatto]{fawzi2018empirical}
Fawzi, A., Moosavi-Dezfooli, S.-M., Frossard, P., and Soatto, S.
\newblock Empirical study of the topology and geometry of deep networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  3762--3770, 2018.

\bibitem[Feldman(2019)]{feldman2019does}
Feldman, V.
\newblock Does learning require memorization? a short tale about a long tail.
\newblock \emph{arXiv preprint arXiv:1906.05271}, 2019.

\bibitem[Feldman \& Vondrak(2019)Feldman and Vondrak]{feldman2019high}
Feldman, V. and Vondrak, J.
\newblock High probability generalization bounds for uniformly stable
  algorithms with nearly optimal rate.
\newblock In \emph{Conference on Learning Theory}, pp.\  1270--1279. PMLR,
  2019.

\bibitem[Feldman \& Zhang(2020)Feldman and Zhang]{feldman2020neural}
Feldman, V. and Zhang, C.
\newblock What neural networks memorize and why: Discovering the long tail via
  influence estimation.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2881--2891, 2020.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2021sharpnessaware}
Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=6Tm1mposlrM}.

\bibitem[Garg \& Roy(2023)Garg and Roy]{garg2023samples}
Garg, I. and Roy, K.
\newblock Samples with low loss curvature improve data efficiency.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  20290--20300, 2023.

\bibitem[Garg et~al.(2023)Garg, Ravikumar, and Roy]{garg2023memorization}
Garg, I., Ravikumar, D., and Roy, K.
\newblock Memorization through the lens of curvature of loss function around
  samples.
\newblock \emph{arXiv preprint arXiv:2307.05831}, 2023.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Hardt, M., Recht, B., and Singer, Y.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International conference on machine learning}, pp.\
  1225--1234. PMLR, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hutchinson(1989)]{hutchinson1989stochastic}
Hutchinson, M.~F.
\newblock A stochastic estimator of the trace of the influence matrix for
  laplacian smoothing splines.
\newblock \emph{Communications in Statistics-Simulation and Computation},
  18\penalty0 (3):\penalty0 1059--1076, 1989.

\bibitem[Jiang* et~al.(2020)Jiang*, Neyshabur*, Mobahi, Krishnan, and
  Bengio]{Jiang2020Fantastic}
Jiang*, Y., Neyshabur*, B., Mobahi, H., Krishnan, D., and Bengio, S.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJgIPJBFvH}.

\bibitem[Jiang et~al.(2020)Jiang, Zhang, Talwar, and
  Mozer]{jiang2020characterizing}
Jiang, Z., Zhang, C., Talwar, K., and Mozer, M.~C.
\newblock Characterizing structural regularities of labeled data in
  overparameterized models.
\newblock \emph{arXiv preprint arXiv:2002.03206}, 2020.

\bibitem[Kearns \& Ron(1997)Kearns and Ron]{kearns1997algorithmic}
Kearns, M. and Ron, D.
\newblock Algorithmic stability and sanity-check bounds for leave-one-out
  cross-validation.
\newblock In \emph{Proceedings of the tenth annual conference on Computational
  learning theory}, pp.\  152--162, 1997.

\bibitem[Keskar et~al.(2017)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2017on}
Keskar, N.~S., Mudigere, D., Nocedal, J., Smelyanskiy, M., and Tang, P. T.~P.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=H1oyRlYgg}.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International conference on machine learning}, pp.\
  1885--1894. PMLR, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021asam}
Kwon, J., Kim, J., Park, H., and Choi, I.~K.
\newblock Asam: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5905--5914. PMLR, 2021.

\bibitem[Liu et~al.(2020)Liu, Niles-Weed, Razavian, and
  Fernandez-Granda]{liu2020early}
Liu, S., Niles-Weed, J., Razavian, N., and Fernandez-Granda, C.
\newblock Early-learning regularization prevents memorization of noisy labels.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 20331--20342, 2020.

\bibitem[Lukasik et~al.(2023)Lukasik, Nagarajan, Rawat, Menon, and
  Kumar]{lukasik2023larger}
Lukasik, M., Nagarajan, V., Rawat, A.~S., Menon, A.~K., and Kumar, S.
\newblock What do larger image classifiers memorise?
\newblock \emph{arXiv preprint arXiv:2310.05337}, 2023.

\bibitem[Maini et~al.(2022)Maini, Garg, Lipton, and
  Kolter]{maini2022characterizing}
Maini, P., Garg, S., Lipton, Z., and Kolter, J.~Z.
\newblock Characterizing datapoints via second-split forgetting.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 30044--30057, 2022.

\bibitem[Moosavi-Dezfooli et~al.(2019)Moosavi-Dezfooli, Fawzi, Uesato, and
  Frossard]{moosavi2019robustness}
Moosavi-Dezfooli, S.-M., Fawzi, A., Uesato, J., and Frossard, P.
\newblock Robustness via curvature regularization, and vice versa.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9078--9086, 2019.

\bibitem[Nasr et~al.(2021)Nasr, Songi, Thakurta, Papernot, and
  Carlin]{nasr2021adversary}
Nasr, M., Songi, S., Thakurta, A., Papernot, N., and Carlin, N.
\newblock Adversary instantiation: Lower bounds for differentially private
  machine learning.
\newblock In \emph{2021 IEEE Symposium on security and privacy (SP)}, pp.\
  866--882. IEEE, 2021.

\bibitem[Nesterov \& Polyak(2006)Nesterov and Polyak]{nesterov2006cubic}
Nesterov, Y. and Polyak, B.~T.
\newblock Cubic regularization of newton method and its global performance.
\newblock \emph{Mathematical Programming}, 108\penalty0 (1):\penalty0 177--205,
  2006.

\bibitem[Pleiss et~al.(2020)Pleiss, Zhang, Elenberg, and
  Weinberger]{pleiss2020identifying}
Pleiss, G., Zhang, T., Elenberg, E., and Weinberger, K.~Q.
\newblock Identifying mislabeled data using the area under the margin ranking.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17044--17056, 2020.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Khosla, A., Bernstein, M., Berg, A.~C., and Fei-Fei, L.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Schioppa et~al.(2022)Schioppa, Zablotskaia, Vilar, and
  Sokolov]{schioppa2022scaling}
Schioppa, A., Zablotskaia, P., Vilar, D., and Sokolov, A.
\newblock Scaling up influence functions.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~36, pp.\  8179--8186, 2022.

\bibitem[Schioppa et~al.(2023)Schioppa, Filippova, Titov, and
  Zablotskaia]{schioppa2023theoretical}
Schioppa, A., Filippova, K., Titov, I., and Zablotskaia, P.
\newblock Theoretical and practical perspectives on what influence functions
  do.
\newblock \emph{arXiv preprint arXiv:2305.16971}, 2023.

\bibitem[Shokri et~al.(2017)Shokri, Stronati, Song, and
  Shmatikov]{shokri2017membership}
Shokri, R., Stronati, M., Song, C., and Shmatikov, V.
\newblock Membership inference attacks against machine learning models.
\newblock In \emph{2017 IEEE symposium on security and privacy (SP)}, pp.\
  3--18. IEEE, 2017.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy2015going}
Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D.,
  Vanhoucke, V., and Rabinovich, A.
\newblock Going deeper with convolutions.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  1--9, 2015.

\bibitem[Toneva et~al.(2019)Toneva, Sordoni, des Combes, Trischler, Bengio, and
  Gordon]{toneva2018an}
Toneva, M., Sordoni, A., des Combes, R.~T., Trischler, A., Bengio, Y., and
  Gordon, G.~J.
\newblock An empirical study of example forgetting during deep neural network
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=BJlxm30cKm}.

\bibitem[Wang et~al.(2016)Wang, Lei, and Fienberg]{wang2016learning}
Wang, Y.-X., Lei, J., and Fienberg, S.~E.
\newblock Learning with differential privacy: Stability, learnability and the
  sufficiency and necessity of erm principle.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 6353--6392, 2016.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020adversarial}
Wu, D., Xia, S.-T., and Wang, Y.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2958--2969, 2020.

\bibitem[Yousefpour et~al.(2021)Yousefpour, Shilov, Sablayrolles, Testuggine,
  Prasad, Malek, Nguyen, Ghosh, Bharadwaj, Zhao, Cormode, and Mironov]{opacus}
Yousefpour, A., Shilov, I., Sablayrolles, A., Testuggine, D., Prasad, K.,
  Malek, M., Nguyen, J., Ghosh, S., Bharadwaj, A., Zhao, J., Cormode, G., and
  Mironov, I.
\newblock Opacus: {U}ser-friendly differential privacy library in {PyTorch}.
\newblock \emph{arXiv preprint arXiv:2109.12298}, 2021.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.
\newblock URL \url{https://openreview.net/forum?id=Sy8gdB9xx}.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\end{thebibliography}
