\begin{thebibliography}{81}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Farhadi and Tabrizi(2008)]{farhadi2008learning}
Ali Farhadi and Mostafa~Kamali Tabrizi.
\newblock Learning to recognize activities from the wrong view point.
\newblock In \emph{European conference on computer vision}, pages 154--166.
  Springer, 2008.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Saenko et~al.(2010)Saenko, Kulis, Fritz, and
  Darrell]{saenko2010adapting}
Kate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell.
\newblock Adapting visual category models to new domains.
\newblock In \emph{European conference on computer vision}, pages 213--226.
  Springer, 2010.

\bibitem[Ben-David et~al.(2007)Ben-David, Blitzer, Crammer, and
  Pereira]{bendavid2006theory}
Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira.
\newblock Analysis of representations for domain adaptation.
\newblock In B.~Sch\"{o}lkopf, J.~Platt, and T.~Hoffman, editors,
  \emph{Advances in Neural Information Processing Systems}, volume~19. MIT
  Press, 2007.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{bendavid2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79:\penalty0 151--175, 2010.

\bibitem[Ganin and Lempitsky(2015)]{ganin2015dann}
Yaroslav Ganin and Victor~S. Lempitsky.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In Francis~R. Bach and David~M. Blei, editors, \emph{ICML}, volume~37
  of \emph{JMLR Workshop and Conference Proceedings}, pages 1180--1189.
  JMLR.org, 2015.

\bibitem[Long et~al.(2017)Long, Zhu, Wang, and Jordan]{long2017jan}
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael~I. Jordan.
\newblock Deep transfer learning with joint adaptation networks.
\newblock In Doina Precup and Yee~Whye Teh, editors, \emph{Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of
  \emph{Proceedings of Machine Learning Research}, pages 2208--2217. PMLR,
  06--11 Aug 2017.

\bibitem[Long et~al.(2015)Long, Cao, Wang, and Jordan]{long2015dan}
Mingsheng Long, Yue Cao, Jianmin Wang, and Michael~I. Jordan.
\newblock Learning transferable features with deep adaptation networks.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning, {ICML} 2015, Lille, France, 6-11 July 2015}, pages 97--105, 2015.

\bibitem[Courty et~al.(2017)Courty, Flamary, Habrard, and
  Rakotomamonjy]{courty2017jdot}
Nicolas Courty, R\'{e}mi Flamary, Amaury Habrard, and Alain Rakotomamonjy.
\newblock Joint distribution optimal transportation for domain adaptation.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.

\bibitem[Damodaran et~al.(2018)Damodaran, Kellenberger, Flamary, Tuia, and
  Courty]{damodaran2018deepjdot}
Bharath~Bhushan Damodaran, Benjamin Kellenberger, R{\'{e}}mi Flamary, Devis
  Tuia, and Nicolas Courty.
\newblock {DeepJDOT}: Deep joint distribution optimal transport for
  unsupervised domain adaptation.
\newblock \emph{CoRR}, abs/1803.10081, 2018.

\bibitem[Lee et~al.(2019)Lee, Batra, Baig, and Ulbricht]{lee2019sliced}
Chen-Yu Lee, Tanmay Batra, Mohammad~Haris Baig, and Daniel Ulbricht.
\newblock Sliced {W}asserstein discrepancy for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 10285--10295, 2019.

\bibitem[Xu et~al.(2020)Xu, Liu, Wang, Chen, and Wang]{Xu_2020_CVPR}
Renjun Xu, Pelen Liu, Liyan Wang, Chao Chen, and Jindong Wang.
\newblock Reliable weighted optimal transport for unsupervised domain
  adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2020.

\bibitem[Gretton et~al.(2012)Gretton, Borgwardt, Rasch, Sch{\"o}lkopf, and
  Smola]{gretton2012kernel}
Arthur Gretton, Karsten~M Borgwardt, Malte~J Rasch, Bernhard Sch{\"o}lkopf, and
  Alexander Smola.
\newblock A kernel two-sample test.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 723--773, 2012.

\bibitem[Kantorovich(2006)]{kantorovich2006translocation}
Leonid~V Kantorovich.
\newblock On the translocation of masses.
\newblock \emph{Journal of Mathematical Sciences}, 133\penalty0 (4):\penalty0
  1381--1382, 2006.

\bibitem[Peyré and Cuturi(2019)]{COTFNT}
Gabriel Peyré and Marco Cuturi.
\newblock Computational optimal transport.
\newblock \emph{Foundations and Trends in Machine Learning}, 11\penalty0
  (5-6):\penalty0 355--607, 2019.

\bibitem[Lerasle et~al.(2019)Lerasle, Szab{\'o}, Mathieu, and
  Lecu{\'e}]{lerasle2019monk}
Matthieu Lerasle, Zolt{\'a}n Szab{\'o}, Timoth{\'e}e Mathieu, and Guillaume
  Lecu{\'e}.
\newblock Monk outlier-robust mean embedding estimation by median-of-means.
\newblock In \emph{International Conference on Machine Learning}, pages
  3782--3793. PMLR, 2019.

\bibitem[Balaji et~al.(2020)Balaji, Chellappa, and Feizi]{balaji2020robust}
Yogesh Balaji, Rama Chellappa, and Soheil Feizi.
\newblock Robust optimal transport with applications in generative modeling and
  domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Tachet~des Combes et~al.(2020)Tachet~des Combes, Zhao, Wang, and
  Gordon]{tachet2020labelshift}
Remi Tachet~des Combes, Han Zhao, Yu-Xiang Wang, and Geoff Gordon.
\newblock Domain adaptation with conditional distribution matching and
  generalized label shift.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Johansson et~al.(2019)Johansson, Sontag, and
  Ranganath]{pmlr-v89-johansson19a}
Fredrik~D. Johansson, David Sontag, and Rajesh Ranganath.
\newblock Support and invertibility in domain-invariant representations.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors,
  \emph{Proceedings of Machine Learning Research}, volume~89 of
  \emph{Proceedings of Machine Learning Research}, pages 527--536, 16--18 Apr
  2019.

\bibitem[Grandvalet et~al.(2005)Grandvalet, Bengio, et~al.]{grandvalet2005semi}
Yves Grandvalet, Yoshua Bengio, et~al.
\newblock Semi-supervised learning by entropy minimization.
\newblock In \emph{CAP}, pages 281--296, 2005.

\bibitem[Zhang et~al.(2013)Zhang, Sch{\"o}lkopf, Muandet, and
  Wang]{zhang2013domain}
Kun Zhang, Bernhard Sch{\"o}lkopf, Krikamol Muandet, and Zhikun Wang.
\newblock Domain adaptation under target and conditional shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  819--827. PMLR, 2013.

\bibitem[Sch{\"o}lkopf et~al.(2012)Sch{\"o}lkopf, Janzing, Peters, Sgouritsa,
  Zhang, and Mooij]{scholkopf2012causal}
Bernhard Sch{\"o}lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun
  Zhang, and Joris Mooij.
\newblock On causal and anticausal learning.
\newblock \emph{arXiv preprint arXiv:1206.6471}, 2012.

\bibitem[Gong et~al.(2016)Gong, Zhang, Liu, Tao, Glymour, and
  Sch{\"o}lkopf]{gong2016domain}
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and
  Bernhard Sch{\"o}lkopf.
\newblock Domain adaptation with conditional transferable components.
\newblock In \emph{International conference on machine learning}, pages
  2839--2848. PMLR, 2016.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{lipton2018detecting}
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{International conference on machine learning}, pages
  3122--3130. PMLR, 2018.

\bibitem[Azizzadenesheli et~al.(2019)Azizzadenesheli, Liu, Yang, and
  Anandkumar]{azizzadenesheli2019regularized}
Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar.
\newblock Regularized learning for domain adaptation under label shifts.
\newblock \emph{arXiv preprint arXiv:1903.09734}, 2019.

\bibitem[Long et~al.(2018)Long, Cao, Wang, and Jordan]{long2018cdan}
Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael~I Jordan.
\newblock Conditional adversarial domain adaptation.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.

\bibitem[Zheng and Zhou(2020)]{zheng2020act}
Huangjie Zheng and Mingyuan Zhou.
\newblock Comparing probability distributions with conditional transport.
\newblock \emph{arXiv preprint arXiv:2012.14100}, 2020.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pages 4080--4090, 2017.

\bibitem[Pan et~al.(2019)Pan, Yao, Li, Wang, Ngo, and
  Mei]{pan2019transferrable}
Yingwei Pan, Ting Yao, Yehao Li, Yu~Wang, Chong-Wah Ngo, and Tao Mei.
\newblock Transferrable prototypical networks for unsupervised domain
  adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, 2019.

\bibitem[Kang et~al.(2019)Kang, Jiang, Yang, and
  Hauptmann]{kang2019contrastive}
Guoliang Kang, Lu~Jiang, Yi~Yang, and Alexander~G Hauptmann.
\newblock Contrastive adaptation network for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4893--4902, 2019.

\bibitem[Liang et~al.(2020)Liang, Hu, and Feng]{liang2020we}
Jian Liang, Dapeng Hu, and Jiashi Feng.
\newblock Do we really need to access the source data? source hypothesis
  transfer for unsupervised domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pages
  6028--6039. PMLR, 2020.

\bibitem[Yue et~al.(2021)Yue, Zheng, Zhang, Gao, Darrell, Keutzer, and
  Vincentelli]{yue2021prototypical}
Xiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao, Trevor Darrell, Kurt
  Keutzer, and Alberto~Sangiovanni Vincentelli.
\newblock Prototypical cross-domain self-supervised learning for few-shot
  unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13834--13844, 2021.

\bibitem[Saito et~al.(2019)Saito, Kim, Sclaroff, Darrell, and
  Saenko]{saito2019semi}
Kuniaki Saito, Donghyun Kim, Stan Sclaroff, Trevor Darrell, and Kate Saenko.
\newblock Semi-supervised domain adaptation via minimax entropy.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8050--8058, 2019.

\bibitem[Miyato et~al.(2018)Miyato, Maeda, Koyama, and
  Ishii]{miyato2018virtual}
Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii.
\newblock Virtual adversarial training: a regularization method for supervised
  and semi-supervised learning.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 41\penalty0 (8):\penalty0 1979--1993, 2018.

\bibitem[Laine and Aila(2016)]{laine2016temporal}
Samuli Laine and Timo Aila.
\newblock Temporal ensembling for semi-supervised learning.
\newblock \emph{arXiv preprint arXiv:1610.02242}, 2016.

\bibitem[Shi and Sha(2012)]{shi2012information}
Yuan Shi and Fei Sha.
\newblock Information-theoretical learning of discriminative clusters for
  unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:1206.6438}, 2012.

\bibitem[Shu et~al.(2018)Shu, Bui, Narui, and Ermon]{shu2018dirt}
Rui Shu, Hung~H Bui, Hirokazu Narui, and Stefano Ermon.
\newblock A dirt-t approach to unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:1802.08735}, 2018.

\bibitem[French et~al.(2017)French, Mackiewicz, and Fisher]{french2017self}
Geoffrey French, Michal Mackiewicz, and Mark Fisher.
\newblock Self-ensembling for visual domain adaptation.
\newblock \emph{arXiv preprint arXiv:1706.05208}, 2017.

\bibitem[Vu et~al.(2019)Vu, Jain, Bucher, Cord, and P{\'e}rez]{vu2019advent}
Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick
  P{\'e}rez.
\newblock Advent: Adversarial entropy minimization for domain adaptation in
  semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 2517--2526, 2019.

\bibitem[Saito et~al.(2020)Saito, Kim, Sclaroff, and
  Saenko]{saito2020universal}
Kuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko.
\newblock Universal domain adaptation through self supervision.
\newblock \emph{arXiv preprint arXiv:2002.07953}, 2020.

\bibitem[Morerio et~al.(2017)Morerio, Cavazza, and Murino]{morerio2017minimal}
Pietro Morerio, Jacopo Cavazza, and Vittorio Murino.
\newblock Minimal-entropy correlation alignment for unsupervised deep domain
  adaptation.
\newblock \emph{arXiv preprint arXiv:1711.10288}, 2017.

\bibitem[Wu et~al.(2020)Wu, Zhou, Yang, Zhao, Latecki, et~al.]{wu2020entropy}
Xiaofu Wu, Quan Zhou, Zhen Yang, Chunming Zhao, Longin~Jan Latecki, et~al.
\newblock Entropy minimization vs. diversity maximization for domain
  adaptation.
\newblock \emph{arXiv preprint arXiv:2002.01690}, 2020.

\bibitem[Saerens et~al.(2002)Saerens, Latinne, and
  Decaestecker]{saerens2002adjusting}
Marco Saerens, Patrice Latinne, and Christine Decaestecker.
\newblock Adjusting the outputs of a classifier to new a priori probabilities:
  a simple procedure.
\newblock \emph{Neural computation}, 14\penalty0 (1):\penalty0 21--41, 2002.

\bibitem[Kang et~al.(2018)Kang, Zheng, Yan, and Yang]{kang2018deep}
Guoliang Kang, Liang Zheng, Yan Yan, and Yi~Yang.
\newblock Deep adversarial attention alignment for unsupervised domain
  adaptation: the benefit of target expectation maximization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 401--416, 2018.

\bibitem[Alexandari et~al.(2020)Alexandari, Kundaje, and
  Shrikumar]{alexandari2020maximum}
Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar.
\newblock Maximum likelihood with bias-corrected calibration is hard-to-beat at
  label shift adaptation.
\newblock In \emph{International Conference on Machine Learning}, pages
  222--232. PMLR, 2020.

\bibitem[Tzeng et~al.(2014)Tzeng, Hoffman, Zhang, Saenko, and
  Darrell]{tzeng2014deep}
Eric Tzeng, Judy Hoffman, Ning Zhang, Kate Saenko, and Trevor Darrell.
\newblock Deep domain confusion: Maximizing for domain invariance.
\newblock \emph{arXiv preprint arXiv:1412.3474}, 2014.

\bibitem[Tzeng et~al.(2017)Tzeng, Hoffman, Saenko, and
  Darrell]{tzeng2017adversarial}
Eric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell.
\newblock Adversarial discriminative domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7167--7176, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Fan, Zheng, Tanwisuth, and
  Zhou]{zhang2021alignment}
Shujian Zhang, Xinjie Fan, Huangjie Zheng, Korawat Tanwisuth, and Mingyuan
  Zhou.
\newblock Alignment attention by matching key and query distributions.
\newblock In \emph{NeurIPS 2021: Neural Information Processing Systems}, Dec.
  2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian~J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1406.2661}, 2014.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International conference on machine learning}, pages
  214--223. PMLR, 2017.

\bibitem[Kolouri et~al.(2018)Kolouri, Pope, Martin, and
  Rohde]{kolouri2018sliced}
Soheil Kolouri, Phillip~E Pope, Charles~E Martin, and Gustavo~K Rohde.
\newblock Sliced {W}asserstein auto-encoders.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Villani(2008)]{villani2008optimal}
C{\'e}dric Villani.
\newblock \emph{Optimal transport: old and new}, volume 338.
\newblock Springer Science \& Business Media, 2008.

\bibitem[M{\'e}rigot and Oudet(2016)]{merigot2016discrete}
Quentin M{\'e}rigot and Edouard Oudet.
\newblock Discrete optimal transport: complexity, geometry and applications.
\newblock \emph{Discrete \& Computational Geometry}, 55\penalty0 (2):\penalty0
  263--283, 2016.

\bibitem[Zhang et~al.(2019{\natexlab{a}})Zhang, Zhang, Liu, and
  Tao]{zhang2019category}
Qiming Zhang, Jing Zhang, Wei Liu, and Dacheng Tao.
\newblock Category anchor-guided unsupervised domain adaptation for semantic
  segmentation.
\newblock \emph{arXiv preprint arXiv:1910.13049}, 2019{\natexlab{a}}.

\bibitem[Xie et~al.(2018)Xie, Zheng, Chen, and Chen]{xie2018learning}
Shaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen.
\newblock Learning semantic representations for unsupervised domain adaptation.
\newblock In \emph{International conference on machine learning}, pages
  5423--5432. PMLR, 2018.

\bibitem[Du~Plessis and Sugiyama(2014)]{du2014semi}
Marthinus~Christoffel Du~Plessis and Masashi Sugiyama.
\newblock Semi-supervised learning of class balance under class-prior change by
  distribution matching.
\newblock \emph{Neural Networks}, 50:\penalty0 110--119, 2014.

\bibitem[Nguyen et~al.(2016)Nguyen, Christoffel, and
  Sugiyama]{nguyen2016continuous}
Tuan~Duong Nguyen, Marthinus Christoffel, and Masashi Sugiyama.
\newblock Continuous target shift adaptation in supervised learning.
\newblock In \emph{Asian Conference on Machine Learning}, pages 285--300. PMLR,
  2016.

\bibitem[Chan and Ng(2005)]{chan2005word}
Yee~Seng Chan and Hwee~Tou Ng.
\newblock Word sense disambiguation with distribution estimation.
\newblock In \emph{IJCAI}, volume~5, pages 1010--5. Citeseer, 2005.

\bibitem[Storkey(2009)]{storkey2009training}
Amos Storkey.
\newblock When training and test sets are different: Characterizing learning
  transfer.
\newblock \emph{Dataset shift in machine learning}, 30:\penalty0 3--28, 2009.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Jiao, Cao, Wong, and Wu]{li2020model}
Rui Li, Qianfen Jiao, Wenming Cao, Hau-San Wong, and Si~Wu.
\newblock Model adaptation: Unsupervised domain adaptation without source data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9641--9650, 2020{\natexlab{a}}.

\bibitem[Kurmi et~al.(2021)Kurmi, Subramanian, and Namboodiri]{kurmi2021domain}
Vinod~K Kurmi, Venkatesh~K Subramanian, and Vinay~P Namboodiri.
\newblock Domain impression: A source data free domain adaptation method.
\newblock In \emph{Proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision}, pages 615--625, 2021.

\bibitem[Kundu et~al.(2020{\natexlab{a}})Kundu, Venkatesh, Venkat, Revanur, and
  Babu]{kundu2020class}
Jogendra~Nath Kundu, Rahul~Mysore Venkatesh, Naveen Venkat, Ambareesh Revanur,
  and R~Venkatesh Babu.
\newblock Class-incremental domain adaptation.
\newblock In \emph{Computer Vision--ECCV 2020: 16th European Conference,
  Glasgow, UK, August 23--28, 2020, Proceedings, Part XIII 16}, pages 53--69.
  Springer, 2020{\natexlab{a}}.

\bibitem[Kundu et~al.(2020{\natexlab{b}})Kundu, Venkat, Babu,
  et~al.]{kundu2020universal}
Jogendra~Nath Kundu, Naveen Venkat, R~Venkatesh Babu, et~al.
\newblock Universal source-free domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4544--4553, 2020{\natexlab{b}}.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and
  Panchanathan]{venkateswara2017deep}
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman
  Panchanathan.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5018--5027, 2017.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and
  Wang]{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1406--1415, 2019.

\bibitem[Junguang~Jiang(2020)]{dalib}
Mingsheng~Long Junguang~Jiang, Bo~Fu.
\newblock Transfer-learning-library.
\newblock \url{https://github.com/thuml/Transfer-Learning-Library}, 2020.

\bibitem[Zhang et~al.(2019{\natexlab{b}})Zhang, Liu, Long, and
  Jordan]{zhang2019bridging}
Yuchen Zhang, Tianle Liu, Mingsheng Long, and Michael Jordan.
\newblock Bridging theory and algorithm for domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, pages
  7404--7413. PMLR, 2019{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Zhai, Luo, Ge, and
  Ren]{li2020enhanced}
Mengxue Li, Yi-Ming Zhai, You-Wei Luo, Peng-Fei Ge, and Chuan-Xian Ren.
\newblock Enhanced transport distance for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 13936--13944, 2020{\natexlab{b}}.

\bibitem[Venkat et~al.(2021)Venkat, Kundu, Singh, Revanur, and
  Babu]{venkat2021your}
Naveen Venkat, Jogendra~Nath Kundu, Durgesh~Kumar Singh, Ambareesh Revanur, and
  R~Venkatesh Babu.
\newblock Your classifier can secretly suffice multi-source domain adaptation.
\newblock \emph{arXiv preprint arXiv:2103.11169}, 2021.

\bibitem[Sun and Saenko(2016)]{sun2016deep}
Baochen Sun and Kate Saenko.
\newblock Deep coral: Correlation alignment for deep domain adaptation.
\newblock In \emph{European conference on computer vision}, pages 443--450.
  Springer, 2016.

\bibitem[Zhu et~al.(2019)Zhu, Zhuang, and Wang]{zhu2019aligning}
Yongchun Zhu, Fuzhen Zhuang, and Deqing Wang.
\newblock Aligning domain-specific distribution and classifier for cross-domain
  classification from multiple sources.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 5989--5996, 2019.

\bibitem[Xu et~al.(2018)Xu, Chen, Zuo, Yan, and Lin]{xu2018dctn}
Ruijia Xu, Ziliang Chen, Wangmeng Zuo, Junjie Yan, and Liang Lin.
\newblock Deep cocktail network: Multi-source unsupervised domain adaptation
  with category shift.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 3964--3973, 2018.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Zhao, Guo, Shen, and
  Ye]{li2020mutual}
Zhenpeng Li, Zhen Zhao, Yuhong Guo, Haifeng Shen, and Jieping Ye.
\newblock Mutual learning network for multi-source domain adaptation.
\newblock \emph{arXiv preprint arXiv:2003.12944}, 2020{\natexlab{c}}.

\bibitem[Long et~al.(2016)Long, Zhu, Wang, and Jordan]{long2016rtn}
Mingsheng Long, Han Zhu, Jianmin Wang, and Michael~I Jordan.
\newblock Unsupervised domain adaptation with residual transfer networks.
\newblock \emph{arXiv preprint arXiv:1602.04433}, 2016.

\bibitem[Saito et~al.(2017)Saito, Watanabe, Ushiku, and Harada]{saito2017mcd}
Kuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada.
\newblock Maximum classifier discrepancy for unsupervised domain adaptation.
\newblock \emph{CoRR}, abs/1712.02560, 2017.

\bibitem[Chen and He(2021)]{chen2021exploring}
Xinlei Chen and Kaiming He.
\newblock Exploring simple siamese representation learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15750--15758, 2021.

\bibitem[Tommasi et~al.(2017)Tommasi, Patricia, Caputo, and
  Tuytelaars]{tommasi2017deeper}
Tatiana Tommasi, Novi Patricia, Barbara Caputo, and Tinne Tuytelaars.
\newblock A deeper look at dataset bias.
\newblock In \emph{Domain adaptation in computer vision applications}, pages
  37--55. Springer, 2017.

\bibitem[Gong et~al.(2012)Gong, Sha, and Grauman]{gong2012overcoming}
Boqing Gong, Fei Sha, and Kristen Grauman.
\newblock Overcoming dataset bias: An unsupervised domain adaptation approach.
\newblock In \emph{NIPS Workshop on Large Scale Visual Recognition and
  Retrieval}, volume~3. Citeseer, 2012.

\bibitem[Bradley et~al.(2000)Bradley, Bennett, and
  Demiriz]{bradley2000constrained}
Paul~S Bradley, Kristin~P Bennett, and Ayhan Demiriz.
\newblock Constrained {K}-means clustering.
\newblock \emph{Microsoft Research, Redmond}, 20\penalty0 (0):\penalty0 0,
  2000.

\bibitem[Asano et~al.(2019)Asano, Rupprecht, and Vedaldi]{asano2019self}
Yuki~Markus Asano, Christian Rupprecht, and Andrea Vedaldi.
\newblock Self-labelling via simultaneous clustering and representation
  learning.
\newblock \emph{arXiv preprint arXiv:1911.05371}, 2019.

\end{thebibliography}
