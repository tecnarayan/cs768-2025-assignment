\begin{thebibliography}{57}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal-et-al21}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (98):\penalty0 1--76, 2021.

\bibitem[Altman(1999)]{altman99cmdps}
Altman, E.
\newblock Constrained markov decision processes.
\newblock 1999.

\bibitem[Bach \& Moulines(2013)Bach and Moulines]{bach-moulines13}
Bach, F. and Moulines, E.
\newblock Non-strongly-convex smooth stochastic approximation with convergence
  rate o(1/n).
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~26, 2013.

\bibitem[Bedi et~al.(2021)Bedi, Parayil, Zhang, Wang, and
  Koppel]{bedi-et-al21heavy-tailed-policy-search}
Bedi, A.~S., Parayil, A., Zhang, J., Wang, M., and Koppel, A.
\newblock On the sample complexity and metastability of heavy-tailed policy
  search in continuous control.
\newblock \emph{arXiv preprint arXiv:2106.08414}, 2021.

\bibitem[Bedi et~al.(2022)Bedi, Chakraborty, Parayil, Sadler, Tokekar, and
  Koppel]{bedi-et-al22contin-action-space}
Bedi, A.~S., Chakraborty, S., Parayil, A., Sadler, B.~M., Tokekar, P., and
  Koppel, A.
\newblock On the hidden biases of policy mirror ascent in continuous action
  spaces.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, volume 162 of \emph{Proceedings of Machine Learning Research},
  pp.\  1716--1731. PMLR, 17--23 Jul 2022.

\bibitem[Bertsekas(2019)]{bertsekas19book}
Bertsekas, D.
\newblock \emph{Reinforcement learning and optimal control}.
\newblock Athena Scientific, 2019.

\bibitem[Bhatnagar \& Lakshmanan(2012)Bhatnagar and
  Lakshmanan]{bhatnagar-lakshmanan12}
Bhatnagar, S. and Lakshmanan, K.
\newblock An online actor--critic algorithm with function approximation for
  constrained markov decision processes.
\newblock \emph{Journal of Optimization Theory and Applications}, 153\penalty0
  (3):\penalty0 688--708, 2012.

\bibitem[Borkar(2005)]{borkar05}
Borkar, V.~S.
\newblock An actor-critic algorithm for constrained markov decision processes.
\newblock \emph{Systems \& control letters}, 54\penalty0 (3):\penalty0
  207--213, 2005.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{brockman-et-al16openaigym}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W.
\newblock Openai gym.
\newblock \emph{arXiv preprint arXiv:1606.01540}, 2016.

\bibitem[Cutkosky \& Orabona(2019)Cutkosky and Orabona]{cutkosky-orabona19}
Cutkosky, A. and Orabona, F.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Derman \& Klein(1965)Derman and Klein]{derman-klein65}
Derman, C. and Klein, M.
\newblock Some remarks on finite horizon markovian decision models.
\newblock \emph{Operations research}, 13\penalty0 (2):\penalty0 272--278, 1965.

\bibitem[Ding et~al.(2021)Ding, Zhang, and Lavaei]{ding-et-al21}
Ding, Y., Zhang, J., and Lavaei, J.
\newblock Beyond exact gradients: Convergence of stochastic soft-max policy
  gradient methods with entropy regularization.
\newblock \emph{arXiv preprint arXiv:2110.10117}, 2021.

\bibitem[Ding et~al.(2022)Ding, Zhang, and Lavaei]{ding-et-al22}
Ding, Y., Zhang, J., and Lavaei, J.
\newblock On the global optimum convergence of momentum-based policy gradient.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1910--1934. PMLR, 2022.

\bibitem[Efroni et~al.(2020)Efroni, Mannor, and
  Pirotta]{efroni-mannor-pirotta20}
Efroni, Y., Mannor, S., and Pirotta, M.
\newblock Exploration-exploitation in constrained mdps.
\newblock \emph{arXiv preprint arXiv:2003.02189}, 2020.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach-et-al19}
Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fatkhullin et~al.(2022)Fatkhullin, Etesami, He, and
  Kiyavash]{KL_PAGER_Fatkhullin}
Fatkhullin, I., Etesami, J., He, N., and Kiyavash, N.
\newblock Sharp analysis of stochastic optimization under global
  {K}urdyka-{\l}ojasiewicz inequality.
\newblock \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Fatkhullin et~al.(2023)Fatkhullin, Barakat, Kireeva, and
  He]{Fatkhullin_SPG_FND_2023}
Fatkhullin, I., Barakat, A., Kireeva, A., and He, N.
\newblock Stochastic policy gradient methods: Improved sample complexity for
  fisher-non-degenerate policies.
\newblock \emph{To appear at the International Conference on Machine Learning,
  arXiv preprint arXiv:2302.01734}, 2023.

\bibitem[Filar et~al.(1989)Filar, Kallenberg, and
  Lee]{filar-et-al89variance-pen}
Filar, J.~A., Kallenberg, L.~C., and Lee, H.-M.
\newblock Variance-penalized markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 14\penalty0 (1):\penalty0
  147--161, 1989.

\bibitem[Gadat et~al.(2018)Gadat, Panloup, and Saadane]{Gadat_SHB_18}
Gadat, S., Panloup, F., and Saadane, S.
\newblock Stochastic {{Heavy}} ball.
\newblock \emph{Electronic Journal of Statistics}, 12\penalty0 (1):\penalty0
  461--529, 2018.

\bibitem[Garc{\i}a \& Fern{\'a}ndez(2015)Garc{\i}a and
  Fern{\'a}ndez]{garcia-fernandez15}
Garc{\i}a, J. and Fern{\'a}ndez, F.
\newblock A comprehensive survey on safe reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 1437--1480, 2015.

\bibitem[Gargiani et~al.(2022)Gargiani, Zanelli, Martinelli, Summers, and
  Lygeros]{gargiani-et-al22}
Gargiani, M., Zanelli, A., Martinelli, A., Summers, T., and Lygeros, J.
\newblock {PAGE}-{PG}: A simple and loopless variance-reduced policy gradient
  method with probabilistic gradient estimation.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, volume 162 of \emph{Proceedings of Machine Learning Research},
  pp.\  7223--7240. PMLR, 2022.

\bibitem[Geist et~al.(2022)Geist, P\'{e}rolat, Lauri\`{e}re, Elie, Perrin,
  Bachem, Munos, and Pietquin]{geist-et-al22}
Geist, M., P\'{e}rolat, J., Lauri\`{e}re, M., Elie, R., Perrin, S., Bachem, O.,
  Munos, R., and Pietquin, O.
\newblock Concave utility reinforcement learning: The mean-field game
  viewpoint.
\newblock In \emph{Proceedings of the 21st International Conference on
  Autonomous Agents and Multiagent Systems}, AAMAS '22, pp.\  489–497, 2022.

\bibitem[Ghasemipour et~al.(2020)Ghasemipour, Zemel, and
  Gu]{ghasemipour-et-al20}
Ghasemipour, S. K.~S., Zemel, R., and Gu, S.
\newblock A divergence minimization perspective on imitation learning methods.
\newblock In \emph{Conference on Robot Learning}, pp.\  1259--1277. PMLR, 2020.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and Van~Soest]{hazan-et-al19}
Hazan, E., Kakade, S., Singh, K., and Van~Soest, A.
\newblock Provably efficient maximum entropy exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2681--2691. PMLR, 2019.

\bibitem[Huang et~al.(2020)Huang, Gao, Pei, and Huang]{huang-et-al20}
Huang, F., Gao, S., Pei, J., and Huang, H.
\newblock Momentum-based policy gradient methods.
\newblock In \emph{International conference on machine learning}, pp.\
  4422--4433. PMLR, 2020.

\bibitem[Kallenberg(1994)]{kallenberg94survey}
Kallenberg, L.~C.
\newblock Survey of linear programming for standard and nonstandard markovian
  control problems. part i: Theory.
\newblock \emph{Zeitschrift f{\"u}r Operations Research}, 40\penalty0
  (1):\penalty0 1--42, 1994.

\bibitem[Kumar et~al.(2022)Kumar, Wang, Levy, and
  Mannor]{kumar-et-al22pg-general}
Kumar, N., Wang, K., Levy, K., and Mannor, S.
\newblock Policy gradient for reinforcement learning with general utilities.
\newblock \emph{arXiv preprint arXiv:2210.00991}, 2022.

\bibitem[Lan(2022)]{lan22}
Lan, G.
\newblock Policy mirror descent for reinforcement learning: Linear convergence,
  new sampling complexity, and generalized problem classes.
\newblock \emph{Mathematical programming}, pp.\  1--48, 2022.

\bibitem[Liu et~al.(2020)Liu, Zhang, Basar, and Yin]{liu-et-al20}
Liu, Y., Zhang, K., Basar, T., and Yin, W.
\newblock An improved analysis of (variance-reduced) policy gradient and
  natural policy gradient methods.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7624--7636, 2020.

\bibitem[Masiha et~al.(2022)Masiha, Salehkaleybar, He, Kiyavash, and
  Thiran]{Masiha_SCRN_KL}
Masiha, S., Salehkaleybar, S., He, N., Kiyavash, N., and Thiran, P.
\newblock Stochastic second-order methods improve best-known sample complexity
  of {SGD} for gradient-dominated functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Miryoosefi et~al.(2019)Miryoosefi, Brantley, Daume~III, Dudik, and
  Schapire]{miryoosefi-et-al19rl-with-convex-constraints}
Miryoosefi, S., Brantley, K., Daume~III, H., Dudik, M., and Schapire, R.~E.
\newblock Reinforcement learning with convex constraints.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Mutny et~al.(2023)Mutny, Janik, and Krause]{mutny-et-al23}
Mutny, M., Janik, T., and Krause, A.
\newblock Active exploration via experiment design in markov chains.
\newblock In Ruiz, F., Dy, J., and van~de Meent, J.-W. (eds.),
  \emph{Proceedings of The 26th International Conference on Artificial
  Intelligence and Statistics}, volume 206 of \emph{Proceedings of Machine
  Learning Research}, pp.\  7349--7374. PMLR, 25--27 Apr 2023.

\bibitem[Mutti et~al.(2022{\natexlab{a}})Mutti, De~Santi, and
  Restelli]{mutti-desanti-et-al22}
Mutti, M., De~Santi, R., and Restelli, M.
\newblock The importance of non-markovianity in maximum state entropy
  exploration.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, volume 162 of \emph{Proceedings of Machine Learning Research},
  pp.\  16223--16239. PMLR, 17--23 Jul 2022{\natexlab{a}}.

\bibitem[Mutti et~al.(2022{\natexlab{b}})Mutti, Santi, Bartolomeis, and
  Restelli]{mutti-et-al22}
Mutti, M., Santi, R.~D., Bartolomeis, P.~D., and Restelli, M.
\newblock Challenging common assumptions in convex reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Papini et~al.(2018)Papini, Binaghi, Canonaco, Pirotta, and
  Restelli]{papini-et-al18}
Papini, M., Binaghi, D., Canonaco, G., Pirotta, M., and Restelli, M.
\newblock Stochastic variance-reduced policy gradient.
\newblock In \emph{International conference on machine learning}, pp.\
  4026--4035. PMLR, 2018.

\bibitem[Pham et~al.(2020)Pham, Nguyen, Phan, Nguyen, Dijk, and
  Tran-Dinh]{pham-et-al20}
Pham, N., Nguyen, L., Phan, D., Nguyen, P.~H., Dijk, M., and Tran-Dinh, Q.
\newblock A hybrid stochastic policy gradient algorithm for reinforcement
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  374--385. PMLR, 2020.

\bibitem[Puterman(2014)]{puterman14}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Salehkaleybar et~al.(2022)Salehkaleybar, Khorasani, Kiyavash, He, and
  Thiran]{salehkaleybar-et-al22}
Salehkaleybar, S., Khorasani, S., Kiyavash, N., He, N., and Thiran, P.
\newblock Adaptive momentum-based policy gradient with second-order
  information.
\newblock \emph{arXiv preprint arXiv:2205.08253}, 2022.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman-et-al15trpo}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman-et-al17ppo}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shen et~al.(2019)Shen, Ribeiro, Hassani, Qian, and Mi]{shen-et-al19}
Shen, Z., Ribeiro, A., Hassani, H., Qian, H., and Mi, C.
\newblock Hessian aided policy gradient.
\newblock In \emph{International conference on machine learning}, pp.\
  5729--5738. PMLR, 2019.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton-barto18}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{PGM_Sutton_1999}
Sutton, R.~S., McAllester, D., Singh, S., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~12. MIT Press, 1999.

\bibitem[Tsitsiklis \& Van~Roy(1997)Tsitsiklis and
  Van~Roy]{tsitsiklis-vanroy97}
Tsitsiklis, J. and Van~Roy, B.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{IEEE Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.
\newblock \doi{10.1109/9.580874}.

\bibitem[Wang et~al.(2020)Wang, Cai, Yang, and Wang]{wang-et-al20}
Wang, L., Cai, Q., Yang, Z., and Wang, Z.
\newblock Neural policy gradient methods: Global optimality and rates of
  convergence.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Williams(1992)]{REINFORCE_Williams_1992}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine Learning}, 8:\penalty0 229–256, 1992.

\bibitem[Xiao(2022)]{xiao22}
Xiao, L.
\newblock On the convergence rates of policy gradient methods.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (282):\penalty0 1--36, 2022.

\bibitem[Xu et~al.(2020{\natexlab{a}})Xu, Gao, and Gu]{xu-et-al20iclr}
Xu, P., Gao, F., and Gu, Q.
\newblock Sample efficient policy gradient methods with recursive variance
  reduction.
\newblock In \emph{International Conference on Learning Representations},
  2020{\natexlab{a}}.

\bibitem[Xu et~al.(2020{\natexlab{b}})Xu, Gao, and Gu]{xu-et-al20uai}
Xu, P., Gao, F., and Gu, Q.
\newblock An improved convergence analysis of stochastic variance-reduced
  policy gradient.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  541--551.
  PMLR, 2020{\natexlab{b}}.

\bibitem[Yuan et~al.(2020)Yuan, Lian, Liu, and Zhou]{yuan-et-al20}
Yuan, H., Lian, X., Liu, J., and Zhou, Y.
\newblock Stochastic {{Recursive Momentum}} for {{Policy Gradient Methods}},
  2020.

\bibitem[Yuan et~al.(2022)Yuan, Gower, and Lazaric]{Vanilla_PL_Yuan_21}
Yuan, R., Gower, R.~M., and Lazaric, A.
\newblock A general sample complexity analysis of vanilla policy gradient.
\newblock In \emph{Proceedings of The 25th International Conference on
  Artificial Intelligence and Statistics}, volume 151 of \emph{Proceedings of
  Machine Learning Research}, pp.\  3332--3380. PMLR, 2022.

\bibitem[Yuan et~al.(2023)Yuan, Du, Gower, Lazaric, and
  Xiao]{yuan-et-al22log-linear}
Yuan, R., Du, S.~S., Gower, R.~M., Lazaric, A., and Xiao, L.
\newblock Linear convergence of natural policy gradient methods with log-linear
  policies.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Zahavy et~al.(2021)Zahavy, O'Donoghue, Desjardins, and
  Singh]{zahavy-et-al21}
Zahavy, T., O'Donoghue, B., Desjardins, G., and Singh, S.
\newblock Reward is enough for convex mdps.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 25746--25759, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Koppel, Bedi, Szepesvari, and
  Wang]{zhang-et-al20variational}
Zhang, J., Koppel, A., Bedi, A.~S., Szepesvari, C., and Wang, M.
\newblock Variational policy gradient method for reinforcement learning with
  general utilities.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4572--4583, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Bedi, Wang, and
  Koppel]{zhang-et-al21cautious}
Zhang, J., Bedi, A.~S., Wang, M., and Koppel, A.
\newblock Cautious reinforcement learning via distributional risk in the dual
  domain.
\newblock \emph{IEEE Journal on Selected Areas in Information Theory},
  2\penalty0 (2):\penalty0 611--626, 2021{\natexlab{a}}.
\newblock \doi{10.1109/JSAIT.2021.3081108}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Ni, Szepesvari, Wang,
  et~al.]{zhang-et-al21}
Zhang, J., Ni, C., Szepesvari, C., Wang, M., et~al.
\newblock On the convergence and sample efficiency of variance-reduced policy
  gradient method.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 2228--2240, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Bedi, Wang, and
  Koppel]{zhang_bedi_wang_koppel22}
Zhang, J., Bedi, A.~S., Wang, M., and Koppel, A.
\newblock Multi-agent reinforcement learning with general utilities via
  decentralized shadow reward actor-critic.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence},
  36\penalty0 (8):\penalty0 9031--9039, Jun. 2022.

\end{thebibliography}
