\begin{thebibliography}{10}

\bibitem{ailerons}
Ailerons data set.
\newblock \url{https://www.dcc.fc.up.pt/~ltorgo/Regression/ailerons.html}.

\bibitem{bank8FM}
Bank datasets (bank32nh and bank8fm).
\newblock \url{https://www.dcc.fc.up.pt/~ltorgo/Regression/bank.html}.

\bibitem{navalT}
Condition based maintenance of naval propulsion plants data set.
\newblock
  \url{http://archive.ics.uci.edu/ml/datasets/Condition+Based+Maintenance+of+Naval+Propulsion+Plants}.

\bibitem{creditcard}
Credit card fraud detection.
\newblock \url{https://www.kaggle.com/mlg-ulb/creditcardfraud}.

\bibitem{eigen}
Eigen: a {C++} template library for linear algebra: matrices, vectors,
  numerical solvers, and related algorithms.).
\newblock \url{http://eigen.tuxfamily.org/index.php?title=Main_Page}.

\bibitem{elevators}
Elevators data set.
\newblock \url{https://www.dcc.fc.up.pt/~ltorgo/Regression/elevators.html}.

\bibitem{kaggle}
Kaggle: Your machine learning and data science community.
\newblock \url{https://www.kaggle.com/}.

\bibitem{priceprediction}
Mercari price suggestion challenge.
\newblock \url{https://www.kaggle.com/c/mercari-price-suggestion-challenge}.

\bibitem{parkinsons}
Parkinsons data set.
\newblock \url{https://archive.ics.uci.edu/ml/datasets/Parkinsons}.

\bibitem{puma32h}
Pumadyn datasets (puma32h and puma8nh).
\newblock \url{https://www.dcc.fc.up.pt/~ltorgo/Regression/puma.html}.

\bibitem{rossmann}
Rossmann store sales.
\newblock \url{https://www.kaggle.com/c/rossmann-store-sales}.

\bibitem{arik2019tabnet}
Sercan~O Arik and Tomas Pfister.
\newblock {TabNet}: Attentive interpretable tabular learning.
\newblock {\em arXiv preprint arXiv:1908.07442}, 2019.

\bibitem{JMLR:v17:benavoli16a}
Alessio Benavoli, Giorgio Corani, and Francesca Mangili.
\newblock Should we really use post-hoc tests based on mean-ranks?
\newblock {\em Journal of Machine Learning Research}, 17(5):1--10, 2016.

\bibitem{KDD16_xgboost}
T.~Chen and C.~Guestrin.
\newblock {XGBoost}: A scalable tree boosting system.
\newblock In {\em KDD}, pages 785--794, 2016.

\bibitem{navalT-paper}
Andrea Coraddu, Luca Oneto, Alessandro Ghio, Stefano Savio, Davide Anguita, and
  Massimo Figari.
\newblock Machine learning approaches for improving condition-based maintenance
  of naval propulsion plants.
\newblock {\em Journal of Engineering for the Maritime Environment}, --(--):--,
  2014.

\bibitem{DBLP:books/daglib/0023376}
Thomas~H. Cormen, Charles~E. Leiserson, Ronald~L. Rivest, and Clifford Stein.
\newblock {\em Introduction to Algorithms, 3rd Edition}.
\newblock {MIT} Press, 2009.

\bibitem{cortes2019regularized}
Corinna Cortes, Mehryar Mohri, and Dmitry Storcheus.
\newblock Regularized gradient boosting.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5450--5459, 2019.

\bibitem{cortes2014deep}
Corinna Cortes, Mehryar Mohri, and Umar Syed.
\newblock Deep boosting.
\newblock In {\em Proceedings of the 31st International Conference on
  International Conference on Machine Learning-Volume 32}, pages II--1179,
  2014.

\bibitem{demvsar2006statistical}
Janez Dem{\v{s}}ar.
\newblock Statistical comparisons of classifiers over multiple data sets.
\newblock {\em Journal of Machine learning research}, 7(Jan):1--30, 2006.

\bibitem{fish2015fair}
Benjamin Fish, Jeremy Kun, and Ad{\'a}m~D Lelkes.
\newblock Fair boosting: a case study.
\newblock In {\em Workshop on Fairness, Accountability, and Transparency in
  Machine Learning}. Citeseer, 2015.

\bibitem{freund1995boosting}
Yoav Freund.
\newblock Boosting a weak learning algorithm by majority.
\newblock {\em Information and computation}, 121(2):256--285, 1995.

\bibitem{freund1995decision}
Yoav Freund and Robert~E Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock In {\em European conference on computational learning theory}.
  Springer, 1995.

\bibitem{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock {\em Annals of statistics}, pages 1189--1232, 2001.

\bibitem{friedman2002stochastic}
Jerome~H Friedman.
\newblock Stochastic gradient boosting.
\newblock {\em Computational statistics \& data analysis}, 38(4):367--378,
  2002.

\bibitem{grari2019fair}
Vincent Grari, Boris Ruf, Sylvain Lamprier, and Marcin Detyniecki.
\newblock Fair adversarial gradient tree boosting.
\newblock {\em arXiv preprint arXiv:1911.05369}, 2019.

\bibitem{guillame2018arxiv}
Mathieu Guillame{-}Bert and Olivier Teytaud.
\newblock Exact distributed training: Random forest with billions of examples.
\newblock {\em CoRR}, abs/1804.06755, 2018.

\bibitem{NIPS17_Ke}
Ke~Guolin, Meng Qi, Finley Thomas, Wang Taifeng, Chen Wei, Ma~Weidong,
  Ye~Qiwei, and Liu Tie-Yan.
\newblock {LightGBM}: A highly efficient gradient boosting decision tree.
\newblock In {\em NIPS}, pages 3149--3157, 2017.

\bibitem{NIPS2019_9645}
Bulat Ibragimov and Gleb Gusev.
\newblock Minimal variance sampling in stochastic gradient boosting.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  15087--15097. Curran Associates, Inc., 2019.

\bibitem{iman1980approximations}
Ronald~L Iman and James~M Davenport.
\newblock Approximations of the critical region of the friedman statistic.
\newblock {\em Communications in Statistics-Theory and Methods}, 9(6):571--595,
  1980.

\bibitem{pmlr-v51-jamieson16}
Kevin Jamieson and Ameet Talwalkar.
\newblock Non-stochastic best arm identification and hyperparameter
  optimization.
\newblock In Arthur Gretton and Christian~C. Robert, editors, {\em Proceedings
  of the 19th International Conference on Artificial Intelligence and
  Statistics}, volume~51 of {\em Proceedings of Machine Learning Research},
  pages 240--248, Cadiz, Spain, 09--11 May 2016. PMLR.

\bibitem{karimireddy2018global}
Sai~Praneeth Karimireddy, Sebastian~U Stich, and Martin Jaggi.
\newblock Global linear convergence of newton's method without strong-convexity
  or lipschitz gradients.
\newblock {\em arXiv preprint arXiv:1806.00413}, 2018.

\bibitem{ke2019deepgbm}
Guolin Ke, Zhenhui Xu, Jia Zhang, Jiang Bian, and Tie-Yan Liu.
\newblock Deepgbm: A deep learning framework distilled by gbdt for online
  prediction tasks.
\newblock In {\em KDD '19 Proceedings of the 25th ACM SIGKDD International
  Conference on Knowledge Discovery and Data Mining}, August 2019.

\bibitem{li2008two}
Jianjun~David Li.
\newblock A two-step rejection procedure for testing multiple hypotheses.
\newblock {\em Journal of Statistical Planning and Inference},
  138(6):1521--1527, 2008.

\bibitem{li2019privacy}
Qinbin Li, Zhaomin Wu, Zeyi Wen, and Bingsheng He.
\newblock Privacy-preserving gradient boosting decision trees.
\newblock {\em arXiv preprint arXiv:1911.04209}, 2019.

\bibitem{parkinsons-paper}
Max Little, Patrick Mcsharry, Stephen Roberts, Declan Costello, and Irene
  Moroz.
\newblock Exploiting nonlinear recurrence and fractal scaling properties for
  voice disorder detection.
\newblock {\em Biomedical engineering online}, 6:23, 02 2007.

\bibitem{lu2018randomized}
Haihao Lu and Rahul Mazumder.
\newblock Randomized gradient boosting machine.
\newblock {\em arXiv preprint arXiv:1810.10158}, 2018.

\bibitem{mehta1996sliq}
Manish Mehta, Rakesh Agrawal, and Jorma Rissanen.
\newblock Sliq: A fast scalable classifier for data mining.
\newblock In {\em International conference on extending database technology},
  pages 18--32. Springer, 1996.

\bibitem{popov2019neural}
Sergei Popov, Stanislav Morozov, and Artem Babenko.
\newblock Neural oblivious decision ensembles for deep learning on tabular
  data.
\newblock {\em arXiv preprint arXiv:1909.06312}, 2019.

\bibitem{prokhorenkova2018catboost}
Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna~Veronika Dorogush,
  and Andrey Gulin.
\newblock {CatBoost}: unbiased boosting with categorical features.
\newblock In {\em Advances in neural information processing systems}, pages
  6638--6648, 2018.

\bibitem{10.5555/2981562.2981710}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Proceedings of the 20th International Conference on Neural
  Information Processing Systems}, NIPS’07, page 1177–1184, Red Hook, NY,
  USA, 2007. Curran Associates Inc.

\bibitem{schapire1990strength}
Robert~E Schapire.
\newblock The strength of weak learnability.
\newblock {\em Machine learning}, 5(2):197--227, 1990.

\bibitem{shafer96vldb}
John~C. Shafer, Rakesh Agrawal, and Manish Mehta.
\newblock Sprint: A scalable parallel classifier for data mining.
\newblock In {\em Proceedings of the 22th International Conference on Very
  Large Data Bases}, VLDB '96, pages 544--555, San Francisco, CA, USA, 1996.
  Morgan Kaufmann Publishers Inc.

\bibitem{sigrist2018gradient}
Fabio Sigrist.
\newblock Gradient and newton boosting for classification and regression.
\newblock {\em arXiv preprint arXiv:1808.03064}, 2018.

\bibitem{sigrist2019ktboost}
Fabio Sigrist.
\newblock {KTBoost: Combined Kernel and Tree Boosting}.
\newblock {\em arXiv preprint arXiv:1902.03999}, 2019.

\bibitem{OpenML2013}
Joaquin Vanschoren, Jan~N. van Rijn, Bernd Bischl, and Luis Torgo.
\newblock {OpenML}: Networked science in machine learning.
\newblock {\em SIGKDD Explorations}, 15(2):49--60, 2013.

\bibitem{Zhang2017GPUaccelerationFL}
Huan Zhang, Si~Si, and Cho-Jui Hsieh.
\newblock {GPU}-acceleration for large-scale tree boosting.
\newblock {\em ArXiv}, abs/1706.08359, 2017.

\end{thebibliography}
