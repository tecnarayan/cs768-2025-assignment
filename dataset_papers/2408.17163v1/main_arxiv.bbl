\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghasi et~al.(2017)Aghasi, Abdi, Nguyen, and
  Romberg]{Aghasi2017Net-Trim}
Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg.
\newblock {Net-Trim: Convex Pruning of Deep Neural Networks with Performance
  Guarantee}.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Amari(1998)]{amari1998natural}
Shun-Ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural computation}, 10\penalty0 (2):\penalty0 251--276, 1998.

\bibitem[Bahmani et~al.(2013)Bahmani, Raj, and Boufounos]{bahmani2013greedy}
Sohail Bahmani, Bhiksha Raj, and Petros~T Boufounos.
\newblock Greedy sparsity-constrained optimization.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 807--841, 2013.

\bibitem[Benbaki et~al.(2023)Benbaki, Chen, Meng, Hazimeh, Ponomareva, Zhao,
  and Mazumder]{benbaki2023fast}
Riade Benbaki, Wenyu Chen, Xiang Meng, Hussein Hazimeh, Natalia Ponomareva, Zhe
  Zhao, and Rahul Mazumder.
\newblock Fast as chita: Neural network pruning with combinatorial
  optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  2031--2049. PMLR, 2023.

\bibitem[Blumensath(2012)]{blumensath2012accelerated}
Thomas Blumensath.
\newblock Accelerated iterative hard thresholding.
\newblock \emph{Signal Processing}, 92\penalty0 (3):\penalty0 752--756, 2012.

\bibitem[Blumensath and Davies(2009)]{blumensath2009iterative}
Thomas Blumensath and Mike~E. Davies.
\newblock Iterative hard thresholding for compressed sensing.
\newblock \emph{Applied and Computational Harmonic Analysis}, 27\penalty0
  (3):\penalty0 265--274, 2009.

\bibitem[Chen and Gu(2017)]{chen2017fast}
Jinghui Chen and Quanquan Gu.
\newblock Fast newton hard thresholding pursuit for sparsity constrained
  nonconvex optimization.
\newblock In \emph{Proceedings of the 23rd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 757--766, 2017.

\bibitem[Chen et~al.(2014)Chen, Ge, Wang, and Ye]{Chen2014complexity}
Xiaojun Chen, Dongdong Ge, Zizhuo Wang, and Yinyu Ye.
\newblock Complexity of unconstrained {$L_2-L_p$} minimization.
\newblock \emph{Mathematical Programming}, 143\penalty0 (1):\penalty0 371--383,
  2014.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and
  Zettlemoyer]{Dettmers2022LLMint8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Dong et~al.(2017)Dong, Chen, and Pan]{dong2017learning}
Xin Dong, Shangyu Chen, and Sinno Pan.
\newblock Learning to prune deep neural networks via layer-wise optimal brain
  surgeon.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale, 2021.

\bibitem[Foster et~al.(2015)Foster, Karloff, and Thaler]{Foster2015variable}
D.~Foster, H.~Karloff, and J.~Thaler.
\newblock Variable selection is hard.
\newblock \emph{Conference on Learning Theory}, pages 696--709, 2015.

\bibitem[Frantar and Alistarh(2022)]{frantar2022optimal}
Elias Frantar and Dan Alistarh.
\newblock Optimal brain compression: A framework for accurate post-training
  quantization and pruning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=ksVGCOlOEba}.

\bibitem[Frantar and Alistarh(2023)]{frantar2023sparsegpt}
Elias Frantar and Dan Alistarh.
\newblock {SparseGPT: Massive Language Models Can be Accurately Pruned in
  One-Shot}.
\newblock \emph{International Conference on Machine Learning (ICML)}, 2023.

\bibitem[Frantar et~al.(2023)Frantar, Ashkboos, Hoefler, and
  Alistarh]{Frantar2023OPTQ}
Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh.
\newblock {OPTQ: Accurate Quantization for Generative Pre-trained
  Transformers}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Guo et~al.(2020)Guo, Wang, Wu, Yu, Liang, Hu, and
  Luo]{Guo2020OnlineKD}
Qiushan Guo, Xinjiang Wang, Yichao Wu, Zhipeng Yu, Ding Liang, Xiaolin Hu, and
  Ping Luo.
\newblock {Online knowledge distillation via collaborative learning}.
\newblock \emph{In Proceedings of the IEEE/CVF Conference on Com- puter Vision
  and Pattern Recognition}, page 11020â€“11029, 2020.

\bibitem[Harutyunyan et~al.(2023)Harutyunyan, Rawat, Menon, Kim, and
  Kumar]{Harutyunyan2023KD}
Hrayr Harutyunyan, Ankit~Singh Rawat, Aditya~Krishna Menon, Seungyeon Kim, and
  Sanjiv Kumar.
\newblock {Supervision Complexity and its Role in Knowledge Distillation}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Hassibi and Stork(1992)]{hassibi1992second}
Babak Hassibi and David Stork.
\newblock Second order derivatives for network pruning: Optimal brain surgeon.
\newblock \emph{Advances in neural information processing systems}, 5, 1992.

\bibitem[Hassibi et~al.(1993)Hassibi, Stork, and Wolff]{hassibi1993optimal}
Babak Hassibi, David Stork, and Gregory Wolff.
\newblock Optimal brain surgeon: Extensions and performance comparisons.
\newblock \emph{Advances in neural information processing systems}, 6, 1993.

\bibitem[Hinton et~al.(2014)Hinton, Vinyals, and Dean]{Hinton2014KD}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock {Distilling the Knowledge in a Neural Network}.
\newblock \emph{Deep Learning Workshop at NIPS}, 2014.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{hoefler2021sparsity}
Torsten Hoefler, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 10882--11005, 2021.

\bibitem[Horn and Johnson(2012)]{horn2012matrix}
Roger~A Horn and Charles~R Johnson.
\newblock \emph{Matrix analysis}.
\newblock Cambridge university press, 2012.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and
  Chen]{Hu2022LoRA}
Edward~J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
  Wang, Lu~Wang, and Weizhu Chen.
\newblock {LoRA: Low-Rank Adaptation of Large Language Models}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2022.

\bibitem[Kim et~al.(2023)Kim, Hooper, Gholami, Dong, Li, Shen, Mahoney, and
  Keutzer]{Kim2023SqueezeLLM}
Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen,
  Michael~W. Mahoney, and Kurt Keutzer.
\newblock {SqueezeLLM: Dense-and-Sparse Quantization}.
\newblock \emph{arXiv preprint arXiv:2306.07629}, 2023.

\bibitem[LeCun et~al.(1989)LeCun, Denker, and Solla]{lecun90brain}
Yann LeCun, John Denker, and Sara Solla.
\newblock Optimal brain damage.
\newblock In D.~Touretzky, editor, \emph{Advances in Neural Information
  Processing Systems}, volume~2. Morgan-Kaufmann, 1989.
\newblock URL
  \url{https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf}.

\bibitem[Lialin et~al.(2024)Lialin, Muckatira, Shivagunde, and
  Rumshisky]{Lialin2024ReLoRA}
Vladislav Lialin, Sherin Muckatira, Namrata Shivagunde, and Anna Rumshisky.
\newblock {ReLoRA: High-Rank Training Through Low-Rank Updates}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2024.

\bibitem[Lin et~al.(2020)Lin, Stich, Barba, Dmitriev, and Jaggi]{Lin2020DPF}
Tao Lin, Sebastian~U. Stich, Luis Barba, Daniil Dmitriev, and Martin Jaggi.
\newblock {Dynamic Model Pruning with Feedback}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2020.

\bibitem[Meng and Zhao(2020)]{meng2020newton}
Nan Meng and Yun-Bin Zhao.
\newblock Newton-step-based hard thresholding algorithms for sparse signal
  recovery.
\newblock \emph{IEEE Transactions on Signal Processing}, 68:\penalty0
  6594--6606, 2020.

\bibitem[Mishchenko(2023)]{mishchenko2021regularized}
Konstantin Mishchenko.
\newblock Regularized {Newton} method with global {$O(1/k^2)$} convergence.
\newblock \emph{SIAM Journal on Optimization}, 33\penalty0 (3):\penalty0
  1440--1462, 2023.
\newblock \doi{10.1137/22M1488752}.

\bibitem[Mishra and Marr(2018)]{Mishra2018Apprentice}
Asit Mishra and Debbie Marr.
\newblock {Apprentice: Using Knowledge Distillation Techniques To Improve
  Low-Precision Network Accuracy}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Natarajan(1995)]{Natarajan1995sparse}
B.~K. Natarajan.
\newblock Sparse approximate solutions to linear systems.
\newblock \emph{SIAM Journal on Computing}, 24\penalty0 (2):\penalty0 227--234,
  1995.

\bibitem[Nesterov(2018)]{nesterov2018lectures}
Yurii Nesterov.
\newblock \emph{Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[Nikdan et~al.(2024)Nikdan, Tabesh, and Alistarh]{Nikdan2024RoSA}
Mahdi Nikdan, Soroush Tabesh, and Dan Alistarh.
\newblock {RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust
  Adaptation}.
\newblock \emph{arXiv preprint arXiv:2401.04679}, 2024.

\bibitem[Peste et~al.(2021)Peste, Iofinova, Vladu, and Alistarh]{peste2021ac}
Alexandra Peste, Eugenia Iofinova, Adrian Vladu, and Dan Alistarh.
\newblock {AC/DC}: Alternating compressed/decompressed training of deep neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 8557--8570, 2021.

\bibitem[Polino et~al.(2018)Polino, Pascanu, and Alistarh]{Polino2018KDQ}
Antonio Polino, Razvan Pascanu, and Dan Alistarh.
\newblock {Model compression via distillation and quantization}.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2018.

\bibitem[Singh and Alistarh(2020)]{singh2020woodfisher}
Sidak~Pal Singh and Dan Alistarh.
\newblock {WoodFisher: Efficient second-order approximation for neural network
  compression}.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18098--18109, 2020.

\bibitem[Tropp and Gilbert(2007)]{tropp2007signal}
Joel~A. Tropp and Anna~C. Gilbert.
\newblock Signal recovery from random measurements via orthogonal matching
  pursuit.
\newblock \emph{IEEE Transactions on Information Theory}, 53\penalty0
  (12):\penalty0 4655--4666, 2007.

\bibitem[Wang et~al.(2022)Wang, Zhang, Zhang, Meng, Ma, Liu, and
  Chen]{Wang2022Adam}
Bohan Wang, Yushun Zhang, Huishuai Zhang, Qi~Meng, Zhi-Ming Ma, Tie-Yan Liu,
  and Wei Chen.
\newblock {Provable adaptivity in Adam}.
\newblock \emph{arXiv preprint arXiv:2208.09900}, 2022.

\bibitem[Xu et~al.(2023)Xu, Xu, and Mandic]{Xu2023TensorGPT}
Mingxue Xu, Yao~Lei Xu, and Danilo~P. Mandic.
\newblock {TensorGPT: Efficient Compression of the Embedding Layer in LLMs
  based on the Tensor-Train Decomposition}.
\newblock \emph{arXiv preprint arXiv:2307.00526}, 2023.

\bibitem[Yao et~al.(2022)Yao, Aminabadi, Zhang, Wu, Li, and
  He]{Yao2022ZeroQuant}
Zhewei Yao, Reza~Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, and
  Yuxiong He.
\newblock {ZeroQuant: Efficient and Affordable Post-Training Quantization for
  Large-Scale Transformers}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2022.

\bibitem[Yu et~al.(2022)Yu, Serra, Ramalingam, and Zhe]{yu2022combinatorial}
Xin Yu, Thiago Serra, Srikumar Ramalingam, and Shandian Zhe.
\newblock The combinatorial brain surgeon: pruning weights that cancel one
  another in neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  25668--25683. PMLR, 2022.

\bibitem[Zhang et~al.(2020)Zhang, He, Sra, and Jadbabaie]{Zhang2020clipping}
Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie.
\newblock Why gradient clipping accelerates training: {A} theoretical
  justification for adaptivity.
\newblock \emph{8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJgnXpVYwS}.

\bibitem[Zhao(2018)]{zhao2018sparse}
Yun-Bin Zhao.
\newblock \emph{Sparse optimization theory and methods}.
\newblock CRC Press, 2018.

\bibitem[Zhou et~al.(2021)Zhou, Xiu, and Qi]{zhou2021global}
Shenglong Zhou, Naihua Xiu, and Hou-Duo Qi.
\newblock Global and quadratic convergence of newton hard-thresholding pursuit.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 599--643, 2021.

\end{thebibliography}
