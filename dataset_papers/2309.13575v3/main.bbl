\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achterhold et~al.(2018)Achterhold, Koehler, Schmeink, and
  Genewein]{achterhold2018variational}
Jan Achterhold, Jan~Mathias Koehler, Anke Schmeink, and Tim Genewein.
\newblock Variational network quantization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Barsbey et~al.(2021)Barsbey, Sefidgaran, Erdogdu, Richard, and
  Simsekli]{barsbey2021heavy}
Melih Barsbey, Milad Sefidgaran, Murat~A Erdogdu, Gael Richard, and Umut
  Simsekli.
\newblock Heavy tails in sgd and compressibility of overparametrized neural
  networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 29364--29378, 2021.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{International conference on machine learning}, pages
  1613--1622. PMLR, 2015.

\bibitem[Fan et~al.(2020)Fan, Stock, Graham, Grave, Gribonval, Jegou, and
  Joulin]{fan2020training}
Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, R{\'e}mi Gribonval,
  Herve Jegou, and Armand Joulin.
\newblock Training with quantization noise for extreme model compression.
\newblock \emph{arXiv preprint arXiv:2004.07320}, 2020.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Fortuin(2022)]{fortuin2022priors}
Vincent Fortuin.
\newblock Priors in bayesian deep learning: A review.
\newblock \emph{International Statistical Review}, 90\penalty0 (3):\penalty0
  563--591, 2022.

\bibitem[Gal and Ghahramani(2016)]{gal2016dropout}
Yarin Gal and Zoubin Ghahramani.
\newblock Dropout as a bayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{international conference on machine learning}, pages
  1050--1059. PMLR, 2016.

\bibitem[Gurbuzbalaban et~al.(2021)Gurbuzbalaban, Simsekli, and
  Zhu]{gurbuzbalaban2021heavy}
Mert Gurbuzbalaban, Umut Simsekli, and Lingjiong Zhu.
\newblock The heavy-tail phenomenon in sgd.
\newblock In \emph{International Conference on Machine Learning}, pages
  3964--3975. PMLR, 2021.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Han et~al.(2016)Han, Liu, Mao, Pu, Pedram, Horowitz, and
  Dally]{han2016eie}
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark~A Horowitz, and
  William~J Dally.
\newblock Eie: Efficient inference engine on compressed deep neural network.
\newblock \emph{ACM SIGARCH Computer Architecture News}, 44\penalty0
  (3):\penalty0 243--254, 2016.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Hern{\'a}ndez-Lobato and Adams(2015)]{hernandez2015probabilistic}
Jos{\'e}~Miguel Hern{\'a}ndez-Lobato and Ryan Adams.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1861--1869. PMLR, 2015.

\bibitem[Hodgkinson and Mahoney(2021)]{hodgkinson2021multiplicative}
Liam Hodgkinson and Michael Mahoney.
\newblock Multiplicative noise and heavy tails in stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  4262--4274. PMLR, 2021.

\bibitem[Horowitz(2014)]{horowitz20141}
Mark Horowitz.
\newblock 1.1 computing's energy problem (and what we can do about it).
\newblock In \emph{2014 IEEE International Solid-State Circuits Conference
  Digest of Technical Papers (ISSCC)}, pages 10--14. IEEE, 2014.

\bibitem[Huang et~al.(2017)Huang, Liu, Van Der~Maaten, and
  Weinberger]{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem[Jacob et~al.(2018)Jacob, Kligys, Chen, Zhu, Tang, Howard, Adam, and
  Kalenichenko]{jacob2018quantization}
Benoit Jacob, Skirmantas Kligys, Bo~Chen, Menglong Zhu, Matthew Tang, Andrew
  Howard, Hartwig Adam, and Dmitry Kalenichenko.
\newblock Quantization and training of neural networks for efficient
  integer-arithmetic-only inference.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2704--2713, 2018.

\bibitem[Jospin et~al.(2022)Jospin, Laga, Boussaid, Buntine, and
  Bennamoun]{jospin2022hands}
Laurent~Valentin Jospin, Hamid Laga, Farid Boussaid, Wray Buntine, and Mohammed
  Bennamoun.
\newblock Hands-on bayesian neural networksâ€”a tutorial for deep learning
  users.
\newblock \emph{IEEE Computational Intelligence Magazine}, 17\penalty0
  (2):\penalty0 29--48, 2022.

\bibitem[Jouppi et~al.(2017)Jouppi, Young, Patil, Patterson, Agrawal, Bajwa,
  Bates, Bhatia, Boden, Borchers, et~al.]{jouppi2017datacenter}
Norman~P Jouppi, Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal,
  Raminder Bajwa, Sarah Bates, Suresh Bhatia, Nan Boden, Al~Borchers, et~al.
\newblock In-datacenter performance analysis of a tensor processing unit.
\newblock In \emph{Proceedings of the 44th annual international symposium on
  computer architecture}, pages 1--12, 2017.

\bibitem[Jung et~al.(2019)Jung, Son, Lee, Son, Han, Kwak, Hwang, and
  Choi]{jung2019learning}
Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae-Joon Han, Youngjun
  Kwak, Sung~Ju Hwang, and Changkyu Choi.
\newblock Learning to quantize deep networks by optimizing quantization
  intervals with task loss.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 4350--4359, 2019.

\bibitem[Kaddour et~al.(2022)Kaddour, Liu, Silva, and Kusner]{kaddour2022flat}
Jean Kaddour, Linqing Liu, Ricardo Silva, and Matt~J Kusner.
\newblock When do flat minima optimizers work?
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 16577--16595, 2022.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Lee et~al.(2017)Lee, Miyashita, Chai, Murmann, and
  Wong]{lee2017lognet}
Edward~H Lee, Daisuke Miyashita, Elaina Chai, Boris Murmann, and S~Simon Wong.
\newblock Lognet: Energy-efficient neural networks using logarithmic
  computation.
\newblock In \emph{2017 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 5900--5904. IEEE, 2017.

\bibitem[Li et~al.(2019{\natexlab{a}})Li, Dong, and Wang]{YuhangLiXinDong2020}
Yuhang Li, Xin Dong, and Wei Wang.
\newblock Additive powers-of-two quantization: An efficient non-uniform
  discretization for neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Li et~al.(2019{\natexlab{b}})Li, Dong, and Wang]{li2019additive}
Yuhang Li, Xin Dong, and Wei Wang.
\newblock Additive powers-of-two quantization: An efficient non-uniform
  discretization for neural networks.
\newblock \emph{arXiv preprint arXiv:1909.13144}, 2019{\natexlab{b}}.

\bibitem[Li et~al.(2022)Li, Yang, Wang, and Cheng]{li2022q}
Zhexin Li, Tong Yang, Peisong Wang, and Jian Cheng.
\newblock Q-vit: Fully differentiable quantization for vision transformer.
\newblock \emph{arXiv preprint arXiv:2201.07703}, 2022.

\bibitem[MacKay(1992)]{mackay1992practical}
David~JC MacKay.
\newblock A practical bayesian framework for backpropagation networks.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Mitchell and Beauchamp(1988)]{mitchell1988bayesian}
Toby~J Mitchell and John~J Beauchamp.
\newblock Bayesian variable selection in linear regression.
\newblock \emph{Journal of the american statistical association}, 83\penalty0
  (404):\penalty0 1023--1032, 1988.

\bibitem[Oh et~al.(2021)Oh, Sim, Lee, and Lee]{oh2021automated}
Sangyun Oh, Hyeonuk Sim, Sugil Lee, and Jongeun Lee.
\newblock Automated log-scale quantization for low-cost deep neural networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 742--751, 2021.

\bibitem[Przewlocka-Rus and Kryjak(2023)]{przewlocka2023energy}
Dominika Przewlocka-Rus and Tomasz Kryjak.
\newblock Energy efficient hardware acceleration of neural networks with
  power-of-two quantisation.
\newblock In \emph{Computer Vision and Graphics: Proceedings of the
  International Conference on Computer Vision and Graphics ICCVG 2022}, pages
  225--236. Springer, 2023.

\bibitem[Stock et~al.(2020)Stock, Joulin, Gribonval, Graham, and
  J{\'e}gou]{Stock2019}
Pierre Stock, Armand Joulin, R{\'e}mi Gribonval, Benjamin Graham, and Herv{\'e}
  J{\'e}gou.
\newblock And the bit goes down: Revisiting the quantization of neural
  networks.
\newblock In \emph{ICLR 2020-Eighth International Conference on Learning
  Representations}, pages 1--11, 2020.

\bibitem[Subia-Waud and Dasmahapatra(2022)]{subia2022weight}
Christopher Subia-Waud and Srinandan Dasmahapatra.
\newblock Weight fixing networks.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XI}, pages 415--431.
  Springer, 2022.

\bibitem[Sze et~al.(2017)Sze, Chen, Yang, and Emer]{sze2017efficient}
Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel~S Emer.
\newblock Efficient processing of deep neural networks: A tutorial and survey.
\newblock \emph{Proceedings of the IEEE}, 105\penalty0 (12):\penalty0
  2295--2329, 2017.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International conference on machine learning}, pages
  10347--10357. PMLR, 2021.

\bibitem[Ullrich et~al.(2017)Ullrich, Meeds, and Welling]{ullrich2017soft}
Karen Ullrich, Edward Meeds, and Max Welling.
\newblock Soft weight-sharing for neural network compression.
\newblock \emph{arXiv preprint arXiv:1702.04008}, 2017.

\bibitem[Vogel et~al.(2019)Vogel, Raghunath, Guntoro, Van~Laerhoven, and
  Ascheid]{vogel2019bit}
Sebastian Vogel, Rajatha~B Raghunath, Andre Guntoro, Kristof Van~Laerhoven, and
  Gerd Ascheid.
\newblock Bit-shift-based accelerator for cnns with selectable accuracy and
  throughput.
\newblock In \emph{2019 22nd Euromicro Conference on Digital System Design
  (DSD)}, pages 663--667. IEEE, 2019.

\bibitem[Welling and Teh(2011)]{welling2011bayesian}
Max Welling and Yee~W Teh.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning (ICML-11)}, pages 681--688, 2011.

\bibitem[Wu et~al.(2018{\natexlab{a}})Wu, Wang, Wu, Wang, Veeraraghavan, and
  Lin]{Wu2018}
Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, and Yingyan
  Lin.
\newblock {Deep $\kappa$-means: Re-training and parameter sharing with harder
  cluster assignments for compressing deep convolutions}.
\newblock \emph{35th Int. Conf. Mach. Learn. ICML 2018}, 12:\penalty0
  8523--8532, 2018{\natexlab{a}}.

\bibitem[Wu et~al.(2018{\natexlab{b}})Wu, Wang, Wu, Wang, Veeraraghavan, and
  Lin]{wu2018deep}
Junru Wu, Yue Wang, Zhenyu Wu, Zhangyang Wang, Ashok Veeraraghavan, and Yingyan
  Lin.
\newblock Deep k-means: Re-training and parameter sharing with harder cluster
  assignments for compressing deep convolutions.
\newblock In \emph{International Conference on Machine Learning}, pages
  5363--5372. PMLR, 2018{\natexlab{b}}.

\bibitem[Yamamoto(2021)]{Yamamoto2021}
Kohei Yamamoto.
\newblock Learnable companding quantization for accurate low-bit neural
  networks.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 5029--5038, 2021.

\bibitem[Zhou et~al.(2017)Zhou, Yao, Guo, Xu, and Chen]{zhou2017}
Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, and Yurong Chen.
\newblock Incremental network quantization: Towards lossless cnns with
  low-precision weights.
\newblock \emph{arXiv preprint arXiv:1702.03044}, 2017.

\bibitem[Zhou et~al.(2016)Zhou, Wu, Ni, Zhou, Wen, and Zou]{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock Dorefa-net: Training low bitwidth convolutional neural networks with
  low bitwidth gradients.
\newblock \emph{arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
