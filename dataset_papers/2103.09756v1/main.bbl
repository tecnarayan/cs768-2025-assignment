\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2015)Abdolmaleki, Lioutikov, Peters, Lau, Reis, and
  Neumann]{abdolmaleki2015model}
Abdolmaleki, A., Lioutikov, R., Peters, J.~R., Lau, N., Reis, L.~P., and
  Neumann, G.
\newblock Model-based relative entropy stochastic search.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3537--3545, 2015.

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Munos, R., Heess, N., and
  Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock \emph{arXiv preprint arXiv:1806.06920}, 2018.

\bibitem[Allen-Zhu \& Orecchia(2014)Allen-Zhu and Orecchia]{allen2014linear}
Allen-Zhu, Z. and Orecchia, L.
\newblock Linear coupling: An ultimate unification of gradient and mirror
  descent.
\newblock \emph{arXiv preprint arXiv:1407.1537}, 2014.

\bibitem[Bas-Serrano \& Neu(2019)Bas-Serrano and Neu]{bas2019faster}
Bas-Serrano, J. and Neu, G.
\newblock Faster saddle-point optimization for solving large-scale markov
  decision processes.
\newblock \emph{arXiv preprint arXiv:1909.10904}, 2019.

\bibitem[Bas-Serrano et~al.(2020)Bas-Serrano, Curi, Krause, and
  Neu]{basserrano2020logistic}
Bas-Serrano, J., Curi, S., Krause, A., and Neu, G.
\newblock Logistic $q$-learning, 2020.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{beck2003mirror}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Belousov \& Peters(2017)Belousov and Peters]{belousov2017f}
Belousov, B. and Peters, J.
\newblock f-divergence constrained policy improvement.
\newblock \emph{arXiv preprint arXiv:1801.00056}, 2017.

\bibitem[Boularias et~al.(2011)Boularias, Kober, and
  Peters]{boularias2011relative}
Boularias, A., Kober, J., and Peters, J.
\newblock Relative entropy inverse reinforcement learning.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  182--189, 2011.

\bibitem[Chen \& Wang(2016)Chen and Wang]{chen2016stochastic}
Chen, Y. and Wang, M.
\newblock Stochastic primal-dual methods and sample complexity of reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1612.02516}, 2016.

\bibitem[Chen et~al.(2018)Chen, Li, and Wang]{chen2018scalable}
Chen, Y., Li, L., and Wang, M.
\newblock Scalable bilinear $\pi$ learning using state and action features.
\newblock \emph{arXiv preprint arXiv:1804.10328}, 2018.

\bibitem[Cheng et~al.(2020)Cheng, Combes, Boots, and
  Gordon]{cheng2020reduction}
Cheng, C.-A., Combes, R.~T., Boots, B., and Gordon, G.
\newblock A reduction from reinforcement learning to no-regret online learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3514--3524. PMLR, 2020.

\bibitem[Daniel et~al.(2012)Daniel, Neumann, and
  Peters]{daniel2012hierarchical}
Daniel, C., Neumann, G., and Peters, J.
\newblock Hierarchical relative entropy policy search.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  273--281,
  2012.

\bibitem[Fox et~al.(2017)Fox, Pakman, and Tishby]{fox2017taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates, 2017.

\bibitem[Geist et~al.(2019)Geist, Scherrer, and Pietquin]{geist2019theory}
Geist, M., Scherrer, B., and Pietquin, O.
\newblock A theory of regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1901.11275}, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Hartikainen, Tucker, Ha, Tan,
  Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018.

\bibitem[Howard et~al.(2018)Howard, Ramdas, McAuliffe, and
  Sekhon]{howard2018uniform}
Howard, S.~R., Ramdas, A., McAuliffe, J., and Sekhon, J.
\newblock Uniform, nonparametric, non-asymptotic confidence sequences.
\newblock \emph{arXiv preprint arXiv:1810.08240}, 2018.

\bibitem[Jin \& Sidford(2020)Jin and Sidford]{jin2020efficiently}
Jin, Y. and Sidford, A.
\newblock Efficiently solving mdps with stochastic mirror descent.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4890--4900. PMLR, 2020.

\bibitem[Kostrikov et~al.(2019)Kostrikov, Nachum, and
  Tompson]{kostrikov2019imitation}
Kostrikov, I., Nachum, O., and Tompson, J.
\newblock Imitation learning via off-policy distribution matching, 2019.

\bibitem[Nachum \& Dai(2020)Nachum and Dai]{nachum2020reinforcement}
Nachum, O. and Dai, B.
\newblock Reinforcement learning via fenchel-rockafellar duality.
\newblock \emph{arXiv preprint arXiv:2001.01866}, 2020.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2775--2785, 2017.

\bibitem[Nachum et~al.(2019)Nachum, Dai, Kostrikov, Chow, Li, and
  Schuurmans]{nachum2019algaedice}
Nachum, O., Dai, B., Kostrikov, I., Chow, Y., Li, L., and Schuurmans, D.
\newblock Algaedice: Policy gradient from arbitrary experience, 2019.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized markov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Peters et~al.(2010)Peters, M{\"u}lling, and Altun]{peters2010relative}
Peters, J., M{\"u}lling, K., and Altun, Y.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI}, volume~10, pp.\  1607--1612. Atlanta, 2010.

\bibitem[Puterman(2014)]{puterman2014markov}
Puterman, M.~L.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015.

\bibitem[Vieillard et~al.(2020)Vieillard, Kozuno, Scherrer, Pietquin, Munos,
  and Geist]{vieillard2020leverage}
Vieillard, N., Kozuno, T., Scherrer, B., Pietquin, O., Munos, R., and Geist, M.
\newblock Leverage the average: an analysis of regularization in rl.
\newblock \emph{arXiv preprint arXiv:2003.14089}, 2020.

\bibitem[Wang(2017{\natexlab{a}})]{wang2017primal}
Wang, M.
\newblock Primal-dual $\pi$ learning: Sample complexity and sublinear run time
  for ergodic markov decision problems.
\newblock \emph{arXiv preprint arXiv:1710.06100}, 2017{\natexlab{a}}.

\bibitem[Wang(2017{\natexlab{b}})]{wang2017randomized}
Wang, M.
\newblock Randomized linear programming solves the discounted markov decision
  problem in nearly-linear (sometimes sublinear) running time.
\newblock \emph{arXiv preprint arXiv:1704.01869}, 2017{\natexlab{b}}.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1583--1591, 2013.

\end{thebibliography}
