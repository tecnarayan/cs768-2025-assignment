\begin{thebibliography}{10}

\bibitem{advani2017high}
Madhu~S Advani and Andrew~M Saxe.
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em arXiv preprint arXiv:1710.03667}, 2017.

\bibitem{Baity18}
Marco Baity-Jesi, Levent Sagun, Mario Geiger, Stefano Spigler, Gerard~Ben
  Arous, Chiara Cammarota, Yann LeCun, Matthieu Wyart, and Giulio Biroli.
\newblock Comparing dynamics: Deep neural networks versus glassy systems.
\newblock In {\em International Conference on Machine Learning}, volume~80,
  pages 314--323, Stockholm Sweden, 10--15 Jul 2018.

\bibitem{bass1988uniqueness}
R.~F. Bass.
\newblock Uniqueness in law for pure jump {M}arkov processes.
\newblock {\em Probability Theory and Related Fields}, 79(2):271--287, 1988.

\bibitem{bottou2010large}
{L{\'e}on} Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Physica-Verlag
  HD, 2010.

\bibitem{bottou2008tradeoffs}
L{\'e}on Bottou and Olivier Bousquet.
\newblock The tradeoffs of large scale learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  161--168, 2008.

\bibitem{bovier2004metastability}
A.~Bovier, M.~Eckhoff, V.~Gayrard, and M.~Klein.
\newblock Metastability in reversible diffusion processes i: Sharp asymptotics
  for capacities and exit times.
\newblock {\em Journal of the European Mathematical Society}, 6(4):399--424,
  2004.

\bibitem{bovier2005metastability}
Anton Bovier, V{\'e}ronique Gayrard, and Markus Klein.
\newblock Metastability in reversible diffusion processes ii: Precise
  asymptotics for small eigenvalues.
\newblock {\em Journal of the European Mathematical Society}, 7(1):69--99,
  2005.

\bibitem{entropy-sgd}
P.~Chaudhari, Anna Choromanska, S.~Soatto, Yann LeCun, C.~Baldassi, C.~Borgs,
  J.~Chayes, Levent Sagun, and R.~Zecchina.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2017.

\bibitem{chaudhari2018stochastic}
P.~Chaudhari and S.~Soatto.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{Day83}
Martin~V. Day.
\newblock On the exponential exit law in the small parameter exit problem.
\newblock {\em Stochastics}, 8(4):297--323, 1983.

\bibitem{de1998comparison}
L.~De~Haan and L.~Peng.
\newblock Comparison of tail index estimators.
\newblock {\em Statistica Neerlandica}, 52(1):60--70, 1998.

\bibitem{dekkers1989moment}
A.~L.~M. Dekkers, J.~H.~J. Einmahl, and L.~De~Haan.
\newblock A moment estimator for the index of an extreme-value distribution.
\newblock {\em The Annals of Statistics}, pages 1833--1855, 1989.

\bibitem{duan}
J.~Duan.
\newblock {\em An Introduction to Stochastic Dynamics}.
\newblock Cambridge University Press, New York, 2015.

\bibitem{durmus2015non}
A.~Durmus and E.~Moulines.
\newblock Non-asymptotic convergence analysis for the unadjusted {Langevin}
  algorithm.
\newblock {\em arXiv preprint arXiv:1507.05021}, 2015.

\bibitem{fischer2010history}
Hans Fischer.
\newblock {\em A history of the central limit theorem: From classical to modern
  probability theory}.
\newblock Springer Science \& Business Media, 2010.

\bibitem{freidlin1998random}
M.~I. Freidlin and A.~D. Wentzell.
\newblock Random perturbations.
\newblock In {\em Random perturbations of dynamical systems}, pages 15--43.
  Springer, 1998.

\bibitem{Geiger18}
Mario Geiger, Stefano Spigler, {St{\'e}phane} d'Ascoli, Levent Sagun, Marco
  Baity-Jesi, Giulio Biroli, and Matthieu Wyart.
\newblock The jamming transition as a paradigm to understand the loss landscape
  of deep neural networks.
\newblock {\em arXiv preprint arXiv:1809.09349}, 2018.

\bibitem{hill1975simple}
B.~M. Hill.
\newblock A simple general approach to inference about the tail of a
  distribution.
\newblock {\em The Annals of Statistics}, pages 1163--1174, 1975.

\bibitem{Hinton12}
Geoffrey Hinton, Li~Deng, Dong Yu, George~E Dahl, Abdel-rahman Mohamed, Navdeep
  Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara~N Sainath,
  et~al.
\newblock Deep neural networks for acoustic modeling in speech recognition: The
  shared views of four research groups.
\newblock {\em IEEE Signal processing magazine}, 29(6):82--97, 2012.

\bibitem{hochreiter1997flat}
Sepp Hochreiter and {J{\"u}rgen} Schmidhuber.
\newblock Flat minima.
\newblock {\em Neural Computation}, 9(1):1--42, 1997.

\bibitem{hoffer2017train}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Train longer, generalize better: closing the generalization gap in
  large batch training of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1731--1741, 2017.

\bibitem{hu2017diffusion}
W.~Hu, C.~J. Li, L.~Li, and J.-G. Liu.
\newblock On the diffusion approximation of nonconvex stochastic gradient
  descent.
\newblock {\em arXiv preprint arXiv:1705.07562}, 2017.

\bibitem{imkeller2010first}
P.~Imkeller, I.~Pavlyukevich, and M.~Stauch.
\newblock First exit times of non-linear dynamical systems in rd perturbed by
  multifractal {L}\'{e}vy noise.
\newblock {\em Journal of Statistical Physics}, 141(1):94--119, 2010.

\bibitem{Imkeller2010}
P.~Imkeller, I.~Pavlyukevich, and T.~Wetzel.
\newblock The hierarchy of exit times of {L}{\'e}vy-driven {L}angevin
  equations.
\newblock {\em The European Physical Journal Special Topics}, 191(1):211--222,
  Dec 2010.

\bibitem{jastrzkebski2017three}
S.~Jastrzebski, Z.~Kenton, D.~Arpit, N.~Ballas, A.~Fischer, Y.~Bengio, and
  A.~Storkey.
\newblock Three factors influencing minima in sgd.
\newblock {\em arXiv preprint arXiv:1711.04623}, 2017.

\bibitem{jourdain2012levy}
B.~Jourdain, S.~M{\'e}l{\'e}ard, and W.~A. Woyczynski.
\newblock L{\'e}vy flights in evolutionary ecology.
\newblock {\em Journal of Mathematical Biology}, 65(4):677--707, 2012.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem{Krizhevsky12}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1097--1105, 2012.

\bibitem{kuhwald2016bistable}
I.~Kuhwald and I.~Pavlyukevich.
\newblock Bistable behaviour of a jump-diffusion driven by a periodic
  stable-like additive process.
\newblock {\em Discrete \& Continuous Dynamical Systems-Series B}, 21(9), 2016.

\bibitem{lamberton2003recursive}
D.~Lamberton and G.~Pages.
\newblock Recursive computation of the invariant distribution of a diffusion:
  the case of a weakly mean reverting drift.
\newblock {\em Stochastics and dynamics}, 3(04):435--451, 2003.

\bibitem{hinton-nature}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521:436 EP --, 05 2015.

\bibitem{paul1937theorie}
P.~L{\'e}vy.
\newblock Th{\'e}orie de l'addition des variables al{\'e}atoires.
\newblock {\em Gauthiers-Villars, Paris}, 1937.

\bibitem{pmlr-v70-li17f}
Q.~Li, C.~Tai, and W.~E.
\newblock Stochastic modified equations and adaptive stochastic gradient
  algorithms.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, pages 2101--2110, 06--11 Aug 2017.

\bibitem{liutkus2015generalized}
A.~Liutkus and R.~Badeau.
\newblock Generalized {W}iener filtering with fractional power spectrograms.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, pages 266--270. IEEE, 2015.

\bibitem{mandelbrot2013fractals}
B.~B. Mandelbrot.
\newblock {\em Fractals and Scaling in Finance: Discontinuity, Concentration,
  Risk. Selecta Volume E}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{mandt2016variational}
S.~Mandt, M.~Hoffman, and D.~Blei.
\newblock A variational analysis of stochastic gradient algorithms.
\newblock In {\em International Conference on Machine Learning}, pages
  354--363, 2016.

\bibitem{masters2018revisiting}
Dominic Masters and Carlo Luschi.
\newblock Revisiting small batch training for deep neural networks.
\newblock {\em arXiv preprint arXiv:1804.07612}, 2018.

\bibitem{mittnik1996tail}
S.~Mittnik and S.~T. Rachev.
\newblock Tail estimation of the stable index $\alpha$.
\newblock {\em Applied Mathematics Letters}, 9(3):53--56, 1996.

\bibitem{mohammadi2015estimating}
M.~Mohammadi, A.~Mohammadpour, and H.~Ogata.
\newblock On estimating the tail index and the spectral measure of multivariate
  $\alpha$-stable distributions.
\newblock {\em Metrika}, 78(5):549--561, 2015.

\bibitem{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017.

\bibitem{paulauskas2011once}
V.~Paulauskas and M.~Vai{\v{c}}iulis.
\newblock Once more on comparison of tail index estimators.
\newblock {\em arXiv preprint arXiv:1104.1242}, 2011.

\bibitem{pavlyukevich2007cooling}
Ilya Pavlyukevich.
\newblock Cooling down l{\'e}vy flights.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  40(41):12299, 2007.

\bibitem{pickands1975statistical}
J.~Pickands.
\newblock Statistical inference using extreme order statistics.
\newblock {\em The Annals of Statistics}, 3(1):119--131, 1975.

\bibitem{raginsky17a}
M.~Raginsky, A.~Rakhlin, and M.~Telgarsky.
\newblock Non-convex learning via stochastic gradient {L}angevin dynamics: a
  nonasymptotic analysis.
\newblock In {\em Proceedings of the 2017 Conference on Learning Theory},
  volume~65, pages 1674--1703, 2017.

\bibitem{Roberts03}
G.~O. Roberts and O.~Stramer.
\newblock {Langevin Diffusions and Metropolis-Hastings Algorithms}.
\newblock {\em Methodology and Computing in Applied Probability},
  4(4):337--357, December 2002.

\bibitem{sagun2017empirical}
Levent Sagun, Utku Evci, V.~{U\u{g}ur} {G\"uney}, Yann Dauphin, and {L\'eon}
  Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em ICLR 2018 Workshop Contribution, arXiv:1706.04454}, 2017.

\bibitem{sagun2014explorations}
Levent Sagun, V.~{U\u{g}ur} {G\"uney}, {G{\'{e}}rard} {Ben Arous}, and Yann
  LeCun.
\newblock Explorations on high dimensional landscapes.
\newblock {\em International Conference on Learning Representations Workshop
  Contribution, arXiv:1412.6615}, 2015.

\bibitem{Samorodnitsky2003}
G.~Samorodnitsky and M.~Grigoriu.
\newblock Tails of solutions of certain nonlinear stochastic differential
  equations driven by heavy tailed {L}\'{e}vy motions.
\newblock {\em Stochastic Processes and their Applications}, 105(1):69 -- 97,
  2003.

\bibitem{smith2017don}
Samuel~L Smith, Pieter-Jan Kindermans, Chris Ying, and Quoc~V Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock {\em arXiv preprint arXiv:1711.00489}, 2017.

\bibitem{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In {\em International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem{tzen2018local}
B.~Tzen, T.~Liang, and M.~Raginsky.
\newblock Local optimality and generalization guarantees for the langevin
  algorithm via empirical metastability.
\newblock In {\em Proceedings of the 2018 Conference on Learning Theory}, 2018.

\bibitem{weeks1995observation}
E.~R. Weeks, T.~H. Solomon, J.~S. Urbach, and H.~L. Swinney.
\newblock Observation of anomalous diffusion and {L}{\'e}vy flights.
\newblock In {\em L{\'e}vy flights and related topics in physics}, pages
  51--71. Springer, 1995.

\bibitem{woyczynski2001levy}
W.~A. Woyczy{\'n}ski.
\newblock L{\'e}vy processes in the physical sciences.
\newblock In {\em L{\'e}vy processes}, pages 241--266. Springer, 2001.

\bibitem{wu2018sgd}
Lei Wu, Chao Ma, and E~Weinan.
\newblock How sgd selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8289--8298, 2018.

\bibitem{xing2018walk}
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio.
\newblock A walk with sgd.
\newblock {\em arXiv preprint arXiv:1802.08770}, 2018.

\bibitem{xu2018global}
Pan Xu, Jinghui Chen, Difan Zou, and Quanquan Gu.
\newblock Global convergence of langevin dynamics based algorithms for
  nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3125--3136, 2018.

\bibitem{yaida2018fluctuationdissipation}
S.~Yaida.
\newblock Fluctuation-dissipation relations for stochastic gradient descent.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{yanovsky2000levy}
V.~V. Yanovsky, A.~V. Chechkin, D.~Schertzer, and A.~V. Tur.
\newblock L{\'e}vy anomalous diffusion and fractional {F}okker--{P}lanck
  equation.
\newblock {\em Physica A: Statistical Mechanics and its Applications},
  282(1):13--34, 2000.

\bibitem{Zhang16}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em International Conference on Learning Representations}, 2017.

\bibitem{zhang17b}
Y.~Zhang, P.~Liang, and M.~Charikar.
\newblock A hitting time analysis of stochastic gradient langevin dynamics.
\newblock In {\em Proceedings of the 2017 Conference on Learning Theory},
  volume~65, pages 1980--2022, 2017.

\bibitem{zhu2018anisotropic}
Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, and Jinwen Ma.
\newblock The anisotropic noise in stochastic gradient descent: Its behavior of
  escaping from minima and regularization effects.
\newblock {\em arXiv preprint arXiv:1803.00195}, 2018.

\end{thebibliography}
