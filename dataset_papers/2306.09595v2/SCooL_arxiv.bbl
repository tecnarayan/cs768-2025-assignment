\begin{thebibliography}{53}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Lian et~al.(2017)Lian, Zhang, Zhang, Hsieh, Zhang, and
  Liu]{NIPS2017_f7552665}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/f75526659f31040afeb61cb7133e4e6d-Paper.pdf}.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise~Aguera
  y~Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial intelligence and statistics}, pages 1273--1282.
  PMLR, 2017.

\bibitem[Hsieh et~al.(2020)Hsieh, Phanishayee, Mutlu, and
  Gibbons]{hsieh2020non}
Kevin Hsieh, Amar Phanishayee, Onur Mutlu, and Phillip Gibbons.
\newblock The non-iid data quagmire of decentralized machine learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  4387--4398. PMLR, 2020.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Sai~Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebastian
  Stich, and Ananda~Theertha Suresh.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5132--5143. PMLR, 2020.

\bibitem[Lin et~al.(2020)Lin, Kong, Stich, and Jaggi]{NEURIPS2020_18df51b9}
Tao Lin, Lingjing Kong, Sebastian~U Stich, and Martin Jaggi.
\newblock Ensemble distillation for robust model fusion in federated learning.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 2351--2363. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/18df51b97ccd68128e994804f3eccc87-Paper.pdf}.

\bibitem[Fraboni et~al.(2021)Fraboni, Vidal, Kameni, and
  Lorenzi]{pmlr-v139-fraboni21a}
Yann Fraboni, Richard Vidal, Laetitia Kameni, and Marco Lorenzi.
\newblock Clustered sampling: Low-variance and improved representativity for
  clients selection in federated learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 3407--3416. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/fraboni21a.html}.

\bibitem[Chen and Chao(2021)]{chen2021fedbe}
Hong-You Chen and Wei-Lun Chao.
\newblock Fed{\{}be{\}}: Making bayesian model ensemble applicable to federated
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=dgtpE6gKjHn}.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{Wang2020Federated}
Hongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris Papailiopoulos, and
  Yasaman Khazaeni.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BkluqlSFDS}.

\bibitem[Balakrishnan et~al.(2022)Balakrishnan, Li, Zhou, Himayat, Smith, and
  Bilmes]{balakrishnan2022diverse}
Ravikumar Balakrishnan, Tian Li, Tianyi Zhou, Nageen Himayat, Virginia Smith,
  and Jeff Bilmes.
\newblock Diverse client selection for federated learning via submodular
  maximization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nwKXyFvaUm}.

\bibitem[Acar et~al.(2021)Acar, Zhao, Matas, Mattina, Whatmough, and
  Saligrama]{acar2021federated}
Durmus Alp~Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough,
  and Venkatesh Saligrama.
\newblock Federated learning based on dynamic regularization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=B7v4QMR6Z9w}.

\bibitem[Li et~al.(2018)Li, Sahu, Zaheer, Sanjabi, Talwalkar, and
  Smith]{li2018federated}
Tian Li, Anit~Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and
  Virginia Smith.
\newblock Federated optimization in heterogeneous networks.
\newblock \emph{arXiv preprint arXiv:1812.06127}, 2018.

\bibitem[Xu et~al.(2022)Xu, Hong, Huang, and Jiang]{xu2022acceleration}
Chencheng Xu, Zhiwei Hong, Minlie Huang, and Tao Jiang.
\newblock Acceleration of federated learning with alleviated forgetting in
  local training.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=541PxiEKN3F}.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Hu, Beirami, and Smith]{li2021ditto}
Tian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith.
\newblock Ditto: Fair and robust federated learning through personalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  6357--6368. PMLR, 2021{\natexlab{a}}.

\bibitem[T.~Dinh et~al.(2020)T.~Dinh, Tran, and Nguyen]{NEURIPS2020_f4f1f13c}
Canh T.~Dinh, Nguyen Tran, and Josh Nguyen.
\newblock Personalized federated learning with moreau envelopes.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 21394--21405. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/f4f1f13c8289ac1b1ee0ff176b56fc60-Paper.pdf}.

\bibitem[Sattler et~al.(2020)Sattler, M{\"u}ller, and
  Samek]{sattler2020clustered}
Felix Sattler, Klaus-Robert M{\"u}ller, and Wojciech Samek.
\newblock Clustered federated learning: Model-agnostic distributed multitask
  optimization under privacy constraints.
\newblock \emph{IEEE transactions on neural networks and learning systems},
  2020.

\bibitem[Ghosh et~al.(2020)Ghosh, Chung, Yin, and
  Ramchandran]{ghosh2020efficient}
Avishek Ghosh, Jichan Chung, Dong Yin, and Kannan Ramchandran.
\newblock An efficient framework for clustered federated learning.
\newblock \emph{arXiv preprint arXiv:2006.04088}, 2020.

\bibitem[Xie et~al.(2021)Xie, Long, Shen, Zhou, Wang, Jiang, and
  Zhang]{xie2021multi}
Ming Xie, Guodong Long, Tao Shen, Tianyi Zhou, Xianzhi Wang, Jing Jiang, and
  Chengqi Zhang.
\newblock Multi-center federated learning.
\newblock \emph{arXiv preprint arXiv:2108.08647}, 2021.

\bibitem[Long et~al.(2022)Long, Xie, Shen, Zhou, Wang, and
  Jiang]{Multi-Center_Federated_Learning}
Guodong Long, Ming Xie, Tao Shen, Tianyi Zhou, Xianzhi Wang, and Jing Jiang.
\newblock Multi-center federated learning: clients clustering for better
  personalization.
\newblock \emph{World Wide Web Journal (Springer)}, 2022.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, JIANG, Zhang, Kamp, and
  Dou]{li2021fedbn}
Xiaoxiao Li, Meirui JIANG, Xiaofei Zhang, Michael Kamp, and Qi~Dou.
\newblock Fed{BN}: Federated learning on non-{IID} features via local batch
  normalization.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=6YEQUn0QICG}.

\bibitem[Liang et~al.(2020)Liang, Liu, Ziyin, Allen, Auerbach, Brent,
  Salakhutdinov, and Morency]{liang2020think}
Paul~Pu Liang, Terrance Liu, Liu Ziyin, Nicholas~B Allen, Randy~P Auerbach,
  David Brent, Ruslan Salakhutdinov, and Louis-Philippe Morency.
\newblock Think locally, act globally: Federated learning with local and global
  representations.
\newblock \emph{arXiv preprint arXiv:2001.01523}, 2020.

\bibitem[Collins et~al.(2021)Collins, Hassani, Mokhtari, and
  Shakkottai]{pmlr-v139-collins21a}
Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock Exploiting shared representations for personalized federated
  learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 2089--2099. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/collins21a.html}.

\bibitem[Oh et~al.(2022)Oh, Kim, and Yun]{oh2022fedbabu}
Jaehoon Oh, SangMook Kim, and Se-Young Yun.
\newblock Fed{BABU}: Toward enhanced representation for federated image
  classification.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=HuaYQfggn5u}.

\bibitem[Zhang et~al.(2023)Zhang, Long, Zhou, Yan, Zhang, Zhang, and
  Yang]{DualPersonalization}
Chunxu Zhang, Guodong Long, Tianyi Zhou, Peng Yan, Zijian Zhang, Chengqi Zhang,
  and Bo~Yang.
\newblock Dual personalization on federated recommendation.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2023.

\bibitem[Zhu et~al.(2021)Zhu, Hong, and Zhou]{zhu2021data}
Zhuangdi Zhu, Junyuan Hong, and Jiayu Zhou.
\newblock Data-free knowledge distillation for heterogeneous federated
  learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 12878--12889. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/zhu21b.html}.

\bibitem[Afonin and Karimireddy(2022)]{afonin2022towards}
Andrei Afonin and Sai~Praneeth Karimireddy.
\newblock Towards model agnostic federated learning using knowledge
  distillation.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=lQI_mZjvBxj}.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020personalized}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Personalized federated learning with theoretical guarantees: A
  model-agnostic meta-learning approach.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Shamsian et~al.(2021)Shamsian, Navon, Fetaya, and
  Chechik]{pmlr-v139-shamsian21a}
Aviv Shamsian, Aviv Navon, Ethan Fetaya, and Gal Chechik.
\newblock Personalized federated learning using hypernetworks.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 9489--9502. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/shamsian21a.html}.

\bibitem[Tan et~al.(2022{\natexlab{a}})Tan, Long, Liu, Zhou, Lu, Jiang, and
  Zhang]{tan2022fedproto}
Yue Tan, Guodong Long, Lu~Liu, Tianyi Zhou, Qinghua Lu, Jing Jiang, and Chengqi
  Zhang.
\newblock Fedproto: Federated prototype learning across heterogeneous clients.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, volume~1,
  2022{\natexlab{a}}.

\bibitem[Tan et~al.(2022{\natexlab{b}})Tan, Long, Ma, Liu, Zhou, and
  Jiang]{tan2022federated}
Yue Tan, Guodong Long, Jie Ma, Lu~Liu, Tianyi Zhou, and Jing Jiang.
\newblock Federated learning from pre-trained models: A contrastive learning
  approach.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=mhQLcMjWw75}.

\bibitem[Dai et~al.(2022)Dai, Shen, He, Tian, and
  Tao]{DBLP:conf/icml/Dai0H0T22}
Rong Dai, Li~Shen, Fengxiang He, Xinmei Tian, and Dacheng Tao.
\newblock Dispfl: Towards communication-efficient personalized federated
  learning via decentralized sparse training.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, \emph{International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pages 4587--4604. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/dai22b.html}.

\bibitem[Chen et~al.(2022)Chen, Long, Wu, Zhou, and
  Jiang]{Personalized_Federated_Learning_With_Structural_Information}
Fengwen Chen, Guodong Long, Zonghan Wu, Tianyi Zhou, and Jing Jiang.
\newblock Personalized federated learning with structural information.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2022.

\bibitem[Blot et~al.(2016)Blot, Picard, Cord, and Thome]{blot2016gossip}
Michael Blot, David Picard, Matthieu Cord, and Nicolas Thome.
\newblock Gossip training for deep learning.
\newblock \emph{arXiv preprint arXiv:1611.09726}, 2016.

\bibitem[Jiang et~al.(2017)Jiang, Balu, Hegde, and Sarkar]{NIPS2017_a74c3bae}
Zhanhong Jiang, Aditya Balu, Chinmay Hegde, and Soumik Sarkar.
\newblock Collaborative deep learning in fixed topology networks.
\newblock In I.~Guyon, U.~V. Luxburg, S.~Bengio, H.~Wallach, R.~Fergus,
  S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2017/file/a74c3bae3e13616104c1b25f9da1f11f-Paper.pdf}.

\bibitem[Lin et~al.(2021)Lin, Karimireddy, Stich, and Jaggi]{lin2021quasi}
Tao Lin, Sai~Praneeth Karimireddy, Sebastian Stich, and Martin Jaggi.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 6654--6665. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/lin21c.html}.

\bibitem[Khawatmi et~al.(2017)Khawatmi, Sayed, and
  Zoubir]{khawatmi2017decentralized}
Sahar Khawatmi, Ali~H Sayed, and Abdelhak~M Zoubir.
\newblock Decentralized clustering and linking by networked agents.
\newblock \emph{IEEE Transactions on Signal Processing}, 65\penalty0
  (13):\penalty0 3526--3537, 2017.

\bibitem[Esfandiari et~al.(2021)Esfandiari, Tan, Jiang, Balu, Herron, Hegde,
  and Sarkar]{esfandiari2021cross}
Yasaman Esfandiari, Sin~Yong Tan, Zhanhong Jiang, Aditya Balu, Ethan Herron,
  Chinmay Hegde, and Soumik Sarkar.
\newblock Cross-gradient aggregation for decentralized learning from non-iid
  data.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 3036--3046. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/esfandiari21a.html}.

\bibitem[Huang et~al.(2022)Huang, Sun, Zhu, Yan, and
  Xu]{DBLP:conf/icml/HuangSZYX22}
Yan Huang, Ying Sun, Zehan Zhu, Changzhi Yan, and Jinming Xu.
\newblock Tackling data heterogeneity: {A} new unified framework for
  decentralized {SGD} with sample-induced topology.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, \emph{International
  Conference on Machine Learning, {ICML} 2022, 17-23 July 2022, Baltimore,
  Maryland, {USA}}, volume 162 of \emph{Proceedings of Machine Learning
  Research}, pages 9310--9345. {PMLR}, 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/huang22i.html}.

\bibitem[Yuan et~al.(2022)Yuan, Huang, Chen, Zhang, Zhang, and
  Pan]{yuan2022revisiting}
Kun Yuan, Xinmeng Huang, Yiming Chen, Xiaohan Zhang, Yingya Zhang, and Pan Pan.
\newblock Revisiting optimal convergence rate for smooth and non-convex
  stochastic decentralized optimization.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=eHePKMLuNmy}.

\bibitem[Song et~al.(2022)Song, Li, Jin, Shi, Yan, Yin, and
  Yuan]{song2022communicationefficient}
Zhuoqing Song, Weijian Li, Kexin Jin, Lei Shi, Ming Yan, Wotao Yin, and Kun
  Yuan.
\newblock Communication-efficient topologies for decentralized learning with
  \$o(1)\$ consensus rate.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=AyiiHcRzTd}.

\bibitem[Vogels et~al.(2022)Vogels, Hendrikx, and Jaggi]{vogels2022beyond}
Thijs Vogels, Hadrien Hendrikx, and Martin Jaggi.
\newblock Beyond spectral gap: the role of the topology in decentralized
  learning.
\newblock In Alice~H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho,
  editors, \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=AQgmyyEWg8}.

\bibitem[Lu et~al.(2022)Lu, Cui, Squillante, Kingsbury, and
  Horesh]{lu2022decentralized}
Songtao Lu, Xiaodong Cui, Mark~S Squillante, Brian Kingsbury, and Lior Horesh.
\newblock Decentralized bilevel optimization for personalized client learning.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 5543--5547. IEEE, 2022.

\bibitem[Jordan et~al.(1999)Jordan, Ghahramani, Jaakkola, and
  Saul]{DBLP:journals/ml/JordanGJS99}
Michael~I. Jordan, Zoubin Ghahramani, Tommi~S. Jaakkola, and Lawrence~K. Saul.
\newblock An introduction to variational methods for graphical models.
\newblock \emph{Mach. Learn.}, 37\penalty0 (2):\penalty0 183--233, 1999.
\newblock \doi{10.1023/A:1007665907178}.
\newblock URL \url{https://doi.org/10.1023/A:1007665907178}.

\bibitem[Belkin et~al.(2006)Belkin, Niyogi, and Sindhwani]{belkin2006manifold}
Mikhail Belkin, Partha Niyogi, and Vikas Sindhwani.
\newblock Manifold regularization: A geometric framework for learning from
  labeled and unlabeled examples.
\newblock \emph{Journal of machine learning research}, 7\penalty0 (11), 2006.

\bibitem[Holland et~al.(1983)Holland, Laskey, and
  Leinhardt]{holland1983stochastic}
Paul~W Holland, Kathryn~Blackmond Laskey, and Samuel Leinhardt.
\newblock Stochastic blockmodels: First steps.
\newblock \emph{Social networks}, 5\penalty0 (2):\penalty0 109--137, 1983.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Ravi and Larochelle(2017)]{ravi2017optimization}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Zhang et~al.(2021)Zhang, Sapra, Fidler, Yeung, and
  Alvarez]{zhang2021personalized}
Michael Zhang, Karan Sapra, Sanja Fidler, Serena Yeung, and Jose~M. Alvarez.
\newblock Personalized federated learning with first order model optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=ehJqJQk9cw}.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  1126--1135. PMLR, 2017.

\bibitem[Ioffe and Szegedy(2015)]{pmlr-v37-ioffe15}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In Francis Bach and David Blei, editors, \emph{Proceedings of the
  32nd International Conference on Machine Learning}, volume~37 of
  \emph{Proceedings of Machine Learning Research}, pages 448--456, Lille,
  France, 07--09 Jul 2015. PMLR.
\newblock URL \url{https://proceedings.mlr.press/v37/ioffe15.html}.

\bibitem[Wu and He(2018)]{Wu_2018_ECCV}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, September 2018.

\bibitem[Li et~al.(2022)Li, Zhou, Tian, and Tao]{li2022learning}
Shuangtong Li, Tianyi Zhou, Xinmei Tian, and Dacheng Tao.
\newblock Learning to collaborate in decentralized learning of personalized
  models.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2022.

\bibitem[Zantedeschi et~al.(2020)Zantedeschi, Bellet, and
  Tommasi]{zantedeschi2020fully}
Valentina Zantedeschi, Aur{\'e}lien Bellet, and Marc Tommasi.
\newblock Fully decentralized joint learning of personalized models and
  collaboration graphs.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 864--874. PMLR, 2020.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\end{thebibliography}
