@inproceedings{jacot2018,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clément},
	title = {Neural tangent kernel: Convergence and generalization in neural networks},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	pages = {8571 -- 8580},
	year = {2018}
}

@inproceedings{du2019,
	author = {Du, Simon S. and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
	title = {Gradient descent provably optimizes over-parameterized neural networks},
	booktitle = {International Conference on Learning Representations (ICLR)},
	year = {2019}
}

@inproceedings{arora2019,
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
	title = {Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks.},
	booktitle = {International Conference on Machine Learning (ICML)},
	year = {2019}
}

@article{montanari2021,
	author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	title = {Linearized two-layers neural networks in high dimension},
	journal = {The Annals of Statistics},
	year = {2021},
	volume = {49},
	issue = {2},
	pages = {1029–1054}
}

@article{mei2022,
	author = {Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	title = {Generalization error of random features and kernel methods: hypercontractivity and kernel matrix concentration},
	journal = {Applied and Computational Harmonic Analysis, Special Issue on Harmonic Analysis and Machine Learning},
	year = {2022},
	volume = {59},
	pages = {3--84}
}

@misc{montanari2020,
	author = {Montanari, Andrea and Zhong, Yiqiao},
	title = {The Interpolation Phase Transition in Neural Networks: Memorization and Generalization under Lazy Training},
	publisher={arxiv},
	year={2020},
	url = {https://arxiv.org/abs/2007.12826},
}

@inproceedings{jin2017,
	author = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M. and Jordan, Michael I.},
	title = {How to Escape Saddle Points Efficiently},
	booktitle = {International Conference on Machine Learning (ICML)},
	year = {2017}
}

@article{jin2019,
	author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
	title = {On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points},
	journal = {Journal of the ACM},
	volume= {68},
	issue = {2},
	article = {11},
	year = {2021}
}

@inproceedings{bai2020,
	author = {Bai, Yu and Lee, Jason D.},
	title = {Beyond Linearization: On Quadratic and Higher-Order Approximation of Wide Neural Networks},
	booktitle = {International Conference on Learning Representations (ICLR)},
	year = {2020}
}

@inproceedings{ghosh2022,
	author = {Ghosh, Nikhil and Mei, Song and Yu, Bin},
	title = {The Three Stages of Learning Dynamics in High-dimensional Kernel Methods},
	booktitle = {International Conference on Learning Representations (ICLR)},
	year = {2022}
}

@article{bartlett2021,
	author = {Bartlett, Peter L. and Montanari, Andrea and Rakhlin, Alexander},
	title = {Deep Learning: A statistical viewpoint},
	journal = {Acta Numerica},
	year = {2021},
	volume = {30},
	pages = {87--201}
}

@inproceedings{hu2020,
	author = {Hu, Wei and Xiao, Lechao and Adlam, Ben and Pennington, Jeffrey},
	title = {The Surprising Simplicity of the Early-Time Learning Dynamics of Neural Networks},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2020}
}

@inproceedings{arora2019CNTK,
	author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
	title = {On Exact Computation with an Infinitely Wide Neural Net},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2019}
}

@inproceedings{chizat2018,
	author = {Chizat, Lénaïc and Oyallon, Edouard and Bach, Francis},
	title = {On Lazy Training in Differentiable Programming},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2019}
}

@article{soltanolkotabi2018,
	author = {Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D.},
	title = {Theoretical insights into the optimization landscape of over-parameterized shallow neural networks},
	journal = {IEEE Transactions on Information Theory},
	year = {2018},
	volume = {65},
	issue={2},
	pages = {742--769}
}

@book{vershynin,
	author = {Vershynin, Roman},
	title = {High-dimensional probability: An introduction with applications in data science},
	volume = {47},
	publisher = {Cambridge University Press},
	year={2018}
}

@book{wainwright,
	author = {Wainwright, Martin J.},
	title = {High-dimensional statistics: A non-asymptotic viewpoint},
	volume = {48},
	publisher = {Cambridge University Press},
	year={2019}
}

@article{daviskahan,
	author = {Davis, Chandler and Kahan, W. M.},
	title = {The Rotation of Eigenvectors by a Perturbation. III},
	journal = {SIAM Journal on Numerical Analysis},
	year = {1970},
	volume = {7},
	issue={1},
	pages = {1--46}
}

@inproceedings{leefinite2020,
 author = {Lee, Jaehoon and Schoenholz, Samuel and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Finite Versus Infinite Neural Networks: an Empirical Study},
 year = {2020}
}

@misc{regmatters2018,
  
  url = {https://arxiv.org/abs/1810.05369},
  
  author = {Wei, Colin and Lee, Jason D. and Liu, Qiang and Ma, Tengyu},
  
  
  title = {Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  
  publisher = {arXiv},
  
  year = {2018},

}


@InProceedings{woodworth2020,
  title = 	 {Kernel and Rich Regimes in Overparametrized Models},
  author =    {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle = 	 {Conference on Learning Theory},
  pages = 	 {3635--3673},
  year = 	 {2020},
  series = 	 {Proceedings of Machine Learning Research},
}


@InProceedings{malach2021,
  title = 	 {Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels},
  author =       {Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7379--7389},
  year = 	 {2021},
  series = 	 {Proceedings of Machine Learning Research},
}

@inproceedings{daniely2020,
	author = {Daniely, Amit and Malach, Eran},
	title = {Learning Parities with Neural Networks},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2020}
}

@inproceedings{yehudai2019,
 author = {Yehudai, Gilad and Shamir, Ohad},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {On the Power and Limitations of Random Features for Understanding Neural Networks},
 year = {2019}
}

@inproceedings{ghorbani2020,
 author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {When Do Neural Networks Outperform Kernel Methods?},
 year = {2020}
}


@InProceedings{li2020,
  title = 	 {Learning Over-Parametrized Two-Layer Neural Networks beyond NTK},
  author =       {Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R.},
  booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
  pages = 	 {2613--2682},
  year = 	 {2020},
  series = 	 {Proceedings of Machine Learning Research},
}

@inproceedings{zhu2019,
 author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {What Can ResNet Learn Efficiently, Going Beyond Kernels?},
 year = {2019}
}

@inproceedings{du2018b,
 author = {Du, Simon S. and Lee, Jason D. and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
 booktitle = {International Conference on Machine Learning},
 title = {Gradient Descent Finds Global Minima of Deep Neural Networks},
 year = {2019}
}

@inproceedings{zhu2019b,
 author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers},
 year = {2019}
}


@InProceedings{zhu2019c,
  title = 	 {A Convergence Theory for Deep Learning via Over-Parameterization},
  author =       {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle = 	 {International Conference on Machine Learning},
  pages = 	 {242--252},
  year = 	 {2019},
  series = 	 {Proceedings of Machine Learning Research},

}

@article{zhu2020,
  author    = {Zeyuan Allen{-}Zhu and
               Yuanzhi Li},
  title     = {Backward Feature Correction: How Deep Learning Performs Deep Learning},
  year      = {2020},
  url       = {https://arxiv.org/abs/2001.04413},
  eprinttype = {arXiv},
  eprint    = {2001.04413},
}

@inproceedings{leewide2019,
 author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
 year = {2019}
}

@inproceedings{li2018,
 author = {Li, Yuanzhi and Liang, Yingyu},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data},
 year = {2018}
}

@misc{zou2018,

  url = {https://arxiv.org/abs/1811.08888},
  
  author = {Zou, Difan and Cao, Yuan and Zhou, Dongruo and Gu, Quanquan},

  title = {Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks},
  
  publisher = {arXiv},
  
  year = {2018},

}

@inproceedings{cao2019,
 author = {Cao, Yuan and Gu, Quanquan},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks},
 year = {2019}
}

@inproceedings{ghorbani2019b,
 author = {Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Limitations of Lazy Training of Two-layers Neural Network},
 year = {2019}
}


@InProceedings{du2018quad,
  title = 	 {On the Power of Over-parametrization in Neural Networks with Quadratic Activation},
  author =       {Du, Simon S. and Lee, Jason D.},
  booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
  pages = 	 {1329--1338},
  year = 	 {2018},
  series = 	 {Proceedings of Machine Learning Research},
}


@InProceedings{huang2020,
  title = 	 {Dynamics of Deep Neural Networks and Neural Tangent Hierarchy},
  author =       {Huang, Jiaoyang and Yau, Horng-Tzer},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {4542--4551},
  year = 	 {2020},
  series = 	 {Proceedings of Machine Learning Research},
}

@inproceedings{chen2020,
 author = {Chen, Minshuo and Bai, Yu and Lee, Jason D. and Zhao, Tuo and Wang, Huan and Xiong, Caiming and Socher, Richard},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Towards Understanding Hierarchical Learning: Benefits of Neural Representations},
 year = {2020}
}

@inproceedings{ge2017,

  
  author = {Ge, Rong and Lee, Jason D. and Ma, Tengyu},
  
  booktitle= {International Conference on Learning Representations},
  
  title = {Learning One-hidden-layer Neural Networks with Landscape Design},
  
  publisher = {arXiv},
  
  year = {2017},
  

}

@inproceedings{ge2016,
 author = {Ge, Rong and Lee, Jason D. and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 title = {Matrix Completion has No Spurious Local Minimum},
 year = {2016}
}

@inproceedings{ge2015,
  
  author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  
  
  title = {Escaping From Saddle Points --- Online Stochastic Gradient for Tensor Decomposition},

  booktitle={Proceedings of The 28th Conference on Learning Theory},

  pages={797--842},
  
  year = {2015},
  
}


@InProceedings{lee2016,
  author = 	 {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  booktitle = 	 {29th Annual Conference on Learning Theory},
  pages = 	 {1246--1257},
  year = 	 {2016},
  series = 	 {Proceedings of Machine Learning Research},
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}

@inproceedings{chen2020generalized,
	author = {Chen, Zixiang and Cao, Yuan and Gu, Quanquan and Zhang, Tong},
	title = {A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	year = {2020}
}

@misc{bai2020taylorized,

  url = {https://arxiv.org/abs/2002.04010.abs},
  
  author = {Bai, Yu and Krause, Ben and Wang, Huan and Xiong, Caiming and Socher, Richard},

  title = {Taylorized Training: Towards Better Approximation of Neural Network Training at Finite Width},
  
  publisher = {arXiv},
  
  year = {2020},

}

@article{spectral_methods,
year = {2021},
volume = {14},
journal = {Foundations and Trends® in Machine Learning},
title = {Spectral Methods for Data Science: A Statistical Perspective},
doi = {10.1561/2200000079},
issn = {1935-8237},
number = {5},
pages = {566-806},
author = {Yuxin Chen and Yuejie Chi and Jianqing Fan and Cong Ma}
}

@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
    pdf={https://arxiv.org/abs/1912.02803},
    url={https://github.com/google/neural-tangents}
}






