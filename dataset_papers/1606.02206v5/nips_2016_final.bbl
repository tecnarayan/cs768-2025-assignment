\begin{thebibliography}{10}

\bibitem{vapnik}
Vladimir Vapnik.
\newblock {\em The nature of statistical learning theory}.
\newblock Springer Science \& Business Media, 2013.

\bibitem{NP_01}
Vitaly Feldman, Venkatesan Guruswami, Prasad Raghavendra, and Yi~Wu.
\newblock Agnostic learning of monomials by halfspaces is hard.
\newblock {\em SIAM Journal on Computing}, 41(6):1558--1590, 2012.

\bibitem{mpm}
Gert~RG Lanckriet, Laurent~El Ghaoui, Chiranjib Bhattacharyya, and Michael~I
  Jordan.
\newblock A robust minimax approach to classification.
\newblock {\em The Journal of Machine Learning Research}, 3:555--582, 2003.

\bibitem{DCC}
Elad Eban, Elad Mezuman, and Amir Globerson.
\newblock Discrete chebyshev classifiers.
\newblock In {\em Proceedings of the 31st International Conference on Machine
  Learning (ICML-14)}, pages 1233--1241, 2014.

\bibitem{DRC}
Meisam Razaviyayn, Farzan Farnia, and David Tse.
\newblock Discrete r\'{e}nyi classifiers.
\newblock In {\em Advances in Neural Information Processing Systems 28}, pages
  3258--3266, 2015.

\bibitem{topsoe1979information}
Flemming Tops{\o}e.
\newblock Information-theoretical optimization techniques.
\newblock {\em Kybernetika}, 15(1):8--27, 1979.

\bibitem{sion}
Maurice Sion.
\newblock On general minimax theorems.
\newblock {\em Pacific J. Math}, 8(1):171--176, 1958.

\bibitem{jaynes}
Edwin~T Jaynes.
\newblock Information theory and statistical mechanics.
\newblock {\em Physical review}, 106(4):620, 1957.

\bibitem{MaxEnt}
Peter~D. Gr√ºnwald and Philip Dawid.
\newblock Game theory, maximum entropy, minimum discrepancy and robust bayesian
  decision theory.
\newblock {\em The Annals of Statistics}, 32(4):1367--1433, 2004.

\bibitem{berger}
Adam~L Berger, Vincent J~Della Pietra, and Stephen A~Della Pietra.
\newblock A maximum entropy approach to natural language processing.
\newblock {\em Computational linguistics}, 22(1):39--71, 1996.

\bibitem{minMI}
Amir Globerson and Naftali Tishby.
\newblock The minimum information principle for discriminative learning.
\newblock In {\em Proceedings of the 20th conference on Uncertainty in
  artificial intelligence}, pages 193--200, 2004.

\bibitem{SVM}
Corinna Cortes and Vladimir Vapnik.
\newblock Support-vector networks.
\newblock {\em Machine learning}, 20(3):273--297, 1995.

\bibitem{Dawid}
Philip Dawid.
\newblock Coherent measures of discrepancy, uncertainty and dependence, with
  applications to bayesian predictive experimental design.
\newblock {\em Technical Report 139, University College London}, 1998.
\newblock http://www.ucl.ac.uk/Stats/research/abs94.html.

\bibitem{cover}
Thomas~M Cover and Joy~A Thomas.
\newblock {\em Elements of information theory}.
\newblock John Wiley \& Sons, 2012.

\bibitem{altun2006}
Yasemin Altun and Alexander Smola.
\newblock Unifying divergence minimisation and statistical inference via convex
  duality.
\newblock In {\em Learning Theory: Conference on Learning Theory COLT 2006,
  Proceedings}, 2006.

\bibitem{dudik}
Miroslav Dud{\'\i}k, Steven~J Phillips, and Robert~E Schapire.
\newblock Maximum entropy density estimation with generalized regularization
  and an application to species distribution modeling.
\newblock {\em Journal of Machine Learning Research}, 8(6):1217--1260, 2007.

\bibitem{semi2010}
Ayse Erkan and Yasemin Altun.
\newblock Semi-supervised learning via generalized maximum entropy.
\newblock In {\em AISTATS}, pages 209--216, 2010.

\bibitem{GLM}
Peter McCullagh and John~A Nelder.
\newblock {\em Generalized linear models}, volume~37.
\newblock CRC press, 1989.

\bibitem{elements}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock {\em The elements of statistical learning}, volume~1.
\newblock Springer, 2001.

\bibitem{huber1981}
Peter~J Huber.
\newblock {\em Robust Statistics}.
\newblock Wiley, 1981.

\bibitem{lasso}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the lasso.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 267--288, 1996.

\bibitem{Lasso_Donoho}
Scott~Shaobing Chen, David~L. Donoho, and Michael~A. Saunders.
\newblock Atomic decomposition by basis pursuit.
\newblock {\em SIAM Journal on Scientific Computing}, 20(1):33--61, 1998.

\bibitem{Ridge}
Arthur~E Hoerl and Robert~W Kennard.
\newblock Ridge regression: Biased estimation for nonorthogonal problems.
\newblock {\em Technometrics}, 12(1):55--67, 1970.

\bibitem{grouplasso}
Ming Yuan and Yi~Lin.
\newblock Model selection and estimation in regression with grouped variables.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 68(1):49--67, 2006.

\bibitem{GLoverlap}
Laurent Jacob, Guillaume Obozinski, and Jean-Philippe Vert.
\newblock Group lasso with overlap and graph lasso.
\newblock In {\em Proceedings of the 26th annual international conference on
  machine learning}, pages 433--440, 2009.

\bibitem{RobustLasso}
Huan Xu, Constantine Caramanis, and Shie Mannor.
\newblock Robust regression and lasso.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1801--1808, 2009.

\bibitem{RobustLassoLike}
Wenzhuo Yang and Huan Xu.
\newblock A unified robust regression model for lasso-like algorithms.
\newblock In {\em Proceedings of The International Conference on Machine
  Learning}, pages 585--593, 2013.

\bibitem{pengfeature}
Hanchuan Peng, Fuhui Long, and Chris Ding.
\newblock Feature selection based on mutual information criteria of
  max-dependency, max-relevance, and min-redundancy.
\newblock {\em Pattern Analysis and Machine Intelligence, IEEE Transactions
  on}, 27(8):1226--1238, 2005.

\bibitem{feature2}
Pablo Est{\'e}vez, Michel Tesmer, Claudio Perez, Jacek~M Zurada, et~al.
\newblock Normalized mutual information feature selection.
\newblock {\em Neural Networks, IEEE Transactions on}, 20(2):189--201, 2009.

\bibitem{tan}
CK~Chow and CN~Liu.
\newblock Approximating discrete probability distributions with dependence
  trees.
\newblock {\em Information Theory, IEEE Transactions on}, 14(3):462--467, 1968.

\bibitem{rockafellar}
Ralph Rockafellar.
\newblock Characterization of the subdifferentials of convex functions.
\newblock {\em Pacific Journal of Mathematics}, 17(3):497--510, 1966.

\bibitem{bartlett}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock {\em Journal of Machine Learning Research}, 3(Nov):463--482, 2002.

\bibitem{kakade}
Sham~M Kakade, Karthik Sridharan, and Ambuj Tewari.
\newblock On the complexity of linear prediction: Risk bounds, margin bounds,
  and regularization.
\newblock In {\em Advances in neural information processing systems}, pages
  793--800, 2009.

\bibitem{Rademacher_multiclass}
Andreas Maurer.
\newblock A vector-contraction inequality for rademacher complexities.
\newblock In {\em Algorithmic Learning Theory (ALT)}, pages 3--17, 2016.

\end{thebibliography}
