\begin{thebibliography}{39}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ackley et~al.(1985)Ackley, Hinton, and Sejnowski]{ackley1985learning}
Ackley, David~H, Hinton, Geoffrey~E, and Sejnowski, Terrence~J.
\newblock A learning algorithm for boltzmann machines.
\newblock \emph{Cognitive science}, 9\penalty0 (1):\penalty0 147--169, 1985.

\bibitem[Almeida(1990)]{almeida1990learning}
Almeida, Luis~B.
\newblock A learning rule for asynchronous perceptrons with feedback in a
  combinatorial environment.
\newblock In \emph{Artificial neural networks}, pp.\  102--111. IEEE Press,
  1990.

\bibitem[Bengio(2014)]{bengio2014auto}
Bengio, Yoshua.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock \emph{arXiv preprint arXiv:1407.7906}, 2014.

\bibitem[Bengio \& Fischer(2015)Bengio and Fischer]{bengio2015early}
Bengio, Yoshua and Fischer, Asja.
\newblock Early inference in energy-based models approximates back-propagation.
\newblock \emph{arXiv preprint arXiv:1510.02777}, 2015.

\bibitem[Bengio et~al.(2015)Bengio, Lee, Bornschein, Mesnard, and
  Lin]{bengio2015towards}
Bengio, Yoshua, Lee, Dong-Hyun, Bornschein, Jorg, Mesnard, Thomas, and Lin,
  Zhouhan.
\newblock Towards biologically plausible deep learning.
\newblock \emph{arXiv preprint arXiv:1502.04156}, 2015.

\bibitem[Bengio et~al.(2016)Bengio, Scellier, Bilaniuk, Sacramento, and
  Senn]{bengio2016feedforward}
Bengio, Yoshua, Scellier, Benjamin, Bilaniuk, Olexa, Sacramento, Joao, and
  Senn, Walter.
\newblock Feedforward initialization for fast inference of deep generative
  networks is biologically plausible.
\newblock \emph{arXiv preprint arXiv:1606.01651}, 2016.

\bibitem[Bengio et~al.(2017)Bengio, Mesnard, Fischer, Zhang, and
  Wu]{bengio2017stdp}
Bengio, Yoshua, Mesnard, Thomas, Fischer, Asja, Zhang, Saizheng, and Wu,
  Yuhuai.
\newblock Stdp-compatible approximation of backpropagation in an energy-based
  model.
\newblock \emph{Neural computation}, 2017.

\bibitem[Crick(1989)]{crick1989recent}
Crick, Francis.
\newblock The recent excitement about neural networks.
\newblock \emph{Nature}, 337\penalty0 (6203):\penalty0 129--132, 1989.

\bibitem[Dumoulin \& Visin(2016)Dumoulin and Visin]{dumoulin2016guide}
Dumoulin, Vincent and Visin, Francesco.
\newblock A guide to convolution arithmetic for deep learning.
\newblock \emph{arXiv preprint arXiv:1603.07285}, 2016.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, Xavier and Bengio, Yoshua.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  249--256, 2010.

\bibitem[Grossberg(1987)]{grossberg1987competitive}
Grossberg, Stephen.
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock \emph{Cognitive science}, 11\penalty0 (1):\penalty0 23--63, 1987.

\bibitem[Guerguiev et~al.(2017)Guerguiev, Lillicrap, and
  Richards]{guerguiev2017towards}
Guerguiev, Jordan, Lillicrap, Timothy~P, and Richards, Blake~A.
\newblock Towards deep learning with segregated dendrites.
\newblock \emph{ELife}, 6:\penalty0 e22901, 2017.

\bibitem[Hinton(2007)]{hinton2007howto}
Hinton, G.E.
\newblock How to do backpropagation in a brain.
\newblock \emph{NIPS 2007 Deep Learning Workshop}, 2007.

\bibitem[Hinton \& McClelland(1988)Hinton and McClelland]{hinton1988learning}
Hinton, Geoffrey~E and McClelland, James~L.
\newblock Learning representations by recirculation.
\newblock In \emph{Neural information processing systems}, pp.\  358--366. New
  York: American Institute of Physics, 1988.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, Diederik and Ba, Jimmy.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[K{\"o}rding \& K{\"o}nig(2001)K{\"o}rding and
  K{\"o}nig]{kording2001supervised}
K{\"o}rding, Konrad~P and K{\"o}nig, Peter.
\newblock Supervised and unsupervised learning with two sites of synaptic
  integration.
\newblock \emph{Journal of computational neuroscience}, 11\penalty0
  (3):\penalty0 207--215, 2001.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, Alex.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
Krizhevsky, Alex, Sutskever, Ilya, and Hinton, Geoffrey~E.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1097--1105, 2012.

\bibitem[LeCun(1986)]{le1986learning}
LeCun, Yann.
\newblock Learning process in an asymmetric threshold network.
\newblock In \emph{Disordered systems and biological organization}, pp.\
  233--240. Springer, 1986.

\bibitem[LeCun(1987)]{yann1987modeles}
LeCun, Yann.
\newblock \emph{Mod{\`e}les connexionnistes de lâ€™apprentissage}.
\newblock PhD thesis, PhD thesis, These de Doctorat, Universit{\'e} Paris 6,
  1987.

\bibitem[Lee et~al.(2015)Lee, Zhang, Fischer, and Bengio]{lee2015difference}
Lee, Dong-Hyun, Zhang, Saizheng, Fischer, Asja, and Bengio, Yoshua.
\newblock Difference target propagation.
\newblock In \emph{Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pp.\  498--515. Springer, 2015.

\bibitem[Lillicrap et~al.(2014)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2014random}
Lillicrap, Timothy~P, Cownden, Daniel, Tweed, Douglas~B, and Akerman, Colin~J.
\newblock Random feedback weights support learning in deep neural networks.
\newblock \emph{arXiv preprint arXiv:1411.0247}, 2014.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2016random}
Lillicrap, Timothy~P, Cownden, Daniel, Tweed, Douglas~B, and Akerman, Colin~J.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature Communications}, 7, 2016.

\bibitem[Movellan(1991)]{movellan1991contrastive}
Movellan, Javier~R.
\newblock Contrastive hebbian learning in the continuous hopfield model.
\newblock In \emph{Connectionist models: Proceedings of the 1990 summer
  school}, pp.\  10--17, 1991.

\bibitem[N{\o}kland(2016)]{nokland2016direct}
N{\o}kland, Arild.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In \emph{Advances In Neural Information Processing Systems}, pp.\
  1037--1045, 2016.

\bibitem[O'Reilly(1996)]{o1996biologically}
O'Reilly, Randall~C.
\newblock Biologically plausible error-driven learning using local activation
  differences: The generalized recirculation algorithm.
\newblock \emph{Neural computation}, 8\penalty0 (5):\penalty0 895--938, 1996.

\bibitem[Ororbia \& Mali(2018)Ororbia and Mali]{ororbia2018biologically}
Ororbia, Alexander~G and Mali, Ankur.
\newblock Biologically motivated algorithms for propagating local target
  representations.
\newblock \emph{arXiv preprint arXiv:1805.11703}, 2018.

\bibitem[Ororbia et~al.(2018)Ororbia, Mali, Kifer, and
  Giles]{ororbia2018conducting}
Ororbia, Alexander~G, Mali, Ankur, Kifer, Daniel, and Giles, C~Lee.
\newblock Conducting credit assignment by aligning local representations.
\newblock \emph{arXiv preprint arXiv:1803.01834}, 2018.

\bibitem[Parisien et~al.(2008)Parisien, Anderson, and Eliasmith]{parisien2008}
Parisien, Christopher, Anderson, Charles~H, and Eliasmith, Chris.
\newblock Solving the problem of negative synaptic weights in cortical models.
\newblock \emph{Neural computation}, 20\penalty0 (6):\penalty0 1473--1494,
  2008.

\bibitem[Pineda(1987)]{pineda1987generalization}
Pineda, Fernando~J.
\newblock Generalization of back-propagation to recurrent neural networks.
\newblock \emph{Physical review letters}, 59\penalty0 (19):\penalty0 2229,
  1987.

\bibitem[Pineda(1988)]{pineda1988dynamics}
Pineda, Fernando~J.
\newblock Dynamics and architecture for neural computation.
\newblock \emph{Journal of Complexity}, 4\penalty0 (3):\penalty0 216--245,
  1988.

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
Rumelhart, DE, Hinton, GE, and Williams, RJ.
\newblock Learning representations by back-propagation errors.
\newblock \emph{Nature}, 323:\penalty0 533--536, 1986.

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and Fei-Fei]{ILSVRC15}
Russakovsky, Olga, Deng, Jia, Su, Hao, Krause, Jonathan, Satheesh, Sanjeev, Ma,
  Sean, Huang, Zhiheng, Karpathy, Andrej, Khosla, Aditya, Bernstein, Michael,
  Berg, Alexander~C., and Fei-Fei, Li.
\newblock {ImageNet Large Scale Visual Recognition Challenge}.
\newblock \emph{International Journal of Computer Vision (IJCV)}, 115\penalty0
  (3):\penalty0 211--252, 2015.
\newblock \doi{10.1007/s11263-015-0816-y}.

\bibitem[Sacramento et~al.(2017)Sacramento, Costa, Bengio, and
  Senn]{sacramento2017dendritic}
Sacramento, Joao, Costa, Rui~Ponte, Bengio, Yoshua, and Senn, Walter.
\newblock Dendritic error backpropagation in deep cortical microcircuits.
\newblock \emph{arXiv preprint arXiv:1801.00062}, 2017.

\bibitem[Samadi et~al.(2017)Samadi, Lillicrap, and Tweed]{samadi2017deep}
Samadi, Arash, Lillicrap, Timothy~P, and Tweed, Douglas~B.
\newblock Deep learning with dynamic spiking neurons and fixed feedback
  weights.
\newblock \emph{Neural computation}, 2017.

\bibitem[Scellier \& Bengio(2017)Scellier and Bengio]{scellier2017equilibrium}
Scellier, Benjamin and Bengio, Yoshua.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock \emph{Frontiers in computational neuroscience}, 11, 2017.

\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg2014striving}
Springenberg, Jost~Tobias, Dosovitskiy, Alexey, Brox, Thomas, and Riedmiller,
  Martin.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{arXiv preprint arXiv:1412.6806}, 2014.

\bibitem[Whittington \& Bogacz(2017)Whittington and
  Bogacz]{whittington2017approximation}
Whittington, James~CR and Bogacz, Rafal.
\newblock An approximation of the error backpropagation algorithm in a
  predictive coding network with local hebbian synaptic plasticity.
\newblock \emph{Neural computation}, 2017.

\bibitem[Xie \& Seung(2003)Xie and Seung]{xie2003equivalence}
Xie, Xiaohui and Seung, H~Sebastian.
\newblock Equivalence of backpropagation and contrastive hebbian learning in a
  layered network.
\newblock \emph{Neural computation}, 15\penalty0 (2):\penalty0 441--454, 2003.

\end{thebibliography}
