\begin{thebibliography}{10}

\bibitem{Ioffe15}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em 32nd International Conference on Machine Learning, {ICML}
  2015}, pages 448--456, 2015.

\bibitem{Ioffe17}
Sergey Ioffe.
\newblock Batch renormalization: Towards reducing minibatch dependence in
  batch-normalized models.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 1945--1953,
  2017.

\bibitem{Wu18}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In {\em Computer Vision - {ECCV} 2018 - 15th European Conference,
  Proceedings, Part {XIII}}, pages 3--19, 2018.

\bibitem{Ying18}
Chris Ying, Sameer Kumar, Dehao Chen, Tao Wang, and Youlong Cheng.
\newblock Image classification at supercomputer scale.
\newblock {\em CoRR}, abs/1811.06992, 2018.

\bibitem{Singh19b}
Saurabh Singh and Shankar Krishnan.
\newblock Filter response normalization layer: Eliminating batch dependence in
  the training of deep neural networks.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2020}, pages 11234--11243, 2020.

\bibitem{Summers20}
Cecilia Summers and Michael~J. Dinneen.
\newblock Four things everyone should know to improve batch normalization.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020}, 2020.

\bibitem{Yan20}
Junjie Yan, Ruosi Wan, Xiangyu Zhang, Wei Zhang, Yichen Wei, and Jian Sun.
\newblock Towards stabilizing batch statistics in backward propagation of batch
  normalization.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020}, 2020.

\bibitem{Chiley19}
Vitaliy Chiley, Ilya Sharapov, Atli Kosson, Urs K{\"{o}}ster, Ryan Reece,
  Sofia~Samaniego de~la Fuente, Vishal Subbiah, and Michael James.
\newblock Online normalization for training neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019},
  pages 8431--8441, 2019.

\bibitem{Ba16}
Lei~Jimmy Ba, Jamie~Ryan Kiros, and Geoffrey~E. Hinton.
\newblock Layer normalization.
\newblock {\em CoRR}, abs/1607.06450, 2016.

\bibitem{Ulyanov16b}
Dmitry Ulyanov, Andrea Vedaldi, and Victor~S. Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock {\em CoRR}, abs/1607.08022, 2016.

\bibitem{Ren17}
Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian~H. Sinz, and Richard~S. Zemel.
\newblock Normalizing the normalizers: Comparing and extending network
  normalization schemes.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Conference Track Proceedings}, 2017.

\bibitem{Luo19b}
Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, and Jingyu Li.
\newblock Differentiable learning-to-normalize via switchable normalization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{Luo19c}
Ping Luo, Zhanglin Peng, Wenqi Shao, Ruimao Zhang, Jiamin Ren, and Lingyun Wu.
\newblock Differentiable dynamic normalization for learning deep
  representation.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 4203--4211, 2019.

\bibitem{Zhang19c}
Biao Zhang and Rico Sennrich.
\newblock Root mean square layer normalization.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019},
  pages 12360--12371, 2019.

\bibitem{Liu20}
Hanxiao Liu, Andy Brock, Karen Simonyan, and Quoc Le.
\newblock Evolving normalization-activation layers.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem{Salimans16}
Tim Salimans and Diederik~P. Kingma.
\newblock Weight normalization: {A} simple reparameterization to accelerate
  training of deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016}, page 901, 2016.

\bibitem{Huang17a}
Lei Huang, Xianglong Liu, Yang Liu, Bo~Lang, and Dacheng Tao.
\newblock Centered weight normalization in accelerating training of deep neural
  networks.
\newblock In {\em {IEEE} International Conference on Computer Vision, {ICCV}
  2017}, pages 2822--2830, 2017.

\bibitem{Qiao19}
Siyuan Qiao, Huiyu Wang, Chenxi Liu, Wei Shen, and Alan~L. Yuille.
\newblock Weight standardization.
\newblock {\em CoRR}, abs/1903.10520, 2019.

\bibitem{Ruff19}
Brendan Ruff, Taylor Beck, and Joscha Bach.
\newblock Mean shift rejection: Training deep neural networks without minibatch
  statistics or normalization.
\newblock {\em CoRR}, abs/1911.13173, 2019.

\bibitem{Brock21a}
Andrew Brock, Soham De, and Samuel~L. Smith.
\newblock Characterizing signal propagation to close the performance gap in
  unnormalized {ResNets}.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021}, 2021.

\bibitem{Arpit16}
Devansh Arpit, Yingbo Zhou, Bhargava~Urala Kota, and Venu Govindaraju.
\newblock Normalization propagation: {A} parametric technique for removing
  internal covariate shift in deep networks.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016}, volume~48 of {\em {JMLR} Workshop and Conference
  Proceedings}, pages 1168--1176, 2016.

\bibitem{Laurent17}
C{\'{e}}sar Laurent, Nicolas Ballas, and Pascal Vincent.
\newblock Recurrent normalization propagation.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Workshop Track Proceedings}, 2017.

\bibitem{Shekhovtsov18b}
Alexander Shekhovtsov and Boris Flach.
\newblock Normalization of neural networks using analytic variance propagation.
\newblock {\em CoRR}, abs/1803.10560, 2018.

\bibitem{Klambauer17}
G{\"{u}}nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
\newblock Self-normalizing neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 971--980,
  2017.

\bibitem{Laarhoven17}
Twan van Laarhoven.
\newblock {L2} regularization versus batch and weight normalization.
\newblock {\em CoRR}, abs/1706.05350, 2017.

\bibitem{Arora19}
Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu.
\newblock Theoretical analysis of auto rate-tuning by batch normalization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{Hoffer18}
Elad Hoffer, Ron Banner, Itay Golan, and Daniel Soudry.
\newblock Norm matters: efficient and accurate normalization schemes in deep
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  pages 2164--2174, 2018.

\bibitem{Li20}
Zhiyuan Li and Sanjeev Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020}, 2020.

\bibitem{Zhang19b}
Guodong Zhang, Chaoqi Wang, Bowen Xu, and Roger~B. Grosse.
\newblock Three mechanisms of weight decay regularization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{Wan20}
Ruosi Wan, Zhanxing Zhu, Xiangyu Zhang, and Jian Sun.
\newblock Spherical motion dynamics of deep neural networks with batch
  normalization and weight decay.
\newblock {\em CoRR}, abs/2006.08419, 2020.

\bibitem{Cho17}
Minhyung Cho and Jaehyung Lee.
\newblock Riemannian approach to batch normalization.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 5225--5235,
  2017.

\bibitem{Kohler19}
Jonas~Moritz Kohler, Hadi Daneshmand, Aur{\'{e}}lien Lucchi, Thomas Hofmann,
  Ming Zhou, and Klaus Neymeyr.
\newblock Exponential convergence rates for batch normalization: The power of
  length-direction decoupling in non-convex optimization.
\newblock In {\em The 22nd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2019}, volume~89 of {\em Proceedings of Machine
  Learning Research}, pages 806--815, 2019.

\bibitem{Hanin18}
Boris Hanin and David Rolnick.
\newblock How to start training: The effect of initialization and architecture.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  pages 569--579, 2018.

\bibitem{Zhang19a}
Hongyi Zhang, Yann~N. Dauphin, and Tengyu Ma.
\newblock Fixup initialization: Residual learning without normalization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{De20}
Soham De and Samuel~L. Smith.
\newblock Batch normalization biases residual blocks towards the identity
  function in deep networks.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem{Shao20}
Jie Shao, Kai Hu, Changhu Wang, Xiangyang Xue, and Bhiksha Raj.
\newblock Is normalization indispensable for training deep neural network?
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem{Lubana21}
Ekdeep~Singh Lubana, Robert~P. Dick, and Hidenori Tanaka.
\newblock Beyond batchnorm: Towards a general understanding of normalization in
  deep learning.
\newblock {\em CoRR}, abs/2106.05956, 2021.

\bibitem{Veit16}
Andreas Veit, Michael~J. Wilber, and Serge~J. Belongie.
\newblock Residual networks behave like ensembles of relatively shallow
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016}, pages 550--558,
  2016.

\bibitem{Philipp18}
George Philipp, Dawn Song, and Jaime~G. Carbonell.
\newblock Gradients explode - deep networks are shallow - {ResNet} explained.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Workshop Track Proceedings}, 2018.

\bibitem{Yang19}
Greg Yang, Jeffrey Pennington, Vinay Rao, Jascha Sohl{-}Dickstein, and
  Samuel~S. Schoenholz.
\newblock A mean field theory of batch normalization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{Labatie19}
Antoine Labatie.
\newblock Characterizing well-behaved vs. pathological deep neural networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 3611--3621, 2019.

\bibitem{Galloway19}
Angus Galloway, Anna Golubeva, Thomas Tanay, Medhat Moussa, and Graham~W.
  Taylor.
\newblock Batch normalization is a cause of adversarial vulnerability.
\newblock {\em CoRR}, abs/1905.02161, 2019.

\bibitem{Teye18}
Mattias Teye, Hossein Azizpour, and Kevin Smith.
\newblock Bayesian uncertainty estimation for batch normalized deep networks.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning, {ICML} 2018}, volume~80 of {\em Proceedings of Machine Learning
  Research}, pages 4914--4923, 2018.

\bibitem{Shekhovtsov18a}
Alexander Shekhovtsov and Boris Flach.
\newblock Stochastic normalizations as {Bayesian} learning.
\newblock In {\em Computer Vision - {ACCV} 2018 - 14th Asian Conference on
  Computer Vision, Revised Selected Papers, Part {II}}, volume 11362 of {\em
  Lecture Notes in Computer Science}, pages 463--479, 2018.

\bibitem{Luo19a}
Ping Luo, Xinjiang Wang, Wenqi Shao, and Zhanglin Peng.
\newblock Towards understanding regularization in batch normalization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{Bjorck18}
Johan Bjorck, Carla~P. Gomes, Bart Selman, and Kilian~Q. Weinberger.
\newblock Understanding batch normalization.
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  pages 7705--7716, 2018.

\bibitem{Jacot19}
Arthur Jacot, Franck Gabriel, and Clément Hongler.
\newblock Freeze and chaos for {DNNs}: an {NTK} view of batch normalization,
  checkerboard and boundary effects.
\newblock {\em CoRR}, abs/1907.05715, 2019.

\bibitem{Daneshmand20}
Hadi Daneshmand, Jonas~Moritz Kohler, Francis~R. Bach, Thomas Hofmann, and
  Aur{\'{e}}lien Lucchi.
\newblock Batch normalization provably avoids ranks collapse for randomly
  initialised deep networks.
\newblock In {\em Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020},
  2020.

\bibitem{Rao20}
Vinay Rao and Jascha Sohl{-}Dickstein.
\newblock Is batch norm unique? {An} empirical investigation and prescription
  to emulate the best properties of common normalizers without batch
  dependence.
\newblock {\em CoRR}, abs/2010.10687, 2020.

\bibitem{Hanin19a}
Boris Hanin and David Rolnick.
\newblock Complexity of linear regions in deep networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 2596--2604, 2019.

\bibitem{Hanin19b}
Boris Hanin and David Rolnick.
\newblock Deep relu networks have surprisingly few activation patterns.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019},
  pages 359--368, 2019.

\bibitem{Tan19}
Mingxing Tan and Quoc~V. Le.
\newblock {EfficientNet}: Rethinking model scaling for convolutional neural
  networks.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 6105--6114, 2019.

\bibitem{Huang20}
Lei Huang, Yi~Zhou, Li~Liu, Fan Zhu, and Ling Shao.
\newblock Group whitening: Balancing learning efficiency and representational
  capacity.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition,
  {CVPR} 2021}, pages 9512--9521, 2021.

\bibitem{Xie20a}
Cihang Xie, Mingxing Tan, Boqing Gong, Jiang Wang, Alan~L. Yuille, and Quoc~V.
  Le.
\newblock Adversarial examples improve image recognition.
\newblock In {\em 2020 {IEEE/CVF} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2020}, pages 816--825, 2020.

\bibitem{Xie20b}
Cihang Xie and Alan~L. Yuille.
\newblock Intriguing properties of adversarial training at scale.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020}, 2020.

\bibitem{Miyato18}
Takeru Miyato and Masanori Koyama.
\newblock {cGANs} with projection discriminator.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Conference Track Proceedings}, 2018.

\bibitem{Vries17}
Harm de~Vries, Florian Strub, J{\'{e}}r{\'{e}}mie Mary, Hugo Larochelle,
  Olivier Pietquin, and Aaron~C. Courville.
\newblock Modulating early visual processing by language.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 6594--6604,
  2017.

\bibitem{Deecke18}
Lucas Deecke, Iain Murray, and Hakan Bilen.
\newblock Mode normalization.
\newblock In {\em 7th International Conference on Learning Representations,
  {ICLR} 2019}, 2019.

\bibitem{Wang19}
Ximei Wang, Ying Jin, Mingsheng Long, Jianmin Wang, and Michael~I. Jordan.
\newblock Transferable normalization: Towards improving transferability of deep
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019},
  pages 1951--1961, 2019.

\bibitem{Chang19}
Woong{-}Gi Chang, Tackgeun You, Seonguk Seo, Suha Kwak, and Bohyung Han.
\newblock Domain-specific batch normalization for unsupervised domain
  adaptation.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition,
  {CVPR} 2019}, pages 7354--7362, 2019.

\bibitem{Wu21}
Yuxin Wu and Justin Johnson.
\newblock Rethinking "batch" in batchnorm.
\newblock {\em CoRR}, abs/2105.07576, 2021.

\bibitem{Page19}
David Page.
\newblock How to train your {ResNet} 7: Batch norm, 2019.

\bibitem{Pennington17}
Jeffrey Pennington and Pratik Worah.
\newblock Nonlinear random matrix theory for deep learning.
\newblock In {\em Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017}, pages 2637--2646,
  2017.

\bibitem{Ghorbani19}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via {Hessian}
  eigenvalue density.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019}, volume~97 of {\em Proceedings of Machine Learning
  Research}, pages 2232--2241, 2019.

\bibitem{Masters18}
Dominic Masters and Carlo Luschi.
\newblock Revisiting small batch training for deep neural networks.
\newblock {\em CoRR}, abs/1804.07612, 2018.

\bibitem{Ulyanov16a}
Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor~S. Lempitsky.
\newblock Texture networks: Feed-forward synthesis of textures and stylized
  images.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016}, volume~48 of {\em {JMLR} Workshop and Conference
  Proceedings}, pages 1349--1357, 2016.

\bibitem{Gatys16}
Leon~A. Gatys, Alexander~S. Ecker, and Matthias Bethge.
\newblock Image style transfer using convolutional neural networks.
\newblock In {\em 2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016}, pages 2414--2423, 2016.

\bibitem{Ulyanov17}
Dmitry Ulyanov, Andrea Vedaldi, and Victor~S. Lempitsky.
\newblock Improved texture networks: Maximizing quality and diversity in
  feed-forward stylization and texture synthesis.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017}, pages 4105--4113, 2017.

\bibitem{Dumoulin17}
Vincent Dumoulin, Jonathon Shlens, and Manjunath Kudlur.
\newblock A learned representation for artistic style.
\newblock In {\em 5th International Conference on Learning Representations,
  {ICLR} 2017, Conference Track Proceedings}, 2017.

\bibitem{Huang17b}
Xun Huang and Serge~J. Belongie.
\newblock Arbitrary style transfer in real-time with adaptive instance
  normalization.
\newblock In {\em {IEEE} International Conference on Computer Vision, {ICCV}
  2017}, pages 1510--1519, 2017.

\bibitem{He16b}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em Computer Vision - {ECCV} 2016 - 14th European Conference},
  2016.

\bibitem{Brock21b}
Andy Brock, Soham De, Samuel~L. Smith, and Karen Simonyan.
\newblock High-performance large-scale image recognition without normalization.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning, {ICML} 2021}, volume 139 of {\em Proceedings of Machine Learning
  Research}, pages 1059--1071, 2021.

\bibitem{Shang17}
Wenling Shang, Justin Chiu, and Kihyuk Sohn.
\newblock Exploring normalization in deep residual networks with concatenated
  rectified linear units.
\newblock In {\em Proceedings of the Thirty-First {AAAI} Conference on
  Artificial Intelligence}, pages 1509--1516, 2017.

\bibitem{Gitman17}
Igor Gitman and Boris Ginsburg.
\newblock Comparison of batch normalization and weight normalization algorithms
  for the large-scale image classification.
\newblock {\em CoRR}, abs/1709.08145, 2017.

\bibitem{Deng09}
Jia Deng, Wei Dong, Richard Socher, Li{-}Jia Li, Kai Li, and Li~Fei{-}Fei.
\newblock {ImageNet}: {A} large-scale hierarchical image database.
\newblock In {\em 2009 {IEEE} Computer Society Conference on Computer Vision
  and Pattern Recognition {(CVPR} 2009)}, pages 248--255, 2009.

\bibitem{Krizhevsky09}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{Goyal17}
Priya Goyal, Piotr Doll{\'{a}}r, Ross~B. Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD:} training {ImageNet} in 1 hour.
\newblock {\em CoRR}, abs/1706.02677, 2017.

\bibitem{Xie17}
Saining Xie, Ross~B. Girshick, Piotr Doll{\'{a}}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em 2017 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2017}, 2017.

\bibitem{Masters21}
Dominic Masters, Antoine Labatie, Zach Eaton-Rosen, and Carlo Luschi.
\newblock Making {EfficientNet} more efficient: Exploring batch-independent
  normalization, group convolutions and reduced resolution training.
\newblock {\em CoRR}, abs/2106.03640, 2021.

\bibitem{Xiang17}
Sitao Xiang and Hao Li.
\newblock On the effect of batch normalization and weight normalization in
  generative adversarial networks.
\newblock {\em CoRR}, abs/1704.03971, 2017.

\bibitem{Kolesnikov20}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock {Big Transfer (BiT)}: General visual representation learning.
\newblock In {\em Computer Vision - {ECCV} 2020 - 16th European Conference},
  2020.

\bibitem{Szegedy16}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
  Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em 2016 {IEEE} Conference on Computer Vision and Pattern
  Recognition, {CVPR} 2016}, pages 2818--2826, 2016.

\bibitem{Srivastava14}
Nitish Srivastava, Geoffrey~E. Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em Journal of Machine Learning Research}, 15(1):1929--1958, 2014.

\bibitem{Huang16}
Gao Huang, Yu~Sun, Zhuang Liu, Daniel Sedra, and Kilian~Q. Weinberger.
\newblock Deep networks with stochastic depth.
\newblock In {\em Computer Vision - {ECCV} 2016 - 14th European Conference,
  Proceedings, Part {IV}}, volume 9908, pages 646--661, 2016.

\bibitem{Zhang18}
Hongyi Zhang, Moustapha Ciss{\'{e}}, Yann~N. Dauphin, and David Lopez{-}Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em 6th International Conference on Learning Representations,
  {ICLR} 2018, Conference Track Proceedings}, 2018.

\bibitem{Yun19}
Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong~Joon Oh, Youngjoon Yoo, and
  Junsuk Choe.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em 2019 {IEEE/CVF} International Conference on Computer Vision,
  {ICCV} 2019}, pages 6022--6031, 2019.

\bibitem{Cubuk19}
Ekin~D. Cubuk, Barret Zoph, Dandelion Man{\'{e}}, Vijay Vasudevan, and Quoc~V.
  Le.
\newblock {AutoAugment}: Learning augmentation strategies from data.
\newblock In {\em {IEEE} Conference on Computer Vision and Pattern Recognition,
  {CVPR} 2019}, pages 113--123, 2019.

\bibitem{Collins19}
Jasmine Collins, Johannes Ball{\'{e}}, and Jonathon Shlens.
\newblock Accelerating training of deep neural networks with a standardization
  loss.
\newblock {\em CoRR}, abs/1903.00925, 2019.

\bibitem{Dauphin21}
Yann Dauphin and Ekin~Dogus Cubuk.
\newblock Deconstructing the regularization of {BatchNorm}.
\newblock In {\em 9th International Conference on Learning Representations,
  {ICLR} 2021}, 2021.

\bibitem{Santurkar18}
Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, and Aleksander Madry.
\newblock How does batch normalization help optimization?
\newblock In {\em Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018},
  pages 2488--2498, 2018.

\bibitem{Leshno93}
Moshe Leshno, Vladimir~Ya. Lin, Allan Pinkus, and Shimon Schocken.
\newblock Multilayer feedforward networks with a nonpolynomial activation
  function can approximate any function.
\newblock {\em Neural Networks}, 6(6):861--867, 1993.

\end{thebibliography}
