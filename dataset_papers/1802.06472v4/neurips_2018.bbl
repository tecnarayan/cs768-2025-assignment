\begin{thebibliography}{21}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Blum et~al.(2004)Blum, Kumar, Rudra, and Wu]{blum2004online}
Avrim Blum, Vijay Kumar, Atri Rudra, and Felix Wu.
\newblock Online learning in online auctions.
\newblock \emph{Theoretical Computer Science}, 324\penalty0 (2-3):\penalty0
  137--146, 2004.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Crammer et~al.(2006)Crammer, Dekel, Keshet, Shalev-Shwartz, and
  Singer]{crammer2006online}
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
\newblock Online passive-aggressive algorithms.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0
  (Mar):\penalty0 551--585, 2006.

\bibitem[Defazio et~al.(2014)Defazio, Bach, and
  Lacoste-Julien]{defazio2014saga}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem[Diamond and Boyd(2016)]{cvxpy}
Steven Diamond and Stephen Boyd.
\newblock {CVXPY}: A {P}ython-embedded modeling language for convex
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (83):\penalty0 1--5, 2016.

\bibitem[Duchi et~al.(2008)Duchi, Shalev-Shwartz, Singer, and
  Chandra]{duchi2008efficient}
John Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra.
\newblock Efficient projections onto the l 1-ball for learning in high
  dimensions.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 272--279. ACM, 2008.

\bibitem[Duchi et~al.(2010)Duchi, Shalev-Shwartz, Singer, and
  Tewari]{duchi2010composite}
John~C Duchi, Shai Shalev-Shwartz, Yoram Singer, and Ambuj Tewari.
\newblock Composite objective mirror descent.
\newblock In \emph{COLT}, pages 14--26, 2010.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Maryam Fazel, Rong Ge, Sham~M Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for linearized control
  problems.
\newblock \emph{arXiv preprint arXiv:1801.05039}, 2018.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
Elad Hazan, Amit Agarwal, and Satyen Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2):\penalty0 169--192, 2007.

\bibitem[Jenatton et~al.(2016)Jenatton, Huang, and
  Archambeau]{jenatton2016adaptive}
Rodolphe Jenatton, Jim Huang, and C{\'e}dric Archambeau.
\newblock Adaptive algorithms for online convex optimization with long-term
  constraints.
\newblock In \emph{International Conference on Machine Learning}, pages
  402--411, 2016.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2018)Li, Qu, and Li]{li2018online}
Yingying Li, Guannan Qu, and Na~Li.
\newblock Online optimization with predictions and switching costs: Fast
  algorithms and the fundamental limit.
\newblock \emph{arXiv preprint arXiv:1801.07780}, 2018.

\bibitem[Mahdavi et~al.(2012)Mahdavi, Jin, and Yang]{mahdavi2012trading}
Mehrdad Mahdavi, Rong Jin, and Tianbao Yang.
\newblock Trading regret for efficiency: online convex optimization with long
  term constraints.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Sep):\penalty0 2503--2528, 2012.

\bibitem[Mairal et~al.(2009)Mairal, Bach, Ponce, and Sapiro]{mairal2009online}
Julien Mairal, Francis Bach, Jean Ponce, and Guillermo Sapiro.
\newblock Online dictionary learning for sparse coding.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pages 689--696. ACM, 2009.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Yu~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Senthil and Manikandan(2010)]{senthil2010economic}
K~Senthil and K~Manikandan.
\newblock Economic thermal power dispatch with emission constraint and valve
  point effect loading using improved tabu search algorithm.
\newblock \emph{International Journal of Computer Applications}, 2010.

\bibitem[Yu et~al.(2017)Yu, Neely, and Wei]{yu2017online}
Hao Yu, Michael Neely, and Xiaohan Wei.
\newblock Online convex optimization with stochastic constraints.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1427--1437, 2017.

\bibitem[Yuan and Lamperski(2017)]{yuan2017online}
Jianjun Yuan and Andrew Lamperski.
\newblock Online control basis selection by a regularized actor critic
  algorithm.
\newblock In \emph{American Control Conference (ACC), 2017}, pages 4448--4453.
  IEEE, 2017.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pages 928--936, 2003.

\end{thebibliography}
