\begin{thebibliography}{100}

\bibitem{agafonov2021accelerated}
Artem Agafonov, Pavel Dvurechensky, Gesualdo Scutari, Alexander Gasnikov,
  Dmitry Kamzolov, Aleksandr Lukashevich, and Amir Daneshmand.
\newblock An accelerated second-order method for distributed stochastic
  optimization.
\newblock In {\em 2021 60th IEEE Conference on Decision and Control (CDC)},
  pages 2407--2413, 2021.

\bibitem{alghunaim2022unified}
Sulaiman~A Alghunaim and Kun Yuan.
\newblock A unified and refined convergence analysis for non-convex
  decentralized learning.
\newblock {\em IEEE Transactions on Signal Processing}, 2022.

\bibitem{antonakopoulos2019adaptive}
Kimon Antonakopoulos, Veronica Belmega, and Panayotis Mertikopoulos.
\newblock An adaptive mirror-prox method for variational inequalities with
  singular operators.
\newblock In {\em Advances in Neural Information Processing Systems 32
  (NeurIPS)}, pages 8455--8465. Curran Associates, Inc., 2019.

\bibitem{arjevani2015communication}
Yossi Arjevani and Ohad Shamir.
\newblock Communication complexity of distributed convex learning and
  optimization.
\newblock {\em arXiv preprint arXiv:1506.01900}, 2015.

\bibitem{Assran:2018sdggradpush}
Mahmoud Assran, Nicolas Loizou, Nicolas Ballas, and Michael Rabbat.
\newblock Stochastic gradient push for distributed deep learning.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}. PMLR, 2019.

\bibitem{BARAZANDEH2021108245}
Babak Barazandeh, Tianjian Huang, and George Michailidis.
\newblock A decentralized adaptive momentum method for solving a class of
  min-max optimization problems.
\newblock {\em Signal Processing}, 189:108245, 2021.

\bibitem{barazandeh2021solving}
Babak Barazandeh, Davoud~Ataee Tarzanagh, and George Michailidis.
\newblock Solving a class of non-convex min-max games using adaptive momentum
  methods.
\newblock In {\em IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP)}, pages 3625--3629, 2021.

\bibitem{Bellet2018:p2p}
Aur{\'e}lien Bellet, Rachid Guerraoui, Mahsa Taziki, and Marc Tommasi.
\newblock Personalized and private peer-to-peer machine learning.
\newblock In {\em Proceedings of the Twenty-First International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, volume~84, pages 473--481.
  PMLR, 2018.

\bibitem{BenTal2009:book}
Aharon Ben-Tal, Laurent~El Ghaoui, and Arkadi Nemirovski.
\newblock {\em Robust Optimization}.
\newblock Princeton University Press, 2009.

\bibitem{beznosikov2022stochastic}
Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou.
\newblock Stochastic gradient descent-ascent: Unified theory and new efficient
  methods.
\newblock {\em arXiv preprint arXiv:2202.07262}, 2022.

\bibitem{beznosikov2021distributed}
Aleksandr Beznosikov, Valentin Samokhin, and Alexander Gasnikov.
\newblock Distributed saddle-point problems: Lower bounds, optimal algorithms
  and federated {GANs}.
\newblock {\em arXiv preprint arXiv:2010.13112}, 2021.

\bibitem{boyd2006randomized}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Randomized gossip algorithms.
\newblock {\em IEEE transactions on information theory}, 52(6):2508--2530,
  2006.

\bibitem{Bullins2020:higherorder}
Brian Bullins and Kevin~A. Lai.
\newblock Higher-order methods for convex-concave min-max optimization and
  monotone variational inequalities.
\newblock {\em arXiv preprint arXiv:2007.04528}, 2020.

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock {\em Journal of mathematical imaging and vision}, 40(1):120--145,
  2011.

\bibitem{chavdarova2019}
Tatjana Chavdarova, Gauthier Gidel, François Fleuret, and Simon
  Lacoste-Julien.
\newblock Reducing noise in {GAN} training with variance reduced extragradient.
\newblock In {\em Advances in Neural Information Processing Systems 32
  (NeurIPS)}, 2019.

\bibitem{Chavdarova2020:tamingGANs}
Tatjana Chavdarova, Matteo Pagliardini, Sebastian~U. Stich, Francois Fleuret,
  and Martin Jaggi.
\newblock Taming {GANs} with lookahead-minmax.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2021.

\bibitem{daneshmand2021newton}
Amir Daneshmand, Gesualdo Scutari, Pavel Dvurechensky, and Alexander Gasnikov.
\newblock Newton method over networks is fast up to the statistical precision.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 2398--2409. PMLR, 18--24 Jul 2021.

\bibitem{dang2015convergence}
Cong~D Dang and Guanghui Lan.
\newblock On the convergence properties of non-{E}uclidean extragradient
  methods for variational inequalities with generalized monotone operators.
\newblock {\em Computational Optimization and Applications}, 60(2):277--310,
  2015.

\bibitem{Daskalakis2018:gan}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training {GANs} with optimism.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{deng2021local}
Yuyang Deng and Mehrdad Mahdavi.
\newblock Local stochastic gradient descent ascent: Convergence analysis and
  communication efficiency.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 1387--1395. PMLR, 2021.

\bibitem{diakonikolas2021efficient}
Jelena Diakonikolas, Constantinos Daskalakis, and Michael Jordan.
\newblock Efficient methods for structured nonconvex-nonconcave min-max
  optimization.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 2746--2754. PMLR, 2021.

\bibitem{dou2021one}
Zehao Dou and Yuanzhi Li.
\newblock On the one-sided convergence of adam-type algorithms in non-convex
  non-concave min-max optimization.
\newblock {\em arXiv preprint arXiv:2109.14213}, 2021.

\bibitem{dvurechensky2018decentralize}
Pavel Dvurechensky, Darina Dvinskikh, Alexander Gasnikov, C\'esar~A. Uribe, and
  Angelia Nedi\'c.
\newblock Decentralize and randomize: Faster algorithm for {W}asserstein
  barycenters.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, {\em Advances in Neural Information Processing
  Systems 31}, NeurIPS 2018, pages 10783--10793. Curran Associates, Inc., 2018.

\bibitem{dvurechensky2022hyperfast}
Pavel Dvurechensky, Dmitry Kamzolov, Aleksandr Lukashevich, Soomin Lee, Erik
  Ordentlich, César~A. Uribe, and Alexander Gasnikov.
\newblock Hyperfast second-order local solvers for efficient statistically
  preconditioned distributed optimization.
\newblock {\em EURO Journal on Computational Optimization}, page 100045, 2022.
\newblock (accepted), arXiv:2102.08246.

\bibitem{esser2010general}
Ernie Esser, Xiaoqun Zhang, and Tony~F Chan.
\newblock A general framework for a class of first order primal-dual algorithms
  for convex optimization in imaging science.
\newblock {\em SIAM Journal on Imaging Sciences}, 3(4):1015--1046, 2010.

\bibitem{facchinei2007finite}
F.~Facchinei and J.S. Pang.
\newblock {\em Finite-Dimensional Variational Inequalities and Complementarity
  Problems}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer New York, 2007.

\bibitem{gidel2019variational}
Gauthier Gidel, Hugo Berard, Pascal Vincent, and Simon Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial nets.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem{gorbunov2018accelerated}
Eduard Gorbunov, Pavel Dvurechensky, and Alexander Gasnikov.
\newblock An accelerated method for derivative-free smooth stochastic convex
  optimization.
\newblock {\em arXiv preprint arXiv:1802.09022}, 2018.

\bibitem{gorbunov2021local}
Eduard Gorbunov, Filip Hanzely, and Peter Richt{\'a}rik.
\newblock Local sgd: Unified theory and new efficient methods.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 3556--3564. PMLR, 2021.

\bibitem{hendrikx2020statistically}
Hadrien Hendrikx, Lin Xiao, Sebastien Bubeck, Francis Bach, and Laurent
  Massoulie.
\newblock Statistically preconditioned accelerated gradient method for
  distributed optimization.
\newblock In {\em International Conference on Machine Learning}, pages
  4203--4227. PMLR, 2020.

\bibitem{hou2021efficient}
Charlie Hou, Kiran~K Thekumparampil, Giulia Fanti, and Sewoong Oh.
\newblock Efficient algorithms for federated saddle point optimization.
\newblock {\em arXiv preprint arXiv:2102.06333}, 2021.

\bibitem{hsieh2020explore}
Yu-Guan Hsieh, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Panayotis
  Mertikopoulos.
\newblock Explore aggressively, update conservatively: Stochastic extragradient
  methods with variable stepsize scaling.
\newblock {\em Advances in Neural Information Processing Systems},
  33:16223--16234, 2020.

\bibitem{iusem2017extragradient}
Alfredo~N Iusem, Alejandro Jofr{\'e}, Roberto~Imbuzeiro Oliveira, and Philip
  Thompson.
\newblock Extragradient method with variance reduction for stochastic
  variational inequalities.
\newblock {\em SIAM Journal on Optimization}, 27(2):686--724, 2017.

\bibitem{jadbabaie2003coordination}
A.~Jadbabaie, Jie Lin, and A.S. Morse.
\newblock Coordination of groups of mobile autonomous agents using nearest
  neighbor rules.
\newblock {\em IEEE Transactions on Automatic Control}, 48(6):988--1001, 2003.

\bibitem{Jin2020:mdp}
Yujia Jin and Aaron Sidford.
\newblock Efficiently solving {MDP}s with stochastic mirror descent.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, volume 119, pages 4890--4900. PMLR, 2020.

\bibitem{juditsky2011solving}
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm.
\newblock {\em Stochastic Systems}, 1(1):17--58, 2011.

\bibitem{Kairouz2019:federated}
Peter Kairouz, H.~Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
  Rachel Cummings, Rafael G.~L. D'Oliveira, Salim~El Rouayheb, David Evans,
  Josh Gardner, Zachary Garrett, Adrià Gascón, Badih Ghazi, Phillip~B.
  Gibbons, Marco Gruteser, Zaid Harchaoui, Chaoyang He, Lie He, Zhouyuan Huo,
  Ben Hutchinson, Justin Hsu, Martin Jaggi, Tara Javidi, Gauri Joshi, Mikhail
  Khodak, Jakub Konečný, Aleksandra Korolova, Farinaz Koushanfar, Sanmi
  Koyejo, Tancrède Lepoint, Yang Liu, Prateek Mittal, Mehryar Mohri, Richard
  Nock, Ayfer Özgür, Rasmus Pagh, Mariana Raykova, Hang Qi, Daniel Ramage,
  Ramesh Raskar, Dawn Song, Weikang Song, Sebastian~U. Stich, Ziteng Sun,
  Ananda~Theertha Suresh, Florian Tramèr, Praneeth Vepakomma, Jianyu Wang,
  Li~Xiong, Zheng Xu, Qiang Yang, Felix~X. Yu, Han Yu, and Sen Zhao.
\newblock Advances and open problems in federated learning.
\newblock {\em arXiv preprint arXiv:1912.04977}, 2019.

\bibitem{kannan2019optimal}
Aswin Kannan and Uday~V Shanbhag.
\newblock Optimal stochastic extragradient schemes for pseudomonotone
  stochastic variational inequality problems and their variants.
\newblock {\em Computational Optimization and Applications}, 74(3):779--820,
  2019.

\bibitem{khaled2020tighter}
Ahmed Khaled, Konstantin Mishchenko, and Peter Richt{\'a}rik.
\newblock Tighter theory for local {SGD} on identical and heterogeneous data.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pages 4519--4529, 2020.

\bibitem{Kinderlehrer}
David Kinderlehrer and Guido Stampacchia.
\newblock {\em An Introduction to Variational Inequalities and Their
  Applications}.
\newblock Society for Industrial and Applied Mathematics, 2000.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kleinberg2018alternative}
Bobby Kleinberg, Yuanzhi Li, and Yang Yuan.
\newblock An alternative view: When does sgd escape local minima?
\newblock In {\em International Conference on Machine Learning}, pages
  2698--2707. PMLR, 2018.

\bibitem{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML)}. PMLR, 2020.

\bibitem{Koloskova:2019choco}
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning (ICML)}, volume~97, pages 3478--3487. PMLR, 2019.

\bibitem{koloskova2019decentralized}
Anastasia Koloskova, Sebastian Stich, and Martin Jaggi.
\newblock Decentralized stochastic optimization and gossip algorithms with
  compressed communication.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  3478--3487. PMLR, 09--15 Jun 2019.

\bibitem{Kong2021:cc}
Lingjing Kong, Tao Lin, Anastasia Koloskova, Martin Jaggi, and Sebastian~U.
  Stich.
\newblock Consensus control for decentralized deep learning.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning (ICML)}. PMLR, 2021.

\bibitem{korpelevich1976extragradient}
Galina Korpelevich.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock {\em Eknomika i Matematicheskie Metody}, 12:747--756, 1976.

\bibitem{kovalev2021lower}
Dmitry Kovalev, Elnur Gasanov, Peter Richt{\'a}rik, and Alexander Gasnikov.
\newblock Lower bounds and optimal algorithms for smooth and strongly convex
  decentralized optimization over time-varying networks.
\newblock {\em arXiv preprint arXiv:2106.04469}, 2021.

\bibitem{krawtschenko2020distributed}
Roman Krawtschenko, C\'esar~A. Uribe, Alexander Gasnikov, and Pavel
  Dvurechensky.
\newblock Distributed optimization with quantization for computing wasserstein
  barycenters.
\newblock {\em arXiv:2010.14325}, 2020.
\newblock WIAS preprint 2782.

\bibitem{cifar10}
Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton.
\newblock Cifar-10 (canadian institute for advanced research).
\newblock \url{http://www.cs.toronto.edu/~kriz/cifar.html}, 2009.

\bibitem{Lan2018:decentralized}
Guanghui Lan, Soomin Lee, and Yi~Zhou.
\newblock Communication-efficient algorithms for decentralized and stochastic
  optimization.
\newblock {\em Mathematical Programming}, Dec 2018.

\bibitem{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock {\em arXiv preprint arXiv:1705.09886}, 2017.

\bibitem{Lian2017:decentralizedSGD}
Xiangru Lian, Ce~Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji~Liu.
\newblock Can decentralized algorithms outperform centralized algorithms? a
  case study for decentralized parallel stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems 30 (NIPS)},
  pages 5330--5340. Curran Associates, Inc., 2017.

\bibitem{Lin2021:quasi}
Tao Lin, Sai~Praneeth Karimireddy, Sebastian~U. Stich, and Martin Jaggi.
\newblock Quasi-global momentum: Accelerating decentralized deep learning on
  heterogeneous data.
\newblock In {\em Proceedings of the 38th International Conference on Machine
  Learning (ICML)}, 2021.

\bibitem{Lin2020:nearoptimal}
Tianyi Lin, Chi Jin, and Michael~I. Jordan.
\newblock Near-optimal algorithms for minimax optimization.
\newblock In {\em Proceedings of Thirty Third Conference on Learning Theory
  (COLT)}, volume 125, pages 2738--2779. PMLR, 2020.

\bibitem{liu2019towards}
Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das,
  and Tianbao Yang.
\newblock Towards better understanding of adaptive gradient algorithms in
  generative adversarial nets.
\newblock {\em arXiv preprint arXiv:1912.11940}, 2019.

\bibitem{liu2019decentralized}
Mingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jerret Ross, Tianbao
  Yang, and Payel Das.
\newblock A decentralized parallel algorithm for training generative
  adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{liu2019decentralizedprox}
Weijie Liu, Aryan Mokhtari, Asuman Ozdaglar, Sarath Pattathil, Zebang Shen, and
  Nenggan Zheng.
\newblock A decentralized proximal point-type method for saddle point problems.
\newblock {\em arXiv preprint arXiv:1910.14380}, 2019.

\bibitem{Madry2017:adv}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{9548800}
Alexey~S. Matveev, Mostafa Almodarresi, Romeo Ortega, Anton Pyrkin, and Siyu
  Xie.
\newblock Diffusion-based distributed parameter estimation through directed
  graphs with switching topology: Application of dynamic regressor extension
  and mixing.
\newblock {\em IEEE Transactions on Automatic Control}, 67(8):4256--4263, 2022.

\bibitem{McMahan16:FedLearning}
H.~Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise~Ag{\"{u}}era
  y~Arcas.
\newblock Federated learning of deep networks using model averaging.
\newblock {\em arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{mertikopoulos2018optimistic}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
  Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock {\em arXiv preprint arXiv:1807.02629}, 2018.

\bibitem{minty62}
George~J. Minty.
\newblock Monotone (nonlinear) operators in {Hilbert} space.
\newblock {\em Duke Mathematical Journal}, 29(3):341 -- 346, 1962.

\bibitem{cgan}
Mehdi Mirza and Simon Osindero.
\newblock Conditional generative adversarial nets.
\newblock {\em arXiv preprint arXiv:1411.1784}, 2014.

\bibitem{Mishchenko2019:extragradient}
Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richt{\'a}rik, and
  Yura Malitsky.
\newblock Revisiting stochastic extragradient.
\newblock {\em arXiv preprint arXiv:1905.11373}, 2019.

\bibitem{moritz2018ray}
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw,
  Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael~I Jordan,
  et~al.
\newblock Ray: A distributed framework for emerging $\{$AI$\}$ applications.
\newblock In {\em 13th $\{$USENIX$\}$ Symposium on Operating Systems Design and
  Implementation ($\{$OSDI$\}$ 18)}, pages 561--577, 2018.

\bibitem{ray}
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw,
  Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael~I. Jordan, and
  Ion Stoica.
\newblock Ray: A distributed framework for emerging ai applications, 2018.

\bibitem{Mukherjee2020:decentralizedminmax}
Soham Mukherjee and Mrityunjoy Chakraborty.
\newblock A decentralized algorithm for large scale min-max problems.
\newblock In {\em 2020 59th IEEE Conference on Decision and Control (CDC)},
  pages 2967--2972, 2020.

\bibitem{nedic2009distributedAveraging}
Angelia Nedic, Alex Olshevsky, Asuman Ozdaglar, and John~N. Tsitsiklis.
\newblock On distributed averaging algorithms and quantization effects.
\newblock {\em IEEE Transactions on Automatic Control}, 54(11):2506--2517,
  2009.

\bibitem{nedic2009distributed}
Angelia Nedi\'{c} and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48--61, 2009.

\bibitem{nedich2016geometrically}
Angelia Nedich, Alex Olshevsky, and Wei Shi.
\newblock A geometrically convergent method for distributed optimization over
  time-varying graphs.
\newblock In {\em 2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages 1023--1029. IEEE, 2016.

\bibitem{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence {O(1/t)} for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 15(1):229--251, 2004.

\bibitem{nesterov2005smooth}
Yu~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Mathematical programming}, 103(1):127--152, 2005.

\bibitem{nesterov2007dual}
Yurii Nesterov.
\newblock Dual extrapolation and its applications to solving variational
  inequalities and related problems.
\newblock {\em Mathematical Programming}, 109(2):319--344, 2007.

\bibitem{Omidshafiei2017:rl}
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan~P. How, and John
  Vian.
\newblock Deep decentralized multi-task multi-agent reinforcement learning
  under partial observability.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, volume~70, pages 2681--2690. PMLR, 2017.

\bibitem{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban
  Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan
  Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu~Fang, Junjie Bai, and Soumith
  Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d~Alch\'{e}-Buc,
  E.~Fox, and R.~Garnett, editors, {\em Advances in Neural Information
  Processing Systems}, volume~32, pages 8026--8037. Curran Associates, Inc.,
  2019.

\bibitem{Pu2020:tracking}
S.~Pu and A.~Nedi\'{c}.
\newblock Distributed stochastic gradient tracking methods.
\newblock {\em Math. Program.}, 2020.

\bibitem{radford2015unsupervised}
Alec Radford, Luke Metz, and Soumith Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1511.06434}, 2015.

\bibitem{rogozin2021decentralized}
Alexander Rogozin, Alexander Beznosikov, Darina Dvinskikh, Dmitry Kovalev,
  Pavel Dvurechensky, and Alexander Gasnikov.
\newblock Decentralized distributed optimization for saddle point problems.
\newblock {\em arXiv preprint arXiv:2102.07758}, 2021.

\bibitem{rogozin2021accelerated}
Alexander Rogozin, Mikhail Bochko, Pavel Dvurechensky, Alexander Gasnikov, and
  Vladislav Lukoshkin.
\newblock An accelerated method for decentralized distributed stochastic
  optimization over time-varying graphs.
\newblock In {\em 2021 60th IEEE Conference on Decision and Control (CDC)},
  pages 3367--3373, 2021.

\bibitem{SAYED2014323}
Ali~H. Sayed.
\newblock Chapter 9 - diffusion adaptation over networks*the work was supported
  in part by nsf grants eecs-060126, eecs-0725441, ccf-0942936, and
  ccf-1011918*.
\newblock In Abdelhak~M. Zoubir, Mats Viberg, Rama Chellappa, and Sergios
  Theodoridis, editors, {\em Academic Press Library in Signal Processing:
  Volume 3}, volume~3 of {\em Academic Press Library in Signal Processing},
  pages 323--453. Elsevier, 2014.

\bibitem{sayed2013diffusion}
Ali~H Sayed, Sheng-Yuan Tu, Jianshu Chen, Xiaochuan Zhao, and Zaid~J Towfic.
\newblock Diffusion strategies for adaptation and learning over networks: an
  examination of distributed strategies and network behavior.
\newblock {\em IEEE Signal Processing Magazine}, 30(3):155--171, 2013.

\bibitem{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, 2017.

\bibitem{pmlr-v32-shamir14}
Ohad Shamir, Nati Srebro, and Tong Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In Eric~P. Xing and Tony Jebara, editors, {\em Proceedings of the
  31st International Conference on Machine Learning}, volume~32 of {\em
  Proceedings of Machine Learning Research}, pages 1000--1008, Bejing, China,
  22--24 Jun 2014. PMLR.

\bibitem{shi2015extra}
Wei Shi, Qing Ling, Gang Wu, and Wotao Yin.
\newblock {EXTRA}: An exact first-order algorithm for decentralized consensus
  optimization.
\newblock {\em SIAM Journal on Optimization}, 25(2):944--966, 2015.

\bibitem{stich2018local}
Sebastian~U Stich.
\newblock Local {SGD} converges fast and communicates little.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2019.

\bibitem{stich2019unified}
Sebastian~U Stich.
\newblock Unified optimal analysis of the (stochastic) gradient method.
\newblock {\em arXiv preprint arXiv:1907.04232}, 2019.

\bibitem{Tang2019:squeeze}
Hanlin Tang, Xiangru Lian, Shuang Qiu, Lei Yuan, Ce~Zhang, Tong Zhang, and
  Ji~Liu.
\newblock Deepsqueeze: Decentralization meets error-compensated compression.
\newblock {\em arXiv preprint arXiv:1907.07346}, 2019.

\bibitem{Tang2018:d2}
Hanlin Tang, Xiangru Lian, Ming Yan, Ce~Zhang, and Ji~Liu.
\newblock {D}$^2$: Decentralized training over decentralized data.
\newblock In {\em Proceedings of the 35th International Conference on Machine
  Learning (ICML)}, volume~80, pages 4848--4856. PMLR, 2018.

\bibitem{9054056}
Ioannis Tsaknakis, Mingyi Hong, and Sijia Liu.
\newblock Decentralized min-max optimization: Formulations, algorithms and
  applications in network poisoning attack.
\newblock In {\em ICASSP 2020 - 2020 IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP)}, pages 5755--5759, 2020.

\bibitem{Uribe:2018uk}
C{\'e}sar~A Uribe, Soomin Lee, and Alexander Gasnikov.
\newblock A dual approach for optimal algorithms in distributed optimization
  over networks.
\newblock {\em arXiv preprint arXiv:1809.00710}, 2018.

\bibitem{Vanhaesebrouck2017:personalization}
Paul Vanhaesebrouck, Aur{\'e}lien Bellet, and Marc Tommasi.
\newblock Decentralized collaborative learning of personalized models over
  networks.
\newblock In {\em Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, volume~54, pages 509--517.
  PMLR, 2017.

\bibitem{Wang2018:cooperativeSGD}
Jianyu Wang and Gauri Joshi.
\newblock Cooperative {SGD:} {A} unified framework for the design and analysis
  of communication-efficient {SGD} algorithms.
\newblock {\em arXiv preprint arXiv:1808.07576}, 2018.

\bibitem{Wei2012:distributedadmm}
E.~Wei and A.~Ozdaglar.
\newblock Distributed alternating direction method of multipliers.
\newblock In {\em 2012 IEEE 51st IEEE Conference on Decision and Control
  (CDC)}, pages 5445--5450, 2012.

\bibitem{woodworth2020minibatch}
Blake Woodworth, Kumar~Kshitij Patel, and Nathan Srebro.
\newblock Minibatch vs local {SGD} for heterogeneous distributed learning.
\newblock In {\em Advances in Neural Information Processing Systems 33
  (NeurIPS)}, 2020.

\bibitem{NEURIPS2021_d994e372}
Wenhan Xian, Feihu Huang, Yanfu Zhang, and Heng Huang.
\newblock A faster decentralized algorithm for nonconvex minimax problems.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 25865--25877. Curran Associates, Inc., 2021.

\bibitem{Xiao2014:averaging}
Lin Xiao and Stephen Boyd.
\newblock Fast linear iterations for distributed averaging.
\newblock {\em Systems \& Control Letters}, 53(1):65--78, 2004.

\bibitem{8272358}
Siyu Xie and Lei Guo.
\newblock Analysis of distributed adaptive filters based on diffusion
  strategies over sensor networks.
\newblock {\em IEEE Transactions on Automatic Control}, 63(11):3643--3658,
  2018.

\bibitem{zhou2019sgd}
Yi~Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh.
\newblock Sgd converges to global minimum in deep learning via star-convex
  path.
\newblock {\em arXiv preprint arXiv:1901.00451}, 2019.

\bibitem{Zinkevich2010:local}
Martin Zinkevich, Markus Weimer, Lihong Li, and Alex~J. Smola.
\newblock Parallelized stochastic gradient descent.
\newblock In {\em Advances in Neural Information Processing Systems 23}, pages
  2595--2603. Curran Associates, Inc., 2010.

\end{thebibliography}
