\begin{thebibliography}{41}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2023)Agarwal, Ghuge, and Nagarajan]{agarwal2023semi}
Arpit Agarwal, Rohan Ghuge, and Viswanath Nagarajan.
\newblock Semi-bandit learning for monotone stochastic optimization.
\newblock \emph{arXiv preprint arXiv:2312.15427}, 2023.

\bibitem[Auer et~al.(2002)Auer, Cesa-Bianchi, Freund, and Schapire]{auer2002nonstochastic}
Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert~E Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM journal on computing}, 32\penalty0 (1):\penalty0 48--77, 2002.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 263--272. PMLR, 2017.

\bibitem[Bertsekas(2023)]{bertsekas2023course}
Dimitri Bertsekas.
\newblock \emph{A course in reinforcement learning}.
\newblock Athena Scientific, 2023.

\bibitem[Biedenkapp et~al.(2021)Biedenkapp, Rajan, Hutter, and Lindauer]{biedenkapp2021temporl}
Andr{\'e} Biedenkapp, Raghu Rajan, Frank Hutter, and Marius Lindauer.
\newblock Temporl: Learning when to act.
\newblock In \emph{International Conference on Machine Learning}, pages 914--924. PMLR, 2021.

\bibitem[Boutilier et~al.(2018)Boutilier, Cohen, Hassidim, Mansour, Meshi, Mladenov, and Schuurmans]{boutilier2018planning}
Craig Boutilier, Alon Cohen, Avinatan Hassidim, Yishay Mansour, Ofer Meshi, Martin Mladenov, and Dale Schuurmans.
\newblock Planning and learning with stochastic action sets.
\newblock In \emph{Proceedings of the 27th International Joint Conference on Artificial Intelligence}, pages 4674--4682, 2018.

\bibitem[Bubeck et~al.(2012)Bubeck, Cesa-Bianchi, et~al.]{bubeck2012regret}
S{\'e}bastien Bubeck, Nicolo Cesa-Bianchi, et~al.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit problems.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 5\penalty0 (1):\penalty0 1--122, 2012.

\bibitem[Camacho et~al.(2007)Camacho, Bordons, Camacho, and Bordons]{camacho2007model}
Eduardo~F Camacho, Carlos Bordons, Eduardo~F Camacho, and Carlos Bordons.
\newblock \emph{Model predictive control}.
\newblock Springer, 2007.

\bibitem[Chung et~al.(2024)Chung, Anokhin, and Krueger]{chung2024thinker}
Stephen Chung, Ivan Anokhin, and David Krueger.
\newblock Thinker: learning to plan and act.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem[Correa et~al.(2017)Correa, Foncea, Hoeksma, Oosterwijk, and Vredeveld]{correa2017posted}
Jos{\'e} Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld.
\newblock Posted price mechanisms for a random stream of customers.
\newblock In \emph{Proceedings of the 2017 ACM Conference on Economics and Computation}, pages 169--186, 2017.

\bibitem[Correa et~al.(2019)Correa, Foncea, Hoeksma, Oosterwijk, and Vredeveld]{correa2019recent}
Jose Correa, Patricio Foncea, Ruben Hoeksma, Tim Oosterwijk, and Tjark Vredeveld.
\newblock Recent developments in prophet inequalities.
\newblock \emph{ACM SIGecom Exchanges}, 17\penalty0 (1):\penalty0 61--70, 2019.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 1507--1516, 2019.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and Valko]{domingues2021episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited.
\newblock In \emph{Algorithmic Learning Theory}, pages 578--598. PMLR, 2021.

\bibitem[Efroni et~al.(2019{\natexlab{a}})Efroni, Dalal, Scherrer, and Mannor]{efroni2019combine}
Yonathan Efroni, Gal Dalal, Bruno Scherrer, and Shie Mannor.
\newblock How to combine tree-search methods in reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~33, pages 3494--3501, 2019{\natexlab{a}}.

\bibitem[Efroni et~al.(2019{\natexlab{b}})Efroni, Merlis, Ghavamzadeh, and Mannor]{efroni2019tight}
Yonathan Efroni, Nadav Merlis, Mohammad Ghavamzadeh, and Shie Mannor.
\newblock Tight regret bounds for model-based reinforcement learning with greedy policies.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 12224--12234, 2019{\natexlab{b}}.

\bibitem[Efroni et~al.(2020)Efroni, Ghavamzadeh, and Mannor]{efroni2020online}
Yonathan Efroni, Mohammad Ghavamzadeh, and Shie Mannor.
\newblock Online planning with lookahead policies.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14024--14033, 2020.

\bibitem[Efroni et~al.(2021)Efroni, Merlis, Saha, and Mannor]{efroni2021confidence}
Yonathan Efroni, Nadav Merlis, Aadirupa Saha, and Shie Mannor.
\newblock Confidence-budget matching for sequential budgeted learning.
\newblock In \emph{International Conference on Machine Learning}, pages 2937--2947. PMLR, 2021.

\bibitem[El~Shar and Jiang(2020)]{el2020lookahead}
Ibrahim El~Shar and Daniel Jiang.
\newblock Lookahead-bounded q-learning.
\newblock In \emph{International Conference on Machine Learning}, pages 8665--8675. PMLR, 2020.

\bibitem[Fournier and Guillin(2015)]{fournier2015rate}
Nicolas Fournier and Arnaud Guillin.
\newblock On the rate of convergence in wasserstein distance of the empirical measure.
\newblock \emph{Probability theory and related fields}, 162\penalty0 (3):\penalty0 707--738, 2015.

\bibitem[Gatmiry et~al.(2024)Gatmiry, Kesselheim, Singla, and Wang]{gatmiry2024bandit}
Khashayar Gatmiry, Thomas Kesselheim, Sahil Singla, and Yifan Wang.
\newblock Bandit algorithms for prophet inequality and pandora's box.
\newblock In \emph{Proceedings of the 2024 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA)}, pages 462--500. SIAM, 2024.

\bibitem[Huang et~al.(2019)Huang, Kavitha, and Zhu]{huang2019continuous}
Yunhan Huang, Veeraruna Kavitha, and Quanyan Zhu.
\newblock Continuous-time markov decision processes with controlled observations.
\newblock In \emph{2019 57th Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, pages 32--39. IEEE, 2019.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Chi Jin, Zeyuan Allen-Zhu, Sebastien Bubeck, and Michael~I Jordan.
\newblock Is q-learning provably efficient?
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function approximation.
\newblock In \emph{Conference on learning theory}, pages 2137--2143. PMLR, 2020.

\bibitem[Li et~al.(2019)Li, Chen, and Li]{li2019online}
Yingying Li, Xin Chen, and Na~Li.
\newblock Online optimal control with linear dynamics and predictions: Algorithms and regret analysis.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Lin et~al.(2021)Lin, Hu, Shi, Sun, Qu, and Wierman]{lin2021perturbation}
Yiheng Lin, Yang Hu, Guanya Shi, Haoyuan Sun, Guannan Qu, and Adam Wierman.
\newblock Perturbation-based regret analysis of predictive control in linear time varying systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 5174--5185, 2021.

\bibitem[Lin et~al.(2022)Lin, Hu, Qu, Li, and Wierman]{lin2022bounded}
Yiheng Lin, Yang Hu, Guannan Qu, Tongxin Li, and Adam Wierman.
\newblock Bounded-regret mpc via perturbation analysis: Prediction error, constraints, and nonlinearity.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 36174--36187, 2022.

\bibitem[Maurer and Pontil(2009)]{maurer2009empirical}
Andreas Maurer and Massimiliano Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock In \emph{Conference on learning theory}, 2009.

\bibitem[Merlis et~al.(2024)Merlis, Baudry, and Perchet]{merlis2024value}
Nadav Merlis, Dorian Baudry, and Vianney Perchet.
\newblock The value of reward lookahead in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2403.11637}, 2024.

\bibitem[Moerland et~al.(2020)Moerland, Deichler, Baldi, Broekens, and Jonker]{moerland2020think}
Thomas~M Moerland, Anna Deichler, Simone Baldi, Joost Broekens, and Catholijn~M Jonker.
\newblock Think neither too fast nor too slow: The computational trade-off between planning and reinforcement learning.
\newblock In \emph{Proceedings of the International Conference on Automated Planning and Scheduling (ICAPS), Nancy, France}, pages 16--20, 2020.

\bibitem[Nikolova and Karger(2008)]{nikolova2008route}
Evdokia Nikolova and David~R Karger.
\newblock Route planning under uncertainty: The canadian traveller problem.
\newblock In \emph{AAAI}, pages 969--974, 2008.

\bibitem[Puterman(2014)]{puterman2014markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic programming}.
\newblock John Wiley \& Sons, 2014.

\bibitem[Rosenberg et~al.(2023)Rosenberg, Hallak, Mannor, Chechik, and Dalal]{rosenberg2023planning}
Aviv Rosenberg, Assaf Hallak, Shie Mannor, Gal Chechik, and Gal Dalal.
\newblock Planning and learning with adaptive lookahead.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~37, pages 9606--9613, 2023.

\bibitem[Schrittwieser et~al.(2020)Schrittwieser, Antonoglou, Hubert, Simonyan, Sifre, Schmitt, Guez, Lockhart, Hassabis, Graepel, et~al.]{schrittwieser2020mastering}
Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, et~al.
\newblock Mastering atari, go, chess and shogi by planning with a learned model.
\newblock \emph{Nature}, 588\penalty0 (7839):\penalty0 604--609, 2020.

\bibitem[Simchowitz and Jamieson(2019)]{simchowitz2019non}
Max Simchowitz and Kevin~G Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages 1153--1162, 2019.

\bibitem[Tamar et~al.(2017)Tamar, Thomas, Zhang, Levine, and Abbeel]{tamar2017learning}
Aviv Tamar, Garrett Thomas, Tianhao Zhang, Sergey Levine, and Pieter Abbeel.
\newblock Learning from the hindsight planâ€”episodic mpc improvement.
\newblock In \emph{2017 IEEE International Conference on Robotics and Automation (ICRA)}, pages 336--343. IEEE, 2017.

\bibitem[Yu et~al.(2020)Yu, Shi, Chung, Yue, and Wierman]{yu2020power}
Chenkai Yu, Guanya Shi, Soon-Jo Chung, Yisong Yue, and Adam Wierman.
\newblock The power of predictions in online control.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 1994--2004, 2020.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds.
\newblock In \emph{International Conference on Machine Learning}, pages 7304--7312. PMLR, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Li, and Li]{zhang2021regret}
Runyu Zhang, Yingying Li, and Na~Li.
\newblock On the regret analysis of online lqr control with predictions.
\newblock In \emph{2021 American Control Conference (ACC)}, pages 697--703. IEEE, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Ji, and Du]{zhang2021reinforcement}
Zihan Zhang, Xiangyang Ji, and Simon Du.
\newblock Is reinforcement learning more difficult than bandits? a near-optimal algorithm escaping the curse of horizon.
\newblock In \emph{Conference on Learning Theory}, pages 4528--4531. PMLR, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2023)Zhang, Chen, Lee, and Du]{zhang2023settling}
Zihan Zhang, Yuxin Chen, Jason~D Lee, and Simon~S Du.
\newblock Settling the sample complexity of online reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2307.13586}, 2023.

\end{thebibliography}
