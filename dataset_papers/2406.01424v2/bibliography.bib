@inproceedings{petrov2024universal,
  title={Prompting a Pretrained Transformer Can Be a Universal Approximator},
  author={Petrov, Aleksandar and Torr, Philip HS and Bibi, Adel},
  booktitle = {International Conference on Machine Learning},
  journal={arXiv preprint arXiv:2402.14753},
  url={https://arxiv.org/abs/2402.14753},
  year={2024}
}

@inproceedings{petrov2023prompting,
  title={When Do Prompting and Prefix-Tuning Work? {A} Theory of Capabilities and Limitations},
  author={Petrov, Aleksandar and Torr, Philip HS and Bibi, Adel},
  booktitle={International Conference on Learning Representations},
  url={https://arxiv.org/abs/2310.19698},
  year={2024}
}

@article{shazeer2019fast,
  title={Fast transformer decoding: One write-head is all you need},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:1911.02150},
  url={https://arxiv.org/abs/1911.02150},
  year={2019}
}

@article{de2024griffin,
  title={Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  author={De, Soham and Smith, Samuel L and Fernando, Anushan and Botev, Aleksandar and Cristian-Muraru, George and Gu, Albert and Haroun, Ruba and Berrada, Leonard and Chen, Yutian and Srinivasan, Srivatsan and others},
  journal={arXiv preprint arXiv:2402.19427},
  url={https://arxiv.org/abs/2402.19427},
  year={2024}
}

@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  url={https://arxiv.org/abs/2004.05150},  
  year={2020}
}

@article{gers2000learning,
  title={Learning to forget: Continual prediction with LSTM},
  author={Gers, Felix A and Schmidhuber, J{\"u}rgen and Cummins, Fred},
  journal={Neural computation},
  volume={12},
  number={10},
  pages={2451--2471},
  year={2000},
}


@inproceedings{vaswani2017attention,
	title        = {Attention is All you Need},
	author       = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year         = 2017,
	booktitle    = {Advances in Neural Information Processing Systems},
	url          = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}


@inproceedings{hambardzumyan2021warp,
    title = "{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming",
    author = "Hambardzumyan, Karen  and
      Khachatrian, Hrant  and
      May, Jonathan",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    year = "2021",
    url = "https://aclanthology.org/2021.acl-long.381",
}

@inproceedings{lester2021power,
    title = "The Power of Scale for Parameter-Efficient Prompt Tuning",
    author = "Lester, Brian  and
      Al-Rfou, Rami  and
      Constant, Noah",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    year = "2021",
    url = "https://aclanthology.org/2021.emnlp-main.243",
}

@inproceedings{qin2021learning,
    title = "Learning How to Ask: {Querying} {LM}s with Mixtures of Soft Prompts",
    author = "Qin, Guanghui  and
      Eisner, Jason",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    year = "2021",
    url = "https://aclanthology.org/2021.naacl-main.410",
}

@article{solazzi2004regularizing,
title = {Regularising neural networks using flexible multivariate activation function},
journal = {Neural Networks},
volume = {17},
number = {2},
pages = {247-260},
year = {2004},
url = {https://www.sciencedirect.com/science/article/pii/S0893608003001898},
author = {Mirko Solazzi and Aurelio Uncini},
}

@book{hassoun1995fundamentals,
  title={Fundamentals of artificial neural networks},
  author={Hassoun, Mohamad H},
  year={1995},
  publisher={MIT press}
}

@inproceedings{li2021prefix,
  title={{Prefix-Tuning}: {Optimizing} Continuous Prompts for Generation},
  author={Li, Xiang Lisa and Liang, Percy},
  booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
  year={2021},
  url={https://aclanthology.org/2021.acl-long.353/}
}

@inproceedings{liu2022p,
    title = "{P-Tuning}: {Prompt} Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
    author = "Liu, Xiao  and
      Ji, Kaixuan  and
      Fu, Yicheng  and
      Tam, Weng  and
      Du, Zhengxiao  and
      Yang, Zhilin  and
      Tang, Jie",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    url = "https://aclanthology.org/2022.acl-short.8",
    year= "2022",
}

@misc{telgarsky2021deep,
  title={Deep learning theory lecture notes},
  author={Matus Telgarsky},
  year={2021},
  url={https://mjt.cs.illinois.edu/dlt}
}


@article{jiang2023brief,
  title={A Brief Survey on the Approximation Theory for Sequence Modelling},
  author={Jiang, Haotian and Li, Qianxiao and Li, Zhong and Wang, Shida},
  journal={arXiv preprint arXiv:2302.13752},
  year={2023},
  url={https://arxiv.org/abs/2302.13752},
}

@inproceedings{yun2019transformers,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://arxiv.org/abs/1912.10077},
}

@article{jiang2023approximation,
  title={Approximation theory of transformer networks for sequence modeling},
  author={Jiang, Haotian and Li, Qianxiao},
  journal={arXiv preprint arXiv:2305.18475},
  year={2023},
  url={https://arxiv.org/abs/2305.18475},
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  url={https://arxiv.org/abs/2312.00752},
  year={2023}
}

@inproceedings{orvieto2023resurrecting,
author = {Orvieto, Antonio and Smith, Samuel L and Gu, Albert and Fernando, Anushan and Gulcehre, Caglar and Pascanu, Razvan and De, Soham},
title = {Resurrecting recurrent neural networks for long sequences},
year = {2023},
booktitle = {International Conference on Machine Learning},
url={https://arxiv.org/abs/2303.06349}
}

@inproceedings{gu2020hippo,
  title={{HiPPO}: {Recurrent} memory with optimal polynomial projections},
  author={Gu, Albert and Dao, Tri and Ermon, Stefano and Rudra, Atri and R{\'e}, Christopher},
  booktitle={Advances in Neural Information Processing Systems},
  url={https://proceedings.neurips.cc/paper/2020/hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html},
  year={2020}
}

@inproceedings{fu2023hungry,
title={{Hungry Hungry Hippos}: {Towards} Language Modeling with State Space Models},
author={Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
booktitle={International Conference on Learning Representations},
year={2023},
url={https://openreview.net/forum?id=COZDy0WYGg}
}

@article{akyurek2024context,
  title={In-Context Language Learning: Arhitectures and Algorithms},
  author={Aky{\"u}rek, Ekin and Wang, Bailin and Kim, Yoon and Andreas, Jacob},
  journal={arXiv preprint arXiv:2401.12973},
  url={https://arxiv.org/abs/2401.12973},
  year={2024}
}

@inproceedings{wang2024state,
  title={State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory},
  author={Wang, Shida and Xue, Beichen},
  booktitle={Advances in Neural Information Processing Systems},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/ea8608c6258450e75b3443ec8022fb2e-Abstract-Conference.html},
  year={2023}
}

@inproceedings{schafer2006recurrent,
  title={Recurrent neural networks are universal approximators},
  author={Sch{\"a}fer, Anton Maximilian and Zimmermann, Hans Georg},
  booktitle={Artificial Neural Networks--ICANN 2006: 16th International Conference, Athens, Greece, September 10-14, 2006. Proceedings, Part I 16},
  pages={632--640},
  year={2006},
  organization={Springer}
}


@inproceedings{kolmogorov1957representation,
  title={On the representation of continuous functions of many variables by superposition of continuous functions of one variable and addition},
  author={Kolmogorov, Andrei Nikolaevich},
  booktitle={Doklady Akademii Nauk},
  volume={114},
  number={5},
  pages={953--956},
  year={1957},
  organization={Russian Academy of Sciences}
}

@article{schmidt2021kolmogorov,
    title = {The {Kolmogorovâ€“Arnold} representation theorem revisited},
    journal = {Neural Networks},
    volume = {137},
    pages = {119-126},
    year = {2021},
    issn = {0893-6080},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608021000289},
    author = {Johannes Schmidt-Hieber},
}

@article{hahm2004approximation,
  title={An approximation by neural networks with a fixed weight},
  author={Hahm, Nahmwoo and Hong, Bum Il},
  journal={Computers \& Mathematics with Applications},
  volume={47},
  number={12},
  pages={1897--1903},
  year={2004},
  url={https://dl.acm.org/doi/abs/10.1016/j.camwa.2003.06.008}
}

@INPROCEEDINGS{feilong2010construction,
  author={Cao, Feilong and Xie, Tingfan},
  booktitle={2010 International Conference on Machine Learning and Cybernetics},
  title={The construction and approximation for feedforword neural networks with fixed weights}, 
  year={2010},
  volume={6},
  pages={3164-3168},
  url={https://ieeexplore.ieee.org/abstract/document/5580706}
}

@article{lin2013approximation,
title = {Approximation by neural networks with scattered data},
journal = {Applied Mathematics and Computation},
volume = {224},
pages = {29-35},
year = {2013},
url = {https://www.sciencedirect.com/science/article/pii/S0096300313008631},
author = {Shaobo Lin and Xiaofei Guo and Feilong Cao and Zongben Xu},
}

@article{guliyev2018approximation,
  title={On the approximation by single hidden layer feedforward neural networks with fixed weights},
  author={Guliyev, Namig J and Ismailov, Vugar E},
  journal={Neural Networks},
  volume={98},
  pages={296--304},
  year={2018},
  url={https://www.sciencedirect.com/science/article/pii/S0893608017302927}
}

@article{guliyev2018approximationTWO,
  title={Approximation capability of two hidden layer feedforward neural networks with fixed weights},
  author={Guliyev, Namig J and Ismailov, Vugar E},
  journal={Neurocomputing},
  volume={316},
  pages={262--269},
  year={2018},
  url={https://www.sciencedirect.com/science/article/pii/S0925231218309111}
}

@article{ito1992approximation,
  title={Approximation of continuous functions on Rd by linear combinations of shifted rotations of a sigmoid function with and without scaling},
  author={Ito, Yoshifusa},
  journal={Neural Networks},
  volume={5},
  number={1},
  pages={105--115},
  year={1992},
  url={https://www.sciencedirect.com/science/article/pii/S0893608005800097}
}

@article{ismailov2012approximation,
  title={Approximation by neural networks with weights varying on a finite set of directions},
  author={Ismailov, Vugar E},
  journal={Journal of Mathematical Analysis and Applications},
  volume={389},
  number={1},
  pages={72--83},
  year={2012},
  url={https://www.sciencedirect.com/science/article/pii/S0022247X11010572}
}

@article{fan2023quasi,
  title={Quasi-Equivalence between Width and Depth of Neural Networks},
  author={Fan, Fenglei and Lai, Rongjie and Wang, Ge},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={183},
  pages={1--22},
  year={2023},
  url={https://www.jmlr.org/papers/v24/21-0579.html}
}

@article{zhang2024deep,
  title={Deep Network Approximation: Beyond ReLU to Diverse Activation Functions},
  author={Zhang, Shijun and Lu, Jianfeng and Zhao, Hongkai},
  journal={Journal of Machine Learning Research},
  volume={25},
  year={2024},
  url={https://www.jmlr.org/papers/v25/23-0912.html}
}

@article{ismailov2024approximation,
  title={Approximation error of single hidden layer neural networks with fixed weights},
  author={Ismailov, Vugar E},
  journal={Information Processing Letters},
  volume={185},
  pages={106467},
  year={2024},
  url={https://www.sciencedirect.com/science/article/pii/S0020019023001102}
}


@inproceedings{lee2023exploring,
  title={Exploring the Relationship Between Model Architecture and In-Context Learning Ability},
  author={Lee, Ivan and Jiang, Nan and Berg-Kirkpatrick, Taylor},
  booktitle={International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=Qwq4cpLtoX}
}

@inproceedings{xie2021explanation,
  title={An Explanation of In-context Learning as Implicit {Bayesian} Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=RdJVFCHjUMI}
}


@article{botev2024recurrentgemma,
  title={{RecurrentGemma}: Moving Past Transformers for Efficient Open Language Models},
  author={Botev, Aleksandar and De, Soham and Smith, Samuel L and Fernando, Anushan and Muraru, George-Cristian and Haroun, Ruba and Berrada, Leonard and Pascanu, Razvan and Sessa, Pier Giuseppe and Dadashi, Robert and others},
  journal={arXiv preprint arXiv:2404.07839},
  url={https://arxiv.org/abs/2404.07839},
  year={2024}
}

@inproceedings{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
  url={https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html}
}

@article{liu2023pretrain,
author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
title = {Pre-Train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing},
year = {2023},
journal={ACM Computing Surveys},
url={https://dl.acm.org/doi/full/10.1145/3560815}
}

@article{lamalfa2023language,
      title={{Language Models as a Service}: Overview of a New Paradigm and its Challenges}, 
      author={Emanuele {La Malfa} and Aleksandar Petrov and Simon Frieder and Christoph Weinhuber and Ryan Burnell and Raza Nazar and Anthony G. Cohn and Nigel Shadbolt and Michael Wooldridge},
      journal={arXiv preprint arXiv:2309.16573},
      url={https://arxiv.org/abs/2309.16573},
      year={2023},
}

@inproceedings{liu2022design,
author = {Liu, Vivian and Chilton, Lydia B},
title = {Design Guidelines for Prompt Engineering Text-to-Image Generative Models},
year = {2022},
url = {https://doi.org/10.1145/3491102.3501825},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
}

@article{sahoo2024systematic,
      title={A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications}, 
      author={Pranab Sahoo and Ayush Kumar Singh and Sriparna Saha and Vinija Jain and Samrat Mondal and Aman Chadha},
      year={2024},
      journal={arXiv preprint arXiv:2402.07927},
      url={https://arxiv.org/abs/2402.07927},
}

@article{lialin2023scaling,
  title={Scaling down to scale up: {A} guide to parameter-efficient fine-tuning},
  author={Lialin, Vladislav and Deshpande, Vijeta and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2303.15647},
  year={2023},
  url={https://arxiv.org/abs/2303.15647}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  url={https://doi.org/10.1007/BF02551274}
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information Theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  url={https://doi.org/10.1109/18.256500},
}

@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  url={https://arxiv.org/abs/1509.08101},
  year={2015}
}

@inproceedings{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
  url={https://papers.nips.cc/paper_files/paper/2022/hash/8bb0d291acd4acf06ef112099c16f326-Abstract-Conference.html}
}

@inproceedings{wei2021finetuned,
  title={Finetuned Language Models are Zero-Shot Learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=gEZrGCozdqR}
}

@article{dong2022survey,
  title={A survey on in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  url={https://arxiv.org/abs/2301.00234},
  year={2022}
}

@inproceedings{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2015},
  url={https://arxiv.org/abs/1409.0473},
}

@ARTICLE{bengio1994learning,
  author={Bengio, Yoshua and Simard, Patrice and Frasconi, Paolo},
  journal={IEEE Transactions on Neural Networks}, 
  title={Learning long-term dependencies with gradient descent is difficult}, 
  year={1994},
  volume={5},
  number={2},
  pages={157-166},
  url={https://ieeexplore.ieee.org/document/279181}
}

@inproceedings{gu2021efficiently,
  title={Efficiently Modeling Long Sequences with Structured State Spaces},
  author={Gu, Albert and Goel, Karan and Re, Christopher},
  booktitle={International Conference on Learning Representations},
  year={2021},
  url={https://openreview.net/forum?id=uYLFoz1vlAC},
}

@ARTICLE{SciPy,
  author  = {Virtanen, Pauli and Gommers, Ralf and Oliphant, Travis E. and
            Haberland, Matt and Reddy, Tyler and Cournapeau, David and
            Burovski, Evgeni and Peterson, Pearu and Weckesser, Warren and
            Bright, Jonathan and {van der Walt}, St{\'e}fan J. and
            Brett, Matthew and Wilson, Joshua and Millman, K. Jarrod and
            Mayorov, Nikolay and Nelson, Andrew R. J. and Jones, Eric and
            Kern, Robert and Larson, Eric and Carey, C J and
            Polat, {\.I}lhan and Feng, Yu and Moore, Eric W. and
            {VanderPlas}, Jake and Laxalde, Denis and Perktold, Josef and
            Cimrman, Robert and Henriksen, Ian and Quintero, E. A. and
            Harris, Charles R. and Archibald, Anne M. and
            Ribeiro, Ant{\^o}nio H. and Pedregosa, Fabian and
            {van Mulbregt}, Paul and {SciPy 1.0 Contributors}},
  title   = {{SciPy} 1.0: Fundamental Algorithms for Scientific
            Computing in {Python}},
  journal = {Nature Methods},
  year    = {2020},
  volume  = {17},
  pages   = {261--272},
  url  = {https://doi.org/10.1038/s41592-019-0686-2},
}

@article{SymPy,
     title = {{SymPy}: Symbolic computing in {Python}},
     author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \v{C}ert\'{i}k, Ond\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, AMiT and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\v{c}ka, \v{S}t\v{e}p\'{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
     year = 2017,
     volume = 3,
     journal = {PeerJ Computer Science},
     url = {https://doi.org/10.7717/peerj-cs.103},
    }


@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  year={2021},
  url={http://proceedings.mlr.press/v139/weiss21a.html}
}

@inproceedings{lindner2024tracr,
  title={Tracr: {Compiled} transformers as a laboratory for interpretability},
  author={Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Tom and Mikulik, Vladimir},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/771155abaae744e08576f1f3b4b7ac0d-Abstract-Conference.html}
}

@article{blum1991approximation,
  title={Approximation theory and feedforward networks},
  author={Blum, Edward K and Li, Leong Kwan},
  journal={Neural networks},
  volume={4},
  number={4},
  pages={511--515},
  year={1991},
}

@article{scarselli1998universal,
  title={Universal approximation using feedforward neural networks: A survey of some existing methods, and some new results},
  author={Scarselli, Franco and Tsoi, Ah Chung},
  journal={Neural networks},
  volume={11},
  number={1},
  pages={15--37},
  year={1998},
}

@article{schmidt2021kolmogorov,
    title = {The {Kolmogorovâ€“Arnold} representation theorem revisited},
    journal = {Neural Networks},
    volume = {137},
    pages = {119-126},
    year = {2021},
    issn = {0893-6080},
    url = {https://www.sciencedirect.com/science/article/pii/S0893608021000289},
    author = {Johannes Schmidt-Hieber},
}

@inproceedings{jayakumar2020multiplicative,
  title={Multiplicative interactions and where to find them},
  author={Jayakumar, Siddhant M and Czarnecki, Wojciech M and Menick, Jacob and Schwarz, Jonathan and Rae, Jack and Osindero, Simon and Teh, Yee Whye and Harley, Tim and Pascanu, Razvan},
  year={2020},
  booktitle={International Conference on Learning Representations},
  url={https://openreview.net/forum?id=rylnK6VtDH},
}

@article{omlin1996constructing,
  title={Constructing deterministic finite-state automata in recurrent neural networks},
  author={Omlin, Christian W and Giles, C Lee},
  journal={Journal of the ACM (JACM)},
  volume={43},
  number={6},
  pages={937--972},
  year={1996},
  url={https://dl.acm.org/doi/abs/10.1145/235809.235811}
}

@inproceedings{gupta2022diagonal,
  title={Diagonal state spaces are as effective as structured state spaces},
  author={Gupta, Ankit and Gu, Albert and Berant, Jonathan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/9156b0f6dfa9bbd18c79cc459ef5d61c-Abstract-Conference.html}
}

@article{girosi1989representation,
    author = {Girosi, Federico and Poggio, Tomaso},
    title = "Representation Properties of Networks: {Kolmogorov's} Theorem Is Irrelevant",
    journal = {Neural Computation},
    volume = {1},
    number = {4},
    pages = {465-469},
    year = {1989},
    url = {https://doi.org/10.1162/neco.1989.1.4.465},
}

@article{la2024code,
  title={Code simulation challenges for large language models},
  author={La Malfa, Emanuele and Weinhuber, Christoph and Torre, Orazio and Lin, Fangru and Cohn, Anthony and Shadbolt, Nigel and Wooldridge, Michael},
  journal={arXiv preprint arXiv:2401.09074},
  url={https://arxiv.org/abs/2401.09074},
  year={2024}
}

@article{sanford2024transformers,
  title={Transformers, parallel computation, and logarithmic depth},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  journal={arXiv preprint arXiv:2402.09268},
  url={https://arxiv.org/abs/2402.09268},
  year={2024}
}

@inproceedings{zhou2023algorithms,
  title={What algorithms can transformers learn? {A} study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  booktitle={International Conference on Learning Representations},
  url={https://arxiv.org/abs/2310.16028},
  year={2024}
}


@InProceedings{giannou23looped,
  title = 	 {Looped Transformers as Programmable Computers},
  author =       {Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-Yong and Lee, Kangwook and Lee, Jason D. and Papailiopoulos, Dimitris},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023},
  url = 	 {https://proceedings.mlr.press/v202/giannou23a.html},
}

@inproceedings{friedman2024learning,
  title={Learning transformer programs},
  author={Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  booktitle={Advances in Neural Information Processing Systems},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/995f693b73050f90977ed2828202645c-Abstract-Conference.html},
  year={2023}
}

@inproceedings{geiger2024finding,
  title={Finding alignments between interpretable causal variables and distributed neural representations},
  author={Geiger, Atticus and Wu, Zhengxuan and Potts, Christopher and Icard, Thomas and Goodman, Noah},
  booktitle={Causal Learning and Reasoning},
  year={2024},
}

@article{conmy2023towards,
  title={Towards automated circuit discovery for mechanistic interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri{\`a}},
  journal={Advances in Neural Information Processing Systems},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/34e1dbe95d34d7ebaf99b9bcaeb5b2be-Abstract-Conference.html},
  year={2023}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  url={https://arxiv.org/abs/2303.03846},
  year={2023}
}
    
@ARTICLE{amari1992rnns,
  author={Amari, Shun-ichi},
  journal={IEEE Transactions on Computers}, 
  title={Learning Patterns and Pattern Sequences by Self-Organizing Nets of Threshold Elements}, 
  year={1972},
  volume={C-21},
  number={11},
  pages={1197-1206},
  url={https://doi.org/10.1109/T-C.1972.223477},
}
    
@ARTICLE{lecunn1989backprop,
  author={LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  journal={Neural Computation}, 
  title={Backpropagation Applied to Handwritten Zip Code Recognition}, 
  year={1989},
  volume={1},
  number={4},
  pages={541-551},
  url={https://doi.org/10.1162/neco.1989.1.4.541},
}

@ARTICLE{waibel1989phoneme,
  author={Waibel, A. and Hanazawa, T. and Hinton, G. and Shikano, K. and Lang, K.J.},
  journal={IEEE Transactions on Acoustics, Speech, and Signal Processing}, 
  title={Phoneme recognition using time-delay neural networks}, 
  year={1989},
  volume={37},
  number={3},
  pages={328-339},
  url={https://doi.org/10.1109/29.21701},
}
    
@inproceedings{cho2014learning,
    title = "Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Gulcehre, Caglar  and
      Bahdanau, Dzmitry  and
      Bougares, Fethi  and
      Schwenk, Holger  and
      Bengio, Yoshua},
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    year = "2014",
    url = "https://aclanthology.org/D14-1179",
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural Computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  url={https://ieeexplore.ieee.org/abstract/document/6795963},
}

@inproceedings{garg2022can,
  title={What can transformers learn in-context? {A} case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  booktitle={Advances in Neural Information Processing Systems},
  year={2022},
  url={https://proceedings.neurips.cc/paper_files/paper/2022/hash/c529dba08a146ea8d6cf715ae8930cbe-Abstract-Conference.html}
}
    

@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? {Investigations} with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=0g0X4H8yN4I}
}

@inproceedings{coda2023meta,
  title={Meta-in-context learning in large language models},
  author={Coda-Forno, Julian and Binz, Marcel and Akata, Zeynep and Botvinick, Matt and Wang, Jane and Schulz, Eric},
  booktitle={Advances in Neural Information Processing Systems},
  pages={65189--65201},
  year={2023},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/cda04d7ea67ea1376bf8c6962d8541e0-Abstract-Conference.html}
}

@inproceedings{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  booktitle={Advances in neural information processing systems},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/b2e63e36c57e153b9015fece2352a9f9-Abstract-Conference.html},
  year={2023}
}
    

@InProceedings{li2023transformers,
  title = 	 {Transformers as Algorithms: Generalization and Stability in In-context Learning},
  author =       {Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023},
  url = 	 {https://proceedings.mlr.press/v202/li23l.html},
}

@article{han2023context,
  title={In-context learning of large language models explained as kernel regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  journal={arXiv preprint arXiv:2305.12766},
  url={https://arxiv.org/abs/2305.12766},
  year={2023}
}
    
@article{fu2023transformers,
  title={Transformers learn higher-order optimization methods for in-context learning: A study with linear models},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2310.17086},
  url={https://arxiv.org/abs/2310.17086},
  year={2023}
}

@article{zhang2023trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  url={https://arxiv.org/abs/2306.09927},  
  year={2023}
}


@InProceedings{oswald23transformers,
  title = 	 {Transformers Learn In-Context by Gradient Descent},
  author =       {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo\~ao and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle = 	 {International Conference on Machine Learning},
  year = 	 {2023},
  url = 	 {https://proceedings.mlr.press/v202/von-oswald23a.html},
}

@article{von2023uncovering,
  title={Uncovering mesa-optimization algorithms in transformers},
  author={von Oswald, Johannes and Niklasson, Eyvind and Schlegel, Maximilian and Kobayashi, Seijin and Zucchet, Nicolas and Scherrer, Nino and Miller, Nolan and Sandler, Mark and Vladymyrov, Max and Pascanu, Razvan and Sacramento, Jo\~ao },
  journal={arXiv preprint arXiv:2309.05858},
  year={2023},
  url={https://arxiv.org/abs/2309.05858},  
}

@inproceedings{dai2023gpt,
    title = "Why Can {GPT} Learn In-Context? {Language} Models Secretly Perform Gradient Descent as Meta-Optimizers",
    author = "Dai, Damai  and
      Sun, Yutao  and
      Dong, Li  and
      Hao, Yaru  and
      Ma, Shuming  and
      Sui, Zhifang  and
      Wei, Furu",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2023",
    year = "2023",
    url = "https://aclanthology.org/2023.findings-acl.247",
}

@inproceedings{ahn2023transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  booktitle={Advances in Neural Information Processing Systems},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/8ed3d610ea4b68e7afb30ea7d01422c6-Abstract-Conference.html},
  year={2023}
}

@inproceedings{ziyin2020neural,
  title={Neural networks fail to learn periodic functions and how to fix it},
  author={Ziyin, Liu and Hartwig, Tilman and Ueda, Masahito},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020},
  url={https://proceedings.neurips.cc/paper/2020/hash/1160453108d3e537255e9f7b931f4e90-Abstract.html},
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and pattern Recognition},
  year={2016},
  url={https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
}
    
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016},
  url={https://arxiv.org/abs/1607.06450},  
}

@inproceedings{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  booktitle={Advances in Neural Information Processing Systems},
  url={https://proceedings.neurips.cc/paper/2019/hash/1e8a19426224ca89e83cef47f1e7f53b-Abstract.html},
  year={2019}
}

@article{su2024roformer,
  title={Roformer: {Enhanced} transformer with rotary position embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  year={2024},
  url={https://arxiv.org/abs/2104.09864},
}

@article{wang2024universality,
  title={Universality and limitations of prompt tuning},
  author={Wang, Yihan and Chauhan, Jatin and Wang, Wei and Hsieh, Cho-Jui},
  journal={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://proceedings.neurips.cc/paper_files/paper/2023/hash/eef6aecfe050b556c6a48d9c16b15558-Abstract-Conference.html},
}

@article{boyd1985fading,
  title={Fading memory and the problem of approximating nonlinear operators with {Volterra} series},
  author={Boyd, Stephen and Chua, Leon},
  journal={IEEE Transactions on Circuits and Systems},
  volume={32},
  number={11},
  pages={1150--1161},
  year={1985},
  publisher={IEEE}
}