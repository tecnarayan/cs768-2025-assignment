\begin{thebibliography}{10}

\bibitem{abu2019mixhop}
S.~Abu-El-Haija, B.~Perozzi, A.~Kapoor, N.~Alipourfard, K.~Lerman,
  H.~Harutyunyan, G.~Ver~Steeg, and A.~Galstyan.
\newblock Mixhop: Higher-order graph convolutional architectures via sparsified
  neighborhood mixing.
\newblock In {\em international conference on machine learning}, pages 21--29.
  PMLR, 2019.

\bibitem{baek2021accurate}
J.~Baek, M.~Kang, and S.~J. Hwang.
\newblock Accurate learning of graph representations with graph multiset
  pooling.
\newblock {\em arXiv preprint arXiv:2102.11533}, 2021.

\bibitem{beani2021directional}
D.~Beani, S.~Passaro, V.~L{\'e}tourneau, W.~Hamilton, G.~Corso, and P.~Li{\`o}.
\newblock Directional graph networks.
\newblock In {\em International Conference on Machine Learning}, pages
  748--758. PMLR, 2021.

\bibitem{bresson2017residual}
X.~Bresson and T.~Laurent.
\newblock Residual gated graph convnets.
\newblock {\em arXiv preprint arXiv:1711.07553}, 2017.

\bibitem{cao2017tensor}
X.~Cao and G.~Rabusseau.
\newblock Tensor regression networks with various low-rank tensor
  approximations.
\newblock {\em arXiv preprint arXiv:1712.09520}, 2017.

\bibitem{castellana2020learning}
D.~Castellana and D.~Bacciu.
\newblock Learning from non-binary constituency trees via tensor decomposition.
\newblock {\em arXiv preprint arXiv:2011.00860}, 2020.

\bibitem{castellana2022tensor}
D.~Castellana and D.~Bacciu.
\newblock A tensor framework for learning in structured domains.
\newblock {\em Neurocomputing}, 470:405--426, 2022.

\bibitem{chen2020simple}
M.~Chen, Z.~Wei, Z.~Huang, B.~Ding, and Y.~Li.
\newblock Simple and deep graph convolutional networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1725--1735. PMLR, 2020.

\bibitem{chien2021adaptive}
E.~Chien, J.~Peng, P.~Li, and O.~Milenkovic.
\newblock Adaptive universal generalized pagerank graph neural network.
\newblock In {\em International Conference on Learning Representations.
  https://openreview. net/forum}, 2021.

\bibitem{cohen2016deep}
N.~Cohen, O.~Sharir, and A.~Shashua.
\newblock Deep simnets.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4782--4791, 2016.

\bibitem{cohen2016expressive}
N.~Cohen, O.~Sharir, and A.~Shashua.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock In {\em Conference on learning theory}, pages 698--728. PMLR, 2016.

\bibitem{comon2008symmetric}
P.~Comon, G.~Golub, L.-H. Lim, and B.~Mourrain.
\newblock Symmetric tensors and symmetric tensor rank.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  30(3):1254--1279, 2008.

\bibitem{corso2020principal}
G.~Corso, L.~Cavalleri, D.~Beaini, P.~Lio, and P.~Velickovic.
\newblock Principal neighbourhood aggregation for graph nets.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13260--13271, 2020.

\bibitem{dwivedi2020benchmarking}
V.~P. Dwivedi, C.~K. Joshi, T.~Laurent, Y.~Bengio, and X.~Bresson.
\newblock Benchmarking graph neural networks.
\newblock {\em arXiv preprint arXiv:2003.00982}, 2020.

\bibitem{errica2019fair}
F.~Errica, M.~Podda, D.~Bacciu, and A.~Micheli.
\newblock A fair comparison of graph neural networks for graph classification.
\newblock {\em arXiv preprint arXiv:1912.09893}, 2019.

\bibitem{gilmer2017neural}
J.~Gilmer, S.~S. Schoenholz, P.~F. Riley, O.~Vinyals, and G.~E. Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1263--1272. JMLR. org, 2017.

\bibitem{grover2016node2vec}
A.~Grover and J.~Leskovec.
\newblock node2vec: Scalable feature learning for networks.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on Knowledge discovery and data mining}, pages 855--864. ACM, 2016.

\bibitem{hamilton2020graph}
W.~L. Hamilton.
\newblock Graph representation learning.
\newblock {\em Synthesis Lectures on Artifical Intelligence and Machine
  Learning}, 14(3):1--159, 2020.

\bibitem{hamilton2017inductive}
W.~L. Hamilton, R.~Ying, and J.~Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock {\em arXiv}, abs/1706.02216, 2017.

\bibitem{hamilton2017representation}
W.~L. Hamilton, R.~Ying, and J.~Leskovec.
\newblock Representation learning on graphs: Methods and applications.
\newblock {\em arXiv preprint arXiv:1709.05584}, 2017.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hitchcock1927expression}
F.~L. Hitchcock.
\newblock The expression of a tensor or a polyadic as a sum of products.
\newblock {\em Journal of Mathematics and Physics}, 6(1-4):164--189, 1927.

\bibitem{hu2020open}
W.~Hu, M.~Fey, M.~Zitnik, Y.~Dong, H.~Ren, B.~Liu, M.~Catasta, and J.~Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em arXiv preprint arXiv:2005.00687}, 2020.

\bibitem{kiers2000towards}
H.~A. Kiers.
\newblock Towards a standardized notation and terminology in multiway analysis.
\newblock {\em Journal of Chemometrics: A Journal of the Chemometrics Society},
  14(3):105--122, 2000.

\bibitem{kim2021transformers}
J.~Kim, S.~Oh, and S.~Hong.
\newblock Transformers generalize deepsets and can be extended to graphs \&
  hypergraphs.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{kipf2016classification}
T.~N. Kipf and M.~Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock {\em arXiv}, abs/1609.02907, 2016.

\bibitem{klicpera2018predict}
J.~Klicpera, A.~Bojchevski, and S.~G{\"u}nnemann.
\newblock Predict then propagate: Graph neural networks meet personalized
  pagerank.
\newblock {\em arXiv preprint arXiv:1810.05997}, 2018.

\bibitem{kolda2015numerical}
T.~G. Kolda.
\newblock Numerical optimization for symmetric tensor decomposition.
\newblock {\em Mathematical Programming}, 151(1):225--248, 2015.

\bibitem{kolda2009tensor}
T.~G. Kolda and B.~W. Bader.
\newblock Tensor decompositions and applications.
\newblock {\em SIAM review}, 51(3):455--500, 2009.

\bibitem{le2021parameterized}
T.~Le, M.~Bertolini, F.~No{\'e}, and D.-A. Clevert.
\newblock Parameterized hypercomplex graph neural networks for graph
  classification.
\newblock {\em arXiv preprint arXiv:2103.16584}, 2021.

\bibitem{levine2018benefits}
Y.~Levine, O.~Sharir, and A.~Shashua.
\newblock Benefits of depth for long-term memory of recurrent networks.
\newblock 2018.

\bibitem{levine2018deep}
Y.~Levine, D.~Yakira, N.~Cohen, and A.~Shashua.
\newblock Deep learning and quantum entanglement: Fundamental connections with
  implications to network design.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{li2020deepergcn}
G.~Li, C.~Xiong, A.~Thabet, and B.~Ghanem.
\newblock Deepergcn: All you need to train deeper gcns.
\newblock {\em arXiv preprint arXiv:2006.07739}, 2020.

\bibitem{luan2021heterophily}
S.~Luan, C.~Hua, Q.~Lu, J.~Zhu, M.~Zhao, S.~Zhang, X.-W. Chang, and D.~Precup.
\newblock Is heterophily a real nightmare for graph neural networks to do node
  classification?
\newblock {\em arXiv preprint arXiv:2109.05641}, 2021.

\bibitem{luan2022revisiting}
S.~Luan, C.~Hua, Q.~Lu, J.~Zhu, M.~Zhao, S.~Zhang, X.-W. Chang, and D.~Precup.
\newblock Revisiting heterophily for graph neural networks.
\newblock {\em arXiv preprint arXiv:2210.07606}, 2022.

\bibitem{luan2020complete}
S.~Luan, M.~Zhao, C.~Hua, X.-W. Chang, and D.~Precup.
\newblock Complete the missing half: Augmenting aggregation filtering with
  diversification for graph convolutional networks.
\newblock {\em arXiv preprint arXiv:2008.08844}, 2020.

\bibitem{monti2017geometric}
F.~Monti, D.~Boscaini, J.~Masci, E.~Rodola, J.~Svoboda, and M.~M. Bronstein.
\newblock Geometric deep learning on graphs and manifolds using mixture model
  cnns.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5115--5124, 2017.

\bibitem{novikov2016exponential}
A.~Novikov, M.~Trofimov, and I.~Oseledets.
\newblock Exponential machines.
\newblock {\em arXiv preprint arXiv:1605.03795}, 2016.

\bibitem{peng2012rasl}
Y.~Peng, A.~Ganesh, J.~Wright, W.~Xu, and Y.~Ma.
\newblock Rasl: Robust alignment by sparse and low-rank decomposition for
  linearly correlated images.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence},
  34(11):2233--2246, 2012.

\bibitem{rabusseau2016low}
G.~Rabusseau and H.~Kadri.
\newblock Low-rank regression with tensor responses.
\newblock {\em Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem{sharir2017expressive}
O.~Sharir and A.~Shashua.
\newblock On the expressive power of overlapping architectures of deep
  learning.
\newblock {\em arXiv preprint arXiv:1703.02065}, 2017.

\bibitem{shi2020graphaf}
C.~Shi, M.~Xu, Z.~Zhu, W.~Zhang, M.~Zhang, and J.~Tang.
\newblock Graphaf: a flow-based autoregressive model for molecular graph
  generation.
\newblock {\em arXiv preprint arXiv:2001.09382}, 2020.

\bibitem{stoudenmire1605supervised}
E.~M. Stoudenmire and D.~J. Schwab.
\newblock Supervised learning with quantum-inspired tensor networks. arxiv
  2016.
\newblock {\em arXiv preprint arXiv:1605.05775}.

\bibitem{sutskever2011generating}
I.~Sutskever, J.~Martens, and G.~E. Hinton.
\newblock Generating text with recurrent neural networks.
\newblock In {\em ICML}, 2011.

\bibitem{velivckovic2017attention}
P.~Velickovic, G.~Cucurull, A.~Casanova, A.~Romero, P.~Lio, and Y.~Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv}, abs/1710.10903, 2017.

\bibitem{wang2020second}
Z.~Wang and S.~Ji.
\newblock Second-order pooling for graph neural networks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2020.

\bibitem{wu2016multiplicative}
Y.~Wu, S.~Zhang, Y.~Zhang, Y.~Bengio, and R.~R. Salakhutdinov.
\newblock On multiplicative integration with recurrent neural networks.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{xu2018powerful}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock {\em arXiv preprint arXiv:1810.00826}, 2018.

\bibitem{ying2018graph}
R.~Ying, R.~He, K.~Chen, P.~Eksombatchai, W.~L. Hamilton, and J.~Leskovec.
\newblock Graph convolutional neural networks for web-scale recommender
  systems.
\newblock In {\em Proceedings of the 24th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pages 974--983, 2018.

\bibitem{ying2018hierarchical}
R.~Ying, J.~You, C.~Morris, X.~Ren, W.~L. Hamilton, and J.~Leskovec.
\newblock Hierarchical graph representation learning with differentiable
  pooling.
\newblock {\em arXiv preprint arXiv:1806.08804}, 2018.

\bibitem{zaheer2017deep}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~R. Salakhutdinov, and A.~J.
  Smola.
\newblock Deep sets.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{zhang2019quaternion}
S.~Zhang, Y.~Tay, L.~Yao, and Q.~Liu.
\newblock Quaternion knowledge graph embeddings.
\newblock {\em arXiv preprint arXiv:1904.10281}, 2019.

\bibitem{zhang2016comon}
X.~Zhang, Z.-H. Huang, and L.~Qi.
\newblock Comon's conjecture, rank decomposition, and symmetric rank
  decomposition of symmetric tensors.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  37(4):1719--1728, 2016.

\bibitem{zhu2020beyond}
J.~Zhu, Y.~Yan, L.~Zhao, M.~Heimann, L.~Akoglu, and D.~Koutra.
\newblock Beyond homophily in graph neural networks: Current limitations and
  effective designs.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{zhu2021neural}
Z.~Zhu, Z.~Zhang, L.-P. Xhonneux, and J.~Tang.
\newblock Neural bellman-ford networks: A general graph neural network
  framework for link prediction.
\newblock {\em arXiv preprint arXiv:2106.06935}, 2021.

\end{thebibliography}
