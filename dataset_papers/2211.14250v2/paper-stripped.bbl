\begin{thebibliography}{33}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal and Zhang(2022{\natexlab{a}})]{agarwal2022model}
Alekh Agarwal and Tong Zhang.
\newblock Model-based rl with optimistic posterior sampling: Structural
  conditions and sample complexity.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 35284--35297, 2022{\natexlab{a}}.

\bibitem[Agarwal and Zhang(2022{\natexlab{b}})]{agarwal2022non}
Alekh Agarwal and Tong Zhang.
\newblock Non-linear reinforcement learning in large action spaces: Structural
  conditions and sample-efficiency of posterior sampling.
\newblock In \emph{Conference on Learning Theory}, pages 2776--2814. PMLR,
  2022{\natexlab{b}}.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{International Conference on Machine Learning}, pages
  463--474. PMLR, 2020.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{PLG}
Nicol{\`o} Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, New York, NY, USA, 2006.
\newblock ISBN 0521841089.

\bibitem[Chen et~al.(2022)Chen, Mei, and Bai]{chen2022unified}
Fan Chen, Song Mei, and Yu~Bai.
\newblock Unified algorithms for {RL} with decision-estimation coefficients:
  No-regret, {PAC}, and reward-free learning.
\newblock \emph{arXiv preprint arXiv:2209.11745}, 2022.

\bibitem[Dann et~al.(2021)Dann, Mohri, Zhang, and Zimmert]{dann2021provably}
Christoph Dann, Mehryar Mohri, Tong Zhang, and Julian Zimmert.
\newblock A provably efficient model-free posterior sampling method for
  episodic reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12040--12051, 2021.

\bibitem[Dean et~al.(2020)Dean, Mania, Matni, Recht, and Tu]{dean2020sample}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock On the sample complexity of the linear quadratic regulator.
\newblock \emph{Foundations of Computational Mathematics}, 20\penalty0
  (4):\penalty0 633--679, 2020.

\bibitem[Domingues et~al.(2021)Domingues, M{\'e}nard, Kaufmann, and
  Valko]{domingues2021episodic}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Emilie Kaufmann, and Michal Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In \emph{Algorithmic Learning Theory}, pages 578--598. PMLR, 2021.

\bibitem[Dong et~al.(2019)Dong, Van~Roy, and Zhou]{dong2019provably}
Shi Dong, Benjamin Van~Roy, and Zhengyuan Zhou.
\newblock Provably efficient reinforcement learning with aggregated states.
\newblock \emph{arXiv preprint arXiv:1912.06366}, 2019.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019latent}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and
  John Langford.
\newblock Provably efficient {RL} with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pages
  1665--1674. PMLR, 2019.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen
  Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in {RL}.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Foster et~al.(2022)Foster, Rakhlin, Sekhari, and
  Sridharan]{foster2022complexity}
Dylan~J Foster, Alexander Rakhlin, Ayush Sekhari, and Karthik Sridharan.
\newblock On the complexity of adversarial decision making.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2022.

\bibitem[Foster et~al.(2023{\natexlab{a}})Foster, Foster, Golowich, and
  Rakhlin]{foster2023complexity}
Dylan~J Foster, Dean~P Foster, Noah Golowich, and Alexander Rakhlin.
\newblock On the complexity of multi-agent decision making: From learning in
  games to partial monitoring.
\newblock \emph{Conference on Learning Theory (COLT)}, 2023{\natexlab{a}}.

\bibitem[Foster et~al.(2023{\natexlab{b}})Foster, Golowich, and
  Han]{foster2023tight}
Dylan~J Foster, Noah Golowich, and Yanjun Han.
\newblock Tight guarantees for interactive decision making with the
  decision-estimation coefficient.
\newblock \emph{Conference on Learning Theory (COLT)}, 2023{\natexlab{b}}.

\bibitem[Freedman(1975)]{Freedman1975tail}
David~A Freedman.
\newblock On tail probabilities for martingales.
\newblock \emph{the Annals of Probability}, pages 100--118, 1975.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low {Bellman} rank are
  {PAC}-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713, 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143, 2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of {RL} problems, and
  sample-efficient algorithms.
\newblock \emph{Neural Information Processing Systems}, 2021.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krishnamurthy2016pac}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock {PAC} reinforcement learning with rich observations.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1840--1848, 2016.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Li(2009)]{li2009unifying}
Lihong Li.
\newblock \emph{A unifying framework for computational reinforcement learning
  theory}.
\newblock Rutgers, The State University of New Jersey---New Brunswick, 2009.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2010--2020. PMLR, 2020.

\bibitem[Osband and Van~Roy(2016)]{osband2016lower}
Ian Osband and Benjamin Van~Roy.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1608.02732}, 2016.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2256--2264, 2013.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based {RL} in contextual decision processes: {PAC} bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pages 2898--2933. PMLR,
  2019.

\bibitem[Wagenmaker and Foster(2023)]{wagenmaker2023instance}
Andrew Wagenmaker and Dylan~J Foster.
\newblock Instance-optimality in interactive decision making: Toward a
  non-asymptotic theory.
\newblock \emph{Conference on Learning Theory (COLT)}, 2023.

\bibitem[Xie et~al.(2023)Xie, Foster, Bai, Jiang, and Kakade]{xie2022role}
Tengyang Xie, Dylan~J Foster, Yu~Bai, Nan Jiang, and Sham~M Kakade.
\newblock The role of coverage in online reinforcement learning.
\newblock \emph{International Conference on Learning Representations (ICLR)},
  2023.

\bibitem[Yang and Wang(2019)]{yang2019sample}
Lin Yang and Mengdi Wang.
\newblock Sample-optimal parametric {Q}-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pages
  6995--7004. PMLR, 2019.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020.

\bibitem[Zhang(2022)]{zhang2022feel}
Tong Zhang.
\newblock Feel-good thompson sampling for contextual bandits and reinforcement
  learning.
\newblock \emph{SIAM Journal on Mathematics of Data Science}, 4\penalty0
  (2):\penalty0 834--857, 2022.

\bibitem[Zhong et~al.(2022)Zhong, Xiong, Zheng, Wang, Wang, Yang, and
  Zhang]{zhong2022posterior}
Han Zhong, Wei Xiong, Sirui Zheng, Liwei Wang, Zhaoran Wang, Zhuoran Yang, and
  Tong Zhang.
\newblock A posterior sampling framework for interactive decision making.
\newblock \emph{arXiv preprint arXiv:2211.01962}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Gu, and Szepesvari]{zhou2021nearly}
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  markov decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 4532--4576. PMLR,
  2021.

\end{thebibliography}
