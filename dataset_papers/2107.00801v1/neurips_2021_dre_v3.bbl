\begin{thebibliography}{10}

\bibitem{abe2019anomaly}
M.~Abe and M.~Sugiyama.
\newblock Anomaly detection by deep direct density ratio estimation.
\newblock 2019.

\bibitem{bekkar2013evaluation}
M.~Bekkar, H.~K. Djemaa, and T.~A. Alitouche.
\newblock Evaluation measures for models assessment over imbalanced data sets.
\newblock {\em J Inf Eng Appl}, 3(10), 2013.

\bibitem{bertinetto2018meta}
L.~Bertinetto, J.~F. Henriques, P.~H. Torr, and A.~Vedaldi.
\newblock Meta-learning with differentiable closed-form solvers.
\newblock {\em arXiv preprint arXiv:1805.08136}, 2018.

\bibitem{bickel2007discriminative}
S.~Bickel, M.~Br{\"u}ckner, and T.~Scheffer.
\newblock Discriminative learning for differing training and test
  distributions.
\newblock In {\em ICML}, 2007.

\bibitem{breunig2000lof}
M.~M. Breunig, H.-P. Kriegel, R.~T. Ng, and J.~Sander.
\newblock Lof: identifying density-based local outliers.
\newblock In {\em SIGMOD}, 2000.

\bibitem{dong2019multistream}
B.~Dong, Y.~Gao, S.~Chandra, and L.~Khan.
\newblock Multistream classification with relative density ratio estimation.
\newblock In {\em AAAI}, 2019.

\bibitem{fang2020rethinking}
T.~Fang, N.~Lu, G.~Niu, and M.~Sugiyama.
\newblock Rethinking importance weighting for deep learning under distribution
  shift.
\newblock In {\em NeurIPS}, 2020.

\bibitem{finn2017model}
C.~Finn, P.~Abbeel, and S.~Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In {\em ICML}, 2017.

\bibitem{garnelo2018conditional}
M.~Garnelo, D.~Rosenbaum, C.~Maddison, T.~Ramalho, D.~Saxton, M.~Shanahan,
  Y.~W. Teh, D.~Rezende, and S.~A. Eslami.
\newblock Conditional neural processes.
\newblock In {\em ICML}, 2018.

\bibitem{garnelo2018neural}
M.~Garnelo, J.~Schwarz, D.~Rosenbaum, F.~Viola, D.~J. Rezende, S.~Eslami, and
  Y.~W. Teh.
\newblock Neural processes.
\newblock {\em arXiv preprint arXiv:1807.01622}, 2018.

\bibitem{ghifary2015domain}
M.~Ghifary, W.~Bastiaan~Kleijn, M.~Zhang, and D.~Balduzzi.
\newblock Domain generalization for object recognition with multi-task
  autoencoders.
\newblock In {\em ICCV}, 2015.

\bibitem{gretton2012kernel}
A.~Gretton, K.~M. Borgwardt, M.~J. Rasch, B.~Sch{\"o}lkopf, and A.~Smola.
\newblock A kernel two-sample test.
\newblock {\em The Journal of Machine Learning Research}, 13(1):723--773, 2012.

\bibitem{gretton2009covariate}
A.~Gretton, A.~Smola, J.~Huang, M.~Schmittfull, K.~Borgwardt, and
  B.~Sch{\"o}lkopf.
\newblock Covariate shift by kernel mean matching.
\newblock {\em Dataset shift in machine learning}, 3(4):5, 2009.

\bibitem{hido2011statistical}
S.~Hido, Y.~Tsuboi, H.~Kashima, M.~Sugiyama, and T.~Kanamori.
\newblock Statistical outlier detection using direct density ratio estimation.
\newblock {\em Knowledge and information systems}, 26(2):309--336, 2011.

\bibitem{ide2017multi}
T.~Id{\'e}, D.~T. Phan, and J.~Kalagnanam.
\newblock Multi-task multi-modal models for collective anomaly detection.
\newblock In {\em ICDM}, 2017.

\bibitem{iwata2020meta}
T.~Iwata and A.~Kumagai.
\newblock Meta-learning from tasks with heterogeneous attribute spaces.
\newblock In {\em NeurIPS}, 2020.

\bibitem{kanamori2009least}
T.~Kanamori, S.~Hido, and M.~Sugiyama.
\newblock A least-squares approach to direct importance estimation.
\newblock {\em The Journal of Machine Learning Research}, 10:1391--1445, 2009.

\bibitem{kato2020non}
M.~Kato and T.~Teshima.
\newblock Non-negative bregman divergence minimization for deep direct density
  ratio estimation.
\newblock In {\em ICML}, 2021.

\bibitem{kato2018learning}
M.~Kato, T.~Teshima, and J.~Honda.
\newblock Learning from positive and unlabeled data with a selection bias.
\newblock In {\em ICLR}, 2018.

\bibitem{kingma2014adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: a method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kiryo2017positive}
R.~Kiryo, G.~Niu, M.~C.~d. Plessis, and M.~Sugiyama.
\newblock Positive-unlabeled learning with non-negative risk estimator.
\newblock In {\em NeurIPS}, 2017.

\bibitem{kumagai2017learning}
A.~Kumagai and T.~Iwata.
\newblock Learning latest classifiers without additional labeled data.
\newblock In {\em IJCAI}, 2017.

\bibitem{kumagai2019transfer}
A.~Kumagai, T.~Iwata, and Y.~Fujiwara.
\newblock Transfer anomaly detection by inferring latent domain
  representations.
\newblock In {\em NeurIPS}, 2019.

\bibitem{lane2002machine}
T.~D. Lane.
\newblock Machine learning techniques for the computer security domain of
  anomaly detection.
\newblock 2002.

\bibitem{lee2019set}
J.~Lee, Y.~Lee, J.~Kim, A.~Kosiorek, S.~Choi, and Y.~W. Teh.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock In {\em ICML}, 2019.

\bibitem{lee2019meta}
K.~Lee, S.~Maji, A.~Ravichandran, and S.~Soatto.
\newblock Meta-learning with differentiable convex optimization.
\newblock In {\em CVPR}, 2019.

\bibitem{li2017self}
C.~Li, J.~Yan, F.~Wei, W.~Dong, Q.~Liu, and H.~Zha.
\newblock Self-paced multi-task learning.
\newblock In {\em AAAI}, 2017.

\bibitem{liu2008isolation}
F.~T. Liu, K.~M. Ting, and Z.-H. Zhou.
\newblock Isolation forest.
\newblock In {\em ICDM}, 2008.

\bibitem{liu2017trimmed}
S.~Liu, A.~Takeda, T.~Suzuki, and K.~Fukumizu.
\newblock Trimmed density ratio estimation.
\newblock In {\em NeurIPS}, 2017.

\bibitem{liu2013change}
S.~Liu, M.~Yamada, N.~Collier, and M.~Sugiyama.
\newblock Change-point detection in time-series data by relative density-ratio
  estimation.
\newblock {\em Neural Networks}, 43:72--83, 2013.

\bibitem{pan2009survey}
S.~J. Pan and Q.~Yang.
\newblock A survey on transfer learning.
\newblock {\em IEEE Transactions on knowledge and data engineering},
  22(10):1345--1359, 2009.

\bibitem{paszke2017automatic}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{qin1998inferences}
J.~Qin.
\newblock Inferences for case-control and semiparametric two-sample density
  ratio models.
\newblock {\em Biometrika}, 85(3):619--630, 1998.

\bibitem{rajeswaran2019meta}
A.~Rajeswaran, C.~Finn, S.~M. Kakade, and S.~Levine.
\newblock Meta-learning with implicit gradients.
\newblock In {\em NeurIPS}, 2019.

\bibitem{rhodes2020telescoping}
B.~Rhodes, K.~Xu, and M.~U. Gutmann.
\newblock Telescoping density-ratio estimation.
\newblock In {\em NeurIPS}, 2020.

\bibitem{ruff2018deep}
L.~Ruff, N.~G{\"o}rnitz, L.~Deecke, S.~A. Siddiqui, R.~Vandermeulen, A.~Binder,
  E.~M{\"u}ller, and M.~Kloft.
\newblock Deep one-class classification.
\newblock In {\em ICML}, 2018.

\bibitem{sakai2019covariate}
T.~Sakai and N.~Shimizu.
\newblock Covariate shift adaptation on learning from positive and unlabeled
  data.
\newblock In {\em AAAI}, 2019.

\bibitem{sakurada2014anomaly}
M.~Sakurada and T.~Yairi.
\newblock Anomaly detection using autoencoders with nonlinear dimensionality
  reduction.
\newblock In {\em Proceedings of the MLSDA 2014 2nd Workshop on Machine
  Learning for Sensory Data Analysis}, page~4. ACM, 2014.

\bibitem{scholkopf2002learning}
B.~Sch{\"o}lkopf, A.~J. Smola, F.~Bach, et~al.
\newblock {\em Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2002.

\bibitem{shimodaira2000improving}
H.~Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock {\em Journal of statistical planning and inference}, 90(2):227--244,
  2000.

\bibitem{snell2017prototypical}
J.~Snell, K.~Swersky, and R.~Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock In {\em NeurIPS}, 2017.

\bibitem{sugiyama2013direct}
M.~Sugiyama.
\newblock Direct approximation of divergences between probability
  distributions.
\newblock In {\em Empirical Inference}, pages 273--283. Springer, 2013.

\bibitem{sugiyama2007covariate}
M.~Sugiyama, M.~Krauledat, and K.-R. M{\~A}{\v{z}}ller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock {\em Journal of Machine Learning Research}, 8(May):985--1005, 2007.

\bibitem{sugiyama2007direct}
M.~Sugiyama, S.~Nakajima, H.~Kashima, P.~Buenau, and M.~Kawanabe.
\newblock Direct importance estimation with model selection and its application
  to covariate shift adaptation.
\newblock In {\em NeurIPS}, 2007.

\bibitem{sugiyama2012density}
M.~Sugiyama, T.~Suzuki, and T.~Kanamori.
\newblock {\em Density ratio estimation in machine learning}.
\newblock Cambridge University Press, 2012.

\bibitem{sugiyama2008direct}
M.~Sugiyama, T.~Suzuki, S.~Nakajima, H.~Kashima, P.~von B{\"u}nau, and
  M.~Kawanabe.
\newblock Direct importance estimation for covariate shift adaptation.
\newblock {\em Annals of the Institute of Statistical Mathematics},
  60(4):699--746, 2008.

\bibitem{takahashi2019variational}
H.~Takahashi, T.~Iwata, Y.~Yamanaka, M.~Yamada, and S.~Yagi.
\newblock Variational autoencoder with implicit optimal priors.
\newblock In {\em AAAI}, 2019.

\bibitem{uehara2016generative}
M.~Uehara, I.~Sato, M.~Suzuki, K.~Nakayama, and Y.~Matsuo.
\newblock Generative adversarial nets from a density ratio estimation
  perspective.
\newblock {\em arXiv preprint arXiv:1610.02920}, 2016.

\bibitem{vapnik1998statistical}
V.~Vapnik.
\newblock Statistical learning theory.
\newblock {\em Wiley}, 1:624, 1998.

\bibitem{yamada2013relative}
M.~Yamada, T.~Suzuki, T.~Kanamori, H.~Hachiya, and M.~Sugiyama.
\newblock Relative density-ratio estimation for robust distribution comparison.
\newblock {\em Neural computation}, 25(5):1324--1370, 2013.

\bibitem{zaheer2017deep}
M.~Zaheer, S.~Kottur, S.~Ravanbakhsh, B.~Poczos, R.~R. Salakhutdinov, and A.~J.
  Smola.
\newblock Deep sets.
\newblock In {\em NeurIPS}, 2017.

\end{thebibliography}
