@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@inproceedings{nie2022time,
  title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},
  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}


@inproceedings{anguita2013public,
  title={A public domain dataset for human activity recognition using smartphones.},
  author={Anguita, Davide and Ghio, Alessandro and Oneto, Luca and Parra, Xavier and Reyes-Ortiz, Jorge Luis and others},
  booktitle={Esann},
  volume={3},
  pages={3},
  year={2013}
}

@inproceedings{tian2023fine,
  title={Fine-Tuning Language Models for Factuality},
  author={Tian, Katherine and Mitchell, Eric and Yao, Huaxiu and Manning, Christopher D and Finn, Chelsea},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{da2023llm,
  title={Llm powered sim-to-real transfer for traffic signal control},
  author={Da, Longchao and Gao, Minchiuan and Mei, Hao and Wei, Hua},
  journal={arXiv preprint arXiv:2308.14284},
  year={2023}
}

@inproceedings{yang2022empirical,
  title={An empirical study of gpt-3 for few-shot knowledge-based vqa},
  author={Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  pages={3081--3089},
  year={2022}
}

@article{da2023open,
  title={Open-ti: Open traffic intelligence with augmented language model},
  author={Da, Longchao and Liou, Kuanru and Chen, Tiejin and Zhou, Xuesong and Luo, Xiangyong and Yang, Yezhou and Wei, Hua},
  journal={International Journal of Machine Learning and Cybernetics},
  pages={1--26},
  year={2024},
  publisher={Springer}
}


@article{zhang2023r,
  title={R-tuning: Teaching large language models to refuse unknown questions},
  author={Zhang, Hanning and Diao, Shizhe and Lin, Yong and Fung, Yi R and Lian, Qing and Wang, Xingyao and Chen, Yangyi and Ji, Heng and Zhang, Tong},
  journal={arXiv preprint arXiv:2311.09677},
  year={2023}
}

@article{madaan2023self,
  title={Self-refine: Iterative refinement with self-feedback},
  author={Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhou2023analyzing,
  title={Analyzing and mitigating object hallucination in large vision-language models},
  author={Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2310.00754},
  year={2023}
}

@article{vu2023freshllms,
  title={Freshllms: Refreshing large language models with search engine augmentation},
  author={Vu, Tu and Iyyer, Mohit and Wang, Xuezhi and Constant, Noah and Wei, Jerry and Wei, Jason and Tar, Chris and Sung, Yun-Hsuan and Zhou, Denny and Le, Quoc and others},
  journal={arXiv preprint arXiv:2310.03214},
  year={2023}
}


@article{li2023helma,
  title={HELMA: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models},
  author={Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2305.11747},
  year={2023}
}

@inproceedings{
anonymous2024sociodojo,
title={SocioDojo: Building Lifelong Analytical Agents with Real-world Text and Time Series},
author={Cheng, Junyan and Chin, Peter},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=s9z0HzWJJp}
}

@inproceedings{singh2023progprompt,
  title={Progprompt: Generating situated robot task plans using large language models},
  author={Singh, Ishika and Blukis, Valts and Mousavian, Arsalan and Goyal, Ankit and Xu, Danfei and Tremblay, Jonathan and Fox, Dieter and Thomason, Jesse and Garg, Animesh},
  booktitle={2023 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={11523--11530},
  year={2023},
  organization={IEEE}
}


@inproceedings{huang2022language,
  title={Language models as zero-shot planners: Extracting actionable knowledge for embodied agents},
  author={Huang, Wenlong and Abbeel, Pieter and Pathak, Deepak and Mordatch, Igor},
  booktitle={International Conference on Machine Learning},
  pages={9118--9147},
  year={2022},
  organization={PMLR}
}


@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}

@article{lee2023aligning,
  title={Aligning text-to-image models using human feedback},
  author={Lee, Kimin and Liu, Hao and Ryu, Moonkyung and Watkins, Olivia and Du, Yuqing and Boutilier, Craig and Abbeel, Pieter and Ghavamzadeh, Mohammad and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2302.12192},
  year={2023}
}


@article{rawte2023survey,
  title={A survey of hallucination in large foundation models},
  author={Rawte, Vipula and Sheth, Amit and Das, Amitava},
  journal={arXiv preprint arXiv:2309.05922},
  year={2023}
}


@inproceedings{beit3,
title={Image as a foreign language: {BEiT} pretraining for vision and vision-language tasks},
author={Wenhui Wang and Hangbo Bao and Li Dong and Johan Bjorck and Zhiliang Peng and Qiang Liu and Kriti Aggarwal and Owais Khan Mohammed and Saksham Singhal and Subhojit Som and Furu Wei},
booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
year={2023}
}

@article{ge2023openagi,
  title={Openagi: When llm meets domain experts},
  author={Ge, Yingqiang and Hua, Wenyue and Mei, Kai and Tan, Juntao and Xu, Shuyuan and Li, Zelong and Zhang, Yongfeng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023}
}

@article{schick2023toolformer,
  title={Toolformer: Language models can teach themselves to use tools},
  author={Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Hambro, Eric and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{tsymbal2004problem,
  title={The problem of concept drift: definitions and related work},
  author={Tsymbal, Alexey},
  journal={Computer Science Department, Trinity College Dublin},
  volume={106},
  number={2},
  pages={58},
  year={2004},
  publisher={Dublin, Ireland}
}

@inproceedings{wenzek2019ccnet,
  title={CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data},
  author={Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzm{\'a}n, Francisco and Joulin, Armand and Grave, {\'E}douard},
  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},
  pages={4003--4012},
  year={2020}
}

@article{zhang2023instruction,
  title={Instruction tuning for large language models: A survey},
  author={Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others},
  journal={arXiv preprint arXiv:2308.10792},
  year={2023}
}

@article{yin2023lamm,
  title={Lamm: Language-assisted multi-modal instruction-tuning dataset, framework, and benchmark},
  author={Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Huang, Xiaoshui and Wang, Zhiyong and Sheng, Lu and Bai, Lei and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{gudibande2023false,
  title={The false promise of imitating proprietary llms},
  author={Gudibande, Arnav and Wallace, Eric and Snell, Charlie and Geng, Xinyang and Liu, Hao and Abbeel, Pieter and Levine, Sergey and Song, Dawn},
  journal={arXiv preprint arXiv:2305.15717},
  year={2023}
}

@inproceedings{wen2020time,
 author = {Wen, Qingsong and Sun, Liang and Yang, Fan and Song, Xiaomin and Gao, Jingkun and Wang, Xue and Xu, Huan},
 booktitle = {IJCAI},
 pages = {4653--4660},
 title = {Time Series Data Augmentation for Deep Learning: A Survey},
 year = {2021}
}

@article{xie2023pixiu,
  title={PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark for Finance},
  author={Xie, Qianqian and Han, Weiguang and Zhang, Xiao and Lai, Yanzhao and Peng, Min and Lopez-Lira, Alejandro and Huang, Jimin},
  journal={arXiv preprint arXiv:2306.05443},
  year={2023}
}

@inproceedings{wen2023transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  booktitle={International Joint Conference on Artificial Intelligence(IJCAI)},
  year={2023}
}

@article{liu2023llava,
      title={Visual Instruction Tuning}, 
      author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
      journal={Advances in neural information processing systems},
      year={2023},
}

@article{zhang2023self,
  title={Self-supervised learning for time series analysis: Taxonomy, progress, and prospects},
  author={Zhang, Kexin and Wen, Qingsong and Zhang, Chaoli and Cai, Rongyao and Jin, Ming and Liu, Yong and Zhang, James Y and Liang, Yuxuan and Pang, Guansong and Song, Dongjin and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{wu2023visual,
  title={Visual chatgpt: Talking, drawing and editing with visual foundation models},
  author={Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
  journal={arXiv preprint arXiv:2303.04671},
  year={2023}
}


@inproceedings{garg2023tic,
  title={TiC-CLIP: Continual Training of CLIP Models},
  author={Garg, Saurabh and Farajtabar, Mehrdad and Pouransari, Hadi and Vemulapalli, Raviteja and Mehta, Sachin and Tuzel, Oncel and Shankar, Vaishaal and Faghri, Fartash},
  booktitle={NeurIPS 2023 Workshop on Distribution Shifts: New Frontiers with Foundation Models},
  year={2023}
}


@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{shridharautomatic,
  title={Automatic Generation of Socratic Questions for Learning to Solve Math Word Problems},
  author={Shridhar, Kumar and Macina, Jakub and El-Assady, Mennatallah and Sinha, Tanmay and Kapur, Manu and Sachan, Mrinmaya}
}

@book{hamilton2020time,
  title={Time series analysis},
  author={Hamilton, James D},
  year={2020},
  publisher={Princeton university press}
}

@book{tsay2005analysis,
  title={Analysis of financial time series},
  author={Tsay, Ruey S},
  year={2005},
  publisher={John wiley \& sons}
}

@inproceedings{alghamdi2019forecasting,
  title={Forecasting traffic congestion using ARIMA modeling},
  author={Alghamdi, Taghreed and Elgazzar, Khalid and Bayoumi, Magdi and Sharaf, Taysseer and Shah, Sumit},
  booktitle={2019 15th international wireless communications \& mobile computing conference (IWCMC)},
  pages={1227--1232},
  year={2019},
  organization={IEEE}
}

@article{mudelsee2019trend,
  title={Trend analysis of climate time series: A review of methods},
  author={Mudelsee, Manfred},
  journal={Earth-science reviews},
  volume={190},
  pages={310--322},
  year={2019},
  publisher={Elsevier}
}

@book{fuller2009introduction,
  title={Introduction to statistical time series},
  author={Fuller, Wayne A},
  year={2009},
  publisher={John Wiley \& Sons}
}

@article{wen2022transformers,
  title={Transformers in time series: A survey},
  author={Wen, Qingsong and Zhou, Tian and Zhang, Chaoli and Chen, Weiqi and Ma, Ziqing and Yan, Junchi and Sun, Liang},
  journal={arXiv preprint arXiv:2202.07125},
  year={2022}
}

@article{jin2023survey,
  title={A survey on graph neural networks for time series: Forecasting, classification, imputation, and anomaly detection},
  author={Jin, Ming and Koh, Huan Yee and Wen, Qingsong and Zambon, Daniele and Alippi, Cesare and Webb, Geoffrey I and King, Irwin and Pan, Shirui},
  journal={arXiv preprint arXiv:2307.03759},
  year={2023}
}

@article{gamboa2017deep,
  title={Deep learning for time-series analysis},
  author={Gamboa, John Cristian Borges},
  journal={arXiv preprint arXiv:1701.01887},
  year={2017}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{zhang2023vision,
  title={Vision-language models for vision tasks: A survey},
  author={Zhang, Jingyi and Huang, Jiaxing and Jin, Sheng and Lu, Shijian},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{bubeck2023sparks,
  title={Sparks of artificial general intelligence: Early experiments with gpt-4},
  author={Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and others},
  journal={arXiv preprint arXiv:2303.12712},
  year={2023}
}

@article{minaee2021deep,
  title={Deep learning--based text classification: a comprehensive review},
  author={Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  journal={ACM computing surveys (CSUR)},
  volume={54},
  number={3},
  pages={1--40},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@article{soares2020literature,
  title={A literature review on question answering techniques, paradigms and systems},
  author={Soares, Marco Antonio Calijorne and Parreiras, Fernando Silva},
  journal={Journal of King Saud University-Computer and Information Sciences},
  volume={32},
  number={6},
  pages={635--646},
  year={2020},
  publisher={Elsevier}
}

@article{stahlberg2020neural,
  title={Neural machine translation: A review},
  author={Stahlberg, Felix},
  journal={Journal of Artificial Intelligence Research},
  volume={69},
  pages={343--418},
  year={2020}
}

@article{kojima2022large,
  title={Large language models are zero-shot reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={22199--22213},
  year={2022}
}

@article{wang2023survey,
  title={A survey on large language model based autonomous agents},
  author={Wang, Lei and Ma, Chen and Feng, Xueyang and Zhang, Zeyu and Yang, Hao and Zhang, Jingsen and Chen, Zhiyuan and Tang, Jiakai and Chen, Xu and Lin, Yankai and others},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={1--26},
  year={2024},
  publisher={Springer}
}

@article{latif2023sparks,
  title={Sparks of large audio models: A survey and outlook},
  author={Latif, Siddique and Shoukat, Moazzam and Shamshad, Fahad and Usama, Muhammad and Cuay{\'a}huitl, Heriberto and Schuller, Bj{\"o}rn W},
  journal={arXiv preprint arXiv:2308.12792},
  year={2023}
}

@article{chen2023exploring,
  title={Exploring the potential of large language models (llms) in learning on graphs},
  author={Chen, Zhikai and Mao, Haitao and Li, Hang and Jin, Wei and Wen, Hongzhi and Wei, Xiaochi and Wang, Shuaiqiang and Yin, Dawei and Fan, Wenqi and Liu, Hui and others},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={25},
  number={2},
  pages={42--61},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{jin2023large,
  title={Large models for time series and spatio-temporal data: A survey and outlook},
  author={Jin, Ming and Wen, Qingsong and Liang, Yuxuan and Zhang, Chaoli and Xue, Siqiao and Wang, Xue and Zhang, James and Wang, Yi and Chen, Haifeng and Li, Xiaoli and others},
  journal={arXiv preprint arXiv:2310.10196},
  year={2023}
}


@article{yan2023urban,
  title={When Urban Region Profiling Meets Large Language Models},
  author={Yan, Yibo and Wen, Haomin and Zhong, Siru and Chen, Wei and Chen, Haodong and Wen, Qingsong and Zimmermann, Roger and Liang, Yuxuan},
  journal={arXiv preprint arXiv:2310.18340},
  year={2023}
}

@article{xue2023weaverbird,
  title={WeaverBird: Empowering Financial Decision-Making with Large Language Model, Knowledge Base, and Search Engine},
  author={Xue, Siqiao and Zhou, Fan and Xu, Yi and Zhao, Hongyu and Xie, Shuo and Jiang, Caigao and Zhang, James and Zhou, Jun and Xu, Peng and Xiu, Dacheng and others},
  journal={arXiv preprint arXiv:2308.05361},
  year={2023}
}

@inproceedings{gu2023anomalygpt,
  title={Anomalygpt: Detecting industrial anomalies using large vision-language models},
  author={Gu, Zhaopeng and Zhu, Bingke and Zhu, Guibo and Chen, Yingying and Tang, Ming and Wang, Jinqiao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={1932--1940},
  year={2024}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{wei2022emergent,
  title={Emergent Abilities of Large Language Models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={Transactions on Machine Learning Research},
  year={2022}
}

@inproceedings{sanh2021multitask,
  title={Multitask prompted training enables zero-shot task generalization},
  author={Victor, Sanh and Albert, Webson and Colin, Raffel and Stephen, Bach and Lintang, Sutawika and Zaid, Alyafeai and Antoine, Chaffin and Arnaud, Stiegler and Arun, Raja and Manan, Dey and others},
  booktitle={International Conference on Learning Representations},
  year={2022}
}

@article{besta2023graph,
  title={Graph of thoughts: Solving elaborate problems with large language models},
  author={Besta, Maciej and Blach, Nils and Kubicek, Ales and Gerstenberger, Robert and Gianinazzi, Lukas and Gajda, Joanna and Lehmann, Tomasz and Podstawski, Michal and Niewiadomski, Hubert and Nyczyk, Piotr and others},
  journal={arXiv preprint arXiv:2308.09687},
  year={2023}
}

@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{shumway2017arima,
  title={ARIMA models},
  author={Shumway, Robert H and Stoffer, David S and Shumway, Robert H and Stoffer, David S},
  journal={Time series analysis and its applications: with R examples},
  pages={75--163},
  year={2017},
  publisher={Springer}
}

@article{kalekar2004time,
  title={Time series forecasting using holt-winters exponential smoothing},
  author={Kalekar, Prajakta S and others},
  journal={Kanwal Rekhi school of information Technology},
  volume={4329008},
  number={13},
  pages={1--13},
  year={2004}
}

@inproceedings{yeh2023toward,
  title={Toward a foundation model for time series data},
  author={Yeh, Chin-Chia Michael and Dai, Xin and Chen, Huiyuan and Zheng, Yan and Fan, Yujie and Der, Audrey and Lai, Vivian and Zhuang, Zhongfang and Wang, Junpeng and Wang, Liang and others},
  booktitle={Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
  pages={4400--4404},
  year={2023}
}

@article{dong2023simmtm,
  title={SimMTM: A Simple Pre-Training Framework for Masked Time-Series Modeling},
  author={Dong, Jiaxiang and Wu, Haixu and Zhang, Haoran and Zhang, Li and Wang, Jianmin and Long, Mingsheng},
  journal={arXiv preprint arXiv:2302.00861},
  year={2023}
}

@article{liao2023ai,
  title={AI Transparency in the Age of LLMs: A Human-Centered Research Roadmap},
  author={Liao, Q Vera and Vaughan, Jennifer Wortman},
  journal={Harvard Data Science Review},
  year={2024},
  publisher={PubPub}
}

@inproceedings{peris2023privacy,
author = {Peris, Charith and Dupuy, Christophe and Majmudar, Jimit and Parikh, Rahil and Smaili, Sami and Zemel, Richard and Gupta, Rahul},
title = {Privacy in the Time of Language Models},
year = {2023},
booktitle = {Proceedings of the Sixteenth ACM International Conference on Web Search and Data Mining},
pages = {1291–1292},
numpages = {2},
}

@article{zhuo2023exploring,
  title={Exploring ai ethics of chatgpt: A diagnostic analysis},
  author={Zhuo, Terry Yue and Huang, Yujin and Chen, Chunyang and Xing, Zhenchang},
  journal={arXiv preprint arXiv:2301.12867},
  year={2023}
}

###################### Enhancer ###############################


@article{liu2023large,
  title={Large Language Models are Few-Shot Health Learners},
  author={Liu, Xin and McDuff, Daniel and Kovacs, Geza and Galatzer-Levy, Isaac and Sunshine, Jacob and Zhan, Jiening and Poh, Ming-Zher and Liao, Shun and Di Achille, Paolo and Patel, Shwetak},
  journal={arXiv preprint arXiv:2305.15525},
  year={2023}
}

@inproceedings{li2023frozen,
  title={Frozen language model helps ECG zero-shot learning},
  author={Li, Jun and Liu, Che and Cheng, Sibo and Arcucci, Rossella and Hong, Shenda},
  booktitle={Medical Imaging with Deep Learning},
  pages={402--415},
  year={2024},
  organization={PMLR}
}

@inproceedings{moon2023imu2clip,
  title={IMU2CLIP: Language-grounded Motion Sensor Translation with Multimodal Contrastive Learning},
  author={Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Saraf, Aparajita and Bearman, Amy and Damavandi, Babak},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={13246--13253},
  year={2023}
}

@article{tsai2022traffic,
  title={Traffic-twitter transformer: A nature language processing-joined framework for network-wide traffic forecasting},
  author={Tsai, Meng-Ju and Cui, Zhiyong and Yang, Hao and Kopca, Cole and Tien, Sophie and Wang, Yinhai},
  journal={arXiv preprint arXiv:2206.11078},
  year={2022}
}

@article{yu2023temporal,
  title={Temporal Data Meets LLM--Explainable Financial Time Series Forecasting},
  author={Yu, Xinli and Chen, Zheng and Ling, Yuan and Dong, Shujing and Liu, Zongyi and Lu, Yanbin},
  journal={arXiv preprint arXiv:2306.11025},
  year={2023}
}

@article{zhang2024semantic,
  title={Semantic understanding and prompt engineering for large-scale traffic data imputation},
  author={Zhang, Kunpeng and Zhou, Feng and Wu, Lan and Xie, Na and He, Zhengbing},
  journal={Information Fusion},
  volume={102},
  pages={102038},
  year={2024},
  publisher={Elsevier}
}


@article{yang2022zero,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={124--141},
  year={2022}
}

@article{jin2020forecastqa,
  title={Forecastqa: a question answering challenge for event forecasting with temporal text data},
  author={Jin, Woojeong and Khanna, Rahul and Kim, Suji and Lee, Dong-Ho and Morstatter, Fred and Galstyan, Aram and Ren, Xiang},
  journal={arXiv preprint arXiv:2005.00792},
  year={2020}
}

@inproceedings{yu2023zero,
  title={Zero-Shot ECG Diagnosis with Large Language Models and Retrieval-Augmented Generation},
  author={Yu, Han and Guo, Peikun and Sano, Akane},
  booktitle={Machine Learning for Health (ML4H)},
  pages={650--663},
  year={2023},
  organization={PMLR}
}

@article{oh2023ecg,
  title={Ecg-qa: A comprehensive question answering dataset combined with electrocardiogram},
  author={Oh, Jungwoo and Lee, Gyubok and Bae, Seongsu and Kwon, Joon-myoung and Choi, Edward},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{lai2023large,
  title={Large Language Models as Traffic Signal Control Agents: Capacity and Opportunity},
  author={Lai, Siqi and Xu, Zhao and Zhang, Weijia and Liu, Hao and Xiong, Hui},
  journal={arXiv preprint arXiv:2312.16044},
  year={2023}
}

###################### Predictor ##############################

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@article{cleveland1990stl,
  title={STL: A seasonal-trend decomposition},
  author={Cleveland, Robert B and Cleveland, William S and McRae, Jean E and Terpenning, Irma},
  journal={J. Off. Stat},
  volume={6},
  number={1},
  pages={3--73},
  year={1990}
}

@article{xue2023promptcast,
  title={Promptcast: A new prompt-based learning paradigm for time series forecasting},
  author={Xue, Hao and Salim, Flora D},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2023},
  publisher={IEEE}
}

@inproceedings{jin2023time,
  title={Time-llm: Time series forecasting by reprogramming large language models},
  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and others},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{zhou2023one1,
  title={One Fits All: Power General Time Series Analysis by Pretrained LM},
  author={Zhou, Tian and Niu, Peisong and Wang, Xue and Sun, Liang and Jin, Rong},
  journal={Advances in neural information processing systems},
  year={2023}
}

@article{gruver2023large,
  title={Large language models are zero-shot time series forecasters},
  author={Gruver, Nate and Finzi, Marc and Qiu, Shikai and Wilson, Andrew Gordon},
  journal={Advances in neural information processing systems},
  year={2023}
}

@inproceedings{sun2023test,
  title={TEST: Text prototype aligned embedding to activate LLM's ability for time series},
  author={Sun, Chenxi and Li, Yaliang and Li, Hongyan and Hong, Shenda},
  booktitle={International Conference on Machine Learning},
  year={2024}
}

@article{chang2023llm4ts,
  title={Llm4ts: Two-stage fine-tuning for time-series forecasting with pre-trained llms},
  author={Chang, Ching and Peng, Wen-Chih and Chen, Tien-Fu},
  journal={arXiv preprint arXiv:2308.08469},
  year={2023}
}

@inproceedings{cao2023tempo,
  title={TEMPO: Prompt-based Generative Pre-trained Transformer for Time Series Forecasting},
  author={Cao, Defu and Jia, Furong and Arik, Sercan O and Pfister, Tomas and Zheng, Yixiang and Ye, Wen and Liu, Yan},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{rasul2023lag,
  title={Lag-llama: Towards foundation models for time series forecasting},
  author={Rasul, Kashif and Ashok, Arjun and Williams, Andrew Robert and Khorasani, Arian and Adamopoulos, George and Bhagwatkar, Rishika and Bilo{\v{s}}, Marin and Ghonia, Hena and Hassen, Nadhir Vincent and Schneider, Anderson and others},
  journal={arXiv preprint arXiv:2310.08278},
  year={2023}
}

@article{chen2023gatgpt,
  title={Gatgpt: A pre-trained large language model with graph attention network for spatiotemporal imputation},
  author={Chen, Yakun and Wang, Xianzhi and Xu, Guandong},
  journal={arXiv preprint arXiv:2311.14332},
  year={2023}
}

@inproceedings{liu2023unitime,
  title={UniTime: A Language-Empowered Unified Model for Cross-Domain Time Series Forecasting},
  author={Liu, Xu and Hu, Junfeng and Li, Yuan and Diao, Shizhe and Liang, Yuxuan and Hooi, Bryan and Zimmermann, Roger},
  booktitle={The Web Conference 2024 (WWW)},
  year={2024} 
}

@article{zhou2023one2,
  title={One Fits All: Universal Time Series Analysis by Pretrained LM and Specially Designed Adaptors},
  author={Zhou, Tian and Niu, Peisong and Wang, Xue and Sun, Liang and Jin, Rong},
  journal={arXiv preprint arXiv:2311.14782},
  year={2023}
}

@article{liu2024spatial,
  title={Spatial-Temporal Large Language Model for Traffic Prediction},
  author={Liu, Chenxi and Yang, Sun and Xu, Qianxiong and Li, Zhishuai and Long, Cheng and Li, Ziyue and Zhao, Rui},
  journal={arXiv preprint arXiv:2401.10134},
  year={2024}
}

@article{caballero2022broken,
  title={Broken neural scaling laws},
  author={Caballero, Ethan and Gupta, Kshitij and Rish, Irina and Krueger, David},
  journal={arXiv preprint arXiv:2210.14891},
  year={2022}
}

@article{garza2023timegpt,
  title={TimeGPT-1},
  author={Garza, Azul and Mergenthaler-Canseco, Max},
  journal={arXiv preprint arXiv:2310.03589},
  year={2023}
}

@article{spathis2023first,
  title={The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models},
  author={Spathis, Dimitris and Kawsar, Fahim},
  journal={arXiv preprint arXiv:2309.06236},
  year={2023}
}

@article{ekambaram2024ttms,
  title={TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series},
  author={Ekambaram, Vijay and Jati, Arindam and Nguyen, Nam H and Dayama, Pankaj and Reddy, Chandra and Gifford, Wesley M and Kalagnanam, Jayant},
  journal={arXiv preprint arXiv:2401.03955},
  year={2024}
}

@inproceedings{das2023decoder,
  title={A decoder-only foundation model for time-series forecasting},
  author={Das, Abhimanyu and Kong, Weihao and Sen, Rajat and Zhou, Yichen},
  booktitle={the 41st International Conference on Machine Learning},
  year={2024}
}

@inproceedings{chen2023prompt,
  title={Prompt federated learning for weather forecasting: toward foundation models on meteorological data},
  author={Chen, Shengchao and Long, Guodong and Shen, Tao and Jiang, Jing},
  booktitle={Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence},
  pages={3532--3540},
  year={2023}
}

@inproceedings{mirchandani2023large,
  title={Large Language Models as General Pattern Machines},
  author={Mirchandani, Suvir and Xia, Fei and Florence, Pete and Driess, Danny and Arenas, Montserrat Gonzalez and Rao, Kanishka and Sadigh, Dorsa and Zeng, Andy and others},
  booktitle={7th Annual Conference on Robot Learning},
  year={2023}
}

@article{sun2023large,
  title={Large Trajectory Models are Scalable Motion Predictors and Planners},
  author={Sun, Qiao and Zhang, Shiduo and Ma, Danjiao and Shi, Jingzhe and Li, Derun and Luo, Simian and Wang, Yu and Xu, Ningyi and Cao, Guangzhi and Zhao, Hang},
  journal={arXiv preprint arXiv:2310.19620},
  year={2023}
}

@article{kamarthi2023pems,
  title={PEMS: Pre-trained Epidmic Time-series Models},
  author={Kamarthi, Harshavardhan and Prakash, B Aditya},
  journal={arXiv preprint arXiv:2311.07841},
  year={2023}
}

@article{zhang2023large,
  title={Large Language Models for Spatial Trajectory Patterns Mining},
  author={Zhang, Zheng and Amiri, Hossein and Liu, Zhenke and Z{\"u}fle, Andreas and Zhao, Liang},
  journal={arXiv preprint arXiv:2310.04942},
  year={2023}
}

@article{liu2023biosignal,
  title={BioSignal Copilot: Leveraging the power of LLMs in drafting reports for biomedical signals},
  author={Liu, Chunyu and Ma, Yongpei and Kothur, Kavitha and Nikpour, Armin and Kavehei, Omid},
  journal={medRxiv},
  pages={2023--06},
  year={2023},
  publisher={Cold Spring Harbor Laboratory Press}
}

@inproceedings{wen2022robust,
  title={Robust time series analysis and applications: An industrial perspective},
  author={Wen, Qingsong and Yang, Linxiao and Zhou, Tian and Sun, Liang},
  booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'22)},
  pages={4836--4837},
  year={2022}
}


@inproceedings{liang2024foundation,
  title={Foundation models for time series analysis: A tutorial and survey},
  author={Liang, Yuxuan and Wen, Haomin and Nie, Yuqi and Jiang, Yushan and Jin, Ming and Song, Dongjin and Pan, Shirui and Wen, Qingsong},
  booktitle={ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD'24)},
  year={2024}
}

@inproceedings{qiu2023automated,
  title={Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report},
  author={Qiu, Jielin and Zhu, Jiacheng and Liu, Shiqi and Han, William and Zhang, Jingqi and Duan, Chaojing and Rosenberg, Michael A and Liu, Emerson and Weber, Douglas and Zhao, Ding},
  booktitle={Machine Learning for Health (ML4H)},
  pages={480--497},
  year={2023},
  organization={PMLR}
}

@article{qiu2023transfer,
  title={Transfer knowledge from natural language to electrocardiography: Can we detect cardiovascular disease through language models?},
  author={Qiu, Jielin and Han, William and Zhu, Jiacheng and Xu, Mengdi and Rosenberg, Michael and Liu, Emerson and Weber, Douglas and Zhao, Ding},
  journal={arXiv preprint arXiv:2301.09017},
  year={2023}
}

@article{zhang2023trafficgpt,
  title={Trafficgpt: Viewing, processing and interacting with traffic foundation models},
  author={Zhang, Siyao and Fu, Daocheng and Liang, Wenzhe and Zhang, Zhao and Yu, Bin and Cai, Pinlong and Yao, Baozhen},
  journal={Transport Policy},
  volume={150},
  pages={95--105},
  year={2024},
  publisher={Elsevier}
}

@article{fatouros2024can,
  title={Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection},
  author={Fatouros, Georgios and Metaxas, Konstantinos and Soldatos, John and Kyriazis, Dimosthenis},
  journal={arXiv preprint arXiv:2401.03737},
  year={2024}
}

@article{ghosh2023spatio,
  title={Spatio-temporal Storytelling? Leveraging Generative Models for Semantic Trajectory Analysis},
  author={Ghosh, Shreya and Sengupta, Saptarshi and Mitra, Prasenjit},
  journal={arXiv preprint arXiv:2306.13905},
  year={2023}
}

@article{chatterjee2023amicron,
  title={AmicroN: A Framework for Generating Annotations for Human Activity Recognition with Granular Micro-Activities},
  author={Chatterjee, Soumyajit and Mitra, Bivas and Chakraborty, Sandip},
  journal={arXiv preprint arXiv:2306.13149},
  year={2023}
}

@inproceedings{zhang2023insight,
  title={Insight Miner: A Time Series Analysis Dataset for Cross-Domain Alignment with Natural Language},
  author={Zhang, Yunkai and Zhang, Yawen and Zheng, Ming and Chen, Kezhen and Gao, Chongyang and Ge, Ruian and Teng, Siyuan and Jelloul, Amine and Rao, Jinmeng and Guo, Xiaoyuan and others},
  booktitle={NeurIPS 2023 AI for Science Workshop},
  year={2023}
}


@article{woo2023pushing,
  title={Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain},
  author={Woo, Gerald and Liu, Chenghao and Kumar, Akshat and Sahoo, Doyen},
  journal={arXiv preprint arXiv:2310.05063},
  year={2023}
}

@article{liang2023exploring,
  title={Exploring large language models for human mobility prediction under public events},
  author={Liang, Yuebing and Liu, Yichao and Wang, Xiaohan and Zhao, Zhan},
  journal={arXiv preprint arXiv:2311.17351},
  year={2023}
}

@inproceedings{
anonymous2024st,
title={Spatio-Temporal Graph Learning With Large Language Model},
author={Zhang, Qianru and Ren, Xubin and Xia, Lianghao and Yiu, Siu Ming and Huang, Chao},
year={2024},
url={https://openreview.net/forum?id=QUkcfqa6GX},
}
% booktitle={Underreview in Twelfth International Conference on Learning Representations}

@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}

@article{lopez2023can,
  title={Can ChatGPT Forecast Stock Price Movements? Return Predictability and Large Language Models},
  author={Lopez-Lira, Alejandro and Tang, Yuehua},
  journal={Return Predictability and Large Language Models (April 6, 2023)},
  year={2023}
}

@article{kim2024health,
  title={Health-LLM: Large Language Models for Health Prediction via Wearable Sensor Data},
  author={Kim, Yubin and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
  journal={arXiv preprint arXiv:2401.06866},
  year={2024}
}


@article{wang2023would,
  title={Where would i go next? large language models as human mobility predictors},
  author={Wang, Xinglei and Fang, Meng and Zeng, Zichao and Cheng, Tao},
  journal={arXiv preprint arXiv:2308.15197},
  year={2023}
}

@inproceedings{xue2023utilizing,
  title={Utilizing Language Models for Energy Load Forecasting},
  author={Xue, Hao and Salim, Flora D},
  booktitle={Proceedings of the 10th ACM International Conference on Systems for Energy-Efficient Buildings, Cities, and Transportation},
  pages={224--227},
  year={2023}
}

@inproceedings{xue2022leveraging,
  title={Leveraging language foundation models for human mobility forecasting},
  author={Xue, Hao and Voutharoja, Bhanu Prakash and Salim, Flora D},
  booktitle={Proceedings of the 30th International Conference on Advances in Geographic Information Systems},
  pages={1--9},
  year={2022}
}

@article{khaokaew2023maple,
  title={MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings},
  author={Khaokaew, Yonchanok and Xue, Hao and Salim, Flora D},
  journal={arXiv preprint arXiv:2309.08648},
  year={2023}
}

@inproceedings{yu-etal-2023-harnessing,
    title = "Harnessing {LLM}s for Temporal Data - A Study on Explainable Financial Time Series Forecasting",
    author = "Yu, Xinli  and
      Chen, Zheng  and
      Lu, Yanbin",
    booktitle = "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = dec,
    year = "2023",
    address = "Singapore",
    pages = "739--753"
}

@article{zhang2024large,
  title={Large Language Models for Time Series: A Survey},
  author={Zhang, Xiyuan and Chowdhury, Ranak Roy and Gupta, Rajesh K and Shang, Jingbo},
  journal={arXiv preprint arXiv:2402.01801},
  year={2024}
}

@article{cheng2024exploring,
  title={Exploring Large Language Model based Intelligent Agents: Definitions, Methods, and Prospects},
  author={Cheng, Yuheng and Zhang, Ceyao and Zhang, Zhengwen and Meng, Xiangrui and Hong, Sirui and Li, Wenhao and Wang, Zihao and Wang, Zekai and Yin, Feng and Zhao, Junhua and others},
  journal={arXiv preprint arXiv:2401.03428},
  year={2024}
}