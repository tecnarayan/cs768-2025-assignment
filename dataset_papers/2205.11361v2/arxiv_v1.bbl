\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{WHX{\etalchar{+}}20}

\bibitem[AGZ21]{agarwal2021acceleration}
Naman Agarwal, Surbhi Goel, and Cyril Zhang.
\newblock Acceleration via fractal learning rate schedules.
\newblock {\em arXiv preprint arXiv:2103.01338}, 2021.

\bibitem[App09]{applebaum2009levy}
David Applebaum.
\newblock {\em L{\'e}vy Processes and Stochastic Calculus}.
\newblock Cambridge University Press, 2009.

\bibitem[BCN18]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311, 2018.

\bibitem[C{\etalchar{+}}47]{cauchy1847methode}
Augustin Cauchy et~al.
\newblock M{\'e}thode g{\'e}n{\'e}rale pour la r{\'e}solution des systemes
  d’{\'e}quations simultan{\'e}es.
\newblock {\em Comp. Rend. Sci. Paris}, 25(1847):536--538, 1847.

\bibitem[CFK{\etalchar{+}}16]{chevyrev2016multiscale}
Ilya Chevyrev, Peter~K Friz, Alexey Korepanov, Ian Melbourne, and Huilin Zhang.
\newblock Multiscale systems, homogenization, and rough paths.
\newblock In {\em International Conference in Honor of the 75th Birthday of SRS
  Varadhan}, pages 17--48. Springer, 2016.

\bibitem[CFKM20]{chevyrev2020superdiffusive}
Ilya Chevyrev, Peter~K Friz, Alexey Korepanov, and Ian Melbourne.
\newblock Superdiffusive limits for deterministic fast--slow dynamical systems.
\newblock {\em Probability Theory and Related Fields}, 178(3):735--770, 2020.

\bibitem[CKL{\etalchar{+}}21]{cohen2021gradient}
Jeremy~M Cohen, Simran Kaur, Yuanzhi Li, J~Zico Kolter, and Ameet Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock In {\em ICLR}, 2021.

\bibitem[CP14]{chechkin2014marcus}
Alexei Chechkin and Ilya Pavlyukevich.
\newblock Marcus versus {S}tratonovich for systems with jump noise.
\newblock {\em Journal of Physics A: Mathematical and Theoretical},
  47(34):342001, 2014.

\bibitem[CWZ{\etalchar{+}}21]{camuto2021asymmetric}
Alexander Camuto, Xiaoyu Wang, Lingjiong Zhu, Chris Holmes, Mert
  G{\"u}rb{\"u}zbalaban, and Umut {\c{S}}im{\c{s}}ekli.
\newblock Asymmetric heavy tails and implicit bias in {G}aussian noise
  injections.
\newblock {\em arXiv preprint arXiv:2102.07006}, 2021.

\bibitem[DG17]{Dua:2019}
Dheeru Dua and Casey Graff.
\newblock {UCI} machine learning repository, 2017.

\bibitem[FV10]{friz_victoir_2010}
Peter~K. Friz and Nicolas~B. Victoir.
\newblock {\em Multidimensional Stochastic Processes as Rough Paths: Theory and
  Applications}.
\newblock Cambridge Studies in Advanced Mathematics. Cambridge University
  Press, 2010.

\bibitem[GAG{\etalchar{+}}00]{goldberger2000physiobank}
Ary~L Goldberger, Luis~AN Amaral, Leon Glass, Jeffrey~M Hausdorff, Plamen~Ch
  Ivanov, Roger~G Mark, Joseph~E Mietus, George~B Moody, Chung-Kang Peng, and
  H~Eugene Stanley.
\newblock Physio{B}ank, {P}hysio{T}oolkit, and {P}hysio{N}et: components of a
  new research resource for complex physiologic signals.
\newblock {\em Circulation}, 101(23):e215--e220, 2000.

\bibitem[GGP{\etalchar{+}}21]{geiping2021stochastic}
Jonas Geiping, Micah Goldblum, Phillip~E Pope, Michael Moeller, and Tom
  Goldstein.
\newblock Stochastic training is not necessary for generalization.
\newblock {\em arXiv preprint arXiv:2109.14119}, 2021.

\bibitem[GK54]{gnedenko_kolmogorov}
Boris~Vladimirovich Gnedenko and Andrey~Nikolaevich Kolmogorov.
\newblock {\em Limit Distributions for Sums of Independent Random Variables}.
\newblock Cambridge, Addison-Wesley, 1954.

\bibitem[GM13]{gottwald2013homogenization}
Georg~A Gottwald and Ian Melbourne.
\newblock Homogenization for deterministic maps and multiplicative noise.
\newblock {\em Proceedings of the Royal Society A: Mathematical, Physical and
  Engineering Sciences}, 469(2156):20130201, 2013.

\bibitem[GM21]{gottwald2021simulation}
Georg~A Gottwald and Ian Melbourne.
\newblock Simulation of non-{L}ipschitz stochastic differential equations
  driven by $\alpha$-stable noise: A method based on deterministic
  homogenization.
\newblock {\em Multiscale Modeling \& Simulation}, 19(2):665--687, 2021.

\bibitem[Gou04]{gouezel2004central}
S{\'e}bastien Gou{\"e}zel.
\newblock Central limit theorem and stable laws for intermittent maps.
\newblock {\em Probability Theory and Related Fields}, 128(1):82--122, 2004.

\bibitem[H{\c{S}}KM21]{hodgkinson2021generalization}
Liam Hodgkinson, Umut {\c{S}}im{\c{s}}ekli, Rajiv Khanna, and Michael~W
  Mahoney.
\newblock Generalization properties of stochastic optimizers via trajectory
  analysis.
\newblock {\em arXiv preprint arXiv:2108.00781}, 2021.

\bibitem[HZRS16]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[JNM{\etalchar{+}}19]{jiang2019fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock {\em arXiv preprint arXiv:1912.02178}, 2019.

\bibitem[KMN{\etalchar{+}}16]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kri09]{krizhevsky2009learning}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[KT20]{kong2020stochasticity}
Lingkai Kong and Molei Tao.
\newblock Stochasticity of deterministic gradient descent: Large learning rate
  for multiscale objective function.
\newblock {\em Advances in Neural Information Processing Systems},
  33:2625--2638, 2020.

\bibitem[LEHM21]{lim2021noisy}
Soon~Hoe Lim, N~Benjamin Erichson, Liam Hodgkinson, and Michael~W Mahoney.
\newblock Noisy recurrent neural networks.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Mah12]{Mah12}
M.~W. Mahoney.
\newblock Approximate computation and implicit regularization for very
  large-scale data analysis.
\newblock In {\em Proceedings of the 31st ACM Symposium on Principles of
  Database Systems}, pages 143--154, 2012.

\bibitem[Mar81]{marcus1981modeling}
Steven~I Marcus.
\newblock Modeling and approximation of stochastic differential equations
  driven by semimartingales.
\newblock {\em Stochastics: An International Journal of Probability and
  Stochastic Processes}, 4(3):223--245, 1981.

\bibitem[NVL{\etalchar{+}}15]{neelakantan2015adding}
Arvind Neelakantan, Luke Vilnis, Quoc~V Le, Ilya Sutskever, Lukasz Kaiser,
  Karol Kurach, and James Martens.
\newblock Adding gradient noise improves learning for very deep networks.
\newblock {\em arXiv preprint arXiv:1511.06807}, 2015.

\bibitem[Oym21]{oymak2021provable}
Samet Oymak.
\newblock Provable super-convergence with a large cyclical learning rate.
\newblock {\em IEEE Signal Processing Letters}, 28:1645--1649, 2021.

\bibitem[PS08]{pavliotis2008multiscale}
Grigoris Pavliotis and Andrew Stuart.
\newblock {\em Multiscale Methods: Averaging and Homogenization}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Sam17]{samoradnitsky2017stable}
G.~Samoradnitsky.
\newblock {\em Stable Non-Gaussian Random Processes: Stochastic Models with
  Infinite Variance}.
\newblock CRC Press, 2017.

\bibitem[Sch98]{schilling1998feller}
Ren{\'e}~L Schilling.
\newblock Feller processes generated by pseudo-differential operators: On the
  {H}ausdorff dimension of their sample paths.
\newblock {\em Journal of Theoretical Probability}, 11(2):303--330, 1998.

\bibitem[Smi17]{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In {\em 2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem[{\c{S}}SDE20]{csimcsekli2020hausdorff}
Umut {\c{S}}im{\c{s}}ekli, Ozan Sener, George Deligiannidis, and Murat~A
  Erdogdu.
\newblock Hausdorff dimension, heavy tails, and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:2006.09313}, 2020.

\bibitem[SSG19]{simsekli2019tail}
Umut Simsekli, Levent Sagun, and Mert Gurbuzbalaban.
\newblock A tail-index analysis of stochastic gradient noise in deep neural
  networks.
\newblock In {\em International Conference on Machine Learning}, pages
  5827--5837. PMLR, 2019.

\bibitem[SZTG20]{simsekli2020fractional}
Umut Simsekli, Lingjiong Zhu, Yee~Whye Teh, and Mert Gurbuzbalaban.
\newblock Fractional underdamped {L}angevin dynamics: Retargeting {SGD} with
  momentum under heavy-tailed gradient noise.
\newblock In {\em International Conference on Machine Learning}, pages
  8970--8980. PMLR, 2020.

\bibitem[Tha80]{thaler1980estimates}
Maximilian Thaler.
\newblock Estimates of the invariant densities of endomorphisms with
  indifferent fixed points.
\newblock {\em Israel Journal of Mathematics}, 37(4):303--314, 1980.

\bibitem[WHX{\etalchar{+}}20]{wu2020noisy}
Jingfeng Wu, Wenqing Hu, Haoyi Xiong, Jun Huan, Vladimir Braverman, and
  Zhanxing Zhu.
\newblock On the noisy gradient descent that generalizes as {S}{G}{D}.
\newblock In {\em International Conference on Machine Learning}, pages
  10367--10376. PMLR, 2020.

\bibitem[You36]{young1936}
L.~C. Young.
\newblock {An inequality of the Hölder type, connected with Stieltjes
  integration}.
\newblock {\em Acta Mathematica}, 67(none):251 -- 282, 1936.

\end{thebibliography}
