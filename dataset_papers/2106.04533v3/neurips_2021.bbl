\begin{thebibliography}{100}

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{han2020survey}
Kai Han, Yunhe Wang, Hanting Chen, Xinghao Chen, Jianyuan Guo, Zhenhua Liu,
  Yehui Tang, An~Xiao, Chunjing Xu, Yixing Xu, et~al.
\newblock A survey on visual transformer.
\newblock {\em arXiv preprint arXiv:2012.12556}, 2020.

\bibitem{zheng2020end}
Minghang Zheng, Peng Gao, Xiaogang Wang, Hongsheng Li, and Hao Dong.
\newblock End-to-end object detection with adaptive clustering transformer.
\newblock {\em arXiv preprint arXiv:2011.09315}, 2020.

\bibitem{carion2020end}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In {\em European Conference on Computer Vision}, pages 213--229.
  Springer, 2020.

\bibitem{dai2020up}
Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen.
\newblock Up-detr: Unsupervised pre-training for object detection with
  transformers.
\newblock {\em arXiv preprint arXiv:2011.09094}, 2020.

\bibitem{zhu2021deformable}
Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai.
\newblock Deformable {\{}detr{\}}: Deformable transformers for end-to-end
  object detection.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{chen2020pre}
Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei
  Ma, Chunjing Xu, Chao Xu, and Wen Gao.
\newblock Pre-trained image processing transformer.
\newblock {\em arXiv preprint arXiv:2012.00364}, 2020.

\bibitem{yang2020learning}
Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Baining Guo.
\newblock Learning texture transformer network for image super-resolution.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 5791--5800, 2020.

\bibitem{parmar2018image}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In {\em International Conference on Machine Learning}, pages
  4055--4064. PMLR, 2018.

\bibitem{pmlr-v119-chen20s}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In Hal~Daumé III and Aarti Singh, editors, {\em Proceedings of the
  37th International Conference on Machine Learning}, volume 119 of {\em
  Proceedings of Machine Learning Research}, pages 1691--1703. PMLR, 13--18 Jul
  2020.

\bibitem{jiang2021transgan}
Yifan Jiang, Shiyu Chang, and Zhangyang Wang.
\newblock Transgan: Two transformers can make one strong gan.
\newblock {\em arXiv preprint arXiv:2102.07074}, 2021.

\bibitem{zhu2021visual}
Mingjian Zhu, Kai Han, Yehui Tang, and Yunhe Wang.
\newblock Visual transformer pruning, 2021.

\bibitem{cheng2017survey}
Yu~Cheng, Duo Wang, Pan Zhou, and Tao Zhang.
\newblock A survey of model compression and acceleration for deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1710.09282}, 2017.

\bibitem{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock In {\em International Conference on Learning Representations}, 2016.

\bibitem{molchanov2019importance}
Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Frosio, and Jan Kautz.
\newblock Importance estimation for neural network pruning.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 11264--11272, 2019.

\bibitem{zhu2017prune}
Michael Zhu and Suyog Gupta.
\newblock To prune, or not to prune: exploring the efficacy of pruning for
  model compression.
\newblock {\em arXiv preprint arXiv:1710.01878}, 2017.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{frankle2019linear}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock Linear mode connectivity and the lottery ticket hypothesis.
\newblock {\em arXiv preprint arXiv:1912.05671}, 2019.

\bibitem{pmlr-v139-zhang21c}
Zhenyu Zhang, Xuxi Chen, Tianlong Chen, and Zhangyang Wang.
\newblock Efficient lottery ticket finding: Less data is more.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 12380--12390. PMLR, 18--24 Jul 2021.

\bibitem{chen2020lottery}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Zhangyang
  Wang, and Michael Carbin.
\newblock The lottery ticket hypothesis for pre-trained bert networks.
\newblock {\em arXiv preprint arXiv:2007.12223}, 2020.

\bibitem{chen2020lottery2}
Tianlong Chen, Jonathan Frankle, Shiyu Chang, Sijia Liu, Yang Zhang, Michael
  Carbin, and Zhangyang Wang.
\newblock The lottery tickets hypothesis for supervised and self-supervised
  pre-training in computer vision models.
\newblock {\em arXiv preprint arXiv:2012.06908}, 2020.

\bibitem{chen2021gans}
Xuxi Chen, Zhenyu Zhang, Yongduo Sui, and Tianlong Chen.
\newblock {\{}GAN{\}}s can play lottery tickets too.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{ma2021good}
Haoyu Ma, Tianlong Chen, Ting-Kuei Hu, Chenyu You, Xiaohui Xie, and Zhangyang
  Wang.
\newblock Good students play big lottery better.
\newblock {\em arXiv preprint arXiv:2101.03255}, 2021.

\bibitem{gan2021playing}
Zhe Gan, Yen-Chun Chen, Linjie Li, Tianlong Chen, Yu~Cheng, Shuohang Wang, and
  Jingjing Liu.
\newblock Playing lottery tickets with vision and language.
\newblock {\em arXiv preprint arXiv:2104.11832}, 2021.

\bibitem{chen2021unified}
Tianlong Chen, Yongduo Sui, Xuxi Chen, Aston Zhang, and Zhangyang Wang.
\newblock A unified lottery ticket hypothesis for graph neural networks.
\newblock {\em arXiv preprint arXiv:2102.06790}, 2021.

\bibitem{chen2021ultra}
Tianlong Chen, Yu~Cheng, Zhe Gan, Jingjing Liu, and Zhangyang Wang.
\newblock Ultra-data-efficient gan training: Drawing a lottery ticket first,
  then training it toughly.
\newblock {\em arXiv preprint arXiv:2103.00397}, 2021.

\bibitem{lee2018snip}
Namhoon Lee, Thalaiyasingam Ajanthan, and Philip Torr.
\newblock Snip: Single-shot network pruning based on connection sensitivity.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{grasp}
Chaoqi Wang, Guodong Zhang, and Roger Grosse.
\newblock Picking winning tickets before training by preserving gradient flow.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{You2020Drawing}
Haoran You, Chaojian Li, Pengfei Xu, Yonggan Fu, Yue Wang, Xiaohan Chen,
  Richard~G. Baraniuk, Zhangyang Wang, and Yingyan Lin.
\newblock Drawing early-bird tickets: Toward more efficient training of deep
  networks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{chen2020earlybert}
Xiaohan Chen, Yu~Cheng, Shuohang Wang, Zhe Gan, Zhangyang Wang, and Jingjing
  Liu.
\newblock Earlybert: Efficient bert training via early-bird lottery tickets.
\newblock {\em arXiv preprint arXiv:2101.00063}, 2020.

\bibitem{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature communications}, 9(1):1--12, 2018.

\bibitem{onemillionneurons}
Shiwei Liu, Decebal~Constantin Mocanu, Amarsagar Reddy~Ramapuram Matavalam,
  Yulong Pei, and Mykola Pechenizkiy.
\newblock Sparse evolutionary deep learning with over one million artificial
  neurons on commodity hardware.
\newblock {\em Neural Computing and Applications}, 2020.

\bibitem{evci2020rigging}
Utku Evci, Trevor Gale, Jacob Menick, Pablo~Samuel Castro, and Erich Elsen.
\newblock Rigging the lottery: Making all tickets winners.
\newblock In {\em International Conference on Machine Learning}, pages
  2943--2952. PMLR, 2020.

\bibitem{liu2021we}
Shiwei Liu, Lu~Yin, Decebal~Constantin Mocanu, and Mykola Pechenizkiy.
\newblock Do we actually need dense over-parameterization? in-time
  over-parameterization in sparse training.
\newblock {\em arXiv preprint arXiv:2102.02887}, 2021.

\bibitem{evci2019difficulty}
Utku Evci, Fabian Pedregosa, Aidan Gomez, and Erich Elsen.
\newblock The difficulty of training sparse neural networks.
\newblock {\em arXiv preprint arXiv:1906.10732}, 2019.

\bibitem{mostafa2019parameter}
Hesham Mostafa and Xin Wang.
\newblock Parameter efficient training of deep convolutional neural networks by
  dynamic sparse reparameterization.
\newblock In {\em International Conference on Machine Learning}, 2019.

\bibitem{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{liu2021selfish}
Shiwei Liu, Decebal~Constantin Mocanu, Yulong Pei, and Mykola Pechenizkiy.
\newblock Selfish sparse rnn training.
\newblock {\em arXiv preprint arXiv:2101.09048}, 2021.

\bibitem{jayakumar2020top}
Siddhant Jayakumar, Razvan Pascanu, Jack Rae, Simon Osindero, and Erich Elsen.
\newblock Top-kast: Top-k always sparse training.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{raihan2020sparse}
Md~Aamir Raihan and Tor~M Aamodt.
\newblock Sparse weight activation training.
\newblock {\em arXiv preprint arXiv:2001.01969}, 2020.

\bibitem{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?, 2019.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{guo2021cmt}
Jianyuan Guo, Kai Han, Han Wu, Chang Xu, Yehui Tang, Chunjing Xu, and Yunhe
  Wang.
\newblock Cmt: Convolutional neural networks meet vision transformers.
\newblock {\em arXiv preprint arXiv:2107.06263}, 2021.

\bibitem{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis~EH
  Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token vit: Training vision transformers from scratch on
  imagenet.
\newblock {\em arXiv preprint arXiv:2101.11986}, 2021.

\bibitem{wang2020max}
Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.
\newblock Max-deeplab: End-to-end panoptic segmentation with mask transformers.
\newblock {\em arXiv preprint arXiv:2012.00759}, 2020.

\bibitem{wang2020end}
Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen,
  and Huaxia Xia.
\newblock End-to-end video instance segmentation with transformers.
\newblock {\em arXiv preprint arXiv:2011.14503}, 2020.

\bibitem{zeng2020learning}
Yanhong Zeng, Jianlong Fu, and Hongyang Chao.
\newblock Learning joint spatial-temporal transformations for video inpainting.
\newblock In {\em European Conference on Computer Vision}, pages 528--543.
  Springer, 2020.

\bibitem{zhou2018end}
Luowei Zhou, Yingbo Zhou, Jason~J Corso, Richard Socher, and Caiming Xiong.
\newblock End-to-end dense video captioning with masked transformer.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 8739--8748, 2018.

\bibitem{lu2019vilbert}
Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.
\newblock Vilbert: Pretraining task-agnostic visiolinguistic representations
  for vision-and-language tasks.
\newblock {\em arXiv preprint arXiv:1908.02265}, 2019.

\bibitem{tan2019lxmert}
Hao Tan and Mohit Bansal.
\newblock Lxmert: Learning cross-modality encoder representations from
  transformers.
\newblock {\em arXiv preprint arXiv:1908.07490}, 2019.

\bibitem{chen2020uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El~Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Universal image-text representation learning.
\newblock In {\em European Conference on Computer Vision}, pages 104--120.
  Springer, 2020.

\bibitem{su2019vl}
Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai.
\newblock Vl-bert: Pre-training of generic visual-linguistic representations.
\newblock {\em arXiv preprint arXiv:1908.08530}, 2019.

\bibitem{li2019visualbert}
Liunian~Harold Li, Mark Yatskar, Da~Yin, Cho-Jui Hsieh, and Kai-Wei Chang.
\newblock Visualbert: A simple and performant baseline for vision and language.
\newblock {\em arXiv preprint arXiv:1908.03557}, 2019.

\bibitem{li2020unicoder}
Gen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang.
\newblock Unicoder-vl: A universal encoder for vision and language by
  cross-modal pre-training.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 11336--11344, 2020.

\bibitem{li2020oscar}
Xiujun Li, Xi~Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan
  Wang, Houdong Hu, Li~Dong, Furu Wei, et~al.
\newblock Oscar: Object-semantics aligned pre-training for vision-language
  tasks.
\newblock In {\em European Conference on Computer Vision}, pages 121--137.
  Springer, 2020.

\bibitem{zhou2020unified}
Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng
  Gao.
\newblock Unified vision-language pre-training for image captioning and vqa.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 13041--13049, 2020.

\bibitem{zhao2020point}
Hengshuang Zhao, Li~Jiang, Jiaya Jia, Philip Torr, and Vladlen Koltun.
\newblock Point transformer.
\newblock {\em arXiv preprint arXiv:2012.09164}, 2020.

\bibitem{Fan2020Reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{guo2020reweighted}
Fu-Ming Guo, Sijia Liu, Finlay~S Mungall, Xue Lin, and Yanzhi Wang.
\newblock Reweighted proximal pruning for large-scale language representation.
\newblock {\em arXiv preprint arXiv:1909.12486}, 2019.

\bibitem{ganesh2020compressing}
Prakhar Ganesh, Yao Chen, Xin Lou, Mohammad~Ali Khan, Yin Yang, Deming Chen,
  Marianne Winslett, Hassan Sajjad, and Preslav Nakov.
\newblock Compressing large-scale transformer-based models: A case study on
  bert.
\newblock {\em arXiv preprint arXiv:2002.11985}, 2020.

\bibitem{mccarley2019structured}
J.~S. McCarley, Rishav Chakravarti, and Avirup Sil.
\newblock Structured pruning of a bert-based question answering model.
\newblock {\em arXiv preprint arXiv:1910.06360}, 2019.

\bibitem{tang2021patch}
Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo, Chao Xu, and Dacheng
  Tao.
\newblock Patch slimming for efficient vision transformers.
\newblock {\em arXiv preprint arXiv:2106.02852}, 2021.

\bibitem{zhang2021multi}
Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu~Yuan, Lei Zhang, and
  Jianfeng Gao.
\newblock Multi-scale vision longformer: A new vision transformer for
  high-resolution image encoding.
\newblock {\em arXiv preprint arXiv:2103.15358}, 2021.

\bibitem{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{kitaev2020reformer}
Nikita Kitaev, {\L}ukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock {\em arXiv preprint arXiv:2001.04451}, 2020.

\bibitem{lee2019set}
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee~Whye
  Teh.
\newblock Set transformer: A framework for attention-based
  permutation-invariant neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  3744--3753. PMLR, 2019.

\bibitem{roy2021efficient}
Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock {\em Transactions of the Association for Computational Linguistics},
  9:53--68, 2021.

\bibitem{rae2019compressive}
Jack~W Rae, Anna Potapenko, Siddhant~M Jayakumar, and Timothy~P Lillicrap.
\newblock Compressive transformers for long-range sequence modelling.
\newblock {\em arXiv preprint arXiv:1911.05507}, 2019.

\bibitem{ho2019axial}
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans.
\newblock Axial attention in multidimensional transformers.
\newblock {\em arXiv preprint arXiv:1912.12180}, 2019.

\bibitem{katharopoulos2020transformers}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois
  Fleuret.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In {\em International Conference on Machine Learning}, pages
  5156--5165. PMLR, 2020.

\bibitem{choromanski2020rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,
  Lukasz Kaiser, et~al.
\newblock Rethinking attention with performers.
\newblock {\em arXiv preprint arXiv:2009.14794}, 2020.

\bibitem{tay2020long}
Yi~Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham,
  Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock {\em arXiv preprint arXiv:2011.04006}, 2020.

\bibitem{tay2020efficient}
Yi~Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler.
\newblock Efficient transformers: A survey.
\newblock {\em arXiv preprint arXiv:2009.06732}, 2020.

\bibitem{wang2020linformer}
Sinong Wang, Belinda~Z. Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity, 2020.

\bibitem{pan2021iared2}
Bowen Pan, Yifan Jiang, Rameswar Panda, Zhangyang Wang, Rogerio Feris, and Aude
  Oliva.
\newblock Ia-red$^2$: Interpretability-aware redundancy reduction for vision
  transformers, 2021.

\bibitem{lecun1990optimal}
Yann LeCun, John~S Denker, and Sara~A Solla.
\newblock Optimal brain damage.
\newblock In {\em Advances in neural information processing systems}, pages
  598--605, 1990.

\bibitem{magnitude}
Song Han, Jeff Pool, John Tran, and William Dally.
\newblock Learning both weights and connections for efficient neural network.
\newblock In C.~Cortes, N.~D. Lawrence, D.~D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, {\em Advances in Neural Information Processing Systems 28}, pages
  1135--1143. Curran Associates, Inc., 2015.

\bibitem{hessian}
Yann LeCun, John~S. Denker, and Sara~A. Solla.
\newblock Optimal brain damage.
\newblock In D.~S. Touretzky, editor, {\em Advances in Neural Information
  Processing Systems 2}, pages 598--605. Morgan-Kaufmann, 1990.

\bibitem{liu2017learning}
Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui
  Zhang.
\newblock Learning efficient convolutional networks through network slimming.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, pages 2736--2744, 2017.

\bibitem{he2017channel}
Yihui He, Xiangyu Zhang, and Jian Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In {\em Proceedings of the IEEE International Conference on Computer
  Vision}, 2017.

\bibitem{zhou2016less}
Hao Zhou, Jose~M Alvarez, and Fatih Porikli.
\newblock Less is more: Towards compact cnns.
\newblock In {\em European Conference on Computer Vision}, pages 662--677.
  Springer, 2016.

\bibitem{mocanu2016topological}
Decebal~Constantin Mocanu, Elena Mocanu, Phuong~H Nguyen, Madeleine Gibescu,
  and Antonio Liotta.
\newblock A topological insight into restricted boltzmann machines.
\newblock {\em Machine Learning}, 104(2-3):243--270, 2016.

\bibitem{lym2019prunetrain}
Sangkug Lym, Esha Choukse, Siavash Zangeneh, Wei Wen, Sujay Sanghavi, and
  Mattan Erez.
\newblock Prunetrain: fast neural network training by dynamic sparse model
  reconfiguration.
\newblock In {\em Proceedings of the International Conference for High
  Performance Computing, Networking, Storage and Analysis}, pages 1--13, 2019.

\bibitem{bartoldson2019generalization}
Brian~R Bartoldson, Ari~S Morcos, Adrian Barbu, and Gordon Erlebacher.
\newblock The generalization-stability tradeoff in neural network pruning.
\newblock {\em arXiv preprint arXiv:1906.03728}, 2019.

\bibitem{gumbel}
Emit~J. Gumbel.
\newblock Statistical theory of extreme values and some practical applications.
\newblock {\em The Journal of the Royal Aeronautical Society},
  58(527):792–793, 1954.

\bibitem{maddison2014sampling}
Chris~J Maddison, Daniel Tarlow, and Tom Minka.
\newblock A* sampling.
\newblock {\em arXiv preprint arXiv:1411.0030}, 2014.

\bibitem{yin2019understanding}
Penghang Yin, Jiancheng Lyu, Shuai Zhang, Stanley Osher, Yingyong Qi, and Jack
  Xin.
\newblock Understanding straight-through estimator in training activation
  quantized neural nets.
\newblock {\em arXiv preprint arXiv:1903.05662}, 2019.

\bibitem{frankle2019lottery}
Jonathan Frankle, Gintare~Karolina Dziugaite, Daniel~M Roy, and Michael Carbin.
\newblock The lottery ticket hypothesis at scale.
\newblock {\em arXiv preprint arXiv:1903.01611}, 2019.

\bibitem{gale2019state}
Trevor Gale, Erich Elsen, and Sara Hooker.
\newblock The state of sparsity in deep neural networks.
\newblock {\em arXiv preprint arXiv:1902.09574}, 2019.

\bibitem{han2021transformer}
Kai Han, An~Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang.
\newblock Transformer in transformer.
\newblock {\em arXiv preprint arXiv:2103.00112}, 2021.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock {\em arXiv preprint arXiv:2102.12122}, 2021.

\bibitem{zhou2021deepvit}
Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xiaochen Lian, Zihang
  Jiang, Qibin Hou, and Jiashi Feng.
\newblock Deepvit: Towards deeper vision transformer.
\newblock {\em arXiv preprint arXiv:2103.11886}, 2021.

\bibitem{Cordonnier2020On}
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.
\newblock On the relationship between self-attention and convolutional layers.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{elsen2020fast}
Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan.
\newblock Fast sparse convnets.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 14629--14638, 2020.

\bibitem{8465793}
Peiqi Wang, Yu~Ji, Chi Hong, Yongqiang Lyu, Dongsheng Wang, and Yuan Xie.
\newblock Snrram: An efficient sparse neural network computation architecture
  based on resistive random-access memory.
\newblock In {\em 2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)},
  pages 1--6, 2018.

\bibitem{ashbyexploiting}
Mike Ashby, Christiaan Baaij, Peter Baldwin, Martijn Bastiaan, Oliver Bunting,
  Aiken Cairncross, Christopher Chalmers, Liz Corrigan, Sam Davis, Nathan van
  Doorn, et~al.
\newblock Exploiting unstructured sparsity on next-generation datacenter
  hardware.
\newblock {\em None}, 2019.

\bibitem{liu2018memory}
Chen Liu, Guillaume Bellec, Bernhard Vogginger, David Kappel, Johannes
  Partzsch, Felix Neum{\"a}rker, Sebastian H{\"o}ppner, Wolfgang Maass, Steve~B
  Furber, Robert Legenstein, et~al.
\newblock Memory-efficient deep learning on a spinnaker 2 prototype.
\newblock {\em Frontiers in neuroscience}, 12:840, 2018.

\bibitem{7551397}
Song Han, Xingyu Liu, Huizi Mao, Jing Pu, Ardavan Pedram, Mark~A. Horowitz, and
  William~J. Dally.
\newblock Eie: Efficient inference engine on compressed deep neural network.
\newblock In {\em 2016 ACM/IEEE 43rd Annual International Symposium on Computer
  Architecture (ISCA)}, pages 243--254, 2016.

\bibitem{8686088}
Yu-Hsin Chen, Tien-Ju Yang, Joel Emer, and Vivienne Sze.
\newblock Eyeriss v2: A flexible accelerator for emerging deep neural networks
  on mobile devices.
\newblock {\em IEEE Journal on Emerging and Selected Topics in Circuits and
  Systems}, 9(2):292--308, 2019.

\end{thebibliography}
