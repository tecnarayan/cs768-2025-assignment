\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2021)Agarwal, Schwarzer, Castro, Courville, and
  Bellemare]{agarwal2021deep}
Agarwal, R., Schwarzer, M., Castro, P.~S., Courville, A.~C., and Bellemare, M.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 29304--29320, 2021.

\bibitem[Bai et~al.(2022)Bai, Wang, Yang, Deng, Garg, Liu, and
  Wang]{bai2022pessimistic}
Bai, C., Wang, L., Yang, Z., Deng, Z., Garg, A., Liu, P., and Wang, Z.
\newblock Pessimistic bootstrapping for uncertainty-driven offline
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2202.11566}, 2022.

\bibitem[Berkeley()]{rlkit}
Berkeley, R.
\newblock Rlkit: Reinforcement learning framework and algorithms implemented in
  pytorch.
\newblock URL \url{https://github.com/rail-berkeley/rlkit}.

\bibitem[Bishop \& Svens{\'e}n(2012)Bishop and Svens{\'e}n]{bishop2012bayesian}
Bishop, C.~M. and Svens{\'e}n, M.
\newblock Bayesian hierarchical mixtures of experts.
\newblock \emph{arXiv preprint arXiv:1212.2447}, 2012.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Boyd, S., Boyd, S.~P., and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and
  Bruna]{brandfonbrener2021offline}
Brandfonbrener, D., Whitney, W.~F., Ranganath, R., and Bruna, J.
\newblock Offline rl without off-policy evaluation.
\newblock \emph{arXiv preprint arXiv:2106.08909}, 2021.

\bibitem[Cabi et~al.(2019)Cabi, Colmenarejo, Novikov, Konyushkova, Reed, Jeong,
  Zolna, Aytar, Budden, Vecerik, et~al.]{cabi2019framework}
Cabi, S., Colmenarejo, S.~G., Novikov, A., Konyushkova, K., Reed, S., Jeong,
  R., Zolna, K., Aytar, Y., Budden, D., Vecerik, M., et~al.
\newblock A framework for data-driven robotics.
\newblock \emph{arXiv preprint arXiv:1909.12200}, 2019.

\bibitem[Callahan(2010)]{callahan2010advanced}
Callahan, J.~J.
\newblock \emph{Advanced calculus: a geometric view}, volume~1.
\newblock Springer, 2010.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{chen2019information}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1042--1051. PMLR, 2019.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel,
  Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34, 2021.

\bibitem[Ciosek et~al.(2019)Ciosek, Vuong, Loftin, and Hofmann]{oac}
Ciosek, K., Vuong, Q., Loftin, R., and Hofmann, K.
\newblock Better exploration with optimistic actor-critic, 2019.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Dabney, W., Ostrovski, G., Silver, D., and Munos, R.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In \emph{International conference on machine learning}, pp.\
  1096--1105. PMLR, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Dabney, W., Rowland, M., Bellemare, M., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018{\natexlab{b}}.

\bibitem[Duan et~al.(2020)Duan, Jia, and Wang]{duan2020minimax}
Duan, Y., Jia, Z., and Wang, M.
\newblock Minimax-optimal off-policy evaluation with linear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2701--2709. PMLR, 2020.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst2005tree}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 6:\penalty0 503--556,
  2005.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock In \emph{Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and Meger]{TD3}
Fujimoto, S., van Hoof, H., and Meger, D.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{CoRR}, abs/1802.09477, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.09477}.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2052--2062, 2019.

\bibitem[Fujimoto et~al.(2022)Fujimoto, Meger, Precup, Nachum, and
  Gu]{fujimoto2022should}
Fujimoto, S., Meger, D., Precup, D., Nachum, O., and Gu, S.~S.
\newblock Why should i trust you, bellman? the bellman error is a poor
  replacement for value error.
\newblock \emph{arXiv preprint arXiv:2201.12417}, 2022.

\bibitem[Ghasemipour et~al.(2021)Ghasemipour, Schuurmans, and
  Gu]{ghasemipour2021emaq}
Ghasemipour, S. K.~S., Schuurmans, D., and Gu, S.~S.
\newblock Emaq: Expected-max q-learning operator for simple yet effective
  offline and online rl.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3682--3691. PMLR, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{SAC}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock \emph{CoRR}, abs/1801.01290, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.01290}.

\bibitem[Islam et~al.(2017)Islam, Henderson, Gomrokchi, and
  Precup]{islam2017reproducibility}
Islam, R., Henderson, P., Gomrokchi, M., and Precup, D.
\newblock Reproducibility of benchmarked deep reinforcement learning tasks for
  continuous control.
\newblock \emph{arXiv preprint arXiv:1708.04133}, 2017.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021sequence}
Janner, M., Li, Q., and Levine, S.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Jaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,
  N., Gu, S., and Picard, R.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Jin et~al.(2016)Jin, Zhang, Balakrishnan, Wainwright, and
  Jordan]{jin2016local}
Jin, C., Zhang, Y., Balakrishnan, S., Wainwright, M.~J., and Jordan, M.~I.
\newblock Local maxima in the likelihood of gaussian mixture models: Structural
  results and algorithmic consequences.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Jin et~al.(2021)Jin, Yang, and Wang]{jin2021pessimism}
Jin, Y., Yang, Z., and Wang, Z.
\newblock Is pessimism provably efficient for offline rl?
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5084--5096. PMLR, 2021.

\bibitem[Jordan \& Jacobs(1994)Jordan and Jacobs]{jordan1994hierarchical}
Jordan, M.~I. and Jacobs, R.~A.
\newblock Hierarchical mixtures of experts and the em algorithm.
\newblock \emph{Neural computation}, 6\penalty0 (2):\penalty0 181--214, 1994.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{In Proc. 19th International Conference on Machine Learning}.
  Citeseer, 2002.

\bibitem[Kakade(2003)]{kakade2003sample}
Kakade, S.~M.
\newblock \emph{On the sample complexity of reinforcement learning}.
\newblock University of London, University College London (United Kingdom),
  2003.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Konda \& Tsitsiklis(1999)Konda and Tsitsiklis]{konda1999actor}
Konda, V. and Tsitsiklis, J.
\newblock Actor-critic algorithms.
\newblock \emph{Advances in neural information processing systems}, 12, 1999.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and Levine]{kostrikov2021iql}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}, pp.\  45--73. Springer, 2012.

\bibitem[Le et~al.(2019)Le, Voloshin, and Yue]{le2019batch}
Le, H., Voloshin, C., and Yue, Y.
\newblock Batch policy learning under constraints.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3703--3712. PMLR, 2019.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Li et~al.(2020)Li, Vuong, Liu, Liu, Ciosek, Christensen, and
  Su]{li2020multi}
Li, J., Vuong, Q., Liu, S., Liu, M., Ciosek, K., Christensen, H., and Su, H.
\newblock Multi-task batch reinforcement learning with metric learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6197--6210, 2020.

\bibitem[Li et~al.(2006)Li, Walsh, and Littman]{li2006towards}
Li, L., Walsh, T.~J., and Littman, M.~L.
\newblock Towards a unified theory of state abstraction for mdps.
\newblock \emph{ISAIM}, 4\penalty0 (5):\penalty0 9, 2006.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{ddpg}
Lillicrap, T.~P., Hunt, J.~J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
  Silver, D., and Wierstra, D.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Lyu et~al.(2022)Lyu, Ma, Li, and Lu]{lyu2022mildly}
Lyu, J., Ma, X., Li, X., and Lu, Z.
\newblock Mildly conservative q-learning for offline reinforcement learning.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=VYYf6S67pQc}.

\bibitem[Ma et~al.(2020)Ma, Xia, Zhou, Yang, and Zhao]{ma2020dsac}
Ma, X., Xia, L., Zhou, Z., Yang, J., and Zhao, Q.
\newblock Dsac: Distributional soft actor critic for risk-sensitive
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.14547}, 2020.

\bibitem[Metelli et~al.(2018)Metelli, Papini, Faccio, and
  Restelli]{metelli2018policy}
Metelli, A.~M., Papini, M., Faccio, F., and Restelli, M.
\newblock Policy optimization via importance sampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Metelli et~al.(2020)Metelli, Papini, Montali, and
  Restelli]{metelli2020importance}
Metelli, A.~M., Papini, M., Montali, N., and Restelli, M.
\newblock Importance sampling techniques for policy optimization.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 5552--5626, 2020.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[M{\"u}ller(1997)]{muller1997integral}
M{\"u}ller, A.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock \emph{Advances in Applied Probability}, 29\penalty0 (2):\penalty0
  429--443, 1997.

\bibitem[Parisotto et~al.(2015)Parisotto, Ba, and
  Salakhutdinov]{ActorMimicParisotto2015}
Parisotto, E., Ba, J., and Salakhutdinov, R.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock \emph{CoRR}, abs/1511.06342, 2015.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Peng, X.~B., Kumar, A., Zhang, G., and Levine, S.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Perkins \& Precup(2002)Perkins and Precup]{perkins2002convergent}
Perkins, T. and Precup, D.
\newblock A convergent form of approximate policy iteration.
\newblock \emph{Advances in neural information processing systems}, 15, 2002.

\bibitem[Sallab et~al.(2017)Sallab, Abdou, Perot, and Yogamani]{sallab2017deep}
Sallab, A.~E., Abdou, M., Perot, E., and Yogamani, S.
\newblock Deep reinforcement learning framework for autonomous driving.
\newblock \emph{Electronic Imaging}, 2017\penalty0 (19):\penalty0 70--76, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{TRPO}
Schulman, J., Levine, S., Moritz, P., Jordan, M.~I., and Abbeel, P.
\newblock Trust region policy optimization.
\newblock \emph{CoRR}, abs/1502.05477, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.05477}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{PPO}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{CoRR}, abs/1707.06347, 2017.
\newblock URL \url{http://arxiv.org/abs/1707.06347}.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
Silver, D., Lever, G., Heess, N., Degris, T., Wierstra, D., and Riedmiller, M.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International conference on machine learning}, pp.\
  387--395. PMLR, 2014.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tang et~al.(2020)Tang, Valko, and Munos]{tang2020taylor}
Tang, Y., Valko, M., and Munos, R.
\newblock Taylor expansion policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9397--9406. PMLR, 2020.

\bibitem[Vuong et~al.(2022)Vuong, Kumar, Levine, and Chebotar]{vuong2022dual}
Vuong, Q., Kumar, A., Levine, S., and Chebotar, Y.
\newblock Dasco: Dual-generator adversarial support constrained offline
  reinforcement learning.
\newblock In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and
  Oh, A. (eds.), \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  38937--38949. Curran Associates, Inc., 2022.

\bibitem[Wu et~al.(2022)Wu, Wu, Qiu, Wang, and Long]{wu2022supported}
Wu, J., Wu, H., Qiu, Z., Wang, J., and Long, M.
\newblock Supported policy optimization for offline reinforcement learning.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=KCXQ5HoM-fy}.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Wu, Y., Tucker, G., and Nachum, O.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Xu et~al.(1994)Xu, Jordan, and Hinton]{xu1994alternative}
Xu, L., Jordan, M., and Hinton, G.~E.
\newblock An alternative model for mixtures of experts.
\newblock \emph{Advances in neural information processing systems}, 7, 1994.

\bibitem[Yang et~al.(2022)Yang, Bai, Ma, Wang, Zhang, and Han]{yang2022rorl}
Yang, R., Bai, C., Ma, X., Wang, Z., Zhang, C., and Han, L.
\newblock {RORL}: Robust offline reinforcement learning via conservative
  smoothing.
\newblock In Oh, A.~H., Agarwal, A., Belgrave, D., and Cho, K. (eds.),
  \emph{Advances in Neural Information Processing Systems}, 2022.
\newblock URL \url{https://openreview.net/forum?id=_QzJJGH_KE}.

\bibitem[Yin \& Wang(2020)Yin and Wang]{yin2020asymptotically}
Yin, M. and Wang, Y.-X.
\newblock Asymptotically efficient off-policy evaluation for tabular
  reinforcement learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3948--3958. PMLR, 2020.

\bibitem[Yin et~al.(2022)Yin, Duan, Wang, and Wang]{yin2022near}
Yin, M., Duan, Y., Wang, M., and Wang, Y.-X.
\newblock Near-optimal offline reinforcement learning with linear
  representation: Leveraging variance information with pessimism.
\newblock \emph{International Conference on Learning Representations}, 2022.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and
  Ma, T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Yu et~al.(2021)Yu, Kumar, Rafailov, Rajeswaran, Levine, and
  Finn]{yu2021combo}
Yu, T., Kumar, A., Rafailov, R., Rajeswaran, A., Levine, S., and Finn, C.
\newblock Combo: Conservative offline model-based policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Ni, and Wang]{zhang2022off}
Zhang, R., Zhang, X., Ni, C., and Wang, M.
\newblock Off-policy fitted q-evaluation with differentiable function
  approximators: Z-estimation and inference theory.
\newblock \emph{International Conference on Machine Learning}, 2022.

\bibitem[Zou et~al.(2019)Zou, Xu, and Liang]{zou2019finite}
Zou, S., Xu, T., and Liang, Y.
\newblock Finite-sample analysis for sarsa with linear function approximation.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\end{thebibliography}
