\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{NvNvdG19}

\bibitem[ADG{\etalchar{+}}16]{andrychowicz2016learning}
Marcin Andrychowicz, Misha Denil, Sergio Gomez, Matthew~W Hoffman, David Pfau,
  Tom Schaul, Brendan Shillingford, and Nando De~Freitas.
\newblock Learning to learn by gradient descent by gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  3981--3989, 2016.

\bibitem[AI19]{quacsota}
WeChat AI.
\newblock Tr-mt (ensemble), December 2019.

\bibitem[AJF19]{aharoni201950B}
Roee Aharoni, Melvin Johnson, and Orhan Firat.
\newblock Massively multilingual neural machine translation.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, 2019.

\bibitem[BBDIW20]{blodgett2020language}
Su~Lin Blodgett, Solon Barocas, Hal Daum{\'e}~III, and Hanna Wallach.
\newblock Language (technology) is power: A critical survey of ``bias'' in nlp.
\newblock {\em arXiv preprint arXiv:2005.14050}, 2020.

\bibitem[BCFL13]{berant2013semantic}
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang.
\newblock Semantic parsing on freebase from question-answer pairs.
\newblock In {\em Proceedings of the 2013 conference on empirical methods in
  natural language processing}, pages 1533--1544, 2013.

\bibitem[BDD{\etalchar{+}}09]{bentivogli2009fifth}
Luisa Bentivogli, Ido Dagan, Hoa~Trang Dang, Danilo Giampiccolo, and Bernardo
  Magnini.
\newblock The fifth {PASCAL} recognizing textual entailment challenge.
\newblock 2009.

\bibitem[BES10]{baccianella2010sentiwordnet}
Stefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani.
\newblock Sentiwordnet 3.0: an enhanced lexical resource for sentiment analysis
  and opinion mining.
\newblock In {\em Lrec}, volume~10, pages 2200--2204, 2010.

\bibitem[BHDD{\etalchar{+}}06]{bar2006second}
Roy Bar~Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo
  Magnini, and Idan Szpektor.
\newblock The second {PASCAL} recognising textual entailment challenge.
\newblock 2006.

\bibitem[BHT{\etalchar{+}}20]{bisk2020experience}
Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce
  Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich,
  et~al.
\newblock Experience grounds language.
\newblock {\em arXiv preprint arXiv:2004.10151}, 2020.

\bibitem[BLC13]{bengio2013cond}
Yoshua Bengio, Nicholas L{\'{e}}onard, and Aaron~C. Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em Arxiv}, 2013.

\bibitem[BZB{\etalchar{+}}19]{bisk2019piqa}
Yonatan Bisk, Rowan Zellers, Ronan~Le Bras, Jianfeng Gao, and Yejin Choi.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock {\em arXiv preprint arXiv:1911.11641}, 2019.

\bibitem[Car97]{caruana1997multitask}
Rich Caruana.
\newblock Multitask learning.
\newblock {\em Machine learning}, 28(1), 1997.

\bibitem[CB78]{carey1978acquiring}
Susan Carey and Elsa Bartlett.
\newblock Acquiring a single new word.
\newblock {\em Proceedings of the Stanford Child Language Conference}, 1978.

\bibitem[CCE{\etalchar{+}}18]{Clark2018ThinkYH}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock {\em ArXiv}, abs/1803.05457, 2018.

\bibitem[CGRS19]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers, 2019.

\bibitem[CHI{\etalchar{+}}18]{choi2018quac}
Eunsol Choi, He~He, Mohit Iyyer, Mark Yatskar, Wen{-}tau Yih, Yejin Choi, Percy
  Liang, and Luke Zettlemoyer.
\newblock Quac : Question answering in context.
\newblock {\em Arxiv}, 2018.

\bibitem[CLC{\etalchar{+}}19]{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock {BoolQ}: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock {\em arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[CLY{\etalchar{+}}19]{chen2019uniter}
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed~El Kholy, Faisal Ahmed, Zhe Gan,
  Yu~Cheng, and Jingjing Liu.
\newblock Uniter: Learning universal image-text representations.
\newblock {\em arXiv preprint arXiv:1909.11740}, 2019.

\bibitem[Cra17]{youtube}
Kate Crawford.
\newblock The trouble with bias.
\newblock {\em NIPS 2017 Keynote}, 2017.

\bibitem[DCLT18]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[DGM06]{dagan2006pascal}
Ido Dagan, Oren Glickman, and Bernardo Magnini.
\newblock The {PASCAL} recognising textual entailment challenge.
\newblock In {\em Machine learning challenges. evaluating predictive
  uncertainty, visual object classification, and recognising textual
  entailment}, pages 177--190. Springer, 2006.

\bibitem[DGV{\etalchar{+}}18]{dehghani2018ut}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock {\em Arxiv}, 2018.

\bibitem[DHKH14]{durrani2014edinburgh}
Nadir Durrani, Barry Haddow, Philipp Koehn, and Kenneth Heafield.
\newblock Edinburghâ€™s phrase-based machine translation systems for wmt-14.
\newblock In {\em Proceedings of the Ninth Workshop on Statistical Machine
  Translation}, pages 97--104, 2014.

\bibitem[DL15]{dai2015semi}
Andrew~M. Dai and Quoc~V. Le.
\newblock Semi-supervised sequence learning.
\newblock In {\em Advances in neural information processing systems}, 2015.

\bibitem[DMST19]{demarneffe:cb}
Marie-Catherine De~Marneffe, Mandy Simons, and Judith Tonhauser.
\newblock {The CommitmentBank}: Investigating projection in naturally occurring
  discourse.
\newblock 2019.
\newblock To appear in proceedings of Sinn und Bedeutung 23. Data can be found
  at https://github.com/mcdm/CommitmentBank/.

\bibitem[DSC{\etalchar{+}}16]{Duan2016RL2FR}
Yan Duan, John Schulman, Xi~Chen, Peter~L. Bartlett, Ilya Sutskever, and Pieter
  Abbeel.
\newblock Rl$^2$: Fast reinforcement learning via slow reinforcement learning.
\newblock {\em ArXiv}, abs/1611.02779, 2016.

\bibitem[DWD{\etalchar{+}}19]{dua2019drop}
Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and
  Matt Gardner.
\newblock Drop: A reading comprehension benchmark requiring discrete reasoning
  over paragraphs.
\newblock {\em arXiv preprint arXiv:1903.00161}, 2019.

\bibitem[DYY{\etalchar{+}}19]{dai2019transformerXL}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime~G. Carbonell, Quoc~V. Le, and
  Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock {\em Arxiv}, 2019.

\bibitem[EOAG18]{edunov2018understanding}
Sergey Edunov, Myle Ott, Michael Auli, and David Grangier.
\newblock Understanding back-translation at scale.
\newblock {\em arXiv preprint arXiv:1808.09381}, 2018.

\bibitem[FAL17]{Finn2017ModelAgnosticMF}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock {\em ArXiv}, abs/1703.03400, 2017.

\bibitem[Fyo00]{fyodorov2000natural}
Yaroslav Fyodorov.
\newblock A natural logic inference system, 2000.

\bibitem[GG19]{gonen2019lipstick}
Hila Gonen and Yoav Goldberg.
\newblock Lipstick on a pig: Debiasing methods cover up systematic gender
  biases in word embeddings but do not remove them.
\newblock {\em arXiv preprint arXiv:1903.03862}, 2019.

\bibitem[GLT{\etalchar{+}}20]{guu2020realm}
Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang.
\newblock Realm: Retrieval-augmented language model pre-training.
\newblock {\em arXiv preprint arXiv:2002.08909}, 2020.

\bibitem[GMDD07]{giampiccolo2007third}
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan.
\newblock The third {PASCAL} recognizing textual entailment challenge.
\newblock In {\em Proceedings of the ACL-PASCAL workshop on textual entailment
  and paraphrasing}, pages 1--9. Association for Computational Linguistics,
  2007.

\bibitem[Gra16]{graves2016act}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock {\em Arxiv}, 2016.

\bibitem[GSL{\etalchar{+}}18]{gururangan2018annotation}
Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel~R
  Bowman, and Noah~A Smith.
\newblock Annotation artifacts in natural language inference data.
\newblock {\em arXiv preprint arXiv:1803.02324}, 2018.

\bibitem[GSR19]{gehrmann2019gltr}
Sebastian Gehrmann, Hendrik Strobelt, and Alexander~M. Rush.
\newblock Gltr: Statistical detection and visualization of generated text.
\newblock {\em arXiv preprint arXiv: 1906.04043}, 2019.

\bibitem[GWC{\etalchar{+}}18]{gu2018meta}
Jiatao Gu, Yong Wang, Yun Chen, Kyunghyun Cho, and Victor~OK Li.
\newblock Meta-learning for low-resource neural machine translation.
\newblock {\em arXiv preprint arXiv:1808.08437}, 2018.

\bibitem[HB20]{hernandez2020}
Daniel Hernandez and Tom Brown.
\newblock Ai and efficiency, May 2020.

\bibitem[HBFC19]{DBLP:journals/corr/abs-1904-09751}
Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi.
\newblock The curious case of neural text degeneration.
\newblock {\em CoRR}, abs/1904.09751, 2019.

\bibitem[HLW{\etalchar{+}}20]{hendrycks2020pretrained}
Dan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam Dziedzic, Rishabh Krishnan, and
  Dawn Song.
\newblock Pretrained transformers improve out of distribution robustness.
\newblock {\em arXiv preprint arXiv:2004.06100}, 2020.

\bibitem[HNA{\etalchar{+}}17]{hestness2017deep}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md. Mostofa~Ali Patwary, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock {\em arXiv preprint arXiv:1712.00409}, 2017.

\bibitem[HR18]{howard2018universal}
Jeremy Howard and Sebastian Ruder.
\newblock Universal language model fine-tuning for text classification.
\newblock {\em arXiv preprint arXiv:1801.06146}, 2018.

\bibitem[HVD15]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock {\em arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[HYC01]{hochreiter2001learning}
Sepp Hochreiter, A~Steven Younger, and Peter~R Conwell.
\newblock {Learning to Learn Using Gradient Descent}.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 87--94. Springer, 2001.

\bibitem[HZJ{\etalchar{+}}19]{huang2019reducing}
Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack
  Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli.
\newblock Reducing sentiment bias in language models via counterfactual
  evaluation.
\newblock {\em arXiv preprint arXiv:1911.03064}, 2019.

\bibitem[IBGC{\etalchar{+}}14]{Iyyer2014quiz}
Mohit Iyyer, Jordan Boyd-Graber, Leonardo Claudino, Richard Socher, and Hal
  {Daum\'e III}.
\newblock A neural network for factoid question answering over paragraphs.
\newblock In {\em Empirical Methods in Natural Language Processing}, 2014.

\bibitem[IDCBE19]{ippolito2019automatic}
Daphne Ippolito, Daniel Duckworth, Chris Callison-Burch, and Douglas Eck.
\newblock Automatic detection of generated text is easiest when humans are
  fooled.
\newblock {\em arXiv preprint arXiv:1911.00650}, 2019.

\bibitem[JCWZ17]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S. Weld, and Luke Zettlemoyer.
\newblock {TriviaQA}: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1705.03551}, 2017.

\bibitem[JN20]{dropsota}
Zheng Junyuan and Gamma~Lab NYC.
\newblock Numeric transformer - albert, March 2020.

\bibitem[JVS{\etalchar{+}}16]{jozefowicz2016exploring}
Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu.
\newblock Exploring the limits of language modeling.
\newblock {\em arXiv preprint arXiv:1602.02410}, 2016.

\bibitem[JYS{\etalchar{+}}19]{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock {TinyBERT}: Distilling {BERT} for natural language understanding.
\newblock {\em arXiv preprint arXiv:1909.10351}, 2019.

\bibitem[JZC{\etalchar{+}}19]{ju2019technical}
Ying Ju, Fubang Zhao, Shijie Chen, Bowen Zheng, Xuefeng Yang, and Yunfeng Liu.
\newblock Technical report on conversational question answering.
\newblock {\em arXiv preprint arXiv:1909.10772}, 2019.

\bibitem[KCR{\etalchar{+}}18]{khashabi2018looking}
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan
  Roth.
\newblock Looking beyond the surface: A challenge set for reading comprehension
  over multiple sentences.
\newblock In {\em Proceedings of North American Chapter of the Association for
  Computational Linguistics (NAACL)}, 2018.

\bibitem[KKS{\etalchar{+}}20]{khashabi2020unifiedqa}
Daniel Khashabi, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark,
  and Hannaneh Hajishirzi.
\newblock Unifiedqa: Crossing format boundaries with a single qa system.
\newblock {\em arXiv preprint arXiv:2005.00700}, 2020.

\bibitem[KMB20]{kreps-all-the-news}
Sarah~E. Kreps, Miles McCain, and Miles Brundage.
\newblock All the news thatâ€™s fit to fabricate: Ai-generated text as a tool
  of media misinformation, 2020.

\bibitem[KMH{\etalchar{+}}20]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B. Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models, 2020.

\bibitem[KPR{\etalchar{+}}19]{Kwiatkowski2019nq}
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur
  Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Matthew Kelcey,
  Jacob Devlin, Kenton Lee, Kristina~N. Toutanova, Llion Jones, Ming-Wei Chang,
  Andrew Dai, Jakob Uszkoreit, Quoc Le, and Slav Petrov.
\newblock Natural questions: a benchmark for question answering research.
\newblock {\em Transactions of the Association of Computational Linguistics},
  2019.

\bibitem[KR16]{yoon2016sequencedistil}
Yoon Kim and Alexander~M. Rush.
\newblock Sequence-level knowledge distillation.
\newblock {\em Arxiv}, 2016.

\bibitem[LB02]{loper2002nltk}
Edward Loper and Steven Bird.
\newblock Nltk: The natural language toolkit, 2002.

\bibitem[LC19]{lample2019cross}
Guillaume Lample and Alexis Conneau.
\newblock Cross-lingual language model pretraining.
\newblock {\em arXiv preprint arXiv:1901.07291}, 2019.

\bibitem[LCG{\etalchar{+}}19]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT}: A lite {BERT} for self-supervised learning of language
  representations.
\newblock {\em arXiv preprint arXiv:1909.11942}, 2019.

\bibitem[LCH{\etalchar{+}}20]{liu2020alum}
Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu~Wang, Hoifung Poon, and
  Jianfeng Gao.
\newblock Adversarial training for large neural language models.
\newblock {\em arXiv preprint arXiv:2004.08994}, 2020.

\bibitem[LDL19]{li2019story}
Zhongyang Li, Xiao Ding, and Ting Liu.
\newblock Story ending prediction by transferable bert.
\newblock {\em arXiv preprint arXiv:1905.07504}, 2019.

\bibitem[LDM12]{levesque2012winograd}
Hector Levesque, Ernest Davis, and Leora Morgenstern.
\newblock The {Winograd} schema challenge.
\newblock In {\em Thirteenth International Conference on the Principles of
  Knowledge Representation and Reasoning}, 2012.

\bibitem[LGG{\etalchar{+}}20]{liu2020multilingual}
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan
  Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.
\newblock Multilingual denoising pre-training for neural machine translation.
\newblock {\em arXiv preprint arXiv:2001.08210}, 2020.

\bibitem[LGH{\etalchar{+}}15]{liu2015representation}
Xiaodong Liu, Jianfeng Gao, Xiaodong He, Li~Deng, Kevin Duh, and Ye-Yi Wang.
\newblock Representation learning using multi-task deep neural networks for
  semantic classification and information retrieval.
\newblock In {\em Proceedings of the 2015 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, 2015.

\bibitem[LH17]{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[LHCG19a]{liu2019improving}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Improving multi-task deep neural networks via knowledge distillation
  for natural language understanding.
\newblock {\em arXiv preprint arXiv:1904.09482}, 2019.

\bibitem[LHCG19b]{liu2019multi}
Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao.
\newblock Multi-task deep neural networks for natural language understanding.
\newblock {\em arXiv preprint arXiv:1901.11504}, 2019.

\bibitem[Lin20]{linzen2020can}
Tal Linzen.
\newblock How can we accelerate progress towards human-like linguistic
  generalization?
\newblock {\em arXiv preprint arXiv:2005.00955}, 2020.

\bibitem[LLG{\etalchar{+}}19]{lewis2019bart}
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
  Omer Levy, Ves Stoyanov, and Luke Zettlemoyer.
\newblock Bart: Denoising sequence-to-sequence pre-training for natural
  language generation, translation, and comprehension.
\newblock {\em arXiv preprint arXiv:1910.13461}, 2019.

\bibitem[LM17]{li2017learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize neural nets.
\newblock {\em arXiv preprint arXiv:1703.00441}, 2017.

\bibitem[LOG{\etalchar{+}}19]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock {RoBERTa}: A robustly optimized {BERT} pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[LPP{\etalchar{+}}20]{lewis2020retrieval}
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir
  Karpukhin, Naman Goyal, Heinrich K{\"u}ttler, Mike Lewis, Wen-tau Yih, Tim
  Rockt{\"a}schel, Sebastian Riedel, and Kiela Douwe.
\newblock Retrieval-augmented generation for knowledge-intensive nlp tasks.
\newblock {\em arXiv preprint arXiv:2005.11401}, 2020.

\bibitem[LSP{\etalchar{+}}18]{liu2018generating}
Peter~J. Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz
  Kaiser, and Noam Shazeer.
\newblock Generating {Wikipedia} by summarizing long sequences.
\newblock {\em arXiv preprint arXiv:1801.10198}, 2018.

\bibitem[LWS{\etalchar{+}}20]{li2020train}
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and
  Joseph~E. Gonzalez.
\newblock Train large, then compress: Rethinking model size for efficient
  training and inference of transformers, 2020.

\bibitem[LXL{\etalchar{+}}17]{lai2017race}
Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy.
\newblock Race: Large-scale reading comprehension dataset from examinations.
\newblock {\em arXiv preprint arXiv:1704.04683}, 2017.

\bibitem[LYN{\etalchar{+}}20]{lin2020tttttackling}
Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Nogueira, Ming-Feng Tsai, Chuan-Ju
  Wang, and Jimmy Lin.
\newblock Tttttackling winogrande schemas.
\newblock {\em arXiv preprint arXiv:2003.08380}, 2020.

\bibitem[Mac92]{mackay1992active}
David. MacKay.
\newblock {Information-based objective functions for active data selection}.
\newblock {\em Neural Computation}, 1992.

\bibitem[MBXS17]{mccann2017learned}
Bryan McCann, James Bradbury, Caiming Xiong, and Richard Socher.
\newblock Learned in translation: Contextualized word vectors.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6294--6305, 2017.

\bibitem[MCCD13]{mikolov2013efficient}
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
\newblock Efficient estimation of word representations in vector space.
\newblock {\em arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[MCH{\etalchar{+}}16]{mostafazadeh2016corpus}
Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,
  Lucy Vanderwende, Pushmeet Kohli, and James Allen.
\newblock A corpus and evaluation framework for deeper understanding of
  commonsense stories.
\newblock {\em arXiv preprint arXiv:1604.01696}, 2016.

\bibitem[MCKS18]{Mihaylov2018CanAS}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock {\em ArXiv}, abs/1809.02789, 2018.

\bibitem[MKAT18]{mcc2018batchsize}
Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI~Dota Team.
\newblock An empirical model of large-batch training, 2018.

\bibitem[MKM{\etalchar{+}}94]{marcus1994penn}
Mitchell Marcus, Grace Kim, Mary~Ann Marcinkiewicz, Robert MacIntyre, Ann Bies,
  Mark Ferguson, Karen Katz, and Britta Schasberger.
\newblock The penn treebank: annotating predicate argument structure.
\newblock In {\em Proceedings of the workshop on Human Language Technology},
  pages 114--119. Association for Computational Linguistics, 1994.

\bibitem[MKXS18]{mccann2018natural}
Bryan McCann, Nitish~Shirish Keskar, Caiming Xiong, and Richard Socher.
\newblock The natural language decathlon: Multitask learning as question
  answering.
\newblock {\em arXiv preprint arXiv:1806.08730}, 2018.

\bibitem[MPL19]{mccoy2019right}
R~Thomas McCoy, Ellie Pavlick, and Tal Linzen.
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock {\em arXiv preprint arXiv:1902.01007}, 2019.

\bibitem[MWZ{\etalchar{+}}18]{1810.03993}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting, 2018.

\bibitem[NBR20]{nadeem2020stereoset}
Moin Nadeem, Anna Bethke, and Siva Reddy.
\newblock Stereoset: Measuring stereotypical bias in pretrained language
  models.
\newblock {\em arXiv preprint arXiv:2004.09456}, 2020.

\bibitem[NK19]{niven2019probing}
Timothy Niven and Hung-Yu Kao.
\newblock Probing neural network comprehension of natural language arguments.
\newblock {\em arXiv preprint arXiv:1907.07355}, 2019.

\bibitem[Nor09]{norvig2009ngram}
Peter Norvig.
\newblock Natural language corpus data, 2009.

\bibitem[NvNvdG19]{nissim2019fair}
Malvina Nissim, Rik van Noord, and Rob van~der Goot.
\newblock Fair is better than sensational: Man is to doctor as woman is to
  doctor.
\newblock {\em arXiv preprint arXiv:1905.09866}, 2019.

\bibitem[NWD{\etalchar{+}}19]{nie2019adversarial}
Yixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe
  Kiela.
\newblock Adversarial nli: A new benchmark for natural language understanding.
\newblock {\em arXiv preprint arXiv:1910.14599}, 2019.

\bibitem[oR16]{wmt16deensota}
University of~Regensburg.
\newblock Fascha, 2016.

\bibitem[PCC18]{pilehvar2018wic}
Mohammad~Taher Pilehvar and Jose Camacho-Collados.
\newblock {WIC}: 10,000 example pairs for evaluating context-sensitive
  representations.
\newblock {\em arXiv preprint arXiv:1808.09121}, 2018.

\bibitem[PFB18]{phang2018sentence}
Jason Phang, Thibault F{\'e}vry, and Samuel~R. Bowman.
\newblock Sentence encoders on {STILTs}: Supplementary training on intermediate
  labeled-data tasks.
\newblock {\em arXiv preprint arXiv:1811.01088}, 2018.

\bibitem[PHR{\etalchar{+}}18]{poliak2018dnc}
Adam Poliak, Aparajita Haldar, Rachel Rudinger, J.~Edward Hu, Ellie Pavlick,
  Aaron~Steven White, and Benjamin {Van Durme}.
\newblock Collecting diverse natural language inference problems for sentence
  representation evaluation.
\newblock In {\em Proceedings of EMNLP}, 2018.

\bibitem[PKL{\etalchar{+}}16]{paperno2016lambada}
Denis Paperno, Germ{\'a}n Kruszewski, Angeliki Lazaridou, Quan~Ngoc Pham,
  Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel
  Fern{\'a}ndez.
\newblock The lambada dataset: Word prediction requiring a broad discourse
  context.
\newblock {\em arXiv preprint arXiv:1606.06031}, 2016.

\bibitem[PNZtY18]{peters2018dissecting}
Matthew~E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen tau Yih.
\newblock Dissecting contextual word embeddings: Architecture and
  representation, 2018.

\bibitem[Pos18]{post2018call}
Matt Post.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock {\em arXiv preprint arXiv:1804.08771}, 2018.

\bibitem[PSM14]{pennington2014glove}
Jeffrey Pennington, Richard Socher, and Christopher Manning.
\newblock {GloVe}: Global vectors for word representation.
\newblock In {\em Proceedings of the 2014 conference on empirical methods in
  natural language processing (EMNLP)}, 2014.

\bibitem[QIA20]{squadv2sota}
QIANXIN.
\newblock Sa-net on albert (ensemble), April 2020.

\bibitem[QMZH19]{qian2019reducing}
Yusu Qian, Urwa Muaz, Ben Zhang, and Jae~Won Hyun.
\newblock Reducing gender bias in word-level language models with a
  gender-equalizing loss function.
\newblock {\em arXiv preprint arXiv:1905.12801}, 2019.

\bibitem[RBG11]{roemmele2011choice}
Melissa Roemmele, Cosmin~Adrian Bejan, and Andrew~S Gordon.
\newblock Choice of plausible alternatives: An evaluation of commonsense causal
  reasoning.
\newblock In {\em 2011 AAAI Spring Symposium Series}, 2011.

\bibitem[RCM19]{reddy2019coqa}
Siva Reddy, Danqi Chen, and Christopher~D Manning.
\newblock Coqa: A conversational question answering challenge.
\newblock {\em Transactions of the Association for Computational Linguistics},
  7:249--266, 2019.

\bibitem[RCP{\etalchar{+}}17]{reed2017few}
Scott Reed, Yutian Chen, Thomas Paine, A{\"a}ron van~den Oord, SM~Eslami,
  Danilo Rezende, Oriol Vinyals, and Nando de~Freitas.
\newblock Few-shot autoregressive density estimation: Towards learning to learn
  distributions.
\newblock {\em arXiv preprint arXiv:1710.10304}, 2017.

\bibitem[RJL18]{rajpurkar2018know}
Pranav Rajpurkar, Robin Jia, and Percy Liang.
\newblock Know what you don't know: Unanswerable questions for squad.
\newblock {\em arXiv preprint arXiv:1806.03822}, 2018.

\bibitem[RL16]{ravi2016optimization}
Sachin Ravi and Hugo Larochelle.
\newblock Optimization as a model for few-shot learning.
\newblock {\em ICLR 2017 (oral)}, 2016.

\bibitem[RLL{\etalchar{+}}19]{ran2019numnet}
Qiu Ran, Yankai Lin, Peng Li, Jie Zhou, and Zhiyuan Liu.
\newblock {NumNet}: Machine reading comprehension with numerical reasoning.
\newblock In {\em Proceedings of EMNLP}, 2019.

\bibitem[RNLVD18]{rudinger2018gender}
Rachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van~Durme.
\newblock Gender bias in coreference resolution.
\newblock {\em arXiv preprint arXiv:1804.09301}, 2018.

\bibitem[RNSS18]{radford2018gpt1}
Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
\newblock Improving language understanding by generative pre-training, 2018.

\bibitem[Ros12]{ross2012risk}
R.S. Ross.
\newblock Guide for conducting risk assessments.
\newblock {\em NIST Special Publication}, 2012.

\bibitem[RRBS19]{rosenfeld2019constructive}
Jonathan~S. Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit.
\newblock A constructive prediction of the generalization error across scales,
  2019.

\bibitem[RRS20]{roberts2020much}
Adam Roberts, Colin Raffel, and Noam Shazeer.
\newblock How much knowledge can you pack into the parameters of a language
  model?
\newblock {\em arXiv preprint arXiv:2002.08910}, 2020.

\bibitem[RSR{\etalchar{+}}19]{raffel2019t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer, 2019.

\bibitem[RWC{\etalchar{+}}19]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners, 2019.

\bibitem[SBBC19]{sakaguchi2019winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale, 2019.

\bibitem[SBC{\etalchar{+}}19]{solaiman2019release}
Irene Solaiman, Miles Brundage, Jack Clark, Amanda Askell, Ariel Herbert-Voss,
  Jeff Wu, Alec Radford, Gretchen Krueger, Jong~Wook Kim, Sarah Kreps, Miles
  McCain, Alex Newhouse, Jason Blazakis, Kris McGuffie, and Jasmine Wang.
\newblock Release strategies and the social impacts of language models, 2019.

\bibitem[SCNP19]{sheng2019woman}
Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng.
\newblock The woman worked as a babysitter: On biases in language generation.
\newblock {\em arXiv preprint arXiv:1909.01326}, 2019.

\bibitem[SDCW19]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock {DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper
  and lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[SDSE19]{schwartz2019}
Roy Schwartz, Jesse Dodge, Noah~A. Smith, and Oren Etzioni.
\newblock Green {AI}.
\newblock {\em CoRR}, abs/1907.10597, 2019.

\bibitem[SHB15]{sennrich2015improving}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Improving neural machine translation models with monolingual data.
\newblock {\em arXiv preprint arXiv:1511.06709}, 2015.

\bibitem[SMM{\etalchar{+}}17]{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem[SPP{\etalchar{+}}19]{shoeybi2019megatronlm}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism, 2019.

\bibitem[SS20]{schick2020exploiting}
Timo Schick and Hinrich Sch{\"u}tze.
\newblock Exploiting cloze questions for few-shot text classification and
  natural language inference.
\newblock {\em arXiv preprint arXiv:2001.07676}, 2020.

\bibitem[STQ{\etalchar{+}}19]{song2019mass}
Kaitao Song, Xu~Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu.
\newblock {MASS}: Masked sequence to sequence pre-training for language
  generation.
\newblock {\em arXiv preprint arXiv:1905.02450}, 2019.

\bibitem[TFR{\etalchar{+}}17]{tobin2017domain}
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
  Pieter Abbeel.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In {\em 2017 IEEE/RSJ international conference on intelligent robots
  and systems (IROS)}, pages 23--30. IEEE, 2017.

\bibitem[TL05]{DBLP:journals/corr/abs-cs-0508103}
Peter~D. Turney and Michael~L. Littman.
\newblock Corpus-based learning of analogies and semantic relations.
\newblock {\em CoRR}, abs/cs/0508103, 2005.

\bibitem[TL18]{trinh2018simple}
Trieu~H. Trinh and Quoc~V. Le.
\newblock A simple method for commonsense reasoning.
\newblock {\em arXiv preprint arXiv:1806.02847}, 2018.

\bibitem[TLBS03]{DBLP:journals/corr/cs-CL-0309035}
Peter~D. Turney, Michael~L. Littman, Jeffrey Bigham, and Victor Shnayder.
\newblock Combining independent modules to solve multiple-choice synonym and
  analogy problems.
\newblock {\em CoRR}, cs.CL/0309035, 2003.

\bibitem[Tur20]{turing_17m}
Project Turing.
\newblock Microsoft research blog, Feb 2020.

\bibitem[VBL{\etalchar{+}}16]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock {Matching Networks for One Shot Learning}.
\newblock In {\em Advances in neural information processing systems}, pages
  3630--3638, 2016.

\bibitem[VSP{\etalchar{+}}17]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, 2017.

\bibitem[WPN{\etalchar{+}}19]{wang2019superglue}
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael,
  Felix Hill, Omer Levy, and Samuel Bowman.
\newblock Superglue: A stickier benchmark for general-purpose language
  understanding systems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3261--3275, 2019.

\bibitem[WXH{\etalchar{+}}18]{wang2018multi}
Yiren Wang, Yingce Xia, Tianyu He, Fei Tian, Tao Qin, ChengXiang Zhai, and
  Tie-Yan Liu.
\newblock Multi-agent dual learning.
\newblock {\em ICLR 2019}, 2018.

\bibitem[XDH{\etalchar{+}}19]{xie2019unsupervised}
Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong, and Quoc~V. Le.
\newblock Unsupervised data augmentation for consistency training, 2019.

\bibitem[YdC{\etalchar{+}}19]{yogatama2019learning}
Dani Yogatama, Cyprien de~Masson d'Autume, Jerome Connor, Tomas Kocisky, Mike
  Chrzanowski, Lingpeng Kong, Angeliki Lazaridou, Wang Ling, Lei Yu, Chris
  Dyer, et~al.
\newblock Learning and evaluating general linguistic intelligence.
\newblock {\em arXiv preprint arXiv:1901.11373}, 2019.

\bibitem[YDY{\etalchar{+}}19]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov,
  and Quoc~V. Le.
\newblock {XLNet}: Generalized autoregressive pretraining for language
  understanding.
\newblock {\em arXiv preprint arXiv:1906.08237}, 2019.

\bibitem[ZHB{\etalchar{+}}19]{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock {\em arXiv preprint arXiv:1905.07830}, 2019.

\bibitem[ZHR{\etalchar{+}}19]{zellers2019defending}
Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi,
  Franziska Roesner, and Yejin Choi.
\newblock Defending against neural fake news.
\newblock {\em arXiv preprint arXiv:1905.12616}, 2019.

\bibitem[ZLL{\etalchar{+}}18]{zhang2018record}
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin
  Van~Durme.
\newblock {ReCoRD}: Bridging the gap between human and machine commonsense
  reading comprehension.
\newblock {\em arXiv preprint arXiv:1810.12885}, 2018.

\bibitem[ZSW{\etalchar{+}}19a]{ziegler2019finetuning}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford,
  Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences, 2019.

\bibitem[ZSW{\etalchar{+}}19b]{Ziegler2019FineTuningLM}
Daniel~M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom~B. Brown, Alec Radford,
  Dario Amodei, Paul Christiano, and Geoffrey Irving.
\newblock Fine-tuning language models from human preferences.
\newblock {\em ArXiv}, abs/1909.08593, 2019.

\end{thebibliography}
