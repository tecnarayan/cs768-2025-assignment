\begin{thebibliography}{51}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajay et~al.(2020)Ajay, Kumar, Agrawal, Levine, and
  Nachum]{ajay2020opal}
Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum.
\newblock Opal: Offline primitive discovery for accelerating offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Pertsch et~al.(2020)Pertsch, Lee, and Lim]{pertsch2020spirl}
Karl Pertsch, Youngwoon Lee, and Joseph~J. Lim.
\newblock Accelerating reinforcement learning with learned skill priors.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2020.

\bibitem[Singh et~al.(2020)Singh, Liu, Zhou, Yu, Rhinehart, and
  Levine]{singh2020parrot}
Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey
  Levine.
\newblock Parrot: Data-driven behavioral priors for reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and
  Fu]{DBLP:journals/corr/abs-2005-01643}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{CoRR}, abs/2005.01643, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.01643}.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Tucker, and
  Levine]{Kumar2019StabilizingOQ}
Aviral Kumar, Justin Fu, G.~Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Wu et~al.(2019)Wu, Tucker, and
  Nachum]{DBLP:journals/corr/abs-1911-11361}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{CoRR}, abs/1911.11361, 2019.
\newblock URL \url{http://arxiv.org/abs/1911.11361}.

\bibitem[Liu et~al.(2019)Liu, Swaminathan, Agarwal, and
  Brunskill]{DBLP:journals/corr/abs-1904-08473}
Yao Liu, Adith Swaminathan, Alekh Agarwal, and Emma Brunskill.
\newblock Off-policy policy gradient with state distribution correction.
\newblock \emph{CoRR}, abs/1904.08473, 2019.
\newblock URL \url{http://arxiv.org/abs/1904.08473}.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and
  Precup]{Fujimoto2019OffPolicyDR}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{ICML}, 2019.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[Siegel et~al.(2019)Siegel, Springenberg, Berkenkamp, Abdolmaleki,
  Neunert, Lampe, Hafner, Heess, and Riedmiller]{siegel2019keep}
Noah Siegel, Jost~Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki,
  Michael Neunert, Thomas Lampe, Roland Hafner, Nicolas Heess, and Martin
  Riedmiller.
\newblock Keep doing what worked: Behavior modelling priors for offline
  reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Wang et~al.(2020)Wang, Novikov, {\.Z}o{\l}na, Springenberg, Reed,
  Shahriari, Siegel, Merel, Gulcehre, Heess, et~al.]{wang2020critic}
Ziyu Wang, Alexander Novikov, Konrad {\.Z}o{\l}na, Jost~Tobias Springenberg,
  Scott Reed, Bobak Shahriari, Noah Siegel, Josh Merel, Caglar Gulcehre,
  Nicolas Heess, et~al.
\newblock Critic regularized regression.
\newblock In \emph{Proceedings of the 34th International Conference on Neural
  Information Processing Systems}, pages 7768--7778, 2020.

\bibitem[Agarwal et~al.(2019)Agarwal, Schuurmans, and
  Norouzi]{DBLP:journals/corr/abs-1907-04543}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock Striving for simplicity in off-policy deep reinforcement learning.
\newblock \emph{CoRR}, abs/1907.04543, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.04543}.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{DBLP:journals/corr/abs-1907-00456}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson,
  {\`{A}}gata Lapedriza, Noah Jones, Shixiang Gu, and Rosalind~W. Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{CoRR}, abs/1907.00456, 2019.
\newblock URL \url{http://arxiv.org/abs/1907.00456}.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21810--21823, 2020.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Argenson and Dulac-Arnold(2020)]{argenson2020model}
Arthur Argenson and Gabriel Dulac-Arnold.
\newblock Model-based offline planning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and
  Gu]{matsushima2020deployment}
Tatsuya Matsushima, Hiroki Furuta, Yutaka Matsuo, Ofir Nachum, and Shixiang Gu.
\newblock Deployment-efficient reinforcement learning via model-based offline
  optimization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Cang et~al.(2021)Cang, Rajeswaran, Abbeel, and
  Laskin]{cang2021behavioral}
Catherine Cang, Aravind Rajeswaran, Pieter Abbeel, and Michael Laskin.
\newblock Behavioral priors and dynamics models: Improving performance and
  domain transfer in offline rl.
\newblock In \emph{Deep RL Workshop NeurIPS 2021}, 2021.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and
  Levine]{DBLP:journals/corr/abs-2006-09359}
Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{CoRR}, abs/2006.09359, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.09359}.

\bibitem[Rao et~al.(2021)Rao, Sadeghi, Hasenclever, Wulfmeier, Zambelli,
  Vezzani, Tirumala, Aytar, Merel, Heess, et~al.]{rao2021learning}
Dushyant Rao, Fereshteh Sadeghi, Leonard Hasenclever, Markus Wulfmeier, Martina
  Zambelli, Giulia Vezzani, Dhruva Tirumala, Yusuf Aytar, Josh Merel, Nicolas
  Heess, et~al.
\newblock Learning transferable motor skills with hierarchical latent mixture
  policies.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Fern\'{a}ndez and Veloso(2006)]{10.1145/1160633.1160762}
Fernando Fern\'{a}ndez and Manuela Veloso.
\newblock Probabilistic policy reuse in a reinforcement learning agent.
\newblock In \emph{Proceedings of the Fifth International Joint Conference on
  Autonomous Agents and Multiagent Systems}, AAMAS '06, page 720â€“727, New
  York, NY, USA, 2006. Association for Computing Machinery.
\newblock ISBN 1595933034.
\newblock \doi{10.1145/1160633.1160762}.
\newblock URL \url{https://doi.org/10.1145/1160633.1160762}.

\bibitem[Browning et~al.(2004)Browning, Bruce, Bowling, and Veloso]{article}
Brett Browning, James Bruce, Michael Bowling, and Manuela Veloso.
\newblock Stp: Skills, tactics, and plays for multi-robot control in
  adversarial environments.
\newblock \emph{Proceedings of The Institution of Mechanical Engineers Part
  I-journal of Systems and Control Engineering - PROC INST MECH ENG I-J SYST
  C}, 219, 12 2004.
\newblock \doi{10.1243/095965105X9470}.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Pong, Zhou, Dalal,
  Abbeel, and Levine]{DBLP:journals/corr/abs-1803-06773}
Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and
  Sergey Levine.
\newblock Composable deep reinforcement learning for robotic manipulation.
\newblock \emph{CoRR}, abs/1803.06773, 2018{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1803.06773}.

\bibitem[Hunt et~al.(2019)Hunt, Barreto, Lillicrap, and
  Heess]{Hunt2019ComposingEP}
Jonathan~J. Hunt, Andr{\'e} Barreto, Timothy~P. Lillicrap, and Nicolas
  Manfred~Otto Heess.
\newblock Composing entropic policies using divergence correction.
\newblock In \emph{ICML}, 2019.

\bibitem[Van~Niekerk et~al.(2019)Van~Niekerk, James, Earle, and
  Rosman]{pmlr-v97-van-niekerk19a}
Benjamin Van~Niekerk, Steven James, Adam Earle, and Benjamin Rosman.
\newblock Composing value functions in reinforcement learning.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  6401--6409. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/van-niekerk19a.html}.

\bibitem[Barreto et~al.(2017)Barreto, Dabney, Munos, Hunt, Schaul, van Hasselt,
  and Silver]{barreto2017successor}
Andr{\'e} Barreto, Will Dabney, R{\'e}mi Munos, Jonathan~J Hunt, Tom Schaul,
  Hado~P van Hasselt, and David Silver.
\newblock Successor features for transfer in reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Barreto et~al.(2018)Barreto, Borsa, Quan, Schaul, Silver, Hessel,
  Mankowitz, Zidek, and Munos]{barreto2018transfer}
Andre Barreto, Diana Borsa, John Quan, Tom Schaul, David Silver, Matteo Hessel,
  Daniel Mankowitz, Augustin Zidek, and Remi Munos.
\newblock Transfer in deep reinforcement learning using successor features and
  generalised policy improvement.
\newblock In \emph{International Conference on Machine Learning}, pages
  501--510. PMLR, 2018.

\bibitem[Borsa et~al.(2018)Borsa, Barreto, Quan, Mankowitz, van Hasselt, Munos,
  Silver, and Schaul]{borsa2018universal}
Diana Borsa, Andre Barreto, John Quan, Daniel~J Mankowitz, Hado van Hasselt,
  Remi Munos, David Silver, and Tom Schaul.
\newblock Universal successor features approximators.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Barreto et~al.(2019)Barreto, Borsa, Hou, Comanici, Ayg{\"u}n, Hamel,
  Toyama, Hunt, Mourad, Silver, and Precup]{Barreto2019TheOK}
Andr{\'e} Barreto, Diana Borsa, Shaobo Hou, Gheorghe Comanici, Eser Ayg{\"u}n,
  Philippe Hamel, Daniel Toyama, Jonathan~J. Hunt, Shibl Mourad, David Silver,
  and Doina Precup.
\newblock The option keyboard: Combining skills in reinforcement learning.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Bacon et~al.(2016)Bacon, Harb, and Precup]{bacon2016optioncritic}
Pierre-Luc Bacon, Jean Harb, and Doina Precup.
\newblock The option-critic architecture, 2016.

\bibitem[Hausknecht and Stone(2016)]{hausknecht2016deep}
Matthew Hausknecht and Peter Stone.
\newblock Deep reinforcement learning in parameterized action space, 2016.

\bibitem[Frans et~al.(2018)Frans, Ho, Chen, Abbeel, and
  Schulman]{frans2018meta}
Kevin Frans, Jonathan Ho, Xi~Chen, Pieter Abbeel, and John Schulman.
\newblock Meta learning shared hierarchies.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Merel et~al.(2018{\natexlab{a}})Merel, Ahuja, Pham, Tunyasuvunakool,
  Liu, Tirumala, Heess, and Wayne]{merel2018hierarchical}
Josh Merel, Arun Ahuja, Vu~Pham, Saran Tunyasuvunakool, Siqi Liu, Dhruva
  Tirumala, Nicolas Heess, and Greg Wayne.
\newblock Hierarchical visuomotor control of humanoids.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Wulfmeier et~al.(2019)Wulfmeier, Abdolmaleki, Hafner, Springenberg,
  Neunert, Hertweck, Lampe, Siegel, Heess, and
  Riedmiller]{DBLP:journals/corr/abs-1906-11228}
Markus Wulfmeier, Abbas Abdolmaleki, Roland Hafner, Jost~Tobias Springenberg,
  Michael Neunert, Tim Hertweck, Thomas Lampe, Noah~Y. Siegel, Nicolas Heess,
  and Martin~A. Riedmiller.
\newblock Regularized hierarchical policies for compositional transfer in
  robotics.
\newblock \emph{CoRR}, abs/1906.11228, 2019.
\newblock URL \url{http://arxiv.org/abs/1906.11228}.

\bibitem[Heess et~al.(2016)Heess, Wayne, Tassa, Lillicrap, Riedmiller, and
  Silver]{heess2016learning}
Nicolas Heess, Greg Wayne, Yuval Tassa, Timothy Lillicrap, Martin Riedmiller,
  and David Silver.
\newblock Learning and transfer of modulated locomotor controllers, 2016.

\bibitem[Florensa et~al.(2017)Florensa, Duan, and
  Abbeel]{DBLP:journals/corr/FlorensaDA17}
Carlos Florensa, Yan Duan, and Pieter Abbeel.
\newblock Stochastic neural networks for hierarchical reinforcement learning.
\newblock \emph{CoRR}, abs/1704.03012, 2017.
\newblock URL \url{http://arxiv.org/abs/1704.03012}.

\bibitem[Merel et~al.(2018{\natexlab{b}})Merel, Hasenclever, Galashov, Ahuja,
  Pham, Wayne, Teh, and Heess]{merel2018neural}
Josh Merel, Leonard Hasenclever, Alexandre Galashov, Arun Ahuja, Vu~Pham, Greg
  Wayne, Yee~Whye Teh, and Nicolas Heess.
\newblock Neural probabilistic motor primitives for humanoid control.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{b}}.

\bibitem[Hausman et~al.(2018)Hausman, Springenberg, Wang, Heess, and
  Riedmiller]{hausman2018learning}
Karol Hausman, Jost~Tobias Springenberg, Ziyu Wang, Nicolas Heess, and Martin
  Riedmiller.
\newblock Learning an embedding space for transferable robot skills.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Eysenbach et~al.(2018)Eysenbach, Gupta, Ibarz, and
  Levine]{eysenbach2018diversity}
Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine.
\newblock Diversity is all you need: Learning skills without a reward function.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Hartikainen, Abbeel, and
  Levine]{Haarnoja2018LatentSP}
Tuomas Haarnoja, Kristian Hartikainen, P.~Abbeel, and Sergey Levine.
\newblock Latent space policies for hierarchical reinforcement learning.
\newblock In \emph{ICML}, 2018{\natexlab{b}}.

\bibitem[Jacobs et~al.(1991)Jacobs, Jordan, Nowlan, and Hinton]{6797059}
Robert~A. Jacobs, Michael~I. Jordan, Steven~J. Nowlan, and Geoffrey~E. Hinton.
\newblock Adaptive mixtures of local experts.
\newblock \emph{Neural Computation}, 3\penalty0 (1):\penalty0 79--87, 1991.
\newblock \doi{10.1162/neco.1991.3.1.79}.

\bibitem[Qureshi et~al.(2020)Qureshi, Johnson, Qin, Henderson, Boots, and
  Yip]{Qureshi2020ComposingTP}
A.~H. Qureshi, Jacob~J. Johnson, Yuzhe Qin, Taylor Henderson, Byron Boots, and
  Michael~C. Yip.
\newblock Composing task-agnostic policies with deep reinforcement learning.
\newblock In \emph{ICLR}, 2020.

\bibitem[Tseng et~al.(2021)Tseng, Lin, Feng, and Sun]{tseng2021toward}
Wei-Cheng Tseng, Jin-Siang Lin, Yao-Min Feng, and Min Sun.
\newblock Toward robust long range policy transfer.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 9958--9966, 2021.

\bibitem[Ren et~al.(2021)Ren, Li, Ding, Pan, and Dong]{ren2021probabilistic}
Jie Ren, Yewen Li, Zihan Ding, Wei Pan, and Hao Dong.
\newblock Probabilistic mixture-of-experts for efficient deep reinforcement
  learning, 2021.

\bibitem[Wang et~al.(2022)Wang, Lee, Hakhamaneshi, Abbeel, and
  Laskin]{wang2022skill}
Xiaofei Wang, Kimin Lee, Kourosh Hakhamaneshi, Pieter Abbeel, and Michael
  Laskin.
\newblock Skill preferences: Learning to extract and execute robotic skills
  from human feedback.
\newblock In \emph{Conference on Robot Learning}, pages 1259--1268. PMLR, 2022.

\bibitem[Liu et~al.(2017)Liu, Ramachandran, Liu, and Peng]{liu2017stein}
Yang Liu, Prajit Ramachandran, Qiang Liu, and Jian Peng.
\newblock Stein variational policy gradient.
\newblock In \emph{33rd Conference on Uncertainty in Artificial Intelligence,
  UAI 2017}, 2017.

\bibitem[Liu and Wang(2016)]{liu2016stein}
Qiang Liu and Dilin Wang.
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning, 2020.

\bibitem[Zhu et~al.(2020)Zhu, Wong, Mandlekar, and
  Mart\'{i}n-Mart\'{i}n]{robosuite2020}
Yuke Zhu, Josiah Wong, Ajay Mandlekar, and Roberto Mart\'{i}n-Mart\'{i}n.
\newblock robosuite: A modular simulation framework and benchmark for robot
  learning.
\newblock In \emph{arXiv preprint arXiv:2009.12293}, 2020.

\bibitem[Haarnoja et~al.(2018{\natexlab{c}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pages
  1861--1870. PMLR, 2018{\natexlab{c}}.

\bibitem[Peng et~al.(2019)Peng, Chang, Zhang, Abbeel, and
  Levine]{Peng2019MCPLC}
Xue~Bin Peng, Michael Chang, Grace~H. Zhang, P.~Abbeel, and Sergey Levine.
\newblock Mcp: Learning composable hierarchical control with multiplicative
  compositional policies.
\newblock In \emph{NeurIPS}, 2019.

\end{thebibliography}
