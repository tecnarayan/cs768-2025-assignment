\begin{thebibliography}{10}

\bibitem{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}, 2016.

\bibitem{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5947--5956, 2017.

\bibitem{poggio2017theory}
Tomaso Poggio, Kenji Kawaguchi, Qianli Liao, Brando Miranda, Lorenzo Rosasco,
  Xavier Boix, Jack Hidary, and Hrushikesh Mhaskar.
\newblock Theory of deep learning iii: explaining the non-overfitting puzzle.
\newblock {\em arXiv preprint arXiv:1801.00173}, 2017.

\bibitem{novak2018sensitivity}
Roman Novak, Yasaman Bahri, Daniel~A Abolafia, Jeffrey Pennington, and Jascha
  Sohl-Dickstein.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock {\em arXiv preprint arXiv:1802.08760}, 2018.

\bibitem{brutzkus2017sgd}
Alon Brutzkus, Amir Globerson, Eran Malach, and Shai Shalev-Shwartz.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock {\em arXiv preprint arXiv:1710.10174}, 2017.

\bibitem{belkin2018understand}
Mikhail Belkin, Siyuan Ma, and Soumik Mandal.
\newblock To understand deep learning we need to understand kernel learning.
\newblock {\em arXiv preprint arXiv:1802.01396}, 2018.

\bibitem{ma2017power}
Siyuan Ma, Raef Bassily, and Mikhail Belkin.
\newblock The power of interpolation: Understanding the effectiveness of sgd in
  modern over-parametrized learning.
\newblock {\em arXiv preprint arXiv:1712.06559}, 2017.

\bibitem{shah2018minimum}
Vatsal Shah, Anastasios Kyrillidis, and Sujay Sanghavi.
\newblock Minimum weight norm models do not always generalize well for
  over-parameterized problems.
\newblock {\em arXiv preprint arXiv:1811.07055}, 2018.

\bibitem{wilson2017marginal}
Ashia~C Wilson, Rebecca Roelofs, Mitchell Stern, Nati Srebro, and Benjamin
  Recht.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In {\em Advances in neural information processing systems}, pages
  4148--4158, 2017.

\bibitem{poggio2020theoretical}
Tomaso Poggio, Andrzej Banburski, and Qianli Liao.
\newblock Theoretical issues in deep networks.
\newblock {\em Proceedings of the National Academy of Sciences}, 2020.

\bibitem{2014arXiv1412.6572G}
Ian~J. {Goodfellow}, Jonathon {Shlens}, and Christian {Szegedy}.
\newblock {Explaining and Harnessing Adversarial Examples}.
\newblock {\em arXiv e-prints}, page arXiv:1412.6572, Dec 2014.

\bibitem{zielinski2020explaining}
Piotr Zielinski, Shankar Krishnan, and Satrajit Chatterjee.
\newblock Explaining memorization and generalization: A large-scale study with
  coherent gradients.
\newblock {\em arXiv preprint arXiv:2003.07422}, 2020.

\bibitem{liao2018classical}
Qianli Liao, Brando Miranda, Jack Hidary, and Tomaso Poggio.
\newblock Classical generalization bounds are surprisingly tight for deep
  networks.
\newblock Technical report, Center for Brains, Minds and Machines (CBMM), 2018.

\bibitem{gunasekar_implicit_nodate}
Suriya Gunasekar, Blake Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and
  Nathan Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In {\em 2018 Information Theory and Applications Workshop (ITA)},
  pages 1--10. IEEE, 2018.

\bibitem{neyshabur_geometry_2017}
Behnam Neyshabur, Ryota Tomioka, Ruslan Salakhutdinov, and Nathan Srebro.
\newblock Geometry of {Optimization} and {Implicit} {Regularization} in {Deep}
  {Learning}.
\newblock {\em arXiv:1705.03071 [cs]}, May 2017.
\newblock arXiv: 1705.03071.

\bibitem{lin_generalization_nodate}
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Generalization properties and implicit regularization for multiple
  passes sgm.
\newblock In {\em International Conference on Machine Learning}, pages
  2340--2348. PMLR, 2016.

\bibitem{neyshabur_implicit_2017}
Behnam Neyshabur.
\newblock Implicit {Regularization} in {Deep} {Learning}.
\newblock {\em arXiv:1709.01953 [cs]}, September 2017.
\newblock arXiv: 1709.01953.

\bibitem{neyshabur_search_2014}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In {Search} of the {Real} {Inductive} {Bias}: {On} the {Role} of
  {Implicit} {Regularization} in {Deep} {Learning}.
\newblock {\em arXiv:1412.6614 [cs, stat]}, December 2014.
\newblock arXiv: 1412.6614.

\bibitem{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em The Journal of Machine Learning Research}, 19(1):2822--2878,
  2018.

\bibitem{wei2019regularization}
Colin Wei, Jason~D Lee, Qiang Liu, and Tengyu Ma.
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9712--9724, 2019.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem{storkey2018cinic}
Luke~N Darlow, Elliot~J Crowley, Antreas Antoniou, and Amos~J Storkey.
\newblock Cinic-10 is not imagenet or cifar-10.
\newblock {\em arXiv preprint arXiv:1810.03505}, 2018.

\bibitem{tsipras2018robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock {\em stat}, 1050:11, 2018.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{he_deep_2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep {Residual} {Learning} for {Image} {Recognition}.
\newblock In {\em 2016 {IEEE} {Conference} on {Computer} {Vision} and {Pattern}
  {Recognition} ({CVPR})}, pages 770--778, June 2016.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Laurens Van Der~Maaten, and Kilian~Q Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4700--4708, 2017.

\bibitem{smith2017cyclical}
Leslie~N Smith.
\newblock Cyclical learning rates for training neural networks.
\newblock In {\em 2017 IEEE Winter Conference on Applications of Computer
  Vision (WACV)}, pages 464--472. IEEE, 2017.

\bibitem{li2019exponential}
Zhiyuan Li and Sanjeev Arora.
\newblock An exponential learning rate schedule for deep learning.
\newblock {\em arXiv preprint arXiv:1910.07454}, 2019.

\end{thebibliography}
