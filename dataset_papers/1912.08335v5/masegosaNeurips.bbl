\begin{thebibliography}{10}

\bibitem{alquier2018simpler}
P.~Alquier and B.~Guedj.
\newblock Simpler {PAC-B}ayesian bounds for hostile data.
\newblock {\em Machine Learning}, 107(5):887--902, 2018.

\bibitem{alquier2016properties}
P.~Alquier, J.~Ridgway, and N.~Chopin.
\newblock On the properties of variational approximations of {G}ibbs
  posteriors.
\newblock {\em The Journal of Machine Learning Research}, 17(1):8374--8414,
  2016.

\bibitem{becker2012variance}
R.~A. Becker.
\newblock The variance drain and {J}ensen's inequality.
\newblock {\em CAEPR Working Paper No. 2012-004.}, 2012.

\bibitem{berger1994overview}
J.~O. Berger, E.~Moreno, L.~R. Pericchi, M.~J. Bayarri, J.~M. Bernardo, J.~A.
  Cano, J.~De~la Horra, J.~Mart{\'\i}n, D.~R{\'\i}os-Ins{\'u}a, B.~Betr{\`o},
  et~al.
\newblock An overview of robust {B}ayesian analysis.
\newblock {\em Test}, 3(1):5--124, 1994.

\bibitem{bishop2006pattern}
C.~M. Bishop.
\newblock {\em Pattern recognition and machine learning}.
\newblock {S}pringer, 2006.

\bibitem{bissiri2016general_6}
P.~G. Bissiri, C.~C. Holmes, and S.~G. Walker.
\newblock A general framework for updating belief distributions.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 78(5):1103--1130, 2016.

\bibitem{blei2017variational}
D.~M. Blei, A.~Kucukelbir, and J.~D. McAuliffe.
\newblock Variational inference: A review for statisticians.
\newblock {\em Journal of the American Statistical Association},
  112(518):859--877, 2017.

\bibitem{blei2003latent}
D.~M. Blei, A.~Y. Ng, and M.~I. Jordan.
\newblock Latent {D}irichlet allocation.
\newblock {\em Journal of Machine Learning Research}, 3:993--1022, 2003.

\bibitem{box1976science}
G.~E. Box.
\newblock Science and statistics.
\newblock {\em Journal of the American Statistical Association},
  71(356):791--799, 1976.

\bibitem{breiman2001random}
L.~Breiman.
\newblock Random forests.
\newblock {\em Machine learning}, 45(1):5--32, 2001.

\bibitem{cherief2019mmd_5}
B.-E. Ch{\'e}rief-Abdellatif and P.~Alquier.
\newblock Mmd-{B}ayes: Robust {B}ayesian estimation via maximum mean
  discrepancy.
\newblock {\em arXiv preprint arXiv:1909.13339}, 2019.

\bibitem{dietterich2000ensemble}
T.~G. Dietterich.
\newblock Ensemble methods in machine learning.
\newblock In {\em International workshop on multiple classifier systems}, pages
  1--15. {S}pringer, 2000.

\bibitem{dziugaite2017computing}
G.~K. Dziugaite and D.~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock {\em arXiv preprint arXiv:1703.11008}, 2017.

\bibitem{fort2019deep}
S.~Fort, H.~Hu, and B.~Lakshminarayanan.
\newblock Deep ensembles: A loss landscape perspective.
\newblock {\em arXiv preprint arXiv:1912.02757}, 2019.

\bibitem{freund1999short}
Y.~Freund, R.~Schapire, and N.~Abe.
\newblock A short introduction to boosting.
\newblock {\em Journal-Japanese Society For Artificial Intelligence},
  14(771-780):1612, 1999.

\bibitem{friedman1990principles}
B.~Friedman.
\newblock {\em Principles and techniques of applied mathematics}.
\newblock Courier Dover Publications, 1990.

\bibitem{friedman2002stochastic}
J.~H. Friedman.
\newblock Stochastic gradient boosting.
\newblock {\em Computational statistics \& data analysis}, 38(4):367--378,
  2002.

\bibitem{germain2016pac}
P.~Germain, F.~Bach, A.~Lacoste, and S.~Lacoste-Julien.
\newblock {PAC}-{B}ayesian theory meets {B}ayesian inference.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1884--1892, 2016.

\bibitem{ghahramani2015probabilistic}
Z.~Ghahramani.
\newblock Probabilistic machine learning and artificial intelligence.
\newblock {\em Nature}, 521(7553):452--459, 2015.

\bibitem{grunwald2012safe}
P.~Gr{\"u}nwald.
\newblock The safe {B}ayesian: learning the learning rate via the mixability
  gap.
\newblock In {\em International Conference on Algorithmic Learning Theory},
  pages 169--183. {S}pringer, 2012.

\bibitem{grunwald2018safe}
P.~Gr{\"u}nwald.
\newblock Safe probability.
\newblock {\em Journal of Statistical Planning and Inference}, 195:47--63,
  2018.

\bibitem{grunwald2017inconsistency}
P.~Gr{\"u}nwald and T.~Van~Ommen.
\newblock Inconsistency of {B}ayesian inference for misspecified linear models,
  and a proposal for repairing it.
\newblock {\em {B}ayesian Analysis}, 12(4):1069--1103, 2017.

\bibitem{HoffmanBleiWangPaisley13}
M.~D. Hoffman, D.~M. Blei, C.~Wang, and J.~Paisley.
\newblock Stochastic variational inference.
\newblock {\em Journal of Machine Learning Research}, 14:1303--1347, 2013.

\bibitem{insua2012robust}
D.~R. Insua and F.~Ruggeri.
\newblock {\em Robust {B}ayesian Analysis}, volume 152.
\newblock Springer Science \& Business Media, 2012.

\bibitem{jankowiak2019parametric_3}
M.~Jankowiak, G.~Pleiss, and J.~R. Gardner.
\newblock Parametric {G}aussian process regressors.
\newblock {\em arXiv preprint arXiv:1910.07123}, 2019.

\bibitem{jewson2018principles_8}
J.~Jewson, J.~Q. Smith, and C.~Holmes.
\newblock Principles of {B}ayesian inference using general divergence criteria.
\newblock {\em Entropy}, 20(6):442, 2018.

\bibitem{kleijn2012bernstein}
B.~J.~K. Kleijn, A.~W. Van~der Vaart, et~al.
\newblock The {B}ernstein-von-{M}ises theorem under misspecification.
\newblock {\em Electronic Journal of Statistics}, 6:354--381, 2012.

\bibitem{knoblauch2019robust_4}
J.~Knoblauch.
\newblock Robust deep {G}aussian processes.
\newblock {\em arXiv preprint arXiv:1904.02303}, 2019.

\bibitem{knoblauch2019generalized_2}
J.~Knoblauch, J.~Jewson, and T.~Damoulas.
\newblock Generalized variational inference.
\newblock {\em arXiv preprint arXiv:1904.02063}, 2019.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem{kuncheva2003measures}
L.~I. Kuncheva and C.~J. Whitaker.
\newblock Measures of diversity in classifier ensembles and their relationship
  with the ensemble accuracy.
\newblock {\em Machine learning}, 51(2):181--207, 2003.

\bibitem{lakshminarayanan2017simple}
B.~Lakshminarayanan, A.~Pritzel, and C.~Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6402--6413, 2017.

\bibitem{langford2001bounds}
J.~Langford and M.~Seeger.
\newblock Bounds for averaging classifiers.
\newblock Technical report, 2001.

\bibitem{letarte2019dichotomize_1}
G.~Letarte, P.~Germain, B.~Guedj, and F.~Laviolette.
\newblock Dichotomize and generalize: {PAC-B}ayesian binary activated deep
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6869--6879, 2019.

\bibitem{liao2019sharpening}
J.~Liao and A.~Berg.
\newblock Sharpening {J}ensen's inequality.
\newblock {\em The American Statistician}, 73(3):278--281, 2019.

\bibitem{liu2019accurate}
J.~Liu, J.~Paisley, M.-A. Kioumourtzoglou, and B.~Coull.
\newblock Accurate uncertainty estimation and decomposition in ensemble
  learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8950--8961, 2019.

\bibitem{liu2016stein}
Q.~Liu and D.~Wang.
\newblock Stein variational gradient descent: A general purpose {B}ayesian
  inference algorithm.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2378--2386, 2016.

\bibitem{lyddon2018nonparametric}
S.~Lyddon, S.~Walker, and C.~C. Holmes.
\newblock Nonparametric learning from {B}ayesian models with randomized
  objective functions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2071--2081, 2018.

\bibitem{masegosa2019probabilistic}
A.~R. Masegosa, R.~Caba{\~n}as, H.~Langseth, T.~D. Nielsen, and
  A.~Salmer{\'o}n.
\newblock Probabilistic models with deep neural networks.
\newblock {\em arXiv preprint arXiv:1908.03442}, 2019.

\bibitem{masegosa2020second}
A.~R. Masegosa, S.~S. Lorenzen, C.~Igel, and Y.~Seldin.
\newblock Second order {PAC}-{B}ayesian bounds for the weighted majority vote.
\newblock {\em arXiv preprint arXiv:2007.13532}, 2020.

\bibitem{masegosa2017bayesian}
A.~R. Masegosa, T.~D. Nielsen, H.~Langseth, D.~Ramos-L{\'o}pez,
  A.~Salmer{\'o}n, and A.~L. Madsen.
\newblock {B}ayesian models of data streams with hierarchical power priors.
\newblock In {\em International Conference on Machine Learning}, pages
  2334--2343, 2017.

\bibitem{mcallester1999pac}
D.~A. McAllester.
\newblock {PAC}-{B}ayesian model averaging.
\newblock In {\em COLT}, volume~99, pages 164--170. Citeseer, 1999.

\bibitem{mohamed2019monte}
S.~Mohamed, M.~Rosca, M.~Figurnov, and A.~Mnih.
\newblock Monte {C}arlo gradient estimation in machine learning.
\newblock {\em arXiv preprint arXiv:1906.10652}, 2019.

\bibitem{osawa2019practical}
K.~Osawa, S.~Swaroop, M.~E.~E. Khan, A.~Jain, R.~Eschenhagen, R.~E. Turner, and
  R.~Yokota.
\newblock Practical deep learning with {B}ayesian principles.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4289--4301, 2019.

\bibitem{rezende2015variational}
D.~J. Rezende and S.~Mohamed.
\newblock Variational inference with normalizing flows.
\newblock {\em arXiv preprint arXiv:1505.05770}, 2015.

\bibitem{seeger2003bayesian}
M.~Seeger.
\newblock Bayesian {G}aussian process models: {PAC}-{B}ayesian generalisation
  error bounds and sparse approximations.
\newblock Technical report, University of Edinburgh, 2003.

\bibitem{shalaeva2019improved}
V.~Shalaeva, A.~F. Esfahani, P.~Germain, and M.~Petreczky.
\newblock Improved {PAC-B}ayesian bounds for linear regression.
\newblock {\em arXiv preprint arXiv:1912.03036}, 2019.

\bibitem{sheth2020pseudo}
R.~Sheth and R.~Khardon.
\newblock Pseudo-bayesian learning via direct loss minimization with
  applications to sparse gaussian process models.
\newblock In {\em Symposium on Advances in Approximate Bayesian Inference},
  pages 1--18, 2020.

\bibitem{snoek2019can}
J.~Snoek, Y.~Ovadia, E.~Fertig, B.~Lakshminarayanan, S.~Nowozin, D.~Sculley,
  J.~Dillon, J.~Ren, and Z.~Nado.
\newblock Can you trust your model's uncertainty? {E}valuating predictive
  uncertainty under dataset shift.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13969--13980, 2019.

\bibitem{tipping1999probabilistic}
M.~E. Tipping and C.~M. Bishop.
\newblock Probabilistic principal component analysis.
\newblock {\em Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 61(3):611--622, 1999.

\bibitem{walker2013bayesian_7}
S.~G. Walker.
\newblock Bayesian inference with misspecified models.
\newblock {\em Journal of Statistical Planning and Inference},
  143(10):1621--1633, 2013.

\bibitem{wang2018general}
C.~Wang and D.~M. Blei.
\newblock A general method for robust {B}ayesian modeling.
\newblock {\em Bayesian Analysis}, 13(4):1163--1191, 2018.

\bibitem{wang2019variational}
Y.~Wang and D.~Blei.
\newblock Variational {B}ayes under model misspecification.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  13357--13367, 2019.

\bibitem{wang2017robust}
Y.~Wang, A.~Kucukelbir, and D.~M. Blei.
\newblock Robust probabilistic modeling with {B}ayesian data reweighting.
\newblock In {\em International Conference on Machine Learningd}, pages
  3646--3655. JMLR. org, 2017.

\bibitem{wei2020direct}
Y.~Wei, R.~Sheth, and R.~Khardon.
\newblock Direct loss minimization for sparse gaussian processes.
\newblock {\em arXiv preprint arXiv:2004.03083}, 2020.

\bibitem{wenzel2020good}
F.~Wenzel, K.~Roth, B.~S. Veeling, J.~{\'S}wi{\k{a}}tkowski, L.~Tran, S.~Mandt,
  J.~Snoek, T.~Salimans, R.~Jenatton, and S.~Nowozin.
\newblock How good is the {B}ayes posterior in deep neural networks really?
\newblock {\em arXiv preprint arXiv:2002.02405}, 2020.

\bibitem{wilson2020bayesian}
A.~G. Wilson and P.~Izmailov.
\newblock Bayesian deep learning and a probabilistic perspective of
  generalization.
\newblock {\em arXiv preprint arXiv:2002.08791}, 2020.

\bibitem{xiao2017fashion}
H.~Xiao, K.~Rasul, and R.~Vollgraf.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock {\em arXiv preprint arXiv:1708.07747}, 2017.

\bibitem{zhang2018advances}
C.~Zhang, J.~Butepage, H.~Kjellstrom, and S.~Mandt.
\newblock Advances in variational inference.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  2018.

\bibitem{zhang2006information}
T.~Zhang.
\newblock Information-theoretic upper and lower bounds for statistical
  estimation.
\newblock {\em IEEE Transactions on Information Theory}, 52(4):1307--1321,
  2006.

\bibitem{zhang2006epsilon}
T.~Zhang et~al.
\newblock From epsilon-entropy to kl-entropy: Analysis of minimum information
  complexity density estimation.
\newblock {\em The Annals of Statistics}, 34(5):2180--2210, 2006.

\end{thebibliography}
