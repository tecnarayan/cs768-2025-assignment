\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn and Sra(2020)]{ahn2020nesterov}
Kwangjun Ahn and Suvrit Sra.
\newblock From nesterovâ€™s estimate sequence to riemannian acceleration.
\newblock In \emph{Conference on Learning Theory}, pages 84--118. PMLR, 2020.

\bibitem[Alimisis et~al.(2020)Alimisis, Orvieto, B{\'e}cigneul, and Lucchi]{alimisis2020continuous}
Foivos Alimisis, Antonio Orvieto, Gary B{\'e}cigneul, and Aurelien Lucchi.
\newblock A continuous-time perspective for modeling acceleration in riemannian optimization.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1297--1307. PMLR, 2020.

\bibitem[Alimisis et~al.(2021)Alimisis, Orvieto, Becigneul, and Lucchi]{alimisis2021momentum}
Foivos Alimisis, Antonio Orvieto, Gary Becigneul, and Aurelien Lucchi.
\newblock Momentum improves optimization on riemannian manifolds.
\newblock In \emph{International conference on artificial intelligence and statistics}, pages 1351--1359. PMLR, 2021.

\bibitem[Arjovsky et~al.(2016)Arjovsky, Shah, and Bengio]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In \emph{International conference on machine learning}, pages 1120--1128. PMLR, 2016.

\bibitem[Bonnabel(2013)]{bonnabel2013stochastic}
Silvere Bonnabel.
\newblock Stochastic gradient descent on riemannian manifolds.
\newblock \emph{IEEE Transactions on Automatic Control}, 58\penalty0 (9):\penalty0 2217--2229, 2013.

\bibitem[Brockett(1989)]{brockett1989least}
Roger~W Brockett.
\newblock Least squares matching problems.
\newblock \emph{Linear Algebra and its applications}, 122:\penalty0 761--777, 1989.

\bibitem[Chen et~al.(2019)Chen, Li, Yang, Haupt, and Zhao]{chen2019constrained}
Zhehui Chen, Xingguo Li, Lin Yang, Jarvis Haupt, and Tuo Zhao.
\newblock On constrained nonconvex stochastic optimization: A case study for generalized eigenvalue decomposition.
\newblock In \emph{The 22nd International Conference on Artificial Intelligence and Statistics}, pages 916--925. PMLR, 2019.

\bibitem[Cisse et~al.(2017)Cisse, Bojanowski, Grave, Dauphin, and Usunier]{cisse2017parseval}
Moustapha Cisse, Piotr Bojanowski, Edouard Grave, Yann Dauphin, and Nicolas Usunier.
\newblock Parseval networks: Improving robustness to adversarial examples.
\newblock In \emph{International conference on machine learning}, pages 854--863. PMLR, 2017.

\bibitem[Dynkin(2000)]{dynkin2000calculation}
EB~Dynkin.
\newblock Calculation of the coefficients in the campbell--hausdorff formula.
\newblock \emph{DYNKIN, EB Selected Papers of EB Dynkin with Commentary. Ed. by YUSHKEVICH, AA}, pages 31--35, 2000.

\bibitem[Guigui and Pennec(2021)]{guigui2021reduced}
Nicolas Guigui and Xavier Pennec.
\newblock A reduced parallel transport equation on lie groups with a left-invariant metric.
\newblock In \emph{Geometric Science of Information: 5th International Conference, GSI 2021, Paris, France, July 21--23, 2021, Proceedings 5}, pages 119--126. Springer, 2021.

\bibitem[Hairer et~al.(2006)Hairer, Hochbruck, Iserles, and Lubich]{hairer2006geometric}
Ernst Hairer, Marlis Hochbruck, Arieh Iserles, and Christian Lubich.
\newblock Geometric numerical integration.
\newblock \emph{Oberwolfach Reports}, 3\penalty0 (1):\penalty0 805--882, 2006.

\bibitem[Helfrich et~al.(2018)Helfrich, Willmott, and Ye]{helfrich2018orthogonal}
Kyle Helfrich, Devin Willmott, and Qiang Ye.
\newblock Orthogonal recurrent neural networks with scaled cayley transform.
\newblock In \emph{International Conference on Machine Learning}, pages 1969--1978. PMLR, 2018.

\bibitem[Kong and Tao(2024)]{kong2024convergence}
Lingkai Kong and Molei Tao.
\newblock Convergence of kinetic langevin monte carlo on lie groups.
\newblock \emph{COLT}, 2024.

\bibitem[Kong et~al.(2023)Kong, Wang, and Tao]{kong2022momentum}
Lingkai Kong, Yuqing Wang, and Molei Tao.
\newblock Momentum stiefel optimizer, with applications to suitably-orthogonal attention, and optimal transport.
\newblock \emph{ICLR}, 2023.

\bibitem[Milnor(1976)]{milnor1976curvatures}
John Milnor.
\newblock Curvatures of left invariant metrics on lie groups, 1976.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course}, volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[O'Hagan(2007)]{o2007uniform}
Sean O'Hagan.
\newblock Uniform sampling methods for various compact spaces.
\newblock 2007.

\bibitem[Polyak(1987)]{polyak1987introduction}
Boris~T Polyak.
\newblock Introduction to optimization.
\newblock 1987.

\bibitem[Qiu et~al.(2023)Qiu, Liu, Feng, Xue, Feng, Liu, Zhang, Weller, and Sch{\"o}lkopf]{qiu2023controlling}
Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Sch{\"o}lkopf.
\newblock Controlling text-to-image diffusion by orthogonal finetuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 79320--79362, 2023.

\bibitem[Shi et~al.(2021)Shi, Du, Jordan, and Su]{shi2021understanding}
Bin Shi, Simon~S Du, Michael~I Jordan, and Weijie~J Su.
\newblock Understanding the acceleration phenomenon via high-resolution differential equations.
\newblock \emph{Mathematical Programming}, pages 1--70, 2021.

\bibitem[Tao and Ohsawa(2020)]{tao2020variational}
Molei Tao and Tomoki Ohsawa.
\newblock Variational optimization on lie groups, with examples of leading (generalized) eigenvalue problems.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 4269--4280. PMLR, 2020.

\bibitem[Tripuraneni et~al.(2018)Tripuraneni, Flammarion, Bach, and Jordan]{tripuraneni2018averaging}
Nilesh Tripuraneni, Nicolas Flammarion, Francis Bach, and Michael~I Jordan.
\newblock Averaging stochastic gradient descent on riemannian manifolds.
\newblock In \emph{Conference On Learning Theory}, pages 650--687. PMLR, 2018.

\bibitem[Wen and Yin(2013)]{wen2013feasible}
Zaiwen Wen and Wotao Yin.
\newblock A feasible method for optimization with orthogonality constraints.
\newblock \emph{Mathematical Programming}, 142\penalty0 (1):\penalty0 397--434, 2013.

\bibitem[Wibisono et~al.(2016)Wibisono, Wilson, and Jordan]{wibisono2016variational}
Andre Wibisono, Ashia~C Wilson, and Michael~I Jordan.
\newblock A variational perspective on accelerated methods in optimization.
\newblock \emph{proceedings of the National Academy of Sciences}, 113\penalty0 (47):\penalty0 E7351--E7358, 2016.

\bibitem[Yau(1974)]{yau1974non}
Shing-Tung Yau.
\newblock Non-existence of continuous convex functions on certain riemannian manifolds.
\newblock \emph{Mathematische Annalen}, 207:\penalty0 269--270, 1974.

\bibitem[Zhang and Sra(2016)]{zhang2016first}
Hongyi Zhang and Suvrit Sra.
\newblock First-order methods for geodesically convex optimization.
\newblock In \emph{Conference on learning theory}, pages 1617--1638. PMLR, 2016.

\bibitem[Zhang and Sra(2018)]{zhang2018towards}
Hongyi Zhang and Suvrit Sra.
\newblock Towards riemannian accelerated gradient methods.
\newblock \emph{arXiv preprint arXiv:1806.02812}, 2018.

\end{thebibliography}
