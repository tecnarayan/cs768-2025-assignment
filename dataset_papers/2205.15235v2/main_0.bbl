\begin{thebibliography}{10}

\bibitem{agarwal2017finding}
Naman Agarwal, Zeyuan Allen-Zhu, Brian Bullins, Elad Hazan, and Tengyu Ma.
\newblock Finding approximate local minima faster than gradient descent.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 1195--1199, 2017.

\bibitem{allenzhu2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization,
  2019.

\bibitem{amid2020winnowing}
Ehsan Amid and Manfred~K Warmuth.
\newblock Winnowing with gradient descent.
\newblock In {\em Conference on Learning Theory}, pages 163--182. PMLR, 2020.

\bibitem{amid2020reparameterizing}
Ehsan Amid and Manfred~KK Warmuth.
\newblock Reparameterizing mirror descent as gradient descent.
\newblock {\em Advances in Neural Information Processing Systems},
  33:8430--8439, 2020.

\bibitem{amid2019robust}
Ehsan Amid, Manfred~KK Warmuth, Rohan Anil, and Tomer Koren.
\newblock Robust bi-tempered logistic loss based on bregman divergences.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{arora2018optimization}
Sanjeev Arora, Nadav Cohen, and Elad Hazan.
\newblock On the optimization of deep networks: Implicit acceleration by
  overparameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  244--253. PMLR, 2018.

\bibitem{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{arora2012multiplicative}
Sanjeev Arora, Elad Hazan, and Satyen Kale.
\newblock The multiplicative weights update method: a meta-algorithm and
  applications.
\newblock {\em Theory of computing}, 8(1):121--164, 2012.

\bibitem{bai2019beyond}
Yu~Bai and Jason~D Lee.
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock {\em arXiv preprint arXiv:1910.01619}, 2019.

\bibitem{bauschke2017descent}
Heinz~H Bauschke, J{\'e}r{\^o}me Bolte, and Marc Teboulle.
\newblock A descent lemma beyond lipschitz gradient continuity: first-order
  methods revisited and applications.
\newblock {\em Mathematics of Operations Research}, 42(2):330--348, 2017.

\bibitem{beck2003mirror}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock {\em Operations Research Letters}, 31(3):167--175, 2003.

\bibitem{du2018gradient_deep}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv preprint arXiv:1811.03804}, 2018.

\bibitem{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{ghai2020exponentiated}
Udaya Ghai, Elad Hazan, and Yoram Singer.
\newblock Exponentiated gradient meets gradient descent.
\newblock In {\em Algorithmic Learning Theory}, pages 386--407. PMLR, 2020.

\bibitem{gunasekar2021mirrorless}
Suriya Gunasekar, Blake Woodworth, and Nathan Srebro.
\newblock Mirrorless mirror descent: A natural derivation of mirror descent.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 2305--2313. PMLR, 2021.

\bibitem{hazan2019introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock {\em arXiv preprint arXiv:1909.05207}, 2019.

\bibitem{hazan2017efficient}
Elad Hazan, Karan Singh, and Cyril Zhang.
\newblock Efficient regret minimization in non-convex games.
\newblock In {\em International Conference on Machine Learning}, pages
  1433--1441. PMLR, 2017.

\bibitem{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock {\em arXiv preprint arXiv:1806.07572}, 2018.

\bibitem{jin2017escape}
Chi Jin, Rong Ge, Praneeth Netrapalli, Sham~M Kakade, and Michael~I Jordan.
\newblock How to escape saddle points efficiently.
\newblock In {\em International Conference on Machine Learning}, pages
  1724--1732. PMLR, 2017.

\bibitem{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock {\em Advances in neural information processing systems}, 29, 2016.

\bibitem{laurent2018deep}
Thomas Laurent and James Brecht.
\newblock Deep linear networks with arbitrary loss: All local minima are
  global.
\newblock In {\em International conference on machine learning}, pages
  2902--2907. PMLR, 2018.

\bibitem{lee2016gradient}
Jason~D Lee, Max Simchowitz, Michael~I Jordan, and Benjamin Recht.
\newblock Gradient descent only converges to minimizers.
\newblock In {\em Conference on learning theory}, pages 1246--1257. PMLR, 2016.

\bibitem{li2022implicit}
Zhiyuan Li, Tianhao Wang, JasonD Lee, and Sanjeev Arora.
\newblock Implicit bias of gradient descent on reparametrized models: On
  equivalence to mirror descent.
\newblock {\em arXiv preprint arXiv:2207.04036}, 2022.

\bibitem{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{nemirovskij1983problem}
Arkadij~Semenovi{\v{c}} Nemirovskij and David~Borisovich Yudin.
\newblock Problem complexity and method efficiency in optimization.
\newblock {\em SIAM Review}, 1983.

\bibitem{soltanolkotabi2018theoretical}
Mahdi Soltanolkotabi, Adel Javanmard, and Jason~D Lee.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock {\em IEEE Transactions on Information Theory}, 2018.

\bibitem{srebro2011universality}
Nati Srebro, Karthik Sridharan, and Ambuj Tewari.
\newblock On the universality of online mirror descent.
\newblock {\em Advances in neural information processing systems}, 24, 2011.

\bibitem{JMLR:v6:tsuda05a}
Koji Tsuda, Gunnar R{{\"a}}tsch, and Manfred~K. Warmuth.
\newblock Matrix exponentiated gradient updates for on-line learning and
  bregman projection.
\newblock {\em Journal of Machine Learning Research}, 6(34):995--1018, 2005.

\bibitem{usmanova2021fast}
Ilnura Usmanova, Maryam Kamgarpour, Andreas Krause, and Kfir Levy.
\newblock Fast projection onto convex smooth constraints.
\newblock In {\em International Conference on Machine Learning}, pages
  10476--10486. PMLR, 2021.

\bibitem{OGD}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In {\em Proceedings of the Twentieth International Conference on
  International Conference on Machine Learning}, ICML'03, page 928â€“935, 2003.

\bibitem{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In {\em Proceedings of the 20th international conference on machine
  learning (icml-03)}, pages 928--936, 2003.

\bibitem{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock {\em arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
