\begin{thebibliography}{10}

\bibitem{aljundi2019online}
Rahaf Aljundi, Lucas Caccia, Eugene Belilovsky, Massimo Caccia, Min Lin,
  Laurent Charlin, and Tinne Tuytelaars.
\newblock Online continual learning with maximally interfered retrieval, 2019.

\bibitem{aljundi2017expert}
Rahaf Aljundi, Punarjay Chakravarty, and Tinne Tuytelaars.
\newblock Expert gate: Lifelong learning with a network of experts, 2017.

\bibitem{GSS}
Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Bengio.
\newblock Gradient based sample selection for online continual learning, 2019.

\bibitem{Bengio_curriculumlearning}
Yoshua Bengio, Jérôme Louradour, Ronan Collobert, and Jason Weston.
\newblock Curriculum learning.

\bibitem{benjamin2019measuring}
Ari~S. Benjamin, David Rolnick, and Konrad Kording.
\newblock Measuring and regularizing networks in function space, 2019.

\bibitem{buzzega2020dark}
Pietro Buzzega, Matteo Boschini, Angelo Porrello, Davide Abati, and Simone
  Calderara.
\newblock Dark experience for general continual learning: a strong, simple
  baseline, 2020.

\bibitem{castro2018endtoend}
Francisco~M. Castro, Manuel~J. Marín-Jiménez, Nicolás Guil, Cordelia Schmid,
  and Karteek Alahari.
\newblock End-to-end incremental learning, 2018.

\bibitem{Chaudhry_2018}
Arslan Chaudhry, Puneet~K. Dokania, Thalaiyasingam Ajanthan, and Philip H.~S.
  Torr.
\newblock Riemannian walk for incremental learning: Understanding forgetting
  and intransigence.
\newblock {\em Lecture Notes in Computer Science}, page 556–572, 2018.

\bibitem{chaudhry2021using}
Arslan Chaudhry, Albert Gordo, Puneet~K. Dokania, Philip Torr, and David
  Lopez-Paz.
\newblock Using hindsight to anchor past knowledge in continual learning, 2021.

\bibitem{chaudhry2019efficient}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with a-gem, 2019.

\bibitem{chaudhry2019tiny}
Arslan Chaudhry, Marcus Rohrbach, Mohamed Elhoseiny, Thalaiyasingam Ajanthan,
  Puneet~K. Dokania, Philip H.~S. Torr, and Marc'Aurelio Ranzato.
\newblock On tiny episodic memories in continual learning, 2019.

\bibitem{lifelong}
Zhiyuan Chen, Bing Liu, Ronald Brachman, Peter Stone, and Francesca Rossi.
\newblock {\em Lifelong Machine Learning}.
\newblock Morgan amp; Claypool Publishers, 2nd edition, 2018.

\bibitem{Delange_2021}
Matthias Delange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu~Jia, Ales
  Leonardis, Greg Slabaugh, and Tinne Tuytelaars.
\newblock A continual learning survey: Defying forgetting in classification
  tasks.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  page 1–1, 2021.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE Conference on Computer Vision and Pattern
  Recognition}, pages 248--255, 2009.

\bibitem{dousb}
Arthur Douillard, Eduardo Valle, Charles Ollion, Thomas Robert, and Matthieu
  Cord.
\newblock Insights from the future for continual learning, 2020.

\bibitem{el93}
Jeffrey Elman.
\newblock Learning and development in neural networks: the importance of
  starting small.
\newblock {\em Cognition}, 48:71--99, 08 1993.

\bibitem{cifar10}
Alex~Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem{goodfellow2015empirical}
Ian~J. Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks, 2015.

\bibitem{jung2016lessforgetting}
Heechul Jung, Jeongwoo Ju, Minju Jung, and Junmo Kim.
\newblock Less-forgetting learning in deep neural networks, 2016.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks, 2017.

\bibitem{knoblauch2020optimal}
Jeremias Knoblauch, Hisham Husain, and Tom Diethe.
\newblock Optimal continual learning has perfect memory and is np-hard, 2020.

\bibitem{Krizhevsky2009LearningML}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem{mnist}
Y.~Lecun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{lee2018overcoming}
Sang-Woo Lee, Jin-Hwa Kim, Jaehyun Jun, Jung-Woo Ha, and Byoung-Tak Zhang.
\newblock Overcoming catastrophic forgetting by incremental moment matching,
  2018.

\bibitem{lee2021continual}
Sebastian Lee, Sebastian Goldt, and Andrew Saxe.
\newblock Continual learning in the teacher-student setup: Impact of task
  similarity, 2021.

\bibitem{lesort2019marginal}
Timothée Lesort, Alexander Gepperth, Andrei Stoian, and David Filliat.
\newblock Marginal replay vs conditional replay for continual learning, 2019.

\bibitem{li2017learning}
Zhizhong Li and Derek Hoiem.
\newblock Learning without forgetting, 2017.

\bibitem{lopezpaz2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning, 2017.

\bibitem{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning, 2018.

\bibitem{MCCLOSKEY1989109}
Michael McCloskey and Neal~J. Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem, 1989.

\bibitem{nur2018training}
Melike~Nur Mermer and Mehmet~Fatih Amasyali.
\newblock Training with growing sets: A simple alternative to curriculum
  learning and self paced learning, 2018.

\bibitem{mirzadeh2020understanding}
Seyed~Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, and Hassan
  Ghasemzadeh.
\newblock Understanding the role of training regimes in continual learning,
  2020.

\bibitem{neyshabur2021transferred}
Behnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.
\newblock What is being transferred in transfer learning?, 2021.

\bibitem{nguyen2019understanding}
Cuong~V. Nguyen, Alessandro Achille, Michael Lam, Tal Hassner, Vijay Mahadevan,
  and Stefano Soatto.
\newblock Toward understanding catastrophic forgetting in continual learning,
  2019.

\bibitem{prabhu2020greedy}
Ameya Prabhu, Philip Torr, and Puneet Dokania.
\newblock Gdumb: A simple approach that questions our progress in continual
  learning.
\newblock In {\em The European Conference on Computer Vision (ECCV)}, August
  2020.

\bibitem{2017encoder}
Amal Rannen, Rahaf Aljundi, Matthew~B. Blaschko, and Tinne Tuytelaars.
\newblock Encoder based lifelong learning.
\newblock {\em 2017 IEEE International Conference on Computer Vision (ICCV)},
  Oct 2017.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H.
  Lampert.
\newblock icarl: Incremental classifier and representation learning, 2017.

\bibitem{riemer2019learning}
Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu,
  and Gerald Tesauro.
\newblock Learning to learn without forgetting by maximizing transfer and
  minimizing interference, 2019.

\bibitem{Ring1998ChildAF}
Mark~B. Ring.
\newblock Child: A first step towards continual learning.
\newblock In {\em Learning to Learn}, 1998.

\bibitem{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy~P. Lillicrap, and Greg
  Wayne.
\newblock Experience replay for continual learning, 2019.

\bibitem{ruder2017learning}
Sebastian Ruder and Barbara Plank.
\newblock Learning to select data for transfer learning with bayesian
  optimization, 2017.

\bibitem{rusu2016progressive}
Andrei~A. Rusu, Neil~C. Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks, 2016.

\bibitem{saha2021gradient}
Gobinda Saha, Isha Garg, and Kaushik Roy.
\newblock Gradient projection memory for continual learning, 2021.

\bibitem{schwarz2018progress}
Jonathan Schwarz, Jelena Luketina, Wojciech~M. Czarnecki, Agnieszka
  Grabska-Barwinska, Yee~Whye Teh, Razvan Pascanu, and Raia Hadsell.
\newblock Progress \& compress: A scalable framework for continual learning,
  2018.

\bibitem{serr2018overcoming}
Joan Serrà, Dídac Surís, Marius Miron, and Alexandros Karatzoglou.
\newblock Overcoming catastrophic forgetting with hard attention to the task,
  2018.

\bibitem{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay, 2017.

\bibitem{shmelkov2017incremental}
Konstantin Shmelkov, Cordelia Schmid, and Karteek Alahari.
\newblock Incremental learning of object detectors without catastrophic
  forgetting, 2017.

\bibitem{tinyimgnet}
Stanford.
\newblock {Tiny ImageNet Challenge (CS231n)}, 2015.
\newblock \url{http://tiny-imagenet.herokuapp.com/}.

\bibitem{Thrun1998LifelongLA}
Sebastian Thrun.
\newblock Lifelong learning algorithms.
\newblock In {\em Learning to Learn}, 1998.

\bibitem{wu2021curricula}
Xiaoxia Wu, Ethan Dyer, and Behnam Neyshabur.
\newblock When do curricula work?, 2021.

\bibitem{wu2019large}
Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, and
  Yun Fu.
\newblock Large scale incremental learning, 2019.

\bibitem{DEN}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks, 2017.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence, 2017.

\bibitem{zhang2017understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization, 2017.

\end{thebibliography}
