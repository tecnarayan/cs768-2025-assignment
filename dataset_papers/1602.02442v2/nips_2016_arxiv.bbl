\begin{thebibliography}{17}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Chang and Lin(2011)]{libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm : a library for support vector machines.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  2:27:1--27:27, 2011.

\bibitem[Defazio et~al.(2014{\natexlab{a}})Defazio, Bach, and
  Lacoste-Julien]{adefazio-nips2014}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock Saga: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock \emph{Advances in Neural Information Processing Systems 27 (NIPS
  2014)}, 2014{\natexlab{a}}.

\bibitem[Defazio et~al.(2014{\natexlab{b}})Defazio, Caetano, and Domke]{finito}
Aaron Defazio, Tiberio Caetano, and Justin Domke.
\newblock Finito: A faster, permutable incremental gradient method for big data
  problems.
\newblock \emph{Proceedings of the 31st International Conference on Machine
  Learning}, 2014{\natexlab{b}}.

\bibitem[Hofmann et~al.(2015)Hofmann, Lucchi, Lacoste-Julien, and
  McWilliams]{lacoste-neighbors}
Thomas Hofmann, Aurelien Lucchi, Simon Lacoste-Julien, and Brian McWilliams.
\newblock Variance reduced stochastic gradient descent with neighbors.
\newblock In C.~Cortes, N.D. Lawrence, D.D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  2296--2304. Curran Associates, Inc., 2015.

\bibitem[Johnson and Zhang(2013)]{svrg}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{NIPS}, 2013.

\bibitem[{Kone{\v c}n{\'y}} and {Richt{\'a}rik}(2013)]{semi}
Jakub {Kone{\v c}n{\'y}} and Peter {Richt{\'a}rik}.
\newblock {Semi-Stochastic Gradient Descent Methods}.
\newblock \emph{ArXiv e-prints}, December 2013.

\bibitem[{Lan} and {Zhou}(2015)]{lan-accel}
G.~{Lan} and Y.~{Zhou}.
\newblock {An optimal randomized incremental gradient method}.
\newblock \emph{ArXiv e-prints}, July 2015.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{mairal-catalyst}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In C.~Cortes, N.D. Lawrence, D.D. Lee, M.~Sugiyama, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems 28}, pages
  3366--3374. Curran Associates, Inc., 2015.

\bibitem[Mairal(2014)]{miso2}
Julien Mairal.
\newblock Incremental majorization-minimization optimization with application
  to large-scale machine learning.
\newblock Technical report, INRIA Grenoble Rh\^{o}ne-Alpes / LJK Laboratoire
  Jean Kuntzmann, 2014.

\bibitem[Nesterov(1998)]{nes-book}
Yu. Nesterov.
\newblock \emph{Introductory Lectures On Convex Programming}.
\newblock Springer, 1998.

\bibitem[Nitanda(2014)]{svrg-accel-nitanda}
Atsushi Nitanda.
\newblock Stochastic proximal gradient descent with acceleration techniques.
\newblock In Z.~Ghahramani, M.~Welling, C.~Cortes, N.D. Lawrence, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  27}, pages 1574--1582. Curran Associates, Inc., 2014.

\bibitem[Rockafellar(1976)]{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock \emph{SIAM journal on control and optimization}, 14\penalty0
  (5):\penalty0 877--898, 1976.

\bibitem[Schmidt et~al.(2013)Schmidt, Roux, and Bach]{SAG}
Mark Schmidt, Nicolas~Le Roux, and Francis Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock Technical report, INRIA, 2013.

\bibitem[Shalev-Shwartz and Zhang(2013{\natexlab{a}})]{SDCA}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Stochastic dual coordinate ascent methods for regularized loss
  minimization.
\newblock \emph{JMLR}, 2013{\natexlab{a}}.

\bibitem[Shalev-Shwartz and Zhang(2013{\natexlab{b}})]{accel-sdca}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated mini-batch stochastic dual coordinate ascent.
\newblock In C.J.C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing Systems
  26}, pages 378--385. Curran Associates, Inc., 2013{\natexlab{b}}.

\bibitem[Shalev-Shwartz and Zhang(2013{\natexlab{c}})]{sdca-accel}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Accelerated proximal stochastic dual coordinate ascent for
  regularized loss minimization.
\newblock Technical report, The Hebrew University, Jerusalem and Rutgers
  University, NJ, USA, 2013{\natexlab{c}}.

\bibitem[Shalev-Shwartz et~al.(2011)Shalev-Shwartz, Singer, Srebro, and
  Cotter]{pegasos}
Shai Shalev-Shwartz, Yoram Singer, Nathan Srebro, and Andrew Cotter.
\newblock Pegasos: Primal estimated sub-gradient solver for svm.
\newblock \emph{Mathematical programming}, 127\penalty0 (1):\penalty0 3--30,
  2011.

\end{thebibliography}
