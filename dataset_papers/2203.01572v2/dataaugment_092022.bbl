\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2020{\natexlab{a}})]{allen2020feature}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Feature purification: How adversarial training performs robust deep
  learning.
\newblock \emph{arXiv preprint arXiv:2005.10190}, 2020{\natexlab{a}}.

\bibitem[Allen-Zhu and Li(2020{\natexlab{b}})]{allen2020towards}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Towards understanding ensemble, knowledge distillation and
  self-distillation in deep learning.
\newblock \emph{arXiv preprint arXiv:2012.09816}, 2020{\natexlab{b}}.

\bibitem[Azulay and Weiss(2019)]{azulay2019deep}
Aharon Azulay and Yair Weiss.
\newblock Why do deep convolutional networks generalize so poorly to small
  image transformations?
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--25,
  2019.

\bibitem[Berry(1941)]{berry1941accuracy}
Andrew~C Berry.
\newblock The accuracy of the gaussian approximation to the sum of independent
  variates.
\newblock \emph{Transactions of the american mathematical society}, 49\penalty0
  (1):\penalty0 122--136, 1941.

\bibitem[Bishop(1995)]{bishop1995training}
Chris~M Bishop.
\newblock Training with noise is equivalent to tikhonov regularization.
\newblock \emph{Neural computation}, 7\penalty0 (1):\penalty0 108--116, 1995.

\bibitem[Bubeck et~al.(2021)Bubeck, Li, and Nagaraj]{BLN21}
S{\'e}bastien Bubeck, Yuanzhi Li, and Dheeraj Nagaraj.
\newblock A law of robustness for two-layers neural networks.
\newblock \emph{Conference on Learning Theory (COLT)}, 2021.

\bibitem[Chapelle et~al.(2001)Chapelle, Weston, Bottou, and
  Vapnik]{chapelle2001vicinal}
Olivier Chapelle, Jason Weston, L{\'e}on Bottou, and Vladimir Vapnik.
\newblock Vicinal risk minimization.
\newblock \emph{Advances in neural information processing systems}, pages
  416--422, 2001.

\bibitem[Chen et~al.(2020)Chen, Dobriban, and Lee]{chen2020group}
Shuxiao Chen, Edgar Dobriban, and Jane~H Lee.
\newblock A group-theoretic framework for data augmentation.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (245):\penalty0 1--71, 2020.

\bibitem[Ciregan et~al.(2012)Ciregan, Meier, and Schmidhuber]{ciregan2012multi}
Dan Ciregan, Ueli Meier, and J{\"u}rgen Schmidhuber.
\newblock Multi-column deep neural networks for image classification.
\newblock In \emph{2012 IEEE conference on computer vision and pattern
  recognition}, pages 3642--3649. IEEE, 2012.

\bibitem[Dao et~al.(2019)Dao, Gu, Ratner, Smith, De~Sa, and
  R{\'e}]{dao2019kernel}
Tri Dao, Albert Gu, Alexander Ratner, Virginia Smith, Chris De~Sa, and
  Christopher R{\'e}.
\newblock A kernel theory of modern data augmentation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1528--1537. PMLR, 2019.

\bibitem[DeVries and Taylor(2017)]{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Elandt(1961)]{elandt1961folded}
Regina~C Elandt.
\newblock The folded normal distribution: Two methods of estimating parameters
  from moments.
\newblock \emph{Technometrics}, 3\penalty0 (4):\penalty0 551--562, 1961.

\bibitem[Feng et~al.(2021)Feng, Gangal, Wei, Chandar, Vosoughi, Mitamura, and
  Hovy]{feng2021survey}
Steven~Y Feng, Varun Gangal, Jason Wei, Sarath Chandar, Soroush Vosoughi,
  Teruko Mitamura, and Eduard Hovy.
\newblock A survey of data augmentation approaches for nlp.
\newblock \emph{arXiv preprint arXiv:2105.03075}, 2021.

\bibitem[Hanin and Sun(2021)]{hanin2021data}
Boris Hanin and Yi~Sun.
\newblock How data augmentation affects optimization for linear regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Krizhevsky et~al.(2017)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2017imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock \emph{Communications of the ACM}, 60\penalty0 (6):\penalty0 84--90,
  2017.

\bibitem[Mei et~al.(2021)Mei, Misiakiewicz, and Montanari]{mei2021learning}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Learning with invariances in random features and kernel models.
\newblock \emph{arXiv preprint arXiv:2102.13219}, 2021.

\bibitem[Nagarajan and Kolter(2019)]{nagarajan2019uniform}
Vaishnavh Nagarajan and J.~Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2019/file/05e97c207235d63ceb1db43c60db7bbb-Paper.pdf}.

\bibitem[Neyshabur et~al.(2015)Neyshabur, Tomioka, and
  Srebro]{neyshabur2015search}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock In \emph{ICLR (Workshop)}, 2015.

\bibitem[Rajput et~al.(2019)Rajput, Feng, Charles, Loh, and
  Papailiopoulos]{rajput2019does}
Shashank Rajput, Zhili Feng, Zachary Charles, Po-Ling Loh, and Dimitris
  Papailiopoulos.
\newblock Does data augmentation lead to positive margin?
\newblock In \emph{International Conference on Machine Learning}, pages
  5321--5330. PMLR, 2019.

\bibitem[Shorten and Khoshgoftaar(2019)]{shorten2019survey}
Connor Shorten and Taghi~M Khoshgoftaar.
\newblock A survey on image data augmentation for deep learning.
\newblock \emph{Journal of Big Data}, 6\penalty0 (1):\penalty0 1--48, 2019.

\bibitem[Simard et~al.(2000)Simard, Le~Cun, Denker, and
  Victorri]{simard2000transformation}
Patrice~Y Simard, Yann~A Le~Cun, John~S Denker, and Bernard Victorri.
\newblock Transformation invariance in pattern recognition: Tangent distance
  and propagation.
\newblock \emph{International Journal of Imaging Systems and Technology},
  11\penalty0 (3):\penalty0 181--197, 2000.

\bibitem[Simard et~al.(2003)Simard, Steinkraus, and Platt]{simard2003best}
Patrice~Y Simard, Dave Steinkraus, and John~C Platt.
\newblock Best practices for convolutional neural networks applied to visual
  document analysis.
\newblock In \emph{Seventh International Conference on Document Analysis and
  Recognition, 2003. Proceedings.}, volume~3, pages 958--958. IEEE Computer
  Society, 2003.

\bibitem[Wu et~al.(2020)Wu, Zhang, Valiant, and R{\'e}]{wu2020generalization}
Sen Wu, Hongyang Zhang, Gregory Valiant, and Christopher R{\'e}.
\newblock On the generalization effects of linear transformations in data
  augmentation.
\newblock In \emph{International Conference on Machine Learning}, pages
  10410--10420. PMLR, 2020.

\bibitem[Yaeger et~al.(1996)Yaeger, Lyon, and Webb]{yaeger1996effective}
Larry Yaeger, Richard Lyon, and Brandyn Webb.
\newblock Effective training of a neural network character classifier for word
  recognition.
\newblock \emph{Advances in neural information processing systems}, 9:\penalty0
  807--816, 1996.

\bibitem[Yang et~al.(2022)Yang, Dong, Ward, Dhillon, Sanghavi, and
  Lei]{yang2022sample}
Shuo Yang, Yijun Dong, Rachel Ward, Inderjit~S Dhillon, Sujay Sanghavi, and
  Qi~Lei.
\newblock Sample efficiency of data augmentation consistency regularization.
\newblock \emph{arXiv preprint arXiv:2202.12230}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\end{thebibliography}
