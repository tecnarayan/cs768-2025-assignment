@misc{lopezpaz2016unifying,
      title={Unifying distillation and privileged information}, 
      author={David Lopez-Paz and Léon Bottou and Bernhard Schölkopf and Vladimir Vapnik},
      year={2016},
      eprint={1511.03643},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}


@misc{2018superconvergence,
title={Super-Convergence: Very Fast Training of Residual Networks Using Large Learning Rates},
author={Leslie N. Smith and Nicholay Topin},
year={2018},
}

@misc{lewkowycz2020large,
      title={The large learning rate phase of deep learning: the catapult mechanism}, 
      author={Aitor Lewkowycz and Yasaman Bahri and Ethan Dyer and Jascha Sohl-Dickstein and Guy Gur-Ari},
      year={2020},
      eprint={2003.02218},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@inproceedings{Arora2019FineGrainedAO,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Sanjeev Arora and Simon Shaolei Du and Wei Hu and Zhiyuan Li and Ruosong Wang},
  booktitle={International Conference on Machine Learning},
  year={2019}
}

@inproceedings{lee2021abc,
title={{ABC}: Auxiliary Balanced Classifier for Class-imbalanced Semi-supervised Learning},
author={Hyuck Lee and Seungjae Shin and Heeyoung Kim},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
}

@article{Boucheron_2012,
   title={Concentration inequalities for order statistics},
   volume={17},
   ISSN={1083-589X},
   number={none},
   journal={Electronic Communications in Probability},
   publisher={Institute of Mathematical Statistics},
   author={Boucheron, Stéphane and Thomas, Maud},
   year={2012},
   month={Jan}
}

@book{Henneke,
    author = {Steve Hanneke},
    title = {Theory of Active Learning},
    year={2014},
    version={1.1}
}

@inproceedings{Henneke_agnostic,
author = {Hanneke, Steve},
title = {A Bound on the Label Complexity of Agnostic Active Learning},
year = {2007},
isbn = {9781595937933},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {We study the label complexity of pool-based active learning in the agnostic PAC model. Specifically, we derive general bounds on the number of label requests made by the A2 algorithm proposed by Balcan, Beygelzimer &amp; Langford (Balcan et al., 2006). This represents the first nontrivial general-purpose upper bound on label complexity in the agnostic PAC model.},
booktitle = {Proceedings of the 24th International Conference on Machine Learning},
pages = {353–360},
numpages = {8},
location = {Corvalis, Oregon, USA},
series = {ICML '07}
}

@article{massart2006,
author = {Massart, Pascal and N\'{e}d\'{e}lec, \'{E}lodie},
year = {2006},
month = {10},
pages = {},
title = {Risk bounds for statistical learning},
volume = {34},
journal = {The Annals of Statistics},
}

@inproceedings{asymmetric,
 author = {He, Haowei and Huang, Gao and Yuan, Yang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {2553--2564},
 publisher = {Curran Associates, Inc.},
 title = {Asymmetric Valleys: Beyond Sharp and Flat Local Minima},
 volume = {32},
 year = {2019}
}

@inproceedings{
Arora2020Harnessing,
title={Harnessing the Power of Infinitely Wide Deep Nets on Small-data Tasks},
author={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},
booktitle={International Conference on Learning Representations},
year={2020},
}

@article{zhu19,
      title={A Convergence Theory for Deep Learning via Over-Parameterization}, 
      author={Zeyuan Allen-Zhu and Yuanzhi Li and Zhao Song},
      year={2019},
      journal={arXiv preprint arXiv:1811.03962}
}



@article{berthelot2019mixmatch,
  title={MixMatch: A Holistic Approach to Semi-Supervised Learning},
  author={Berthelot, David and Carlini, Nicholas and Goodfellow, Ian and Papernot, Nicolas and Oliver, Avital and Raffel, Colin},
  journal={arXiv preprint arXiv:1905.02249},
  year={2019}
}



@inproceedings{srikant19,
  title={Finite-time error bounds for linear stochastic approximation andtd learning},
  author={Srikant, Rayadurgam and Ying, Lei},
  booktitle={Conference on Learning Theory},
  pages={2803--2830},
  year={2019},
  organization={PMLR}
}

@INPROCEEDINGS{wide_resnet,
    author = {Sergey Zagoruyko and Nikos Komodakis},
    title = {Wide Residual Networks},
    booktitle = {BMVC},
    year = {2016}
}


@InProceedings{bayesian_dnn,
  title = 	 {Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning},
  author = 	 {Gal, Yarin and Ghahramani, Zoubin},
  booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
  pages = 	 {1050--1059},
  year = 	 {2016},
  editor = 	 {Balcan, Maria Florina and Weinberger, Kilian Q.},
  volume = 	 {48},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {New York, New York, USA},
  month = 	 {20--22 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v48/gal16.pdf},
}



@inproceedings{frequency,
 author = {Ronen, Basri and Jacobs, David and Kasten, Yoni and Kritchman, Shira},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {4761--4771},
 publisher = {Curran Associates, Inc.},
 title = {The Convergence Rate of Neural Networks for Learned Functions of Different Frequencies},
 volume = {32},
 year = {2019}
}

@InProceedings{du19, 
title = {Gradient Descent Finds Global Minima of Deep Neural Networks}, 
author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu}, 
pages = {1675--1685}, 
year = {2019}, 
editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, 
volume = {97}, 
series = {Proceedings of Machine Learning Research}, 
address = {Long Beach, California, USA}, 
month = {09--15 Jun}, 
publisher = {PMLR}, 
}





@incollection{exactComp,
title = {On Exact Computation with an Infinitely Wide Neural Net},
author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8141--8150},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@article{Polyak92,
author = {Polyak, Boris and Juditsky, Anatoli},
year = {1992},
month = {07},
pages = {838-855},
title = {Acceleration of Stochastic Approximation by Averaging},
volume = {30},
journal = {SIAM Journal on Control and Optimization},
}


@inproceedings{neuraltangents2020,
    title={Neural Tangents: Fast and Easy Infinite Neural Networks in Python},
    author={Roman Novak and Lechao Xiao and Jiri Hron and Jaehoon Lee and Alexander A. Alemi and Jascha Sohl-Dickstein and Samuel S. Schoenholz},
    booktitle={International Conference on Learning Representations},
    year={2020},
}


@inproceedings{calibration,
author = {Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
title = {On Calibration of Modern Neural Networks},
year = {2017},
publisher = {JMLR.org},
abstract = {Confidence calibration - the problem of predicting probability estimates representative of the true correctness likelihood - is important for classification models in many applications. We discover that modern neural networks, unlike those from a decade ago, are poorly calibrated. Through extensive experiments, we observe that depth, width, weight decay, and Batch Normalization are important factors influencing calibration. We evaluate the performance of various post-processing calibration methods on state-of-the-art architectures with image and document classification datasets. Our analysis and experiments not only offer insights into neural network learning, but also provide a simple and straightforward recipe for practical settings: on most datasets, temperature scaling - a single-parameter variant of Platt Scaling - is surprisingly effective at calibrating predictions.},
booktitle = {Proceedings of the 34th International Conference on Machine Learning - Volume 70},
pages = {1321–1330},
numpages = {10},
location = {Sydney, NSW, Australia},
series = {ICML'17}
}

@inproceedings{ssl_lr,
  title={When can unlabeled data improve the learning rate?},
  author={Christina G{\"o}pfert and Shai Ben-David and O. Bousquet and S. Gelly and I. Tolstikhin and Ruth Urner},
  booktitle={COLT},
  year={2019}
}


@article{Hdivergence,
title	= {A theory of learning from different domains},
author	= {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jennifer Vaughan},
year	= {2010},
journal	= {Machine Learning},
pages	= {151--175},
volume	= {79}
}


@inproceedings{scarce,
  title={Semi-supervised learning with scarce annotations},
  author={Rebuffi, Sylvestre-Alvise and Ehrhardt, Sebastien and Han, Kai and Vedaldi, Andrea and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={762--763},
  year={2020}
}
@inproceedings{fastSWA,
title={There Are Many Consistent Explanations of Unlabeled Data: Why You Should Average},
author={Ben Athiwaratkun and Marc Finzi and Pavel Izmailov and Andrew Gordon Wilson},
booktitle={International Conference on Learning Representations},
year={2019},
}

@incollection{learning_schedules,
title = {The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure For Least Squares},
author = {Ge, Rong and Kakade, Sham M and Kidambi, Rahul and Netrapalli, Praneeth},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {14977--14988},
year = {2019},
publisher = {Curran Associates, Inc.},
}


@incollection{svcca,
title = {SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability},
author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6076--6085},
year = {2017},
publisher = {Curran Associates, Inc.},
}

@article{pseudolabel,
author = {Lee, Dong-Hyun},
year = {2013},
month = {07},
pages = {},
title = {Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks},
journal = {ICML 2013 Workshop : Challenges in Representation Learning (WREPL)}
}

@article{CEAL,
  title={Cost-effective active learning for deep image classification},
  author={Wang, Keze and Zhang, Dongyu and Li, Ya and Zhang, Ruimao and Lin, Liang},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={27},
  number={12},
  pages={2591--2600},
  year={2016},
  publisher={IEEE}
}


@article{marginAL,
  title={Adversarial active learning for deep networks: a margin based approach},
  author={Ducoffe, Melanie and Precioso, Frederic},
  journal={arXiv preprint arXiv:1802.09841},
  year={2018}
}

@article{VapnikPrivileged,
  author  = {Vladimir Vapnik and Rauf Izmailov},
  title   = {Learning Using Privileged Information: Similarity Control and Knowledge Transfer},
  journal = {Journal of Machine Learning Research},
  year    = {2015},
  volume  = {16},
  number  = {61},
  pages   = {2023-2049},
}


@incollection{OliverRealistic,
title = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
author = {Oliver, Avital and Odena, Augustus and Raffel, Colin A and Cubuk, Ekin Dogus and Goodfellow, Ian},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {3235--3246},
year = {2018},
publisher = {Curran Associates, Inc.},
}


@inproceedings{Linear,
 author = {Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Wide Neural Networks of Any Depth Evolve as Linear Models Under Gradient Descent},
 volume = {32},
 year = {2019}
}

@inproceedings{darp,
 author = {Kim, Jaehyung and Hur, Youngbum and Park, Sejun and Yang, Eunho and Hwang, Sung Ju and Shin, Jinwoo},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {14567--14579},
 publisher = {Curran Associates, Inc.},
 title = {Distribution Aligning Refinery of Pseudo-label for Imbalanced Semi-supervised Learning},
 volume = {33},
 year = {2020}
}



@inproceedings{polylog,
title={How Much Over-parameterization Is Sufficient to Learn Deep Re{\{}LU{\}} Networks?},
author={Zixiang Chen and Yuan Cao and Difan Zou and Quanquan Gu},
booktitle={International Conference on Learning Representations},
year={2021},
}

@inproceedings{
Ji2020Polylogarithmic,
title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow ReLU networks},
author={Ziwei Ji and Matus Telgarsky},
booktitle={International Conference on Learning Representations},
year={2020},
}
@article{MixMatchAL,
  title={Combining MixMatch and Active Learning for Better Accuracy with Fewer Labels},
  author={Song, Shuang and Berthelot, David and Rostamizadeh, Afshin},
  journal={arXiv preprint arXiv:1912.00594},
  year={2019}
}


@incollection{NTK,
title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8571--8580},
year = {2018},
publisher = {Curran Associates, Inc.},
}

@article{ShalevSchwartz,
  title={Discriminative active learning},
  author={Gissin, Daniel and Shalev-Shwartz, Shai},
  journal={arXiv preprint arXiv:1907.06347},
  year={2019}
}

@Inbook{AL_Modern,
author="Balcan, Maria-Florina
and Urner, Ruth",
editor="Kao, Ming-Yang",
title="Active Learning -- Modern Learning Theory",
bookTitle="Encyclopedia of Algorithms",
year="2016",
publisher="Springer New York",
address="New York, NY",
pages="8--13",
isbn="978-1-4939-2864-4",
}

@article{truesample,
author = {Balcan, Maria-Florina and Hanneke, Steve and Vaughan, Jennifer},
year = {2010},
month = {09},
pages = {111-139},
title = {The true sample complexity of active learning},
volume = {80},
journal = {Machine Learning},
}


@article{ALGeneralization,
author = {Cohn, David and Atlas, Les and Ladner, Richard},
title = {Improving Generalization with Active Learning},
year = {1994},
issue_date = {May 1994},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {15},
number = {2},
issn = {0885-6125},
journal = {Mach. Learn.},
month = may,
pages = {201–221},
numpages = {21},
keywords = {generalization, queries, version space, neural networks, active learning}
}





@article{Belkin2018ReconcilingMM,
  title={Reconciling modern machine learning and the bias-variance trade-off},
  author={Mikhail Belkin and Daniel Hsu and Siyuan Ma and Soumik Mandal},
  journal={arXiv preprint arXiv:1812.11118},
  year={2018}
}

@inproceedings{
Nakkiran2020Deep,
title={Deep Double Descent: Where Bigger Models and More Data Hurt},
author={Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
booktitle={International Conference on Learning Representations},
year={2020},
}

@incollection{NIPS2019_8457,
title = {The Label Complexity of Active Learning from Observational Data},
author = {Yan, Songbai and Chaudhuri, Kamalika and Javidi, Tara},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {1810--1819},
year = {2019},
publisher = {Curran Associates, Inc.},
}


@InProceedings{Yoo_2019_CVPR,
author = {Yoo, Donggeun and Kweon, In So},
title = {Learning Loss for Active Learning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}


@incollection{NIPS2019_8500,
title = {Integrating Bayesian and Discriminative Sparse Kernel Machines for  Multi-class Active Learning},
author = {Shi, Weishi and Yu, Qi},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {2285--2294},
year = {2019},
publisher = {Curran Associates, Inc.},
}




@inproceedings{Arpit2017ACL,
  title={A closer look at memorization in deep networks},
  author={Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and others},
  booktitle={International Conference on Machine Learning},
  pages={233--242},
  year={2017},
  organization={PMLR}
}


@inproceedings{uncertainty,
  title={A new active labeling method for deep learning},
  author={Wang, Dan and Shang, Yi},
  booktitle={2014 International joint conference on neural networks (IJCNN)},
  pages={112--119},
  year={2014},
  organization={IEEE}
}



@article{coreset,
  title={Active learning for convolutional neural networks: A core-set approach},
  author={Sener, Ozan and Savarese, Silvio},
  journal={arXiv preprint arXiv:1708.00489},
  year={2017}
}


@incollection{batchbald,
title = {BatchBALD: Efficient and Diverse Batch Acquisition for Deep Bayesian Active Learning},
author = {Kirsch, Andreas and van Amersfoort, Joost and Gal, Yarin},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {7026--7037},
year = {2019},
publisher = {Curran Associates, Inc.},
}


@inproceedings{badge,
title={Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds},
author={Jordan T. Ash and Chicheng Zhang and Akshay Krishnamurthy and John Langford and Alekh Agarwal},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{remixmatch,
title={ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring},
author={David Berthelot and Nicholas Carlini and Ekin D. Cubuk and Alex Kurakin and Kihyuk Sohn and Han Zhang and Colin Raffel},
booktitle={International Conference on Learning Representations},
year={2020},
}

@inproceedings{largeLearningRates,
 author = {Bjorck, Nils and Gomes, Carla P and Selman, Bart and Weinberger, Kilian Q},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Understanding Batch Normalization},
 volume = {31},
 year = {2018}
}

@inproceedings{Pennington,
author = {Lee, Jaehoon and Schoenholz, Samuel S. and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and Sohl-Dickstein, Jascha},
title = {Finite versus Infinite Neural Networks: An Empirical Study},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1271},
numpages = {17},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

@article{EGL,
author = {Huang, Jiaji and Child, Rewon and Rao, Vinay and Liu, Hairong and Satheesh, Sanjeev and Coates, Adam},
year = {2016},
month = {12},
pages = {},
journal={arXiv preprint},
title = {Active Learning for Speech Recognition: the Power of Gradients}
}

@article{fixmatch,
    title={FixMatch: Simplifying Semi-Supervised Learning with Consistency and Confidence},
    author={Kihyuk Sohn and David Berthelot and Chun-Liang Li and Zizhao Zhang and Nicholas Carlini and Ekin D. Cubuk and Alex Kurakin and Han Zhang and Colin Raffel},
    journal={arXiv preprint arXiv:2001.07685},
    year={2020},
}

@incollection{mean_teacher,
title = {Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results},
author = {Tarvainen, Antti and Valpola, Harri},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {1195--1204},
year = {2017},
publisher = {Curran Associates, Inc.},
}



@unknown{meanNTK,
author = {Nitanda, Atsushi and Suzuki, Taiji},
year = {2020},
month = {06},
pages = {},
title = {Optimal Rates for Averaged Stochastic Gradient Descent under Neural Tangent Kernel Regime}
}

@inproceedings{Yuanzhi19,
 author = {Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Towards Explaining the Regularization Effect of Initial Large Learning Rate in Training Neural Networks},
 volume = {32},
 year = {2019}
}
@inbook{TP1,
author = {Yang, Greg},
title = {Tensor Programs I: Wide Feedforward or Recurrent Neural Networks of Any Architecture Are Gaussian Processes},
year = {2019},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 33rd International Conference on Neural Information Processing Systems},
articleno = {892},
numpages = {10}
}

@inproceedings{He19,
 author = {He, Fengxiang and Liu, Tongliang and Tao, Dacheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence},
 volume = {32},
 year = {2019}
}

@article{Arora19,
  author    = {Sanjeev Arora and
               Simon S. Du and
               Wei Hu and
               Zhiyuan Li and
               Ruslan Salakhutdinov and
               Ruosong Wang},
  title     = {On Exact Computation with an Infinitely Wide Neural Net},
  journal   = {CoRR},
  volume    = {abs/1904.11955},
  year      = {2019},
  eprinttype = {arXiv},
  eprint    = {1904.11955},
  timestamp = {Mon, 25 Nov 2019 14:34:49 +0100},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{margin,
author="Roth, Dan
and Small, Kevin",
editor="F{\"u}rnkranz, Johannes
and Scheffer, Tobias
and Spiliopoulou, Myra",
title="Margin-Based Active Learning for Structured Output Spaces",
booktitle="Machine Learning: ECML 2006",
year="2006",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="413--424",
}


@article{ALBL, 
title={Active Learning by Learning}, volume={29}, 
number={1}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Hsu, Wei-Ning and Lin, Hsuan-Tien}, year={2015}, month={Feb.} }

@misc{bordelon2021spectrum,
      title={Spectrum Dependent Learning Curves in Kernel Regression and Wide Neural Networks}, 
      author={Blake Bordelon and Abdulkadir Canatar and Cengiz Pehlevan},
      year={2021},
      eprint={2002.02561},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@inproceedings{lee2018deep,
title={Deep Neural Networks as Gaussian Processes},
author={Jaehoon Lee and Jascha Sohl-dickstein and Jeffrey Pennington and Roman Novak and Sam Schoenholz and Yasaman Bahri},
booktitle={International Conference on Learning Representations},
year={2018},
}

@inproceedings{pi_model,
  added-at = {2019-07-25T00:00:00.000+0200},
  author = {Laine, Samuli and Aila, Timo},
  booktitle = {ICLR (Poster)},
  ee = {https://openreview.net/forum?id=BJ6oOfqge},
  interhash = {eb1d4108b37816325733374ec8afc08c},
  intrahash = {6c2328777c82425076334f108ee4f1ff},
  keywords = {dblp},
  publisher = {OpenReview.net},
  timestamp = {2019-07-26T11:44:03.000+0200},
  title = {Temporal Ensembling for Semi-Supervised Learning.},
  year = 2017
}

@article{survey,
author = {Ren, Pengzhen and Xiao, Yun and Chang, Xiaojun and Huang, Po-Yao and Li, Zhihui and Gupta, Brij B. and Chen, Xiaojiang and Wang, Xin},
title = {A Survey of Deep Active Learning},
year = {2021},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {9},
issn = {0360-0300},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {180},
numpages = {40},
keywords = {active learning, deep active learning, Deep learning}
}


@article{lobpcg,
author = {Stathopoulos, Andreas and Wu, Kesheng},
title = {A Block Orthogonalization Procedure with Constant Synchronization Requirements},
journal = {SIAM Journal on Scientific Computing},
volume = {23},
number = {6},
pages = {2165-2182},
year = {2002},
}