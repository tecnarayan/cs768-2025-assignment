\begin{thebibliography}{10}

\bibitem{Arora19a}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em ICML}, 2019.

\bibitem{Arora18a}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock In {\em ICML}, 2018.

\bibitem{Bartlett17a}
Peter Bartlett, Dylan Foster, and Matus Telgarsky.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In {\em NeurIPS}, 2017.

\bibitem{Bartlett02a}
Peter Bartlett and Shahar Mendelson.
\newblock Rademacher and {Gaussian} complexities: Risk bounds and structural
  results.
\newblock {\em J. Mach. Learn. Res. (JMLR)}, 3(Nov):463--482, 2002.

\bibitem{Bauschke96a}
Heinz~H. Bauschke and Jonathan~M. Borwein.
\newblock On projection algorithms for solving convex feasibility problems.
\newblock {\em SIAM Review}, 36(3):367--426, 1996.

\bibitem{Baykal19a}
Cenk Baykal, Lucas Liebenwein, Igor Gilitschenski, Dan Feldman, and Daniela
  Rus.
\newblock Data-dependent coresets for compressing neural networks with
  application to generalization bounds.
\newblock In {\em ICLR}, 2019.

\bibitem{Bertsekas99a}
D.P. Bertsekas.
\newblock {\em Nonlinear Programming}.
\newblock Athena Scientific, 1999.

\bibitem{Cao19a}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In {\em NeurIPS}, 2019.

\bibitem{Chuang21a}
Ching-Yao Chuang, Youssef Mroueh, Kristjan Greenwald, Antonio Torralba, and
  Stefanie Jegelka.
\newblock Measuring generalization with optimal transport.
\newblock In {\em NeurIPS}, 2021.

\bibitem{DeVries17a}
Terrance DeVries and Graham~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout,
  2017.
\newblock arXiv preprint \url{https://arxiv.org/abs/1708.04552}.

\bibitem{Dudley67a}
Richard~M. Dudley.
\newblock The sizes of compact subsets of {Hilbert} space and continuity of
  {Gaussian} processes.
\newblock {\em J. Funct. Anal.}, 1(3):290--330, 1967.

\bibitem{Dykstra83a}
Richard~L. Dykstra.
\newblock An algorithm for restricted least squares regression.
\newblock {\em Journal of the American Statistical Association},
  78(384):837--842, 1983.

\bibitem{Gine01}
Evarist Gin\'{e} and Armelle Guillou.
\newblock On consistency of kernel density estimators for randomly censored
  data: rates holding uniformly over adaptive intervals.
\newblock {\em Annales de l’IHP Probabilit\'{e}s et statistiques},
  37(4):503–522, 2001.

\bibitem{Golowich18a}
Noah Golowich, Alexander Rakhlin, and Ohad Shamir.
\newblock Size-independent sample complexity of neural networks.
\newblock In {\em COLT}, 2018.

\bibitem{Gouk21b}
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael~J. Cree.
\newblock Regularisation of neural networks by enforcing {Lipschitz}
  continuity.
\newblock {\em Machine Learning}, 110(2):393--416, Feb 2021.

\bibitem{Gouk21a}
Henry Gouk, Timothy~M. Hospedales, and Massimiliano Pontil.
\newblock Distance-based regularization of deep networks for fine-tuning.
\newblock In {\em ICLR}, 2021.

\bibitem{Graf21a}
Florian Graf, Christoph Hofer, Marc Niethammer, and Roland Kwitt.
\newblock Dissecting supervised contrastive learning.
\newblock In {\em ICML}, 2021.

\bibitem{Hardt17a}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock In {\em ICLR}, 2017.

\bibitem{HeF20a}
Fengxiang He, Tongliang Liu, and Dacheng Tao.
\newblock Why {ResNet} works? {Residuals} generalize.
\newblock {\em IEEE Trans Neural Netw. Learn. Syst.}, 31(12):5349--5362, 2020.

\bibitem{He16a}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{He16b}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Identity mappings in deep residual networks.
\newblock In {\em ECCV}, 2016.

\bibitem{Huang17a}
Gao Huang, Zhuang Liu, Laurens van~der Maaten, and Kilian~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, 2017.

\bibitem{Jacot18a}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em NeurIPS}, 2018.

\bibitem{TinyImagnet}
Justin Johnson.
\newblock {Tiny ImageNet}.
\newblock \url{https://github.com/jcjohnson/tiny-imagenet}.
\newblock Accessed: 28.09.2022 via
  \url{http://cs231n.stanford.edu/tiny-imagenet-200.zip}.

\bibitem{Krizhevsky09a}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem{Ledent21a}
Antoine Ledent, Waleed Mustafa, Yunwen Lei, and Marius Kloft.
\newblock Norm-based generalisation bounds for multi-class convolutional neural
  networks.
\newblock In {\em AAAI}, 2021.

\bibitem{Li19a}
Qiyang Li, Saminul Haque, Cem Anil, James Lucas, Roger Grosse, and
  J{\"o}rn-Henrik Jacobsen.
\newblock Preventing gradient attenuation in lipschitz constrained
  convolutional networks.
\newblock {\em NeurIPS}, 2019.

\bibitem{Lin18a}
Hongzhou Lin and Stefanie Jegelka.
\newblock {ResNet} with one-neuron hidden layers is a universal approximator.
\newblock In {\em NeurIPS}, 2018.

\bibitem{Lin19a}
Shan Lin and Jingwei Zhang.
\newblock Generalization bounds for convolutional neural networks, 2019.
\newblock arXiv preprint \url{https://arxiv.org/abs/1910.01487}.

\bibitem{Liu09a}
Jun Liu, Shuiwang Ji, and Jieping Ye.
\newblock Multi-task feature learning via efficient $l_{2,1}$-norm
  minimization.
\newblock In {\em UAI}, 2009.

\bibitem{Long20a}
Philip Long and Hanie Sedghi.
\newblock Generalization bounds for deep convolutional neural networks.
\newblock In {\em ICLR}, 2020.

\bibitem{Maurer16a}
Andreas Maurer.
\newblock A vector-contraction inequality for {Rademacher} complexities.
\newblock In {\em ALT}, 2016.

\bibitem{Mohri18a}
Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar.
\newblock {\em Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem{Nagarajan17a}
Vaishnavh Nagarajan and Zico Kolter.
\newblock Generalization in deep networks: The role of distance from
  initialization.
\newblock In {\em NeurIPS workshop on {Deep Learning: Bridging Theory and
  Practice}}, 2017.

\bibitem{Nagarajan19b}
Vaishnavh Nagarajan and Zico Kolter.
\newblock Deterministic {PAC}-{Bayesian} generalization bounds for deep
  networks via generalizing noise-resilience.
\newblock In {\em ICLR}, 2019.

\bibitem{Nagarajan19a}
Vaishnavh Nagarajan and Zico Kolter.
\newblock Uniform convergence may be unable to explain generalization in deep
  learning.
\newblock In {\em NeurIPS}, 2019.

\bibitem{Neyshabur17a}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A {PAC}-{Bayesian} approach to spectrally-normalized margin bounds
  for neural networks.
\newblock In {\em ICLR}, 2018.

\bibitem{Neyshabur15a}
Behnam Neyshabur, Ryota Tomioka, and Nathan Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In {\em COLT}, 2015.

\bibitem{rauber2017foolboxnative}
Jonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel.
\newblock {Foolbox Native}: Fast adversarial attacks to benchmark the
  robustness of machine learning models in {PyTorch}, {TensorFlow}, and {JAX}.
\newblock {\em J. Open Source Softw.}, 5(53):2607, 2020.

\bibitem{Sedghi19a}
Hanie Sedghi, Vineet Gupta, and Philip Long.
\newblock The singular values of convolutional layers.
\newblock In {\em ICLR}, 2019.

\bibitem{Soudry18a}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, and Nati Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock In {\em ICLR}, 2018.

\bibitem{Suzuki20a}
Taiji Suzuki, Hiroshi Abe, and Tomoaki Nishimura.
\newblock Compression based bound for non-compressed network: unified
  generalization error analysis of large compressible deep neural network.
\newblock In {\em ICLR}, 2020.

\bibitem{Wei19a}
Colin Wei and Tengyu Ma.
\newblock Data-dependent sample complexity of deep neural networks via
  lipschitz augmentation.
\newblock In {\em NeurIPS}, 2019.

\bibitem{Yun19a}
Chulhee Yun, Suvrit Sra, and Ali Jadbabaie.
\newblock Are deep {ResNets} provably better than linear predictors?
\newblock In {\em NeurIPS}, 2019.

\bibitem{Zhang16a}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In {\em ICLR}, 2017.

\bibitem{Zhang21a}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock {\em Communications of the {ACM}}, 64(3):107--115, 2021.

\bibitem{Zhang02a}
Tong Zhang.
\newblock Covering number bounds of certain regularized linear function
  classes.
\newblock {\em J. Mach. Learn. Res. (JMLR)}, (2):527--550, March 2002.

\bibitem{Zhu21a}
Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and
  Qing Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock In {\em NeurIPS}, 2021.

\end{thebibliography}
