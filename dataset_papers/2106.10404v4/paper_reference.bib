@article{fort2019stiffness,
  title={Stiffness: A New Perspective on Generalization in Neural Networks},
  author={Fort, Stanislav and Nowak, Pawe{\l} Krzysztof and Narayanan, Srini},
  journal={arXiv preprint arXiv:1901.09491},
  year={2019}
}

@article{nagappan2020neuroregeneration,
  title={Neuroregeneration and plasticity: a review of the physiological mechanisms for achieving functional recovery postinjury},
  author={Nagappan, Palaniappan Ganesh and Chen, Hong and Wang, De-Yun},
  journal={Military Medical Research},
  volume={7},
  number={1},
  pages={1--16},
  year={2020},
  publisher={BioMed Central}
}

@inproceedings{arora2019fine,
  title={Fine-Grained Analysis of Optimization and Generalization for Overparameterized Two-Layer Neural Networks},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},  
  booktitle={Proceedings of the 36th International Conference on Machine Learning},
  pages={322--332},
  year={2019}
}

@article{lee2018snip,
  title={SNIP: Single-shot network pruning based on connection sensitivity},
  author={Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip HS},
  journal={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{
lee2019signal,
title={A Signal Propagation Perspective for Pruning Neural Networks at Initialization},
author={Namhoon Lee and Thalaiyasingam Ajanthan and Stephen Gould and Philip H. S. Torr},
booktitle={International Conference on Learning Representations},
year={2020.  arXiv:1906.06307},
url={https://openreview.net/forum?id=HJeTo2VFwH}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={International Conference on Learning Representations},
  year={2019}
}

@article{zhang2019fast,
  title={Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks},
  author={Zhang, Guodong and Martens, James and Grosse, Roger},
  journal={arXiv preprint arXiv:1905.10961},
  year={2019}
}

@misc{
zeng2019mlprune,
title={{MLP}rune: Multi-Layer Pruning for Automated Neural Network Compression},
author={Wenyuan Zeng and Raquel Urtasun},
year={2019},
url={https://openreview.net/forum?id=r1g5b2RcKm},
}
@article{Krizhevsky09,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, A. and Hinton, G.},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009},
  publisher={Citeseer}
}

@article{atashgahi2020quick,
  title={Quick and Robust Feature Selection: the Strength of Energy-efficient Sparse Training for Autoencoders},
  author={Atashgahi, Zahra and Sokar, Ghada and van der Lee, Tim and Mocanu, Elena and Mocanu, Decebal Constantin and Veldhuis, Raymond and Pechenizkiy, Mykola},
  journal={arXiv:2012.00560},
  year={2020}
}

@article{mocanu2021sparse,
  title={Sparse Training Theory for Scalable and Efficient Agents},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Pinto, Tiago and Curci, Selima and Nguyen, Phuong H and Gibescu, Madeleine and Ernst, Damien and Vale, Zita A},
  journal={International Conference on Autonomous Agents and Multiagent Systems (AAMAS). arXiv:2103.01636},
  year={2021}
}


@article{hoefler2021sparsity,
  author  = {Torsten Hoefler and Dan Alistarh and Tal Ben-Nun and Nikoli Dryden and Alexandra Peste},
  title   = {Sparsity in Deep Learning: Pruning and growth for efficient inference and training in neural networks},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {241},
  pages   = {1-124},
  url     = {http://jmlr.org/papers/v22/21-0366.html}
}


@article{strubell2019energy,
  title={Energy and policy considerations for deep learning in NLP},
  author={Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  journal={arXiv preprint arXiv:1906.02243},
  year={2019}
}

@article{schwartz2019green,
  title={Green ai},
  author={Schwartz, Roy and Dodge, Jesse and Smith, Noah A and Etzioni, Oren},
  journal={arXiv preprint arXiv:1907.10597},
  year={2019}
}

@article{garcia2019estimation,
  title={Estimation of energy consumption in machine learning},
  author={Garc{\'\i}a-Mart{\'\i}n, Eva and Rodrigues, Crefeda Faviola and Riley, Graham and Grahn, H{\aa}kan},
  journal={Journal of Parallel and Distributed Computing},
  volume={134},
  pages={75--88},
  year={2019},
  publisher={Elsevier}
}

@article{patterson2021carbon,
  title={Carbon Emissions and Large Neural Network Training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021}
}

@inproceedings{
Lin2020Dynamic,
title={Dynamic Model Pruning with Feedback},
author={Tao Lin and Sebastian U. Stich and Luis Barba and Daniil Dmitriev and Martin Jaggi},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJem8lSFwB}
}

@article{liu2021sparse,
  title={Sparse evolutionary deep learning with over one million artificial neurons on commodity hardware},
  author={Liu, Shiwei and Mocanu, Decebal Constantin and Matavalam, Amarsagar Reddy Ramapuram and Pei, Yulong and Pechenizkiy, Mykola},
  journal={Neural Computing and Applications},
  volume={33},
  number={7},
  pages={2589--2604},
  year={2021},
  publisher={Springer}
}

@article{jayakumar2020top,
  title={Top-kast: Top-k always sparse training},
  author={Jayakumar, Siddhant and Pascanu, Razvan and Rae, Jack and Osindero, Simon and Elsen, Erich},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={20744--20754},
  year={2020}
}

@inproceedings{
zhou2021learning,
title={Learning N:M  Fine-grained Structured Sparse Neural Networks From Scratch},
author={Aojun Zhou and Yukun Ma and Junnan Zhu and Jianbo Liu and Zhijie Zhang and Kun Yuan and Wenxiu Sun and Hongsheng Li},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=K9bw7vqp_s}
}

@article{nvidia2020,
  title={NVIDIA A100 Tensor Core GPU Architecture},
  author={Nvidia},
  journal={https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/nvidia-ampere-architecture-whitepaper.pdf},
  year={2020}
}


@article{gale2020sparse,
  title={Sparse GPU kernels for deep learning},
  author={Gale, Trevor and Zaharia, Matei and Young, Cliff and Elsen, Erich},
  journal={arXiv preprint arXiv:2006.10901},
  year={2020}
}

@article{chen2019eyeriss,
  title={Eyeriss v2: A flexible accelerator for emerging deep neural networks on mobile devices},
  author={Chen, Yu-Hsin and Yang, Tien-Ju and Emer, Joel and Sze, Vivienne},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  volume={9},
  number={2},
  pages={292--308},
  year={2019},
  publisher={IEEE}
}

@inproceedings{wang2018snrram,
  title={Snrram: an efficient sparse neural network computation architecture based on resistive random-access memory},
  author={Wang, Peiqi and Ji, Yu and Hong, Chi and Lyu, Yongqiang and Wang, Dongsheng and Xie, Yuan},
  booktitle={2018 55th ACM/ESDA/IEEE Design Automation Conference (DAC)},
  pages={1--6},
  year={2018},
  organization={IEEE}
}

@article{azarian2021cascade,
  title={Cascade Weight Shedding in Deep Neural Networks: Benefits and Pitfalls for Network Pruning},
  author={Azarian, Kambiz and Porikli, Fatih},
  journal={arXiv preprint arXiv:2103.10629},
  year={2021}
}


@misc{
zhu2017prune,
title={To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
author={Michael H. Zhu and Suyog Gupta},
year={2018},
url={https://openreview.net/forum?id=S1lN69AT-},
}

@inproceedings{mittal2018recovering,
  title={Recovering from random pruning: On the plasticity of deep convolutional neural networks},
  author={Mittal, Deepak and Bhardwaj, Shweta and Khapra, Mitesh M and Ravindran, Balaraman},
  booktitle={2018 IEEE Winter Conference on Applications of Computer Vision (WACV)},
  pages={848--857},
  year={2018},
  organization={IEEE}
}

@article{hubara2021accelerated,
  title={Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks},
  author={Hubara, Itay and Chmiel, Brian and Island, Moshe and Banner, Ron and Naor, Seffi and Soudry, Daniel},
  journal={arXiv preprint arXiv:2102.08124},
  year={2021}
}


@inproceedings{bartoldson2019generalization,
 author = {Bartoldson, Brian and Morcos, Ari and Barbu, Adrian and Erlebacher, Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20852--20864},
 publisher = {Curran Associates, Inc.},
 title = {The Generalization-Stability Tradeoff In Neural Network Pruning},
 url = {https://proceedings.neurips.cc/paper/2020/file/ef2ee09ea9551de88bc11fd7eeea93b0-Paper.pdf},
 volume = {33},
 year = {2020}
}



@article{mahar2018intrinsic,
  title={Intrinsic mechanisms of neuronal axon regeneration},
  author={Mahar, Marcus and Cavalli, Valeria},
  journal={Nature Reviews Neuroscience},
  volume={19},
  number={6},
  pages={323--337},
  year={2018},
  publisher={Nature Publishing Group}
}

@inproceedings{chen2020long,
  title={Long live the lottery: The existence of winning tickets in lifelong learning},
  author={Chen, Tianlong and Zhang, Zhenyu and Liu, Sijia and Chang, Shiyu and Wang, Zhangyang},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

@article{chen2020earlybert,
  title={Earlybert: Efficient bert training via early-bird lottery tickets},
  author={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},
  journal={arXiv preprint arXiv:2101.00063},
  year={2020}
}

@inproceedings{chen2020lottery,
 author = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Wang, Zhangyang and Carbin, Michael},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {15834--15846},
 publisher = {Curran Associates, Inc.},
 title = {The Lottery Ticket Hypothesis for Pre-trained BERT Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/b6af2c9703f203a2794be03d443af2e3-Paper.pdf},
 volume = {33},
 year = {2020}
}



@InProceedings{chen2020lottery2,
    author    = {Chen, Tianlong and Frankle, Jonathan and Chang, Shiyu and Liu, Sijia and Zhang, Yang and Carbin, Michael and Wang, Zhangyang},
    title     = {The Lottery Tickets Hypothesis for Supervised and Self-Supervised Pre-Training in Computer Vision Models},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {16306-16316}
}


@article{you2019drawing,
  title={Drawing early-bird tickets: Towards more efficient training of deep networks},
  author={You, Haoran and Li, Chaojian and Xu, Pengfei and Fu, Yonggan and Wang, Yue and Chen, Xiaohan and Baraniuk, Richard G and Wang, Zhangyang and Lin, Yingyan},
  journal={arXiv preprint arXiv:1909.11957},
  year={2019}
}


@article{yiu2006glial,
  title={Glial inhibition of CNS axon regeneration},
  author={Yiu, Glenn and He, Zhigang},
  journal={Nature Reviews Neuroscience},
  volume={7},
  number={8},
  pages={617--627},
  year={2006},
  publisher={Nature Publishing Group}
}

@book{kandel2000principles,
  title={Principles of neural science},
  author={Kandel, Eric R and Schwartz, James H and Jessell, Thomas M and Siegelbaum, Steven and Hudspeth, A James and Mack, Sarah},
  volume={4},
  year={2000},
  publisher={McGraw-hill New York}
}

@inproceedings{yang2020procrustes,
  title={Procrustes: a dataflow and accelerator for sparse deep neural network training},
  author={Yang, Dingqing and Ghasemazar, Amin and Ren, Xiaowei and Golub, Maximilian and Lemieux, Guy and Lis, Mieszko},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={711--724},
  year={2020},
  organization={IEEE}
}


@inproceedings{wortsman2019discovering,
 author = {Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Discovering Neural Wirings},
 url = {https://proceedings.neurips.cc/paper/2019/file/d010396ca8abf6ead8cacc2c2f2f26c7-Paper.pdf},
 volume = {32},
 year = {2019}
}





@InProceedings{verma2021sparsifying,
  title = 	 {Sparsifying Networks via Subdifferential Inclusion},
  author =       {Verma, Sagar and Pesquet, Jean-Christophe},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {10542--10552},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/verma21b/verma21b.pdf},
  url = 	 {https://proceedings.mlr.press/v139/verma21b.html},
  abstract = 	 {Sparsifying deep neural networks is of paramount interest in many areas, especially when those networks have to be implemented on low-memory devices. In this article, we propose a new formulation of the problem of generating sparse weights for a pre-trained neural network. By leveraging the properties of standard nonlinear activation functions, we show that the problem is equivalent to an approximate subdifferential inclusion problem. The accuracy of the approximation controls the sparsity. We show that the proposed approach is valid for a broad class of activation functions (ReLU, sigmoid, softmax). We propose an iterative optimization algorithm to induce sparsity whose convergence is guaranteed. Because of the algorithm flexibility, the sparsity can be ensured from partial training data in a minibatch manner. To demonstrate the effectiveness of our method, we perform experiments on various networks in different applicative contexts: image classification, speech recognition, natural language processing, and time-series forecasting.}
}


@inproceedings{lecun1990optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John S and Solla, Sara A},
  booktitle={Advances in neural information processing systems},
  pages={598--605},
  year={1990}
}


@inproceedings{liu2021we,
 author = {Liu, Shiwei and Yin, Lu and Mocanu, Decebal Constantin and Pechenizkiy, Mykola},
 booktitle = {Proceedings of the 39th International Conference on Machine Learning},
 pages = {6989--7000},
 title = {Do We Actually Need Dense Over-Parameterization? In-Time Over-Parameterization in Sparse Training},
 year = {2021},
 organization={PMLR}
}

@inproceedings{liu2021selfish,
 author = {Liu, Shiwei and Mocanu, Decebal Constantin and Pei, Yulong and Pechenizkiy, Mykola},
 booktitle = {Proceedings of the 39th International Conference on Machine Learning},
 pages = {6893--6904},
 title = {Selfish sparse RNN training},
 year = {2021},
 organization={PMLR}
}

@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}

@article{gale2019state,
  title={The state of sparsity in deep neural networks},
  author={Gale, Trevor and Elsen, Erich and Hooker, Sara},
  journal={arXiv preprint arXiv:1902.09574},
  year={2019}
}


@inproceedings{
renda2020comparing,
title={Comparing Rewinding and Fine-tuning in Neural Network Pruning},
author={Alex Renda and Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2020. arXiv:2003.02389},
url={https://openreview.net/forum?id=S1gSj0NKvB}
}


@inproceedings{evci2020rigging,
  title={Rigging the lottery: Making all tickets winners},
  author={Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  booktitle={International Conference on Machine Learning},
  pages={2943--2952},
  year={2020},
  organization={PMLR}
}

@article{lee2019wide,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel S and Bahri, Yasaman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
  journal={arXiv preprint arXiv:1902.06720},
  year={2019}
}

@article{natarajan1995sparse,
  title={Sparse approximate solutions to linear systems},
  author={Natarajan, Balas Kausik},
  journal={SIAM journal on computing},
  volume={24},
  number={2},
  pages={227--234},
  year={1995},
  publisher={SIAM}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={1126--1135},
  year={2017},
  organization={JMLR. org}
}

@article{liu2018rethinking,
  title={Rethinking the value of network pruning},
  author={Liu, Zhuang and Sun, Mingjie and Zhou, Tinghui and Huang, Gao and Darrell, Trevor},
  journal={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on {imagenet} classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}

@article{shen2020cpot,
  title={Cpot: Channel pruning via optimal transport},
  author={Shen, Yucong and Shen, Li and Huang, Hao-Zhi and Wang, Xuan and Liu, Wei},
  journal={arXiv preprint arXiv:2005.10451},
  year={2020}
}

@article{an2020real,
  title={Real-time universal style transfer on high-resolution images via zero-channel pruning},
  author={An, Jie and Li, Tao and Huang, Haozhi and Shen, Li and Wang, Xuan and Tang, Yongyi and Ma, Jinwen and Liu, Wei and Luo, Jiebo},
  journal={arXiv preprint arXiv:2006.09029},
  year={2020}
}

@inproceedings{yuan2020block,
  title={A block decomposition algorithm for sparse optimization},
  author={Yuan, Ganzhao and Shen, Li and Zheng, Wei-Shi},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={275--285},
  year={2020}
}

@inproceedings{glorot2010understanding,
  title={Understanding the difficulty of training deep feedforward neural networks},
  author={Glorot, Xavier and Bengio, Yoshua},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={249--256},
  year={2010}
}

@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={International Conference on Learning Representations},
  year={2014}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009}
}

@techreport{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex},
  year={2009},
  institution={Citeseer}
}

@inproceedings{zagoruyko2016wide,
  author={Sergey Zagoruyko and Nikos Komodakis},
  title={Wide Residual Networks},
  booktitle={BMVC},
  year={2016}
}

@book{nesterov2013introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@inproceedings{neyshabur2018towards,
  title={The role of over-parametrization in generalization of neural networks},
  author={Behnam Neyshabur and Zhiyuan Li and Srinadh Bhojanapalli and Yann LeCun and Nathan Srebro},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=BygfghAcYX},
}

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{narang2017exploring,
  title={Exploring sparsity in recurrent neural networks},
  author={Narang, Sharan and Elsen, Erich and Diamos, Gregory and Sengupta, Shubho},
  journal={arXiv preprint arXiv:1704.05119},
  year={2017}
}

@inproceedings{
You:2019tz,
title={Drawing Early-Bird Tickets: Toward More Efficient Training of Deep Networks},
author={Haoran You and Chaojian Li and Pengfei Xu and Yonggan Fu and Yue Wang and Xiaohan Chen and Richard G. Baraniuk and Zhangyang Wang and Yingyan Lin},
booktitle={8th International Conference on Learning Representations},
year={2020}
}

@article{chen2020earlybert,
  title={EarlyBERT: Efficient BERT Training via Early-bird Lottery Tickets},
  author={Chen, Xiaohan and Cheng, Yu and Wang, Shuohang and Gan, Zhe and Wang, Zhangyang and Liu, Jingjing},
  journal={arXiv preprint arXiv:2101.00063},
  year={2020}
}

@inproceedings{
frankle2018lottery,
title={The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks},
author={Jonathan Frankle and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2019. arXiv:1803.03635},
url={https://openreview.net/forum?id=rJl-b3RcF7},
}

@article{li2016pruning,
  title={Pruning filters for efficient convnets},
  author={Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@inproceedings{mozer1989skeletonization,
  title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
  author={Mozer, Michael C and Smolensky, Paul},
  booktitle={Advances in neural information processing systems},
  pages={107--115},
  year={1989}
}

@inproceedings{han2015learning,
  title={Learning both weights and connections for efficient neural network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in neural information processing systems},
  pages={1135--1143},
  year={2015}
}

@inproceedings{dong2017learning,
  title={Learning to prune deep neural networks via layer-wise optimal brain surgeon},
  author={Dong, Xin and Chen, Shangyu and Pan, Sinno},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4857--4867},
  year={2017}
}

@inproceedings{luo2017thinet,
  title={Thinet: A filter level pruning method for deep neural network compression},
  author={Luo, Jian-Hao and Wu, Jianxin and Lin, Weiyao},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={5058--5066},
  year={2017}
}

@inproceedings{he2017channel,
  title={Channel pruning for accelerating very deep neural networks},
  author={He, Yihui and Zhang, Xiangyu and Sun, Jian},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={1389--1397},
  year={2017}
}

@inproceedings{he2018amc,
  title={Amc: Automl for model compression and acceleration on mobile devices},
  author={He, Yihui and Lin, Ji and Liu, Zhijian and Wang, Hanrui and Li, Li-Jia and Han, Song},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={784--800},
  year={2018}
}

@inproceedings{liu2017learning,
  title={Learning efficient convolutional networks through network slimming},
  author={Liu, Zhuang and Li, Jianguo and Shen, Zhiqiang and Huang, Gao and Yan, Shoumeng and Zhang, Changshui},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={2736--2744},
  year={2017}
}

@inproceedings{huang2018data,
  title={Data-driven sparse structure selection for deep neural networks},
  author={Huang, Zehao and Wang, Naiyan},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={304--320},
  year={2018}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the {Hessian}},
  author={Pearlmutter, Barak A},
  journal={Neural computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{zhang2018noisy,
  title={Noisy Natural Gradient as Variational Inference},
  author={Zhang, Guodong and Sun, Shengyang and Duvenaud, David and Grosse, Roger},
  booktitle={International Conference on Machine Learning},
  pages={5847--5856},
  year={2018}
}


@InProceedings{wang2019eigen,
  title = 	 {{E}igen{D}amage: Structured Pruning in the {K}ronecker-Factored Eigenbasis},
  author = 	 {Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {6566--6575},
  year = 	 {2019},
  volume = 	 {97},
  publisher = {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/wang19g/wang19g.pdf},
  url = 	 {http://proceedings.mlr.press/v97/wang19g.html},
}



@article{frankle2019lottery,
  title={The Lottery Ticket Hypothesis at Scale},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M and Carbin, Michael},
  journal={arXiv preprint arXiv:1903.01611},
  year={2019}
}

@inproceedings{lin2017runtime,
  title={Runtime neural pruning},
  author={Lin, Ji and Rao, Yongming and Lu, Jiwen and Zhou, Jie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2181--2191},
  year={2017}
}

@inproceedings{wang2018skipnet,
  title={Skipnet: Learning dynamic routing in convolutional networks},
  author={Wang, Xin and Yu, Fisher and Dou, Zi-Yi and Darrell, Trevor and Gonzalez, Joseph E},
  booktitle={Proceedings of the European Conference on Computer Vision},
  pages={409--424},
  year={2018}
}

@inproceedings{he2018soft,
 author = {He, Yang and Kang, Guoliang and Dong, Xuanyi and Fu, Yanwei and Yang, Yi},
 title = {Soft Filter Pruning for Accelerating Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 27th International Joint Conference on Artificial Intelligence},
 series = {IJCAI'18},
 year = {2018},
 isbn = {978-0-9992411-2-7},
 location = {Stockholm, Sweden},
 pages = {2234--2240},
 numpages = {7},
 url = {http://dl.acm.org/citation.cfm?id=3304889.3304970},
 acmid = {3304970},
 publisher = {AAAI Press},
} 

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}


@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2017-12-14T14:05:27.000+0100},
  author = {LeCun, Yann and Cortes, Corinna},
  biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/slicside},
  description = {Nur für Referenzzwecke verwendet (MNIST)},
  groups = {public},}

@article{louizos2017learning,
  title={Learning Sparse Neural Networks through $ L\_0 $ Regularization},
  author={Louizos, Christos and Welling, Max and Kingma, Diederik P},
  journal={International Conference on Learning Representations},
  year={2018}
}


@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{touvron2020fixing,
 author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and Jegou, Herve},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Fixing the train-test resolution discrepancy},
 url = {https://proceedings.neurips.cc/paper/2019/file/d03a857a23b5285736c4d55e0bb067c8-Paper.pdf},
 volume = {32},
 year = {2019}
}



@inproceedings{frankle2020linear,
  title={Linear mode connectivity and the lottery ticket hypothesis},
  author={Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  booktitle={International Conference on Machine Learning},
  pages={3259--3269},
  year={2020},
  organization={PMLR}
}

@article{janowsky1989pruning,
  title={Pruning versus clipping in neural networks},
  author={Janowsky, Steven A},
  journal={Physical Review A},
  volume={39},
  number={12},
  pages={6600},
  year={1989},
  publisher={APS}
}

@article{mozer1989using,
  title={Using relevance to reduce network size automatically},
  author={Mozer, Michael C and Smolensky, Paul},
  journal={Connection Science},
  volume={1},
  number={1},
  pages={3--16},
  year={1989},
  publisher={Taylor \& Francis}
}

@inproceedings{srinivas2017training,
  title={Training sparse neural networks},
  author={Srinivas, Suraj and Subramanya, Akshayvarun and Venkatesh Babu, R},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops},
  pages={138--145},
  year={2017}
}

@inproceedings{
frankle2020pruning,
title={Pruning Neural Networks at Initialization: Why Are We Missing the Mark?},
author={Jonathan Frankle and Gintare Karolina Dziugaite and Daniel Roy and Michael Carbin},
booktitle={International Conference on Learning Representations},
year={2021. arXiv:2009.08576},
url={https://openreview.net/forum?id=Ig-VyQc-MLK}
}

@article{tanaka2020pruning,
  title={Pruning neural networks without any data by iteratively conserving synaptic flow},
  author={Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel LK and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems. arXiv:2006.05467},
  year={2020}
}

@inproceedings{
de2020progressive,
title={Progressive Skeletonization: Trimming more fat from a network at initialization},
author={Pau de Jorge and Amartya Sanyal and Harkirat Behl and Philip Torr and Gr{\'e}gory Rogez and Puneet K. Dokania},
booktitle={International Conference on Learning Representations},
year={2021. arXiv:cs.CV/2006.09081},
url={https://openreview.net/forum?id=9GsFOUyUPi}
}

@article{verdenius2020pruning,
  title={Pruning via iterative ranking of sensitivity statistics},
  author={Verdenius, Stijn and Stol, Maarten and Forr{\'e}, Patrick},
  journal={arXiv preprint arXiv:2006.00896},
  year={2020}
}

@article{xiao2019autoprune,
  title={Autoprune: Automatic network pruning by regularizing auxiliary parameters},
  author={Xiao, Xia and Wang, Zigeng and Rajasekaran, Sanguthevar},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{savarese2019winning,
  title={Winning the Lottery with Continuous Sparsification},
  author={Savarese, Pedro and Silva, Hugo and Maire, Michael},
  journal={arXiv preprint arXiv:1912.04427},
  year={2019}
}

@inproceedings{
LIU2020Dynamic,
title={Dynamic Sparse Training: Find Efficient Sparse Network From Scratch With Trainable Masked Layers},
author={Junjie Liu and Zhe Xu and Runbin Shi and Ray C. C. Cheung and Hayden K.H. So},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SJlbGJrtDB}
}

@inproceedings{NIPS2016_41bfd20a,
 author = {Wen, Wei and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning Structured Sparsity in Deep Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2016/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf},
 volume = {29},
 year = {2016}
}


@article{srinivas2016generalized,
  title={Generalized dropout},
  author={Srinivas, Suraj and Babu, R Venkatesh},
  journal={arXiv preprint arXiv:1611.06791},
  year={2016}
}

@article{paszke2017automatic,
  title={Automatic differentiation in PyTorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}


@article{gebhart2021unified,
  title={A Unified Paths Perspective for Pruning at Initialization},
  author={Gebhart, Thomas and Saxena, Udit and Schrater, Paul},
  journal={arXiv preprint arXiv:2101.10552},
  year={2021}
}

@inproceedings{liu2020finding,
  title={Finding trainable sparse networks through Neural Tangent Transfer},
  author={Liu, Tianlin and Zenke, Friedemann},
  booktitle={International Conference on Machine Learning},
  pages={6336--6347},
  year={2020},
  organization={PMLR}
}

@inproceedings{
bellec2018deep,
title={Deep Rewiring: Training very sparse deep networks},
author={Guillaume Bellec and David Kappel and Wolfgang Maass and Robert Legenstein},
booktitle={International Conference on Learning Representations},
year={2018. arXiv:1711.05136 (2017)},
url={https://openreview.net/forum?id=BJ_wN01C-},
}


@article{mostafa2019parameter,
  title={Parameter efficient training of deep convolutional neural networks by dynamic sparse reparameterization},
  author={Mostafa, Hesham and Wang, Xin},
  journal={International Conference on Machine Learning},
  year={2019}
}
@article{mocanu2018scalable,
  title={Scalable training of artificial neural networks with adaptive sparse connectivity inspired by network science},
  author={Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H and Gibescu, Madeleine and Liotta, Antonio},
  journal={Nature communications},
  volume={9},
  number={1},
  pages={2383},
  year={2018},
  publisher={Nature Publishing Group}
}

@article{shawahna2018fpga,
  title={Fpga-based accelerators of deep learning networks for learning and classification: A review},
  author={Shawahna, Ahmad and Sait, Sadiq M and El-Maleh, Aiman},
  journal={IEEE Access},
  volume={7},
  pages={7823--7859},
  year={2018},
  publisher={IEEE}
}

@inproceedings{kusupati2020soft,
  title={Soft threshold weight reparameterization for learnable sparsity},
  author={Kusupati, Aditya and Ramanujan, Vivek and Somani, Raghav and Wortsman, Mitchell and Jain, Prateek and Kakade, Sham and Farhadi, Ali},
  booktitle={International Conference on Machine Learning},
  pages={5544--5555},
  year={2020},
  organization={PMLR}
}

@inproceedings{molchanov2017variational,
  title={Variational dropout sparsifies deep neural networks},
  author={Molchanov, Dmitry and Ashukha, Arsenii and Vetrov, Dmitry},
  booktitle={International Conference on Machine Learning},
  pages={2498--2507},
  year={2017},
  organization={PMLR}
}

@article{molchanov2016pruning,
  title={Pruning convolutional neural networks for resource efficient inference},
  author={Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{xu2018trained,
  title={Trained rank pruning for efficient deep neural networks},
  author={Xu, Yuhui and Li, Yuxi and Zhang, Shuai and Wen, Wei and Wang, Botao and Qi, Yingyong and Chen, Yiran and Lin, Weiyao and Xiong, Hongkai},
  journal={arXiv preprint arXiv:1812.02402},
  year={2018}
}

@article{guo2018network,
  title={Network decoupling: From regular to depthwise separable convolutions},
  author={Guo, Jianbo and Li, Yuxi and Lin, Weiyao and Chen, Yurong and Li, Jianguo},
  journal={arXiv preprint arXiv:1808.05517},
  year={2018}
}

@article{jaderberg2014speeding,
  title={Speeding up convolutional neural networks with low rank expansions},
  author={Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1405.3866},
  year={2014}
}

@article{denton2014exploiting,
  title={Exploiting linear structure within convolutional networks for efficient evaluation},
  author={Denton, Emily and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  journal={Twenty-eighth Conference on Neural Information Processing Systems. arXiv:1404.0736},
  year={2014}
}

@article{singh2020woodfisher,
  title={WoodFisher: Efficient Second-Order Approximation for Neural Network Compression},
  author={Singh, Sidak Pal and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@book{hassibi1993second,
  title={Second order derivatives for network pruning: Optimal brain surgeon},
  author={Hassibi, Babak and Stork, David G},
  year={1993},
  publisher={Morgan Kaufmann}
}

@inproceedings{strom2015scalable,
  title={Scalable distributed DNN training using commodity GPU cloud computing},
  author={Strom, Nikko},
  booktitle={Sixteenth Annual Conference of the International Speech Communication Association},
  year={2015}
}

@inproceedings{molchanov2019importance,
  title={Importance estimation for neural network pruning},
  author={Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11264--11272},
  year={2019}
}

@article{evci2019difficulty,
  title={The Difficulty of Training Sparse Neural Networks},
  author={Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  journal={arXiv preprint arXiv:1906.10732},
  year={2019}
}

@article{schraudolph2002fast,
  title={Fast curvature matrix-vector products for second-order gradient descent},
  author={Schraudolph, Nicol N},
  journal={Neural computation},
  volume={14},
  number={7},
  pages={1723--1738},
  year={2002},
  publisher={MIT Press}
}

@article{zhang2019all,
  title={Are All Layers Created Equal?},
  author={Zhang, Chiyuan and Bengio, Samy and Singer, Yoram},
  journal={arXiv preprint arXiv:1902.01996},
  year={2019}
}

@inproceedings{han2016eie,
  title={EIE: efficient inference engine on compressed deep neural network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  booktitle={2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)},
  pages={243--254},
  year={2016},
  organization={IEEE}
}

@article{dey2019pre,
  title={Pre-Defined Sparse Neural Networks with Hardware Acceleration},
  author={Dey, Sourya and Huang, Kuan-Wen and Beerel, Peter A and Chugg, Keith M},
  journal={IEEE Journal on Emerging and Selected Topics in Circuits and Systems},
  year={2019},
  publisher={IEEE}
}


@inproceedings{
Wang2020Picking,
title={Picking Winning Tickets Before Training by Preserving Gradient Flow},
author={Chaoqi Wang and Guodong Zhang and Roger Grosse},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=SkgsACVKPH}
}

@article{dettmers2019sparse,
  title={Sparse Networks from Scratch: Faster Training without Losing Performance},
  author={Dettmers, Tim and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1907.04840},
  year={2019}
}

@article{saxe2013exact,
  title={Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author={Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal={arXiv preprint arXiv:1312.6120},
  year={2013}
}

@inproceedings{xiao2018dynamical,
  title={Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks},
  author={Xiao, Lechao and Bahri, Yasaman and Sohl-Dickstein, Jascha and Schoenholz, Samuel and Pennington, Jeffrey},
  booktitle={International Conference on Machine Learning},
  pages={5393--5402},
  year={2018}
}

@inproceedings{yang2017mean,
  title={Mean field residual networks: On the edge of chaos},
  author={Yang, Ge and Schoenholz, Samuel},
  booktitle={Advances in neural information processing systems},
  pages={7103--7114},
  year={2017}
}

@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={3360--3368},
  year={2016}
}