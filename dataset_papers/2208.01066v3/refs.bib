@misc{pesut2022who, 
title={Who models the models that model models? An exploration of GPT-3's in-context model fitting ability},
url={https://www.alignmentforum.org/posts/c2RzFadrxkzyRAFXa/who-models-the-models-that-model-models-an-exploration-of}, 
journal={AI Alignment Forum}, 
author={Pesut, Lovre},
year={2022}, 
} 

@article{dinh2022lift,
  title={LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks},
  author={Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Rajput, Shashank and Gira, Michael and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
  journal={arXiv preprint arXiv:2206.06565},
  year={2022}
}


@inproceedings{garnelo2018conditional,
  title={Conditional neural processes},
  author={Garnelo, Marta and Rosenbaum, Dan and Maddison, Christopher and Ramalho, Tiago and Saxton, David and Shanahan, Murray and Teh, Yee Whye and Rezende, Danilo and Eslami, SM Ali},
  booktitle={International Conference on Machine Learning},
  pages={1704--1713},
  year={2018},
  organization={PMLR}
}


@article{garnelo2018neural,
  title={Neural processes},
  author={Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J and Eslami, SM and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1807.01622},
  year={2018}
}

@article{kim2019attentive,
  title={Attentive neural processes},
  author={Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  journal={arXiv preprint arXiv:1901.05761},
  year={2019}
}

@article{nguyen2022transformer,
  title={Transformer neural processes: Uncertainty-aware meta learning via sequence modeling},
  author={Nguyen, Tung and Grover, Aditya},
  journal={arXiv preprint arXiv:2207.04179},
  year={2022}
}

@article{blanc2021decision,
  title={Decision tree heuristics can fail, even in the smoothed setting},
  author={Blanc, Guy and Lange, Jane and Qiao, Mingda and Tan, Li-Yang},
  journal={arXiv preprint arXiv:2107.00819},
  year={2021}
}

@inproceedings{brutzkus2020id3,
  title={Id3 learns juntas for smoothed product distributions},
  author={Brutzkus, Alon and Daniely, Amit and Malach, Eran},
  booktitle={Conference on Learning Theory},
  pages={902--915},
  year={2020},
  organization={PMLR}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}


@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    year = "2020",
    publisher = "Association for Computational Linguistics (ACL)",
}

@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 year={2011}
}

@inproceedings{bengio2009curriculum,
  title={Curriculum learning},
  author={Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  booktitle={International Conference on Machine Learning (ICML)},
  pages={41--48},
  year={2009}
}

@article{elman1993learning,
  title={Learning and development in neural networks: The importance of starting small},
  author={Elman, Jeffrey L},
  journal={Cognition},
  year={1993},
  publisher={Elsevier}
}

@article{sanger1994neural,
  title={Neural network learning control of robot manipulators using gradually increasing task difficulty},
  author={Sanger, Terence D},
  journal={IEEE transactions on Robotics and Automation},
  year={1994},
}

@article{wu2020curricula,
  title={When do curricula work?},
  author={Wu, Xiaoxia and Dyer, Ethan and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2012.03107},
  year={2020}
}

@inproceedings{bengio1995optimization,
  title={On the optimization of a synaptic learning rule},
  author={Bengio, Samy and Bengio, Yoshua and Cloutier, Jocelyn and Gecsei, Jan},
  booktitle={Preprints Conf. Optimality in Artificial and Biological Neural Networks},
  year={1995}
}


@article{li2016learning,
  title={Learning to optimize},
  author={Li, Ke and Malik, Jitendra},
  journal={arXiv preprint arXiv:1606.01885},
  year={2016}
}

@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning (ICML)},
  year={2017},
}

@article{ravi2016optimization,
  title={Optimization as a model for few-shot learning},
  author={Ravi, Sachin and Larochelle, Hugo},
  year={2017},
  journal={International Conference for Learning Representations (ICLR)}
}


@article{hospedales2020meta,
  title={Meta-learning in neural networks: A survey},
  author={Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  journal={arXiv preprint arXiv:2004.05439},
  year={2020}
}

@book{thrun2012learning,
  title={Learning to learn},
  author={Thrun, Sebastian and Pratt, Lorien},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@inproceedings{naik1992meta,
  title={Meta-neural networks that learn by learning},
  author={Naik, Devang K and Mammone, Richard J},
  booktitle={International Joint Conference on Neural Networks (IJCNN)},
  year={1992},
}

@article{muller2021transformers,
  title={Transformers Can Do Bayesian Inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}

@article{lu2021pretrained,
  title={Pretrained transformers as universal computation engines},
  author={Lu, Kevin and Grover, Aditya and Abbeel, Pieter and Mordatch, Igor},
  journal={arXiv preprint arXiv:2103.05247},
  year={2021}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{parmar2018image,
  title={Image transformer},
  author={Parmar, Niki and Vaswani, Ashish and Uszkoreit, Jakob and Kaiser, Lukasz and Shazeer, Noam and Ku, Alexander and Tran, Dustin},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2018},
}

@article{yao2021self,
  title={Self-attention networks can process bounded hierarchical languages},
  author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2105.11115},
  year={2021}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020},
}

@article{bhattamishra2020ability,
  title={On the ability and limitations of transformers to recognize formal languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2009.11264},
  year={2020}
}

@article{bhattamishra2020computational,
  title={On the computational power of transformers and its implications in sequence modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  journal={arXiv preprint arXiv:2006.09286},
  year={2020}
}

@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}

@article{perez2019turing,
  title={On the turing completeness of modern neural network architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@article{olsson2022context,
   title={In-context Learning and Induction Heads},
   author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{chan2022data,
  title={Data Distributional Properties Drive Emergent Few-Shot Learning in Transformers},
  author={Chan, Stephanie CY and Santoro, Adam and Lampinen, Andrew K and Wang, Jane X and Singh, Aaditya and Richemond, Pierre H and McClelland, Jay and Hill, Felix},
  journal={arXiv preprint arXiv:2205.05055},
  year={2022}
}

@article{lampinen2022can,
  title={Can language models learn from explanations in context?},
  author={Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie CY and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L and Wang, Jane X and Hill, Felix},
  journal={arXiv preprint arXiv:2204.02329},
  year={2022}
}

@article{razeghi2022impact,
  title={Impact of pretraining term frequencies on few-shot reasoning},
  author={Razeghi, Yasaman and Logan IV, Robert L and Gardner, Matt and Singh, Sameer},
  journal={arXiv preprint arXiv:2202.07206},
  year={2022}
}

@misc{rong_2021, 
title={Extrapolating to Unnatural Language Processing with GPT-3's In-context Learning: The Good, the Bad, and the Mysterious)},
url={http://ai.stanford.edu/blog/in-context-learning/}, 
journal={The Stanford AI Lab Blog}, 
author={Rong, Frieda },
year={2021}, 
} 

@inproceedings{zhao2021calibrate,
  title={Calibrate before use: Improving few-shot performance of language models},
  author={Zhao, Zihao and Wallace, Eric and Feng, Shi and Klein, Dan and Singh, Sameer},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2021},

}

@article{min2021noisy,
  title={Noisy channel language model prompting for few-shot text classification},
  author={Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2108.04106},
  year={2021}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}

@article{lu2021fantastically,
  title={Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
  author={Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
  journal={arXiv preprint arXiv:2104.08786},
  year={2021}
}

@article{rubin2021learning,
  title={Learning To Retrieve Prompts for In-Context Learning},
  author={Rubin, Ohad and Herzig, Jonathan and Berant, Jonathan},
  journal={arXiv preprint arXiv:2112.08633},
  year={2021}
}

@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021}
}

@article{mishra2021reframing,
  title={Reframing Instructional Prompts to GPTk's Language},
  author={Mishra, Swaroop and Khashabi, Daniel and Baral, Chitta and Choi, Yejin and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2109.07830},
  year={2021}
}

@article{chen2021meta,
  title={Meta-learning via language model in-context tuning},
  author={Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  journal={arXiv preprint arXiv:2110.07814},
  year={2021}
}

@article{nakkiran2019more,
  title={More data can hurt for linear regression: Sample-wise double descent},
  author={Nakkiran, Preetum},
  journal={arXiv preprint arXiv:1912.07242},
  year={2019}
}

@article{nakkiran2021deep,
  title={Deep double descent: Where bigger models and more data hurt},
  author={Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  year={2021},
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  year={2019},
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2020}
}

@article{min2022rethinking,
  title={Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@inproceedings{xie2022an,
    title={An Explanation of In-context Learning as Implicit Bayesian Inference},
    author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
    booktitle={International Conference on Learning Representations (ICLR)},
    year={2022},
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year={2018},
  journal={OpenAI blog}
}

@article{rae2021scaling,
  title={Scaling language models: Methods, analysis \& insights from training gopher},
  author={Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal={arXiv preprint arXiv:2112.11446},
  year={2021}
}

@article{black2022gpt,
  title={Gpt-neox-20b: An open-source autoregressive language model},
  author={Black, Sid and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and others},
  journal={arXiv preprint arXiv:2204.06745},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}

@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  year={2021}
}

@article{tibshirani1996regression,
  title={Regression shrinkage and selection via the lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society: Series B (Methodological)},
  year={1996},
}

@article{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@inproceedings{mishra2018simple,
  title={A Simple Neural Attentive Meta-Learner},
  author={Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@inproceedings{santoro2016meta,
  title={Meta-learning with memory-augmented neural networks},
  author={Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
  booktitle={International conference on machine learning (ICML)},
  year={2016},
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  year={1997},
}

@article{zhang2022unveiling,
  title={Unveiling Transformers with LEGO: a synthetic reasoning task},
  author={Zhang, Yi and Backurs, Arturs and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
  journal={arXiv preprint arXiv:2206.04301},
  year={2022}
}

@article{suzgun2019memory,
  title={Memory-augmented recurrent neural networks can learn generalized dyck languages},
  author={Suzgun, Mirac and Gehrmann, Sebastian and Belinkov, Yonatan and Shieber, Stuart M},
  journal={arXiv preprint arXiv:1911.03329},
  year={2019}
}

@article{suzgun2019lstm,
  title={LSTM Networks Can Perform Dynamic Counting},
  author={Suzgun, Mirac and Gehrmann, Sebastian and Belinkov12, Yonatan and Shieber, Stuart M},
  journal={ACL},
  year={2019}
}

@inproceedings{weiss2018practical,
  title={On the Practical Computational Power of Finite Precision RNNs for Language Recognition},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={Association for Computational Linguistics (ACL) (Short Papers)},
  year={2018}
}

@inproceedings{miller2018stable,
  title={Stable Recurrent Models},
  author={Miller, John and Hardt, Moritz},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@article{siegelmann1995computational,
  title={On the computational power of neural nets},
  author={Siegelmann, Hava T and Sontag, Eduardo D},
  journal={Journal of computer and system sciences},
  year={1995},
}

@article{gers2001lstm,
  title={LSTM recurrent networks learn simple context-free and context-sensitive languages},
  author={Gers, Felix A and Schmidhuber, E},
  journal={Transactions on neural networks},
  year={2001},
}

@article{merrill2019sequential,
  title={Sequential neural networks as automata},
  author={Merrill, William},
  journal={arXiv preprint arXiv:1906.01615},
  year={2019}
}

@inproceedings{skachkova2018closing,
  title={Closing brackets with recurrent neural networks},
  author={Skachkova, Natalia and Trost, Thomas Alexander and Klakow, Dietrich},
  booktitle={EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP},
  year={2018}
}

@inproceedings{edelman2022inductive,
  title={Inductive Biases and Variable Creation in Self-Attention Mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2022},
}

@article{snell2021approximating,
  title={Approximating how single head attention learns},
  author={Snell, Charlie and Zhong, Ruiqi and Klein, Dan and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.07601},
  year={2021}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  year={2021},
}

@inproceedings{balcan2021much,
  title={How much data is sufficient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design},
  author={Balcan, Maria-Florina and DeBlasio, Dan and Dick, Travis and Kingsford, Carl and Sandholm, Tuomas and Vitercik, Ellen},
  booktitle={Symposium on Theory of Computing (STOC)},
  year={2021}
}

@article{schwarzschild2021can,
  title={Can you learn an algorithm? generalizing from easy to hard problems with recurrent networks},
  author={Schwarzschild, Avi and Borgnia, Eitan and Gupta, Arjun and Huang, Furong and Vishkin, Uzi and Goldblum, Micah and Goldstein, Tom},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}

@inproceedings{selsam2018learning,
  title={Learning a SAT Solver from Single-Bit Supervision},
  author={Selsam, Daniel and Lamm, Matthew and Benedikt, B and Liang, Percy and de Moura, Leonardo and Dill, David L and others},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2018}
}

@incollection{balcan2020data,
  title={Data-driven algorithm design},
  author={Balcan, Maria-Florina},
  editor={Tim Roughgarden},
  booktitle={Beyond Worst Case Analysis of Algorithms},
  publisher={Cambridge University Press},
  year=2020,
}

@article{horvitz2001bayesian,
  title={A Bayesian Approach to Tackling Hard Computational Problems (Preliminary Report)},
  author={Horvitz, Eric and Ruan, Yongshao and Gomes, Carla and Kautz, Henry and Selman, Bart and Chickering, Max},
  journal={Electronic Notes in Discrete Mathematics},
  year={2001},
}
@article{xu2008satzilla,
  title={SATzilla: portfolio-based algorithm selection for SAT},
  author={Xu, Lin and Hutter, Frank and Hoos, Holger H and Leyton-Brown, Kevin},
  journal={Journal of artificial intelligence research},
  year={2008}
}

@inproceedings{vinyals2015pointer,
 author = {Vinyals, Oriol and Fortunato, Meire and Jaitly, Navdeep},
 booktitle = {Neural Information Processing Systems (NeurIPS)},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 title = {Pointer Networks},
 year = {2015}
}

@article{bello2016neural,
  title={Neural combinatorial optimization with reinforcement learning},
  author={Bello, Irwan and Pham, Hieu and Le, Quoc V and Norouzi, Mohammad and Bengio, Samy},
  journal={arXiv preprint arXiv:1611.09940},
  year={2016}
}

@article{khalil2017learning,
  title={Learning combinatorial optimization algorithms over graphs},
  author={Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2017}
}

@inproceedings{chen2016xgboost,
  title={Xgboost: A scalable tree boosting system},
  author={Chen, Tianqi and Guestrin, Carlos},
  booktitle={conference on knowledge discovery and data mining (KDD)},
  year={2016}
}

@article{friedman2001greedy,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  year={2001},
}

@phdthesis{schmidhuber1987evolutionary,
  title={Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook},
  author={Schmidhuber, J{\"u}rgen},
  year={1987},
  school={Technische Universit{\"a}t M{\"u}nchen}
}

@inproceedings{hochreiter2001learning,
  title={Learning to learn using gradient descent},
  author={Hochreiter, Sepp and Younger, A Steven and Conwell, Peter R},
  booktitle={International conference on artificial neural networks (ICANN)},
  year={2001},
}

@article{kirsch2021meta,
  title={Meta learning backpropagation and improving it},
  author={Kirsch, Louis and Schmidhuber, J{\"u}rgen},
  journal={Neural Information Processing Systems (NeurIPS)},
  year={2021}
}