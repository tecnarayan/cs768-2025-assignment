@article{koloskova2022sharper,
  title={Sharper Convergence Guarantees for Asynchronous {SGD} for Distributed and Federated Learning},
  author={Koloskova, Anastasia and Stich, Sebastian U. and Jaggi, Martin},
  journal={arXiv preprint arXiv:2206.08307},
  year={2022}
}

@inproceedings{arjevani2020tight,
  title={A Tight Convergence Analysis for Stochastic Gradient Descent with Delayed Updates},
  author={Arjevani, Yossi and Shamir, Ohad and Srebro, Nathan},
  booktitle={Algorithmic Learning Theory},
  pages={111--132},
  year={2020},
  organization={PMLR}
}


@article{woodworth2018graph,
  title={Graph oracle models, lower bounds, and gaps for parallel stochastic optimization},
  author={Woodworth, Blake E. and Wang, Jialei and Smith, Adam and McMahan, Brendan and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}


@inproceedings{stich2021critical,
  title={Critical Parameters for Scalable Distributed Learning with Large Batches and Asynchronous Updates},
  author={Stich, Sebastian U. and Mohtashami, Amirkeivan and Jaggi, Martin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4042--4050},
  year={2021},
  organization={PMLR}
}

@article{stich2020error,
  title={The Error-Feedback Framework: Better Rates for {SGD} with Delayed Gradients and Compressed Updates},
  author={Stich, Sebastian U. and Karimireddy, Sai Praneeth},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--36},
  year={2020}
}


@article{lian2015asynchronous,
  title={Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization},
  author={Lian, Xiangru and Huang, Yijun and Li, Yuncheng and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={28},
  year={2015}
}

@article{leblond2018improved,
  title={Improved Asynchronous Parallel Optimization Analysis for Stochastic Incremental Methods},
  author={Leblond, R{\'e}mi and Pedregosa, Fabian and Lacoste-Julien, Simon},
  journal={Journal of Machine Learning Research},
  volume={19},
  pages={1--68},
  year={2018}
}

@article{recht2011hogwild,
  title={Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@inproceedings{gower2019sgd,
	title={{SGD}: General analysis and improved rates},
	author={Gower, Robert M. and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richt{\'a}rik, Peter},
	booktitle={International Conference on Machine Learning},
	pages={5200--5209},
	year={2019}
}

@article{agarwal2011distributed,
  title={Distributed Delayed Stochastic Optimization},
  author={Agarwal, Alekh and Duchi, John C.},
  journal={Advances in Neural Information Processing Systems},
  volume={24},
  year={2011}
}

@article{nedic2001distributed,
  title={Distributed asynchronous incremental subgradient methods},
  author={Nedi{\'c}, Angelia and Bertsekas, Dimitri P. and Borkar, Vivek S.},
  journal={Studies in Computational Mathematics},
  volume={8},
  number={C},
  pages={381--407},
  year={2001},
  publisher={Elsevier}
}

@article{khaled2019better,
  author={Khaled, Ahmed and Richt\'{a}rik, Peter},
  journal={arXiv Preprint arXiv:2002.03329},
  title={Better theory for {SGD} in the nonconvex world},
  year={2020},
}

@inproceedings{alistarh2018convergence,
  title={The convergence of stochastic gradient descent in asynchronous shared memory},
  author={Alistarh, Dan and De Sa, Christopher and Konstantinov, Nikola},
  booktitle={Proceedings of the 2018 ACM Symposium on Principles of Distributed Computing},
  pages={169--178},
  year={2018}
}

@article{mania2017perturbed,
  author={Mania, Horia and Pan, Xinghao and Papailiopoulos, Dimitris and Recht, Benjamin and Ramchandran, Kannan and Jordan, Michael I.},
  title={Perturbed Iterate Analysis for Asynchronous Stochastic Optimization},
  journal={SIAM Journal on Optimization},
  volume={27},
  number={4},
  pages={2202-2229},
  year={2017},
  doi={10.1137/16M1057000},
}

@InProceedings{aviv21asynchronous,
  title={Asynchronous Distributed Learning: Adapting to Gradient Delays without Prior Knowledge},
  author={Aviv, Rotem Zamir and Hakimi, Ido and Schuster, Assaf and Levy, Kfir Yehuda},
  booktitle={Proceedings of the International Conference on Machine Learning},
  pages={436--445},
  year={2021},
   volume={139}}

@article{cohen2021asynchronous,
  title={Asynchronous Stochastic Optimization Robust to Arbitrary Delays},
  author={Cohen, Alon and Daniely, Amit and Drori, Yoel and Koren, Tomer and Schain, Mariano},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{convexoptim_bubeck,
year = {2015},
volume = {8},
journal = {Foundations and Trends in Machine Learning},
title = {Convex Optimization: Algorithms and Complexity},
number = {3-4},
pages = {231-357},
author = {Sébastien Bubeck}
}


% Encoding: UTF-8

@article{stich2019unified,
  title={Unified optimal analysis of the (stochastic) gradient method},
  author={Stich, Sebastian U},
  journal={arXiv preprint arXiv:1907.04232},
  year={2019}
}

@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}

@inproceedings{garg2014communication,
  Author = {Garg, Ankit and Ma, Tengyu and Nguyen, Huy},
  Booktitle = {Advances in Neural Information Processing Systems},
  Pages = {2726--2734},
  Title = {On communication cost of distributed statistical estimation and dimensionality},
  Year = {2014}}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{tsitsiklis1987communication,
  title={Communication complexity of convex optimization},
  author={Tsitsiklis, John N and Luo, Zhi-Quan},
  journal={Journal of Complexity},
  volume={3},
  number={3},
  pages={231--243},
  year={1987},
  publisher={Elsevier}
}

@article{rubinstein1990optimization,
  title={Optimization of static simulation models by the score function method},
  author={Rubinstein, Reuven Y and Shapiro, Alexander},
  journal={Mathematics and Computers in Simulation},
  volume={32},
  number={4},
  pages={373--392},
  year={1990},
  publisher={Elsevier}
}

@article{kleijnen1996optimization,
  title={Optimization and sensitivity analysis of computer simulation models by the score function method},
  author={Kleijnen, Jack PC and Rubinstein, Reuven Y},
  journal={European Journal of Operational Research},
  volume={88},
  number={3},
  pages={413--427},
  year={1996},
  publisher={Elsevier}
}

@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}

@article{shamir2017optimal,
  title={An optimal algorithm for bandit and zero-order convex optimization with two-point feedback},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={1703--1713},
  year={2017},
  publisher={JMLR. org}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{shamir2016without,
  title={Without-replacement sampling for stochastic gradient methods},
  author={Shamir, Ohad},
  booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages={46--54},
  year={2016}
}

@article{lee2017distributed,
  title={Distributed stochastic variance reduced gradient methods by sampling extra data with replacement},
  author={Lee, Jason D and Lin, Qihang and Ma, Tengyu and Yang, Tianbao},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4404--4446},
  year={2017},
  publisher={JMLR. org}
}

@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  number={2},
  pages={527--566},
  year={2017},
  publisher={Springer}
}

@article{even2021delays,
  author = {Even, Mathieu and Hendrikx, Hadrien and Massoulié, Laurent},
  journal = {arXiv:2106.03585},
  title={Decentralized Optimization with Heterogeneous Delays: a Continuous-Time Approach}, 
  year = {2021}}

@inproceedings{even2021continuized,
 author = {Even, Mathieu and Berthier, Rapha\"{e}l and Bach, Francis and Flammarion, Nicolas and Hendrikx, Hadrien and Gaillard, Pierre and Massouli\'{e}, Laurent and Taylor, Adrien},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {28054--28066},
 publisher = {Curran Associates, Inc.},
 title = {Continuized Accelerations of Deterministic and Stochastic Gradient Descents, and of Gossip Algorithms},
 url = {https://proceedings.neurips.cc/paper/2021/file/ec26fc2eb2b75aece19c70392dc744c2-Paper.pdf},
 volume = {34},
 year = {2021}
}

@article{arjevani2020complexity,
  title={On the Complexity of Minimizing Convex Finite Sums Without Using the Indices of the Individual Functions},
  author={Arjevani, Yossi and Daniely, Amit and Jegelka, Stefanie and Lin, Hongzhou},
  journal={arXiv preprint arXiv:2002.03273},
  year={2020}
}

@inproceedings{agarwal2010optimal,
  title={Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback.},
  author={Agarwal, Alekh and Dekel, Ofer and Xiao, Lin},
  booktitle={COLT},
  pages={28--40},
  year={2010},
  organization={Citeseer}
}

@article{duchi2015optimal,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}

@article{nguyen2019new,
  title={New Convergence Aspects of Stochastic Gradient Algorithms.},
  author={Nguyen, Lam M and Nguyen, Phuong Ha and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak{\'a}c, Martin and van Dijk, Marten},
  journal={Journal of Machine Learning Research},
  volume={20},
  number={176},
  pages={1--49},
  year={2019}
}

@article{hanzely2021personalized,
  title={Personalized Federated Learning: A Unified Framework and Universal Optimization Techniques},
  author={Hanzely, Filip and Zhao, Boxin and Kolar, Mladen},
  journal={arXiv preprint arXiv:2102.09743},
  year={2021}
}

@book{bertsekas1989parallel,
  title={Parallel and distributed computation: numerical methods},
  author={Bertsekas, Dimitri P and Tsitsiklis, John N},
  volume={23},
  year={1989},
  publisher={Prentice hall Englewood Cliffs, NJ}
}

@article{nedic2009distributed,
  title={Distributed Subgradient Methods for Multi-Agent Optimization},
  author={Nedi{\'c}, Angelia and Ozdaglar, Asuman},
  journal={IEEE Transactions on Automatic Control},
  volume={54},
  number={1},
  pages={48--61},
  year={2009},
  publisher={IEEE}
}

@article{nedic2010constrained,
  title={Constrained consensus and optimization in multi-agent networks},
  author={Nedic, Angelia and Ozdaglar, Asuman and Parrilo, Pablo A},
  journal={IEEE Transactions on Automatic Control},
  volume={55},
  number={4},
  pages={922--938},
  year={2010},
  publisher={IEEE}
}

@article{ram2010distributed,
  title={Distributed stochastic subgradient projection algorithms for convex optimization},
  author={Ram, S Sundhar and Nedi{\'c}, Angelia and Veeravalli, Venugopal V},
  journal={Journal of optimization theory and applications},
  volume={147},
  number={3},
  pages={516--545},
  year={2010},
  publisher={Springer}
}

@article{boyd2011distributed,
  title={Distributed optimization and statistical learning via the alternating direction method of multipliers},
  author={Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
  journal={Foundations and Trends{\textregistered} in Machine learning},
  volume={3},
  number={1},
  pages={1--122},
  year={2011},
  publisher={Now Publishers Inc.}
}

@book{carmon2020complexity,
  title={The Complexity of Optimization Beyond Convexity},
  author={Carmon, Yair},
  year={2020},
  publisher={Stanford University}
}

@article{defazio2014saga,
  title={SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives},
  author={Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  journal={arXiv preprint arXiv:1407.0202},
  year={2014}
}

@article{schmidt2017minimizing,
  title={Minimizing finite sums with the stochastic average gradient},
  author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
  journal={Mathematical Programming},
  volume={162},
  number={1-2},
  pages={83--112},
  year={2017},
  publisher={Springer}
}

@inproceedings{zeyuan2016optimal,
 author = {Allen-Zhu, Zeyuan and Hazan, Elad},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Optimal Black-Box Reductions Between Optimization Objectives},
 url = {https://proceedings.neurips.cc/paper/2016/file/1f50893f80d6830d62765ffad7721742-Paper.pdf},
 volume = {29},
 year = {2016}
}

@article{liu2018mass,
  title={Mass: an accelerated stochastic method for over-parametrized learning},
  author={Liu, Chaoyue and Belkin, Mikhail},
  journal={arXiv preprint arXiv:1810.13395},
  year={2018}
}

@article{ball1997elementary,
  title={An elementary introduction to modern convex geometry},
  author={Ball, Keith and others},
  journal={Flavors of geometry},
  volume={31},
  pages={1--58},
  year={1997}
}

@book{yairthesis,
  title={The Complexity of Optimization Beyond Convexity},
  author={Carmon, Yair},
  year={2020},
  publisher={Stanford University}
}

@inproceedings{koloskova2020unified,
  title={A unified theory of decentralized SGD with changing topology and local updates},
  author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian},
  booktitle={International Conference on Machine Learning},
  pages={5381--5393},
  year={2020},
  organization={PMLR}
}

@article{karimireddy2018global,
  title={Global linear convergence of Newton's method without strong-convexity or Lipschitz gradients},
  author={Karimireddy, Sai Praneeth and Stich, Sebastian U and Jaggi, Martin},
  journal={arXiv preprint arXiv:1806.00413},
  year={2018}
}

@article{mangasarian1994backpropagation,
  title={Backpropagation convergence via deterministic nonmonotone perturbed minimization},
  author={Mangasarian, Olvi L and Solodov, Mikhail V},
  journal={Advances in Neural Information Processing Systems},
  pages={383--383},
  year={1994},
  publisher={Morgan Kaufmann Publishers}
}

@article{carmon2020acceleration,
  title={Acceleration with a Ball Optimization Oracle},
  author={Carmon, Yair and Jambulapati, Arun and Jiang, Qijia and Jin, Yujia and Lee, Yin Tat and Sidford, Aaron and Tian, Kevin},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{bach2010self,
  title={Self-concordant analysis for logistic regression},
  author={Bach, Francis and others},
  journal={Electronic Journal of Statistics},
  volume={4},
  pages={384--414},
  year={2010},
  publisher={The Institute of Mathematical Statistics and the Bernoulli Society}
}

@inproceedings{bullins2020highly,
  title={Highly smooth minimization of non-smooth problems},
  author={Bullins, Brian},
  booktitle={Conference on Learning Theory},
  pages={988--1030},
  year={2020},
  organization={PMLR}
}

@article{nesterov2019implementable,
  title={Implementable tensor methods in unconstrained convex optimization},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  pages={1--27},
  year={2019},
  publisher={Springer}
}

@article{nesterov2006cubic,
  title={Cubic regularization of Newton method and its global performance},
  author={Nesterov, Yurii and Polyak, Boris T},
  journal={Mathematical Programming},
  volume={108},
  number={1},
  pages={177--205},
  year={2006},
  publisher={Springer}
}

@book{nocedal2006numerical,
  title={Numerical optimization},
  author={Nocedal, Jorge and Wright, Stephen},
  year={2006},
  publisher={Springer Science \& Business Media}
}

@book{nesterov1994interior,
  title={Interior-point polynomial algorithms in convex programming},
  author={Nesterov, Yurii and Nemirovskii, Arkadii},
  year={1994},
  publisher={SIAM}
}

@article{nesterov1998introductory,
  title={Introductory lectures on convex programming volume i: Basic course},
  author={Nesterov, Yurii},
  journal={Lecture notes},
  volume={3},
  number={4},
  pages={5},
  year={1998}
}

@article{woodworth2020minibatch,
  title={Minibatch vs local sgd for heterogeneous distributed learning},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Srebro, Nathan},
  journal={arXiv preprint arXiv:2006.04735},
  year={2020}
}

@inproceedings{yuan2020federated,
  title={Federated accelerated stochastic gradient descent},
  author={Yuan, Honglin and Ma, Tengyu},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2020},
}

@inproceedings{woodworth2020local,
  title={Is local {SGD} better than minibatch {SGD}?},
  author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={10334--10343},
  year={2020},
  organization={PMLR}
}

@inproceedings{drori2020complexity,
  title={The complexity of finding stationary points with stochastic gradient descent},
  author={Drori, Yoel and Shamir, Ohad},
  booktitle={International Conference on Machine Learning},
  pages={2658--2667},
  year={2020},
  organization={PMLR}
}

@inproceedings{dunner2018distributed,
  title={A distributed second-order algorithm you can trust},
  author={D{\"u}nner, Celestine and Lucchi, Aurelien and Gargiani, Matilde and Bian, An and Hofmann, Thomas and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={1358--1366},
  year={2018},
  organization={PMLR}
}

@inproceedings{arjevani2015communication,
  title={Communication complexity of distributed convex learning and optimization},
  author={Arjevani, Yossi and Shamir, Ohad},
  booktitle={Advances in neural information processing systems},
  pages={1756--1764},
  year={2015}
}

@article{arjevani2019lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C. and Foster, Dylan J. and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  pages={1--50},
  year={2022},
  publisher={Springer}
}

@inproceedings{allen2018natasha,
  title={Natasha 2: faster non-convex optimization than SGD},
  author={Allen-Zhu, Zeyuan},
  booktitle={Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages={2680--2691},
  year={2018}
}

@article{carmon2018accelerated,
  title={Accelerated methods for nonconvex optimization},
  author={Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  journal={SIAM Journal on Optimization},
  volume={28},
  number={2},
  pages={1751--1772},
  year={2018},
  publisher={SIAM}
}

@inproceedings{agarwal2017finding,
  title={Finding approximate local minima faster than gradient descent},
  author={Agarwal, Naman and Allen-Zhu, Zeyuan and Bullins, Brian and Hazan, Elad and Ma, Tengyu},
  booktitle={Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1195--1199},
  year={2017}
}

@article{agarwal2017second,
  title={Second-order stochastic optimization for machine learning in linear time},
  author={Agarwal, Naman and Bullins, Brian and Hazan, Elad},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={4148--4187},
  year={2017},
  publisher={JMLR. org}
}

@inproceedings{zhang2015disco,
  title={DiSCO: Distributed optimization for self-concordant empirical loss},
  author={Zhang, Yuchen and Xiao, Lin},
  booktitle={International Conference on Machine Learning},
  pages={362--370},
  year={2015},
  organization={PMLR}
}

@inproceedings{shamir2014communication,
  title={Communication-efficient distributed optimization using an approximate newton-type method},
  author={Shamir, Ohad and Srebro, Nati and Zhang, Tong},
  booktitle={International Conference on Machine Learning},
  pages={1000--1008},
  year={2014},
  organization={PMLR}
}

@article{pearlmutter1994fast,
  title={Fast exact multiplication by the Hessian},
  author={Pearlmutter, Barak A},
  journal={Neural Computation},
  volume={6},
  number={1},
  pages={147--160},
  year={1994},
  publisher={MIT Press}
}

@inproceedings{mcmahan2010adaptive,
  author    = {H. Brendan McMahan and
               Matthew J. Streeter},
  title     = {Adaptive Bound Optimization for Online Convex Optimization},
  booktitle = {{COLT} 2010 - The 23rd Conference on Learning Theory, Haifa, Israel,
               June 27-29, 2010},
  pages     = {244--256},
  year      = {2010},
  url       = {http://colt2010.haifa.il.ibm.com/papers/COLT2010proceedings.pdf\#page=252},
  timestamp = {Tue, 19 Feb 2013 17:09:31 +0100},
  biburl    = {https://dblp.org/rec/bib/conf/colt/McMahanS10},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{vandenbergheLecture,
  author={Lieven Vandenberghe},
  title={Lecture Notes 1 for Optimization Methods for Large-Scale Systems},
  year={2019},
  publisher={UCLA}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@article{karimireddy2019scaffold,
  title={{SCAFFOLD}: Stochastic controlled averaging for on-device federated learning},
  author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
  journal={arXiv preprint arXiv:1910.06378},
  year={2019}
}

@inproceedings{mcmahan2017communication,
  title={Communication-Efficient Learning of Deep Networks from Decentralized Data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial intelligence and statistics},
  pages={1273--1282},
  year={2017},
  organization={PMLR}
}

@Article{zhang2013communication,
  author    = {Zhang, Yuchen and Duchi, John C and Wainwright, Martin J},
  title     = {Communication-efficient algorithms for statistical optimization},
  journal   = {The Journal of Machine Learning Research},
  year      = {2013},
  volume    = {14},
  number    = {1},
  pages     = {3321--3363},
  publisher = {JMLR. org},
}

@InProceedings{zhang2013divide,
  author    = {Zhang, Yuchen and Duchi, John and Wainwright, Martin},
  title     = {Divide and conquer kernel ridge regression},
  booktitle = {Conference on learning theory},
  year      = {2013},
  pages     = {592--617},
}

@Misc{kairouz2019advances,
  author  = {Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Keith Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D'Oliveira and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konečný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Mariana Raykova and Hang Qi and Daniel Ramage and Ramesh Raskar and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
  title   = {Advances and Open Problems in Federated Learning},
  year    = {2019},
  journal = {arXiv preprint arXiv:1912.04977},
}

@Article{simchowitz2018randomized,
  author  = {Simchowitz, Max},
  title   = {On the randomized complexity of minimizing a convex quadratic function},
  journal = {arXiv preprint arXiv:1807.09386},
  year    = {2018},
}

@InProceedings{zinkevich2010parallelized,
  author    = {Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex J.},
  title     = {Parallelized stochastic gradient descent},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2010},
  pages     = {2595--2603},
}

@InProceedings{liu2014distributed,
  author    = {Liu, Qiang and Ihler, Alexander T},
  title     = {Distributed estimation, information loss and exponential families},
  booktitle = {Advances in neural information processing systems},
  year      = {2014},
  pages     = {1098--1106},
}

@Article{rosenblatt2016optimality,
  author    = {Rosenblatt, Jonathan D and Nadler, Boaz},
  title     = {On the optimality of averaging in distributed statistical learning},
  journal   = {Information and Inference: A Journal of the IMA},
  year      = {2016},
  volume    = {5},
  number    = {4},
  pages     = {379--404},
  publisher = {Oxford University Press},
}

@Article{godichon2017rates,
  author  = {Godichon-Baggioni, Antoine and Saadane, Sofiane},
  title   = {On the rates of convergence of parallelized averaged stochastic gradient algorithms},
  journal = {arXiv preprint arXiv:1710.07926},
  year    = {2017},
}

@Article{jain2017parallelizing,
  author    = {Jain, Prateek and Netrapalli, Praneeth and Kakade, Sham M and Kidambi, Rahul and Sidford, Aaron},
  title     = {Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  journal   = {The Journal of Machine Learning Research},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {8258--8299},
  publisher = {JMLR. org},
}

@Article{zhou2017convergence,
  author  = {Zhou, Fan and Cong, Guojing},
  title   = {On the convergence properties of a $ K $-step averaging stochastic gradient descent algorithm for nonconvex optimization},
  journal = {arXiv preprint arXiv:1708.01012},
  year    = {2017},
}

@InProceedings{yu2019parallel,
  author    = {Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  title     = {Parallel restarted SGD with faster convergence and less communication: Demystifying why model averaging works for deep learning},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  year      = {2019},
  volume    = {33},
  pages     = {5693--5700},
}

@Article{wang2018cooperative,
  author  = {Wang, Jianyu and Joshi, Gauri},
  title   = {Cooperative SGD: A unified framework for the design and analysis of communication-efficient SGD algorithms},
  journal = {arXiv preprint arXiv:1808.07576},
  year    = {2018},
}

@InProceedings{haddadpour2019trading,
  author    = {Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  title     = {Trading redundancy for communication: Speeding up distributed SGD for non-convex optimization},
  booktitle = {International Conference on Machine Learning},
  year      = {2019},
  pages     = {2545--2554},
}

@InProceedings{basu2019qsparse,
  author    = {Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  title     = {Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification and Local Computations},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019},
  pages     = {14668--14679},
}

@InProceedings{haddadpour2019local,
  author    = {Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mahdavi, Mehrdad and Cadambe, Viveck},
  title     = {Local SGD with periodic averaging: Tighter analysis and adaptive synchronization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019},
  pages     = {11080--11092},
}


@inproceedings{khaled2020tighter,
  title={Tighter theory for local {SGD} on identical and heterogeneous data},
  author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={4519--4529},
  year={2020},
  organization={PMLR}
}

@Article{khaled2019first,
  author  = {Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
  title   = {First analysis of local gd on heterogeneous data},
  journal = {arXiv preprint arXiv:1909.04715},
  year    = {2019},
}

@InProceedings{dieuleveut2019communication,
  author    = {Dieuleveut, Aymeric and Patel, Kumar Kshitij},
  title     = {Communication trade-offs for Local-SGD with large step size},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2019},
  pages     = {13579--13590},
}

@Article{zhang2016parallel,
  author  = {Zhang, Jian and De Sa, Christopher and Mitliagkas, Ioannis and R{\'e}, Christopher},
  title   = {Parallel SGD: When does averaging help?},
  journal = {arXiv preprint arXiv:1606.07365},
  year    = {2016},
}

@Article{lacoste2012simpler,
  author  = {Lacoste-Julien, Simon and Schmidt, Mark and Bach, Francis},
  title   = {A simpler approach to obtaining an o (1/t) convergence rate for the projected stochastic subgradient method},
  journal = {arXiv preprint arXiv:1212.2002},
  year    = {2012},
}

@Article{DBLP:journals/corr/ArjevaniS15,
  author        = {Yossi Arjevani and Ohad Shamir},
  title         = {Communication Complexity of Distributed Convex Learning and Optimization},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1506.01900},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/ArjevaniS15},
  eprint        = {1506.01900},
  timestamp     = {Mon, 13 Aug 2018 16:47:48 +0200},
  url           = {http://arxiv.org/abs/1506.01900},
}

@InProceedings{li2014efficient,
  author       = {Li, Mu and Zhang, Tong and Chen, Yuqiang and Smola, Alexander J.},
  title        = {Efficient mini-batch training for stochastic optimization},
  booktitle    = {Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining},
  year         = {2014},
  pages        = {661--670},
  organization = {ACM},
}

@Article{ghadimi2012optimal,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  title     = {Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  journal   = {SIAM Journal on Optimization},
  year      = {2012},
  volume    = {22},
  number    = {4},
  pages     = {1469--1492},
  publisher = {SIAM},
}

@InProceedings{zhang2012communication,
  author    = {Zhang, Yuchen and Wainwright, Martin J and Duchi, John C},
  title     = {Communication-efficient algorithms for statistical optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2012},
  pages     = {1502--1510},
}

@InProceedings{bach2013non,
  author    = {Bach, Francis and Moulines, Eric},
  title     = {Non-strongly-convex smooth stochastic approximation with convergence rate O (1/n)},
  booktitle = {Advances in neural information processing systems},
  year      = {2013},
  pages     = {773--781},
}

@Article{dieuleveut2017harder,
  author    = {Dieuleveut, Aymeric and Flammarion, Nicolas and Bach, Francis},
  title     = {Harder, better, faster, stronger convergence rates for least-squares regression},
  journal   = {The Journal of Machine Learning Research},
  year      = {2017},
  volume    = {18},
  number    = {1},
  pages     = {3520--3570},
  publisher = {JMLR. org},
}

@InProceedings{defossez2015averaged,
  author    = {D{\'e}fossez, Alexandre and Bach, Francis},
  title     = {Averaged least-mean-squares: Bias-variance trade-offs and optimal sampling distributions},
  booktitle = {Artificial Intelligence and Statistics},
  year      = {2015},
  pages     = {205--213},
}

@Article{jain2018parallelizing,
  author  = {Jain, Prateek and Sidford, Aaron},
  title   = {Parallelizing stochastic gradient descent for least squares regression: mini-batching, averaging, and model misspecification},
  journal = {Journal of Machine Learning Research},
  year    = {2018},
  volume  = {18},
  number  = {223},
  pages   = {1--42},
}

@article{vavasis1993black,
  title={Black-box complexity of local minimization},
  author={Vavasis, Stephen A},
  journal={{SIAM} Journal on Optimization},
  volume={3},
  number={1},
  pages={60--80},
  year={1993},
  publisher={SIAM}
}

@inproceedings{fang2018spider,
  title={Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator},
  author={Fang, Cong and Li, Chris Junchi and Lin, Zhouchen and Zhang, Tong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={689--699},
  year={2018}
}

@InProceedings{fang2019sharp,
  title =    {Sharp Analysis for Nonconvex {SGD} Escaping from Saddle 
  Points},
  author =   {Fang, Cong and Lin, Zhouchen and Zhang, Tong},
  booktitle =    {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages =    {1192--1234},
  year =   {2019},
  editor =   {Beygelzimer, Alina and Hsu, Daniel},
  volume =   {99},
  publisher =    {PMLR}
}

@article{goyal2017accurate,
  title={Accurate, Large Minibatch {SGD}: Training ImageNet in 1 Hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@InProceedings{takac2013mini,
  author = {Tak{\'a}c, Martin and Bijral, Avleen Singh and Richt{\'a}rik, Peter and Srebro, Nati},
  title  = {Mini-Batch Primal and Dual Methods for {SVMs}},
}

@Article{dekel2012optimal,
  author  = {Dekel, Ofer and Gilad-Bachrach, Ran and Shamir, Ohad and Xiao, Lin},
  title   = {Optimal distributed online prediction using mini-batches},
  journal = {Journal of Machine Learning Research},
  year    = {2012},
  volume  = {13},
  number  = {Jan},
  pages   = {165--202},
}

@Article{ghadimi2013optimal,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  title     = {Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization, II: shrinking procedures and optimal algorithms},
  journal   = {SIAM Journal on Optimization},
  year      = {2013},
  volume    = {23},
  number    = {4},
  pages     = {2061--2089},
  publisher = {SIAM},
}

@Article{woodworth2017lower,
  author  = {Woodworth, Blake and Srebro, Nathan},
  title   = {Lower Bound for Randomized First Order Convex Optimization},
  journal = {arXiv preprint arXiv:1709.03594},
  year    = {2017},
}

@Article{davis2018stochastic,
  author  = {Davis, Damek and Drusvyatskiy, Dmitriy and Kakade, Sham and Lee, Jason D},
  title   = {Stochastic subgradient method converges on tame functions},
  journal = {arXiv preprint arXiv:1804.07795},
  year    = {2018},
  url     = {https://arxiv.org/abs/1804.07795},
}

@Article{foster2018uniform,
  author  = {Foster, Dylan J and Sekhari, Ayush and Sridharan, Karthik},
  title   = {Uniform convergence of gradients for non-convex learning and optimization},
  journal = {arXiv preprint arXiv:1810.11059},
  year    = {2018},
  url     = {https://arxiv.org/abs/1810.11059},
}

@InCollection{woodworth16tight,
  author    = {Woodworth, Blake E and Srebro, Nati},
  title     = {Tight Complexity Bounds for Optimizing Composite Objectives},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {3639--3647},
  url       = {http://papers.nips.cc/paper/6058-tight-complexity-bounds-for-optimizing-composite-objectives.pdf},
}

@Article{davis2018uniform,
  author  = {Davis, Damek and Drusvyatskiy, Dmitriy},
  title   = {Uniform Graphical Convergence of Subgradients in Nonconvex Optimization and Learning},
  journal = {arXiv preprint arXiv:1810.07590},
  year    = {2018},
  url     = {https://arxiv.org/abs/1810.07590},
}

@Book{bauschke2011convex,
  title     = {Convex analysis and monotone operator theory in Hilbert spaces},
  publisher = {Springer},
  author    = {Bauschke, Heinz H and Combettes, Patrick L and others},
  volume    = {408},
  year      = {2011}
}

@inproceedings{diakonikolas2019lower,
  title={Lower bounds for parallel and randomized convex optimization},
  author={Diakonikolas, Jelena and Guzm{\'a}n, Crist{\'o}bal},
  booktitle={Conference on Learning Theory},
  pages={1132--1157},
  year={2019},
  organization={PMLR}
}

@Article{lan2012optimal,
  author    = {Lan, Guanghui},
  title     = {An optimal method for stochastic composite optimization},
  journal   = {Mathematical Programming},
  year      = {2012},
  volume    = {133},
  number    = {1-2},
  pages     = {365--397},
  publisher = {Springer},
  url       = {https://pdfs.semanticscholar.org/1621/f05894ad5fd6a8fcb8827a8c7aca36c81775.pdf},
}

@InProceedings{cotter2011better,
  author    = {Cotter, Andrew and Shamir, Ohad and Srebro, Nati and Sridharan, Karthik},
  title     = {Better Mini-Batch Algorithms via Accelerated Gradient Methods},
  booktitle = {Advances in Neural Information Processing Systems 24},
  year      = {2011},
 
  pages     = {1647--1655},
  
}

@InProceedings{agarwal2009information,
  author    = {Agarwal, Alekh and Wainwright, Martin J. and Bartlett, Peter L. and Ravikumar, Pradeep K.},
  title     = {Information-theoretic lower bounds on the oracle complexity of convex optimization},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2009},
  pages     = {1--9},
  url       = {http://papers.nips.cc/paper/3689-information-theoretic-lower-bounds-on-the-oracle-complexity-of-convex-optimization.pdf},
}

@InProceedings{ge2015escaping,
  author    = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  title     = {Escaping from saddle points—online stochastic gradient for tensor decomposition},
  booktitle = {Conference on Learning Theory},
  year      = {2015},
  pages     = {797--842},
  url       = {https://arxiv.org/abs/1503.02101},
}

@InCollection{NIPS2016_6048,
  author    = {Ge, Rong and Lee, Jason D and Ma, Tengyu},
  title     = {Matrix Completion has No Spurious Local Minimum},
  booktitle = {Advances in Neural Information Processing Systems 29},
  publisher = {Curran Associates, Inc.},
  year      = {2016},
  editor    = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
  pages     = {2973--2981},
  url       = {http://papers.nips.cc/paper/6048-matrix-completion-has-no-spurious-local-minimum.pdf},
}

@InProceedings{johnson2013accelerating,
  author    = {Johnson, Rie and Zhang, Tong},
  title     = {Accelerating stochastic gradient descent using predictive variance reduction},
  booktitle = {Advances in neural information processing systems},
  year      = {2013},
  pages     = {315--323},
  url       = {https://papers.nips.cc/paper/4937-accelerating-stochastic-gradient-descent-using-predictive-variance-reduction.pdf},
}

@Article{zhou2018stochastic,
  author  = {Zhou, Dongruo and Xu, Pan and Gu, Quanquan},
  title   = {Stochastic Nested Variance Reduction for Nonconvex Optimization},
  journal = {arXiv preprint arXiv:1806.07811},
  year    = {2018},
  url     = {https://arxiv.org/abs/1806.07811},
}

@Article{jin2017escape,
  author  = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M and Jordan, Michael I},
  title   = {How to escape saddle points efficiently},
  journal = {arXiv preprint arXiv:1703.00887},
  year    = {2017},
  url     = {https://arxiv.org/abs/1703.00887},
}

@Article{DBLP:abs-1801-02982,
  author        = {Zeyuan Allen{-}Zhu},
  title         = {How To Make the Gradients Small Stochastically},
  journal       = {CoRR},
  year          = {2018},
  volume        = {abs/1801.02982},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1801-02982},
  eprint        = {1801.02982},
  timestamp     = {Mon, 13 Aug 2018 16:49:15 +0200},
  url           = {http://arxiv.org/abs/1801.02982},
}

@Article{DBLP:GhadimiL16,
  author     = {Saeed Ghadimi and Guanghui Lan},
  title      = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  journal    = {Math. Program.},
  year       = {2016},
  volume     = {156},
  number     = {1-2},
  pages      = {59--99},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/bib/journals/mp/GhadimiL16},
  doi        = {10.1007/s10107-015-0871-8},
  journalurl = {https://doi.org/10.1007/s10107-015-0871-8},
  timestamp  = {Sat, 27 May 2017 14:25:32 +0200}
}

@Article{nesterov2012make,
  author  = {Nesterov, Yurii},
  title   = {How to make the gradients small},
  journal = {Optima},
  year    = {2012},
  volume  = {88},
  pages   = {10--11},
  url     = {http://www.mathopt.org/Optima-Issues/optima88.pdf},
}

@Article{allen2017natasha2,
  author  = {Allen-Zhu, Zeyuan},
  title   = {Natasha 2: Faster non-convex optimization than sgd},
  journal = {arXiv preprint arXiv:1708.08694},
  year    = {2017},
  url     = {https://arxiv.org/abs/1708.08694},
}

@Article{allen2017neon2,
  author  = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
  title   = {Neon2: Finding Local Minima via First-Order Oracles},
  journal = {arXiv preprint arXiv:1711.06673},
  year    = {2017},
  url     = {https://arxiv.org/abs/1711.06673},
}

@InProceedings{duchi18a,
  author    = {Duchi, John and Ruan, Feng and Yun, Chulhee},
  title     = {Minimax Bounds on Stochastic Batched Convex Optimization},
  booktitle = {Proceedings of the 31st Conference On Learning Theory},
  year      = {2018},
  editor    = {Bubeck, S\'ebastien and Perchet, Vianney and Rigollet, Philippe},
  volume    = {75},
  series    = {Proceedings of Machine Learning Research},
  pages     = {3065--3162},
  month     = {06--09 Jul},
  publisher = {PMLR},
  url       = {http://proceedings.mlr.press/v75/duchi18a.html},
}

@Article{carmon2018accelerated,
  author    = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  title     = {Accelerated methods for nonconvex optimization},
  journal   = {SIAM Journal on Optimization},
  year      = {2018},
  volume    = {28},
  number    = {2},
  pages     = {1751--1772},
  publisher = {SIAM},
  url       = {https://epubs.siam.org/doi/abs/10.1137/17M1114296},
}

@Article{carmon2017lower1,
  author  = {Carmon, Yair and Duchi, John C. and Hinder, Oliver and Sidford, Aaron},
  title   = {Lower bounds for finding stationary points {I}},
  journal = {arXiv preprint arXiv:1710.11606},
  year    = {2017},
 
}

@Article{carmon2017lower2,
  author  = {Carmon, Yair and Duchi, John C and Hinder, Oliver and Sidford, Aaron},
  title   = {Lower Bounds for Finding Stationary Points II: First-Order Methods},
  journal = {arXiv preprint arXiv:1711.00841},
  year    = {2017},
  url     = {https://arxiv.org/abs/1711.00841},
}

@Article{carmon2017convex,
  author  = {Carmon, Yair and Hinder, Oliver and Duchi, John C and Sidford, Aaron},
  title   = {"Convex Until Proven Guilty": Dimension-Free Acceleration of Gradient Descent on Non-Convex Functions},
  journal = {arXiv preprint arXiv:1705.02766},
  year    = {2017},
  url     = {https://arxiv.org/abs/1705.02766},
}

@article{lin2018don,
  title={Don't Use Large Mini-Batches, Use Local SGD},
  author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
  journal={arXiv preprint arXiv:1808.07217},
  year={2018}
}

@inproceedings{stich2018local,
  title={Local {SGD} Converges Fast and Communicates Little},
  author={Stich, Sebastian U.},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@Article{stich2019error,
  author  = {Stich, Sebastian U and Karimireddy, Sai Praneeth},
  title   = {The error-feedback framework: Better rates for SGD with delayed gradients and compressed communication},
  journal = {arXiv preprint arXiv:1909.05350},
  year    = {2019},
}

@InProceedings{ijcai2018-447,
  author    = {Fan Zhou and Guojing Cong},
  title     = {On the Convergence Properties of a K-step Averaging Stochastic Gradient Descent Algorithm for Nonconvex Optimization},
  booktitle = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, {IJCAI-18}},
  year      = {2018},
  pages     = {3219--3227},
  month     = {7},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  doi       = {10.24963/ijcai.2018/447},
  url       = {https://arxiv.org/abs/1708.01012},
}

@Article{yu2018parallel,
  author  = {Yu, Hao and Yang, Sen and Zhu, Shenghuo},
  title   = {Parallel Restarted SGD for Non-Convex Optimization with Faster Convergence and Less Communication},
  journal = {arXiv preprint arXiv:1807.06629},
  year    = {2018},
  url     = {https://arxiv.org/abs/1807.06629},
}

@Article{wang2017memory,
  author  = {Wang, Jialei and Wang, Weiran and Srebro, Nathan},
  title   = {Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch-Prox},
  journal = {arXiv preprint arXiv:1702.06269},
  year    = {2017},
  url     = {https://arxiv.org/abs/1702.06269},
}

@inproceedings{braverman2016communication,
  title={Communication lower bounds for statistical estimation problems via a distributed data processing inequality},
  author={Braverman, Mark and Garg, Ankit and Ma, Tengyu and Nguyen, Huy L and Woodruff, David P},
  booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
  pages={1011--1020},
  year={2016}
}

@inproceedings{zhang2013information,
  title={Information-theoretic lower bounds for distributed statistical estimation with communication constraints.},
  author={Zhang, Yuchen and Duchi, John C and Jordan, Michael I and Wainwright, Martin J},
  booktitle={NIPS},
  pages={2328--2336},
  year={2013},
  organization={Citeseer}
}

@INPROCEEDINGS{shamir2014distributed,
  author={O. {Shamir} and N. {Srebro}},
  booktitle={2014 52nd Annual Allerton Conference on Communication, Control, and Computing (Allerton)}, 
  title={Distributed stochastic optimization and learning}, 
  year={2014},
  volume={},
  number={},
  pages={850-857},
  doi={10.1109/ALLERTON.2014.7028543}}


@InProceedings{lei2017non,
  author    = {Lei, Lihua and Ju, Cheng and Chen, Jianbo and Jordan, Michael I},
  title     = {Non-convex finite-sum optimization via scsg methods},
  booktitle = {Advances in Neural Information Processing Systems},
  year      = {2017},
  pages     = {2348--2358},
  url       = {https://arxiv.org/abs/1706.09156},
}

@Article{BabanezhadAVSKS15,
  author        = {Reza Babanezhad and Mohamed Osama Ahmed and Alim Virani and Mark W. Schmidt and Jakub Konecn{\'{y}} and Scott Sallinen},
  title         = {Stop Wasting My Gradients: Practical {SVRG}},
  journal       = {CoRR},
  year          = {2015},
  volume        = {abs/1511.01942},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/BabanezhadAVSKS15},
  eprint        = {1511.01942},
  timestamp     = {Mon, 13 Aug 2018 16:47:08 +0200},
  url           = {http://arxiv.org/abs/1511.01942},
}

@Article{ReddiHSPS16,
  author  = {{Reddi}, S.~J. and {Hefny}, A. and {Sra}, S. and {Poczos}, B. and {Smola}, A.},
  title   = {Stochastic Variance Reduction for Nonconvex Optimization},
  journal = {arXiv preprint arXiv:1603.06160},
  year    = {2016},
  url     = {https://arxiv.org/abs/1603.06160},
}

@InProceedings{reddi2016fast,
  author       = {Reddi, Sashank J and Sra, Suvrit and P{\'o}czos, Barnab{\'a}s and Smola, Alex},
  title        = {Fast incremental method for smooth nonconvex optimization},
  booktitle    = {Decision and Control (CDC), 2016 IEEE 55th Conference on},
  year         = {2016},
  pages        = {1971--1977},
  organization = {IEEE},
  url          = {https://arxiv.org/abs/1603.06159},
}

@Article{WangSrebro17,
  author        = {Weiran Wang and Nathan Srebro},
  title         = {Stochastic Nonconvex Optimization with Large Minibatches},
  journal       = {CoRR},
  year          = {2017},
  volume        = {abs/1709.08728},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/bib/journals/corr/abs-1709-08728},
  eprint        = {1709.08728},
  timestamp     = {Mon, 13 Aug 2018 16:48:38 +0200},
  url           = {http://arxiv.org/abs/1709.08728},
}

@Article{crammer2006online,
  author  = {Crammer, Koby and Dekel, Ofer and Keshet, Joseph and Shalev-Shwartz, Shai and Singer, Yoram},
  title   = {Online passive-aggressive algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2006},
  volume  = {7},
  number  = {Mar},
  pages   = {551--585},
  url     = {http://www.jmlr.org/papers/v7/crammer06a},
}

@Article{ghadimi2013stochastic,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  title     = {Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  journal   = {SIAM Journal on Optimization},
  year      = {2013},
  volume    = {23},
  number    = {4},
  pages     = {2341--2368},
  publisher = {SIAM},
  url       = {https://arxiv.org/abs/1309.5549},
}

@Article{ghadimi2016accelerated,
  author    = {Ghadimi, Saeed and Lan, Guanghui},
  title     = {Accelerated gradient methods for nonconvex nonlinear and stochastic programming},
  journal   = {Mathematical Programming},
  year      = {2016},
  volume    = {156},
  number    = {1-2},
  pages     = {59--99},
  publisher = {Springer},
  url       = {https://arxiv.org/abs/1310.3787},
}

@Article{feyzmahdavian2016asynchronous,
  author    = {Feyzmahdavian, Hamid Reza and Aytekin, Arda and Johansson, Mikael},
  title     = {An asynchronous mini-batch algorithm for regularized stochastic optimization},
  journal   = {IEEE Transactions on Automatic Control},
  year      = {2016},
  volume    = {61},
  number    = {12},
  pages     = {3740--3754},
  publisher = {IEEE},
  url       = {https://arxiv.org/abs/1505.04824},
}

@Article{ghadimi2016mini,
  author    = {Ghadimi, Saeed and Lan, Guanghui and Zhang, Hongchao},
  title     = {Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization},
  journal   = {Mathematical Programming},
  year      = {2016},
  volume    = {155},
  number    = {1-2},
  pages     = {267--305},
  publisher = {Springer},
  url       = {https://arxiv.org/abs/1308.6594},
}

@Article{hardt2015train,
  author  = {Hardt, Moritz and Recht, Benjamin and Singer, Yoram},
  title   = {Train faster, generalize better: Stability of stochastic gradient descent},
  journal = {arXiv preprint arXiv:1509.01240},
  year    = {2015},
  url     = {https://arxiv.org/abs/1509.01240},
}

@Article{mou2017generalization,
  author  = {Mou, Wenlong and Wang, Liwei and Zhai, Xiyu and Zheng, Kai},
  title   = {Generalization bounds of SGLD for non-convex learning: Two theoretical viewpoints},
  journal = {arXiv preprint arXiv:1707.05947},
  year    = {2017},
  url     = {https://arxiv.org/abs/1707.05947},
}

@Article{Murty1987,
  author   = {Murty, Katta G. and Kabadi, Santosh N.},
  title    = {Some NP-complete problems in quadratic and nonlinear programming},
  journal  = {Mathematical Programming},
  year     = {1987},
  volume   = {39},
  number   = {2},
  pages    = {117--129},
  month    = {Jun},
  issn     = {1436-4646},
  abstract = {In continuous variable, smooth, nonconvex nonlinear programming, we analyze the complexity of checking whether(a)a given feasible solution is not a local minimum, and(b)the objective function is not bounded below on the set of feasible solutions.},
  day      = {01},
  doi      = {10.1007/BF02592948},
  url      = {https://doi.org/10.1007/BF02592948},
}

@book{nemirovskyyudin1983,
  author = {Nemirovsky, Arkadii Semenovich and Yudin, David Borisovich},
  title  = {Problem complexity and method efficiency in optimization},
  year   = {1983},
  publisher={Wiley-Interscience}
}

@Article{nesterov2004introductory,
  author    = {Nesterov, Yurii},
  title     = {Introductory lectures on convex optimization: a basic course},
  year      = {2004},
  publisher = {Springer},
}

@PhdThesis{Coppola2015:IPM,
  author = {Greg Coppola},
  title  = {Iterative parameter mixing for distributed large-margin training of structured predictors for natural language processing},
  school = {The University of Edinburgh},
  year   = {2015},
}

@article{horvath2021fjord,
  title={{FjORD}:  Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout},
  author={Horv{\'a}th, Samuel and Laskaridis, Stefanos and Almeida, Mario and Leontiadis, Ilias and Venieris, Stylianos I. and Lane, Nicholas D.},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{ryabinin2021moshpit,
  title={Moshpit {SGD}: Communication-Efficient Decentralized Training on Heterogeneous Unreliable Devices},
  author={Ryabinin, Max and Gorbunov, Eduard and Plokhotnyuk, Vsevolod and Pekhimenko, Gennady},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{dean2012large,
  title={Large Scale Distributed Deep Networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in Neural Information Processing Systems},
  volume={25},
  year={2012}
}


 


@inproceedings{mnih2016asynchronous,
  title={Asynchronous Methods for Deep Reinforcement Learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={1928--1937},
  year={2016}
}

@article{nair2015massively,
  title={Massively Parallel Methods for Deep Reinforcement Learning},
  author={Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and others},
  journal={arXiv preprint arXiv:1507.04296},
  year={2015}
}

@inproceedings{nguyen2022federated,
  title={Federated Learning with Buffered Asynchronous Aggregation},
  author={Nguyen, John and Malik, Kshitiz and Zhan, Hongyuan and Yousefpour, Ashkan and Rabbat, Mike and Malek, Mani and Huba, Dzmitry},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={3581--3607},
  year={2022}
}

@article{chen2016revisiting,
  title={Revisiting distributed synchronous {SGD}},
  author={Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz, Rafal},
  journal={arXiv preprint arXiv:1604.00981},
  year={2016}
}

@inproceedings{chai2021fedat,
  title={{FedAT}: a high-performance and communication-efficient federated learning system with asynchronous tiers},
  author={Chai, Zheng and Chen, Yujing and Anwar, Ali and Zhao, Liang and Cheng, Yue and Rangwala, Huzefa},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2021}
}

@article{assran2020advances,
  title={Advances in Asynchronous Parallel and Distributed Optimization},
  author={Assran, Mahmoud and Aytekin, Arda and Feyzmahdavian, Hamid Reza and Johansson, Mikael and Rabbat, Michael G.},
  journal={Proceedings of the IEEE},
  volume={108},
  number={11},
  pages={2013--2031},
  year={2020},
  publisher={IEEE}
}

@article{baudet1978asynchronous,
  title={Asynchronous Iterative Methods for Multiprocessors},
  author={Baudet, Gerard M.},
  journal={Journal of the ACM (JACM)},
  volume={25},
  number={2},
  pages={226--244},
  year={1978},
  publisher={ACM New York, NY, USA}
}

@article{tsitsiklis1986distributed,
  title={Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms},
  author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
  journal={IEEE transactions on automatic control},
  volume={31},
  number={9},
  pages={803--812},
  year={1986},
  publisher={IEEE}
}

@article{mcmahan2014delay,
  title={Delay-tolerant algorithms for asynchronous distributed online learning},
  author={McMahan, Brendan and Streeter, Matthew J.},
  journal={Advances in Neural Information Processing Systems},
  volume={27},
  year={2014}
}

@inproceedings{joulani2016delay,
  title={Delay-Tolerant Online Convex Optimization: Unified Analysis and Adaptive-Gradient Algorithms},
  author={Joulani, Pooria and Gy{\"o}rgy, Andr{\'a}s and Szepesv{\'a}ri, Csaba},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={30},
  year={2016}
}

@inproceedings{sra2016adadelay,
  title={AdaDelay: Delay Adaptive Distributed Stochastic Optimization},
  author={Sra, Suvrit and Yu, Adams Wei and Li, Mu and Smola, Alexander J.},
  booktitle={Artificial Intelligence and Statistics},
  pages={957--965},
  year={2016},
  organization={PMLR}
}

@inproceedings{zhou2018distributed,
  title={Distributed Asynchronous Optimization with Unbounded Delays: How Slow Can You Go?},
  author={Zhou, Zhengyuan and Mertikopoulos, Panayotis and Bambos, Nicholas and Glynn, Peter and Ye, Yinyu and Li, Li-Jia and Fei-Fei, Li},
  booktitle={International Conference on Machine Learning},
  pages={5970--5979},
  year={2018},
  organization={PMLR}
}

@article{ben2019demystifying,
  title={Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency Analysis},
  author={Ben-Nun, Tal and Hoefler, Torsten},
  journal={ACM Computing Surveys (CSUR)},
  volume={52},
  number={4},
  pages={1--43},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@inproceedings{bernstein2018signsgd,
  title={{signSGD}: Compressed Optimisation for Non-Convex Problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018}
}

@inproceedings{zheng2017asynchronous,
  title={Asynchronous Stochastic Gradient Descent with Delay Compensation},
  author={Zheng, Shuxin and Meng, Qi and Wang, Taifeng and Chen, Wei and Yu, Nenghai and Ma, Zhi-Ming and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={4120--4129},
  year={2017}
}

@inproceedings{mishchenko2018delay,
  title={A Delay-tolerant Proximal-Gradient Algorithm for Distributed Learning},
  author={Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Amini, Massih-Reza},
  booktitle={International Conference on Machine Learning},
  pages={3584--3592},
  year={2018}
}

@article{alistarh2017qsgd,
  title={QSGD: Communication-Efficient SGD via Gradient Quantization and Encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{lian2017can,
  title={Can Decentralized Algorithms Outperform Centralized Algorithms? A Case Study for Decentralized Parallel Stochastic Gradient Descent},
  author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@inproceedings{assran2019stochastic,
  title={Stochastic gradient push for distributed deep learning},
  author={Assran, Mahmoud and Loizou, Nicolas and Ballas, Nicolas and Rabbat, Mike},
  booktitle={International Conference on Machine Learning},
  pages={344--353},
  year={2019},
  organization={PMLR}
}

@article{nadiradze2021asynchronous,
  title={Asynchronous decentralized {SGD} with quantized and local updates},
  author={Nadiradze, Giorgi and Sabour, Amirmojtaba and Davies, Peter and Li, Shigang and Alistarh, Dan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@Comment{jabref-meta: databaseType:bibtex;}


@InProceedings{asaga,
  title = 	 {{ASAGA: Asynchronous Parallel SAGA}},
  author = 	 {Leblond, Rémi and Pedregosa, Fabian and Lacoste-Julien, Simon},
  booktitle = 	 {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics},
  pages = 	 {46--54},
  year = 	 {2017},
  editor = 	 {Singh, Aarti and Zhu, Jerry},
  volume = 	 {54},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {20--22 Apr},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v54/leblond17a/leblond17a.pdf},
  url = 	 {https://proceedings.mlr.press/v54/leblond17a.html}
}

@inproceedings{moritz2018ray,
  title={Ray: A distributed framework for emerging {AI} applications},
  author={Moritz, Philipp and Nishihara, Robert and Wang, Stephanie and Tumanov, Alexey and Liaw, Richard and Liang, Eric and Elibol, Melih and Yang, Zongheng and Paul, William and Jordan, Michael I. and Stoica, Ion},
  booktitle={13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)},
  pages={561--577},
  year={2018}
}

@article{wu2022delay,
  title={Delay-adaptive step-sizes for asynchronous learning},
  author={Wu, Xuyang and Magnusson, Sindri and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
  journal={arXiv preprint arXiv:2202.08550},
  year={2022}
}
