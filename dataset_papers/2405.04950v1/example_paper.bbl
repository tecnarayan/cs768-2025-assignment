\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson, S{\"u}nderhauf, Reid, Gould, and Van Den~Hengel]{anderson2018vision}
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S{\"u}nderhauf, N., Reid, I., Gould, S., and Van Den~Hengel, A.
\newblock Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pp.\  3674--3683, 2018.

\bibitem[Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{bai2023qwen}
Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J.
\newblock Qwen-vl: A frontier large vision-language model with versatile abilities.
\newblock \emph{arXiv preprint arXiv:2308.12966}, 2023.

\bibitem[Chen et~al.(2024)Chen, Pitawela, Zhao, Zhou, Chen, and Wu]{chen2024webvln}
Chen, Q., Pitawela, D., Zhao, C., Zhou, G., Chen, H.-T., and Wu, Q.
\newblock Webvln: Vision-and-language navigation on websites.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pp.\  1165--1173, 2024.

\bibitem[Dai et~al.(2023)Dai, Xiong, and Ku]{dai2023llm}
Dai, S.-C., Xiong, A., and Ku, L.-W.
\newblock Llm-in-the-loop: Leveraging large language model for thematic analysis.
\newblock \emph{arXiv preprint arXiv:2310.15100}, 2023.

\bibitem[Ektefaie et~al.(2022)Ektefaie, Dasoulas, Noori, Farhat, and Zitnik]{ektefaie2022geometric}
Ektefaie, Y., Dasoulas, G., Noori, A., Farhat, M., and Zitnik, M.
\newblock Geometric multimodal representation learning.
\newblock \emph{arXiv preprint arXiv:2209.03299}, 2022.

\bibitem[Gu et~al.(2022)Gu, Stefani, Wu, Thomason, and Wang]{gu2022vision}
Gu, J., Stefani, E., Wu, Q., Thomason, J., and Wang, X.~E.
\newblock Vision-and-language navigation: A survey of tasks, methods, and future directions.
\newblock \emph{arXiv preprint arXiv:2203.12667}, 2022.

\bibitem[Imani et~al.(2023)Imani, Du, and Shrivastava]{imani2023mathprompter}
Imani, S., Du, L., and Shrivastava, H.
\newblock Mathprompter: Mathematical reasoning using large language models.
\newblock \emph{arXiv preprint arXiv:2303.05398}, 2023.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Li, Savarese, and Hoi]{li2023blip2}
Li, J., Li, D., Savarese, S., and Hoi, S.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock \emph{ICML}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Hu, Chen, Ma, and Zhang]{li2023lmeye}
Li, Y., Hu, B., Chen, X., Ma, L., and Zhang, M.
\newblock Lmeye: An interactive perception network for large language models.
\newblock \emph{arXiv preprint arXiv:2305.03701}, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Liu, Wang, Liang, Liu, Wang, Cui, Tu, Wang, and Zhou]{li2023comprehensive_medical}
Li, Y., Liu, Y., Wang, Z., Liang, X., Liu, L., Wang, L., Cui, L., Tu, Z., Wang, L., and Zhou, L.
\newblock A comprehensive study of gpt-4v's multimodal capabilities in medical imaging.
\newblock \emph{medRxiv}, pp.\  2023--11, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Wang, Hu, Chen, Zhong, Lyu, and Zhang]{li2023comprehensive}
Li, Y., Wang, L., Hu, B., Chen, X., Zhong, W., Lyu, C., and Zhang, M.
\newblock A comprehensive evaluation of gpt-4v on knowledge-intensive visual question answering.
\newblock \emph{arXiv preprint arXiv:2311.07536}, 2023{\natexlab{d}}.

\bibitem[Lin et~al.(2023)Lin, Liu, Zhang, Gao, Qiu, Xiao, Qiu, Lin, Shao, Chen, et~al.]{lin2023sphinx}
Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., et~al.
\newblock Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models.
\newblock \emph{arXiv preprint arXiv:2311.07575}, 2023.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{liu2023visual}
Liu, H., Li, C., Wu, Q., and Lee, Y.~J.
\newblock Visual instruction tuning.
\newblock \emph{NeurIPS}, 2023.

\bibitem[Lu et~al.(2023)Lu, Bansal, Xia, Liu, Li, Hajishirzi, Cheng, Chang, Galley, and Gao]{lu2023mathvista}
Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J.
\newblock Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts.
\newblock \emph{arXiv preprint arXiv:2310.02255}, 2023.

\bibitem[Luo et~al.(2023)Luo, Sun, Xu, Zhao, Lou, Tao, Geng, Lin, Chen, and Zhang]{luo2023wizardmath}
Luo, H., Sun, Q., Xu, C., Zhao, P., Lou, J., Tao, C., Geng, X., Lin, Q., Chen, S., and Zhang, D.
\newblock Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct.
\newblock \emph{arXiv preprint arXiv:2308.09583}, 2023.

\bibitem[OpenAI(2023)]{gpt4}
OpenAI.
\newblock Gpt-4 technical report.
\newblock \emph{https://arxiv.org/abs/2303.08774}, 2023.

\bibitem[Qin et~al.(2023)Qin, Liang, Ye, Zhu, Yan, Lu, Lin, Cong, Tang, Qian, et~al.]{qin2023toolllm}
Qin, Y., Liang, S., Ye, Y., Zhu, K., Yan, L., Lu, Y., Lin, Y., Cong, X., Tang, X., Qian, B., et~al.
\newblock Toolllm: Facilitating large language models to master 16000+ real-world apis.
\newblock \emph{arXiv preprint arXiv:2307.16789}, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Wake et~al.(2023)Wake, Kanehira, Sasabuchi, Takamatsu, and Ikeuchi]{wake2023gpt_robotics}
Wake, N., Kanehira, A., Sasabuchi, K., Takamatsu, J., and Ikeuchi, K.
\newblock Gpt-4v (ision) for robotics: Multimodal task planning from human demonstration.
\newblock \emph{arXiv preprint arXiv:2311.12015}, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Feng, He, Tan, Han, and Tsvetkov]{wang2023can}
Wang, H., Feng, S., He, T., Tan, Z., Han, X., and Tsvetkov, Y.
\newblock Can language models solve graph problems in natural language?
\newblock \emph{NeurIPS}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Ma, Feng, Zhang, Yang, Zhang, Chen, Tang, Chen, Lin, et~al.]{wang2023survey_agents}
Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., et~al.
\newblock A survey on large language model based autonomous agents.
\newblock \emph{arXiv preprint arXiv:2308.11432}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{selfinstruct}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language model with self generated instructions, 2022.

\bibitem[Wen et~al.(2023)Wen, Yang, Fu, Wang, Cai, Li, Ma, Li, Xu, Shang, et~al.]{wen2023road}
Wen, L., Yang, X., Fu, D., Wang, X., Cai, P., Li, X., Ma, T., Li, Y., Xu, L., Shang, D., et~al.
\newblock On the road with gpt-4v (ision): Early explorations of visual-language model on autonomous driving.
\newblock \emph{arXiv preprint arXiv:2311.05332}, 2023.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react}
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., and Cao, Y.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Collins, Weller, and Tenenbaum]{zhang2023ai}
Zhang, C.~E., Collins, K.~M., Weller, A., and Tenenbaum, J.~B.
\newblock Ai for mathematics: A cognitive science perspective.
\newblock \emph{arXiv preprint arXiv:2310.13021}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Wang, Cao, Xu, Ouyang, Zhao, Ding, Zhang, Duan, Yan, et~al.]{zhang2023internlm}
Zhang, P., Wang, X. D.~B., Cao, Y., Xu, C., Ouyang, L., Zhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et~al.
\newblock Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition.
\newblock \emph{arXiv preprint arXiv:2309.15112}, 2023{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt}
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
