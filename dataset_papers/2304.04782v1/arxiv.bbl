\begin{thebibliography}{38}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and Norouzi]{Agarwal2020AnOP}
Rishabh Agarwal, D.~Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{ICML}, 2020.

\bibitem[Baker et~al.(2022)Baker, Akkaya, Zhokhov, Huizinga, Tang, Ecoffet,
  Houghton, Sampedro, and Clune]{Baker2022VideoP}
Bowen Baker, Ilge Akkaya, Peter Zhokhov, Joost Huizinga, Jie Tang, Adrien
  Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune.
\newblock Video pretraining (vpt): Learning to act by watching unlabeled online
  videos.
\newblock \emph{ArXiv}, abs/2206.11795, 2022.

\bibitem[Barreto et~al.(2016)Barreto, Dabney, Munos, Hunt, Schaul, Silver, and
  Hasselt]{Barreto2016SuccessorFF}
Andr{\'e} Barreto, Will Dabney, R{\'e}mi Munos, Jonathan~J. Hunt, Tom Schaul,
  David Silver, and H.~V. Hasselt.
\newblock Successor features for transfer in reinforcement learning.
\newblock \emph{ArXiv}, abs/1606.05312, 2016.

\bibitem[{Bellemare} et~al.(2013){Bellemare}, {Naddaf}, {Veness}, and
  {Bowling}]{bellemare13arcade}
M.~G. {Bellemare}, Y.~{Naddaf}, J.~{Veness}, and M.~{Bowling}.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, jun 2013.

\bibitem[Borsa et~al.(2018)Borsa, Barreto, Quan, Mankowitz, Munos, Hasselt,
  Silver, and Schaul]{Borsa2018UniversalSF}
Diana Borsa, Andr{\'e} Barreto, John Quan, Daniel~Jaymin Mankowitz, R{\'e}mi
  Munos, H.~V. Hasselt, David Silver, and Tom Schaul.
\newblock Universal successor features approximators.
\newblock \emph{ArXiv}, abs/1812.07626, 2018.

\bibitem[Chang et~al.(2022)Chang, Gupta, and Gupta]{Chang2022LearningVF}
Matthew Chang, Arjun Gupta, and Saurabh Gupta.
\newblock Learning value functions from undirected state-only experience.
\newblock \emph{ArXiv}, abs/2204.12458, 2022.

\bibitem[Dayan(1993)]{Dayan1993ImprovingGF}
Peter Dayan.
\newblock Improving generalization for temporal difference learning: The
  successor representation.
\newblock \emph{Neural Computation}, 5:\penalty0 613--624, 1993.

\bibitem[Edwards and Isbell(2019)]{Edwards2019PerceptualVF}
Ashley~D. Edwards and Charles~Lee Isbell.
\newblock Perceptual values from observation.
\newblock \emph{ArXiv}, abs/1905.07861, 2019.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{Espeholt2018IMPALASD}
Lasse Espeholt, Hubert Soyer, R{\'e}mi Munos, Karen Simonyan, Volodymyr Mnih,
  Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, Shane Legg, and
  Koray Kavukcuoglu.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock \emph{ArXiv}, abs/1802.01561, 2018.

\bibitem[Eysenbach et~al.(2022)Eysenbach, Zhang, Salakhutdinov, and
  Levine]{Eysenbach2022ContrastiveLA}
Benjamin Eysenbach, Tianjun Zhang, Ruslan Salakhutdinov, and Sergey Levine.
\newblock Contrastive learning as goal-conditioned reinforcement learning.
\newblock \emph{ArXiv}, abs/2206.07568, 2022.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{Fu2020D4RLDF}
Justin Fu, Aviral Kumar, Ofir Nachum, G.~Tucker, and Sergey Levine.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{ArXiv}, abs/2004.07219, 2020.

\bibitem[Grauman et~al.(2021)Grauman, Westbury, Byrne, Chavis, Furnari,
  Girdhar, Hamburger, Jiang, Liu, Liu, Martin, Nagarajan, Radosavovic,
  Ramakrishnan, Ryan, Sharma, Wray, Xu, Xu, Zhao, Bansal, Batra, Cartillier,
  Crane, Do, Doulaty, Erapalli, Feichtenhofer, Fragomeni, Fu, Fuegen,
  Gebreselasie, Gonz{\'{a}}lez, Hillis, Huang, Huang, Jia, Khoo, Kol{\'{a}}r,
  Kottur, Kumar, Landini, Li, Li, Li, Mangalam, Modhugu, Munro, Murrell,
  Nishiyasu, Price, Puentes, Ramazanova, Sari, Somasundaram, Southerland,
  Sugano, Tao, Vo, Wang, Wu, Yagi, Zhu, Arbelaez, Crandall, Damen, Farinella,
  Ghanem, Ithapu, Jawahar, Joo, Kitani, Li, Newcombe, Oliva, Park, Rehg, Sato,
  Shi, Shou, Torralba, Torresani, Yan, and
  Malik]{DBLP:journals/corr/abs-2110-07058}
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino
  Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu,
  Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh~Kumar
  Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu,
  Eric~Zhongcong Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent
  Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph
  Feichtenhofer, Adriano Fragomeni, Qichen Fu, Christian Fuegen, Abrham
  Gebreselasie, Cristina Gonz{\'{a}}lez, James Hillis, Xuhua Huang, Yifei
  Huang, Wenqi Jia, Weslie Khoo, J{\'{a}}chym Kol{\'{a}}r, Satwik Kottur,
  Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya
  Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu,
  Will Price, Paola~Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran
  Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen
  Wang, Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbelaez, David Crandall, Dima
  Damen, Giovanni~Maria Farinella, Bernard Ghanem, Vamsi~Krishna Ithapu, C.~V.
  Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard~A. Newcombe, Aude
  Oliva, Hyun~Soo Park, James~M. Rehg, Yoichi Sato, Jianbo Shi, Mike~Zheng
  Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik.
\newblock Ego4d: Around the world in 3, 000 hours of egocentric video.
\newblock \emph{CoRR}, abs/2110.07058, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.07058}.

\bibitem[Kostrikov et~al.(2020)Kostrikov, Yarats, and
  Fergus]{Kostrikov2020ImageAI}
Ilya Kostrikov, Denis Yarats, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock \emph{ArXiv}, abs/2004.13649, 2020.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and
  Levine]{kostrikov2021offline}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Saeedi, Gautam, and
  Gershman]{Kulkarni2016DeepSR}
Tejas~D. Kulkarni, Ardavan Saeedi, Simanta Gautam, and Samuel~J. Gershman.
\newblock Deep successor reinforcement learning.
\newblock \emph{ArXiv}, abs/1606.02396, 2016.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2006.04779}, 2020.

\bibitem[Ma et~al.(2022)Ma, Sodhani, Jayaraman, Bastani, Kumar, and
  Zhang]{Ma2022VIPTU}
Yecheng~Jason Ma, Shagun Sodhani, Dinesh Jayaraman, Osbert Bastani, Vikash
  Kumar, and Amy Zhang.
\newblock Vip: Towards universal visual reward and representation via
  value-implicit pre-training.
\newblock \emph{ArXiv}, abs/2210.00030, 2022.

\bibitem[Machado et~al.(2017)Machado, Rosenbaum, Guo, Liu, Tesauro, and
  Campbell]{Machado2017EigenoptionDT}
Marlos~C. Machado, Clemens Rosenbaum, Xiaoxiao Guo, Miao Liu, Gerald Tesauro,
  and Murray Campbell.
\newblock Eigenoption discovery through the deep successor representation.
\newblock \emph{ArXiv}, abs/1710.11089, 2017.

\bibitem[Nair et~al.(2018)Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{Nair2018VisualRL}
Ashvin Nair, Vitchyr~H. Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and
  Sergey Levine.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Nair et~al.(2022)Nair, Rajeswaran, Kumar, Finn, and
  Gupta]{Nair2022R3MAU}
Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhi Gupta.
\newblock R3m: A universal visual representation for robot manipulation.
\newblock \emph{ArXiv}, abs/2203.12601, 2022.

\bibitem[Paster et~al.(2022)Paster, McIlraith, and Ba]{Paster2022YouCC}
Keiran Paster, Sheila~A. McIlraith, and Jimmy Ba.
\newblock You can't count on luck: Why decision transformers fail in stochastic
  environments.
\newblock \emph{ArXiv}, abs/2205.15967, 2022.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{Schaul2015UniversalVF}
Tom Schaul, Dan Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In \emph{International Conference on Machine Learning}, 2015.

\bibitem[Schmeckpeper et~al.(2020)Schmeckpeper, Rybkin, Daniilidis, Levine, and
  Finn]{Schmeckpeper2020ReinforcementLW}
Karl Schmeckpeper, Oleh Rybkin, Kostas Daniilidis, Sergey Levine, and Chelsea
  Finn.
\newblock Reinforcement learning with videos: Combining offline observations
  with interaction.
\newblock In \emph{Conference on Robot Learning}, 2020.

\bibitem[Seo et~al.(2022{\natexlab{a}})Seo, Hafner, Liu, Liu, James, Lee, and
  Abbeel]{Seo2022MaskedWM}
Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu, Stephen James, Kimin Lee,
  and P.~Abbeel.
\newblock Masked world models for visual control.
\newblock \emph{ArXiv}, abs/2206.14244, 2022{\natexlab{a}}.

\bibitem[Seo et~al.(2022{\natexlab{b}})Seo, Lee, James, and
  Abbeel]{Seo2022ReinforcementLW}
Younggyo Seo, Kimin Lee, Stephen James, and P.~Abbeel.
\newblock Reinforcement learning with action-free pre-training from videos.
\newblock \emph{ArXiv}, abs/2203.13880, 2022{\natexlab{b}}.

\bibitem[Sermanet et~al.(2017)Sermanet, Lynch, Chebotar, Hsu, Jang, Schaal, and
  Levine]{Sermanet2017TimeContrastiveNS}
Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan
  Schaal, and Sergey Levine.
\newblock Time-contrastive networks: Self-supervised learning from video.
\newblock \emph{2018 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 1134--1141, 2017.

\bibitem[Srinivas et~al.(2020)Srinivas, Laskin, and Abbeel]{Srinivas2020CURLCU}
A.~Srinivas, Michael Laskin, and P.~Abbeel.
\newblock Curl: Contrastive unsupervised representations for reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Stadie et~al.(2017)Stadie, Abbeel, and
  Sutskever]{Stadie2017ThirdPersonIL}
Bradly~C. Stadie, P.~Abbeel, and Ilya Sutskever.
\newblock Third-person imitation learning.
\newblock \emph{ArXiv}, abs/1703.01703, 2017.

\bibitem[Sutton et~al.(2011)Sutton, Modayil, Delp, Degris, Pilarski, White, and
  Precup]{Sutton2011HordeAS}
Richard~S. Sutton, Joseph Modayil, Michael Delp, Thomas Degris, Patrick~M.
  Pilarski, Adam White, and Doina Precup.
\newblock Horde: a scalable real-time architecture for learning knowledge from
  unsupervised sensorimotor interaction.
\newblock In \emph{Adaptive Agents and Multi-Agent Systems}, 2011.

\bibitem[Torabi et~al.(2018{\natexlab{a}})Torabi, Warnell, and
  Stone]{Torabi2018BehavioralCF}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Behavioral cloning from observation.
\newblock \emph{ArXiv}, abs/1805.01954, 2018{\natexlab{a}}.

\bibitem[Torabi et~al.(2018{\natexlab{b}})Torabi, Warnell, and
  Stone]{Torabi2018GenerativeAI}
Faraz Torabi, Garrett Warnell, and Peter Stone.
\newblock Generative adversarial imitation from observation.
\newblock \emph{ArXiv}, abs/1807.06158, 2018{\natexlab{b}}.

\bibitem[Touati and Ollivier(2021)]{Touati2021LearningOR}
Ahmed Touati and Yann Ollivier.
\newblock Learning one representation to optimize all rewards.
\newblock In \emph{Neural Information Processing Systems}, 2021.

\bibitem[Toyer et~al.(2020)Toyer, Shah, Critch, and Russell]{toyer2020magical}
Sam Toyer, Rohin Shah, Andrew Critch, and Stuart Russell.
\newblock The {MAGICAL} benchmark for robust imitation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, and
  Vinyals]{Oord2018RepresentationLW}
A{\"a}ron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{ArXiv}, abs/1807.03748, 2018.

\bibitem[Villaflor et~al.(2022)Villaflor, Huang, Pande, Dolan, and
  Schneider]{pmlr-v162-villaflor22a}
Adam~R Villaflor, Zhe Huang, Swapnil Pande, John~M Dolan, and Jeff Schneider.
\newblock Addressing optimism bias in sequence modeling for reinforcement
  learning.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 22270--22283. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/villaflor22a.html}.

\bibitem[Xiao et~al.(2022)Xiao, Radosavovic, Darrell, and
  Malik]{Xiao2022MaskedVP}
Tete Xiao, Ilija Radosavovic, Trevor Darrell, and Jitendra Malik.
\newblock Masked visual pre-training for motor control.
\newblock \emph{ArXiv}, abs/2203.06173, 2022.

\bibitem[Yang et~al.(2022)Yang, Schuurmans, Abbeel, and
  Nachum]{Yang2022DichotomyOC}
Mengjiao Yang, Dale Schuurmans, P.~Abbeel, and Ofir Nachum.
\newblock Dichotomy of control: Separating what you can control from what you
  cannot.
\newblock \emph{ArXiv}, abs/2210.13435, 2022.

\bibitem[Zakka et~al.(2021)Zakka, Zeng, Florence, Tompson, Bohg, and
  Dwibedi]{zakka2021xirl}
Kevin Zakka, Andy Zeng, Pete Florence, Jonathan Tompson, Jeannette Bohg, and
  Debidatta Dwibedi.
\newblock Xirl: Cross-embodiment inverse reinforcement learning.
\newblock \emph{Conference on Robot Learning (CoRL)}, 2021.

\end{thebibliography}
