@inproceedings{bernstein2018asignsgd,
  title={sign{SGD}: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={Proceedings of the 35th International Conference on Machine Learning (ICML)},
  pages={560--569},
  year={2018},
  address={Stockholm, Sweden}
}

@inproceedings{bernstein2018bsignsgd,
  title={sign{SGD} with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  booktitle={Proceedings of the 7th International Conference on Learning Representations},
  year={2019},
  address={New Orleans, LA}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of the 19th International Conference on Computational Statistics (COMPSTAT)},
  pages={177--186},
  year={2010},
  address={Paris, France}
}

@inproceedings{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and Le, Quoc and Ng, Andrew},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  volume={25},
  pages={1223--1231},
  year={2012},
  address={Lake Tahoe, NV}
}

@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  journal={arXiv preprint arXiv:2303.18223},
  year={2023}
}

@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Peter Kairouz and H. Brendan McMahan and Brendan Avent and Aurélien Bellet and Mehdi Bennis and Arjun Nitin Bhagoji and Kallista Bonawitz and Zachary Charles and Graham Cormode and Rachel Cummings and Rafael G. L. D’Oliveira and Hubert Eichner and Salim El Rouayheb and David Evans and Josh Gardner and Zachary Garrett and Adrià Gascón and Badih Ghazi and Phillip B. Gibbons and Marco Gruteser and Zaid Harchaoui and Chaoyang He and Lie He and Zhouyuan Huo and Ben Hutchinson and Justin Hsu and Martin Jaggi and Tara Javidi and Gauri Joshi and Mikhail Khodak and Jakub Konecný and Aleksandra Korolova and Farinaz Koushanfar and Sanmi Koyejo and Tancrède Lepoint and Yang Liu and Prateek Mittal and Mehryar Mohri and Richard Nock and Ayfer Özgür and Rasmus Pagh and Hang Qi and Daniel Ramage and Ramesh Raskar and Mariana Raykova and Dawn Song and Weikang Song and Sebastian U. Stich and Ziteng Sun and Ananda Theertha Suresh and Florian Tramèr and Praneeth Vepakomma and Jianyu Wang and Li Xiong and Zheng Xu and Qiang Yang and Felix X. Yu and Han Yu and Sen Zhao},
  journal={Foundations and  Trends{\textregistered} in  Machine Learning},
  volume={14},
  number={1--2},
  pages={1--210},
  year={2021}
}

@inproceedings{baruch2019little,
  title={A little is enough: Circumventing defenses for distributed learning},
  author={Baruch, Gilad and Baruch, Moran and Goldberg, Yoav},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={32},
  year={2019},
  address={Vancouver, Canada}
}

@inproceedings{xie2020fall,
  title={Fall of empires: Breaking {B}yzantine-tolerant {SGD} by inner product manipulation},
  author={Xie, Cong and Koyejo, Oluwasanmi and Gupta, Indranil},
  booktitle={Proceedings of the 35th Uncertainty in Artificial Intelligence Conference (UAI)},
  volume={115},
  pages={261--270},
  year={2020},
  address={Online}
}

@article{lamport2019byzantine,
  title={The {B}yzantine generals problem},
  author={Lamport, Leslie and Shostak, Robert and Pease, Marshall},
  journal={ACM Transactions on Programming Languages and Systems},
  volume={4},
  number={3},
  pages={382--401},
  year={1982}
}

@article{lyu2020threats,
  title={Threats to federated learning: A survey},
  author={Lyu, Lingjuan and Yu, Han and Yang, Qiang},
  journal={arXiv preprint arXiv:2003.02133},
  year={2020}
}

@inproceedings{blanchard2017machine,
  title={Machine learning with adversaries: Byzantine tolerant gradient descent},
  author={Blanchard, Peva and El Mhamdi, El Mahdi and Guerraoui, Rachid and Stainer, Julien},
  booktitle={Advances in Neural Information Processsing Systems},
  volume={30},
  year={2017},
  address={Long Beach, CA}
}

@inproceedings{alistarh2018byzantine,
  title={Byzantine stochastic gradient descent},
  author={Alistarh, Dan and Allen-Zhu, Zeyuan and Li, Jerry},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  volume={31},
  year={2018},
  address={Montréal, Canada}
}

@inproceedings{alistarh2017qsgd,
  title={{QSGD}: Communication-efficient {SGD} via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  pages={1709--1720},
  volume={30},
  year={2017},
  address={Long Beach, CA}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech {DNN}s},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Proceedings of the 15th Annual Conference of the International Speech Communications Association (INTERSPEECH)},
  pages={1058--1062},
  year={2014},
  address={Singapore}
}

@inproceedings{gandikota2021vqsgd,
  title={vq{SGD}: Vector quantized stochastic gradient descent},
  author={Gandikota, Venkata and Kane, Daniel and Maity, Raj Kumar and Mazumdar, Arya},
  booktitle={Proceedings of the 24th International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pages={2197--2205},
  year={2021},
  address={Online}
}

@inproceedings{honig2022dadaquant,
  title={{DAdaQuant}: Doubly-adaptive quantization for communication-efficient Federated Learning},
  author={H{\"o}nig, Robert and Zhao, Yiren and Mullins, Robert},
  booktitle={Proceedings of the 39th International Conference on Machine Learning (ICML)},
  pages={8852--8866},
  year={2022},
  address={Baltimore, MD}
}

@article{aji2017sparse,
  title={Sparse communication for distributed gradient descent},
  author={Aji, Alham Fikri and Heafield, Kenneth},
  journal={arXiv preprint arXiv:1704.05021},
  year={2017}
}

@inproceedings{wangni2018gradient,
  title={Gradient sparsification for communication-efficient distributed optimization},
  author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  pages={1299--1309},
  volume={31},
  year={2018},
  address={Montréal, Canada}
}

@inproceedings{stich2018sparsified,
  title={Sparsified {SGD} with memory},
  author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  pages={4448--4459},
  volume={31},
  year={2018},
  address={Montréal, Canada}
}

@article{shi2019understanding,
  title={Understanding top-{K} sparsification in distributed deep learning},
  author={Shi, Shaohuai and Chu, Xiaowen and Cheung, Ka Chun and See, Simon},
  journal={arXiv preprint arXiv:1911.08772},
  year={2019}
}

@inproceedings{rothchild2020fetchsgd,
  title={Fetch{SGD}: Communication-efficient federated learning with sketching},
  author={Rothchild, Daniel and Panda, Ashwinee and Ullah, Enayat and Ivkin, Nikita and Stoica, Ion and Braverman, Vladimir and Gonzalez, Joseph and Arora, Raman},
  booktitle={Proceedings of the 37th International Conference on Machine Learning (ICML)},
  pages={8253--8265},
  year={2020},
  address={Online}
}

@inproceedings{li2022near,
  title={Near-optimal sparse allreduce for distributed deep learning},
  author={Li, Shigang and Hoefler, Torsten},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP)},
  pages={135--149},
  year={2022},
  address={Seoul, South Korea}
}

@inproceedings{basu2019qsparse,
  title={Qsparse-local-{SGD}: Distributed {SGD} with quantization, sparsification and local computations},
  author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  pages={14695--14706},
  volume={32},
  year={2019},
  address={Vancouver, Canada}
}

@inproceedings{wen2017terngrad,
  title={Tern{G}rad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in Neural Information Processsing Systems (NIPS)},
  pages={1--13},
  volume={30},
  year={2017},
  address={Long Beach, CA}
}

@article{sattler2019robust,
  title={Robust and communication-efficient federated learning from non-i.i.d. data},
  author={Sattler, Felix and Wiedemann, Simon and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={31},
  number={9},
  pages={3400--3413},
  year={2019}
}

@article{park2023sparse,
  title={Sparse-{S}ign{SGD} with majority vote for communication-efficient distributed learning},
  author={Park, Chanho and Lee, Namyoon},
  journal={arXiv preprint arXiv:2302.07475},
  year={2023}
}

@inproceedings{park2023s,
  title={{$\mathsf{S}^3$GD-MV}: {Sparse-SignSGD} with Majority Vote for Communication-Efficient Distributed Learning},
  author={Park, Chanho and Lee, Namyoon},
  booktitle={Proceedings of IEEE International Symposium on Information Theory (ISIT)},
  pages={2266--2271},
  year={2023},
  address={Taipei, Taiwan}
}


@inproceedings{li2023analysis,
  title={Analysis of error feedback in federated non-convex optimization with biased compression: Fast convergence and partial participation},
  author={Li, Xiaoyun and Li, Ping},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages={19638--19688},
  year={2023},
  address={Honolulu, HI}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes sign{SGD} and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={Proceedings of the 36th International Conference on Machine Learning (ICML)},
  pages={3252--3261},
  year={2019},
  address={Long Beach, CA}
}

@inproceedings{sun2023momentum,
  title={Momentum ensures convergence of signsgd under weaker assumptions},
  author={Sun, Tao and Wang, Qingsong and Li, Dongsheng and Wang, Bao},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages={33077--33099},
  year={2023},
  address={Honolulu, HI}
}

@inproceedings{zheng2019communication,
  title={Communication-efficient distributed blockwise momentum {SGD} with error-feedback},
  author={Zheng, Shuai and Huang, Ziyue and Kwok, James},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  volume={32},
  year={2019},
  address={Vancouver, Canada}
}

@inproceedings{safaryan2021stochastic,
  title={Stochastic sign descent methods: New algorithms and better theory},
  author={Safaryan, Mher and Richt{\'a}rik, Peter},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages={9224--9234},
  year={2021},
  address={Online}
}

@article{jin2020stochastic,
  title={Stochastic-sign {SGD} for federated learning with theoretical guarantees},
  author={Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal={arXiv preprint arXiv:2002.10940},
  year={2020}
}

@article{jin2023magnitude,
  title={Magnitude matters: Fixing {SIGNSGD} Through Magnitude-Aware Sparsification in the Presence of Data Heterogeneity},
  author={Jin, Richeng and He, Xiaofan and Zhong, Caijun and Zhang, Zhaoyang and Quek, Tony and Dai, Huaiyu},
  journal={arXiv preprint arXiv:2302.09634},
  year={2023}
}

@article{jin2024sign,
  title={Sign-Based Gradient Descent With Heterogeneous Data: Convergence and {B}yzantine Resilience},
  author={Jin, Richeng and Liu, Yuding and Huang, Yufan and He, Xiaofan and Wu, Tianfu and Dai, Huaiyu},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2024},
  note={{E}arly Access.}
}

@inproceedings{yin2018byzantine,
  title={Byzantine-robust distributed learning: Towards optimal statistical rates},
  author={Yin, Dong and Chen, Yudong and Kannan, Ramchandran and Bartlett, Peter},
  booktitle={Proceedings of the 35th International Conference on Machine Learning (ICML)},
  pages={5650--5659},
  year={2018},
  address={Stockholm, Sweden}
}

@inproceedings{karimireddy2021learning,
  title={Learning from history for {B}yzantine robust optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  pages={5311--5319},
  year={2021},
  address={Online}
}

@inproceedings{sohn2020election,
  title={Election coding for distributed learning: Protecting sign{SGD} against {B}yzantine attacks},
  author={Sohn, Jy-yong and Han, Dong-Jun and Choi, Beongjun and Moon, Jaekyun},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  volume={33},
  pages={14615--14625},
  year={2020},
  address={Online}
}

@inproceedings{chen2020distributed,
  title={Distributed training with heterogeneous data: Bridging median-and mean-based algorithms},
  author={Chen, Xiangyi and Chen, Tiancong and Sun, Haoran and Wu, Steven Z and Hong, Mingyi},
  booktitle={Advances in Neural Information Processsing Systems (NeurIPS)},
  volume={33},
  pages={21616--21626},
  year={2020},
  address={Online}
}

@inproceedings{guerraoui2018hidden,
  title={The hidden vulnerability of distributed learning in {B}yzantium},
  author={Guerraoui, Rachid and Rouault, S{\'e}bastien and others},
  booktitle={Proceedings of the 35th International Conference on Machine Learning (ICML)},
  pages={3521--3530},
  year={2018},
  address={Stockholm, Sweden}
}

@inproceedings{bagdasaryan2020backdoor,
  title={How to backdoor federated learning},
  author={Bagdasaryan, Eugene and Veit, Andreas and Hua, Yiqing and Estrin, Deborah and Shmatikov, Vitaly},
  booktitle={Proceedings of the 23rd International Conference on Artificial Intelligenec and Statistics (AISTATS)},
  volume={108},
  pages={2938--2948},
  year={2020},
  address={Online}
}

@inproceedings{wang2020attack,
  title={Attack of the tails: Yes, you really can backdoor federated learning},
  author={Wang, Hongyi and Sreenivasan, Kartik and Rajput, Shashank and Vishwakarma, Harit and Agarwal, Saurabh and Sohn, Jy-yong and Lee, Kangwook and Papailiopoulos, Dimitris},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  volume={33},
  pages={16070--16084},
  year={2020},
  address={Online}
}

@article{hong2017weighted,
  title={A weighted minimum distance decoding for uplink multiuser {MIMO} systems with low-resolution {ADC}s},
  author={Hong, Song-Nam and Kim, Seonho and Lee, Namyoon},
  journal={IEEE Transactions on Communications},
  volume={66},
  number={5},
  pages={1912--1924},
  year={2017}
}

@article{kim2019supervised,
  author={Kim, Daeun and Hong, Song-Nam and Lee, Namyoon},
  journal={IEEE Journals on Selected Areas in Communications}, 
  title={Supervised-Learning for Multi-Hop {MU-MIMO} Communications With One-Bit Transceivers}, 
  year={2019},
  volume={37},
  number={11},
  pages={2559-2572}
}

@article{li2014error,
  title={Error rate bounds and iterative weighted majority voting for crowdsourcing},
  author={Li, Hongwei and Yu, Bin},
  journal={arXiv preprint arXiv:1411.4086},
  year={2014}
}

@article{kim2023worker,
  title={A worker-task specialization model for crowdsourcing: Efficient inference and fundamental limits},
  author={Kim, Doyeon and Lee, Jeonghwan and Chung, Hye Won},
  journal={IEEE Transactions on Information Theory},
  year={2023},
  Note={{E}arly {A}ccess.}
}

@article{berend2015finite,
  title={A finite sample analysis of the Naive Bayes classifier},
  author={Berend, Daniel and Kontorovich, Aryeh},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1519--1545},
  year={2015}
}

@article{kim2023distributed,
  author={Kim, Yongjune and Shin, Junyoung and Cassuto, Yuval and Varshney, Lav R.},
  journal={IEEE Journals on Selected Areas in Communications}, 
  title={Distributed Boosting Classification Over Noisy Communication Channels}, 
  year={2023},
  volume={41},
  number={1},
  pages={141-154}
}

@article{jeon2018one,
  title={One-bit sphere decoding for uplink massive {MIMO} systems with one-bit {ADC}s},
  author={Jeon, Yo-Seb and Lee, Namyoon and Hong, Song-Nam and Heath, Robert W},
  journal={IEEE Transactions on Wireless Communications},
  volume={17},
  number={7},
  pages={4509--4521},
  year={2018}
}

@inproceedings{li2023revisiting,
  title={Revisiting weighted aggregation in federated learning with neural networks},
  author={Li, Zexi and Lin, Tao and Shang, Xinyi and Wu, Chao},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages={19767--19788},
  year={2023},
  address={Honolulu, HI}
}

@article{wu2021fast,
  title={Fast-convergent federated learning with adaptive weighting},
  author={Wu, Hongda and Wang, Ping},
  journal={IEEE Transactions on Cognitive Communications and Networking},
  volume={7},
  number={4},
  pages={1078--1088},
  year={2021}
}

@article{pillutla2022robust,
  title={Robust aggregation for federated learning},
  author={Pillutla, Krishna and Kakade, Sham M and Harchaoui, Zaid},
  journal={IEEE Transactions on Signal Processing},
  volume={70},
  pages={1142--1154},
  year={2022}
}

@book{cover1999elements,
  title={Elements of information theory},
  author={Cover, Thomas M},
  year={1999},
  publisher={John Wiley \& Sons}
}

@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
}

@TechReport{krizhevsky2009learning,
  author = {Krizhevsky, Alex and Hinton, Geoffrey},
  title = {Learning multiple layers of features from tiny images},
  institution = {University of Toronto},
  year = {2009},
  address = {Toronto, Canada}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages={770--778},
  year={2016},
  address={Las Vegas, NV}
}

@inproceedings{kearns2013large,
  title={Large deviation methods for approximate probabilistic inference},
  author={Kearns, Michael and Saul, Lawrence},
  booktitle={Proceedings of the 14th Conference on Uncertainty in Artificial Intelligence (UAI)},
  year={1998},
  address={Madison, WI}
}

@article{li2019convergence,
  title={On the convergence of fedavg on non-iid data},
  author={Li, Xiang and Huang, Kaixuan and Yang, Wenhao and Wang, Shusen and Zhang, Zhihua},
  journal={arXiv preprint arXiv:1907.02189},
  year={2019}
}

@inproceedings{zinkevich2010parallelized,
  title={Parallelized stochastic gradient descent},
  author={Zinkevich, Martin and Weimer, Markus and Li, Lihong and Smola, Alex},
  booktitle={Advances in Neural Information Processsing Systems (NIPS)},
  volume={23},
  pages={2595--2603},
  year={2010},
  address={Vancouver, Canada}
}