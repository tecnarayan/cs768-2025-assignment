\begin{thebibliography}{10}

\bibitem{bcq}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of {\em Proceedings of Machine Learning Research}, pages
  2052--2062. {PMLR}, 2019.

\bibitem{offline-survey}
Rafael~Figueiredo Prudencio, Marcos R. O.~A. M{\'{a}}ximo, and Esther~Luna
  Colombini.
\newblock A survey on offline reinforcement learning: Taxonomy, review, and
  open problems.
\newblock {\em CoRR}, abs/2203.01387, 2022.

\bibitem{healthcare}
Shengpu Tang and Jenna Wiens.
\newblock Model selection for offline reinforcement learning: Practical
  considerations for healthcare settings.
\newblock In Ken Jung, Serena Yeung, Mark~P. Sendak, Michael~W. Sjoding, and
  Rajesh Ranganath, editors, {\em Proceedings of the Machine Learning for
  Healthcare Conference}, volume 149 of {\em Proceedings of Machine Learning
  Research}, pages 2--35. {PMLR}, 2021.

\bibitem{auto-driving}
Christopher Diehl, Timo Sievernich, Martin Kr{\"{u}}ger, Frank Hoffmann, and
  Torsten Bertram.
\newblock Uncertainty-aware model-based offline reinforcement learning for
  automated driving.
\newblock {\em {IEEE} Robotics Autom. Lett.}, 8(2):1167--1174, 2023.

\bibitem{industrial-auto}
Tony~Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Nicolas Heess,
  Jon Scholz, Stefan Schaal, and Sergey Levine.
\newblock Offline meta-reinforcement learning for industrial insertion.
\newblock In {\em 2022 International Conference on Robotics and Automation},
  pages 6386--6393. {IEEE}, 2022.

\bibitem{ood-action}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances
  in Neural Information Processing Systems 32}, pages 11761--11771, 2019.

\bibitem{cql}
Jiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu.
\newblock Mildly conservative q-learning for offline reinforcement learning.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho,
  and A.~Oh, editors, {\em Advances in Neural Information Processing Systems
  35}, 2022.

\bibitem{mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y. Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock {MOPO:} model-based offline policy optimization.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33}, 2020.

\bibitem{morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33}, 2020.

\bibitem{bayes-model-based}
Porter Jenkins, Hua Wei, J.~Stockton Jenkins, and Zhenhui Li.
\newblock Bayesian model-based offline reinforcement learning for product
  allocation.
\newblock In {\em Thirty-Sixth {AAAI} Conference on Artificial Intelligence},
  pages 12531--12537. {AAAI} Press, 2022.

\bibitem{model-based}
Sen Lin, Jialin Wan, Tengyu Xu, Yingbin Liang, and Junshan Zhang.
\newblock Model-based offline meta-reinforcement learning with regularization.
\newblock In {\em The Tenth International Conference on Learning
  Representations}. OpenReview.net, 2022.

\bibitem{q-uncertainty}
Gaon An, Seungyong Moon, Jang{-}Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural
  Information Processing Systems 34}, pages 7436--7447, 2021.

\bibitem{q-uncertainty1}
Yue Wu, Shuangfei Zhai, Nitish Srivastava, Joshua~M. Susskind, Jian Zhang,
  Ruslan Salakhutdinov, and Hanlin Goh.
\newblock Uncertainty weighted actor-critic for offline reinforcement learning.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 11319--11328. {PMLR}, 2021.

\bibitem{q-uncertainty2}
Chenjia Bai, Lingxiao Wang, Zhuoran Yang, Zhi{-}Hong Deng, Animesh Garg, Peng
  Liu, and Zhaoran Wang.
\newblock Pessimistic bootstrapping for uncertainty-driven offline
  reinforcement learning.
\newblock In {\em The Tenth International Conference on Learning
  Representations}. OpenReview.net, 2022.

\bibitem{bayes-q-uncertainty}
Filippo Valdettaro and A.~Aldo Faisal.
\newblock Towards offline reinforcement learning with pessimistic value priors.
\newblock In Fabio Cuzzolin and Maryam Sultana, editors, {\em Epistemic
  Uncertainty in Artificial Intelligence - First International Workshop},
  volume 14523 of {\em Lecture Notes in Computer Science}, pages 89--100.
  Springer, 2024.

\bibitem{corrupt-robust-adversarial}
Xuezhou Zhang, Yiding Chen, Xiaojin Zhu, and Wen Sun.
\newblock Corruption-robust offline reinforcement learning.
\newblock In Gustau Camps{-}Valls, Francisco J.~R. Ruiz, and Isabel Valera,
  editors, {\em International Conference on Artificial Intelligence and
  Statistics}, volume 151 of {\em Proceedings of Machine Learning Research},
  pages 5757--5773. {PMLR}, 2022.

\bibitem{corrupt-robust-general-func}
Chenlu Ye, Rui Yang, Quanquan Gu, and Tong Zhang.
\newblock Corruption-robust offline reinforcement learning with general
  function approximation.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz
  Hardt, and Sergey Levine, editors, {\em Advances in Neural Information
  Processing Systems 36}, 2023.

\bibitem{riql}
Rui Yang, Han Zhong, Jiawei Xu, Amy Zhang, Chongjie Zhang, Lei Han, and Tong
  Zhang.
\newblock Towards robust offline reinforcement learning under diverse data
  corruption.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{robust-offline-rl1}
Kishan Panaganti, Zaiyan Xu, Dileep Kalathil, and Mohammad Ghavamzadeh.
\newblock Robust reinforcement learning using offline data.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho,
  and A.~Oh, editors, {\em Advances in Neural Information Processing Systems
  35}, 2022.

\bibitem{robust-offline-rl2}
Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han.
\newblock {RORL:} robust offline reinforcement learning via conservative
  smoothing.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho,
  and A.~Oh, editors, {\em Advances in Neural Information Processing Systems
  35}, 2022.

\bibitem{robust-offline-rl3}
Jose~H. Blanchet, Miao Lu, Tong Zhang, and Han Zhong.
\newblock Double pessimism is provably efficient for distributionally robust
  offline reinforcement learning: Generic algorithm and robust partial
  coverage.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz
  Hardt, and Sergey Levine, editors, {\em Advances in Neural Information
  Processing Systems 36}, 2023.

\bibitem{corrupt-robust-policy}
Yuzhe Ma, Xuezhou Zhang, Wen Sun, and Jerry Zhu.
\newblock Policy poisoning in batch reinforcement learning and control.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances
  in Neural Information Processing Systems 32}, pages 14543--14553, 2019.

\bibitem{corrupt-robust-adversarial1}
Fan Wu, Linyi Li, Huan Zhang, Bhavya Kailkhura, Krishnaram Kenthapadi, Ding
  Zhao, and Bo~Li.
\newblock {COPA:} certifying robust policies for offline reinforcement learning
  against poisoning attacks.
\newblock In {\em The Tenth International Conference on Learning
  Representations}. OpenReview.net, 2022.

\bibitem{corrupt-robust-diffusion}
Zhihe Yang and Yunjian Xu.
\newblock Dmbp: Diffusion model based predictor for robust offline
  reinforcement learning against state observation perturbations.
\newblock In {\em The Twelfth International Conference on Learning
  Representations}, 2023.

\bibitem{weight-pi}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock {\em CoRR}, abs/1910.00177, 2019.

\bibitem{iql}
Ilya Kostrikov, Ashvin Nair, and Sergey Levine.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In {\em The Tenth International Conference on Learning
  Representations}. OpenReview.net, 2022.

\bibitem{van2021bayesian}
Rens van~de Schoot, Sarah Depaoli, Ruth King, Bianca Kramer, Kaspar
  M{\"a}rtens, Mahlet~G Tadesse, Marina Vannucci, Andrew Gelman, Duco Veen,
  Joukje Willemsen, et~al.
\newblock Bayesian statistics and modelling.
\newblock {\em Nature Reviews Methods Primers}, 1(1):1, 2021.

\bibitem{bayes-survey}
Box~George EP and Tiao~George C.
\newblock {\em Bayesian inference in statistical analysis}.
\newblock John Wiley \& Sons, 2011.

\bibitem{bayes-rl}
Esther Derman, Daniel~J. Mankowitz, Timothy~A. Mann, and Shie Mannor.
\newblock A bayesian approach to robust reinforcement learning.
\newblock In Amir Globerson and Ricardo Silva, editors, {\em Proceedings of the
  Thirty-Fifth Conference on Uncertainty in Artificial Intelligence}, volume
  115 of {\em Proceedings of Machine Learning Research}, pages 648--658. {AUAI}
  Press, 2019.

\bibitem{bayes-rl1}
Matthew Fellows, Anuj Mahajan, Tim G.~J. Rudner, and Shimon Whiteson.
\newblock {VIREL:} {A} variational inference framework for reinforcement
  learning.
\newblock In Hanna~M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence
  d'Alch{\'{e}}{-}Buc, Emily~B. Fox, and Roman Garnett, editors, {\em Advances
  in Neural Information Processing Systems 32}, pages 7120--7134, 2019.

\bibitem{bayes-rl2}
Mattie Fellows, Kristian Hartikainen, and Shimon Whiteson.
\newblock Bayesian bellman operators.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural
  Information Processing Systems 34}, pages 13641--13656, 2021.

\bibitem{bayes-rl3}
Brendan O'Donoghue.
\newblock Variational bayesian reinforcement learning with regret bounds.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural
  Information Processing Systems 34}, pages 28208--28221, 2021.

\bibitem{bayes-offline-rl}
Ron Dorfman, Idan Shenfeld, and Aviv Tamar.
\newblock Offline meta reinforcement learning - identifiability challenges and
  effective data collection strategies.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural
  Information Processing Systems 34}, pages 4607--4618, 2021.

\bibitem{bayes-offline-rl1}
Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine.
\newblock Offline {RL} policies should be trained to be adaptive.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, {\em International
  Conference on Machine Learning}, volume 162 of {\em Proceedings of Machine
  Learning Research}, pages 7513--7530. {PMLR}, 2022.

\bibitem{bayes-offline-rl2}
Yuhao Wang and Enlu Zhou.
\newblock Bayesian risk-averse q-learning with streaming observations.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz
  Hardt, and Sergey Levine, editors, {\em Advances in Neural Information
  Processing Systems 36}, 2023.

\bibitem{bayes-rl-survey}
Mohammad Ghavamzadeh, Shie Mannor, Joelle Pineau, and Aviv Tamar.
\newblock Bayesian reinforcement learning: {A} survey.
\newblock {\em Found. Trends Mach. Learn.}, 8(5-6):359--483, 2015.

\bibitem{vae-tutorial}
Carl Doersch.
\newblock Tutorial on variational autoencoders.
\newblock {\em CoRR}, abs/1606.05908, 2016.

\bibitem{vae-intro}
Diederik~P. Kingma and Max Welling.
\newblock An introduction to variational autoencoders.
\newblock {\em Found. Trends Mach. Learn.}, 12(4):307--392, 2019.

\bibitem{c51}
Marc~G. Bellemare, Will Dabney, and R{\'{e}}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 449--458. {PMLR}, 2017.

\bibitem{kotz2012breakthroughs}
Kotz Samuel and Johnson~Norman L.
\newblock {\em Breakthroughs in statistics: methodology and distribution}.
\newblock Springer Science \& Business Media, 2012.

\bibitem{generalized-bayes}
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas.
\newblock Generalized variational inference.
\newblock {\em CoRR}, abs/1904.02063, 2019.

\bibitem{qr-dqn}
Will Dabney, Mark Rowland, Marc~G. Bellemare, and R{\'{e}}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In Sheila~A. McIlraith and Kilian~Q. Weinberger, editors, {\em
  Proceedings of the Thirty-Second {AAAI} Conference on Artificial
  Intelligence}, pages 2892--2901. {AAAI} Press, 2018.

\bibitem{iqn}
Will Dabney, Georg Ostrovski, David Silver, and R{\'{e}}mi Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock In Jennifer~G. Dy and Andreas Krause, editors, {\em Proceedings of
  the 35th International Conference on Machine Learning}, volume~80 of {\em
  Proceedings of Machine Learning Research}, pages 1104--1113. {PMLR}, 2018.

\bibitem{quantile}
Koenker Roger and Hallock~Kevin F.
\newblock Quantile regression.
\newblock {\em Journal of economic perspectives}, 15(4):143--156, 2001.

\bibitem{quantile-func}
M{\"u}ller Alfred.
\newblock Integral probability metrics and their generating classes of
  functions.
\newblock {\em Advances in applied probability}, 29(2):429--443, 1997.

\bibitem{heavy-tail}
Abhishek Roy, Krishnakumar Balasubramanian, and Murat~A. Erdogdu.
\newblock On empirical risk minimization with dependent and heavy-tailed data.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural
  Information Processing Systems 34}, pages 8913--8926, 2021.

\bibitem{caprio2023credal}
Michele Caprio, Souradeep Dutta, Kuk~Jin Jang, Vivian Lin, Radoslav Ivanov,
  Oleg Sokolsky, and Insup Lee.
\newblock Credal bayesian deep learning.
\newblock {\em arXiv e-prints}, pages arXiv--2302, 2023.

\bibitem{CaprioSHL24}
Michele Caprio, Yusuf Sale, Eyke H{\"{u}}llermeier, and Insup Lee.
\newblock A novel bayes' theorem for upper probabilities.
\newblock In Fabio Cuzzolin and Maryam Sultana, editors, {\em Epistemic
  Uncertainty in Artificial Intelligence - First International Workshop},
  volume 14523 of {\em Lecture Notes in Computer Science}, pages 1--12.
  Springer, 2024.

\bibitem{abs-2308-14815}
Souradeep Dutta, Michele Caprio, Vivian Lin, Matthew Cleaveland, Kuk~Jin Jang,
  Ivan Ruchkin, Oleg Sokolsky, and Insup Lee.
\newblock Distributionally robust statistical verification with imprecise
  neural networks.
\newblock {\em CoRR}, abs/2308.14815, 2023.

\bibitem{entropy}
Jim~W. Hall.
\newblock Uncertainty-based sensitivity indices for imprecise probability
  distributions.
\newblock {\em Reliab. Eng. Syst. Saf.}, 91(10-11):1443--1451, 2006.

\bibitem{exp-entropy}
L~Lorne Campbell.
\newblock Exponential entropy as a measure of extent of a distribution.
\newblock {\em Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte
  Gebiete}, 5(3):217--225, 1966.

\bibitem{differential-entropy}
Sheri Edwards.
\newblock Thomas m. cover and joy a. thomas, elements of information theory
  (2nd ed.), john wiley {\&} sons, inc. {(2006)}.
\newblock {\em Inf. Process. Manag.}, 44(1):400--401, 2008.

\bibitem{pgd1}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em 6th International Conference on Learning Representations},
  2018.

\bibitem{pgd2}
Huan Zhang, Hongge Chen, Chaowei Xiao, Bo~Li, Mingyan Liu, Duane~S. Boning, and
  Cho{-}Jui Hsieh.
\newblock Robust deep reinforcement learning against adversarial perturbations
  on state observations.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33}, 2020.

\bibitem{d4rl}
Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine.
\newblock {D4RL:} datasets for deep data-driven reinforcement learning.
\newblock {\em CoRR}, 2020.

\bibitem{edac}
Gaon An, Seungyong Moon, Jang{-}Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann~N. Dauphin, Percy
  Liang, and Jennifer~Wortman Vaughan, editors, {\em Advances in Neural
  Information Processing Systems 34}, 2021.

\bibitem{msg}
Seyed Kamyar~Seyed Ghasemipour, Shixiang~Shane Gu, and Ofir Nachum.
\newblock Why so pessimistic? estimating uncertainties for offline {RL} through
  ensembles, and why their independence matters.
\newblock In Sanmi Koyejo, S.~Mohamed, A.~Agarwal, Danielle Belgrave, K.~Cho,
  and A.~Oh, editors, {\em Advances in Neural Information Processing Systems
  35}, 2022.

\bibitem{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: {A} physics engine for model-based control.
\newblock In {\em 2012 {IEEE/RSJ} International Conference on Intelligent
  Robots and Systems, {IROS} 2012, Vilamoura, Algarve, Portugal, October 7-12,
  2012}, pages 5026--5033. {IEEE}, 2012.

\bibitem{carla}
Alexey Dosovitskiy, Germ{\'{a}}n Ros, Felipe Codevilla, Antonio~M. L{\'{o}}pez,
  and Vladlen Koltun.
\newblock {CARLA:} an open urban driving simulator.
\newblock In {\em 1st Annual Conference on Robot Learning}, volume~78 of {\em
  Proceedings of Machine Learning Research}, pages 1--16. {PMLR}, 2017.

\bibitem{corruption_rubust_online1}
Chi Jin, Tiancheng Jin, Haipeng Luo, Suvrit Sra, and Tiancheng Yu.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning}, 2020.

\bibitem{corruption_rubust_online2}
Thodoris Lykouris, Vahab~S. Mirrokni, and Renato~Paes Leme.
\newblock Stochastic bandits robust to adversarial corruptions.
\newblock In Ilias Diakonikolas, David Kempe, and Monika Henzinger, editors,
  {\em Proceedings of the 50th Annual {ACM} {SIGACT} Symposium on Theory of
  Computing}, 2018.

\bibitem{corruption_rubust_online3}
Aviv Rosenberg and Yishay Mansour.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, {\em
  Proceedings of the 36th International Conference on Machine Learning}, 2019.

\bibitem{corruption_rubust_online4}
Anupam Gupta, Tomer Koren, and Kunal Talwar.
\newblock Better algorithms for stochastic bandits with adversarial
  corruptions.
\newblock In Alina Beygelzimer and Daniel Hsu, editors, {\em Conference on
  Learning Theory}, 2019.

\bibitem{corruption_rubust_online5}
Chenlu Ye, Wei Xiong, Quanquan Gu, and Tong Zhang.
\newblock Corruption-robust algorithms with uncertainty weighting for nonlinear
  contextual bandits and markov decision processes.
\newblock In Andreas Krause, Emma Brunskill, Kyunghyun Cho, Barbara Engelhardt,
  Sivan Sabato, and Jonathan Scarlett, editors, {\em International Conference
  on Machine Learning}, 2023.

\bibitem{corruption_rubust_online6}
Thodoris Lykouris, Max Simchowitz, Alex Slivkins, and Wen Sun.
\newblock Corruption-robust exploration in episodic reinforcement learning.
\newblock In Mikhail Belkin and Samory Kpotufe, editors, {\em Conference on
  Learning Theory}, 2021.

\bibitem{ktd}
Matthieu Geist and Olivier Pietquin.
\newblock Kalman temporal differences.
\newblock {\em J. Artif. Intell. Res.}, 39:483--532, 2010.

\bibitem{psrl}
Ian Osband, Daniel Russo, and Benjamin~Van Roy.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In Christopher J.~C. Burges, L{\'{e}}on Bottou, Zoubin Ghahramani,
  and Kilian~Q. Weinberger, editors, {\em Advances in Neural Information
  Processing Systems 26}, pages 3003--3011, 2013.

\bibitem{model-based-bayesian1}
Marc~Peter Deisenroth and Carl~Edward Rasmussen.
\newblock {PILCO:} {A} model-based and data-efficient approach to policy
  search.
\newblock In Lise Getoor and Tobias Scheffer, editors, {\em Proceedings of the
  28th International Conference on Machine Learning}, 2011.

\bibitem{value-based-bayesian1}
Richard Dearden, Nir Friedman, and Stuart Russell.
\newblock Bayesian q-learning.
\newblock In Jack Mostow and Chuck Rich, editors, {\em Proceedings of the
  Fifteenth National Conference on Artificial Intelligence and Tenth Innovative
  Applications of Artificial Intelligence Conference}, 1998.

\bibitem{model-based-bayesian2}
Gal Yarin, McAllister Rowan, and Rasmussen~Carl Edward.
\newblock Improving pilco with bayesian neural network dynamics models.
\newblock In {\em Data-efficient machine learning workshop, ICML}, volume~4,
  page~25, 2016.

\bibitem{Wang00LL22}
Zhihai Wang, Jie Wang, Qi~Zhou, Bin Li, and Houqiang Li.
\newblock Sample-efficient reinforcement learning via conservative model-based
  actor-critic.
\newblock In {\em Thirty-Sixth {AAAI} Conference on Artificial Intelligence},
  pages 8612--8620. {AAAI} Press, 2022.

\bibitem{value-based-bayesian2}
Yaakov Engel, Shie Mannor, and Ron Meir.
\newblock Reinforcement learning with gaussian processes.
\newblock In Luc~De Raedt and Stefan Wrobel, editors, {\em Machine Learning,
  Proceedings of the Twenty-Second International Conference {(ICML} 2005)},
  2005.

\bibitem{policy-based-bayesian1}
Mohammad Ghavamzadeh and Yaakov Engel.
\newblock Bayesian policy gradient algorithms.
\newblock In Bernhard Sch{\"{o}}lkopf, John~C. Platt, and Thomas Hofmann,
  editors, {\em Advances in Neural Information Processing Systems 19}, 2006.

\bibitem{policy-based-bayesian2}
Mohammad Ghavamzadeh, Yaakov Engel, and Michal Valko.
\newblock Bayesian policy gradient and actor-critic algorithms.
\newblock {\em J. Mach. Learn. Res.}, 2016.

\bibitem{offline-bayesian1}
Dibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine.
\newblock Offline {RL} policies should be trained to be adaptive.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba
  Szepesv{\'{a}}ri, Gang Niu, and Sivan Sabato, editors, {\em International
  Conference on Machine Learning}, 2022.

\bibitem{offline-bayesian2}
Hao Hu, Yiqin Yang, Jianing Ye, Ziqing Mai, Yujing Hu, Tangjie Lv, Changjie
  Fan, Qianchuan Zhao, and Chongjie Zhang.
\newblock Bayesian offline-to-online reinforcement learning : A realist
  approach, 2024.

\bibitem{offline-bayesian3}
Yuhao Wang and Enlu Zhou.
\newblock Bayesian risk-averse q-learning with streaming observations.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz
  Hardt, and Sergey Levine, editors, {\em Advances in Neural Information
  Processing Systems 36}, 2023.

\bibitem{offline-bayesian4}
Toru Hishinuma and Kei Senda.
\newblock Importance-weighted variational inference model estimation for
  offline bayesian model-based reinforcement learning.
\newblock {\em {IEEE} Access}, 2023.

\bibitem{lemma6}
Sham~M. Kakade and John Langford.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In Claude Sammut and Achim~G. Hoffmann, editors, {\em Machine
  Learning, Proceedings of the Nineteenth International Conference {(ICML}
  2002)}, pages 267--274. Morgan Kaufmann, 2002.

\bibitem{wasserstein}
Vallender SS.
\newblock Calculation of the wasserstein distance between probability
  distributions on the line.
\newblock {\em Theory of Probability \& Its Applications}, 18:784--786, 1974.

\bibitem{survial_instinct}
Anqi Li, Dipendra Misra, Andrey Kolobov, and Ching{-}An Cheng.
\newblock Survival instinct in offline reinforcement learning.
\newblock In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz
  Hardt, and Sergey Levine, editors, {\em Advances in Neural Information
  Processing Systems 36: Annual Conference on Neural Information Processing
  Systems 2023}, 2023.

\bibitem{dt}
Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Michael
  Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In {\em Advances in Neural Information Processing Systems 34}, pages
  15084--15097, 2021.

\bibitem{sac}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem{tqc}
Arsenii Kuznetsov, Pavel Shvechikov, Alexander Grishin, and Dmitry Vetrov.
\newblock Controlling overestimation bias with truncated mixture of continuous
  distributional quantile critics.
\newblock In {\em International Conference on Machine Learning}, pages
  5556--5566. PMLR, 2020.

\bibitem{raeb}
Zhihai Wang, Taoxing Pan, Qi~Zhou, and Jie Wang.
\newblock Efficient exploration in resource-restricted reinforcement learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~37, pages 10279--10287, 2023.

\bibitem{mbpo}
Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine.
\newblock When to trust your model: Model-based policy optimization.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{mankowitz2023faster}
Daniel~J Mankowitz, Andrea Michi, Anton Zhernov, Marco Gelmi, Marco Selvi,
  Cosmin Paduraru, Edouard Leurent, Shariq Iqbal, Jean-Baptiste Lespiau, Alex
  Ahern, et~al.
\newblock Faster sorting algorithms discovered using deep reinforcement
  learning.
\newblock {\em Nature}, 618(7964):257--263, 2023.

\bibitem{gamble2018safety}
Chris Gamble and Jim Gao.
\newblock Safety-first ai for autonomous data centre cooling and industrial
  control.
\newblock {\em DeepMind, August}, 17, 2018.

\bibitem{yu2021reinforcement}
Chao Yu, Jiming Liu, Shamim Nemati, and Guosheng Yin.
\newblock Reinforcement learning in healthcare: A survey.
\newblock {\em ACM Computing Surveys (CSUR)}, 55(1):1--36, 2021.

\bibitem{wang2024ai4mul}
Zhihai Wang, Jie Wang, Dongsheng Zuo, Yunjie Ji, Xinli Xia, Yuzhe Ma, Jianye
  Hao, Mingxuan Yuan, Yongdong Zhang, and Feng Wu.
\newblock A hierarchical adaptive multi-task reinforcement learning framework
  for multiplier circuit design.
\newblock In {\em Forty-first International Conference on Machine Learning}.
  PMLR, 2024.

\bibitem{wang2023learning}
Zhihai Wang, Xijun Li, Jie Wang, Yufei Kuang, Mingxuan Yuan, Jia Zeng, Yongdong
  Zhang, and Feng Wu.
\newblock Learning cut selection for mixed-integer linear programming via
  hierarchical sequence model.
\newblock In {\em The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem{wangHEM}
Jie Wang, Zhihai Wang, Xijun Li, Yufei Kuang, Zhihao Shi, Fangzhou Zhu,
  Mingxuan Yuan, Jia Zeng, Yongdong Zhang, and Feng Wu.
\newblock Learning to cut via hierarchical sequence/set model for efficient
  mixed-integer programming.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages 1--17, 2024.

\bibitem{wang2024benchmarking}
Zhihai Wang, Zijie Geng, Zhaojie Tu, Jie Wang, Yuxi Qian, Zhexuan Xu, Ziyan
  Liu, Siyuan Xu, Zhentao Tang, Shixiong Kai, et~al.
\newblock Benchmarking end-to-end performance of ai-based chip placement
  algorithms.
\newblock {\em arXiv preprint arXiv:2407.15026}, 2024.

\end{thebibliography}
