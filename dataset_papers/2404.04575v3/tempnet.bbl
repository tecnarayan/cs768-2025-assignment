\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andonian et~al.(2023)Andonian, Anthony, Biderman, Black, Gali, Gao,
  Hallahan, Levy-Kramer, Leahy, Nestler, Parker, Pieler, Phang, Purohit,
  Schoelkopf, Stander, Songz, Tigges, Thérien, Wang, and
  Weinbach]{gpt-neox-library}
Andonian, A., Anthony, Q., Biderman, S., Black, S., Gali, P., Gao, L.,
  Hallahan, E., Levy-Kramer, J., Leahy, C., Nestler, L., Parker, K., Pieler,
  M., Phang, J., Purohit, S., Schoelkopf, H., Stander, D., Songz, T., Tigges,
  C., Thérien, B., Wang, P., and Weinbach, S.
\newblock {GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch},
  9 2023.
\newblock URL \url{https://www.github.com/eleutherai/gpt-neox}.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang,
  E., Cai, C., Terry, M., Le, Q., et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Balanya et~al.(2022)Balanya, Maro{\~n}as, and
  Ramos]{balanya2022adaptive}
Balanya, S.~A., Maro{\~n}as, J., and Ramos, D.
\newblock Adaptive temperature scaling for robust calibration of deep neural
  networks.
\newblock \emph{arXiv preprint arXiv:2208.00461}, 2022.

\bibitem[Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley,
  O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff,
  et~al.]{biderman2023pythia}
Biderman, S., Schoelkopf, H., Anthony, Q.~G., Bradley, H., O’Brien, K.,
  Hallahan, E., Khan, M.~A., Purohit, S., Prashanth, U.~S., Raff, E., et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, pp.\  2397--2430, 2023.

\bibitem[Bjerva et~al.(2020)Bjerva, Bhutani, Golshan, Tan, and
  Augenstein]{bjerva2020subjqa}
Bjerva, J., Bhutani, N., Golshan, B., Tan, W.-C., and Augenstein, I.
\newblock Subjqa: a dataset for subjectivity and review comprehension.
\newblock \emph{arXiv preprint arXiv:2004.14283}, 2020.

\bibitem[Brown et~al.(2022)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pp.\  6704--6719, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert{-}Voss, Krueger,
  Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin,
  Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{DBLP:journals/corr/abs-2005-14165}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert{-}Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A.,
  Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin,
  M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A.,
  Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock \emph{CoRR}, abs/2005.14165, 2020.
\newblock URL \url{https://arxiv.org/abs/2005.14165}.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and
  Soricut]{changpinyo2021conceptual}
Changpinyo, S., Sharma, P., Ding, N., and Soricut, R.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to
  recognize long-tail visual concepts.
\newblock In \emph{Proceedings of the 34th IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, pp.\  3558--3568, 2021.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J.,
  Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{Proceedings of the 37th International conference on machine
  learning}, pp.\  1597--1607, 2020.

\bibitem[Cherti et~al.(2023)Cherti, Beaumont, Wightman, Wortsman, Ilharco,
  Gordon, Schuhmann, Schmidt, and Jitsev]{cherti2023reproducible}
Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C.,
  Schuhmann, C., Schmidt, L., and Jitsev, J.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  2818--2829, 2023.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser,
  Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert,
  M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dou et~al.(2022)Dou, Kamath, Gan, Zhang, Wang, Li, Liu, Liu, LeCun,
  Peng, et~al.]{dou2022coarse}
Dou, Z.-Y., Kamath, A., Gan, Z., Zhang, P., Wang, J., Li, L., Liu, Z., Liu, C.,
  LeCun, Y., Peng, N., et~al.
\newblock Coarse-to-fine vision-language pre-training with fusion in the
  backbone.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  9694--9705, 2022.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, et~al.]{gao2020pile}
Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang,
  J., He, H., Thite, A., Nabeshima, N., et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gao et~al.(2023)Gao, Tow, Abbasi, Biderman, Black, DiPofi, Foster,
  Golding, Hsu, Le~Noac'h, Li, McDonell, Muennighoff, Ociepa, Phang, Reynolds,
  Schoelkopf, Skowron, Sutawika, Tang, Thite, Wang, Wang, and
  Zou]{eval-harness}
Gao, L., Tow, J., Abbasi, B., Biderman, S., Black, S., DiPofi, A., Foster, C.,
  Golding, L., Hsu, J., Le~Noac'h, A., Li, H., McDonell, K., Muennighoff, N.,
  Ociepa, C., Phang, J., Reynolds, L., Schoelkopf, H., Skowron, A., Sutawika,
  L., Tang, E., Thite, A., Wang, B., Wang, K., and Zou, A.
\newblock A framework for few-shot language model evaluation, 12 2023.
\newblock URL \url{https://zenodo.org/records/10256836}.

\bibitem[Gloeckle et~al.(2023)Gloeckle, Roziere, Hayat, and
  Synnaeve]{gloeckle2023temperature}
Gloeckle, F., Roziere, B., Hayat, A., and Synnaeve, G.
\newblock Temperature-scaled large language models for lean proofstep
  prediction.
\newblock In \emph{The 3rd Workshop on Mathematical Reasoning and AI at
  NeurIPS'23}, 2023.

\bibitem[Goel et~al.(2022)Goel, Bansal, Bhatia, Rossi, Vinay, and
  Grover]{goel2022cyclip}
Goel, S., Bansal, H., Bhatia, S., Rossi, R., Vinay, V., and Grover, A.
\newblock Cyclip: Cyclic contrastive language-image pretraining.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  6704--6719, 2022.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  1321--1330, 2017.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  1026--1034, 2015.

\bibitem[He et~al.(2018)He, Zhang, Ao, and Huang]{he2018determining}
He, Y.-L., Zhang, X.-L., Ao, W., and Huang, J.~Z.
\newblock Determining the optimal temperature parameter for softmax function in
  reinforcement learning.
\newblock \emph{Applied Soft Computing}, 70:\penalty0 80--85, 2018.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Hinton, G., Vinyals, O., and Dean, J.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Hornik et~al.(1989)Hornik, Stinchcombe, and White]{HornikEtAl89}
Hornik, K., Stinchcombe, M., and White, H.
\newblock Multilayer feedforward networks are universal approximators.
\newblock \emph{Neural Networks}, 2\penalty0 (5):\penalty0 359--366, 1989.

\bibitem[Hu et~al.(2017)Hu, Yang, Liang, Salakhutdinov, and Xing]{hu2017toward}
Hu, Z., Yang, Z., Liang, X., Salakhutdinov, R., and Xing, E.~P.
\newblock Toward controlled generation of text.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pp.\  1587--1596, 2017.

\bibitem[Huang et~al.(2023)Huang, Yu, Ma, Zhong, Feng, Wang, Chen, Peng, Feng,
  Qin, et~al.]{huang2023survey}
Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W.,
  Feng, X., Qin, B., et~al.
\newblock A survey on hallucination in large language models: Principles,
  taxonomy, challenges, and open questions.
\newblock \emph{arXiv preprint arXiv:2311.05232}, 2023.

\bibitem[Jaynes(1957)]{jaynes1957information}
Jaynes, E.~T.
\newblock Information theory and statistical mechanics.
\newblock \emph{Physical review}, 106\penalty0 (4):\penalty0 620, 1957.

\bibitem[Joy et~al.(2023)Joy, Pinto, Lim, Torr, and Dokania]{joy2023sample}
Joy, T., Pinto, F., Lim, S.-N., Torr, P.~H., and Dokania, P.~K.
\newblock Sample-dependent adaptive temperature scaling for improved
  calibration.
\newblock In \emph{Proceedings of the 37th AAAI Conference on Artificial
  Intelligence}, pp.\  14919--14926, 2023.

\bibitem[Karpathy \& Fei-Fei(2015)Karpathy and Fei-Fei]{karpathy2015deep}
Karpathy, A. and Fei-Fei, L.
\newblock Deep visual-semantic alignments for generating image descriptions.
\newblock In \emph{Proceedings of the 28th IEEE/CVF Conference on Computer
  Vision and Pattern Recognition}, pp.\  3128--3137, 2015.

\bibitem[Kim(2019)]{Kim2019AdaptiveTT}
Kim, S.
\newblock Adaptive temperature tuning for mellowmax in deep reinforcement
  learning.
\newblock 2019.
\newblock URL \url{https://api.semanticscholar.org/CorpusID:208980169}.

\bibitem[Kukleva et~al.(2023)Kukleva, B{\"o}hle, Schiele, Kuehne, and
  Rupprecht]{kukleva2023temperature}
Kukleva, A., B{\"o}hle, M., Schiele, B., Kuehne, H., and Rupprecht, C.
\newblock Temperature schedules for self-supervised contrastive methods on
  long-tail data.
\newblock In \emph{the 11th International Conference on Learning
  Representations}, 2023.

\bibitem[Li et~al.(2021)Li, Selvaraju, Gotmare, Joty, Xiong, and
  Hoi]{li2021align}
Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., and Hoi, S. C.~H.
\newblock Align before fuse: Vision and language representation learning with
  momentum distillation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  9694--9705, 2021.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Dubois, Taori, Gulrajani,
  Guestrin, Liang, and Hashimoto]{alpaca_eval}
Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I., Guestrin, C., Liang,
  P., and Hashimoto, T.~B.
\newblock Alpacaeval: An automatic evaluator of instruction-following models.
\newblock \url{https://github.com/tatsu-lab/alpaca_eval}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Yang, Zhao, Song, Luo, Li, and
  Yang]{li2023curriculum}
Li, Z., Li, X., Yang, L., Zhao, B., Song, R., Luo, L., Li, J., and Yang, J.
\newblock Curriculum temperature for knowledge distillation.
\newblock In \emph{Proceedings of the 37th AAAI Conference on Artificial
  Intelligence}, volume~37, pp.\  1504--1512, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2014)Lin, Maire, Belongie, Hays, Perona, Ramanan,
  Doll{\'a}r, and Zitnick]{lin2014microsoft}
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
  Doll{\'a}r, P., and Zitnick, C.~L.
\newblock Microsoft coco: Common objects in context.
\newblock In \emph{Proceedings of the 11th European Conference on Computer
  Vision}, pp.\  740--755, 2014.

\bibitem[Liu et~al.(2022)Liu, Liu, Li, and Liu]{liu2022meta}
Liu, J., Liu, B., Li, H., and Liu, Y.
\newblock Meta knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2202.07940}, 2022.

\bibitem[Loshchilov \& Hutter(2017)Loshchilov and
  Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Ma et~al.(2017)Ma, Yin, Liu, Neubig, and Hovy]{ma2017softmax}
Ma, X., Yin, P., Liu, J., Neubig, G., and Hovy, E.
\newblock Softmax q-distribution estimation for structured prediction: A
  theoretical interpretation for raml.
\newblock \emph{arXiv preprint arXiv:1705.07136}, 2017.

\bibitem[Manna et~al.(2023)Manna, Chattopadhyay, Dey, Bhattacharya, and
  Pal]{manna2023dystress}
Manna, S., Chattopadhyay, S., Dey, R., Bhattacharya, S., and Pal, U.
\newblock Dystress: Dynamically scaled temperature in self-supervised
  contrastive learning.
\newblock \emph{arXiv preprint arXiv:2308.01140}, 2023.

\bibitem[Mitchell(1980)]{mitchell1980need}
Mitchell, T.~M.
\newblock The need for biases in learning generalizations.
\newblock 1980.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Oord, A. v.~d., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Plummer et~al.(2015)Plummer, Wang, Cervantes, Caicedo, Hockenmaier,
  and Lazebnik]{plummer2015flickr30k}
Plummer, B.~A., Wang, L., Cervantes, C.~M., Caicedo, J.~C., Hockenmaier, J.,
  and Lazebnik, S.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for
  richer image-to-sentence models.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pp.\  2641--2649, 2015.

\bibitem[Qi et~al.(2023)Qi, Lyu, sik Chan, Bai, and Yang]{qi2023stochastic}
Qi, Q., Lyu, J., sik Chan, K., Bai, E.~W., and Yang, T.
\newblock Stochastic constrained dro with a complexity independent of sample
  size.
\newblock \emph{Transcations of Machine Learning and Research}, 2023.

\bibitem[Qiu et~al.(2023)Qiu, Hu, Yuan, Zhou, Zhang, and Yang]{qiu2023not}
Qiu, Z.-H., Hu, Q., Yuan, Z., Zhou, D., Zhang, L., and Yang, T.
\newblock Not all semantics are created equal: Contrastive self-supervised
  learning with automatic temperature individualization.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, pp.\  28389--28421, 2023.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{Proceedings of the 38th International Conference on Machine
  Learning}, pp.\  8748--8763, 2021.

\bibitem[Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and
  He]{rasley2020deepspeed}
Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y.
\newblock Deepspeed: System optimizations enable training deep learning models
  with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  3505--3506, 2020.

\bibitem[Rockafellar \& Wets(2009)Rockafellar and
  Wets]{rockafellar2009variational}
Rockafellar, R.~T. and Wets, R. J.-B.
\newblock \emph{Variational analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Sanh, V., Debut, L., Chaumond, J., and Wolf, T.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and
  Soricut]{sharma2018conceptual}
Sharma, P., Ding, N., Goodman, S., and Soricut, R.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset
  for automatic image captioning.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2556--2565, 2018.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro,
  B.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Sion(1958)]{sion1958general}
Sion, M.
\newblock On general minimax theorems.
\newblock \emph{Pacific Journal of mathematics}, 8\penalty0 (1):\penalty0
  171--176, 1958.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin,
  Liang, and Hashimoto]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang,
  P., and Hashimoto, T.~B.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama1}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,
  et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Wang \& Liu(2021)Wang and Liu]{wang2021understanding}
Wang, F. and Liu, H.
\newblock Understanding the behaviour of contrastive loss.
\newblock In \emph{Proceedings of the 34th IEEE/CVF conference on computer
  vision and pattern recognition}, pp.\  2495--2504, 2021.

\bibitem[Wang et~al.(2020)Wang, Hsieh, Chang, Chen, Pan, Wei, and
  Juan]{wang2020contextual}
Wang, P.-H., Hsieh, S.-I., Chang, S.-C., Chen, Y.-T., Pan, J.-Y., Wei, W., and
  Juan, D.-C.
\newblock Contextual temperature for language modeling.
\newblock \emph{arXiv preprint arXiv:2012.13575}, 2020.

\bibitem[Wang et~al.(2022)Wang, Kordi, Mishra, Liu, Smith, Khashabi, and
  Hajishirzi]{selfinstruct}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and
  Hajishirzi, H.
\newblock Self-instruct: Aligning language model with self generated
  instructions, 2022.

\bibitem[Wu et~al.(2023{\natexlab{a}})Wu, Chen, Wu, Shi, Wang, and
  He]{wu2023understanding}
Wu, J., Chen, J., Wu, J., Shi, W., Wang, X., and He, X.
\newblock Understanding contrastive learning via distributionally robust
  optimization.
\newblock \emph{arXiv preprint arXiv:2310.11048}, 2023{\natexlab{a}}.

\bibitem[Wu et~al.(2023{\natexlab{b}})Wu, Chen, Wu, Shi, Zhang, and
  Wang]{wu2023bsl}
Wu, J., Chen, J., Wu, J., Shi, W., Zhang, J., and Wang, X.
\newblock Bsl: Understanding and improving softmax loss for recommendation,
  2023{\natexlab{b}}.

\bibitem[Xu et~al.(2022)Xu, Alon, Neubig, and Hellendoorn]{xu2022systematic}
Xu, F.~F., Alon, U., Neubig, G., and Hellendoorn, V.~J.
\newblock A systematic evaluation of large language models of code.
\newblock In \emph{Proceedings of the 6th ACM SIGPLAN International Symposium
  on Machine Programming}, pp.\  1--10, 2022.

\bibitem[Yuan et~al.(2022)Yuan, Wu, Qiu, Du, Zhang, Zhou, and
  Yang]{yuan2022provable}
Yuan, Z., Wu, Y., Qiu, Z.-H., Du, X., Zhang, L., Zhou, D., and Yang, T.
\newblock Provable stochastic optimization for global contrastive learning:
  Small batch does not harm performance.
\newblock In \emph{Proceedings of the 39th International Conference on Machine
  Learning}, pp.\  25760--25782, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Wu, Bayrooti, and
  Goodman]{zhang2021temperature}
Zhang, O., Wu, M., Bayrooti, J., and Goodman, N.
\newblock Temperature as uncertainty in contrastive learning.
\newblock \emph{arXiv preprint arXiv:2110.04403}, 2021.

\bibitem[Zhang et~al.(2018)Zhang, Yu, Karaman, Zhang, and
  Chang]{DBLP:journals/corr/abs-1809-04157}
Zhang, X., Yu, F.~X., Karaman, S., Zhang, W., and Chang, S.
\newblock Heated-up softmax embedding.
\newblock \emph{CoRR}, abs/1809.04157, 2018.
\newblock URL \url{http://arxiv.org/abs/1809.04157}.

\bibitem[Zheng et~al.(2024)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li,
  Li, Xing, et~al.]{zheng2024judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z.,
  Li, Z., Li, D., Xing, E., et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock volume~36, 2024.

\bibitem[Zhu et~al.(2023)Zhu, Ying, and Yang]{zhu2023label}
Zhu, D., Ying, Y., and Yang, T.
\newblock Label distributionally robust losses for multi-class classification:
  Consistency, robustness and adaptivity.
\newblock In \emph{Proceedings of the 40th International Conference on Machine
  Learning}, pp.\  43289--43325, 2023.

\end{thebibliography}
