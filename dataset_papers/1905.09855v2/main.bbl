\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andrychowicz et~al.(2018)Andrychowicz, Baker, Chociej, Jozefowicz,
  McGrew, Pachocki, Petron, Plappert, Powell, Ray,
  et~al.]{andrychowicz2018learning}
Marcin Andrychowicz, Bowen Baker, Maciek Chociej, Rafal Jozefowicz, Bob McGrew,
  Jakub Pachocki, Arthur Petron, Matthias Plappert, Glenn Powell, Alex Ray,
  et~al.
\newblock Learning dexterous in-hand manipulation.
\newblock \emph{arXiv preprint arXiv:1808.00177}, 2018.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  214--223, 2017.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{bellemare2017distributional}
Marc~G Bellemare, Will Dabney, and R{\'e}mi Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 449--458. JMLR. org, 2017.

\bibitem[Bhatnagar and Lakshmanan(2012)]{bhatnagar2012online}
Shalabh Bhatnagar and K~Lakshmanan.
\newblock An online actor--critic algorithm with function approximation for
  constrained markov decision processes.
\newblock \emph{Journal of Optimization Theory and Applications}, 153\penalty0
  (3):\penalty0 688--708, 2012.

\bibitem[Borkar(2009)]{borkar2009stochastic}
Vivek~S Borkar.
\newblock \emph{Stochastic approximation: a dynamical systems viewpoint},
  volume~48.
\newblock Springer, 2009.

\bibitem[Chou et~al.(2017)Chou, Maturana, and Scherer]{beta_gradients}
Po-Wei Chou, Daniel Maturana, and Sebastian Scherer.
\newblock Improving stochastic policy gradients in continuous control with deep
  reinforcement learning using the beta distribution.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 834--843. JMLR. org, 2017.

\bibitem[Chow et~al.(2017)Chow, Ghavamzadeh, Janson, and Pavone]{chow2017risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6070--6120, 2017.

\bibitem[Dabney et~al.(2017)Dabney, Rowland, Bellemare, and
  Munos]{dabney2017distributional}
Will Dabney, Mark Rowland, Marc~G Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock \emph{arXiv preprint arXiv:1710.10044}, 2017.

\bibitem[Dabney et~al.(2018{\natexlab{a}})Dabney, Ostrovski, Silver, and
  Munos]{dabney2018implicit}
Will Dabney, Georg Ostrovski, David Silver, and R{\'e}mi Munos.
\newblock Implicit quantile networks for distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.06923}, 2018{\natexlab{a}}.

\bibitem[Dabney et~al.(2018{\natexlab{b}})Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Will Dabney, Mark Rowland, Marc~G Bellemare, and R{\'e}mi Munos.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018{\natexlab{b}}.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{baselines}
Prafulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex Nichol, Matthias
  Plappert, Alec Radford, John Schulman, Szymon Sidor, Yuhuai Wu, and Peter
  Zhokhov.
\newblock Openai baselines.
\newblock \url{https://github.com/openai/baselines}, 2017.

\bibitem[Fujimoto et~al.(2018)Fujimoto, van Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke van Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1802.09477}, 2018.

\bibitem[Ghavamzadeh et~al.(2016)Ghavamzadeh, Engel, and
  Valko]{ghavamzadeh2016bayesian}
Mohammad Ghavamzadeh, Yaakov Engel, and Michal Valko.
\newblock Bayesian policy gradient and actor-critic algorithms.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2319--2371, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pages
  2672--2680, 2014.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1352--1361. JMLR. org, 2017.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning}, pages
  1856--1865, 2018.

\bibitem[Huber(1992)]{huber1992robust}
Peter~J Huber.
\newblock Robust estimation of a location parameter.
\newblock In \emph{Breakthroughs in statistics}, pages 492--518. Springer,
  1992.

\bibitem[Kallenberg(2006)]{kallenberg2006foundations}
Olav Kallenberg.
\newblock \emph{Foundations of modern probability}.
\newblock Springer Science \& Business Media, 2006.

\bibitem[Koenker and Hallock(2001)]{koenker2001quantile}
Roger Koenker and Kevin Hallock.
\newblock Quantile regression: An introduction.
\newblock \emph{Journal of Economic Perspectives}, 15\penalty0 (4):\penalty0
  43--56, 2001.

\bibitem[Koenker and Xiao(2006)]{koenker2006quantile}
Roger Koenker and Zhijie Xiao.
\newblock Quantile autoregression.
\newblock \emph{Journal of the American Statistical Association}, 101\penalty0
  (475):\penalty0 980--990, 2006.

\bibitem[Konda and Tsitsiklis(2000)]{konda2000actor}
Vijay~R Konda and John~N Tsitsiklis.
\newblock Actor-critic algorithms.
\newblock In \emph{Advances in neural information processing systems}, pages
  1008--1014, 2000.

\bibitem[Korenkevych et~al.(2019)Korenkevych, Mahmood, Vasan, and
  Bergstra]{korenkevych2019autoregressive}
Dmytro Korenkevych, A~Rupam Mahmood, Gautham Vasan, and James Bergstra.
\newblock Autoregressive policies for continuous control deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1903.11524}, 2019.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{levine2016end}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Ostrovski et~al.(2018)Ostrovski, Dabney, and
  Munos]{ostrovski2018autoregressive}
Georg Ostrovski, Will Dabney, and R{\'e}mi Munos.
\newblock Autoregressive quantile networks for generative modeling.
\newblock \emph{arXiv preprint arXiv:1806.05575}, 2018.

\bibitem[Peng et~al.(2018)Peng, Abbeel, Levine, and van~de
  Panne]{peng2018deepmimic}
Xue~Bin Peng, Pieter Abbeel, Sergey Levine, and Michiel van~de Panne.
\newblock Deepmimic: Example-guided deep reinforcement learning of
  physics-based character skills.
\newblock \emph{arXiv preprint arXiv:1804.02717}, 2018.

\bibitem[Polyak(1990)]{polyak1990new}
Boris~T Polyak.
\newblock New stochastic approximation type procedures.
\newblock \emph{Automat. i Telemekh}, 7\penalty0 (98-107):\penalty0 2, 1990.

\bibitem[Puterman(1994)]{puterman1994markov}
Martin~L Puterman.
\newblock \emph{Markov decision processes: discrete stochastic dynamic
  programming}.
\newblock John Wiley \& Sons, 1994.

\bibitem[Puterman and Brumelle(1979)]{puterman1979convergence}
Martin~L Puterman and Shelby~L Brumelle.
\newblock On the convergence of policy iteration in stationary dynamic
  programming.
\newblock \emph{Mathematics of Operations Research}, 4\penalty0 (1):\penalty0
  60--69, 1979.

\bibitem[Qu et~al.(2018)Qu, Mannor, and Xu]{qu2018nonlinear}
Chao Qu, Shie Mannor, and Huan Xu.
\newblock Nonlinear distributional gradient temporal-difference learning.
\newblock \emph{arXiv preprint arXiv:1805.07732}, 2018.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  Wiele, Mnih, Heess, and Springenberg]{riedmiller2018learning}
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave,
  Tom Wiele, Vlad Mnih, Nicolas Heess, and Jost~Tobias Springenberg.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{International Conference on Machine Learning}, pages
  4341--4350, 2018.

\bibitem[Rowland et~al.(2018)Rowland, Bellemare, Dabney, Munos, and
  Teh]{rowland2018analysis}
Mark Rowland, Marc~G Bellemare, Will Dabney, R{\'e}mi Munos, and Yee~Whye Teh.
\newblock An analysis of categorical distributional reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1802.08163}, 2018.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in neural information processing systems}, pages
  2234--2242, 2016.

\bibitem[Salimans et~al.(2017)Salimans, Ho, Chen, Sidor, and
  Sutskever]{salimans2017evolution}
Tim Salimans, Jonathan Ho, Xi~Chen, Szymon Sidor, and Ilya Sutskever.
\newblock Evolution strategies as a scalable alternative to reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:1703.03864}, 2017.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{ICML}, 2014.

\bibitem[Sutton and Barto(1998)]{sutton1998reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}, volume~1.
\newblock MIT press Cambridge, 1998.

\bibitem[Sutton et~al.(2000{\natexlab{a}})Sutton, McAllester, Singh, and
  Mansour]{pg_theorem}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pages
  1057--1063, 2000{\natexlab{a}}.

\bibitem[Sutton et~al.(2000{\natexlab{b}})Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems}, pages
  1057--1063, 2000{\natexlab{b}}.

\bibitem[Tang and Agrawal(2019)]{tang2019discretizing}
Yunhao Tang and Shipra Agrawal.
\newblock Discretizing continuous action space for on-policy optimization.
\newblock \emph{arXiv preprint arXiv:1901.10500}, 2019.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{Intelligent Robots and Systems (IROS), 2012 IEEE/RSJ
  International Conference on}, pages 5026--5033. IEEE, 2012.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem[Williams(1992)]{reinforce}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Wolpert et~al.(1997)Wolpert, Macready, et~al.]{wolpert1997no}
David~H Wolpert, William~G Macready, et~al.
\newblock No free lunch theorems for optimization.
\newblock \emph{IEEE transactions on evolutionary computation}, 1\penalty0
  (1):\penalty0 67--82, 1997.

\end{thebibliography}
