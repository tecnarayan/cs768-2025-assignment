\begin{thebibliography}{10}

\bibitem{berman2019multigrain}
Maxim Berman, Herv{\'e} J{\'e}gou, Andrea Vedaldi, Iasonas Kokkinos, and
  Matthijs Douze.
\newblock {MultiGrain}: a unified image embedding for classes and instances.
\newblock {\em arXiv:1902.05509}, 2019.

\bibitem{Buades2005non}
Antoni Buades, Bartomeu Coll, and J-M Morel.
\newblock A non-local algorithm for image denoising.
\newblock In {\em CVPR}, 2005.

\bibitem{chen2020generative}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em ICML}, 2020.

\bibitem{chen2021empirical}
Xinlei Chen, Saining Xie, and Kaiming He.
\newblock An empirical study of training self-supervised vision transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{chen2021visformer}
Zhengsu Chen, Lingxi Xie, Jianwei Niu, Xuefeng Liu, Longhui Wei, and Qi~Tian.
\newblock Visformer: The vision-friendly transformer.
\newblock In {\em ICCV}, 2021.

\bibitem{Cordonnier2020relationship}
Jean-Baptiste Cordonnier, Andreas Loukas, and Martin Jaggi.
\newblock On the relationship between self-attention and convolutional layers.
\newblock {\em ICLR}, 2020.

\bibitem{Cubuk2018}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock {AutoAugment}: Learning augmentation policies from data.
\newblock In {\em CVPR}, 2019.

\bibitem{dai2020fbnetv3}
Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zijian He, Zhen Wei, Kan
  Chen, Yuandong Tian, Matthew Yu, Peter Vajda, et~al.
\newblock {FBNetV3}: Joint architecture-recipe search using neural acquisition
  function.
\newblock {\em arXiv:2006.02049}, 2020.

\bibitem{dascoli2021convit}
St{\'e}phane d'Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio
  Biroli, and Levent Sagun.
\newblock {ConViT}: Improving vision transformers with soft convolutional
  inductive biases.
\newblock In {\em ICML}, 2021.

\bibitem{Deng2009}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock {ImageNet}: A large-scale hierarchical image database.
\newblock In {\em CVPR}, 2009.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em {NACCL}}, 2019.

\bibitem{dollar2021fast}
Piotr Doll{\'a}r, Mannat Singh, and Ross Girshick.
\newblock Fast and accurate model scaling.
\newblock In {\em CVPR}, 2021.

\bibitem{Dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em ICLR}, 2021.

\bibitem{fan2021multiscale}
Haoqi Fan, Bo~Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra
  Malik, and Christoph Feichtenhofer.
\newblock Multiscale vision transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{Fukushima1980neocognitron}
Kunihiko Fukushima.
\newblock Neocognitron: A self-organizing neural network model for a mechanism
  of pattern recognition unaffected by shift in position.
\newblock {\em Biological cybernetics}, 36(4):193--202, 1980.

\bibitem{Goyal2017}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch {SGD}: Training {ImageNet} in 1 hour.
\newblock {\em arXiv:1706.02677}, 2017.

\bibitem{graham2021levit}
Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre Stock, Armand Joulin,
  Herv{\'e} J{\'e}gou, and Matthijs Douze.
\newblock {LeViT}: a vision transformer in {ConvNet}'s clothing for faster
  inference.
\newblock In {\em ICCV}, 2021.

\bibitem{He2017}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock {Mask R-CNN}.
\newblock In {\em ICCV}, 2017.

\bibitem{He2016}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, 2016.

\bibitem{hoffer2019augment}
Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel
  Soudry.
\newblock Augment your batch: better training with larger batches.
\newblock {\em arXiv:1901.09335}, 2019.

\bibitem{Ioffe2015}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em ICML}, 2015.

\bibitem{kolesnikov2019big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big {Transfer} ({BiT}): General visual representation learning.
\newblock In {\em ECCV}, 2020.

\bibitem{Krizhevsky2012}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock {ImageNet} classification with deep convolutional neural networks.
\newblock In {\em NeurIPS}, 2012.

\bibitem{LeCun1989}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard,
  Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1989.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In {\em ICCV}, 2021.

\bibitem{Long2015fully}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In {\em CVPR}, 2015.

\bibitem{Loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In {\em ICLR}, 2019.

\bibitem{miller1995wordnet}
George~A Miller.
\newblock Wordnet: a lexical database for english.
\newblock {\em Communications of the ACM}, 1995.

\bibitem{Nair2010}
Vinod Nair and Geoffrey~E Hinton.
\newblock Rectified linear units improve restricted boltzmann machines.
\newblock In {\em ICML}, 2010.

\bibitem{Radosavovic2019}
Ilija Radosavovic, Justin Johnson, Saining Xie, Wan-Yen Lo, and Piotr
  Doll{\'a}r.
\newblock On network design spaces for visual recognition.
\newblock In {\em ICCV}, 2019.

\bibitem{Radosavovic2020}
Ilija Radosavovic, Raj~Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr
  Doll{\'a}r.
\newblock Designing network design spaces.
\newblock In {\em CVPR}, 2020.

\bibitem{Ramachandran2019stand}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jonathon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock {\em NeurIPS}, 2019.

\bibitem{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In {\em ICML}, 2019.

\bibitem{Ren2015}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster {R-CNN}: Towards real-time object detection with region
  proposal networks.
\newblock In {\em NeurIPS}, 2015.

\bibitem{ridnik2021imagenet}
Tal Ridnik, Emanuel Ben-Baruch, Asaf Noy, and Lihi Zelnik-Manor.
\newblock Imagenet-21k pretraining for the masses.
\newblock In {\em NeurIPS}, 2021.

\bibitem{Simonyan2015}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em ICLR}, 2015.

\bibitem{Szegedy2015}
Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
  Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.
\newblock Going deeper with convolutions.
\newblock In {\em CVPR}, 2015.

\bibitem{Szegedy2016a}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and
  Zbigniew Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In {\em CVPR}, 2016.

\bibitem{Tan2019}
Mingxing Tan and Quoc~V Le.
\newblock {EfficientNet}: Rethinking model scaling for convolutional neural
  networks.
\newblock {\em ICML}, 2019.

\bibitem{tan2021efficientnetv2}
Mingxing Tan and Quoc~V Le.
\newblock Efficientnetv2: Smaller models and faster training.
\newblock In {\em ICML}, 2021.

\bibitem{Touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In {\em ICML}, 2021.

\bibitem{touvron2021going}
Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and
  Herv{\'e} J{\'e}gou.
\newblock Going deeper with image transformers.
\newblock {\em arXiv:2103.17239}, 2021.

\bibitem{Vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NeurIPS}, 2017.

\bibitem{wang2019learning}
Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek~F Wong, and
  Lidia~S Chao.
\newblock Learning deep transformer models for machine translation.
\newblock In {\em {ACL}}, 2019.

\bibitem{wang2021pyramid}
Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong
  Lu, Ping Luo, and Ling Shao.
\newblock Pyramid vision transformer: A versatile backbone for dense prediction
  without convolutions.
\newblock In {\em ICCV}, 2021.

\bibitem{Wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In {\em CVPR}, 2018.

\bibitem{wu2021cvt}
Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu~Yuan, and Lei
  Zhang.
\newblock {CvT}: Introducing convolutions to vision transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{Xie2017}
Saining Xie, Ross Girshick, Piotr Doll{\'a}r, Zhuowen Tu, and Kaiming He.
\newblock Aggregated residual transformations for deep neural networks.
\newblock In {\em CVPR}, 2017.

\bibitem{yuan2021incorporating}
Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei Yu, and Wei Wu.
\newblock Incorporating convolution designs into visual transformers.
\newblock In {\em ICCV}, 2021.

\bibitem{yuan2021tokens}
Li~Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis~EH
  Tay, Jiashi Feng, and Shuicheng Yan.
\newblock Tokens-to-token {ViT}: Training vision transformers from scratch on
  {ImageNet}.
\newblock In {\em ICCV}, 2021.

\bibitem{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock {CutMix}: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In {\em CVPR}, 2019.

\bibitem{Zhang2018mixup}
Hongyi Zhang, Moustapha Ciss{\'{e}}, Yann~N. Dauphin, and David Lopez{-}Paz.
\newblock Mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{Zhao2020exploring}
Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun.
\newblock Exploring self-attention for image recognition.
\newblock In {\em CVPR}, 2020.

\bibitem{zhong2020random}
Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi~Yang.
\newblock Random erasing data augmentation.
\newblock In {\em AAAI}, 2020.

\end{thebibliography}
