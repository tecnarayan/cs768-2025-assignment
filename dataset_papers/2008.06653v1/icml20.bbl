\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alemi et~al.(2018)Alemi, Poole, Fischer, Dillon, Saurous, and
  Murphy]{alemi2018fixing}
Alemi, A., Poole, B., Fischer, I., Dillon, J., Saurous, R.~A., and Murphy, K.
\newblock Fixing a broken elbo.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  159--168, 2018.

\bibitem[Alemi et~al.(2016)Alemi, Fischer, Dillon, and Murphy]{alemi2016deep}
Alemi, A.~A., Fischer, I., Dillon, J.~V., and Murphy, K.
\newblock Deep variational information bottleneck.
\newblock \emph{arXiv preprint arXiv:1612.00410}, 2016.

\bibitem[Arimoto(1972)]{arimoto1972algorithm}
Arimoto, S.
\newblock An algorithm for computing the capacity of arbitrary discrete
  memoryless channels.
\newblock \emph{IEEE Transactions on Information Theory}, 18\penalty0
  (1):\penalty0 14--20, 1972.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein {GAN}.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Ball{\'e} et~al.(2018)Ball{\'e}, Minnen, Singh, Hwang, and
  Johnston]{balle2018variational}
Ball{\'e}, J., Minnen, D., Singh, S., Hwang, S.~J., and Johnston, N.
\newblock Variational image compression with a scale hyperprior.
\newblock \emph{arXiv preprint arXiv:1802.01436}, 2018.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004convex}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge university press, 2004.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Brock, A., Donahue, J., and Simonyan, K.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Cao et~al.(2018)Cao, Ding, Lui, and Huang]{BRE2018}
Cao, Y., Ding, G.~W., Lui, K. Y.-C., and Huang, R.
\newblock Improving {GAN} training via binarized representation entropy (bre)
  regularization.
\newblock \emph{ICLR}, 2018.
\newblock accepted as poster.

\bibitem[Cover \& Thomas(2012)Cover and Thomas]{cover2012elements}
Cover, T.~M. and Thomas, J.~A.
\newblock \emph{Elements of information theory}.
\newblock John Wiley \& Sons, 2012.

\bibitem[Danihelka et~al.(2017)Danihelka, Lakshminarayanan, Uria, Wierstra, and
  Dayan]{danihelka2017comparison}
Danihelka, I., Lakshminarayanan, B., Uria, B., Wierstra, D., and Dayan, P.
\newblock Comparison of maximum likelihood and gan-based training of real nvps.
\newblock \emph{arXiv preprint arXiv:1705.05263}, 2017.

\bibitem[Domke \& Sheldon(2018)Domke and Sheldon]{domke2018importance}
Domke, J. and Sheldon, D.~R.
\newblock Importance weighting and variational inference.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  4470--4479, 2018.

\bibitem[Frey \& Hinton(1996)Frey and Hinton]{frey1996free}
Frey, B.~J. and Hinton, G.~E.
\newblock Free energy coding.
\newblock In \emph{Proceedings of Data Compression Conference-DCC'96}, pp.\
  73--81. IEEE, 1996.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2672--2680, 2014.

\bibitem[Grosse et~al.(2015)Grosse, Ghahramani, and
  Adams]{grosse2015sandwiching}
Grosse, R.~B., Ghahramani, Z., and Adams, R.~P.
\newblock Sandwiching the marginal likelihood using bidirectional monte carlo.
\newblock \emph{arXiv preprint arXiv:1511.02543}, 2015.

\bibitem[Grover et~al.(2018)Grover, Dhar, and Ermon]{grover2018flow}
Grover, A., Dhar, M., and Ermon, S.
\newblock Flow-{GAN}: Combining maximum likelihood and adversarial learning in
  generative models.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.~C.
\newblock Improved training of wasserstein {GAN}s.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5767--5777, 2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{fid}
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S.
\newblock {GAN}s trained by a two time-scale update rule converge to a local
  nash equilibrium.
\newblock In Guyon, I., Luxburg, U.~V., Bengio, S., Wallach, H., Fergus, R.,
  Vishwanathan, S., and Garnett, R. (eds.), \emph{Advances in Neural
  Information Processing Systems 30}, pp.\  6626--6637. Curran Associates,
  Inc., 2017.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G. and Van~Camp, D.
\newblock Keeping neural networks simple by minimizing the description length
  of the weights.
\newblock In \emph{in Proc. of the 6th Ann. ACM Conf. on Computational Learning
  Theory}. Citeseer, 1993.

\bibitem[Huang et~al.(2018)Huang, Yuan, Xu, Guo, Sun, Wu, and
  Weinberger]{huang2018an}
Huang, G., Yuan, Y., Xu, Q., Guo, C., Sun, Y., Wu, F., and Weinberger, K.
\newblock An empirical study on evaluation metrics of generative adversarial
  networks, 2018.

\bibitem[Johnson et~al.(2016)Johnson, Alahi, and
  Fei-Fei]{johnson2016perceptual}
Johnson, J., Alahi, A., and Fei-Fei, L.
\newblock Perceptual losses for real-time style transfer and super-resolution.
\newblock In \emph{European conference on computer vision}, pp.\  694--711.
  Springer, 2016.

\bibitem[Karras et~al.(2018{\natexlab{a}})Karras, Aila, Laine, and
  Lehtinen]{karras2018progressive}
Karras, T., Aila, T., Laine, S., and Lehtinen, J.
\newblock Progressive growing of {GAN}s for improved quality, stability, and
  variation.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Karras et~al.(2018{\natexlab{b}})Karras, Laine, and
  Aila]{karras2018style}
Karras, T., Laine, S., and Aila, T.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:1812.04948}, 2018{\natexlab{b}}.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock \emph{CoRR}, abs/1412.6980, 2014.

\bibitem[Kingma \& Welling(2013)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational bayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Kingma et~al.(2019)Kingma, Abbeel, and Ho]{kingma2019bit}
Kingma, F.~H., Abbeel, P., and Ho, J.
\newblock Bit-swap: Recursive bits-back coding for lossless compression with
  hierarchical latent variables.
\newblock \emph{arXiv preprint arXiv:1905.06845}, 2019.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, Citeseer, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, Haffner,
  et~al.]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Makhzani et~al.(2015)Makhzani, Shlens, Jaitly, Goodfellow, and
  Frey]{makhzani2015adversarial}
Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B.
\newblock Adversarial autoencoders.
\newblock \emph{arXiv preprint arXiv:1511.05644}, 2015.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
Miyato, T., Kataoka, T., Koyama, M., and Yoshida, Y.
\newblock Spectral normalization for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1802.05957}, 2018.

\bibitem[Neal(2001)]{neal2001annealed}
Neal, R.~M.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11\penalty0 (2):\penalty0 125--139,
  2001.

\bibitem[Neal(2005)]{neal2005estimating}
Neal, R.~M.
\newblock Estimating ratios of normalizing constants using linked importance
  sampling.
\newblock \emph{arXiv preprint math/0511216}, 2005.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain Monte Carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Radford et~al.(2015)Radford, Metz, and
  Chintala]{radford2015unsupervised}
Radford, A., Metz, L., and Chintala, S.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1511.06434}, 2015.

\bibitem[Rezende \& Viola(2018)Rezende and Viola]{rezende2018taming}
Rezende, D.~J. and Viola, F.
\newblock Taming vaes.
\newblock \emph{arXiv preprint arXiv:1810.00597}, 2018.

\bibitem[Sajjadi et~al.(2018)Sajjadi, Bachem, Lucic, Bousquet, and
  Gelly]{sajjadi2018assessing}
Sajjadi, M.~S., Bachem, O., Lucic, M., Bousquet, O., and Gelly, S.
\newblock Assessing generative models via precision and recall.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5228--5237, 2018.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen,
  X.
\newblock Improved techniques for training gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2234--2242, 2016.

\bibitem[Salimans et~al.(2018)Salimans, Zhang, Radford, and
  Metaxas]{salimans2018improving}
Salimans, T., Zhang, H., Radford, A., and Metaxas, D.
\newblock Improving {GAN}s using optimal transport.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Theis et~al.(2015)Theis, Oord, and Bethge]{theis2015note}
Theis, L., Oord, A. v.~d., and Bethge, M.
\newblock A note on the evaluation of generative models.
\newblock \emph{arXiv preprint arXiv:1511.01844}, 2015.

\bibitem[Theis et~al.(2017)Theis, Shi, Cunningham, and
  Husz{\'a}r]{theis2017lossy}
Theis, L., Shi, W., Cunningham, A., and Husz{\'a}r, F.
\newblock Lossy image compression with compressive autoencoders.
\newblock \emph{arXiv preprint arXiv:1703.00395}, 2017.

\bibitem[Townsend et~al.(2019)Townsend, Bird, and
  Barber]{townsend2019practical}
Townsend, J., Bird, T., and Barber, D.
\newblock Practical lossless compression with latent variables using bits back
  coding.
\newblock \emph{arXiv preprint arXiv:1901.04866}, 2019.

\bibitem[Wallace(1990)]{wallace1990classification}
Wallace, C.~S.
\newblock Classification by minimum-message-length inference.
\newblock In \emph{International Conference on Computing and Information}, pp.\
   72--81. Springer, 1990.

\bibitem[Wang et~al.(2003)Wang, Simoncelli, and Bovik]{wang2003multiscale}
Wang, Z., Simoncelli, E.~P., and Bovik, A.~C.
\newblock Multiscale structural similarity for image quality assessment.
\newblock In \emph{The Thrity-Seventh Asilomar Conference on Signals, Systems
  \& Computers, 2003}, volume~2, pp.\  1398--1402. Ieee, 2003.

\bibitem[Wang et~al.(2004)Wang, Bovik, Sheikh, Simoncelli,
  et~al.]{wang2004image}
Wang, Z., Bovik, A.~C., Sheikh, H.~R., Simoncelli, E.~P., et~al.
\newblock Image quality assessment: from error visibility to structural
  similarity.
\newblock \emph{IEEE transactions on image processing}, 13\penalty0
  (4):\penalty0 600--612, 2004.

\bibitem[Wu et~al.(2016)Wu, Burda, Salakhutdinov, and
  Grosse]{wu2016quantitative}
Wu, Y., Burda, Y., Salakhutdinov, R., and Grosse, R.
\newblock On the quantitative analysis of decoder-based generative models.
\newblock \emph{arXiv preprint arXiv:1611.04273}, 2016.

\bibitem[Yang et~al.(2020)Yang, Bamler, and Mandt]{yang2020improving}
Yang, Y., Bamler, R., and Mandt, S.
\newblock Improving inference for neural image compression.
\newblock \emph{arXiv preprint arXiv:2006.04240}, 2020.

\end{thebibliography}
