\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Hume(1896)]{hume1896treatise}
David Hume.
\newblock \emph{A treatise of human nature}.
\newblock Clarendon Press, 1896.

\bibitem[Brown et~al.(1989)Brown, Collins, and Duguid]{brown1989situated}
John~Seely Brown, Allan Collins, and Paul Duguid.
\newblock Situated cognition and the culture of learning.
\newblock \emph{1989}, 18\penalty0 (1):\penalty0 32--42, 1989.

\bibitem[Roth and Jornet(2013)]{roth2013situated}
Wolff-Michael Roth and Alfredo Jornet.
\newblock Situated cognition.
\newblock \emph{Wiley Interdisciplinary Reviews: Cognitive Science}, 4\penalty0 (5):\penalty0 463--478, 2013.

\bibitem[Greeno(1998)]{greeno1998situativity}
James~G Greeno.
\newblock The situativity of knowing, learning, and research.
\newblock \emph{American psychologist}, 53\penalty0 (1):\penalty0 5, 1998.

\bibitem[Lave and Wenger(1991)]{lave1991situated}
Jean Lave and Etienne Wenger.
\newblock \emph{Situated learning: Legitimate peripheral participation}.
\newblock Cambridge university press, 1991.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Jia, Edmonds, Zhu, and Zhu]{zhang2021acre}
Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, and Yixin Zhu.
\newblock Acre: Abstract causal reasoning beyond covariation.
\newblock In \emph{Proceedings of the ieee/cvf conference on computer vision and pattern recognition}, pages 10643--10653, 2021{\natexlab{a}}.

\bibitem[Raven(2003)]{raven2003raven}
Jean Raven.
\newblock Raven progressive matrices.
\newblock In \emph{Handbook of nonverbal assessment}, pages 223--237. Springer, 2003.

\bibitem[Nye et~al.(2020)Nye, Solar-Lezama, Tenenbaum, and Lake]{nye2020learning}
Maxwell Nye, Armando Solar-Lezama, Josh Tenenbaum, and Brenden~M Lake.
\newblock Learning compositional rules via neural program synthesis.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 10832--10842, 2020.

\bibitem[Hafner(2021)]{hafner2021benchmarking}
Danijar Hafner.
\newblock Benchmarking the spectrum of agent capabilities.
\newblock \emph{arXiv preprint arXiv:2109.06780}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Zhang, Li, and Smola]{zhang2022automatic}
Zhuosheng Zhang, Aston Zhang, Mu~Li, and Alex Smola.
\newblock Automatic chain of thought prompting in large language models.
\newblock \emph{arXiv preprint arXiv:2210.03493}, 2022.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0 (240):\penalty0 1--113, 2023.

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu, Gopalakrishnan, Hausman, et~al.]{ahn2022can}
Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Du et~al.(2023)Du, Watkins, Wang, Colas, Darrell, Abbeel, Gupta, and Andreas]{du2023guiding}
Yuqing Du, Olivia Watkins, Zihan Wang, C{\'e}dric Colas, Trevor Darrell, Pieter Abbeel, Abhishek Gupta, and Jacob Andreas.
\newblock Guiding pretraining in reinforcement learning with large language models.
\newblock In \emph{International Conference on Machine Learning}, pages 8657--8677. PMLR, 2023.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, Cai, Chen, Liu, Ma, and Liang]{wang2024describe}
Zihao Wang, Shaofei Cai, Guanzhou Chen, Anji Liu, Xiaojian~Shawn Ma, and Yitao Liang.
\newblock Describe, explain, plan and select: interactive planning with llms enables open-world multi-task agents.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024{\natexlab{a}}.

\bibitem[Shinn et~al.(2023)Shinn, Cassano, Berman, Gopinath, Narasimhan, and Yao]{shinn2023reflexion}
Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao.
\newblock Reflexion: Language agents with verbal reinforcement learning, 2023.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz, Kamar, Lee, Lee, Li, Lundberg, et~al.]{bubeck2023sparks}
S{\'e}bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg, et~al.
\newblock Sparks of artificial general intelligence: Early experiments with gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Gao et~al.(2023)Gao, Madaan, Zhou, Alon, Liu, Yang, Callan, and Neubig]{gao2023pal}
Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig.
\newblock Pal: Program-aided language models.
\newblock In \emph{International Conference on Machine Learning}, pages 10764--10799. PMLR, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Cai, Liu, Jin, Hou, Zhang, Lin, He, Zheng, Yang, et~al.]{wang2023jarvis}
Zihao Wang, Shaofei Cai, Anji Liu, Yonggang Jin, Jinbing Hou, Bowei Zhang, Haowei Lin, Zhaofeng He, Zilong Zheng, Yaodong Yang, et~al.
\newblock Jarvis-1: Open-world multi-task agents with memory-augmented multimodal language models.
\newblock \emph{arXiv preprint arXiv:2311.05997}, 2023{\natexlab{a}}.

\bibitem[Mihaylov et~al.(2018)Mihaylov, Clark, Khot, and Sabharwal]{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book question answering.
\newblock \emph{arXiv preprint arXiv:1809.02789}, 2018.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Liu, Lin, Li, Ma, and Liang]{wang2024ratretrievalaugmentedthoughts}
Zihao Wang, Anji Liu, Haowei Lin, Jiaqi Li, Xiaojian Ma, and Yitao Liang.
\newblock Rat: Retrieval augmented thoughts elicit context-aware reasoning in long-horizon generation, 2024{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2403.05313}.

\bibitem[Zhang et~al.(2024)Zhang, Yang, Hu, Wang, Li, Sun, Zhang, Zhang, Liu, Zhu, et~al.]{zhang2024proagent}
Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, et~al.
\newblock Proagent: building proactive cooperative agents with large language models.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 17591--17599, 2024.

\bibitem[Cai et~al.(2023{\natexlab{a}})Cai, Wang, Ma, Liu, and Liang]{cai2023open}
Shaofei Cai, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang.
\newblock Open-world multi-task control through goal-aware representation learning and adaptive horizon prediction.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13734--13744, 2023{\natexlab{a}}.

\bibitem[Cai et~al.(2023{\natexlab{b}})Cai, Zhang, Wang, Ma, Liu, and Liang]{cai2023groot}
Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang.
\newblock Groot: Learning to follow instructions by watching gameplay videos.
\newblock \emph{arXiv preprint arXiv:2310.08235}, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2023)Lin, Wang, Ma, and Liang]{lin2023mcutaskcentricframeworkopenended}
Haowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang.
\newblock Mcu: A task-centric framework for open-ended agent evaluation in minecraft, 2023.
\newblock URL \url{https://arxiv.org/abs/2310.08367}.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, Cai, Mu, Lin, Zhang, Liu, Li, Liu, Ma, and Liang]{wang2024omnijarvis}
Zihao Wang, Shaofei Cai, Zhancun Mu, Haowei Lin, Ceyao Zhang, Xuejie Liu, Qing Li, Anji Liu, Xiaojian Ma, and Yitao Liang.
\newblock Omnijarvis: Unified vision-language-action tokenization enables open-world instruction following agents.
\newblock \emph{arXiv preprint arXiv:2407.00114}, 2024{\natexlab{c}}.

\bibitem[Cai et~al.()Cai, Zhang, Wang, Ma, Liu, and Liang]{caigroot}
Shaofei Cai, Bowei Zhang, Zihao Wang, Xiaojian Ma, Anji Liu, and Yitao Liang.
\newblock Groot-1.5: Learning to follow multi-modal instructions from weak supervision.
\newblock In \emph{Multi-modal Foundation Model meets Embodied AI Workshop@ ICML2024}.

\bibitem[Wu et~al.(2023)Wu, Qiu, Ross, Aky{\"u}rek, Chen, Wang, Kim, Andreas, and Kim]{wu2023reasoning}
Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin Aky{\"u}rek, Boyuan Chen, Bailin Wang, Najoung Kim, Jacob Andreas, and Yoon Kim.
\newblock Reasoning or reciting? exploring the capabilities and limitations of language models through counterfactual tasks.
\newblock \emph{arXiv preprint arXiv:2307.02477}, 2023.

\bibitem[Saparov and He(2022)]{saparov2022language}
Abulhair Saparov and He~He.
\newblock Language models are greedy reasoners: A systematic formal analysis of chain-of-thought.
\newblock \emph{arXiv preprint arXiv:2210.01240}, 2022.

\bibitem[Dasgupta et~al.(2022)Dasgupta, Lampinen, Chan, Creswell, Kumaran, McClelland, and Hill]{dasgupta2022language}
Ishita Dasgupta, Andrew~K Lampinen, Stephanie~CY Chan, Antonia Creswell, Dharshan Kumaran, James~L McClelland, and Felix Hill.
\newblock Language models show human-like content effects on reasoning.
\newblock \emph{arXiv preprint arXiv:2207.07051}, 2022.

\bibitem[Tang et~al.(2023)Tang, Zheng, Li, Meng, Zhu, Liang, and Zhang]{tang2023large}
Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, and Muhan Zhang.
\newblock Large language models are in-context semantic reasoners rather than symbolic reasoners, 2023.

\bibitem[Han et~al.(2022)Han, Schoelkopf, Zhao, Qi, Riddell, Benson, Sun, Zubova, Qiao, Burtell, et~al.]{han2022folio}
Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Luke Benson, Lucy Sun, Ekaterina Zubova, Yujie Qiao, Matthew Burtell, et~al.
\newblock Folio: Natural language reasoning with first-order logic.
\newblock \emph{arXiv preprint arXiv:2209.00840}, 2022.

\bibitem[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Ichter, Driess, Arenas, Rao, Sadigh, and Zeng]{mirchandani2023large}
Suvir Mirchandani, Fei Xia, Pete Florence, Brian Ichter, Danny Driess, Montserrat~Gonzalez Arenas, Kanishka Rao, Dorsa Sadigh, and Andy Zeng.
\newblock Large language models as general pattern machines.
\newblock \emph{arXiv preprint arXiv:2307.04721}, 2023.

\bibitem[Kim et~al.(2022)Kim, Phunyaphibarn, Ahn, and Kim]{kim2022playgrounds}
Subin Kim, Prin Phunyaphibarn, Donghyun Ahn, and Sundong Kim.
\newblock Playgrounds for abstraction and reasoning.
\newblock In \emph{NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI)}, 2022.

\bibitem[Weston et~al.(2015)Weston, Bordes, Chopra, Rush, Van~Merri{\"e}nboer, Joulin, and Mikolov]{weston2015towards}
Jason Weston, Antoine Bordes, Sumit Chopra, Alexander~M Rush, Bart Van~Merri{\"e}nboer, Armand Joulin, and Tomas Mikolov.
\newblock Towards ai-complete question answering: A set of prerequisite toy tasks.
\newblock \emph{arXiv preprint arXiv:1502.05698}, 2015.

\bibitem[Yang et~al.(2022)Yang, Dong, Du, Cheng, Cambria, Liu, Gao, and Wei]{yang2022language}
Zonglin Yang, Li~Dong, Xinya Du, Hao Cheng, Erik Cambria, Xiaodong Liu, Jianfeng Gao, and Furu Wei.
\newblock Language models as inductive reasoners.
\newblock \emph{arXiv preprint arXiv:2212.10923}, 2022.

\bibitem[Chollet(2019)]{chollet2019measure}
Fran{\c{c}}ois Chollet.
\newblock On the measure of intelligence.
\newblock \emph{arXiv preprint arXiv:1911.01547}, 2019.

\bibitem[Rule(2020)]{rule2020child}
Joshua~Stewart Rule.
\newblock \emph{The child as hacker: building more human-like models of learning}.
\newblock PhD thesis, Massachusetts Institute of Technology, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Raghu, Kleinberg, and Bengio]{zhang2021pointer}
Chiyuan Zhang, Maithra Raghu, Jon Kleinberg, and Samy Bengio.
\newblock Pointer value retrieval: A new benchmark for understanding the limits of neural network generalization.
\newblock \emph{arXiv preprint arXiv:2107.12580}, 2021{\natexlab{b}}.

\bibitem[Srivastava et~al.(2022)Srivastava, Rastogi, Rao, Shoeb, Abid, Fisch, Brown, Santoro, Gupta, Garriga-Alonso, et~al.]{srivastava2022beyond}
Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal~Md Shoeb, Abubakar Abid, Adam Fisch, Adam~R Brown, Adam Santoro, Aditya Gupta, Adri{\`a} Garriga-Alonso, et~al.
\newblock Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.
\newblock \emph{arXiv preprint arXiv:2206.04615}, 2022.

\bibitem[Zhang et~al.(2019)Zhang, Gao, Jia, Zhu, and Zhu]{zhang2019raven}
Chi Zhang, Feng Gao, Baoxiong Jia, Yixin Zhu, and Song-Chun Zhu.
\newblock Raven: A dataset for relational and analogical visual reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 5317--5327, 2019.

\bibitem[Wu et~al.(2024)Wu, Yu, Chen, Tenenbaum, and Gan]{wu2024star}
Bo~Wu, Shoubin Yu, Zhenfang Chen, Joshua~B Tenenbaum, and Chuang Gan.
\newblock Star: A benchmark for situated reasoning in real-world videos.
\newblock \emph{arXiv preprint arXiv:2405.09711}, 2024.

\bibitem[Ma et~al.(2022)Ma, Yong, Zheng, Li, Liang, Zhu, and Huang]{ma2022sqa3d}
Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang.
\newblock Sqa3d: Situated question answering in 3d scenes.
\newblock \emph{arXiv preprint arXiv:2210.07474}, 2022.

\bibitem[Wang et~al.(2024{\natexlab{d}})Wang, Wu, Chen, Chen, Guan, Lee, Li, Tenenbaum, and Gan]{wang2024sok}
Andong Wang, Bo~Wu, Sunli Chen, Zhenfang Chen, Haotian Guan, Wei-Ning Lee, Li~Erran Li, Joshua~B Tenenbaum, and Chuang Gan.
\newblock Sok-bench: A situated video reasoning benchmark with aligned open-world knowledge.
\newblock \emph{arXiv preprint arXiv:2405.09713}, 2024{\natexlab{d}}.

\bibitem[Gordon et~al.(2018)Gordon, Kembhavi, Rastegari, Redmon, Fox, and Farhadi]{gordon2018iqa}
Daniel Gordon, Aniruddha Kembhavi, Mohammad Rastegari, Joseph Redmon, Dieter Fox, and Ali Farhadi.
\newblock Iqa: Visual question answering in interactive environments.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 4089--4098, 2018.

\bibitem[Wijmans et~al.(2019)Wijmans, Datta, Maksymets, Das, Gkioxari, Lee, Essa, Parikh, and Batra]{wijmans2019embodied}
Erik Wijmans, Samyak Datta, Oleksandr Maksymets, Abhishek Das, Georgia Gkioxari, Stefan Lee, Irfan Essa, Devi Parikh, and Dhruv Batra.
\newblock Embodied question answering in photorealistic environments with point cloud perception.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6659--6668, 2019.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Hafner et~al.(2023)Hafner, Pasukonis, Ba, and Lillicrap]{hafner2023mastering}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock \emph{arXiv preprint arXiv:2301.04104}, 2023.

\bibitem[Yao et~al.(2022)Yao, Zhao, Yu, Du, Shafran, Narasimhan, and Cao]{yao2022react}
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
\newblock React: Synergizing reasoning and acting in language models.
\newblock \emph{arXiv preprint arXiv:2210.03629}, 2022.

\bibitem[Xin et~al.(2023)Xin, Wang, Zheng, Li, Liu, Cao, Huang, Xiong, Shi, Xie, et~al.]{xin2023lego}
Huajian Xin, Haiming Wang, Chuanyang Zheng, Lin Li, Zhengying Liu, Qingxing Cao, Yinya Huang, Jing Xiong, Han Shi, Enze Xie, et~al.
\newblock Lego-prover: Neural theorem proving with growing libraries.
\newblock \emph{arXiv preprint arXiv:2310.00656}, 2023.

\bibitem[Raffin et~al.(2021)Raffin, Hill, Gleave, Kanervisto, Ernestus, and Dormann]{raffin2021stable}
Antonin Raffin, Ashley Hill, Adam Gleave, Anssi Kanervisto, Maximilian Ernestus, and Noah Dormann.
\newblock Stable-baselines3: Reliable reinforcement learning implementations.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (268):\penalty0 1--8, 2021.

\bibitem[Hafner et~al.(2024)Hafner, Pasukonis, Ba, and Lillicrap]{hafner2024mastering}
Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.
\newblock Mastering diverse domains through world models.
\newblock 2024.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu, Fan, and Anandkumar]{wang2023voyager}
Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}, 2023{\natexlab{b}}.

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{arXiv preprint arXiv:2303.08774}, 2023.

\bibitem[Peirce(1868)]{peirce1868questions}
Charles~S Peirce.
\newblock Questions concerning certain faculties claimed for man.
\newblock \emph{The Journal of Speculative Philosophy}, 2\penalty0 (2):\penalty0 103--114, 1868.

\bibitem[Xu et~al.(2023{\natexlab{a}})Xu, Li, Vaezipoor, Sanner, and Khalil]{xu2023llms}
Yudong Xu, Wenhao Li, Pashootan Vaezipoor, Scott Sanner, and Elias~B Khalil.
\newblock Llms and the abstraction and reasoning corpus: Successes, failures, and the importance of object-based representations.
\newblock \emph{arXiv preprint arXiv:2305.18354}, 2023{\natexlab{a}}.

\bibitem[Moskvichev et~al.(2023)Moskvichev, Odouard, and Mitchell]{moskvichev2023conceptarc}
Arseny Moskvichev, Victor~Vikram Odouard, and Melanie Mitchell.
\newblock The conceptarc benchmark: Evaluating understanding and generalization in the arc domain.
\newblock \emph{arXiv preprint arXiv:2305.07141}, 2023.

\bibitem[Barrett et~al.(2018)Barrett, Hill, Santoro, Morcos, and Lillicrap]{barrett2018measuring}
David Barrett, Felix Hill, Adam Santoro, Ari Morcos, and Timothy Lillicrap.
\newblock Measuring abstract reasoning in neural networks.
\newblock In \emph{International conference on machine learning}, pages 511--520. PMLR, 2018.

\bibitem[Webb et~al.(2020)Webb, Dulberg, Frankland, Petrov, O’Reilly, and Cohen]{webb2020learning}
Taylor Webb, Zachary Dulberg, Steven Frankland, Alexander Petrov, Randall O’Reilly, and Jonathan Cohen.
\newblock Learning representations that support extrapolation.
\newblock In \emph{International conference on machine learning}, pages 10136--10146. PMLR, 2020.

\bibitem[Hill et~al.(2019)Hill, Santoro, Barrett, Morcos, and Lillicrap]{hill2019learning}
Felix Hill, Adam Santoro, David~GT Barrett, Ari~S Morcos, and Timothy Lillicrap.
\newblock Learning to make analogies by contrasting abstract relational structure.
\newblock \emph{arXiv preprint arXiv:1902.00120}, 2019.

\bibitem[Gendron et~al.(2023)Gendron, Bao, Witbrock, and Dobbie]{gendron2023large}
Gael Gendron, Qiming Bao, Michael Witbrock, and Gillian Dobbie.
\newblock Large language models are not strong abstract reasoners.
\newblock 2023.

\bibitem[Xu et~al.(2023{\natexlab{b}})Xu, Lin, Han, Zhao, Liu, and Cambria]{xu2023large}
Fangzhi Xu, Qika Lin, Jiawei Han, Tianzhe Zhao, Jun Liu, and Erik Cambria.
\newblock Are large language models really good logical reasoners? a comprehensive evaluation from deductive, inductive and abductive views.
\newblock \emph{arXiv preprint arXiv:2306.09841}, 2023{\natexlab{b}}.

\bibitem[Han et~al.(2024)Han, Ransom, Perfors, and Kemp]{han2024inductive}
Simon~Jerome Han, Keith~J Ransom, Andrew Perfors, and Charles Kemp.
\newblock Inductive reasoning in humans and large language models.
\newblock \emph{Cognitive Systems Research}, 83:\penalty0 101155, 2024.

\bibitem[Alet et~al.(2021)Alet, Lopez-Contreras, Koppel, Nye, Solar-Lezama, Lozano-Perez, Kaelbling, and Tenenbaum]{alet2021large}
Ferran Alet, Javier Lopez-Contreras, James Koppel, Maxwell Nye, Armando Solar-Lezama, Tomas Lozano-Perez, Leslie Kaelbling, and Joshua Tenenbaum.
\newblock A large-scale benchmark for few-shot program induction and synthesis.
\newblock In \emph{International Conference on Machine Learning}, pages 175--186. PMLR, 2021.

\bibitem[Honovich et~al.(2022)Honovich, Shaham, Bowman, and Levy]{honovich2022instruction}
Or~Honovich, Uri Shaham, Samuel~R Bowman, and Omer Levy.
\newblock Instruction induction: From few examples to natural language task descriptions.
\newblock \emph{arXiv preprint arXiv:2205.10782}, 2022.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Zelikman, Poesia, Pu, Haber, and Goodman]{wang2023hypothesis}
Ruocheng Wang, Eric Zelikman, Gabriel Poesia, Yewen Pu, Nick Haber, and Noah~D Goodman.
\newblock Hypothesis search: Inductive reasoning with language models.
\newblock \emph{arXiv preprint arXiv:2309.05660}, 2023{\natexlab{c}}.

\bibitem[Qiu et~al.(2023)Qiu, Jiang, Lu, Sclar, Pyatkin, Bhagavatula, Wang, Kim, Choi, Dziri, et~al.]{qiu2023phenomenal}
Linlu Qiu, Liwei Jiang, Ximing Lu, Melanie Sclar, Valentina Pyatkin, Chandra Bhagavatula, Bailin Wang, Yoon Kim, Yejin Choi, Nouha Dziri, et~al.
\newblock Phenomenal yet puzzling: Testing inductive reasoning capabilities of language models with hypothesis refinement.
\newblock \emph{arXiv preprint arXiv:2310.08559}, 2023.

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson, S{\"u}nderhauf, Reid, Gould, and Van Den~Hengel]{anderson2018vision}
Peter Anderson, Qi~Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S{\"u}nderhauf, Ian Reid, Stephen Gould, and Anton Van Den~Hengel.
\newblock Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3674--3683, 2018.

\bibitem[Das et~al.(2018)Das, Datta, Gkioxari, Lee, Parikh, and Batra]{das2018embodied}
Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra.
\newblock Embodied question answering.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 1--10, 2018.

\bibitem[Zhang and Lu(2024)]{zhang2024adarefiner}
Wanpeng Zhang and Zongqing Lu.
\newblock Adarefiner: Refining decisions of language models with adaptive feedback.
\newblock In \emph{Findings of the Association for Computational Linguistics: NAACL 2024}, pages 782--799, 2024.

\bibitem[Work()]{workrethinking}
What Makes In-Context~Learning Work.
\newblock Rethinking the role of demonstrations: What makes in-context learning work?

\bibitem[Liu et~al.(2021)Liu, Shen, Zhang, Dolan, Carin, and Chen]{liu2021makes}
Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen.
\newblock What makes good in-context examples for gpt-$3 $?
\newblock \emph{arXiv preprint arXiv:2101.06804}, 2021.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and Singh]{zhao2021calibrate}
Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language models.
\newblock In \emph{International conference on machine learning}, pages 12697--12706. PMLR, 2021.

\bibitem[Aky{\"u}rek et~al.(2022)Aky{\"u}rek, Schuurmans, Andreas, Ma, and Zhou]{akyurek2022learning}
Ekin Aky{\"u}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with linear models.
\newblock \emph{arXiv preprint arXiv:2211.15661}, 2022.

\end{thebibliography}
