\begin{thebibliography}{10}

\bibitem{Abel:2018}
D.~Abel, D.~Arumugam, L.~Lehnert, and M.~Littman.
\newblock State abstractions for lifelong reinforcement learning.
\newblock In {\em Proc. of ICML}, pages 10--19, 2018.

\bibitem{Alipanahi:2015}
B.~Alipanahi, A.~Delong, M.~T. Weirauch, and B.~J. Frey.
\newblock Predicting the sequence specificities of dna-and rna-binding proteins
  by deep learning.
\newblock {\em Nature biotechnology}, 33(8):831, 2015.

\bibitem{Argyriou:2008}
A.~Argyriou, T.~Evgeniou, and M.~Pontil.
\newblock Convex multi-task feature learning.
\newblock {\em Machine Learning}, 73(3):243--272, 2008.

\bibitem{Bartlett:1997}
P.~L. Bartlett.
\newblock For valid generalization the size of the weights is more important
  than the size of the network.
\newblock In {\em NIPS}, pages 134--140, 1997.

\bibitem{Bilen:2016}
H.~Bilen and A.~Vedaldi.
\newblock Integrated perception with recurrent multi-task neural networks.
\newblock In {\em NIPS}, pages 235--243. 2016.

\bibitem{Brunskill:2014}
E.~Brunskill and L.~Li.
\newblock Pac-inspired option discovery in lifelong reinforcement learning.
\newblock In {\em Proc. of ICML}, pages 316--324, 2014.

\bibitem{Caruana:1998}
R.~Caruana.
\newblock Multitask learning.
\newblock In {\em Learning to learn}, pages 95--133. Springer US, 1998.

\bibitem{Chen:2018}
Z.~Chen, V.~Badrinarayanan, C.-Y. Lee, and A.~Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em Proc. of ICML 2018}, 2018.

\bibitem{Collobert:2008}
R.~Collobert and J.~Weston.
\newblock A unified architecture for natural language processing: Deep neural
  networks with multitask learning.
\newblock In {\em Proc. of ICML}, pages 160--167, 2008.

\bibitem{Deb:2016}
K.~Deb and C.~Myburgh.
\newblock Breaking the billion-variable barrier in real-world optimization
  using a customized evolutionary algorithm.
\newblock In {\em Proc. of GECCO}, pages 653--660, 2016.

\bibitem{Devin:2017}
C.~Devin, A.~Gupta, T.~Darrell, P.~Abbeel, and S.~Levine.
\newblock Learning modular neural network policies for multi-task and
  multi-robot transfer.
\newblock In {\em Proc. of ICRA}, pages 2169--2176, 2017.

\bibitem{Doerr:2008}
B.~Doerr, T.~Jansen, and C.~Klein.
\newblock Comparing global and local mutations on bit strings.
\newblock In {\em Proc. of GECCO}, pages 929--936, 2008.

\bibitem{Dong:2015}
D.~Dong, H.~Wu, W.~He, D.~Yu, and H.~Wang.
\newblock Multi-task learning for multiple language translation.
\newblock In {\em Proc. of ACL}, pages 1723--1732, 2015.

\bibitem{Eisenberg:2008}
B.~Eisenberg.
\newblock On the expectation of the maximum of iid geometric random variables.
\newblock {\em Statistics \& Probability Letters}, 78(2):135--143, 2008.

\bibitem{Ha:2017}
D.~Ha, A.~M. Dai, and Q.~V. Le.
\newblock Hypernetworks.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{Hashimoto:2017}
K.~Hashimoto, C.~Xiong, Y.~Tsuruoka, and R.~Socher.
\newblock A joint many-task model: Growing a neural network for multiple {NLP}
  tasks.
\newblock In {\em Proc. of EMNLP}, pages 1923--1933, 2017.

\bibitem{He:2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proc. of CVPR}, pages 770--778, 2016.

\bibitem{Huang:2013}
J.~T. Huang, J.~Li, D.~Yu, L.~Deng, and Y.~Gong.
\newblock Cross-language knowledge transfer using multilingual deep neural
  network with shared hidden layers.
\newblock In {\em Proc. of ICASSP}, pages 7304--7308, 2013.

\bibitem{Huang:2015}
Z.~Huang, J.~Li, S.~M. Siniscalchi, et~al.
\newblock Rapid adaptation for deep neural networks through multi-task
  learning.
\newblock In {\em Proc. of Interspeech}, pages 3625--3629, 2015.

\bibitem{Jaderberg:2016}
M.~Jaderberg, V.~Mnih, W.~M. Czarnecki, T.~Schaul, J.~Z. Leibo, D.~Silver, and
  K.~Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{Jung:2017}
C.~Jung, J.~A. Hawkins, S.~K. Jones, et~al.
\newblock Massively parallel biophysical analysis of crispr-cas complexes on
  next generation sequencing chips.
\newblock {\em Cell}, 170(1):35--47, 2017.

\bibitem{Kaiser:2017}
L.~Kaiser, A.~N. Gomez, N.~Shazeer, A.~Vaswani, N.~Parmar, L.~Jones, and
  J.~Uszkoreit.
\newblock One model to learn them all.
\newblock {\em CoRR}, abs/1706.05137, 2017.

\bibitem{Kang:2011}
Z.~Kang, K.~Grauman, and F.~Sha.
\newblock Learning with whom to share in multi-task feature learning.
\newblock In {\em Proc. of ICML}, pages 521--528, 2011.

\bibitem{Kingma:14}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock {\em CoRR}, abs/1412.6980, 2014.

\bibitem{Kolda:2009}
T.~G. Kolda and B.~W. Bader.
\newblock Tensor decompositions and applications.
\newblock {\em SIAM Review}, 51:455--500, 2009.

\bibitem{Krizhevsky:2009}
A.~Krizhevsky.
\newblock {\em Learning Multiple Layers of Features from Tiny Images}.
\newblock 2009.

\bibitem{Krogh:1992}
A.~Krogh and J.~A. Hertz.
\newblock A simple weight decay can improve generalization.
\newblock In {\em NIPS}, pages 950--957, 1992.

\bibitem{Kumar:2012}
A.~Kumar and H.~Daum{\'e}, III.
\newblock Learning task grouping and overlap in multi-task learning.
\newblock In {\em Proc. of ICML}, pages 1723--1730, 2012.

\bibitem{Lecun:1998}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proc. of the IEEE}, 86(11):2278--2324, 1998.

\bibitem{Liang:2018}
J.~Liang, E.~Meyerson, and R.~Miikkulainen.
\newblock Evolutionary architecture search for deep multitask networks.
\newblock In {\em Proc. of GECCO}, 2018.

\bibitem{Liu:2015}
X.~Liu, J.~Gao, X.~He, L.~Deng, K.~Duh, and Y.~Y. Wang.
\newblock Representation learning using multi-task deep neural networks for
  semantic classification and information retrieval.
\newblock In {\em Proc. of NAACL}, pages 912--921, 2015.

\bibitem{Long:2017}
M.~Long, Z.~Cao, J.~Wang, and P.~S. Yu.
\newblock Learning multiple tasks with multilinear relationship networks.
\newblock In {\em NIPS}, pages 1593--1602. 2017.

\bibitem{Lu:2016}
Y.~Lu, A.~Kumar, S.~Zhai, Y.~Cheng, T.~Javidi, and R.~S. Feris.
\newblock Fully-adaptive feature sharing in multi-task networks with
  applications in person attribute classification.
\newblock {\em Proc. of CVPR}, 2017.

\bibitem{Luong:2016}
M.~T. Luong, Q.~V. Le, I.~Sutskever, O.~Vinyals, and L.~Kaiser.
\newblock Multi-task sequence to sequence learning.
\newblock In {\em Proc. of ICLR}, 2016.

\bibitem{Merity:2018}
S.~Merity, N.~S. Keskar, and R.~Socher.
\newblock Regularizing and optimizing {LSTM} language models.
\newblock In {\em Proc. of ICLR}, 2018.

\bibitem{Merity:2016}
S.~Merity, C.~Xiong, J.~Bradbury, and R.~Socher.
\newblock Pointer sentinel mixture models.
\newblock {\em CoRR}, abs/1609.07843, 2016.

\bibitem{Meyerson:2018}
E.~Meyerson and R.~Miikkulainen.
\newblock Beyond shared hierarchies: Deep multitask learning through soft layer
  ordering.
\newblock In {\em Proc. of ICLR}, 2018.

\bibitem{Meyerson:2018b}
E.~Meyerson and R.~Miikkulainen.
\newblock Pseudo-task augmentation: From deep multitask learning to intratask
  sharing---and back.
\newblock In {\em Proc. of ICML}, 2018.

\bibitem{Misra:2016}
I.~Misra, A.~Shrivastava, A.~Gupta, and M.~Hebert.
\newblock Cross-stitch networks for multi-task learning.
\newblock In {\em Proc. of CVPR}, 2016.

\bibitem{Neumann:2015}
F.~Neumann and C.~Witt.
\newblock On the runtime of randomized local search and simple evolutionary
  algorithms for dynamic makespan scheduling.
\newblock In {\em Proc. of IJCAI}, pages 3742--3748, 2015.

\bibitem{Paske:2017}
A.~Paske et~al.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem{Ranjan:2016}
R.~Ranjan, V.~M. Patel, and R.~Chellappa.
\newblock Hyperface: {A} deep multi-task learning framework for face detection,
  landmark localization, pose estimation, and gender recognition.
\newblock {\em CoRR}, abs/1603.01249, 2016.

\bibitem{Rebuffi:2017}
S.-A. Rebuffi, H.~Bilen, and A.~Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In {\em NIPS}, pages 506--516. 2017.

\bibitem{Rebuffi:2018}
S.-A. Rebuffi, H.~Bilen, and A.~Vedaldi.
\newblock Efficient parametrization of multi-domain deep neural networks.
\newblock In {\em Proc. of CVPR}, pages 8119--8127, 2018.

\bibitem{Rosenbaum:2018}
C.~Rosenbaum, T.~Klinger, and M.~Riemer.
\newblock Routing networks: Adaptive selection of non-linear functions for
  multi-task learning.
\newblock In {\em Proc. of ICLR}, 2018.

\bibitem{Seltzer:2013}
M.~L. Seltzer and J.~Droppo.
\newblock Multi-task learning in deep neural networks for improved phoneme
  recognition.
\newblock In {\em Proc. of ICASSP}, pages 6965--6969, 2013.

\bibitem{Shazeer:2017}
N.~Shazeer, A.~Mirhoseini, K.~Maziarz, A.~Davis, Q.~V. Le, G.~E. Hinton, and
  J.~Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{Stanley:2009}
K.~O. Stanley, D.~B. D'Ambrosio, and J.~Gauci.
\newblock A hypercube-based encoding for evolving large-scale neural networks.
\newblock {\em Artificial Life}, 15:185--212, 2009.

\bibitem{Sudholt:2018}
D.~Sudholt.
\newblock On the robustness of evolutionary algorithms to noise: Refined
  results and an example where noise helps.
\newblock In {\em Proc. of GECCO}, pages 1523--1530, 2018.

\bibitem{Sutton:1998}
R.~S. Sutton and A.~G. Barto.
\newblock {\em Introduction to reinforcement learning}.
\newblock MIT Press, 1998.

\bibitem{Teh:2017}
Y.~Teh, V.~Bapst, W.~M. Czarnecki, J.~Quan, J.~Kirkpatrick, R.~Hadsell,
  N.~Heess, and R.~Pascanu.
\newblock Distral: Robust multitask reinforcement learning.
\newblock In {\em NIPS}, pages 4499--4509. 2017.

\bibitem{Thrun:2012}
S.~Thrun and L.~Pratt.
\newblock {\em Learning to Learn}.
\newblock 2012.

\bibitem{Witt:2013}
C.~Witt.
\newblock Tight bounds on the optimization time of a randomized search
  heuristic on linear functions.
\newblock {\em Combinatorics, Probability and Computing}, 22(2):294--318, 2013.

\bibitem{Wolsey:2014}
L.~A. Wolsey and G.~L. Nemhauser.
\newblock {\em Integer and combinatorial optimization}.
\newblock John Wiley \& Sons, 2014.

\bibitem{Wu:2015}
Z.~Wu, C.~Valentini-Botinhao, O.~Watts, and S.~King.
\newblock Deep neural networks employing multi-task learning and stacked
  bottleneck features for speech synthesis.
\newblock In {\em Proc. of ICASSP}, pages 4460--4464, 2015.

\bibitem{Yang:2015c}
Y.~Yang and T.~Hospedales.
\newblock A unified perspective on multi-domain and multi-task learning.
\newblock In {\em Proceedings of ICLR}, 2015.

\bibitem{Yang:2017}
Y.~Yang and T.~Hospedales.
\newblock Deep multi-task representation learning: A tensor factorisation
  approach.
\newblock In {\em Proc. of ICLR}, 2017.

\bibitem{Zagoruyko:2016}
S.~Zagoruyko and N.~Komodakis.
\newblock Wide residual networks.
\newblock {\em CoRR}, abs/1605.07146, 2016.

\bibitem{Zaremba:2014}
W.~Zaremba, I.~Sutskever, and O.~Vinyals.
\newblock Recurrent neural network regularization.
\newblock {\em CoRR}, abs/1409.2329, 2014.

\bibitem{Zeng:2016}
H.~Zeng, M.~D. Edwards, G.~Liu, and D.~K. Gifford.
\newblock Convolutional neural network architectures for predicting dna-protein
  binding.
\newblock {\em Bioinformatics}, 32(12):i121--i127, 2016.

\bibitem{Zhang:2014}
Z.~Zhang, L.~Ping, L.~C. Chen, and T.~Xiaoou.
\newblock Facial landmark detection by deep multi-task learning.
\newblock In {\em Proc. of ECCV}, pages 94--108, 2014.

\end{thebibliography}
