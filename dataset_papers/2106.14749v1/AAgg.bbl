\begin{thebibliography}{10}

\bibitem{he2020momentum}
K.~He, H.~Fan, Y.~Wu, S.~Xie, and R.~Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  pages 9729--9738, 2020.

\bibitem{chen2020improved}
X.~Chen, H.~Fan, R.~Girshick, and K.~He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{chen2020big}
T.~Chen, S.~Kornblith, K.~Swersky, M.~Norouzi, and G.~Hinton.
\newblock Big self-supervised models are strong semi-supervised learners.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, 2020.

\bibitem{chen2020simple}
T.~Chen, S.~Kornblith, M.~Norouzi, and G.~Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In {\em {Proc. Int'l Conf. Machine Learning}}, 2020.

\bibitem{caron2020unsupervised}
M.~Caron, I.~Misra, J.~Mairal, P.~Goyal, P.~Bojanowski, and A.~Joulin.
\newblock Unsupervised learning of visual features by contrasting cluster
  assignments.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, 2020.

\bibitem{grill2020bootstrap}
J.~Grill, F.~Strub, F.~Altch{\'e}, C.~Tallec, P.~Richemond, E.~Buchatskaya,
  C.~Doersch, B.~Pires, Z.~Guo, and M.~Azar.
\newblock Bootstrap your own latent: A new approach to self-supervised
  learning.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, 2020.

\bibitem{li2020prototypical}
J.~Li, P.~Zhou, C.~Xiong, and S.~Hoi.
\newblock Prototypical contrastive learning of unsupervised representations.
\newblock In {\em {Int'l Conf. Learning Representations}}, 2020.

\bibitem{noroozi2016unsupervised}
M.~Noroozi and P.~Favaro.
\newblock Unsupervised learning of visual representations by solving jigsaw
  puzzles.
\newblock In {\em {Proc. European Conf. Computer Vision}}, pages 69--84.
  Springer, 2016.

\bibitem{doersch2017multi}
C.~Doersch and A.~Zisserman.
\newblock Multi-task self-supervised visual learning.
\newblock In {\em {IEEE International Conference on Computer Vision}}, pages
  2051--2060, 2017.

\bibitem{komodakis2018unsupervised}
N.~Komodakis and S.~Gidaris.
\newblock Unsupervised representation learning by predicting image rotations.
\newblock 2018.

\bibitem{zhang2016colorful}
R.~Zhang, P.~Isola, and A.~Efros.
\newblock Colorful image colorization.
\newblock In {\em {Proc. European Conf. Computer Vision}}, pages 649--666.
  Springer, 2016.

\bibitem{aa2020}
X.~Wang and G.~Qi.
\newblock Contrastive learning with stronger augmentations.
\newblock 2021.

\bibitem{hadsell2006dimensionality}
R.~Hadsell, S.~Chopra, and Y.~LeCun.
\newblock Dimensionality reduction by learning an invariant mapping.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  volume~2, pages 1735--1742. IEEE, 2006.

\bibitem{hjelm2018learning}
R.~Hjelm, A.~Fedorov, S.~Lavoie-Marchildon, K.~Grewal, P.~Bachman,
  A.~Trischler, and Y.~Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock {\em arXiv preprint arXiv:1808.06670}, 2018.

\bibitem{oord2018representation}
A.~Oord, Y.~Li, and O.~Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{bachman2019learning}
P.~Bachman, R.~Hjelm, and W.~Buchwalter.
\newblock Learning representations by maximizing mutual information across
  views.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, pages
  15535--15545, 2019.

\bibitem{arora2019theoretical}
S.~Arora, H.~Khandeparkar, M.~Khodak, O.~Plevrakis, and N.~Saunshi.
\newblock A theoretical analysis of contrastive unsupervised representation
  learning.
\newblock In {\em {Proc. Int'l Conf. Machine Learning}}, pages 5628--5637,
  2019.

\bibitem{chuang2020debiased}
C.~Chuang, J.~Robinson, Y.~Lin, A.~Torralba, and S.~Jegelka.
\newblock Debiased contrastive learning.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}},
  volume~33, 2020.

\bibitem{wei2020co2}
C.~Wei, H.~Wang, W.~Shen, and A.~Yuille.
\newblock Co2: Consistent contrast for unsupervised visual representation
  learning.
\newblock {\em arXiv preprint arXiv:2010.02217}, 2020.

\bibitem{reed2014training}
S.~Reed, H.~Lee, D.~Anguelov, C.~Szegedy, D.~Erhan, and A.~Rabinovich.
\newblock Training deep neural networks on noisy labels with bootstrapping.
\newblock {\em arXiv preprint arXiv:1412.6596}, 2014.

\bibitem{bagherinezhad2018label}
H.~Bagherinezhad, M.~Horton, M.~Rastegari, and A.~Farhadi.
\newblock Label refinery: Improving imagenet classification through label
  progression.
\newblock {\em arXiv preprint arXiv:1805.02641}, 2018.

\bibitem{kim2020mixco}
S.~Kim, G.~Lee, S.~Bae, and S.~Yun.
\newblock Mixco: Mix-up contrastive learning for visual representation.
\newblock {\em arXiv preprint arXiv:2010.06300}, 2020.

\bibitem{lee2020mix}
K.~Lee, Y.~Zhu, K.~Sohn, C.~Li, J.~Shin, and H.~Lee.
\newblock i-mix: A strategy for regularizing contrastive representation
  learning.
\newblock {\em arXiv preprint arXiv:2010.08887}, 2020.

\bibitem{verma2020towards}
V.~Verma, M.~Luong, K.~Kawaguchi, H.~Pham, and Q.~Le.
\newblock Towards domain-agnostic contrastive learning.
\newblock {\em arXiv preprint arXiv:2011.04419}, 2020.

\bibitem{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, 2015.

\bibitem{muller2019does}
R.~M{\"u}ller, S.~Kornblith, and G.~Hinton.
\newblock When does label smoothing help?
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, pages
  4694--4703, 2019.

\bibitem{li2018learning}
Y.~Li and Y.~Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}},
  volume~31, pages 8157--8166, 2018.

\bibitem{shortest}
S.~Oymak and M.~Soltanolkotabi.
\newblock Overparameterized nonlinear learning: Gradient descent takes the
  shortest path?
\newblock In {\em {Proc. Int'l Conf. Machine Learning}}, pages 4951--4960,
  2019.

\bibitem{li2020gradient}
M.~Li, M.~Soltanolkotabi, and S.~Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock In {\em {Proc. Int'l Conf. Artificial Intelligence and Statistics}},
  pages 4313--4324, 2020.

\bibitem{allen2019convergence}
Z.~Allen-Zhu, Y.~Li, and Z.~Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em {Proc. Int'l Conf. Machine Learning}}, pages 242--252, 2019.

\bibitem{xie2017diverse}
B.~Xie, Y.~Liang, and L.~Song.
\newblock Diverse neural network learns true target functions.
\newblock In {\em {Proc. Int'l Conf. Artificial Intelligence and Statistics}},
  pages 1216--1224, 2017.

\bibitem{du2019gradient}
S.~Du, J.~Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In {\em {Proc. Int'l Conf. Machine Learning}}, pages 1675--1685,
  2019.

\bibitem{du2018gradient}
S.~Du, X.~Zhai, B.~Poczos, and A.~Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}, 2018.

\bibitem{zhang2017mixup}
H.~Zhang, M.~Cisse, Y.~Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock {\em arXiv preprint arXiv:1710.09412}, 2017.

\bibitem{miotto2018deep}
R.~Miotto, F.~Wang, S.~Wang, X.~Jiang, and J.~Dudley.
\newblock Deep learning for healthcare: review, opportunities and challenges.
\newblock {\em Briefings in bioinformatics}, 19(6):1236--1246, 2018.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  pages 770--778, 2016.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky and G.~Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{deng2009imagenet}
J.~Deng, W.~Dong, R.~Socher, L.~Li, K.~Li, and F.~Li.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  pages 248--255. IEEE, 2009.

\bibitem{misra2020self}
I.~Misra and L.~Maaten.
\newblock Self-supervised learning of pretext-invariant representations.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  pages 6707--6717, 2020.

\bibitem{tian2019contrastive}
Y.~Tian, D.~Krishnan, and P.~Isola.
\newblock Contrastive multiview coding.
\newblock {\em arXiv preprint arXiv:1906.05849}, 2019.

\bibitem{henaff2019data}
O.~H{\'e}naff, A.~Srinivas, J.~De Fauw, A.~Razavi, C.~Doersch, S.~Eslami, and
  A.~Oord.
\newblock Data-efficient image recognition with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1905.09272}, 2019.

\bibitem{chen2020exploring}
X.~Chen and K.~He.
\newblock Exploring simple siamese representation learning.
\newblock {\em arXiv preprint arXiv:2011.10566}, 2020.

\bibitem{tian2020makes}
Y.~Tian, C.~Sun, B.~Poole, D.~Krishnan, C.~Schmid, and P.~Isola.
\newblock What makes for good views for contrastive learning.
\newblock {\em arXiv preprint arXiv:2005.10243}, 2020.

\bibitem{wu2018unsupervised}
Z.~Wu, Y.~Xiong, S.~Yu, and D.~Lin.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  pages 3733--3742, 2018.

\bibitem{donahue2019large}
J.~Donahue and K.~Simonyan.
\newblock Large scale adversarial representation learning.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, pages
  10542--10552, 2019.

\bibitem{zhuang2019local}
C.~Zhuang, A.~Lin, and D.~Yamins.
\newblock Local aggregation for unsupervised learning of visual embeddings.
\newblock In {\em {IEEE International Conference on Computer Vision}}, pages
  6002--6012, 2019.

\bibitem{asano2019self}
Y.~Asano, C.~Rupprecht, and A.~Vedaldi.
\newblock Self-labelling via simultaneous clustering and representation
  learning.
\newblock {\em arXiv preprint arXiv:1911.05371}, 2019.

\bibitem{gidaris2020learning}
S.~Gidaris, A.~Bursuc, N.~Komodakis, P.~P{\'e}rez, and M.~Cord.
\newblock Learning representations by predicting bags of visual words.
\newblock In {\em {Proc. IEEE Conf. Computer Vision and Pattern Recognition}},
  pages 6928--6938, 2020.

\bibitem{everingham2010pascal}
M.~Everingham, G.~Van, C.~Williams, J.~Winn, and A.~Zisserman.
\newblock The pascal visual object classes (voc) challenge.
\newblock {\em {Int'l. J. Computer Vision}}, 88(2):303--338, 2010.

\bibitem{lin2014microsoft}
T.~Lin, M.~Maire, S.~Belongie, J.~Hays, P.~Perona, D.~Ramanan, P.~Doll{\'a}r,
  and C.~Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em {Proc. European Conf. Computer Vision}}, pages 740--755.
  Springer, 2014.

\bibitem{wu2019detectron2}
Y.~Wu, A.~Kirillov, F.~Massa, W.~Lo, and R.~Girshick.
\newblock Detectron2, 2019.

\bibitem{loshchilov2016sgdr}
I.~Loshchilov and F.~Hutter.
\newblock {SGDR}: Stochastic gradient descent with warm restarts.
\newblock In {\em {Int'l Conf. Learning Representations}}, 2016.

\bibitem{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em {Int'l Conf. Learning Representations}}, 2015.

\bibitem{maurer2009empirical}
A.~Maurer and M.~Pontil.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock {\em arXiv preprint arXiv:0907.3740}, 2009.

\bibitem{liang2016deep}
S.~Liang and R.~Srikant.
\newblock Why deep neural networks for function approximation?
\newblock {\em arXiv preprint arXiv:1610.04161}, 2016.

\bibitem{lu2017expressive}
Z.~Lu, H.~Pu, F.~Wang, Z.~Hu, and L.~Wang.
\newblock The expressive power of neural networks: A view from the width.
\newblock In {\em {Proc. Conf. Neural Information Processing Systems}}, pages
  6231--6239, 2017.

\bibitem{telgarsky2016benefits}
M.~Telgarsky.
\newblock Benefits of depth in neural networks.
\newblock In {\em {Conf. on Learning Theory}}, 2016.

\bibitem{cohen2016expressive}
N.~Cohen, O.~Sharir, and A.~Shashua.
\newblock On the expressive power of deep learning: A tensor analysis.
\newblock In {\em {Conf. on Learning Theory}}, pages 698--728, 2016.

\bibitem{eldan2016power}
R.~Eldan and O.~Shamir.
\newblock The power of depth for feedforward neural networks.
\newblock In {\em {Conf. on Learning Theory}}, pages 907--940, 2016.

\bibitem{sagun2017empirical}
L.~Sagun, U.~Evci, V.~Guney, Y.~Dauphin, and L.~Bottou.
\newblock Empirical analysis of the hessian of over-parametrized neural
  networks.
\newblock {\em arXiv preprint arXiv:1706.04454}, 2017.

\bibitem{hochreiter1997flat}
S.~Hochreiter and J.~Schmidhuber.
\newblock Flat minima.
\newblock {\em Neural Computation}, 9(1):1--42, 1997.

\bibitem{anon2019overparam}
S.~Oymak and M.~Soltanolkotabi.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock {\em IEEE Journal on Selected Areas in Information Theory}, 2020.

\end{thebibliography}
