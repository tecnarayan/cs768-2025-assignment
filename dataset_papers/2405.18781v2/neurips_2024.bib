@inproceedings{Dong2021AttentionIN,
  title={Attention is Not All You Need: Pure Attention Loses Rank Doubly Exponentially with Depth},
  author={Yihe Dong and Jean-Baptiste Cordonnier and Andreas Loukas},
  booktitle={ICML},
  year={2021}
}

@inproceedings{Yun2020OnCA,
  title={O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers},
  author={Chulhee Yun and Yin-Wen Chang and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  booktitle={NeurIPS},
  year={2020}
}


@book{Hartfiel2002NonhomogeneousMP,
  title={Nonhomogeneous Matrix Products},
  author={Darald J. Hartfiel},
  year={2002}
}

@inproceedings{Fan2021MaskAN,
  title={Mask Attention Networks: Rethinking and Strengthen Transformer},
  author={Zhihao Fan and Yeyun Gong and Dayiheng Liu and Zhongyu Wei and Siyuan Wang and Jian Jiao and Nan Duan and Ruofei Zhang and Xuanjing Huang},
  booktitle={ACL},
  year={2021}
}

@article{Castin2023UnderstandingTR,
  title={Understanding the Regularity of Self-Attention with Optimal Transport},
  author={Val'erie Castin and Pierre Ablin and Gabriel Peyr'e},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.14820}
}

@article{Godey2024AnisotropyII,
  title={Anisotropy Is Inherent to Self-Attention in Transformers},
  author={Nathan Godey and Eric Villemonte de la Clergerie and Benoit Sagot},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.12143}
}

@inproceedings{Tian2023ScanAS,
  title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer},
  author={Yuandong Tian and Yiping Wang and Beidi Chen and Simon Shaolei Du},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{Alman2023FastAR,
  title={Fast Attention Requires Bounded Entries},
  author={Josh Alman and Zhao Song},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{Nguyen2023MitigatingOI,
  title={Mitigating Over-smoothing in Transformers via Regularized Nonlocal Functionals},
  author={Tam Nguyen and Tan M. Nguyen and Richard G. Baraniuk},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{Noci2022SignalPI,
  title={Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse},
  author={Lorenzo Noci and Sotiris Anagnostidis and Luca Biggio and Antonio Orvieto and Sidak Pal Singh and Aur{\'e}lien Lucchi},
  booktitle={NeurIPS},
  year={2022}
}

@inproceedings{Joudaki2023OnTI,
  title={On the impact of activation and normalization in obtaining isometric embeddings at initialization},
  author={Amir Joudaki and Hadi Daneshmand and Francis R. Bach},
  booktitle={NeurIPS},
  year={2023}
}

@inproceedings{Daneshmand2021BatchNO,
  title={Batch Normalization Orthogonalizes Representations in Deep Random Networks},
  author={Hadi Daneshmand and Amir Joudaki and Francis R. Bach},
  booktitle={NeurIPS},
  year={2021}
}


@inproceedings{Xiong2020OnLN,
  title={On Layer Normalization in the Transformer Architecture},
  author={Ruibin Xiong and Yunchang Yang and Di He and Kai Zheng and Shuxin Zheng and Chen Xing and Huishuai Zhang and Yanyan Lan and Liwei Wang and Tie-Yan Liu},
  booktitle={ICML},
  year={2020}
}

@inproceedings{Wu2023Demystify,
  title={Demystifying Oversmoothing in Attention-Based Graph Neural Networks},
  author={Xinyi Wu and Amir Ajorlou and Zihui Wu and Ali Jadbabaie},
  booktitle={NeurIPS},
  year={2023}
}

@article{Geshkovski2023AMP,
  title={A mathematical perspective on Transformers},
  author={Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.10794}
}

@inproceedings{Geshkovski2023TheEO,
  title={The emergence of clusters in self-attention dynamics},
  author={Borjan Geshkovski and Cyril Letrouit and Yury Polyanskiy and Philippe Rigollet},
  booktitle={NeurIPS},
  year={2023}
}


@inproceedings{Shi2022RevisitingOI,
  title={Revisiting Over-smoothing in BERT from the Perspective of Graph},
  author={Han Shi and Jiahui Gao and Hang Xu and Xiaodan Liang and Zhenguo Li and Lingpeng Kong and Stephen M. S. Lee and James Tin-Yau Kwok},
  booktitle={ICLR},
  year={2022}
}

@inproceedings{Zaheer2020BigBT,
  title={Big Bird: Transformers for Longer Sequences},
  author={Manzil Zaheer and Guru Guruganesh and Kumar Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Onta{\~n}{\'o}n and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and Amr Ahmed},
  booktitle={NeurIPS},
  year={2020}
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={NeurIPS},
  year={2017}
}

@inproceedings{Kobayashi2020AttentionIN,
  title={Attention Is Not Only a Weight: Analyzing Transformers with Vector Norms},
  author={Goro Kobayashi and Tatsuki Kuribayashi and Sho Yokoi and Kentaro Inui},
  booktitle={EMNLP},
  year={2020},
}

@article{Beltagy2020LongformerTL,
  title={Longformer: The Long-Document Transformer},
  author={Iz Beltagy and Matthew E. Peters and Arman Cohan},
  journal={ArXiv},
  year={2020},
  volume={abs/2004.05150}
}

@article{radford2019language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{Brown2020LanguageMA,
  title={Language Models are Few-Shot Learners},
  author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeff Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
  booktitle={NeurIPS},
  year={2020}
}

@article{Child2019GeneratingLS,
  title={Generating Long Sequences with Sparse Transformers},
  author={Rewon Child and Scott Gray and Alec Radford and Ilya Sutskever},
  journal={ArXiv},
  year={2019},
  volume={abs/1904.10509}
}

@article{Roy2020EfficientCS,
  title={Efficient Content-Based Sparse Attention with Routing Transformers},
  author={Aurko Roy and Mohammad Taghi Saffar and Ashish Vaswani and David Grangier},
  journal={Transactions of the Association for Computational Linguistics},
  year={2020}
}

@article{hwang2024transformerfam,
      title={TransformerFAM: Feedback attention is working memory}, 
      author={Dongseong Hwang and Weiran Wang and Zhuoyuan Huo and Khe Chai Sim and Pedro Moreno Mengibar},
      year={2024},
      volume={abs/2404.09173},
     journal={ArXiv},
}

@inproceedings{He2023DeepTW,
  title={Deep Transformers without Shortcuts: Modifying Self-attention for Faithful Signal Propagation},
  author={Bobby He and James Martens and Guodong Zhang and Aleksandar Botev and Andy Brock and Samuel L. Smith and Yee Whye Teh},
  booktitle={ICLR},
  year={2023}
}


@inproceedings{Cai2021IsotropyIT,
  title={Isotropy in the Contextual Embedding Space: Clusters and Manifolds},
  author={Xingyu Cai and Jiaji Huang and Yu-Lan Bian and Kenneth Ward Church},
  booktitle={ICLR},
  year={2021}
}


@inproceedings{Ethayarajh2019HowCA,
  title={How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
  author={Kawin Ethayarajh},
  booktitle={EMNLP},
  year={2019}
}

@article{Rudelson2005SamplingFL,
  title={Sampling from large matrices: An approach through geometric functional analysis},
  author={Mark Rudelson and Roman Vershynin},
  journal={Journal of the ACM},
  year={2005}
}

@inproceedings{Devlin2019BERTPO,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={ACL},
  year={2019}
}

@inproceedings{Brody2023OnTE,
  title={On the Expressivity Role of LayerNorm in Transformers' Attention},
  author={Shaked Brody and Uri Alon and Eran Yahav},
  booktitle={ACL},
  year={2023}
}

@inproceedings{Wolf2020TransformersSN,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R{\'e}mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
  booktitle={EMNLP},
  year={2020}
}

@inproceedings{Paszke2019PyTorchAI,
  title={PyTorch: An Imperative Style, High-Performance Deep Learning Library},
  author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas K{\"o}pf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
  booktitle={NeurIPS},
  year={2019}
}

@article{Ba2016LayerN,
  title={Layer Normalization},
  author={Jimmy Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@inproceedings{Zhang2019RootMS,
  title={Root Mean Square Layer Normalization},
  author={Biao Zhang and Rico Sennrich},
  booktitle={NeurIPS},
  year={2019}
}

@inproceedings{Lan2019ALBERTAL,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
  booktitle={ICLR},
  year={2020}
}

@article{2020t5,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020}
}

@inproceedings{Gao2019RepresentationDP,
  title={Representation Degeneration Problem in Training Natural Language Generation Models},
  author={Jun Gao and Di He and Xu Tan and Tao Qin and Liwei Wang and Tie-Yan Liu},
  booktitle={ICLR},
  year={2019}
}


@inproceedings{AitSaada2023IsAT,
  title={Is Anisotropy Truly Harmful? A Case Study on Text Clustering},
  author={Mira Ait-Saada and Mohamed Nadif},
  booktitle={ACL},
  year={2023}
}


@inproceedings{Yun2019AreTU,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Chulhee Yun and Srinadh Bhojanapalli and Ankit Singh Rawat and Sashank J. Reddi and Sanjiv Kumar},
  booktitle={ICLR},
  year={2020}
}


@article{Mickus2024IsotropyCA,
  title={Isotropy, Clusters, and Classifiers},
  author={Timothee Mickus and Stig-Arne Gr{\"o}nroos and Joseph Attieh},
  journal={ArXiv},
  year={2024},
  volume={abs/2402.03191}
}


@article{Jiang2023Mistral7,
  title={Mistral 7B},
  author={Albert Qiaochu Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de Las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and L'elio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timoth{\'e}e Lacroix and William El Sayed},
  journal={ArXiv},
  year={2023}
}

@inproceedings{Xiao2023EfficientSL,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle={ICLR},
  year={2024}
}

@inproceedings{Bulatov2022RecurrentMT,
  title={Recurrent Memory Transformer},
  author={Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
  booktitle={NeurIPS},
  year={2022}
}


@article{Bulatov2023ScalingTT,
  title={Scaling Transformer to 1M tokens and beyond with RMT},
  author={Aydar Bulatov and Yuri Kuratov and Mikhail S. Burtsev},
  journal={ArXiv},
  year={2023},
  volume={abs/2304.11062}
}

@inproceedings{CV1,
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows},
  author={Ze Liu and Yutong Lin and Yue Cao and Han Hu and Yixuan Wei and Zheng Zhang and Stephen Lin and Baining Guo},
  booktitle={ICCV},
  year={2021}
}

@inproceedings{CV2,
  title={Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention},
  author={Xuran Pan and Tianzhu Ye and Zhuofan Xia and Shiji Song and Gao Huang},
  booktitle={CVPR},
  year={2023}
}

@inproceedings{CV3,
  title={Neighborhood Attention Transformer},
  author={Ali Hassani and Steven Walton and Jiacheng Li and Shengjia Li and Humphrey Shi},
  booktitle={CVPR},
  year={2023}
}