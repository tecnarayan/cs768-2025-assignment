\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahuja et~al.(1993)Ahuja, Magnanti, and Orlin]{networkflow}
Ravindra~K. Ahuja, Thomas~L. Magnanti, and James~B. Orlin.
\newblock \emph{Network Flows: Theory, Algorithms, and Applications}.
\newblock Prentice-Hall, Inc., Upper Saddle River, NJ, USA, 1993.
\newblock ISBN 0-13-617549-X.

\bibitem[Andrychowicz et~al.(2017)Andrychowicz, Wolski, Ray, Schneider, Fong,
  Welinder, McGrew, Tobin, Abbeel, and Zaremba]{andrychowicz2017hindsight}
Marcin Andrychowicz, Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong,
  Peter Welinder, Bob McGrew, Josh Tobin, OpenAI~Pieter Abbeel, and Wojciech
  Zaremba.
\newblock Hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5048--5058, 2017.

\bibitem[Asadi et~al.(2018)Asadi, Misra, and Littman]{asadi2018lipschitz}
Kavosh Asadi, Dipendra Misra, and Michael Littman.
\newblock Lipschitz continuity in model-based reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  264--273, 2018.

\bibitem[Bai et~al.(2019)Bai, Liu, Zhao, and Tang]{bai2019guided}
Chenjia Bai, Peng Liu, Wei Zhao, and Xianglong Tang.
\newblock Guided goal generation for hindsight multi-goal reinforcement
  learning.
\newblock \emph{Neurocomputing}, 2019.

\bibitem[Baranes and Oudeyer(2013)]{baranes2013active}
Adrien Baranes and Pierre-Yves Oudeyer.
\newblock Active learning of inverse models with intrinsically motivated goal
  exploration in robots.
\newblock \emph{Robotics and Autonomous Systems}, 61\penalty0 (1):\penalty0
  49--73, 2013.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openaigym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Colas et~al.(2018)Colas, Sigaud, and Oudeyer]{colas2018gep}
C{\'e}dric Colas, Olivier Sigaud, and Pierre-Yves Oudeyer.
\newblock Gep-pg: Decoupling exploration and exploitation in deep reinforcement
  learning algorithms.
\newblock In \emph{International Conference on Machine Learning}, pages
  1038--1047, 2018.

\bibitem[Colas et~al.(2019)Colas, Oudeyer, Sigaud, Fournier, and
  Chetouani]{colas2019curious}
C{\'e}dric Colas, Pierre-Yves Oudeyer, Olivier Sigaud, Pierre Fournier, and
  Mohamed Chetouani.
\newblock Curious: Intrinsically motivated modular multi-goal reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1331--1340, 2019.

\bibitem[Ding et~al.(2019)Ding, Florensa, Abbeel, and Phielipp]{ding2019goal}
Yiming Ding, Carlos Florensa, Pieter Abbeel, and Mariano Phielipp.
\newblock Goal-conditioned imitation learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Duan and Su(2012)]{duan2012scaling}
Ran Duan and Hsin-Hao Su.
\newblock A scaling algorithm for maximum weight matching in bipartite graphs.
\newblock In \emph{Proceedings of the twenty-third annual ACM-SIAM symposium on
  Discrete Algorithms}, pages 1413--1424. Society for Industrial and Applied
  Mathematics, 2012.

\bibitem[Ecoffet et~al.(2019)Ecoffet, Huizinga, Lehman, Stanley, and
  Clune]{ecoffet2019go}
Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth~O Stanley, and Jeff Clune.
\newblock Go-explore: a new approach for hard-exploration problems.
\newblock \emph{arXiv preprint arXiv:1901.10995}, 2019.

\bibitem[Eysenbach et~al.(2019)Eysenbach, Salakhutdinov, and
  Levine]{eysenbach2019search}
Benjamin Eysenbach, Ruslan Salakhutdinov, and Sergey Levine.
\newblock Search on the replay buffer: Bridging planning and reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Fang et~al.(2019{\natexlab{a}})Fang, Zhou, Shi, Gong, Xu, and
  Zhang]{fang2019dher}
Meng Fang, Cheng Zhou, Bei Shi, Boqing Gong, Jia Xu, and Tong Zhang.
\newblock Dher: Hindsight experience replay for dynamic goals.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Fang et~al.(2019{\natexlab{b}})Fang, Zhou, Du, Han, and
  Zhang]{fang2019curriculum}
Meng Fang, Tianyi Zhou, Yali Du, Lei Han, and Zhengyou Zhang.
\newblock Curriculum-guided hindsight experience replay.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{b}}.

\bibitem[Florensa et~al.(2017)Florensa, Held, Wulfmeier, Zhang, and
  Abbeel]{florensa2017reverse}
Carlos Florensa, David Held, Markus Wulfmeier, Michael Zhang, and Pieter
  Abbeel.
\newblock Reverse curriculum generation for reinforcement learning.
\newblock In \emph{Conference on Robot Learning}, pages 482--495, 2017.

\bibitem[Florensa et~al.(2018)Florensa, Held, Geng, and
  Abbeel]{florensa2018automatic}
Carlos Florensa, David Held, Xinyang Geng, and Pieter Abbeel.
\newblock Automatic goal generation for reinforcement learning agents.
\newblock In \emph{International Conference on Machine Learning}, pages
  1514--1523, 2018.

\bibitem[Forestier et~al.(2017)Forestier, Mollard, and
  Oudeyer]{forestier2017intrinsically}
S{\'e}bastien Forestier, Yoan Mollard, and Pierre-Yves Oudeyer.
\newblock Intrinsically motivated goal exploration processes with automatic
  curriculum learning.
\newblock \emph{arXiv preprint arXiv:1708.02190}, 2017.

\bibitem[Ghosh et~al.(2019)Ghosh, Gupta, and Levine]{ghosh2018learning}
Dibya Ghosh, Abhishek Gupta, and Sergey Levine.
\newblock Learning actionable representations with goal-conditioned policies.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Goyal et~al.(2019{\natexlab{a}})Goyal, Brakel, Fedus, Singhal,
  Lillicrap, Levine, Larochelle, and Bengio]{goyal2018recall}
Anirudh Goyal, Philemon Brakel, William Fedus, Soumye Singhal, Timothy
  Lillicrap, Sergey Levine, Hugo Larochelle, and Yoshua Bengio.
\newblock Recall traces: Backtracking models for efficient reinforcement
  learning.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Goyal et~al.(2019{\natexlab{b}})Goyal, Islam, Strouse, Ahmed,
  Botvinick, Larochelle, Levine, and Bengio]{goyal2019infobot}
Anirudh Goyal, Riashat Islam, Daniel Strouse, Zafarali Ahmed, Matthew
  Botvinick, Hugo Larochelle, Sergey Levine, and Yoshua Bengio.
\newblock Infobot: Transfer and exploration via the information bottleneck.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Huang et~al.(2019)Huang, Su, and Liu]{huang2019mapping}
Zhiao Huang, Hao Su, and Fangchen Liu.
\newblock Mapping state space using landmarks for universal goal reaching.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kaelbling(1993)]{kaelbling1993learning}
Leslie~Pack Kaelbling.
\newblock Learning to achieve goals.
\newblock In \emph{IJCAI}, pages 1094--1099. Citeseer, 1993.

\bibitem[Karatzoglou et~al.(2013)Karatzoglou, Baltrunas, and
  Shi]{Karatzoglou2013}
Alexandros Karatzoglou, Linas Baltrunas, and Yue Shi.
\newblock Learning to rank for recommender systems.
\newblock In \emph{Proceedings of the 7th ACM conference on Recommender
  systems}, pages 493--494. ACM, 2013.

\bibitem[Levine et~al.(2016)Levine, Finn, Darrell, and Abbeel]{Levine2016}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 1334--1373, 2016.

\bibitem[Levy et~al.(2019)Levy, Konidaris, Platt, and Saenko]{levy2019learning}
Andrew Levy, George Konidaris, Robert Platt, and Kate Saenko.
\newblock Learning multi-level hierarchies with hindsight.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2016continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Luo et~al.(2019)Luo, Xu, Li, Tian, Darrell, and
  Ma]{luo2019algorithmic}
Yuping Luo, Huazhe Xu, Yuanzhi Li, Yuandong Tian, Trevor Darrell, and Tengyu
  Ma.
\newblock Algorithmic framework for model-based deep reinforcement learning
  with theoretical guarantees.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Mao et~al.(2018{\natexlab{a}})Mao, Dong, and Lim]{mao2018universal}
Jiayuan Mao, Honghua Dong, and Joseph~J Lim.
\newblock Universal agent for disentangling environments and tasks.
\newblock In \emph{International Conference on Learning Representations},
  2018{\natexlab{a}}.

\bibitem[Mao et~al.(2018{\natexlab{b}})Mao, Li, Xie, Lau, Wang, and
  Smolley]{mao2018effectiveness}
Xudong Mao, Qing Li, Haoran Xie, Raymond Yiu~Keung Lau, Zhen Wang, and
  Stephen~Paul Smolley.
\newblock On the effectiveness of least squares generative adversarial
  networks.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2018{\natexlab{b}}.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529, 2015.

\bibitem[Munkres(1957)]{munkres1957algorithms}
James Munkres.
\newblock Algorithms for the assignment and transportation problems.
\newblock \emph{Journal of the society for industrial and applied mathematics},
  5\penalty0 (1):\penalty0 32--38, 1957.

\bibitem[Nachum et~al.(2018)Nachum, Gu, Lee, and Levine]{nachum2018data}
Ofir Nachum, Shixiang~Shane Gu, Honglak Lee, and Sergey Levine.
\newblock Data-efficient hierarchical reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3303--3313, 2018.

\bibitem[Nair et~al.(2018)Nair, Pong, Dalal, Bahl, Lin, and
  Levine]{nair2018visual}
Ashvin~V Nair, Vitchyr Pong, Murtaza Dalal, Shikhar Bahl, Steven Lin, and
  Sergey Levine.
\newblock Visual reinforcement learning with imagined goals.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9191--9200, 2018.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{Ng1999}
Andrew~Y Ng, Daishi Harada, and Stuart~J Russell.
\newblock Policy invariance under reward transformations: Theory and
  application to reward shaping.
\newblock In \emph{Proceedings of the Sixteenth International Conference on
  Machine Learning}, pages 278--287. Morgan Kaufmann Publishers Inc., 1999.

\bibitem[Oh et~al.(2017)Oh, Singh, Lee, and Kohli]{oh2017zero}
Junhyuk Oh, Satinder Singh, Honglak Lee, and Pushmeet Kohli.
\newblock Zero-shot task generalization with multi-task deep reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2661--2670, 2017.

\bibitem[Pathak et~al.(2018)Pathak, Mahmoudieh, Luo, Agrawal, Chen, Shentu,
  Shelhamer, Malik, Efros, and Darrell]{pathak2018zero}
Deepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide
  Shentu, Evan Shelhamer, Jitendra Malik, Alexei~A Efros, and Trevor Darrell.
\newblock Zero-shot visual imitation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[P{\'e}r{\'e} et~al.(2018)P{\'e}r{\'e}, Forestier, Sigaud, and
  Oudeyer]{pere2018unsupervised}
Alexandre P{\'e}r{\'e}, S{\'e}bastien Forestier, Olivier Sigaud, and
  Pierre-Yves Oudeyer.
\newblock Unsupervised learning of goal spaces for intrinsically motivated goal
  exploration.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Plappert et~al.(2018)Plappert, Andrychowicz, Ray, McGrew, Baker,
  Powell, Schneider, Tobin, Chociej, Welinder, et~al.]{plappert2018multi}
Matthias Plappert, Marcin Andrychowicz, Alex Ray, Bob McGrew, Bowen Baker,
  Glenn Powell, Jonas Schneider, Josh Tobin, Maciek Chociej, Peter Welinder,
  et~al.
\newblock Multi-goal reinforcement learning: Challenging robotics environments
  and request for research.
\newblock \emph{arXiv preprint arXiv:1802.09464}, 2018.

\bibitem[Pong et~al.(2019)Pong, Dalal, Lin, Nair, Bahl, and
  Levine]{pong2019skew}
Vitchyr~H Pong, Murtaza Dalal, Steven Lin, Ashvin Nair, Shikhar Bahl, and
  Sergey Levine.
\newblock Skew-fit: State-covering self-supervised reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1903.03698}, 2019.

\bibitem[Rauber et~al.(2019)Rauber, Ummadisingu, Mutz, and
  Schmidhuber]{rauber2019hindsight}
Paulo Rauber, Avinash Ummadisingu, Filipe Mutz, and J{\"u}rgen Schmidhuber.
\newblock Hindsight policy gradients.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  Wiele, Mnih, Heess, and Springenberg]{riedmiller2018learning}
Martin Riedmiller, Roland Hafner, Thomas Lampe, Michael Neunert, Jonas Degrave,
  Tom Wiele, Vlad Mnih, Nicolas Heess, and Jost~Tobias Springenberg.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{International Conference on Machine Learning}, pages
  4341--4350, 2018.

\bibitem[Sahni et~al.(2019)Sahni, Buckley, Abbeel, and
  Kuzovkin]{sahni2019addressing}
Himanshu Sahni, Toby Buckley, Pieter Abbeel, and Ilya Kuzovkin.
\newblock Addressing sample complexity in visual tasks using her and
  hallucinatory gans.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Schaul et~al.(2015)Schaul, Horgan, Gregor, and
  Silver]{schaul2015universal}
Tom Schaul, Daniel Horgan, Karol Gregor, and David Silver.
\newblock Universal value function approximators.
\newblock In \emph{International conference on machine learning}, pages
  1312--1320, 2015.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{schaul2016prioritized}
Tom Schaul, John Quan, Ioannis Antonoglou, and David Silver.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{Schulman2015}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1889--1897, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{Schulman2017ProximalPO}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{Silver16}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484, 2016.

\bibitem[Srinivas et~al.(2018)Srinivas, Jabri, Abbeel, Levine, and
  Finn]{srinivas2018universal}
Aravind Srinivas, Allan Jabri, Pieter Abbeel, Sergey Levine, and Chelsea Finn.
\newblock Universal planning networks: Learning generalizable representations
  for visuomotor control.
\newblock In \emph{International Conference on Machine Learning}, pages
  4739--4748, 2018.

\bibitem[Sukhbaatar et~al.(2018)Sukhbaatar, Lin, Kostrikov, Synnaeve, Szlam,
  and Fergus]{sukhbaatar2018intrinsic}
Sainbayar Sukhbaatar, Zeming Lin, Ilya Kostrikov, Gabriel Synnaeve, Arthur
  Szlam, and Rob Fergus.
\newblock Intrinsic motivation and automatic curricula via asymmetric
  self-play.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Sun et~al.(2019)Sun, Li, Liu, Lin, and Zhou]{sun2019policy}
Hao Sun, Zhizhong Li, Xiaotong Liu, Dahua Lin, and Bolei Zhou.
\newblock Policy continuation with hindsight inverse dynamics.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Szepesv{\'a}ri(1998)]{Szepesvari1998}
Csaba Szepesv{\'a}ri.
\newblock The asymptotic convergence-rate of q-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1064--1070, 1998.

\bibitem[Thomas et~al.(2017)Thomas, Pondard, Bengio, Sarfati, Beaudoin, Meurs,
  Pineau, Precup, and Bengio]{thomas2017independently}
Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe
  Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio.
\newblock Independently controllable features.
\newblock \emph{arXiv preprint arXiv:1708.01289}, 2017.

\bibitem[Veeriah et~al.(2018)Veeriah, Oh, and Singh]{veeriah2018many}
Vivek Veeriah, Junhyuk Oh, and Satinder Singh.
\newblock Many-goals reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1806.09605}, 2018.

\bibitem[Zhao and Tresp(2018)]{zhao2018energy}
Rui Zhao and Volker Tresp.
\newblock Energy-based hindsight experience prioritization.
\newblock In \emph{Conference on Robot Learning}, pages 113--122, 2018.

\bibitem[Zhao et~al.(2019)Zhao, Sun, and Tresp]{zhao2019maximum}
Rui Zhao, Xudong Sun, and Volker Tresp.
\newblock Maximum entropy-regularized multi-goal reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  7553--7562, 2019.

\end{thebibliography}
