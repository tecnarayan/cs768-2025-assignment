\begin{thebibliography}{43}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon et~al.(2015)Alon, Cesa-Bianchi, Dekel, and Koren]{alon2015online}
N.~Alon, N.~Cesa-Bianchi, O.~Dekel, and T.~Koren.
\newblock Online learning with feedback graphs: Beyond bandits.
\newblock \emph{Journal of Machine Learning Research}, 40\penalty0 (2015),
  2015.

\bibitem[Alon et~al.(2017)Alon, Cesa-Bianchi, Gentile, Mannor, Mansour, and
  Shamir]{alon2017nonstochastic}
N.~Alon, N.~Cesa-Bianchi, C.~Gentile, S.~Mannor, Y.~Mansour, and O.~Shamir.
\newblock Nonstochastic multi-armed bandits with graph-structured feedback.
\newblock \emph{SIAM Journal on Computing}, 46\penalty0 (6):\penalty0
  1785--1826, 2017.

\bibitem[Amir et~al.(2020)Amir, Attias, Koren, Mansour, and
  Livni]{amir2020prediction}
I.~Amir, I.~Attias, T.~Koren, Y.~Mansour, and R.~Livni.
\newblock Prediction with corrupted expert advice.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14315--14325, 2020.

\bibitem[Amir et~al.(2022)Amir, Azov, Koren, and Livni]{amir2022better}
I.~Amir, G.~Azov, T.~Koren, and R.~Livni.
\newblock Better best of both worlds bounds for bandits with switching costs.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Auer et~al.(2002{\natexlab{a}})Auer, Cesa-Bianchi, and
  Fischer]{auer2002finite}
P.~Auer, N.~Cesa-Bianchi, and P.~Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2):\penalty0 235--256,
  2002{\natexlab{a}}.

\bibitem[Auer et~al.(2002{\natexlab{b}})Auer, Cesa-Bianchi, Freund, and
  Schapire]{auer2002nonstochastic}
P.~Auer, N.~Cesa-Bianchi, Y.~Freund, and R.~E. Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77,
  2002{\natexlab{b}}.

\bibitem[Bubeck and Slivkins(2012)]{bubeck2012best}
S.~Bubeck and A.~Slivkins.
\newblock The best of both worlds: Stochastic and adversarial bandits.
\newblock In \emph{Conference on Learning Theory}, pages 42--1. JMLR Workshop
  and Conference Proceedings, 2012.

\bibitem[Caron et~al.(2012)Caron, Kveton, Lelarge, and
  Bhagat]{caron2012leveraging}
S.~Caron, B.~Kveton, M.~Lelarge, and S.~Bhagat.
\newblock Leveraging side observations in stochastic bandits.
\newblock In \emph{Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence}, pages 142--151, 2012.

\bibitem[Cohen et~al.(2016)Cohen, Hazan, and Koren]{cohen2016online}
A.~Cohen, T.~Hazan, and T.~Koren.
\newblock Online learning with feedback graphs without the graphs.
\newblock In \emph{International Conference on Machine Learning}, pages
  811--819. PMLR, 2016.

\bibitem[Erez and Koren(2021)]{erez2021towards}
L.~Erez and T.~Koren.
\newblock Towards best-of-all-worlds online learning with feedback graphs.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Esposito et~al.(2022)Esposito, Fusco, van~der Hoeven, and
  Cesa-Bianchi]{esposito2022learning}
E.~Esposito, F.~Fusco, D.~van~der Hoeven, and N.~Cesa-Bianchi.
\newblock Learning on the edge: Online learning with stochastic feedback
  graphs.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Freund and Schapire(1997)]{freund1997decision}
Y.~Freund and R.~E. Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Gaillard et~al.(2014)Gaillard, Stoltz, and
  Van~Erven]{gaillard2014second}
P.~Gaillard, G.~Stoltz, and T.~Van~Erven.
\newblock A second-order bound with excess losses.
\newblock In \emph{Conference on Learning Theory}, pages 176--196. PMLR, 2014.

\bibitem[Ghari and Shen(2022)]{ghari2022online}
P.~M. Ghari and Y.~Shen.
\newblock Online learning with probabilistic feedback.
\newblock In \emph{ICASSP 2022-2022 IEEE International Conference on Acoustics,
  Speech and Signal Processing (ICASSP)}, pages 4183--4187. IEEE, 2022.

\bibitem[Gupta et~al.(2019)Gupta, Koren, and Talwar]{gupta2019better}
A.~Gupta, T.~Koren, and K.~Talwar.
\newblock Better algorithms for stochastic bandits with adversarial
  corruptions.
\newblock In \emph{Conference on Learning Theory}, pages 1562--1578. PMLR,
  2019.

\bibitem[Hu et~al.(2020)Hu, Mehta, and Pan]{hu2020problem}
B.~Hu, N.~A. Mehta, and J.~Pan.
\newblock Problem-dependent regret bounds for online learning with feedback
  graphs.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 852--861.
  PMLR, 2020.

\bibitem[Ito(2021{\natexlab{a}})]{ito2021hybrid}
S.~Ito.
\newblock Hybrid regret bounds for combinatorial semi-bandits and adversarial
  linear bandits.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Ito(2021{\natexlab{b}})]{ito2021optimal}
S.~Ito.
\newblock On optimal robustness to adversarial corruption in online decision
  problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{b}}.

\bibitem[Ito(2021{\natexlab{c}})]{ito2021parameter}
S.~Ito.
\newblock Parameter-free multi-armed bandit algorithms with hybrid
  data-dependent regret bounds.
\newblock In \emph{Conference on Learning Theory}, pages 2552--2583. PMLR,
  2021{\natexlab{c}}.

\bibitem[Ito(2022)]{ito2022revisiting}
S.~Ito.
\newblock Revisiting online submodular minimization: Gap-dependent regret
  bounds, best of both worlds and adversarial robustness.
\newblock In \emph{International Conference on Machine Learning}, pages
  9678--9694. PMLR, 2022.

\bibitem[Ito et~al.(2022)Ito, Tsuchiya, and Honda]{ito2022adversarially}
S.~Ito, T.~Tsuchiya, and J.~Honda.
\newblock Adversarially robust multi-armed bandit algorithm with
  variance-dependent regret bounds.
\newblock In \emph{Conference on Learning Theory}, pages 1421--1422. PMLR,
  2022.

\bibitem[Jin and Luo(2020)]{jin2020simultaneously}
T.~Jin and H.~Luo.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16557--16566, 2020.

\bibitem[Jin et~al.(2021)Jin, Huang, and Luo]{jin2021best}
T.~Jin, L.~Huang, and H.~Luo.
\newblock The best of both worlds: stochastic and adversarial episodic mdps
  with unknown transition.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Koc{\'a}k et~al.(2016)Koc{\'a}k, Neu, and Valko]{kocak2016online}
T.~Koc{\'a}k, G.~Neu, and M.~Valko.
\newblock Online learning with erd{\H{o}}s-r{\'e}nyi side-observation graphs.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2016.

\bibitem[Kong et~al.(2022)Kong, Zhou, and Li]{kong2022simultaneously}
F.~Kong, Y.~Zhou, and S.~Li.
\newblock Simultaneously learning stochastic and adversarial bandits with
  general graph feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  11473--11482. PMLR, 2022.

\bibitem[Lai et~al.(1985)Lai, Robbins, et~al.]{lai1985asymptotically}
T.~L. Lai, H.~Robbins, et~al.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2018bandit}
T.~Lattimore and C.~Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lee et~al.(2021)Lee, Luo, Wei, Zhang, and Zhang]{lee2021achieving}
C.-W. Lee, H.~Luo, C.-Y. Wei, M.~Zhang, and X.~Zhang.
\newblock Achieving near instance-optimality and minimax-optimality in
  stochastic and adversarial linear bandits simultaneously.
\newblock In \emph{International Conference on Machine Learning}, pages
  6142--6151, 2021.

\bibitem[Littlestone and Warmuth(1994)]{littlestone1994weighted}
N.~Littlestone and M.~K. Warmuth.
\newblock The weighted majority algorithm.
\newblock \emph{Information and computation}, 108\penalty0 (2):\penalty0
  212--261, 1994.

\bibitem[Lu et~al.(2021{\natexlab{a}})Lu, Hu, and
  Zhang]{lu2021stochastic-nonstationary}
S.~Lu, Y.~Hu, and L.~Zhang.
\newblock Stochastic bandits with graph feedback in non-stationary
  environments.
\newblock In \emph{Proceedings of the 35th AAAI Conference on Artificial
  Intelligence (AAAI)}, 2021{\natexlab{a}}.

\bibitem[Lu et~al.(2021{\natexlab{b}})Lu, Wang, and
  Zhang]{lu2021stochastic-corruptions}
S.~Lu, G.~Wang, and L.~Zhang.
\newblock Stochastic graphical bandits with adversarial corruptions.
\newblock In \emph{Proceedings of the 35th AAAI Conference on Artificial
  Intelligence (AAAI)}, 2021{\natexlab{b}}.

\bibitem[Luo and Schapire(2015)]{luo2015achieving}
H.~Luo and R.~E. Schapire.
\newblock Achieving all with no parameters: {AdaNormalHedge}.
\newblock In \emph{Conference on Learning Theory}, pages 1286--1304. PMLR,
  2015.

\bibitem[Lykouris et~al.(2018)Lykouris, Mirrokni, and
  Paes~Leme]{lykouris2018stochastic}
T.~Lykouris, V.~Mirrokni, and R.~Paes~Leme.
\newblock Stochastic bandits robust to adversarial corruptions.
\newblock In \emph{Proceedings of the 50th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 114--122, 2018.

\bibitem[Mannor and Shamir(2011)]{mannor2011bandits}
S.~Mannor and O.~Shamir.
\newblock From bandits to experts: On the value of side-observations.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Masoudian et~al.(2022)Masoudian, Zimmert, and
  Seldin]{masoudian2022best}
S.~Masoudian, J.~Zimmert, and Y.~Seldin.
\newblock A best-of-both-worlds algorithm for bandits with delayed feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[McMahan(2011)]{mcmahan2011follow}
B.~McMahan.
\newblock Follow-the-regularized-leader and mirror descent: Equivalence
  theorems and l1 regularization.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 525--533. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Rouyer et~al.(2021)Rouyer, Seldin, and
  Cesa-Bianchi]{rouyer2021algorithm}
C.~Rouyer, Y.~Seldin, and N.~Cesa-Bianchi.
\newblock An algorithm for stochastic and adversarial bandits with switching
  costs.
\newblock In \emph{International Conference on Machine Learning}, pages
  9127--9135. PMLR, 2021.

\bibitem[Rouyer et~al.(2022)Rouyer, van~der Hoeven, Cesa-Bianchi, and
  Seldin]{rouyer2022near}
C.~Rouyer, D.~van~der Hoeven, N.~Cesa-Bianchi, and Y.~Seldin.
\newblock A near-optimal best-of-both-worlds algorithm for online learning with
  feedback graphs.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Seldin and Lugosi(2017)]{seldin2017improved}
Y.~Seldin and G.~Lugosi.
\newblock An improved parametrization and analysis of the {EXP}3++ algorithm
  for stochastic and adversarial bandits.
\newblock In \emph{Conference on Learning Theory}, pages 1743--1759. PMLR,
  2017.

\bibitem[Seldin and Slivkins(2014)]{seldin2014one}
Y.~Seldin and A.~Slivkins.
\newblock One practical algorithm for both stochastic and adversarial bandits.
\newblock In \emph{International Conference on Machine Learning}, pages
  1287--1295. PMLR, 2014.

\bibitem[Wei and Luo(2018)]{wei2018more}
C.-Y. Wei and H.~Luo.
\newblock More adaptive algorithms for adversarial bandits.
\newblock In \emph{Conference on Learning Theory}, pages 1263--1291, 2018.

\bibitem[Zimmert and Seldin(2021)]{zimmert2021tsallis}
J.~Zimmert and Y.~Seldin.
\newblock Tsallis-{INF}: An optimal algorithm for stochastic and adversarial
  bandits.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0
  (28):\penalty0 1--49, 2021.

\bibitem[Zimmert et~al.(2019)Zimmert, Luo, and Wei]{zimmert2019beating}
J.~Zimmert, H.~Luo, and C.-Y. Wei.
\newblock Beating stochastic and adversarial semi-bandits optimally and
  simultaneously.
\newblock In \emph{International Conference on Machine Learning}, pages
  7683--7692. PMLR, 2019.

\end{thebibliography}
