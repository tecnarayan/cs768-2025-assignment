\begin{thebibliography}{10}

\bibitem{allen2017katyusha}
Zeyuan Allen-Zhu.
\newblock Katyusha: The first direct acceleration of stochastic gradient
  methods.
\newblock In {\em Proceedings of the 49th Annual ACM SIGACT Symposium on Theory
  of Computing}, pages 1200--1205. ACM, 2017.

\bibitem{allen2014linear}
Zeyuan Allen-Zhu and Lorenzo Orecchia.
\newblock Linear coupling: {A}n ultimate unification of gradient and mirror
  descent.
\newblock In {\em Innovations in Theoretical Computer Science}, 2017.

\bibitem{allen2016even}
Zeyuan Allen-Zhu, Zheng Qu, Peter Richt\'{a}rik, and Yang Yuan.
\newblock Even faster accelerated coordinate descent using non-uniform
  sampling.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48 of {\em Proceedings of Machine Learning Research}, pages
  1110--1119, 2016.

\bibitem{beck2009fista}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock {\em SIAM Journal on Imaging Sciences}, 2(1):183--202, 2009.

\bibitem{RDS}
El~Houcine Bergou, Peter Richt{\'a}rik, and Eduard Gorbunov.
\newblock Random direct search method for minimizing nonconvex, convex and
  strongly convex functions.
\newblock {\em Manuscript}, 2018.

\bibitem{scp}
Antonin Chambolle, Matthias~J. Ehrhardt, Peter Richt\'{a}rik, and
  Carola-Bibiane Sch\"{o}enlieb.
\newblock Stochastic primal-dual hybrid gradient algorithm with arbitrary
  sampling and imaging applications.
\newblock {\em SIAM Journal on Optimization}, 28(4):2783â€“2808, 2018.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Lib{SVM}: {A} library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):27, 2011.

\bibitem{conn2009introduction}
Andrew~R Conn, Katya Scheinberg, and Luis~N Vicente.
\newblock {\em Introduction to derivative-free optimization}, volume~8.
\newblock Siam, 2009.

\bibitem{d2008smooth}
Alexandre d'Aspremont.
\newblock Smooth optimization with approximate gradient.
\newblock {\em SIAM Journal on Optimization}, 19(3):1171--1183, 2008.

\bibitem{SAGA}
Aaron Defazio, Francis Bach, and Simon Lacoste-Julien.
\newblock {SAGA}: A fast incremental gradient method with support for
  non-strongly convex composite objectives.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1646--1654, 2014.

\bibitem{Devolder:2011inexact}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock {\em Mathematical Programming}, 146(1-2):37--75, 2014.

\bibitem{APPROX}
Olivier Fercoq and Peter Richt\'{a}rik.
\newblock Accelerated, parallel and proximal coordinate descent.
\newblock {\em SIAM Journal on Optimization}, (25):1997--2023, 2015.

\bibitem{gower2018stochastic}
Robert~M Gower, Peter Richt{\'a}rik, and Francis Bach.
\newblock Stochastic quasi-gradient methods: Variance reduction via {J}acobian
  sketching.
\newblock {\em arXiv preprint arXiv:1805.02632}, 2018.

\bibitem{SBFGS}
Robert~Mansel Gower, Donald Goldfarb, and Peter Richt\'{a}rik.
\newblock Stochastic block {BFGS}: squeezing more curvature out of data.
\newblock In {\em 33rd International Conference on Machine Learning}, pages
  1869--1878, 2016.

\bibitem{ASBFGS}
Robert~Mansel Gower, Filip Hanzely, Peter Richt\'{a}rik, and Sebastian Stich.
\newblock Accelerated stochastic matrix inversion: general theory and speeding
  up {BFGS} rules for faster second-order optimization.
\newblock {\em arXiv:1802.04079}, 2018.

\bibitem{SIMAX2015}
Robert~Mansel Gower and Peter Richt\'{a}rik.
\newblock Randomized iterative methods for linear systems.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  36(4):1660--1690, 2015.

\bibitem{SDA}
Robert~Mansel Gower and Peter Richt\'{a}rik.
\newblock Stochastic dual ascent for solving linear systems.
\newblock {\em arXiv preprint arXiv:1512.06890}, 2015.

\bibitem{PSEUDOINVERSE}
Robert~Mansel Gower and Peter Richt\'{a}rik.
\newblock Linearly convergent randomized iterative methods for computing the
  pseudoinverse.
\newblock {\em arXiv:1612.06255}, 2016.

\bibitem{inverse}
Robert~Mansel Gower and Peter Richt\'{a}rik.
\newblock Randomized quasi-{N}ewton updates are linearly convergent matrix
  inversion algorithms.
\newblock {\em SIAM Journal on Matrix Analysis and Applications},
  38(4):1380--1409, 2017.

\bibitem{AccMbCd}
Filip Hanzely and Peter Richt{\'a}rik.
\newblock Accelerated coordinate descent with arbitrary sampling and best rates
  for minibatches.
\newblock {\em arXiv preprint arXiv:1809.09354}, 2018.

\bibitem{hooke1961direct}
Robert Hooke and Terry~A Jeeves.
\newblock ``{D}irect search'' solution of numerical and statistical problems.
\newblock {\em Journal of the ACM (JACM)}, 8(2):212--229, 1961.

\bibitem{SVRG}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{L}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{kolda2003optimization}
Tamara~G Kolda, Robert~Michael Lewis, and Virginia Torczon.
\newblock Optimization by direct search: {N}ew perspectives on some classical
  and modern methods.
\newblock {\em SIAM Review}, 45(3):385--482, 2003.

\bibitem{konevcny2014simple}
Jakub Kone{\v{c}}n{\'y} and Peter Richt{\'a}rik.
\newblock Simple complexity analysis of simplified direct search.
\newblock {\em arXiv preprint arXiv:1410.0390}, 2014.

\bibitem{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3384--3392, 2015.

\bibitem{lin2014accelerated}
Qihang Lin, Zhaosong Lu, and Lin Xiao.
\newblock An accelerated proximal coordinate gradient method.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3059--3067, 2014.

\bibitem{agossip}
Nicoas Loizou and Peter Richt\'{a}rik.
\newblock Accelerated gossip via stochastic heavy ball method.
\newblock In {\em 56th Annual Allerton Conference on Communication, Control,
  and Computing}, 2018.

\bibitem{NEW-PERSPECTIVE}
Nicolas Loizou and Peter Richt\'{a}rik.
\newblock A new perspective on randomized gossip algorithms.
\newblock In {\em IEEE Global Conference on Signal and Information Processing
  (GlobalSIP)}, pages 440--444, 2016.

\bibitem{SHB-NIPS}
Nicolas Loizou and Peter Richt\'{a}rik.
\newblock Linearly convergent stochastic heavy ball method for minimizing
  generalization error.
\newblock In {\em NIPS Workshop on Optimization for Machine Learning}, 2017.

\bibitem{SMOMENTUM}
Nicolas Loizou and Peter Richt\'{a}rik.
\newblock Momentum and stochastic momentum for stochastic gradient, {N}ewton,
  proximal point and subspace descent methods.
\newblock {\em arXiv:1712.09677}, 2017.

\bibitem{SPM}
Ion Necoara, Peter Richt\'{a}rik, and Andrei Patrascu.
\newblock Randomized projection methods for convex feasibility problems:
  conditioning and convergence rates.
\newblock {\em arXiv:1801.04873}, 2018.

\bibitem{nesterov1983method}
Yurii Nesterov.
\newblock A method of solving a convex programming problem with convergence
  rate ${O}(1/k^2)$.
\newblock {\em Soviet Mathematics Doklady}, 27(2):372--376, 1983.

\bibitem{NesterovBook}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: {A} basic course}.
\newblock Kluwer Academic Publishers, 2004.

\bibitem{Nesterov05:smooth}
Yurii Nesterov.
\newblock Smooth minimization of nonsmooth functions.
\newblock {\em Mathematical Programming}, 103:127--152, 2005.

\bibitem{Nesterov:2010RCDM}
Yurii Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nguyen2017sarah}
Lam~M. Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning}, volume~70 of {\em Proceedings of Machine Learning Research}, pages
  2613--2621. PMLR, 2017.

\bibitem{ALPHA}
Zheng Qu and Peter Richt{\'a}rik.
\newblock Coordinate descent with arbitrary sampling {I}: {A}lgorithms and
  complexity.
\newblock {\em Optimization Methods and Software}, 31(5):829--857, 2016.

\bibitem{alfa}
Zheng Qu and Peter Richt{\'a}rik.
\newblock Coordinate descent with arbitrary sampling {I}: {A}lgorithms and
  complexity.
\newblock {\em Optimization Methods and Software}, 31(5):829--857, 2016.

\bibitem{ESO}
Zheng Qu and Peter Richt\'{a}rik.
\newblock Coordinate descent with arbitrary sampling {II}: {E}xpected separable
  overapproximation.
\newblock {\em Optimization Methods and Software}, 31(5):858--884, 2016.

\bibitem{SDNA}
Zheng Qu, Peter Richt\'{a}rik, Martin Tak\'{a}\v{c}, and Olivier Fercoq.
\newblock {SDNA}: {S}tochastic dual {N}ewton ascent for empirical risk
  minimization.
\newblock In {\em Proceedings of The 33rd International Conference on Machine
  Learning}, volume~48 of {\em Proceedings of Machine Learning Research}, pages
  1823--1832. PMLR, 2016.

\bibitem{quartz}
Zheng Qu, Peter Richt{\'a}rik, and Tong Zhang.
\newblock Quartz: Randomized dual coordinate ascent with arbitrary sampling.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  865--873, 2015.

\bibitem{NSync}
Peter Richt{\'a}rik and Martin Tak\'{a}\v{c}.
\newblock On optimal probabilities in stochastic coordinate descent methods.
\newblock {\em Optimization Letters}, 10(6):1233--1243, 2016.

\bibitem{ASDA}
Peter Richt\'{a}rik and Martin Tak\'{a}\v{c}.
\newblock Stochastic reformulations of linear systems: algorithms and
  convergence theory.
\newblock {\em arXiv:1706.01108}, 2017.

\bibitem{RobbinsMonro:1951}
H.~Robbins and S.~Monro.
\newblock A stochastic approximation method.
\newblock {\em Annals of Mathematical Statistics}, 22:400--407, 1951.

\bibitem{SAG}
Nicolas~Le Roux, Mark Schmidt, and Francis Bach.
\newblock A stochastic gradient method with an exponential convergence rate for
  finite training sets.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2663--2671, 2012.

\bibitem{schmidt2011convergence}
Mark Schmidt, Nicolas~L Roux, and Francis~R Bach.
\newblock Convergence rates of inexact proximal-gradient methods for convex
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1458--1466, 2011.

\bibitem{ProxSDCA}
Shai Shalev-Shwartz and Tong Zhang.
\newblock Proximal stochastic dual coordinate ascent.
\newblock {\em arXiv preprint arXiv:1211.2717}, 2012.

\end{thebibliography}
