\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Rumelhart et~al.(1986)Rumelhart, Hinton, and
  Williams]{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Nature}, 323\penalty0 (6088):\penalty0 533--536, 1986.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Richards et~al.(2019)Richards, Lillicrap, Beaudoin, Bengio, Bogacz,
  Christensen, Clopath, Costa, de~Berker, Ganguli, et~al.]{richards2019deep}
Blake~A Richards, Timothy~P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal
  Bogacz, Amelia Christensen, Claudia Clopath, Rui~Ponte Costa, Archy
  de~Berker, Surya Ganguli, et~al.
\newblock A deep learning framework for neuroscience.
\newblock \emph{Nature neuroscience}, 22\penalty0 (11):\penalty0 1761--1770,
  2019.

\bibitem[Kandel et~al.(2000)Kandel, Schwartz, Jessell, Siegelbaum, Hudspeth,
  and Mack]{kandel2000principles}
Eric~R Kandel, James~H Schwartz, Thomas~M Jessell, Steven Siegelbaum, A~James
  Hudspeth, and Sarah Mack.
\newblock \emph{Principles of neural science}, volume~4.
\newblock McGraw-hill New York, 2000.

\bibitem[Crick(1989)]{crick1989recent}
Francis Crick.
\newblock The recent excitement about neural networks.
\newblock \emph{Nature}, 337\penalty0 (6203):\penalty0 129--132, 1989.

\bibitem[Whittington and Bogacz(2019)]{whittington2019theories}
James~CR Whittington and Rafal Bogacz.
\newblock Theories of error back-propagation in the brain.
\newblock \emph{Trends in cognitive sciences}, 23\penalty0 (3):\penalty0
  235--250, 2019.

\bibitem[Lillicrap et~al.(2020)Lillicrap, Santoro, Marris, Akerman, and
  Hinton]{lillicrap2020backpropagation}
Timothy~P Lillicrap, Adam Santoro, Luke Marris, Colin~J Akerman, and Geoffrey
  Hinton.
\newblock Backpropagation and the brain.
\newblock \emph{Nature Reviews Neuroscience}, 21\penalty0 (6):\penalty0
  335--346, 2020.

\bibitem[Latham(2019)]{latham_2019}
Peter Latham.
\newblock Node perturbation in vanilla deep networks, Feb 2019.
\newblock URL
  \url{http://www.gatsby.ucl.ac.uk/~pel/tn/notes/node_perturbation.pdf}.

\bibitem[Mazzoni et~al.(1991)Mazzoni, Andersen, and Jordan]{mazzoni1991more}
Pietro Mazzoni, Richard~A Andersen, and Michael~I Jordan.
\newblock A more biologically plausible learning rule for neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 88\penalty0
  (10):\penalty0 4433--4437, 1991.

\bibitem[Williams(1992)]{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Werfel et~al.(2005)Werfel, Xie, and Seung]{werfel2005learning}
Justin Werfel, Xiaohui Xie, and H~Sebastian Seung.
\newblock Learning curves for stochastic gradient descent in linear feedforward
  networks.
\newblock \emph{Neural computation}, 17\penalty0 (12):\penalty0 2699--2718,
  2005.

\bibitem[Grossberg(1987)]{grossberg1987competitive}
Stephen Grossberg.
\newblock Competitive learning: From interactive activation to adaptive
  resonance.
\newblock \emph{Cognitive science}, 11\penalty0 (1):\penalty0 23--63, 1987.

\bibitem[Lillicrap et~al.(2016)Lillicrap, Cownden, Tweed, and
  Akerman]{lillicrap2016random}
Timothy~P Lillicrap, Daniel Cownden, Douglas~B Tweed, and Colin~J Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock \emph{Nature communications}, 7\penalty0 (1):\penalty0 1--10, 2016.

\bibitem[N\o~kland(2016)]{nokland2016direct}
Arild N\o~kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29, 2016.

\bibitem[Samadi et~al.(2017)Samadi, Lillicrap, and Tweed]{samadi2017deep}
Arash Samadi, Timothy~P Lillicrap, and Douglas~B Tweed.
\newblock Deep learning with dynamic spiking neurons and fixed feedback
  weights.
\newblock \emph{Neural computation}, 29\penalty0 (3):\penalty0 578--602, 2017.

\bibitem[Launay et~al.(2020)Launay, Poli, Boniface, and
  Krzakala]{launay2020direct}
Julien Launay, Iacopo Poli, Fran\c{c}ois Boniface, and Florent Krzakala.
\newblock Direct feedback alignment scales to modern deep learning tasks and
  architectures.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Bartunov et~al.(2018)Bartunov, Santoro, Richards, Marris, Hinton, and
  Lillicrap]{bartunov2018assessing}
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey~E Hinton,
  and Timothy Lillicrap.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Lechner(2019)]{lechner2019learning}
Mathias Lechner.
\newblock Learning representations for binary-classification without
  backpropagation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Launay et~al.(2019)Launay, Poli, and Krzakala]{launay2019principled}
Julien Launay, Iacopo Poli, and Florent Krzakala.
\newblock Principled training of neural networks with direct feedback
  alignment.
\newblock \emph{arXiv preprint arXiv:1906.04554}, 2019.

\bibitem[Refinetti et~al.(2020)Refinetti, d'Ascoli, Ohana, and
  Goldt]{refinetti2020dynamics}
Maria Refinetti, St{\'e}phane d'Ascoli, Ruben Ohana, and Sebastian Goldt.
\newblock The dynamics of learning with feedback alignment.
\newblock \emph{arXiv preprint arXiv:2011.12428}, 2020.

\bibitem[Guerguiev et~al.(2017)Guerguiev, Lillicrap, and
  Richards]{guerguiev2017towards}
Jordan Guerguiev, Timothy~P Lillicrap, and Blake~A Richards.
\newblock Towards deep learning with segregated dendrites.
\newblock \emph{ELife}, 6:\penalty0 e22901, 2017.

\bibitem[Richards and Lillicrap(2019)]{richards2019dendritic}
Blake~A Richards and Timothy~P Lillicrap.
\newblock Dendritic solutions to the credit assignment problem.
\newblock \emph{Current opinion in neurobiology}, 54:\penalty0 28--36, 2019.

\bibitem[Payeur et~al.(2021)Payeur, Guerguiev, Zenke, Richards, and
  Naud]{payeur2021burst}
Alexandre Payeur, Jordan Guerguiev, Friedemann Zenke, Blake~A Richards, and
  Richard Naud.
\newblock Burst-dependent synaptic plasticity can coordinate learning in
  hierarchical circuits.
\newblock \emph{Nature neuroscience}, pages 1--10, 2021.

\bibitem[Fr{\'e}maux and Gerstner(2016)]{fremaux2016neuromodulated}
Nicolas Fr{\'e}maux and Wulfram Gerstner.
\newblock Neuromodulated spike-timing-dependent plasticity, and theory of
  three-factor learning rules.
\newblock \emph{Frontiers in neural circuits}, 9:\penalty0 85, 2016.

\bibitem[Ku{\'s}mierz et~al.(2017)Ku{\'s}mierz, Isomura, and
  Toyoizumi]{kusmierz2017learning}
{\L}ukasz Ku{\'s}mierz, Takuya Isomura, and Taro Toyoizumi.
\newblock Learning with three factors: modulating hebbian plasticity with
  errors.
\newblock \emph{Current opinion in neurobiology}, 46:\penalty0 170--177, 2017.

\bibitem[Raman and O’Leary(2021)]{raman2021frozen}
Dhruva~V Raman and Timothy O’Leary.
\newblock Frozen algorithms: how the brain's wiring facilitates learning.
\newblock \emph{Current Opinion in Neurobiology}, 67:\penalty0 207--214, 2021.

\bibitem[Balduzzi et~al.(2015)Balduzzi, Vanchinathan, and
  Buhmann]{balduzzi2015kickback}
David Balduzzi, Hastagiri Vanchinathan, and Joachim Buhmann.
\newblock Kickback cuts backprop's red-tape: Biologically plausible credit
  assignment in neural networks.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem[Bernstein et~al.(2018)Bernstein, Wang, Azizzadenesheli, and
  Anandkumar]{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Animashree
  Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  560--569. PMLR, 2018.

\bibitem[Liao et~al.(2016)Liao, Leibo, and Poggio]{liao2016important}
Qianli Liao, Joel Leibo, and Tomaso Poggio.
\newblock How important is weight symmetry in backpropagation?
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30, 2016.

\bibitem[Xiao et~al.(2018)Xiao, Chen, Liao, and Poggio]{xiao2018biologically}
Will Xiao, Honglin Chen, Qianli Liao, and Tomaso Poggio.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Moskovitz et~al.(2018)Moskovitz, Litwin-Kumar, and
  Abbott]{moskovitz2018feedback}
Theodore~H Moskovitz, Ashok Litwin-Kumar, and LF~Abbott.
\newblock Feedback alignment in deep convolutional networks.
\newblock \emph{arXiv preprint arXiv:1812.06488}, 2018.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Clement Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\bibitem[Lee et~al.(2020)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Roman Novak,
  Jascha Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (12), dec 2020.

\bibitem[wik(2021)]{wikipedia_2021}
Block matrix, Jul 2021.
\newblock URL \url{https://en.wikipedia.org/wiki/Block_matrix}.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
A~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master's thesis, University of Tronto}, 2009.

\bibitem[Chan et~al.(2018)Chan, Rao, Huang, and Canny]{chan2018t}
David~M Chan, Roshan Rao, Forrest Huang, and John~F Canny.
\newblock t-sne-cuda: Gpu-accelerated t-sne and its applications to modern
  data.
\newblock In \emph{2018 30th International Symposium on Computer Architecture
  and High Performance Computing (SBAC-PAD)}, pages 330--338. IEEE, 2018.

\bibitem[Cao et~al.(2020)Cao, Summerfield, and Saxe]{cao2020characterizing}
Yinan Cao, Christopher Summerfield, and Andrew Saxe.
\newblock Characterizing emergent representations in a space of candidate
  learning rules for deep networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Nayebi et~al.(2020)Nayebi, Srivastava, Ganguli, and
  Yamins]{nayebi2020identifying}
Aran Nayebi, Sanjana Srivastava, Surya Ganguli, and Daniel~L Yamins.
\newblock Identifying learning rules from neural network observables.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Pogodin et~al.(2021)Pogodin, Mehta, Lillicrap, and
  Latham]{pogodin2021towards}
Roman Pogodin, Yash Mehta, Timothy~P Lillicrap, and Peter~E Latham.
\newblock Towards biologically plausible convolutional networks.
\newblock \emph{arXiv preprint arXiv:2106.13031}, 2021.

\bibitem[Veness et~al.(2019)Veness, Lattimore, Budden, Bhoopchand, Mattern,
  Grabska-Barwinska, Sezener, Wang, Toth, Schmitt, et~al.]{veness2019gated}
Joel Veness, Tor Lattimore, David Budden, Avishkar Bhoopchand, Christopher
  Mattern, Agnieszka Grabska-Barwinska, Eren Sezener, Jianan Wang, Peter Toth,
  Simon Schmitt, et~al.
\newblock Gated linear networks.
\newblock \emph{arXiv preprint arXiv:1910.01526}, 2019.

\bibitem[Budden et~al.(2020)Budden, Marblestone, Sezener, Lattimore, Wayne, and
  Veness]{budden2020gaussian}
David Budden, Adam Marblestone, Eren Sezener, Tor Lattimore, Gregory Wayne, and
  Joel Veness.
\newblock Gaussian gated linear networks.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Sezener et~al.(2021)Sezener, Grabska-Barwinska, Kostadinov, Beau,
  Krishnagopal, Budden, Hutter, Veness, Botvinick, Clopath,
  et~al.]{sezener2021rapid}
Eren Sezener, Agnieszka Grabska-Barwinska, Dimitar Kostadinov, Maxime Beau,
  Sanjukta Krishnagopal, David Budden, Marcus Hutter, Joel Veness, Matthew
  Botvinick, Claudia Clopath, et~al.
\newblock A rapid and efficient learning rule for biological neural circuits.
\newblock \emph{bioRxiv}, 2021.

\bibitem[Lakshminarayanan and Vikram~Singh(2020)]{lakshminarayanan2020neural}
Chandrashekar Lakshminarayanan and Amit Vikram~Singh.
\newblock Neural path features and neural path kernel : Understanding the role
  of gates in deep learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Stanley(1968)]{stanley1968dependence}
H~Eugene Stanley.
\newblock Dependence of critical properties on dimensionality of spins.
\newblock \emph{Physical Review Letters}, 20\penalty0 (12):\penalty0 589, 1968.

\bibitem[Sabour et~al.(2017)Sabour, Frosst, and Hinton]{sabour2017dynamic}
Sara Sabour, Nicholas Frosst, and Geoffrey~E Hinton.
\newblock Dynamic routing between capsules.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, 2017.

\bibitem[Hinton(2021)]{hinton2021represent}
Geoffrey Hinton.
\newblock How to represent part-whole hierarchies in a neural network.
\newblock \emph{arXiv preprint arXiv:2102.12627}, 2021.

\bibitem[Deng et~al.(2021)Deng, Litany, Duan, Poulenard, Tagliasacchi, and
  Guibas]{deng2021vector}
Congyue Deng, Or~Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and
  Leonidas Guibas.
\newblock Vector neurons: A general framework for so (3)-equivariant networks.
\newblock \emph{arXiv preprint arXiv:2104.12229}, 2021.

\bibitem[Akrout et~al.(2019)Akrout, Wilson, Humphreys, Lillicrap, and
  Tweed]{akrout2019deep}
Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, and
  Douglas~B Tweed.
\newblock Deep learning without weight transport.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Kunin et~al.(2020)Kunin, Nayebi, Sagastuy-Brena, Ganguli, Bloom, and
  Yamins]{kunin2020two}
Daniel Kunin, Aran Nayebi, Javier Sagastuy-Brena, Surya Ganguli, Jonathan
  Bloom, and Daniel Yamins.
\newblock Two routes to scalable credit assignment without weight symmetry.
\newblock In \emph{International Conference on Machine Learning}, pages
  5511--5521. PMLR, 2020.

\bibitem[Bengio(2014)]{bengio2014auto}
Yoshua Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock \emph{arXiv preprint arXiv:1407.7906}, 2014.

\bibitem[Mostafa et~al.(2018)Mostafa, Ramesh, and
  Cauwenberghs]{mostafa2018deep}
Hesham Mostafa, Vishwajith Ramesh, and Gert Cauwenberghs.
\newblock Deep supervised learning using local errors.
\newblock \emph{Frontiers in neuroscience}, 12:\penalty0 608, 2018.

\bibitem[N{\o}kland and Eidnes(2019)]{nokland2019training}
Arild N{\o}kland and Lars~Hiller Eidnes.
\newblock Training neural networks with local error signals.
\newblock In \emph{International Conference on Machine Learning}, pages
  4839--4850. PMLR, 2019.

\bibitem[Qin et~al.(2021)Qin, Mudur, and Pehlevan]{qin2021contrastive}
Shanshan Qin, Nayantara Mudur, and Cengiz Pehlevan.
\newblock Contrastive similarity matching for supervised learning.
\newblock \emph{Neural Computation}, 33\penalty0 (5):\penalty0 1300--1328,
  2021.

\bibitem[Pogodin and Latham(2020)]{pogodin2020kernelized}
R~Pogodin and PE~Latham.
\newblock Kernelized information bottleneck leads to biologically plausible
  3-factor hebbian learning in deep networks.
\newblock In \emph{CoRR}, 2020.

\bibitem[Illing et~al.(2020)Illing, Ventura, Bellec, and
  Gerstner]{illing2020local}
Bernd Illing, Jean Ventura, Guillaume Bellec, and Wulfram Gerstner.
\newblock Local plasticity rules can learn deep representations using
  self-supervised contrastive predictions.
\newblock \emph{arXiv preprint arXiv:2010.08262}, 2020.

\bibitem[Bahroun and Soltoggio(2017)]{bahroun2017online}
Yanis Bahroun and Andrea Soltoggio.
\newblock Online representation learning with single and multi-layer hebbian
  networks for image classification.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pages 354--363. Springer, 2017.

\bibitem[Krotov and Hopfield(2019)]{krotov2019unsupervised}
Dmitry Krotov and John~J Hopfield.
\newblock Unsupervised learning by competing hidden units.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (16):\penalty0 7723--7731, 2019.

\bibitem[Lipshutz et~al.(2021)Lipshutz, Bahroun, Golkar, Sengupta, and
  Chklovskii]{lipshutz2021biologically}
David Lipshutz, Yanis Bahroun, Siavash Golkar, Anirvan~M Sengupta, and Dmitri~B
  Chklovskii.
\newblock A biologically plausible neural network for multichannel canonical
  correlation analysis.
\newblock \emph{Neural Computation}, 33\penalty0 (9):\penalty0 2309--2352,
  2021.

\bibitem[Golkar et~al.(2020)Golkar, Lipshutz, Bahroun, Sengupta, and
  Chklovskii]{golkar2020simple}
Siavash Golkar, David Lipshutz, Yanis Bahroun, Anirvan Sengupta, and Dmitri
  Chklovskii.
\newblock A simple normative network approximates local non-hebbian learning in
  the cortex.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Scellier and Bengio(2017)]{scellier2017equilibrium}
Benjamin Scellier and Yoshua Bengio.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock \emph{Frontiers in computational neuroscience}, 11:\penalty0 24,
  2017.

\bibitem[Laborieux et~al.(2021)Laborieux, Ernoult, Scellier, Bengio, Grollier,
  and Querlioz]{laborieux2021scaling}
Axel Laborieux, Maxence Ernoult, Benjamin Scellier, Yoshua Bengio, Julie
  Grollier, and Damien Querlioz.
\newblock Scaling equilibrium propagation to deep convnets by drastically
  reducing its gradient estimator bias.
\newblock \emph{Frontiers in neuroscience}, 15:\penalty0 129, 2021.

\bibitem[Metz et~al.(2018)Metz, Maheswaranathan, Cheung, and
  Sohl-Dickstein]{metz2018meta}
Luke Metz, Niru Maheswaranathan, Brian Cheung, and Jascha Sohl-Dickstein.
\newblock Meta-learning update rules for unsupervised representation learning.
\newblock \emph{arXiv preprint arXiv:1804.00222}, 2018.

\bibitem[Lansdell et~al.(2019)Lansdell, Prakash, and
  Kording]{lansdell2019learning}
Benjamin~James Lansdell, Prashanth~Ravi Prakash, and Konrad~Paul Kording.
\newblock Learning to solve the credit assignment problem.
\newblock \emph{arXiv preprint arXiv:1906.00889}, 2019.

\bibitem[Lindsey and Litwin-Kumar(2020)]{NEURIPS2020_f291e10e}
Jack Lindsey and Ashok Litwin-Kumar.
\newblock Learning to learn with feedback and local plasticity.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem[Tyulmankov et~al.(2021)Tyulmankov, Yang, and
  Abbott]{tyulmankov2021meta}
Danil Tyulmankov, Guangyu~Robert Yang, and LF~Abbott.
\newblock Meta-learning local synaptic plasticity for continual familiarity
  detection.
\newblock \emph{bioRxiv}, 2021.

\bibitem[Jiang and Litwin-Kumar(2021)]{jiang2021models}
Linnie Jiang and Ashok Litwin-Kumar.
\newblock Models of heterogeneous dopamine signaling in an insect learning and
  memory center.
\newblock \emph{PLoS Computational Biology}, 17\penalty0 (8):\penalty0
  e1009205, 2021.

\bibitem[Muller et~al.(2019)Muller, Zadina, Abbott, and
  Sawtell]{muller2019continual}
Salomon~Z Muller, Abigail~N Zadina, LF~Abbott, and Nathaniel~B Sawtell.
\newblock Continual learning in a multi-layer network of an electric fish.
\newblock \emph{Cell}, 179\penalty0 (6):\penalty0 1382--1392, 2019.

\bibitem[Yan et~al.(2015)Yan, Zhang, Piramuthu, Jagadeesh, DeCoste, Di, and
  Yu]{yan2015hd}
Zhicheng Yan, Hao Zhang, Robinson Piramuthu, Vignesh Jagadeesh, Dennis DeCoste,
  Wei Di, and Yizhou Yu.
\newblock Hd-cnn: hierarchical deep convolutional neural networks for large
  scale visual recognition.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2740--2748, 2015.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and Hadsell]{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\end{thebibliography}
