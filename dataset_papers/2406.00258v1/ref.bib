@String(PAMI = {IEEE Trans. Pattern Anal. Mach. Intell.})
@String(IJCV = {Int. J. Comput. Vis.})
@String(CVPR= {IEEE Conf. Comput. Vis. Pattern Recog.})
@String(ICCV= {Int. Conf. Comput. Vis.})
@String(ECCV= {Eur. Conf. Comput. Vis.})
@String(NIPS= {Adv. Neural Inform. Process. Syst.})
@String(ICPR = {Int. Conf. Pattern Recog.})
@String(BMVC= {Brit. Mach. Vis. Conf.})
@String(TOG= {ACM Trans. Graph.})
@String(TIP  = {IEEE Trans. Image Process.})
@String(TVCG  = {IEEE Trans. Vis. Comput. Graph.})
@String(TMM  = {IEEE Trans. Multimedia})
@String(ACMMM= {ACM Int. Conf. Multimedia})
@String(ICME = {Int. Conf. Multimedia and Expo})
@String(ICASSP=	{ICASSP})
@String(ICIP = {IEEE Int. Conf. Image Process.})
@String(ACCV  = {ACCV})
@String(ICLR = {Int. Conf. Learn. Represent.})
@String(IJCAI = {IJCAI})
@String(PR   = {Pattern Recognition})
@String(AAAI = {AAAI})
@String(CVPRW= {IEEE Conf. Comput. Vis. Pattern Recog. Worksh.})
@String(CSVT = {IEEE Trans. Circuit Syst. Video Technol.})

@String(SPL	= {IEEE Sign. Process. Letters})
@String(VR   = {Vis. Res.})
@String(JOV	 = {J. Vis.})
@String(TVC  = {The Vis. Comput.})
@String(JCST  = {J. Comput. Sci. Tech.})
@String(CGF  = {Comput. Graph. Forum})
@String(CVM = {Computational Visual Media})


@String(PAMI  = {IEEE TPAMI})
@String(IJCV  = {IJCV})
@String(CVPR  = {CVPR})
@String(ICCV  = {ICCV})
@String(ECCV  = {ECCV})
@String(NIPS  = {NeurIPS})
@String(ICPR  = {ICPR})
@String(BMVC  =	{BMVC})
@String(TOG   = {ACM TOG})
@String(TIP   = {IEEE TIP})
@String(TVCG  = {IEEE TVCG})
@String(TCSVT = {IEEE TCSVT})
@String(TMM   =	{IEEE TMM})
@String(ACMMM = {ACM MM})
@String(ICME  =	{ICME})
@String(ICASSP=	{ICASSP})
@String(ICIP  = {ICIP})
@String(ACCV  = {ACCV})
@String(ICLR  = {ICLR})
@String(IJCAI = {IJCAI})
@String(PR = {PR})
@String(AAAI = {AAAI})
@String(CVPRW= {CVPRW})
@String(CSVT = {IEEE TCSVT})



@misc{Authors14,
 author = {FirstName LastName},
 title = {The frobnicatable foo filter},
 note = {Face and Gesture submission ID 324. Supplied as supplemental material {\tt fg324.pdf}},
 year = 2014
}

@misc{Authors14b,
 author = {FirstName LastName},
 title = {Frobnication tutorial},
 note = {Supplied as supplemental material {\tt tr.pdf}},
 year = 2014
}

@article{Alpher02,
author = {FirstName Alpher},
title = {Frobnication},
journal = PAMI,
volume = 12,
number = 1,
pages = {234--778},
year = 2002
}

@article{Alpher03,
author = {FirstName Alpher and  FirstName Fotheringham-Smythe},
title = {Frobnication revisited},
journal = {Journal of Foo},
volume = 13,
number = 1,
pages = {234--778},
year = 2003
}

@article{Alpher04,
author = {FirstName Alpher and FirstName Fotheringham-Smythe and FirstName Gamow},
title = {Can a machine frobnicate?},
journal = {Journal of Foo},
volume = 14,
number = 1,
pages = {234--778},
year = 2004
}

@inproceedings{Alpher05,
author = {FirstName Alpher and FirstName Gamow},
title = {Can a computer frobnicate?},
booktitle = CVPR,
pages = {234--778},
year = 2005
}



@article{brownLanguageModelsAre2020,
 author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 journal = {arXiv preprint arXiv:2005.14165},
 month = {July},
 title = {Language {{Models}} Are {{Few-Shot Learners}}},
 year = {2020}
}

@article{chenPix2seqLanguageModeling2022,
 author = {Chen, Ting and Saxena, Saurabh and Li, Lala and Fleet, David J. and Hinton, Geoffrey},
 journal = {arXiv preprint arXiv:2109.10852},
 month = {March},
 title = {Pix2seq: {{A Language Modeling Framework}} for {{Object Detection}}},
 year = {2022}
}

@article{chenPositionEnhancedVisualInstruction2023,
 author = {Chen, Chi and Qin, Ruoyu and Luo, Fuwen and Mi, Xiaoyue and Li, Peng and Sun, Maosong and Liu, Yang},
 journal = {arXiv preprint arXiv:2308.13437},
 month = {August},
 title = {Position-{{Enhanced Visual Instruction Tuning}} for {{Multimodal Large Language Models}}},
 year = {2023}
}

@article{chenShikraUnleashingMultimodal2023,
 author = {Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
 journal = {arXiv preprint arXiv:2306.15195},
 month = {July},
 title = {Shikra: {{Unleashing Multimodal LLM}}'s {{Referential Dialogue Magic}}},
 year = {2023}
}

@misc{ExtremeSpeedScale2023,
 copyright = {Apache-2.0},
 howpublished = {Microsoft},
 month = {October},
 title = {Extreme {{Speed}} and {{Scale}} for {{DL Training}} and {{Inference}}},
 year = {2023}
}

@article{heMaskRCNN2018,
 author = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
 journal = {arXiv preprint arXiv:1703.06870},
 month = {January},
 title = {Mask {{R-CNN}}},
 year = {2018}
}

@inproceedings{kazemzadehReferItGameReferringObjects2014,
 author = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
 booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
 doi = {10.3115/v1/D14-1086},
 month = {October},
 pages = {787--798},
 publisher = {{Association for Computational Linguistics}},
 title = {{{ReferItGame}}: {{Referring}} to {{Objects}} in {{Photographs}} of {{Natural Scenes}}},
 year = {2014}
}

@article{kirillovSegmentAnything2023,
 author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
 journal = {arXiv preprint arXiv:2304.02643},
 month = {April},
 title = {Segment {{Anything}}},
 year = {2023}
}

@article{krishnaVisualGenomeConnecting2016,
 author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A. and Bernstein, Michael S. and Li, Fei-Fei},
 journal = {arXiv preprint arXiv:1602.07332},
 month = {February},
 title = {Visual {{Genome}}: {{Connecting Language}} and {{Vision Using Crowdsourced Dense Image Annotations}}},
 year = {2016}
}

@article{laiLISAReasoningSegmentation2023,
 author = {Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
 journal = {arXiv preprint arXiv:2308.00692},
 month = {August},
 title = {{{LISA}}: {{Reasoning Segmentation}} via {{Large Language Model}}},
 year = {2023}
}

@article{liBLIP2BootstrappingLanguageImage2023,
 author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
 journal = {arXiv preprint arXiv:2301.12597},
 month = {May},
 title = {{{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}},
 year = {2023}
}

@article{yuMMVetEvaluatingLarge2023,
  title = {{{MM-Vet}}: {{Evaluating Large Multimodal Models}} for {{Integrated Capabilities}}},
  author = {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
  year = {2023},
  journal = {arXiv preprint arXiv:2308.02490},
}

@article{liSEEDBenchBenchmarkingMultimodal2023,
  title = {{{SEED-Bench}}: {{Benchmarking Multimodal LLMs}} with {{Generative Comprehension}}},
  author = {Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
  year = {2023},
  month = aug,
  eprint = {2307.16125},
  journal = {arXiv preprint arXiv:2307.16125},
}

@article{baiTouchStoneEvaluatingVisionLanguage2023,
  title = {{{TouchStone}}: {{Evaluating Vision-Language Models}} by {{Language Models}}},
  author = {Bai, Shuai and Yang, Shusheng and Bai, Jinze and Wang, Peng and Zhang, Xingxuan and Lin, Junyang and Wang, Xinggang and Zhou, Chang and Zhou, Jingren},
  year = {2023},
  journal = {arXiv preprint arXiv:2308.16890}
}

@article{zhengDDCoTDutyDistinctChainofThought2023,
  title = {{{DDCoT}}: {{Duty-Distinct Chain-of-Thought Prompting}} for {{Multimodal Reasoning}} in {{Language Models}}},
  author = {Zheng, Ge and Yang, Bin and Tang, Jiajin and Zhou, Hong-Yu and Yang, Sibei},
  year = {2023},
  journal = {arXiv preprint arXiv:2310.16436}
}


@inproceedings{liGroundedLanguageImagePretraining2022a,
 author = {Li, Liunian Harold and Zhang, Pengchuan and Zhang, Haotian and Yang, Jianwei and Li, Chunyuan and Zhong, Yiwu and Wang, Lijuan and Yuan, Lu and Zhang, Lei and Hwang, Jenq-Neng and Chang, Kai-Wei and Gao, Jianfeng},
 booktitle = {2022 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
 doi = {10.1109/CVPR52688.2022.01069},
 issn = {2575-7075},
 month = {June},
 pages = {10955--10965},
 title = {Grounded {{Language-Image Pre-training}}},
 year = {2022}
}

@inproceedings{liuReferringExpressionGeneration2017,
 author = {Liu, Jingyu and Wang, Liang and Yang, Ming-Hsuan},
 booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
 doi = {10.1109/ICCV.2017.520},
 isbn = {978-1-5386-1032-9},
 langid = {english},
 month = {October},
 pages = {4866--4874},
 publisher = {{IEEE}},
 title = {Referring {{Expression Generation}} and {{Comprehension}} via {{Attributes}}},
 year = {2017}
}

@article{liuVisualInstructionTuning2023,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 journal = {arXiv preprint arXiv:2304.08485},
 month = {April},
 title = {Visual {{Instruction Tuning}}},
 year = {2023}
}

@article{openaiGPT4TechnicalReport2023,
 author = {OpenAI},
 journal = {arXiv preprint arXiv:2303.08774},
 month = {March},
 title = {{{GPT-4 Technical Report}}},
 year = {2023}
}

@article{pengKosmos2GroundingMultimodal2023,
 author = {Peng, Zhiliang and Wang, Wenhui and Dong, Li and Hao, Yaru and Huang, Shaohan and Ma, Shuming and Wei, Furu},
 journal = {arXiv preprint arXiv:2306.14824},
 month = {July},
 title = {Kosmos-2: {{Grounding Multimodal Large Language Models}} to the {{World}}},
 year = {2023}
}

@inproceedings{tianIntegrallyPreTrainedTransformer2023,
 author = {Tian, Yunjie and Xie, Lingxi and Wang, Zhaozhi and Wei, Longhui and Zhang, Xiaopeng and Jiao, Jianbin and Wang, Yaowei and Tian, Qi and Ye, Qixiang},
 booktitle = {2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
 doi = {10.1109/CVPR52729.2023.01785},
 isbn = {9798350301298},
 langid = {english},
 month = {June},
 pages = {18610--18620},
 publisher = {{IEEE}},
 title = {Integrally {{Pre-Trained Transformer Pyramid Networks}}},
 year = {2023}
}

@article{wangVisionLLMLargeLanguage2023,
 author = {Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and Dai, Jifeng},
 journal = {arXiv preprint arXiv:2305.11175},
 langid = {english},
 month = {May},
 title = {{{VisionLLM}}: {{Large Language Model}} Is Also an {{Open-Ended Decoder}} for {{Vision-Centric Tasks}}},
 year = {2023}
}

@article{wangWhatMakesGood2023,
 author = {Wang, Guangzhi and Ge, Yixiao and Ding, Xiaohan and Kankanhalli, Mohan and Shan, Ying},
 journal = {arXiv preprint arXiv:2305.12223},
 month = {May},
 title = {What {{Makes}} for {{Good Visual Tokenizers}} for {{Large Language Models}}?},
 year = {2023}
}

@article{xuanPinkUnveilingPower2023,
 author = {Xuan, Shiyu and Guo, Qingpei and Yang, Ming and Zhang, Shiliang},
 journal = {arXiv preprint arXiv:2310.00582},
 month = {October},
 title = {Pink: {{Unveiling}} the {{Power}} of {{Referential Comprehension}} for {{Multi-modal LLMs}}},
 year = {2023}
}

@article{youFerretReferGround2023,
 author = {You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
 journal = {arXiv preprint arXiv:2310.07704},
 month = {October},
 title = {Ferret: {{Refer}} and {{Ground Anything Anywhere}} at {{Any Granularity}}},
 year = {2023}
}

@article{zhangDINODETRImproved2022,
 author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M. and Shum, Heung-Yeung},
 journal = {arXiv preprint arXiv:2203.03605},
 month = {July},
 title = {{{DINO}}: {{DETR}} with {{Improved DeNoising Anchor Boxes}} for {{End-to-End Object Detection}}},
 year = {2022}
}

@article{zhangGLIPv2UnifyingLocalization2022,
 author = {Zhang, Haotian and Zhang, Pengchuan and Hu, Xiaowei and Chen, Yen-Chun and Li, Liunian Harold and Dai, Xiyang and Wang, Lijuan and Yuan, Lu and Hwang, Jenq-Neng and Gao, Jianfeng},
 journal = {arXiv preprint arXiv:2206.05836},
 month = {October},
 title = {{{GLIPv2}}: {{Unifying Localization}} and {{Vision-Language Understanding}}},
 year = {2022}
}

@article{zhangGPT4RoIInstructionTuning2023,
 author = {Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
 journal = {arXiv preprint arXiv:2307.03601},
 month = {July},
 title = {{{GPT4RoI}}: {{Instruction Tuning Large Language Model}} on {{Region-of-Interest}}},
 year = {2023}
}

@article{zhuMiniGPT4EnhancingVisionLanguage2023,
 author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
 journal = {arXiv preprint arXiv:2304.10592},
 langid = {english},
 month = {April},
 title = {{{MiniGPT-4}}: {{Enhancing Vision-Language Understanding}} with {{Advanced Large Language Models}}},
 year = {2023}
}

@article{ouyangTrainingLanguageModels2022,
  title = {Training Language Models to Follow Instructions with Human Feedback},
  author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.02155}
}



%--10.23--%


@inproceedings{li2022blip,
  title={Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
  author={Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
  booktitle={International Conference on Machine Learning},
  pages={12888--12900},
  year={2022},
  organization={PMLR}
}

@inproceedings{kazemzadeh2014refcoco,
  title={Referitgame: Referring to objects in photographs of natural scenes},
  author={Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
  booktitle={Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)},
  pages={787--798},
  year={2014}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{zeng2022glm,
  title={Glm-130b: An open bilingual pre-trained model},
  author={Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
  journal={arXiv preprint arXiv:2210.02414},
  year={2022}
}

@article{thoppilan2022lamda,
  title={Lamda: Language models for dialog applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}


@article{zhang2022opt,
  title={Opt: Open pre-trained transformer language models},
  author={Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
  journal={arXiv preprint arXiv:2205.01068},
  year={2022}
}

@article{huang2023kosmos1,
  title={Language is not all you need: Aligning perception with language models},
  author={Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
  journal={arXiv preprint arXiv:2302.14045},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@article{chung2022scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={arXiv preprint arXiv:2210.11416},
  year={2022}
}

@article{gong2023multimodal,
  title={Multimodal-gpt: A vision and language model for dialogue with humans},
  author={Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
  journal={arXiv preprint arXiv:2305.04790},
  year={2023}
}


@article{liu2023grounding,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  year={2023}
}

@article{liu2023internchat,
  title={Internchat: Solving vision-centric tasks by interacting with chatbots beyond language},
  author={Liu, Zhaoyang and He, Yinan and Wang, Wenhai and Wang, Weiyun and Wang, Yi and Chen, Shoufa and Zhang, Qinglong and Yang, Yang and Li, Qingyun and Yu, Jiashuo and others},
  journal={arXiv preprint arXiv:2305.05662},
  year={2023}
}

@inproceedings{plummer2015flickr30k,
  title={Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
  author={Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2641--2649},
  year={2015}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}

@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}

@article{instructblip,
  author       = {Wenliang Dai and
                  Junnan Li and
                  Dongxu Li and
                  Anthony Meng Huat Tiong and
                  Junqi Zhao and
                  Weisheng Wang and
                  Boyang Li and
                  Pascale Fung and
                  Steven C. H. Hoi},
  title        = {InstructBLIP: Towards General-purpose Vision-Language Models with
                  Instruction Tuning},
  year         = {2023},
  journal={arXiv preprint arXiv:2305.06500},
}

@article{wang2023visionllm,
  title={Visionllm: Large language model is also an open-ended decoder for vision-centric tasks},
  author={Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
  journal={arXiv preprint arXiv:2305.11175},
  year={2023}
}

@article{wang2023cogvlm,
  title={CogVLM: Visual Expert for Pretrained Language Models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
  journal={arXiv preprint arXiv:2311.03079},
  year={2023}
}


@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@article{zhang2022dino,
  title={Dino: Detr with improved denoising anchor boxes for end-to-end object detection},
  author={Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M and Shum, Heung-Yeung},
  journal={arXiv preprint arXiv:2203.03605},
  year={2022}
}

@article{kirillov2023segment,
  title={Segment anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
  journal={arXiv preprint arXiv:2304.02643},
  year={2023}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}


@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}


@inproceedings{mao2016generation,
  title={Generation and comprehension of unambiguous object descriptions},
  author={Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={11--20},
  year={2016}
}


@inproceedings{li2023gligen,
  title={Gligen: Open-set grounded text-to-image generation},
  author={Li, Yuheng and Liu, Haotian and Wu, Qingyang and Mu, Fangzhou and Yang, Jianwei and Gao, Jianfeng and Li, Chunyuan and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={22511--22521},
  year={2023}
}

@article{suris2023vipergpt,
  title={Vipergpt: Visual inference via python execution for reasoning},
  author={Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
  journal={arXiv preprint arXiv:2303.08128},
  year={2023}
}


@article{shen2023hugginggpt,
  title={Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  author={Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
  journal={arXiv preprint arXiv:2303.17580},
  year={2023}
}

@article{yu2023scaling,
  title={Scaling autoregressive multi-modal models: Pretraining and instruction tuning},
  author={Yu, Lili and Shi, Bowen and Pasunuru, Ramakanth and Muller, Benjamin and Golovneva, Olga and Wang, Tianlu and Babu, Arun and Tang, Binh and Karrer, Brian and Sheynin, Shelly and others},
  journal={arXiv preprint arXiv:2309.02591},
  year={2023}
}


@inproceedings{shao2019objects365,
  title={Objects365: A large-scale, high-quality dataset for object detection},
  author={Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={8430--8439},
  year={2019}
}

@inproceedings{carion2020detr,
  title={End-to-end object detection with transformers},
  author={Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  booktitle={European conference on computer vision},
  pages={213--229},
  year={2020},
  organization={Springer}
}

@article{zhang2019bertscore,
  title={Bertscore: Evaluating text generation with bert},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q and Artzi, Yoav},
  journal={arXiv preprint arXiv:1904.09675},
  year={2019}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@inproceedings{zhou2020more,
  title={More grounded image captioning by distilling image-text matching model},
  author={Zhou, Yuanen and Wang, Meng and Liu, Daqing and Hu, Zhenzhen and Zhang, Hanwang},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4777--4786},
  year={2020}
}


@inproceedings{he2017mask,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={2961--2969},
  year={2017}
}

@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{krishna2017visual,
  title={Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  author={Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  journal={International journal of computer vision},
  volume={123},
  pages={32--73},
  year={2017},
  publisher={Springer}
}

@inproceedings{zhang2022hivit,
  title={Hivit: A simpler and more efficient design of hierarchical vision transformer},
  author={Zhang, Xiaosong and Tian, Yunjie and Xie, Lingxi and Huang, Wei and Dai, Qi and Ye, Qixiang and Tian, Qi},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}


@article{tian2024chatterbox,
  title={ChatterBox: Multi-round Multimodal Referring and Grounding},
  author={Tian, Yunjie and Ma, Tianren and Xie, Lingxi and Qiu, Jihao and Tang, Xi and Zhang, Yuan and Jiao, Jianbin and Tian, Qi and Ye, Qixiang},
  journal={arXiv preprint arXiv:2401.13307},
  year={2024}
}

@misc{li2024videochat,
      title={VideoChat: Chat-Centric Video Understanding}, 
      author={KunChang Li and Yinan He and Yi Wang and Yizhuo Li and Wenhai Wang and Ping Luo and Yali Wang and Limin Wang and Yu Qiao},
      year={2024},
      eprint={2305.06355},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{maaz2023videochatgpt,
      title={Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models}, 
      author={Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Shahbaz Khan},
      year={2023},
      eprint={2306.05424},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{zhang2023videollama,
      title={Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding}, 
      author={Hang Zhang and Xin Li and Lidong Bing},
      year={2023},
      eprint={2306.02858},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{zhu2024languagebind,
      title={LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment}, 
      author={Bin Zhu and Bin Lin and Munan Ning and Yang Yan and Jiaxi Cui and HongFa Wang and Yatian Pang and Wenhao Jiang and Junwu Zhang and Zongwei Li and Wancai Zhang and Zhifeng Li and Wei Liu and Li Yuan},
      year={2024},
      eprint={2310.01852},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{lin2023videollava,
      title={Video-LLaVA: Learning United Visual Representation by Alignment Before Projection}, 
      author={Bin Lin and Yang Ye and Bin Zhu and Jiaxi Cui and Munan Ning and Peng Jin and Li Yuan},
      year={2023},
      eprint={2311.10122},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{luo2023valley,
      title={Valley: Video Assistant with Large Language model Enhanced abilitY}, 
      author={Ruipu Luo and Ziwang Zhao and Min Yang and Junwei Dong and Da Li and Pengcheng Lu and Tao Wang and Linmei Hu and Minghui Qiu and Zhongyu Wei},
      year={2023},
      eprint={2306.07207},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{yu2023merlinempowering,
      title={Merlin:Empowering Multimodal LLMs with Foresight Minds}, 
      author={En Yu and Liang Zhao and Yana Wei and Jinrong Yang and Dongming Wu and Lingyu Kong and Haoran Wei and Tiancai Wang and Zheng Ge and Xiangyu Zhang and Wenbing Tao},
      year={2023},
      eprint={2312.00589},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{munasinghe2023pgvideollava,
      title={PG-Video-LLaVA: Pixel Grounding Large Video-Language Models}, 
      author={Shehan Munasinghe and Rusiru Thushara and Muhammad Maaz and Hanoona Abdul Rasheed and Salman Khan and Mubarak Shah and Fahad Khan},
      year={2023},
      eprint={2311.13435},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{li2024groundinggptlanguage,
      title={GroundingGPT:Language Enhanced Multi-modal Grounding Model}, 
      author={Zhaowei Li and Qi Xu and Dong Zhang and Hang Song and Yiqing Cai and Qi Qi and Ran Zhou and Junting Pan and Zefeng Li and Van Tu Vu and Zhida Huang and Tao Wang},
      year={2024},
      eprint={2401.06071},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhu2023tracking,
  title={Tracking anything in high quality},
  author={Zhu, Jiawen and Chen, Zhenyu and Hao, Zeqi and Chang, Shijie and Zhang, Lu and Wang, Dong and Lu, Huchuan and Luo, Bin and He, Jun-Yan and Lan, Jin-Peng and others},
  journal={arXiv preprint arXiv:2307.13974},
  year={2023}
}

@article{flood1956traveling,
  title={The traveling-salesman problem},
  author={Flood, Merrill M},
  journal={Operations research},
  volume={4},
  number={1},
  pages={61--75},
  year={1956},
  publisher={INFORMS}
}

@inproceedings{sharma2018conceptual,
  title={Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
  author={Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
  booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2556--2565},
  year={2018}
}

@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{bain2021frozen,
  title={Frozen in time: A joint video and image encoder for end-to-end retrieval},
  author={Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={1728--1738},
  year={2021}
}

@article{tang2021human,
  title={Human-centric spatio-temporal video grounding with visual transformers},
  author={Tang, Zongheng and Liao, Yue and Liu, Si and Li, Guanbin and Jin, Xiaojie and Jiang, Hongxu and Yu, Qian and Xu, Dong},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  volume={32},
  number={12},
  pages={8238--8249},
  year={2021},
  publisher={IEEE}
}

@article{chen2019weakly,
  title={Weakly-supervised spatio-temporally grounding natural sentence in video},
  author={Chen, Zhenfang and Ma, Lin and Luo, Wenhan and Wong, Kwan-Yee K},
  journal={arXiv preprint arXiv:1906.02549},
  year={2019}
}

@inproceedings{gavrilyuk2018actor,
  title={Actor and action video segmentation from a sentence},
  author={Gavrilyuk, Kirill and Ghodrati, Amir and Li, Zhenyang and Snoek, Cees GM},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5958--5966},
  year={2018}
}

@inproceedings{fan2019lasot,
  title={Lasot: A high-quality benchmark for large-scale single object tracking},
  author={Fan, Heng and Lin, Liting and Yang, Fan and Chu, Peng and Deng, Ge and Yu, Sijia and Bai, Hexin and Xu, Yong and Liao, Chunyuan and Ling, Haibin},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5374--5383},
  year={2019}
}

@inproceedings{ding2023mevis,
  title={MeViS: A large-scale benchmark for video segmentation with motion expressions},
  author={Ding, Henghui and Liu, Chang and He, Shuting and Jiang, Xudong and Loy, Chen Change},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={2694--2703},
  year={2023}
}

@article{huang2019got,
  title={Got-10k: A large high-diversity benchmark for generic object tracking in the wild},
  author={Huang, Lianghua and Zhao, Xin and Huang, Kaiqi},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={43},
  number={5},
  pages={1562--1577},
  year={2019},
  publisher={IEEE}
}

@article{hu2024multi,
  title={A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship},
  author={Hu, Shiyu and Zhang, Dailing and Feng, Xiaokun and Li, Xuchen and Zhao, Xin and Huang, Kaiqi and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{papineni2002bleu,
  title={Bleu: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
  booktitle={Proceedings of the 40th annual meeting of the Association for Computational Linguistics},
  pages={311--318},
  year={2002}
}

@inproceedings{banerjee2005meteor,
  title={METEOR: An automatic metric for MT evaluation with improved correlation with human judgments},
  author={Banerjee, Satanjeev and Lavie, Alon},
  booktitle={Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization},
  pages={65--72},
  year={2005}
}

@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}

@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}

@inproceedings{anderson2016spice,
  title={Spice: Semantic propositional image caption evaluation},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14},
  pages={382--398},
  year={2016},
  organization={Springer}
}

@misc{yuan2024osprey,
      title={Osprey: Pixel Understanding with Visual Instruction Tuning}, 
      author={Yuqian Yuan and Wentong Li and Jian Liu and Dongqi Tang and Xinjie Luo and Chi Qin and Lei Zhang and Jianke Zhu},
      year={2024},
      eprint={2312.10032},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

@article{gao2023llama,
  title={Llama-adapter v2: Parameter-efficient visual instruction model},
  author={Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
  journal={arXiv preprint arXiv:2304.15010},
  year={2023}
}

@article{yang2022zero,
  title={Zero-shot video question answering via frozen bidirectional language models},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={124--141},
  year={2022}
}

@inproceedings{chen2011collecting,
  title={Collecting highly parallel data for paraphrase evaluation},
  author={Chen, David and Dolan, William B},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies},
  pages={190--200},
  year={2011}
}

@inproceedings{xu2016msr,
  title={Msr-vtt: A large video description dataset for bridging video and language},
  author={Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5288--5296},
  year={2016}
}

@inproceedings{caba2015activitynet,
  title={Activitynet: A large-scale video benchmark for human activity understanding},
  author={Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
  booktitle={Proceedings of the ieee conference on computer vision and pattern recognition},
  pages={961--970},
  year={2015}
}

@misc{yu2019activitynetqa,
      title={ActivityNet-QA: A Dataset for Understanding Complex Web Videos via Question Answering}, 
      author={Zhou Yu and Dejing Xu and Jun Yu and Ting Yu and Zhou Zhao and Yueting Zhuang and Dacheng Tao},
      year={2019},
      eprint={1906.02467},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{liu2021swin,
  title={Swin transformer: Hierarchical vision transformer using shifted windows},
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={10012--10022},
  year={2021}
}

@article{zhu2024visionmamba,
  title={Vision mamba: Efficient visual representation learning with bidirectional state space model},
  author={Zhu, Lianghui and Liao, Bencheng and Zhang, Qian and Wang, Xinlong and Liu, Wenyu and Wang, Xinggang},
  journal={arXiv preprint arXiv:2401.09417},
  year={2024}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{gu2021efficiently,
  title={Efficiently modeling long sequences with structured state spaces},
  author={Gu, Albert and Goel, Karan and R{\'e}, Christopher},
  journal={arXiv preprint arXiv:2111.00396},
  year={2021}
}

@article{liu2024vmamba,
  title={Vmamba: Visual state space model},
  author={Liu, Yue and Tian, Yunjie and Zhao, Yuzhong and Yu, Hongtian and Xie, Lingxi and Wang, Yaowei and Ye, Qixiang and Liu, Yunfan},
  journal={arXiv preprint arXiv:2401.10166},
  year={2024}
}

@misc{zheng2023judging,
      title={Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
      year={2023},
      eprint={2306.05685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{tian2021semantic,
  title={Semantic-aware generation for self-supervised visual representation learning},
  author={Tian, Yunjie and Xie, Lingxi and Zhang, Xiaopeng and Fang, Jiemin and Xu, Haohang and Huang, Wei and Jiao, Jianbin and Tian, Qi and Ye, Qixiang},
  journal={arXiv preprint arXiv:2111.13163},
  year={2021}
}