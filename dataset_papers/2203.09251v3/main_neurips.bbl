\begin{thebibliography}{10}

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{Fiechter94}
Claude{-}Nicolas Fiechter.
\newblock Efficient reinforcement learning.
\newblock In {\em Proceedings of the Seventh Conference on Computational
  Learning Theory (COLT)}, 1994.

\bibitem{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  1507--1516. PMLR, 2019.

\bibitem{dann15PAC}
Christoph Dann and Emma Brunskill.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2015.

\bibitem{Menard21RFE}
Pierre M{\'{e}}nard, Omar~Darwiche Domingues, Anders Jonsson, Emilie Kaufmann,
  Edouard Leurent, and Michal Valko.
\newblock Fast active learning for pure exploration in reinforcement learning.
\newblock In {\em International Conference on Machine Learning (ICML)}, 2021.

\bibitem{Omar21LB}
Omar~Darwiche Domingues, Pierre M{\'{e}}nard, Emilie Kaufmann, and Michal
  Valko.
\newblock Episodic reinforcement learning in finite mdps: Minimax lower bounds
  revisited.
\newblock In {\em Algorithmic Learning Theory (ALT)}, 2021.

\bibitem{zanette2019almost}
Andrea Zanette, Mykel~J. Kochenderfer, and Emma Brunskill.
\newblock Almost horizon-free structure-aware best policy identification with a
  generative model.
\newblock In {\em NeurIPS}, pages 5626--5635, 2019.

\bibitem{al2021adaptive}
Aymen Al~Marjani and Alexandre Proutiere.
\newblock Adaptive sampling for best policy identification in markov decision
  processes.
\newblock In {\em International Conference on Machine Learning}, pages
  7459--7468. PMLR, 2021.

\bibitem{al2021navigating}
Aymen Al~Marjani, Aur{\'e}lien Garivier, and Alexandre Proutiere.
\newblock Navigating to the best policy in markov decision processes.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{wagenmaker21IDPAC}
Andrew Wagenmaker, Max Simchowitz, and Kevin~G. Jamieson.
\newblock Beyond no regret: Instance-dependent {PAC} reinforcement learning.
\newblock In {\em Conference On Learning Theory (COLT)}, 2022.

\bibitem{tirinzoni2021fully}
Andrea Tirinzoni, Matteo Pirotta, and Alessandro Lazaric.
\newblock A fully problem-dependent regret lower bound for finite-horizon mdps.
\newblock {\em arXiv preprint arXiv:2106.13013}, 2021.

\bibitem{dann21ReturnGap}
Christoph Dann, Teodor~V. Marinov, Mehryar Mohri, and Julian Zimmert.
\newblock Beyond value-function gaps: Improved instance-dependent regret bounds
  for episodic reinforcement learning.
\newblock {\em CoRR}, abs/2107.01264, 2021.

\bibitem{garivier2016optimal}
Aur{\'e}lien Garivier and Emilie Kaufmann.
\newblock Optimal best arm identification with fixed confidence.
\newblock In {\em Conference on Learning Theory}, pages 998--1027. PMLR, 2016.

\bibitem{voitishin1980algorithms}
Yu~V Voitishin.
\newblock Algorithms for solving for the minimal flow in a network.
\newblock {\em Cybernetics}, 16(1):131--134, 1980.

\bibitem{adlakha1991minimum}
Veena Adlakha, Barbara Gladysz, and Jerzy Kamburowski.
\newblock Minimum flows in (s, t) planar networks.
\newblock {\em Networks}, 21(7):767--773, 1991.

\bibitem{adlakha1999alternate}
Veena~G Adlakha.
\newblock An alternate linear algorithm for the minimum flow problem.
\newblock {\em Journal of the Operational Research Society}, 50(2):177--182,
  1999.

\bibitem{ciurea2004sequential}
Eleonor Ciurea and Laura Ciupala.
\newblock Sequential and parallel algorithms for minimum flows.
\newblock {\em Journal of Applied Mathematics and Computing}, 15(1):53--75,
  2004.

\bibitem{simchowitz2019non}
Max Simchowitz and Kevin~G. Jamieson.
\newblock Non-asymptotic gap-dependent regret bounds for tabular mdps.
\newblock In {\em NeurIPS}, pages 1151--1160, 2019.

\bibitem{xu2021fine}
Haike Xu, Tengyu Ma, and Simon~S Du.
\newblock Fine-grained gap-dependent bounds for tabular mdps via adaptive
  multi-step bootstrap.
\newblock {\em arXiv preprint arXiv:2102.04692}, 2021.

\bibitem{MannorTsi04}
S.~Mannor and J.~Tsitsiklis.
\newblock {The Sample Complexity of Exploration in the Multi-Armed Bandit
  Problem}.
\newblock {\em Journal of Machine Learning Research}, pages 623--648, 2004.

\bibitem{DegenneK19}
R{\'{e}}my Degenne and Wouter~M. Koolen.
\newblock Pure exploration with multiple correct answers.
\newblock In {\em NeurIPS}, 2019.

\bibitem{garivier2021nonasymptotic}
Aur{\'e}lien Garivier and Emilie Kaufmann.
\newblock Nonasymptotic sequential tests for overlapping hypotheses applied to
  near-optimal arm identification in bandit models.
\newblock {\em Sequential Analysis}, 40(1):61--96, 2021.

\bibitem{Yin2021TowardsIO}
Ming Yin and Yu-Xiang Wang.
\newblock Towards instance-optimal offline reinforcement learning with
  pessimism.
\newblock In {\em NeurIPS}, 2021.

\bibitem{Kaufmann21RFE}
Emilie Kaufmann, Pierre M{\'{e}}nard, Omar~Darwiche Domingues, Anders Jonsson,
  Edouard Leurent, and Michal Valko.
\newblock Adaptive reward-free exploration.
\newblock In {\em Algorithmic Learning Theory (ALT)}, 2021.

\bibitem{EvenDaral06}
E.~Even-Dar, S.~Mannor, and Y.~Mansour.
\newblock {Action Elimination and Stopping Conditions for the Multi-Armed
  Bandit and Reinforcement Learning Problems}.
\newblock {\em Journal of Machine Learning Research}, 7:1079--1105, 2006.

\bibitem{lai1985asymptotically}
Tze~Leung Lai and Herbert Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock {\em Advances in applied mathematics}, 6(1):4--22, 1985.

\bibitem{krause2014submodular}
Andreas Krause and Daniel Golovin.
\newblock Submodular function maximization.
\newblock {\em Tractability}, 3:71--104, 2014.

\bibitem{kaufmann2016complexity}
Emilie Kaufmann, Olivier Capp{\'e}, and Aur{\'e}lien Garivier.
\newblock On the complexity of best-arm identification in multi-armed bandit
  models.
\newblock {\em The Journal of Machine Learning Research}, 17(1):1--42, 2016.

\bibitem{efroni2020exploration}
Yonathan Efroni, Shie Mannor, and Matteo Pirotta.
\newblock Exploration-exploitation in constrained mdps.
\newblock {\em arXiv preprint arXiv:2003.02189}, 2020.

\bibitem{Kakade03PhD}
Sham Kakade.
\newblock {\em On the Sample Complexity of Reinforcement Learning}.
\newblock PhD thesis, University College London, 2003.

\bibitem{strehl2009reinforcement}
Alexander~L Strehl, Lihong Li, and Michael~L Littman.
\newblock Reinforcement learning in finite mdps: Pac analysis.
\newblock {\em Journal of Machine Learning Research}, 10(11), 2009.

\bibitem{Kearns02SS}
Michael~J. Kearns, Yishay Mansour, and Andrew~Y. Ng.
\newblock A sparse sampling algorithm for near-optimal planning in large markov
  decision processes.
\newblock {\em Machine Learning}, 49(2-3):193--208, 2002.

\bibitem{bubeck2010olop}
S~Bubeck and R~Munos.
\newblock Open loop optimistic planning.
\newblock In {\em Conference on Learning Theory}, 2010.

\bibitem{Grill16TrailBlazer}
J.-B. Grill, M.~Valko, and R.~Munos.
\newblock Blazing the trails before beating the path: Sample-efficient
  monte-carlo planning.
\newblock In {\em Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem{Feldman14BRUE}
Zohar Feldman and Carmel Domshlak.
\newblock Simple regret optimization in online planning for markov decision
  processes.
\newblock {\em Journal of Artifial Intelligence Research}, 51:165--205, 2014.

\bibitem{jonsson2020planning}
Anders Jonsson, Emilie Kaufmann, Pierre M{\'e}nard, Omar Darwiche~Domingues,
  Edouard Leurent, and Michal Valko.
\newblock Planning in markov decision processes with gap-dependent sample
  complexity.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1253--1263, 2020.

\bibitem{Jin20RewardFree}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock Reward-free exploration for reinforcement learning.
\newblock {\em arXiv:2002.02794}, 2020.

\bibitem{Du21OptimalRFE}
Zihan Zhang, Simon Du, and Xiangyang Ji.
\newblock Near optimal reward-free reinforcement learning.
\newblock In {\em International Conference on Machine Learning, (ICML)}, 2021.

\bibitem{Zhang20TAE}
Xuezhou Zhang, Yuzhe Ma, and Adish Singla.
\newblock Task-agnostic exploration in reinforcement learning.
\newblock In {\em NeurIPS}, 2020.

\bibitem{burnetas1997optimal}
Apostolos~N Burnetas and Michael~N Katehakis.
\newblock Optimal adaptive policies for markov decision processes.
\newblock {\em Mathematics of Operations Research}, 22(1):222--255, 1997.

\bibitem{tewari2007opotimistic}
Ambuj Tewari and Peter~L. Bartlett.
\newblock Optimistic linear programming gives logarithmic regret for
  irreducible mdps.
\newblock In {\em {NIPS}}, pages 1505--1512. Curran Associates, Inc., 2007.

\bibitem{ok2018exploration}
Jungseul Ok, Alexandre Proutiere, and Damianos Tranos.
\newblock Exploration in structured reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8874--8882, 2018.

\bibitem{auer2008near}
Peter Auer, Thomas Jaksch, and Ronald Ortner.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock {\em Advances in neural information processing systems}, 21, 2008.

\bibitem{Tranos21}
Damianos Tranos and Alexandre Prouti{\`{e}}re.
\newblock Regret analysis in deterministic reinforcement learning.
\newblock In {\em {CDC}}. {IEEE}, 2021.

\bibitem{degenne2019non}
R{\'e}my Degenne, Wouter~M Koolen, and Pierre M{\'e}nard.
\newblock Non-asymptotic pure exploration by solving games.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  14492--14501, 2019.

\bibitem{tirinzoni2020asymptotically}
Andrea Tirinzoni, Matteo Pirotta, Marcello Restelli, and Alessandro Lazaric.
\newblock An asymptotically optimal primal-dual incremental algorithm for
  contextual linear bandits.
\newblock {\em Advances in Neural Information Processing Systems},
  33:1417--1427, 2020.

\bibitem{wang2021fast}
Po-An Wang, Ruo-Chun Tzeng, and Alexandre Proutiere.
\newblock Fast pure exploration via frank-wolfe.
\newblock In {\em Thirty-Fifth Conference on Neural Information Processing
  Systems}, 2021.

\bibitem{brandizi2012graph2tab}
Marco Brandizi, Natalja Kurbatova, Ugis Sarkans, and Philippe Rocca-Serra.
\newblock graph2tab, a library to convert experimental workflow graphs into
  tabular formats.
\newblock {\em Bioinformatics}, 28(12):1665--1667, 2012.

\bibitem{BanditBook}
Tor Lattimore and Csaba Szepesvari.
\newblock {\em {Bandit Algorithms}}.
\newblock Cambridge University Press, 2019.

\bibitem{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:13399--13412, 2020.

\bibitem{nemhauser1978analysis}
George~L Nemhauser, Laurence~A Wolsey, and Marshall~L Fisher.
\newblock An analysis of approximations for maximizing submodular set
  functions—i.
\newblock {\em Mathematical programming}, 14(1):265--294, 1978.

\bibitem{rlberry}
Omar~Darwiche Domingues, Yannis Flet-Berliac, Edouard Leurent, Pierre
  M{\'e}nard, Xuedong Shang, and Michal Valko.
\newblock {rlberry - A Reinforcement Learning Library for Research and
  Education}.
\newblock \url{https://github.com/rlberry-py/rlberry}, 2021.

\end{thebibliography}
