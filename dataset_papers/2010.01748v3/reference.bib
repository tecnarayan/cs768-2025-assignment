@inproceedings{wang2020rlnoisy,
  title={Reinforcement Learning with Perturbed Rewards},
  author={Wang, Jingkang and Liu, Yang and Li, Bo},
  booktitle={AAAI},
  year={2020}
}


@article{yang2020peerloss,
  author    = {Yang Liu and
               Hongyi Guo},
  title     = {Peer Loss Functions: Learning from Noisy Labels without Knowing Noise
               Rates},
  journal   = {ICML},
  volume    = {abs/1910.03231},
  year      = {2020}
}

@article{liu2015classification,
  title={Classification with noisy labels by importance reweighting},
  author={Liu, Tongliang and Tao, Dacheng},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={38},
  number={3},
  pages={447--461},
  year={2015},
  publisher={IEEE}
}

@INPROCEEDINGS{Watkins92q-learning,
    author = {Christopher J. C. H. Watkins and Peter Dayan},
    title = {Q-learning},
    booktitle = {Machine Learning},
    year = {1992},
    pages = {279--292}
}

@book{sarsa,
  author    = {Richard S. Sutton and
               Andrew G. Barto},
  title     = {Reinforcement learning - an introduction},
  series    = {Adaptive computation and machine learning},
  year      = {1998}
}

@article{dqn1,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013}
}

@inproceedings{double-dqn,
  author    = {Hado van Hasselt and
               Arthur Guez and
               David Silver},
  title     = {Deep Reinforcement Learning with Double Q-Learning},
  booktitle = {{AAAI}},
  pages     = {2094--2100},
  year      = {2016}
}

@article{ddpg,
  author    = {Timothy P. Lillicrap and
               Jonathan J. Hunt and
               Alexander Pritzel and
               Nicolas Heess and
               Tom Erez and
               Yuval Tassa and
               David Silver and
               Daan Wierstra},
  title     = {Continuous control with deep reinforcement learning},
  journal   = {CoRR},
  volume    = {abs/1509.02971},
  year      = {2015}
}

@inproceedings{naf,
  author    = {Shixiang Gu and
               Timothy P. Lillicrap and
               Ilya Sutskever and
               Sergey Levine},
  title     = {Continuous Deep Q-Learning with Model-based Acceleration},
  booktitle = {{ICML}},
  volume    = {48},
  pages     = {2829--2838},
  year      = {2016}
}

@article{cem,
  author    = {Istvan Szita and
               Andr{\'{a}}s L{\"{o}}rincz},
  title     = {Learning Tetris Using the Noisy Cross-Entropy Method},
  journal   = {Neural Computation},
  volume    = {18},
  number    = {12},
  pages     = {2936--2941},
  year      = {2006}
}

@inproceedings{dueling-dqn,
  author    = {Ziyu Wang and
               Tom Schaul and
               Matteo Hessel and
               Hado van Hasselt and
               Marc Lanctot and
               Nando de Freitas},
  title     = {Dueling Network Architectures for Deep Reinforcement Learning},
  booktitle = {{ICML}},
  volume    = {48},
  pages     = {1995--2003},
  year      = {2016}
}

@article{silver2016mastering,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529},
  year={2015}
}

@inproceedings{natarajan2013learning,
  title={Learning with noisy labels},
  author={Natarajan, Nagarajan and Dhillon, Inderjit S and Ravikumar, Pradeep K and Tewari, Ambuj},
  booktitle={Advances in neural information processing systems},
  pages={1196--1204},
  year={2013}
}

@article{dmi,
  author    = {Yilun Xu and
               Peng Cao and
               Yuqing Kong and
               Yizhou Wang},
  title     = {L{\_}DMI: An Information-theoretic Noise-robust Loss Function},
  journal   = {CoRR},
  volume    = {abs/1909.03388},
  year      = {2019}
}

@article{GhoshMS15,
  author    = {Aritra Ghosh and
               Naresh Manwani and
               P. S. Sastry},
  title     = {Making risk minimization tolerant to label noise},
  journal   = {Neurocomputing},
  volume    = {160},
  pages     = {93--107},
  year      = {2015}
}

@inproceedings{ShnayderAFP16,
  author    = {Victor Shnayder and
               Arpit Agarwal and
               Rafael M. Frongillo and
               David C. Parkes},
  title     = {Informed Truthfulness in Multi-Task Peer Prediction},
  booktitle = {{EC}},
  pages     = {179--196},
  publisher = {{ACM}},
  year      = {2016}
}

@ARTICLE{sig15, 
author={Liu, Yang and Liu, Mingyan}, 
journal={IEEE/ACM Transactions on Networking}, 
title={An Online Learning Approach to Improving the Quality of Crowd-Sourcing}, 
year={2017}, 
volume={25}, 
number={4}, 
pages={2166-2179}, 
month={Aug},}


@article{dawid1979maximum,
  title={Maximum likelihood estimation of observer error-rates using the EM algorithm},
  author={Dawid, Alexander Philip and Skene, Allan M},
  journal={Applied statistics},
  pages={20--28},
  year={1979},
  publisher={JSTOR}
}

@inproceedings{karger2011iterative,
  title={Iterative learning for reliable crowdsourcing systems},
  author={Karger, David R and Oh, Sewoong and Shah, Devavrat},
  booktitle={NIPS},
  pages={1953--1961},
  year={2011}
}

@inproceedings{liu2012variational,
  author    = {Qiang Liu and
               Jian Peng and
               Alexander T. Ihler},
  title     = {Variational Inference for Crowdsourcing},
  booktitle = {{NIPS}},
  pages     = {701--709},
  year      = {2012}
}

@article{van2015learning,
  title={Learning in the Presence of Corruption},
  author={van Rooyen, Brendan and Williamson, Robert C},
  journal={arXiv preprint arXiv:1504.00091},
  year={2015}
}

@inproceedings{menon2015learning,
  title={Learning from corrupted binary labels via class-probability estimation},
  author={Menon, Aditya and Van Rooyen, Brendan and Ong, Cheng Soon and Williamson, Bob},
  booktitle={ICML},
  pages={125--134},
  year={2015}
}

@article{gao2016incentivizing,
  title={Incentivizing evaluation via limited access to ground truth: Peer-prediction makes things worse},
  author={Gao, Alice and Wright, James R and Leyton-Brown, Kevin},
  journal={arXiv preprint arXiv:1606.07042},
  year={2016}
}

@inproceedings{witkowski2017proper,
  title={Proper proxy scoring rules},
  author={Witkowski, Jens and Atanasov, Pavel and Ungar, Lyle H and Krause, Andreas},
  year={2017}
}

@article{VulPashler2008,
Author = {E Vul and H Pashler},
Title = {Measuring the crowd within: Probabilistic representations within individuals},
Journal = {Psychological Science},
Volume = {19},
Number = {7},
Pages = {645-647},
Year = {2008}}


@inproceedings{xiao2015learning,
  title={Learning from massive noisy labeled data for image classification},
  author={Xiao, Tong and Xia, Tian and Yang, Yi and Huang, Chang and Wang, Xiaogang},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={2691--2699},
  year={2015}
}

@article{sukhbaatar2014learning,
  title={Learning from noisy labels with deep neural networks},
  author={Sukhbaatar, Sainbayar and Fergus, Rob},
  journal={arXiv preprint arXiv:1406.2080},
  volume={2},
  number={3},
  pages={4},
  year={2014},
  publisher={Citeseer}
}

@incollection{aggarwal2012survey,
  title={A survey of text classification algorithms},
  author={Aggarwal, Charu C and Zhai, ChengXiang},
  booktitle={Mining text data},
  pages={163--222},
  year={2012},
  publisher={Springer}
}

@article{liu2016classification,
  title={Classification with noisy labels by importance reweighting},
  author={Liu, Tongliang and Tao, Dacheng},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={38},
  number={3},
  pages={447--461},
  year={2016},
  publisher={IEEE}
}

@inproceedings{stempfel2009learning,
  title={Learning SVMs from sloppily labeled data},
  author={Stempfel, Guillaume and Ralaivola, Liva},
  booktitle={International Conference on Artificial Neural Networks},
  pages={884--893},
  year={2009},
  organization={Springer}
}

@article{cesa2011online,
  title={Online learning of noisy data},
  author={Cesa-Bianchi, Nicolo and Shalev-Shwartz, Shai and Shamir, Ohad},
  journal={IEEE Transactions on Information Theory},
  volume={57},
  number={12},
  pages={7907--7931},
  year={2011},
  publisher={IEEE}
}

@article{cesa1999sample,
  title={Sample-efficient strategies for learning in the presence of noise},
  author={Cesa-Bianchi, Nicolo and Dichterman, Eli and Fischer, Paul and Shamir, Eli and Simon, Hans Ulrich},
  journal={Journal of the ACM (JACM)},
  volume={46},
  number={5},
  pages={684--719},
  year={1999},
  publisher={ACM}
}

@inproceedings{ben2009agnostic,
  title={Agnostic Online Learning.},
    booktitle={COLT 2009},
  author={Ben-David, Shai and P{\'a}l, D{\'a}vid and Shalev-Shwartz, Shai}
}

@article{frenay2014classification,
  title={Classification in the presence of label noise: a survey},
  author={Fr{\'e}nay, Beno{\^\i}t and Verleysen, Michel},
  journal={IEEE transactions on neural networks and learning systems},
  volume={25},
  number={5},
  pages={845--869},
  year={2014},
  publisher={IEEE}
}


@inproceedings{scott2015rate,
  title={A Rate of Convergence for Mixture Proportion Estimation, with Application to Learning from Noisy Labels.},
  author={Scott, Clayton},
  booktitle={AISTATS},
  year={2015}
}


@inproceedings{scott2013classification,
  title={Classification with Asymmetric Label Noise: Consistency and Maximal Denoising.},
  author={Scott, Clayton and Blanchard, Gilles and Handy, Gregory and Pozzi, Sara and Flaska, Marek},
  booktitle={COLT},
  pages={489--511},
  year={2013}
}

@ARTICLE{2016arXiv160303151S,
   author = {{Shnayder}, V. and {Agarwal}, A. and {Frongillo}, R. and {Parkes}, D.~C.
	},
    title = "{Informed Truthfulness in Multi-Task Peer Prediction}",
  journal = {ACM EC},
archivePrefix = "arXiv",
   eprint = {1603.03151},
 primaryClass = "cs.GT",
 keywords = {Computer Science - Computer Science and Game Theory},
     year = 2016,
    month = mar,
   adsurl = {http://adsabs.harvard.edu/abs/2016arXiv160303151S},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}


@ARTICLE{MRZ:2005,
AUTHOR={Miller,Nolan and Resnick, Paul and Zeckhauser, Richard},
TITLE = {Eliciting Informative Feedback: {T}he Peer-Prediction Method},
JOURNAL={Management Science},
VOLUME = {51},
number = {9},
PAGES = {1359 --1373},
YEAR = 2005,
}

@inproceedings{bylander1994learning,
  title={Learning linear threshold functions in the presence of classification noise},
  author={Bylander, Tom},
  booktitle={Proceedings of the seventh annual conference on Computational learning theory},
  pages={340--347},
  year={1994},
  organization={ACM}
}

@article{LC17,
 title = {{Machine Learning aided Peer Prediction}},
 author = {Liu, Yang and Chen, Yiling},
 journal  ={ACM EC},
 month = {June},
 year = {2017}
}


@article{gneiting2007strictly,
  title={Strictly proper scoring rules, prediction, and estimation},
  author={Gneiting, Tilmann and Raftery, Adrian E},
  journal={Journal of the American Statistical Association},
  volume={102},
  number={477},
  pages={359--378},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{bartlett2006convexity,
  title={Convexity, classification, and risk bounds},
  author={Bartlett, Peter L and Jordan, Michael I and McAuliffe, Jon D},
  journal={Journal of the American Statistical Association},
  volume={101},
  number={473},
  pages={138--156},
  year={2006},
  publisher={Taylor \& Francis}
}


@inproceedings{abernethy2012characterization,
  title={A Characterization of Scoring Rules for Linear Properties.},
  booktitle={COLT 2012},
  author={Abernethy, Jacob D and Frongillo, Rafael M}
}

@inproceedings{steinwart2014elicitation,
  title={Elicitation and Identification of Properties.},
    booktitle={COLT 2014},
  author={Steinwart, Ingo and Pasin, Chlo{\'e} and Williamson, Robert C and Zhang, Siyu and others}
}

@article{reid2011information,
  title={Information, divergence and risk for binary experiments},
  author={Reid, Mark D and Williamson, Robert C},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Mar},
  pages={731--817},
  year={2011}
}

@inproceedings{agarwal2015consistent,
  title={On Consistent Surrogate Risk Minimization and Property Elicitation.},
      booktitle={COLT 2015},
  author={Agarwal, Arpit and Agarwal, Shivani}
}

@incollection{MillerRZ09,
  author    = {Nolan Miller and
               Paul Resnick and
               Richard Zeckhauser},
  title     = {Eliciting Informative Feedback: The Peer-Prediction Method},
  booktitle = {Computing with Social Trust},
  series    = {Human-Computer Interaction Series},
  pages     = {185--212},
  publisher = {Springer},
  year      = {2009}
}

@ARTICLE{score_rules,
title = {Strictly Proper Scoring Rules, Prediction, and Estimation},
author = {Gneiting, Tilmann and Raftery, Adrian E.},
year = {2007},
journal = {Journal of the American Statistical Association},
volume = {102},
pages = {359-378},
url = {https://EconPapers.repec.org/RePEc:bes:jnlasa:v:102:y:2007:p:359-378}
}

@inproceedings{dasgupta2013crowdsourced,
  title={Crowdsourced judgement elicitation with endogenous proficiency},
  author={Dasgupta, Anirban and Ghosh, Arpita},
  booktitle={Proceedings of the 22nd international conference on World Wide Web},
  pages={319--330},
  year={2013}
}

@inproceedings{witkowski2012robust,
  title={A robust bayesian truth serum for small populations},
  author={Witkowski, Jens and Parkes, David C},
  booktitle={Twenty-Sixth AAAI Conference on Artificial Intelligence},
  year={2012}
}

@inproceedings{radanovic2013robust,
  title={A robust bayesian truth serum for non-binary signals},
  author={Radanovic, Goran and Faltings, Boi},
  booktitle={Twenty-Seventh AAAI Conference on Artificial Intelligence},
  year={2013}
}

@inproceedings{liu2017machine,
  title={Machine-learning aided peer prediction},
  author={Liu, Yang and Chen, Yiling},
  booktitle={Proceedings of the 2017 ACM Conference on Economics and Computation},
  pages={63--80},
  year={2017}
}

@inproceedings{SongLYO19,
  author    = {Jialin Song and
               Ravi Lanka and
               Yisong Yue and
               Masahiro Ono},
  title     = {Co-training for Policy Learning},
  booktitle = {{UAI}},
  pages     = {441},
  publisher = {{AUAI} Press},
  year      = {2019}
}

@inproceedings{RenZYU18,
  author    = {Mengye Ren and
               Wenyuan Zeng and
               Bin Yang and
               Raquel Urtasun},
  title     = {Learning to Reweight Examples for Robust Deep Learning},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {80},
  pages     = {4331--4340},
  publisher = {{PMLR}},
  year      = {2018}
}

@inproceedings{ShuXY0ZXM19,
  author    = {Jun Shu and
               Qi Xie and
               Lixuan Yi and
               Qian Zhao and
               Sanping Zhou and
               Zongben Xu and
               Deyu Meng},
  title     = {Meta-Weight-Net: Learning an Explicit Mapping For Sample Weighting},
  booktitle = {NeurIPS},
  pages     = {1917--1928},
  year      = {2019}
}

@article{DBLP:journals/ml/Tsitsiklis94,
  author    = {John N. Tsitsiklis},
  title     = {Asynchronous Stochastic Approximation and Q-Learning},
  journal   = {Machine Learning},
  volume    = {16},
  number    = {3},
  pages     = {185--202},
  year      = {1994}
}

@inproceedings{DBLP:conf/nips/JaakkolaJS93,
  author    = {Tommi S. Jaakkola and
               Michael I. Jordan and
               Satinder P. Singh},
  title     = {Convergence of Stochastic Iterative Dynamic Programming Algorithms},
  booktitle = {{NIPS}},
  pages     = {703--710},
  year      = {1993}
}

@inproceedings{AsmuthLZ08,
  author    = {John Asmuth and
               Michael L. Littman and
               Robert Zinkov},
  title     = {Potential-based Shaping in Model-based Reinforcement Learning},
  booktitle = {{AAAI}},
  pages     = {604--609},
  publisher = {{AAAI} Press},
  year      = {2008}
}

@inproceedings{NgHR99,
  author    = {Andrew Y. Ng and
               Daishi Harada and
               Stuart J. Russell},
  title     = {Policy Invariance Under Reward Transformations: Theory and Application
               to Reward Shaping},
  booktitle = {{ICML}},
  pages     = {278--287},
  publisher = {Morgan Kaufmann},
  year      = {1999}
}

@book{von2007theory,
  title={Theory of games and economic behavior (commemorative edition)},
  author={Von Neumann, John and Morgenstern, Oskar},
  year={2007},
  publisher={Princeton university press}
}

@inproceedings{KearnsS98a,
  author    = {Michael J. Kearns and
               Satinder P. Singh},
  title     = {Finite-Sample Convergence Rates for Q-Learning and Indirect Algorithms},
  booktitle = {{NIPS}},
  pages     = {996--1002},
  year      = {1998}
}

@inproceedings{KearnsS00,
  author    = {Michael J. Kearns and
               Satinder P. Singh},
  title     = {Bias-Variance Error Bounds for Temporal Difference Updates},
  booktitle = {{COLT}},
  pages     = {142--147},
  year      = {2000}
}

@inproceedings{KearnsMN99,
  author    = {Michael J. Kearns and
               Yishay Mansour and
               Andrew Y. Ng},
  title     = {A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov
               Decision Processes},
  booktitle = {{IJCAI}},
  pages     = {1324--1231},
  year      = {1999}
}

@phdthesis{Kakade2003OnTS,
  author       = {Sham Machandranath Kakade}, 
  title        = {On the Sample Complexity of Reinforcement Learning},
  school       = {University of London},
  year         = 2003,
}

@inproceedings{HesterVPLSPHQSO18,
  author    = {Todd Hester and
               Matej Vecer{\'{\i}}k and
               Olivier Pietquin and
               Marc Lanctot and
               Tom Schaul and
               Bilal Piot and
               Dan Horgan and
               John Quan and
               Andrew Sendonaris and
               Ian Osband and
               Gabriel Dulac{-}Arnold and
               John Agapiou and
               Joel Z. Leibo and
               Audrunas Gruslys},
  title     = {Deep Q-learning From Demonstrations},
  booktitle = {{AAAI}},
  pages     = {3223--3230},
  publisher = {{AAAI} Press},
  year      = {2018}
}

@inproceedings{BrysHSCTN15,
  author    = {Tim Brys and
               Anna Harutyunyan and
               Halit Bener Suay and
               Sonia Chernova and
               Matthew E. Taylor and
               Ann Now{\'{e}}},
  title     = {Reinforcement Learning from Demonstration through Shaping},
  booktitle = {{IJCAI}},
  pages     = {3352--3358},
  publisher = {{AAAI} Press},
  year      = {2015}
}

@inproceedings{GuoCYTC19,
  author    = {Xiaoxiao Guo and
               Shiyu Chang and
               Mo Yu and
               Gerald Tesauro and
               Murray Campbell},
  title     = {Hybrid Reinforcement Learning with Expert State Sequences},
  booktitle = {{AAAI}},
  pages     = {3739--3746},
  publisher = {{AAAI} Press},
  year      = {2019}
}

@misc{rlblogpost,
    title={Deep Reinforcement Learning Doesn't Work Yet},
    author={Irpan, Alex},
    howpublished={\url{https://www.alexirpan.com/2018/02/14/rl-hard.html}},
    year={2018}
}

@inproceedings{RomoffP0FP18,
  author    = {Joshua Romoff and
               Alexandre Pich{\'{e}} and
               Peter Henderson and
               Vincent Fran{\c{c}}ois{-}Lavet and
               Joelle Pineau},
  title     = {Reward Estimation for Variance Reduction in Deep Reinforcement Learning},
  booktitle = {{ICLR} (Workshop)},
  publisher = {OpenReview.net},
  year      = {2018}
}

@article{dulac2019challenges,
  title={Challenges of real-world reinforcement learning},
  author={Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal={arXiv preprint arXiv:1904.12901},
  year={2019}
}

@inproceedings{SinghYFL19,
  author    = {Avi Singh and
               Larry Yang and
               Chelsea Finn and
               Sergey Levine},
  title     = {End-To-End Robotic Reinforcement Learning without Reward Engineering},
  booktitle = {Robotics: Science and Systems},
  year      = {2019}
}

@article{dota2,
  author    = {Christopher Berner and
               Greg Brockman and
               Brooke Chan and
               Vicki Cheung and
               Przemyslaw Debiak and
               Christy Dennison and
               David Farhi and
               Quirin Fischer and
               Shariq Hashme and
               Chris Hesse and
               Rafal J{\'{o}}zefowicz and
               Scott Gray and
               Catherine Olsson and
               Jakub Pachocki and
               Michael Petrov and
               Henrique Pond{\'{e}} de Oliveira Pinto and
               Jonathan Raiman and
               Tim Salimans and
               Jeremy Schlatter and
               Jonas Schneider and
               Szymon Sidor and
               Ilya Sutskever and
               Jie Tang and
               Filip Wolski and
               Susan Zhang},
  title     = {Dota 2 with Large Scale Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1912.06680},
  year      = {2019}
}

@article{cube_hand,
  author    = {Ilge Akkaya and
               Marcin Andrychowicz and
               Maciek Chociej and
               Mateusz Litwin and
               Bob McGrew and
               Arthur Petron and
               Alex Paino and
               Matthias Plappert and
               Glenn Powell and
               Raphael Ribas and
               Jonas Schneider and
               Nikolas Tezak and
               Jerry Tworek and
               Peter Welinder and
               Lilian Weng and
               Qiming Yuan and
               Wojciech Zaremba and
               Lei Zhang},
  title     = {Solving Rubik's Cube with a Robot Hand},
  journal   = {CoRR},
  volume    = {abs/1910.07113},
  year      = {2019}
}

@article{bojarski2016end,
  title={End to end learning for self-driving cars},
  author={Bojarski, Mariusz and Del Testa, Davide and Dworakowski, Daniel and Firner, Bernhard and Flepp, Beat and Goyal, Prasoon and Jackel, Lawrence D and Monfort, Mathew and Muller, Urs and Zhang, Jiakai and others},
  journal={arXiv preprint arXiv:1604.07316},
  year={2016}
}

@inproceedings{codevilla2018end,
  title={End-to-end driving via conditional behavior cloning},
  author={Codevilla, Felipe and Miiller, Matthias and L{\'o}pez, Antonio and Koltun, Vladlen and Dosovitskiy, Alexey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1--9},
  year={2018},
  organization={IEEE}
}

@inproceedings{EverittKOL17,
  author    = {Tom Everitt and
               Victoria Krakovna and
               Laurent Orseau and
               Shane Legg},
  title     = {Reinforcement Learning with a Corrupted Reward Channel},
  booktitle = {{IJCAI}},
  pages     = {4705--4713},
  year      = {2017}
}

@inproceedings{LaskeyLFDG17,
  author    = {Michael Laskey and
               Jonathan Lee and
               Roy Fox and
               Anca D. Dragan and
               Ken Goldberg},
  title     = {{DART:} Noise Injection for Robust Behavior Cloning},
  booktitle = {CoRL},
  series    = {Proceedings of Machine Learning Research},
  volume    = {78},
  pages     = {143--156},
  publisher = {{PMLR}},
  year      = {2017}
}

@inproceedings{sasaki2020behavioral,
  title={Behavioral Cloning from Noisy Demonstrations},
  author={Sasaki, Fumihiro and Yamashina, Ryota},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{huang2017adversarial,
  title={Adversarial attacks on neural network policies},
  author={Huang, Sandy and Papernot, Nicolas and Goodfellow, Ian and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1702.02284},
  year={2017}
}

@inproceedings{loftin2014learning,
  title={Learning something from nothing: Leveraging implicit human feedback strategies},
  author={Loftin, Robert and Peng, Bei and MacGlashan, James and Littman, Michael L and Taylor, Matthew E and Huang, Jeff and Roberts, David L},
  booktitle={The 23rd IEEE international symposium on robot and human interactive communication},
  pages={607--612},
  year={2014},
  organization={IEEE}
}

@inproceedings{ReddyDL20,
  author    = {Siddharth Reddy and
               Anca D. Dragan and
               Sergey Levine},
  title     = {{SQIL:} Behavior Cloning via Reinforcement Learning with Sparse
               Rewards},
  booktitle = {{ICLR}},
  publisher = {OpenReview.net},
  year      = {2020}
}

@article{pomerleau1991efficient,
  title={Efficient training of artificial neural networks for autonomous navigation},
  author={Pomerleau, Dean A},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={88--97},
  year={1991},
  publisher={MIT Press}
}

@inproceedings{ross2011reduction,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}

@inproceedings{ross2010efficient,
  title={Efficient reductions for imitation learning},
  author={Ross, St{\'e}phane and Bagnell, Drew},
  booktitle={Proceedings of the thirteenth international conference on artificial intelligence and statistics},
  pages={661--668},
  year={2010}
}

@inproceedings{Torabi2018,
  author    = {Faraz Torabi and
               Garrett Warnell and
               Peter Stone},
  title     = {Behavioral Cloning from Observation},
  booktitle = {{IJCAI}},
  pages     = {4950--4957},
  publisher = {ijcai.org},
  year      = {2018}
}

@article{Farag2018,
abstract = {In this paper, we propose using a Convolutional Neural Network (CNN) to learn safe driving behavior and smooth steering maneuvering as an empowerment of autonomous driving technologies. The training data is collected from a front-facing camera and the steering commands issued by an experienced driver driving in traffic as well as urban roads. This data is then used to train the proposed CNN to facilitate what we call it behavioral cloning. The proposed Behavior Cloning CNN is named as 'BCNet' and its deep seventeen-layer architecture has been selected after extensive trials. The BCNet got trained using Adam's optimization algorithm as a variant of the Scholastic Gradient Descent (SGD) technique. The paper goes through the development and training process in details and shows the image processing pipeline harnessed in the development. The proposed approach proved successful in cloning the driving behavior embedded in the training data set after extensive simulations.},
author = {Farag, Wael and Saleh, Zakaria},
doi = {10.1109/3ICT.2018.8855753},
file = {:Users/henryguo/Documents/Mendeley/Farag, Saleh - 2018 - Behavior cloning for autonomous driving using convolutional neural networks.pdf:pdf},
isbn = {9781538692073},
journal = {2018 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies, 3ICT 2018},
keywords = {Autonomous Driving,Behavioral Cloning,Convolutional Neural Network,Machine Learning},
publisher = {IEEE},
title = {{Behavior cloning for autonomous driving using convolutional neural networks}},
year = {2018}
}


@article{Giusti2016a,
abstract = {We study the problem of perceiving forest or mountain trails from a single monocular image acquired from the viewpoint of a robot traveling on the trail itself. Previous literature focused on trail segmentation, and used low-level features such as image saliency or appearance contrast; we propose a different approach based on a deep neural network used as a supervised image classifier. By operating on the whole image at once, our system outputs the main direction of the trail compared to the viewing direction. Qualitative and quantitative results computed on a large real-world dataset (which we provide for download) show that our approach outperforms alternatives, and yields an accuracy comparable to the accuracy of humans that are tested on the same image classification task. Preliminary results on using this information for quadrotor control in unseen trails are reported. To the best of our knowledge, this is the first letter that describes an approach to perceive forest trials, which is demonstrated on a quadrotor micro aerial vehicle.},
author = {Giusti, Alessandro and Guzzi, Jerome and Ciresan, Dan C. and He, Fang Lin and Rodriguez, Juan P. and Fontana, Flavio and Faessler, Matthias and Forster, Christian and Schmidhuber, Jurgen and Caro, Gianni Di and Scaramuzza, Davide and Gambardella, Luca M.},
doi = {10.1109/LRA.2015.2509024},
file = {:Users/henryguo/Documents/Mendeley/Giusti et al. - 2016 - A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots.pdf:pdf},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Aerial Robotics,Deep Learning,Machine Learning,Visual-Based Navigation},
number = {2},
pages = {661--667},
title = {{A Machine Learning Approach to Visual Perception of Forest Trails for Mobile Robots}},
volume = {1},
year = {2016}
}

@article{Reddy2019,
abstract = {Learning to imitate expert behavior from demonstrations can be challenging, especially in environments with high-dimensional, continuous observations and unknown dynamics. Supervised learning methods based on behavioral cloning (BC) suffer from distribution shift: because the agent greedily imitates demonstrated actions, it can drift away from demonstrated states due to error accumulation. Recent methods based on reinforcement learning (RL), such as inverse RL and generative adversarial behavior cloning (GAIL), overcome this issue by training an RL agent to match the demonstrations over a long horizon. Since the true reward function for the task is unknown, these methods learn a reward function from the demonstrations, often using complex and brittle approximation techniques that involve adversarial training. We propose a simple alternative that still uses RL, but does not require learning a reward function. The key idea is to provide the agent with an incentive to match the demonstrations over a long horizon, by encouraging it to return to demonstrated states upon encountering new, out-of-distribution states. We accomplish this by giving the agent a constant reward of r=+1 for matching the demonstrated action in a demonstrated state, and a constant reward of r=0 for all other behavior. Our method, which we call soft Q behavior cloning (SQIL), can be implemented with a handful of minor modifications to any standard Q-learning or off-policy actor-critic algorithm. Theoretically, we show that SQIL can be interpreted as a regularized variant of BC that uses a sparsity prior to encourage long-horizon imitation. Empirically, we show that SQIL outperforms BC and achieves competitive results compared to GAIL, on a variety of image-based and low-dimensional tasks in Box2D, Atari, and MuJoCo.},
archivePrefix = {arXiv},
arxivId = {1905.11108},
author = {Reddy, Siddharth and Dragan, Anca D. and Levine, Sergey},
eprint = {1905.11108},
file = {:Users/henryguo/Documents/Mendeley/Reddy, Dragan, Levine - 2019 - SQIL Behavior Cloning via Reinforcement Learning with Sparse Rewards.pdf:pdf},
title = {{SQIL: Behavior Cloning via Reinforcement Learning with Sparse Rewards}},
url = {http://arxiv.org/abs/1905.11108},
year = {2019}
}

@inproceedings{Ho2016a,
abstract = {Consider learning a policy from example expert behavior, without interaction with the expert or access to a reinforcement signal. One approach is to recover the expert's cost function with inverse reinforcement learning, then extract a policy from that cost function with reinforcement learning. This approach is indirect and can be slow. We propose a new general framework for directly extracting a policy from data as if it were obtained by reinforcement learning following inverse reinforcement learning. We show that a certain instantiation of our framework draws an analogy between behavior cloning and generative adversarial networks, from which we derive a model-free behavior cloning algorithm that obtains significant performance gains over existing model-free methods in imitating complex behaviors in large, high-dimensional environments.},
archivePrefix = {arXiv},
arxivId = {1606.03476},
author = {Ho, Jonathan and Ermon, Stefano},
booktitle = {Advances in Neural Information Processing Systems},
eprint = {1606.03476},
file = {:Users/henryguo/Documents/Mendeley/Ho, Ermon - 2016 - Generative adversarial behavior cloning(2).pdf:pdf},
issn = {10495258},
pages = {4572--4580},
title = {{Generative adversarial behavior cloning}},
year = {2016}
}


@inproceedings{Fu2018,
abstract = {Reinforcement learning provides a powerful and general framework for decision making and control, but its application in practice is often hindered by the need for extensive feature and reward engineering. Deep reinforcement learning methods can remove the need for explicit engineering of policy or value features, but still require a manually specified reward function. Inverse reinforcement learning holds the promise of automatic reward acquisition, but has proven exceptionally difficult to apply to large, high-dimensional problems with unknown dynamics. In this work, we propose AIRL, a practical and scalable inverse reinforcement learning algorithm based on an adversarial reward learning formulation. We demonstrate that AIRL is able to recover reward functions that are robust to changes in dynamics, enabling us to learn policies even under significant variation in the environment seen during training. Our experiments show that AIRL greatly outperforms prior methods in these transfer settings.},
archivePrefix = {arXiv},
arxivId = {1710.11248},
author = {Fu, Justin and Luo, Katie and Levine, Sergey},
booktitle = {6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings},
eprint = {1710.11248},
file = {:Users/henryguo/Documents/Mendeley/Fu, Luo, Levine - 2018 - Learning robust rewards with adversarial inverse reinforcement learning.pdf:pdf},
pages = {1--15},
title = {{Learning robust rewards with adversarial inverse reinforcement learning}},
year = {2018}
}


@article{Kamyar2019,
archivePrefix = {arXiv},
arxivId = {arXiv:1911.02256v1},
author = {Kamyar, Seyed and Ghasemipour, Seyed and Zemel, Richard and Gu, Shixiang},
eprint = {arXiv:1911.02256v1},
file = {:Users/henryguo/Documents/Mendeley/Kamyar et al. - 2019 - A Divergence Minimization Perspective on Behavior Cloning Methods.pdf:pdf},
keywords = {behavior cloning,state-marginal matching},
number = {CoRL},
title = {{A Divergence Minimization Perspective on Behavior Cloning Methods}},
year = {2019}
}

@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

@InProceedings{brantley2020disagreement-regularized,
author = {Brantley, Kiante and Sun, Wen and Henaff, Mikael},
title = {Disagreement-Regularized Behavior Cloning},
booktitle = {Eighth International Conference on Learning Representations (ICLR)},
year = {2020},
month = {April},
abstract = {We present a simple and effective algorithm designed to address the covariate shift problem in behavior cloning. It operates by training an ensemble of policies on the expert demonstration data, and using the variance of their predictions as a cost which is minimized with RL together with a supervised behavioral cloning cost. Unlike adversarial imitation methods, it uses a fixed reward function which is easy to optimize. We prove a regret bound for the algorithm which is linear in the time horizon multiplied by a coefficient which we show to be low for certain problems in which behavioral cloning fails. We evaluate our algorithm empirically across multiple pixel-based Atari environments and continuous control tasks, and show that it matches or significantly outperforms behavioral cloning and generative adversarial behavior cloning.},
}

@inproceedings{SuttonMSM99,
  author    = {Richard S. Sutton and
               David A. McAllester and
               Satinder P. Singh and
               Yishay Mansour},
  title     = {Policy Gradient Methods for Reinforcement Learning with Function Approximation},
  booktitle = {{NIPS}},
  pages     = {1057--1063},
  publisher = {The {MIT} Press},
  year      = {1999}
}

@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017}
}

@inproceedings{NgR00,
  author    = {Andrew Y. Ng and
               Stuart J. Russell},
  title     = {Algorithms for Inverse Reinforcement Learning},
  booktitle = {{ICML}},
  pages     = {663--670},
  publisher = {Morgan Kaufmann},
  year      = {2000}
}

@inproceedings{justesen2017learning,
  title={Learning macromanagement in starcraft from replays using deep learning},
  author={Justesen, Niels and Risi, Sebastian},
  booktitle={2017 IEEE Conference on Computational Intelligence and Games (CIG)},
  pages={162--169},
  year={2017},
  organization={IEEE}
}

@inproceedings{wu2019imitation,
  title={Imitation learning from imperfect demonstration},
  author={Wu, Yueh-Hua and Charoenphakdee, Nontawat and Bao, Han and Tangkaratt, Voot and Sugiyama, Masashi},
  booktitle={International Conference on Machine Learning},
  pages={6818--6827},
  year={2019},
  organization={PMLR}
}

@article{gao2018reinforcement,
  title={Reinforcement learning from imperfect demonstrations},
  author={Gao, Yang and Xu, Huazhe and Lin, Ji and Yu, Fisher and Levine, Sergey and Darrell, Trevor},
  journal={arXiv preprint arXiv:1802.05313},
  year={2018}
}

@article{agarwal2016learning,
  title={Learning statistical models of phenotypes using noisy labeled training data},
  author={Agarwal, Vibhu and Podchiyska, Tanya and Banda, Juan M and Goel, Veena and Leung, Tiffany I and Minty, Evan P and Sweeney, Timothy E and Gyang, Elsie and Shah, Nigam H},
  journal={Journal of the American Medical Informatics Association},
  volume={23},
  number={6},
  pages={1166--1173},
  year={2016},
  publisher={Oxford University Press}
}

@article{northcutt2021confident,
  title={Confident learning: Estimating uncertainty in dataset labels},
  author={Northcutt, Curtis and Jiang, Lu and Chuang, Isaac},
  journal={Journal of Artificial Intelligence Research},
  volume={70},
  pages={1373--1411},
  year={2021}
}

@article{li2021provably,
  title={Provably end-to-end label-noise learning without anchor points},
  author={Li, Xuefeng and Liu, Tongliang and Han, Bo and Niu, Gang and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2102.02400},
  year={2021}
}

@inproceedings{zhu2021second,
  title={A second-order approach to learning with instance-dependent label noise},
  author={Zhu, Zhaowei and Liu, Tongliang and Liu, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10113--10123},
  year={2021}
}



@inproceedings{
wei2021when,
title={When Optimizing  {\$}f{\$}-Divergence is Robust with Label Noise},
author={Jiaheng Wei and Yang Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=WesiCoRVQ15}
}

@inproceedings{zhu2021federated,
  title={Federated bandit: A gossiping approach},
  author={Zhu, Zhaowei and Zhu, Jingxuan and Liu, Ji and Liu, Yang},
  booktitle={Abstract Proceedings of the 2021 ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems},
  pages={3--4},
  year={2021}
}

@inproceedings{
cheng2021learning,
title={Learning with Instance-Dependent Label Noise: A Sample Sieve Approach},
author={Hao Cheng and Zhaowei Zhu and Xingyu Li and Yifei Gong and Xing Sun and Yang Liu},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=2VXyy9mIyU3}
}

@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={International conference on machine learning},
  pages={2778--2787},
  year={2017},
  organization={PMLR}
}

@article{liu2021importance,
  title={The importance of understanding instance-level noisy labels},
  author={Liu, Yang},
  journal={arXiv preprint arXiv:2102.05336},
  year={2021}
}

@article{zhu2021clusterability,
  title={Clusterability as an alternative to anchor points when learning with noisy labels},
  author={Zhu, Zhaowei and Song, Yiwen and Liu, Yang},
  journal={arXiv preprint arXiv:2102.05291},
  year={2021}
}

@misc{lee2020weaklysupervised,
    title={Weakly-Supervised Reinforcement Learning for Controllable Behavior},
    author={Lisa Lee and Benjamin Eysenbach and Ruslan Salakhutdinov and Shixiang and Gu and Chelsea Finn},
    year={2020},
    eprint={2004.02860},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@article{augmented_BCO,
  author    = {Juarez Monteiro and
               Nathan Gavenski and
               Roger Granada and
               Felipe Meneguzzi and
               Rodrigo Coelho Barros},
  title     = {Augmented Behavioral Cloning from Observation},
  journal   = {CoRR},
  volume    = {abs/2004.13529},
  year      = {2020}
}

@article{end_bc_sdv,
  author    = {Mariusz Bojarski and
               Davide Del Testa and
               Daniel Dworakowski and
               Bernhard Firner and
               Beat Flepp and
               Prasoon Goyal and
               Lawrence D. Jackel and
               Mathew Monfort and
               Urs Muller and
               Jiakai Zhang and
               Xin Zhang and
               Jake Zhao and
               Karol Zieba},
  title     = {End to End Learning for Self-Driving Cars},
  journal   = {CoRR},
  volume    = {abs/1604.07316},
  year      = {2016}
}

@inproceedings{yang21understand,
  author    = {Yang Liu},
  title     = {Understanding Instance-Level Label Noise: Disparate Impacts and Treatments},
  booktitle = {{ICML}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  pages     = {6725--6735},
  publisher = {{PMLR}},
  year      = {2021}
}

@inproceedings{dart,
  author    = {Michael Laskey and
               Jonathan Lee and
               Roy Fox and
               Anca D. Dragan and
               Ken Goldberg},
  title     = {{DART:} Noise Injection for Robust Imitation Learning},
  booktitle = {CoRL},
  series    = {Proceedings of Machine Learning Research},
  volume    = {78},
  pages     = {143--156},
  publisher = {{PMLR}},
  year      = {2017}
}

@inproceedings{prioritized_experience_replay,
  author    = {Tom Schaul and
               John Quan and
               Ioannis Antonoglou and
               David Silver},
  title     = {Prioritized Experience Replay},
  booktitle = {{ICLR} (Poster)},
  year      = {2016}
}