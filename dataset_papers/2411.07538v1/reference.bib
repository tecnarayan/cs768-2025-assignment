@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}
@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{li2023provable,
  title={Provable Identifiability of Two-Layer ReLU Neural Networks via LASSO Regularization},
  author={Li, Gen and Wang, Ganghua and Ding, Jie},
  journal={IEEE Transactions on Information Theory},
  year={2023},
  publisher={IEEE}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}
@article{tay2020long,
  title={Long range arena: A benchmark for efficient transformers},
  author={Tay, Yi and Dehghani, Mostafa and Abnar, Samira and Shen, Yikang and Bahri, Dara and Pham, Philip and Rao, Jinfeng and Yang, Liu and Ruder, Sebastian and Metzler, Donald},
  journal={arXiv preprint arXiv:2011.04006},
  year={2020}
}
@article{nangia2018listops,
  title={Listops: A diagnostic dataset for latent tree learning},
  author={Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1804.06028},
  year={2018}
}
@inproceedings{maas2011learning,
  title={Learning word vectors for sentiment analysis},
  author={Maas, Andrew and Daly, Raymond E and Pham, Peter T and Huang, Dan and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies},
  pages={142--150},
  year={2011}
}
@article{radev2013acl,
  title={The ACL anthology network corpus},
  author={Radev, Dragomir R and Muthukrishnan, Pradeep and Qazvinian, Vahed and Abu-Jbara, Amjad},
  journal={Language Resources and Evaluation},
  volume={47},
  pages={919--944},
  year={2013},
  publisher={Springer}
}
@article{linsley2018learning,
  title={Learning long-range spatial dependencies with horizontal gated recurrent units},
  author={Linsley, Drew and Kim, Junkyung and Veerabadran, Vijay and Windolf, Charles and Serre, Thomas},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{chen2021skyformer,
  title={Skyformer: Remodel self-attention with gaussian kernel and nystr$\backslash$" om method},
  author={Chen, Yifan and Zeng, Qi and Ji, Heng and Yang, Yun},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2122--2135},
  year={2021}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}
@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International conference on machine learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@article{nguyen2020global,
  title={Global convergence of deep networks with one wide layer followed by pyramidal topology},
  author={Nguyen, Quynh N and Mondelli, Marco},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={11961--11972},
  year={2020}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
@article{beltagy2020longformer,
  title={Longformer: The long-document transformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}
@article{jain2017non,
  title={Non-convex optimization for machine learning},
  author={Jain, Prateek and Kar, Purushottam and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={10},
  number={3-4},
  pages={142--363},
  year={2017},
  publisher={Now Publishers, Inc.}
}
@article{jin2021nonconvex,
  title={On nonconvex optimization for machine learning: Gradients, stochasticity, and saddle points},
  author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
  journal={Journal of the ACM (JACM)},
  volume={68},
  number={2},
  pages={1--29},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@incollection{danilova2022recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  booktitle={High-Dimensional Optimization and Probability: With a View Towards Data Science},
  pages={79--163},
  year={2022},
  publisher={Springer}
}
@article{li2018visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@inproceedings{huang2020improving,
  title={Improving transformer optimization through better initialization},
  author={Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims},
  booktitle={International Conference on Machine Learning},
  pages={4475--4483},
  year={2020},
  organization={PMLR}
}
@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}
@article{pan2023toward,
  title={Toward Understanding Why Adam Converges Faster Than SGD for Transformers},
  author={Pan, Yan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2306.00204},
  year={2023}
}
@article{huang2022convergence,
  title={Convergence analysis of deep residual networks},
  author={Huang, Wentao and Zhang, Haizhang},
  journal={arXiv preprint arXiv:2205.06571},
  year={2022}
}
@article{zou2020global,
  title={On the global convergence of training deep linear ResNets},
  author={Zou, Difan and Long, Philip M and Gu, Quanquan},
  journal={arXiv preprint arXiv:2003.01094},
  year={2020}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@article{he2020deberta,
  title={Deberta: Decoding-enhanced bert with disentangled attention},
  author={He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
  journal={arXiv preprint arXiv:2006.03654},
  year={2020}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}
@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}
@article{liu2020understanding,
  title={Understanding the difficulty of training transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  journal={arXiv preprint arXiv:2004.08249},
  year={2020}
}
@article{tian2023scan,
  title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon},
  journal={arXiv preprint arXiv:2305.16380},
  year={2023}
}
@inproceedings{bhojanapalli2020low,
  title={Low-rank bottleneck in multi-head attention models},
  author={Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International conference on machine learning},
  pages={864--873},
  year={2020},
  organization={PMLR}
}
@article{yang2022transformers,
  title={Transformers from an optimization perspective},
  author={Yang, Yongyi and Wipf, David P and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36958--36971},
  year={2022}
}
@article{noci2022signal,
  title={Signal propagation in transformers: Theoretical perspectives and the role of rank collapse},
  author={Noci, Lorenzo and Anagnostidis, Sotiris and Biggio, Luca and Orvieto, Antonio and Singh, Sidak Pal and Lucchi, Aurelien},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27198--27211},
  year={2022}
}

@misc{lu2022soft,
      title={SOFT: Softmax-free Transformer with Linear Complexity}, 
      author={Jiachen Lu and Jinghan Yao and Junge Zhang and Xiatian Zhu and Hang Xu and Weiguo Gao and Chunjing Xu and Tao Xiang and Li Zhang},
      year={2022},
      eprint={2110.11945},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@article{wu2024convergence,
  title={On the convergence of encoder-only shallow transformers},
  author={Wu, Yongtao and Liu, Fanghui and Chrysos, Grigorios and Cevher, Volkan},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{polyak1963gradient,
  title={Gradient methods for the minimisation of functionals},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={3},
  number={4},
  pages={864--878},
  year={1963},
  publisher={Elsevier}
}
@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}
@article{zhang2023trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}

@article{xie2021explanation,
  title={An explanation of in-context learning as implicit bayesian inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  journal={arXiv preprint arXiv:2111.02080},
  year={2021}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}
@article{ahn2024transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{blei2003latent,
  title={Latent dirichlet allocation},
  author={Blei, David M and Ng, Andrew Y and Jordan, Michael I},
  journal={Journal of machine Learning research},
  volume={3},
  number={Jan},
  pages={993--1022},
  year={2003}
}
@inproceedings{li2023transformers,
  title={How do transformers learn topic structure: Towards a mechanistic understanding},
  author={Li, Yuchen and Li, Yuanzhi and Risteski, Andrej},
  booktitle={International Conference on Machine Learning},
  pages={19689--19729},
  year={2023},
  organization={PMLR}
}
@inproceedings{li2023transformer,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}
@article{han2023explaining,
  title={Explaining emergent in-context learning as kernel regression},
  author={Han, Chi and Wang, Ziqi and Zhao, Han and Ji, Heng},
  year={2023}
}
@article{zhang2023and,
  title={What and how does in-context learning learn? bayesian model averaging, parameterization, and generalization},
  author={Zhang, Yufeng and Zhang, Fengzhuo and Yang, Zhuoran and Wang, Zhaoran},
  journal={arXiv preprint arXiv:2305.19420},
  year={2023}
}
@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@article{fu2023transformers,
  title={Transformers learn higher-order optimization methods for in-context learning: A study with linear models},
  author={Fu, Deqing and Chen, Tian-Qi and Jia, Robin and Sharan, Vatsal},
  journal={arXiv preprint arXiv:2310.17086},
  year={2023}
}
@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}
@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}
@article{dai2022can,
  title={Why can gpt learn in-context? language models implicitly perform gradient descent as meta-optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  journal={arXiv preprint arXiv:2212.10559},
  year={2022}
}

@article{lieber2021jurassic,
  title={Jurassic-1: Technical details and evaluation},
  author={Lieber, Opher and Sharir, Or and Lenz, Barak and Shoham, Yoav},
  journal={White Paper. AI21 Labs},
  volume={1},
  pages={9},
  year={2021}
}

@article{li2024dissecting,
  title={Dissecting chain-of-thought: Compositionality through in-context filtering and learning},
  author={Li, Yingcong and Sreenivasan, Kartik and Giannou, Angeliki and Papailiopoulos, Dimitris and Oymak, Samet},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{kim2024transformers,
  title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2402.01258},
  year={2024}
}
@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}
@article{muller2021transformers,
  title={Transformers can do bayesian inference},
  author={M{\"u}ller, Samuel and Hollmann, Noah and Arango, Sebastian Pineda and Grabocka, Josif and Hutter, Frank},
  journal={arXiv preprint arXiv:2112.10510},
  year={2021}
}
@article{lin2023transformers,
  title={Transformers as decision makers: Provable in-context reinforcement learning via supervised pretraining},
  author={Lin, Licong and Bai, Yu and Mei, Song},
  journal={arXiv preprint arXiv:2310.08566},
  year={2023}
}
@article{xing2024benefits,
  title={Benefits of Transformer: In-Context Learning in Linear Regression Tasks with Unstructured Data},
  author={Xing, Yue and Lin, Xiaofeng and Suh, Namjoon and Song, Qifan and Cheng, Guang},
  journal={arXiv preprint arXiv:2402.00743},
  year={2024}
}
@article{ahuja2023context,
  title={In-context learning through the bayesian prism},
  author={Ahuja, Kabir and Panwar, Madhur and Goyal, Navin},
  journal={arXiv preprint arXiv:2306.04891},
  year={2023}
}
@article{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-Bayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}
@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}
@article{yang2022transformers,
  title={Transformers from an optimization perspective},
  author={Yang, Yongyi and Wipf, David P and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={36958--36971},
  year={2022}
}
@article{chen2024training,
  title={Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
  author={Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2402.19442},
  year={2024}
}


@article{song2022distributed,
  title={Distributed Optimization for Overparameterized
Problems: Achieving Optimal Dimension Independent
Communication Complexity},
  author={Song, Bingqing and Tsaknakis, Ioannis and Yau, Chung-Yiu and Wai, Hoi-To and Hong, Mingyi},
  journal={Preprint},
  year={2022}
}
@article{li2017visualizing,
  title={Visualizing the loss landscape of neural nets},
  author={Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  journal={arXiv preprint arXiv:1712.09913},
  year={2017}
}

@article{sun2020global,
  title={The global landscape of neural networks: An overview},
  author={Sun, Ruoyu and Li, Dawei and Liang, Shiyu and Ding, Tian and Srikant, Rayadurgam},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={5},
  pages={95--108},
  year={2020},
  publisher={IEEE}
}

@article{han2015deep,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}
@article{nazir2021power,
  title={Power and Resource Allocation in Wireless Communication Network},
  author={Nazir, Mohsin and Sabah, Aneeqa and Sarwar, Sana and Yaseen, Azeema and Jurcut, Anca},
  journal={Wireless Personal Communications},
  pages={1--24},
  year={2021},
  publisher={Springer}
}
@article{yu2004iterative,
  title={Iterative water-filling for Gaussian vector multiple-access channels},
  author={Yu, Wei and Rhee, Wonjong and Boyd, Stephen and Cioffi, John M},
  journal={IEEE Transactions on Information Theory},
  volume={50},
  number={1},
  pages={145--152},
  year={2004},
  publisher={IEEE}
}
@article{wu2023many,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2310.08391},
  year={2023}
}
@article{baligh2014cross,
  title={Cross-layer provision of future cellular networks: A WMMSE-based approach},
  author={Baligh, Hadi and Hong, Mingyi and Liao, Wei-Cheng and Luo, Zhi-Quan and Razaviyayn, Meisam and Sanjabi, Maziar and Sun, Ruoyu},
  journal={IEEE Signal Processing Magazine},
  volume={31},
  number={6},
  pages={56--68},
  year={2014},
  publisher={IEEE}
}
@article{sun2018learning,
  title={Learning to optimize: Training deep neural networks for interference management},
  author={Sun, Haoran and Chen, Xiangyi and Shi, Qingjiang and Hong, Mingyi and Fu, Xiao and Sidiropoulos, Nicholas D},
  journal={IEEE Transactions on Signal Processing},
  volume={66},
  number={20},
  pages={5438--5453},
  year={2018},
  publisher={IEEE}
}
@article{liang2019towards,
  title={Towards optimal power control via ensembling deep neural networks},
  author={Liang, Fei and Shen, Cong and Yu, Wei and Wu, Feng},
  journal={IEEE Transactions on Communications},
  volume={68},
  number={3},
  pages={1760--1776},
  year={2019},
  publisher={IEEE}
}
@inproceedings{he2015delving,
  title={Delving deep into rectifiers: Surpassing human-level performance on imagenet classification},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1026--1034},
  year={2015}
}
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}
@INPROCEEDINGS{9593184,
  author={Song, Bingqing and Sun, Haoran and Pu, Wenqiang and Liu, Sijia and Hong, Mingyi},
  booktitle={2021 IEEE 22nd International Workshop on Signal Processing Advances in Wireless Communications (SPAWC)}, 
  title={To Supervise or Not to Supervise: How to Effectively Learn Wireless Interference Management Models?}, 
  year={2021},
  volume={},
  number={},
  pages={211-215},
  doi={10.1109/SPAWC51858.2021.9593184}}

@inproceedings{rudelson2010non,
  title={Non-asymptotic theory of random matrices: extreme singular values},
  author={Rudelson, Mark and Vershynin, Roman},
  booktitle={Proceedings of the International Congress of Mathematicians 2010 (ICM 2010) (In 4 Volumes) Vol. I: Plenary Lectures and Ceremonies Vols. II--IV: Invited Lectures},
  pages={1576--1602},
  year={2010},
  organization={World Scientific}
}

@article{wang2023ICL,
    author = {Wang, Xinyi and Zhu, Wanrong and Saxon, Michael and Steyvers, Mark and Wang, Yang William},
    title = {Large Language Models Are Latent Variable Models: Explaining and Finding Good Demonstrations for In-Context Learning},
    journal = {arXiv preprint arXiv:2301.11916},
    year = {2023}
}

@inproceedings{min-etal-2022-metaicl,
    title = {MetaICL: Learning to Learn In Context},
    author = {Min, Sewon  and
      Lewis, Mike  and
      Zettlemoyer, Luke  and
      Hajishirzi, Hannaneh",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir},
    booktitle = {Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    month = {Jul},
    year = {2022},
    address = {Seattle, United States},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2022.naacl-main.201},
    doi = {10.18653/v1/2022.naacl-main.201},
    pages = {2791--2809},

}