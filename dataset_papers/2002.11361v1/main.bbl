\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Vergara et~al.(2012)Vergara, Vembu, Ayhan, Ryan, Homer, and
  Huerta]{vergara2012Chemical}
A.~Vergara, S.~Vembu, T.~Ayhan, M.~A. Ryan, M.~L. Homer, and R.~Huerta.
\newblock Chemical gas sensor drift compensation using classifier ensembles.
\newblock \emph{Journal of the American Statistical Association}, -1:\penalty0
  320--329, 2012.

\bibitem[Bobu et~al.(2018)Bobu, Tzeng, Hoffman, and Darrell]{bobu2018adapting}
A.~Bobu, E.~Tzeng, J.~Hoffman, and T.~Darrell.
\newblock Adapting to continuously shifting domains.
\newblock In \emph{International Conference on Learning Representations
  Workshop (ICLR)}, 2018.

\bibitem[Farshchian et~al.(2019)Farshchian, Gallego, Cohen, Bengio, Miller, and
  Solla]{farshchian2019adversarial}
A.~Farshchian, J.~A. Gallego, J.~P. Cohen, Y.~Bengio, L.~E. Miller, and S.~A.
  Solla.
\newblock Adversarial domain adaptation for stable brain-machine interfaces.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Sethi and Kantardzic(2017)]{sethi2017reliable}
T.~S. Sethi and M.~Kantardzic.
\newblock On the reliable detection of concept drift from streaming unlabeled
  data.
\newblock \emph{Expert Systems with Applications}, 82:\penalty0 77--99, 2017.

\bibitem[Chapelle et~al.(2006)Chapelle, Zien, and
  Scholkopf]{chapelle2006semisupervised}
O.~Chapelle, A.~Zien, and B.~Scholkopf.
\newblock \emph{Semi-Supervised Learning}.
\newblock MIT Press, 2006.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020selftraining}
Q.~Xie, M.~Luong, E.~Hovy, and Q.~V. Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock \emph{arXiv}, 2020.

\bibitem[Uesato et~al.(2019)Uesato, Alayrac, Huang, Stanforth, Fawzi, and
  Kohli]{uesato2019are}
J.~Uesato, J.~Alayrac, P.~Huang, R.~Stanforth, A.~Fawzi, and P.~Kohli.
\newblock Are labels required for improving adversarial robustness?
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Carmon et~al.(2019)Carmon, Raghunathan, Schmidt, Liang, and
  Duchi]{carmon2019unlabeled}
Y.~Carmon, A.~Raghunathan, L.~Schmidt, P.~Liang, and J.~C. Duchi.
\newblock Unlabeled data improves adversarial robustness.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Najafi et~al.(2019)Najafi, Maeda, Koyama, and
  Miyato]{najafi2019robustness}
A.~Najafi, S.~Maeda, M.~Koyama, and T.~Miyato.
\newblock Robustness to adversarial perturbations in learning from incomplete
  data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Ginosar et~al.(2017)Ginosar, Rakelly, Sachs, Yin, Lee, Kr채henb체hl,
  and Efros]{ginosar2017portraits}
S.~Ginosar, K.~Rakelly, S.~M. Sachs, B.~Yin, C.~Lee, P.~Kr채henb체hl, and A.~A.
  Efros.
\newblock A century of portraits: A visual historical record of american high
  school yearbooks.
\newblock \emph{IEEE Transactions on Computational Imaging}, 3, 2017.

\bibitem[Zhao et~al.(2019)Zhao, des Combes, Zhang, and Gordon]{zhao2019zhao}
H.~Zhao, R.~T. des Combes, K.~Zhang, and G.~J. Gordon.
\newblock On learning invariant representation for domain adaptation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2019.

\bibitem[Shu et~al.(2018)Shu, Bui, Narui, and Ermon]{shu2018dirtt}
R.~Shu, H.~H. Bui, H.~Narui, and S.~Ermon.
\newblock A {DIRT}-{T} approach to unsupervised domain adaptation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Jiayuan et~al.(2006)Jiayuan, J., Arthur, M., and
  Bernhard]{huang2006correcting}
H.~Jiayuan, S.~A. J., G.~Arthur, B.~K. M., and S.~Bernhard.
\newblock Correcting sample selection bias by unlabeled data.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2006.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
S.~Ben-David, J.~Blitzer, K.~Crammer, A.~Kulesza, F.~Pereira, and J.~W.
  Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Amini and Gallinari(2003)]{amini2003semisupervised}
M.~Amini and P.~Gallinari.
\newblock Semi-supervised learning with explicit misclassification modeling.
\newblock In \emph{International Joint Conference on Artificial Intelligence
  (IJCAI)}, 2003.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1225--1234, 2016.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  448--456, 2015.

\bibitem[Lee(2013)]{lee2013pseudo}
D.~Lee.
\newblock Pseudo-label: The simple and efficient semi-supervised learning
  method for deep neural networks.
\newblock In \emph{ICML Workshop on Challenges in Representation Learning},
  2013.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin,
  Zhang, and Raffel]{sohn2020fixmatch}
K.~Sohn, D.~Berthelot, C.~Li, Z.~Zhang, N.~Carlini, E.~D. Cubuk, A.~Kurakin,
  H.~Zhang, and C.~Raffel.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock \emph{arXiv}, 2020.

\bibitem[Long et~al.(2013)Long, Wang, Ding, Sun, and Yu]{long2013transfer}
M.~Long, J.~Wang, G.~Ding, J.~Sun, and P.~S. Yu.
\newblock Transfer feature learning with joint distribution adaptation.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 2200--2207, 2013.

\bibitem[Zou et~al.(2019)Zou, Yu, Liu, Kumar, and Wang]{zou2019confidence}
Y.~Zou, Z.~Yu, X.~Liu, B.~Kumar, and J.~Wang.
\newblock Confidence regularized self-training.
\newblock \emph{arXiv preprint arXiv:1908.09822}, 2019.

\bibitem[Inoue et~al.(2018)Inoue, Furuta, Yamasaki, and Aizawa]{inoue2018cross}
N.~Inoue, R.~Furuta, T.~Yamasaki, and K.~Aizawa.
\newblock Cross-domain weakly-supervised object detection through progressive
  domain adaptation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 5001--5009, 2018.

\bibitem[Grandvalet and Bengio(2005)]{grandvalet05entropy}
Y.~Grandvalet and Y.~Bengio.
\newblock Entropy regularization.
\newblock In \emph{Semi-Supervised Learning}, 2005.

\bibitem[Rigollet(2007)]{rigollet2007generalization}
P.~Rigollet.
\newblock Generalization error bounds in semi-supervised classification under
  the cluster assumption.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 8:\penalty0
  1369--1392, 2007.

\bibitem[Singh et~al.(2008)Singh, Nowak, and Zhu]{singh2008unlabeled}
A.~Singh, R.~Nowak, and J.~Zhu.
\newblock Unlabeled data: Now it helps, now it doesn't.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2008.

\bibitem[Ben-David et~al.(2008)Ben-David, Lu, and Pal]{shai2008unlabeled}
S.~Ben-David, T.~Lu, and D.~Pal.
\newblock Does unlabeled data provably help? worst-case analysis of the sample
  complexity of semi-supervised learning.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2008.

\bibitem[Raghunathan et~al.(2020)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2020understanding}
A.~Raghunathan, S.~M. Xie, F.~Yang, J.~C. Duchi, and P.~Liang.
\newblock Understanding and mitigating the tradeoff between robustness and
  accuracy.
\newblock \emph{arXiv}, 2020.

\bibitem[Blum and Mitchell(1998)]{blum98cotraining}
A.~Blum and T.~Mitchell.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In \emph{Conference on Learning Theory (COLT)}, 1998.

\bibitem[{Qui\~nonero-Candela} et~al.(2009){Qui\~nonero-Candela}, Sugiyama,
  Schwaighofer, and Lawrence]{quinonero2009dataset}
J.~{Qui\~nonero-Candela}, M.~Sugiyama, A.~Schwaighofer, and N.~D. Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock The MIT Press, 2009.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
H.~Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of Statistical Planning and Inference}, 90:\penalty0
  227--244, 2000.

\bibitem[Sugiyama et~al.(2007)Sugiyama, Krauledat, and
  Muller]{sugiyama2007covariate}
M.~Sugiyama, M.~Krauledat, and K.~Muller.
\newblock Covariate shift adaptation by importance weighted cross validation.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 8:\penalty0
  985--1005, 2007.

\bibitem[Mansour et~al.(2009)Mansour, Mohri, and
  Rostamizadeh]{mansour2009domain}
Y.~Mansour, M.~Mohri, and A.~Rostamizadeh.
\newblock Domain adaptation: Learning bounds and algorithms.
\newblock In \emph{Conference on Learning Theory (COLT)}, 2009.

\bibitem[Tzeng et~al.(2014)Tzeng, Hoffman, Zhang, Saenko, and
  Darrell]{tzeng2014domain}
E.~Tzeng, J.~Hoffman, N.~Zhang, K.~Saenko, and T.~Darrell.
\newblock Deep domain confusion: Maximizing for domain invariance.
\newblock \emph{arXiv preprint arXiv:1412.3474}, 2014.

\bibitem[Ganin and Lempitsky(2015)]{ganin2015domain}
Y.~Ganin and V.~Lempitsky.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pages
  1180--1189, 2015.

\bibitem[Tzeng et~al.(2017)Tzeng, Hoffman, Saenko, and
  Darrell]{tzeng2017domain}
E.~Tzeng, J.~Hoffman, K.~Saenko, and T.~Darrell.
\newblock Adversarial discriminative domain adaptation.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2017.

\bibitem[Hoffman et~al.(2018)Hoffman, Tzeng, Park, Zhu, Isola, Saenko, Efros,
  and Darrell]{hoffman2018cycada}
J.~Hoffman, E.~Tzeng, T.~Park, J.~Zhu, P.~Isola, K.~Saenko, A.~A. Efros, and
  T.~Darrell.
\newblock Cycada: Cycle consistent adversarial domain adaptation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and
  Wang]{peng2019moment}
X.~Peng, Q.~Bai, X.~Xia, Z.~Huang, K.~Saenko, and B.~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{International Conference on Computer Vision (ICCV)}, 2019.

\bibitem[Hoffman et~al.(2014)Hoffman, Darrell, and
  Saenko]{hoffman2014continuous}
J.~Hoffman, T.~Darrell, and K.~Saenko.
\newblock Continuous manifold based adaptation for evolving visual domains.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR)}, 2014.

\bibitem[Michael et~al.(2018)Michael, Dennis, Mara, Peter, and
  Dorit]{gadermayr2018gradual}
G.~Michael, E.~Dennis, K.~B. Mara, B.~Peter, and M.~Dorit.
\newblock Gradual domain adaptation for segmenting whole slide images showing
  pathological variability.
\newblock In \emph{Image and Signal Processing}, 2018.

\bibitem[Markus et~al.(2018)Markus, Alex, and Ingmar]{wulfmeier2018incremental}
W.~Markus, B.~Alex, and P.~Ingmar.
\newblock Incremental adversarial domain adaptation for continually changing
  environments.
\newblock In \emph{International Conference on Robotics and Automation (ICRA)},
  2018.

\bibitem[Shalev-Shwartz(2007)]{shalev07online}
S.~Shalev-Shwartz.
\newblock \emph{Online Learning: Theory, Algorithms, and Applications}.
\newblock PhD thesis, The Hebrew University of Jerusalem, 2007.

\bibitem[Silver et~al.(2013)Silver, Yang, and Li]{silver2013lifelong}
D.~L. Silver, Q.~Yang, and L.~Li.
\newblock Lifelong machine learning systems: Beyond learning algorithms.
\newblock In \emph{Association for the Advancement of Artificial Intelligence
  (AAAI)}, volume~13, 2013.

\bibitem[Kramer(1988)]{kramer1988learning}
A.~H. Kramer.
\newblock Learning despite distribution shift.
\newblock In \emph{Connectionist Models Summer School}, 1988.

\bibitem[Bartlett(1992)]{bartlett1992learning}
P.~L. Bartlett.
\newblock Learning with a slowly changing distribution.
\newblock In \emph{Conference on Learning Theory (COLT)}, 1992.

\bibitem[Bartlett et~al.(1996)Bartlett, Ben-David, and
  Kulkarni]{bartlett1996learning}
P.~L. Bartlett, S.~Ben-David, and S.~R. Kulkarni.
\newblock Learning changing concepts by exploiting the structure of change.
\newblock \emph{Machine Learning}, 41, 1996.

\bibitem[Liang(2016)]{liang2016statistical}
Percy Liang.
\newblock Statistical learning theory.
\newblock \url{https://web.stanford.edu/class/cs229t/notes.pdf}, 2016.

\end{thebibliography}
