\begin{thebibliography}{81}
\expandafter\ifx\csname natexlab\endcsname\relax\def\natexlab#1{#1}\fi

\bibitem[{Almazrouei et~al.(2023)Almazrouei, Alobeidli, Alshamsi, Cappelli, Cojocaru, Alhammadi, Daniele, Heslow, Launay, Malartic, Noune, Pannier, and Penedo}]{falcon180b}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Maitha Alhammadi, Mazzotta Daniele, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023.
\newblock The falcon series of language models: Towards open frontier models.

\bibitem[{Bengio et~al.(2007)Bengio, Lamblin, Popovici, and Larochelle}]{Bengio_Lamblin_Popovici_Larochelle_2007}
Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. 2007.
\newblock \href {https://doi.org/10.7551/mitpress/7503.003.0024} {\emph{Greedy Layer-Wise Training of Deep Networks}}, page 153–160.

\bibitem[{Biderman et~al.(2023)Biderman, Schoelkopf, Anthony, Bradley, O’Brien, Hallahan, Khan, Purohit, Prashanth, Raff et~al.}]{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit, USVSN~Sai Prashanth, Edward Raff, et~al. 2023.
\newblock Pythia: A suite for analyzing large language models across training and scaling.
\newblock In \emph{International Conference on Machine Learning}, pages 2397--2430. PMLR.

\bibitem[{Brock et~al.(2017)Brock, Lim, Ritchie, and Weston}]{brocker2017freezeout}
Andrew Brock, Theodore Lim, James~M. Ritchie, and Nick Weston. 2017.
\newblock \href {http://dblp.uni-trier.de/db/journals/corr/corr1706.html#BrockLRW17} {Freezeout: Accelerate training by progressively freezing layers.}
\newblock \emph{CoRR}, abs/1706.04983.

\bibitem[{Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei}]{brown_language_2020}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020.
\newblock \href {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf} {Language {Models} are {Few}-{Shot} {Learners}}.
\newblock In \emph{Advances in {Neural} {Information} {Processing} {Systems}}, volume~33, pages 1877--1901. Curran Associates, Inc.

\bibitem[{Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin}]{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. 2016.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}.

\bibitem[{Cheng et~al.(2016)Cheng, Koc, Harmsen, Shaked, Chandra, Aradhye, Anderson, Corrado, Chai, Ispir et~al.}]{cheng2016wide}
Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et~al. 2016.
\newblock Wide \& deep learning for recommender systems.
\newblock In \emph{Proceedings of the 1st workshop on deep learning for recommender systems}, pages 7--10.

\bibitem[{Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing}]{vicuna2023}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing. 2023.
\newblock \href {https://lmsys.org/blog/2023-03-30-vicuna/} {Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality}.

\bibitem[{Chowdhery et~al.(2022)Chowdhery, Narang, Devlin, Bosma, Mishra, Roberts, Barham, Chung, Sutton, Gehrmann, Schuh, Shi, Tsvyashchenko, Maynez, Rao, Barnes, Tay, Shazeer, Prabhakaran, Reif, Du, Hutchinson, Pope, Bradbury, Austin, Isard, Gur-Ari, Yin, Duke, Levskaya, Ghemawat, Dev, Michalewski, Garc{\'i}a, Misra, Robinson, Fedus, Zhou, Ippolito, Luan, Lim, Zoph, Spiridonov, Sepassi, Dohan, Agrawal, Omernick, Dai, Pillai, Pellat, Lewkowycz, Moreira, Child, Polozov, Lee, Zhou, Wang, Saeta, D{\'i}az, Firat, Catasta, Wei, Meier-Hellstern, Eck, Dean, Petrov, and Fiedel}]{Chowdhery2022PaLMSL}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi~Tay, Noam~M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Benton~C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc{\'i}a, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew~M. Dai, Thanumalayan~Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D{\'i}az, Orhan Firat, Michele Catasta, Jason Wei, Kathleen~S. Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,
  and Noah Fiedel. 2022.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{J. Mach. Learn. Res.}, 24:240:1--240:113.

\bibitem[{Chuang et~al.(2023)Chuang, Xie, Luo, Kim, Glass, and He}]{chuang2023dola}
Yung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon Kim, James Glass, and Pengcheng He. 2023.
\newblock Dola: Decoding by contrasting layers improves factuality in large language models.
\newblock \emph{arXiv preprint arXiv:2309.03883}.

\bibitem[{Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman}]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021.
\newblock \href {http://arxiv.org/abs/2110.14168} {Training verifiers to solve math word problems}.

\bibitem[{Dao(2023)}]{dao2023flashattention2}
Tri Dao. 2023.
\newblock Flash{A}ttention-2: Faster attention with better parallelism and work partitioning.

\bibitem[{Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}}]{dao2022flashattention}
Tri Dao, Daniel~Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}. 2022.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}.

\bibitem[{Dettmers et~al.(2023)Dettmers, Pagnoni, Holtzman, and Zettlemoyer}]{dettmers2023qlora}
Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.
\newblock Qlora: Efficient finetuning of quantized llms.
\newblock \emph{arXiv preprint arXiv:2305.14314}.

\bibitem[{Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova}]{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
\newblock \href {https://doi.org/10.18653/v1/N19-1423} {{BERT}: Pre-training of deep bidirectional transformers for language understanding}.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186, Minneapolis, Minnesota. Association for Computational Linguistics.

\bibitem[{Diao et~al.(2022)Diao, Huang, Xu, Li, Yong, Zhou, and Zhang}]{diao2022black}
Shizhe Diao, Zhichao Huang, Ruijia Xu, Xuechun Li, LIN Yong, Xiao Zhou, and Tong Zhang. 2022.
\newblock Black-box prompt learning for pre-trained language models.
\newblock \emph{Transactions on Machine Learning Research}.

\bibitem[{Diao et~al.(2023{\natexlab{a}})Diao, Pan, Dong, Shum, Zhang, Xiong, and Zhang}]{diao2023lmflow}
Shizhe Diao, Rui Pan, Hanze Dong, Ka~Shun Shum, Jipeng Zhang, Wei Xiong, and Tong Zhang. 2023{\natexlab{a}}.
\newblock Lmflow: An extensible toolkit for finetuning and inference of large foundation models.
\newblock \emph{arXiv preprint arXiv:2306.12420}.

\bibitem[{Diao et~al.(2023{\natexlab{b}})Diao, Wang, Lin, and Zhang}]{diao2023active}
Shizhe Diao, Pengcheng Wang, Yong Lin, and Tong Zhang. 2023{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2302.12246} {Active prompting with chain-of-thought for large language models}.

\bibitem[{Diao et~al.(2021)Diao, Xu, Su, Jiang, Song, and Zhang}]{diao2021taming}
Shizhe Diao, Ruijia Xu, Hongjin Su, Yilei Jiang, Yan Song, and Tong Zhang. 2021.
\newblock Taming pre-trained language models with n-gram representations for low-resource domain adaptation.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 3336--3349.

\bibitem[{Diao et~al.(2023{\natexlab{c}})Diao, Xu, Xu, Wang, and Zhang}]{diao2023mixture}
Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, and Tong Zhang. 2023{\natexlab{c}}.
\newblock Mixture-of-domain-adapters: Decoupling and injecting domain knowledge to pre-trained language models memories.
\newblock \emph{arXiv preprint arXiv:2306.05406}.

\bibitem[{Ding et~al.(2022)Ding, Qin, Yang, Wei, Yang, Su, Hu, Chen, Chan, Chen et~al.}]{ding2022delta}
Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et~al. 2022.
\newblock Delta tuning: A comprehensive study of parameter efficient methods for pre-trained language models.
\newblock \emph{arXiv preprint arXiv:2203.06904}.

\bibitem[{Fan et~al.(2024)Fan, Jiang, Li, Meng, Han, Shang, Sun, Wang, and Wang}]{fan2024not}
Siqi Fan, Xin Jiang, Xiang Li, Xuying Meng, Peng Han, Shuo Shang, Aixin Sun, Yequan Wang, and Zhongyuan Wang. 2024.
\newblock Not all layers of llms are necessary during inference.
\newblock \emph{arXiv preprint arXiv:2403.02181}.

\bibitem[{Gou et~al.(2023)Gou, Liu, Chen, Hong, Xu, Li, Yeung, Kwok, and Zhang}]{gou2023mixture}
Yunhao Gou, Zhili Liu, Kai Chen, Lanqing Hong, Hang Xu, Aoxue Li, Dit-Yan Yeung, James~T Kwok, and Yu~Zhang. 2023.
\newblock Mixture of cluster-conditional lora experts for vision-language instruction tuning.
\newblock \emph{arXiv preprint arXiv:2312.12379}.

\bibitem[{Hambardzumyan et~al.(2021)Hambardzumyan, Khachatrian, and May}]{hambardzumyan2021warp}
Karen Hambardzumyan, Hrant Khachatrian, and Jonathan May. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.acl-long.381} {{WARP}: {W}ord-level {A}dversarial {R}e{P}rogramming}.
\newblock In \emph{Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)}, pages 4921--4933, Online. Association for Computational Linguistics.

\bibitem[{Han et~al.(2021)Han, Zhao, Ding, Liu, and Sun}]{han2021ptr}
Xu~Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and Maosong Sun. 2021.
\newblock \href {https://arxiv.org/abs/2105.11259} {{PTR: Prompt Tuning with Rules for Text Classification}}.
\newblock \emph{ArXiv preprint}, abs/2105.11259.

\bibitem[{Hendrycks et~al.(2020)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt}]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.
\newblock Measuring massive multitask language understanding.
\newblock \emph{arXiv preprint arXiv:2009.03300}.

\bibitem[{Hinton et~al.(2006)Hinton, Osindero, and Teh}]{Hinton_Osindero_Teh_2006}
Geoffrey~E. Hinton, Simon Osindero, and Yee-Whye Teh. 2006.
\newblock \href {https://doi.org/10.1162/neco.2006.18.7.1527} {A fast learning algorithm for deep belief nets}.
\newblock \emph{Neural Computation}, page 1527–1554.

\bibitem[{Ho et~al.(2020)Ho, Jain, and Abbeel}]{Ho2020DenoisingDP}
Jonathan Ho, Ajay Jain, and P.~Abbeel. 2020.
\newblock \href {https://api.semanticscholar.org/CorpusID:219955663} {Denoising diffusion probabilistic models}.
\newblock \emph{ArXiv}, abs/2006.11239.

\bibitem[{Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone, De~Laroussilhe, Gesmundo, Attariyan, and Gelly}]{houlsby2019peft}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019.
\newblock Parameter-efficient transfer learning for nlp.
\newblock In \emph{International Conference on Machine Learning}, pages 2790--2799. PMLR.

\bibitem[{Hu et~al.(2022)Hu, yelong shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen}]{hu2022lora}
Edward~J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen. 2022.
\newblock \href {https://openreview.net/forum?id=nZeVKeeFYf9} {Lo{RA}: Low-rank adaptation of large language models}.
\newblock In \emph{International Conference on Learning Representations}.

\bibitem[{Ji et~al.(2024)Ji, Liu, Zhang, Zhang, Zhao, Zhou, Zhang, Liu, and Zheng}]{ji2024advlora}
Yuheng Ji, Yue Liu, Zhicheng Zhang, Zhao Zhang, Yuting Zhao, Gang Zhou, Xingwei Zhang, Xinwang Liu, and Xiaolong Zheng. 2024.
\newblock \href {http://arxiv.org/abs/2404.13425} {Advlora: Adversarial low-rank adaptation of vision-language models}.

\bibitem[{Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, de~las Casas, Bressand, Lengyel, Lample, Saulnier, Lavaud, Lachaux, Stock, Scao, Lavril, Wang, Lacroix, and Sayed}]{jiang2023mistral}
Albert~Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio~Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven~Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William~El Sayed. 2023.
\newblock \href {http://arxiv.org/abs/2310.06825} {Mistral 7b}.

\bibitem[{Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand et~al.}]{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al. 2024.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}.

\bibitem[{Jin et~al.(2019)Jin, Dhingra, Liu, Cohen, and Lu}]{jin2019pubmedqa}
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019.
\newblock Pubmedqa: A dataset for biomedical research question answering.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)}, pages 2567--2577.

\bibitem[{Kingma and Ba(2014)}]{kingma2014adam}
Diederik~P Kingma and Jimmy Ba. 2014.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}.

\bibitem[{Kloek and Van~Dijk(1978)}]{kloek1978bayesian}
Teun Kloek and Herman~K Van~Dijk. 1978.
\newblock Bayesian estimates of equation system parameters: an application of integration by monte carlo.
\newblock \emph{Econometrica: Journal of the Econometric Society}, pages 1--19.

\bibitem[{Li et~al.(2023)Li, Yuan, Dai, Zhang, Wang, and Tang}]{li2023smartfrz}
Sheng Li, Geng Yuan, Yue Dai, Youtao Zhang, Yanzhi Wang, and Xulong Tang. 2023.
\newblock \href {https://openreview.net/forum?id=i9UlAr1T_xl} {Smart{FRZ}: An efficient training framework using attention-based layer freezing}.
\newblock In \emph{The Eleventh International Conference on Learning Representations}.

\bibitem[{Li and Liang(2021)}]{li2021prefix}
Xiang~Lisa Li and Percy Liang. 2021.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}.

\bibitem[{Lialin et~al.(2023)Lialin, Shivagunde, Muckatira, and Rumshisky}]{lialin2023relora}
Vladislav Lialin, Namrata Shivagunde, Sherin Muckatira, and Anna Rumshisky. 2023.
\newblock \href {http://arxiv.org/abs/2307.05695} {Relora: High-rank training through low-rank updates}.

\bibitem[{Liu et~al.(2023)Liu, Li, Hall, Liang, and Ma}]{liu2023sophia}
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. 2023.
\newblock Sophia: A scalable stochastic second-order optimizer for language model pre-training.
\newblock \emph{arXiv preprint arXiv:2305.14342}.

\bibitem[{Liu et~al.(2021{\natexlab{a}})Liu, Zheng, Du, Ding, Qian, Yang, and Tang}]{liu2021gpt}
Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. 2021{\natexlab{a}}.
\newblock Gpt understands, too.
\newblock \emph{arXiv preprint arXiv:2103.10385}.

\bibitem[{Liu et~al.(2021{\natexlab{b}})Liu, Agarwal, and Venkataraman}]{liu2021autofreeze}
Yuhan Liu, Saurabh Agarwal, and Shivaram Venkataraman. 2021{\natexlab{b}}.
\newblock \href {http://arxiv.org/abs/2102.01386} {Autofreeze: Automatically freezing model blocks to accelerate fine-tuning}.

\bibitem[{Loshchilov and Hutter(2017)}]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter. 2017.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}.

\bibitem[{Luo et~al.(2024)Luo, Yu, and Li}]{luo2024badam}
Qijun Luo, Hengxu Yu, and Xiao Li. 2024.
\newblock \href {http://arxiv.org/abs/2404.02827} {Badam: A memory efficient full parameter training method for large language models}.

\bibitem[{Luo et~al.(2023)Luo, Tan, Huang, Li, and Zhao}]{luo2023latent}
Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. 2023.
\newblock Latent consistency models: Synthesizing high-resolution images with few-step inference.
\newblock \emph{arXiv preprint arXiv:2310.04378}.

\bibitem[{Malladi et~al.(2023)Malladi, Gao, Nichani, Damian, Lee, Chen, and Arora}]{malladi2023finetuning}
Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason~D. Lee, Danqi Chen, and Sanjeev Arora. 2023.
\newblock \href {https://openreview.net/forum?id=Vota6rFhBQ} {Fine-tuning language models with just forward passes}.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}.

\bibitem[{Meng et~al.(2024)Meng, Wang, and Zhang}]{meng2024pissa}
Fanxu Meng, Zhaohui Wang, and Muhan Zhang. 2024.
\newblock \href {http://arxiv.org/abs/2404.02948} {Pissa: Principal singular values and singular vectors adaptation of large language models}.

\bibitem[{OpenAI et~al.(2023)OpenAI, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji, Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro, Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button, Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen, Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai, Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet, Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges, Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene, Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey, Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang, Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali, Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros, Knight, Kokotajlo, Łukasz Kondraciuk,
  Kondrich, Konstantinidis, Kosic, Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin, Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin, Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta, Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati, Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe, Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita, Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov, de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power, Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted, Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr, Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler, Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers, Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle, Turley, Tworek, Uribe, Vallone,
  Vijayvergiya, Voss, Wainwright, Wang, Wang, Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff, Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan, Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph}]{openai2023gpt4}
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mo~Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung~Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón~Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao,
  Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang~Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish~Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong~Wook Kim, Christina Kim, Yongjik Kim, Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak~Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe,
  Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott~Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de~Avila Belbute~Peres, Michael Petrov, Henrique~Ponde de~Oliveira~Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder,
  Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe~Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe~Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin~Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ~Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao
  Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2023.
\newblock \href {http://arxiv.org/abs/2303.08774} {Gpt-4 technical report}.

\bibitem[{Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, Schulman, Hilton, Kelton, Miller, Simens, Askell, Welinder, Christiano, Leike, and Lowe}]{ouyang2022instructgpt}
Long Ouyang, Jeffrey Wu, Xu~Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul~F Christiano, Jan Leike, and Ryan Lowe. 2022.
\newblock Training language models to follow instructions with human feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, volume~35, pages 27730--27744. Curran Associates, Inc.

\bibitem[{Paster et~al.(2023)Paster, Santos, Azerbayev, and Ba}]{paster2023openwebmath}
Keiran Paster, Marco~Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2023.
\newblock \href {http://arxiv.org/abs/2310.06786} {Openwebmath: An open dataset of high-quality mathematical web text}.

\bibitem[{Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan, Killeen, Lin, Gimelshein, Antiga et~al.}]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al. 2019.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32.

\bibitem[{Peng et~al.(2023)Peng, Li, He, Galley, and Gao}]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023.
\newblock Instruction tuning with gpt-4.
\newblock \emph{arXiv preprint arXiv:2304.03277}.

\bibitem[{Qin and Eisner(2021)}]{qin2021learning}
Guanghui Qin and Jason Eisner. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.410} {Learning how to ask: Querying {LM}s with mixtures of soft prompts}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5203--5212, Online. Association for Computational Linguistics.

\bibitem[{Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and Sutskever}]{radford2019gpt2}
Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019.
\newblock Language models are unsupervised multitask learners.

\bibitem[{Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena, Zhou, Li, and Liu}]{raffel2020t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J. Liu. 2020.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock \emph{J. Mach. Learn. Res.}, 21(1).

\bibitem[{Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and He}]{rajbhandari2020zero}
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--16. IEEE.

\bibitem[{Rasley et~al.(2020)Rasley, Rajbhandari, Ruwase, and He}]{rasley2020deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020.
\newblock Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 3505--3506.

\bibitem[{Reddi et~al.(2019)Reddi, Kale, and Kumar}]{reddi2019amsgrad}
Sashank~J Reddi, Satyen Kale, and Sanjiv Kumar. 2019.
\newblock On the convergence of adam and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}.

\bibitem[{Ren et~al.(2021)Ren, Rajbhandari, Aminabadi, Ruwase, Yang, Zhang, Li, and He}]{ren2021zerooffload}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. 2021.
\newblock \href {http://arxiv.org/abs/2101.06840} {Zero-offload: Democratizing billion-scale model training}.

\bibitem[{Rozière et~al.(2023)Rozière, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, Kozhevnikov, Evtimov, Bitton, Bhatt, Ferrer, Grattafiori, Xiong, Défossez, Copet, Azhar, Touvron, Martin, Usunier, Scialom, and Synnaeve}]{rozière2023code}
Baptiste Rozière, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian~Canton Ferrer, Aaron Grattafiori, Wenhan Xiong, Alexandre Défossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. 2023.
\newblock \href {http://arxiv.org/abs/2308.12950} {Code llama: Open foundation models for code}.

\bibitem[{Sakaguchi et~al.(2021)Sakaguchi, Bras, Bhagavatula, and Choi}]{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock \emph{Communications of the ACM}, 64(9):99--106.

\bibitem[{Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow, Castagn{\'e}, Luccioni, Yvon et~al.}]{workshop2022bloom}
Teven~Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili{\'c}, Daniel Hesslow, Roman Castagn{\'e}, Alexandra~Sasha Luccioni, Fran{\c{c}}ois Yvon, et~al. 2022.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{arXiv preprint arXiv:2211.05100}.

\bibitem[{Shum et~al.(2023)Shum, Diao, and Zhang}]{shum2023automatic}
Kashun Shum, Shizhe Diao, and Tong Zhang. 2023.
\newblock Automatic prompt augmentation and selection with chain-of-thought from labeled data.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pages 12113--12139.

\bibitem[{SONG et~al.(2024)SONG, Zhao, Majumder, and Lin}]{song2024increasing}
Haobo SONG, Hao Zhao, Soumajit Majumder, and Tao Lin. 2024.
\newblock \href {https://openreview.net/forum?id=H3IUunLy8s} {Increasing model capacity for free: A simple strategy for parameter efficient fine-tuning}.
\newblock In \emph{The Twelfth International Conference on Learning Representations}.

\bibitem[{Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and Salakhutdinov}]{JMLR:v15:srivastava14a:dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. 2014.
\newblock \href {http://jmlr.org/papers/v15/srivastava14a.html} {Dropout: A simple way to prevent neural networks from overfitting}.
\newblock \emph{Journal of Machine Learning Research}, 15(56):1929--1958.

\bibitem[{Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto}]{alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto. 2023.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}.

\bibitem[{Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar et~al.}]{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al. 2023{\natexlab{a}}.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}.

\bibitem[{Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale et~al.}]{touvron2023llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al. 2023{\natexlab{b}}.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}.

\bibitem[{Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin}]{Vaswani_Shazeer_Parmar_Uszkoreit_Jones_Gomez_Kaiser_Polosukhin_2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, AidanN. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017.
\newblock Attention is all you need.
\newblock \emph{Neural Information Processing Systems,Neural Information Processing Systems}.

\bibitem[{Wangni et~al.(2018)Wangni, Wang, Liu, and Zhang}]{wangni2018gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang. 2018.
\newblock Gradient sparsification for communication-efficient distributed optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 31.

\bibitem[{Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou et~al.}]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al. 2022.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:24824--24837.

\bibitem[{You et~al.(2017)You, Gitman, and Ginsburg}]{you2017lars}
Yang You, Igor Gitman, and Boris Ginsburg. 2017.
\newblock Large batch training of convolutional networks.
\newblock \emph{arXiv preprint arXiv:1708.03888}.

\bibitem[{You et~al.(2019)You, Li, Reddi, Hseu, Kumar, Bhojanapalli, Song, Demmel, Keutzer, and Hsieh}]{you2019lamb}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. 2019.
\newblock Large batch optimization for deep learning: Training bert in 76 minutes.
\newblock \emph{arXiv preprint arXiv:1904.00962}.

\bibitem[{Zhang et~al.(2024)Zhang, Zeng, Wang, and Lu}]{zhang2024tinyllama}
Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024.
\newblock \href {http://arxiv.org/abs/2401.02385} {Tinyllama: An open-source small language model}.

\bibitem[{Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et~al.}]{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al. 2022.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}.

\bibitem[{Zhao et~al.(2024)Zhao, Zhang, Chen, Wang, Anandkumar, and Tian}]{zhao2024galore}
Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. 2024.
\newblock Galore: Memory-efficient llm training by gradient low-rank projection.
\newblock \emph{arXiv preprint arXiv:2403.03507}.

\bibitem[{Zhao and Zhang(2015)}]{zhao2015stochastic}
Peilin Zhao and Tong Zhang. 2015.
\newblock Stochastic optimization with importance sampling for regularized loss minimization.
\newblock In \emph{international conference on machine learning}, pages 1--9. PMLR.

\bibitem[{Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica}]{zheng2023judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric.~P Xing, Hao Zhang, Joseph~E. Gonzalez, and Ion Stoica. 2023.
\newblock \href {http://arxiv.org/abs/2306.05685} {Judging llm-as-a-judge with mt-bench and chatbot arena}.

\bibitem[{Zhong et~al.(2023)Zhong, Cui, Guo, Liang, Lu, Wang, Saied, Chen, and Duan}]{zhong2023agieval}
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023.
\newblock \href {http://arxiv.org/abs/2304.06364} {Agieval: A human-centric benchmark for evaluating foundation models}.

\bibitem[{Zhong et~al.(2021)Zhong, Friedman, and Chen}]{zhong2021factual}
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021.
\newblock \href {https://doi.org/10.18653/v1/2021.naacl-main.398} {Factual probing is [{MASK}]: Learning vs. learning to recall}.
\newblock In \emph{Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies}, pages 5017--5033, Online. Association for Computational Linguistics.

\bibitem[{Zhou et~al.(2020)Zhou, Zhang, Zhu, Zheng, and Wu}]{zhou2020randomized}
Yangfan Zhou, Mingchuan Zhang, Junlong Zhu, Ruijuan Zheng, and Qingtao Wu. 2020.
\newblock A randomized block-coordinate adam online learning optimization algorithm.
\newblock \emph{Neural Computing and Applications}, 32(16):12671--12684.

\end{thebibliography}
