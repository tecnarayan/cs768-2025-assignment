\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
  Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Clark et~al.(2019)Clark, Lee, Chang, Kwiatkowski, Collins, and
  Toutanova]{clark2019boolq}
Clark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., and Toutanova,
  K.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock \emph{arXiv preprint arXiv:1905.10044}, 2019.

\bibitem[Desai \& Durrett(2020)Desai and Durrett]{desai2020calibration}
Desai, S. and Durrett, G.
\newblock Calibration of pre-trained transformers.
\newblock \emph{arXiv preprint arXiv:2003.07892}, 2020.

\bibitem[Fan et~al.(2018)Fan, Lewis, and Dauphin]{fan2018hierarchical}
Fan, A., Lewis, M., and Dauphin, Y.
\newblock Hierarchical neural story generation.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  889--898, 2018.

\bibitem[Gershman \& Goodman(2014)Gershman and Goodman]{gershman2014amortized}
Gershman, S. and Goodman, N.
\newblock Amortized inference in probabilistic reasoning.
\newblock In \emph{Proceedings of the annual meeting of the cognitive science
  society}, volume~36, 2014.

\bibitem[Gokaslan \& Cohen(2019)Gokaslan and Cohen]{Gokaslan2019OpenWeb}
Gokaslan, A. and Cohen, V.
\newblock Openwebtext corpus, 2019.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock In \emph{International conference on machine learning}. PMLR, 2017.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{ho2020denoising}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6840--6851, 2020.

\bibitem[Holtzman et~al.(2019)Holtzman, Buys, Du, Forbes, and
  Choi]{holtzman2019curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration.
\newblock \emph{arXiv preprint arXiv:1904.09751}, 2019.

\bibitem[Kingma \& Dhariwal(2018)Kingma and Dhariwal]{kingma2018glow}
Kingma, D.~P. and Dhariwal, P.
\newblock Glow: Generative flow with invertible 1x1 convolutions.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Koller \& Friedman(2009)Koller and Friedman]{koller2009probabilistic}
Koller, D. and Friedman, N.
\newblock \emph{Probabilistic graphical models: principles and techniques}.
\newblock MIT press, 2009.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images, 2009.

\bibitem[Kwiatkowski et~al.(2019)Kwiatkowski, Palomaki, Redfield, Collins,
  Parikh, Alberti, Epstein, Polosukhin, Devlin, Lee,
  et~al.]{kwiatkowski2019natural}
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M., Parikh, A., Alberti,
  C., Epstein, D., Polosukhin, I., Devlin, J., Lee, K., et~al.
\newblock Natural questions: a benchmark for question answering research.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  7:\penalty0 453--466, 2019.

\bibitem[Li et~al.(2016)Li, Monroe, and Jurafsky]{li2016simple}
Li, J., Monroe, W., and Jurafsky, D.
\newblock A simple, fast diverse decoding algorithm for neural generation.
\newblock \emph{arXiv preprint arXiv:1611.08562}, 2016.

\bibitem[Liang et~al.(2022)Liang, Bommasani, Lee, Tsipras, Soylu, Yasunaga,
  Zhang, Narayanan, Wu, et~al.]{liang2022holistic}
Liang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang,
  Y., Narayanan, D., Wu, Y., et~al.
\newblock Holistic evaluation of language models.
\newblock \emph{arXiv preprint arXiv:2211.09110}, 2022.

\bibitem[Lu et~al.(2022)Lu, Welleck, Jiang, Hessel, Qin, West, Ammanabrolu, and
  Choi]{lu2022quark}
Lu, X., Welleck, S., Jiang, L., Hessel, J., Qin, L., West, P., Ammanabrolu, P.,
  and Choi, Y.
\newblock Quark: Controllable text generation with reinforced unlearning.
\newblock \emph{arXiv preprint arXiv:2205.13636}, 2022.

\bibitem[Mahoney(2011)]{mahoney2011large}
Mahoney, M.
\newblock Large text compression benchmark, 2011.

\bibitem[Mikolov et~al.(2013)Mikolov, Chen, Corrado, and
  Dean]{mikolov2013efficient}
Mikolov, T., Chen, K., Corrado, G., and Dean, J.
\newblock Efficient estimation of word representations in vector space.
\newblock \emph{arXiv preprint arXiv:1301.3781}, 2013.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{narayan2018don}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Don't give me the details, just the summary! topic-aware
  convolutional neural networks for extreme summarization.
\newblock \emph{arXiv preprint arXiv:1808.08745}, 2018.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{nichol2021improved}
Nichol, A.~Q. and Dhariwal, P.
\newblock Improved denoising diffusion probabilistic models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8162--8171. PMLR, 2021.

\bibitem[Nixon et~al.(2019)Nixon, Dusenberry, Zhang, Jerfel, and
  Tran]{nixon2019measuring}
Nixon, J., Dusenberry, M.~W., Zhang, L., Jerfel, G., and Tran, D.
\newblock Measuring calibration in deep learning.
\newblock In \emph{CVPR Workshops}, volume~2, 2019.

\bibitem[Pillutla et~al.(2021)Pillutla, Swayamdipta, Zellers, Thickstun,
  Welleck, Choi, and Harchaoui]{pillutla2021mauve}
Pillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Welleck, S., Choi,
  Y., and Harchaoui, Z.
\newblock Mauve: Measuring the gap between neural text and human text using
  divergence frontiers.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 4816--4828, 2021.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, Sutskever,
  et~al.]{radford2019language}
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et~al.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI blog}, 1\penalty0 (8):\penalty0 9, 2019.

\bibitem[Vahdat \& Kautz(2020)Vahdat and Kautz]{vahdat2020nvae}
Vahdat, A. and Kautz, J.
\newblock Nvae: A deep hierarchical variational autoencoder.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 19667--19679, 2020.

\bibitem[Vijayakumar et~al.(2018)Vijayakumar, Cogswell, Selvaraju, Sun, Lee,
  Crandall, and Batra]{vijayakumar2018diverse}
Vijayakumar, A., Cogswell, M., Selvaraju, R., Sun, Q., Lee, S., Crandall, D.,
  and Batra, D.
\newblock Diverse beam search for improved description of complex scenes.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Welleck et~al.(2019)Welleck, Kulikov, Roller, Dinan, Cho, and
  Weston]{welleck2019neural}
Welleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K., and Weston, J.
\newblock Neural text generation with unlikelihood training.
\newblock \emph{arXiv preprint arXiv:1908.04319}, 2019.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P.,
  Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen,
  P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M.,
  Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, October 2020.

\end{thebibliography}
