@incollection{Bengio+chapter2007,
author = {Bengio, Yoshua and LeCun, Yann},
booktitle = {Large Scale Kernel Machines},
publisher = {MIT Press},
title = {Scaling Learning Algorithms Towards {AI}},
year = {2007}
}

@article{Hinton06,
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee Whye},
journal = {Neural Computation},
pages = {1527--1554},
title = {A Fast Learning Algorithm for Deep Belief Nets},
volume = {18},
year = {2006}
}

@book{goodfellow2016deep,
title={Deep learning},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
volume={1},
year={2016},
publisher={MIT Press}
}


@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.}
}





@InProceedings{seno2021d3rlpy,
  author = {Takuma Seno, Michita Imai},
  title = {d3rlpy: An Offline Deep Reinforcement Library},
  booktitle = {NeurIPS 2021 Offline Reinforcement Learning Workshop},
  month = {December},
  year = {2021}
}


@misc{baselines,
  author = {Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai and Zhokhov, Peter},
  title = {OpenAI Baselines},
  year = {2017},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/openai/baselines}},
}

@misc{stable-baselines,
  author = {Hill, Ashley and Raffin, Antonin and Ernestus, Maximilian and Gleave, Adam and Kanervisto, Anssi and Traore, Rene and Dhariwal, Prafulla and Hesse, Christopher and Klimov, Oleg and Nichol, Alex and Plappert, Matthias and Radford, Alec and Schulman, John and Sidor, Szymon and Wu, Yuhuai},
  title = {Stable Baselines},
  year = {2018},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/hill-a/stable-baselines}},
}

@article{castro2018dopamine,
  title={Dopamine: A research framework for deep reinforcement learning},
  author={Castro, Pablo Samuel and Moitra, Subhodeep and Gelada, Carles and Kumar, Saurabh and Bellemare, Marc G},
  journal={arXiv preprint arXiv:1812.06110},
  year={2018}
}

@article{gauci2018horizon,
  title={Horizon: Facebook's Open Source Applied Reinforcement Learning Platform},
  author={Gauci, Jason and Conti, Edoardo and Liang, Yitao and Virochsiri, Kittipat and Chen, Zhengxing and He, Yuchen and Kaden, Zachary and Narayanan, Vivek and Ye, Xiaohui},
  journal={arXiv preprint arXiv:1811.00260},
  year={2018}
}

@misc{kenggraesser2017slmlab,
    author = {Keng, Wah Loon and Graesser, Laura},
    title = {SLM Lab},
    year = {2017},
    publisher = {GitHub},
    journal = {GitHub repository},
    howpublished = {\url{https://github.com/kengz/SLM-Lab}},
}

@misc{garage,
 author = {The garage contributors},
 title = {Garage: A toolkit for reproducible reinforcement learning research},
 year = {2019},
 publisher = {GitHub},
 journal = {GitHub repository},
 howpublished = {\url{https://github.com/rlworkgroup/garage}},
 commit = {be070842071f736eb24f28e4b902a9f144f5c97b}
}

@inproceedings{10.5555/3045390.3045531,
author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
year = {2016},
publisher = {JMLR.org},
abstract = {Recently, researchers have made significant progress combining the advances in deep learning for learning feature representations with reinforcement learning. Some notable examples include training agents to play Atari games based on raw pixel data and to acquire advanced manipulation skills using raw sensory inputs. However, it has been difficult to quantify progress in the domain of continuous control due to the lack of a commonly adopted benchmark. In this work, we present a benchmark suite of continuous control tasks, including classic tasks like cart-pole swing-up, tasks with very high state and action dimensionality such as 3D humanoid locomotion, tasks with partial observations, and tasks with hierarchical structure. We report novel findings based on the systematic evaluation of a range of implemented reinforcement learning algorithms. Both the benchmark and reference implementations are released at https://github.com/rllab/rllab in order to facilitate experimental reproducibility and to encourage adoption by other researchers.},
booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
pages = {1329–1338},
numpages = {10},
location = {New York, NY, USA},
series = {ICML'16}
}

@misc{1903.00027,
  doi = {10.48550/ARXIV.1903.00027},
  url = {https://arxiv.org/abs/1903.00027},
  author = {Kolesnikov, Sergey and Hrinchuk, Oleksii},
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {Catalyst.RL: A Distributed Framework for Reproducible RL Research},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{JMLR:v22:20-376,
  author  = {Yasuhiro Fujita and Prabhat Nagarajan and Toshiki Kataoka and Takahiro Ishikawa},
  title   = {ChainerRL: A Deep Reinforcement Learning Library},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {77},
  pages   = {1-14},
  url     = {http://jmlr.org/papers/v22/20-376.html}
}

@inproceedings{liang2018rllib,
    author = {Eric Liang and
              Richard Liaw and
              Robert Nishihara and
              Philipp Moritz and
              Roy Fox and
              Ken Goldberg and
              Joseph E. Gonzalez and
              Michael I. Jordan and
              Ion Stoica},
    title = {{RLlib}: Abstractions for Distributed Reinforcement Learning},
    booktitle = {International Conference on Machine Learning ({ICML})},
    year = {2018}
}


@misc{erl,
  author = {Liu, Xiao-Yang and Li, Zechu and Wang, Zhaoran and Zheng, Jiahao},
  title = {{ElegantRL}: Massively Parallel Framework for Cloud-native Deep Reinforcement Learning},
  year = {2021},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/AI4Finance-Foundation/ElegantRL}},
}

@article{huang2021cleanrl,
  title={CleanRL: High-quality Single-file Implementations of Deep Reinforcement Learning Algorithms},
  author={Huang, Shengyi and Dossa, Rousslan Fernand Julien and Ye, Chang and Braga, Jeff},
  journal={arXiv preprint arXiv:2111.08819},
  year={2021}
}

@article{tianshou,
  title={Tianshou: A Highly Modularized Deep Reinforcement Learning Library},
  author={Weng, Jiayi and Chen, Huayu and Yan, Dong and You, Kaichao and Duburcq, Alexis and Zhang, Minghao and Su, Yi and Su, Hang and Zhu, Jun},
  journal={arXiv preprint arXiv:2107.14171},
  year={2021}
}

@misc{1909.01500,
  doi = {10.48550/ARXIV.1909.01500},
  url = {https://arxiv.org/abs/1909.01500},
  author = {Stooke, Adam and Abbeel, Pieter},
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {rlpyt: A Research Code Base for Deep Reinforcement Learning in PyTorch},
  publisher = {arXiv},
  year = {2019},
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@inproceedings{Engstrom2020ImplementationMI,
  title={Implementation Matters in Deep RL: A Case Study on PPO and TRPO},
  author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras and Firdaus Janoos and L. Rudolph and Aleksander Madry},
  booktitle={ICLR},
  year={2020}
}

@inproceedings{10.5555/3504035.3504427,
author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
title = {Deep Reinforcement Learning That Matters},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {392},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}

@article{kostrikov2021offline,
  title={Offline reinforcement learning with implicit q-learning},
  author={Kostrikov, Ilya and Nair, Ashvin and Levine, Sergey},
  journal={arXiv preprint arXiv:2110.06169},
  year={2021}
}

@article{fujimoto2021minimalist,
  title={A minimalist approach to offline reinforcement learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={20132--20145},
  year={2021}
}

@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{adamw,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{d4rl,
  title={D4rl: Datasets for deep data-driven reinforcement learning},
  author={Fu, Justin and Kumar, Aviral and Nachum, Ofir and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2004.07219},
  year={2020}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1179--1191},
  year={2020}
}

@article{nair2020accelerating,
  title={Accelerating online reinforcement learning with offline datasets},
  author={Nair, Ashvin and Dalal, Murtaza and Gupta, Abhishek and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.09359},
  year={2020}
}

@article{an2021uncertainty,
  title={Uncertainty-based offline reinforcement learning with diversified q-ensemble},
  author={An, Gaon and Moon, Seungyong and Kim, Jang-Hyun and Song, Hyun Oh},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={7436--7447},
  year={2021}
}

@article{chen2021decision,
  title={Decision transformer: Reinforcement learning via sequence modeling},
  author={Chen, Lili and Lu, Kevin and Rajeswaran, Aravind and Lee, Kimin and Grover, Aditya and Laskin, Misha and Abbeel, Pieter and Srinivas, Aravind and Mordatch, Igor},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={15084--15097},
  year={2021}
}
@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@misc{smithWalkParkLearning2022,
  title = {A {{Walk}} in the {{Park}}: {{Learning}} to {{Walk}} in 20 {{Minutes With Model-Free Reinforcement Learning}}},
  shorttitle = {A {{Walk}} in the {{Park}}},
  author = {Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  year = {2022},
  month = aug,
  number = {arXiv:2208.07860},
  eprint = {2208.07860},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Deep reinforcement learning is a promising approach to learning policies in uncontrolled environments that do not require domain knowledge. Unfortunately, due to sample inefficiency, deep RL applications have primarily focused on simulated environments. In this work, we demonstrate that the recent advancements in machine learning algorithms and libraries combined with a carefully tuned robot controller lead to learning quadruped locomotion in only 20 minutes in the real world. We evaluate our approach on several indoor and outdoor terrains which are known to be challenging for classical model-based controllers. We observe the robot to be able to learn walking gait consistently on all of these terrains. Finally, we evaluate our design decisions in a simulated environment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Robotics},
}



@inproceedings{
kumar2021a,
title={A Workflow for Offline Model-Free Robotic Reinforcement Learning},
author={Aviral Kumar and Anikait Singh and Stephen Tian and Chelsea Finn and Sergey Levine},
booktitle={5th Annual Conference on Robot Learning },
year={2021},
url={https://openreview.net/forum?id=fy4ZBWxYbIo}
}

@article{Diehl2021UMBRELLAUM,
  title={UMBRELLA: Uncertainty-Aware Model-Based Offline Reinforcement Learning Leveraging Planning},
  author={Christopher P. Diehl and Timo Sievernich and Martin Kr{\"u}ger and Frank Hoffmann and Torsten Bertram},
  journal={ArXiv},
  year={2021},
  volume={abs/2111.11097}
}

@article{Chen2022OffPolicyAF,
  title={Off-Policy Actor-critic for Recommender Systems},
  author={Minmin Chen and Can Xu and Vince Gatto and Devanshu Jain and Aviral Kumar and Ed H. Chi},
  journal={Proceedings of the 16th ACM Conference on Recommender Systems},
  year={2022}
}


@inproceedings{Saito2021OpenBD,
  title={Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation},
  author={Yuta Saito and Shunsuke Aihara and Megumi Matsutani and Yusuke Narita},
  booktitle={NeurIPS Datasets and Benchmarks},
  year={2021}
}


@misc{polixir_orl,
 author = {Polixir Technolgies},
 title = {OfflineRL},
 year = {2021},
 publisher = {GitHub},
 journal = {GitHub repository},
 howpublished = {\url{https://agit.ai/Polixir/OfflineRL}}
}

@InProceedings{pmlr-v162-kurenkov22a,
  title =   {Showing Your Offline Reinforcement Learning Work: Online Evaluation Budget Matters},
  author =       {Kurenkov, Vladislav and Kolesnikov, Sergey},
  booktitle =   {Proceedings of the 39th International Conference on Machine Learning},
  pages =   {11729--11752},
  year =   {2022},
  editor =   {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume =   {162},
  series =   {Proceedings of Machine Learning Research},
  month =   {17--23 Jul},
  publisher =    {PMLR},
  pdf =   {https://proceedings.mlr.press/v162/kurenkov22a/kurenkov22a.pdf},
  url =   {https://proceedings.mlr.press/v162/kurenkov22a.html},
  abstract =   {In this work, we argue for the importance of an online evaluation budget for a reliable comparison of deep offline RL algorithms. First, we delineate that the online evaluation budget is problem-dependent, where some problems allow for less but others for more. And second, we demonstrate that the preference between algorithms is budget-dependent across a diverse range of decision-making domains such as Robotics, Finance, and Energy Management. Following the points above, we suggest reporting the performance of deep offline RL algorithms under varying online evaluation budgets. To facilitate this, we propose to use a reporting tool from the NLP field, Expected Validation Performance. This technique makes it possible to reliably estimate expected maximum performance under different budgets while not requiring any additional computation beyond hyperparameter search. By employing this tool, we also show that Behavioral Cloning is often more favorable to offline RL algorithms when working within a limited budget.}
}

@inproceedings{hambrodungeons,
  title={Dungeons and Data: A Large-Scale NetHack Dataset},
  author={Hambro, Eric and Raileanu, Roberta and Rothermel, Danielle and Mella, Vegard and Rockt{\"a}schel, Tim and Kuttler, Heinrich and Murray, Naila},
  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year={2022}
}

@article{kuttler2020nethack,
  title={The nethack learning environment},
  author={K{\"u}ttler, Heinrich and Nardelli, Nantas and Miller, Alexander and Raileanu, Roberta and Selvatici, Marco and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={7671--7684},
  year={2020}
}

@article{piterbarg2023nethack,
  title={NetHack is Hard to Hack},
  author={Piterbarg, Ulyana and Pinto, Lerrel and Fergus, Rob},
  journal={arXiv preprint arXiv:2305.19240},
  year={2023}
}

@article{agarwal2021deep,
  title={Deep reinforcement learning at the edge of the statistical precipice},
  author={Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C and Bellemare, Marc},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={29304--29320},
  year={2021}
}

@inproceedings{hambro2022insights,
  title={Insights from the neurips 2021 nethack challenge},
  author={Hambro, Eric and Mohanty, Sharada and Babaev, Dmitrii and Byeon, Minwoo and Chakraborty, Dipam and Grefenstette, Edward and Jiang, Minqi and Daejin, Jo and Kanervisto, Anssi and Kim, Jongmin and others},
  booktitle={NeurIPS 2021 Competitions and Demonstrations Track},
  pages={41--52},
  year={2022},
  organization={PMLR}
}

@misc{wandb,
title = {Experiment Tracking with Weights and Biases},
year = {2020},
note = {Software available from wandb.com},
url={https://www.wandb.com/},
author = {Biewald, Lukas},
}

@inproceedings{
tarasov2022corl,
  title={{CORL}: Research-oriented Deep Offline Reinforcement Learning Library},
  author={Denis Tarasov and Alexander Nikulin and Dmitry Akimov and Vladislav Kurenkov and Sergey Kolesnikov},
  booktitle={3rd Offline RL Workshop: Offline RL as a ''Launchpad''},
  year={2022},
  url={https://openreview.net/forum?id=SyAS49bBcv}
}

@inproceedings{lhoest2021datasets,
  title={Datasets: A Community Library for Natural Language Processing},
  author={Lhoest, Quentin and del Moral, Albert Villanova and Jernite, Yacine and Thakur, Abhishek and von Platen, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and others},
  booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={175--184},
  year={2021}
}

@article{vinyals2019grandmaster,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={Nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group UK London}
}

@inproceedings{guss2019minerl,
  title={MineRL: a large-scale dataset of minecraft demonstrations},
  author={Guss, William H and Houghton, Brandon and Topin, Nicholay and Wang, Phillip and Codel, Cayden and Veloso, Manuela and Salakhutdinov, Ruslan},
  booktitle={Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  pages={2442--2448},
  year={2019}
}

@inproceedings{agarwal2020optimistic,
  title={An optimistic perspective on offline reinforcement learning},
  author={Agarwal, Rishabh and Schuurmans, Dale and Norouzi, Mohammad},
  booktitle={International Conference on Machine Learning},
  pages={104--114},
  year={2020},
  organization={PMLR}
}

@article{moolib2022,
  title  = {{moolib:  A Platform for Distributed RL}},
  author = {Vegard Mella and Eric Hambro and Danielle Rothermel and Heinrich K{\"{u}}ttler},
  year   = {2022},
  url    = {https://github.com/facebookresearch/moolib},
}

@article{Kumar2022OfflineQO,
  title={Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes},
  author={Aviral Kumar and Rishabh Agarwal and Xinyang Geng and G. Tucker and Sergey Levine},
  journal={ArXiv},
  year={2022},
  volume={abs/2211.15144}
}

@inproceedings{ghosh2022offline,
  title={Offline rl policies should be trained to be adaptive},
  author={Ghosh, Dibya and Ajay, Anurag and Agrawal, Pulkit and Levine, Sergey},
  booktitle={International Conference on Machine Learning},
  pages={7513--7530},
  year={2022},
  organization={PMLR}
}

@misc{orvieto2023resurrecting,
      title={Resurrecting Recurrent Neural Networks for Long Sequences}, 
      author={Antonio Orvieto and Samuel L Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
      year={2023},
      eprint={2303.06349},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{arulkumaran2017brief,
  title={A brief survey of deep reinforcement learning},
  author={Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  journal={arXiv preprint arXiv:1708.05866},
  year={2017}
}

@inproceedings{chen2020toward,
  title={Toward a thousand lights: Decentralized deep reinforcement learning for large-scale traffic signal control},
  author={Chen, Chacha and Wei, Hua and Xu, Nan and Zheng, Guanjie and Yang, Ming and Xiong, Yuanhao and Xu, Kai and Li, Zhenhui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={04},
  pages={3414--3421},
  year={2020}
}

@article{yu2019deep,
  title={Deep reinforcement learning for smart home energy management},
  author={Yu, Liang and Xie, Weiwei and Xie, Di and Zou, Yulong and Zhang, Dengyin and Sun, Zhixin and Zhang, Linghua and Zhang, Yue and Jiang, Tao},
  journal={IEEE Internet of Things Journal},
  volume={7},
  number={4},
  pages={2751--2762},
  year={2019},
  publisher={IEEE}
}

@article{mazyavkina2021reinforcement,
  title={Reinforcement learning for combinatorial optimization: A survey},
  author={Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
  journal={Computers \& Operations Research},
  volume={134},
  pages={105400},
  year={2021},
  publisher={Elsevier}
}

@article{michie1990cognitive,
  title={Cognitive models from subcognitive skills},
  author={Michie, Donald and Bain, Michael and Hayes-Miches, J},
  journal={IEE control engineering series},
  volume={44},
  pages={71--99},
  year={1990}
}

@article{nikulin2023anti,
  title={Anti-Exploration by Random Network Distillation},
  author={Nikulin, Alexander and Kurenkov, Vladislav and Tarasov, Denis and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2301.13616},
  year={2023}
}

@article{tarasov2023revisiting,
  title={Revisiting the Minimalist Approach to Offline Reinforcement Learning},
  author={Denis Tarasov and Vladislav Kurenkov and Alexander Nikulin and Sergey Kolesnikov},
  journal={arXiv preprint arXiv:2305.09836},
  year={2023}
}

@article{chen2022latent,
  title={Latent-variable advantage-weighted policy optimization for offline rl},
  author={Chen, Xi and Ghadirzadeh, Ali and Yu, Tianhe and Gao, Yuan and Wang, Jianhao and Li, Wenzhe and Liang, Bin and Finn, Chelsea and Zhang, Chongjie},
  journal={arXiv preprint arXiv:2203.08949},
  year={2022}
}

@inproceedings{PLAS_corl2020,
 title={PLAS: Latent Action Space for Offline Reinforcement Learning},
 author={Zhou, Wenxuan and Bajracharya, Sujay and Held, David},
 booktitle={Conference on Robot Learning},
 year={2020}
}

@article{akimov2022let,
  title={Let Offline RL Flow: Training Conservative Agents in the Latent Space of Normalizing Flows},
  author={Akimov, Dmitriy and Kurenkov, Vladislav and Nikulin, Alexander and Tarasov, Denis and Kolesnikov, Sergey},
  journal={arXiv preprint arXiv:2211.11096},
  year={2022}
}

@misc{lu2022challenges,
      title={Challenges and Opportunities in Offline Reinforcement Learning from Visual Observations}, 
      author={Cong Lu and Philip J. Ball and Tim G. J. Rudner and Jack Parker-Holder and Michael A. Osborne and Yee Whye Teh},
      year={2022},
      eprint={2206.04779},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{qin2022neorl,
  title={NeoRL: A near real-world benchmark for offline reinforcement learning},
  author={Qin, Rong-Jun and Zhang, Xingyuan and Gao, Songyi and Chen, Xiong-Hui and Li, Zewen and Zhang, Weinan and Yu, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24753--24765},
  year={2022}
}

@article{baker2022video,
  title={Video pretraining (vpt): Learning to act by watching unlabeled online videos},
  author={Baker, Bowen and Akkaya, Ilge and Zhokov, Peter and Huizinga, Joost and Tang, Jie and Ecoffet, Adrien and Houghton, Brandon and Sampedro, Raul and Clune, Jeff},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24639--24654},
  year={2022}
}

@article{hafner2023mastering,
  title={Mastering Diverse Domains through World Models},
  author={Hafner, Danijar and Pasukonis, Jurgis and Ba, Jimmy and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:2301.04104},
  year={2023}
}

@article{kumar2022offline,
  title={Offline Q-Learning on Diverse Multi-Task Data Both Scales And Generalizes},
  author={Kumar, Aviral and Agarwal, Rishabh and Geng, Xinyang and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2211.15144},
  year={2022}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{lstm,
author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
year = {1997},
month = {12},
pages = {1735-80},
title = {Long Short-term Memory},
volume = {9},
journal = {Neural computation},
doi = {10.1162/neco.1997.9.8.1735}
}