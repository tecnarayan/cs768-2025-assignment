\begin{thebibliography}{}

\bibitem[Advani and Saxe, 2017]{advani2017high}
Advani, M.~S. and Saxe, A.~M. (2017).
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em arXiv preprint arXiv:1710.03667}.

\bibitem[Advani et~al., 2020]{advani2020high}
Advani, M.~S., Saxe, A.~M., and Sompolinsky, H. (2020).
\newblock High-dimensional dynamics of generalization error in neural networks.
\newblock {\em Neural Networks}.

\bibitem[Ahuja et~al., 2020a]{ahuja2020linear}
Ahuja, K., Shanmugam, K., and Dhurandhar, A. (2020a).
\newblock Linear regression games: Convergence guarantees to approximate
  out-of-distribution solutions.
\newblock {\em arXiv preprint arXiv:2010.15234}.

\bibitem[Ahuja et~al., 2020b]{ahuja2020invariant}
Ahuja, K., Shanmugam, K., Varshney, K., and Dhurandhar, A. (2020b).
\newblock Invariant risk minimization games.
\newblock {\em arXiv preprint arXiv:2002.04692}.

\bibitem[Akhtar and Mian, 2018]{akhtar2018threat}
Akhtar, N. and Mian, A. (2018).
\newblock Threat of adversarial attacks on deep learning in computer vision: A
  survey.
\newblock {\em IEEE Access}, 6:14410--14430.

\bibitem[Albert and Anderson, 1984]{albert1984existence}
Albert, A. and Anderson, J.~A. (1984).
\newblock On the existence of maximum likelihood estimates in logistic
  regression models.
\newblock {\em Biometrika}, 71(1):1--10.

\bibitem[Allen-Zhu et~al., 2019a]{allen2019learning}
Allen-Zhu, Z., Li, Y., and Liang, Y. (2019a).
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In {\em Advances in neural information processing systems}, pages
  6158--6169.

\bibitem[Allen-Zhu et~al., 2019b]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z. (2019b).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em International Conference on Machine Learning}, pages
  242--252. PMLR.

\bibitem[Arjovsky et~al., 2019]{arjovsky2019invariant}
Arjovsky, M., Bottou, L., Gulrajani, I., and Lopez-Paz, D. (2019).
\newblock Invariant risk minimization.
\newblock {\em arXiv preprint arXiv:1907.02893}.

\bibitem[Arora et~al., 2019a]{arora2019implicit}
Arora, S., Cohen, N., Hu, W., and Luo, Y. (2019a).
\newblock Implicit regularization in deep matrix factorization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7413--7424.

\bibitem[Arora et~al., 2019b]{arora2019exact}
Arora, S., Du, S.~S., Hu, W., Li, Z., Salakhutdinov, R.~R., and Wang, R.
  (2019b).
\newblock On exact computation with an infinitely wide neural net.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8141--8150.

\bibitem[Arora et~al., 2019c]{arora2019fine}
Arora, S., Du, S.~S., Hu, W., Li, Z., and Wang, R. (2019c).
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em arXiv preprint arXiv:1901.08584}.

\bibitem[Arpit et~al., 2017]{arpit2017closer}
Arpit, D., Jastrz{\k{e}}bski, S., Ballas, N., Krueger, D., Bengio, E., Kanwal,
  M.~S., Maharaj, T., Fischer, A., Courville, A., Bengio, Y., et~al. (2017).
\newblock A closer look at memorization in deep networks.
\newblock {\em arXiv preprint arXiv:1706.05394}.

\bibitem[Baker et~al., 2018]{baker2018deep}
Baker, N., Lu, H., Erlikhman, G., and Kellman, P.~J. (2018).
\newblock Deep convolutional networks do not classify based on global object
  shape.
\newblock {\em PLoS computational biology}, 14(12):e1006613.

\bibitem[Baratin et~al., 2020]{baratin2020implicit}
Baratin, A., George, T., Laurent, C., Hjelm, R.~D., Lajoie, G., Vincent, P.,
  and Lacoste-Julien, S. (2020).
\newblock Implicit regularization in deep learning: A view from function space.
\newblock {\em arXiv preprint arXiv:2008.00938}.

\bibitem[Beery et~al., 2018]{beery2018recognition}
Beery, S., Van~Horn, G., and Perona, P. (2018).
\newblock Recognition in terra incognita.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 456--473.

\bibitem[Belinkov and Bisk, 2017]{belinkov2017synthetic}
Belinkov, Y. and Bisk, Y. (2017).
\newblock Synthetic and natural noise both break neural machine translation.
\newblock {\em arXiv preprint arXiv:1711.02173}.

\bibitem[Bietti and Mairal, 2019]{bietti2019inductive}
Bietti, A. and Mairal, J. (2019).
\newblock On the inductive bias of neural tangent kernels.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  12893--12904.

\bibitem[Brendel and Bethge, 2019]{brendel2019approximating}
Brendel, W. and Bethge, M. (2019).
\newblock Approximating cnns with bag-of-local-features models works
  surprisingly well on imagenet.
\newblock {\em arXiv preprint arXiv:1904.00760}.

\bibitem[Brutzkus et~al., 2017]{brutzkus2017sgd}
Brutzkus, A., Globerson, A., Malach, E., and Shalev-Shwartz, S. (2017).
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock {\em arXiv preprint arXiv:1710.10174}.

\bibitem[Burges and Crisp, 2000]{burges2000uniqueness}
Burges, C.~J. and Crisp, D.~J. (2000).
\newblock Uniqueness of the svm solution.
\newblock {\em Advances in neural information processing systems}, 12:223--229.

\bibitem[Cao et~al., 2019]{cao2019towards}
Cao, Y., Fang, Z., Wu, Y., Zhou, D.-X., and Gu, Q. (2019).
\newblock Towards understanding the spectral bias of deep learning.
\newblock {\em arXiv preprint arXiv:1912.01198}.

\bibitem[Cao and Gu, 2019]{cao2019generalization}
Cao, Y. and Gu, Q. (2019).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10836--10846.

\bibitem[Chen et~al., 2020]{chen2020generalized}
Chen, Z., Cao, Y., Gu, Q., and Zhang, T. (2020).
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock {\em arXiv preprint arXiv:2002.04026}.

\bibitem[Chizat and Bach, 2018]{chizat2018note}
Chizat, L. and Bach, F. (2018).
\newblock A note on lazy training in supervised differentiable programming.
\newblock {\em arXiv preprint arXiv:1812.07956}, 1.

\bibitem[Combes et~al., 2018]{combes2018learning}
Combes, R. T.~d., Pezeshki, M., Shabanian, S., Courville, A., and Bengio, Y.
  (2018).
\newblock On the learning dynamics of deep neural networks.
\newblock {\em arXiv preprint arXiv:1809.06848}.

\bibitem[Cover, 1965]{cover1965geometrical}
Cover, T.~M. (1965).
\newblock Geometrical and statistical properties of systems of linear
  inequalities with applications in pattern recognition.
\newblock {\em IEEE transactions on electronic computers}, 0(3):326--334.

\bibitem[Du et~al., 2018a]{du2018algorithmic}
Du, S.~S., Hu, W., and Lee, J.~D. (2018a).
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  384--395.

\bibitem[Du et~al., 2018b]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A. (2018b).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv preprint arXiv:1810.02054}.

\bibitem[Geirhos et~al., 2020]{geirhos2020shortcut}
Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge,
  M., and Wichmann, F.~A. (2020).
\newblock Shortcut learning in deep neural networks.
\newblock {\em arXiv preprint arXiv:2004.07780}.

\bibitem[George, 2020]{george2020nngeometry}
George, T. (2020).
\newblock Nngeometry: Easy and fast fisher information matrices and neural
  tangent kernels in pytorch.
\newblock {\em 0}.

\bibitem[Gidel et~al., 2019]{gidel2019implicit}
Gidel, G., Bach, F., and Lacoste-Julien, S. (2019).
\newblock Implicit regularization of discrete gradient dynamics in linear
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3202--3211.

\bibitem[Goldt et~al., 2019]{goldt2019dynamics}
Goldt, S., Advani, M., Saxe, A.~M., Krzakala, F., and Zdeborov{\'a}, L. (2019).
\newblock Dynamics of stochastic gradient descent for two-layer neural networks
  in the teacher-student setup.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6981--6991.

\bibitem[Goodfellow et~al., 2014]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C. (2014).
\newblock Explaining and harnessing adversarial examples.
\newblock {\em arXiv preprint arXiv:1412.6572}.

\bibitem[Gunasekar et~al., 2018]{gunasekar2018implicit}
Gunasekar, S., Lee, J.~D., Soudry, D., and Srebro, N. (2018).
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9461--9471.

\bibitem[Gunasekar et~al., 2017]{gunasekar2017implicit}
Gunasekar, S., Woodworth, B.~E., Bhojanapalli, S., Neyshabur, B., and Srebro,
  N. (2017).
\newblock Implicit regularization in matrix factorization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6151--6159.

\bibitem[Gururangan et~al., 2018]{gururangan2018annotation}
Gururangan, S., Swayamdipta, S., Levy, O., Schwartz, R., Bowman, S.~R., and
  Smith, N.~A. (2018).
\newblock Annotation artifacts in natural language inference data.
\newblock {\em arXiv preprint arXiv:1803.02324}.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[Heinze-Deml and Meinshausen, 2017]{heinze2017conditional}
Heinze-Deml, C. and Meinshausen, N. (2017).
\newblock Conditional variance penalties and domain shift robustness.
\newblock {\em arXiv preprint arXiv:1710.11469}.

\bibitem[Hendrycks and Dietterich, 2019]{hendrycks2019benchmarking}
Hendrycks, D. and Dietterich, T. (2019).
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock {\em arXiv preprint arXiv:1903.12261}.

\bibitem[Hendrycks and Gimpel, 2016]{hendrycks2016baseline}
Hendrycks, D. and Gimpel, K. (2016).
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock {\em arXiv preprint arXiv:1610.02136}.

\bibitem[Hermann and Lampinen, 2020]{hermann2020shapes}
Hermann, K.~L. and Lampinen, A.~K. (2020).
\newblock What shapes feature representations? exploring datasets,
  architectures, and training.
\newblock {\em arXiv preprint arXiv:2006.12433}.

\bibitem[Hsieh et~al., 2008]{hsieh2008dual}
Hsieh, C.-J., Chang, K.-W., Lin, C.-J., Keerthi, S.~S., and Sundararajan, S.
  (2008).
\newblock A dual coordinate descent method for large-scale linear svm.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 408--415.

\bibitem[Huang and Yau, 2019]{huang2019dynamics}
Huang, J. and Yau, H.-T. (2019).
\newblock Dynamics of deep neural networks and neural tangent hierarchy.
\newblock {\em arXiv preprint arXiv:1909.08156}.

\bibitem[Huang et~al., 2020]{huang2020deep}
Huang, K., Wang, Y., Tao, M., and Zhao, T. (2020).
\newblock Why do deep residual networks generalize better than deep feedforward
  networks?---a neural tangent kernel perspective.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Hui and Belkin, 2020]{hui2020evaluation}
Hui, L. and Belkin, M. (2020).
\newblock Evaluation of neural architectures trained with square loss vs
  cross-entropy in classification tasks.
\newblock {\em arXiv preprint arXiv:2006.07322}.

\bibitem[Ilyas et~al., 2018]{ilyas2018black}
Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. (2018).
\newblock Black-box adversarial attacks with limited queries and information.
\newblock {\em arXiv preprint arXiv:1804.08598}.

\bibitem[Ilyas et~al., 2019]{ilyas2019adversarial}
Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A.
  (2019).
\newblock Adversarial examples are not bugs, they are features.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  125--136.

\bibitem[Ioffe and Szegedy, 2015]{ioffe2015batch}
Ioffe, S. and Szegedy, C. (2015).
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}.

\bibitem[Jaakkola and Haussler, 1999]{jaakkola1999probabilistic}
Jaakkola, T.~S. and Haussler, D. (1999).
\newblock Probabilistic kernel regression models.
\newblock In {\em AISTATS}.

\bibitem[Jacobsen et~al., 2018]{jacobsen2018excessive}
Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M. (2018).
\newblock Excessive invariance causes adversarial vulnerability.
\newblock {\em arXiv preprint arXiv:1811.00401}.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580.

\bibitem[Ji and Telgarsky, 2019]{ji2019implicit}
Ji, Z. and Telgarsky, M. (2019).
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In {\em Conference on Learning Theory}, pages 1772--1798.

\bibitem[Jo and Bengio, 2017]{jo2017measuring}
Jo, J. and Bengio, Y. (2017).
\newblock Measuring the tendency of cnns to learn surface statistical
  regularities.
\newblock {\em arXiv preprint arXiv:1711.11561}.

\bibitem[Jolicoeur-Martineau and Mitliagkas, 2019]{jolicoeur2019connections}
Jolicoeur-Martineau, A. and Mitliagkas, I. (2019).
\newblock Connections between support vector machines, wasserstein distance and
  gradient-penalty gans.
\newblock {\em arXiv preprint arXiv:1910.06922}.

\bibitem[Kingma and Ba, 2014]{kingma2014adam}
Kingma, D.~P. and Ba, J. (2014).
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}.

\bibitem[Krizhevsky et~al., 2009]{krizhevsky2009learning}
Krizhevsky, A., Hinton, G., et~al. (2009).
\newblock Learning multiple layers of features from tiny images.
\newblock {\em 0}.

\bibitem[Krogh and Hertz, 1992]{krogh1992simple}
Krogh, A. and Hertz, J.~A. (1992).
\newblock A simple weight decay can improve generalization.
\newblock In {\em Advances in neural information processing systems}, pages
  950--957.

\bibitem[Krueger et~al., 2020]{krueger2020out}
Krueger, D., Caballero, E., Jacobsen, J.-H., Zhang, A., Binas, J., Priol,
  R.~L., and Courville, A. (2020).
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock {\em arXiv preprint arXiv:2003.00688}.

\bibitem[Lampinen and Ganguli, 2018]{lampinen2018analytic}
Lampinen, A.~K. and Ganguli, S. (2018).
\newblock An analytic theory of generalization dynamics and transfer learning
  in deep linear networks.
\newblock {\em arXiv preprint arXiv:1809.10374}.

\bibitem[Lapuschkin et~al., 2019]{lapuschkin2019unmasking}
Lapuschkin, S., W{\"a}ldchen, S., Binder, A., Montavon, G., Samek, W., and
  M{\"u}ller, K.-R. (2019).
\newblock Unmasking clever hans predictors and assessing what machines really
  learn.
\newblock {\em Nature communications}, 10(1):1--8.

\bibitem[Lee et~al., 2019]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J. (2019).
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In {\em Advances in neural information processing systems}, pages
  8570--8581.

\bibitem[Lee et~al., 2018]{lee2018simple}
Lee, K., Lee, K., Lee, H., and Shin, J. (2018).
\newblock A simple unified framework for detecting out-of-distribution samples
  and adversarial attacks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7167--7177.

\bibitem[Liang et~al., 2017]{liang2017enhancing}
Liang, S., Li, Y., and Srikant, R. (2017).
\newblock Enhancing the reliability of out-of-distribution image detection in
  neural networks.
\newblock {\em arXiv preprint arXiv:1706.02690}.

\bibitem[Liu et~al., 2015]{liu2015faceattributes}
Liu, Z., Luo, P., Wang, X., and Tang, X. (2015).
\newblock Deep learning face attributes in the wild.
\newblock In {\em Proceedings of International Conference on Computer Vision
  (ICCV)}.

\bibitem[Ma et~al., 2018]{ma2018implicit}
Ma, C., Wang, K., Chi, Y., and Chen, Y. (2018).
\newblock Implicit regularization in nonconvex statistical estimation: Gradient
  descent converges linearly for phase retrieval and matrix completion.
\newblock In {\em International Conference on Machine Learning}, pages
  3345--3354. PMLR.

\bibitem[Madry et~al., 2017]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2017).
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock {\em arXiv preprint arXiv:1706.06083}.

\bibitem[McCoy et~al., 2019]{mccoy2019right}
McCoy, R.~T., Pavlick, E., and Linzen, T. (2019).
\newblock Right for the wrong reasons: Diagnosing syntactic heuristics in
  natural language inference.
\newblock {\em arXiv preprint arXiv:1902.01007}.

\bibitem[Mei and Montanari, 2019]{mei2019generalization}
Mei, S. and Montanari, A. (2019).
\newblock The generalization error of random features regression: Precise
  asymptotics and double descent curve.
\newblock {\em arXiv preprint arXiv:1908.05355}.

\bibitem[Nakkiran et~al., 2019]{nakkiran2019sgd}
Nakkiran, P., Kaplun, G., Kalimeris, D., Yang, T., Edelman, B.~L., Zhang, F.,
  and Barak, B. (2019).
\newblock Sgd on neural networks learns functions of increasing complexity.
\newblock {\em arXiv preprint arXiv:1905.11604}.

\bibitem[Nam et~al., 2020]{nam2020learning}
Nam, J., Cha, H., Ahn, S., Lee, J., and Shin, J. (2020).
\newblock Learning from failure: Training debiased classifier from biased
  classifier.
\newblock {\em arXiv preprint arXiv:2007.02561}.

\bibitem[Nar et~al., 2019]{nar2019cross}
Nar, K., Ocal, O., Sastry, S.~S., and Ramchandran, K. (2019).
\newblock Cross-entropy loss and low-rank features have responsibility for
  adversarial examples.
\newblock {\em arXiv preprint arXiv:1901.08360}.

\bibitem[Nar and Sastry, 2019]{nar2019persistency}
Nar, K. and Sastry, S.~S. (2019).
\newblock Persistency of excitation for robustness of neural networks.
\newblock {\em arXiv preprint arXiv:1911.01043}.

\bibitem[Neyshabur et~al., 2017]{neyshabur2017geometry}
Neyshabur, B., Tomioka, R., Salakhutdinov, R., and Srebro, N. (2017).
\newblock Geometry of optimization and implicit regularization in deep
  learning.
\newblock {\em arXiv preprint arXiv:1705.03071}.

\bibitem[Neyshabur et~al., 2014]{neyshabur2014search}
Neyshabur, B., Tomioka, R., and Srebro, N. (2014).
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock {\em arXiv preprint arXiv:1412.6614}.

\bibitem[Niven and Kao, 2019]{niven2019probing}
Niven, T. and Kao, H.-Y. (2019).
\newblock Probing neural network comprehension of natural language arguments.
\newblock {\em arXiv preprint arXiv:1907.07355}.

\bibitem[Oakden-Rayner et~al., 2020]{oakden2020hidden}
Oakden-Rayner, L., Dunnmon, J., Carneiro, G., and R{\'e}, C. (2020).
\newblock Hidden stratification causes clinically meaningful failures in
  machine learning for medical imaging.
\newblock In {\em Proceedings of the ACM Conference on Health, Inference, and
  Learning}, pages 151--159.

\bibitem[Oymak et~al., 2019]{oymak2019generalization}
Oymak, S., Fabian, Z., Li, M., and Soltanolkotabi, M. (2019).
\newblock Generalization guarantees for neural networks via harnessing the
  low-rank structure of the jacobian.
\newblock {\em arXiv preprint arXiv:1906.05392}.

\bibitem[Parascandolo et~al., 2020]{parascandolo2020learning}
Parascandolo, G., Neitz, A., Orvieto, A., Gresele, L., and Sch{\"o}lkopf, B.
  (2020).
\newblock Learning explanations that are hard to vary.
\newblock {\em arXiv preprint arXiv:2009.00329}.

\bibitem[Paszke et~al., 2017]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A. (2017).
\newblock Automatic differentiation in pytorch.
\newblock {\em openreview id=BJJsrmfCZ}.

\bibitem[Pfungst, 1911]{pfungst1911clever}
Pfungst, O. (1911).
\newblock {\em Clever Hans:(the horse of Mr. Von Osten.) a contribution to
  experimental animal and human psychology}.
\newblock Holt, Rinehart and Winston.

\bibitem[Poggio et~al., 2017]{poggio2017theory}
Poggio, T., Kawaguchi, K., Liao, Q., Miranda, B., Rosasco, L., Boix, X.,
  Hidary, J., and Mhaskar, H. (2017).
\newblock Theory of deep learning iii: explaining the non-overfitting puzzle.
\newblock {\em arXiv preprint arXiv:1801.00173}.

\bibitem[Rahaman et~al., 2019]{rahaman2019spectral}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A. (2019).
\newblock On the spectral bias of neural networks.
\newblock In {\em International Conference on Machine Learning}, pages
  5301--5310. PMLR.

\bibitem[Rauber et~al., 2017]{rauber2017foolbox}
Rauber, J., Brendel, W., and Bethge, M. (2017).
\newblock Foolbox: A python toolbox to benchmark the robustness of machine
  learning models.
\newblock {\em arXiv preprint arXiv:1707.04131}.

\bibitem[Ribeiro et~al., 2016]{ribeiro2016should}
Ribeiro, M.~T., Singh, S., and Guestrin, C. (2016).
\newblock " why should i trust you?" explaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1135--1144.

\bibitem[Roberts, 2021]{roberts_2021}
Roberts, M. (2021).
\newblock Machine learning for covid-19 diagnosis: Promising, but still too
  flawed.

\bibitem[Ronen et~al., 2019]{ronen2019convergence}
Ronen, B., Jacobs, D., Kasten, Y., and Kritchman, S. (2019).
\newblock The convergence rate of neural networks for learned functions of
  different frequencies.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4761--4771.

\bibitem[Rosenfeld et~al., 2018]{rosenfeld2018elephant}
Rosenfeld, A., Zemel, R., and Tsotsos, J.~K. (2018).
\newblock The elephant in the room.
\newblock {\em arXiv preprint arXiv:1808.03305}.

\bibitem[Sagawa et~al., 2019]{sagawa2019distributionally}
Sagawa, S., Koh, P.~W., Hashimoto, T.~B., and Liang, P. (2019).
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock {\em arXiv preprint arXiv:1911.08731}.

\bibitem[Saxe et~al., 2013]{saxe2013exact}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S. (2013).
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}.

\bibitem[Saxe et~al., 2019]{saxe2019mathematical}
Saxe, A.~M., McClelland, J.~L., and Ganguli, S. (2019).
\newblock A mathematical theory of semantic development in deep neural
  networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(23):11537--11546.

\bibitem[Shah et~al., 2020]{shah2020pitfalls}
Shah, H., Tamuly, K., Raghunathan, A., Jain, P., and Netrapalli, P. (2020).
\newblock The pitfalls of simplicity bias in neural networks.
\newblock {\em arXiv preprint arXiv:2006.07710}.

\bibitem[Shalev-Shwartz and Ben-David, 2014]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S. (2014).
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press.

\bibitem[Soudry et~al., 2018]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N. (2018).
\newblock The implicit bias of gradient descent on separable data.
\newblock {\em The Journal of Machine Learning Research}, 19(1):2822--2878.

\bibitem[Srivastava et~al., 2014]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R. (2014).
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958.

\bibitem[Szegedy et~al., 2013]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R. (2013).
\newblock Intriguing properties of neural networks.
\newblock {\em arXiv preprint arXiv:1312.6199}.

\bibitem[Valle-P{\'e}rez et~al., 2018]{valle2018deep}
Valle-P{\'e}rez, G., Camargo, C.~Q., and Louis, A.~A. (2018).
\newblock Deep learning generalizes because the parameter-function map is
  biased towards simple functions.
\newblock {\em arXiv preprint arXiv:1805.08522}.

\bibitem[Vapnik and Vapnik, 1998]{vapnik1998statistical}
Vapnik, V. and Vapnik, V. (1998).
\newblock Statistical learning theory wiley.
\newblock {\em New York}, 1:624.

\bibitem[Vempala and Wilmes, 2019]{vempala2019gradient}
Vempala, S. and Wilmes, J. (2019).
\newblock Gradient descent for one-hidden-layer neural networks: Polynomial
  convergence and sq lower bounds.
\newblock In {\em Conference on Learning Theory}, pages 3115--3117.

\bibitem[Wang et~al., 2019]{wang2019learning}
Wang, H., He, Z., Lipton, Z.~C., and Xing, E.~P. (2019).
\newblock Learning robust representations by projecting superficial statistics
  out.
\newblock {\em arXiv preprint arXiv:1903.06256}.

\bibitem[Wang et~al., 2021]{wang2021and}
Wang, S., Yu, X., and Perdikaris, P. (2021).
\newblock When and why pinns fail to train: A neural tangent kernel
  perspective.
\newblock {\em Journal of Computational Physics}, page 110768.

\bibitem[Wolpert, 1996]{wolpert1996lack}
Wolpert, D.~H. (1996).
\newblock The lack of a priori distinctions between learning algorithms.
\newblock {\em Neural computation}, 8(7):1341--1390.

\bibitem[Xu and Frank, 2004]{xu2004logistic}
Xu, X. and Frank, E. (2004).
\newblock Logistic regression and boosting for labeled bags of instances.
\newblock In {\em Pacific-Asia conference on knowledge discovery and data
  mining}, pages 272--281. Springer.

\bibitem[Xu et~al., 2019a]{xu2019frequency}
Xu, Z.-Q.~J., Zhang, Y., Luo, T., Xiao, Y., and Ma, Z. (2019a).
\newblock Frequency principle: Fourier analysis sheds light on deep neural
  networks.
\newblock {\em arXiv preprint arXiv:1901.06523}.

\bibitem[Xu et~al., 2019b]{xu2019training}
Xu, Z.-Q.~J., Zhang, Y., and Xiao, Y. (2019b).
\newblock Training behavior of deep neural network in frequency domain.
\newblock In {\em International Conference on Neural Information Processing},
  pages 264--274. Springer.

\bibitem[Yang and Salman, 2019]{yang2019fine}
Yang, G. and Salman, H. (2019).
\newblock A fine-grained spectral perspective on neural networks.
\newblock {\em arXiv preprint arXiv:1907.10599}.

\bibitem[Zech et~al., 2018]{zech2018confounding}
Zech, J.~R., Badgeley, M.~A., Liu, M., Costa, A.~B., Titano, J.~J., and
  Oermann, E.~K. (2018).
\newblock Confounding variables can degrade generalization performance of
  radiological deep learning models.
\newblock {\em arXiv preprint arXiv:1807.00431}.

\bibitem[Zhang et~al., 2016]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2016).
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint arXiv:1611.03530}.

\bibitem[Zhao et~al., 2017]{zhao2017men}
Zhao, J., Wang, T., Yatskar, M., Ordonez, V., and Chang, K.-W. (2017).
\newblock Men also like shopping: Reducing gender bias amplification using
  corpus-level constraints.
\newblock {\em arXiv preprint arXiv:1707.09457}.

\bibitem[Zou et~al., 2020]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q. (2020).
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109(3):467--492.

\end{thebibliography}
