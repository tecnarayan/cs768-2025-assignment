\begin{thebibliography}{65}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anderson et~al.(2018)Anderson, Wu, Teney, Bruce, Johnson,
  S{\"{u}}nderhauf, Reid, Gould, and van~den Hengel]{anderson2018vision}
Anderson, P., Wu, Q., Teney, D., Bruce, J., Johnson, M., S{\"{u}}nderhauf, N.,
  Reid, I.~D., Gould, S., and van~den Hengel, A.
\newblock Vision-and-language navigation: Interpreting visually-grounded
  navigation instructions in real environments.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Anthony \& Bartlett(2002)Anthony and Bartlett]{anthony2009neural}
Anthony, M. and Bartlett, P.~L.
\newblock \emph{Neural Network Learning - Theoretical Foundations}.
\newblock Cambridge University Press, 2002.

\bibitem[Baker et~al.(2020)Baker, Kanitscheider, Markov, Wu, Powell, McGrew,
  and Mordatch]{baker2019emergent}
Baker, B., Kanitscheider, I., Markov, T.~M., Wu, Y., Powell, G., McGrew, B.,
  and Mordatch, I.
\newblock Emergent tool use from multi-agent autocurricula.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Bard et~al.(2020)Bard, Foerster, Chandar, Burch, Lanctot, Song,
  Parisotto, Dumoulin, Moitra, Hughes, et~al.]{bard2020hanabi}
Bard, N., Foerster, J.~N., Chandar, S., Burch, N., Lanctot, M., Song, H.~F.,
  Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., et~al.
\newblock The hanabi challenge: A new frontier for ai research.
\newblock \emph{Artificial Intelligence}, 280:\penalty0 103216, 2020.

\bibitem[Ba{\c{s}}ar \& Olsder(1998)Ba{\c{s}}ar and Olsder]{bacsar1998dynamic}
Ba{\c{s}}ar, T. and Olsder, G.~J.
\newblock \emph{Dynamic noncooperative game theory}.
\newblock SIAM, 1998.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., D{\k{e}}biak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Brown \& Sandholm(2019)Brown and Sandholm]{brown2019superhuman}
Brown, N. and Sandholm, T.
\newblock Superhuman ai for multiplayer poker.
\newblock \emph{Science}, 365\penalty0 (6456):\penalty0 885--890, 2019.

\bibitem[Christianos et~al.(2021)Christianos, Papoudakis, Rahman, and
  Albrecht]{christianos2021scaling}
Christianos, F., Papoudakis, G., Rahman, M.~A., and Albrecht, S.~V.
\newblock Scaling multi-agent reinforcement learning with selective parameter
  sharing.
\newblock In \emph{{ICML}}, 2021.

\bibitem[Ernst et~al.(2005)Ernst, Geurts, and Wehenkel]{ernst05a}
Ernst, D., Geurts, P., and Wehenkel, L.
\newblock Tree-based batch mode reinforcement learning.
\newblock \emph{J. Mach. Learn. Res.}, 6:\penalty0 503--556, 2005.

\bibitem[Fan et~al.(2020)Fan, Wang, Xie, and Yang]{fan2020theoretical}
Fan, J., Wang, Z., Xie, Y., and Yang, Z.
\newblock A theoretical analysis of deep q-learning.
\newblock In \emph{{L4DC}}, 2020.

\bibitem[Farahmand et~al.(2010)Farahmand, Munos, and
  Szepesv{\'{a}}ri]{farahmand2010error}
Farahmand, A.~M., Munos, R., and Szepesv{\'{a}}ri, C.
\newblock Error propagation for approximate policy and value iteration.
\newblock In \emph{{NIPS}}, 2010.

\bibitem[Farahmand et~al.(2016)Farahmand, Ghavamzadeh, Szepesv{\'a}ri, and
  Mannor]{farahmand2016regularized}
Farahmand, A.-m., Ghavamzadeh, M., Szepesv{\'a}ri, C., and Mannor, S.
\newblock Regularized policy iteration with nonparametric function spaces.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 4809--4874, 2016.

\bibitem[Foerster et~al.(2018)Foerster, Farquhar, Afouras, Nardelli, and
  Whiteson]{foerster2017counterfactual}
Foerster, J.~N., Farquhar, G., Afouras, T., Nardelli, N., and Whiteson, S.
\newblock Counterfactual multi-agent policy gradients.
\newblock In \emph{{AAAI}}, 2018.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{{ICML}}, 2018.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{{AAAI}}, 2018.

\bibitem[Hostallero et~al.(2019)Hostallero, Son, Kim, and
  Qtran]{hostallero2019learning}
Hostallero, W. J. K. D.~E., Son, K., Kim, D., and Qtran, Y.~Y.
\newblock Learning to factorize with transformation for cooperative multi-agent
  reinforcement learning.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Hu \& Wellman(2003)Hu and Wellman]{hu2003nash}
Hu, J. and Wellman, M.~P.
\newblock Nash q-learning for general-sum stochastic games.
\newblock \emph{Journal of machine learning research}, 4\penalty0
  (Nov):\penalty0 1039--1069, 2003.

\bibitem[Hu et~al.(2021)Hu, Zhu, Chang, and Liang]{HuZCL21}
Hu, S., Zhu, F., Chang, X., and Liang, X.
\newblock Updet: Universal multi-agent {RL} via policy decoupling with
  transformers.
\newblock In \emph{ICLR}, 2021.

\bibitem[Jiang \& Lu(2018)Jiang and Lu]{jiang2018learning}
Jiang, J. and Lu, Z.
\newblock Learning attentional communication for multi-agent cooperation.
\newblock In \emph{{NeurIPS}}, 2018.

\bibitem[Kim et~al.(2019)Kim, Moon, Hostallero, Kang, Lee, Son, and
  Yi]{kim2019learning}
Kim, D., Moon, S., Hostallero, D., Kang, W.~J., Lee, T., Son, K., and Yi, Y.
\newblock Learning to schedule communication in multi-agent reinforcement
  learning.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Kulkarni et~al.(2016)Kulkarni, Narasimhan, Saeedi, and
  Tenenbaum]{kulkarni2016hierarchical}
Kulkarni, T.~D., Narasimhan, K., Saeedi, A., and Tenenbaum, J.
\newblock Hierarchical deep reinforcement learning: Integrating temporal
  abstraction and intrinsic motivation.
\newblock \emph{{NIPS}}, 2016.

\bibitem[Kurach et~al.(2020)Kurach, Raichuk, Stanczyk, Zajac, Bachem, Espeholt,
  Riquelme, Vincent, Michalski, Bousquet, and Gelly]{kurach2019google}
Kurach, K., Raichuk, A., Stanczyk, P., Zajac, M., Bachem, O., Espeholt, L.,
  Riquelme, C., Vincent, D., Michalski, M., Bousquet, O., and Gelly, S.
\newblock Google research football: {A} novel reinforcement learning
  environment.
\newblock In \emph{{AAAI}}, 2020.

\bibitem[Lazaric et~al.(2016)Lazaric, Ghavamzadeh, and
  Munos]{lazaric2016analysis}
Lazaric, A., Ghavamzadeh, M., and Munos, R.
\newblock Analysis of classification-based policy iteration algorithms.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0 1--30,
  2016.

\bibitem[Lazaridou \& Baroni(2020)Lazaridou and Baroni]{lazaridou2020emergent}
Lazaridou, A. and Baroni, M.
\newblock Emergent multi-agent communication in the deep learning era.
\newblock \emph{arXiv preprint arXiv:2006.02419}, 2020.

\bibitem[Le et~al.(2017)Le, Yue, Carr, and Lucey]{le2017coordinated}
Le, H.~M., Yue, Y., Carr, P., and Lucey, P.
\newblock Coordinated multi-agent imitation learning.
\newblock In \emph{{ICML}}, 2017.

\bibitem[Li et~al.(2020)Li, Koyamada, Ye, Liu, Wang, Yang, Zhao, Qin, Liu, and
  Hon]{li2020suphx}
Li, J., Koyamada, S., Ye, Q., Liu, G., Wang, C., Yang, R., Zhao, L., Qin, T.,
  Liu, T.-Y., and Hon, H.-W.
\newblock Suphx: Mastering mahjong with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2003.13590}, 2020.

\bibitem[Lin(1992)]{lin1992self}
Lin, L.~J.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Mach. Learn.}, 8:\penalty0 293--321, 1992.

\bibitem[Littman(2001)]{littman2001friend}
Littman, M.~L.
\newblock Friend-or-foe q-learning in general-sum games.
\newblock In \emph{ICML}, 2001.

\bibitem[Lowe et~al.(2017)Lowe, Wu, Tamar, Harb, Abbeel, and
  Mordatch]{lowe2017multi}
Lowe, R., Wu, Y., Tamar, A., Harb, J., Abbeel, P., and Mordatch, I.
\newblock Multi-agent actor-critic for mixed cooperative-competitive
  environments.
\newblock In \emph{{NIPS}}, 2017.

\bibitem[Mguni et~al.(2021)Mguni, Wu, Du, Yang, Wang, Li, Wen, Jennings, and
  Wang]{mguni2021learning}
Mguni, D.~H., Wu, Y., Du, Y., Yang, Y., Wang, Z., Li, M., Wen, Y., Jennings,
  J., and Wang, J.
\newblock Learning in nonzero-sum stochastic games with potentials.
\newblock In \emph{{ICML}}, 2021.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{{ICML}}, 2016.

\bibitem[Munos \& Szepesv{\'a}ri(2008)Munos and
  Szepesv{\'a}ri]{munos2008finite}
Munos, R. and Szepesv{\'a}ri, C.
\newblock Finite-time bounds for fitted value iteration.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (5), 2008.

\bibitem[Oliehoek et~al.(2016)Oliehoek, Amato, et~al.]{oliehoek2016concise}
Oliehoek, F.~A., Amato, C., et~al.
\newblock \emph{A concise introduction to decentralized POMDPs}, volume~1.
\newblock Springer, 2016.

\bibitem[Papoudakis et~al.(2021)Papoudakis, Christianos, Sch{\"a}fer, and
  Albrecht]{papoudakis2021benchmarking}
Papoudakis, G., Christianos, F., Sch{\"a}fer, L., and Albrecht, S.~V.
\newblock Benchmarking multi-agent deep reinforcement learning algorithms in
  cooperative tasks.
\newblock In \emph{{NeurIPS}}, 2021.

\bibitem[Rashid et~al.(2018)Rashid, Samvelyan, de~Witt, Farquhar, Foerster, and
  Whiteson]{rashid2018qmix}
Rashid, T., Samvelyan, M., de~Witt, C.~S., Farquhar, G., Foerster, J.~N., and
  Whiteson, S.
\newblock {QMIX:} monotonic value function factorisation for deep multi-agent
  reinforcement learning.
\newblock In \emph{{ICML}}, 2018.

\bibitem[Redmon et~al.(2016)Redmon, Divvala, Girshick, and
  Farhadi]{redmon2016you}
Redmon, J., Divvala, S., Girshick, R., and Farhadi, A.
\newblock You only look once: Unified, real-time object detection.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Ren, S., He, K., Girshick, R., and Sun, J.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock \emph{{NIPS}}, 28:\penalty0 91--99, 2015.

\bibitem[Riedmiller(2005)]{riedmiller2005neural}
Riedmiller, M.~A.
\newblock Neural fitted {Q} iteration - first experiences with a data efficient
  neural reinforcement learning method.
\newblock In \emph{{ECML}}, 2005.

\bibitem[Samvelyan et~al.(2019)Samvelyan, Rashid, de~Witt, Farquhar, Nardelli,
  Rudner, Hung, Torr, Foerster, and Whiteson]{samvelyan19smac}
Samvelyan, M., Rashid, T., de~Witt, C.~S., Farquhar, G., Nardelli, N., Rudner,
  T. G.~J., Hung, C.-M., Torr, P. H.~S., Foerster, J., and Whiteson, S.
\newblock {The} {StarCraft} {Multi}-{Agent} {Challenge}.
\newblock \emph{CoRR}, abs/1902.04043, 2019.

\bibitem[Scherrer et~al.(2015)Scherrer, Ghavamzadeh, Gabillon, Lesner, and
  Geist]{scherrer2015approximate}
Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M.
\newblock Approximate modified policy iteration and its application to the game
  of tetris.
\newblock \emph{Journal of Machine Learning Research}, 16:\penalty0 1629--1676,
  2015.

\bibitem[Schmidt-Hieber(2020)]{schmidt2020nonparametric}
Schmidt-Hieber, J.
\newblock Nonparametric regression using deep neural networks with relu
  activation function.
\newblock \emph{The Annals of Statistics}, 48\penalty0 (4):\penalty0
  1875--1897, 2020.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{{ICML}}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Silver et~al.(2017)Silver, Hubert, Schrittwieser, Antonoglou, Lai,
  Guez, Lanctot, Sifre, Kumaran, Graepel, et~al.]{silver2017mastering}
Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,
  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al.
\newblock Mastering chess and shogi by self-play with a general reinforcement
  learning algorithm.
\newblock \emph{arXiv preprint arXiv:1712.01815}, 2017.

\bibitem[Singh et~al.(2019)Singh, Jain, and Sukhbaatar]{singh2018learning}
Singh, A., Jain, T., and Sukhbaatar, S.
\newblock Learning when to communicate at scale in multiagent cooperative and
  competitive tasks.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Suarez et~al.(2021)Suarez, Du, Zhu, Mordatch, and
  Isola]{suarez2021neural}
Suarez, J., Du, Y., Zhu, C., Mordatch, I., and Isola, P.
\newblock The neural {MMO} platform for massively multiagent research.
\newblock \emph{CoRR}, abs/2110.07594, 2021.

\bibitem[Sukhbaatar et~al.(2016)Sukhbaatar, Szlam, and
  Fergus]{sukhbaatar2016learning}
Sukhbaatar, S., Szlam, A., and Fergus, R.
\newblock Learning multiagent communication with backpropagation.
\newblock In \emph{{NIPS}}, 2016.

\bibitem[Sunehag et~al.(2018)Sunehag, Lever, Gruslys, Czarnecki, Zambaldi,
  Jaderberg, Lanctot, Sonnerat, Leibo, Tuyls, and Graepel]{sunehag2017value}
Sunehag, P., Lever, G., Gruslys, A., Czarnecki, W.~M., Zambaldi, V.~F.,
  Jaderberg, M., Lanctot, M., Sonnerat, N., Leibo, J.~Z., Tuyls, K., and
  Graepel, T.
\newblock Value-decomposition networks for cooperative multi-agent learning
  based on team reward.
\newblock In \emph{{AAMAS}}, 2018.

\bibitem[Tan(1993)]{tan1993multi}
Tan, M.
\newblock Multi-agent reinforcement learning: Independent vs. cooperative
  agents.
\newblock In \emph{{ICML}}, 1993.

\bibitem[Terry et~al.(2020)Terry, Grammel, Hari, Santos, and
  Black]{terry2020revisiting}
Terry, J.~K., Grammel, N., Hari, A., Santos, L., and Black, B.
\newblock Revisiting parameter sharing in multi-agent deep reinforcement
  learning.
\newblock \emph{arXiv preprint arXiv:2005.13625}, 2020.

\bibitem[Tuyls et~al.(2021)Tuyls, Omidshafiei, Muller, Wang, Connor, Hennes,
  Graham, Spearman, Waskett, Steel, et~al.]{tuyls2021game}
Tuyls, K., Omidshafiei, S., Muller, P., Wang, Z., Connor, J., Hennes, D.,
  Graham, I., Spearman, W., Waskett, T., Steel, D., et~al.
\newblock Game plan: What ai can do for football, and what football can do for
  ai.
\newblock \emph{Journal of Artificial Intelligence Research}, 71:\penalty0
  41--88, 2021.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, et~al.]{vinyals2019grandmaster}
Vinyals, O., Babuschkin, I., Czarnecki, W.~M., Mathieu, M., Dudzik, A., Chung,
  J., Choi, D.~H., Powell, R., Ewalds, T., Georgiev, P., et~al.
\newblock Grandmaster level in starcraft ii using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 575\penalty0 (7782):\penalty0 350--354, 2019.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Ren, Han, Ye, and
  Zhang]{wang2020towards}
Wang, J., Ren, Z., Han, B., Ye, J., and Zhang, C.
\newblock Towards understanding linear value decomposition in cooperative
  multi-agent q-learning.
\newblock \emph{arXiv preprint arXiv:2006.00587}, 2020{\natexlab{a}}.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Dong, Lesser, and
  Zhang]{wang2020roma}
Wang, T., Dong, H., Lesser, V.~R., and Zhang, C.
\newblock {ROMA:} multi-agent reinforcement learning with emergent roles.
\newblock In \emph{{ICML}}, 2020{\natexlab{b}}.

\bibitem[Wang et~al.(2021)Wang, Gupta, Mahajan, Peng, Whiteson, and
  Zhang]{wang2020rode}
Wang, T., Gupta, T., Mahajan, A., Peng, B., Whiteson, S., and Zhang, C.
\newblock {RODE:} learning roles to decompose multi-agent tasks.
\newblock In \emph{{ICLR}}, 2021.

\bibitem[Yang et~al.(2018)Yang, Luo, Li, Zhou, Zhang, and Wang]{yang2018mean}
Yang, Y., Luo, R., Li, M., Zhou, M., Zhang, W., and Wang, J.
\newblock Mean field multi-agent reinforcement learning.
\newblock In \emph{{ICML}}, 2018.

\bibitem[Ye et~al.(2020)Ye, Liu, Sun, Shi, Zhao, Wu, Yu, Yang, Wu, Guo,
  et~al.]{ye2020mastering}
Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang, S., Wu, X.,
  Guo, Q., et~al.
\newblock Mastering complex control in moba games with deep reinforcement
  learning.
\newblock In \emph{{AAAI}}, 2020.

\bibitem[Yu et~al.(2021)Yu, Velu, Vinitsky, Wang, Bayen, and
  Wu]{yu2021surprising}
Yu, C., Velu, A., Vinitsky, E., Wang, Y., Bayen, A., and Wu, Y.
\newblock The surprising effectiveness of mappo in cooperative, multi-agent
  games.
\newblock \emph{arXiv preprint arXiv:2103.01955}, 2021.

\bibitem[Zha et~al.(2021)Zha, Xie, Ma, Zhang, Lian, Hu, and
  Liu]{zha2021douzero}
Zha, D., Xie, J., Ma, W., Zhang, S., Lian, X., Hu, X., and Liu, J.
\newblock Douzero: Mastering doudizhu with self-play deep reinforcement
  learning.
\newblock In \emph{{ICML}}, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Yang, Liu, Zhang, and Basar]{zhang2021finite}
Zhang, K., Yang, Z., Liu, H., Zhang, T., and Basar, T.
\newblock Finite-sample analysis for decentralized batch multi-agent
  reinforcement learning with networked agents.
\newblock \emph{IEEE Transactions on Automatic Control}, 2021.
\newblock \doi{10.1109/TAC.2021.3049345}.

\bibitem[Zheng et~al.(2018)Zheng, Yang, Cai, Zhou, Zhang, Wang, and
  Yu]{zheng2017magent}
Zheng, L., Yang, J., Cai, H., Zhou, M., Zhang, W., Wang, J., and Yu, Y.
\newblock Magent: {A} many-agent reinforcement learning platform for artificial
  collective intelligence.
\newblock In \emph{{AAAI}}, 2018.

\bibitem[Zhong et~al.(2021)Zhong, Sun, Luo, Yan, and Wang]{zhong2021towards}
Zhong, F., Sun, P., Luo, W., Yan, T., and Wang, Y.
\newblock Towards distraction-robust active visual tracking.
\newblock In \emph{{ICML}}, 2021.

\bibitem[Zhou et~al.(2020)Zhou, Luo, Villella, Yang, Rusu, Miao, Zhang, Alban,
  Fadakar, Chen, et~al.]{zhou2020smarts}
Zhou, M., Luo, J., Villella, J., Yang, Y., Rusu, D., Miao, J., Zhang, W.,
  Alban, M., Fadakar, I., Chen, Z., et~al.
\newblock Smarts: Scalable multi-agent reinforcement learning training school
  for autonomous driving.
\newblock \emph{arXiv preprint arXiv:2010.09776}, 2020.

\bibitem[Zhu et~al.(2021)Zhu, Hu, Zhang, Hong, Zhu, Chang, and
  Liang]{zhu2021main}
Zhu, F., Hu, S., Zhang, Y., Hong, H., Zhu, Y., Chang, X., and Liang, X.
\newblock Main: A multi-agent indoor navigation benchmark for cooperative
  learning.
\newblock 2021.

\end{thebibliography}
