\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu and Li(2017)]{allen2017faster}
Zeyuan Allen-Zhu and Yuanzhi Li.
\newblock Faster principal component regression and stable matrix chebyshev
  approximation.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 107--115. JMLR. org, 2017.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and Liang]{all19}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock In \emph{NeurIPS}. arXiv preprint arXiv:1811.04918,
  2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and Song]{als19a}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock In \emph{NeurIPS}. \url{https://arxiv.org/pdf/1810.12065},
  2019{\natexlab{b}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{c}})Allen-Zhu, Li, and Song]{als19b}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{ICML}. \url{https://arxiv.org/pdf/1811.03962},
  2019{\natexlab{c}}.

\bibitem[Arora et~al.(2019{\natexlab{a}})Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{adhlsw19}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Ruslan Salakhutdinov, and
  Ruosong Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{NeurIPS}, 2019{\natexlab{a}}.

\bibitem[Arora et~al.(2019{\natexlab{b}})Arora, Du, Hu, Li, and Wang]{adhlw19}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{ICML}. arXiv preprint arXiv:1901.08584, 2019{\natexlab{b}}.

\bibitem[Athalye et~al.(2018)Athalye, Carlini, and
  Wagner]{athalye2018obfuscated}
Anish Athalye, Nicholas Carlini, and David Wagner.
\newblock Obfuscated gradients give a false sense of security: Circumventing
  defenses to adversarial examples.
\newblock \emph{arXiv preprint arXiv:1802.00420}, 2018.

\bibitem[Buckman et~al.(2018)Buckman, Roy, Raffel, and
  Goodfellow]{buckman2018thermometer}
Jacob Buckman, Aurko Roy, Colin Raffel, and Ian Goodfellow.
\newblock Thermometer encoding: One hot way to resist adversarial examples.
\newblock 2018.

\bibitem[Carlini and Wagner(2017)]{cw17c}
Nicholas Carlini and David Wagner.
\newblock Towards evaluating the robustness of neural networks.
\newblock In \emph{2017 IEEE Symposium on Security and Privacy (SP)}, pages
  39--57. IEEE, 2017.

\bibitem[Du et~al.(2019{\natexlab{a}})Du, Lee, Li, Wang, and Zhai]{dllwz19}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{ICML}. \url{https://arxiv.org/pdf/1811.03804},
  2019{\natexlab{a}}.

\bibitem[Du et~al.(2019{\natexlab{b}})Du, Zhai, Poczos, and Singh]{dzps19}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{ICLR}. \url{https://arxiv.org/pdf/1810.02054},
  2019{\natexlab{b}}.

\bibitem[Eremenko and Yuditskii(2006)]{eremenko2006uniform}
Alexandre Eremenko and Peter Yuditskii.
\newblock Uniform approximation of sgn (x) by polynomials and entire functions.
\newblock \emph{arXiv preprint math/0604324}, 2006.

\bibitem[Frostig et~al.(2016)Frostig, Musco, Musco, and
  Sidford]{frostig2016principal}
Roy Frostig, Cameron Musco, Christopher Musco, and Aaron Sidford.
\newblock Principal component projection without principal component analysis.
\newblock In \emph{International Conference on Machine Learning}, pages
  2349--2357, 2016.

\bibitem[Gao et~al.(2019)Gao, Cai, Li, Hsieh, Wang, and Lee]{gclhwl19}
Ruiqi Gao, Tianle Cai, Haochuan Li, Cho-Jui Hsieh, Liwei Wang, and Jason~D Lee.
\newblock Convergence of adversarial training in overparametrized neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pages 13009--13020, 2019.

\bibitem[Guo et~al.(2017)Guo, Rana, Cisse, and Van
  Der~Maaten]{guo2017countering}
Chuan Guo, Mayank Rana, Moustapha Cisse, and Laurens Van Der~Maaten.
\newblock Countering adversarial images using input transformations.
\newblock \emph{arXiv preprint arXiv:1711.00117}, 2017.

\bibitem[Hazan(2016)]{h16}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
Elad Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jgh18}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Li and Liang(2018)]{ll18}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{mmstv18}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{ICLR}. arXiv preprint arXiv:1706.06083, 2018.

\bibitem[Oymak and Soltanolkotabi(2019)]{oymak2019towards}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock \emph{arXiv preprint arXiv:1902.04674}, 2019.

\bibitem[Raghunathan et~al.(2018)Raghunathan, Steinhardt, and
  Liang]{raghunathan2018certified}
Aditi Raghunathan, Jacob Steinhardt, and Percy Liang.
\newblock Certified defenses against adversarial examples.
\newblock \emph{arXiv preprint arXiv:1801.09344}, 2018.

\bibitem[Sachdeva et~al.(2014)Sachdeva, Vishnoi, et~al.]{sachdeva2014faster}
Sushant Sachdeva, Nisheeth~K Vishnoi, et~al.
\newblock Faster algorithms via approximation theory.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 9\penalty0 (2):\penalty0 125--210, 2014.

\bibitem[Song and Yang(2019)]{sy19}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szsbegf13}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Wong and Kolter(2017)]{wong2017provable}
Eric Wong and J~Zico Kolter.
\newblock Provable defenses against adversarial examples via the convex outer
  adversarial polytope.
\newblock \emph{arXiv preprint arXiv:1711.00851}, 2017.

\bibitem[Xie et~al.(2017)Xie, Wang, Zhang, Ren, and Yuille]{xie2017mitigating}
Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan Yuille.
\newblock Mitigating adversarial effects through randomization.
\newblock \emph{arXiv preprint arXiv:1711.01991}, 2017.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
