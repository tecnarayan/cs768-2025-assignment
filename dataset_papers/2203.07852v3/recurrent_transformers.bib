@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  year={2017}
}

@article{hochreiter97lstm,
  author      = {Sepp Hochreiter and JÃ¼rgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  volume      = {9},
}

@inproceedings{cho2014gru,
  author    = {Kyunghyun Cho and
               Bart van Merrienboer and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Dzmitry Bahdanau and
               Fethi Bougares and
               Holger Schwenk and
               Yoshua Bengio},
  title     = {Learning Phrase Representations using {RNN} Encoder-Decoder for Statistical
               Machine Translation},
  booktitle = {EMNLP},
  year      = {2014},
}

@inproceedings{srivastava2015highway,
  author    = {Rupesh Kumar Srivastava and
               Klaus Greff and
               J{\"{u}}rgen Schmidhuber},
  editor    = {Corinna Cortes and
               Neil D. Lawrence and
               Daniel D. Lee and
               Masashi Sugiyama and
               Roman Garnett},
  title     = {Training Very Deep Networks},
  booktitle = {NIPS},
  year      = {2015},
}

@inproceedings{henry2020querykeynorm,
  author    = {Alex Henry and
               Prudhvi Raj Dachapally and
               Shubham Shantaram Pawar and
               Yuxuan Chen},
  title     = {Query-Key Normalization for Transformers},
  booktitle = {EMNLP},
  year      = {2020},
}

@inproceedings{kingma2015adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {ICLR},
  year      = {2015},
}

@inproceedings{shazeer2018adafactor,
  author    = {Noam Shazeer and
               Mitchell Stern},
  title     = {Adafactor: Adaptive Learning Rates with Sublinear Memory Cost},
  booktitle = {ICML},
  year      = {2018},
}

@inproceedings{
kitaev2020reformer,
title={Reformer: The Efficient Transformer},
author={Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=rkgNKkHtvB}
}

@software{flax2020github,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.3.5},
  year = {2020},
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@inproceedings{
tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@inproceedings{katharopoulos2020transformersRNNs,
  author    = {Angelos Katharopoulos and
               Apoorv Vyas and
               Nikolaos Pappas and
               Fran{\c{c}}ois Fleuret},
  title     = {Transformers are {RNNs}: Fast Autoregressive Transformers with Linear
               Attention},
  booktitle = {ICML},
  year      = {2020},
}

@inproceedings{choromanski2021performers,
  author    = {Krzysztof Marcin Choromanski and
               Valerii Likhosherstov and
               David Dohan and
               Xingyou Song and
               Andreea Gane and
               Tam{\'{a}}s Sarl{\'{o}}s and
               Peter Hawkins and
               Jared Quincy Davis and
               Afroz Mohiuddin and
               Lukasz Kaiser and
               David Benjamin Belanger and
               Lucy J. Colwell and
               Adrian Weller},
  title     = {Rethinking Attention with Performers},
  booktitle = {ICLR},
  year      = {2021}
}

@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal   = {CoRR},
  volume    = {abs/2006.04768},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.04768},
}

@inproceedings{rae2020compressive,
  author    = {Jack W. Rae and
               Anna Potapenko and
               Siddhant M. Jayakumar and
               Chloe Hillier and
               Timothy P. Lillicrap},
  title     = {Compressive Transformers for Long-Range Sequence Modelling},
  booktitle = {ICLR},
  year      = {2020},
}

@inproceedings{dai2020funnel,
  author    = {Zihang Dai and
               Guokun Lai and
               Yiming Yang and
               Quoc Le},
  title     = {Funnel-Transformer: Filtering out Sequential Redundancy for Efficient
               Language Processing},
  booktitle = {NeurIPS},
  year      = {2020},
}

@inproceedings{
ren2021combiner,
title={Combiner: Full Attention Transformer with Sparse Computation Cost},
author={Hongyu Ren and Hanjun Dai and Zihang Dai and Mengjiao Yang and Jure Leskovec and Dale Schuurmans and Bo Dai},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=MQQeeDiO5vv}
}

@article{roy2021efficient,
  title={Efficient content-based sparse attention with routing transformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={Transactions of the Association for Computational Linguistics},
  volume={9},
  pages={53--68},
  year={2021},
  publisher={MIT Press}
}

@article{fan2020feedback,
  title={Addressing some limitations of transformers with feedback memory},
  author={Fan, Angela and Lavril, Thibaut and Grave, Edouard and Joulin, Armand and Sukhbaatar, Sainbayar},
  journal={arXiv preprint arXiv:2002.09402},
  year={2020}
}

@inproceedings{lei18sru,
  author    = {Tao Lei and
               Yu Zhang and
               Sida I. Wang and
               Hui Dai and
               Yoav Artzi},
  title     = {Simple Recurrent Units for Highly Parallelizable Recurrence},
  booktitle = {EMNLP},
  year      = {2018},
}

@inproceedings{lei21srupp,
  author    = {Tao Lei},
  title     = {When Attention Meets Fast Recurrence: Training Language Models with
               Reduced Compute},
  booktitle = {EMNLP},
  year      = {2021},
}

@inproceedings{dai2019transformerXL,
  author    = {Zihang Dai and
               Zhilin Yang and
               Yiming Yang and
               Jaime G. Carbonell and
               Quoc Viet Le and
               Ruslan Salakhutdinov},
  title     = {Transformer-{XL}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {ACL},
  year      = {2019},
}

@inproceedings{Zaheer2020bigbird,
  author    = {Manzil Zaheer and
               Guru Guruganesh and
               Kumar Avinava Dubey and
               Joshua Ainslie and
               Chris Alberti and
               Santiago Onta{\~{n}}{\'{o}}n and
               Philip Pham and
               Anirudh Ravula and
               Qifan Wang and
               Li Yang and
               Amr Ahmed},
  title     = {Big Bird: Transformers for Longer Sequences},
  booktitle = {NeurIPS},
  year      = {2020},
}

@article{beltagy2020longformer,
  author    = {Iz Beltagy and
               Matthew E. Peters and
               Arman Cohan},
  title     = {Longformer: The Long-Document Transformer},
  year      = {2020},
  journal    = {arXiv preprint arXiv:2004.05150},
}

@inproceedings{Zhu2021hierarchical1D,
  author    = {Zhenhai Zhu and
               Radu Soricut},
  editor    = {Chengqing Zong and
               Fei Xia and
               Wenjie Li and
               Roberto Navigli},
  title     = {H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for
               Sequences},
  booktitle = {ACL},
  year      = {2021},
}

@inproceedings{hellendoorn2020great,
  title	= {Global Relational Models of Source Code},
  author = {Vincent Josua Hellendoorn and Petros Maniatis and Rishabh Singh and Charles Sutton and David Bieber},
  booktitle = {ICLR},
  year	= {2020},
}

@inproceedings{brown2020gpt3,
  author    = {Tom B. Brown and
               Benjamin Mann and
               Nick Ryder and
               Melanie Subbiah and
               Jared Kaplan and
               Prafulla Dhariwal and
               Arvind Neelakantan and
               Pranav Shyam and
               Girish Sastry and
               Amanda Askell and
               Sandhini Agarwal and
               Ariel Herbert{-}Voss and
               Gretchen Krueger and
               Tom Henighan and
               Rewon Child and
               Aditya Ramesh and
               Daniel M. Ziegler and
               Jeffrey Wu and
               Clemens Winter and
               Christopher Hesse and
               Mark Chen and
               Eric Sigler and
               Mateusz Litwin and
               Scott Gray and
               Benjamin Chess and
               Jack Clark and
               Christopher Berner and
               Sam McCandlish and
               Alec Radford and
               Ilya Sutskever and
               Dario Amodei},
  title     = {Language Models are Few-Shot Learners},
  booktitle = {NeurIPS},
  year      = {2020},
}

@inproceedings{Ainslie2020etc_hierarchical_structured,
  author    = {Joshua Ainslie and
               Santiago Onta{\~{n}}{\'{o}}n and
               Chris Alberti and
               Vaclav Cvicek and
               Zachary Fisher and
               Philip Pham and
               Anirudh Ravula and
               Sumit Sanghai and
               Qifan Wang and
               Li Yang},
  title     = {{ETC:} Encoding Long and Structured Inputs in Transformers},
  booktitle = {EMNLP},
  year      = {2020},
}

@phdthesis{Sutskever2013trainingrecurrent,
    author = {Ilya Sutskever},
    title = {Training Recurrent Neural Networks},
    school = {University of Toronto Toronto, Ontario, Canada},
    year = {2013}
}

@article{williams1990tbptt,
  author    = {Ronald J. Williams and
               Jing Peng},
  title     = {An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent
               Network Trajectories},
  journal   = {Neural Computation},
  year      = {1990}
}

@inproceedings{sukhbaatar2021expire,
  author    = {Sainbayar Sukhbaatar and
               Da Ju and
               Spencer Poff and
               Stephen Roller and
               Arthur Szlam and
               Jason Weston and
               Angela Fan},
  title     = {Not All Memories are Created Equal: Learning to Forget by Expiring},
  booktitle = {ICML},
  year      = {2021},
}

@inproceedings{GomezRUG2017reversible,
  author    = {Aidan N. Gomez and
               Mengye Ren and
               Raquel Urtasun and
               Roger B. Grosse},
  title     = {The Reversible Residual Network: Backpropagation Without Storing Activations},
  booktitle = {NeurIPS},
  year      = {2017},
}

@article{Raffel2020t5journal,
  author    = {Colin Raffel and
               Noam Shazeer and
               Adam Roberts and
               Katherine Lee and
               Sharan Narang and
               Michael Matena and
               Yanqi Zhou and
               Wei Li and
               Peter J. Liu},
  title     = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
               Transformer},
  journal   = {Journal of Machine Learning Research},
  year      = {2020},
}

@inproceedings{simengsun2021pg19_long_range_context,
    title = "Do Long-Range Language Models Actually Use Long-Range Context?",
    author = "Sun, Simeng  and
      Krishna, Kalpesh  and
      Mattarella-Micke, Andrew  and
      Iyyer, Mohit",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2021",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.62",
    doi = "10.18653/v1/2021.emnlp-main.62",
    pages = "807--822",
}

@inproceedings{kudo18sentencepiece,
  author    = {Taku Kudo and
               John Richardson},
  title     = {SentencePiece: {A} simple and language independent subword tokenizer
               and detokenizer for Neural Text Processing},
  booktitle = {EMNLP},
  year      = {2018},
}

@inproceedings{BahdanauCB2015attention,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {ICLR},
  year      = {2015},
}

@article{press2021linearattentionbias,
  author    = {Ofir Press and
               Noah A. Smith and
               Mike Lewis},
  title     = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  journal   = {CoRR},
  volume    = {arXiv preprint abs/2108.12409},
  year      = {2021},
}

@inproceedings{wu2022memorizing,
  author    = {Yuhuai Wu and 
               Markus Rabe and 
               DeLesley Hutchins and 
               Christian Szegedy},
  title     = {Memorizing Transformers},
  booktitle = {ICLR},
  year      = {2022}
}

@inproceedings{schlag2021linear,
      title={Linear {T}ransformers Are Secretly Fast Weight Programmers}, 
      author={Imanol Schlag and Kazuki Irie and J\"urgen Schmidhuber},
      booktitle= {ICML},
      year={2021}
}

@inproceedings{peng2021random,
  title={Random Feature Attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah A and Kong, Lingpeng},
  booktitle = {ICLR},
  year={2021}
}

@inproceedings{irie2021going,
      title={Going Beyond Linear Transformers with Recurrent Fast Weight Programmers}, 
      author={Kazuki Irie and Imanol Schlag and R\'obert Csord\'as and J\"urgen Schmidhuber},
      booktitle={confNEU},
      year={2021}
}

@article{wang2020rtransformer,
  title={R-Transformer: Recurrent Neural Network Enhanced Transformer},
  author={Zhiwei Wang and Yao Ma and Zitao Liu and Jiliang Tang},
  journal={arXiv preprint arXiv:1907.05572},
  year={2019}
}

@INPROCEEDINGS{adel2021memorytransformer,
  author={Al Adel, Arij and Burtsev, Mikhail S.},
  booktitle={2021 International Conference Engineering and Telecommunication (En T)}, 
  title={Memory transformer with hierarchical attention for long document processing}, 
  year={2021},
}

@article{ju2021staircase,
  title={Staircase Attention for Recurrent Processing of Sequences},
  author={Ju, Da and Roller, Stephen and Sukhbaatar, Sainbayar and Weston, Jason},
  journal={arXiv preprint arXiv:2106.04279},
  year={2021}
}

@article{greff2016lstm,
  title={{LSTM}: A search space odyssey},
  author={Greff, Klaus and Srivastava, Rupesh K and Koutn{\'\i}k, Jan and Steunebrink, Bas R and Schmidhuber, J{\"u}rgen},
  journal={IEEE transactions on neural networks and learning systems},
  volume={28},
  number={10},
  year={2016},
  publisher={IEEE}
}

@inproceedings{dong2021attention:not:all:need,
  author    = {Yihe Dong and
               Jean{-}Baptiste Cordonnier and
               Andreas Loukas},
  editor    = {Marina Meila and
               Tong Zhang},
  title     = {Attention is not all you need: pure attention loses rank doubly exponentially
               with depth},
  booktitle = {ICML},
  year      = {2021},
}

@inproceedings{khandelwal-etal-2018-sharp,
    title = "Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context",
    author = "Khandelwal, Urvashi  and
      He, He  and
      Qi, Peng  and
      Jurafsky, Dan",
    booktitle = "Association for Computational Linguistics",
    year = "2018",
}

@inproceedings{schmidhuber1993reducing,
  title={Reducing the ratio between learning complexity and number of time varying variables in fully recurrent nets},
  author={Schmidhuber, J{\"u}rgen},
  booktitle={International Conference on Artificial Neural Networks (ICANN)},
  year={1993},
}

@inproceedings{chen2018bestRNMT,
  author    = {Mia Xu Chen and
               Orhan Firat and
               Ankur Bapna and
               Melvin Johnson and
               Wolfgang Macherey and
               George F. Foster and
               Llion Jones and
               Mike Schuster and
               Noam Shazeer and
               Niki Parmar and
               Ashish Vaswani and
               Jakob Uszkoreit and
               Lukasz Kaiser and
               Zhifeng Chen and
               Yonghui Wu and
               Macduff Hughes},
  title     = {The Best of Both Worlds: Combining Recent Advances in Neural Machine
               Translation},
  booktitle = {ACL},
  year      = {2018},
}

@article{hawthorne2022general,
  title={General-purpose, long-context autoregressive modeling with Perceiver AR},
  author={Hawthorne, Curtis and Jaegle, Andrew and Cangea, C{\u{a}}t{\u{a}}lina and Borgeaud, Sebastian and Nash, Charlie and Malinowski, Mateusz and Dieleman, Sander and Vinyals, Oriol and Botvinick, Matthew and Simon, Ian and others},
  journal={arXiv preprint arXiv:2202.07765},
  year={2022}
}

@article{hoffmann2022training,
  title={Training Compute-Optimal Large Language Models},
  author={Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal={arXiv preprint arXiv:2203.15556},
  year={2022}
}

@article{adiwardana2020towards,
  title={Towards a human-like open-domain chatbot},
  author={Adiwardana, Daniel and Luong, Minh-Thang and So, David R and Hall, Jamie and Fiedel, Noah and Thoppilan, Romal and Yang, Zi and Kulshreshtha, Apoorv and Nemade, Gaurav and Lu, Yifeng and others},
  journal={arXiv preprint arXiv:2001.09977},
  year={2020}
}

@inproceedings{
press2022train,
title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
author={Ofir Press and Noah Smith and Mike Lewis},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=R8sQPpGCv0}
}

@article{Hua2022lineartime,
  title={Transformer Quality in Linear Time},
  author={Weizhe Hua and Zihang Dai and Hanxiao Liu and Quoc V. Le},
  journal={arXiv preprint arXiv:2202.10447},
  year={2022}
}

@article{thoppilan2022lamda,
  title={LaMDA: Language Models for Dialog Applications},
  author={Thoppilan, Romal and De Freitas, Daniel and Hall, Jamie and Shazeer, Noam and Kulshreshtha, Apoorv and Cheng, Heng-Tze and Jin, Alicia and Bos, Taylor and Baker, Leslie and Du, Yu and others},
  journal={arXiv preprint arXiv:2201.08239},
  year={2022}
}

@inproceedings{
csordas2022datarouter,
title={The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization},
author={R{\'o}bert Csord{\'a}s and Kazuki Irie and J{\"u}rgen Schmidhuber},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=KBQP4A_J1K}
}

@inproceedings{jaegle21perceiver,
  author    = {Andrew Jaegle and
               Felix Gimeno and
               Andy Brock and
               Oriol Vinyals and
               Andrew Zisserman and
               Jo{\~{a}}o Carreira},
  title     = {Perceiver: General Perception with Iterative Attention},
  booktitle = {icml},
  year      = {2021}
}

@misc{shaham2022scrolls,
    title={SCROLLS: Standardized CompaRison Over Long Language Sequences},
    author={Uri Shaham and Elad Segal and Maor Ivgi and Avia Efrat and Ori Yoran and Adi Haviv and Ankit Gupta and Wenhan Xiong and Mor Geva and Jonathan Berant and Omer Levy},
    year={2022},
    eprint={2201.03533},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{wang2022squality,
  title={SQuALITY: Building a Long-Document Summarization Dataset the Hard Way},
  author={Wang, Alex and Pang, Richard Yuanzhe and Chen, Angelica and Phang, Jason and Bowman, Samuel R},
  journal={arXiv preprint arXiv:2205.11465},
  year={2022}
}

@article{srivastava2022beyond,
  title={Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},
  author={Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adri{\`a} and others},
  journal={arXiv preprint arXiv:2206.04615},
  year={2022}
}

@misc{meliad2022github,
  author = {DeLesley Hutchins and Markus Rabe and Yuhuai Wu and Immanol Schlag and Charles Staats},
  title = {Meliad},
  howpublished ={Github source code repository. \url{https://github.com/google-research/meliad}},
  year = {2022},
}