\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjevani et~al.(2019)Arjevani, Carmon, Duchi, Foster, Srebro, and
  Woodworth]{Arjevani2019}
Arjevani, Y., Carmon, Y., Duchi, J.~C., Foster, D.~J., Srebro, N., and
  Woodworth, B.
\newblock Lower bounds for non-convex stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1912.02365}, 2019.

\bibitem[Bhojanapalli et~al.(2021)Bhojanapalli, Wilber, Veit, Rawat, Kim,
  Menon, and Kumar]{Bhojanapalli2021}
Bhojanapalli, S., Wilber, K., Veit, A., Rawat, A.~S., Kim, S., Menon, A., and
  Kumar, S.
\newblock On the reproducibility of neural network predictions.
\newblock February 2021.

\bibitem[Carmon et~al.(2017)Carmon, Duchi, Hinder, and Sidford]{Carmon2017a}
Carmon, Y., Duchi, J.~C., Hinder, O., and Sidford, A.
\newblock Lower bounds for finding stationary points i.
\newblock \emph{arXiv preprint arXiv:1710.11606}, 2017.

\bibitem[Cheng et~al.(2020)Cheng, Yin, Bartlett, and
  Jordan]{cheng2020stochastic}
Cheng, X., Yin, D., Bartlett, P., and Jordan, M.
\newblock Stochastic gradient and langevin processes.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1810--1819. PMLR, 2020.

\bibitem[Cheung \& Piliouras(2019)Cheung and Piliouras]{cheung2019vortices}
Cheung, Y.~K. and Piliouras, G.
\newblock Vortices instead of equilibria in minmax optimization: Chaos and
  butterfly effects of online learning in zero-sum games.
\newblock In \emph{Conference on Learning Theory}, pp.\  807--834. PMLR, 2019.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018global}
Chizat, L. and Bach, F.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock \emph{arXiv preprint arXiv:1805.09545}, 2018.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and Talwalkar]{Cohen2021}
Cohen, J.~M., Kaur, S., Li, Y., Kolter, J.~Z., and Talwalkar, A.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability.
\newblock February 2021.

\bibitem[Dai et~al.(2019{\natexlab{a}})Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019{\natexlab{a}}.

\bibitem[Dai et~al.(2019{\natexlab{b}})Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{Dai2019}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J.~G., Le, Q.~V., and Salakhutdinov, R.
\newblock {Transformer-XL}: Attentive language models beyond a fixed-length
  context.
\newblock \emph{CoRR}, abs/1901.02860, 2019{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1901.02860}.

\bibitem[Defazio \& Bottou(2018)Defazio and Bottou]{Defazio2018}
Defazio, A. and Bottou, L.
\newblock On the ineffectiveness of variance reduced optimization for deep
  learning.
\newblock \emph{arXiv preprint arXiv:1812.04529}, 2018.

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{Fang2018a}
Fang, C., Li, C.~J., Lin, Z., and Zhang, T.
\newblock Spider: Near-optimal non-convex optimization via stochastic path
  integrated differential estimator, 2018.

\bibitem[Flokas et~al.(2020)Flokas, Vlatakis-Gkaragkounis, Lianeas,
  Mertikopoulos, and Piliouras]{flokas2020no}
Flokas, L., Vlatakis-Gkaragkounis, E.-V., Lianeas, T., Mertikopoulos, P., and
  Piliouras, G.
\newblock No-regret learning and mixed nash equilibria: They do not mix.
\newblock \emph{arXiv preprint arXiv:2010.09514}, 2020.

\bibitem[Gurbuzbalaban et~al.(2021)Gurbuzbalaban, Simsekli, and
  Zhu]{gurbuzbalaban2021heavy}
Gurbuzbalaban, M., Simsekli, U., and Zhu, L.
\newblock The heavy-tail phenomenon in sgd.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3964--3975. PMLR, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{Henderson2017}
Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., and Meger, D.
\newblock Deep reinforcement learning that matters.
\newblock September 2017.

\bibitem[Hsieh et~al.(2019)Hsieh, Iutzeler, Malick, and
  Mertikopoulos]{Hsieh2019}
Hsieh, Y.-G., Iutzeler, F., Malick, J., and Mertikopoulos, P.
\newblock On the convergence of single-call stochastic extra-gradient methods.
\newblock \emph{arXiv preprint arXiv:1908.08465}, 2019.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{Kingma2014}
Kingma, D.~P. and Ba, J.
\newblock {ADAM}: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kunin et~al.(2021)Kunin, Sagastuy-Brena, Gillespie, Margalit, Tanaka,
  Ganguli, and Yamins]{kunin2021rethinking}
Kunin, D., Sagastuy-Brena, J., Gillespie, L., Margalit, E., Tanaka, H.,
  Ganguli, S., and Yamins, D.~L.
\newblock Rethinking the limiting dynamics of sgd: modified loss, phase space
  oscillations, and anomalous diffusion.
\newblock \emph{arXiv preprint arXiv:2107.09133}, 2021.

\bibitem[Letcher(2020)]{letcher2020impossibility}
Letcher, A.
\newblock On the impossibility of global convergence in multi-loss
  optimization.
\newblock \emph{arXiv preprint arXiv:2005.12649}, 2020.

\bibitem[Li et~al.(2020)Li, Lyu, and Arora]{li2020reconciling}
Li, Z., Lyu, K., and Arora, S.
\newblock Reconciling modern deep learning with traditional optimization
  analyses: The intrinsic learning rate.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Lobacheva et~al.(2021)Lobacheva, Kodryan, Chirkova, Malinin, and
  Vetrov]{lobacheva2021periodic}
Lobacheva, E., Kodryan, M., Chirkova, N., Malinin, A., and Vetrov, D.
\newblock On the periodic behavior of neural network training with batch
  normalization and weight decay.
\newblock \emph{arXiv preprint arXiv:2106.15739}, 2021.

\bibitem[Madhyastha \& Jain(2019)Madhyastha and Jain]{Madhyastha2019}
Madhyastha, P. and Jain, R.
\newblock On model stability as a function of random seed.
\newblock September 2019.

\bibitem[Mei et~al.(2019)Mei, Misiakiewicz, and Montanari]{mei2019mean}
Mei, S., Misiakiewicz, T., and Montanari, A.
\newblock Mean-field theory of two-layers neural networks: dimension-free
  bounds and kernel limit.
\newblock In \emph{Conference on Learning Theory}, pp.\  2388--2464. PMLR,
  2019.

\bibitem[Papadimitriou \& Piliouras(2019)Papadimitriou and
  Piliouras]{papadimitriou2019game}
Papadimitriou, C. and Piliouras, G.
\newblock Game dynamics as the meaning of a game.
\newblock \emph{ACM SIGecom Exchanges}, 16\penalty0 (2):\penalty0 53--63, 2019.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and Kumar]{Reddi2019}
Reddi, S.~J., Kale, S., and Kumar, S.
\newblock On the convergence of {ADAM} and beyond.
\newblock \emph{arXiv preprint arXiv:1904.09237}, 2019.

\bibitem[Wu et~al.(2018)Wu, Ma, et~al.]{wu2018sgd}
Wu, L., Ma, C., et~al.
\newblock How sgd selects the global minima in over-parameterized learning: A
  dynamical stability perspective.
\newblock \emph{Advances in Neural Information Processing Systems},
  31:\penalty0 8279--8288, 2018.

\bibitem[Yoccoz()]{Yoccoz}
Yoccoz, J.-C.
\newblock An example of non convergence of birkhoff sums.
\newblock
  \url{https://www.college-de-france.fr/media/jean-christophe-yoccoz/UPL54030_birkhoff.pdf}.

\bibitem[Zhang et~al.(2019)Zhang, He, Sra, and Jadbabaie]{Zhang2019e}
Zhang, J., He, T., Sra, S., and Jadbabaie, A.
\newblock Why gradient clipping accelerates training: A theoretical
  justification for adaptivity.
\newblock May 2019.

\bibitem[Zhang et~al.(2020)Zhang, Lin, Das, Sra, and
  Jadbabaie]{zhang2020stochastic}
Zhang, J., Lin, H., Das, S., Sra, S., and Jadbabaie, A.
\newblock Stochastic optimization with non-stationary noise.
\newblock \emph{arXiv preprint arXiv:2006.04429}, 2020.

\end{thebibliography}
