\begin{thebibliography}{127}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[lar()]{large-scale-kmflow}
\emph{Large-Scale Dynamics and Transition to Turbulence in the Two-Dimensional Kolmogorov Flow}, pages 374--396.
\newblock \doi{10.2514/5.9781600865831.0374.0396}.
\newblock URL \url{https://arc.aiaa.org/doi/abs/10.2514/5.9781600865831.0374.0396}.

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{Neural-Machine-Translation-2014}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and translate, 2014.
\newblock URL \url{https://arxiv.org/abs/1409.0473}.

\bibitem[Bar-Sinai et~al.(2019)Bar-Sinai, Hoyer, Hickey, and Brenner]{bar2019learning}
Yohai Bar-Sinai, Stephan Hoyer, Jason Hickey, and Michael~P Brenner.
\newblock Learning data-driven discretizations for partial differential equations.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0 (31):\penalty0 15344--15349, 2019.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Iz~Beltagy, Matthew~E. Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer, 2020.

\bibitem[Bhattacharya et~al.(2021)Bhattacharya, Hosseini, Kovachki, and Stuart]{bhattacharya2021pcanet}
Kaushik Bhattacharya, Bamdad Hosseini, Nikola~B. Kovachki, and Andrew~M. Stuart.
\newblock Model reduction and neural networks for parametric pdes, 2021.

\bibitem[Bhojanapalli et~al.(2021)Bhojanapalli, Chakrabarti, Jain, Kumar, Lukasik, and Veit]{bhojanapalli2021eigen}
Srinadh Bhojanapalli, Ayan Chakrabarti, Himanshu Jain, Sanjiv Kumar, Michal Lukasik, and Andreas Veit.
\newblock Eigen analysis of self-attention and its reconstruction from partial computation, 2021.

\bibitem[Boull{\'e} et~al.(2022)Boull{\'e}, Kim, Shi, and Townsend]{boulle2022learning}
Nicolas Boull{\'e}, Seick Kim, Tianyi Shi, and Alex Townsend.
\newblock Learning green’s functions associated with time-dependent partial differential equations.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0 (218):\penalty0 1--34, 2022.

\bibitem[Brandstetter et~al.(2022{\natexlab{a}})Brandstetter, Berg, Welling, and Gupta]{brandstetter2022clifford}
Johannes Brandstetter, Rianne van~den Berg, Max Welling, and Jayesh~K Gupta.
\newblock Clifford neural layers for pde modeling.
\newblock \emph{arXiv preprint arXiv:2209.04934}, 2022{\natexlab{a}}.

\bibitem[Brandstetter et~al.(2022{\natexlab{b}})Brandstetter, Welling, and Worrall]{brandstetter2022lipoint}
Johannes Brandstetter, Max Welling, and Daniel~E Worrall.
\newblock Lie point symmetry data augmentation for neural {PDE} solvers.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th International Conference on Machine Learning}, volume 162 of \emph{Proceedings of Machine Learning Research}, pages 2241--2256. PMLR, 17--23 Jul 2022{\natexlab{b}}.
\newblock URL \url{https://proceedings.mlr.press/v162/brandstetter22a.html}.

\bibitem[Brandstetter et~al.(2022{\natexlab{c}})Brandstetter, Worrall, and Welling]{brandstetter2022message}
Johannes Brandstetter, Daniel Worrall, and Max Welling.
\newblock Message passing neural pde solvers.
\newblock \emph{arXiv preprint arXiv:2202.03376}, 2022{\natexlab{c}}.

\bibitem[Brandstetter et~al.(2023{\natexlab{a}})Brandstetter, van~den Berg, Welling, and Gupta]{brandstetter2023clifford}
Johannes Brandstetter, Rianne van~den Berg, Max Welling, and Jayesh~K. Gupta.
\newblock Clifford neural layers for pde modeling, 2023{\natexlab{a}}.

\bibitem[Brandstetter et~al.(2023{\natexlab{b}})Brandstetter, Worrall, and Welling]{brandstetter2023message}
Johannes Brandstetter, Daniel Worrall, and Max Welling.
\newblock Message passing neural pde solvers, 2023{\natexlab{b}}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 1877--1901, 2020.

\bibitem[Cachay et~al.()Cachay, Mitra, Hirasawa, Kim, Hazarika, Hingmire, Rasch, Singh, and Ramea]{cachayclimformer}
Salva~R{\"u}hling Cachay, Peetak Mitra, Haruki Hirasawa, Sookyung Kim, Subhashis Hazarika, Dipti Hingmire, Phil Rasch, Hansi Singh, and Kalai Ramea.
\newblock Climformer--a spherical transformer model for long-term climate projections.

\bibitem[Cai et~al.(2021)Cai, Mao, Wang, Yin, and Karniadakis]{cai2021physics}
Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and George~Em Karniadakis.
\newblock Physics-informed neural networks (pinns) for fluid mechanics: A review.
\newblock \emph{Acta Mechanica Sinica}, 37\penalty0 (12):\penalty0 1727--1738, 2021.

\bibitem[Cao(2021)]{cao2021galerkin}
Shuhao Cao.
\newblock Choose a transformer: Fourier or galerkin.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman Vaughan, editors, \emph{Advances in Neural Information Processing Systems}, volume~34, pages 24924--24940. Curran Associates, Inc., 2021.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2021/file/d0921d442ee91b896ad95059d13df618-Paper.pdf}.

\bibitem[Cao et~al.(2022)Cao, Xu, and Clifton]{cao2022understand}
Shuhao Cao, Peng Xu, and David~A. Clifton.
\newblock How to understand masked autoencoders, 2022.

\bibitem[Carion et~al.(2020)Carion, Massa, Synnaeve, Usunier, Kirillov, and Zagoruyko]{carion2020detr}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers, 2020.

\bibitem[Chandler and Kerswell(2013)]{chandler2013kmflow}
Gary~J. Chandler and Rich~R. Kerswell.
\newblock Invariant recurrent solutions embedded in a turbulent two-dimensional kolmogorov flow.
\newblock \emph{Journal of Fluid Mechanics}, 722:\penalty0 554–595, 2013.
\newblock \doi{10.1017/jfm.2013.122}.

\bibitem[Chattopadhyay et~al.(2020)Chattopadhyay, Mustafa, Hassanzadeh, and Kashinath]{chattopadhyay2020deep}
Ashesh Chattopadhyay, Mustafa Mustafa, Pedram Hassanzadeh, and Karthik Kashinath.
\newblock Deep spatial transformers for autoregressive data-driven forecasting of geophysical turbulence.
\newblock In \emph{Proceedings of the 10th international conference on climate informatics}, pages 106--112, 2020.

\bibitem[Chen and Chen(1995)]{Universal-apprx-operator-IEEE-1995}
Tianping Chen and Hong Chen.
\newblock Universal approximation to nonlinear operators by neural networks with arbitrary activation functions and its application to dynamical systems.
\newblock \emph{IEEE Transactions on Neural Networks}, 6\penalty0 (4):\penalty0 911--917, 1995.
\newblock \doi{10.1109/72.392253}.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and Sutskever]{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers, 2019.

\bibitem[Choromanski et~al.(2022)Choromanski, Likhosherstov, Dohan, Song, Gane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and Weller]{choromanski2022rethinking}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller.
\newblock Rethinking attention with performers, 2022.

\bibitem[Chu and Thuerey(2017)]{Chu2017tempoGAN}
Mengyu Chu and Nils Thuerey.
\newblock Data-driven synthesis of smoke flows with {CNN}-based feature descriptors.
\newblock \emph{{ACM} Transactions on Graphics}, 36\penalty0 (4):\penalty0 1--14, jul 2017.
\newblock \doi{10.1145/3072959.3073643}.
\newblock URL \url{https://doi.org/10.1145%2F3072959.3073643}.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dong et~al.(2021)Dong, Cordonnier, and Loukas]{dong2021attention}
Yihe Dong, Jean-Baptiste Cordonnier, and Andreas Loukas.
\newblock Attention is not all you need: Pure attention loses rank doubly exponentially with depth, 2021.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Dresdner et~al.(2022)Dresdner, Kochkov, Norgaard, Zepeda-Núñez, Smith, Brenner, and Hoyer]{dresdner2022correctspectral}
Gideon Dresdner, Dmitrii Kochkov, Peter Norgaard, Leonardo Zepeda-Núñez, Jamie~A. Smith, Michael~P. Brenner, and Stephan Hoyer.
\newblock Learning to correct spectral methods for simulating turbulent flows, 2022.

\bibitem[Fonseca et~al.(2023)Fonseca, Zappala, Caro, and van Dijk]{fonseca2023continuous}
Antonio H de~O Fonseca, Emanuele Zappala, Josue~Ortega Caro, and David van Dijk.
\newblock Continuous spatiotemporal transformers.
\newblock \emph{arXiv preprint arXiv:2301.13338}, 2023.

\bibitem[Fukami et~al.(2019)Fukami, Fukagata, and Taira]{fukami2019superres}
Kai Fukami, Koji Fukagata, and Kunihiko Taira.
\newblock Super-resolution reconstruction of turbulent flows with machine learning.
\newblock \emph{Journal of Fluid Mechanics}, 870:\penalty0 106–120, 2019.
\newblock \doi{10.1017/jfm.2019.238}.

\bibitem[Gao et~al.(2022)Gao, Shi, Wang, Zhu, Wang, Li, and Yeung]{gao2022earthformer}
Zhihan Gao, Xingjian Shi, Hao Wang, Yi~Zhu, Yuyang~Bernie Wang, Mu~Li, and Dit-Yan Yeung.
\newblock Earthformer: Exploring space-time transformers for earth system forecasting.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25390--25403, 2022.

\bibitem[Geneva and Zabaras(2022)]{geneva2022transformers}
Nicholas Geneva and Nicholas Zabaras.
\newblock Transformers for modeling physical systems.
\newblock \emph{Neural Networks}, 146:\penalty0 272--289, 2022.

\bibitem[Graves et~al.(2014)Graves, Wayne, and Danihelka]{neural-turing-machine-2014}
Alex Graves, Greg Wayne, and Ivo Danihelka.
\newblock Neural turing machines, 2014.
\newblock URL \url{https://arxiv.org/abs/1410.5401}.

\bibitem[Guibas et~al.(2021)Guibas, Mardani, Li, Tao, Anandkumar, and Catanzaro]{guibas2021adaptive}
John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and Bryan Catanzaro.
\newblock Adaptive fourier neural operators: Efficient token mixers for transformers.
\newblock \emph{arXiv preprint arXiv:2111.13587}, 2021.

\bibitem[Guo et~al.(2022)Guo, Cao, and Chen]{guo2022transformer}
Ruchi Guo, Shuhao Cao, and Long Chen.
\newblock Transformer meets boundary value inverse problems.
\newblock \emph{arXiv preprint arXiv:2209.14977}, 2022.

\bibitem[Gupta et~al.(2021)Gupta, Xiao, and Bogdan]{gupta2021multiwaveletbased}
Gaurav Gupta, Xiongye Xiao, and Paul Bogdan.
\newblock Multiwavelet-based operator learning for differential equations, 2021.

\bibitem[Gupta and Brandstetter(2022{\natexlab{a}})]{gupta2022multispatiotemporalscale}
Jayesh~K. Gupta and Johannes Brandstetter.
\newblock Towards multi-spatiotemporal-scale generalized pde modeling, 2022{\natexlab{a}}.

\bibitem[Gupta and Brandstetter(2022{\natexlab{b}})]{gupta2022towards}
Jayesh~K Gupta and Johannes Brandstetter.
\newblock Towards multi-spatiotemporal-scale generalized pde modeling.
\newblock \emph{arXiv preprint arXiv:2209.15616}, 2022{\natexlab{b}}.

\bibitem[Halko et~al.(2010)Halko, Martinsson, and Tropp]{halko2010finding}
Nathan Halko, Per-Gunnar Martinsson, and Joel~A. Tropp.
\newblock Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions, 2010.

\bibitem[Han et~al.(2018)Han, Jentzen, and E]{han2018solving}
Jiequn Han, Arnulf Jentzen, and Weinan E.
\newblock Solving high-dimensional partial differential equations using deep learning.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0 (34):\penalty0 8505--8510, 2018.

\bibitem[Han et~al.(2022)Han, Gao, Pfaff, Wang, and Liu]{han2022predicting}
Xu~Han, Han Gao, Tobias Pfaff, Jian-Xun Wang, and Li-Ping Liu.
\newblock Predicting physics in mesh-reduced space with temporal attention.
\newblock \emph{arXiv preprint arXiv:2201.09113}, 2022.

\bibitem[Hao et~al.(2023{\natexlab{a}})Hao, Liu, Zhang, Ying, Feng, Su, and Zhu]{hao2023physicsinformed}
Zhongkai Hao, Songming Liu, Yichi Zhang, Chengyang Ying, Yao Feng, Hang Su, and Jun Zhu.
\newblock Physics-informed machine learning: A survey on problems, methods and applications, 2023{\natexlab{a}}.

\bibitem[Hao et~al.(2023{\natexlab{b}})Hao, Ying, Wang, Su, Dong, Liu, Cheng, Zhu, and Song]{hao2023gnot}
Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze~Cheng, Jun Zhu, and Jian Song.
\newblock Gnot: A general neural operator transformer for operator learning.
\newblock \emph{arXiv preprint arXiv:2302.14376}, 2023{\natexlab{b}}.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016Residual}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, June 2016.

\bibitem[Ho et~al.(2020)Ho, Kalchbrenner, Weissenborn, and Salimans]{ho2020axial}
Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans.
\newblock Axial attention in multidimensional transformers, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1e5GJBtDr}.

\bibitem[Holl et~al.()Holl, Koltun, and Um]{holl2020phiflow}
Philipp Holl, Vladlen Koltun, and Kiwon Um.
\newblock phiflow: A differentiable pde solving framework for deep learning via physical simulations.

\bibitem[Jagtap and Karniadakis(2020)]{jagtap2020extended}
Ameya~D Jagtap and George~Em Karniadakis.
\newblock Extended physics-informed neural networks (xpinns): A generalized space-time domain decomposition based deep learning framework for nonlinear partial differential equations.
\newblock \emph{Communications in Computational Physics}, 28\penalty0 (5):\penalty0 2002--2041, 2020.

\bibitem[JANNY et~al.(2023)JANNY, B{\'e}n{\'e}teau, Nadri, Digne, THOME, and Wolf]{janny2023eagle}
Steeven JANNY, Aur{\'e}lien B{\'e}n{\'e}teau, Madiha Nadri, Julie Digne, Nicolas THOME, and Christian Wolf.
\newblock {EAGLE}: Large-scale learning of turbulent fluid dynamics with mesh transformers.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023.
\newblock URL \url{https://openreview.net/forum?id=mfIX4QpsARJ}.

\bibitem[Jiang et~al.(2020)Jiang, Esmaeilzadeh, Azizzadenesheli, Kashinath, Mustafa, Tchelepi, Marcus, Prabhat, and Anandkumar]{meshfreeflownet}
Chiyu~“Max” Jiang, Soheil Esmaeilzadeh, Kamyar Azizzadenesheli, Karthik Kashinath, Mustafa Mustafa, Hamdi~A. Tchelepi, Philip Marcus, Mr~Prabhat, and Anima Anandkumar.
\newblock Meshfreeflownet: A physics-constrained deep continuous space-time super-resolution framework.
\newblock In \emph{SC20: International Conference for High Performance Computing, Networking, Storage and Analysis}, pages 1--15, 2020.
\newblock \doi{10.1109/SC41405.2020.00013}.

\bibitem[Jin et~al.(2022)Jin, Meng, and Lu]{jin2022mionet}
Pengzhan Jin, Shuai Meng, and Lu~Lu.
\newblock Mionet: Learning multiple-input operators via tensor product.
\newblock \emph{SIAM Journal on Scientific Computing}, 44\penalty0 (6):\penalty0 A3490--A3514, 2022.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov, Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko, et~al.]{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Karniadakis et~al.(2021)Karniadakis, Kevrekidis, Lu, Perdikaris, Wang, and Yang]{karniadakis2021physics}
George~Em Karniadakis, Ioannis~G Kevrekidis, Lu~Lu, Paris Perdikaris, Sifan Wang, and Liu Yang.
\newblock Physics-informed machine learning.
\newblock \emph{Nature Reviews Physics}, 3\penalty0 (6):\penalty0 422--440, 2021.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and Fleuret]{transformer-rnn}
Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran{\c{c}}ois Fleuret.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear attention.
\newblock In Hal~Daumé III and Aarti Singh, editors, \emph{Proceedings of the 37th International Conference on Machine Learning}, volume 119 of \emph{Proceedings of Machine Learning Research}, pages 5156--5165. PMLR, 13--18 Jul 2020.
\newblock URL \url{https://proceedings.mlr.press/v119/katharopoulos20a.html}.

\bibitem[Kissas et~al.(2022)Kissas, Seidman, Guilhoto, Preciado, Pappas, and Perdikaris]{kissas2022learning}
Georgios Kissas, Jacob Seidman, Leonardo~Ferreira Guilhoto, Victor~M. Preciado, George~J. Pappas, and Paris Perdikaris.
\newblock Learning operators with coupled attention, 2022.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{Kitaev2020Reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=rkgNKkHtvB}.

\bibitem[Kochkov et~al.(2021)Kochkov, Smith, Alieva, Wang, Brenner, and Hoyer]{Kochkov2021mlcfd}
Dmitrii Kochkov, Jamie~A. Smith, Ayya Alieva, Qing Wang, Michael~P. Brenner, and Stephan Hoyer.
\newblock Machine learning{\textendash}accelerated computational fluid dynamics.
\newblock \emph{Proceedings of the National Academy of Sciences}, 118\penalty0 (21), may 2021.
\newblock \doi{10.1073/pnas.2101784118}.
\newblock URL \url{https://doi.org/10.1073%2Fpnas.2101784118}.

\bibitem[Kolda and Bader(2009)]{tensor-decomp-review}
Tamara~G. Kolda and Brett~W. Bader.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM Rev.}, 51\penalty0 (3):\penalty0 455–500, aug 2009.
\newblock ISSN 0036-1445.
\newblock \doi{10.1137/07070111X}.
\newblock URL \url{https://doi.org/10.1137/07070111X}.

\bibitem[Kossaifi et~al.(2023)Kossaifi, Kovachki, Azizzadenesheli, and Anandkumar]{kossaifi2023multigrid}
Jean Kossaifi, Nikola~Borislavov Kovachki, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock Multi-grid tensorized fourier neural operator for high resolution {PDE}s, 2023.
\newblock URL \url{https://openreview.net/forum?id=po-oqRst4Xm}.

\bibitem[Kovachki et~al.(2021{\natexlab{a}})Kovachki, Lanthaler, and Mishra]{kovachki2021universal}
Nikola Kovachki, Samuel Lanthaler, and Siddhartha Mishra.
\newblock On universal approximation and error bounds for fourier neural operators, 2021{\natexlab{a}}.

\bibitem[Kovachki et~al.(2021{\natexlab{b}})Kovachki, Li, Liu, Azizzadenesheli, Bhattacharya, Stuart, and Anandkumar]{kovachki2021neural}
Nikola Kovachki, Zongyi Li, Burigede Liu, Kamyar Azizzadenesheli, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Neural operator: Learning maps between function spaces.
\newblock \emph{arXiv preprint arXiv:2108.08481}, 2021{\natexlab{b}}.

\bibitem[Lam et~al.(2022)Lam, Sanchez-Gonzalez, Willson, Wirnsberger, Fortunato, Pritzel, Ravuri, Ewalds, Alet, Eaton-Rosen, Hu, Merose, Hoyer, Holland, Stott, Vinyals, Mohamed, and Battaglia]{lam2022graphcast}
Remi Lam, Alvaro Sanchez-Gonzalez, Matthew Willson, Peter Wirnsberger, Meire Fortunato, Alexander Pritzel, Suman Ravuri, Timo Ewalds, Ferran Alet, Zach Eaton-Rosen, Weihua Hu, Alexander Merose, Stephan Hoyer, George Holland, Jacklynn Stott, Oriol Vinyals, Shakir Mohamed, and Peter Battaglia.
\newblock Graphcast: Learning skillful medium-range global weather forecasting, 2022.

\bibitem[Lamorgese et~al.(2004)Lamorgese, Caughey, and Pope]{3dturb-dns}
A.~G. Lamorgese, D.~A. Caughey, and S.~B. Pope.
\newblock {Direct numerical simulation of homogeneous turbulence with hyperviscosity}.
\newblock \emph{Physics of Fluids}, 17\penalty0 (1), 12 2004.
\newblock ISSN 1070-6631.
\newblock \doi{10.1063/1.1833415}.
\newblock URL \url{https://doi.org/10.1063/1.1833415}.
\newblock 015106.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and Lempitsky]{lebedev2015speedingup}
Vadim Lebedev, Yaroslav Ganin, Maksim Rakhuba, Ivan Oseledets, and Victor Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned cp-decomposition, 2015.

\bibitem[Li et~al.(2019)Li, Wu, Tedrake, Tenenbaum, and Torralba]{li2019dpi}
Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua~B. Tenenbaum, and Antonio Torralba.
\newblock Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids, 2019.

\bibitem[Li and Farimani(2022)]{li2022fgn}
Zijie Li and Amir~Barati Farimani.
\newblock Graph neural network-accelerated lagrangian fluid simulation.
\newblock \emph{Computers \& Graphics}, 103:\penalty0 201--211, 2022.
\newblock ISSN 0097-8493.
\newblock \doi{https://doi.org/10.1016/j.cag.2022.02.004}.
\newblock URL \url{https://www.sciencedirect.com/science/article/pii/S0097849322000206}.

\bibitem[Li et~al.(2022)Li, Li, and Farimani]{li2022tpugan}
Zijie Li, Tianqin Li, and Amir~Barati Farimani.
\newblock {TPU}-{GAN}: Learning temporal coherence from dynamic point cloud sequences.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=FEBFJ98FKx}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Meidani, and Farimani]{li2023transformer}
Zijie Li, Kazem Meidani, and Amir~Barati Farimani.
\newblock Transformer for partial differential equations{\textquoteright} operator learning.
\newblock \emph{Transactions on Machine Learning Research}, 2023{\natexlab{a}}.
\newblock ISSN 2835-8856.
\newblock URL \url{https://openreview.net/forum?id=EPPqt3uERT}.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{li2020fourier}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Fourier neural operator for parametric partial differential equations.
\newblock \emph{arXiv preprint arXiv:2010.08895}, 2020{\natexlab{a}}.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{li2020gno}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Neural operator: Graph kernel network for partial differential equations, 2020{\natexlab{b}}.

\bibitem[Li et~al.(2020{\natexlab{c}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{li2020multipole}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Multipole graph neural operator for parametric partial differential equations, 2020{\natexlab{c}}.

\bibitem[Li et~al.(2020{\natexlab{d}})Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar]{li2020neural}
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar.
\newblock Neural operator: Graph kernel network for partial differential equations.
\newblock \emph{arXiv preprint arXiv:2003.03485}, 2020{\natexlab{d}}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Zheng, Kovachki, Jin, Chen, Liu, Azizzadenesheli, and Anandkumar]{li2023physicsinformed}
Zongyi Li, Hongkai Zheng, Nikola Kovachki, David Jin, Haoxuan Chen, Burigede Liu, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock Physics-informed neural operator for learning partial differential equations, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Xu, and Zhang]{liu2022ht}
Xinliang Liu, Bo~Xu, and Lei Zhang.
\newblock Ht-net: Hierarchical transformer based operator learning model for multiscale pdes.
\newblock \emph{arXiv preprint arXiv:2210.10890}, 2022.

\bibitem[Liu et~al.(2023)Liu, Xu, and Zhang]{liu2023mitigating}
Xinliang Liu, Bo~Xu, and Lei Zhang.
\newblock Mitigating spectral bias for the multiscale operator learning with hierarchical attention, 2023.

\bibitem[L{\"o}tzsch et~al.(2022)L{\"o}tzsch, Ohler, and Otterbach]{lotzsch2022learning}
Winfried L{\"o}tzsch, Simon Ohler, and Johannes Otterbach.
\newblock Learning the solution operator of boundary value problems using graph neural networks.
\newblock In \emph{ICML 2022 2nd AI for Science Workshop}, 2022.
\newblock URL \url{https://openreview.net/forum?id=4vx9FQA7wiC}.

\bibitem[Lu et~al.(2019)Lu, Jin, and Karniadakis]{lu2019deeponet}
Lu~Lu, Pengzhan Jin, and George~Em Karniadakis.
\newblock Deeponet: Learning nonlinear operators for identifying differential equations based on the universal approximation theorem of operators.
\newblock \emph{arXiv preprint arXiv:1910.03193}, 2019.

\bibitem[Lu et~al.(2021)Lu, Meng, Mao, and Karniadakis]{lu2021deepxde}
Lu~Lu, Xuhui Meng, Zhiping Mao, and George~Em Karniadakis.
\newblock Deepxde: A deep learning library for solving differential equations.
\newblock \emph{SIAM review}, 63\penalty0 (1):\penalty0 208--228, 2021.

\bibitem[Lu et~al.(2022)Lu, Meng, Cai, Mao, Goswami, Zhang, and Karniadakis]{Lu2022fair}
Lu~Lu, Xuhui Meng, Shengze Cai, Zhiping Mao, Somdatta Goswami, Zhongqiang Zhang, and George~Em Karniadakis.
\newblock A comprehensive and fair comparison of two neural operators (with practical extensions) based on {FAIR} data.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 393:\penalty0 114778, apr 2022.
\newblock \doi{10.1016/j.cma.2022.114778}.
\newblock URL \url{https://doi.org/10.1016%2Fj.cma.2022.114778}.

\bibitem[Luong et~al.(2015)Luong, Pham, and Manning]{attention-based-nmt-2015}
Minh-Thang Luong, Hieu Pham, and Christopher~D. Manning.
\newblock Effective approaches to attention-based neural machine translation, 2015.
\newblock URL \url{https://arxiv.org/abs/1508.04025}.

\bibitem[Ma et~al.(2019)Ma, Zhang, Zhang, Duan, Hou, Song, and Zhou]{ma2019tensorized}
Xindian Ma, Peng Zhang, Shuai Zhang, Nan Duan, Yuexian Hou, Dawei Song, and Ming Zhou.
\newblock A tensorized transformer for language modeling, 2019.

\bibitem[Mortensen and Langtangen(2016)]{Mortensen2016spectralDNS}
Mikael Mortensen and Hans~Petter Langtangen.
\newblock High performance python for direct numerical simulations of turbulent flows.
\newblock \emph{Computer Physics Communications}, 203:\penalty0 53--65, jun 2016.
\newblock \doi{10.1016/j.cpc.2016.02.005}.
\newblock URL \url{https://doi.org/10.1016%2Fj.cpc.2016.02.005}.

\bibitem[Nguyen et~al.(2022)Nguyen, Pham, Nguyen, Nguyen, Osher, and Ho]{nguyen2022fourierformer}
Tan Nguyen, Minh Pham, Tam Nguyen, Khai Nguyen, Stanley Osher, and Nhat Ho.
\newblock Fourierformer: Transformer meets generalized fourier integral theorem.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 29319--29335, 2022.

\bibitem[Nguyen et~al.(2023)Nguyen, Brandstetter, Kapoor, Gupta, and Grover]{nguyen2023climax}
Tung Nguyen, Johannes Brandstetter, Ashish Kapoor, Jayesh~K. Gupta, and Aditya Grover.
\newblock Climax: A foundation model for weather and climate, 2023.

\bibitem[Novikov et~al.(2015)Novikov, Podoprikhin, Osokin, and Vetrov]{novikov2015tensorizing}
Alexander Novikov, Dmitry Podoprikhin, Anton Osokin, and Dmitry Vetrov.
\newblock Tensorizing neural networks, 2015.

\bibitem[Oseledets(2011)]{tensor-train-decomposition}
I.~V. Oseledets.
\newblock Tensor-train decomposition.
\newblock \emph{SIAM Journal on Scientific Computing}, 33\penalty0 (5):\penalty0 2295--2317, 2011.
\newblock \doi{10.1137/090752286}.
\newblock URL \url{https://doi.org/10.1137/090752286}.

\bibitem[Ovadia et~al.(2023)Ovadia, Kahana, Stinis, Turkel, and Karniadakis]{ovadia2023vito}
Oded Ovadia, Adar Kahana, Panos Stinis, Eli Turkel, and George~Em Karniadakis.
\newblock Vito: Vision transformer-operator.
\newblock \emph{arXiv preprint arXiv:2303.08891}, 2023.

\bibitem[Pang et~al.(2019)Pang, Lu, and Karniadakis]{pang2019fpinns}
Guofei Pang, Lu~Lu, and George~Em Karniadakis.
\newblock fpinns: Fractional physics-informed neural networks.
\newblock \emph{SIAM Journal on Scientific Computing}, 41\penalty0 (4):\penalty0 A2603--A2626, 2019.

\bibitem[Pant et~al.(2021)Pant, Doshi, Bahl, and Farimani]{Pant2021dlrom}
Pranshu Pant, Ruchit Doshi, Pranav Bahl, and Amir~Barati Farimani.
\newblock Deep learning for reduced order modelling and efficient temporal evolution of fluid simulations.
\newblock \emph{Physics of Fluids}, 33\penalty0 (10):\penalty0 107101, oct 2021.
\newblock \doi{10.1063/5.0062546}.
\newblock URL \url{https://doi.org/10.1063%2F5.0062546}.

\bibitem[Pathak et~al.(2020)Pathak, Mustafa, Kashinath, Motheau, Kurth, and Day]{pathak2020using}
Jaideep Pathak, Mustafa Mustafa, Karthik Kashinath, Emmanuel Motheau, Thorsten Kurth, and Marcus Day.
\newblock Using machine learning to augment coarse-grid computational fluid dynamics simulations.
\newblock \emph{arXiv preprint arXiv:2010.00072}, 2020.

\bibitem[Pathak et~al.(2022)Pathak, Subramanian, Harrington, Raja, Chattopadhyay, Mardani, Kurth, Hall, Li, Azizzadenesheli, Hassanzadeh, Kashinath, and Anandkumar]{pathak2022fourcastnet}
Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, Pedram Hassanzadeh, Karthik Kashinath, and Animashree Anandkumar.
\newblock Fourcastnet: A global data-driven high-resolution weather model using adaptive fourier neural operators, 2022.

\bibitem[Pfaff et~al.(2021)Pfaff, Fortunato, Sanchez-Gonzalez, and Battaglia]{pfaff2021learning}
Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez, and Peter~W. Battaglia.
\newblock Learning mesh-based simulation with graph networks, 2021.

\bibitem[Prantl et~al.(2022)Prantl, Ummenhofer, Koltun, and Thuerey]{prantl2022guaranteed}
Lukas Prantl, Benjamin Ummenhofer, Vladlen Koltun, and Nils Thuerey.
\newblock Guaranteed conservation of momentum for learning particle-based fluid dynamics, 2022.

\bibitem[Quarteroni and Valli(1999)]{quarteroni1999domain}
Alfio Quarteroni and Alberto Valli.
\newblock \emph{Domain decomposition methods for partial differential equations}.
\newblock Number BOOK. Oxford University Press, 1999.

\bibitem[Rahimi and Recht(2007)]{rff2007}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In J.~Platt, D.~Koller, Y.~Singer, and S.~Roweis, editors, \emph{Advances in Neural Information Processing Systems}, volume~20. Curran Associates, Inc., 2007.
\newblock URL \url{https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf}.

\bibitem[Rahman et~al.(2023)Rahman, Ross, and Azizzadenesheli]{rahman2023uno}
Md~Ashiqur Rahman, Zachary~E. Ross, and Kamyar Azizzadenesheli.
\newblock U-no: U-shaped neural operators, 2023.

\bibitem[Raissi et~al.(2019)Raissi, Perdikaris, and Karniadakis]{raissi2019physics}
Maziar Raissi, Paris Perdikaris, and George~E Karniadakis.
\newblock Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations.
\newblock \emph{Journal of Computational physics}, 378:\penalty0 686--707, 2019.

\bibitem[Rasp et~al.(2020)Rasp, Dueben, Scher, Weyn, Mouatadid, and Thuerey]{Rasp2020weatherbench}
Stephan Rasp, Peter~D. Dueben, Sebastian Scher, Jonathan~A. Weyn, Soukayna Mouatadid, and Nils Thuerey.
\newblock {WeatherBench}: A benchmark data set for data-driven weather forecasting.
\newblock \emph{Journal of Advances in Modeling Earth Systems}, 12\penalty0 (11), nov 2020.
\newblock \doi{10.1029/2020ms002203}.
\newblock URL \url{https://doi.org/10.1029%2F2020ms002203}.

\bibitem[Rogallo(1981)]{Rogallo1981NumericalEI}
Robert~S. Rogallo.
\newblock Numerical experiments in homogeneous turbulence.
\newblock 1981.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022highresolution}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
\newblock High-resolution image synthesis with latent diffusion models, 2022.

\bibitem[Sanchez-Gonzalez et~al.(2020)Sanchez-Gonzalez, Godwin, Pfaff, Ying, Leskovec, and Battaglia]{sanchezgonzalez2020learning}
Alvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter~W. Battaglia.
\newblock Learning to simulate complex physics with graph networks, 2020.

\bibitem[Shen et~al.(2020)Shen, Zhang, Zhao, Yi, and Li]{shen2020efficient}
Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li.
\newblock Efficient attention: Attention with linear complexities, 2020.

\bibitem[Shu et~al.(2023)Shu, Li, and Farimani]{shu2023physics}
Dule Shu, Zijie Li, and Amir~Barati Farimani.
\newblock A physics-informed diffusion model for high-fidelity flow field reconstruction.
\newblock \emph{Journal of Computational Physics}, 478:\penalty0 111972, 2023.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot, et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Stachenfeld et~al.(2022)Stachenfeld, Fielding, Kochkov, Cranmer, Pfaff, Godwin, Cui, Ho, Battaglia, and Sanchez-Gonzalez]{stachenfeld2022learned}
Kimberly Stachenfeld, Drummond~B. Fielding, Dmitrii Kochkov, Miles Cranmer, Tobias Pfaff, Jonathan Godwin, Can Cui, Shirley Ho, Peter Battaglia, and Alvaro Sanchez-Gonzalez.
\newblock Learned coarse models for efficient turbulence simulation, 2022.

\bibitem[Su et~al.(2022)Su, Lu, Pan, Murtadha, Wen, and Liu]{su2022roformer}
Jianlin Su, Yu~Lu, Shengfeng Pan, Ahmed Murtadha, Bo~Wen, and Yunfeng Liu.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2022.

\bibitem[Sun et~al.(2020)Sun, Gao, Pan, and Wang]{sun2020surrogate}
Luning Sun, Han Gao, Shaowu Pan, and Jian-Xun Wang.
\newblock Surrogate modeling for fluid flows based on physics-constrained deep learning without simulation data.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 361:\penalty0 112732, 2020.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil, Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Matthew Tancik, Pratul~P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ramamoorthi, Jonathan~T. Barron, and Ren Ng.
\newblock Fourier features let networks learn high frequency functions in low dimensional domains, 2020.

\bibitem[Tang et~al.(2022)Tang, Azevedo, Cordonnier, and Solenthaler]{tang2022neural}
Jingwei Tang, Vinicius~C Azevedo, Guillaume Cordonnier, and Barbara Solenthaler.
\newblock Neural green’s function for laplacian systems.
\newblock \emph{Computers \& Graphics}, 107:\penalty0 186--196, 2022.

\bibitem[Thuerey et~al.(2020)Thuerey, Wei{\ss}enow, Prantl, and Hu]{thuerey2020deep}
Nils Thuerey, Konstantin Wei{\ss}enow, Lukas Prantl, and Xiangyu Hu.
\newblock Deep learning methods for reynolds-averaged navier--stokes simulations of airfoil flows.
\newblock \emph{AIAA Journal}, 58\penalty0 (1):\penalty0 25--36, 2020.

\bibitem[Tran et~al.(2023)Tran, Mathews, Xie, and Ong]{tran2023factfno}
Alasdair Tran, Alexander Mathews, Lexing Xie, and Cheng~Soon Ong.
\newblock Factorized fourier neural operators, 2023.

\bibitem[Tsai et~al.(2019)Tsai, Bai, Yamada, Morency, and Salakhutdinov]{tsai2019transformer}
Yao-Hung~Hubert Tsai, Shaojie Bai, Makoto Yamada, Louis-Philippe Morency, and Ruslan Salakhutdinov.
\newblock Transformer dissection: A unified understanding of transformer's attention via the lens of kernel, 2019.

\bibitem[Ulyanov et~al.(2017)Ulyanov, Vedaldi, and Lempitsky]{ulyanov2017instance}
Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization, 2017.

\bibitem[Um et~al.(2020)Um, Brand, Fei, Holl, and Thuerey]{um2020solver}
Kiwon Um, Robert Brand, Yun~Raymond Fei, Philipp Holl, and Nils Thuerey.
\newblock Solver-in-the-loop: Learning from differentiable physics to interact with iterative pde-solvers.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 6111--6122, 2020.

\bibitem[Ummenhofer et~al.(2020)Ummenhofer, Prantl, Thuerey, and Koltun]{Ummenhofer2020Lagrangian}
Benjamin Ummenhofer, Lukas Prantl, Nils Thuerey, and Vladlen Koltun.
\newblock Lagrangian fluid simulation with continuous convolutions.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=B1lDoJSYDH}.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{Attention-NIPS-2017}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, \L~ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In I.~Guyon, U.~Von Luxburg, S.~Bengio, H.~Wallach, R.~Fergus, S.~Vishwanathan, and R.~Garnett, editors, \emph{Advances in Neural Information Processing Systems}, volume~30. Curran Associates, Inc., 2017.
\newblock URL \url{https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf}.

\bibitem[Wang et~al.(2020{\natexlab{a}})Wang, Kashinath, Mustafa, Albert, and Yu]{fluidunet-kdd2020}
Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert, and Rose Yu.
\newblock Towards physics-informed deep learning for turbulent flow prediction.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, KDD '20, page 1457–1466, New York, NY, USA, 2020{\natexlab{a}}. Association for Computing Machinery.
\newblock ISBN 9781450379984.
\newblock \doi{10.1145/3394486.3403198}.
\newblock URL \url{https://doi.org/10.1145/3394486.3403198}.

\bibitem[Wang et~al.(2021{\natexlab{a}})Wang, Wang, and Perdikaris]{pi-deeponet}
Sifan Wang, Hanwen Wang, and Paris Perdikaris.
\newblock Learning the solution operator of parametric partial differential equations with physics-informed deeponets.
\newblock \emph{Science Advances}, 7\penalty0 (40):\penalty0 eabi8605, 2021{\natexlab{a}}.
\newblock \doi{10.1126/sciadv.abi8605}.
\newblock URL \url{https://www.science.org/doi/abs/10.1126/sciadv.abi8605}.

\bibitem[Wang et~al.(2021{\natexlab{b}})Wang, Wang, and Perdikaris]{wang2021learning}
Sifan Wang, Hanwen Wang, and Paris Perdikaris.
\newblock Learning the solution operator of parametric partial differential equations with physics-informed deeponets, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Sankaran, and Perdikaris]{wang2022respecting}
Sifan Wang, Shyam Sankaran, and Paris Perdikaris.
\newblock Respecting causality is all you need for training physics-informed neural networks, 2022.

\bibitem[Wang et~al.(2020{\natexlab{b}})Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Sinong Wang, Belinda~Z Li, Madian Khabsa, Han Fang, and Hao Ma.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020{\natexlab{b}}.

\bibitem[Wen et~al.(2022)Wen, Li, Azizzadenesheli, Anandkumar, and Benson]{wen2022ufno}
Gege Wen, Zongyi Li, Kamyar Azizzadenesheli, Anima Anandkumar, and Sally~M Benson.
\newblock U-fno—an enhanced fourier neural operator-based deep-learning model for multiphase flow.
\newblock \emph{Advances in Water Resources}, 163:\penalty0 104180, 2022.

\bibitem[Wright and Gonzalez(2021)]{wright2021transformers}
Matthew~A Wright and Joseph~E Gonzalez.
\newblock Transformers are deep infinite-dimensional non-mercer binary kernel machines.
\newblock \emph{arXiv preprint arXiv:2106.01506}, 2021.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and Singh]{xiong2021nystromformer}
Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh.
\newblock Nystr\"omformer: A nystr\"om-based algorithm for approximating self-attention, 2021.

\bibitem[Yang et~al.(2017)Yang, Krompass, and Tresp]{yang2017tensorrnn}
Yinchong Yang, Denis Krompass, and Volker Tresp.
\newblock Tensor-train recurrent neural networks for video classification, 2017.

\bibitem[Yu and Koltun(2016)]{yu2016multiscale}
Fisher Yu and Vladlen Koltun.
\newblock Multi-scale context aggregation by dilated convolutions, 2016.

\bibitem[Zaheer et~al.(2021)Zaheer, Guruganesh, Dubey, Ainslie, Alberti, Ontanon, Pham, Ravula, Wang, Yang, and Ahmed]{zaheer2021bigbird}
Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li~Yang, and Amr Ahmed.
\newblock Big bird: Transformers for longer sequences, 2021.

\bibitem[Zhu et~al.(2023)Zhu, Zhang, Jiao, Karniadakis, and Lu]{zhu2023reliable}
Min Zhu, Handi Zhang, Anran Jiao, George~Em Karniadakis, and Lu~Lu.
\newblock Reliable extrapolation of deep neural operators informed by physics or sparse observations.
\newblock \emph{Computer Methods in Applied Mechanics and Engineering}, 412:\penalty0 116064, 2023.

\end{thebibliography}
