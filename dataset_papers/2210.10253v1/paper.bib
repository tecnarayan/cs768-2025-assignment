@inproceedings{du2022glam,
  title={Glam: Efficient scaling of language models with mixture-of-experts},
  author={Du, Nan and Huang, Yanping and Dai, Andrew M and Tong, Simon and Lepikhin, Dmitry and Xu, Yuanzhong and Krikun, Maxim and Zhou, Yanqi and Yu, Adams Wei and Firat, Orhan and others},
  booktitle={International Conference on Machine Learning},
  pages={5547--5569},
  year={2022},
  organization={PMLR}
}
@article{kovanic1979pseudoinverse,
  title={On the pseudoinverse of a sum of symmetric matrices with applications to estimation},
  author={Kovanic, Pavel},
  journal={Kybernetika},
  volume={15},
  number={5},
  pages={341--348},
  year={1979},
  publisher={Institute of Information Theory and Automation AS CR}
}

@article{desoer1963note,
  title={A note on pseudoinverses},
  author={Desoer, Charles A and Whalen, Barry H},
  journal={Journal of the Society for Industrial and Applied Mathematics},
  volume={11},
  number={2},
  pages={442--447},
  year={1963},
  publisher={SIAM}
}
@article{bartlett2020benign,
  title={Benign overfitting in linear regression},
  author={Bartlett, Peter L and Long, Philip M and Lugosi, G{\'a}bor and Tsigler, Alexander},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={48},
  pages={30063--30070},
  year={2020},
  publisher={National Acad Sciences}
}
@inproceedings{
fgsm,
title={Explaining and harnessing adversarial examples},
author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
booktitle={International Conference on Learning Representations},
year={2015},
}

@article{switch,
  author  = {William Fedus and Barret Zoph and Noam Shazeer},
  title   = {Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {120},
  pages   = {1-39},
  url     = {http://jmlr.org/papers/v23/21-0998.html}
}
@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}
@article{bubeck2021universal,
  title={A universal law of robustness via isoperimetry},
  author={Bubeck, S{\'e}bastien and Sellke, Mark},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{zhang2021understanding,
  title={Understanding deep learning (still) requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={Communications of the ACM},
  volume={64},
  number={3},
  pages={107--115},
  year={2021},
  publisher={ACM New York, NY, USA}
}
@article{yun2019small,
  title={Small ReLU networks are powerful memorizers: a tight analysis of memorization capacity},
  author={Yun, Chulhee and Sra, Suvrit and Jadbabaie, Ali},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={15558--15569},
  year={2019}
}
@inproceedings{neyshabur2018pac,
  title={A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},
  author={Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE Conference on Computer Vision and Pattern Recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@article{bhojanapalli2021eigen,
  title={Eigen Analysis of Self-Attention and its Reconstruction from Partial Computation},
  author={Bhojanapalli, Srinadh and Chakrabarti, Ayan and Jain, Himanshu and Kumar, Sanjiv and Lukasik, Michal and Veit, Andreas},
  journal={arXiv preprint arXiv:2106.08823},
  year={2021}
}
@article{vershynin2010introduction,
  title={Introduction to the non-asymptotic analysis of random matrices},
  author={Vershynin, Roman},
  journal={arXiv preprint arXiv:1011.3027},
  year={2010}
}
@inproceedings{rej2018findings,
  title={Findings of the 2018 conference on machine translation (wmt18)},
  author={Bojar, Ond{\v{r}}ej and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Huck, Matthias and Koehn, Philipp and Monz, Christof},
  booktitle={Proceedings of the Third Conference on Machine Translation},
  volume={2},
  pages={272--307},
  year={2018}
}
@inproceedings{post2018call,
  title={A Call for Clarity in Reporting BLEU Scores},
  author={Post, Matt},
  booktitle={Proceedings of the Third Conference on Machine Translation: Research Papers},
  pages={186--191},
  year={2018}
}
@inproceedings{NEURIPS2020_ff4dfdf5,
 author = {Levine, Yoav and Wies, Noam and Sharir, Or and Bata, Hofit and Shashua, Amnon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {22640--22651},
 publisher = {Curran Associates, Inc.},
 title = {Limits to Depth Efficiencies of Self-Attention},
 url = {https://proceedings.neurips.cc/paper/2020/file/ff4dfdf5904e920ce52b48c1cef97829-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{wiegreffe2019attention,
  title={Attention is not not Explanation},
  author={Wiegreffe, Sarah and Pinter, Yuval},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={11--20},
  year={2019}
}
@inproceedings{jain2019attention,
  title={Attention is not Explanation},
  author={Jain, Sarthak and Wallace, Byron C},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={3543--3556},
  year={2019}
}
@inproceedings{serrano2019attention,
  title={Is Attention Interpretable?},
  author={Serrano, Sofia and Smith, Noah A},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={2931--2951},
  year={2019}
}
@inproceedings{lan2019albert,
  title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}
@inproceedings{peng2020random,
  title={Random Feature Attention},
  author={Peng, Hao and Pappas, Nikolaos and Yogatama, Dani and Schwartz, Roy and Smith, Noah and Kong, Lingpeng},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{voita2019analyzing,
  title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
  author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  pages={5797--5808},
  year={2019}
}
@inproceedings{
tay2021long,
title={Long Range Arena : A Benchmark for Efficient Transformers },
author={Yi Tay and Mostafa Dehghani and Samira Abnar and Yikang Shen and Dara Bahri and Philip Pham and Jinfeng Rao and Liu Yang and Sebastian Ruder and Donald Metzler},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qVyeW-grC2k}
}

@inproceedings{gong2019efficient,
  title={Efficient training of bert by progressively stacking},
  author={Gong, Linyuan and He, Di and Li, Zhuohan and Qin, Tao and Wang, Liwei and Liu, Tieyan},
  booktitle={International Conference on Machine Learning},
  pages={2337--2346},
  year={2019},
  organization={PMLR}
}
@misc{
jiao2020tinybert,
title={Tiny{\{}BERT{\}}: Distilling {\{}BERT{\}} for Natural Language Understanding},
author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
year={2020},
url={https://openreview.net/forum?id=rJx0Q6EFPB}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{Clark2020ELECTRA,
title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1xMH1BtvB}
}

@article{choromanski2020rethinking,
  title={Rethinking attention with performers},
  author={Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others},
  journal={arXiv preprint arXiv:2009.14794},
  year={2020}
}
@article{wang2020linformer,
  title={Linformer: Self-attention with linear complexity},
  author={Wang, Sinong and Li, Belinda and Khabsa, Madian and Fang, Han and Ma, Hao},
  journal={arXiv preprint arXiv:2006.04768},
  year={2020}
}
@article{brown1992estimate,
  title={An estimate of an upper bound for the entropy of English},
  author={Brown, Peter F and Della Pietra, Stephen A and Della Pietra, Vincent J and Lai, Jennifer C and Mercer, Robert L},
  journal={Computational Linguistics},
  volume={18},
  number={1},
  pages={31--40},
  year={1992}
}
@article{tay2020synthesizer,
  title={Synthesizer: Rethinking self-attention in transformer models},
  author={Tay, Yi and Bahri, Dara and Metzler, Donald and Juan, Da-Cheng and Zhao, Zhe and Zheng, Che},
  journal={arXiv preprint arXiv:2005.00743},
  year={2020}
}
@inproceedings{raganato2020fixed,
  title={Fixed Encoder Self-Attention Patterns in Transformer-Based Machine Translation},
  author={Raganato, Alessandro and Scherrer, Yves and Tiedemann, J{\"o}rg},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings},
  pages={556--568},
  year={2020}
}
@inproceedings{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
   booktitle={International Conference on Learning Representations, ICLR},
 year={2021}
}
@article{raffel2020exploring,
  title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of Machine Learning Research},
  volume={21},
  pages={1--67},
  year={2020}
}

@inproceedings{yun2020,
 author = {Yun, Chulhee and Chang, Yin-Wen and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {13783--13794},
 publisher = {Curran Associates, Inc.},
 title = {O(n) Connections are Expressive Enough: Universal Approximability of Sparse Transformers},
 url = {https://proceedings.neurips.cc/paper/2020/file/9ed27554c893b5bad850a422c3538c15-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{cordonnier2019relationship,
  title={On the Relationship between Self-Attention and Convolutional Layers},
  author={Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@article{rogers2020primer,
  title={A Primer in BERTology: What We Know About How BERT Works},
  author={Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={842--866},
  year={2020}
}
@article{vig2019analyzing,
  title={Analyzing the Structure of Attention in a Transformer Language Model},
  author={Vig, Jesse and Learning, Machine and Belinkov, Yonatan},
  journal={ACL 2019},
  pages={63},
  year={2019}
}
@misc{
jiao2020tinybert,
title={Tiny{\{}BERT{\}}: Distilling {\{}BERT{\}} for Natural Language Understanding},
author={Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
year={2020},
url={https://openreview.net/forum?id=rJx0Q6EFPB}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@inproceedings{Clark2020ELECTRA,
title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators},
author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=r1xMH1BtvB}
}
@article{lm1b,
  author    = {Ciprian Chelba and
               Tomas Mikolov and
               Mike Schuster and
               Qi Ge and
               Thorsten Brants and
               Phillipp Koehn},
  title     = {One Billion Word Benchmark for Measuring Progress in Statistical Language
               Modeling},
  journal   = {CoRR},
  volume    = {abs/1312.3005},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.3005},
  archivePrefix = {arXiv},
  eprint    = {1312.3005},
  timestamp = {Mon, 13 Aug 2018 16:46:16 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/ChelbaMSGBK13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in {N}eural {I}nformation {P}rocessing {S}ystems},
  pages={5998--6008},
  year={2017}
}

@article{devlin2018bert,
  title={{BERT}: Pre-training of deep bidirectional {T}ransformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{roberta2019,
  author    = {Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  title     = {{RoBERTa}: {A} Robustly Optimized {BERT} Pretraining Approach},
  journal   = {arXiv preprint arXiv:1907.11692},
  year      = {2019}
}

@article{xlnet2019,
  author    = {Zhilin Yang and Zihang Dai and Yiming Yang and Jaime G. Carbonell and Ruslan Salakhutdinov and Quoc V. Le},
  title     = {{XLNet}: Generalized Autoregressive Pretraining for Language Understanding},
  journal   = {arXiv preprint arXiv:1906.08237},
  year      = {2019}
}

@article{bahdanau2014additive,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}


@inproceedings{sukhbaatar2015memory,
 author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
 title = {End-to-end Memory Networks},
 booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'15},
 year = {2015},
 location = {Montreal, Canada},
 pages = {2440--2448},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article{britz2017massive,
  title={Massive exploration of neural machine translation architectures},
  author={Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
  journal={arXiv preprint arXiv:1703.03906},
  year={2017}
}

@InProceedings{luong2015multiplicativ,
  author    = {Luong, Minh-Thang  and  Pham, Hieu  and  Manning, Christopher D.},
  title     = {Effective Approaches to Attention-based Neural Machine Translation},
  booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
  month     = {September},
  year      = {2015},
  address   = {Lisbon, Portugal},
  publisher = {Association for Computational Linguistics},
  pages     = {1412--1421},
}

@inproceedings{bahdanau2015neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2015}
}


@inproceedings{peters2018elmo,
    title = {Deep Contextualized Word Representations},
    author = {Peters, Matthew  and Neumann, Mark  and Iyyer, Mohit  and Gardner, Matt  and Clark, Christopher  and Lee, Kenton  and Zettlemoyer, Luke},
    booktitle = {Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
    month = jun,
    year = {2018},
    pages = {2227--2237},
}

@article{radford2019gpt2,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  journal={Technical Report, {O}pen{AI}},
  year={2019}
}

@article{radford2018gpt,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  journal={Technical Report, {O}pen{AI}},
  year={2018}
}

@inproceedings{lin2018resnet,
  title={{ResNet} with one-neuron hidden layers is a universal approximator},
  author={Lin, Hongzhou and Jegelka, Stefanie},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6169--6178},
  year={2018}
}

@article{sifre2014rigid,
  title={Rigid-motion scattering for image classification},
  author={Sifre, Laurent and Mallat, St{\'e}phane},
  journal={Ph. D. dissertation},
  year={2014},
}

@inproceedings{chollet2017xception,
  title={Xception: Deep learning with depthwise separable convolutions},
  author={Chollet, Fran{\c{c}}ois},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1251--1258},
  year={2017}
}
@article{kaiser2017depthwise,
  title={Depthwise separable convolutions for neural machine translation},
  author={Kaiser, Lukasz and Gomez, Aidan N and Chollet, Francois},
  journal={arXiv preprint arXiv:1706.03059},
  year={2017}
}

@article{wu2019pay,
  title={Pay less attention with lightweight and dynamic convolutions},
  author={Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N and Auli, Michael},
  journal={arXiv preprint arXiv:1901.10430},
  year={2019}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}
@inproceedings{rajpurkar2016squad,
  title={{SQuAD}: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
  pages={2383--2392},
  year={2016}
}
@InProceedings{mnli,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}
@article{dehghani2018universal,
  title={Universal {T}ransformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}
@article{perez2019turing,
  title={On the {Turing} Completeness of Modern Neural Network Architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}
@article{sannai2019universal,
  title={Universal approximations of permutation invariant/equivariant functions by deep neural networks},
  author={Sannai, Akiyoshi and Takai, Yuuki and Cordonnier, Matthieu},
  journal={arXiv preprint arXiv:1903.01939},
  year={2019}
}
@article{coenen2019visualizing,
  title={Visualizing and Measuring the Geometry of {BERT}},
  author={Coenen, Andy and Reif, Emily and Yuan, Ann and Kim, Been and Pearce, Adam and Vi{\'e}gas, Fernanda and Wattenberg, Martin},
  journal={arXiv preprint arXiv:1906.02715},
  year={2019}
}
@article{clark2019does,
  title={What Does {BERT} Look At? An Analysis of {BERT}'s Attention},
  author={Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D},
  journal={arXiv preprint arXiv:1906.04341},
  year={2019}
}
@inproceedings{hewitt2019structural,
  title={A structural probe for finding syntax in word representations},
  author={Hewitt, John and Manning, Christopher D},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={4129--4138},
  year={2019}
}

@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@article{hornik1991approximation,
  title={Approximation capabilities of multilayer feedforward networks},
  author={Hornik, Kurt},
  journal={Neural networks},
  volume={4},
  number={2},
  pages={251--257},
  year={1991},
  publisher={Elsevier}
}
@inproceedings{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  booktitle={Advances in neural information processing systems},
  pages={6231--6239},
  year={2017}
}
@article{hanin2017approximating,
  title={Approximating continuous functions by relu nets of minimal width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}
@article{johnson1984extensions,
  title={Extensions of Lipschitz mappings into a Hilbert space},
  author={Johnson, William B and Lindenstrauss, Joram},
  journal={Contemporary mathematics},
  volume={26},
  number={189-206},
  pages={1},
  year={1984}
}
@inproceedings{zhu2015aligning,
  title={Aligning books and movies: Towards story-like visual explanations by watching movies and reading books},
  author={Zhu, Yukun and Kiros, Ryan and Zemel, Rich and Salakhutdinov, Ruslan and Urtasun, Raquel and Torralba, Antonio and Fidler, Sanja},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={19--27},
  year={2015}
}

@article{ailon2009fast,
  title={The fast {J}ohnson--{L}indenstrauss transform and approximate nearest neighbors},
  author={Ailon, Nir and Chazelle, Bernard},
  journal={SIAM Journal on computing},
  volume={39},
  number={1},
  pages={302--322},
  year={2009},
  publisher={SIAM}
}
@inproceedings{gong2013learning,
  title={Learning binary codes for high-dimensional data using bilinear projections},
  author={Gong, Yunchao and Kumar, Sanjiv and Rowley, Henry A and Lazebnik, Svetlana},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={484--491},
  year={2013}
}

@inproceedings{yun2019transformers,
  title={Are {T}ransformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year={2020}
}


@article{child2019generating,
  title={Generating long sequences with sparse {T}ransformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{correia2019adaptively,
  title={Adaptively sparse {T}ransformers},
  author={Correia, Gon{\c{c}}alo M and Niculae, Vlad and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:1909.00015},
  year={2019}
}

@article{zhao2019explicit,
  title={Explicit Sparse {T}ransformer: Concentrated Attention Through Explicit Selection},
  author={Zhao, Guangxiang and Lin, Junyang and Zhang, Zhiyuan and Ren, Xuancheng and Su, Qi and Sun, Xu},
  journal={arXiv preprint arXiv:1912.11637},
  year={2019}
}

@article{beltagy2020longformer,
  title={Longformer: The Long-Document {T}ransformer},
  author={Beltagy, Iz and Peters, Matthew E and Cohan, Arman},
  journal={arXiv preprint arXiv:2004.05150},
  year={2020}
}

@article{ye2019bp,
  title={BP-{T}ransformer: Modelling Long-Range Context via Binary Partitioning},
  author={Ye, Zihao and Guo, Qipeng and Gan, Quan and Qiu, Xipeng and Zhang, Zheng},
  journal={arXiv preprint arXiv:1911.04070},
  year={2019}
}

@article{roy2020efficient,
  title={Efficient Content-Based Sparse Attention with Routing {T}ransformers},
  author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
  journal={arXiv preprint arXiv:2003.05997},
  year={2020}
}

@article{li2020sac,
  title={SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection},
  author={Li, Xiaoya and Meng, Yuxian and Han, Qinghong and Wu, Fei and Li, Jiwei},
  journal={arXiv preprint arXiv:2003.09833},
  year={2020}
}

@article{qiu2019blockwise,
  title={Blockwise Self-Attention for Long Document Understanding},
  author={Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Scott Wen-tau and Wang, Sinong and Tang, Jie},
  journal={arXiv preprint arXiv:1911.02972},
  year={2019}
}

@inproceedings{cui2019fine,
  title={Fine-tune {BERT} with Sparse Self-Attention Mechanism},
  author={Cui, Baiyun and Li, Yingming and Chen, Ming and Zhang, Zhongfei},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={3539--3544},
  year={2019}
}

@article{sukhbaatar2019adaptive,
  title={Adaptive attention span in {T}ransformers},
  author={Sukhbaatar, Sainbayar and Grave, Edouard and Bojanowski, Piotr and Joulin, Armand},
  journal={arXiv preprint arXiv:1905.07799},
  year={2019}
}

@article{guo2019star,
  title={Star-{T}ransformer},
  author={Guo, Qipeng and Qiu, Xipeng and Liu, Pengfei and Shao, Yunfan and Xue, Xiangyang and Zhang, Zheng},
  journal={arXiv preprint arXiv:1902.09113},
  year={2019}
}

@inproceedings{martins2016softmax,
  title={From softmax to sparsemax: A sparse model of attention and multi-label classification},
  author={Martins, Andre and Astudillo, Ramon},
  booktitle={International Conference on Machine Learning},
  pages={1614--1623},
  year={2016}
}

@article{peters2019sparse,
  title={Sparse sequence-to-sequence models},
  author={Peters, Ben and Niculae, Vlad and Martins, Andr{\'e} FT},
  journal={arXiv preprint arXiv:1905.05702},
  year={2019}
}

@article{brunner2019identifiability,
    title={On Identifiability in {T}ransformers},
    author={Gino Brunner and Yang Liu and Damián Pascual and Oliver Richter and Massimiliano Ciaramita and Roger Wattenhofer},
    journal={arXiv preprint arXiv:1908.04211},
    year={2019}
}
@inproceedings{michel2019sixteen,
 author = {Michel, Paul and Levy, Omer and Neubig, Graham},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\' Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Are Sixteen Heads Really Better than One?},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c601ad9d2ff9bc8b282670cdd54f69f-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{bhojanapalli2020low,
  title={Low-Rank Bottleneck in Multi-head Attention Models},
  author={Bhojanapalli, Srinadh and Yun, Chulhee and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:2002.07028},
  year={2020}
}

@article{kitaev2020reformer,
  title={Reformer: The Efficient {T}ransformer},
  author={Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal={arXiv preprint arXiv:2001.04451},
  year={2020}
}

@inproceedings{laha2018controllable,
  title={On controllable sparse alternatives to softmax},
  author={Laha, Anirban and Chemmengath, Saneem Ahmed and Agrawal, Priyanka and Khapra, Mitesh and Sankaranarayanan, Karthik and Ramaswamy, Harish G},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6422--6432},
  year={2018}
}
@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press}
}
@article{vaswani2018tensor2tensor,
  title={Tensor2tensor for neural machine translation},
  author={Vaswani, Ashish and Bengio, Samy and Brevdo, Eugene and Chollet, Francois and Gomez, Aidan N and Gouws, Stephan and Jones, Llion and Kaiser, {\L}ukasz and Kalchbrenner, Nal and Parmar, Niki and others},
  journal={arXiv preprint arXiv:1803.07416},
  year={2018}
}

@article{zaheer2020big,
  title={Big bird: Transformers for longer sequences},
  author={Zaheer, Manzil and Guruganesh, Guru and Dubey, Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal={arXiv preprint arXiv:2007.14062},
  year={2020}
}

@article{tay2020efficient,
  title={Efficient transformers: A survey},
  author={Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  journal={arXiv preprint arXiv:2009.06732},
  year={2020}
}

@inproceedings{kidger2020universal,
  title={Universal approximation with deep narrow networks},
  author={Kidger, Patrick and Lyons, Terry},
  booktitle={Conference on Learning Theory},
  pages={2306--2327},
  year={2020}
}

@article{park2020minimum,
  title={Minimum Width for Universal Approximation},
  author={Park, Sejun and Yun, Chulhee and Lee, Jaeho and Shin, Jinwoo},
  journal={arXiv preprint arXiv:2006.08859},
  year={2020}
}

@inproceedings{johnson2018deep,
  title={Deep, skinny neural networks are not universal approximators},
  author={Johnson, Jesse},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{wang2019superglue,
  title={SuperGLUE: a stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
  booktitle={Proceedings of the 33rd International Conference on Neural Information Processing Systems},
  pages={3266--3280},
  year={2019}
}

@inproceedings{wang2019glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel},
  booktitle={7th International Conference on Learning Representations, ICLR 2019},
  year={2019}
}

@InProceedings{williams2018broad,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}

@InProceedings{conneau2018xnli,
  author = "Conneau, Alexis
                 and Rinott, Ruty
                 and Lample, Guillaume
                 and Williams, Adina
                 and Bowman, Samuel R.
                 and Schwenk, Holger
                 and Stoyanov, Veselin",
  title = "XNLI: Evaluating Cross-lingual Sentence Representations",
  booktitle = "Proceedings of the 2018 Conference on Empirical Methods 
               in Natural Language Processing",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  location = "Brussels, Belgium",
}

#### Added by rjenatton

@inproceedings{vainsencher2011sample,
  title={The sample complexity of dictionary learning},
  author={Vainsencher, Daniel and Mannor, Shie and Bruckstein, Alfred M},
  booktitle={Proceedings of the 24th Annual Conference on Learning Theory},
  pages={773--788},
  year={2011},
  organization={JMLR Workshop and Conference Proceedings}
}

@inproceedings{mehta2013sparsity,
  title={Sparsity-based generalization bounds for predictive sparse coding},
  author={Mehta, Nishant and Gray, Alexander},
  booktitle={International Conference on Machine Learning},
  pages={36--44},
  year={2013},
  organization={PMLR}
}

@article{allingham2021,
  author    = {James Urquhart Allingham and
               Florian Wenzel and
               Zelda E. Mariet and
               Basil Mustafa and
               Joan Puigcerver and
               Neil Houlsby and
               Ghassen Jerfel and
               Vincent Fortuin and
               Balaji Lakshminarayanan and
               Jasper Snoek and
               Dustin Tran and
               Carlos Riquelme Ruiz and
               Rodolphe Jenatton},
  title     = {Sparse MoEs meet Efficient Ensembles},
  journal   = {CoRR},
  volume    = {abs/2110.03360},
  year      = {2021},
  url       = {https://arxiv.org/abs/2110.03360},
  eprinttype = {arXiv},
  eprint    = {2110.03360},
  timestamp = {Thu, 21 Oct 2021 16:20:08 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2110-03360.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
  
@inproceedings{xie2019intriguing,
  title={Intriguing Properties of Adversarial Training at Scale},
  author={Xie, Cihang and Yuille, Alan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@inproceedings{madry2018towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{alayrac2019labels,
  title={Are labels required for improving adversarial robustness?},
  author={Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{gowal2020uncovering,
  title={Uncovering the limits of adversarial training against norm-bounded adversarial examples},
  author={Gowal, Sven and Qin, Chongli and Uesato, Jonathan and Mann, Timothy and Kohli, Pushmeet},
  journal={arXiv preprint arXiv:2010.03593},
  year={2020}
}


@article{vit2020,
  title={{An image is worth 16x16 words: Transformers for image recognition at scale}},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020},
}

@inproceedings{vmoe2021,
 author = {Riquelme, Carlos and Puigcerver, Joan and Mustafa, Basil and Neumann, Maxim and Jenatton, Rodolphe and Susano Pinto, Andr\'{e} and Keysers, Daniel and Houlsby, Neil},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
 pages = {8583--8595},
 publisher = {Curran Associates, Inc.},
 title = {{Scaling Vision with Sparse Mixture of Experts}},
 url = {https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf},
 volume = {34},
 year = {2021},
}

@InProceedings{jft300m,
author = {Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
title = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
booktitle = {Proceedings of the IEEE International Conference on Computer Vision (ICCV)},
month = {Oct},
year = {2017},
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press},
}

@ARTICLE{jordan1994hierarchical,
  author={Jordan, Michael I. and Jacobs, Robert A.},
  journal={Neural Computation}, 
  title={Hierarchical Mixtures of Experts and the EM Algorithm}, 
  year={1994},
  volume={6},
  number={2},
  pages={181-214},
  doi={10.1162/neco.1994.6.2.181},
}

@ARTICLE{Yuksel2012,
  author={Yuksel, Seniha Esen and Wilson, Joseph N. and Gader, Paul D.},
  journal={IEEE Transactions on Neural Networks and Learning Systems}, 
  title={Twenty Years of Mixture of Experts}, 
  year={2012},
  volume={23},
  number={8},
  pages={1177-1193},
  doi={10.1109/TNNLS.2012.2200299},
}

@article{eigen2013learning,
  title={Learning factored representations in a deep mixture of experts},
  author={Eigen, David and Ranzato, Marc'Aurelio and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1312.4314},
  year={2013},
}

@article{shazeer2017outrageously,
  title={Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  author={Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  journal={arXiv preprint arXiv:1701.06538},
  year={2017},
}

@article{carlini2019evaluating,
  title={On evaluating adversarial robustness},
  author={Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
  journal={arXiv preprint arXiv:1902.06705},
  year={2019}
}
@article{gshard2020,
  title={{GShard: Scaling giant models with conditional computation and automatic sharding}},
  author={Lepikhin, Dmitry and Lee, HyoukJoong and Xu, Yuanzhong and Chen, Dehao and Firat, Orhan and Huang, Yanping and Krikun, Maxim and Shazeer, Noam and Chen, Zhifeng},
  journal={arXiv preprint arXiv:2006.16668},
  year={2020},
}

@article{xue2021go,
  title={Go wider instead of deeper},
  author={Xue, Fuzhao and Shi, Ziji and Wei, Futao and Lou, Yuxuan and Liu, Yong and You, Yang},
  journal={arXiv preprint arXiv:2107.11817},
  year={2021},
}

@article{patterson2021carbon,
  title={Carbon emissions and large neural network training},
  author={Patterson, David and Gonzalez, Joseph and Le, Quoc and Liang, Chen and Munguia, Lluis-Miquel and Rothchild, Daniel and So, David and Texier, Maud and Dean, Jeff},
  journal={arXiv preprint arXiv:2104.10350},
  year={2021},
}


@InProceedings{lewis2021base,
  title = 	 {BASE Layers: Simplifying Training of Large, Sparse Models},
  author =       {Lewis, Mike and Bhosale, Shruti and Dettmers, Tim and Goyal, Naman and Zettlemoyer, Luke},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6265--6274},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lewis21a/lewis21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lewis21a.html},
}

@book{hastie2009elements,
  title={The elements of statistical learning: data mining, inference, and prediction},
  author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  volume={2},
  year={2009},
  publisher={Springer}
}

@article{bietti2019group,
  title={Group invariance, stability to deformations, and complexity of deep convolutional representations},
  author={Bietti, Alberto and Mairal, Julien},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={876--924},
  year={2019},
  publisher={JMLR. org}
}

@inproceedings{cranko2021generalised,
  title={Generalised lipschitz regularisation equals distributional robustness},
  author={Cranko, Zac and Shi, Zhan and Zhang, Xinhua and Nock, Richard and Kornblith, Simon},
  booktitle={International Conference on Machine Learning},
  pages={2178--2188},
  year={2021},
  organization={PMLR}
}

@TECHREPORT{krizhevsky2009learning,
    author = {Krizhevsky, Alex},
    title = {Learning multiple layers of features from tiny images},
    institution = {University of Toronto},
    year = {2009}
}

@article{hendrycks2019benchmarking,
  title={Benchmarking neural network robustness to common corruptions and perturbations},
  author={Hendrycks, Dan and Dietterich, Thomas},
  journal={arXiv preprint arXiv:1903.12261},
  year={2019}
}


@InProceedings{pmlr-v119-croce20b,
  title = 	 {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author =       {Croce, Francesco and Hein, Matthias},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {2206--2216},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/croce20b/croce20b.pdf},
  url = 	 {https://proceedings.mlr.press/v119/croce20b.html},
}
