
@article{brunton_extracting_2016,
	title = {Extracting spatial–temporal coherent patterns in large-scale neural recordings using dynamic mode decomposition},
	abstract = {Background
There is a broad need in neuroscience to understand and visualize large-scale recordings of neural activity, big data acquired by tens or hundreds of electrodes recording dynamic brain activity over minutes to hours. Such datasets are characterized by coherent patterns across both space and time, yet existing computational methods are typically restricted to analysis either in space or in time separately.
New method
Here we report the adaptation of dynamic mode decomposition (DMD), an algorithm originally developed for studying fluid physics, to large-scale neural recordings. DMD is a modal decomposition algorithm that describes high-dimensional dynamic data using coupled spatial–temporal modes. The algorithm is robust to variations in noise and subsampling rate; it scales easily to very large numbers of simultaneously acquired measurements.
Results
We first validate the DMD approach on sub-dural electrode array recordings from human subjects performing a known motor task. Next, we combine DMD with unsupervised clustering, developing a novel method to extract spindle networks during sleep. We uncovered several distinct sleep spindle networks identifiable by their stereotypical cortical distribution patterns, frequency, and duration.
Comparison with existing methods
DMD is closely related to principal components analysis (PCA) and discrete Fourier transform (DFT). We may think of DMD as a rotation of the low-dimensional PCA space such that each basis vector has coherent dynamics.
Conclusions
The resulting analysis combines key features of performing PCA in space and power spectral analysis in time, making it particularly suitable for analyzing large-scale neural recordings.},
	language = {en},
	journal = {Journal of Neuroscience Methods},
	author = {Brunton, Bingni W. and Johnson, Lise A. and Ojemann, Jeffrey G. and Kutz, J. Nathan},
	year = {2016},
	keywords = {Dynamic mode decomposition, Electrocorticography, Feature extraction, Sleep spindles, Spatiotemporal modes},
	pages = {1--15},
	file = {Accepted Version:/Users/mitchellostrow/Zotero/storage/68BV6SRT/Brunton et al. - 2016 - Extracting spatial–temporal coherent patterns in l.pdf:application/pdf},
}

@misc{noauthor_higher_nodate,
	title = {Higher {Order} {Dynamic} {Mode} {Decomposition} {\textbar} {SIAM} {Journal} on {Applied} {Dynamical} {Systems}},
	url = {https://epubs.siam.org/doi/epdf/10.1137/15M1054924},
	urldate = {2022-11-27},
}

@inproceedings{williams_generalized_2022,
 author = {Williams, Alex H and Kunz, Erin and Kornblith, Simon and Linderman, Scott},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {4738--4750},
 title = {Generalized Shape Metrics on Neural Representations},
 volume = {34},
 year = {2021}
}

@article{vyas2020computation,
  title={Computation through neural population dynamics},
  author={Vyas, Saurabh and Golub, Matthew D and Sussillo, David and Shenoy, Krishna V},
  journal={Annual Review of Neuroscience},
  volume={43},
  pages={249--275},
  year={2020},
  publisher={Annual Reviews}
}


@book{strogatz_nonlinear_2015,
	title = {Nonlinear dynamics and chaos : with applications to physics, biology, chemistry, and engineering},
	publisher = {Second edition. Boulder, CO : Westview Press, a member of the Perseus Books Group},
	author = {Strogatz, Steven},
	year = {2015},
}

@misc{proctor_dynamic_2014,
	title = {Dynamic mode decomposition with control},
	url = {http://arxiv.org/abs/1409.6358},
	abstract = {We develop a new method which extends Dynamic Mode Decomposition (DMD) to incorporate the effect of control to extract low-order models from high-dimensional, complex systems. DMD finds spatial-temporal coherent modes, connects local-linear analysis to nonlinear operator theory, and provides an equation-free architecture which is compatible with compressive sensing. In actuated systems, DMD is incapable of producing an input-output model; moreover, the dynamics and the modes will be corrupted by external forcing. Our new method, Dynamic Mode Decomposition with control (DMDc), capitalizes on all of the advantages of DMD and provides the additional innovation of being able to disambiguate between the underlying dynamics and the effects of actuation, resulting in accurate input-output models. The method is data-driven in that it does not require knowledge of the underlying governing equations, only snapshots of state and actuation data from historical, experimental, or black-box simulations. We demonstrate the method on high-dimensional dynamical systems, including a model with relevance to the analysis of infectious disease data with mass vaccination (actuation).},
	publisher = {arXiv},
	author = {Proctor, Joshua L. and Brunton, Steven L. and Kutz, J. Nathan},
	month = sep,
	year = {2014},
	keywords = {Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/7SGL2IUP/Proctor et al. - 2014 - Dynamic mode decomposition with control.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/5UHUZ6WF/1409.html:text/html},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction to {DMD} ({CH} 1) {\textbar} {Dynamic} {Mode} {Decomposition}},
	url = {http://www.dmdbook.com/page10/data4bio.html},
	urldate = {2022-11-27},
}

@book{statshapes,
author = {Dryden, Ian and Mardia, Kanti},
year = {2016},
title = {Statistical shape analysis, with applications in R: Second edition},
}

@article{brunton_chaos_2017,
	title = {Chaos as an {Intermittently} {Forced} {Linear} {System}},
	volume = {8},
	abstract = {Understanding the interplay of order and disorder in chaotic systems is a central challenge in modern quantitative science. We present a universal, data-driven decomposition of chaos as an intermittently forced linear system. This work combines Takens' delay embedding with modern Koopman operator theory and sparse regression to obtain linear representations of strongly nonlinear dynamics. The result is a decomposition of chaotic dynamics into a linear model in the leading delay coordinates with forcing by low energy delay coordinates; we call this the Hankel alternative view of Koopman (HAVOK) analysis. This analysis is applied to the canonical Lorenz system, as well as to real-world examples such as the Earth's magnetic field reversal, and data from electrocardiogram, electroencephalogram, and measles outbreaks. In each case, the forcing statistics are non-Gaussian, with long tails corresponding to rare events that trigger intermittent switching and bursting phenomena; this forcing is highly predictive, providing a clear signature that precedes these events. Moreover, the activity of the forcing signal demarcates large coherent regions of phase space where the dynamics are approximately linear from those that are strongly nonlinear.},
	number = {1},
	journal = {Nature Communications},
	author = {Brunton, Steven L. and Brunton, Bingni W. and Proctor, Joshua L. and Kaiser, Eurika and Kutz, J. Nathan},
	year = {2017},
	keywords = {Mathematics - Dynamical Systems, 37XX, 70G60, 65Pxx, 34C28, 37D45, 93B30, Nonlinear Sciences - Chaotic Dynamics},
	pages = {19},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/XRYSXGBS/Brunton et al. - 2017 - Chaos as an Intermittently Forced Linear System.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/GAW3VLTA/1608.html:text/html},
}

@article{blaszka_classification_2017,
	title = {Classification of {Fixed} {Point} {Network} {Dynamics} from {Multiple} {Node} {Timeseries} {Data}},
	abstract = {Fixed point networks are dynamic networks encoding stimuli via distinct output patterns. Although, such networks are common in neural systems, their structures are typically unknown or poorly characterized. It is thereby valuable to use a supervised approach for resolving how a network encodes inputs of interest and the superposition of those inputs from sampled multiple node time series. In this paper, we show that accomplishing such a task involves finding a low-dimensional state space from supervised noisy recordings. We demonstrate that while standard methods for dimension reduction are unable to provide optimal separation of fixed points and transient trajectories approaching them, the combination of dimension reduction with selection (clustering) and optimization can successfully provide such functionality. Specifically, we propose two methods: Exclusive Threshold Reduction (ETR) and Optimal Exclusive Threshold Reduction (OETR) for finding a basis for the classification state space. We show that the classification space—constructed through the combination of dimension reduction and optimal separation—can directly facilitate recognition of stimuli, and classify complex inputs (mixtures) into similarity classes. We test our methodology on a benchmark data-set recorded from the olfactory system. We also use the benchmark to compare our results with the state-of-the-art. The comparison shows that our methods are capable to construct classification spaces and perform recognition at a significantly better rate than previously proposed approaches.},
	journal = {Frontiers in Neuroinformatics},
	author = {Blaszka, David and Sanders, Elischa and Riffell, Jeffrey A. and Shlizerman, Eli},
	month = sep,
	year = {2017},
	file = {PubMed Central Full Text PDF:/Users/mitchellostrow/Zotero/storage/ZW2FKZDM/Blaszka et al. - 2017 - Classification of Fixed Point Network Dynamics fro.pdf:application/pdf},
}

@article{snyder2021koopman,
  title={Koopman operator theory for nonlinear dynamic modeling using dynamic mode decomposition},
  author={Snyder, Gregory and Song, Zhuoyuan},
  journal={arXiv preprint arXiv:2110.08442},
  year={2021}
}

@article{krake_visualization_2021,
	title = {Visualization and selection of {Dynamic} {Mode} {Decomposition} components for unsteady flow},
	abstract = {Dynamic Mode Decomposition (DMD) is a data-driven and model-free decomposition technique. It is suitable for revealing spatio-temporal features of both numerically and experimentally acquired data. Conceptually, DMD performs a low-dimensional spectral decomposition of the data into the following components: the modes, called DMD modes, encode the spatial contribution of the decomposition, whereas the DMD amplitudes specify their impact. Each associated eigenvalue, referred to as DMD eigenvalue, characterizes the frequency and growth rate of the DMD mode. In this paper, we demonstrate how the components of DMD can be utilized to obtain temporal and spatial information from time-dependent flow fields. We begin with the theoretical background of DMD and its application to unsteady flow. Next, we examine the conventional process with DMD mathematically and put it in relationship to the discrete Fourier transform. Our analysis shows that the current use of DMD components has several drawbacks. To resolve these problems we adjust the components and provide new and meaningful insights into the decomposition: we show that our improved components capture the spatio-temporal patterns of the flow better. Moreover, we remove redundancies in the decomposition and clarify the interplay between components, allowing users to understand the impact of components. These new representations, which respect the spatio-temporal character of DMD, enable two clustering methods that segment the flow into physically relevant sections and can therefore be used for the selection of DMD components. With a number of typical examples, we demonstrate that the combination of these techniques allows new insights with DMD for unsteady flow.},
	language = {en},
	journal = {Visual Informatics},
	author = {Krake, T. and Reinhardt, S. and Hlawatsch, M. and Eberhardt, B. and Weiskopf, D.},
	year = {2021},
	keywords = {Dynamic Mode Decomposition, Spectral decomposition},
	pages = {15--27},
	file = {Full Text:/Users/mitchellostrow/Zotero/storage/ALF9IQ6I/Krake et al. - 2021 - Visualization and selection of Dynamic Mode Decomp.pdf:application/pdf},
}


@article{kruskal_multidimensional_1964,
	title = {Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
	volume = {29},
	number = {1},
	journal = {Psychometrika},
	author = {Kruskal, J. B.},
	year = {1964},
	pages = {1--27},
}

@article{hahnloser_digital_2000,
	title = {Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit},
	abstract = {Digital circuits such as the flip-flop use feedback to achieve multi-stability and nonlinearity to restore signals to logical levels, for example 0 and 1. Analogue feedback circuits are generally designed to operate linearly, so that signals are over a range, and the response is unique. By contrast, the response of cortical circuits to sensory stimulation can be both multistable and graded1,2,3,4. We propose that the neocortex combines digital selection of an active set of neurons with analogue response by dynamically varying the positive feedback inherent in its recurrent connections. Strong positive feedback causes differential instabilities that drive the selection of a set of active neurons under the constraints embedded in the synaptic weights. Once selected, the active neurons generate weaker, stable feedback that provides analogue amplification of the input. Here we present our model of cortical processing as an electronic circuit that emulates this hybrid operation, and so is able to perform computations that are similar to stimulus selection, gain modulation and spatiotemporal pattern generation in the neocortex.},
	journal = {Nature},
	author = {Hahnloser, Richard H. R. and Sarpeshkar, Rahul and Mahowald, Misha A. and Douglas, Rodney J. and Seung, H. Sebastian},
	year = {2000},
}


}
@article {schrimpf_brain-score_2018,
	author = {Martin Schrimpf and Jonas Kubilius and Ha Hong and Najib J. Majaj and Rishi Rajalingham and Elias B. Issa and Kohitij Kar and Pouya Bashivan and Jonathan Prescott-Roy and Franziska Geiger and Kailyn Schmidt and Daniel L. K. Yamins and James J. DiCarlo},
	title = {Brain-Score: Which Artificial Neural Network for Object Recognition is most Brain-Like?},
	year = {2020},
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score {\textendash} a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain{\textquoteright}s mechanisms for core object recognition {\textendash} and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at >= 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain{\textquoteright}s network and thus drive next experiments. To facilitate both of these, we release Brain-Score.org: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.},
	URL = {https://www.biorxiv.org/content/early/2020/01/02/407007},
	journal = {bioRxiv}
}



@article{fujii_dynamic_2019,
	title = {Dynamic mode decomposition in vector-valued reproducing kernel {Hilbert} spaces for extracting dynamical structure among observables},
	volume = {117},
	abstract = {Understanding nonlinear dynamical systems (NLDSs) is challenging in a variety of engineering and scientific fields. Dynamic mode decomposition (DMD), which is a numerical algorithm for the spectral analysis of Koopman operators, has been attracting attention as a way of obtaining global modal descriptions of NLDSs without requiring explicit prior knowledge. However, since existing DMD algorithms are in principle formulated based on the concatenation of scalar observables, it is not directly applicable to data with dependent structures among observables, which take, for example, the form of a sequence of graphs. In this paper, we formulate Koopman spectral analysis for NLDSs with structures among observables and propose an estimation algorithm for this problem. This method can extract and visualize the underlying low-dimensional global dynamics of NLDSs with structures among observables from data, which can be useful in understanding the underlying dynamics of such NLDSs. To this end, we first formulate the problem of estimating spectra of the Koopman operator defined in vector-valued reproducing kernel Hilbert spaces, and then develop an estimation procedure for this problem by reformulating tensor-based DMD. As a special case of our method, we propose the method named as Graph DMD, which is a numerical algorithm for Koopman spectral analysis of graph dynamical systems, using a sequence of adjacency matrices. We investigate the empirical performance of our method by using synthetic and real-world data.},
	language = {en},
	journal = {Neural Networks},
	author = {Fujii, Keisuke and Kawahara, Yoshinobu},
	month = sep,
	year = {2019},
	keywords = {Dynamical systems, Dimensionality reduction, Spectral analysis, Unsupervised learning},
	pages = {94--103},
	file = {Submitted Version:/Users/mitchellostrow/Zotero/storage/LRTKKX4N/Fujii and Kawahara - 2019 - Dynamic mode decomposition in vector-valued reprod.pdf:application/pdf},
}

@misc{noauthor_dynamic_nodate,
	title = {Dynamic mode decomposition of resting-state and task {fMRI} - {ScienceDirect}},
	urldate = {2022-11-27},
}

@article{fujii_physically-interpretable_2020,
	title = {Physically-interpretable classification of biological network dynamics for complex collective motions},
	volume = {10},
	abstract = {Understanding biological network dynamics is a fundamental issue in various scientific and engineering fields. Network theory is capable of revealing the relationship between elements and their propagation; however, for complex collective motions, the network properties often transiently and complexly change. A fundamental question addressed here pertains to the classification of collective motion network based on physically-interpretable dynamical properties. Here we apply a data-driven spectral analysis called graph dynamic mode decomposition, which obtains the dynamical properties for collective motion classification. Using a ballgame as an example, we classified the strategic collective motions in different global behaviours and discovered that, in addition to the physical properties, the contextual node information was critical for classification. Furthermore, we discovered the label-specific stronger spectra in the relationship among the nearest agents, providing physical and semantic interpretations. Our approach contributes to the understanding of principles of biological complex network dynamics from the perspective of nonlinear dynamical systems.},
	language = {en},
	number = {1},
	urldate = {2022-11-27},
	journal = {Scientific Reports},
	author = {Fujii, Keisuke and Takeishi, Naoya and Hojo, Motokazu and Inaba, Yuki and Kawahara, Yoshinobu},
	year = {2020},
	pages = {3005},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/D663FTBF/Fujii et al. - 2020 - Physically-interpretable classification of biologi.pdf:application/pdf},
}

@article{brunton_discovering_2016,
	title = {Discovering governing equations from data by sparse identification of nonlinear dynamical systems},
	volume = {113},
	abstract = {Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.},
	number = {15},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Brunton, Steven L. and Proctor, Joshua L. and Kutz, J. Nathan},
	month = apr,
	year = {2016},
	pages = {3932--3937},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/3Z2WZGPY/Brunton et al. - 2016 - Discovering governing equations from data by spars.pdf:application/pdf},
}

@article{ceni_echo_2020,
	title = {The {Echo} {Index} and multistability in input-driven recurrent neural networks},
	volume = {412},
	url = {http://arxiv.org/abs/2001.07694},
	abstract = {A recurrent neural network (RNN) possesses the echo state property (ESP) if, for a given input sequence, it ``forgets'' any internal states of the driven (nonautonomous) system and asymptotically follows a unique, possibly complex trajectory. The lack of ESP is conventionally understood as a lack of reliable behaviour in RNNs. Here, we show that RNNs can reliably perform computations under a more general principle that accounts only for their local behaviour in phase space. To this end, we formulate a generalisation of the ESP and introduce an echo index to characterise the number of simultaneously stable responses of a driven RNN. We show that it is possible for the echo index to change with inputs, highlighting a potential source of computational errors in RNNs due to characteristics of the inputs driving the dynamics.},
	urldate = {2022-11-27},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Ceni, Andrea and Ashwin, Peter and Livi, Lorenzo and Postlethwaite, Claire},
	month = nov,
	year = {2020},
	keywords = {Mathematics - Dynamical Systems},
	pages = {132609},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/CZW3B5RZ/Ceni et al. - 2020 - The Echo Index and multistability in input-driven .pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/MWR73TQK/2001.html:text/html},
}

@article{dubois_data-driven_2020,
	title = {Data-driven predictions of the {Lorenz} system},
	volume = {408},
	journal = {Physica D: Nonlinear Phenomena},
	author = {Dubois, Pierre and Gomez, Thomas and Planckaert, Laurent and Perret, Laurent},
	year = {2020},
	keywords = {ASSIMILATION DE DONNEES, chaotic system, data assimilation, data-driven modeling, MODELISATION ORIENTEE DONNEE, neural networks, RESEAU DE NEURONES, SYSTEME CHAOTIQUE},
	pages = {132495},
	file = {HAL PDF Full Text:/Users/mitchellostrow/Zotero/storage/KYCI6P9B/Dubois et al. - 2020 - Data-driven predictions of the Lorenz system.pdf:application/pdf},
}

@misc{noauthor_cluster-based_nodate,
	title = {Cluster-based reduced-order modelling of a mixing layer {\textbar} {Journal} of {Fluid} {Mechanics} {\textbar} {Cambridge} {Core}},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/clusterbased-reducedorder-modelling-of-a-mixing-layer/0E3C7E709C4D735C22929F5919CC83D9},
	urldate = {2022-11-27},
}

@misc{noauthor_cluster-based_nodate-1,
	title = {Cluster-based reduced-order modelling of a mixing layer {\textbar} {Journal} of {Fluid} {Mechanics} {\textbar} {Cambridge} {Core}},
	url = {https://www.cambridge.org/core/journals/journal-of-fluid-mechanics/article/abs/clusterbased-reducedorder-modelling-of-a-mixing-layer/0E3C7E709C4D735C22929F5919CC83D9},
	urldate = {2022-11-27},
}

@misc{noauthor_kernel_nodate,
	title = {Kernel learning for robust dynamic mode decomposition: linear and nonlinear disambiguation optimization {\textbar} {Proceedings} of the {Royal} {Society} {A}: {Mathematical}, {Physical} and {Engineering} {Sciences}},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2021.0830},
	urldate = {2022-11-27},
}

@misc{iwata_neural_2020,
	title = {Neural {Dynamic} {Mode} {Decomposition} for {End}-to-{End} {Modeling} of {Nonlinear} {Dynamics}},
	url = {http://arxiv.org/abs/2012.06191},
	abstract = {Koopman spectral analysis has attracted attention for understanding nonlinear dynamical systems by which we can analyze nonlinear dynamics with a linear regime by lifting observations using a nonlinear function. For analysis, we need to find an appropriate lift function. Although several methods have been proposed for estimating a lift function based on neural networks, the existing methods train neural networks without spectral analysis. In this paper, we propose neural dynamic mode decomposition, in which neural networks are trained such that the forecast error is minimized when the dynamics is modeled based on spectral decomposition in the lifted space. With our proposed method, the forecast error is backpropagated through the neural networks and the spectral decomposition, enabling end-to-end learning of Koopman spectral analysis. When information is available on the frequencies or the growth rates of the dynamics, the proposed method can exploit it as regularizers for training. We also propose an extension of our approach when observations are influenced by exogenous control time-series. Our experiments demonstrate the effectiveness of our proposed method in terms of eigenvalue estimation and forecast performance.},
	publisher = {arXiv},
	author = {Iwata, Tomoharu and Kawahara, Yoshinobu},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/VDKIMMF3/Iwata and Kawahara - 2020 - Neural Dynamic Mode Decomposition for End-to-End M.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/GPXGNSLA/2012.html:text/html},
}

@misc{noauthor_datadriven_nodate,
	title = {A {Data}–{Driven} {Approximation} of the {Koopman} {Operator}: {Extending} {Dynamic} {Mode} {Decomposition} {\textbar} {SpringerLink}},
	url = {https://link.springer.com/article/10.1007/s00332-015-9258-5},
	urldate = {2022-11-27},
}

@inproceedings{cloos_scaling_2022,
	address = {San Francisco},
	title = {Scaling up the {Evaluation} of {Recurrent} {Neural} {Network} {Models} for {Cognitive} {Neuroscience}},
	url = {https://2022.ccneuro.org/view_paper.php?PaperNum=1271},
	language = {en},
	urldate = {2022-11-27},
	booktitle = {2022 {Conference} on {Cognitive} {Computational} {Neuroscience}},
	publisher = {Cognitive Computational Neuroscience},
	author = {Cloos, Nathan and Li, Moufan and Yang, Guangyu Robert and Cueva, Christopher J.},
	year = {2022},
	file = {Cloos et al. - 2022 - Scaling up the Evaluation of Recurrent Neural Netw.pdf:/Users/mitchellostrow/Zotero/storage/NNXMNSLS/Cloos et al. - 2022 - Scaling up the Evaluation of Recurrent Neural Netw.pdf:application/pdf},
}

@misc{kornblith_similarity_2019,
	title = {Similarity of {Neural} {Network} {Representations} {Revisited}},
	abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
	publisher = {arXiv},
	author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
	month = jul,
	year = {2019},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Neurons and Cognition, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/WRQD4XIN/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/FK2XJKUH/1905.html:text/html},
}

@misc{ding_grounding_2021,
	title = {Grounding {Representation} {Similarity} with {Statistical} {Testing}},
	url = {http://arxiv.org/abs/2108.01661},
	abstract = {To understand neural network behavior, recent works quantitatively compare different networks' learned representations using canonical correlation analysis (CCA), centered kernel alignment (CKA), and other dissimilarity measures. Unfortunately, these widely used measures often disagree on fundamental observations, such as whether deep networks differing only in random initialization learn similar representations. These disagreements raise the question: which, if any, of these dissimilarity measures should we believe? We provide a framework to ground this question through a concrete test: measures should have sensitivity to changes that affect functional behavior, and specificity against changes that do not. We quantify this through a variety of functional behaviors including probing accuracy and robustness to distribution shift, and examine changes such as varying random initialization and deleting principal components. We find that current metrics exhibit different weaknesses, note that a classical baseline performs surprisingly well, and highlight settings where all metrics appear to fail, thus providing a challenge set for further improvement.},
	urldate = {2022-11-27},
	publisher = {arXiv},
	author = {Ding, Frances and Denain, Jean-Stanislas and Steinhardt, Jacob},
	month = nov,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/7IC9T3PV/Ding et al. - 2021 - Grounding Representation Similarity with Statistic.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/HZDNXRMY/2108.html:text/html},
}

@inproceedings{maheswaranathan_universality_2019,
	title = {Universality and individuality in neural dynamics across large populations of recurrent networks},
	volume = {32},
	abstract = {Many recent studies have employed task-based modeling with recurrent neural networks (RNNs) to infer the computational function of different brain regions. These models are often assessed by quantitatively comparing the low-dimensional neural dynamics of the model and the brain, for example using canonical correlation analysis (CCA). However, the nature of the detailed neurobiological inferences one can draw from such efforts remains elusive. For example, to what extent does training neural networks to solve simple tasks, prevalent in neuroscientific studies, uniquely determine the low-dimensional dynamics independent of neural architectures?  Or alternatively, are the learned dynamics highly sensitive to different neural architectures?  Knowing the answer to these questions has strong implications on whether and how to use task-based RNN modeling to understand brain dynamics. To address these foundational questions, we study populations of thousands of networks of commonly used RNN architectures trained to solve neuroscientifically motivated tasks and characterize their low-dimensional dynamics via CCA and nonlinear dynamical systems analysis. We find the geometry of the dynamics can be highly sensitive to different network architectures, and further find striking dissociations between geometric similarity as measured by CCA and network function, yielding a cautionary tale. Moreover, we find that while the geometry of neural dynamics can vary greatly across architectures, the underlying computational scaffold: the topological structure of fixed points, transitions between them, limit cycles, and linearized dynamics, often appears \{{\textbackslash}it universal\} across all architectures.  Overall, this analysis of universality and individuality across large populations of RNNs provides a much needed foundation for interpreting quantitative measures of dynamical similarity between RNN and brain dynamics.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Maheswaranathan, Niru and Williams, Alex and Golub, Matthew and Ganguli, Surya and Sussillo, David},
	year = {2019},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/JKYH93CA/Maheswaranathan et al. - 2019 - Universality and individuality in neural dynamics .pdf:application/pdf},
}

@misc{noauthor_frontiers_nodate,
	title = {Frontiers {\textbar} {Gated} {Recurrent} {Units} {Viewed} {Through} the {Lens} of {Continuous} {Time} {Dynamical} {Systems}},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2021.678158/full},
	urldate = {2022-11-27},
}

@article{jordan_gated_2021,
	title = {Gated {Recurrent} {Units} {Viewed} {Through} the {Lens} of {Continuous} {Time} {Dynamical} {Systems}},
	volume = {15},
 abstract = {Gated recurrent units (GRUs) are specialized memory elements for building recurrent neural networks. Despite their incredible success on various tasks, including extracting dynamics underlying neural data, little is understood about the specific dynamics representable in a GRU network. As a result, it is both difficult to know a priori how successful a GRU network will perform on a given task, and also their capacity to mimic the underlying behavior of their biological counterparts. Using a continuous time analysis, we gain intuition on the inner workings of GRU networks. We restrict our presentation to low dimensions, allowing for a comprehensive visualization. We found a surprisingly rich repertoire of dynamical features that includes stable limit cycles (nonlinear oscillations), multi-stable dynamics with various topologies, and homoclinic bifurcations. At the same time we were unable to train GRU networks to produce continuous attractors, which are hypothesized to exist in biological neural networks. We contextualize the usefulness of different kinds of observed dynamics and support our claims experimentally.},
	urldate = {2022-11-27},
	journal = {Frontiers in Computational Neuroscience},
	author = {Jordan, Ian D. and Sokół, Piotr Aleksander and Park, Il Memming},
	year = {2021},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/MIX59R4G/Jordan et al. - 2021 - Gated Recurrent Units Viewed Through the Lens of C.pdf:application/pdf},
}

@misc{collins_capacity_2017,
	title = {Capacity and {Trainability} in {Recurrent} {Neural} {Networks}},
	abstract = {Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per-task and per-unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store an amount of task information which is linear in the number of parameters, and is approximately 5 bits per parameter. They can additionally store approximately one real number from their input history per hidden unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have slightly higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU for deeply stacked architectures.},
	urldate = {2022-11-27},
	publisher = {arXiv},
	author = {Collins, Jasmine and Sohl-Dickstein, Jascha and Sussillo, David},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/6XM25QMJ/Collins et al. - 2017 - Capacity and Trainability in Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/RJYP6ZXG/1611.html:text/html},
}

@article{azencot_consistent_2019,
	title = {Consistent {Dynamic} {Mode} {Decomposition}},
	volume = {18},
	language = {en},
	number = {3},
	urldate = {2022-11-27},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Azencot, Omri and Yin, Wotao and Bertozzi, Andrea},
	month = jan,
	year = {2019},
	pages = {1565--1585},
	file = {Azencot et al. - 2019 - Consistent Dynamic Mode Decomposition.pdf:/Users/mitchellostrow/Zotero/storage/Y6SCJF6G/Azencot et al. - 2019 - Consistent Dynamic Mode Decomposition.pdf:application/pdf},
}

@misc{krake_visualization_2020,
	title = {Visualization and {Selection} of {Dynamic} {Mode} {Decomposition} {Components} for {Unsteady} {Flow}},
	url = {http://arxiv.org/abs/2012.09633},
	abstract = {Dynamic Mode Decomposition (DMD) is a data-driven and model-free decomposition technique. It is suitable for revealing spatio-temporal features of both numerically and experimentally acquired data. Conceptually, DMD performs a low-dimensional spectral decomposition of the data into the following components: The modes, called DMD modes, encode the spatial contribution of the decomposition, whereas the DMD amplitudes specify their impact. Each associated eigenvalue, referred to as DMD eigenvalue, characterizes the frequency and growth rate of the DMD mode. In this paper, we demonstrate how the components of DMD can be utilized to obtain temporal and spatial information from time-dependent flow fields. We begin with the theoretical background of DMD and its application to unsteady flow. Next, we examine the conventional process with DMD mathematically and put it in relationship to the discrete Fourier transform. Our analysis shows that the current use of DMD components has several drawbacks. To resolve these problems we adjust the components and provide new and meaningful insights into the decomposition: We show that our improved components describe the flow more adequately. Moreover, we remove redundancies in the decomposition and clarify the interplay between components, allowing users to understand the impact of components. These new representations ,which respect the spatio-temporal character of DMD, enable two clustering methods that segment the flow into physically relevant sections and can therefore be used for the selection of DMD components. With a number of typical examples, we demonstrate that the combination of these techniques allow new insights with DMD for unsteady flow.},
	urldate = {2022-11-27},
	publisher = {arXiv},
	author = {Krake, Tim and Reinhardt, Stefan and Hlawatsch, Marcel and Eberhardt, Bernhard and Weiskopf, Daniel},
	month = dec,
	year = {2020},
	keywords = {Computer Science - Graphics, Computer Science - Human-Computer Interaction, Physics - Fluid Dynamics},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/VJT4IK4U/Krake et al. - 2020 - Visualization and Selection of Dynamic Mode Decomp.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/RQLK9BX4/2012.html:text/html},
}

@article{shea-blymyer_general_2021,
	title = {A {General} {Metric} for the {Similarity} of {Both} {Stochastic} and {Deterministic} {System} {Dynamics}},
	volume = {23},
	abstract = {Many problems in the study of dynamical systems—including identiﬁcation of effective order, detection of nonlinearity or chaos, and change detection—can be reframed in terms of assessing the similarity between dynamical systems or between a given dynamical system and a reference. We introduce a general metric of dynamical similarity that is well posed for both stochastic and deterministic systems and is informative of the aforementioned dynamical features even when only partial information about the system is available. We describe methods for estimating this metric in a range of scenarios that differ in respect to contol over the systems under study, the deterministic or stochastic nature of the underlying dynamics, and whether or not a fully informative set of variables is available. Through numerical simulation, we demonstrate the sensitivity of the proposed metric to a range of dynamical properties, its utility in mapping the dynamical properties of parameter space for a given model, and its power for detecting structural changes through time series data.},
	language = {en},
	number = {9},
	urldate = {2022-11-27},
	journal = {Entropy},
	author = {Shea-Blymyer, Colin and Roy, Subhradeep and Jantzen, Benjamin},
	month = sep,
	year = {2021},
	pages = {1191},
	file = {Shea-Blymyer et al. - 2021 - A General Metric for the Similarity of Both Stocha.pdf:/Users/mitchellostrow/Zotero/storage/G4GVL8RZ/Shea-Blymyer et al. - 2021 - A General Metric for the Similarity of Both Stocha.pdf:application/pdf},
}

@article{sussillo_opening_2013,
	title = {Opening the {Black} {Box}: {Low}-{Dimensional} {Dynamics} in {High}-{Dimensional} {Recurrent} {Neural} {Networks}},
	volume = {25},
	shorttitle = {Opening the {Black} {Box}},
	abstract = {Recurrent neural networks (RNNs) are useful tools for learning nonlinear relationships between time-varying inputs and outputs with complex temporal dependencies. Recently developed algorithms have been successful at training RNNs to perform a wide variety of tasks, but the resulting networks have been treated as black boxes: their mechanism of operation remains unknown. Here we explore the hypothesis that fixed points, both stable and unstable, and the linearized dynamics around them, can reveal crucial aspects of how RNNs implement their computations. Further, we explore the utility of linearization in areas of phase space that are not true fixed points but merely points of very slow movement. We present a simple optimization technique that is applied to trained RNNs to find the fixed and slow points of their dynamics. Linearization around these slow regions can be used to explore, or reverse-engineer, the behavior of the RNN. We describe the technique, illustrate it using simple examples, and finally showcase it on three high-dimensional RNN examples: a 3-bit flip-flop device, an input-dependent sine wave generator, and a two-point moving average. In all cases, the mechanisms of trained networks could be inferred from the sets of fixed and slow points and the linearized dynamics around them.},
	language = {en},
	number = {3},
	journal = {Neural Computation},
	author = {Sussillo, David and Barak, Omri},
	year = {2013},
	pages = {626--649},
	file = {Sussillo and Barak - 2013 - Opening the Black Box Low-Dimensional Dynamics in.pdf:/Users/mitchellostrow/Zotero/storage/LJ57FPRB/Sussillo and Barak - 2013 - Opening the Black Box Low-Dimensional Dynamics in.pdf:application/pdf},
}

@misc{kozachkov_rnns_2022,
	title = {{RNNs} of {RNNs}: {Recursive} {Construction} of {Stable} {Assemblies} of {Recurrent} {Neural} {Networks}},
	shorttitle = {{RNNs} of {RNNs}},
	url = {http://arxiv.org/abs/2106.08928},
	abstract = {Recurrent neural networks (RNNs) are widely used throughout neuroscience as models of local neural activity. Many properties of single RNNs are well characterized theoretically, but experimental neuroscience has moved in the direction of studying multiple interacting areas, and RNN theory needs to be likewise extended. We take a constructive approach towards this problem, leveraging tools from nonlinear control theory and machine learning to characterize when combinations of stable RNNs will themselves be stable. Importantly, we derive conditions which allow for massive feedback connections between interacting RNNs. We parameterize these conditions for easy optimization using gradient-based techniques, and show that stability-constrained ‘networks of networks’ can perform well on challenging sequential-processing benchmark tasks. Altogether, our results provide a principled approach towards understanding distributed, modular function in the brain.},
	language = {en},
	publisher = {arXiv},
	author = {Kozachkov, Leo and Ennis, Michaela and Slotine, Jean-Jacques},
	year = {2022},
	file = {Kozachkov et al. - 2022 - RNNs of RNNs Recursive Construction of Stable Ass.pdf:/Users/mitchellostrow/Zotero/storage/VUA8VBKA/Kozachkov et al. - 2022 - RNNs of RNNs Recursive Construction of Stable Ass.pdf:application/pdf},
}

@techreport{durstewitz_reconstructing_2022,
	type = {preprint},
	title = {Reconstructing {Computational} {Dynamics} from {Neural} {Measurements} with {Recurrent} {Neural} {Networks}},
	url = {http://biorxiv.org/lookup/doi/10.1101/2022.10.31.514408},
	abstract = {Mechanistic and computational models in neuroscience usually take the form of systems of differential or time-recursive equations. The spatio-temporal behavior of such systems is the subject of dynamical systems theory (DST). DST provides a powerful mathematical toolbox for describing and analyzing neurobiological processes at any level, from molecules to behavior, and has been a mainstay of computational neuroscience for decades. Recently, recurrent neural networks (RNNs) became a popular machine learning tool for studying the nonlinear dynamics underlying neural or behavioral observations. By training RNNs on the same behavioral tasks as employed for animal subjects and dissecting their inner workings, insights and hypotheses about the neuro-computational underpinnings of behavior could be generated. Alternatively, RNNs may be trained directly on the physiological and behavioral time series at hand. Ideally, the once trained RNN would then be able to generate data with the same temporal and geometrical properties as those observed. This is called dynamical systems reconstruction, a burgeoning field in machine learning and nonlinear dynamics. Through this more powerful approach the trained RNN becomes a surrogate for the experimentally probed system, as far as its dynamical and computational properties are concerned. The trained system can then be systematically analyzed, probed and simulated. Here we will review this highly exciting and rapidly expanding field, including recent trends in machine learning that may as yet be less well known in neuroscience. We will also discuss important validation tests, caveats, and requirements of RNN-based dynamical systems reconstruction. Concepts and applications will be illustrated with various examples from neuroscience.},
	language = {en},
	urldate = {2022-11-27},
	institution = {Neuroscience},
	author = {Durstewitz, Daniel and Koppe, Georgia and Thurm, Max Ingo},
	month = nov,
	year = {2022},
	file = {Durstewitz et al. - 2022 - Reconstructing Computational Dynamics from Neural .pdf:/Users/mitchellostrow/Zotero/storage/Z9XYW5PT/Durstewitz et al. - 2022 - Reconstructing Computational Dynamics from Neural .pdf:application/pdf},
}

@inproceedings{krause_operative_2022,
	title = {Operative dimensions in unconstrained connectivity of recurrent neural networks},
	url = {https://openreview.net/forum?id=xOK40an4ag1},
	abstract = {Recurrent Neural Networks (RNN) are commonly used models to study neural computation. However, a comprehensive understanding of how dynamics in RNN emerge from the underlying connectivity is largely lacking. Previous work derived such an understanding for RNN fulfilling very specific constraints on their connectivity, but it is unclear whether the resulting insights apply more generally. Here we study how network dynamics are related to network connectivity in RNN trained without any specific constraints on several tasks previously employed in neuroscience. Despite the apparent high-dimensional connectivity of these RNN, we show that a low-dimensional, functionally relevant subspace of the weight matrix can be found through the identification of {\textbackslash}textit\{operative\} dimensions, which we define as components of the connectivity whose removal has a large influence on local RNN dynamics. We find that a weight matrix built from only a few operative dimensions is sufficient for the RNN to operate with the original performance, implying that much of the high-dimensional structure of the trained connectivity is functionally irrelevant. The existence of a low-dimensional, operative subspace in the weight matrix simplifies the challenge of linking connectivity to network dynamics and suggests that independent network functions may be placed in specific, separate subspaces of the weight matrix to avoid catastrophic forgetting in continual learning.},
	language = {en},
	urldate = {2022-11-27},
	author = {Krause, Renate Barbara and Cook, Matthew and Kollmorgen, Sepp and Mante, Valerio and Indiveri, Giacomo},
	month = oct,
	year = {2022},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/3RMXTSW2/Krause et al. - 2022 - Operative dimensions in unconstrained connectivity.pdf:application/pdf;Snapshot:/Users/mitchellostrow/Zotero/storage/ARYBP8V9/forum.html:text/html},
}

@inproceedings{mcmahan_learning_2021,
	title = {Learning rule influences recurrent network representations but not attractor structure in decision-making tasks},
	volume = {34},
	abstract = {Recurrent neural networks (RNNs) are popular tools for studying computational dynamics in neurobiological circuits. However, due to the dizzying array of design choices, it is unclear if computational dynamics unearthed from RNNs provide reliable neurobiological inferences. Understanding the effects of design choices on RNN computation is valuable in two ways. First, invariant properties that persist in RNNs across a wide range of design choices are more likely to be candidate neurobiological mechanisms. Second, understanding what design choices lead to similar dynamical solutions reduces the burden of imposing that all design choices be totally faithful replications of biology. We focus our investigation on how RNN learning rule and task design affect RNN computation. We trained large populations of RNNs with different, but commonly used, learning rules on decision-making tasks inspired by neuroscience literature. For relatively complex tasks, we find that attractor topology is invariant to the choice of learning rule, but representational geometry is not. For simple tasks, we find that attractor topology depends on task input noise. However, when a task becomes increasingly complex, RNN attractor topology becomes invariant to input noise. Together, our results suggest that RNN dynamics are robust across learning rules but can be sensitive to the training task design, especially for simpler tasks.},
	urldate = {2022-12-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {McMahan, Brandon and Kleinman, Michael and Kao, Jonathan},
	year = {2021},
	pages = {21972--21983},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/P6NAXPZ2/McMahan et al. - 2021 - Learning rule influences recurrent network represe.pdf:application/pdf},
}

@article{lecun_tutorial_nodate,
	title = {A {Tutorial} on {Energy}-{Based} {Learning}},
	abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each conﬁguration of the variables. Inference consists in clamping the value of observed variables and ﬁnding conﬁgurations of the remaining variables that minimize the energy. Learning consists in ﬁnding an energy function in which observed conﬁgurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random ﬁelds, maximum margin Markov networks, and several manifold learning methods.},
	language = {en},
	author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc’Aurelio and Huang, Fu Jie},
	pages = {59},
	file = {LeCun et al. - A Tutorial on Energy-Based Learning.pdf:/Users/mitchellostrow/Zotero/storage/M8T6KY8V/LeCun et al. - A Tutorial on Energy-Based Learning.pdf:application/pdf},
}

@inproceedings{lake_generalization_2018,
	title = {Generalization without {Systematicity}: {On} the {Compositional} {Skills} of {Sequence}-to-{Sequence} {Recurrent} {Networks}},
	shorttitle = {Generalization without {Systematicity}},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.},
	language = {en},
	urldate = {2022-12-09},
	booktitle = {Proceedings of the 35th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Lake, Brenden and Baroni, Marco},
	month = jul,
	year = {2018},
	pages = {2873--2882},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/ML6DMNXB/Lake and Baroni - 2018 - Generalization without Systematicity On the Compo.pdf:application/pdf;Supplementary PDF:/Users/mitchellostrow/Zotero/storage/5KKHNAYS/Lake and Baroni - 2018 - Generalization without Systematicity On the Compo.pdf:application/pdf},
}

@misc{morcos_insights_2018,
	title = {Insights on representational similarity in neural networks with canonical correlation},
	url = {http://arxiv.org/abs/1806.05759},
	abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al., 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
	publisher = {arXiv},
	author = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
	month = oct,
	year = {2018},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/H7LRIDRY/Morcos et al. - 2018 - Insights on representational similarity in neural .pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/AJT8NET5/1806.html:text/html},
}

@article{wu_challenges_nodate,
	title = {Challenges in dynamic mode decomposition},
	volume = {18},
	abstract = {Dynamic mode decomposition (DMD) is a powerful tool for extracting spatial and temporal patterns from multi-dimensional time series, and it has been used successfully in a wide range of fields, including fluid mechanics, robotics and neuroscience. Two of the main challenges remaining in DMD research are noise sensitivity and issues related to Krylov space closure when modelling nonlinear systems. Here, we investigate the combination of noise and nonlinearity in a controlled setting, by studying a class of systems with linear latent dynamics which are observed via multinomial observables. Our numerical models include system and measurement noise. We explore the influences of dataset metrics, the spectrum of the latent dynamics, the normality of the system matrix and the geometry of the dynamics. Our results show that even for these very mildly nonlinear conditions, DMD methods often fail to recover the spectrum and can have poor predictive ability. Our work is motivated by our experience modelling multilegged robot data, where we have encountered great difficulty in reconstructing time series for oscillatory systems with intermediate transients, which decay only slightly faster than a period.},
	number = {185},
	urldate = {2022-12-28},
	journal = {Journal of The Royal Society Interface},
	author = {Wu, Ziyou and Brunton, Steven L. and Revzen, Shai},
	keywords = {dynamic mode decomposition dynamical systems, locomotion},
	pages = {20210686},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/9SSV4SCV/Wu et al. - Challenges in dynamic mode decomposition.pdf:application/pdf},
}

@misc{linderman_recurrent_2016,
	title = {Recurrent switching linear dynamical systems},
	url = {http://arxiv.org/abs/1610.08466},
	abstract = {Many natural systems, such as neurons firing in the brain or basketball teams traversing a court, give rise to time series data with complex, nonlinear dynamics. We can gain insight into these systems by decomposing the data into segments that are each explained by simpler dynamic units. Building on switching linear dynamical systems (SLDS), we present a new model class that not only discovers these dynamical units, but also explains how their switching behavior depends on observations or continuous latent states. These "recurrent" switching linear dynamical systems provide further insight by discovering the conditions under which each unit is deployed, something that traditional SLDS models fail to do. We leverage recent algorithmic advances in approximate inference to make Bayesian inference in these models easy, fast, and scalable.},
	publisher = {arXiv},
	author = {Linderman, Scott W. and Miller, Andrew C. and Adams, Ryan P. and Blei, David M. and Paninski, Liam and Johnson, Matthew J.},
	year = {2016},
	keywords = {Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/KWWNB22X/Linderman et al. - 2016 - Recurrent switching linear dynamical systems.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/5PK8EXSU/1610.html:text/html},
}


@article{galgali_residual_2023,
	title = {Residual dynamics resolves recurrent contributions to neural computation},
	abstract = {Relating neural activity to behavior requires an understanding of how neural computations arise from the coordinated dynamics of distributed, recurrently connected neural populations. However, inferring the nature of recurrent dynamics from partial recordings of a neural circuit presents considerable challenges. Here we show that some of these challenges can be overcome by a fine-grained analysis of the dynamics of neural residuals—that is, trial-by-trial variability around the mean neural population trajectory for a given task condition. Residual dynamics in macaque prefrontal cortex (PFC) in a saccade-based perceptual decision-making task reveals recurrent dynamics that is time dependent, but consistently stable, and suggests that pronounced rotational structure in PFC trajectories during saccades is driven by inputs from upstream areas. The properties of residual dynamics restrict the possible contributions of PFC to decision-making and saccade generation and suggest a path toward fully characterizing distributed neural computations with large-scale neural recordings and targeted causal perturbations.},
	journal = {Nature Neuroscience},
	author = {Galgali, Aniruddh R. and Sahani, Maneesh and Mante, Valerio},
	year = {2023},
},


@article{duong2022representational,
  title={Representational dissimilarity metric spaces for stochastic neural networks},
  author={Duong, Lyndon R and Zhou, Jingyang and Nassar, Josue and Berman, Jules and Olieslagers, Jeroen and Williams, Alex H},
  journal={arXiv preprint arXiv:2211.11665},
  year={2022}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@misc{raghu_svcca_2017,
	title = {{SVCCA}: {Singular} {Vector} {Canonical} {Correlation} {Analysis} for {Deep} {Learning} {Dynamics} and {Interpretability}},
	shorttitle = {{SVCCA}},
	abstract = {We propose a new technique, Singular Vector Canonical Correlation Analysis (SVCCA), a tool for quickly comparing two representations in a way that is both invariant to affine transform (allowing comparison between different layers and networks) and fast to compute (allowing more comparisons to be calculated than with previous methods). We deploy this tool to measure the intrinsic dimensionality of layers, showing in some cases needless over-parameterization; to probe learning dynamics throughout training, finding that networks converge to final representations from the bottom up; to show where class-specific information in networks is formed; and to suggest new training regimes that simultaneously save computation and overfit less. Code: https://github.com/google/svcca/},
	publisher = {arXiv},
	author = {Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
	year = {2017},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/34K8LUIU/Raghu et al. - 2017 - SVCCA Singular Vector Canonical Correlation Analy.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/RG5SDXGV/1706.html:text/html},
}



@article{gottwald_detecting_nodate,
	title = {{DETECTING} {REGIME} {TRANSITIONS} {IN} {TIME} {SERIES} {USING} {DYNAMIC} {MODE} {DECOMPOSITION}},
	abstract = {We employ the framework of the Koopman operator and dynamic mode decomposition to devise a computationally cheap and easily implementable method to detect transient dynamics and regime changes in time series. We argue that typically transient dynamics experiences the full state space dimension with subsequent fast relaxation towards the attractor. In equilibrium, on the other hand, the dynamics evolves on a slower time scale on a lower dimensional attractor. The reconstruction error of a dynamic mode decomposition is used to monitor the inability of the time series to resolve the fast relaxation towards the attractor as well as the eﬀective dimension of the dynamics. We illustrate our method by detecting transient dynamics in the Kuramoto-Sivashinsky equation. We further apply our method to atmospheric reanalysis data; our diagnostics detects the transition from a predominantly negative North Atlantic Oscillation (NAO) to a predominantly positive NAO around 1970, as well as the recently found regime change in the Southern Hemisphere atmospheric circulation around 1970.},
	language = {en},
	author = {Gottwald, Georg A and Gugole, Federica},
	file = {Gottwald and Gugole - DETECTING REGIME TRANSITIONS IN TIME SERIES USING .pdf:/Users/mitchellostrow/Zotero/storage/NUT58D5D/Gottwald and Gugole - DETECTING REGIME TRANSITIONS IN TIME SERIES USING .pdf:application/pdf},
}

@misc{smith_reverse_2021,
	title = {Reverse engineering recurrent neural networks with {Jacobian} switching linear dynamical systems},
	url = {http://arxiv.org/abs/2111.01256},
	abstract = {Recurrent neural networks (RNNs) are powerful models for processing time-series data, but it remains challenging to understand how they function. Improving this understanding is of substantial interest to both the machine learning and neuroscience communities. The framework of reverse engineering a trained RNN by linearizing around its fixed points has provided insight, but the approach has significant challenges. These include difficulty choosing which fixed point to expand around when studying RNN dynamics and error accumulation when reconstructing the nonlinear dynamics with the linearized dynamics. We present a new model that overcomes these limitations by co-training an RNN with a novel switching linear dynamical system (SLDS) formulation. A first-order Taylor series expansion of the co-trained RNN and an auxiliary function trained to pick out the RNN's fixed points govern the SLDS dynamics. The results are a trained SLDS variant that closely approximates the RNN, an auxiliary function that can produce a fixed point for each point in state-space, and a trained nonlinear RNN whose dynamics have been regularized such that its first-order terms perform the computation, if possible. This model removes the post-training fixed point optimization and allows us to unambiguously study the learned dynamics of the SLDS at any point in state-space. It also generalizes SLDS models to continuous manifolds of switching points while sharing parameters across switches. We validate the utility of the model on two synthetic tasks relevant to previous work reverse engineering RNNs. We then show that our model can be used as a drop-in in more complex architectures, such as LFADS, and apply this LFADS hybrid to analyze single-trial spiking activity from the motor system of a non-human primate.},
	publisher = {arXiv},
	author = {Smith, Jimmy T. H. and Linderman, Scott W. and Sussillo, David},
	year = {2021},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/YJMPVR7F/Smith et al. - 2021 - Reverse engineering recurrent neural networks with.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/MWXYQ2KN/2111.html:text/html},
}

@misc{aitken_geometry_2022,
	title = {The geometry of integration in text classification {RNNs}},
	url = {http://arxiv.org/abs/2010.15114},
	abstract = {Despite the widespread application of recurrent neural networks (RNNs) across a variety of tasks, a unified understanding of how RNNs solve these tasks remains elusive. In particular, it is unclear what dynamical patterns arise in trained RNNs, and how those patterns depend on the training dataset or task. This work addresses these questions in the context of a specific natural language processing task: text classification. Using tools from dynamical systems analysis, we study recurrent networks trained on a battery of both natural and synthetic text classification tasks. We find the dynamics of these trained RNNs to be both interpretable and low-dimensional. Specifically, across architectures and datasets, RNNs accumulate evidence for each class as they process the text, using a low-dimensional attractor manifold as the underlying mechanism. Moreover, the dimensionality and geometry of the attractor manifold are determined by the structure of the training dataset; in particular, we describe how simple word-count statistics computed on the training dataset can be used to predict these properties. Our observations span multiple architectures and datasets, reflecting a common mechanism RNNs employ to perform text classification. To the degree that integration of evidence towards a decision is a common computational primitive, this work lays the foundation for using dynamical systems techniques to study the inner workings of RNNs.},
	publisher = {arXiv},
	author = {Aitken, Kyle and Ramasesh, Vinay V. and Garg, Ankush and Cao, Yuan and Sussillo, David and Maheswaranathan, Niru},
	month = jun,
	year = {2022},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/K5CXFNRW/Aitken et al. - 2022 - The geometry of integration in text classification.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/6CIPHZTZ/2010.html:text/html},
}

@misc{brenner_tractable_2022,
	title = {Tractable {Dendritic} {RNNs} for {Reconstructing} {Nonlinear} {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2207.02542},
	abstract = {In many scientific disciplines, we are interested in inferring the nonlinear dynamical system underlying a set of observed time series, a challenging task in the face of chaotic behavior and noise. Previous deep learning approaches toward this goal often suffered from a lack of interpretability and tractability. In particular, the high-dimensional latent spaces often required for a faithful embedding, even when the underlying dynamics lives on a lower-dimensional manifold, can hamper theoretical analysis. Motivated by the emerging principles of dendritic computation, we augment a dynamically interpretable and mathematically tractable piecewise-linear (PL) recurrent neural network (RNN) by a linear spline basis expansion. We show that this approach retains all the theoretically appealing properties of the simple PLRNN, yet boosts its capacity for approximating arbitrary nonlinear dynamical systems in comparatively low dimensions. We employ two frameworks for training the system, one combining back-propagation-through-time (BPTT) with teacher forcing, and another based on fast and scalable variational inference. We show that the dendritically expanded PLRNN achieves better reconstructions with fewer parameters and dimensions on various dynamical systems benchmarks and compares favorably to other methods, while retaining a tractable and interpretable structure.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Brenner, Manuel and Hess, Florian and Mikhaeil, Jonas M. and Bereska, Leonard and Monfared, Zahra and Kuo, Po-Chen and Durstewitz, Daniel},
	month = jul,
	year = {2022},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Nonlinear Sciences - Chaotic Dynamics, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/GAAZGBVJ/Brenner et al. - 2022 - Tractable Dendritic RNNs for Reconstructing Nonlin.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/7E9AXSWA/2207.html:text/html},
}

@misc{azencot_forecasting_2020,
	title = {Forecasting {Sequential} {Data} using {Consistent} {Koopman} {Autoencoders}},
	url = {http://arxiv.org/abs/2003.02236},
	abstract = {Recurrent neural networks are widely used on time series data, yet such models often ignore the underlying physical structures in such sequences. A new class of physics-based methods related to Koopman theory has been introduced, offering an alternative for processing nonlinear dynamical systems. In this work, we propose a novel Consistent Koopman Autoencoder model which, unlike the majority of existing work, leverages the forward and backward dynamics. Key to our approach is a new analysis which explores the interplay between consistent dynamics and their associated Koopman operators. Our network is directly related to the derived analysis, and its computational requirements are comparable to other baselines. We evaluate our method on a wide range of high-dimensional and short-term dependent problems, and it achieves accurate estimates for significant prediction horizons, while also being robust to noise.},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Azencot, Omri and Erichson, N. Benjamin and Lin, Vanessa and Mahoney, Michael W.},
	year = {2020},
	keywords = {Computer Science - Machine Learning, Mathematics - Dynamical Systems, Physics - Computational Physics},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/ZK79NH8M/Azencot et al. - 2020 - Forecasting Sequential Data using Consistent Koopm.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/I8KSZZ4T/2003.html:text/html},
}

@inproceedings{takeishi_learning_2017,
	title = {Learning {Koopman} {Invariant} {Subspaces} for {Dynamic} {Mode} {Decomposition}},
	volume = {30},
	abstract = {Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Takeishi, Naoya and Kawahara, Yoshinobu and Yairi, Takehisa},
	year = {2017},
}

@article{schmid_dynamic_2010,
	title = {Dynamic mode decomposition of numerical and experimental data},
	volume = {656},
	abstract = {The description of coherent features of fluid flow is essential to our understanding of fluid-dynamical and transport processes. A method is introduced that is able to extract dynamic information from flow fields that are either generated by a (direct) numerical simulation or visualized/measured in a physical experiment. The extracted dynamic modes, which can be interpreted as a generalization of global stability modes, can be used to describe the underlying physical mechanisms captured in the data sequence or to project large-scale problems onto a dynamical system of significantly fewer degrees of freedom. The concentration on subdomains of the flow field where relevant dynamics is expected allows the dissection of a complex flow into regions of localized instability phenomena and further illustrates the flexibility of the method, as does the description of the dynamics within a spatial framework. Demonstrations of the method are presented consisting of a plane channel flow, flow over a two-dimensional cavity, wake flow behind a flexible membrane and a jet passing between two cylinders.},
	language = {en},
	urldate = {2023-02-01},
	journal = {Journal of Fluid Mechanics},
	author = {Schmid, Peter J.},
	year = {2010},
	pages = {5--28},
	file = {Schmid - 2010 - Dynamic mode decomposition of numerical and experi.pdf:/Users/mitchellostrow/Zotero/storage/LWP4BG5Y/Schmid - 2010 - Dynamic mode decomposition of numerical and experi.pdf:application/pdf},
}



@article{arbabi_ergodic_2017,
	title = {Ergodic {Theory}, {Dynamic} {Mode} {Decomposition}, and {Computation} of {Spectral} {Properties} of the {Koopman} {Operator}},
	volume = {16},
	abstract = {We establish the convergence of a class of numerical algorithms, known as dynamic mode decomposition (DMD), for computation of the eigenvalues and eigenfunctions of the inﬁnite-dimensional Koopman operator. The algorithms act on data coming from observables on a state space, arranged in Hankel-type matrices. The proofs utilize the assumption that the underlying dynamical system is ergodic. This includes the classical measure-preserving systems, as well as systems whose attractors support a physical measure. Our approach relies on the observation that vector projections in DMD can be used to approximate the function projections by the virtue of Birkhoﬀ’s ergodic theorem. Using this fact, we show that applying DMD to Hankel data matrices in the limit of inﬁnite-time observations yields the true Koopman eigenfunctions and eigenvalues. We also show that the singular value decomposition, which is the central part of most DMD algorithms, converges to the proper orthogonal decomposition of observables. We use this result to obtain a representation of the dynamics of systems with continuous spectrum based on the lifting of the coordinates to the space of observables. The numerical application of these methods is demonstrated using well-known dynamical systems and examples from computational ﬂuid dynamics.},
	language = {en},
	number = {4},
	urldate = {2023-02-01},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Arbabi, Hassan and Mezić, Igor},
	month = jan,
	year = {2017},
	pages = {2096--2126},
	file = {Arbabi and Mezić - 2017 - Ergodic Theory, Dynamic Mode Decomposition, and Co.pdf:/Users/mitchellostrow/Zotero/storage/KPPJMDUE/Arbabi and Mezić - 2017 - Ergodic Theory, Dynamic Mode Decomposition, and Co.pdf:application/pdf},
}

@article{jordan_gated_2021-1,
	title = {Gated recurrent units viewed through the lens of continuous time dynamical systems},
	volume = {15},
	url = {http://arxiv.org/abs/1906.01005},
	abstract = {Gated recurrent units (GRUs) are specialized memory elements for building recurrent neural networks. Despite their incredible success on various tasks, including extracting dynamics underlying neural data, little is understood about the specific dynamics representable in a GRU network. As a result, it is both difficult to know a priori how successful a GRU network will perform on a given task, and also their capacity to mimic the underlying behavior of their biological counterparts. Using a continuous time analysis, we gain intuition on the inner workings of GRU networks. We restrict our presentation to low dimensions, allowing for a comprehensive visualization. We found a surprisingly rich repertoire of dynamical features that includes stable limit cycles (nonlinear oscillations), multi-stable dynamics with various topologies, and homoclinic bifurcations. At the same time we were unable to train GRU networks to produce continuous attractors, which are hypothesized to exist in biological neural networks. We contextualize the usefulness of different kinds of observed dynamics and support our claims experimentally.},
	urldate = {2023-02-01},
	journal = {Frontiers in Computational Neuroscience},
	author = {Jordan, Ian D. and Sokol, Piotr Aleksander and Park, Il Memming},
	month = jul,
	year = {2021},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {678158},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/HBPT8IZE/Jordan et al. - 2021 - Gated recurrent units viewed through the lens of c.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/ETCSUQUV/1906.html:text/html},
}

@article{okazawa_representational_2021,
	title = {Representational geometry of perceptual decisions in the monkey parietal cortex},
	volume = {184},
	abstract = {Lateral intraparietal (LIP) neurons represent formation of perceptual decisions involving eye movements. In circuit models for these decisions, neural ensembles that encode actions compete to form decisions. Consequently, representation and readout of the decision variables (DVs) are implemented similarly for decisions with identical competing actions, irrespective of input and task context differences. Further, DVs are encoded as partially potentiated action plans through balance of activity of action-selective ensembles. Here, we test those core principles. We show that in a novel face-discrimination task, LIP firing rates decrease with supporting evidence, contrary to conventional motion-discrimination tasks. These opposite response patterns arise from similar mechanisms in which decisions form along curved population-response manifolds misaligned with action representations. These manifolds rotate in state space based on context, indicating distinct optimal readouts for different tasks. We show similar manifolds in lateral and medial prefrontal cortices, suggesting similar representational geometry across decision-making circuits.},
	language = {en},
	number = {14},
	urldate = {2023-02-01},
	journal = {Cell},
	author = {Okazawa, Gouki and Hatch, Christina E. and Mancoo, Allan and Machens, Christian K. and Kiani, Roozbeh},
	month = jul,
	year = {2021},
	keywords = {circuit model, decision making, face perception, frontal cortex, macaque monkey, motion perception, neural response manifold, parietal cortex, representational geometry, task difficulty},
	pages = {3748--3761.e18},
	file = {Full Text:/Users/mitchellostrow/Zotero/storage/7MLXSE2M/Okazawa et al. - 2021 - Representational geometry of perceptual decisions .pdf:application/pdf;ScienceDirect Snapshot:/Users/mitchellostrow/Zotero/storage/USI24HHF/S0092867421006528.html:text/html},
},


@article{kriegeskorte_representational_2008,
	title = {Representational similarity analysis - connecting the branches of systems neuroscience},
	volume = {2},
	abstract = {A fundamental challenge for systems neuroscience is to quantitatively relate its three major branches of research: brain-activity measurement, behavioral measurement, and computational modeling. Using measured brain-activity patterns to evaluate computational network models is complicated by the need to define the correspondency between the units of the model and the channels of the brain-activity data, e.g., single-cell recordings or voxels from functional magnetic resonance imaging (fMRI). Similar correspondency problems complicate relating activity patterns between different modalities of brain-activity measurement (e.g., fMRI and invasive or scalp electrophysiology), and between subjects and species. In order to bridge these divides, we suggest abstracting from the activity patterns themselves and computing representational dissimilarity matrices (RDMs), which characterize the information carried by a given representation in a brain or model. Building on a rich psychological and mathematical literature on similarity analysis, we propose a new experimental and data-analytical framework called representational similarity analysis (RSA), in which multi-channel measures of neural activity are quantitatively related to each other and to computational theory and behavior by comparing RDMs. We demonstrate RSA by relating representations of visual objects as measured with fMRI in early visual cortex and the fusiform face area to computational models spanning a wide range of complexities. The RDMs are simultaneously related via second-level application of multidimensional scaling and tested using randomization and bootstrap techniques. We discuss the broad potential of RSA, including novel approaches to experimental design, and argue that these ideas, which have deep roots in psychology and neuroscience, will allow the integrated quantitative analysis of data from all three branches, thus contributing to a more unified systems neuroscience.},
	journal = {Frontiers in Systems Neuroscience},
	author = {Kriegeskorte, Nikolaus and Mur, Marieke and Bandettini, Peter},
	year = {2008},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/ZABHQ7II/Kriegeskorte et al. - 2008 - Representational similarity analysis - connecting .pdf:application/pdf},
},


@ARTICLE{hopfield,
  title    = "Neural networks and physical systems with emergent collective
              computational abilities",
  author   = "Hopfield, J J",
  abstract = "Computational properties of use of biological organisms or to the
              construction of computers can emerge as collective properties of
              systems having a large number of simple equivalent components (or
              neurons). The physical meaning of content-addressable memory is
              described by an appropriate phase space flow of the state of a
              system. A model of such a system is given, based on aspects of
              neurobiology but readily adapted to integrated circuits. The
              collective properties of this model produce a content-addressable
              memory which correctly yields an entire memory from any subpart
              of sufficient size. The algorithm for the time evolution of the
              state of the system is based on asynchronous parallel processing.
              Additional emergent collective properties include some capacity
              for generalization, familiarity recognition, categorization,
              error correction, and time sequence retention. The collective
              properties are only weakly sensitive to details of the modeling
              or the failure of individual devices.",
  journal  = "Proc. Natl. Acad. Sci. U. S. A.",
  volume   =  79,
  number   =  8,
  pages    = "2554--2558",
  month    =  apr,
  year     =  1982,
  language = "en"
},


@ARTICLE{Favela,
  title    = "The dynamical renaissance in neuroscience",
  author   = "Favela, Luis H",
  abstract = "Although there is a substantial philosophical literature on
              dynamical systems theory in the cognitive sciences, the same is
              not the case for neuroscience. This paper attempts to motivate
              increased discussion via a set of overlapping issues. The first
              aim is primarily historical and is to demonstrate that dynamical
              systems theory is currently experiencing a renaissance in
              neuroscience. Although dynamical concepts and methods are
              becoming increasingly popular in contemporary neuroscience, the
              general approach should not be viewed as something entirely new
              to neuroscience. Instead, it is more appropriate to view the
              current developments as making central again approaches that
              facilitated some of neuroscience's most significant early
              achievements, namely, the Hodgkin--Huxley and FitzHugh--Nagumo
              models. The second aim is primarily critical and defends a
              version of the ``dynamical hypothesis'' in neuroscience. Whereas
              the original version centered on defending a noncomputational and
              nonrepresentational account of cognition, the version I have in
              mind is broader and includes both cognition and the neural
              systems that realize it as well. In view of that, I discuss
              research on motor control as a paradigmatic example demonstrating
              that the concepts and methods of dynamical systems theory are
              increasingly and successfully being applied to neural systems in
              contemporary neuroscience. More significantly, such applications
              are motivating a stronger metaphysical claim, that is,
              understanding neural systems as being dynamical systems, which
              includes not requiring appeal to representations to explain or
              understand those phenomena. Taken together, the historical claim
              and the critical claim demonstrate that the dynamical hypothesis
              is undergoing a renaissance in contemporary neuroscience.",
  journal  = "Synthese",
  volume   =  199,
  number   =  1,
  pages    = "2103--2127",
  month    =  dec,
  year     =  2021
},


@ARTICLE{Sussillo-CTD,
  title    = "Neural circuits as computational dynamical systems",
  author   = "Sussillo, David",
  abstract = "Many recent studies of neurons recorded from cortex reveal
              complex temporal dynamics. How such dynamics embody the
              computations that ultimately lead to behavior remains a mystery.
              Approaching this issue requires developing plausible hypotheses
              couched in terms of neural dynamics. A tool ideally suited to aid
              in this question is the recurrent neural network (RNN). RNNs
              straddle the fields of nonlinear dynamical systems and machine
              learning and have recently seen great advances in both theory and
              application. I summarize recent theoretical and technological
              advances and highlight an example of how RNNs helped to explain
              perplexing high-dimensional neurophysiological data in the
              prefrontal cortex.",
  journal  = "Curr. Opin. Neurobiol.",
  volume   =  25,
  pages    = "156--163",
  month    =  apr,
  year     =  2014,
  language = "en"
}


@UNPUBLISHED{Durstewitz-Dyn,
  title    = "Reconstructing Computational Dynamics from Neural Measurements
              with Recurrent Neural Networks",
  author   = "Durstewitz, Daniel and Koppe, Georgia and Thurm, Max Ingo",
  abstract = "Mechanistic and computational models in neuroscience usually take
              the form of systems of differential or time-recursive equations.
              The spatio-temporal behavior of such systems is the subject of
              dynamical systems theory (DST). DST provides a powerful
              mathematical toolbox for describing and analyzing neurobiological
              processes at any level, from molecules to behavior, and has been
              a mainstay of computational neuroscience for decades. Recently,
              recurrent neural networks (RNNs) became a popular machine
              learning tool for studying the nonlinear dynamics underlying
              neural or behavioral observations. By training RNNs on the same
              behavioral tasks as employed for animal subjects and dissecting
              their inner workings, insights and hypotheses about the
              neuro-computational underpinnings of behavior could be generated.
              Alternatively, RNNs may be trained directly on the physiological
              and behavioral time series at hand. Ideally, the once trained RNN
              would then be able to generate data with the same temporal and
              geometrical properties as those observed. This is called
              dynamical systems reconstruction , a burgeoning field in machine
              learning and nonlinear dynamics. Through this more powerful
              approach the trained RNN becomes a surrogate for the
              experimentally probed system, as far as its dynamical and
              computational properties are concerned. The trained system can
              then be systematically analyzed, probed and simulated. Here we
              will review this highly exciting and rapidly expanding field,
              including recent trends in machine learning that may as yet be
              less well known in neuroscience. We will also discuss important
              validation tests, caveats, and requirements of RNN-based
              dynamical systems reconstruction. Concepts and applications will
              be illustrated with various examples from neuroscience. \#\#\#
              Competing Interest Statement The authors have declared no
              competing interest.",
  journal  = "bioRxiv",
  pages    = "2022.10.31.514408",
  month    =  nov,
  year     =  2022,
  language = "en"
},


@ARTICLE{Mongillo2008-iv,
  title    = "Synaptic Theory of Working Memory",
  author   = "Mongillo, Gianluigi and Barak, Omri and Tsodyks, Misha",
  abstract = "It is usually assumed that enhanced spiking activity in the form
              of persistent reverberation for several seconds is the neural
              correlate of working memory. Here, we propose that working memory
              is sustained by calcium-mediated synaptic facilitation in the
              recurrent connections of neocortical networks. In this account,
              the presynaptic residual calcium is used as a buffer that is
              loaded, refreshed, and read out by spiking activity. Because of
              the long time constants of calcium kinetics, the refresh rate can
              be low, resulting in a mechanism that is metabolically efficient
              and robust. The duration and stability of working memory can be
              regulated by modulating the spontaneous activity in the network.",
  journal  = "Science",
  pages    = "1543--1546",
  year     =  2008
},


@ARTICLE{Churchland2012-ma,
  title    = "Neural population dynamics during reaching",
  author   = "Churchland, Mark M and Cunningham, John P and Kaufman, Matthew T
              and Foster, Justin D and Nuyujukian, Paul and Ryu, Stephen I and
              Shenoy, Krishna V",
  abstract = "Most theories of motor cortex have assumed that neural activity
              represents movement parameters. This view derives from what is
              known about primary visual cortex, where neural activity
              represents patterns of light. Yet it is unclear how well the
              analogy between motor and visual cortex holds. Single-neuron
              responses in motor cortex are complex, and there is marked
              disagreement regarding which movement parameters are represented.
              A better analogy might be with other motor systems, where a
              common principle is rhythmic neural activity. Here we find that
              motor cortex responses during reaching contain a brief but strong
              oscillatory component, something quite unexpected for a
              non-periodic behaviour. Oscillation amplitude and phase followed
              naturally from the preparatory state, suggesting a mechanistic
              role for preparatory neural activity. These results demonstrate
              an unexpected yet surprisingly simple structure in the population
              response. This underlying structure explains many of the
              confusing features of individual neural responses.",
  journal  = "Nature",
  volume   =  487,
  number   =  7405,
  pages    = "51--56",
  month    =  jul,
  year     =  2012,
  language = "en"
},




@article{kalman_mathematical_1963,
	title = {Mathematical {Description} of {Linear} {Dynamical} {Systems}},
	volume = {1},
	url = {http://epubs.siam.org/doi/10.1137/0301010},
	abstract = {There are two different ways of describing dynamical systems: (i) by means of state w.riables and (if) by input/output relations. The first method may be regarded as an axiomatization of Newton’s laws of mechanics and is taken to be the basic definition of a system.},
	language = {en},
	number = {2},
	urldate = {2023-02-01},
	journal = {Journal of the Society for Industrial and Applied Mathematics Series A Control},
	author = {Kalman, R. E.},
	month = jan,
	year = {1963},
	pages = {152--192},
	file = {Kalman - 1963 - Mathematical Description of Linear Dynamical Syste.pdf:/Users/mitchellostrow/Zotero/storage/55CQ2X6H/Kalman - 1963 - Mathematical Description of Linear Dynamical Syste.pdf:application/pdf},
}


@incollection{takens,
	title = {Detecting strange attractors in turbulence},
	language = {en},
	booktitle = {Dynamical {Systems} and {Turbulence}},
    publisher = {Warwick},
	author = {Takens, Floris},
	year = {1981},
	}



@misc{bordelon_influence_2022,
	title = {The {Influence} of {Learning} {Rule} on {Representation} {Dynamics} in {Wide} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2210.02157},
	abstract = {It is unclear how changing the learning rule of a deep neural network alters its learning dynamics and representations. To gain insight into the relationship between learned features, function approximation, and the learning rule, we analyze infinite-width deep networks trained with gradient descent (GD) and biologically-plausible alternatives including feedback alignment (FA), direct feedback alignment (DFA), and error modulated Hebbian learning (Hebb), as well as gated linear networks (GLN). We show that, for each of these learning rules, the evolution of the output function at infinite width is governed by a time varying effective neural tangent kernel (eNTK). In the lazy training limit, this eNTK is static and does not evolve, while in the rich mean-field regime this kernel's evolution can be determined self-consistently with dynamical mean field theory (DMFT). This DMFT enables comparisons of the feature and prediction dynamics induced by each of these learning rules. In the lazy limit, we find that DFA and Hebb can only learn using the last layer features, while full FA can utilize earlier layers with a scale determined by the initial correlation between feedforward and feedback weight matrices. In the rich regime, DFA and FA utilize a temporally evolving and depth-dependent NTK. Counterintuitively, we find that FA networks trained in the rich regime exhibit more feature learning if initialized with smaller correlation between the forward and backward pass weights. GLNs admit a very simple formula for their lazy limit kernel and preserve conditional Gaussianity of their preactivations under gating functions. Error modulated Hebb rules show very small task-relevant alignment of their kernels and perform most task relevant learning in the last layer.},
	urldate = {2023-05-02},
	publisher = {arXiv},
	author = {Bordelon, Blake and Pehlevan, Cengiz},
	year = {2022},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Condensed Matter - Statistical Mechanics, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/ZQGZYEPQ/Bordelon and Pehlevan - 2022 - The Influence of Learning Rule on Representation D.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/XQGC5WGX/2210.html:text/html},
}


@inproceedings{nayebi_identifying_2020,
	title = {Identifying {Learning} {Rules} {From} {Neural} {Network} {Observables}},
	volume = {33},
	abstract = {The brain modifies its synaptic strengths during learning in order to better adapt to its environment. However, the underlying plasticity rules that govern learning are unknown. Many proposals have been suggested, including Hebbian mechanisms, explicit error backpropagation, and a variety of alternatives. It is an open question as to what specific experimental measurements would need to be made to determine whether any given learning rule is operative in a real biological system. In this work, we take a "virtual experimental" approach to this problem. Simulating idealized neuroscience experiments with artificial neural networks, we generate a large-scale dataset of learning trajectories of aggregate statistics measured in a variety of neural network architectures, loss functions, learning rule hyperparameters, and parameter initializations. We then take a discriminative approach, training linear and simple non-linear classifiers to identify learning rules from features based on these observables. We show that different classes of learning rules can be separated solely on the basis of aggregate statistics of the weights, activations, or instantaneous layer-wise activity changes, and that these results generalize to limited access to the trajectory and held-out architectures and learning curricula. We identify the statistics of each observable that are most relevant for rule identification, finding that statistics from network activities across training are more robust to unit undersampling and measurement noise than those obtained from the synaptic strengths. Our results suggest that activation patterns, available from electrophysiological recordings of post-synaptic activities on the order of several hundred units, frequently measured at wider intervals over the course of learning, may provide a good basis on which to identify learning rules.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Nayebi, Aran and Srivastava, Sanjana and Ganguli, Surya and Yamins, Daniel L},
	year = {2020},
	pages = {2639--2650},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/QWVI5LJF/Nayebi et al. - 2020 - Identifying Learning Rules From Neural Network Obs.pdf:application/pdf},
}


@article{kamb_time_delay_2020,
	title = {Time-{Delay} {Observables} for {Koopman}: {Theory} and {Applications}},
	volume = {19},
	shorttitle = {Time-{Delay} {Observables} for {Koopman}},
	abstract = {A major challenge in the study of dynamical systems is that of model discovery: turning data into models that are not just predictive, but provide insight into the nature of the underlying dynamical system that generated the data. This problem is made more difficult by the fact that many systems of interest exhibit diverse behaviors across multiple time scales. We introduce a number of data-driven strategies for discovering nonlinear multiscale dynamical systems and their embeddings from data. We consider two canonical cases: (i) systems for which we have full measurements of the governing variables and (ii) systems for which we have incomplete measurements. For systems with full state measurements, we show that the recent sparse identification of nonlinear dynamical systems (SINDy) method can discover governing equations with relatively little data, provided that accurate measurements of the derivatives can be computed from the data. We introduce a sampling method that allows SINDy to scale efficiently to problems with multiple time scales; specifically, we can discover distinct governing equations at slow and fast scales. For systems with incomplete observations, we show that the Hankel alternative view of Koopman (HAVOK) method, based on time-delay embedding coordinates, can be used to obtain a linear model and Koopman invariant measurement system that nearly perfectly captures the dynamics of nonlinear quasiperiodic systems on the attractor. We introduce two strategies for using HAVOK on systems with multiple time scales. Together, our approaches provide a suite of mathematical strategies for reducing the data required to discover and model nonlinear multiscale systems.},
	number = {2},
	journal = {SIAM Journal on Applied Dynamical Systems},
	author = {Kamb, Mason and Kaiser, Eurika and Brunton, Steven L. and Kutz, J. Nathan},
	year = {2020},
	pages = {886--917},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/SRQWT6SQ/Kamb et al. - 2020 - Time-Delay Observables for Koopman Theory and App.pdf:application/pdf},
}


@misc{brunton_modern_2021,
	title = {Modern {Koopman} {Theory} for {Dynamical} {Systems}},
	url = {http://arxiv.org/abs/2102.12086},
	abstract = {The field of dynamical systems is being transformed by the mathematical tools and algorithms emerging from modern computing and data science. First-principles derivations and asymptotic reductions are giving way to data-driven approaches that formulate models in operator theoretic or probabilistic frameworks. Koopman spectral theory has emerged as a dominant perspective over the past decade, in which nonlinear dynamics are represented in terms of an infinite-dimensional linear operator acting on the space of all possible measurement functions of the system. This linear representation of nonlinear dynamics has tremendous potential to enable the prediction, estimation, and control of nonlinear systems with standard textbook methods developed for linear systems. However, obtaining finite-dimensional coordinate systems and embeddings in which the dynamics appear approximately linear remains a central open challenge. The success of Koopman analysis is due primarily to three key factors: 1) there exists rigorous theory connecting it to classical geometric approaches for dynamical systems, 2) the approach is formulated in terms of measurements, making it ideal for leveraging big-data and machine learning techniques, and 3) simple, yet powerful numerical algorithms, such as the dynamic mode decomposition (DMD), have been developed and extended to reduce Koopman theory to practice in real-world applications. In this review, we provide an overview of modern Koopman operator theory, describing recent theoretical and algorithmic developments and highlighting these methods with a diverse range of applications. We also discuss key advances and challenges in the rapidly growing field of machine learning that are likely to drive future developments and significantly transform the theoretical landscape of dynamical systems.},
	publisher = {arXiv},
	author = {Brunton, Steven L. and Budišić, Marko and Kaiser, Eurika and Kutz, J. Nathan},
	year = {2021},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/APE8MM3E/Brunton et al. - 2021 - Modern Koopman Theory for Dynamical Systems.pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/FEHWGGW2/2102.html:text/html},
}



@article{chaudhuri_intrinsic_2019,
	title = {The intrinsic attractor manifold and population dynamics of a canonical cognitive circuit across waking and sleep},
	volume = {22},
	number = {9},
	journal = {Nature Neuroscience},
	author = {Chaudhuri, Rishidev and Gerçek, Berk and Pandey, Biraj and Peyrache, Adrien and Fiete, Ila},
	year = {2019},
	pages = {1512--1520},
}

@article{yoon_specific_2013,
	title = {Specific evidence of low-dimensional continuous attractor dynamics in grid cells},
	volume = {16},
	number = {8},
	journal = {Nature neuroscience},
	author = {Yoon, KiJung and Buice, Michael A. and Barry, Caswell and Hayman, Robin and Burgess, Neil and Fiete, Ila R.},
	year = {2013},
	pages = {1077--1084},
}


@inproceedings{schaeffer_reverse-engineering_2020,
	title = {Reverse-engineering recurrent neural network solutions to a hierarchical inference task for mice},
	abstract = {We study how recurrent neural networks (RNNs) solve a hierarchical inference task involving two latent variables and disparate timescales separated by 1-2 orders of magnitude. The task is of interest to the International Brain Laboratory, a global collaboration of experimental and theoretical neuroscientists studying how the mammalian brain generates behavior. We make four discoveries. First, RNNs learn behavior that is quantitatively similar to ideal Bayesian baselines. Second, RNNs perform inference by learning a two-dimensional subspace defining beliefs about the latent variables. Third, the geometry of RNN dynamics reflects an induced coupling between the two separate inference processes necessary to solve the task. Fourth, we perform model compression through a novel form of knowledge distillation on hidden representations  -- Representations and Dynamics Distillation (RADD)-- to reduce the RNN dynamics to a low-dimensional, highly interpretable model. This technique promises a useful tool for interpretability of high dimensional nonlinear dynamical systems. Altogether, this work yields predictions to guide exploration and analysis of mouse neural data and circuity.},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	author = {Schaeffer, Rylan and Khona, Mikail and Meshulam, Leenoy and International, Brain Laboratory and Fiete, Ila},
	year = {2020},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/PVTEYA8K/Schaeffer et al. - 2020 - Reverse-engineering recurrent neural network solut.pdf:application/pdf},
}


@inproceedings{boopathy_how_2022,
	title = {How to {Train} {Your} {Wide} {Neural} {Network} {Without} {Backprop}: {An} {Input}-{Weight} {Alignment} {Perspective}},
	shorttitle = {How to {Train} {Your} {Wide} {Neural} {Network} {Without} {Backprop}},
	abstract = {Recent works have examined theoretical and empirical properties of wide neural networks trained in the Neural Tangent Kernel (NTK) regime. Given that biological neural networks are much wider than their artificial counterparts, we consider NTK regime wide neural networks as a possible model of biological neural networks. Leveraging NTK theory, we show theoretically that gradient descent drives layerwise weight updates that are aligned with their input activity correlations weighted by error, and demonstrate empirically that the result also holds in finite-width wide networks. The alignment result allows us to formulate a family of biologically-motivated, backpropagation-free learning rules that are theoretically equivalent to backpropagation in infinite-width networks. We test these learning rules on benchmark problems in feedforward and recurrent neural networks and demonstrate, in wide networks, comparable performance to backpropagation. The proposed rules are particularly effective in low data regimes, which are common in biological learning settings.},
	language = {en},
	urldate = {2023-05-05},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Boopathy, Akhilan and Fiete, Ila},
	year = {2022},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/Y33NDFSI/Boopathy and Fiete - 2022 - How to Train Your Wide Neural Network Without Back.pdf:application/pdf},
}

@article{lillicrap_random_2016,
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	volume = {7},
	abstract = {The brain processes information through multiple layers of neurons. This deep architecture is representationally powerful, but complicates learning because it is difficult to identify the responsible neurons when a mistake is made. In machine learning, the backpropagation algorithm assigns blame by multiplying error signals with all the synaptic weights on each neuron’s axon and further downstream. However, this involves a precise, symmetric backward connectivity pattern, which is thought to be impossible in the brain. Here we demonstrate that this strong architectural constraint is not required for effective error propagation. We present a surprisingly simple mechanism that assigns blame by multiplying errors by even random synaptic weights. This mechanism can transmit teaching signals across multiple layers of neurons and performs as effectively as backpropagation on a variety of tasks. Our results help reopen questions about how the brain could use error signals and dispel long-held assumptions about algorithmic constraints on learning.},
	language = {en},
	number = {1},
	journal = {Nature Communications},
	author = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
	year = {2016},
	pages = {13276},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/BYLNHAIB/Lillicrap et al. - 2016 - Random synaptic feedback weights support error bac.pdf:application/pdf},
}

@inproceedings{kunin_two_2020,
  title={Two routes to scalable credit assignment without weight symmetry},
  author={Kunin, Daniel and Nayebi, Aran and Sagastuy-Brena, Javier and Ganguli, Surya and Bloom, Jonathan and Yamins, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={5511--5521},
  year={2020},
  organization={PMLR}
}

@misc{li_convergent_2016,
	title = {Convergent {Learning}: {Do} different neural networks learn the same representations?},
	shorttitle = {Convergent {Learning}},
	url = {http://arxiv.org/abs/1511.07543},
	abstract = {Recent success in training deep neural networks have prompted active investigation into the features learned on their intermediate layers. Such research is difficult because it requires making sense of non-linear computations performed by millions of parameters, but valuable because it increases our ability to understand current models and create improved versions of them. In this paper we investigate the extent to which neural networks exhibit what we call convergent learning, which is when the representations learned by multiple nets converge to a set of features which are either individually similar between networks or where subsets of features span similar low-dimensional spaces. We propose a specific method of probing representations: training multiple networks and then comparing and contrasting their individual, learned representations at the level of neurons or groups of neurons. We begin research into this question using three techniques to approximately align different neural networks on a feature level: a bipartite matching approach that makes one-to-one assignments between neurons, a sparse prediction approach that finds one-to-many mappings, and a spectral clustering approach that finds many-to-many mappings. This initial investigation reveals a few previously unknown properties of neural networks, and we argue that future research into the question of convergent learning will yield many more. The insights described here include (1) that some features are learned reliably in multiple networks, yet other features are not consistently learned; (2) that units learn to span low-dimensional subspaces and, while these subspaces are common to multiple networks, the specific basis vectors learned are not; (3) that the representation codes show evidence of being a mix between a local code and slightly, but not fully, distributed codes across multiple units.},
	urldate = {2023-05-05},
	publisher = {arXiv},
	author = {Li, Yixuan and Yosinski, Jason and Clune, Jeff and Lipson, Hod and Hopcroft, John},
	month = feb,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/mitchellostrow/Zotero/storage/2NYRVS32/Li et al. - 2016 - Convergent Learning Do different neural networks .pdf:application/pdf;arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/FC944A9W/1511.html:text/html},
}

@inproceedings{sutskever_importance_2013,
	title = {On the importance of initialization and momentum in deep learning},
	abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.     Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
	language = {en},
	urldate = {2023-05-05},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	year = {2013},
	pages = {1139--1147},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/QB46TNGS/Sutskever et al. - 2013 - On the importance of initialization and momentum i.pdf:application/pdf},
}

@book{d_o_hebb_organization_1949,
	title = {The {Organization} {Of} {Behavior} {A} {Neuropsychological} {Theory}},
	abstract = {Book Source: Digital Library of India Item 2015.168156
},
	language = {eng},
	urldate = {2023-05-05},
	author = {{D. O. Hebb}},
	year = {1949},
}



@article{degenhart_stabilization_2020,
	title = {Stabilization of a brain–computer interface via the alignment of low-dimensional spaces of neural activity},
	language = {en},
	journal = {Nature Biomedical Engineering},
	author = {Degenhart, Alan D. and Bishop, William E. and Oby, Emily R. and Tyler-Kabara, Elizabeth C. and Chase, Steven M. and Batista, Aaron P. and Yu, Byron M.},
	year = {2020},
	pages = {672--685},
	file = {Degenhart et al. - 2020 - Stabilization of a brain–computer interface via th.pdf:/Users/mitchellostrow/Zotero/storage/RV8Z2KCI/Degenhart et al. - 2020 - Stabilization of a brain–computer interface via th.pdf:application/pdf},
}

@article{gallego_cortical_2018,
	title = {Cortical population activity within a preserved neural manifold underlies multiple motor behaviors},
	volume = {9},
	abstract = {Populations of cortical neurons flexibly perform different functions; for the primary motor cortex (M1) this means a rich repertoire of motor behaviors. We investigate the flexibility of M1 movement control by analyzing neural population activity during a variety of skilled wrist and reach-to-grasp tasks. We compare across tasks the neural modes that capture dominant neural covariance patterns during each task. While each task requires different patterns of muscle and single unit activity, we find unexpected similarities at the neural population level: the structure and activity of the neural modes is largely preserved across tasks. Furthermore, we find two sets of neural modes with task-independent activity that capture, respectively, generic temporal features of the set of tasks and a task-independent mapping onto muscle activity. This system of flexibly combined, well-preserved neural modes may underlie the ability of M1 to learn and generate a wide-ranging behavioral repertoire., Motor cortical neurons enable performance of a wide range of movements. Here, the authors report that dominant population activity patterns, the neural modes, are largely preserved across various tasks, with many displaying consistent temporal dynamics and reliably mapping onto muscle activity.},
	urldate = {2023-05-05},
	journal = {Nature Communications},
	author = {Gallego, Juan A. and Perich, Matthew G. and Naufel, Stephanie N. and Ethier, Christian and Solla, Sara A. and Miller, Lee E.},
	month = oct,
	year = {2018},
	pmid = {30315158},
	pmcid = {PMC6185944},
	pages = {4233},
	file = {PubMed Central Full Text PDF:/Users/mitchellostrow/Zotero/storage/5K6YU6Z4/Gallego et al. - 2018 - Cortical population activity within a preserved ne.pdf:application/pdf},
}

@techreport{willett_high-performance_2023,
	type = {preprint},
	title = {A high-performance speech neuroprosthesis},
	url = {http://biorxiv.org/lookup/doi/10.1101/2023.01.21.524489},
	abstract = {Abstract
          
            Speech brain-computer interfaces (BCIs) have the potential to restore rapid communication to people with paralysis by decoding neural activity evoked by attempted speaking movements into text
            1,2
            or sound
            3,4
            . Early demonstrations, while promising, have not yet achieved accuracies high enough for communication of unconstrainted sentences from a large vocabulary
            1–7
            . Here, we demonstrate the first speech-to-text BCI that records spiking activity from intracortical microelectrode arrays. Enabled by these high-resolution recordings, our study participant, who can no longer speak intelligibly due amyotrophic lateral sclerosis (ALS), achieved a 9.1\% word error rate on a 50 word vocabulary (2.7 times fewer errors than the prior state of the art speech BCI
            2
            ) and a 23.8\% word error rate on a 125,000 word vocabulary (the first successful demonstration of large-vocabulary decoding). Our BCI decoded speech at 62 words per minute, which is 3.4 times faster than the prior record for any kind of BCI
            8
            and begins to approach the speed of natural conversation (160 words per minute
            9
            ). Finally, we highlight two aspects of the neural code for speech that are encouraging for speech BCIs: spatially intermixed tuning to speech articulators that makes accurate decoding possible from only a small region of cortex, and a detailed articulatory representation of phonemes that persists years after paralysis. These results show a feasible path forward for using intracortical speech BCIs to restore rapid communication to people with paralysis who can no longer speak.},
	language = {en},
	urldate = {2023-05-05},
	institution = {Neuroscience},
	author = {Willett, Francis R. and Kunz, Erin M. and Fan, Chaofei and Avansino, Donald T. and Wilson, Guy H. and Choi, Eun Young and Kamdar, Foram and Hochberg, Leigh R. and Druckmann, Shaul and Shenoy, Krishna V. and Henderson, Jaimie M.},
	year = {2023},
	file = {Willett et al. - 2023 - A high-performance speech neuroprosthesis.pdf:/Users/mitchellostrow/Zotero/storage/AM6DXXHG/Willett et al. - 2023 - A high-performance speech neuroprosthesis.pdf:application/pdf},
}


@article {Karpowicz2022,
	author = {Brianna M. Karpowicz and Yahia H. Ali and Lahiru N. Wimalasena and Andrew R. Sedler and Mohammad Reza Keshtkaran and Kevin Bodkin and Xuan Ma and Lee E. Miller and Chethan Pandarinath},
	title = {Stabilizing brain-computer interfaces through alignment of latent dynamics},
	elocation-id = {2022.04.06.487388},
	year = {2022},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Intracortical brain-computer interfaces (iBCIs) restore motor function to people with paralysis by translating brain activity into control signals for external devices. In current iBCIs, instabilities at the neural interface result in a degradation of decoding performance, which necessitates frequent supervised recalibration using new labeled data. One potential solution is to use the latent manifold structure that underlies neural population activity to facilitate a stable mapping between brain activity and behavior. Recent efforts using unsupervised approaches have improved iBCI stability using this principle; however, existing methods treat each time step as an independent sample and do not account for latent dynamics. Dynamics have been used to enable high performance prediction of movement intention, and may also help improve stabilization. Here, we present a platform for Nonlinear Manifold Alignment with Dynamics (NoMAD), which stabilizes iBCI decoding using recurrent neural network models of dynamics. NoMAD uses unsupervised distribution alignment to update the mapping of nonstationary neural data to a consistent set of neural dynamics, thereby providing stable input to the iBCI decoder. In applications to data from monkey motor cortex collected during motor tasks, NoMAD enables accurate behavioral decoding with unparalleled stability over weeks-to months-long timescales without any supervised recalibration.Competing Interest StatementC.P. is a consultant to Synchron and Meta (Reality Labs).},
	URL = {https://www.biorxiv.org/content/early/2022/11/08/2022.04.06.487388},
	eprint = {https://www.biorxiv.org/content/early/2022/11/08/2022.04.06.487388.full.pdf},
	journal = {bioRxiv}
}




@article{biran_neuronal_2005,
	title = {Neuronal cell loss accompanies the brain tissue response to chronically implanted silicon microelectrode arrays},
	abstract = {Implantable silicon microelectrode array technology is a useful technique for obtaining high-density, high-spatial resolution sampling of neuronal activity within the brain and holds promise for a wide range of neuroprosthetic applications. One of the limitations of the current technology is inconsistent performance in long-term applications. Although the brain tissue response is believed to be a major cause of performance degradation, the precise mechanisms that lead to failure of recordings are unknown. We observed persistent ED1 immunoreactivity around implanted silicon microelectrode arrays implanted in adult rat cortex that was accompanied by a significant reduction in nerve fiber density and nerve cell bodies in the tissue immediately surrounding the implanted silicon microelectrode arrays. Persistent ED1 up-regulation and neuronal loss was not observed in microelectrode stab controls indicating that the phenotype did not result from the initial mechanical trauma of electrode implantation, but was associated with the foreign body response. In addition, we found that explanted electrodes were covered with ED1/MAC-1 immunoreactive cells and that the cells released MCP-1 and TNF-alpha under serum-free conditions in vitro. Our findings suggest a potential new mechanism for chronic recording failure that involves neuronal cell loss, which we speculate is caused by chronic inflammation at the microelectrode brain tissue interface.},
	language = {eng},
	journal = {Experimental Neurology},
	author = {Biran, Roy and Martin, David C. and Tresco, Patrick A.},
	year = {2005},
	keywords = {Animals, Astrocytes, Brain, Cell Count, Cell Death, Cytokines, Diagnostic Imaging, Ectodysplasins, Electrodes, Implanted, Glial Fibrillary Acidic Protein, Gliosis, Immunohistochemistry, Inflammation, Macrophages, Male, Membrane Proteins, Naphthalenes, Neurofilament Proteins, Neurons, Oxepins, Phosphopyruvate Hydratase, Rats, Rats, Inbred F344, Silicon, Time Factors},
}

@article{perge_intra-day_2013,
	title = {Intra-day signal instabilities affect decoding performance in an intracortical neural interface system},
	abstract = {OBJECTIVE: Motor neural interface systems (NIS) aim to convert neural signals into motor prosthetic or assistive device control, allowing people with paralysis to regain movement or control over their immediate environment. Effector or prosthetic control can degrade if the relationship between recorded neural signals and intended motor behavior changes. Therefore, characterizing both biological and technological sources of signal variability is important for a reliable NIS.
APPROACH: To address the frequency and causes of neural signal variability in a spike-based NIS, we analyzed within-day fluctuations in spiking activity and action potential amplitude recorded with silicon microelectrode arrays implanted in the motor cortex of three people with tetraplegia (BrainGate pilot clinical trial, IDE).
MAIN RESULTS: 84\% of the recorded units showed a statistically significant change in apparent firing rate (3.8 ± 8.71 Hz or 49\% of the mean rate) across several-minute epochs of tasks performed on a single session, and 74\% of the units showed a significant change in spike amplitude (3.7 ± 6.5 µV or 5.5\% of mean spike amplitude). 40\% of the recording sessions showed a significant correlation in the occurrence of amplitude changes across electrodes, suggesting array micro-movement. Despite the relatively frequent amplitude changes, only 15\% of the observed within-day rate changes originated from recording artifacts such as spike amplitude change or electrical noise, while 85\% of the rate changes most likely emerged from physiological mechanisms. Computer simulations confirmed that systematic rate changes of individual neurons could produce a directional 'bias' in the decoded neural cursor movements. Instability in apparent neuronal spike rates indeed yielded a directional bias in 56\% of all performance assessments in participant cursor control (n = 2 participants, 108 and 20 assessments over two years), resulting in suboptimal performance in these sessions.
SIGNIFICANCE: We anticipate that signal acquisition and decoding methods that can adapt to the reported instabilities will further improve the performance of intracortically-based NISs.},
	language = {eng},
	journal = {Journal of Neural Engineering},
	author = {Perge, János A. and Homer, Mark L. and Malik, Wasim Q. and Cash, Sydney and Eskandar, Emad and Friehs, Gerhard and Donoghue, John P. and Hochberg, Leigh R.},
	year = {2013},
	file = {Accepted Version:/Users/mitchellostrow/Zotero/storage/F69YMWZR/Perge et al. - 2013 - Intra-day signal instabilities affect decoding per.pdf:application/pdf},
}




@article{mante_context-dependent_2013,
	title = {Context-dependent computation by recurrent dynamics in prefrontal cortex},
	copyright = {2013 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	abstract = {Prefrontal cortex is thought to have a fundamental role in flexible, context-dependent behaviour, but the exact nature of the computations underlying this role remains largely unknown. In particular, individual prefrontal neurons often generate remarkably complex responses that defy deep understanding of their contribution to behaviour. Here we study prefrontal cortex activity in macaque monkeys trained to flexibly select and integrate noisy sensory inputs towards a choice. We find that the observed complexity and functional roles of single neurons are readily understood in the framework of a dynamical process unfolding at the level of the population. The population dynamics can be reproduced by a trained recurrent neural network, which suggests a previously unknown mechanism for selection and integration of task-relevant inputs. This mechanism indicates that selection and integration are two aspects of a single dynamical process unfolding within the same prefrontal circuits, and potentially provides a novel, general framework for understanding context-dependent computations.},
	language = {en},
	journal = {Nature},
	author = {Mante, Valerio and Sussillo, David and Shenoy, Krishna V. and Newsome, William T.},
	year = {2013},
	file = {Accepted Version:/Users/mitchellostrow/Zotero/storage/YC9DVQLT/Mante et al. - 2013 - Context-dependent computation by recurrent dynamic.pdf:application/pdf},
}
@article{CHAISANGMONGKON20171504,
title = {Computing by Robust Transience: How the Fronto-Parietal Network Performs Sequential, Category-Based Decisions},
journal = {Neuron},
year = {2017},
author = {Warasinee Chaisangmongkon and Sruthi K. Swaminathan and David J. Freedman and Xiao-Jing Wang},
keywords = {prefrontal cortex, lateral intraparietal cortex, recurrent neural network, delayed match-to-category task, hessian-free algorithm, decision making, working memory, PFC, LIP, category learning},
abstract = {Summary
Decision making involves dynamic interplay between internal judgements and external perception, which has been investigated in delayed match-to-category (DMC) experiments. Our analysis of neural recordings shows that, during DMC tasks, LIP and PFC neurons demonstrate mixed, time-varying, and heterogeneous selectivity, but previous theoretical work has not established the link between these neural characteristics and population-level computations. We trained a recurrent network model to perform DMC tasks and found that the model can remarkably reproduce key features of neuronal selectivity at the single-neuron and population levels. Analysis of the trained networks elucidates that robust transient trajectories of the neural population are the key driver of sequential categorical decisions. The directions of trajectories are governed by network self-organized connectivity, defining a “neural landscape” consisting of a task-tailored arrangement of slow states and dynamical tunnels. With this model, we can identify functionally relevant circuit motifs and generalize the framework to solve other categorization tasks.}
}

@article {Pagan2022.11.28.518207,
	author = {Marino Pagan and Vincent D Tang and Mikio C. Aoi and Jonathan W. Pillow and Valerio Mante and David Sussillo and Carlos D. Brody},
	title = {A new theoretical framework jointly explains behavioral and neural variability across subjects performing flexible decision-making},
	year = {2022},
	abstract = {The ability to flexibly select and accumulate relevant information to form decisions, while ignoring irrelevant information, is a fundamental component of higher cognition. Yet its neural mechanisms remain unclear. Here we demonstrate that, under assumptions supported by both monkey and rat data, the space of possible network mechanisms to implement this ability is spanned by the combination of three different components, each with specific behavioral and anatomical implications. We further show that existing electrophysiological and modeling data are compatible with the full variety of possible combinations of these components, suggesting that different individuals could use different component combinations. To study variations across subjects, we developed a rat task requiring context-dependent evidence accumulation, and trained many subjects on it. Our task delivers sensory evidence through pulses that have random but precisely known timing, providing high statistical power to characterize each individual{\textquoteright}s neural and behavioral responses. Consistent with theoretical predictions, neural and behavioral analysis revealed remarkable heterogeneity across rats, despite uniformly good task performance. The theory further predicts a specific link between behavioral and neural signatures, which was robustly supported in the data. Our results provide a new experimentally-supported theoretical framework to analyze biological and artificial systems performing flexible decision-making tasks, and open the door to the study of individual variability in neural computations underlying higher cognition.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/11/28/2022.11.28.518207},
	journal = {bioRxiv}
}


@article{hinton2022forwardforward,
  title={The forward-forward algorithm: Some preliminary investigations},
  author={Hinton, Geoffrey},
  journal={arXiv preprint arXiv:2212.13345},
  year={2022}
}

@article{brunton2016koopman,
  title={Koopman invariant subspaces and finite linear representations of nonlinear dynamical systems for control},
  author={Brunton, Steven L and Brunton, Bingni W and Proctor, Joshua L and Kutz, J Nathan},
  journal={PloS one},
  volume={11},
  number={2},
  pages={e0150171},
  year={2016},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{mezic2005spectral,
  title={Spectral properties of dynamical systems, model reduction and decompositions},
  author={Mezi{\'c}, Igor},
  journal={Nonlinear Dynamics},
  volume={41},
  pages={309--325},
  year={2005},
  publisher={Springer}
}

@article{koopman1931hamiltonian,
  title={Hamiltonian systems and transformation in Hilbert space},
  author={Koopman, Bernard O},
  journal={Proceedings of the National Academy of Sciences},
  volume={17},
  number={5},
  pages={315--318},
  year={1931},
  publisher={National Acad Sciences}
}

@article{raghu2017svcca,
  title={SVCCA: Singular vector canonical correlation analysis for deep learning dynamics and interpretability},
  author={Raghu, Maithra and Gilmer, Justin and Yosinski, Jason and Sohl-Dickstein, Jascha},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{churchland2012neural,
  title={Neural population dynamics during reaching},
  author={Churchland, Mark M and Cunningham, John P and Kaufman, Matthew T and Foster, Justin D and Nuyujukian, Paul and Ryu, Stephen I and Shenoy, Krishna V},
  journal={Nature},
  volume={487},
  number={7405},
  pages={51--56},
  year={2012},
  publisher={Nature Publishing Group UK London}
}


@article{hart_embedding_2020,
	title = {Embedding and approximation theorems for echo state networks},
	volume = {128},
	abstract = {Echo State Networks (ESNs) are a class of single-layer recurrent neural networks that have enjoyed recent attention. In this paper we prove that a suitable ESN, trained on a series of measurements of an invertible dynamical system, induces a C1 map from the dynamical system’s phase space to the ESN’s reservoir space. We call this the Echo State Map. We then prove that the Echo State Map is generically an embedding with positive probability. Under additional mild assumptions, we further conjecture that the Echo State Map is almost surely an embedding. For sufficiently large, and specially structured, but still randomly generated ESNs, we prove that there exists a linear readout layer that allows the ESN to predict the next observation of a dynamical system arbitrarily well. Consequently, if the dynamical system under observation is structurally stable then the trained ESN will exhibit dynamics that are topologically conjugate to the future behaviour of the observed dynamical system. Our theoretical results connect the theory of ESNs to the delay-embedding literature for dynamical systems, and are supported by numerical evidence from simulations of the traditional Lorenz equations. The simulations confirm that, from a one dimensional observation function, an ESN can accurately infer a range of geometric and topological features of the dynamics such as the eigenvalues of equilibrium points, Lyapunov exponents and homology groups.},
	journal = {Neural Networks},
	author = {Hart, Allen and Hook, James and Dawes, Jonathan},
	year = {2020},
	keywords = {Delay embedding, Dynamical system, Lorenz equations, Persistent homology, Recurrent neural networks, Reservoir computing},
	pages = {234--247},
}

@article{Lusch_2018,
  
  
	year = 2018,  
	publisher = {Springer Science and Business Media {LLC}
},
  
	volume = {9},
  
	number = {1},
  
	author = {Bethany Lusch and J. Nathan Kutz and Steven L. Brunton},
  
	title = {Deep learning for universal linear embeddings of nonlinear dynamics},
  
	journal = {Nature Communications}
}


@article{kriegeskorte_neural_2021,
	title = {Neural tuning and representational geometry},
	volume = {22},
	abstract = {A central goal of neuroscience is to understand the representations formed by brain activity patterns and their connection to behaviour. The classic approach is to investigate how individual neurons encode stimuli and how their tuning determines the fidelity of the neural representation. Tuning analyses often use the Fisher information to characterize the sensitivity of neural responses to small changes of the stimulus. In recent decades, measurements of large populations of neurons have motivated a complementary approach, which focuses on the information available to linear decoders. The decodable information is captured by the geometry of the representational patterns in the multivariate response space. Here we review neural tuning and representational geometry with the goal of clarifying the relationship between them. The tuning induces the geometry, but different sets of tuned neurons can induce the same geometry. The geometry determines the Fisher information, the mutual information and the behavioural performance of an ideal observer in a range of psychophysical tasks. We argue that future studies can benefit from considering both tuning and geometry to understand neural codes and reveal the connections between stimuli, brain activity and behaviour.},
	number = {11},
	journal = {Nature Reviews Neuroscience},
	author = {Kriegeskorte, Nikolaus and Wei, Xue-Xin},
	year = {2021},
	pages = {703--718},
}

@article {Zhang2112,
	author = {K Zhang},
	title = {Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory},
	volume = {16},
	number = {6},
	pages = {2112--2126},
	year = {1996},
	publisher = {Society for Neuroscience},
	abstract = {The head-direction (HD) cells found in the limbic system in freely mov ing rats represent the instantaneous head direction of the animal in the horizontal plane regardless of the location of the animal. The internal direction represented by these cells uses both self-motion information for inertially based updating and familiar visual landmarks for calibration. Here, a model of the dynamics of the HD cell ensemble is presented. The stability of a localized static activity profile in the network and a dynamic shift mechanism are explained naturally by synaptic weight distribution components with even and odd symmetry, respectively. Under symmetric weights or symmetric reciprocal connections, a stable activity profile close to the known directional tuning curves will emerge. By adding a slight asymmetry to the weights, the activity profile will shift continuously without disturbances to its shape, and the shift speed can be controlled accurately by the strength of the odd-weight component. The generic formulation of the shift mechanism is determined uniquely within the current theoretical framework. The attractor dynamics of the system ensures modality- independence of the internal representation and facilitates the correction for cumulative error by the putative local-view detectors. The model offers a specific one-dimensional example of a computational mechanism in which a truly world-centered representation can be derived from observer-centered sensory inputs by integrating self-motion information.},
	journal = {Journal of Neuroscience}
}


@article{skaggs_model_1995,
	title = {A model of the neural basis of the rat's sense of direction},
	volume = {7},
	abstract = {In the last decade the outlines of the neural structures subserving the sense of direction have begun to emerge. Several investigations have shed light on the effects of vestibular input and visual input on the head direction representation. In this paper, a model is formulated of the neural mechanisms underlying the head direction system. The model is built out of simple ingredients, depending on nothing more complicated than connectional specificity, attractor dynamics, Hebbian learning, and sigmoidal nonlinearities, but it behaves in a sophisticated way and is consistent with most of the observed properties of real head direction cells. In addition it makes a number of predictions that ought to be testable by reasonably straightforward experiments.},
	language = {eng},
	journal = {Advances in Neural Information Processing Systems},
	author = {Skaggs, W. E. and Knierim, J. J. and Kudrimoti, H. S. and McNaughton, B. L.},
	year = {1995},
	pmid = {11539168},
	keywords = {Animals, Head, Hippocampus, Models, Neurological, Movement, Neurons, Orientation, Rats, Rotation, Thalamus, Vestibule, Labyrinth, Visual Pathways},
	pages = {173--180},
}

@article{wang2022,
    author = {Wang, Raymond AND Kang, Louis},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Multiple bumps can enhance robustness to noise in continuous attractor networks},
    year = {2022},
    month = {10},
    volume = {18},
    pages = {1-38},
    abstract = {A central function of continuous attractor networks is encoding coordinates and accurately updating their values through path integration. To do so, these networks produce localized bumps of activity that move coherently in response to velocity inputs. In the brain, continuous attractors are believed to underlie grid cells and head direction cells, which maintain periodic representations of position and orientation, respectively. These representations can be achieved with any number of activity bumps, and the consequences of having more or fewer bumps are unclear. We address this knowledge gap by constructing 1D ring attractor networks with different bump numbers and characterizing their responses to three types of noise: fluctuating inputs, spiking noise, and deviations in connectivity away from ideal attractor configurations. Across all three types, networks with more bumps experience less noise-driven deviations in bump motion. This translates to more robust encodings of linear coordinates, like position, assuming that each neuron represents a fixed length no matter the bump number. Alternatively, we consider encoding a circular coordinate, like orientation, such that the network distance between adjacent bumps always maps onto 360 degrees. Under this mapping, bump number does not significantly affect the amount of error in the coordinate readout. Our simulation results are intuitively explained and quantitatively matched by a unified theory for path integration and noise in multi-bump networks. Thus, to suppress the effects of biologically relevant noise, continuous attractor networks can employ more bumps when encoding linear coordinates; this advantage disappears when encoding circular coordinates. Our findings provide motivation for multiple bumps in the mammalian grid network.},
    number = {10},

}


@article{bevanda_koopman_2021,
	title = {Koopman {Operator} {Dynamical} {Models}: {Learning}, {Analysis} and {Control}},
	volume = {52},
	shorttitle = {Koopman {Operator} {Dynamical} {Models}},
	url = {http://arxiv.org/abs/2102.02522},
	abstract = {The Koopman operator allows for handling nonlinear systems through a (globally) linear representation. In general, the operator is infinite-dimensional - necessitating finite approximations - for which there is no overarching framework. Although there are principled ways of learning such finite approximations, they are in many instances overlooked in favor of, often ill-posed and unstructured methods. Also, Koopman operator theory has long-standing connections to known system-theoretic and dynamical system notions that are not universally recognized. Given the former and latter realities, this work aims to bridge the gap between various concepts regarding both theory and tractable realizations. Firstly, we review data-driven representations (both unstructured and structured) for Koopman operator dynamical models, categorizing various existing methodologies and highlighting their differences. Furthermore, we provide concise insight into the paradigm's relation to system-theoretic notions and analyze the prospect of using the paradigm for modeling control systems. Additionally, we outline the current challenges and comment on future perspectives.},
	urldate = {2023-05-16},
	journal = {Annual Reviews in Control},
	author = {Bevanda, Petar and Sosnowski, Stefan and Hirche, Sandra},
	year = {2021},
	pages = {197--212},
	file = {arXiv.org Snapshot:/Users/mitchellostrow/Zotero/storage/LWDGV93D/2102.html:text/html;Full Text PDF:/Users/mitchellostrow/Zotero/storage/8EVNBHCI/Bevanda et al. - 2021 - Koopman Operator Dynamical Models Learning, Analy.pdf:application/pdf},
}


@article{budisic_applied_2012,
	title = {Applied {Koopmanism}},
	volume = {22},
	abstract = {A majority of methods from dynamical system analysis, especially those in applied settings, rely on Poincaré's geometric picture that focuses on “dynamics of states.” While this picture has fueled our field for a century, it has shown difficulties in handling high-dimensional, ill-described, and uncertain systems, which are more and more common in engineered systems design and analysis of “big data” measurements. This overview article presents an alternative framework for dynamical systems, based on the “dynamics of observables” picture. The central object is the Koopman operator: an infinite-dimensional, linear operator that is nonetheless capable of capturing the full nonlinear dynamics. The first goal of this paper is to make it clear how methods that appeared in different papers and contexts all relate to each other through spectral properties of the Koopman operator. The second goal is to present these methods in a concise manner in an effort to make the framework accessible to researchers who would like to apply them, but also, expand and improve them. Finally, we aim to provide a road map through the literature where each of the topics was described in detail. We describe three main concepts: Koopman mode analysis, Koopman eigenquotients, and continuous indicators of ergodicity. For each concept, we provide a summary of theoretical concepts required to define and study them, numerical methods that have been developed for their analysis, and, when possible, applications that made use of them. The Koopman framework is showing potential for crossing over from academic and theoretical use to industrial practice. Therefore, the paper highlights its strengths in applied and numerical contexts. Additionally, we point out areas where an additional research push is needed before the approach is adopted as an off-the-shelf framework for analysis and design.},
	number = {4},
	urldate = {2023-05-16},
	journal = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
	author = {Budišić, Marko and Mohr, Ryan and Mezić, Igor},
	month = dec,
	year = {2012},
	pages = {047510},
	file = {Full Text PDF:/Users/mitchellostrow/Zotero/storage/HZTBPP7Z/Budišić et al. - 2012 - Applied Koopmanisma).pdf:application/pdf},
}

@article {Humphreys2022.12.08.519453,
	author = {Peter C. Humphreys and Kayvon Daie and Karel Svoboda and Matthew Botvinick and Timothy P. Lillicrap},
	title = {BCI learning phenomena can be explained by gradient-based optimization},
	elocation-id = {2022.12.08.519453},
	year = {2022},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {Brain-computer interface (BCI) experiments have shown that animals are able to adapt their recorded neural activity in order to receive reward. Recent studies have highlighted two phenomena. First, the speed at which a BCI task can be learned is dependent on how closely the required neural activity aligns with pre-existing activity patterns: learning {\textquotedblleft}out-of-manifold{\textquotedblright} tasks is slower than {\textquotedblleft}in-manifold{\textquotedblright} tasks. Second, learning happens by {\textquotedblleft}re-association{\textquotedblright}: the overall distribution of neural activity patterns does not change significantly during task learning. These phenomena have been presented as distinctive aspects of BCI learning. Here we show, using simulations and theoretical analysis, that both phenomena result from the simple assumption that behaviour and representations are improved via gradient-based algorithms. We invoke Occam{\textquoteright}s Razor to suggest that this straightforward explanation should be pre-ferred when accounting for these experimental observations.Competing Interest StatementThe authors have declared no competing interest.},
	URL = {https://www.biorxiv.org/content/early/2022/12/08/2022.12.08.519453},
	eprint = {https://www.biorxiv.org/content/early/2022/12/08/2022.12.08.519453.full.pdf},
	journal = {bioRxiv}
}

@article{sadtler_neural_2014,
	title = {Neural constraints on learning},
	volume = {512},
	abstract = {During learning, the new patterns of neural population activity that develop are constrained by the existing network structure so that certain patterns can be generated more readily than others.},
	number = {7515},
	journal = {Nature},
	author = {Sadtler, Patrick T. and Quick, Kristin M. and Golub, Matthew D. and Chase, Steven M. and Ryu, Stephen I. and Tyler-Kabara, Elizabeth C. and Yu, Byron M. and Batista, Aaron P.},
	year = {2014},
	pages = {423--426},
}


@article{payeur_neural_2023,
	title = {Neural manifolds and gradient-based adaptation in neural-interface tasks},
	url = {https://www.biorxiv.org/content/early/2023/03/12/2023.03.11.532146},
	abstract = {Neural activity tends to reside on manifolds whose dimension is much lower than the dimension of the whole neural state space. Experiments using brain-computer interfaces with microelectrode arrays implanted in the motor cortex of nonhuman primates tested the hypothesis that external perturbations should produce different adaptation strategies depending on how “aligned” the perturbation is with respect to a pre-existing intrinsic manifold. On the one hand, perturbations within the manifold (WM) evoked fast reassociations of existing patterns for rapid adaptation. On the other hand, perturbations outside the manifold (OM) triggered the slow emergence of new neural patterns underlying a much slower—and, without adequate training protocols, inconsistent or virtually impossible—adaptation. This suggests that the time scale and the overall difficulty of the brain to adapt depend fundamentally on the structure of neural activity. Here, we used a simplified static Gaussian model to show that gradient-descent learning could explain the differences between adaptation to WM and OM perturbations. For small learning rates, we found that the adaptation speeds were different but the model eventually adapted to both perturbations. Moreover, sufficiently large learning rates could entirely prohibit adaptation to OM perturbations while preserving adaptation to WM perturbations, in agreement with experiments. Adopting an incremental training protocol, as has been done in experiments, permitted a swift recovery of a full adaptation in the cases where OM perturbations were previously impossible to relearn. Finally, we also found that gradient descent was compatible with the reassociation mechanism on short adaptation time scales. Since gradient descent has many biologically plausible variants, our findings thus establish gradient-based learning as a plausible mechanism for adaptation under network-level constraints, with a central role for the learning rate.Competing Interest StatementThe authors have declared no competing interest.},
	journal = {bioRxiv},
	author = {Payeur, Alexandre and Orsborn, Amy L. and Lajoie, Guillaume},
	year = {2023},
}

@article{hopfield1986computing,
  title={Computing with neural circuits: A model},
  author={Hopfield, John J and Tank, David W},
  journal={Science},
  volume={233},
  number={4764},
  pages={625--633},
  year={1986},
  publisher={American Association for the Advancement of Science}
}


@inproceedings{jimenez_fast_2013,
	address = {Zurich},
	title = {Fast {Jacobi}-type algorithm for computing distances between linear dynamical systems},
	abstract = {The alignment distance is a novel metric between linear dynamical systems that has been shown to be very useful in many applications in computer vision. However, since the computation of the alignment distance requires solving a minimization problem on the orthogonal group, it is important to develop computationally efﬁcient algorithms for solving this problem. In this paper, we present a fast and accurate Jacobi-type algorithm that solves this problem. Each step of the algorithm is equivalent to ﬁnding the roots of a quartic polynomial. We show that this rooting may be done efﬁciently and accurately using a careful implementation of Ferrari’s classical closed-form solution for quartic polynomials. For linear systems with orders that commonly arise in computer vision scenarios, our algorithm is roughly twenty times faster than a fast Riemannian gradient descent algorithm implementation and has comparable accuracy.},
	language = {en},
	urldate = {2023-04-11},
	booktitle = {2013 {European} {Control} {Conference} ({ECC})},
	publisher = {IEEE},
	author = {Jimenez, Nicolas D. and Afsari, Bijan and Vidal, Rene},
	year = {2013},
	pages = {3682--3687},
	file = {Jimenez et al. - 2013 - Fast Jacobi-type algorithm for computing distances.pdf:/Users/mitchellostrow/Zotero/storage/PMUKAACU/Jimenez et al. - 2013 - Fast Jacobi-type algorithm for computing distances.pdf:application/pdf},
}
@book{hebb-organization-of-behavior-1949,
  abstract = {{Donald Hebb pioneered many current themes in
                 behavioural neuroscience. He saw psychology as a
                 biological science, but one in which the organization
                 of behaviour must remain the central concern. Through
                 penetrating theoretical concepts, including the "cell
                 assembly," "phase sequence," and "Hebb synapse," he
                 offered a way to bridge the gap between cells, circuits
                 and behaviour. He saw the brain as a dynamically
                 organized system of multiple distributed parts, with
                 roots that extend into foundations of development and
                 evolutionary heritage. He understood that behaviour, as
                 brain, can be sliced at various levels and that one of
                 our challenges is to bring these levels into both
                 conceptual and empirical register. He could move
                 between theory and fact with an ease that continues to
                 inspire both students and professional investigators.
                 Although facts continue to accumulate at an
                 accelerating rate in both psychology and neuroscience,
                 and although these facts continue to force revision in
                 the details of Hebb's earlier contributions, his
                 overall insistence that we look at behaviour and brain
                 together â within a dynamic, relational and
                 multilayered framework â remains. His work touches
                 upon current studies of population coding, contextual
                 factors in brain representations, synaptic plasticity,
                 developmental construction of brain/behaviour
                 relations, clinical syndromes, deterioration of
                 performance with age and disease, and the formal
                 construction of connectionist models. The collection of
                 papers in this volume represent these and related
                 themes that Hebb inspired. We also acknowledge our
                 appreciation for Don Hebb as teacher, colleague and
                 friend.}},
  added-at = {2011-06-02T00:21:57.000+0200},
  address = {New York},
  author = {Hebb, Donald O.},
  file = {:neural_nets/Hebb 1949.pdf:PDF},
  groups = {public},
  howpublished = {Hardcover},
  interhash = {ba8f8b92a0de2c83bdbcc9d742235a59},
  intrahash = {6432ae617e6db0127c8b197bf760d99e},
  keywords = {MSc checked network neural seminal},
  posted-at = {2006-02-10 16:35:34},
  priority = {2},
  publisher = {Wiley},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {The organization of behavior: {A} neuropsychological
                 theory},
  username = {mhwombat},
  year = 1949
}






@article{amari_competition_1977,
	title = {Competition and {Cooperation} in {Neural} {Nets}},
	journal = {Systems Neuroscience, Academic Press, J. Metzler (ed).},
	author = {Amari, Shun-Ichi and Arbib, Michael A},
	year = {1977},
	keywords = {merged\_fiete.bib},
	pages = {119--165},
}


@article{khona_attractor_2021,
	title = {Attractor and integrator networks in the brain},
	volume = {23},
	abstract = {In this Review, we describe the singular success of attractor neural network models in describing how the brain maintains persistent activity states for working memory, corrects errors and integrates noisy cues. We consider the mechanisms by which simple and forgetful units can organize to collectively generate dynamics on the long timescales required for such computations. We discuss the myriad potential uses of attractor dynamics for computation in the brain, and showcase notable examples of brain systems in which inherently low-dimensional continuous-attractor dynamics have been concretely and rigorously identified. Thus, it is now possible to conclusively state that the brain constructs and uses such systems for computation. Finally, we highlight recent theoretical advances in understanding how the fundamental trade-offs between robustness and capacity and between structure and flexibility can be overcome by reusing and recombining the same set of modular attractors for multiple functions, so they together produce representations that are structurally constrained and robust but exhibit high capacity and are flexible.},
	number = {12},
	journal = {Nature Reviews Neuroscience},
	author = {Khona, Mikail and Fiete, Ila R.},
	year = {2022},
	pages = {744--766},
}


@article{churchland_decision-making_2008,
	title = {Decision-making with multiple alternatives},
	volume = {11},
	abstract = {Simple perceptual tasks have laid the groundwork for understanding the neurobiology of decision-making. Here, we examined this foundation to explain how decision-making circuitry adjusts in the face of a more difficult task. We measured behavioral and physiological responses of monkeys on a two- and four-choice direction-discrimination decision task. For both tasks, firing rates in the lateral intraparietal area appeared to reflect the accumulation of evidence for or against each choice. Evidence accumulation began at a lower firing rate for the four-choice task, but reached a common level by the end of the decision process. The larger excursion suggests that the subjects required more evidence before making a choice. Furthermore, on both tasks, we observed a time-dependent rise in firing rates that may impose a deadline for deciding. These physiological observations constitute an effective strategy for handling increased task difficulty. The differences appear to explain subjects' accuracy and reaction times.},
	number = {6},
	journal = {Nat. Neurosci.},
	author = {Churchland, Anne K and Kiani, Roozbeh and Shadlen, Michael N},
	year = {2008},
	keywords = {merged\_fiete.bib},
	pages = {693--702},
}

@article{khona_attractor_2020,
	title = {Attractor and integrator networks in the brain},
	volume = {(in review)},
	journal = {Nat. Rev. Neurosci.},
	author = {Khona, M and Fiete, I R},
	year = {2020},
	keywords = {merged\_fiete.bib},
}

@article{sussillo_generating_2009,
	title = {Generating coherent patterns of activity from chaotic neural networks},
	volume = {63},
	abstract = {Neural circuits display complex activity patterns both spontaneously and when responding to a stimulus or generating a motor output. How are these two forms of activity related? We develop a procedure called FORCE learning for modifying synaptic strengths either external to or within a model neural network to change chaotic spontaneous activity into a wide variety of desired activity patterns. FORCE learning works even though the networks we train are spontaneously chaotic and we leave feedback loops intact and unclamped during learning. Using this approach, we construct networks that produce a wide variety of complex output patterns, input-output transformations that require memory, multiple outputs that can be switched by control inputs, and motor patterns matching human motion capture data. Our results reproduce data on premovement activity in motor and premotor cortex, and suggest that synaptic plasticity may be a more rapid and powerful modulator of network activity than generally appreciated.},
	number = {4},
	journal = {Neuron},
	author = {Sussillo, David and Abbott, L F},
	year = {2009},
	keywords = {merged\_fiete.bib},
	pages = {544--557},
}

@article{seung_simple_1993,
	title = {Simple models for reading neuronal population codes},
	volume = {90},
	abstract = {In many neural systems, sensory information is distributed throughout a population of neurons. We study simple neural network models for extracting this information. The inputs to the networks are the stochastic responses of a population of sensory neurons tuned to directional stimuli. The performance of each network model in psychophysical tasks is compared with that of the optimal maximum likelihood procedure. As a model of direction estimation in two dimensions, we consider a linear network that computes a population vector. Its performance depends on the width of the population tuning curves and is maximal for width, which increases with the level of background activity. Although for narrowly tuned neurons the performance of the population vector is significantly inferior to that of maximum likelihood estimation, the difference between the two is small when the tuning is broad. For direction discrimination, we consider two models: a perceptron with fully adaptive weights and a network made by adding an adaptive second layer to the population vector network. We calculate the error rates of these networks after exhaustive training to a particular direction. By testing on the full range of possible directions, the extent of transfer of training to novel stimuli can be calculated. It is found that for threshold linear networks the transfer of perceptual learning is nonmonotonic. Although performance deteriorates away from the training stimulus, it peaks again at an intermediate angle. This nonmonotonicity provides an important psychophysical test of these models.},
	number = {22},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Seung, H S and Sompolinsky, H},
	year = {1993},
	keywords = {merged\_fiete.bib},
	pages = {10749--10753},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	number = {8},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, John J},
	year = {1982},
	keywords = {merged\_fiete.bib},
	pages = {2554--2558},
}

@article{mongillo_synaptic_2008,
	title = {Synaptic theory of working memory},
	abstract = {It is usually assumed that enhanced spiking activity in the form of persistent reverberation for several seconds is the neural correlate of working memory. Here, we propose that working memory is sustained by calcium-mediated synaptic facilitation in the recurrent connections of neocortical networks. In this account, the presynaptic residual calcium is used as a buffer that is loaded, refreshed, and read out by spiking activity. Because of the long time constants of calcium kinetics, the refresh rate can be low, resulting in a mechanism that is metabolically efficient and robust. The duration and stability of working memory can be regulated by modulating the spontaneous activity in the network.},
	journal = {Science},
	author = {Mongillo, Gianluigi and Barak, Omri and Tsodyks, Misha},
	year = {2008},
	keywords = {merged\_fiete.bib},
	pages = {1543--1546},
}

@article{seung_simple_1993-1,
	title = {Simple models for reading neuronal population codes},
	volume = {90},
	abstract = {In many neural systems, sensory information is distributed throughout a population of neurons. We study simple neural network models for extracting this information. The inputs to the networks are the stochastic responses of a population of sensory neurons tuned to directional stimuli. The performance of each network model in psychophysical tasks is compared with that of the optimal maximum likelihood procedure. As a model of direction estimation in two dimensions, we consider a linear network that computes a population vector. Its performance depends on the width of the population tuning curves and is maximal for width, which increases with the level of background activity. Although for narrowly tuned neurons the performance of the population vector is significantly inferior to that of maximum likelihood estimation, the difference between the two is small when the tuning is broad. For direction discrimination, we consider two models: a perceptron with fully adaptive weights and a network made by adding an adaptive second layer to the population vector network. We calculate the error rates of these networks after exhaustive training to a particular direction. By testing on the full range of possible directions, the extent of transfer of training to novel stimuli can be calculated. It is found that for threshold linear networks the transfer of perceptual learning is nonmonotonic. Although performance deteriorates away from the training stimulus, it peaks again at an intermediate angle. This nonmonotonicity provides an important psychophysical test of these models.},
	number = {22},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Seung, H S and Sompolinsky, H},
	year = {1993},
	pages = {10749--10753},
}

@article{seung_reading_2009,
	title = {Reading the book of memory: sparse sampling versus dense mapping of connectomes},
	volume = {62},
	abstract = {Many theories of neural networks assume rules of connection between pairs of neurons that are based on their cell types or functional properties. It is finally becoming feasible to test such pairwise models of connectivity, due to emerging advances in neuroanatomical techniques. One method will be to measure the functional properties of connected pairs of neurons, sparsely sampling pairs from many specimens. Another method will be to find a “connectome,” a dense map of all connections in a single specimen, and infer functional properties of neurons through computational analysis. For the latter method, the most exciting prospect would be to decode the memories that are hypothesized to be stored in connectomes.},
	language = {eng},
	number = {1},
	journal = {Neuron},
	author = {Seung, H Sebastian},
	year = {2009},
	keywords = {*Brain Mapping, *Models, Animals, Brain/cytology/*physiology, Caenorhabditis elegans/physiology, Humans, Memory/*physiology, merged\_fiete.bib, Nerve Net/physiology, Neural Networks (Computer), Neural Pathways, Neurological, Neurons/*physiology, Retina/cytology/physiology, Synaptic Transmission},
	pages = {17--29},
}

@article{hopfield_neurons_1984,
	title = {Neurons with graded response have collective computational properties like those of two-state neurons},
	volume = {81},
	abstract = {A model for a large network of {\textbackslash}textbackslashtt“neurons{\textbackslash}textbackslashtt” with a graded response (or sigmoid input-output relation) is studied. This deterministic system has collective properties in very close correspondence with the earlier stochastic model based on McCulloch - Pitts neurons. The content- addressable memory and other emergent collective properties of the original model also are present in the graded response model. The idea that such collective properties are used in biological systems is given added credence by the continued presence of such properties for more nearly biological “neurons.” Collective analog electrical circuits of the kind described will certainly function. The collective states of the two models have a simple correspondence. The original model will continue to be useful for simulations, because its connection to graded response systems is established. Equations that include the effect of action potentials in the graded response system are also developed.},
	language = {eng},
	number = {10},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, J J},
	year = {1984},
	keywords = {*Models, Action Potentials, Animals, Electric Conductivity, Mathematics, merged\_fiete.bib, Neurological, Neurons/*physiology},
	pages = {3088--3092},
}

@article{seung_how_1996,
	title = {How the brain keeps the eyes still},
	volume = {93},
	abstract = {The brain can hold the eyes still because it stores a memory of eye position. The brain's memory of horizontal eye position appears to be represented by persistent neural activity in a network known as the neural integrator, which is localized in the brainstem and cerebellum. Existing experimental data are reinterpreted as evidence for an “attractor hypothesis” that the persistent patterns of activity observed in this network form an attractive line of fixed points in its state space. Line attractor dynamics can be produced in linear or nonlinear neural networks by learning mechanisms that precisely tune positive feedback.},
	language = {eng},
	number = {23},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Seung, H S},
	year = {1996},
	keywords = {merged\_fiete.bib},
	pages = {13339--13344},
}

@article{seung_stability_2000,
	title = {Stability of the memory of eye position in a recurrent network of conductance-based model neurons},
	volume = {26},
	abstract = {Studies of the neural correlates of short-term memory in a wide variety of brain areas have found that transient inputs can cause persistent changes in rates of action potential firing, through a mechanism that remains unknown. In a premotor area that is responsible for holding the eyes still during fixation, persistent neural firing encodes the angular position of the eyes in a characteristic manner: below a threshold position the neuron is silent, and above it the firing rate is linearly related to position. Both the threshold and linear slope vary from neuron to neuron. We have reproduced this behavior in a biophysically plausible network model. Persistence depends on precise tuning of the strength of synaptic feedback, and a relatively long synaptic time constant improves the robustness to mistuning.},
	language = {eng},
	number = {1},
	journal = {Neuron},
	author = {Seung, H S and Lee, D D and Reis, B Y and Tank, D W},
	year = {2000},
	keywords = {merged\_fiete.bib},
	pages = {259--271},
}

@article{zhang_representation_1996,
	title = {Representation of spatial orientation by the intrinsic dynamics of the head-direction cell ensemble: a theory},
	volume = {15},
	journal = {J. Neurosci.},
	author = {Zhang, K},
	year = {1996},
	keywords = {merged\_fiete.bib},
	pages = {2112--2126},
}

@article{churchland_variance_2011,
	title = {Variance as a signature of neural computations during decision making},
	volume = {69},
	abstract = {Traditionally, insights into neural computation have been furnished by averaged firing rates from many stimulus repetitions or trials. We pursue an analysis of neural response variance to unveil neural computations that cannot be discerned from measures of average firing rate. We analyzed single-neuron recordings from the lateral intraparietal area (LIP), during a perceptual decision-making task. Spike count variance was divided into two components using the law of total variance for doubly stochastic processes: (1) variance of counts that would be produced by a stochastic point process with a given rate, and loosely (2) the variance of the rates that would produce those counts (i.e., “conditional expectation”). The variance and correlation of the conditional expectation exposed several neural mechanisms: mixtures of firing rate states preceding the decision, accumulation of stochastic “evidence” during decision formation, and a stereotyped response at decision end. These analyses help to differentiate among several alternative decision-making models.},
	number = {4},
	journal = {Neuron},
	author = {Churchland, Anne K and Kiani, R and Chaudhuri, R and Wang, Xiao-Jing and Pouget, Alexandre and Shadlen, M N},
	month = feb,
	year = {2011},
	keywords = {merged\_fiete.bib},
	pages = {818--831},
}

@article{hopfield_neural_1982-1,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	number = {8},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, J J},
	year = {1982},
	keywords = {merged\_fiete.bib},
	pages = {2554--2558},
}

@article{hopfield_neural_nodate,
	title = {Neural networks and physical systems with emergent collective computational abilities},
	volume = {79},
	abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
	number = {8},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, J J},
	keywords = {merged\_fiete.bib},
	pages = {2554--2558},
}

@article{amari_learning_1972,
	title = {Learning patterns and pattern sequences by selforganizing nets of threshold elements},
	volume = {21},
	journal = {IEEE Trans. Comput.},
	author = {Amari, S I},
	year = {1972},
	keywords = {merged\_fiete.bib},
	pages = {1197--1206},
}

@article{seung_autapse_2000,
	title = {The autapse: a simple illustration of short-term analog memory storage by tuned synaptic feedback},
	volume = {9},
	abstract = {According to a popular hypothesis, short-term memories are stored as persistent neural activity maintained by synaptic feedback loops. This hypothesis has been formulated mathematically in a number of recurrent network models. Here we study an abstraction of these models, a single neuron with a synapse onto itself, or autapse. This abstraction cannot simulate the way in which persistent activity patterns are distributed over neural populations in the brain. However, with proper tuning of parameters, it does reproduce the continuously graded, or analog, nature of many examples of persistent activity. The conditions for tuning are derived for the dynamics of a conductance-based model neuron with a slow excitatory autapse. The derivation uses the method of averaging to approximate the spiking model with a nonspiking, reduced model. Short-term analog memory storage is possible if the reduced model is approximately linear, and its feedforward bias and autapse strength are precisely tuned.},
	journal = {J. Comput. Neurosci.},
	author = {Seung, H Sebastian and Lee, Daniel D and Reis, Ben Y and Tank, David W},
	year = {2000},
	keywords = {merged\_fiete.bib, persistent neural activity, reverberating circuit, short-term memory, synaptic feedback},
	pages = {171--185},
}

@article{hopfield_what_2001,
	title = {What is a moment? {Transient} synchrony as a collective mechanism for spatiotemporal integration},
	volume = {98},
	abstract = {A previous paper described a network of simple integrate-and-fire neurons that contained output neurons selective for specific spatiotemporal patterns of inputs; only experimental results were described. We now present the principles behind the operation of this network and discuss how these principles point to a general class of computational operations that can be carried out easily and naturally by networks of spiking neurons. Transient synchrony of the action potentials of a group of neurons is used to signal “recognition” of a space-time pattern across the inputs of those neurons. Appropriate synaptic coupling produces synchrony when the inputs to these neurons are nearly equal, leaving the neurons unsynchronized or only weakly synchronized for other input circumstances. When the input to this system comes from timed past events represented by decaying delay activity, the pattern of synaptic connections can be set such that synchronization occurs only for selected spatiotemporal patterns. We show how the recognition is invariant to uniform time warp and uniform intensity change of the input events. The fundamental recognition event is a transient collective synchronization, representing “many neurons now agree,” an event that is then detected easily by a cell with a small time constant. If such synchronization is used in neurobiological computation, its hallmark will be a brief burst of gamma-band electroencephalogram noise when and where such a recognition event or decision occurs.},
	number = {3},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, J J and Brody, C D},
	year = {2001},
	keywords = {merged\_fiete.bib},
	pages = {1282--1287},
}

@article{hopfield_rapid_1995,
	title = {Rapid local synchronization of action potentials: toward computation with coupled integrate-and-fire neurons},
	volume = {92},
	abstract = {The collective behavior of interconnected spiking nerve cells is investigated. It is shown that a variety of model systems exhibit the same short-time behavior and rapidly converge to (approximately) periodic firing patterns with locally synchronized action potentials. The dynamics of one model can be described by a downhill motion on an abstract energy landscape. Since an energy landscape makes it possible to understand and program computation done by an attractor network, the results will extend our understanding of collective computation from models based on a firing-rate description to biologically more realistic systems with integrate-and-fire neurons.},
	number = {15},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, Jj and Herz, Av},
	month = jul,
	year = {1995},
	keywords = {merged\_fiete.bib},
	pages = {6655--6662},
}

@article{hopfield_pattern_1995,
	title = {Pattern recognition computation using action potential timing for stimulus representation},
	volume = {376},
	abstract = {A computational model is described in which the sizes of variables are represented by the explicit times at which action potentials occur, rather than by the more usual 'firing rate' of neurons. The comparison of patterns over sets of analogue variables is done by a network using different delays for different information paths. This mode of computation explains how one scheme of neuroarchitecture can be used for very different sensory modalities and seemingly different computations. The oscillations and anatomy of the mammalian olfactory systems have a simple interpretation in terms of this representation, and relate to processing in the auditory system. Single-electrode recording would not detect such neural computing. Recognition 'units' in this style respond more like radial basis function units than elementary sigmoid units.},
	number = {6535},
	journal = {Nature},
	author = {Hopfield, Jj},
	year = {1995},
	keywords = {merged\_fiete.bib},
	pages = {33--36},
}

@article{hopfield_what_2000,
	title = {What is a moment? “{Cortical}” sensory integration over a brief interval},
	volume = {97},
	abstract = {Recognition of complex temporal sequences is a general sensory problem that requires integration of information over time. We describe a very simple “organism” that performs this task, exemplified here by recognition of spoken monosyllables. The network's computation can be understood through the application of simple but generally unexploited principles describing neural activity. The organism is a network of very simple neurons and synapses; the experiments are simulations. The network's recognition capabilities are robust to variations across speakers, simple masking noises, and large variations in system parameters. The network principles underlying recognition of short temporal sequences are applied here to speech, but similar ideas can be applied to aspects of vision, touch, and olfaction. In this article, we describe only properties of the system that could be measured if it were a real biological organism. We delay publication of the principles behind the network's operation as an intellectual challenge: the essential principles of operation can be deduced based on the experimental results presented here alone. An interactive web site (http://neuron.princeton.edu/ approximately moment) is available to allow readers to design and carry out their own experiments on the organism.},
	number = {25},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Hopfield, Jj and Brody, Cd},
	month = dec,
	year = {2000},
	keywords = {merged\_fiete.bib},
	pages = {13919--13924},
}

@article{amari_learning_1972-1,
	title = {Learning {Patterns} and {Pattern} {Sequences} by {Self}-{Organizing} {Nets} of {Threshold} {Elements}},
	volume = {C-21},
	abstract = {We suggest a mecpurpose, the stability of state transition in an autonomous logical net of threshold elements is studied by the use of characteristics of threshold elements. It is also shown that a self-organizing net forms a representative pattern from a given set of stimulus patterns and fixes it as a stable state. The representative pattern can be recalled from any member of the set. This kind of self-organizing net may be regarded as a model for associative memory, sequential recalling, and concept formation.},
	number = {11},
	journal = {IEEE Trans. Comput.},
	author = {Amari, Shun-Ichi},
	year = {1972},
	keywords = {merged\_fiete.bib},
	pages = {1197--1206},
}

@article{amari_characteristics_1972,
	title = {Characteristics of {Random} {Nets} of {Analog} {Neuron}-{Like} {Elements}},
	volume = {SMC-2},
	abstract = {The dynamic behavior of randomly connected analog neuron-like elements that process pulse-frequency modulated signals is investigated from the macroscopic point of view. By extracting two statistical parameters, the macroscopic state equations are derived in terms of these parameters under some hypotheses on the stochastics of microscopic states. It is shown that a random net of statistically symmetric structure is monostable or bistable, and the stability criteria are explicitly given. Random nets consisting of many different classes of elements are also analyzed. Special attention is paid to nets of randomly connected excitatory and inhibitory elements. It is shown that a stable oscillation exists in such a net - in contrast with the fact that no stable oscillations exist in a net of statistically symmetric structure even if negative as well as positive synaptic weights are permitted at a time. The results are checked by computer-simulated experiments.},
	number = {5},
	journal = {IEEE Transactions on Systems, Man, \& Cybernetics},
	author = {Amari, Shun-Ichi},
	year = {1972},
	keywords = {merged\_fiete.bib},
	pages = {643--657},
}

@article{amari_characteristics_1971,
	title = {Characteristics of {Randomly} {Connected} {Threshold}-{Element} {Networks} and {Network} {Systems}},
	volume = {59},
	abstract = {The characteristics of the networks which are composed of many randomly connected threshold elements are investigated with the intention of understanding some aspects of information processing in nervous systems. In these networks, the statistical properties of connection are sufficient to determine the characteristics. Information is carried by the activity level of a network which designates the rate of exciting elements. Dynamics of the activity level is studied. Two statistical parameters, which are sufficient to determine the characteristics of networks, are extracted and the networks are categorized into three classes by these parameters. One is monostable, having only one stable activity level. Another is monostable or bistable according to the average threshold value of the elements. The third is astable or monostable. The characteristics of these three kinds of networks are analyzed in detail. Various systems can be obtained by connecting random networks, where the average thresholds of component networks can be controlled by other networks. The system performances are given. A stable oscillation of a long period is shown to exist in a system composed of two kinds of elements, i.e., excitatory and inhibitory elements, by randomly connecting them. A model for association of ideas is presented.},
	number = {1},
	journal = {Proc. IEEE},
	author = {Amari, Shun-Ichi},
	year = {1971},
	keywords = {merged\_fiete.bib},
	pages = {35--47},
}

@article{amari_learning_1972-2,
	title = {Learning pattern and pattern sequences by self-organizing nets of threshold elements},
	journal = {IEEE Trans. Comput.},
	author = {Amari, S-I},
	year = {1972},
	keywords = {Jun 12 import},
	pages = {1197--1206},
}

@article{amari_learning_1972-3,
	title = {Learning pattern and pattern sequences by self-organizing nets of threshold elements},
	journal = {IEEE Trans. Comput.},
	author = {Amari, S-I},
	year = {1972},
	keywords = {birdpaper.bib},
	pages = {1197--1206},
}

@misc{amari_mathematical_nodate,
	title = {Mathematical theories of neural networks},
	author = {Amari, Shun-Ichi},
	note = {Publication Title: Handbook of Neural Computation},
}

@article{seung_statistical_1992,
	title = {Statistical mechanics of learning from examples},
	volume = {45},
	language = {en},
	number = {8},
	journal = {Phys. Rev. A},
	author = {Seung, H S and Sompolinsky, H and Tishby, N},
	month = apr,
	year = {1992},
	pages = {6056--6091},
}

@article{hopfield_neural_1985,
	title = {“{Neural}” computation of decisions in optimization problems},
	volume = {52},
	abstract = {Highly-interconnected networks of nonlinear analog neurons are shown to be extremely effective in computing. The networks can rapidly provide a collectively-computed solution (a digital output) to a problem on the basis of analog input information. The problems to be solved must be formulated in terms of desired optima, often subject to constraints. The general principles involved in constructing networks to solve specific problems are discussed. Results of computer simulations of a network designed to solve a difficult but well-defined optimization problem–the Traveling-Salesman Problem–are presented and used to illustrate the computational power of the networks. Good solutions to this problem are collectively computed within an elapsed time of only a few neural time constants. The effectiveness of the computation involves both the nonlinear analog response of the neurons and the large connectivity among them. Dedicated networks of biological or microelectronic neurons could provide the computational capabilities described for a wide class of problems having combinatorial complexity. The power and speed naturally displayed by such collective networks may contribute to the effectiveness of biological information processing.},
	language = {en},
	number = {3},
	journal = {Biol. Cybern.},
	author = {Hopfield, J J and Tank, D W},
	year = {1985},
	pages = {141--152},
}

@article{burak_accurate_2009,
	title = {Accurate path integration in continuous attractor network models of grid cells},
	volume = {5},
	abstract = {Grid cells in the rat entorhinal cortex display strikingly regular firing responses to the animal's position in 2-D space and have been hypothesized to form the neural substrate for dead-reckoning. However, errors accumulate rapidly when velocity inputs are integrated in existing models of grid cell activity. To produce grid-cell-like responses, these models would require frequent resets triggered by external sensory cues. Such inadequacies, shared by various models, cast doubt on the dead-reckoning potential of the grid cell system. Here we focus on the question of accurate path integration, specifically in continuous attractor models of grid cell activity. We show, in contrast to previous models, that continuous attractor models can generate regular triangular grid responses, based on inputs that encode only the rat's velocity and heading direction. We consider the role of the network boundary in the integration performance of the network and show that both periodic and aperiodic networks are capable of accurate path integration, despite important differences in their attractor manifolds. We quantify the rate at which errors in the velocity integration accumulate as a function of network size and intrinsic noise within the network. With a plausible range of parameters and the inclusion of spike variability, our model networks can accurately integrate velocity inputs over a maximum of approximately 10-100 meters and approximately 1-10 minutes. These findings form a proof-of-concept that continuous attractor dynamics may underlie velocity integration in the dorsolateral medial entorhinal cortex. The simulations also generate pertinent upper bounds on the accuracy of integration that may be achieved by continuous attractor dynamics in the grid cell network. We suggest experiments to test the continuous attractor model and differentiate it from models in which single cells establish their responses independently of each other.},
	language = {en},
	number = {2},
	journal = {PLoS Comput. Biol.},
	author = {Burak, Yoram and Fiete, Ila R},
	month = feb,
	year = {2009},
	pages = {e1000291},
}

@article{khona_attractor_2021-1,
	title = {Attractor and integrator networks in the brain},
	abstract = {Attractor neural networks are some of the most-studied circuit models of brain function. We discuss the utility of low-dimensional attractors for computation in the brain, provide a …},
	journal = {arXiv preprint arXiv:2112.03978},
	author = {Khona, M and Fiete, I R},
	year = {2021},
}

@article{seung_query_1992,
	title = {Query by committee},
	abstract = {We propose an algorithm called query by commitee, in which a committee of students is trained on the same data set. The next query is chosen according to the principle of maximal …},
	journal = {of the fifth annual workshop on …},
	author = {Seung, H S and Opper, M and Sompolinsky, H},
	year = {1992},
}

@article{seung_simple_1993-2,
	title = {Simple models for reading neuronal population codes},
	volume = {90},
	abstract = {In many neural systems, sensory information is distributed throughout a population of neurons. We study simple neural network models for extracting this information. The inputs to the networks are the stochastic responses of a population of sensory neurons tuned to directional stimuli. The performance of each network model in psychophysical tasks is compared with that of the optimal maximum likelihood procedure. As a model of direction estimation in two dimensions, we consider a linear network that computes a population vector. Its performance depends on the width of the population tuning curves and is maximal for width, which increases with the level of background activity. Although for narrowly tuned neurons the performance of the population vector is significantly inferior to that of maximum likelihood estimation, the difference between the two is small when the tuning is broad. For direction discrimination, we consider two models: a perceptron with fully adaptive weights and a network made by adding an adaptive second layer to the population vector network. We calculate the error rates of these networks after exhaustive training to a particular direction. By testing on the full range of possible directions, the extent of transfer of training to novel stimuli can be calculated. It is found that for threshold linear networks the transfer of perceptual learning is nonmonotonic. Although performance deteriorates away from the training stimulus, it peaks again at an intermediate angle. This nonmonotonicity provides an important psychophysical test of these models.},
	language = {en},
	number = {22},
	journal = {Proc. Natl. Acad. Sci. U. S. A.},
	author = {Seung, H S and Sompolinsky, H},
	month = nov,
	year = {1993},
	pages = {10749--10753},
}

@article{hopfield_computing_1986,
	title = {Computing with neural circuits: a model},
	volume = {233},
	abstract = {A new conceptual framework and a minimization principle together provide an understanding of computation in model neural circuits. The circuits consist of nonlinear graded-response model neurons organized into networks with effectively symmetric synaptic connections. The neurons represent an approximation to biological neurons in which a simplified set of important computational properties is retained. Complex circuits solving problems similar to those essential in biology can be analyzed and understood without the need to follow the circuit dynamics in detail. Implementation of the model with electronic devices will provide a class of electronic circuits of novel form and function.},
	language = {en},
	number = {4764},
	journal = {Science},
	author = {Hopfield, J J and Tank, D W},
	year = {1986},
	pages = {625--633},
}

@article{Wang2002-co,
title = {Probabilistic Decision Making by Slow Reverberation in Cortical Circuits},
journal = {Neuron},
author = {Xiao-Jing Wang},
year = {2002},
abstract = {Recent physiological studies of alert primates have revealed cortical neural correlates of key steps in a perceptual decision-making process. To elucidate synaptic mechanisms of decision making, I investigated a biophysically realistic cortical network model for a visual discrimination experiment. In the model, slow recurrent excitation and feedback inhibition produce attractor dynamics that amplify the difference between conflicting inputs and generates a binary choice. The model is shown to account for salient characteristics of the observed decision-correlated neural activity, as well as the animal's psychometric function and reaction times. These results suggest that recurrent excitation mediated by NMDA receptors provides a candidate cellular mechanism for the slow time integration of sensory stimuli and the formation of categorical choices in a decision-making neocortical network.}
}

@misc{redman2023equivalent,
      title={On Equivalent Optimization of Machine Learning Methods}, 
      author={William T. Redman and Juan M. Bello-Rivas and Maria Fonoberova and Ryan Mohr and Ioannis G. Kevrekidis and Igor Mezić},
      year={2023},
      eprint={2302.09160},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@INPROCEEDINGS{redmanconjugacy,
  author={Redman, William T. and Fonoberova, Maria and Mohr, Ryan and Kevrekidis, Ioannis G. and Mezić, Igor},
  booktitle={2022 IEEE 61st Conference on Decision and Control (CDC)}, 
  title={Algorithmic (Semi-)Conjugacy via Koopman Operator Theory}, 
  year={2022},
  volume={},
  number={},
  pages={6006-6011},
}

@article{meziclinearization,
title = {Linearization in the large of nonlinear systems and Koopman operator spectrum},
journal = {Physica D: Nonlinear Phenomena},
year = {2013},
author = {Yueheng Lan and Igor Mezić},
keywords = {Linearization, Nonlinear dynamics, Hartman–Grobman Theorem, Hyperbolic systems, Koopman operator},
abstract = {According to the Hartman–Grobman Theorem, a nonlinear system can be linearized in a neighborhood of a hyperbolic stationary point. Here, we extend this linearization around stable (unstable) equilibria or periodic orbits to the whole basin of attraction, for both discrete diffeomorphisms and flows. We discuss the connection of the linearizing transformation to the spectrum of Koopman operator.}
}

@article{casdagli1991,
title = {State space reconstruction in the presence of noise},
journal = {Physica D: Nonlinear Phenomena},
volume = {51},
number = {1},
pages = {52-98},
year = {1991},
issn = {0167-2789},
doi = {https://doi.org/10.1016/0167-2789(91)90222-U},
url = {https://www.sciencedirect.com/science/article/pii/016727899190222U},
author = {Martin Casdagli and Stephen Eubank and J.Doyne Farmer and John Gibson},
}

@article {gaoganguli,
	author = {Peiran Gao and Eric Trautmann and Byron Yu and Gopal Santhanam and Stephen Ryu and Krishna Shenoy and Surya Ganguli},
	title = {A theory of multineuronal dimensionality, dynamics and measurement},
	year = {2017},
	publisher = {Cold Spring Harbor Laboratory},
	abstract = {In many experiments, neuroscientists tightly control behavior, record many trials, and obtain trial-averaged firing rates from hundreds of neurons in circuits containing billions of behaviorally relevant neurons. Di-mensionality reduction methods reveal a striking simplicity underlying such multi-neuronal data: they can be reduced to a low-dimensional space, and the resulting neural trajectories in this space yield a remarkably insightful dynamical portrait of circuit computation. This simplicity raises profound and timely conceptual questions. What are its origins and its implications for the complexity of neural dynamics? How would the situation change if we recorded more neurons? When, if at all, can we trust dynamical portraits obtained from measuring an infinitesimal fraction of task relevant neurons? We present a theory that answers these questions, and test it using physiological recordings from reaching monkeys. This theory reveals conceptual insights into how task complexity governs both neural dimensionality and accurate recovery of dynamic portraits, thereby providing quantitative guidelines for future large-scale experimental design.},
	URL = {https://www.biorxiv.org/content/early/2017/11/12/214262},
	journal = {bioRxiv}
}
