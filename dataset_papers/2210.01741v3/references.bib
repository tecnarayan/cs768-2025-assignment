@article{hanley_2008, 
    title={Existence of Solutions to Poisson's Equation}, volume={51}, 
    DOI={10.4153/CMB-2008-024-8}, 
    number={2},
    journal={Canadian Mathematical Bulletin}, 
    publisher={Cambridge University Press}, 
    author={Hanley, Mary}, year={2008}, pages={229–235}
}


@article{berger2003panoramic,
  title={A panoramic view of Riemannian geometry},
  author={Berger, Marcel},
  year={2003},
  publisher={Springer}
}

@book{morita2001geometry,
  title={Geometry of differential forms},
  author={Morita, Shigeyuki},
  number={201},
  year={2001},
  publisher={American Mathematical Soc.}
}

@book{do1998differential,
  title={Differential forms and applications},
  author={Do Carmo, Manfredo P},
  year={1998},
  publisher={Springer Science \& Business Media}
}


@incollection{mcanally_modeling_2000,
	series = {Proceedings in {Marine} {Science}},
	title = {Modeling mechanisms for the stability of the turbidity maximum in the {Gironde} estuary, {France}},
	volume = {3},
	url = {https://www.sciencedirect.com/science/article/pii/S1568269200801321},
	abstract = {Two numerical models are applied to the Gironde estuary in order to study basic mechanisms of turbidity maximum formation. A 2D depth-averaged model reproduces the turbidity maximum location for different river flow conditions, wherein tidal wave propagation is the only hydrodynamic forcing. Yet, excessive sediment escape to the ocean and associated loss of mass of the tidal turbidity maximum are observed. A 3D model is applied in order to incorporate vertical gradients and density stratification. The turbidity maximum is better reproduced, especially with respect to lateral gradients. The introduction of salinity gradients leads to less seaward dispersion, without any modification of the structure and location of the turbidity maximum. Results suggest that the turbidity maximum in the Gironde is exclusively tidally-induced, while density induced residual circulation imparts stability to the suspended sediment mass in the estuary. This conclusion is preliminary, as sedimentary processes are simplified in the model, and validation of sedimentary patterns is qualitative. In addition, the observation regarding the salinity effect needs to be verified by longer simulations with the 3D model, in order to provide an assessment of seaward fluxes over long periods.},
	booktitle = {Coastal and {Estuarine} {Fine} {Sediment} {Processes}},
	publisher = {Elsevier},
	author = {Sottolichio, A. and Hir, P. Le and Castaing, P.},
	editor = {McAnally, William H. and Mehta, Ashish J.},
	year = {2000},
	doi = {https://doi.org/10.1016/S1568-2692(00)80132-1},
	note = {ISSN: 1568-2692},
	pages = {373--386},
}

@inproceedings{makkuva2020optimal,
  title={Optimal transport mapping via input convex neural networks},
  author={Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle={International Conference on Machine Learning},
  pages={6672--6681},
  year={2020},
  organization={PMLR}
}

@article{huang2020convex,
  title={Convex potential flows: Universal probability distributions with optimal transport and convex optimization},
  author={Huang, Chin-Wei and Chen, Ricky T. Q. and Tsirigotis, Christos and Courville, Aaron},
  journal={arXiv preprint arXiv:2012.05942},
  year={2020}
}

@inproceedings{arjovsky2017wasserstein,
  title={Wasserstein generative adversarial networks},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  booktitle={International conference on machine learning},
  pages={214--223},
  year={2017},
  organization={PMLR}
}

@inproceedings{tong2020trajectorynet,
  title={Trajectorynet: A dynamic optimal transport network for modeling cellular dynamics},
  author={Tong, Alexander and Huang, Jessie and Wolf, Guy and Van Dijk, David and Krishnaswamy, Smita},
  booktitle={International Conference on Machine Learning},
  pages={9526--9536},
  year={2020},
  organization={PMLR}
}

@article{finlay2020train,
  title={How to train your neural ode},
  author={Finlay, Chris and Jacobsen, J{\"o}rn-Henrik and Nurbekyan, Levon and Oberman, Adam M},
  journal={arXiv preprint arXiv:2002.02798},
  year={2020}
}

@article{rout2021generative,
  title={Generative Modeling with Optimal Transport Maps},
  author={Rout, Litu and Korotin, Alexander and Burnaev, Evgeny},
  journal={arXiv preprint arXiv:2110.02999},
  year={2021}
}

@article{benamou2000computational,
  title={A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem},
  author={Benamou, Jean-David and Brenier, Yann},
  journal={Numerische Mathematik},
  volume={84},
  number={3},
  pages={375--393},
  year={2000},
  publisher={Springer}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{rozen2021moser,
  title={Moser Flow: Divergence-based Generative Modeling on Manifolds},
  author={Rozen, Noam and Grover, Aditya and Nickel, Maximilian and Lipman, Yaron},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{rao2020physics,
  title={Physics-informed deep learning for incompressible laminar flows},
  author={Rao, Chengping and Sun, Hao and Liu, Yang},
  journal={Theoretical and Applied Mechanics Letters},
  volume={10},
  number={3},
  pages={207--212},
  year={2020},
  publisher={Elsevier}
}

@article{lagaris1998artificial,
  title={Artificial neural networks for solving ordinary and partial differential equations},
  author={Lagaris, Isaac E and Likas, Aristidis and Fotiadis, Dimitrios I},
  journal={IEEE transactions on neural networks},
  volume={9},
  number={5},
  pages={987--1000},
  year={1998},
  publisher={IEEE}
}

@book{feynman_1989,
	series = {Advanced book program},
	title = {The {Feynman} {Lectures} on {Physics}: {Commemorative} {Issue}},
	isbn = {978-0-201-50064-6},
	url = {https://books.google.ca/books?id=UHp1AQAACAAJ},
	publisher = {Addison-Wesley},
	author = {Feynman, R.P. and Leighton, R.B. and Sands, M.L.},
	year = {1989},
	lccn = {89000433},
}


@article{bronstein2017geometric,
  title={Geometric deep learning: going beyond euclidean data},
  author={Bronstein, Michael M and Bruna, Joan and LeCun, Yann and Szlam, Arthur and Vandergheynst, Pierre},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={18--42},
  year={2017},
  publisher={IEEE}
}
@book{warner1983foundations,
  title={Foundations of Differentiable Manifolds and Lie Groups},
  author={Warner, F.W.},
  isbn={9780387908946},
  lccn={83012395},
  series={Graduate Texts in Mathematics},
  url={https://books.google.ca/books?id=iaeUqc2yQVQC},
  year={1983},
  publisher={Springer}
}
@misc{muller2022noetherconservation,
  doi = {10.48550/ARXIV.2209.11661},
  
  url = {https://arxiv.org/abs/2209.11661},
  
  author = {Müller, Eike Hermann},
  
  keywords = {Dynamical Systems (math.DS), Machine Learning (cs.LG), General Relativity and Quantum Cosmology (gr-qc), Computational Physics (physics.comp-ph), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences, G.1.7, 65L05, 68T07, 70H33, 70H40, 83C10},
  
  title = {Exact conservation laws for neural network integrators of dynamical systems},
  
  publisher = {arXiv},
  
  year = {2022},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@Article{sturm2022discreteconservation,
AUTHOR = {Sturm, P. O. and Wexler, A. S.},
TITLE = {Conservation laws in a neural network architecture: enforcing the atom
balance of a Julia-based photochemical model (v0.2.0)},
JOURNAL = {Geoscientific Model Development},
VOLUME = {15},
YEAR = {2022},
NUMBER = {8},
PAGES = {3417--3431},
URL = {https://gmd.copernicus.org/articles/15/3417/2022/},
DOI = {10.5194/gmd-15-3417-2022}
}


@ARTICLE{Petronetto2009Meshless,

  author={Petronetto, Fabiano and Paiva, Afonso and Lage, Marcos and Tavares, Geovan and Lopes, Helio and Lewiner, Thomas},

  journal={IEEE Transactions on Visualization and Computer Graphics}, 

  title={Meshless Helmholtz-Hodge Decomposition}, 

  year={2010},

  volume={16},

  number={2},

  pages={338-349},

  doi={10.1109/TVCG.2009.61}}

@ARTICLE{Bhatia2013survey,

  author={Bhatia, Harsh and Norgard, Gregory and Pascucci, Valerio and Bremer, Peer-Timo},

  journal={IEEE Transactions on Visualization and Computer Graphics}, 

  title={The Helmholtz-Hodge Decomposition—A Survey}, 

  year={2013},

  volume={19},

  number={8},

  pages={1386-1404},

  doi={10.1109/TVCG.2012.316}}



@article{li20213dmol,
  title={3DMol-Net: learn 3D molecular representation using adaptive graph convolutional network based on rotation invariance},
  author={Li, Chunyan and Wei, Wei and Li, Jin and Yao, Junfeng and Zeng, Xiangxiang and Lv, Zhihan},
  journal={IEEE Journal of Biomedical and Health Informatics},
  year={2021},
  publisher={IEEE}
}

@misc{Gerken2021invariance,
  doi = {10.48550/ARXIV.2105.13926},
  
  url = {https://arxiv.org/abs/2105.13926},
  
  author = {Gerken, Jan E. and Aronsson, Jimmy and Carlsson, Oscar and Linander, Hampus and Ohlsson, Fredrik and Petersson, Christoffer and Persson, Daniel},
  
  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), High Energy Physics - Theory (hep-th), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Physical sciences, FOS: Physical sciences},
  
  title = {Geometric Deep Learning and Equivariant Neural Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}




@incollection{hutchison_manifold_2010,
	address = {Berlin, Heidelberg},
	title = {Manifold {Valued} {Statistics}, {Exact} {Principal} {Geodesic} {Analysis} and the {Effect} of {Linear} {Approximations}},
	volume = {6316},
	isbn = {978-3-642-15566-6 978-3-642-15567-3},
	url = {http://link.springer.com/10.1007/978-3-642-15567-3_4},
	abstract = {Manifolds are widely used to model non-linearity arising in a range of computer vision applications. This paper treats statistics on manifolds and the loss of accuracy occurring when linearizing the manifold prior to performing statistical operations. Using recent advances in manifold computations, we present a comparison between the non-linear analog of Principal Component Analysis, Principal Geodesic Analysis, in its linearized form and its exact counterpart that uses true intrinsic distances. We give examples of datasets for which the linearized version provides good approximations and for which it does not. Indicators for the diﬀerences between the two versions are then developed and applied to two examples of manifold valued data: outlines of vertebrae from a study of vertebral fractures and spacial coordinates of human skeleton end-eﬀectors acquired using a stereo camera and tracking software.},
	language = {en},
	urldate = {2022-05-19},
	booktitle = {Computer {Vision} – {ECCV} 2010},
	publisher = {Springer Berlin Heidelberg},
	author = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Sommer, Stefan and Lauze, François and Hauberg, Søren and Nielsen, Mads},
	editor = {Daniilidis, Kostas and Maragos, Petros and Paragios, Nikos},
	year = {2010},
	doi = {10.1007/978-3-642-15567-3_4},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {43--56},
	file = {Hutchison et al. - 2010 - Manifold Valued Statistics, Exact Principal Geodes.pdf:/Users/colossal/Zotero/storage/8QXKX9AB/Hutchison et al. - 2010 - Manifold Valued Statistics, Exact Principal Geodes.pdf:application/pdf},
}


@misc{arvanitidis_latent_2021,
	title = {Latent {Space} {Oddity}: on the {Curvature} of {Deep} {Generative} {Models}},
	shorttitle = {Latent {Space} {Oddity}},
	url = {http://arxiv.org/abs/1710.11379},
	abstract = {Deep generative models provide a systematic way to learn nonlinear data distributions through a set of latent variables and a nonlinear “generator” function that maps latent points into the input space. The nonlinearity of the generator implies that the latent space gives a distorted view of the input space. Under mild conditions, we show that this distortion can be characterized by a stochastic Riemannian metric, and we demonstrate that distances and interpolants are signiﬁcantly improved under this metric. This in turn improves probability distributions, sampling algorithms and clustering in the latent space. Our geometric analysis further reveals that current generators provide poor variance estimates and we propose a new generator architecture with vastly improved variance estimates. Results are demonstrated on convolutional and fully connected variational autoencoders, but the formalism easily generalizes to other deep generative models.},
	language = {en},
	urldate = {2022-05-19},
	publisher = {arXiv},
	author = {Arvanitidis, Georgios and Hansen, Lars Kai and Hauberg, Søren},
	month = dec,
	year = {2021},
	note = {Number: arXiv:1710.11379
arXiv:1710.11379 [stat]},
	keywords = {Statistics - Machine Learning},
	annote = {Comment: Published at International Conference on Learning Representations (ICLR) 2018},
	file = {Arvanitidis et al. - 2021 - Latent Space Oddity on the Curvature of Deep Gene.pdf:/Users/colossal/Zotero/storage/QL8BA49P/Arvanitidis et al. - 2021 - Latent Space Oddity on the Curvature of Deep Gene.pdf:application/pdf},
}



@misc{eisenberger_divergence-free_2018,
	title = {Divergence-{Free} {Shape} {Interpolation} and {Correspondence}},
	url = {http://arxiv.org/abs/1806.10417},
	abstract = {We present a novel method to model and calculate deformation ﬁelds between shapes embedded in RD. Our framework combines naturally interpolating the two input shapes and calculating correspondences at the same time. The key idea is to compute a divergence-free deformation ﬁeld represented in a coarse-to-ﬁne basis using the Karhunen-Loéve expansion. The advantages are that there is no need to discretize the embedding space and the deformation is volume preserving. Furthermore, the optimization is done on downsampled versions of the shapes but the morphing can be applied to any resolution without a heavy increase in complexity. We show results for shape correspondence, registration, inter- and extrapolation on the TOSCA and FAUST Scan data sets.},
	language = {en},
	urldate = {2022-05-19},
	publisher = {arXiv},
	author = {Eisenberger, Marvin and Lähner, Zorah and Cremers, Daniel},
	month = oct,
	year = {2018},
	note = {Number: arXiv:1806.10417
arXiv:1806.10417 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	file = {Eisenberger et al. - 2018 - Divergence-Free Shape Interpolation and Correspond.pdf:/Users/colossal/Zotero/storage/ML8MBX5Q/Eisenberger et al. - 2018 - Divergence-Free Shape Interpolation and Correspond.pdf:application/pdf},
}



@article{jagtap_conservative_2020,
	title = {Conservative physics-informed neural networks on discrete domains for conservation laws: {Applications} to forward and inverse problems},
	volume = {365},
	issn = {00457825},
	shorttitle = {Conservative physics-informed neural networks on discrete domains for conservation laws},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782520302127},
	doi = {10.1016/j.cma.2020.113028},
	abstract = {We propose a conservative physics-informed neural network (cPINN) on discrete domains for nonlinear conservation laws. Here, the term discrete domain represents the discrete sub-domains obtained after division of the computational domain, where PINN is applied and the conservation property of cPINN is obtained by enforcing the flux continuity in the strong form along the sub-domain interfaces. In case of hyperbolic conservation laws, the convective flux contributes at the interfaces, whereas in case of viscous conservation laws, both convective and diffusive fluxes contribute. Apart from the flux continuity condition, an average solution (given by two different neural networks) is also enforced at the common interface between two sub-domains. One can also employ a deep neural network in the domain, where the solution may have complex structure, whereas a shallow neural network can be used in the sub-domains with relatively simple and smooth solutions. Another advantage of the proposed method is the additional freedom it gives in terms of the choice of optimization algorithm and the various training parameters like residual points, activation function, width and depth of the network etc. Various forms of errors involved in cPINN such as optimization, generalization and approximation errors and their sources are discussed briefly. In cPINN, locally adaptive activation functions are used, hence training the model faster compared to its fixed counterparts. Both, forward and inverse problems are solved using the proposed method. Various test cases ranging from scalar nonlinear conservation laws like Burgers, Korteweg–de Vries (KdV) equations to systems of conservation laws, like compressible Euler equations are solved. The lid-driven cavity test case governed by incompressible Navier–Stokes equation is also solved and the results are compared against a benchmark solution. The proposed method enjoys the property of domain decomposition with separate neural networks in each sub-domain, and it efficiently lends itself to parallelized computation, where each sub-domain can be assigned to a different computational node.},
	language = {en},
	urldate = {2022-05-19},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Jagtap, Ameya D. and Kharazmi, Ehsan and Karniadakis, George Em},
	month = jun,
	year = {2020},
	pages = {113028},
	file = {Jagtap et al. - 2020 - Conservative physics-informed neural networks on d.pdf:/Users/colossal/Zotero/storage/EXWE4UXU/Jagtap et al. - 2020 - Conservative physics-informed neural networks on d.pdf:application/pdf},
}


@article{mao_physics-informed_2020,
	title = {Physics-informed neural networks for high-speed flows},
	volume = {360},
	issn = {00457825},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0045782519306814},
	doi = {10.1016/j.cma.2019.112789},
	abstract = {In this work we investigate the possibility of using physics-informed neural networks (PINNs) to approximate the Euler equations that model high-speed aerodynamic flows. In particular, we solve both the forward and inverse problems in onedimensional and two-dimensional domains. For the forward problem, we utilize the Euler equations and the initial/boundary conditions to formulate the loss function, and solve the one-dimensional Euler equations with smooth solutions and with solutions that have a contact discontinuity as well as a two-dimensional oblique shock wave problem. We demonstrate that we can capture the solutions with only a few scattered points clustered randomly around the discontinuities. For the inverse problem, motivated by mimicking the Schlieren photography experimental technique used traditionally in high-speed aerodynamics, we use the data on density gradient ∇ρ(x, t), the pressure p(x∗, t) at a specified point x = x∗ as well as the conservation laws to infer all states of interest (density, velocity and pressure fields). We present illustrative benchmark examples for both the problem with smooth solutions and Riemann problems (Sod and Lax problems) with PINNs, demonstrating that all inferred states are in good agreement with the reference solutions. Moreover, we show that the choice of the position of the point x∗ plays an important role in the learning process. In particular, for the problem with smooth solutions we can randomly choose the position of the point x∗ from the computational domain, while for the Sod or Lax problem, we have to choose the position of the point x∗ from the domain between the initial discontinuous point and the shock position of the final time. We also solve the inverse problem by combining the aforementioned data and the Euler equations in characteristic form, showing that the results obtained by using the Euler equations in characteristic form are better than that obtained by using the Euler equations in conservative form. Furthermore, we consider another type of inverse problem, specifically, we employ PINNs to learn the value of the parameter γ in the equation of state for the parameterized two-dimensional oblique wave problem by using the given data of the density, velocity and the pressure, and we identify the parameter γ accurately. Taken together, our results demonstrate that in the current form, where the conservation laws are imposed at random points, PINNs are not as accurate as traditional numerical methods for forward problems but they are superior for inverse problems that cannot even be solved with standard techniques.},
	language = {en},
	urldate = {2022-05-19},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Mao, Zhiping and Jagtap, Ameya D. and Karniadakis, George Em},
	month = mar,
	year = {2020},
	pages = {112789},
	file = {Mao et al. - 2020 - Physics-informed neural networks for high-speed fl.pdf:/Users/colossal/Zotero/storage/BNEG3U3J/Mao et al. - 2020 - Physics-informed neural networks for high-speed fl.pdf:application/pdf},
}


@article{jin_nsfnets_2021,
	title = {{NSFnets} ({Navier}-{Stokes} {Flow} nets): {Physics}-informed neural networks for the incompressible {Navier}-{Stokes} equations},
	volume = {426},
	issn = {00219991},
	shorttitle = {{NSFnets} ({Navier}-{Stokes} {Flow} nets)},
	url = {http://arxiv.org/abs/2003.06496},
	doi = {10.1016/j.jcp.2020.109951},
	abstract = {We employ physics-informed neural networks (PINNs) to simulate the incompressible flows ranging from laminar to turbulent flows. We perform PINN simulations by considering two different formulations of the Navier-Stokes equations: the velocity-pressure (VP) formulation and the vorticity-velocity (VV) formulation. We refer to these specific PINNs for the Navier-Stokes flow nets as NSFnets. Analytical solutions and direct numerical simulation (DNS) databases provide proper initial and boundary conditions for the NSFnet simulations. The spatial and temporal coordinates are the inputs of the NSFnets, while the instantaneous velocity and pressure fields are the outputs for the VP-NSFnet, and the instantaneous velocity and vorticity fields are the outputs for the VV-NSFnet. These two different forms of the Navier-Stokes equations together with the initial and boundary conditions are embedded into the loss function of the PINNs. No data is provided for the pressure to the VP-NSFnet, which is a hidden state and is obtained via the incompressibility constraint without splitting the equations. We obtain good accuracy of the NSFnet simulation results upon convergence of the loss function, verifying that NSFnets can effectively simulate complex incompressible flows using either the VP or the VV formulations. We also perform a systematic study on the weights used in the loss function for the data/physics components and investigate a new way of computing the weights dynamically to accelerate training and enhance accuracy. Our results suggest that the accuracy of NSFnets, for both laminar and turbulent flows, can be improved with proper tuning of weights (manual or dynamic) in the loss function.},
	language = {en},
	urldate = {2022-04-16},
	journal = {Journal of Computational Physics},
	author = {Jin, Xiaowei and Cai, Shengze and Li, Hui and Karniadakis, George Em},
	month = feb,
	year = {2021},
	note = {arXiv: 2003.06496},
	keywords = {Physics - Computational Physics},
	pages = {109951},
	file = {Jin et al. - 2021 - NSFnets (Navier-Stokes Flow nets) Physics-informe.pdf:/Users/colossal/Zotero/storage/KE45RHJE/Jin et al. - 2021 - NSFnets (Navier-Stokes Flow nets) Physics-informe.pdf:application/pdf},
}


@article{liu_divergence-free_2021,
	title = {A divergence-free finite element method for the {Stokes} problem with boundary correction},
	url = {http://arxiv.org/abs/2105.10409},
	abstract = {This paper constructs and analyzes a boundary correction ﬁnite element method for the Stokes problem based on the Scott-Vogelius pair on Clough-Tocher splits. The velocity space consists of continuous piecewise quadratic polynomials, and the pressure space consists of piecewise linear polynomials without continuity constraints. A Lagrange multiplier space that consists of continuous piecewise quadratic polynomials with respect to boundary partition is introduced to enforce boundary conditions as well as to mitigate the lack of pressure-robustness. We prove several inf-sup conditions, leading to the well-posedness of the method. In addition, we show that the method converges with optimal order and the velocity approximation is divergence free.},
	language = {en},
	urldate = {2022-04-22},
	journal = {arXiv:2105.10409 [cs, math]},
	author = {Liu, Haoran and Neilan, Michael and Otus, Baris},
	month = may,
	year = {2021},
	note = {arXiv: 2105.10409},
	keywords = {Mathematics - Numerical Analysis},
	file = {Liu et al. - 2021 - A divergence-free finite element method for the St.pdf:/Users/colossal/Zotero/storage/DDWG24MI/Liu et al. - 2021 - A divergence-free finite element method for the St.pdf:application/pdf},
}




@article{almgren_conservative_1998,
	title = {A {Conservative} {Adaptive} {Projection} {Method} for the {Variable} {Density} {Incompressible} {Navier}–{Stokes} {Equations}},
	volume = {142},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999198958909},
	doi = {10.1006/jcph.1998.5890},
	language = {en},
	number = {1},
	urldate = {2022-05-19},
	journal = {Journal of Computational Physics},
	author = {Almgren, Ann S. and Bell, John B. and Colella, Phillip and Howell, Louis H. and Welcome, Michael L.},
	month = may,
	year = {1998},
	pages = {1--46},
	file = {Almgren et al. - 1998 - A Conservative Adaptive Projection Method for the .pdf:/Users/colossal/Zotero/storage/TJHYSP9X/Almgren et al. - 1998 - A Conservative Adaptive Projection Method for the .pdf:application/pdf},
}

@article{Schroeder_2017,
	doi = {10.1515/jnma-2016-1101},
  
	url = {https://doi.org/10.1515%2Fjnma-2016-1101},
  
	year = 2017,
	month = {dec},
  
	publisher = {Walter de Gruyter {GmbH}
},
  
	volume = {25},
  
	number = {4},
  
	author = {Philipp W. Schroeder and Gert Lube},
  
	title = {Pressure-robust analysis of divergence-free and conforming {FEM} for evolutionary incompressible Navier{\textendash}Stokes flows},
  
	journal = {Journal of Numerical Mathematics}
}

@article{arthurs_active_2021,
	title = {Active training of physics-informed neural networks to aggregate and interpolate parametric solutions to the {Navier}-{Stokes} equations},
	volume = {438},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S002199912100259X},
	doi = {10.1016/j.jcp.2021.110364},
	abstract = {The goal of this work is to train a neural network which approximates solutions to the Navier-Stokes equations across a region of parameter space, in which the parameters define physical properties such as domain shape and boundary conditions. The contributions of this work are threefold:1.To demonstrate that neural networks can be efficient aggregators of whole families of parametric solutions to physical problems, trained using data created with traditional, trusted numerical methods such as finite elements. Advantages include extremely fast evaluation of pressure and velocity at any point in physical and parameter space (asymptotically, ∼3 μs/query), and data compression (the network requires 99\% less storage space compared to its own training data).2.To demonstrate that the neural networks can accurately interpolate between finite element solutions in parameter space, allowing them to be instantly queried for pressure and velocity field solutions to problems for which traditional simulations have never been performed.3.To introduce an active learning algorithm, so that during training, a finite element solver can automatically be queried to obtain additional training data in locations where the neural network's predictions are in most need of improvement, thus autonomously acquiring and efficiently distributing training data throughout parameter space. In addition to the obvious utility of Item 2, above, we demonstrate an application of the network in rapid parameter sweeping, very precisely predicting the degree of narrowing in a tube which would result in a 50\% increase in end-to-end pressure difference at a given flow rate. This capability could have applications in both medical diagnosis of arterial disease, and in computer-aided design.},
	language = {en},
	urldate = {2022-05-18},
	journal = {Journal of Computational Physics},
	author = {Arthurs, Christopher J. and King, Andrew P.},
	month = aug,
	year = {2021},
	keywords = {Active learning, Computing methods, Deep learning, Navier-Stokes, Training},
	pages = {110364},
}


@article{guermond_projection_2000,
	title = {A {Projection} {FEM} for {Variable} {Density} {Incompressible} {Flows}},
	volume = {165},
	issn = {00219991},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0021999100966099},
	doi = {10.1006/jcph.2000.6609},
	language = {en},
	number = {1},
	urldate = {2022-05-18},
	journal = {Journal of Computational Physics},
	author = {Guermond, J.-L. and Quartapelle, L.},
	month = nov,
	year = {2000},
	pages = {167--188},
	file = {Guermond and Quartapelle - 2000 - A Projection FEM for Variable Density Incompressib.pdf:/Users/colossal/Zotero/storage/YI7YEFLI/Guermond and Quartapelle - 2000 - A Projection FEM for Variable Density Incompressib.pdf:application/pdf},
}


@misc{raissi_physics_2017,
	title = {Physics {Informed} {Deep} {Learning} ({Part} {I}): {Data}-driven {Solutions} of {Nonlinear} {Partial} {Differential} {Equations}},
	shorttitle = {Physics {Informed} {Deep} {Learning} ({Part} {I})},
	url = {http://arxiv.org/abs/1711.10561},
	abstract = {We introduce physics informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial diﬀerential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial diﬀerential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-eﬃcient universal function approximators that naturally encode any underlying physical laws as prior information. In this ﬁrst part, we demonstrate how these networks can be used to infer solutions to partial diﬀerential equations, and obtain physics-informed surrogate models that are fully diﬀerentiable with respect to all input coordinates and free parameters.},
	language = {en},
	urldate = {2022-05-18},
	publisher = {arXiv},
	author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
	month = nov,
	year = {2017},
	note = {Number: arXiv:1711.10561
arXiv:1711.10561 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Numerical Analysis, Computer Science - Artificial Intelligence, Mathematics - Dynamical Systems},
	file = {Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:/Users/colossal/Zotero/storage/EYEZXGLE/Raissi et al. - 2017 - Physics Informed Deep Learning (Part I) Data-driv.pdf:application/pdf},
}



@article{raissi_physics-informed_2019,
	title = {Physics-informed neural networks: {A} deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations},
	volume = {378},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999118307125},
	doi = {https://doi.org/10.1016/j.jcp.2018.10.045},
	abstract = {We introduce physics-informed neural networks – neural networks that are trained to solve supervised learning tasks while respecting any given laws of physics described by general nonlinear partial differential equations. In this work, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct types of algorithms, namely continuous time and discrete time models. The first type of models forms a new family of data-efficient spatio-temporal function approximators, while the latter type allows the use of arbitrarily accurate implicit Runge–Kutta time stepping schemes with unlimited number of stages. The effectiveness of the proposed framework is demonstrated through a collection of classical problems in fluids, quantum mechanics, reaction–diffusion systems, and the propagation of nonlinear shallow-water waves.},
	journal = {Journal of Computational Physics},
	author = {Raissi, M. and Perdikaris, P. and Karniadakis, G. E.},
	year = {2019},
	keywords = {Data-driven scientific computing, Machine learning, Nonlinear dynamics, Predictive modeling, Runge–Kutta methods},
	pages = {686--707},
}

@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}

@inproceedings{amos2017optnet,
  title={Optnet: Differentiable optimization as a layer in neural networks},
  author={Amos, Brandon and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  pages={136--145},
  year={2017},
  organization={PMLR}
}

@article{bai2019deep,
  title={Deep equilibrium models},
  author={Bai, Shaojie and Kolter, J Zico and Koltun, Vladlen},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{chen2018neural,
  title={Neural ordinary differential equations},
  author={Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{amos2017input,
  title={Input convex neural networks},
  author={Amos, Brandon and Xu, Lei and Kolter, J Zico},
  booktitle={International Conference on Machine Learning},
  pages={146--155},
  year={2017},
  organization={PMLR}
}

@article{miyato2018spectral,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}

@inproceedings{onken2021ot,
  title={Ot-flow: Fast and accurate continuous normalizing flows via optimal transport},
  author={Onken, Derek and Wu Fung, S and Li, Xingjian and Ruthotto, Lars},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={1-18},
  year={2021}
}

@article{chen2019residual,
  title={Residual flows for invertible generative modeling},
  author={Chen, Ricky T. Q. and Behrmann, Jens and Duvenaud, David K and Jacobsen, J{\"o}rn-Henrik},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{lu2021implicit,
  title={Implicit normalizing flows},
  author={Lu, Cheng and Chen, Jianfei and Li, Chongxuan and Wang, Qiuhao and Zhu, Jun},
  journal={arXiv preprint arXiv:2103.09527},
  year={2021}
}

@article{barbarosie2011representation,
  title={Representation of divergence-free vector fields},
  author={Barbarosie, Cristian},
  journal={Quarterly of applied mathematics},
  volume={69},
  number={2},
  pages={309--316},
  year={2011}
}

@article{kelliher2021stream,
  title={Stream functions for divergence-free vector fields},
  author={Kelliher, James},
  journal={Quarterly of Applied Mathematics},
  volume={79},
  number={1},
  pages={163--174},
  year={2021}
}

@inproceedings{bonneel2011displacement,
  title={Displacement interpolation using Lagrangian mass transport},
  author={Bonneel, Nicolas and Van De Panne, Michiel and Paris, Sylvain and Heidrich, Wolfgang},
  booktitle={Proceedings of the 2011 SIGGRAPH Asia conference},
  pages={1--12},
  year={2011}
}

@article{flamary2021pot,
  author  = {R{\'e}mi Flamary and Nicolas Courty and Alexandre Gramfort and Mokhtar Z. Alaya and Aur{\'e}lie Boisbunon and Stanislas Chambon and Laetitia Chapel and Adrien Corenflos and Kilian Fatras and Nemo Fournier and L{\'e}o Gautheron and Nathalie T.H. Gayraud and Hicham Janati and Alain Rakotomamonjy and Ievgen Redko and Antoine Rolet and Antony Schutz and Vivien Seguy and Danica J. Sutherland and Romain Tavenard and Alexander Tong and Titouan Vayer},
  title   = {POT: Python Optimal Transport},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {78},
  pages   = {1-8},
  url     = {http://jmlr.org/papers/v22/20-451.html}
}

@article{nigam1980divergence,
  title={Divergence-free vector fields},
  author={Nigam, SD and Usha, R and Swaminathan, K},
  journal={J. Math. Phys. Sci},
  volume={14},
  number={5},
  pages={523--527},
  year={1980}
}

@article{chen2019neural,
  title={Neural networks with cheap differential operators},
  author={Chen, Ricky T. Q. and Duvenaud, David K},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{cartan1899some,
  title={On certain differential expressions and the Pfaff problem},
  author={Cartan, {\'E}lie},
  booktitle={Scientific Annals of the {\'School}Normal Superior},
  volume={16},
  pages={239--332},
  year={1899}
}


@InProceedings{makkuva20a,
  title = 	 {Optimal transport mapping via input convex neural networks},
  author =       {Makkuva, Ashok and Taghvaei, Amirhossein and Oh, Sewoong and Lee, Jason},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {6672--6681},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}

@article{salimans2016improved,
  title={Improved techniques for training gans},
  author={Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{che2016mode,
  title={Mode regularized generative adversarial networks},
  author={Che, Tong and Li, Yanran and Jacob, Athul Paul and Bengio, Yoshua and Li, Wenjie},
  journal={International Conference on Learning Representations},
  year={2017}
}

@article{srivastava2017veegan,
  title={Veegan: Reducing mode collapse in gans using implicit variational learning},
  author={Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U and Sutton, Charles},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2015unsupervised,
  title={Unsupervised representation learning with deep convolutional generative adversarial networks},
  author={Radford, Alec and Metz, Luke and Chintala, Soumith},
  journal={International Conference on Learning Representations},
  year={2016}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={International Conference on Learning Representations},
  year={2015}
}

@article{song2019generative,
  title={Generative modeling by estimating gradients of the data distribution},
  author={Song, Yang and Ermon, Stefano},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@InProceedings{xue20a,
  title = 	 {Amortized Finite Element Analysis for Fast {PDE}-Constrained Optimization},
  author =       {Xue, Tianju and Beatson, Alex and Adriaenssens, Sigrid and Adams, Ryan},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {10638--10647},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
}


@article{andrychowicz2016learning,
  title={Learning to learn by gradient descent by gradient descent},
  author={Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and De Freitas, Nando},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{hurault2021gradient,
  title={Gradient step denoiser for convergent plug-and-play},
  author={Hurault, Samuel and Leclaire, Arthur and Papadakis, Nicolas},
  journal={International Conference on Learning Representations},
  year={2022}
}

@book{van1995python,
  author    = {Van Rossum, Guido and Drake Jr, Fred L},
  publisher = {Centrum voor Wiskunde en Informatica Amsterdam},
  year = {1995},
  title     = {Python reference manual},
}
@inproceedings{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  booktitle={Advances in neural information processing systems},
  pages={8026--8037},
  year=2019
}
@article{oliphant2007python,
  author       = {Oliphant, Travis E},
  publisher    = {IEEE},
  year = {2007},
  journal = {Computing in Science \& Engineering},
  number       = {3},
  pages        = {10--20},
  title        = {Python for scientific computing},
  volume       = {9},
}
@book{oliphant2006guide,
  author    = {Oliphant, Travis E},
  publisher = {Trelgol Publishing USA},
  year = {2006},
  title     = {A guide to NumPy},
  volume    = {1},
}
@misc{torchdiffeq,
	author={Chen, Ricky T. Q.},
	title={torchdiffeq},
	year={2018},
	url={https://github.com/rtqichen/torchdiffeq},
}
@Misc{Yadan2019Hydra,
  author =       {Omry Yadan},
  title =        {Hydra - A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}
@inproceedings{kluyver2016jupyter,
  author    = {Kluyver, Thomas and Ragan-Kelley, Benjamin and P{\'{e}}rez, Fernando and Granger, Brian E and Bussonnier, Matthias and Frederic, Jonathan and Kelley, Kyle and Hamrick, Jessica B and Grout, Jason and Corlay, Sylvain and others},
  booktitle = {ELPUB},
  year = {2016},
  pages     = {87--90},
  title     = {Jupyter Notebooks-a publishing format for reproducible computational workflows.},
}
@article{hunter2007matplotlib,
  author       = {Hunter, John D},
  publisher    = {IEEE Computer Society},
  year = {2007},
  journal = {Computing in science \& engineering},
  number       = {3},
  pages        = {90},
  title        = {Matplotlib: A 2D graphics environment},
  volume       = {9},
}
@book{mckinney2012python,
  author    = {McKinney, Wes},
  publisher = {" O'Reilly Media, Inc."},
  year = {2012},
  title     = {Python for data analysis: Data wrangling with Pandas, NumPy, and IPython},
}
@article{jones2014scipy,
  author = {Jones, Eric and Oliphant, Travis and Peterson, Pearu},
  year = {2014},
  title  = {$\{$SciPy$\}$: Open source scientific tools for $\{$Python$\}$},
}
@misc{seaborn,
  author       = {Michael Waskom and
                  Olga Botvinnik and
                  Drew O'Kane and
                  Paul Hobson and
                  Joel Ostblom and
                  Saulius Lukauskas and
                  David C Gemperline and
                  Tom Augspurger and
                  Yaroslav Halchenko and
                  John B. Cole and
                  Jordi Warmenhoven and
                  Julian de Ruiter and
                  Cameron Pye and
                  Stephan Hoyer and
                  Jake Vanderplas and
                  Santi Villalba and
                  Gero Kunter and
                  Eric Quintero and
                  Pete Bachant and
                  Marcel Martin and
                  Kyle Meyer and
                  Alistair Miles and
                  Yoav Ram and
                  Thomas Brunner and
                  Tal Yarkoni and
                  Mike Lee Williams and
                  Constantine Evans and
                  Clark Fitzgerald and
                  Brian and
                  Adel Qalieh},
  title        = {mwaskom/seaborn: v0.9.0 (July 2018)},
  month        = jul,
  year         = 2018,
  doi          = {10.5281/zenodo.1313201},
  url          = {https://doi.org/10.5281/zenodo.1313201}
}
@article{van2011numpy,
  author       = {Van Der Walt, Stefan and Colbert, S Chris and Varoquaux, Gael},
  publisher    = {IEEE Computer Society},
  year = {2011},
  journal = {Computing in Science \& Engineering},
  number       = {2},
  pages        = {22},
  title        = {The NumPy array: a structure for efficient numerical computation},
  volume       = {13},
}
@Misc{functorch2021,
  author =       {Horace He, Richard Zou},
  title =        {functorch: JAX-like composable function transforms for PyTorch},
  howpublished = {\url{https://github.com/pytorch/functorch}},
  year =         {2021}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.3.13},
  year = {2018},
}
@software{flax2020github,
  author = {Jonathan Heek and Anselm Levskaya and Avital Oliver and Marvin Ritter and Bertrand Rondepierre and Andreas Steiner and Marc van {Z}ee},
  title = {{F}lax: A neural network library and ecosystem for {JAX}},
  url = {http://github.com/google/flax},
  version = {0.5.3},
  year = {2020},
}