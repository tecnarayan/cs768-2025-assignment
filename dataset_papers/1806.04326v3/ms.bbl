\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Asuncion \& Newman(2007)Asuncion and Newman]{asuncion2007uci}
Asuncion, A. and Newman, D.
\newblock {UCI} machine learning repository, 2007.

\bibitem[Bach(2009)]{bach2009exploring}
Bach, F.~R.
\newblock Exploring large feature spaces with hierarchical multiple kernel
  learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  105--112, 2009.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Bochner(1959)]{bochner1959lectures}
Bochner, S.
\newblock \emph{Lectures on Fourier Integrals: With an Author's Supplement on
  Monotonic Functions, Stieltjes Integrals and Harmonic Analysis; Translated
  from the Original German by Morris Tenenbaum and Harry Pollard}.
\newblock Princeton University Press, 1959.

\bibitem[Brochu et~al.(2010)Brochu, Cora, and De~Freitas]{brochu2010tutorial}
Brochu, E., Cora, V.~M., and De~Freitas, N.
\newblock A tutorial on bayesian optimization of expensive cost functions, with
  application to active user modeling and hierarchical reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1012.2599}, 2010.

\bibitem[Csiszar \& K{\"o}rner(2011)Csiszar and
  K{\"o}rner]{csiszar2011information}
Csiszar, I. and K{\"o}rner, J.
\newblock \emph{Information theory: coding theorems for discrete memoryless
  systems}.
\newblock Cambridge University Press, 2011.

\bibitem[Duvenaud et~al.(2013)Duvenaud, Lloyd, Grosse, Tenenbaum, and
  Ghahramani]{duvenaud2013structure}
Duvenaud, D., Lloyd, J.~R., Grosse, R., Tenenbaum, J.~B., and Ghahramani, Z.
\newblock Structure discovery in nonparametric regression through compositional
  kernel search.
\newblock \emph{arXiv preprint arXiv:1302.4922}, 2013.

\bibitem[Duvenaud et~al.(2011)Duvenaud, Nickisch, and
  Rasmussen]{duvenaud2011additive}
Duvenaud, D.~K., Nickisch, H., and Rasmussen, C.~E.
\newblock Additive {Gaussian} processes.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  226--234, 2011.

\bibitem[Gardner et~al.(2017)Gardner, Guo, Weinberger, Garnett, and
  Grosse]{gardner2017discovering}
Gardner, J., Guo, C., Weinberger, K., Garnett, R., and Grosse, R.
\newblock Discovering and exploiting additive structure for {Bayesian}
  optimization.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1311--1319,
  2017.

\bibitem[Genton(2001)]{genton2001classes}
Genton, M.~G.
\newblock Classes of kernels for machine learning: a statistics perspective.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Dec):\penalty0 299--312, 2001.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of {Bayesian}
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1869, 2015.

\bibitem[Hinton \& Salakhutdinov(2008)Hinton and
  Salakhutdinov]{hinton2008using}
Hinton, G.~E. and Salakhutdinov, R.~R.
\newblock Using deep belief nets to learn covariance kernels for {Gaussian}
  processes.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1249--1256, 2008.

\bibitem[Kakihara(1985)]{kakihara1985note}
Kakihara, Y.
\newblock A note on harmonizable and v-bounded processes.
\newblock \emph{Journal of Multivariate Analysis}, 16\penalty0 (1):\penalty0
  140--156, 1985.

\bibitem[Kandasamy et~al.(2015)Kandasamy, Schneider, and
  P{\'o}czos]{kandasamy2015high}
Kandasamy, K., Schneider, J., and P{\'o}czos, B.
\newblock High dimensional bayesian optimisation and bandits via additive
  models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  295--304, 2015.

\bibitem[Kom~Samo \& Roberts(2015)Kom~Samo and Roberts]{kom2015generalized}
Kom~Samo, Y.-L. and Roberts, S.
\newblock Generalized spectral kernels.
\newblock \emph{arXiv preprint arXiv:1506.02236}, 2015.

\bibitem[L\'azaro-Gredilla et~al.(2010)L\'azaro-Gredilla, Qui\~nonero Candela,
  Rasmussen, and Figueiras-Vidal]{quia2010sparse}
L\'azaro-Gredilla, M., Qui\~nonero Candela, J., Rasmussen, C.~E., and
  Figueiras-Vidal, A.~R.
\newblock Sparse spectrum {Gaussian} process regression.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Jun):\penalty0 1865--1881, 2010.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1989backpropagation}
LeCun, Y., Boser, B., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard, W.,
  and Jackel, L.~D.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Lloyd et~al.(2014)Lloyd, Duvenaud, Grosse, Tenenbaum, and
  Ghahramani]{lloyd2014automatic}
Lloyd, J.~R., Duvenaud, D.~K., Grosse, R.~B., Tenenbaum, J.~B., and Ghahramani,
  Z.
\newblock Automatic construction and natural-language description of
  nonparametric regression models.
\newblock 2014.

\bibitem[Malkomes et~al.(2016)Malkomes, Schaff, and
  Garnett]{malkomes2016bayesian}
Malkomes, G., Schaff, C., and Garnett, R.
\newblock Bayesian optimization for automated model selection.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2900--2908, 2016.

\bibitem[Oliva et~al.(2016)Oliva, Dubey, Wilson, P{\'o}czos, Schneider, and
  Xing]{oliva2016bayesian}
Oliva, J.~B., Dubey, A., Wilson, A.~G., P{\'o}czos, B., Schneider, J., and
  Xing, E.~P.
\newblock Bayesian nonparametric kernel-learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1078--1086,
  2016.

\bibitem[Poon \& Domingos(2011)Poon and Domingos]{poon2011sum}
Poon, H. and Domingos, P.
\newblock Sum-product networks: A new deep architecture.
\newblock In \emph{Computer Vision Workshops (ICCV Workshops), 2011 IEEE
  International Conference on}, pp.\  689--690. IEEE, 2011.

\bibitem[Rasmussen(1999)]{rasmussen1999evaluation}
Rasmussen, C.~E.
\newblock \emph{Evaluation of {Gaussian} processes and other methods for
  non-linear regression.}
\newblock Citeseer, 1999.

\bibitem[Remes et~al.(2017)Remes, Heinonen, and Kaski]{remes2017non}
Remes, S., Heinonen, M., and Kaski, S.
\newblock Non-stationary spectral kernels.
\newblock \emph{arXiv preprint arXiv:1705.08736}, 2017.

\bibitem[Saat{\c{c}}i(2012)]{saatcci2012scalable}
Saat{\c{c}}i, Y.
\newblock \emph{Scalable inference for structured {Gaussian} process models}.
\newblock PhD thesis, Citeseer, 2012.

\bibitem[Schwarz et~al.(1978)]{schwarz1978estimating}
Schwarz, G. et~al.
\newblock Estimating the dimension of a model.
\newblock \emph{The annals of statistics}, 6\penalty0 (2):\penalty0 461--464,
  1978.

\bibitem[Snoek et~al.(2012)Snoek, Larochelle, and Adams]{snoek2012practical}
Snoek, J., Larochelle, H., and Adams, R.~P.
\newblock Practical bayesian optimization of machine learning algorithms.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2951--2959, 2012.

\bibitem[Sutskever \& Hinton(2008)Sutskever and Hinton]{sutskever2008deep}
Sutskever, I. and Hinton, G.~E.
\newblock Deep, narrow sigmoid belief networks are universal approximators.
\newblock \emph{Neural computation}, 20\penalty0 (11):\penalty0 2629--2636,
  2008.

\bibitem[Titsias(2009)]{titsias2009variational}
Titsias, M.
\newblock Variational learning of inducing variables in sparse {Gaussian}
  processes.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  567--574,
  2009.

\bibitem[Tobar et~al.(2015)Tobar, Bui, and Turner]{tobar2015learning}
Tobar, F., Bui, T.~D., and Turner, R.~E.
\newblock Learning stationary time series using gaussian processes with
  nonparametric kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3501--3509, 2015.

\bibitem[Wang et~al.(2017)Wang, Li, Jegelka, and Kohli]{wang2017batched}
Wang, Z., Li, C., Jegelka, S., and Kohli, P.
\newblock Batched high-dimensional bayesian optimization via structural kernel
  learning.
\newblock \emph{arXiv preprint arXiv:1703.01973}, 2017.

\bibitem[Wiener(1932)]{wiener1932tauberian}
Wiener, N.
\newblock Tauberian theorems.
\newblock \emph{Annals of mathematics}, pp.\  1--100, 1932.

\bibitem[Wilson \& Adams(2013)Wilson and Adams]{wilson2013gaussian}
Wilson, A. and Adams, R.
\newblock Gaussian process kernels for pattern discovery and extrapolation.
\newblock In \emph{Proceedings of the 30th International Conference on Machine
  Learning (ICML-13)}, pp.\  1067--1075, 2013.

\bibitem[Wilson et~al.(2014)Wilson, Gilboa, Nehorai, and
  Cunningham]{wilson2014fast}
Wilson, A.~G., Gilboa, E., Nehorai, A., and Cunningham, J.~P.
\newblock Fast kernel learning for multidimensional pattern extrapolation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3626--3634, 2014.

\bibitem[Wilson et~al.(2016)Wilson, Hu, Salakhutdinov, and
  Xing]{wilson2016deep}
Wilson, A.~G., Hu, Z., Salakhutdinov, R., and Xing, E.~P.
\newblock Deep kernel learning.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  370--378,
  2016.

\bibitem[Yaglom(2012)]{yaglom2012correlation}
Yaglom, A.~M.
\newblock \emph{Correlation theory of stationary and related random functions:
  Supplementary notes and references}.
\newblock Springer Science \& Business Media, 2012.

\end{thebibliography}
