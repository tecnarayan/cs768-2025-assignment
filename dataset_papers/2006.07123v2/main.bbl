\begin{thebibliography}{10}

\bibitem{richards2019deep}
Blake~A Richards, Timothy~P Lillicrap, Philippe Beaudoin, Yoshua Bengio, Rafal
  Bogacz, Amelia Christensen, Claudia Clopath, Rui~Ponte Costa, Archy
  de~Berker, Surya Ganguli, et~al.
\newblock A deep learning framework for neuroscience.
\newblock {\em Nature neuroscience}, 22(11):1761--1770, 2019.

\bibitem{akrout2019deep}
Mohamed Akrout, Collin Wilson, Peter Humphreys, Timothy Lillicrap, and
  Douglas~B Tweed.
\newblock Deep learning without weight transport.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  974--982, 2019.

\bibitem{lillicrap2016random}
Timothy~P Lillicrap, Daniel Cownden, Douglas~B Tweed, and Colin~J Akerman.
\newblock Random synaptic feedback weights support error backpropagation for
  deep learning.
\newblock {\em Nature communications}, 7(1):1--10, 2016.

\bibitem{sacramento2018dendritic}
Jo{\~a}o Sacramento, Rui~Ponte Costa, Yoshua Bengio, and Walter Senn.
\newblock Dendritic cortical microcircuits approximate the backpropagation
  algorithm.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8721--8732, 2018.

\bibitem{pmlr-v97-belilovsky19a}
Eugene Belilovsky, Michael Eickenberg, and Edouard Oyallon.
\newblock Greedy layerwise learning can scale to {I}mage{N}et.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pages 583--593. PMLR, 09--15 Jun 2019.

\bibitem{nokland2019training}
Arild N{\o}kland and Lars~Hiller Eidnes.
\newblock Training neural networks with local error signals.
\newblock {\em arXiv preprint arXiv:1901.06656}, 2019.

\bibitem{lowe2019putting}
Sindy L{\"o}we, Peter O'Connor, and Bastiaan Veeling.
\newblock Putting an end to end-to-end: Gradient-isolated learning of
  representations.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3033--3045, 2019.

\bibitem{tishby2000information}
Naftali Tishby, Fernando~C Pereira, and William Bialek.
\newblock The information bottleneck method.
\newblock {\em arXiv preprint physics/0004057}, 2000.

\bibitem{shwartz2017opening}
Ravid Shwartz-Ziv and Naftali Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock {\em arXiv preprint arXiv:1703.00810}, 2017.

\bibitem{alemi2016deep}
Alexander~A Alemi, Ian Fischer, Joshua~V Dillon, and Kevin Murphy.
\newblock Deep variational information bottleneck.
\newblock {\em arXiv preprint arXiv:1612.00410}, 2016.

\bibitem{ma2019hsic}
Kurt~Wan{-}Duo Ma, J.~P. Lewis, and W.~Bastiaan Kleijn.
\newblock The {HSIC} bottleneck: Deep learning without back-propagation.
\newblock In {\em The Thirty-Fourth {AAAI} Conference on Artificial
  Intelligence, {AAAI} 2020, The Thirty-Second Innovative Applications of
  Artificial Intelligence Conference, {IAAI} 2020, The Tenth {AAAI} Symposium
  on Educational Advances in Artificial Intelligence, {EAAI} 2020, New York,
  NY, USA, February 7-12, 2020}, pages 5085--5092. {AAAI} Press, 2020.

\bibitem{gretton2005measuring}
Arthur Gretton, Olivier Bousquet, Alex Smola, and Bernhard Sch{\"o}lkopf.
\newblock Measuring statistical dependence with hilbert-schmidt norms.
\newblock In {\em International conference on algorithmic learning theory},
  pages 63--77. Springer, 2005.

\bibitem{gerstner2018eligibility}
Wulfram Gerstner, Marco Lehmann, Vasiliki Liakoni, Dane Corneil, and Johanni
  Brea.
\newblock Eligibility traces and plasticity on behavioral time scales:
  experimental support of neohebbian three-factor learning rules.
\newblock {\em Frontiers in neural circuits}, 12:53, 2018.

\bibitem{carandini2012normalization}
Matteo Carandini and David~J Heeger.
\newblock Normalization as a canonical neural computation.
\newblock {\em Nature Reviews Neuroscience}, 13(1):51, 2012.

\bibitem{olsen2010divisive}
Shawn~R Olsen, Vikas Bhandawat, and Rachel~I Wilson.
\newblock Divisive normalization in olfactory population codes.
\newblock {\em Neuron}, 66(2):287--299, 2010.

\bibitem{lecun2010mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock {\em ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  2, 2010.

\bibitem{xiao2017fmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms, 2017.

\bibitem{clanuwat2018kmnist}
Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki
  Yamamoto, and David Ha.
\newblock Deep learning for classical japanese literature, 2018.

\bibitem{krizhevsky2009cifar}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{nokland2016direct}
Arild N{\o}kland.
\newblock Direct feedback alignment provides learning in deep neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1037--1045, 2016.

\bibitem{moskovitz2018feedback}
Theodore~H Moskovitz, Ashok Litwin-Kumar, and LF~Abbott.
\newblock Feedback alignment in deep convolutional networks.
\newblock {\em arXiv preprint arXiv:1812.06488}, 2018.

\bibitem{bartunov2018assessing}
Sergey Bartunov, Adam Santoro, Blake Richards, Luke Marris, Geoffrey~E Hinton,
  and Timothy Lillicrap.
\newblock Assessing the scalability of biologically-motivated deep learning
  algorithms and architectures.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9368--9378, 2018.

\bibitem{liao2015important}
Qianli Liao, Joel~Z Leibo, and Tomaso Poggio.
\newblock How important is weight symmetry in backpropagation?
\newblock {\em arXiv preprint arXiv:1510.05067}, 2015.

\bibitem{xiao2018biologically}
Will Xiao, Honglin Chen, Qianli Liao, and Tomaso Poggio.
\newblock Biologically-plausible learning algorithms can scale to large
  datasets.
\newblock {\em arXiv preprint arXiv:1811.03567}, 2018.

\bibitem{bengio2014auto}
Yoshua Bengio.
\newblock How auto-encoders could provide credit assignment in deep networks
  via target propagation.
\newblock {\em arXiv preprint arXiv:1407.7906}, 2014.

\bibitem{lee2015difference}
Dong-Hyun Lee, Saizheng Zhang, Asja Fischer, and Yoshua Bengio.
\newblock Difference target propagation.
\newblock In {\em Joint european conference on machine learning and knowledge
  discovery in databases}, pages 498--515. Springer, 2015.

\bibitem{scellier2017equilibrium}
Benjamin Scellier and Yoshua Bengio.
\newblock Equilibrium propagation: Bridging the gap between energy-based models
  and backpropagation.
\newblock {\em Frontiers in computational neuroscience}, 11:24, 2017.

\bibitem{laborieux2020scaling}
Axel Laborieux, Maxence Ernoult, Benjamin Scellier, Yoshua Bengio, Julie
  Grollier, and Damien Querlioz.
\newblock Scaling equilibrium propagation to deep convnets by drastically
  reducing its gradient estimator bias.
\newblock {\em arXiv preprint arXiv:2006.03824}, 2020.

\bibitem{mostafa2018deep}
Hesham Mostafa, Vishwajith Ramesh, and Gert Cauwenberghs.
\newblock Deep supervised learning using local errors.
\newblock {\em Frontiers in neuroscience}, 12:608, 2018.

\bibitem{veness2017online}
Joel Veness, Tor Lattimore, Avishkar Bhoopchand, Agnieszka Grabska-Barwinska,
  Christopher Mattern, and Peter Toth.
\newblock Online learning with gated linear networks.
\newblock {\em arXiv preprint arXiv:1712.01897}, 2017.

\bibitem{veness2019gated}
Joel Veness, Tor Lattimore, David Budden, Avishkar Bhoopchand, Christopher
  Mattern, Agnieszka Grabska-Barwinska, Eren Sezener, Jianan Wang, Peter Toth,
  Simon Schmitt, et~al.
\newblock Gated linear networks.
\newblock {\em arXiv preprint arXiv:1910.01526}, 2019.

\bibitem{qin2020supervised}
Shanshan Qin, Nayantara Mudur, and Cengiz Pehlevan.
\newblock Supervised deep similarity matching.
\newblock {\em arXiv preprint arXiv:2002.10378}, 2020.

\bibitem{pehlevan2018similarity}
Cengiz Pehlevan, Anirvan~M Sengupta, and Dmitri~B Chklovskii.
\newblock Why do similarity matching objectives lead to hebbian/anti-hebbian
  networks?
\newblock {\em Neural computation}, 30(1):84--124, 2018.

\bibitem{krotov2019unsupervised}
Dmitry Krotov and John~J Hopfield.
\newblock Unsupervised learning by competing hidden units.
\newblock {\em Proceedings of the National Academy of Sciences},
  116(16):7723--7731, 2019.

\bibitem{grabska2017probabilistic}
Agnieszka Grabska-Barwi{\'n}ska, Simon Barthelm{\'e}, Jeff Beck, Zachary~F
  Mainen, Alexandre Pouget, and Peter~E Latham.
\newblock A probabilistic approach to demixing odors.
\newblock {\em Nature neuroscience}, 20(1):98, 2017.

\bibitem{ren2016normalizing}
Mengye Ren, Renjie Liao, Raquel Urtasun, Fabian~H Sinz, and Richard~S Zemel.
\newblock Normalizing the normalizers: Comparing and extending network
  normalization schemes.
\newblock {\em arXiv preprint arXiv:1611.04520}, 2016.

\bibitem{wu2018group}
Yuxin Wu and Kaiming He.
\newblock Group normalization.
\newblock In {\em Proceedings of the European conference on computer vision
  (ECCV)}, pages 3--19, 2018.

\bibitem{movellan1991contrastive}
Javier~R Movellan.
\newblock Contrastive hebbian learning in the continuous hopfield model.
\newblock In {\em Connectionist models}, pages 10--17. Elsevier, 1991.

\bibitem{xie2003equivalence}
Xiaohui Xie and H~Sebastian Seung.
\newblock Equivalence of backpropagation and contrastive hebbian learning in a
  layered network.
\newblock {\em Neural computation}, 15(2):441--454, 2003.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{maas2013rectifier}
Andrew~L Maas, Awni~Y Hannun, and Andrew~Y Ng.
\newblock Rectifier nonlinearities improve neural network acoustic models.
\newblock In {\em Proc. icml}, volume~30, page~3, 2013.

\bibitem{klambauer2017self}
G{\"u}nter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter.
\newblock Self-normalizing neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  971--980, 2017.

\bibitem{illing2019biologically}
Bernd Illing, Wulfram Gerstner, and Johanni Brea.
\newblock Biologically plausible deep learning—but how far can we go with
  shallow networks?
\newblock {\em Neural Networks}, 118:90--101, 2019.

\bibitem{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock {\em The journal of machine learning research}, 15(1):1929--1958,
  2014.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock {\em arXiv preprint arXiv:1807.03748}, 2018.

\bibitem{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock {\em arXiv preprint arXiv:2002.05709}, 2020.

\bibitem{todd2010perisynaptic}
Keith~J Todd, Houssam Darabid, and Richard Robitaille.
\newblock Perisynaptic glia discriminate patterns of motor nerve activity and
  influence plasticity at the neuromuscular junction.
\newblock {\em Journal of Neuroscience}, 30(35):11870--11882, 2010.

\bibitem{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem{duan2020modularizing}
Shiyu Duan, Shujian Yu, and Jose Principe.
\newblock Modularizing deep learning via pairwise learning with kernels.
\newblock {\em arXiv preprint arXiv:2005.05541}, 2020.

\end{thebibliography}
