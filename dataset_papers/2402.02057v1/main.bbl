\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[dee(2023)]{deepspeed2023tp}
Automatic tensor parallelism for huggingface models, 2023.
\newblock URL \url{https://www.deepspeed.ai/tutorials/automatic-tensor-parallelism}.

\bibitem[Aminabadi et~al.(2022)Aminabadi, Rajbhandari, Awan, Li, Li, Zheng, Ruwase, Smith, Zhang, Rasley, et~al.]{aminabadi2022deepspeed}
Aminabadi, R.~Y., Rajbhandari, S., Awan, A.~A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., et~al.
\newblock Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale.
\newblock In \emph{SC22: International Conference for High Performance Computing, Networking, Storage and Analysis}, pp.\  1--15. IEEE, 2022.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan, Jiang, Cai, Terry, Le, and Sutton]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M., Le, Q., and Sutton, C.
\newblock Program synthesis with large language models, 2021.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Ben~Allal et~al.(2022)Ben~Allal, Muennighoff, Kumar~Umapathi, Lipkin, and von Werra]{bigcode-evaluation-harness}
Ben~Allal, L., Muennighoff, N., Kumar~Umapathi, L., Lipkin, B., and von Werra, L.
\newblock A framework for the evaluation of code generation models.
\newblock \url{https://github.com/bigcode-project/bigcode-evaluation-harness}, 2022.

\bibitem[Cai et~al.(2024)Cai, Li, Geng, Peng, Lee, Chen, and Dao]{medusa}
Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J.~D., Chen, D., and Dao, T.
\newblock Medusa: Simple llm inference acceleration framework with multiple decoding heads, 2024.

\bibitem[Chen et~al.(2023)Chen, Borgeaud, Irving, Lespiau, Sifre, and Jumper]{chen2023accelerating}
Chen, C., Borgeaud, S., Irving, G., Lespiau, J.-B., Sifre, L., and Jumper, J.
\newblock Accelerating large language model decoding with speculative sampling.
\newblock \emph{arXiv preprint arXiv:2302.01318}, 2023.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, de~Oliveira~Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, Ray, Puri, Krueger, Petrov, Khlaaf, Sastry, Mishkin, Chan, Gray, Ryder, Pavlov, Power, Kaiser, Bavarian, Winter, Tillet, Such, Cummings, Plappert, Chantzis, Barnes, Herbert-Voss, Guss, Nichol, Paino, Tezak, Tang, Babuschkin, Balaji, Jain, Saunders, Hesse, Carr, Leike, Achiam, Misra, Morikawa, Radford, Knight, Brundage, Murati, Mayer, Welinder, McGrew, Amodei, McCandlish, Sutskever, and Zaremba]{chen2021evaluating}
Chen, M., Tworek, J., Jun, H., Yuan, Q., de~Oliveira~Pinto, H.~P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N., Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter, C., Tillet, P., Such, F.~P., Cummings, D., Plappert, M., Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W.~H., Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A.~N., Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford, A., Knight, M., Brundage, M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Amodei, D., McCandlish, S., Sutskever, I., and Zaremba, W.
\newblock Evaluating large language models trained on code, 2021.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, Hesse, and Schulman]{cobbe2021gsm8k}
Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J.
\newblock Training verifiers to solve math word problems.
\newblock \emph{arXiv preprint arXiv:2110.14168}, 2021.

\bibitem[Dao(2023)]{dao2023flashattention2}
Dao, T.
\newblock Flash{A}ttention-2: Faster attention with better parallelism and work partitioning.
\newblock 2023.

\bibitem[Dao et~al.(2022)Dao, Fu, Ermon, Rudra, and R{\'e}]{dao2022flashattention}
Dao, T., Fu, D.~Y., Ermon, S., Rudra, A., and R{\'e}, C.
\newblock Flash{A}ttention: Fast and memory-efficient exact attention with {IO}-awareness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Du et~al.(2023)Du, Liu, Wang, Wang, Liu, Chen, Feng, Sha, Peng, and Lou]{du2023classeval}
Du, X., Liu, M., Wang, K., Wang, H., Liu, J., Chen, Y., Feng, J., Sha, C., Peng, X., and Lou, Y.
\newblock Classeval: A manually-crafted benchmark for evaluating llms on class-level code generation, 2023.

\bibitem[Gugger et~al.(2022)Gugger, Debut, Wolf, Schmid, Mueller, Mangrulkar, Sun, and Bossan]{accelerate}
Gugger, S., Debut, L., Wolf, T., Schmid, P., Mueller, Z., Mangrulkar, S., Sun, M., and Bossan, B.
\newblock Accelerate: Training and inference at scale made simple, efficient and adaptable.
\newblock \url{https://github.com/huggingface/accelerate}, 2022.

\bibitem[He et~al.(2023)He, Zhong, Cai, Lee, and He]{he2023rest}
He, Z., Zhong, Z., Cai, T., Lee, J.~D., and He, D.
\newblock Rest: Retrieval-based speculative decoding.
\newblock \emph{arXiv preprint arXiv:2311.08252}, 2023.

\bibitem[Holtzman et~al.(2020)Holtzman, Buys, Du, Forbes, and Choi]{holtzman2020curious}
Holtzman, A., Buys, J., Du, L., Forbes, M., and Choi, Y.
\newblock The curious case of neural text degeneration, 2020.

\bibitem[Jiang et~al.(2023)Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.]{jiang2023mistral}
Jiang, A.~Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.~S., Casas, D. d.~l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., et~al.
\newblock Mistral 7b.
\newblock \emph{arXiv preprint arXiv:2310.06825}, 2023.

\bibitem[Kool et~al.(2020)Kool, van Hoof, and Welling]{JMLR:v21:19-985}
Kool, W., van Hoof, H., and Welling, M.
\newblock Ancestral gumbel-top-k sampling for sampling without replacement.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0 (47):\penalty0 1--36, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/19-985.html}.

\bibitem[Leviathan et~al.(2023)Leviathan, Kalman, and Matias]{leviathan2023fast}
Leviathan, Y., Kalman, M., and Matias, Y.
\newblock Fast inference from transformers via speculative decoding.
\newblock In \emph{International Conference on Machine Learning}, pp.\  19274--19286. PMLR, 2023.

\bibitem[Li et~al.(2023)Li, Zhang, and Zhang]{EAGLE}
Li, Y., Zhang, C., and Zhang, H.
\newblock Eagle: Lossless acceleration of llm decoding by feature extrapolation, December 2023.
\newblock URL \url{https://sites.google.com/view/eagle-llm}.

\bibitem[Lin(2004)]{lin-2004-rouge}
Lin, C.-Y.
\newblock {ROUGE}: A package for automatic evaluation of summaries.
\newblock In \emph{Text Summarization Branches Out}, pp.\  74--81, Barcelona, Spain, July 2004. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/W04-1013}.

\bibitem[Liu et~al.(2023)Liu, Hu, Bailis, Stoica, Deng, Cheung, and Zhang]{liu2023online}
Liu, X., Hu, L., Bailis, P., Stoica, I., Deng, Z., Cheung, A., and Zhang, H.
\newblock Online speculative decoding, 2023.

\bibitem[Miao et~al.(2023)Miao, Oliaro, Zhang, Cheng, Wang, Wong, Zhu, Yang, Shi, Shi, Chen, Arfeen, Abhyankar, and Jia]{miao2023specinfer}
Miao, X., Oliaro, G., Zhang, Z., Cheng, X., Wang, Z., Wong, R. Y.~Y., Zhu, A., Yang, L., Shi, X., Shi, C., Chen, Z., Arfeen, D., Abhyankar, R., and Jia, Z.
\newblock Specinfer: Accelerating generative large language model serving with speculative inference and token tree verification, 2023.

\bibitem[Narayan et~al.(2018)Narayan, Cohen, and Lapata]{xsum-emnlp}
Narayan, S., Cohen, S.~B., and Lapata, M.
\newblock Don't give me the details, just the summary! {T}opic-aware convolutional neural networks for extreme summarization.
\newblock In \emph{Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing}, Brussels, Belgium, 2018.

\bibitem[Narayanan et~al.(2021)Narayanan, Shoeybi, Casper, LeGresley, Patwary, Korthikanti, Vainbrand, Kashinkunti, Bernauer, Catanzaro, Phanishayee, and Zaharia]{narayanan2021efficient}
Narayanan, D., Shoeybi, M., Casper, J., LeGresley, P., Patwary, M., Korthikanti, V.~A., Vainbrand, D., Kashinkunti, P., Bernauer, J., Catanzaro, B., Phanishayee, A., and Zaharia, M.
\newblock Efficient large-scale language model training on gpu clusters using megatron-lm, 2021.

\bibitem[OpenAI(2023)]{openai2023gpt4}
OpenAI.
\newblock Gpt-4 technical report, 2023.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin, Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 27730--27744, 2022.

\bibitem[Roziere et~al.(2023)Roziere, Gehring, Gloeckle, Sootla, Gat, Tan, Adi, Liu, Remez, Rapin, et~al.]{roziere2023code}
Roziere, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X.~E., Adi, Y., Liu, J., Remez, T., Rapin, J., et~al.
\newblock Code llama: Open foundation models for code.
\newblock \emph{arXiv preprint arXiv:2308.12950}, 2023.

\bibitem[Ruan et~al.(2023)Ruan, Sabir, and Chopra]{aws-prompt}
Ruan, J.~T., Sabir, F., and Chopra, P.
\newblock Best prompting practices for using the llama 2 chat llm through amazon sagemaker jumpstart, November 2023.
\newblock URL \url{https://aws.amazon.com/cn/blogs/machine-learning/best-prompting-practices-for-using-the-llama-2-chat-llm-through-amazon-sagemaker-jumpstart/}.

\bibitem[Santilli et~al.(2023)Santilli, Severino, Postolache, Maiorca, Mancusi, Marin, and Rodola]{santilli-etal-2023-accelerating}
Santilli, A., Severino, S., Postolache, E., Maiorca, V., Mancusi, M., Marin, R., and Rodola, E.
\newblock Accelerating transformer inference for translation via parallel decoding.
\newblock In \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  12336--12355, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2023.acl-long.689}.

\bibitem[Saxena(2023)]{saxena2023prompt}
Saxena, A.
\newblock Prompt lookup decoding, November 2023.
\newblock URL \url{https://github.com/apoorvumang/prompt-lookup-decoding/}.

\bibitem[See et~al.(2017)See, Liu, and Manning]{see2017get}
See, A., Liu, P.~J., and Manning, C.~D.
\newblock Get to the point: Summarization with pointer-generator networks.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1073--1083, 2017.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and Catanzaro]{shoeybi2019megatron}
Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B.
\newblock Megatron-lm: Training multi-billion parameter language models using model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Song et~al.(2021)Song, Meng, Liao, and Ermon]{song2021accelerating}
Song, Y., Meng, C., Liao, R., and Ermon, S.
\newblock Accelerating feedforward computation via parallel nonlinear equation solving, 2021.

\bibitem[Stern et~al.(2018)Stern, Shazeer, and Uszkoreit]{stern2018blockwise}
Stern, M., Shazeer, N., and Uszkoreit, J.
\newblock Blockwise parallel decoding for deep autoregressive models, 2018.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Vaswani et~al.(2023)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2023attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, L., and Polosukhin, I.
\newblock Attention is all you need, 2023.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac, Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu, Scao, Gugger, Drame, Lhoest, and Rush]{wolf-etal-2020-transformers}
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Scao, T.~L., Gugger, S., Drame, M., Lhoest, Q., and Rush, A.~M.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pp.\  38--45, Online, October 2020. Association for Computational Linguistics.
\newblock URL \url{https://www.aclweb.org/anthology/2020.emnlp-demos.6}.

\bibitem[Yang et~al.(2023)Yang, Ge, Wang, Jiao, Jiang, Yang, Majumder, and Wei]{yang2023inference}
Yang, N., Ge, T., Wang, L., Jiao, B., Jiang, D., Yang, L., Majumder, R., and Wei, F.
\newblock Inference with reference: Lossless acceleration of large language models, 2023.

\bibitem[Zhang \& Sennrich(2019)Zhang and Sennrich]{zhang2019root}
Zhang, B. and Sennrich, R.
\newblock Root mean square layer normalization, 2019.

\bibitem[Zheng et~al.(2023)Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing, Zhang, Gonzalez, and Stoica]{zheng2023judging}
Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E.~P., Zhang, H., Gonzalez, J.~E., and Stoica, I.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena, 2023.

\end{thebibliography}
