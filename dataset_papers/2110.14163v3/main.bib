@article{achille2019information,
  title={Where is the information in a deep neural network?},
  author={Achille, Alessandro and Paolini, Giovanni and Soatto, Stefano},
  journal={arXiv preprint arXiv:1905.12213},
  year={2019}
}

@article{schwarz1978estimating,
  title={Estimating the dimension of a model},
  author={Schwarz, Gideon},
  journal={The annals of statistics},
  pages={461--464},
  year={1978},
  publisher={JSTOR}
}

@article{wilson2020case,
  title={The case for Bayesian deep learning},
  author={Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:2001.10995},
  year={2020}
}

@Article{	  6790500,
  title		= {Natural Gradient Works Efficiently in Learning},
  author	= {Amari, Shun-ichi},
  year		= {1998},
  journal	= {Neural Computation},
  volume	= {10},
  number	= {2},
  pages		= {251--276},
  doi		= {10.1162/089976698300017746}
}

@InProceedings{	  achille2018critical,
  title		= {Critical Learning Periods in Deep Networks},
  booktitle	= {International Conference on Learning Representations},
  author	= {Achille, Alessandro and Rovere, Matteo and Soatto,
		  Stefano},
  year		= {2018}
}

@Article{	  amari2002geometrical,
  title		= {Geometrical Singularities in the Neuromanifold of
		  Multilayer Perceptrons},
  author	= {Amari, Shun-ichi and Park, Hyeyoung and Ozeki, Tomoko},
  year		= {2002},
  journal	= {Advances in neural information processing systems},
  volume	= {1},
  pages		= {343--350},
  publisher	= {{MIT; 1998}}
}

@Article{	  ambroladze2007tighter,
  title		= {Tighter Pac-Bayes Bounds},
  author	= {Ambroladze, Amiran and {Parrado-Hernández}, Emilio and
		  {Shawe-Taylor}, John},
  year		= {2007},
  journal	= {Advances in neural information processing systems},
  volume	= {19},
  pages		= {9},
  publisher	= {{MIT; 1998}}
}

@InProceedings{	  bartlett2017spectrally,
  title		= {Spectrally-Normalized Margin Bounds for Neural Networks},
  booktitle	= {Advances in Neural Information Processing Systems},
  author	= {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus
		  J},
  year		= {2017},
  pages		= {6240--6249}
}

@Article{	  bartlett2020benign,
  title		= {Benign Overfitting in Linear Regression},
  author	= {Bartlett, Peter L and Long, Philip M and Lugosi, Gábor
		  and Tsigler, Alexander},
  year		= {2020},
  journal	= {Proceedings of the National Academy of Sciences},
  volume	= {117},
  number	= {48},
  pages		= {30063--30070},
  publisher	= {{National Acad Sciences}}
}

@Article{	  bartlett2021deep,
  title		= {Deep Learning: A Statistical Viewpoint},
  author	= {Bartlett, Peter L and Montanari, Andrea and Rakhlin,
		  Alexander},
  year		= {2021},
  journal	= {arXiv preprint arXiv:2103.09177},
  eprint	= {2103.09177},
  eprinttype	= {arxiv},
  archiveprefix	= {arXiv},
  file		= {/Users/pratik/Dropbox/zotero/bartlett_et_al_2021_deep_learning.pdf}
}

@Article{	  brown2004statistical,
  title		= {The Statistical Mechanics of Complex Signaling Networks:
		  Nerve Growth Factor Signaling},
  author	= {Brown, Kevin S and Hill, Colin C and Calero, Guillermo A
		  and Myers, Christopher R and Lee, Kelvin H and Sethna,
		  James P and Cerione, Richard A},
  year		= {2004},
  journal	= {Physical biology},
  volume	= {1},
  number	= {3},
  pages		= {184},
  publisher	= {{IOP Publishing}},
  file		= {/Users/pratik/Dropbox/zotero/brown_et_al_2004_the_statistical_mechanics_of_complex_signaling_networks.pdf}
}

@InProceedings{	  chaudhari2016entropy,
  title		= {Entropy-{{SGD}}: {{Biasing}} Gradient Descent into Wide
		  Valleys},
  booktitle	= {Proc. of the {{International Conference}} on {{Learning
		  Representations}} ({{ICLR}})},
  author	= {Chaudhari, Pratik and Choromanska, Anna and Soatto,
		  Stefano and LeCun, Yann and Baldassi, Carlo and Borgs,
		  Christian and Chayes, Jennifer and Sagun, Levent and
		  Zecchina, Riccardo},
  year		= {2017},
  copyright	= {All rights reserved}
}

@Article{	  chaudhari2017stochastic,
  title		= {Stochastic Gradient Descent Performs Variational
		  Inference, Converges to Limit Cycles for Deep Networks},
  author	= {Chaudhari, Pratik and Soatto, Stefano},
  year		= {2018},
  journal	= {Proc. of International Conference of Learning and
		  Representations (ICLR), Apr 30-May 3, 2018},
  copyright	= {All rights reserved}
}

@Article{	  chizat2019lazy,
  title		= {On Lazy Training in Differentiable Programming},
  author	= {Chizat, Lénaïc and Oyallon, Edouard and Bach, Francis},
  year		= {2019},
  journal	= {Advances in Neural Information Processing Systems},
  volume	= {32},
  pages		= {2937--2947}
}

@InProceedings{	  dangel2020backpack,
  title		= {{{BackPACK}}: {{Packing}} More into Backprop},
  booktitle	= {International Conference on Learning Representations},
  author	= {Dangel, Felix and Kunstner, Frederik and Hennig, Philipp},
  year		= {2020}
}

@InProceedings{	  dziugaite2018data,
  title		= {Data-Dependent {{PAC}}-{{Bayes}} Priors via Differential
		  Privacy},
  booktitle	= {Proceedings of the 32nd International Conference on Neural
		  Information Processing Systems},
  author	= {Dziugaite, Gintare Karolina and Roy, Daniel M},
  year		= {2018},
  pages		= {8440--8450}
}

@PhDThesis{	  dziugaite2020revisiting,
  title		= {Revisiting Generalization for Deep Learning:
		  {{PAC}}-{{Bayes}}, Flat Minima, and Generative Models},
  author	= {Dziugaite, Gintare Karolina},
  year		= {2020},
  school	= {University of Cambridge}
}

@InProceedings{	  dziugaitecomputingnonvacuousgeneralization2017,
  ids		= {dziugaite2017computing},
  title		= {Computing {{Nonvacuous Generalization Bounds}} for
		  {{Deep}} ({{Stochastic}}) {{Neural Networks}} with {{Many
		  More Parameters}} than {{Training Data}}},
  booktitle	= {Proc. of the {{Conference}} on {{Uncertainty}} in
		  {{Artificial Intelligence}} ({{UAI}})},
  author	= {Dziugaite, Gintare Karolina and Roy, Daniel M.},
  year		= {2017},
  eprint	= {1703.11008},
  eprinttype	= {arxiv},
  abstract	= {One of the defining properties of deep learning is that
		  models are chosen to have many more parameters than
		  available training data. In light of this capacity for
		  overfitting, it is remarkable that simple algorithms like
		  SGD reliably return solutions with low test error. One
		  roadblock to explaining these phenomena in terms of
		  implicit regularization, structural properties of the
		  solution, and/or easiness of the data is that many learning
		  bounds are quantitatively vacuous when applied to networks
		  learned by SGD in this “deep learning” regime.
		  Logically, in order to explain generalization, we need
		  nonvacuous bounds. We return to an idea by Langford and
		  Caruana (2001), who used PAC-Bayes bounds to compute
		  nonvacuous numerical bounds on generalization error for
		  stochastic two-layer two-hidden-unit neural networks via a
		  sensitivity analysis. By optimizing the PAC-Bayes bound
		  directly, we are able to extend their approach and obtain
		  nonvacuous generalization bounds for deep stochastic neural
		  network classifiers with millions of parameters trained on
		  only tens of thousands of examples. We connect our findings
		  to recent and old work on flat minima and MDL-based
		  explanations of generalization.},
  archiveprefix	= {arXiv},
  date-added	= {2020-02-05 19:04:35 +0530},
  date-modified	= {2020-02-05 19:04:35 +0530},
  language	= {en},
  keywords	= {Computer Science - Machine Learning}
}

@Article{	  fortemergentpropertieslocal2019,
  title		= {Emergent Properties of the Local Geometry of Neural Loss
		  Landscapes},
  author	= {Fort, Stanislav and Ganguli, Surya},
  year		= {2019},
  month		= oct,
  journal	= {arXiv:1910.05929 [cs, stat]},
  eprint	= {1910.05929},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  abstract	= {The local geometry of high dimensional neural network loss
		  landscapes can both challenge our cherished theoretical
		  intuitions as well as dramatically impact the practical
		  success of neural network training. Indeed recent works
		  have observed 4 striking local properties of neural loss
		  landscapes on classification tasks: (1) the landscape
		  exhibits exactly \$C\$ directions of high positive
		  curvature, where \$C\$ is the number of classes; (2)
		  gradient directions are largely confined to this extremely
		  low dimensional subspace of positive Hessian curvature,
		  leaving the vast majority of directions in weight space
		  unexplored; (3) gradient descent transiently explores
		  intermediate regions of higher positive curvature before
		  eventually finding flatter minima; (4) training can be
		  successful even when confined to low dimensional
		  \{\textbackslash it random\} affine hyperplanes, as long as
		  these hyperplanes intersect a Goldilocks zone of higher
		  than average curvature. We develop a simple theoretical
		  model of gradients and Hessians, justified by numerical
		  experiments on architectures and datasets used in practice,
		  that \{\textbackslash it simultaneously\} accounts for all
		  \$4\$ of these surprising and seemingly unrelated
		  properties. Our unified model provides conceptual insights
		  into the emergence of these properties and makes
		  connections with diverse topics in neural networks, random
		  matrix theory, and spin glasses, including the neural
		  tangent kernel, BBP phase transitions, and Derrida's random
		  energy model.},
  archiveprefix	= {arXiv},
  keywords	= {Computer Science - Machine Learning,Computer Science -
		  Neural and Evolutionary Computing,Statistics - Machine
		  Learning}
}

@Article{	  franklelotterytickethypothesis2019,
  title		= {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}},
		  {{Trainable Neural Networks}}},
  shorttitle	= {The {{Lottery Ticket Hypothesis}}},
  author	= {Frankle, Jonathan and Carbin, Michael},
  year		= {2019},
  month		= mar,
  journal	= {arXiv:1803.03635 [cs]},
  eprint	= {1803.03635},
  eprinttype	= {arxiv},
  primaryclass	= {cs},
  abstract	= {Neural network compression techniques are able to reduce
		  the parameter counts of trained networks by over
		  90\%—decreasing storage requirements and improving
		  inference performance—without compromising accuracy.
		  However, contemporary experience is that it is difficult to
		  train small architectures from scratch, which would
		  similarly improve training performance.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Computer Science - Artificial Intelligence,Computer
		  Science - Machine Learning,Computer Science - Neural and
		  Evolutionary Computing}
}

@Article{	  gur-arigradientdescenthappens2018,
  title		= {Gradient {{Descent Happens}} in a {{Tiny Subspace}}},
  author	= {{Gur-Ari}, Guy and Roberts, Daniel A. and Dyer, Ethan},
  year		= {2018},
  month		= dec,
  journal	= {arXiv:1812.04754 [cs, stat]},
  eprint	= {1812.04754},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  archiveprefix	= {arXiv},
  keywords	= {Computer Science - Artificial Intelligence,Computer
		  Science - Machine Learning,Statistics - Machine Learning}
}

@Article{	  gutenkunst2007universally,
  title		= {Universally Sloppy Parameter Sensitivities in Systems
		  Biology Models},
  author	= {Gutenkunst, Ryan N and Waterfall, Joshua J and Casey,
		  Fergal P and Brown, Kevin S and Myers, Christopher R and
		  Sethna, James P},
  year		= {2007},
  journal	= {PLoS computational biology},
  volume	= {3},
  number	= {10},
  pages		= {e189},
  publisher	= {{Public Library of Science}}
}

@Article{	  hochreiterflatminima1997,
  title		= {Flat Minima},
  author	= {Hochreiter, Sepp and Schmidhuber, Jürgen},
  year		= {1997},
  journal	= {Neural Computation},
  volume	= {9},
  number	= {1},
  pages		= {1--42},
  publisher	= {{MIT Press}}
}

@Article{	  karakidauniversalstatisticsfisher2019,
  title		= {Universal {{Statistics}} of {{Fisher Information}} in
		  {{Deep Neural Networks}}: {{Mean Field Approach}}},
  shorttitle	= {Universal {{Statistics}} of {{Fisher Information}} in
		  {{Deep Neural Networks}}},
  author	= {Karakida, Ryo and Akaho, Shotaro and Amari, Shun-ichi},
  year		= {2019},
  month		= oct,
  journal	= {arXiv:1806.01316 [cond-mat, stat]},
  eprint	= {1806.01316},
  eprinttype	= {arxiv},
  primaryclass	= {cond-mat, stat},
  abstract	= {This study analyzes the Fisher information matrix (FIM) by
		  applying mean-field theory to deep neural networks with
		  random weights. We theoretically find novel statistics of
		  the FIM, which are universal among a wide class of deep
		  networks with any number of layers and various activation
		  functions. Although most of the FIM’s eigenvalues are
		  close to zero, the maximum eigenvalue takes on a huge value
		  and the eigenvalue distribution has an extremely long tail.
		  These statistics suggest that the shape of a loss landscape
		  is locally flat in most dimensions, but strongly distorted
		  in the other dimensions. Moreover, our theory of the FIM
		  leads to quantitative evaluation of learning in deep
		  networks. First, the maximum eigenvalue enables us to
		  estimate an appropriate size of a learning rate for
		  steepest gradient methods to converge. Second, the flatness
		  induced by the small eigenvalues is connected to
		  generalization ability through a norm-based capacity
		  measure.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Computer Science - Machine Learning,Condensed Matter -
		  Disordered Systems and Neural Networks,Statistics - Machine
		  Learning}
}

@PhDThesis{	  krizhevsky2009learning,
  ids		= {krizhevskyLearningMultipleLayers2009},
  title		= {Learning Multiple Layers of Features from Tiny Images},
  author	= {Krizhevsky, A.},
  year		= {2009},
  school	= {Computer Science, University of Toronto}
}

@InProceedings{	  kunstner2019limitations,
  title		= {Limitations of the Empirical {{Fisher}} Approximation for
		  Natural Gradient Descent},
  booktitle	= {Advances in Neural Information Processing Systems},
  author	= {Kunstner, Frederik and Hennig, Philipp and Balles, Lukas},
  year		= {2019},
  pages		= {4156--4167}
}





@Article{	  langford2001bounds,
  title		= {Bounds for Averaging Classifiers},
  author	= {Langford, John and Seeger, Matthias},
  year		= {2001},
  publisher	= {{Citeseer}}
}

@Article{	  langford2002not,
  title		= {({{Not}}) Bounding the True Error},
  author	= {Langford, John and Caruana, Rich},
  year		= {2002},
  journal	= {Advances in Neural Information Processing Systems},
  volume	= {2},
  pages		= {809--816},
  publisher	= {{MIT; 1998}}
}

@InProceedings{	  lecun1990handwritten,
  title		= {Handwritten Digit Recognition with a Back-Propagation
		  Network},
  booktitle	= {Advances in Neural Information Processing Systems},
  author	= {LeCun, Yann and Boser, Bernhard E and Denker, John S and
		  Henderson, Donnie and Howard, Richard E and Hubbard, Wayne
		  E and Jackel, Lawrence D},
  year		= {1990},
  pages		= {396--404},
  date-added	= {2020-01-27 13:27:58 +0530},
  date-modified	= {2020-01-27 13:27:58 +0530}
}

@Article{	  martensoptimizingneuralnetworks2016,
  title		= {Optimizing {{Neural Networks}} with {{Kronecker}}-Factored
		  {{Approximate Curvature}}},
  author	= {Martens, James and Grosse, Roger},
  year		= {2016},
  month		= may,
  journal	= {arXiv:1503.05671 [cs, stat]},
  eprint	= {1503.05671},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  abstract	= {We propose an efficient method for approximating natural
		  gradient descent in neural networks which we call
		  Kronecker-factored Approximate Curvature (K-FAC). K-FAC is
		  based on an efficiently invertible approximation of a
		  neural network’s Fisher information matrix which is
		  neither diagonal nor low-rank, and in some cases is
		  completely non-sparse. It is derived by approximating
		  various large blocks of the Fisher (corresponding to entire
		  layers) as being the Kronecker product of two much smaller
		  matrices. While only several times more expensive to
		  compute than the plain stochastic gradient, the updates
		  produced by K-FAC make much more progress optimizing the
		  objective, which results in an algorithm that can be much
		  faster than stochastic gradient descent with momentum in
		  practice. And unlike some previously proposed approximate
		  natural-gradient/Newton methods which use high-quality
		  non-diagonal curvature matrices (such as Hessian-free
		  optimization), K-FAC works very well in highly stochastic
		  optimization regimes. This is because the cost of storing
		  and inverting K-FAC’s approximation to the curvature
		  matrix does not depend on the amount of data used to
		  estimate it, which is a feature typically associated only
		  with diagonal or low-rank approximations to the curvature
		  matrix.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Computer Science - Machine Learning,Computer Science -
		  Neural and Evolutionary Computing,Statistics - Machine
		  Learning}
}

@InProceedings{	  mcallester1999pac,
  title		= {{{PAC}}-{{Bayesian}} Model Averaging},
  booktitle	= {Proceedings of the Twelfth Annual Conference on
		  {{Computational}} Learning Theory},
  author	= {McAllester, David A},
  year		= {1999},
  pages		= {164--170}
}

@Article{	  neyshaburexploringgeneralizationdeep2017,
  title		= {Exploring {{Generalization}} in {{Deep Learning}}},
  author	= {Neyshabur, Behnam and Bhojanapalli, Srinadh and
		  McAllester, David and Srebro, Nathan},
  year		= {2017},
  month		= jul,
  journal	= {arXiv:1706.08947 [cs]},
  eprint	= {1706.08947},
  eprinttype	= {arxiv},
  primaryclass	= {cs},
  abstract	= {With a goal of understanding what drives generalization in
		  deep networks, we consider several recently suggested
		  explanations, including norm-based control, sharpness and
		  robustness. We study how these measures can ensure
		  generalization, highlighting the importance of scale
		  normalization, and making a connection between sharpness
		  and PAC-Bayes theory. We then investigate how well the
		  measures explain different observed phenomena.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Computer Science - Machine Learning}
}

@Article{	  papyan2018full,
  title		= {The Full Spectrum of Deepnet Hessians at Scale:
		  {{Dynamics}} with Sgd Training and Sample Size},
  author	= {Papyan, Vardan},
  year		= {2018},
  journal	= {arXiv preprint arXiv:1811.07062},
  eprint	= {1811.07062},
  eprinttype	= {arxiv},
  archiveprefix	= {arXiv}
}

@Article{	  papyan2019measurements,
  title		= {Measurements of Three-Level Hierarchical Structure in the
		  Outliers in the Spectrum of Deepnet Hessians},
  author	= {Papyan, Vardan},
  year		= {2019},
  journal	= {arXiv preprint arXiv:1901.08244},
  eprint	= {1901.08244},
  eprinttype	= {arxiv},
  archiveprefix	= {arXiv}
}

@Article{	  parrado2012pac,
  title		= {{{PAC}}-{{Bayes}} Bounds with Data Dependent Priors},
  author	= {{Parrado-Hernández}, Emilio and Ambroladze, Amiran and
		  {Shawe-Taylor}, John and Sun, Shiliang},
  year		= {2012},
  journal	= {The Journal of Machine Learning Research},
  volume	= {13},
  number	= {1},
  pages		= {3507--3531},
  publisher	= {{JMLR. org}}
}

@Article{	  pellegrini2021sifting,
  title		= {Sifting out the Features by Pruning: {{Are}} Convolutional
		  Networks the Winning Lottery Ticket of Fully Connected
		  Ones?},
  author	= {Pellegrini, Franco and Biroli, Giulio},
  year		= {2021},
  journal	= {arXiv preprint arXiv:2104.13343},
  eprint	= {2104.13343},
  eprinttype	= {arxiv},
  archiveprefix	= {arXiv}
}

@Article{	  penningtongeometryneuralnetwork,
  title		= {Geometry of {{Neural Network Loss Surfaces}} via {{Random
		  Matrix Theory}}},
  author	= {Pennington, Jeffrey and Bahri, Yasaman},
  pages		= {9},
  language	= {en}
}

@InProceedings{	  pmlr-v70-dinh17b,
  title		= {Sharp Minima Can Generalize for Deep Nets},
  booktitle	= {Proceedings of the 34th International Conference on
		  Machine Learning},
  author	= {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and
		  Bengio, Yoshua},
  editor	= {Precup, Doina and Teh, Yee Whye},
  year		= {2017},
  month		= aug,
  series	= {Proceedings of Machine Learning Research},
  volume	= {70},
  pages		= {1019--1028},
  publisher	= {{PMLR}},
  abstract	= {Despite their overwhelming capacity to overfit, deep
		  learning architectures tend to generalize relatively well
		  to unseen data, allowing them to be deployed in practice.
		  However, explaining why this is the case is still an open
		  area of research. One standing hypothesis that is gaining
		  popularity, e.g. Hochreiter \&amp; Schmidhuber (1997);
		  Keskar et al. (2017), is that the flatness of minima of the
		  loss function found by stochastic gradient based methods
		  results in good generalization. This paper argues that most
		  notions of flatness are problematic for deep models and can
		  not be directly applied to explain generalization.
		  Specifically, when focusing on deep networks with rectifier
		  units, we can exploit the particular geometry of parameter
		  space induced by the inherent symmetries that these
		  architectures exhibit to build equivalent models
		  corresponding to arbitrarily sharper minima. Or, depending
		  on the definition of flatness, it is the same for any given
		  minimum. Furthermore, if we allow to reparametrize a
		  function, the geometry of its parameters can change
		  drastically without affecting its generalization
		  properties.},
  pdf		= {http://proceedings.mlr.press/v70/dinh17b/dinh17b.pdf}
}

@Article{	  sagun2016singularity,
  title		= {Singularity of the Hessian in Deep Learning},
  author	= {Sagun, Levent and Bottou, Leon and LeCun, Yann},
  year		= {2016},
  journal	= {arXiv:1611:07476},
  date-added	= {2018-06-20 23:02:24 +0000},
  date-modified	= {2018-06-20 23:02:24 +0000}
}

@Article{	  springenbergstrivingsimplicityall2015,
  ids		= {springenberg2014striving},
  title		= {Striving for {{Simplicity}}: {{The All Convolutional
		  Net}}},
  shorttitle	= {Striving for {{Simplicity}}},
  author	= {Springenberg, Jost Tobias and Dosovitskiy, Alexey and
		  Brox, Thomas and Riedmiller, Martin},
  year		= {2015},
  month		= apr,
  journal	= {arXiv:1412.6806 [cs]},
  eprint	= {1412.6806},
  eprinttype	= {arxiv},
  primaryclass	= {cs},
  abstract	= {Most modern convolutional neural networks (CNNs) used for
		  object recognition are built using the same principles:
		  Alternating convolution and max-pooling layers followed by
		  a small number of fully connected layers. We re-evaluate
		  the state of the art for object recognition from small
		  images with convolutional networks, questioning the
		  necessity of different components in the pipeline. We find
		  that max-pooling can simply be replaced by a convolutional
		  layer with increased stride without loss in accuracy on
		  several image recognition benchmarks. Following this
		  finding – and building on other recent work for finding
		  simple network structures – we propose a new architecture
		  that consists solely of convolutional layers and yields
		  competitive or state of the art performance on several
		  object recognition datasets (CIFAR-10, CIFAR-100,
		  ImageNet). To analyze the network we introduce a new
		  variant of the “deconvolution approach” for visualizing
		  features learned by CNNs, which can be applied to a broader
		  range of network structures than existing approaches.},
  archiveprefix	= {arXiv},
  date-added	= {2018-06-20 23:02:24 +0000},
  date-modified	= {2018-06-20 23:02:24 +0000},
  language	= {en},
  keywords	= {Computer Science - Computer Vision and Pattern
		  Recognition,Computer Science - Machine Learning,Computer
		  Science - Neural and Evolutionary Computing}
}

@Article{	  sunlightlikeneuromanifoldsoccam2020a,
  title		= {Lightlike {{Neuromanifolds}}, {{Occam}}'s {{Razor}} and
		  {{Deep Learning}}},
  author	= {Sun, Ke and Nielsen, Frank},
  year		= {2020},
  month		= feb,
  journal	= {arXiv:1905.11027 [cs, stat]},
  eprint	= {1905.11027},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  abstract	= {How do deep neural networks benefit from a very high
		  dimensional parameter space? Their high complexity vs
		  stunning generalization performance forms an intriguing
		  paradox. We took an information-theoretic approach. We find
		  that the locally varying dimensionality of the parameter
		  space can be studied by the discipline of singular
		  semi-Riemannian geometry. We adapt Fisher information to
		  this singular neuromanifold. We use a new prior to
		  interpolate between Jeffreys' prior and the Gaussian prior.
		  We derive a minimum description length of a deep learning
		  model, where the spectrum of the Fisher information matrix
		  plays a key role to reduce the model complexity.},
  archiveprefix	= {arXiv},
  keywords	= {Computer Science - Machine Learning,Statistics - Machine
		  Learning}
}

@Article{	  transtrumgeometrynonlinearleast2011,
  ids		= {transtrumGeometryNonlinearLeast2011a},
  title		= {The Geometry of Nonlinear Least Squares with Applications
		  to Sloppy Models and Optimization},
  author	= {Transtrum, Mark K. and Machta, Benjamin B. and Sethna,
		  James P.},
  year		= {2011},
  month		= mar,
  journal	= {Physical Review E},
  volume	= {83},
  number	= {3},
  eprint	= {1010.1449},
  eprinttype	= {arxiv},
  pages		= {036701},
  issn		= {1539-3755, 1550-2376},
  doi		= {10.1103/PhysRevE.83.036701},
  abstract	= {Parameter estimation by nonlinear least squares
		  minimization is a common problem with an elegant geometric
		  interpretation: the possible parameter values of a model
		  induce a manifold in the space of data predictions. The
		  minimization problem is then to find the point on the
		  manifold closest to the data. We show that the model
		  manifolds of a large class of models, known as sloppy
		  models, have many universal features; they are
		  characterized by a geometric series of widths, extrinsic
		  curvatures, and parameter-effects curvatures. A number of
		  common difficulties in optimizing least squares problems
		  are due to this common structure. First, algorithms tend to
		  run into the boundaries of the model manifold, causing
		  parameters to diverge or become unphysical. We introduce
		  the model graph as an extension of the model manifold to
		  remedy this problem. We argue that appropriate priors can
		  remove the boundaries and improve convergence rates. We
		  show that typical fits will have many evaporated
		  parameters. Second, bare model parameters are usually
		  ill-suited to describing model behavior; cost contours in
		  parameter space tend to form hierarchies of plateaus and
		  canyons. Geometrically, we understand this inconvenient
		  parametrization as an extremely skewed coordinate basis and
		  show that it induces a large parameter-effects curvature on
		  the manifold. Using coordinates based on geodesic motion,
		  these narrow canyons are transformed in many cases into a
		  single quadratic, isotropic basin. We interpret the
		  modified Gauss-Newton and Levenberg-Marquardt fitting
		  algorithms as an Euler approximation to geodesic motion in
		  these natural coordinates on the model manifold and the
		  model graph respectively. By adding a geodesic acceleration
		  adjustment to these algorithms, we alleviate the
		  difficulties from parameter-effects curvature, improving
		  both efficiency and success rates at finding good fits.},
  archiveprefix	= {arXiv},
  language	= {en},
  keywords	= {Condensed Matter - Statistical Mechanics,Physics -
		  Computational Physics,Physics - Data Analysis; Statistics
		  and Probability},
  file		= {/Users/pratik/Dropbox/zotero/transtrum_et_al_2011_the_geometry_of_nonlinear_least_squares_with_applications_to_sloppy_models_and2.pdf}
}

@Article{	  waterfallsloppymodeluniversalityclass2006,
  title		= {Sloppy-{{Model Universality Class}} and the {{Vandermonde
		  Matrix}}},
  author	= {Waterfall, Joshua J. and Casey, Fergal P. and Gutenkunst,
		  Ryan N. and Brown, Kevin S. and Myers, Christopher R. and
		  Brouwer, Piet W. and Elser, Veit and Sethna, James P.},
  year		= {2006},
  month		= oct,
  journal	= {Physical Review Letters},
  volume	= {97},
  number	= {15},
  pages		= {150601},
  publisher	= {{American Physical Society}},
  doi		= {10.1103/PhysRevLett.97.150601},
  abstract	= {In a variety of contexts, physicists study complex,
		  nonlinear models with many unknown or tunable parameters to
		  explain experimental data. We explain why such systems so
		  often are sloppy: the system behavior depends only on a few
		  “stiff” combinations of the parameters and is unchanged
		  as other “sloppy” parameter combinations vary by orders
		  of magnitude. We observe that the eigenvalue spectra for
		  the sensitivity of sloppy models have a striking,
		  characteristic form with a density of logarithms of
		  eigenvalues which is roughly constant over a large range.
		  We suggest that the common features of sloppy models
		  indicate that they may belong to a common universality
		  class. In particular, we motivate focusing on a Vandermonde
		  ensemble of multiparameter nonlinear models and show in one
		  limit that they exhibit the universal features of sloppy
		  models.}
}

@Article{	  wudissectinghessianunderstanding2021,
  title		= {Dissecting {{Hessian}}: {{Understanding Common Structure}}
		  of {{Hessian}} in {{Neural Networks}}},
  shorttitle	= {Dissecting {{Hessian}}},
  author	= {Wu, Yikai and Zhu, Xingyu and Wu, Chenwei and Wang, Annie
		  and Ge, Rong},
  year		= {2021},
  month		= jun,
  journal	= {arXiv:2010.04261 [cs, stat]},
  eprint	= {2010.04261},
  eprinttype	= {arxiv},
  primaryclass	= {cs, stat},
  abstract	= {Hessian captures important properties of the deep neural
		  network loss landscape. Previous works have observed low
		  rank structure in the Hessians of neural networks. We make
		  several new observations about the top eigenspace of
		  layer-wise Hessian: top eigenspaces for different models
		  have surprisingly high overlap, and top eigenvectors form
		  low rank matrices when they are reshaped into the same
		  shape as the corresponding weight matrix. Towards formally
		  explaining such structures of the Hessian, we show that the
		  new eigenspace structure can be explained by approximating
		  the Hessian using Kronecker factorization; we also prove
		  the low rank structure for random data at random
		  initialization for over-parametrized two-layer neural nets.
		  Our new understanding can explain why some of these
		  structures become weaker when the network is trained with
		  batch normalization. The Kronecker factorization also leads
		  to better explicit generalization bounds.},
  archiveprefix	= {arXiv},
  keywords	= {Computer Science - Machine Learning,Computer Science -
		  Neural and Evolutionary Computing,I.2.6,Statistics -
		  Machine Learning}
}

@InProceedings{	  yin2018gradient,
  title		= {Gradient Diversity: A Key Ingredient for Scalable
		  Distributed Learning},
  booktitle	= {International Conference on Artificial Intelligence and
		  Statistics},
  author	= {Yin, Dong and Pananjady, Ashwin and Lam, Max and
		  Papailiopoulos, Dimitris and Ramchandran, Kannan and
		  Bartlett, Peter},
  year		= {2018},
  pages		= {1998--2007},
  organization	= {{PMLR}}
}

@InProceedings{	  zagoruyko2016wide,
  title		= {Wide Residual Networks},
  booktitle	= {British Machine Vision Conference 2016},
  author	= {Zagoruyko, Sergey and Komodakis, Nikos},
  year		= {2016},
  organization	= {{British Machine Vision Association}}
}

@InProceedings{	  zhou2018non,
  title		= {Non-Vacuous Generalization Bounds at the {{ImageNet}}
		  Scale: A {{PAC}}-{{Bayesian}} Compression Approach},
  booktitle	= {International Conference on Learning Representations},
  author	= {Zhou, Wenda and Veitch, Victor and Austern, Morgane and
		  Adams, Ryan P and Orbanz, Peter},
  year		= {2018}
}





@inproceedings{10.1145/307400.307435,
author = {McAllester, David A.},
title = {PAC-Bayesian Model Averaging},
year = {1999},
isbn = {1581131674},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/307400.307435},
doi = {10.1145/307400.307435},
booktitle = {Proceedings of the Twelfth Annual Conference on Computational Learning Theory},
pages = {164–170},
numpages = {7},
location = {Santa Cruz, California, USA},
series = {COLT '99}
}


@inproceedings{ICML-2017-BotevRB,
	author        = "Aleksandar Botev and Hippolyt Ritter and David Barber",
	booktitle     = "{Proceedings of the 34th International Conference on Machine Learning}",
	ee            = "http://proceedings.mlr.press/v70/botev17a.html",
	pages         = "557--565",
	publisher     = "{PMLR}",
	title         = "{Practical Gauss-Newton Optimisation for Deep Learning}",
	year          = 2017,
}


@article{JustInterpolate,
  author    = {Tengyuan Liang and
               Alexander Rakhlin},
  title     = {Just Interpolate: Kernel "Ridgeless" Regression Can Generalize},
  journal   = {CoRR},
  volume    = {abs/1808.00387},
  year      = {2018},
  url       = {http://arxiv.org/abs/1808.00387},
  eprinttype = {arXiv},
  eprint    = {1808.00387},
  timestamp = {Sun, 02 Sep 2018 15:01:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1808-00387.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{Karoui2010TheSO,
  title={The spectrum of kernel random matrices},
  author={Noureddine El Karoui},
  journal={Annals of Statistics},
  year={2010},
  volume={38},
  pages={1-50}
}

@inproceedings{Maddox2019ASB,
  title={A Simple Baseline for Bayesian Uncertainty in Deep Learning},
  author={Wesley Maddox and T. Garipov and Pavel Izmailov and Dmitry P. Vetrov and Andrew Gordon Wilson},
  booktitle={NeurIPS},
  year={2019}
}
