\begin{thebibliography}{10}\itemsep=-1pt

\bibitem{arora2019fine}
S.~Arora, S.~Du, W.~Hu, Z.~Li, and R.~Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In {\em ICML}, pages 322--332. PMLR, 2019.

\bibitem{Cao_nips}
K.~Cao, C.~Wei, A.~Gaidon, N.~Arechiga, and T.~Ma.
\newblock Learning imbalanced datasets with label-distribution-aware margin
  loss.
\newblock In {\em NeurIPS}, volume~32, 2019.

\bibitem{coates2011analysis}
A.~Coates, A.~Ng, and H.~Lee.
\newblock An analysis of single-layer networks in unsupervised feature
  learning.
\newblock In {\em AISTATS}, pages 215--223, 2011.

\bibitem{ergen2021revealing}
T.~Ergen and M.~Pilanci.
\newblock Revealing the structure of deep neural networks via convex duality.
\newblock In {\em ICML}, pages 3004--3014. PMLR, 2021.

\bibitem{fang2021exploring}
C.~Fang, H.~He, Q.~Long, and W.~J. Su.
\newblock Exploring deep neural networks via layer-peeled model: Minority
  collapse in imbalanced training.
\newblock {\em Proceedings of the National Academy of Sciences}, 118(43), 2021.

\bibitem{fisher1936use}
R.~A. Fisher.
\newblock The use of multiple measurements in taxonomic problems.
\newblock {\em Annals of eugenics}, 7(2):179--188, 1936.

\bibitem{ge2018learning}
R.~Ge, J.~D. Lee, and T.~Ma.
\newblock Learning one-hidden-layer neural networks with landscape design.
\newblock In {\em ICLR}, 2018.

\bibitem{graf2021dissecting}
F.~Graf, C.~Hofer, M.~Niethammer, and R.~Kwitt.
\newblock Dissecting supervised constrastive learning.
\newblock In {\em ICML}, pages 3821--3830. PMLR, 2021.

\bibitem{han2021neural}
X.~Han, V.~Papyan, and D.~L. Donoho.
\newblock Neural collapse under mse loss: Proximity to and dynamics on the
  central path.
\newblock In {\em ICLR}, 2022.

\bibitem{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em CVPR}, pages 770--778, 2016.

\bibitem{hershey2017cnn}
S.~Hershey, S.~Chaudhuri, D.~P. Ellis, J.~F. Gemmeke, A.~Jansen, R.~C. Moore,
  M.~Plakal, D.~Platt, R.~A. Saurous, B.~Seybold, et~al.
\newblock Cnn architectures for large-scale audio classification.
\newblock In {\em ICASSP}, pages 131--135. IEEE, 2017.

\bibitem{hoffer2018fix}
E.~Hoffer, I.~Hubara, and D.~Soudry.
\newblock Fix your classifier: the marginal value of training the last weight
  layer.
\newblock In {\em ICLR}, 2018.

\bibitem{huang2017densely}
G.~Huang, Z.~Liu, L.~Van Der~Maaten, and K.~Q. Weinberger.
\newblock Densely connected convolutional networks.
\newblock In {\em CVPR}, pages 4700--4708, 2017.

\bibitem{NTK}
A.~Jacot, C.~Hongler, and F.~Gabriel.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em NeurIPS}, pages 8580--8589, 2018.

\bibitem{ji2021unconstrained}
W.~Ji, Y.~Lu, Y.~Zhang, Z.~Deng, and W.~J. Su.
\newblock An unconstrained layer-peeled perspective on neural collapse.
\newblock In {\em ICLR}, 2022.

\bibitem{kang2019decoupling}
B.~Kang, S.~Xie, M.~Rohrbach, Z.~Yan, A.~Gordo, J.~Feng, and Y.~Kalantidis.
\newblock Decoupling representation and classifier for long-tailed recognition.
\newblock In {\em ICLR}, 2020.

\bibitem{krizhevsky2009learning}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{le2014distributed}
Q.~Le and T.~Mikolov.
\newblock Distributed representations of sentences and documents.
\newblock In {\em ICML}, pages 1188--1196. PMLR, 2014.

\bibitem{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock {\em Nature}, 521(7553):436--444, 2015.

\bibitem{li2018optimization}
H.~Li, Y.~Yang, D.~Chen, and Z.~Lin.
\newblock Optimization algorithm inspired deep neural network structure design.
\newblock In {\em ACML}, 2018.

\bibitem{liu2019large}
Z.~Liu, Z.~Miao, X.~Zhan, J.~Wang, B.~Gong, and S.~X. Yu.
\newblock Large-scale long-tailed recognition in an open world.
\newblock In {\em CVPR}, pages 2537--2546, 2019.

\bibitem{lu2020neural}
J.~Lu and S.~Steinerberger.
\newblock Neural collapse with cross-entropy loss.
\newblock {\em arXiv preprint arXiv:2012.08465}, 2020.

\bibitem{mei2018mean}
S.~Mei, A.~Montanari, and P.-M. Nguyen.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671, 2018.

\bibitem{mixon2020neural}
D.~G. Mixon, H.~Parshall, and J.~Pi.
\newblock Neural collapse with unconstrained features.
\newblock {\em arXiv preprint arXiv:2011.11619}, 2020.

\bibitem{netzer2011reading}
Y.~Netzer, T.~Wang, A.~Coates, A.~Bissacco, B.~Wu, and A.~Y. Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock 2011.

\bibitem{oyallon2017building}
E.~Oyallon.
\newblock Building a regular decision boundary with deep networks.
\newblock In {\em CVPR}, pages 5106--5114, 2017.

\bibitem{papyan2020traces}
V.~Papyan.
\newblock Traces of class/cross-class structure pervade deep learning spectra.
\newblock {\em Journal of Machine Learning Research}, 21(252):1--64, 2020.

\bibitem{papyan2020prevalence}
V.~Papyan, X.~Han, and D.~L. Donoho.
\newblock Prevalence of neural collapse during the terminal phase of deep
  learning training.
\newblock {\em Proceedings of the National Academy of Sciences},
  117(40):24652--24663, 2020.

\bibitem{pernici2019maximally}
F.~Pernici, M.~Bruni, C.~Baecchi, and A.~Del~Bimbo.
\newblock Maximally compact and separated features with regular polytope
  networks.
\newblock In {\em CVPR Workshops}, pages 46--53, 2019.

\bibitem{pernici2021regular}
F.~Pernici, M.~Bruni, C.~Baecchi, and A.~Del~Bimbo.
\newblock Regular polytope networks.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  2021.

\bibitem{poggio2020explicit}
T.~Poggio and Q.~Liao.
\newblock Explicit regularization and implicit bias in deep network classifiers
  trained with the square loss.
\newblock {\em arXiv preprint arXiv:2101.00072}, 2020.

\bibitem{rangamani2022neural}
A.~Rangamani and A.~Banburski-Fahey.
\newblock Neural collapse in deep homogeneous classifiers and the role of
  weight decay.
\newblock In {\em ICASSP}, pages 4243--4247. IEEE, 2022.

\bibitem{rao1948utilization}
C.~R. Rao.
\newblock The utilization of multiple measurements in problems of biological
  classification.
\newblock {\em Journal of the Royal Statistical Society. Series B
  (Methodological)}, 10(2):159--203, 1948.

\bibitem{russakovsky2015imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, et~al.
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em IJCV}, 115(3):211--252, 2015.

\bibitem{vgg}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em ICLR}, 2015.

\bibitem{smith1980bayes}
A.~F. Smith and D.~J. Spiegelhalter.
\newblock Bayes factors and choice criteria for linear models.
\newblock {\em Journal of the Royal Statistical Society: Series B
  (Methodological)}, 42(2):213--220, 1980.

\bibitem{stuhlsatz2012feature}
A.~Stuhlsatz, J.~Lippel, and T.~Zielke.
\newblock Feature extraction with deep neural networks by a generalized
  discriminant analysis.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  23(4):596--608, 2012.

\bibitem{tirer2022extended}
T.~Tirer and J.~Bruna.
\newblock Extended unconstrained features model for exploring deep neural
  collapse.
\newblock In {\em ICML}, 2022.

\bibitem{wah2011caltech}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock 2011.

\bibitem{webb1990optimised}
A.~R. Webb and D.~Lowe.
\newblock The optimised internal representation of multilayer classifier
  networks performs nonlinear discriminant analysis.
\newblock {\em Neural Networks}, 3(4):367--375, 1990.

\bibitem{weinan2020emergence}
E.~Weinan and S.~Wojtowytsch.
\newblock On the emergence of tetrahedral symmetry in the final and penultimate
  layers of neural network classifiers.
\newblock {\em arXiv preprint arXiv:2012.05420}, 2020.

\bibitem{xiao2020disentangling}
L.~Xiao, J.~Pennington, and S.~Schoenholz.
\newblock Disentangling trainability and generalization in deep neural
  networks.
\newblock In {\em ICML}, pages 10462--10472. PMLR, 2020.

\bibitem{xie2022neural}
L.~Xie, Y.~Yang, D.~Cai, and X.~He.
\newblock Neural collapse inspired attraction-repulsion-balanced loss for
  imbalanced learning.
\newblock {\em arXiv preprint arXiv:2204.08735}, 2022.

\bibitem{yang2019dynamical}
Y.~Yang, J.~Wu, H.~Li, X.~Li, T.~Shen, and Z.~Lin.
\newblock Dynamical system inspired adaptive time stepping controller for
  residual network families.
\newblock In {\em AAAI}, 2020.

\bibitem{yang2018convolutional}
Y.~Yang, Z.~Zhong, T.~Shen, and Z.~Lin.
\newblock Convolutional neural networks with alternately updated clique.
\newblock In {\em CVPR}, pages 2413--2422, 2018.

\bibitem{zarka2020separation}
J.~Zarka, F.~Guth, and S.~Mallat.
\newblock Separation and concentration in deep networks.
\newblock In {\em ICLR}, 2020.

\bibitem{zhang2018mixup}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock In {\em ICLR}, 2018.

\bibitem{zhong2021improving}
Z.~Zhong, J.~Cui, S.~Liu, and J.~Jia.
\newblock Improving calibration for long-tailed recognition.
\newblock In {\em CVPR}, pages 16489--16498, 2021.

\bibitem{zhou2022optimization}
J.~Zhou, X.~Li, T.~Ding, C.~You, Q.~Qu, and Z.~Zhu.
\newblock On the optimization landscape of neural collapse under mse loss:
  Global optimality with unconstrained features.
\newblock {\em arXiv preprint arXiv:2203.01238}, 2022.

\bibitem{zhu2021geometric}
Z.~Zhu, T.~DING, J.~Zhou, X.~Li, C.~You, J.~Sulam, and Q.~Qu.
\newblock A geometric analysis of neural collapse with unconstrained features.
\newblock In {\em NeurIPS}, 2021.

\end{thebibliography}
