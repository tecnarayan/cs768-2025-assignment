\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anschel et~al.(2017)Anschel, Baram, and Shimkin]{AnschelBaramShimkin}
Anschel, O., Baram, N., and Shimkin, N.
\newblock Averaged-{DQN:} variance reduction and stabilization for deep
  reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  176--185, 2017.

\bibitem[Asadi \& Littman(2017)Asadi and Littman]{AsadiLittman}
Asadi, K. and Littman, M.~L.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  243--252, 2017.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{BellemareNaddafVenessBowling}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{Journal of Artificial Intelligence Research}, 47:\penalty0
  253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and
  Munos]{BellemareDabneyMunos}
Bellemare, M.~G., Dabney, W., and Munos, R.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  449--458, 2017.

\bibitem[Bellman(1957)]{bellman57}
Bellman, R.
\newblock \emph{Dynamic Programming}.
\newblock Princeton University Press, Princeton, NJ, 1957.

\bibitem[Dabney et~al.(2018)Dabney, Rowland, Bellemare, and
  Munos]{dabney2018distributional}
Dabney, W., Rowland, M., Bellemare, M.~G., and Munos, R.
\newblock Distributional reinforcement learning with quantile regression.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2016taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Proceedings of the Thirty-Second Conference on Uncertainty
  in Artificial Intelligence}, pp.\  202--211. AUAI Press, 2016.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1352--1361, 2017.

\bibitem[He et~al.(2017)He, Liu, Schwing, and Peng]{HeLiuSchwingPeng}
He, F.~S., Liu, Y., Schwing, A.~G., and Peng, J.
\newblock Learning to play in a day: Faster deep reinforcement learning by
  optimality tightening.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{rainbow}
Hessel, M., Modayil, J., Van~Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
  W., Horgan, D., Piot, B., Azar, M., and Silver, D.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Lee et~al.(2018)Lee, Choi, and Oh]{LeeChoiOh}
Lee, K., Choi, S., and Oh, S.
\newblock Sparse {M}arkov decision processes with causal sparse {T}sallis
  entropy regularization for reinforcement learning.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (3):\penalty0
  1466--1473, July 2018.

\bibitem[Littman(1996)]{littman}
Littman, M.~L.
\newblock \emph{Algorithms for Sequential Decision Making}.
\newblock PhD thesis, Department of Computer Science, Brown University,
  February 1996.

\bibitem[Mitra et~al.(1986)Mitra, Romeo, and
  Sangiovanni-Vincentelli]{MitraRomeoSangiovanni}
Mitra, D., Romeo, F., and Sangiovanni-Vincentelli, A.
\newblock Convergence and finite-time behavior of simulated annealing.
\newblock \emph{Advances in applied probability}, 18\penalty0 (3):\penalty0
  747--771, 1986.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih+al:2015}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare,
  M.~G., Graves, A., Riedmiller, M., Fidjeland, A.~K., Ostrovski, G., Petersen,
  S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra,
  D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 02 2015.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{MnihA3C}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1928--1937, 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{NachumNorouziXuSchuurmans}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2772--2782, 2017.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock A unified view of entropy-regularized {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[O'Donoghue et~al.(2017)O'Donoghue, Munos, Kavukcuoglu, and
  Mnih]{DonoghueEtAl}
O'Donoghue, B., Munos, R., Kavukcuoglu, K., and Mnih, V.
\newblock Combining policy gradient and {Q}-learning.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Reverdy \& Leonard(2016)Reverdy and Leonard]{ReverdyLeonard}
Reverdy, P. and Leonard, N.~E.
\newblock Parameter estimation in softmax decision-making models with linear
  objective functions.
\newblock \emph{IEEE Transactions on Automation Science and Engineering},
  13\penalty0 (1):\penalty0 54--67, 2016.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and
  Silver]{SchaulQuanAntonoglouSilver}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations}, 2016.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and Abbeel]{SchulmanAbbeelChen}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft {Q}-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton98}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: {A}n Introduction}.
\newblock The MIT Press, 1998.

\bibitem[Thrun \& Schwartz(1994)Thrun and Schwartz]{Thrun93b}
Thrun, S. and Schwartz, A.
\newblock Issues in using function approximation for reinforcement learning.
\newblock In Mozer, M.~C., Smolensky, P., Touretzky, D.~S., Elman, J.~L., and
  Weigend, A.~S. (eds.), \emph{Proceedings of the 1993 Connectionist Models
  Summer School}, Hillsdale, NJ, 1994. Lawrence Erlbaum.

\bibitem[Thrun(1992)]{Thrun92}
Thrun, S.~B.
\newblock The role of exploration in learning control.
\newblock In White, D.~A. and Sofge, D.~A. (eds.), \emph{Handbook of
  Intelligent Control: {N}eural, Fuzzy, and Adaptive Approaches}, pp.\
  527--559. Van Nostrand Reinhold, New York, NY, 1992.

\bibitem[Tieleman \& Hinton(2012)Tieleman and Hinton]{TielemanHinton}
Tieleman, T. and Hinton, G.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock \emph{COURSERA: Neural networks for machine learning}, 4\penalty0
  (2):\penalty0 26--31, 2012.

\bibitem[Todorov(2007)]{todorov2007linearly}
Todorov, E.
\newblock Linearly-solvable {M}arkov decision problems.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1369--1376, 2007.

\bibitem[van Hasselt(2010)]{Hasselt}
van Hasselt, H.
\newblock Double {Q}-learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2613--2621, 2010.

\bibitem[van Hasselt et~al.(2016{\natexlab{a}})van Hasselt, Guez, and
  Silver]{HasseltGuezSilver}
van Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double {Q}-learning.
\newblock In \emph{Proceedings of the Thirtieth AAAI Conference on Artificial
  Intelligence}, pp.\  2094--2100. AAAI Press, 2016{\natexlab{a}}.

\bibitem[van Hasselt et~al.(2016{\natexlab{b}})van Hasselt, Guez, Hessel, Mnih,
  and Silver]{HasseltGuezHesselMnihSilver}
van Hasselt, H.~P., Guez, A., Hessel, M., Mnih, V., and Silver, D.
\newblock Learning values across many orders of magnitude.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4287--4295, 2016{\natexlab{b}}.

\bibitem[Wang et~al.(2016)Wang, Schaul, Hessel, van Hasselt, Lanctot, and
  De~Freitas]{WangEtAl2016}
Wang, Z., Schaul, T., Hessel, M., van Hasselt, H., Lanctot, M., and De~Freitas,
  N.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pp.\  1995--2003. JMLR. org, 2016.

\bibitem[Williams \& Baird~III(1993)Williams and Baird~III]{WilliamsBaird}
Williams, R.~J. and Baird~III, L.~C.
\newblock Tight performance bounds on greedy policies based on imperfect value
  functions.
\newblock Technical report, Northeastern University, 1993.

\end{thebibliography}
