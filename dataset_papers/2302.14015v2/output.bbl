\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Abbasi-Yadkori, Y., P{\'a}l, D., and Szepesv{\'a}ri, C.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Abe et~al.(2003)Abe, Biermann, and Long]{abe2003reinforcement}
Abe, N., Biermann, A.~W., and Long, P.~M.
\newblock Reinforcement learning with immediate rewards and linear hypotheses.
\newblock \emph{Algorithmica}, 37\penalty0 (4):\penalty0 263--293, 2003.

\bibitem[Agrawal \& Goyal(2013)Agrawal and Goyal]{agrawal2013thompson}
Agrawal, S. and Goyal, N.
\newblock Thompson sampling for contextual bandits with linear payoffs.
\newblock In \emph{International conference on machine learning}, pp.\
  127--135. PMLR, 2013.

\bibitem[Annadani et~al.(2021)Annadani, Rothfuss, Lacoste, Scherrer, Goyal,
  Bengio, and Bauer]{annadani2021variational}
Annadani, Y., Rothfuss, J., Lacoste, A., Scherrer, N., Goyal, A., Bengio, Y.,
  and Bauer, S.
\newblock Variational causal networks: Approximate bayesian inference over
  causal structures.
\newblock \emph{arXiv preprint arXiv:2106.07635}, 2021.

\bibitem[Auer(2002)]{auer2002using}
Auer, P.
\newblock Using confidence bounds for exploitation-exploration trade-offs.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 397--422, 2002.

\bibitem[Baydin et~al.(2018)Baydin, Pearlmutter, Radul, and
  Siskind]{baydin2018ad}
Baydin, A.~G., Pearlmutter, B.~A., Radul, A.~A., and Siskind, J.~M.
\newblock Automatic differentiation in machine learning: a survey.
\newblock \emph{Journal of machine learning research}, 18, 2018.

\bibitem[Bingham et~al.(2018)Bingham, Chen, Jankowiak, Obermeyer, Pradhan,
  Karaletsos, Singh, Szerlip, Horsfall, and Goodman]{pyro}
Bingham, E., Chen, J.~P., Jankowiak, M., Obermeyer, F., Pradhan, N.,
  Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and Goodman, N.~D.
\newblock Pyro: Deep universal probabilistic programming.
\newblock \emph{Journal of Machine Learning Research}, 2018.

\bibitem[Chaloner \& Verdinelli(1995)Chaloner and Verdinelli]{chaloner1995}
Chaloner, K. and Verdinelli, I.
\newblock {Bayesian experimental design: A review}.
\newblock \emph{Statistical Science}, pp.\  273--304, 1995.

\bibitem[Char et~al.(2019)Char, Chung, Neiswanger, Kandasamy, Nelson, Boyer,
  Kolemen, and Schneider]{char2019offline}
Char, I., Chung, Y., Neiswanger, W., Kandasamy, K., Nelson, A.~O., Boyer, M.,
  Kolemen, E., and Schneider, J.
\newblock Offline contextual bayesian optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Chevalier \& Ginsbourger(2013)Chevalier and
  Ginsbourger]{chevalier2013fast}
Chevalier, C. and Ginsbourger, D.
\newblock Fast computation of the multi-points expected improvement with
  applications in batch selection.
\newblock In \emph{International Conference on Learning and Intelligent
  Optimization}, pp.\  59--69. Springer, 2013.

\bibitem[Chu et~al.(2011)Chu, Li, Reyzin, and Schapire]{chu2011contextual}
Chu, W., Li, L., Reyzin, L., and Schapire, R.
\newblock Contextual bandits with linear payoff functions.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pp.\  208--214. JMLR Workshop and
  Conference Proceedings, 2011.

\bibitem[Chung et~al.(2020)Chung, Char, Neiswanger, Kandasamy, Nelson, Boyer,
  Kolemen, and Schneider]{chung2020offline}
Chung, Y., Char, I., Neiswanger, W., Kandasamy, K., Nelson, A.~O., Boyer,
  M.~D., Kolemen, E., and Schneider, J.
\newblock Offline contextual bayesian optimization for nuclear fusion.
\newblock \emph{arXiv preprint arXiv:2001.01793}, 2020.

\bibitem[Dani et~al.(2008)Dani, Hayes, and Kakade]{dani2008stochastic}
Dani, V., Hayes, T.~P., and Kakade, S.~M.
\newblock Stochastic linear optimization under bandit feedback.
\newblock 2008.

\bibitem[Deisenroth et~al.(2014)Deisenroth, Englert, Peters, and
  Fox]{deisenroth2014multi}
Deisenroth, M.~P., Englert, P., Peters, J., and Fox, D.
\newblock Multi-task policy search for robotics.
\newblock In \emph{2014 IEEE international conference on robotics and
  automation (ICRA)}, pp.\  3876--3881. IEEE, 2014.

\bibitem[Foster et~al.(2019)Foster, Jankowiak, Bingham, Horsfall, Teh,
  Rainforth, and Goodman]{foster2019variational}
Foster, A., Jankowiak, M., Bingham, E., Horsfall, P., Teh, Y.~W., Rainforth,
  T., and Goodman, N.
\newblock {Variational Bayesian Optimal Experimental Design}.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  14036--14047. Curran Associates, Inc., 2019.

\bibitem[Foster et~al.(2020)Foster, Jankowiak, O’Meara, Teh, and
  Rainforth]{foster2020unified}
Foster, A., Jankowiak, M., O’Meara, M., Teh, Y.~W., and Rainforth, T.
\newblock A unified stochastic gradient approach to designing bayesian-optimal
  experiments.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  2959--2969. PMLR, 2020.

\bibitem[Foster et~al.(2021)Foster, Ivanova, Malik, and
  Rainforth]{foster2021dad}
Foster, A., Ivanova, D.~R., Malik, I., and Rainforth, T.
\newblock Deep adaptive design: Amortizing sequential bayesian experimental
  design.
\newblock \emph{Proceedings of the 38th International Conference on Machine
  Learning (ICML), PMLR 139}, 2021.

\bibitem[Foster(2021)]{foster2021variational}
Foster, A.~E.
\newblock \emph{Variational, {M}onte {C}arlo and Policy-Based Approaches to
  {B}ayesian Experimental Design}.
\newblock PhD thesis, University of Oxford, 2021.

\bibitem[Friedman et~al.(2008)Friedman, Hastie, and
  Tibshirani]{friedman2008sparse}
Friedman, J., Hastie, T., and Tibshirani, R.
\newblock Sparse inverse covariance estimation with the graphical lasso.
\newblock \emph{Biostatistics}, 9\penalty0 (3):\penalty0 432--441, 2008.

\bibitem[Geffner et~al.(2022)Geffner, Antoran, Foster, Gong, Ma, Kiciman,
  Sharma, Lamb, Kukla, Pawlowski, et~al.]{geffner2022deep}
Geffner, T., Antoran, J., Foster, A., Gong, W., Ma, C., Kiciman, E., Sharma,
  A., Lamb, A., Kukla, M., Pawlowski, N., et~al.
\newblock Deep end-to-end causal inference.
\newblock \emph{arXiv preprint arXiv:2202.02195}, 2022.

\bibitem[Ginsbourger et~al.(2014)Ginsbourger, Baccou, Chevalier, Perales,
  Garland, and Monerie]{ginsbourger2014bayesian}
Ginsbourger, D., Baccou, J., Chevalier, C., Perales, F., Garland, N., and
  Monerie, Y.
\newblock Bayesian adaptive reconstruction of profile optima and optimizers.
\newblock \emph{SIAM/ASA Journal on Uncertainty Quantification}, 2\penalty0
  (1):\penalty0 490--510, 2014.

\bibitem[Gonz{\'a}lez et~al.(2016)Gonz{\'a}lez, Dai, Hennig, and
  Lawrence]{gonzalez2016batch}
Gonz{\'a}lez, J., Dai, Z., Hennig, P., and Lawrence, N.
\newblock Batch bayesian optimization via local penalization.
\newblock In \emph{Artificial intelligence and statistics}, pp.\  648--657.
  PMLR, 2016.

\bibitem[Greenland et~al.(1999)Greenland, Pearl, and
  Robins]{greenland1999confounding}
Greenland, S., Pearl, J., and Robins, J.~M.
\newblock Confounding and collapsibility in causal inference.
\newblock \emph{Statistical science}, 14\penalty0 (1):\penalty0 29--46, 1999.

\bibitem[Groves et~al.(2018)Groves, Pearce, and
  Branke]{groves2018parallelizing}
Groves, M., Pearce, M., and Branke, J.
\newblock On parallelizing multi-task {B}ayesian optimization.
\newblock In \emph{2018 Winter Simulation Conference (WSC)}, pp.\  1993--2002.
  IEEE, 2018.

\bibitem[Han et~al.(2020)Han, Zhou, Zhou, Blanchet, Glynn, and
  Ye]{han2020sequential}
Han, Y., Zhou, Z., Zhou, Z., Blanchet, J., Glynn, P.~W., and Ye, Y.
\newblock Sequential batch learning in finite-action linear contextual bandits.
\newblock \emph{arXiv preprint arXiv:2004.06321}, 2020.

\bibitem[Hennig \& Schuler(2012)Hennig and Schuler]{hennig2012entropy}
Hennig, P. and Schuler, C.~J.
\newblock Entropy search for information-efficient global optimization.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0
  (Jun):\penalty0 1809--1837, 2012.

\bibitem[Hern{\'a}ndez-Lobato et~al.(2014)Hern{\'a}ndez-Lobato, Hoffman, and
  Ghahramani]{hernandez2014predictive}
Hern{\'a}ndez-Lobato, J.~M., Hoffman, M.~W., and Ghahramani, Z.
\newblock Predictive entropy search for efficient global optimization of
  black-box functions.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  918--926, 2014.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and
  Lengyel]{houlsby2011bayesian}
Houlsby, N., Husz{\'a}r, F., Ghahramani, Z., and Lengyel, M.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{arXiv preprint arXiv:1112.5745}, 2011.

\bibitem[Ivanova et~al.(2021)Ivanova, Foster, Kleinegesse, Gutmann, and
  Rainforth]{ivanova2021implicit}
Ivanova, D.~R., Foster, A., Kleinegesse, S., Gutmann, M., and Rainforth, T.
\newblock {Implicit Deep Adaptive Design: Policy–Based Experimental Design
  without Likelihoods}.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  25785--25798. Curran Associates, Inc., 2021.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2021/file/d811406316b669ad3d370d78b51b1d2e-Paper.pdf}.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
Jang, E., Gu, S., and Poole, B.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: {A} method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kirsch et~al.(2019)Kirsch, Van~Amersfoort, and
  Gal]{kirsch2019batchbald}
Kirsch, A., Van~Amersfoort, J., and Gal, Y.
\newblock Batchbald: {E}fficient and diverse batch acquisition for deep
  {B}ayesian active learning.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Kleinegesse \& Gutmann(2020)Kleinegesse and
  Gutmann]{kleinegesse2020minebed}
Kleinegesse, S. and Gutmann, M.
\newblock {B}ayesian experimental design for implicit models by mutual
  information neural estimation.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, Proceedings of Machine Learning Research, pp.\  5316--5326. PMLR,
  2020.

\bibitem[Kleinegesse \& Gutmann(2021)Kleinegesse and
  Gutmann]{kleinegesse2021gradientbased}
Kleinegesse, S. and Gutmann, M.~U.
\newblock Gradient-based bayesian experimental design for implicit models using
  mutual information lower bounds.
\newblock \emph{arXiv preprint arXiv:2105.04379}, 2021.

\bibitem[Krause \& Ong(2011)Krause and Ong]{krause2011contextual}
Krause, A. and Ong, C.
\newblock Contextual gaussian process bandit optimization.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Kupcsik et~al.(2017)Kupcsik, Deisenroth, Peters, Loh, Vadakkepat, and
  Neumann]{kupcsik2017model}
Kupcsik, A., Deisenroth, M.~P., Peters, J., Loh, A.~P., Vadakkepat, P., and
  Neumann, G.
\newblock Model-based contextual policy search for data-efficient
  generalization of robot skills.
\newblock \emph{Artificial Intelligence}, 247:\penalty0 415--439, 2017.

\bibitem[Langford \& Zhang(2007)Langford and Zhang]{langford2007epoch}
Langford, J. and Zhang, T.
\newblock The epoch-greedy algorithm for contextual multi-armed bandits.
\newblock \emph{Advances in neural information processing systems}, 20\penalty0
  (1):\penalty0 96--1, 2007.

\bibitem[Li et~al.(2019)Li, Wang, and Zhou]{li2019nearly}
Li, Y., Wang, Y., and Zhou, Y.
\newblock Nearly minimax-optimal regret for linearly parameterized bandits.
\newblock In \emph{Conference on Learning Theory}, pp.\  2173--2174. PMLR,
  2019.

\bibitem[Lindley(1956)]{lindley1956}
Lindley, D.~V.
\newblock On a measure of the information provided by an experiment.
\newblock \emph{The Annals of Mathematical Statistics}, pp.\  986--1005, 1956.

\bibitem[MacKay(1992)]{mackay1992information}
MacKay, D.~J.
\newblock Information-based objective functions for active data selection.
\newblock \emph{Neural computation}, 4\penalty0 (4):\penalty0 590--604, 1992.

\bibitem[Maddison et~al.(2016)Maddison, Mnih, and Teh]{maddison2016concrete}
Maddison, C.~J., Mnih, A., and Teh, Y.~W.
\newblock The concrete distribution: A continuous relaxation of discrete random
  variables.
\newblock \emph{arXiv preprint arXiv:1611.00712}, 2016.

\bibitem[Mohamed et~al.(2020)Mohamed, Rosca, Figurnov, and
  Mnih]{mohamed2020monte}
Mohamed, S., Rosca, M., Figurnov, M., and Mnih, A.
\newblock Monte carlo gradient estimation in machine learning.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (132):\penalty0 1--62, 2020.

\bibitem[Murphy(2001)]{murphy2001active}
Murphy, K.~P.
\newblock Active learning of causal bayes net structure.
\newblock 2001.

\bibitem[Neiswanger et~al.(2021)Neiswanger, Wang, and
  Ermon]{neiswanger2021bayesian}
Neiswanger, W., Wang, K.~A., and Ermon, S.
\newblock Bayesian algorithm execution: Estimating computable properties of
  black-box functions using mutual information.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8005--8015. PMLR, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems 32}, pp.\
  8024--8035. Curran Associates, Inc., 2019.

\bibitem[Pearce \& Branke(2018)Pearce and Branke]{pearce2018continuous}
Pearce, M. and Branke, J.
\newblock Continuous multi-task {B}ayesian optimisation with correlation.
\newblock \emph{European Journal of Operational Research}, 270\penalty0
  (3):\penalty0 1074--1085, 2018.

\bibitem[Pearce et~al.(2020)Pearce, Klaise, and Groves]{pearce2020practical}
Pearce, M., Klaise, J., and Groves, M.
\newblock Practical bayesian optimization of objectives with conditioning
  variables.
\newblock \emph{arXiv preprint arXiv:2002.09996}, 2020.

\bibitem[Pearl(2009)]{pearl2009causal}
Pearl, J.
\newblock Causal inference in statistics: An overview.
\newblock \emph{Statistics surveys}, 3:\penalty0 96--146, 2009.

\bibitem[Pearl et~al.(2000)]{pearl2000models}
Pearl, J. et~al.
\newblock Models, reasoning and inference.
\newblock \emph{Cambridge, UK: CambridgeUniversityPress}, 19, 2000.

\bibitem[Poole et~al.(2019)Poole, Ozair, van~den Oord, Alemi, and
  Tucker]{poole2018variational}
Poole, B., Ozair, S., van~den Oord, A., Alemi, A., and Tucker, G.
\newblock On variational bounds of mutual information.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5171--5180, 2019.

\bibitem[Rainforth et~al.(2018)Rainforth, Cornish, Yang, Warrington, and
  Wood]{rainforth2018nesting}
Rainforth, T., Cornish, R., Yang, H., Warrington, A., and Wood, F.
\newblock On nesting monte carlo estimators.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4267--4276. PMLR, 2018.

\bibitem[Reitmaier et~al.(2015)Reitmaier, Calma, and
  Sick]{reitmaier2015transductive}
Reitmaier, Calma, and Sick.
\newblock Transductive active learning---a new semi-supervised learning
  approach based on iteratively refined generative models to capture structure
  in data.
\newblock \emph{Information Sciences}, 2015.

\bibitem[Ruan et~al.(2021)Ruan, Yang, and Zhou]{ruan2021linear}
Ruan, Y., Yang, J., and Zhou, Y.
\newblock Linear bandits with limited adaptivity and learning distributional
  optimal design.
\newblock In \emph{Proceedings of the 53rd Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  74--87, 2021.

\bibitem[Shortreed \& Ertefaie(2017)Shortreed and
  Ertefaie]{shortreed2017outcome}
Shortreed, S.~M. and Ertefaie, A.
\newblock Outcome-adaptive lasso: variable selection for causal inference.
\newblock \emph{Biometrics}, 73\penalty0 (4):\penalty0 1111--1122, 2017.

\bibitem[Srinivas et~al.(2010)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2010gaussian}
Srinivas, N., Krause, A., Kakade, S.~M., and Seeger, M.
\newblock {G}aussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock \emph{International Conference on Machine Learning}, 2010.

\bibitem[Sussex et~al.(2022)Sussex, Makarova, and Krause]{sussex2022model}
Sussex, S., Makarova, A., and Krause, A.
\newblock Model-based causal bayesian optimization.
\newblock \emph{arXiv preprint arXiv:2211.10257}, 2022.

\bibitem[Swersky et~al.(2013)Swersky, Snoek, and Adams]{swersky2013multi}
Swersky, K., Snoek, J., and Adams, R.~P.
\newblock {Multi-task Bayesian optimization}.
\newblock \emph{Advances in neural information processing systems}, 26, 2013.

\bibitem[Thompson(1933)]{thompson1933likelihood}
Thompson, W.~R.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3-4):\penalty0 285--294, 1933.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Tibshirani, R.
\newblock Regression shrinkage and selection via the lasso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Tigas et~al.(2022)Tigas, Annadani, Jesson, Sch{\"o}lkopf, Gal, and
  Bauer]{tigas2022interventions}
Tigas, P., Annadani, Y., Jesson, A., Sch{\"o}lkopf, B., Gal, Y., and Bauer, S.
\newblock Interventions, where and how? experimental design for causal models
  at scale.
\newblock \emph{arXiv preprint arXiv:2203.02016}, 2022.

\bibitem[Tong \& Koller(2001)Tong and Koller]{tong2001active}
Tong, S. and Koller, D.
\newblock Active learning for structure in bayesian networks.
\newblock In \emph{International joint conference on artificial intelligence},
  pp.\  863--869. Citeseer, 2001.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, and
  Vinyals]{oord2018representation}
van~den Oord, A., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Wang et~al.(2021)Wang, Sun, and Grosse]{wang2021beyond}
Wang, Sun, and Grosse.
\newblock Beyond marginal uncertainty: how accurately can {B}ayesian regression
  models estimate posterior predictive correlations?
\newblock \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem[Wang \& Jegelka(2017)Wang and Jegelka]{wang2017max}
Wang, Z. and Jegelka, S.
\newblock Max-value entropy search for efficient bayesian optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3627--3635. PMLR, 2017.

\bibitem[Wang et~al.(2018)Wang, Gehring, Kohli, and Jegelka]{wang2018batched}
Wang, Z., Gehring, C., Kohli, P., and Jegelka, S.
\newblock Batched large-scale bayesian optimization in
  high-dimensryan2016reviewional spaces.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  745--754. PMLR, 2018.

\bibitem[Williams \& Rasmussen(2006)Williams and
  Rasmussen]{williams2006gaussian}
Williams, C.~K. and Rasmussen, C.~E.
\newblock \emph{Gaussian processes for machine learning}, volume~2.
\newblock MIT press Cambridge, MA, 2006.

\bibitem[Wu \& Frazier(2016)Wu and Frazier]{wu2016parallel}
Wu, J. and Frazier, P.
\newblock The parallel knowledge gradient method for batch bayesian
  optimization.
\newblock \emph{Advances in neural information processing systems}, 29, 2016.

\bibitem[Wu et~al.(2018)Wu, Xiong, Yu, and Lin]{wu2018unsupervised}
Wu, Z., Xiong, Y., Yu, S.~X., and Lin, D.
\newblock Unsupervised feature learning via non-parametric instance
  discrimination.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  3733--3742, 2018.

\bibitem[Yu et~al.(2006)Yu, Bi, and Tresp]{yu2006active}
Yu, Bi, and Tresp.
\newblock Active learning via transductive experimental design.
\newblock \emph{International Conference on Machine Learning}, 2006.

\bibitem[Zanette et~al.(2021)Zanette, Dong, Lee, and
  Brunskill]{zanette2021design}
Zanette, A., Dong, K., Lee, J.~N., and Brunskill, E.
\newblock Design of experiments for stochastic contextual linear bandits.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 22720--22731, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Ji, and Zhou]{zhang2021almost}
Zhang, Z., Ji, X., and Zhou, Y.
\newblock Almost optimal batch-regret tradeoff for batch linear contextual
  bandits.
\newblock \emph{arXiv preprint arXiv:2110.08057}, 2021.

\bibitem[Zheng et~al.(2018)Zheng, Pacheco, and Fisher]{zheng2018robust}
Zheng, S., Pacheco, J., and Fisher, J.
\newblock A robust approach to sequential information theoretic planning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5941--5949, 2018.

\end{thebibliography}
