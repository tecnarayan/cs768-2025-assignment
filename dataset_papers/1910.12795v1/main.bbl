\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018)Abdolmaleki, Springenberg, Tassa, Munos,
  Heess, and Riedmiller]{abdolmaleki2018maximum}
A.~Abdolmaleki, J.~T. Springenberg, Y.~Tassa, R.~Munos, N.~Heess, and
  M.~Riedmiller.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Andreas et~al.(2016)Andreas, Rohrbach, Darrell, and
  Klein]{andreas2016learning}
J.~Andreas, M.~Rohrbach, T.~Darrell, and D.~Klein.
\newblock Learning to compose neural networks for question answering.
\newblock \emph{arXiv preprint arXiv:1601.01705}, 2016.

\bibitem[Baluja and Fischer(2017)]{baluja2017adversarial}
S.~Baluja and I.~Fischer.
\newblock Adversarial transformation networks: Learning to generate adversarial
  examples.
\newblock \emph{arXiv preprint arXiv:1703.09387}, 2017.

\bibitem[Chang et~al.(2017)Chang, Learned-Miller, and
  McCallum]{chang2017active}
H.-S. Chang, E.~Learned-Miller, and A.~McCallum.
\newblock Active bias: Training more accurate neural networks by emphasizing
  high variance samples.
\newblock In \emph{NeurIPS}, pages 1002--1012, 2017.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock In \emph{CVPR}, 2019.

\bibitem[Dayan and Hinton(1997)]{dayan1997using}
P.~Dayan and G.~E. Hinton.
\newblock Using expectation-maximization for reinforcement learning.
\newblock \emph{Neural Computation}, 9\penalty0 (2):\penalty0 271--278, 1997.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Fan et~al.(2018)Fan, Tian, Qin, Li, and Liu]{fan2018learning}
Y.~Fan, F.~Tian, T.~Qin, X.-Y. Li, and T.-Y. Liu.
\newblock Learning to teach.
\newblock In \emph{ICLR}, 2018.

\bibitem[Finn et~al.(2016)Finn, Christiano, Abbeel, and
  Levine]{finn2016connection}
C.~Finn, P.~Christiano, P.~Abbeel, and S.~Levine.
\newblock A connection between generative adversarial networks, inverse
  reinforcement learning, and energy-based models.
\newblock \emph{arXiv preprint arXiv:1611.03852}, 2016.

\bibitem[Freund and Schapire(1997)]{freund1997decision}
Y.~Freund and R.~E. Schapire.
\newblock A decision-theoretic generalization of on-line learning and an
  application to boosting.
\newblock \emph{Journal of computer and system sciences}, 55\penalty0
  (1):\penalty0 119--139, 1997.

\bibitem[Ganchev et~al.(2010)Ganchev, Gillenwater, Taskar,
  et~al.]{ganchev2010posterior}
K.~Ganchev, J.~Gillenwater, B.~Taskar, et~al.
\newblock Posterior regularization for structured latent variable models.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Jul):\penalty0 2001--2049, 2010.

\bibitem[Giridhara et~al.(2019)Giridhara, Chinmaya, Venkataramana, Bukhari, and
  Dengel]{giridhara2019survey}
P.~K.~B. Giridhara, M.~Chinmaya, R.~K.~M. Venkataramana, S.~S. Bukhari, and
  A.~Dengel.
\newblock A study of various text augmentation techniques for relation
  classification in free text.
\newblock In \emph{ICPRAM}, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{NIPS}, pages 2672--2680, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[Hinton et~al.(1995)Hinton, Dayan, Frey, and Neal]{hinton1995wake}
G.~E. Hinton, P.~Dayan, B.~J. Frey, and R.~M. Neal.
\newblock The" wake-sleep" algorithm for unsupervised neural networks.
\newblock \emph{Science}, 268\penalty0 (5214):\penalty0 1158, 1995.

\bibitem[Hu et~al.(2016)Hu, Ma, Liu, Hovy, and Xing]{hu2016harnessing}
Z.~Hu, X.~Ma, Z.~Liu, E.~Hovy, and E.~Xing.
\newblock Harnessing deep neural networks with logic rules.
\newblock In \emph{ACL}, 2016.

\bibitem[Hu et~al.(2017)Hu, Yang, Liang, Salakhutdinov, and
  Xing]{hu2017controllable}
Z.~Hu, Z.~Yang, X.~Liang, R.~Salakhutdinov, and E.~P. Xing.
\newblock Toward controlled generation of text.
\newblock In \emph{ICML}, 2017.

\bibitem[Hu et~al.(2018{\natexlab{a}})Hu, Yang, Salakhutdinov, Liang, Qin,
  Dong, and Xing]{hu2018deep}
Z.~Hu, Z.~Yang, R.~Salakhutdinov, X.~Liang, L.~Qin, H.~Dong, and E.~Xing.
\newblock Deep generative models with learnable knowledge constraints.
\newblock In \emph{NIPS}, 2018{\natexlab{a}}.

\bibitem[Hu et~al.(2018{\natexlab{b}})Hu, Yang, Salakhutdinov, and
  Xing]{hu2018unifying}
Z.~Hu, Z.~Yang, R.~Salakhutdinov, and E.~P. Xing.
\newblock On unifying deep generative models.
\newblock In \emph{ICLR}, 2018{\natexlab{b}}.

\bibitem[Jang et~al.(2016)Jang, Gu, and Poole]{jang2016categorical}
E.~Jang, S.~Gu, and B.~Poole.
\newblock Categorical reparameterization with gumbel-softmax.
\newblock \emph{arXiv preprint arXiv:1611.01144}, 2016.

\bibitem[Jiang et~al.(2018)Jiang, Zhou, Leung, Li, and
  Fei-Fei]{jiang2017mentornet}
L.~Jiang, Z.~Zhou, T.~Leung, L.-J. Li, and L.~Fei-Fei.
\newblock Mentornet: Learning data-driven curriculum for very deep neural
  networks on corrupted labels.
\newblock In \emph{ICML}, 2018.

\bibitem[Katharopoulos and Fleuret(2018)]{katharopoulos2018not}
A.~Katharopoulos and F.~Fleuret.
\newblock Not all samples are created equal: Deep learning with importance
  sampling.
\newblock In \emph{ICML}, 2018.

\bibitem[Kingma and Welling(2013)]{kingma2013auto}
D.~P. Kingma and M.~Welling.
\newblock Auto-encoding variational {B}ayes.
\newblock \emph{arXiv preprint arXiv:1312.6114}, 2013.

\bibitem[Ko et~al.(2015)Ko, Peddinti, Povey, and Khudanpur]{ko2015audio}
T.~Ko, V.~Peddinti, D.~Povey, and S.~Khudanpur.
\newblock Audio augmentation for speech recognition.
\newblock In \emph{INTERSPEECH}, 2015.

\bibitem[Kobayashi(2018)]{kobayashi2018contextual}
S.~Kobayashi.
\newblock Contextual augmentation: Data augmentation by words with paradigmatic
  relations.
\newblock In \emph{NAACL}, 2018.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[Kumar et~al.(2010)Kumar, Packer, and Koller]{kumar2010self}
M.~P. Kumar, B.~Packer, and D.~Koller.
\newblock Self-paced learning for latent variable models.
\newblock In \emph{NeurIPS}, pages 1189--1197, 2010.

\bibitem[Lemley et~al.(2017)Lemley, Bazrafkan, and Corcoran]{lemley2017smart}
J.~Lemley, S.~Bazrafkan, and P.~Corcoran.
\newblock Smart augmentation learning an optimal data augmentation strategy.
\newblock \emph{IEEE Access}, 5:\penalty0 5858--5869, 2017.

\bibitem[Levine(2018)]{levine2018reinforcement}
S.~Levine.
\newblock Reinforcement learning and control as probabilistic inference:
  Tutorial and review.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Li and Roth(2002)]{li2002learning}
X.~Li and D.~Roth.
\newblock Learning question classifiers.
\newblock In \emph{Proceedings of the 19th international conference on
  Computational linguistics}, pages 1--7, 2002.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{maas2011learning}
A.~L. Maas, R.~E. Daly, P.~T. Pham, D.~Huang, A.~Y. Ng, and C.~Potts.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{Proceedings of the 49th annual meeting of the association
  for computational linguistics: Human language technologies-volume 1}, pages
  142--150. Association for Computational Linguistics, 2011.

\bibitem[Malisiewicz et~al.(2011)Malisiewicz, Gupta, Efros,
  et~al.]{malisiewicz2011ensemble}
T.~Malisiewicz, A.~Gupta, A.~A. Efros, et~al.
\newblock Ensemble of exemplar-{SVM}s for object detection and beyond.
\newblock In \emph{ICCV}, volume~1, page~6. Citeseer, 2011.

\bibitem[Miller(1995)]{miller1995wordnet}
G.~A. Miller.
\newblock {WordNet}: a lexical database for {E}nglish.
\newblock \emph{Communications of the ACM}, 38\penalty0 (11):\penalty0 39--41,
  1995.

\bibitem[Mirza and Osindero(2014)]{mirza2014conditional}
M.~Mirza and S.~Osindero.
\newblock Conditional generative adversarial nets.
\newblock \emph{arXiv preprint arXiv:1411.1784}, 2014.

\bibitem[Norouzi et~al.(2016)Norouzi, Bengio, Jaitly, Schuster, Wu, Schuurmans,
  et~al.]{norouzi2016reward}
M.~Norouzi, S.~Bengio, N.~Jaitly, M.~Schuster, Y.~Wu, D.~Schuurmans, et~al.
\newblock Reward augmented maximum likelihood for neural structured prediction.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  1723--1731, 2016.

\bibitem[Park et~al.()Park, Chan, Zhang, Chiu, Zoph, Cubuk, and
  Le]{park2019specaugment}
D.~S. Park, W.~Chan, Y.~Zhang, C.-C. Chiu, B.~Zoph, E.~D. Cubuk, and Q.~V. Le.
\newblock Specaugment: A simple data augmentation method for automatic speech
  recognition.
\newblock \emph{arXiv preprint arXiv 1904.08779}.

\bibitem[Peng et~al.(2018)Peng, Tang, Yang, Feris, and
  Metaxas]{peng2018jointly}
X.~Peng, Z.~Tang, F.~Yang, R.~S. Feris, and D.~Metaxas.
\newblock Jointly optimize data augmentation and network training: Adversarial
  data augmentation in human pose estimation.
\newblock In \emph{CVPR}, 2018.

\bibitem[Ratner et~al.(2017)Ratner, Ehrenberg, Hussain, Dunnmon, and
  R{\'e}]{ratner2017learning}
A.~J. Ratner, H.~Ehrenberg, Z.~Hussain, J.~Dunnmon, and C.~R{\'e}.
\newblock Learning to compose domain-specific transformations for data
  augmentation.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
M.~Ren, W.~Zeng, B.~Yang, and R.~Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Roweis and Ghahramani(1999)]{roweis1999unifying}
S.~Roweis and Z.~Ghahramani.
\newblock A unifying review of linear gaussian models.
\newblock \emph{Neural computation}, 1999.

\bibitem[Samdani et~al.(2012)Samdani, Chang, and Roth]{samdani2012unified}
R.~Samdani, M.-W. Chang, and D.~Roth.
\newblock Unified expectation maximization.
\newblock In \emph{ACL}, 2012.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and
  Birch]{sennrich2015improving}
R.~Sennrich, B.~Haddow, and A.~Birch.
\newblock Improving neural machine translation models with monolingual data.
\newblock In \emph{ACL}, 2016.

\bibitem[Shrivastava et~al.(2016)Shrivastava, Gupta, and
  Girshick]{shrivastava2016training}
A.~Shrivastava, A.~Gupta, and R.~Girshick.
\newblock Training region-based object detectors with online hard example
  mining.
\newblock In \emph{CVPR}, pages 761--769, 2016.

\bibitem[Simard et~al.(1998)Simard, LeCun, Denker, and
  Victorri]{simard1998transformation}
P.~Y. Simard, Y.~A. LeCun, J.~S. Denker, and B.~Victorri.
\newblock Transformation invariance in pattern recognition—tangent distance
  and tangent propagation.
\newblock In \emph{Neural networks: tricks of the trade}, pages 239--274.
  Springer, 1998.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{socher2013recursive}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Ng, and C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{EMNLP}, pages 1631--1642, 2013.

\bibitem[Tan et~al.(2018)Tan, Hu, Yang, Salakhutdinov, and
  Xing]{tan2018connecting}
B.~Tan, Z.~Hu, Z.~Yang, R.~Salakhutdinov, and E.~Xing.
\newblock Connecting the dots between {MLE} and {RL} for sequence generation.
\newblock \emph{arXiv preprint arXiv:1811.09740}, 2018.

\bibitem[Tran et~al.(2017)Tran, Pham, Carneiro, Palmer, and
  Reid]{tran2017bayesian}
T.~Tran, T.~Pham, G.~Carneiro, L.~Palmer, and I.~Reid.
\newblock A {B}ayesian data augmentation approach for learning deep models.
\newblock In \emph{NeurIPS}, pages 2797--2806, 2017.

\bibitem[Wei and Zou(2019)]{wei2019eda}
J.~W. Wei and K.~Zou.
\newblock {EDA}: Easy data augmentation techniques for boosting performance on
  text classification tasks.
\newblock \emph{arXiv preprint arXiv:1901.11196}, 2019.

\bibitem[Wu et~al.(2018)Wu, Lv, Zang, Han, and Hu]{wu2018conditional}
X.~Wu, S.~Lv, L.~Zang, J.~Han, and S.~Hu.
\newblock Conditional {BERT} contextual augmentation.
\newblock \emph{arXiv preprint arXiv:1812.06705}, 2018.

\bibitem[Xie et~al.(2019)Xie, Dai, Hovy, Luong, and V.]{xie2019unsupervised}
Q.~Xie, Z.~Dai, E.~Hovy, M.-T. Luong, and L.~Q. V.
\newblock Unsupervised data augmentation.
\newblock \emph{arXiv preprint arXiv 1904.12848}, 2019.

\bibitem[Xie et~al.(2017)Xie, Wang, Li, L{\'e}vy, Nie, Jurafsky, and
  Ng]{xie2017data}
Z.~Xie, S.~I. Wang, J.~Li, D.~L{\'e}vy, A.~Nie, D.~Jurafsky, and A.~Y. Ng.
\newblock Data noising as smoothing in neural network language models.
\newblock In \emph{ICLR}, 2017.

\bibitem[Zheng et~al.(2018)Zheng, Oh, and Singh]{zheng2018learning}
Z.~Zheng, J.~Oh, and S.~Singh.
\newblock On learning intrinsic rewards for policy gradient methods.
\newblock In \emph{NeurIPS}, 2018.

\end{thebibliography}
