\begin{thebibliography}{60}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
Brooks, S., Gelman, A., Jones, G., and Meng, X.-L.
\newblock \emph{Handbook of markov chain monte carlo}.
\newblock CRC press, 2011.

\bibitem[Buza(2014)]{buza2014feedback}
Buza, K.
\newblock Feedback prediction for blogs.
\newblock In \emph{Data analysis, machine learning and knowledge discovery},
  pp.\  145--152. Springer, 2014.

\bibitem[Cho \& Meyer(2001)Cho and Meyer]{cho2001comparison}
Cho, G.~E. and Meyer, C.~D.
\newblock Comparison of perturbation bounds for the stationary distribution of
  a markov chain.
\newblock \emph{Linear Algebra and its Applications}, 335\penalty0
  (1-3):\penalty0 137--150, 2001.

\bibitem[Courbariaux et~al.(2016)Courbariaux, Hubara, Soudry, El-Yaniv, and
  Bengio]{courbariaux2016binarized}
Courbariaux, M., Hubara, I., Soudry, D., El-Yaniv, R., and Bengio, Y.
\newblock Binarized neural networks: Training deep neural networks with weights
  and activations constrained to+ 1 or-1.
\newblock \emph{arXiv preprint arXiv:1602.02830}, 2016.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{Association for Computational Linguistics}, 2019.

\bibitem[Donahue et~al.(2020)Donahue, Lee, and Liang]{donahue2020enabling}
Donahue, C., Lee, M., and Liang, P.
\newblock Enabling language models to fill in the blanks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association
  for Computational Linguistics}, pp.\  2492--2501, 2020.

\bibitem[Du \& Mordatch(2019)Du and Mordatch]{du2019implicit}
Du, Y. and Mordatch, I.
\newblock Implicit generation and generalization in energy-based models.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.
\newblock URL \url{http://archive.ics.uci.edu/ml}.

\bibitem[Duane et~al.(1987)Duane, Kennedy, Pendleton, and
  Roweth]{duane1987hybrid}
Duane, S., Kennedy, A.~D., Pendleton, B.~J., and Roweth, D.
\newblock Hybrid monte carlo.
\newblock \emph{Physics letters B}, 195\penalty0 (2):\penalty0 216--222, 1987.

\bibitem[Durmus \& Moulines(2019)Durmus and Moulines]{durmus2019high}
Durmus, A. and Moulines, E.
\newblock High-dimensional bayesian inference via the unadjusted langevin
  algorithm.
\newblock \emph{Bernoulli}, 25\penalty0 (4A):\penalty0 2854--2882, 2019.

\bibitem[Grathwohl et~al.(2019)Grathwohl, Wang, Jacobsen, Duvenaud, Norouzi,
  and Swersky]{grathwohl2019your}
Grathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D., Norouzi, M., and
  Swersky, K.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Grathwohl et~al.(2021)Grathwohl, Swersky, Hashemi, Duvenaud, and
  Maddison]{grathwohl2021oops}
Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud, D., and Maddison, C.~J.
\newblock Oops i took a gradient: Scalable sampling for discrete distributions.
\newblock \emph{International Conference on Machine Learning}, 2021.

\bibitem[Grenander \& Miller(1994)Grenander and
  Miller]{grenander1994representations}
Grenander, U. and Miller, M.~I.
\newblock Representations of knowledge in complex systems.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 56\penalty0 (4):\penalty0 549--581, 1994.

\bibitem[Gu et~al.(2018)Gu, Bradbury, Xiong, Li, and Socher]{gu2018non}
Gu, J., Bradbury, J., Xiong, C., Li, V.~O., and Socher, R.
\newblock Non-autoregressive neural machine translation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Han et~al.(2020)Han, Ding, Liu, Torresani, Peng, and
  Liu]{han2020stein}
Han, J., Ding, F., Liu, X., Torresani, L., Peng, J., and Liu, Q.
\newblock Stein variational inference for discrete distributions.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  4563--4572. PMLR, 2020.

\bibitem[Hastings(1970)]{hastings1970monte}
Hastings, W.~K.
\newblock Monte carlo sampling methods using markov chains and their
  applications.
\newblock 1970.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pp.\  770--778, 2016.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of bayesian
  neural networks.
\newblock In \emph{International conference on machine learning}, pp.\
  1861--1869. PMLR, 2015.

\bibitem[Hinton(2002)]{hinton2002training}
Hinton, G.~E.
\newblock Training products of experts by minimizing contrastive divergence.
\newblock \emph{Neural computation}, 14\penalty0 (8):\penalty0 1771--1800,
  2002.

\bibitem[J.~Angwin \& Kirchner(2016)J.~Angwin and Kirchner]{compas}
J.~Angwin, J.~Larson, S.~M. and Kirchner, L.
\newblock Machine bias: There’s software used across the country to predict
  future criminals. and it’s biased against blacks.
\newblock \emph{ProPublica}, 2016.

\bibitem[Jaini et~al.(2021)Jaini, Nielsen, and Welling]{jaini2021sampling}
Jaini, P., Nielsen, D., and Welling, M.
\newblock Sampling in combinatorial spaces with survae flow augmented mcmc.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  3349--3357. PMLR, 2021.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{International Conference for Learning Representations}, 2015.

\bibitem[LeCun et~al.(2006)LeCun, Chopra, Hadsell, Ranzato, and
  Huang]{lecun2006tutorial}
LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.
\newblock A tutorial on energy-based learning.
\newblock \emph{Predicting structured data}, 1\penalty0 (0), 2006.

\bibitem[Levin \& Peres(2017)Levin and Peres]{levin2017markov}
Levin, D.~A. and Peres, Y.
\newblock \emph{Markov chains and mixing times}, volume 107.
\newblock American Mathematical Soc., 2017.

\bibitem[Li et~al.(2016)Li, Chen, Carlson, and Carin]{li2016preconditioned}
Li, C., Chen, C., Carlson, D., and Carin, L.
\newblock Preconditioned stochastic gradient langevin dynamics for deep neural
  networks.
\newblock In \emph{Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Tong, and Liu]{liu2021sampling}
Liu, X., Tong, X., and Liu, Q.
\newblock Sampling with trusthworthy constraints: A variational gradient
  framework.
\newblock \emph{Advances in Neural Information Processing Systems}, 34,
  2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Ye, Zhou, and Liu]{liu2021post}
Liu, X., Ye, M., Zhou, D., and Liu, Q.
\newblock Post-training quantization with multiple points: Mixed precision
  without mixed precision.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pp.\  8697--8705, 2021{\natexlab{b}}.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{International conference on machine learning}, 2017.

\bibitem[Metropolis et~al.(1953)Metropolis, Rosenbluth, Rosenbluth, Teller, and
  Teller]{metropolis1953equation}
Metropolis, N., Rosenbluth, A.~W., Rosenbluth, M.~N., Teller, A.~H., and
  Teller, E.
\newblock Equation of state calculations by fast computing machines.
\newblock \emph{The journal of chemical physics}, 21\penalty0 (6):\penalty0
  1087--1092, 1953.

\bibitem[Neal(2001)]{neal2001annealed}
Neal, R.~M.
\newblock Annealed importance sampling.
\newblock \emph{Statistics and computing}, 11\penalty0 (2):\penalty0 125--139,
  2001.

\bibitem[Neal et~al.(2011)]{neal2011mcmc}
Neal, R.~M. et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock \emph{Handbook of markov chain monte carlo}, 2\penalty0
  (11):\penalty0 2, 2011.

\bibitem[Nishimura et~al.(2020)Nishimura, Dunson, and
  Lu]{nishimura2020discontinuous}
Nishimura, A., Dunson, D.~B., and Lu, J.
\newblock Discontinuous hamiltonian monte carlo for discrete parameters and
  discontinuous likelihoods.
\newblock \emph{Biometrika}, 107\penalty0 (2):\penalty0 365--380, 2020.

\bibitem[Pakman \& Paninski(2013)Pakman and Paninski]{pakman2013auxiliary}
Pakman, A. and Paninski, L.
\newblock Auxiliary-variable exact hamiltonian monte carlo samplers for binary
  distributions.
\newblock \emph{Advances in Neural Information Processing Systems}, 2013.

\bibitem[Papineni et~al.(2002)Papineni, Roukos, Ward, and
  Zhu]{papineni2002bleu}
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In \emph{Proceedings of the 40th annual meeting of the Association
  for Computational Linguistics}, pp.\  311--318, 2002.

\bibitem[Peters \& Welling(2018)Peters and Welling]{peters2018probabilistic}
Peters, J.~W. and Welling, M.
\newblock Probabilistic binary neural networks.
\newblock \emph{arXiv preprint arXiv:1809.03368}, 2018.

\bibitem[Power \& Goldman(2019)Power and Goldman]{power2019accelerated}
Power, S. and Goldman, J.~V.
\newblock Accelerated sampling on discrete spaces with non-reversible markov
  processes.
\newblock \emph{arXiv preprint arXiv:1912.04681}, 2019.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le]{ramachandran2017searching}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions.
\newblock \emph{arXiv preprint arXiv:1710.05941}, 2017.

\bibitem[Rastegari et~al.(2016)Rastegari, Ordonez, Redmon, and
  Farhadi]{rastegari2016xnor}
Rastegari, M., Ordonez, V., Redmon, J., and Farhadi, A.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In \emph{European conference on computer vision}, pp.\  525--542.
  Springer, 2016.

\bibitem[Roberts \& Stramer(2002)Roberts and Stramer]{roberts2002langevin}
Roberts, G.~O. and Stramer, O.
\newblock Langevin diffusions and metropolis-hastings algorithms.
\newblock \emph{Methodology and computing in applied probability}, 4\penalty0
  (4):\penalty0 337--357, 2002.

\bibitem[Roberts \& Tweedie(1996)Roberts and Tweedie]{roberts1996exponential}
Roberts, G.~O. and Tweedie, R.~L.
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock \emph{Bernoulli}, pp.\  341--363, 1996.

\bibitem[Sansone(2021)]{sansone2021lsb}
Sansone, E.
\newblock Lsb: Local self-balancing mcmc in discrete spaces.
\newblock \emph{arXiv preprint arXiv:2109.03867}, 2021.

\bibitem[Schweitzer(1968)]{schweitzer1968perturbation}
Schweitzer, P.~J.
\newblock Perturbation theory and finite markov chains.
\newblock \emph{Journal of Applied Probability}, 5\penalty0 (2):\penalty0
  401--413, 1968.

\bibitem[Song \& Ermon(2019)Song and Ermon]{song2019generative}
Song, Y. and Ermon, S.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Sun et~al.(2022)Sun, Dai, Xia, and Ramamurthy]{sun2021path}
Sun, H., Dai, H., Xia, W., and Ramamurthy, A.
\newblock Path auxiliary proposal for mcmc in discrete space.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Tieleman(2008)]{tieleman2008training}
Tieleman, T.
\newblock Training restricted boltzmann machines using approximations to the
  likelihood gradient.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pp.\  1064--1071, 2008.

\bibitem[Tieleman \& Hinton(2009)Tieleman and Hinton]{tieleman2009using}
Tieleman, T. and Hinton, G.
\newblock Using fast weights to improve persistent contrastive divergence.
\newblock In \emph{Proceedings of the 26th annual international conference on
  machine learning}, pp.\  1033--1040, 2009.

\bibitem[Titsias \& Yau(2017)Titsias and Yau]{titsias2017hamming}
Titsias, M.~K. and Yau, C.
\newblock The hamming ball sampler.
\newblock \emph{Journal of the American Statistical Association}, 112\penalty0
  (520):\penalty0 1598--1611, 2017.

\bibitem[Tomczak \& Welling(2018)Tomczak and Welling]{tomczak2018vae}
Tomczak, J. and Welling, M.
\newblock Vae with a vampprior.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1214--1223. PMLR, 2018.

\bibitem[Wang \& Cho(2019)Wang and Cho]{wang2019bert}
Wang, A. and Cho, K.
\newblock Bert has a mouth, and it must speak: Bert as a markov random field
  language model.
\newblock \emph{arXiv preprint arXiv:1902.04094}, 2019.

\bibitem[Wang et~al.(2010)Wang, Huda, Lunyak, and Jordan]{wang2010gibbs}
Wang, J., Huda, A., Lunyak, V.~V., and Jordan, I.~K.
\newblock A gibbs sampling strategy applied to the mapping of ambiguous
  short-sequence tags.
\newblock \emph{Bioinformatics}, 26\penalty0 (20):\penalty0 2501--2508, 2010.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient langevin dynamics.
\newblock In \emph{Proceedings of the 28th international conference on machine
  learning}, pp.\  681--688. Citeseer, 2011.

\bibitem[Yu et~al.(2017)Yu, Zhang, Wang, and Yu]{yu2017seqgan}
Yu, L., Zhang, W., Wang, J., and Yu, Y.
\newblock Seqgan: Sequence generative adversarial nets with policy gradient.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~31, 2017.

\bibitem[Zanella(2020)]{zanella2020informed}
Zanella, G.
\newblock Informed proposals for local mcmc in discrete spaces.
\newblock \emph{Journal of the American Statistical Association}, 115\penalty0
  (530):\penalty0 852--865, 2020.

\bibitem[Zhang et~al.(2022)Zhang, Malkin, Liu, Volokhova, Courville, and
  Bengio]{zhang2022generative}
Zhang, D., Malkin, N., Liu, Z., Volokhova, A., Courville, A., and Bengio, Y.
\newblock Generative flow networks for discrete probabilistic modeling.
\newblock \emph{arXiv preprint arXiv:2202.01361}, 2022.

\bibitem[Zhang et~al.(2020)Zhang, Li, Zhang, Chen, and
  Wilson]{zhang2019cyclical}
Zhang, R., Li, C., Zhang, J., Chen, C., and Wilson, A.~G.
\newblock Cyclical stochastic gradient mcmc for bayesian deep learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhou et~al.(2019)Zhou, Gu, and Neubig]{zhou2019understanding}
Zhou, C., Gu, J., and Neubig, G.
\newblock Understanding knowledge distillation in non-autoregressive machine
  translation.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhou(2020)]{zhou2020mixed}
Zhou, G.
\newblock Mixed hamiltonian monte carlo for mixed discrete and continuous
  variables.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 17094--17104, 2020.

\bibitem[Zhu et~al.(2019)Zhu, Hu, and Xing]{zhu2019text}
Zhu, W., Hu, Z., and Xing, E.
\newblock Text infilling.
\newblock \emph{arXiv preprint arXiv:1901.00158}, 2019.

\bibitem[Zhu et~al.(2015)Zhu, Kiros, Zemel, Salakhutdinov, Urtasun, Torralba,
  and Fidler]{zhu2015aligning}
Zhu, Y., Kiros, R., Zemel, R., Salakhutdinov, R., Urtasun, R., Torralba, A.,
  and Fidler, S.
\newblock Aligning books and movies: Towards story-like visual explanations by
  watching movies and reading books.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  19--27, 2015.

\bibitem[Zhu et~al.(2018)Zhu, Lu, Zheng, Guo, Zhang, Wang, and
  Yu]{zhu2018texygen}
Zhu, Y., Lu, S., Zheng, L., Guo, J., Zhang, W., Wang, J., and Yu, Y.
\newblock Texygen: A benchmarking platform for text generation models.
\newblock In \emph{The 41st International ACM SIGIR Conference on Research \&
  Development in Information Retrieval}, pp.\  1097--1100, 2018.

\end{thebibliography}
