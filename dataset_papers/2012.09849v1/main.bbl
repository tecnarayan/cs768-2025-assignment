\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Babaeizadeh et~al.(2017)Babaeizadeh, Frosio, Tyree, Clemons, and
  Kautz]{ga3c}
M.~Babaeizadeh, I.~Frosio, S.~Tyree, J.~Clemons, and J.~Kautz.
\newblock Reinforcement learning through asynchronous advantage actor-critic on
  a {GPU}.
\newblock In \emph{Proc. ICLR}, 2017.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and Bowling]{atari}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{JAIR}, 2013.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{gym}
G.~Brockman, V.~Cheung, L.~Pettersson, J.~Schneider, J.~Schulman, J.~Tang, and
  W.~Zaremba.
\newblock {OpenAI Gym}, 2016.

\bibitem[Colas et~al.(2018)Colas, Sigaud, and Oudeyer]{Colas18}
C.~Colas, O.~Sigaud, and P.-Y. Oudeyer.
\newblock {GEP-PG}: Decoupling exploration and exploitation in deep
  reinforcement learning algorithms.
\newblock In \emph{Proc. ICML}, 2018.

\bibitem[de~Haan and Ferreira(2006)]{Hann06}
L.~de~Haan and A.~Ferreira.
\newblock \emph{Extreme Value Theory: An Introduction}.
\newblock Springer, 2006.

\bibitem[Dhariwal et~al.(2017)Dhariwal, Hesse, Klimov, Nichol, Plappert,
  Radford, Schulman, Sidor, Wu, and Zhokhov]{a2c}
P.~Dhariwal, C.~Hesse, O.~Klimov, A.~Nichol, M.~Plappert, A.~Radford,
  J.~Schulman, S.~Sidor, Y.~Wu, and P.~Zhokhov.
\newblock {OpenAI} baselines, 2017.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{impala}
L.~Espeholt, H.~Soyer, R.~Munos, K.~Simonyan, V.~Mnih, T.~Ward, Y.~Doron,
  V.~Firoiu, T.~Harley, I.~Dunning, S.~Legg, and K.~Kavukcuoglu.
\newblock Impala: Scalable distributed deep-rl with importance weighted
  actor-learner architectures.
\newblock In \emph{Proc. ICML}, 2018.

\bibitem[Espeholt et~al.(2020)Espeholt, Marinier, Stanczyk, Wang, and
  Michalski]{seed_rl}
L.~Espeholt, R.~Marinier, P.~Stanczyk, K.~Wang, and M.~Michalski.
\newblock Seed rl: Scalable and efficient deep-rl with accelerated central
  inference.
\newblock In \emph{arXiv.}, 2020.

\bibitem[Fisher and Tippett(1928)]{Fisher28}
R.~A. Fisher and L.~H.~C. Tippett.
\newblock Limiting forms of the frequency distribution of the largest or
  smallest member of a sample.
\newblock \emph{Math. Proc. Cambridge Philos. Soc.}, 1928.

\bibitem[Gu et~al.(2017)Gu, Holly, Lillicrap, and Levine]{Gu17}
S.~Gu, E.~Holly, T.~Lillicrap, and S.~Levine.
\newblock Deep reinforcement learning for robotic manipulation with
  asynchronous off-policy updates.
\newblock In \emph{Proc. ICRA}, 2017.

\bibitem[Henderson et~al.(2017)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{Henderson17}
P.~Henderson, R.~Islam, P.~Bachman, J.~Pineau, D.~Precup, and D.~Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proc. AAAI}, 2017.

\bibitem[Jain et~al.(2019)Jain, Weihs, Kolve, Rastegari, Lazebnik, Farhadi,
  Schwing, and Kembhavi]{Jain19}
U.~Jain, L.~Weihs, E.~Kolve, M.~Rastegari, S.~Lazebnik, A.~Farhadi, A.~Schwing,
  and A.~Kembhavi.
\newblock Two body problem: Collaborative visual task completion.
\newblock In \emph{Proc. CVPR}, 2019.

\bibitem[Jain$^\ast$ et~al.(2020)Jain$^\ast$, Weihs$^\ast$, Kolve, Farhadi,
  Lazebnik, Kembhavi, and Schwing]{JainECCV2020}
U.~Jain$^\ast$, L.~Weihs$^\ast$, E.~Kolve, A.~Farhadi, S.~Lazebnik,
  A.~Kembhavi, and A.~G. Schwing.
\newblock {A Cordial Sync: Going Beyond Marginal Policies For Multi-Agent
  Embodied Tasks}.
\newblock In \emph{Proc. ECCV}, 2020.
\newblock $^\ast$ equal contribution.

\bibitem[Kostrikov(2018)]{pytorchrl}
I.~Kostrikov.
\newblock Pytorch implementations of reinforcement learning algorithms, 2018.

\bibitem[Kurach et~al.(2019)Kurach, Raichuk, Sta\'nczyk, Zajac, Bachem,
  Espeholt, Riquelme, Vincent, Michalski, Bousquet, and Gelly]{gfootball}
K.~Kurach, A.~Raichuk, P.~Sta\'nczyk, M.~Zajac, O.~Bachem, L.~Espeholt,
  C.~Riquelme, D.~Vincent, M.~Michalski, O.~Bousquet, and S.~Gelly.
\newblock Google research football: A novel reinforcement learning environment.
\newblock \emph{arXiv.}, 2019.

\bibitem[K\"{u}ttler et~al.(2019)K\"{u}ttler, Nardelli, Lavril, Selvatici,
  Sivakumar, Rockt\"{a}schel, and Grefenstette]{torchbeast}
H.~K\"{u}ttler, N.~Nardelli, T.~Lavril, M.~Selvatici, V.~Sivakumar,
  T.~Rockt\"{a}schel, and E.~Grefenstette.
\newblock {TorchBeast: A PyTorch Platform for Distributed RL}.
\newblock \emph{arXiv.}, 2019.

\bibitem[Langford et~al.(2009)Langford, Smola, and Zinkevich]{langford2009slow}
J.~Langford, A.~Smola, and M.~Zinkevich.
\newblock Slow learners are fast.
\newblock \emph{arXiv preprint arXiv:0911.0491}, 2009.

\bibitem[Levine et~al.(2015)Levine, Finn, Darrell, and Abbeel]{Levine15}
S.~Levine, C.~Finn, T.~Darrell, and P.~Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock In \emph{Proc. ICRA}, 2015.

\bibitem[Li et~al.(2019)Li, Liu, Yuan, Chen, Schwing, and Huang]{iswitch}
Y.~Li, I.-J. Liu, Y.~Yuan, D.~Chen, A.~Schwing, and J.~Huang.
\newblock Accelerating distributed reinforcement learning with in-switch
  computing.
\newblock In \emph{Proc. ISCA}, 2019.

\bibitem[Liu et~al.(2019{\natexlab{a}})Liu, Peng, and Schwing]{kf}
I.-J. Liu, J.~Peng, and A.~Schwing.
\newblock Knowledge flow: Improve upon your teachers.
\newblock In \emph{Proc. ICLR}, 2019{\natexlab{a}}.

\bibitem[Liu et~al.(2019{\natexlab{b}})Liu, Yeh, and Schwing]{pic}
I.-J. Liu, R.~A. Yeh, and A.~G. Schwing.
\newblock Pic: Permutation invariant critic for multi-agent deep reinforcement
  learning.
\newblock In \emph{Proc. CoRL}, 2019{\natexlab{b}}.

\bibitem[Luo et~al.(2019)Luo, Solowjow, Wen, Ojea, Agogino, Tamar, and
  Abbeel]{Luo19}
J.~Luo, E.~Solowjow, C.~Wen, J.~A. Ojea, A.~M. Agogino, A.~Tamar, and
  P.~Abbeel.
\newblock Reinforcement learning on variable impedance controller for
  high-precision robotic assembly.
\newblock In \emph{Proc. ICRA}, 2019.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{dqn1}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock In \emph{NeurIPS Deep Learning Workshop}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{dqn2}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, S.~Petersen,
  C.~Beattie, A.~Sadik, I.~Antonoglou, H.~King, D.~Kumaran, D.~Wierstra,
  S.~Legg, and D.~Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 2015.

\bibitem[Mnih et~al.(2016)Mnih, Adri\`a, Badia, Mirza, Graves, Lillicrap,
  Harley, Silver, and Kavukcuoglu]{a3c}
V.~Mnih, Adri\`a, P.~Badia, M.~Mirza, A.~Graves, T.~P. Lillicrap, T.~Harley,
  D.~Silver, and K.~Kavukcuoglu.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{Proc. ICML}, 2016.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Moritz, Jordan, and
  Abbeel]{trpo}
J.~Schulman, S.~Levine, P.~Moritz, M.~I. Jordan, and P.~Abbeel.
\newblock Trust region policy optimization.
\newblock In \emph{Proc. ICML}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{ppo}
J.~Schulman, F.~Wolski, P.~Dhariwal, A.~Radford, and O.~Klimov.
\newblock Proximal policy optimization algorithms.
\newblock In \emph{arxiv}, 2017.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, Chen, Lillicrap, Hui, Sifre, van~den
  Driessche, Graepel, and Hassabis]{Silver17}
D.~Silver, J.~Schrittwieser, K.~Simonyan, I.~Antonoglou, A.~Huang, A.~Guez,
  T.~Hubert, L.~Baker, M.~Lai, A.~Bolton, Y.~Chen, T.~Lillicrap, F.~Hui,
  L.~Sifre, G.~van~den Driessche, T.~Graepel, and D.~Hassabis.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{Nature}, 2017.

\bibitem[Stooke and Abbeel(2019)]{rlpyt}
A.~Stooke and P.~Abbeel.
\newblock rlpyt: A research code base for deep reinforcement learning in
  pytorch.
\newblock In \emph{arXiv.}, 2019.

\bibitem[Sutton and Barto(2018)]{SuttonRL}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock The MIT Press, 2018.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and Mansour]{pg}
R.~S. Sutton, D.~McAllester, S.~Singh, and Y.~Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Proc. NeurIPS}, 2000.

\bibitem[Vinyals et~al.(2019)Vinyals, Babuschkin, Czarnecki, Mathieu, Dudzik,
  Chung, Choi, Powell, Ewalds, Georgiev, Oh, Horgan, Kroiss, Danihelka, Huang,
  Sifre, Cai, Agapiou, Jaderberg, Vezhnevets, Leblond, Pohlen, Dalibard,
  Budden, Sulsky, Molloy, Paine, Gulcehre, Wang, Pfaff, Wu, Ring, Yogatama,
  W\"{u}nsch, McKinney, Smith, Schaul, Lillicrap, Kavukcuoglu, Hassabis, Apps,
  and Silver]{starcraft2}
O.~Vinyals, I.~Babuschkin, W.~M. Czarnecki, M.~Mathieu, A.~Dudzik, J.~Chung,
  D.~H. Choi, R.~Powell, T.~Ewalds, P.~Georgiev, J.~Oh, D.~Horgan, M.~Kroiss,
  I.~Danihelka, A.~Huang, L.~Sifre, T.~Cai, J.~P. Agapiou, M.~Jaderberg, A.~S.
  Vezhnevets, R.~Leblond, T.~Pohlen, V.~Dalibard, D.~Budden, Y.~Sulsky,
  J.~Molloy, T.~L. Paine, C.~Gulcehre, Z.~Wang, T.~Pfaff, Y.~Wu, R.~Ring,
  D.~Yogatama, D.~W\"{u}nsch, K.~McKinney, O.~Smith, T.~Schaul, T.~Lillicrap,
  K.~Kavukcuoglu, D.~Hassabis, C.~Apps, and D.~Silver.
\newblock Grandmaster level in {StarCraft II} using multi-agent reinforcement
  learning.
\newblock \emph{Nature}, 2019.

\bibitem[Wang et~al.(2017)Wang, Bapst, Heess, Mnih, Munos, Kavukcuoglu, and
  de~Freitas]{acer}
Z.~Wang, V.~Bapst, N.~Heess, V.~Mnih, R.~Munos, K.~Kavukcuoglu, and
  N.~de~Freitas.
\newblock Sample efficient actor-critic with experience replay.
\newblock In \emph{Proc. ICLR}, 2017.

\bibitem[Wijmans et~al.(2020)Wijmans, Kadian, Morcos, Lee, Essa, Parikh, Savva,
  and Batra]{ddppo}
E.~Wijmans, A.~Kadian, A.~Morcos, S.~Lee, I.~Essa, D.~Parikh, M.~Savva, and
  D.~Batra.
\newblock {DD-PPO}: Learning near-perfect pointgoal navigators from 2.5 billion
  frames.
\newblock In \emph{Proc. ICLR}, 2020.

\bibitem[Wu et~al.(2017)Wu, Mansimov, Liao, Grosse, and Ba]{acktr}
Y.~Wu, E.~Mansimov, S.~Liao, R.~Grosse, and J.~Ba.
\newblock Scalable trust-region method for deep reinforcement learning using
  {Kronecker}-factored approximation.
\newblock In \emph{Proc. NeurIPS}, 2017.

\end{thebibliography}
