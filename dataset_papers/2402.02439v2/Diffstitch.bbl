\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and Norouzi]{agarwal2020optimistic}
Agarwal, R., Schuurmans, D., and Norouzi, M.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\  104--114. PMLR, 2020.

\bibitem[Ajay et~al.(2022)Ajay, Du, Gupta, Tenenbaum, Jaakkola, and Agrawal]{ajay2022conditional}
Ajay, A., Du, Y., Gupta, A., Tenenbaum, J., Jaakkola, T., and Agrawal, P.
\newblock Is conditional generative modeling all you need for decision-making?
\newblock \emph{arXiv preprint arXiv:2211.15657}, 2022.

\bibitem[Brandfonbrener et~al.(2021)Brandfonbrener, Whitney, Ranganath, and Bruna]{brandfonbrener2021offline}
Brandfonbrener, D., Whitney, W., Ranganath, R., and Bruna, J.
\newblock Offline rl without off-policy evaluation.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 4933--4946, 2021.

\bibitem[Chen et~al.(2021)Chen, Lu, Rajeswaran, Lee, Grover, Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 15084--15097, 2021.

\bibitem[Chen et~al.(2020)Chen, Zhou, Wang, Wang, Wu, and Ross]{chen2020bail}
Chen, X., Zhou, Z., Wang, Z., Wang, C., Wu, Y., and Ross, K.
\newblock Bail: Best-action imitation learning for batch deep reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 18353--18363, 2020.

\bibitem[Fatemi et~al.(2022)Fatemi, Wu, Petch, Nelson, Connolly, Benz, Carnicelli, and Ghassemi]{fatemi2022semi}
Fatemi, M., Wu, M., Petch, J., Nelson, W., Connolly, S.~J., Benz, A., Carnicelli, A., and Ghassemi, M.
\newblock Semi-markov offline reinforcement learning for healthcare.
\newblock In \emph{Conference on Health, Inference, and Learning}, pp.\  119--137. PMLR, 2022.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{fu2020d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{fujimoto2021minimalist}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 20132--20145, 2021.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{International conference on machine learning}, pp.\  1861--1870. PMLR, 2018.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021offline}
Janner, M., Li, Q., and Levine, S.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 1273--1286, 2021.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and Levine]{janner2022planning}
Janner, M., Du, Y., Tenenbaum, J.~B., and Levine, S.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock \emph{arXiv preprint arXiv:2205.09991}, 2022.

\bibitem[Jaques et~al.(2020)Jaques, Shen, Ghandeharioun, Ferguson, Lapedriza, Jones, Gu, and Picard]{jaques2020human}
Jaques, N., Shen, J.~H., Ghandeharioun, A., Ferguson, C., Lapedriza, A., Jones, N., Gu, S.~S., and Picard, R.
\newblock Human-centric dialog training via offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2010.05848}, 2020.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and Joachims]{kidambi2020morel}
Kidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims, T.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems}, 33:\penalty0 21810--21823, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kostrikov et~al.(2021)Kostrikov, Nair, and Levine]{kostrikov2021offline}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock \emph{arXiv preprint arXiv:2110.06169}, 2021.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Liang et~al.(2023)Liang, Mu, Ding, Ni, Tomizuka, and Luo]{liang2023adaptdiffuser}
Liang, Z., Mu, Y., Ding, M., Ni, F., Tomizuka, M., and Luo, P.
\newblock Adaptdiffuser: Diffusion models as adaptive self-evolving planners.
\newblock \emph{arXiv preprint arXiv:2302.01877}, 2023.

\bibitem[Lu et~al.(2023)Lu, Ball, and Parker-Holder]{lu2023synthetic}
Lu, C., Ball, P.~J., and Parker-Holder, J.
\newblock Synthetic experience replay.
\newblock \emph{arXiv preprint arXiv:2303.06614}, 2023.

\bibitem[Lyu et~al.(2022)Lyu, Li, and Lu]{lyu2022double}
Lyu, J., Li, X., and Lu, Z.
\newblock Double check your state before trusting it: Confidence-aware bidirectional offline model-based imagination.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 38218--38231, 2022.

\bibitem[Matsushima et~al.(2020)Matsushima, Furuta, Matsuo, Nachum, and Gu]{matsushima2020deployment}
Matsushima, T., Furuta, H., Matsuo, Y., Nachum, O., and Gu, S.
\newblock Deployment-efficient reinforcement learning via model-based offline optimization.
\newblock \emph{arXiv preprint arXiv:2006.03647}, 2020.

\bibitem[Prudencio et~al.(2023)Prudencio, Maximo, and Colombini]{prudencio2023survey}
Prudencio, R.~F., Maximo, M.~R., and Colombini, E.~L.
\newblock A survey on offline reinforcement learning: Taxonomy, review, and open problems.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems}, 2023.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and Brox]{ronneberger2015u}
Ronneberger, O., Fischer, P., and Brox, T.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18}, pp.\  234--241. Springer, 2015.

\bibitem[Shi et~al.(2021)Shi, Chen, Chen, and Li]{shi2021offline}
Shi, T., Chen, D., Chen, K., and Li, Z.
\newblock Offline reinforcement learning for autonomous driving with safety and exploration enhancement.
\newblock \emph{arXiv preprint arXiv:2110.07067}, 2021.

\bibitem[Siegel et~al.(2020)Siegel, Springenberg, Berkenkamp, Abdolmaleki, Neunert, Lampe, Hafner, Heess, and Riedmiller]{siegel2020keep}
Siegel, N.~Y., Springenberg, J.~T., Berkenkamp, F., Abdolmaleki, A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M.
\newblock Keep doing what worked: Behavioral modelling priors for offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2002.08396}, 2020.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tarasov et~al.(2022)Tarasov, Nikulin, Akimov, Kurenkov, and Kolesnikov]{tarasov2022corl}
Tarasov, D., Nikulin, A., Akimov, D., Kurenkov, V., and Kolesnikov, S.
\newblock Corl: Research-oriented deep offline reinforcement learning library.
\newblock \emph{arXiv preprint arXiv:2210.07105}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N., Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Wang et~al.(2021)Wang, Li, Jiang, Zhu, Li, and Zhang]{wang2021offline}
Wang, J., Li, W., Jiang, H., Zhu, G., Li, S., and Zhang, C.
\newblock Offline reinforcement learning with reverse model-based imagination.
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 29420--29432, 2021.

\bibitem[Wang et~al.(2022)Wang, Zhao, Luo, Ren, Zhang, and Li]{wang2022bootstrapped}
Wang, K., Zhao, H., Luo, X., Ren, K., Zhang, W., and Li, D.
\newblock Bootstrapped transformer for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 34748--34761, 2022.

\bibitem[Wang et~al.(2020)Wang, Novikov, Zolna, Merel, Springenberg, Reed, Shahriari, Siegel, Gulcehre, Heess, et~al.]{wang2020critic}
Wang, Z., Novikov, A., Zolna, K., Merel, J.~S., Springenberg, J.~T., Reed, S.~E., Shahriari, B., Siegel, N., Gulcehre, C., Heess, N., et~al.
\newblock Critic regularized regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 7768--7778, 2020.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and Ma]{yu2020mopo}
Yu, T., Thomas, G., Yu, L., Ermon, S., Zou, J.~Y., Levine, S., Finn, C., and Ma, T.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 14129--14142, 2020.

\bibitem[Zhang et~al.(2023)Zhang, Lyu, Ma, Yan, Yang, Wan, and Li]{zhang2023uncertainty}
Zhang, J., Lyu, J., Ma, X., Yan, J., Yang, J., Wan, L., and Li, X.
\newblock Uncertainty-driven trajectory truncation for data augmentation in offline reinforcement learning.
\newblock In \emph{ECAI 2023}, pp.\  3018--3025. IOS Press, 2023.

\end{thebibliography}
