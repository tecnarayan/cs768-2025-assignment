@article{longlora,
  title={LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models},
  author={Yukang Chen and Shengju Qian and Haotian Tang and Xin Lai and Zhijian Liu and Song Han and Jiaya Jia},
  journal={arXiv:2309.12307},
  year={2023}
}

@misc{liu2023scaling,
      title={Scaling Laws of RoPE-based Extrapolation}, 
      author={Xiaoran Liu and Hang Yan and Shuo Zhang and Chenxin An and Xipeng Qiu and Dahua Lin},
      year={2023},
      eprint={2310.05209},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{pg19,
  author       = {Jack W. Rae and
                  Anna Potapenko and
                  Siddhant M. Jayakumar and
                  Chloe Hillier and
                  Timothy P. Lillicrap},
  title        = {Compressive Transformers for Long-Range Sequence Modelling},
  booktitle    = {8th International Conference on Learning Representations, {ICLR} 2020,
                  Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher    = {OpenReview.net},
  year         = {2020},
  url          = {https://openreview.net/forum?id=SylKikSYDH},
  timestamp    = {Thu, 07 May 2020 17:11:48 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/RaePJHL20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{rerope2023,
  title={Rectified Rotary Position Embeddings},
  author={Jianlin Su},
  year={2023},
  howpublished={\url{https://github.com/bojone/rerope}},
}

@misc{saadfalcon2023pdftriage,
      title={PDFTriage: Question Answering over Long, Structured Documents}, 
      author={Jon Saad-Falcon and Joe Barrow and Alexa Siu and Ani Nenkova and David Seunghyun Yoon and Ryan A. Rossi and Franck Dernoncourt},
      year={2023},
      eprint={2309.08872},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{bai2023qwen,
      title={Qwen Technical Report}, 
      author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
      year={2023},
      eprint={2309.16609},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{chen2023clex,
      title={CLEX: Continuous Length Extrapolation for Large Language Models}, 
      author={Guanzheng Chen and Xin Li and Zaiqiao Meng and Shangsong Liang and Lidong Bing},
      year={2023},
      eprint={2310.16450},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{llama2long,
  author       = {Wenhan Xiong and
                  Jingyu Liu and
                  Igor Molybog and
                  Hejia Zhang and
                  Prajjwal Bhargava and
                  Rui Hou and
                  Louis Martin and
                  Rashi Rungta and
                  Karthik Abinav Sankararaman and
                  Barlas Oguz and
                  Madian Khabsa and
                  Han Fang and
                  Yashar Mehdad and
                  Sharan Narang and
                  Kshitiz Malik and
                  Angela Fan and
                  Shruti Bhosale and
                  Sergey Edunov and
                  Mike Lewis and
                  Sinong Wang and
                  Hao Ma},
  title        = {Effective Long-Context Scaling of Foundation Models},
  journal      = {CoRR},
  volume       = {abs/2309.16039},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2309.16039},
  doi          = {10.48550/ARXIV.2309.16039},
  eprinttype    = {arXiv},
  eprint       = {2309.16039},
  timestamp    = {Tue, 17 Oct 2023 13:50:54 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2309-16039.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{llama,
      title={LLaMA: Open and Efficient Foundation Language Models}, 
      author={Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
      year={2023},
      eprint={2302.13971},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{rula2023procedural,
      title={Procedural Text Mining with Large Language Models}, 
      author={Anisa Rula and Jennifer D'Souza},
      year={2023},
      eprint={2310.03376},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wei2023leveraging,
      title={Leveraging Large Language Models to Power Chatbots for Collecting User Self-Reported Data}, 
      author={Jing Wei and Sungdong Kim and Hyunhoon Jung and Young-Ho Kim},
      year={2023},
      eprint={2301.05843},
      archivePrefix={arXiv},
      primaryClass={cs.HC}
}

@misc{gpt-4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@inproceedings{Lee_2023,
   title={Prompted LLMs as Chatbot Modules for Long Open-domain Conversation},
   url={http://dx.doi.org/10.18653/v1/2023.findings-acl.277},
   DOI={10.18653/v1/2023.findings-acl.277},
   booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
   publisher={Association for Computational Linguistics},
   author={Lee, Gibbeum and Hartmann, Volker and Park, Jongho and Papailiopoulos, Dimitris and Lee, Kangwook},
   year={2023} }


@article{mbpp,
  title = {Program Synthesis with Large Language Models},
  author = {Austin, Jacob and Odena, Augustus and Nye, Maxwell I. and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie J. and Terry, Michael and Le, Quoc V. and Sutton, Charles},
  biburl = {https://dblp.org/rec/journals/corr/abs-2108-07732.bib},
  journal = {arXiv:abs/2108.07732},
  year = 2021
}

@misc{focused,
      title={Focused Transformer: Contrastive Training for Context Scaling}, 
      author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
      year={2023},
      eprint={2307.03170},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{oasis,
  title={OpenAssistant Conversations--Democratizing Large Language Model Alignment},
  author={K{\"o}pf, Andreas and Kilcher, Yannic and von R{\"u}tte, Dimitri and Anagnostidis, Sotiris and Tam, Zhi-Rui and Stevens, Keith and Barhoum, Abdullah and Duc, Nguyen Minh and Stanley, Oliver and Nagyfi, Rich{\'a}rd and others},
  journal={arXiv preprint arXiv:2304.07327},
  year={2023}
}

@article{xgen,
  title={Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length},
  author={Nijkamp, Erik and Xie, Tian and Hayashi, Hiroaki and Pang, Bo and Xia, Congying and Xing, Chen and Vig, Jesse and Yavuz, Semih and Laban, Philippe and Krause, Ben and others},
  journal={Salesforce AI Research Blog},
  year={2023}
}


@article{longchat,
  title={How long can open-source llms truly promise on context length},
  author={Li, Dacheng and Shao, Rulin and Xie, Anze and Sheng, Ying and Zheng, Lianmin and Gonzalez, Joseph E and Stoica, Ion and Ma, Xuezhe and Zhang, Hao},
  year={2023}
}


@online{vicuna,
    author    = {LMSYS},
    title     = {Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality},
    year      = {2023},
    url       = {https://lmsys.org/blog/2023-03-30-vicuna/}
}


@online{together,
    author    = {Together},
    title     = {Llama-2-7B-32K-Instruct — and fine-tuning for Llama-2 models with Together API},
    year      = {2023},
    url       = {https://together.ai/blog/llama-2-7b-32k-instruct}
}

@online{MPT7b,
    author    = {MosaicML},
    title     = {Introducing MPT-7B: A New Standard for Open-Source,
    ly Usable LLMs},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-7b}
}

@online{MPT30b,
    author    = {MosaicML},
    title     = {Introducing MPT-30B: Raising the bar
for open-source foundation models},
    year      = {2023},
    url       = {www.mosaicml.com/blog/mpt-30b},
    note      = {Accessed: 2023-06-22},
    urldate   = {2023-06-22}
}

@inproceedings{megatron,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}

@article{blocksparse,
  title={Generating long sequences with sparse transformers},
  author={Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1904.10509},
  year={2019}
}

@article{gsm8k,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{math,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{natural_questions,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{piqa,
  title={Piqa: Reasoning about physical commonsense in natural language},
  author={Bisk, Yonatan and Zellers, Rowan and Gao, Jianfeng and Choi, Yejin and others},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={05},
  pages={7432--7439},
  year={2020}
}


@article{siqa,
  title={Socialiqa: Commonsense reasoning about social interactions},
  author={Sap, Maarten and Rashkin, Hannah and Chen, Derek and LeBras, Ronan and Choi, Yejin},
  journal={arXiv preprint arXiv:1904.09728},
  year={2019}
}

@article{triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}


@article{winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}


@article{OpenbookQA,
  title={Can a suit of armor conduct electricity? a new dataset for open book question answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  journal={arXiv preprint arXiv:1809.02789},
  year={2018}
}

@article{commonsenseqa,
  title={Commonsenseqa: A question answering challenge targeting commonsense knowledge},
  author={Talmor, Alon and Herzig, Jonathan and Lourie, Nicholas and Berant, Jonathan},
  journal={arXiv preprint arXiv:1811.00937},
  year={2018}
}

@article{arc,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@online{claude,
  title = {Introducing {{100K Context Windows}}},
  author = {{Anthropic}},
  journal = {Anthropic},
  year = 2023,
  url = {https://www.anthropic.com/index/100k-context-windows}
}

@misc{long_llama,
      title={Focused Transformer: Contrastive Training for Context Scaling}, 
      author={Szymon Tworkowski and Konrad Staniszewski and Mikołaj Pacek and Yuhuai Wu and Henryk Michalewski and Piotr Miłoś},
      year={2023},
      eprint={2307.03170},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{flashattention,
  author       = {Tri Dao and
                  Daniel Y. Fu and
                  Stefano Ermon and
                  Atri Rudra and
                  Christopher R{\'{e}}},
  title        = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  booktitle    = {NeurIPS},
  year         = {2022}
}


@misc{dao2023flashattention2,
      title={FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning}, 
      author={Tri Dao},
      year={2023},
      eprint={2307.08691},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

### PE references
@misc{pi,
      title={Extending Context Window of Large Language Models via Positional Interpolation}, 
      author={Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
      year={2023},
      eprint={2306.15595},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{rope,
      title={RoFormer: Enhanced Transformer with Rotary Position Embedding}, 
      author={Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
      year={2022},
      eprint={2104.09864},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xpos,
      title={A Length-Extrapolatable Transformer}, 
      author={Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},
      year={2022},
      eprint={2212.10554},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{alibi,
      title={Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation}, 
      author={Ofir Press and Noah A. Smith and Mike Lewis},
      year={2022},
      eprint={2108.12409},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{transformers,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{transformers_1,
  added-at = {2020-07-14T16:37:42.000+0200},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  biburl = {https://www.bibsonomy.org/bibtex/273ced32c0d4588eb95b6986dc2c8147c/jonaskaiser},
  interhash = {5c343ed9a31ac52fd17a898f72af228f},
  intrahash = {73ced32c0d4588eb95b6986dc2c8147c},
  keywords = {final thema:transformer},
  timestamp = {2020-07-14T16:49:42.000+0200},
  title = {Improving language understanding by generative pre-training},
  year = 2018
}

@misc{transformers_2,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{humaneval,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{passkey,
  title={Landmark Attention: Random-Access Infinite Context Length for Transformers},
  author={Mohtashami, Amirkeivan and Jaggi, Martin},
  journal={arXiv preprint arXiv:2305.16300},
  year={2023}
}


@article{openai_booksum,
  author       = {Jeff Wu and
                  Long Ouyang and
                  Daniel M. Ziegler and
                  Nisan Stiennon and
                  Ryan Lowe and
                  Jan Leike and
                  Paul F. Christiano},
  title        = {Recursively Summarizing Books with Human Feedback},
  journal      = {CoRR},
  volume       = {abs/2109.10862},
  year         = {2021}
}

### Datasets & tasks


@article{leval,
  title={L-Eval: Instituting Standardized Evaluation for Long Context Language Models},
  author={An, Chenxin and Gong, Shansan and Zhong, Ming and Li, Mukai and Zhang, Jun and Kong, Lingpeng and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2307.11088},
  year={2023}
}

@misc{codellama,
      title={Code Llama: Open Foundation Models for Code}, 
      author={Baptiste Rozière and Jonas Gehring and Fabian Gloeckle and Sten Sootla and Itai Gat and Xiaoqing Ellen Tan and Yossi Adi and Jingyu Liu and Tal Remez and Jérémy Rapin and Artyom Kozhevnikov and Ivan Evtimov and Joanna Bitton and Manish Bhatt and Cristian Canton Ferrer and Aaron Grattafiori and Wenhan Xiong and Alexandre Défossez and Jade Copet and Faisal Azhar and Hugo Touvron and Louis Martin and Nicolas Usunier and Thomas Scialom and Gabriel Synnaeve},
      year={2023},
      eprint={2308.12950},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yarn,
      title={YaRN: Efficient Context Window Extension of Large Language Models}, 
      author={Bowen Peng and Jeffrey Quesnelle and Honglu Fan and Enrico Shippole},
      year={2023},
      eprint={2309.00071},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{c4,
      title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer}, 
      author={Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
      year={2020},
      eprint={1910.10683},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{pile,
      title={The Pile: An 800GB Dataset of Diverse Text for Language Modeling}, 
      author={Leo Gao and Stella Biderman and Sid Black and Laurence Golding and Travis Hoppe and Charles Foster and Jason Phang and Horace He and Anish Thite and Noa Nabeshima and Shawn Presser and Connor Leahy},
      year={2020},
      eprint={2101.00027},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{mmlu,
      title={Measuring Massive Multitask Language Understanding}, 
      author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
      year={2021},
      eprint={2009.03300},
      archivePrefix={arXiv},
      primaryClass={cs.CY}
}

@misc{tqa,
      title={Textbook Question Answering with Multi-modal Context Graph Understanding and Self-supervised Open-set Comprehension}, 
      author={Daesik Kim and Seonhoon Kim and Nojun Kwak},
      year={2019},
      eprint={1811.00232},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{hellaswag,
      title={HellaSwag: Can a Machine Really Finish Your Sentence?}, 
      author={Rowan Zellers and Ari Holtzman and Yonatan Bisk and Ali Farhadi and Yejin Choi},
      year={2019},
      eprint={1905.07830},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{govreport,
    title = "Efficient Attentions for Long Document Summarization",
    author = "Huang, Luyang  and
      Cao, Shuyang  and
      Parulian, Nikolaus  and
      Ji, Heng  and
      Wang, Lu",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.112",
    doi = "10.18653/v1/2021.naacl-main.112",
    pages = "1419--1436"
}

@inproceedings{summscreen,
    title = "{S}umm{S}creen: A Dataset for Abstractive Screenplay Summarization",
    author = "Chen, Mingda  and
      Chu, Zewei  and
      Wiseman, Sam  and
      Gimpel, Kevin",
    booktitle = "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.acl-long.589",
    doi = "10.18653/v1/2022.acl-long.589",
    pages = "8602--8615"
}

@inproceedings{qmsum,
    title = "{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization",
    author = "Zhong, Ming  and
      Yin, Da  and
      Yu, Tao  and
      Zaidi, Ahmad  and
      Mutuma, Mutethia  and
      Jha, Rahul  and
      Awadallah, Ahmed Hassan  and
      Celikyilmaz, Asli  and
      Liu, Yang  and
      Qiu, Xipeng  and
      Radev, Dragomir",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.472",
    doi = "10.18653/v1/2021.naacl-main.472",
    pages = "5905--5921"
}

@inproceedings{qasper,
    title = "A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers",
    author = "Dasigi, Pradeep  and
      Lo, Kyle  and
      Beltagy, Iz  and
      Cohan, Arman  and
      Smith, Noah A.  and
      Gardner, Matt",
    booktitle = "Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.naacl-main.365",
    doi = "10.18653/v1/2021.naacl-main.365",
    pages = "4599--4610"
}

@article{narrativeqa,
    title = "The {N}arrative{QA} Reading Comprehension Challenge",
    author = "Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
      Schwarz, Jonathan  and
      Blunsom, Phil  and
      Dyer, Chris  and
      Hermann, Karl Moritz  and
      Melis, G{\'a}bor  and
      Grefenstette, Edward",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/Q18-1023",
    doi = "10.1162/tacl_a_00023",
    pages = "317--328"
}

@inproceedings{quality,
    title = "{Q}u{ALITY}: Question Answering with Long Input Texts, Yes!",
    author = "Pang, Richard Yuanzhe  and
      Parrish, Alicia  and
      Joshi, Nitish  and
      Nangia, Nikita  and
      Phang, Jason  and
      Chen, Angelica  and
      Padmakumar, Vishakh  and
      Ma, Johnny  and
      Thompson, Jana  and
      He, He  and
      Bowman, Samuel",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.391",
    doi = "10.18653/v1/2022.naacl-main.391",
    pages = "5336--5358"
}

@inproceedings{contractnli,
    title = "{C}ontract{NLI}: A Dataset for Document-level Natural Language Inference for Contracts",
    author = "Koreeda, Yuta  and
      Manning, Christopher",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.164",
    doi = "10.18653/v1/2021.findings-emnlp.164",
    pages = "1907--1919"
}

@inproceedings{squality,
    title = "{SQ}u{ALITY}: Building a Long-Document Summarization Dataset the Hard Way",
    author = "Wang, Alex  and
      Pang, Richard Yuanzhe  and
      Chen, Angelica  and
      Phang, Jason  and
      Bowman, Samuel R.",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.75",
    pages = "1139--1156"
}

@misc{musique,
      title={MuSiQue: Multihop Questions via Single-hop Question Composition}, 
      author={Harsh Trivedi and Niranjan Balasubramanian and Tushar Khot and Ashish Sabharwal},
      year={2022},
      eprint={2108.00573},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{booksum,
    title = "{BOOKSUM}: A Collection of Datasets for Long-form Narrative Summarization",
    author = "Kryscinski, Wojciech  and
      Rajani, Nazneen  and
      Agarwal, Divyansh  and
      Xiong, Caiming  and
      Radev, Dragomir",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2022",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.findings-emnlp.488",
    pages = "6536--6558"
}

@article{spacedigest,
    title = "Extractive Opinion Summarization in Quantized Transformer Spaces",
    author = "Angelidis, Stefanos  and
      Amplayo, Reinald Kim  and
      Suhara, Yoshihiko  and
      Wang, Xiaolan  and
      Lapata, Mirella",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "9",
    year = "2021",
    address = "Cambridge, MA",
    publisher = "MIT Press",
    url = "https://aclanthology.org/2021.tacl-1.17",
    doi = "10.1162/tacl_a_00366",
    pages = "277--293"
}

@inproceedings{scrolls,
    title = "{SCROLLS}: Standardized {C}ompa{R}ison Over Long Language Sequences",
    author = "Shaham, Uri  and
      Segal, Elad  and
      Ivgi, Maor  and
      Efrat, Avia  and
      Yoran, Ori  and
      Haviv, Adi  and
      Gupta, Ankit  and
      Xiong, Wenhan  and
      Geva, Mor  and
      Berant, Jonathan  and
      Levy, Omer",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.823",
    pages = "12007--12021"
}

@misc{zeroscrolls,
      title={ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding}, 
      author={Uri Shaham and Maor Ivgi and Avia Efrat and Jonathan Berant and Omer Levy},
      year={2023},
      eprint={2305.14196},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

### related work 
@misc{lost_in_the_middle,
      title={Lost in the Middle: How Language Models Use Long Contexts}, 
      author={Nelson F. Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
      year={2023},
      eprint={2307.03172},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{long_net,
      title={LongNet: Scaling Transformers to 1,000,000,000 Tokens}, 
      author={Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Furu Wei},
      year={2023},
      eprint={2307.02486},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@ARTICLE{n_gram,
  author={Bahl, Lalit R. and Jelinek, Frederick and Mercer, Robert L.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Maximum Likelihood Approach to Continuous Speech Recognition}, 
  year={1983},
  volume={PAMI-5},
  number={2},
  pages={179-190},
  doi={10.1109/TPAMI.1983.4767370}
}

@ARTICLE{hmm,
  author={Rabiner, L. and Juang, B.},
  journal={IEEE ASSP Magazine}, 
  title={An introduction to hidden Markov models}, 
  year={1986},
  volume={3},
  number={1},
  pages={4-16},
  doi={10.1109/MASSP.1986.1165342}
}

@inproceedings{ffn,
 author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {T. Leen and T. Dietterich and V. Tresp},
 pages = {},
 publisher = {MIT Press},
 title = {A Neural Probabilistic Language Model},
 url = {https://proceedings.neurips.cc/paper_files/paper/2000/file/728f206c2a01bf572b5940d7d9a8fa4c-Paper.pdf},
 volume = {13},
 year = {2000}
}

@article{rnn,
title = {Finding structure in time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
year = {1990},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(90)90002-E},
url = {https://www.sciencedirect.com/science/article/pii/036402139090002E},
author = {Jeffrey L. Elman},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.}
}

@Article{lstm,
  author      = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal     = {Neural Computation},
  title       = {Long Short-Term Memory},
  year        = {1997},
  number      = {8},
  pages       = {1735--1780},
  volume      = {9},
  optabstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  optdoi      = {10.1162/neco.1997.9.8.1735},
  opteprint   = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
  opturl      = {http://dx.doi.org/10.1162/neco.1997.9.8.1735},
}

@misc{lstm_1,
      title={Generating Sequences With Recurrent Neural Networks}, 
      author={Alex Graves},
      year={2014},
      eprint={1308.0850},
      archivePrefix={arXiv},
      primaryClass={cs.NE}
}

@inproceedings{matthew,
  author       = {Matthew V. Mahoney},
  editor       = {Jim Hendler and
                  Devika Subramanian},
  title        = {Text Compression as a Test for Artificial Intelligence},
  booktitle    = {Proceedings of the Sixteenth National Conference on Artificial Intelligence
                  and Eleventh Conference on Innovative Applications of Artificial Intelligence,
                  July 18-22, 1999, Orlando, Florida, {USA}},
  pages        = {970},
  publisher    = {{AAAI} Press / The {MIT} Press},
  year         = {1999},
  url          = {http://www.aaai.org/Library/AAAI/1999/aaai99-177.php},
  timestamp    = {Wed, 10 Feb 2021 08:44:03 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/Mahoney99.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{retrieval-lm-tutorial,
  author    = { Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi },
  title     = { ACL 2023 Tutorial: Retrieval-based LMs and Applications },
  journal   = { ACL 2023 },
  year      = { 2023 },
}

@article{hartvigsen2022toxigen,
  title={Toxigen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  journal={arXiv preprint arXiv:2203.09509},
  year={2022}
}

@article{lin2021truthfulqa,
  title={Truthfulqa: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  journal={arXiv preprint arXiv:2109.07958},
  year={2021}
}

@article{ji2023survey,
  title={Survey of hallucination in natural language generation},
  author={Ji, Ziwei and Lee, Nayeon and Frieske, Rita and Yu, Tiezheng and Su, Dan and Xu, Yan and Ishii, Etsuko and Bang, Ye Jin and Madotto, Andrea and Fung, Pascale},
  journal={ACM Computing Surveys},
  volume={55},
  number={12},
  pages={1--38},
  year={2023},
  publisher={ACM New York, NY}
}

@inproceedings{dhamala2021bold,
  title={Bold: Dataset and metrics for measuring biases in open-ended language generation},
  author={Dhamala, Jwala and Sun, Tony and Kumar, Varun and Krishna, Satyapriya and Pruksachatkun, Yada and Chang, Kai-Wei and Gupta, Rahul},
  booktitle={Proceedings of the 2021 ACM conference on fairness, accountability, and transparency},
  pages={862--872},
  year={2021}
}

@article{greshake2023not,
  title={Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection},
  author={Greshake, Kai and Abdelnabi, Sahar and Mishra, Shailesh and Endres, Christoph and Holz, Thorsten and Fritz, Mario},
  journal={arXiv preprint arXiv:2302.12173},
  year={2023}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{falcon40b,
  title={{Falcon-40B}: an open large language model with state-of-the-art performance},
  author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme},
  year={2023}
}

@misc{fixedNTK,
  title={NTK-Aware Scaled RoPE allows LLaMA models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.
},
  author={LocalLLaMA}, 
  year={2023}, 
  month={June},
  url={https://www.reddit.com/r/LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/}
}

@misc{dynamicNTK,
  title={Dynamically Scaled RoPE further increases performance of long context LLaMA with zero fine-tuning},
  author={LocalLLaMA}, 
  year={2023}, 
  month={July},
  url={https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/}
}



@misc{kaplan2020scaling,
      title={Scaling Laws for Neural Language Models}, 
      author={Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
      year={2020},
      eprint={2001.08361},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{hutto2014vader,
  title={Vader: A parsimonious rule-based model for sentiment analysis of social media text},
  author={Hutto, Clayton and Gilbert, Eric},
  booktitle={Proceedings of the international AAAI conference on web and social media},
  volume={8},
  number={1},
  pages={216--225},
  year={2014}
}

@misc{ratner2023parallel,
      title={Parallel Context Windows for Large Language Models}, 
      author={Nir Ratner and Yoav Levine and Yonatan Belinkov and Ori Ram and Inbal Magar and Omri Abend and Ehud Karpas and Amnon Shashua and Kevin Leyton-Brown and Yoav Shoham},
      year={2023},
      eprint={2212.10947},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{han2023lminfinite,
      title={LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models}, 
      author={Chi Han and Qifan Wang and Wenhan Xiong and Yu Chen and Heng Ji and Sinong Wang},
      year={2023},
      eprint={2308.16137},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xiao2023efficient,
      title={Efficient Streaming Language Models with Attention Sinks}, 
      author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
      year={2023},
      eprint={2309.17453},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@article{Zhang2024SoaringF4,
  title={Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon},
  author={Peitian Zhang and Zheng Liu and Shitao Xiao and Ninglu Shao and Qiwei Ye and Zhicheng Dou},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.03462},
  url={https://api.semanticscholar.org/CorpusID:266844488}
}

@article{Qin2024LightningAA,
  title={Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
  author={Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
  journal={ArXiv},
  year={2024},
  volume={abs/2401.04658},
  url={https://api.semanticscholar.org/CorpusID:266900042}
}

@misc{chi2023dissecting,
      title={Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis}, 
      author={Ta-Chung Chi and Ting-Han Fan and Alexander I. Rudnicky and Peter J. Ramadge},
      year={2023},
      eprint={2212.10356},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}


@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: an Open Dataset for Training Large Language Models},
  month = October,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}


@misc{hu2021lora,
      title={LoRA: Low-Rank Adaptation of Large Language Models}, 
      author={Edward J. Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
      year={2021},
      eprint={2106.09685},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{wang2024learning,
      title={Learning to Retrieve In-Context Examples for Large Language Models}, 
      author={Liang Wang and Nan Yang and Furu Wei},
      year={2024},
      eprint={2307.07164},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{ye2023compositional,
  title={Compositional exemplars for in-context learning},
  author={Ye, Jiacheng and Wu, Zhiyong and Feng, Jiangtao and Yu, Tao and Kong, Lingpeng},
  journal={arXiv preprint arXiv:2302.05698},
  year={2023}
}

@article{robertson2009probabilistic,
  title={The probabilistic relevance framework: BM25 and beyond},
  author={Robertson, Stephen and Zaragoza, Hugo and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={3},
  number={4},
  pages={333--389},
  year={2009},
  publisher={Now Publishers, Inc.}
}


@misc{song2023zebra,
      title={Zebra: Extending Context Window with Layerwise Grouped Local-Global Attention}, 
      author={Kaiqiang Song and Xiaoyang Wang and Sangwoo Cho and Xiaoman Pan and Dong Yu},
      year={2023},
      eprint={2312.08618},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{dai2019transformerxl,
      title={Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context}, 
      author={Zihang Dai and Zhilin Yang and Yiming Yang and Jaime Carbonell and Quoc V. Le and Ruslan Salakhutdinov},
      year={2019},
      eprint={1901.02860},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{shaw2018selfattention,
      title={Self-Attention with Relative Position Representations}, 
      author={Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
      year={2018},
      eprint={1803.02155},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{selfextend,
      title={LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning}, 
      author={Hongye Jin and Xiaotian Han and Jingfeng Yang and Zhimeng Jiang and Zirui Liu and Chia-Yuan Chang and Huiyuan Chen and Xia Hu},
      year={2024},
      eprint={2401.01325},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}


@misc{he2024stones,
      title={Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation}, 
      author={Zhenyu He and Guhao Feng and Shengjie Luo and Kai Yang and Di He and Jingjing Xu and Zhi Zhang and Hongxia Yang and Liwei Wang},
      year={2024},
      eprint={2401.16421},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{ruoss2023randomized,
      title={Randomized Positional Encodings Boost Length Generalization of Transformers}, 
      author={Anian Ruoss and Grégoire Delétang and Tim Genewein and Jordi Grau-Moya and Róbert Csordás and Mehdi Bennani and Shane Legg and Joel Veness},
      year={2023},
      eprint={2305.16843},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{kazemnejad2023impact,
      title={The Impact of Positional Encoding on Length Generalization in Transformers}, 
      author={Amirhossein Kazemnejad and Inkit Padhi and Karthikeyan Natesan Ramamurthy and Payel Das and Siva Reddy},
      year={2023},
      eprint={2305.19466},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{chowdhury2023monotonic,
      title={Monotonic Location Attention for Length Generalization}, 
      author={Jishnu Ray Chowdhury and Cornelia Caragea},
      year={2023},
      eprint={2305.20019},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{li2023functional,
      title={Functional Interpolation for Relative Positions Improves Long Context Transformers}, 
      author={Shanda Li and Chong You and Guru Guruganesh and Joshua Ainslie and Santiago Ontanon and Manzil Zaheer and Sumit Sanghai and Yiming Yang and Sanjiv Kumar and Srinadh Bhojanapalli},
      year={2023},
      eprint={2310.04418},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@misc{zhu2023pose,
      title={PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training}, 
      author={Dawei Zhu and Nan Yang and Liang Wang and Yifan Song and Wenhao Wu and Furu Wei and Sujian Li},
      year={2023},
      eprint={2309.10400},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@article{Zhang2023LinearAV,
  title={Linear Attention via Orthogonal Memory},
  author={Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
  journal={ArXiv},
  year={2023},
  volume={abs/2312.11135},
  url={https://api.semanticscholar.org/CorpusID:266359128}
}


@misc{lv2024longwanjuan,
      title={LongWanjuan: Towards Systematic Measurement for Long Text Quality}, 
      author={Kai Lv and Xiaoran Liu and Qipeng Guo and Hang Yan and Conghui He and Xipeng Qiu and Dahua Lin},
      year={2024},
      eprint={2402.13583},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{xiao2024infllm,
      title={InfLLM: Unveiling the Intrinsic Capacity of LLMs for Understanding Extremely Long Sequences with Training-Free Memory}, 
      author={Chaojun Xiao and Pengle Zhang and Xu Han and Guangxuan Xiao and Yankai Lin and Zhengyan Zhang and Zhiyuan Liu and Song Han and Maosong Sun},
      year={2024},
      eprint={2402.04617},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}