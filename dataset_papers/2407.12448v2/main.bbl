\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ajay et~al.(2021)Ajay, Kumar, Agrawal, Levine, and Nachum]{opal}
Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum, O.
\newblock {OPAL:} offline primitive discovery for accelerating offline reinforcement learning.
\newblock In \emph{Proceedings of the 9th International Conference on Learning Representations (ICLR'21)}, Virtual Event, 2021.

\bibitem[Ajay et~al.(2023)Ajay, Du, Gupta, Tenenbaum, Jaakkola, and Agrawal]{DecisionDiffuser}
Ajay, A., Du, Y., Gupta, A., Tenenbaum, J.~B., Jaakkola, T.~S., and Agrawal, P.
\newblock Is conditional generative modeling all you need for decision making?
\newblock In \emph{The Eleventh International Conference on Learning Representations (ICLR'23)}, Kigali, Rwanda, 2023.

\bibitem[Ball et~al.(2023)Ball, Smith, Kostrikov, and Levine]{efficientonline}
Ball, P.~J., Smith, L.~M., Kostrikov, I., and Levine, S.
\newblock Efficient online reinforcement learning with offline data.
\newblock In \emph{International Conference on Machine Learning (ICML'23)}, Honolulu, USA, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Lu, Ying, Su, and Zhu]{sfbc}
Chen, H., Lu, C., Ying, C., Su, H., and Zhu, J.
\newblock Offline reinforcement learning via high-fidelity generative behavior modeling.
\newblock In \emph{The 11th International Conference on Learning Representations (ICLR'23)}, Kigali, Rwanda, 2023{\natexlab{a}}.

\bibitem[Chen \& Jiang(2019)Chen and Jiang]{fqi}
Chen, J. and Jiang, N.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning (ICML'19)}, Long Beach, USA, 2019.

\bibitem[Chen et~al.(2024)Chen, Liu, Liu, Jiang, Xu, and Yu]{foda}
Chen, R., Liu, X., Liu, T., Jiang, S., Xu, F., and Yu, Y.
\newblock Foresight distribution adjustment for off-policy reinforcement learning.
\newblock In \emph{Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS'24)}, pp.\  317--325, Auckland, New Zealand, 2024.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Yu, Zhu, Yu, Chen, Wang, Wu, Qin, Wu, Ding, and Huang]{xionghui}
Chen, X., Yu, Y., Zhu, Z., Yu, Z., Chen, Z., Wang, C., Wu, Y., Qin, R., Wu, H., Ding, R., and Huang, F.
\newblock Adversarial counterfactual environment model learning.
\newblock In \emph{Advances in Neural Information Processing Systems 36 (NeurIPS'23)}, New Orleans, LA, 2023{\natexlab{b}}.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{d4rl}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock {D4RL:} datasets for deep data-driven reinforcement learning.
\newblock \emph{CoRR}, abs/2004.07219, 2020.

\bibitem[Fujimoto \& Gu(2021)Fujimoto and Gu]{td3bc}
Fujimoto, S. and Gu, S.~S.
\newblock A minimalist approach to offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 34 (NeurIPS'21)}, Virtual Event, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{bcq}
Fujimoto, S., Meger, D., and Precup, D.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{Proceedings of the 36th International Conference on Machine Learning (ICML'19)}, Long Beach, USA, 2019.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget{-}Abadie, Mirza, Xu, Warde{-}Farley, Ozair, Courville, and Bengio]{gan}
Goodfellow, I.~J., Pouget{-}Abadie, J., Mirza, M., Xu, B., Warde{-}Farley, D., Ozair, S., Courville, A.~C., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems 27 (NeurIPS'24)}, pp.\  2672--2680, Montreal, Canada, 2014.

\bibitem[Gupta et~al.(2019)Gupta, Kumar, Lynch, Levine, and Hausman]{relay}
Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K.
\newblock Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning.
\newblock In \emph{Proceedings of the 3rd Annual Conference on Robot Learning (CoRL'19)}, Osaka, Japan, 2019.

\bibitem[Haarnoja et~al.(2018)Haarnoja, Zhou, Abbeel, and Levine]{sac}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor.
\newblock In \emph{Proceedings of the 35th International Conference on Machine Learning (ICML'18)}, Stockholmsm{\"{a}}ssan, Sweden, 2018.

\bibitem[Hester et~al.(2018)Hester, Vecerik, Pietquin, Lanctot, Schaul, Piot, Horgan, Quan, Sendonaris, Osband, et~al.]{deepqdemo}
Hester, T., Vecerik, M., Pietquin, O., Lanctot, M., Schaul, T., Piot, B., Horgan, D., Quan, J., Sendonaris, A., Osband, I., et~al.
\newblock Deep q-learning from demonstrations.
\newblock In \emph{Proceedings of the 32nd AAAI conference on artificial intelligence (AAAI'18)}, New Orleans, USA, 2018.

\bibitem[Ho et~al.(2020)Ho, Jain, and Abbeel]{diff}
Ho, J., Jain, A., and Abbeel, P.
\newblock Denoising diffusion probabilistic models.
\newblock In \emph{Proceedings of the 33rd Neural Information Processing Systems (NeurIPS'20)}, Virtual Event, 2020.

\bibitem[Hubbs et~al.(2020)Hubbs, Perez, Sarwar, Sahinidis, Grossmann, and Wassick]{hubbs2020or}
Hubbs, C.~D., Perez, H.~D., Sarwar, O., Sahinidis, N.~V., Grossmann, I.~E., and Wassick, J.~M.
\newblock Or-gym: A reinforcement learning library for operations research problems.
\newblock \emph{CoRR}, abs/2008.06319, 2020.

\bibitem[Jackson et~al.(2024)Jackson, Matthews, Lu, Ellis, Whiteson, and Foerster]{policyguideddiffusion}
Jackson, M.~T., Matthews, M.~T., Lu, C., Ellis, B., Whiteson, S., and Foerster, J.~N.
\newblock Policy-guided diffusion.
\newblock \emph{CoRR}, abs/2404.06356, 2024.

\bibitem[Janner et~al.(2019)Janner, Fu, Zhang, and Levine]{MBPO}
Janner, M., Fu, J., Zhang, M., and Levine, S.
\newblock When to trust your model: {M}odel-based policy optimization.
\newblock In \emph{Proceedings of the 32nd Neural Information Processing Systems (NeurIPS'19)}, Vancouver, Canada, 2019.

\bibitem[Janner et~al.(2022)Janner, Du, Tenenbaum, and Levine]{diffuser}
Janner, M., Du, Y., Tenenbaum, J., and Levine, S.
\newblock Planning with diffusion for flexible behavior synthesis.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning (ICML'22)}, Baltimore, USA, 2022.

\bibitem[Jia et~al.(2024)Jia, Zhang, Li, Gao, Liu, Yuan, Zhang, and Yu]{ada}
Jia, C., Zhang, F., Li, Y., Gao, C., Liu, X., Yuan, L., Zhang, Z., and Yu, Y.
\newblock Disentangling policy from offline task representation learning via adversarial data augmentation.
\newblock In \emph{Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems (AAMAS'24)}, pp.\  944--953, Auckland, New Zealand, 2024.

\bibitem[Jin et~al.(2022)Jin, Liu, Jiang, and Yu]{hybrid}
Jin, X., Liu, X., Jiang, S., and Yu, Y.
\newblock Hybrid value estimation for off-policy evaluation and offline reinforcement learning.
\newblock \emph{CoRR}, abs/2206.02000, 2022.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and Laine]{elucidate}
Karras, T., Aittala, M., Aila, T., and Laine, S.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock In \emph{Proceedings of the 36th Neural Information Processing Systems (NeurIPS'22)}, LA, USA, 2022.

\bibitem[Kostrikov et~al.(2022)Kostrikov, Nair, and Levine]{iql}
Kostrikov, I., Nair, A., and Levine, S.
\newblock Offline reinforcement learning with implicit q-learning.
\newblock In \emph{Proceedings of the 10-th International Conference on Learning Representations (ICLR'22)}, Virtual Event, 2022.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and Levine]{CQL}
Kumar, A., Zhou, A., Tucker, G., and Levine, S.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems 33 (NeurIPS'20)}, Virtual Event, 2020.

\bibitem[Lee et~al.(2021)Lee, Seo, Lee, Abbeel, and Shin]{balanced}
Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J.
\newblock Offline-to-online reinforcement learning via balanced replay and pessimistic q-ensemble.
\newblock In \emph{Conference on Robot Learning (CoRL'21)}, London, UK, 2021.

\bibitem[Li et~al.(2023)Li, Hu, Xu, Liu, Zhan, and Zhang]{proto}
Li, J., Hu, X., Xu, H., Liu, J., Zhan, X., and Zhang, Y.
\newblock {PROTO:} iterative policy regularized offline-to-online reinforcement learning.
\newblock \emph{CoRR}, abs/2305.15669, 2023.

\bibitem[Ling et~al.(2024)Ling, Wang, and Wang]{Ling_Wang_Wang_2024}
Ling, H., Wang, Z., and Wang, J.
\newblock Learning to stop cut generation for efficient mixed-integer linear programming.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'24)}, 38\penalty0 (18):\penalty0 20759--20767, Mar. 2024.

\bibitem[Liu et~al.(2021)Liu, Xue, Pang, Jiang, Xu, and Yu]{liu2021regret}
Liu, X.-H., Xue, Z., Pang, J., Jiang, S., Xu, F., and Yu, Y.
\newblock Regret minimization experience replay in off-policy reinforcement learning.
\newblock In \emph{Proceedings of 34th conference on Neural Information Processing Systems (NeurIPS'21)}, Virtual Event, 2021.

\bibitem[Lu et~al.(2023{\natexlab{a}})Lu, Ball, and Parker{-}Holder]{synther}
Lu, C., Ball, P.~J., and Parker{-}Holder, J.
\newblock Synthetic experience replay.
\newblock \emph{CoRR}, abs/2303.06614, 2023{\natexlab{a}}.

\bibitem[Lu et~al.(2023{\natexlab{b}})Lu, Chen, Chen, Su, Li, and Zhu]{contrastive}
Lu, C., Chen, H., Chen, J., Su, H., Li, C., and Zhu, J.
\newblock Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning.
\newblock In \emph{Proceedings of the 40th International Conference on Machine Learning(ICML'23)}, volume 202, pp.\  22825--22855, Honolulu, HI, 2023{\natexlab{b}}.

\bibitem[Mazoure et~al.(2023)Mazoure, Talbott, Bautista, Hjelm, Toshev, and Susskind]{conditiondiffusioncontrol}
Mazoure, B., Talbott, W., Bautista, M.~{\'{A}}., Hjelm, R.~D., Toshev, A., and Susskind, J.~M.
\newblock Value function estimation using conditional diffusion models for control.
\newblock \emph{CoRR}, abs/2306.07290, 2023.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare, Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik, Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{dqn}
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.~A., Veness, J., Bellemare, M.~G., Graves, A., Riedmiller, M.~A., Fidjeland, A., Ostrovski, G., Petersen, S., Beattie, C., Sadik, A., Antonoglou, I., King, H., Kumaran, D., Wierstra, D., Legg, S., and Hassabis, D.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Nachum et~al.(2019)Nachum, Chow, Dai, and Li]{dualdice}
Nachum, O., Chow, Y., Dai, B., and Li, L.
\newblock Dualdice: Behavior-agnostic estimation of discounted stationary distribution corrections.
\newblock In \emph{Proceedings of the 32nd Neural Information Processing Systems (NeurIPS'19)}, pp.\  2315--2325, Vancouver, Canada, 2019.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{awac}
Nair, A., Dalal, M., Gupta, A., and Levine, S.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{CoRR}, abs/2006.09359, 2020.

\bibitem[Nakamoto et~al.(2023)Nakamoto, Zhai, Singh, Mark, Ma, Finn, Kumar, and Levine]{calql}
Nakamoto, M., Zhai, Y., Singh, A., Mark, M.~S., Ma, Y., Finn, C., Kumar, A., and Levine, S.
\newblock Cal-ql: Calibrated offline {RL} pre-training for efficient online fine-tuning.
\newblock \emph{CoRR}, abs/2303.05479, 2023.

\bibitem[Nichol \& Dhariwal(2021)Nichol and Dhariwal]{NicholD21}
Nichol, A.~Q. and Dhariwal, P.
\newblock Improved denoising diffusion probabilistic models.
\newblock In Meila, M. and Zhang, T. (eds.), \emph{Proceedings of the 38th International Conference on Machine Learning (ICML'21)}, Virtual Event, 2021.

\bibitem[Nichol et~al.(2022)Nichol, Dhariwal, Ramesh, Shyam, Mishkin, McGrew, Sutskever, and Chen]{NicholDRSMMSC22}
Nichol, A.~Q., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M.
\newblock {GLIDE:} towards photorealistic image generation and editing with text-guided diffusion models.
\newblock In \emph{Proceedings of the 39th International Conference on Machine Learning (ICML'22)}, Baltimore, USA, 2022.

\bibitem[Peng et~al.(2020)Peng, Coumans, Zhang, Lee, Tan, and Levine]{rc1}
Peng, X.~B., Coumans, E., Zhang, T., Lee, T.~E., Tan, J., and Levine, S.
\newblock Learning agile robotic locomotion skills by imitating animals.
\newblock In \emph{Proceedings of the 14th Robotics: Science and Systems (RSS'20)}, Virtual Event, 2020.

\bibitem[Rafailov et~al.(2023)Rafailov, Hatch, Kolev, Martin, Phielipp, and Finn]{moto}
Rafailov, R., Hatch, K.~B., Kolev, V., Martin, J.~D., Phielipp, M., and Finn, C.
\newblock Moto: Offline pre-training to online fine-tuning for model-based robot learning.
\newblock In \emph{Conference on Robot Learning (CoRL'23)}, pp.\  3654--3671, 2023.

\bibitem[Rajeswaran et~al.(2018)Rajeswaran, Kumar, Gupta, Vezzani, Schulman, Todorov, and Levine]{rob2}
Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman, J., Todorov, E., and Levine, S.
\newblock Learning complex dexterous manipulation with deep reinforcement learning and demonstrations.
\newblock In \emph{Proceedings of the 12th Robotics: Science and Systems (RSS'18)}, PA, USA, 2018.

\bibitem[Rigter et~al.(2023)Rigter, Yamada, and Posner]{policyguidedtrajdiff}
Rigter, M., Yamada, J., and Posner, I.
\newblock World models via policy-guided trajectory diffusion.
\newblock \emph{CoRR}, abs/2312.08533, 2023.

\bibitem[Saharia et~al.(2022)Saharia, Chan, Saxena, Li, Whang, Denton, Ghasemipour, Lopes, Ayan, Salimans, Ho, Fleet, and Norouzi]{SahariaCSLWDGLA22}
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.~L., Ghasemipour, S. K.~S., Lopes, R.~G., Ayan, B.~K., Salimans, T., Ho, J., Fleet, D.~J., and Norouzi, M.
\newblock Photorealistic text-to-image diffusion models with deep language understanding.
\newblock In \emph{Proceedings of the 35th Neural Information Processing Systems (NeurIPS'22)}, New Orleans, USA, 2022.

\bibitem[Schaal(1996)]{learnfromdemo}
Schaal, S.
\newblock Learning from demonstration.
\newblock \emph{Advances in neural information processing systems}, 9, 1996.

\bibitem[Sinha et~al.(2022)Sinha, Song, Garg, and Ermon]{lfiw}
Sinha, S., Song, J., Garg, A., and Ermon, S.
\newblock Experience replay with likelihood-free importance weights.
\newblock In \emph{Learning for Dynamics and Control Conference (L4RC'22)}, Stanford, USA, 2022.

\bibitem[Song et~al.(2022)Song, Zhou, Sekhari, Bagnell, Krishnamurthy, and Sun]{hybridrl}
Song, Y., Zhou, Y., Sekhari, A., Bagnell, J.~A., Krishnamurthy, A., and Sun, W.
\newblock Hybrid rl: Using both offline and online data can make rl efficient.
\newblock In \emph{The 11th International Conference on Learning Representations (ICLR'22)}, Virtual Event, 2022.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Tarasov et~al.(2022)Tarasov, Nikulin, Akimov, Kurenkov, and Kolesnikov]{corl}
Tarasov, D., Nikulin, A., Akimov, D., Kurenkov, V., and Kolesnikov, S.
\newblock {CORL}: Research-oriented deep offline reinforcement learning library.
\newblock In \emph{3rd Offline RL Workshop: Offline RL as a ''Launchpad''}, 2022.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, and Vinyals]{infonce}
van~den Oord, A., Li, Y., and Vinyals, O.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{CoRR}, abs/1807.03748, 2018.

\bibitem[Vecerik et~al.(2017)Vecerik, Hester, Scholz, Wang, Pietquin, Piot, Heess, Roth{\"o}rl, Lampe, and Riedmiller]{leveragingdemo}
Vecerik, M., Hester, T., Scholz, J., Wang, F., Pietquin, O., Piot, B., Heess, N., Roth{\"o}rl, T., Lampe, T., and Riedmiller, M.
\newblock Leveraging demonstrations for deep reinforcement learning on robotics problems with sparse rewards.
\newblock \emph{CoRR}, abs/1707.08817, 2017.

\bibitem[von Luxburg \& Sch{\"{o}}lkopf(2011)von Luxburg and Sch{\"{o}}lkopf]{vc}
von Luxburg, U. and Sch{\"{o}}lkopf, B.
\newblock Statistical learning theory: Models, concepts, and results.
\newblock In \emph{Inductive Logic}, volume~10, pp.\  651--706. Elsevier, 2011.

\bibitem[Wang et~al.(2024)Wang, Wang, Li, Kuang, Shi, Zhu, Yuan, Zeng, Zhang, and Wu]{wang2024learning}
Wang, J., Wang, Z., Li, X., Kuang, Y., Shi, Z., Zhu, F., Yuan, M., Zeng, J., Zhang, Y., and Wu, F.
\newblock Learning to cut via hierarchical sequence/set model for efficient mixed-integer programming.
\newblock \emph{CoRR}, abs/2008.06319, 2024.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Yang, Gao, Lin, Chen, Wu, Jia, Song, and Huang]{oncefamily}
Wang, S., Yang, Q., Gao, J., Lin, M.~G., Chen, H., Wu, L., Jia, N., Song, S., and Huang, G.
\newblock Train once, get a family: State-adaptive balances for offline-to-online reinforcement learning.
\newblock \emph{CoRR}, abs/2310.17966, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2018)Wang, Chen, Yang, Wu, Wu, and Xie]{rs1}
Wang, X., Chen, Y., Yang, J., Wu, L., Wu, Z., and Xie, X.
\newblock A reinforcement learning framework for explainable recommendation.
\newblock In \emph{Proceedings of the 18th International Conference on Data Mining (ICDM'18)}, Singapore, 2018.

\bibitem[Wang et~al.(2022)Wang, Wang, Zhou, Li, and Li]{Wang_Wang_Zhou_Li_Li_2022}
Wang, Z., Wang, J., Zhou, Q., Li, B., and Li, H.
\newblock Sample-efficient reinforcement learning via conservative model-based actor-critic.
\newblock \emph{Proceedings of the AAAI Conference on Artificial Intelligence (AAAI'22)}, 36\penalty0 (8):\penalty0 8612--8620, Jun. 2022.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Hunt, and Zhou]{DiffusionQL}
Wang, Z., Hunt, J.~J., and Zhou, M.
\newblock Diffusion policies as an expressive policy class for offline reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations (ICLR'23)}, Kigali, Rwanda, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Li, Wang, Kuang, Yuan, Zeng, Zhang, and Wu]{wang2023learning}
Wang, Z., Li, X., Wang, J., Kuang, Y., Yuan, M., Zeng, J., Zhang, Y., and Wu, F.
\newblock Learning cut selection for mixed-integer linear programming via hierarchical sequence model.
\newblock In \emph{The Eleventh International Conference on Learning Representations (ICLR'23)}, 2023{\natexlab{c}}.

\bibitem[Xu et~al.(2021)Xu, Li, and Yu]{nearly}
Xu, T., Li, Z., and Yu, Y.
\newblock Nearly minimax optimal adversarial imitation learning with known and unknown transitions.
\newblock \emph{CoRR abs/2106.10424}, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Xu, and Yu]{pex}
Zhang, H., Xu, W., and Yu, H.
\newblock Policy expansion for bridging offline-to-online reinforcement learning.
\newblock In \emph{The Eleventh International Conference on Learning Representations(ICLR'23)}, Kigali, Rwanda, 2023.

\bibitem[Zhao et~al.(2018)Zhao, Zhang, Ding, Xia, Tang, and Yin]{rs2}
Zhao, X., Zhang, L., Ding, Z., Xia, L., Tang, J., and Yin, D.
\newblock Recommendations with negative feedback via pairwise deep reinforcement learning.
\newblock In \emph{Proceedings of the 24th International Conference on Knowledge Discovery {\&} Data Mining (KDD'18)}, London, UK, 2018.

\bibitem[Zhao et~al.(2022)Zhao, Boney, Ilin, Kannala, and Pajarinen]{adaptivebc}
Zhao, Y., Boney, R., Ilin, A., Kannala, J., and Pajarinen, J.
\newblock Adaptive behavior cloning regularization for stable offline-to-online reinforcement learning.
\newblock \emph{CoRR}, abs/2210.13846, 2022.

\bibitem[Zhu et~al.(2019)Zhu, Gupta, Rajeswaran, Levine, and Kumar]{rob3}
Zhu, H., Gupta, A., Rajeswaran, A., Levine, S., and Kumar, V.
\newblock Dexterous manipulation with deep reinforcement learning: Efficient, general, and low-cost.
\newblock In \emph{Proceedings of the 36th International Conference on Robotics and Automation (ICRA'19)}, Montreal, Canada, 2019.

\bibitem[Zhu et~al.(2018)Zhu, Wang, Merel, Rusu, Erez, Cabi, Tunyasuvunakool, Kram{\'{a}}r, Hadsell, de~Freitas, and Heess]{rob1}
Zhu, Y., Wang, Z., Merel, J., Rusu, A.~A., Erez, T., Cabi, S., Tunyasuvunakool, S., Kram{\'{a}}r, J., Hadsell, R., de~Freitas, N., and Heess, N.
\newblock Reinforcement and imitation learning for diverse visuomotor skills.
\newblock In \emph{Proceedings of the 12th Robotics: Science and Systems (RSS'18)}, PA, USA, 2018.

\end{thebibliography}
