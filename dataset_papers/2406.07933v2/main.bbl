\begin{thebibliography}{100}

\bibitem{abdin2024phi}
Marah Abdin, Sam~Ade Jacobs, Ammar~Ahmad Awan, Jyoti Aneja, Ahmed Awadallah,
  Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl,
  et~al.
\newblock Phi-3 technical report: A highly capable language model locally on
  your phone.
\newblock {\em arXiv preprint arXiv:2404.14219}, 2024.

\bibitem{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya,
  Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman,
  Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock {\em arXiv preprint arXiv:2303.08774}, 2023.

\bibitem{llama3modelcard}
AI@Meta.
\newblock Llama 3 model card.
\newblock 2024.

\bibitem{falcon}
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli,
  Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien
  Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme
  Penedo.
\newblock The falcon series of language models:towards open frontier models.
\newblock 2023.

\bibitem{OpenBioLLMs}
Malaikannan~Sankarasubbu Ankit~Pal.
\newblock Openbiollms: Advancing open-source large language models for
  healthcare and life sciences.
\newblock \url{https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B}, 2024.

\bibitem{anthropic2024claude}
Anthropic.
\newblock Introducing the next generation of claude.
\newblock \url{https://www.anthropic.com/news/claude-3-family}, Mar 2024.

\bibitem{aryabumi2024aya}
Viraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu
  Lin, Bharat Venkitesh, Madeline Smith, Kelly Marchisio, Sebastian Ruder, Acyr
  Locatelli, Julia Kreutzer, Nick Frosst, Phil Blunsom, Marzieh Fadaee, Ahmet
  Üstün, and Sara Hooker.
\newblock Aya 23: Open weight releases to further multilingual progress, 2024.

\bibitem{bai2023qwen}
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan,
  Wenbin Ge, Yu~Han, Fei Huang, et~al.
\newblock Qwen technical report.
\newblock {\em arXiv preprint arXiv:2309.16609}, 2023.

\bibitem{banerjee2005meteor}
Satanjeev Banerjee and Alon Lavie.
\newblock Meteor: An automatic metric for mt evaluation with improved
  correlation with human judgments.
\newblock In {\em Proceedings of the acl workshop on intrinsic and extrinsic
  evaluation measures for machine translation and/or summarization}, pages
  65--72, 2005.

\bibitem{zephyr_141b}
Alvaro Bartolome, Jiwoo Hong, Noah Lee, Kashif Rasul, and Lewis Tunstall.
\newblock Zephyr 141b a39b.
\newblock
  \url{https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1}, 2024.

\bibitem{baumhauer2022machine}
Thomas Baumhauer, Pascal Sch{\"o}ttle, and Matthias Zeppelzauer.
\newblock Machine unlearning: Linear filtration for logit-based classifiers.
\newblock {\em Machine Learning}, 111(9):3203--3226, 2022.

\bibitem{bellagente2024stable}
Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi,
  Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta,
  et~al.
\newblock Stable lm 2 1.6 b technical report.
\newblock {\em arXiv preprint arXiv:2402.17834}, 2024.

\bibitem{belrose2023leace}
Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward
  Raff, and Stella Biderman.
\newblock Leace: Perfect linear concept erasure in closed form.
\newblock {\em arXiv preprint arXiv:2306.03819}, 2023.

\bibitem{biderman2023pythia}
Stella Biderman, Hailey Schoelkopf, Quentin~Gregory Anthony, Herbie Bradley,
  Kyle O’Brien, Eric Hallahan, Mohammad~Aflah Khan, Shivanshu Purohit,
  USVSN~Sai Prashanth, Edward Raff, et~al.
\newblock Pythia: A suite for analyzing large language models across training
  and scaling.
\newblock In {\em International Conference on Machine Learning}, pages
  2397--2430. PMLR, 2023.

\bibitem{bisk2020piqa}
Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et~al.
\newblock Piqa: Reasoning about physical commonsense in natural language.
\newblock In {\em Proceedings of the AAAI conference on artificial
  intelligence}, pages 7432--7439, 2020.

\bibitem{blanco2024digital}
Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David S{\'a}nchez,
  Josep Domingo-Ferrer, Guillem Collell, and Kuan~Eeik Tan.
\newblock Digital forgetting in large language models: A survey of unlearning
  methods.
\newblock {\em arXiv preprint arXiv:2404.02062}, 2024.

\bibitem{borkar2023can}
Jaydeep Borkar.
\newblock What can we learn from data leakage and unlearning for law?
\newblock {\em arXiv preprint arXiv:2307.10476}, 2023.

\bibitem{botev2024recurrentgemma}
Aleksandar Botev, Soham De, Samuel~L Smith, Anushan Fernando, George-Cristian
  Muraru, Ruba Haroun, Leonard Berrada, Razvan Pascanu, Pier~Giuseppe Sessa,
  Robert Dadashi, et~al.
\newblock Recurrentgemma: Moving past transformers for efficient open language
  models.
\newblock {\em arXiv preprint arXiv:2404.07839}, 2024.

\bibitem{bourtoule2021machine}
Lucas Bourtoule, Varun Chandrasekaran, Christopher~A Choquette-Choo, Hengrui
  Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot.
\newblock Machine unlearning.
\newblock In {\em 2021 IEEE Symposium on Security and Privacy (SP)}, pages
  141--159. IEEE, 2021.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems},
  33:1877--1901, 2020.

\bibitem{cai2024internlm2}
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen,
  Zehui Chen, Zhi Chen, Pei Chu, et~al.
\newblock Internlm2 technical report.
\newblock {\em arXiv preprint arXiv:2403.17297}, 2024.

\bibitem{carlini2022membership}
Nicholas Carlini, Steve Chien, Milad Nasr, Shuang Song, Andreas Terzis, and
  Florian Tramer.
\newblock Membership inference attacks from first principles.
\newblock In {\em 2022 IEEE Symposium on Security and Privacy (SP)}, pages
  1897--1914. IEEE, 2022.

\bibitem{chen2023unlearn}
Jiaao Chen and Diyi Yang.
\newblock Unlearn what you want to forget: Efficient unlearning for llms.
\newblock {\em arXiv preprint arXiv:2310.20150}, 2023.

\bibitem{chen2023boundary}
Min Chen, Weizhuo Gao, Gaoyang Liu, Kai Peng, and Chen Wang.
\newblock Boundary unlearning: Rapid forgetting of deep networks via shifting
  the decision boundary.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 7766--7775, 2023.

\bibitem{chu2024causal}
Zhixuan Chu, Yan Wang, Longfei Li, Zhibo Wang, Zhan Qin, and Kui Ren.
\newblock A causal explainable guardrails for large language models.
\newblock {\em arXiv preprint arXiv:2405.04160}, 2024.

\bibitem{clark2019boolq}
Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael
  Collins, and Kristina Toutanova.
\newblock Boolq: Exploring the surprising difficulty of natural yes/no
  questions.
\newblock {\em arXiv preprint arXiv:1905.10044}, 2019.

\bibitem{clark2018think}
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa
  Schoenick, and Oyvind Tafjord.
\newblock Think you have solved question answering? try arc, the ai2 reasoning
  challenge.
\newblock {\em arXiv preprint arXiv:1803.05457}, 2018.

\bibitem{codegemma_2024}
{CodeGemma Team}, Ale~Jakse Hartman, Andrea Hu, Christopher~A. Choquette-Choo,
  Heri Zhao, Jane Fine, Jeffrey Hui, Jingyue Shen, Joe Kelley, Joshua Howland,
  Kshitij Bansal, Luke Vilnis, Mateo Wirth, Nam Nguyen, Paul Michel, Peter
  Choy, Pratik Joshi, Ravin Kumar, Sarmad Hashmi, Shubham Agrawal, Siqi Zuo,
  Tris Warkentin, and Zhitao et~al. Gong.
\newblock Codegemma: Open code models based on gemma.
\newblock 2024.

\bibitem{cohere2024}
{Cohere Team}.
\newblock Command r: Retrieval-augmented generation at production scale, 2024.

\bibitem{databricks2024}
{Databricks Team}.
\newblock Introducing dbrx: A new state-of-the-art open llm, 2024.

\bibitem{deepseekai2023deepseek}
DeepSeek-AI.
\newblock Deepseek-v2: A strong, economical, and efficient mixture-of-experts
  language model.
\newblock {\em arXiv preprint arXiv:2405.04434}, 2024.

\bibitem{dong2024building}
Yi~Dong, Ronghui Mu, Gaojie Jin, Yi~Qi, Jinwei Hu, Xingyu Zhao, Jie Meng,
  Wenjie Ruan, and Xiaowei Huang.
\newblock Building guardrails for large language models.
\newblock {\em arXiv preprint arXiv:2402.01822}, 2024.

\bibitem{duan2024membership}
Michael Duan, Anshuman Suri, Niloofar Mireshghallah, Sewon Min, Weijia Shi,
  Luke Zettlemoyer, Yulia Tsvetkov, Yejin Choi, David Evans, and Hannaneh
  Hajishirzi.
\newblock Do membership inference attacks work on large language models?
\newblock {\em arXiv preprint arXiv:2402.07841}, 2024.

\bibitem{dubey2024llama}
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad
  Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,
  et~al.
\newblock The llama 3 herd of models.
\newblock {\em arXiv preprint arXiv:2407.21783}, 2024.

\bibitem{ebrahimi2021does}
Javid Ebrahimi, Hao Yang, and Wei Zhang.
\newblock How does adversarial fine-tuning benefit bert?
\newblock {\em arXiv preprint arXiv:2108.13602}, 2021.

\bibitem{eldan2023s}
Ronen Eldan and Mark Russinovich.
\newblock Who's harry potter? approximate unlearning in llms.
\newblock {\em arXiv preprint arXiv:2310.02238}, 2023.

\bibitem{gdpr}
{European Union}.
\newblock General data protection regulation (gdpr).
\newblock \url{https://gdpr-info.eu/}, 2016.

\bibitem{fan2023salun}
Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu.
\newblock Salun: Empowering machine unlearning via gradient-based weight
  saliency in both image classification and generation.
\newblock {\em arXiv preprint arXiv:2310.12508}, 2023.

\bibitem{fang2024llm}
Richard Fang, Rohan Bindu, Akul Gupta, Qiusi Zhan, and Daniel Kang.
\newblock Llm agents can autonomously hack websites.
\newblock {\em arXiv preprint arXiv:2402.06664}, 2024.

\bibitem{fort2023scaling}
Stanislav Fort.
\newblock Scaling laws for adversarial attacks on language model activations.
\newblock {\em arXiv preprint arXiv:2312.02780}, 2023.

\bibitem{foster2024fast}
Jack Foster, Stefan Schoepf, and Alexandra Brintrup.
\newblock Fast machine unlearning without retraining through selective synaptic
  dampening.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 12043--12051, 2024.

\bibitem{gan2023model}
Wensheng Gan, Shicheng Wan, and S~Yu Philip.
\newblock Model-as-a-service (maas): A survey.
\newblock In {\em 2023 IEEE International Conference on Big Data (BigData)},
  pages 4636--4645. IEEE, 2023.

\bibitem{eval-harness}
Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony
  DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le~Noac'h,
  Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang,
  Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric
  Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou.
\newblock A framework for few-shot language model evaluation, 12 2023.

\bibitem{geiping2024coercing}
Jonas Geiping, Alex Stein, Manli Shu, Khalid Saifullah, Yuxin Wen, and Tom
  Goldstein.
\newblock Coercing llms to do and reveal (almost) anything.
\newblock {\em arXiv preprint arXiv:2402.14020}, 2024.

\bibitem{team2023gemini}
Gemini.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{golatkar2020eternal}
Aditya Golatkar, Alessandro Achille, and Stefano Soatto.
\newblock Eternal sunshine of the spotless net: Selective forgetting in deep
  networks.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and
  Pattern Recognition}, pages 9304--9312, 2020.

\bibitem{goyal2024llmguard}
Shubh Goyal, Medha Hira, Shubham Mishra, Sukriti Goyal, Arnav Goel, Niharika
  Dadu, DB~Kirushikesh, Sameep Mehta, and Nishtha Madaan.
\newblock Llmguard: Guarding against unsafe llm behavior.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, pages 23790--23792, 2024.

\bibitem{groeneveld2024olmo}
Dirk Groeneveld, Iz~Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind
  Tafjord, Ananya~Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et~al.
\newblock Olmo: Accelerating the science of language models.
\newblock {\em arXiv preprint arXiv:2402.00838}, 2024.

\bibitem{grynbaum2023times}
Michael~M. Grynbaum and Ryan Mac.
\newblock The times sues openai and microsoft over a.i. use of copyrighted
  work.
\newblock {\em The New York Times}, 12 2023.

\bibitem{gu2024model}
Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and
  Nanyun Peng.
\newblock Model editing can hurt general abilities of large language models.
\newblock {\em arXiv preprint arXiv:2401.04700}, 2024.

\bibitem{guo2024deepseek}
Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting
  Chen, Xiao Bi, Y~Wu, YK~Li, et~al.
\newblock Deepseek-coder: When the large language model meets programming--the
  rise of code intelligence.
\newblock {\em arXiv preprint arXiv:2401.14196}, 2024.

\bibitem{hamborg2017news}
Felix Hamborg, Norman Meuschke, Corinna Breitinger, and Bela Gipp.
\newblock news-please: A generic news crawler and extractor.
\newblock In {\em Proceedings of the 15th International Symposium of
  Information Science}, pages 218--223, March 2017.

\bibitem{harandizadeh2024risk}
Bahareh Harandizadeh, Abel Salinas, and Fred Morstatter.
\newblock Risk and response in large language models: Evaluating key threat
  categories.
\newblock {\em arXiv preprint arXiv:2403.14988}, 2024.

\bibitem{hayes2024inexact}
Jamie Hayes, Ilia Shumailov, Eleni Triantafillou, Amr Khalifa, and Nicolas
  Papernot.
\newblock Inexact unlearning needs more careful evaluations to avoid a false
  sense of privacy.
\newblock {\em arXiv preprint arXiv:2403.01218}, 2024.

\bibitem{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn
  Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock {\em arXiv preprint arXiv:2009.03300}, 2020.

\bibitem{hu2024separate}
Xinshuo Hu, Dongfang Li, Baotian Hu, Zihao Zheng, Zhenyu Liu, and Min Zhang.
\newblock Separate the wheat from the chaff: Model deficiency unlearning via
  parameter-efficient module operation.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 18252--18260, 2024.

\bibitem{huang2024offset}
James~Y Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung
  Poon, and Muhao Chen.
\newblock Offset unlearning for large language models.
\newblock {\em arXiv preprint arXiv:2404.11045}, 2024.

\bibitem{ilharco2022editing}
Gabriel Ilharco, Marco~Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan,
  Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi.
\newblock Editing models with task arithmetic.
\newblock {\em arXiv preprint arXiv:2212.04089}, 2022.

\bibitem{inan2023llama}
Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer,
  Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine,
  et~al.
\newblock Llama guard: Llm-based input-output safeguard for human-ai
  conversations.
\newblock {\em arXiv preprint arXiv:2312.06674}, 2023.

\bibitem{jang2022knowledge}
Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen
  Logeswaran, and Minjoon Seo.
\newblock Knowledge unlearning for mitigating privacy risks in language models.
\newblock {\em arXiv preprint arXiv:2210.01504}, 2022.

\bibitem{jia2023model}
Jinghan Jia, Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu,
  Pranay Sharma, and Sijia Liu.
\newblock Model sparsification can simplify machine unlearning.
\newblock {\em arXiv preprint arXiv:2304.04934}, 2023.

\bibitem{jia2024soul}
Jinghan Jia, Yihua Zhang, Yimeng Zhang, Jiancheng Liu, Bharat Runwal, James
  Diffenderfer, Bhavya Kailkhura, and Sijia Liu.
\newblock Soul: Unlocking the power of second-order optimization for llm
  unlearning.
\newblock {\em arXiv preprint arXiv:2404.18239}, 2024.

\bibitem{jiang2023mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,
  Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel,
  Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{jiang2024mixtral}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche
  Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou
  Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock {\em arXiv preprint arXiv:2401.04088}, 2024.

\bibitem{karamolegkou2023copyright}
Antonia Karamolegkou, Jiaang Li, Li~Zhou, and Anders S{\o}gaard.
\newblock Copyright violations and large language models.
\newblock {\em arXiv preprint arXiv:2310.13771}, 2023.

\bibitem{kim2023robust}
Jinhwa Kim, Ali Derakhshan, and Ian~G Harris.
\newblock Robust safety classifier for large language models: Adversarial
  prompt shield.
\newblock {\em arXiv preprint arXiv:2311.00172}, 2023.

\bibitem{kim2024propile}
Siwon Kim, Sangdoo Yun, Hwaran Lee, Martin Gubri, Sungroh Yoon, and Seong~Joon
  Oh.
\newblock Propile: Probing privacy leakage in large language models.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{kumar2022privacy}
Vinayshekhar~Bannihatti Kumar, Rashmi Gangadharaiah, and Dan Roth.
\newblock Privacy adhering machine un-learning in nlp.
\newblock {\em arXiv preprint arXiv:2212.09573}, 2022.

\bibitem{kurmanji2024towards}
Meghdad Kurmanji, Peter Triantafillou, Jamie Hayes, and Eleni Triantafillou.
\newblock Towards unbounded machine unlearning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{labrak2024biomistral}
Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael
  Rouvier, and Richard Dufour.
\newblock Biomistral: A collection of open-source pretrained large language
  models for medical domains, 2024.

\bibitem{lees2022new}
Alyssa Lees, Vinh~Q Tran, Yi~Tay, Jeffrey Sorensen, Jai Gupta, Donald Metzler,
  and Lucy Vasserman.
\newblock A new generation of perspective api: Efficient multilingual
  character-level transformers.
\newblock In {\em Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 3197--3207, 2022.

\bibitem{li2024digger}
Haodong Li, Gelei Deng, Yi~Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang
  Liu, Guoai Xu, Guosheng Xu, and Haoyu Wang.
\newblock Digger: Detecting copyright content mis-usage in large language model
  training.
\newblock {\em arXiv preprint arXiv:2401.00676}, 2024.

\bibitem{li2024wmdp}
Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice
  Gatti, Justin~D Li, Ann-Kathrin Dombrowski, Shashwat Goel, Long Phan, et~al.
\newblock The wmdp benchmark: Measuring and reducing malicious use with
  unlearning.
\newblock {\em arXiv preprint arXiv:2403.03218}, 2024.

\bibitem{li2023textbooks}
Yuanzhi Li, S{\'e}bastien Bubeck, Ronen Eldan, Allie Del~Giorno, Suriya
  Gunasekar, and Yin~Tat Lee.
\newblock Textbooks are all you need ii: phi-1.5 technical report.
\newblock {\em arXiv preprint arXiv:2309.05463}, 2023.

\bibitem{li2024latesteval}
Yucheng Li, Frank Guerin, and Chenghua Lin.
\newblock Latesteval: Addressing data contamination in language model
  evaluation through dynamic and time-sensitive test construction.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~38, pages 18600--18607, 2024.

\bibitem{liao2024amplegcg}
Zeyi Liao and Huan Sun.
\newblock Amplegcg: Learning a universal and transferable generative model of
  adversarial suffixes for jailbreaking both open and closed llms.
\newblock {\em arXiv preprint arXiv:2404.07921}, 2024.

\bibitem{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In {\em Text summarization branches out}, pages 74--81, 2004.

\bibitem{lin2021truthfulqa}
Stephanie Lin, Jacob Hilton, and Owain Evans.
\newblock Truthfulqa: Measuring how models mimic human falsehoods.
\newblock {\em arXiv preprint arXiv:2109.07958}, 2021.

\bibitem{liu2024model}
Jiancheng Liu, Parikshit Ram, Yuguang Yao, Gaowen Liu, Yang Liu, PRANAY SHARMA,
  Sijia Liu, et~al.
\newblock Model sparsity can simplify machine unlearning.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{liu2024rethinking}
Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter
  Hase, Yuguang Yao, Chris~Yuhao Liu, Xiaojun Xu, Hang Li, Kush~R. Varshney,
  Mohit Bansal, Sanmi Koyejo, and Yang Liu.
\newblock Rethinking machine unlearning for large language models.
\newblock {\em arXiv preprint arXiv:2402.08787}, 2024.

\bibitem{liu2020adversarial}
Xiaodong Liu, Hao Cheng, Pengcheng He, Weizhu Chen, Yu~Wang, Hoifung Poon, and
  Jianfeng Gao.
\newblock Adversarial training for large neural language models.
\newblock {\em arXiv preprint arXiv:2004.08994}, 2020.

\bibitem{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692}, 2019.

\bibitem{liu2024towards}
Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang.
\newblock Towards safer large language models through machine unlearning.
\newblock {\em arXiv preprint arXiv:2402.10058}, 2024.

\bibitem{liu2024chatqa}
Zihan Liu, Wei Ping, Rajarshi Roy, Peng Xu, Chankyu Lee, Mohammad Shoeybi, and
  Bryan Catanzaro.
\newblock Chatqa: Surpassing gpt-4 on conversational qa and rag.
\newblock {\em arXiv preprint arXiv:2401.10225}, 2024.

\bibitem{lozhkov2024starcoder}
Anton Lozhkov, Raymond Li, Loubna~Ben Allal, Federico Cassano, Joel
  Lamy-Poirier, Nouamane Tazi, Ao~Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang
  Wei, Tianyang Liu, Max Tian, Denis Kocetkov, Arthur Zucker, Younes Belkada,
  Zijian Wang, Qian Liu, Dmitry Abulkhanov, Indraneil Paul, Zhuang Li, Wen-Ding
  Li, Megan Risdal, Jia Li, Jian Zhu, Terry~Yue Zhuo, Evgenii Zheltonozhskii,
  Nii Osae~Osae Dade, Wenhao Yu, Lucas Krauß, Naman Jain, Yixuan Su, Xuanli
  He, Manan Dey, Edoardo Abati, Yekun Chai, Niklas Muennighoff, Xiangru Tang,
  Muhtasham Oblokulov, Christopher Akiki, Marc Marone, Chenghao Mou, Mayank
  Mishra, Alex Gu, Binyuan Hui, Tri Dao, Armel Zebaze, Olivier Dehaene, Nicolas
  Patry, Canwen Xu, Julian McAuley, Han Hu, Torsten Scholak, Sebastien Paquet,
  Jennifer Robinson, Carolyn~Jane Anderson, Nicolas Chapados, Mostofa Patwary,
  Nima Tajbakhsh, Yacine Jernite, Carlos~Muñoz Ferrandis, Lingming Zhang, Sean
  Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de~Vries.
\newblock Starcoder 2 and the stack v2: The next generation, 2024.

\bibitem{lukas2023analyzing}
Nils Lukas, Ahmed Salem, Robert Sim, Shruti Tople, Lukas Wutschitz, and
  Santiago Zanella-B{\'e}guelin.
\newblock Analyzing leakage of personally identifiable information in language
  models.
\newblock In {\em 2023 IEEE Symposium on Security and Privacy (SP)}, pages
  346--363. IEEE, 2023.

\bibitem{luo2023biomedgpt}
Yizhen Luo, Jiahuan Zhang, Siqi Fan, Kai Yang, Yushuai Wu, Mu~Qiao, and Zaiqing
  Nie.
\newblock Biomedgpt: Open multimodal generative pre-trained transformer for
  biomedicine.
\newblock {\em arXiv preprint arXiv:2308.09442}, 2023.

\bibitem{lynch2024eight}
Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan
  Hadfield-Menell.
\newblock Eight methods to evaluate robust unlearning in llms.
\newblock {\em arXiv preprint arXiv:2402.16835}, 2024.

\bibitem{StableBelugaModels}
Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian
  Laforte.
\newblock Stable beluga models.

\bibitem{maini2024tofu}
Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary~C Lipton, and J~Zico
  Kolter.
\newblock Tofu: A task of fictitious unlearning for llms.
\newblock {\em arXiv preprint arXiv:2401.06121}, 2024.

\bibitem{maini2024llm}
Pratyush Maini, Hengrui Jia, Nicolas Papernot, and Adam Dziedzic.
\newblock Llm dataset inference: Did you train on my dataset?
\newblock {\em arXiv preprint arXiv:2406.06443}, 2024.

\bibitem{markov2023holistic}
Todor Markov, Chong Zhang, Sandhini Agarwal, Florentine~Eloundou Nekoul,
  Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng.
\newblock A holistic approach to undesired content detection in the real world.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, pages 15009--15018, 2023.

\bibitem{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In {\em Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem{mihaylov2018can}
Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal.
\newblock Can a suit of armor conduct electricity? a new dataset for open book
  question answering.
\newblock {\em arXiv preprint arXiv:1809.02789}, 2018.

\bibitem{mireshghallah2023can}
Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap,
  Reza Shokri, and Yejin Choi.
\newblock Can llms keep a secret? testing privacy implications of language
  models via contextual integrity theory.
\newblock {\em arXiv preprint arXiv:2310.17884}, 2023.

\bibitem{mishra2024granite}
Mayank Mishra, Matt Stallone, Gaoyuan Zhang, Yikang Shen, Aditya Prasad,
  Adriana~Meza Soria, Michele Merler, Parameswaran Selvam, Saptha Surendran,
  Shivdeep Singh, et~al.
\newblock Granite code models: A family of open foundation models for code
  intelligence.
\newblock {\em arXiv preprint arXiv:2405.04324}, 2024.

\bibitem{mosbach2020stability}
Marius Mosbach, Maksym Andriushchenko, and Dietrich Klakow.
\newblock On the stability of fine-tuning bert: Misconceptions, explanations,
  and strong baselines.
\newblock {\em arXiv preprint arXiv:2006.04884}, 2020.

\bibitem{mukherjee2023orca}
Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid
  Palangi, and Ahmed Awadallah.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4.
\newblock {\em arXiv preprint arXiv:2306.02707}, 2023.

\bibitem{muresanu2024unlearnable}
Andrei Muresanu, Anvith Thudi, Michael~R Zhang, and Nicolas Papernot.
\newblock Unlearnable algorithms for in-context learning.
\newblock {\em arXiv preprint arXiv:2402.00751}, 2024.

\bibitem{neel2023privacy}
Seth Neel and Peter Chang.
\newblock Privacy issues in large language models: A survey.
\newblock {\em arXiv preprint arXiv:2312.06717}, 2023.

\bibitem{nguyen2022survey}
Thanh~Tam Nguyen, Thanh~Trung Huynh, Phi~Le Nguyen, Alan Wee-Chung Liew,
  Hongzhi Yin, and Quoc Viet~Hung Nguyen.
\newblock A survey of machine unlearning.
\newblock {\em arXiv preprint arXiv:2209.02299}, 2022.

\bibitem{ni2023forgetting}
Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, and Min Yang.
\newblock Forgetting before learning: Utilizing parametric arithmetic for
  knowledge updating in large language models.
\newblock {\em arXiv preprint arXiv:2311.08011}, 2023.

\bibitem{openai2022chatgpt}
OpenAI.
\newblock Introducing chatgpt, Nov 2022.

\bibitem{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting of the Association for
  Computational Linguistics}, pages 311--318, 2002.

\bibitem{paulus2024advprompter}
Anselm Paulus, Arman Zharmagambetov, Chuan Guo, Brandon Amos, and Yuandong
  Tian.
\newblock Advprompter: Fast adaptive adversarial prompting for llms.
\newblock {\em arXiv preprint arXiv:2404.16873}, 2024.

\bibitem{pawelczyk2023context}
Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju.
\newblock In-context unlearning: Language models as few shot unlearners.
\newblock {\em arXiv preprint arXiv:2310.07579}, 2023.

\bibitem{pinnaparaju2024stable}
Nikhil Pinnaparaju, Reshinth Adithyan, Duy Phung, Jonathan Tow, James
  Baicoianu, Ashish Datta, Maksym Zhuravinskyi, Dakota Mahan, Marco Bellagente,
  Carlos Riquelme, et~al.
\newblock Stable code technical report.
\newblock {\em arXiv preprint arXiv:2404.01226}, 2024.

\bibitem{post2018call}
Matt Post.
\newblock A call for clarity in reporting bleu scores.
\newblock {\em arXiv preprint arXiv:1804.08771}, 2018.

\bibitem{qiu2024pistol}
Xinchi Qiu, William~F Shen, Yihong Chen, Nicola Cancedda, Pontus Stenetorp, and
  Nicholas~D Lane.
\newblock Pistol: Dataset compilation pipeline for structural unlearning of
  llms.
\newblock {\em arXiv preprint arXiv:2406.16810}, 2024.

\bibitem{rafailov2023direct}
Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher~D
  Manning, and Chelsea Finn.
\newblock Direct preference optimization: Your language model is secretly a
  reward model. arxiv 2023.
\newblock {\em arXiv preprint arXiv:2305.18290}, 2023.

\bibitem{rebedea2023nemo}
Traian Rebedea, Razvan Dinu, Makesh Sreedhar, Christopher Parisien, and
  Jonathan Cohen.
\newblock Nemo guardrails: A toolkit for controllable and safe llm applications
  with programmable rails.
\newblock {\em arXiv preprint arXiv:2310.10501}, 2023.

\bibitem{rowling1997harry}
J.K. Rowling.
\newblock {\em Harry Potter and the Sorcerer's Stone}.
\newblock Scholastic, New York, 1997.

\bibitem{roziere2023code}
Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat,
  Xiaoqing~Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J{\'e}r{\'e}my Rapin,
  et~al.
\newblock Code llama: Open foundation models for code.
\newblock {\em arXiv preprint arXiv:2308.12950}, 2023.

\bibitem{sakaguchi2021winogrande}
Keisuke Sakaguchi, Ronan~Le Bras, Chandra Bhagavatula, and Yejin Choi.
\newblock Winogrande: An adversarial winograd schema challenge at scale.
\newblock {\em Communications of the ACM}, 64(9):99--106, 2021.

\bibitem{sandbrink2023artificial}
Jonas~B Sandbrink.
\newblock Artificial intelligence and biological misuse: Differentiating risks
  of language models and biological design tools.
\newblock {\em arXiv preprint arXiv:2306.13952}, 2023.

\bibitem{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock {\em arXiv preprint arXiv:1910.01108}, 2019.

\bibitem{sap2019socialiqa}
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi.
\newblock Socialiqa: Commonsense reasoning about social interactions.
\newblock {\em arXiv preprint arXiv:1904.09728}, 2019.

\bibitem{sepahvand2024data}
Nazanin~Mohammadi Sepahvand, Vincent Dumoulin, Eleni Triantafillou, and
  Gintare~Karolina Dziugaite.
\newblock Data selection for transfer unlearning. arxiv 2024.
\newblock {\em arXiv preprint arXiv:2405.10425}, 2024.

\bibitem{shaik2023exploring}
Thanveer Shaik, Xiaohui Tao, Haoran Xie, Lin Li, Xiaofeng Zhu, and Qing Li.
\newblock Exploring the landscape of machine unlearning: A survey and taxonomy.
\newblock {\em arXiv preprint arXiv:2305.06360}, 2023.

\bibitem{shen2024jetmoe}
Yikang Shen, Zhen Guo, Tianle Cai, and Zengyi Qin.
\newblock Jetmoe: Reaching llama2 performance with 0.1 m dollars.
\newblock {\em arXiv preprint arXiv:2404.07413}, 2024.

\bibitem{shevlane2022structured}
Toby Shevlane.
\newblock Structured access: an emerging paradigm for safe ai deployment.
\newblock {\em arXiv preprint arXiv:2201.05159}, 2022.

\bibitem{shi2023detecting}
Weijia Shi, Anirudh Ajith, Mengzhou Xia, Yangsibo Huang, Daogao Liu, Terra
  Blevins, Danqi Chen, and Luke Zettlemoyer.
\newblock Detecting pretraining data from large language models.
\newblock {\em arXiv preprint arXiv:2310.16789}, 2023.

\bibitem{si2023knowledge}
Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang.
\newblock Knowledge unlearning for llms: Tasks, methods, and challenges.
\newblock {\em arXiv preprint arXiv:2311.15766}, 2023.

\bibitem{spall1992multivariate}
James~C Spall.
\newblock Multivariate stochastic approximation using a simultaneous
  perturbation gradient approximation.
\newblock {\em IEEE transactions on automatic control}, 37(3):332--341, 1992.

\bibitem{spall2005introduction}
James~C Spall.
\newblock {\em Introduction to stochastic search and optimization: estimation,
  simulation, and control}.
\newblock John Wiley \& Sons, 2005.

\bibitem{staab2023beyond}
Robin Staab, Mark Vero, Mislav Balunovi{\'c}, and Martin Vechev.
\newblock Beyond memorization: Violating privacy via inference with large
  language models.
\newblock {\em arXiv preprint arXiv:2310.07298}, 2023.

\bibitem{talmor2018commonsenseqa}
Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant.
\newblock Commonsenseqa: A question answering challenge targeting commonsense
  knowledge.
\newblock {\em arXiv preprint arXiv:1811.00937}, 2018.

\bibitem{team2024gemma}
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju,
  Shreya Pathak, Laurent Sifre, Morgane Rivi{\`e}re, Mihir~Sanjay Kale,
  Juliette Love, et~al.
\newblock Gemma: Open models based on gemini research and technology.
\newblock {\em arXiv preprint arXiv:2403.08295}, 2024.

\bibitem{thaker2024guardrail}
Pratiksha Thaker, Yash Maurya, and Virginia Smith.
\newblock Guardrail baselines for unlearning in llms.
\newblock {\em arXiv preprint arXiv:2403.03329}, 2024.

\bibitem{thudi2022unrolling}
Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot.
\newblock Unrolling sgd: Understanding factors influencing machine unlearning.
\newblock In {\em 2022 IEEE 7th European Symposium on Security and Privacy
  (EuroS\&P)}, pages 303--319. IEEE, 2022.

\bibitem{touvron2023llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{tunstall2023zephyr}
Lewis Tunstall, Edward Beeching, Nathan Lambert, Nazneen Rajani, Kashif Rasul,
  Younes Belkada, Shengyi Huang, Leandro von Werra, Cl{\'e}mentine Fourrier,
  Nathan Habib, et~al.
\newblock Zephyr: Direct distillation of lm alignment.
\newblock {\em arXiv preprint arXiv:2310.16944}, 2023.

\bibitem{zephyr_7b_gemma}
Lewis Tunstall and Philipp Schmid.
\newblock Zephyr 7b gemma.
\newblock \url{https://huggingface.co/HuggingFaceH4/zephyr-7b-gemma-v0.1},
  2024.

\bibitem{vovk2005algorithmic}
Vladimir Vovk, Alexander Gammerman, and Glenn Shafer.
\newblock {\em Algorithmic learning in a random world}, volume~29.
\newblock Springer, 2005.

\bibitem{gpt-j}
Ben Wang and Aran Komatsuzaki.
\newblock {GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model}.
\newblock \url{https://github.com/kingoflolz/mesh-transformer-jax}, May 2021.

\bibitem{wang2023openchat}
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu.
\newblock Openchat: Advancing open-source language models with mixed-quality
  data.
\newblock {\em arXiv preprint arXiv:2309.11235}, 2023.

\bibitem{wang2023kga}
Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi
  Yin.
\newblock Kga: A general machine unlearning framework based on knowledge gap
  alignment.
\newblock {\em arXiv preprint arXiv:2305.06535}, 2023.

\bibitem{wang2023adding}
Yanchen Wang and Lisa Singh.
\newblock Adding guardrails to advanced chatbots.
\newblock {\em arXiv preprint arXiv:2306.07500}, 2023.

\bibitem{wu2023depn}
Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and
  Deyi Xiong.
\newblock Depn: Detecting and editing privacy neurons in pretrained language
  models.
\newblock {\em arXiv preprint arXiv:2310.20138}, 2023.

\bibitem{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang
  Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex
  instructions.
\newblock {\em arXiv preprint arXiv:2304.12244}, 2023.

\bibitem{yang2023baichuan}
Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce~Bian, Chao Yin, Chenxu
  Lv, Da~Pan, Dian Wang, Dong Yan, et~al.
\newblock Baichuan 2: Open large-scale language models.
\newblock {\em arXiv preprint arXiv:2309.10305}, 2023.

\bibitem{yao2024machine}
Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and
  Xiang Yue.
\newblock Machine unlearning of pre-trained large language models.
\newblock {\em arXiv preprint arXiv:2402.15159}, 2024.

\bibitem{yao2023large}
Yuanshun Yao, Xiaojun Xu, and Yang Liu.
\newblock Large language model unlearning.
\newblock {\em arXiv preprint arXiv:2310.10683}, 2023.

\bibitem{young2024yi}
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge~Zhang, Guanwei Zhang, Heng Li,
  Jiangcheng Zhu, Jianqun Chen, Jing Chang, et~al.
\newblock Yi: Open foundation models by 01. ai.
\newblock {\em arXiv preprint arXiv:2403.04652}, 2024.

\bibitem{yuan2024rigorllm}
Zhuowen Yuan, Zidi Xiong, Yi~Zeng, Ning Yu, Ruoxi Jia, Dawn Song, and Bo~Li.
\newblock Rigorllm: Resilient guardrails for large language models against
  undesired content.
\newblock {\em arXiv preprint arXiv:2403.13031}, 2024.

\bibitem{zellers2019hellaswag}
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.
\newblock Hellaswag: Can a machine really finish your sentence?
\newblock {\em arXiv preprint arXiv:1905.07830}, 2019.

\bibitem{zhang2024chemllm}
Di~Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li,
  Weiran Huang, Xiangyu Yue, Dongzhan Zhou, Shufei Zhang, Mao Su, Hansen Zhong,
  Yuqiang Li, and Wanli Ouyang.
\newblock Chemllm: A chemical large language model, 2024.

\bibitem{zhang2023composing}
Jinghan Zhang, Shiqi Chen, Junteng Liu, and Junxian He.
\newblock Composing parameter-efficient modules with arithmetic operations.
\newblock {\em arXiv preprint arXiv:2306.14870}, 2023.

\bibitem{zhang2024negative}
Ruiqi Zhang, Licong Lin, Yu~Bai, and Song Mei.
\newblock Negative preference optimization: From catastrophic collapse to
  effective unlearning.
\newblock {\em arXiv preprint arXiv:2404.05868}, 2024.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock {\em arXiv preprint arXiv:1904.09675}, 2019.

\bibitem{zhao2024chemdfm}
Zihan Zhao, Da~Ma, Lu~Chen, Liangtai Sun, Zihao Li, Hongshen Xu, Zichen Zhu,
  Su~Zhu, Shuai Fan, Guodong Shen, Xin Chen, and Kai Yu.
\newblock Chemdfm: Dialogue foundation model for chemistry, 2024.

\bibitem{zheng2024judging}
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
  Zhuang, Zi~Lin, Zhuohan Li, Dacheng Li, Eric Xing, et~al.
\newblock Judging llm-as-a-judge with mt-bench and chatbot arena.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{starling2023}
Banghua Zhu, Evan Frick, Tianhao Wu, Hanlin Zhu, Karthik Ganesan, Wei-Lin
  Chiang, Jian Zhang, and Jiantao Jiao.
\newblock Starling-7b: Improving llm helpfulness \& harmlessness with rlaif,
  November 2023.

\bibitem{zou2023universal}
Andy Zou, Zifan Wang, J~Zico Kolter, and Matt Fredrikson.
\newblock Universal and transferable adversarial attacks on aligned language
  models.
\newblock {\em arXiv preprint arXiv:2307.15043}, 2023.

\end{thebibliography}
