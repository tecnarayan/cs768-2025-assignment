\begin{thebibliography}{10}

\bibitem{achille2019task2vec}
Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu
  Maji, Charless~C Fowlkes, Stefano Soatto, and Pietro Perona.
\newblock Task2vec: Task embedding for meta-learning.
\newblock In {\em Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6430--6439, 2019.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{bapna2020controlling}
Ankur Bapna, Naveen Arivazhagan, and Orhan Firat.
\newblock Controlling computation versus quality for neural sequence models.
\newblock {\em arXiv preprint arXiv:2002.07106}, 2020.

\bibitem{bengio2013estimating}
Yoshua Bengio, Nicholas L{\'e}onard, and Aaron Courville.
\newblock Estimating or propagating gradients through stochastic neurons for
  conditional computation.
\newblock {\em arXiv preprint arXiv:1308.3432}, 2013.

\bibitem{capecchi2020one}
Alice Capecchi, Daniel Probst, and Jean-Louis Reymond.
\newblock One molecular fingerprint to rule them all: drugs, biomolecules, and
  the metabolome.
\newblock {\em Journal of cheminformatics}, 12(1):1--15, 2020.

\bibitem{capela2019multitask}
Fabio Capela, Vincent Nouchi, Ruud Van~Deursen, Igor~V Tetko, and Guillaume
  Godin.
\newblock Multitask learning on graph neural networks applied to molecular
  property predictions.
\newblock {\em arXiv preprint arXiv:1910.13124}, 2019.

\bibitem{cereto2015molecular}
Adri{\`a} Cereto-Massagu{\'e}, Mar{\'\i}a~Jos{\'e} Ojeda, Cristina Valls,
  Miquel Mulero, Santiago Garcia-Vallv{\'e}, and Gerard Pujadas.
\newblock Molecular fingerprint similarity search in virtual screening.
\newblock {\em Methods}, 71:58--63, 2015.

\bibitem{chen2018gradnorm}
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.
\newblock Gradnorm: Gradient normalization for adaptive loss balancing in deep
  multitask networks.
\newblock In {\em International conference on machine learning}, pages
  794--803. PMLR, 2018.

\bibitem{doersch2016tutorial}
Carl Doersch.
\newblock Tutorial on variational autoencoders.
\newblock {\em arXiv preprint arXiv:1606.05908}, 2016.

\bibitem{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock {\em arXiv preprint arXiv:1909.11556}, 2019.

\bibitem{fernandez2017database}
Eli Fern{\'a}ndez-de Gortari, C{\'e}sar~R Garc{\'\i}a-Jacas, Karina
  Martinez-Mayorga, and Jos{\'e}~L Medina-Franco.
\newblock Database fingerprint (dfp): an approach to represent molecular
  databases.
\newblock {\em Journal of cheminformatics}, 9(1):1--9, 2017.

\bibitem{fifty2021efficiently}
Chris Fifty, Ehsan Amid, Zhe Zhao, Tianhe Yu, Rohan Anil, and Chelsea Finn.
\newblock Efficiently identifying task groupings for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems},
  34:27503--27516, 2021.

\bibitem{freitag2017beam}
Markus Freitag and Yaser Al-Onaizan.
\newblock Beam search strategies for neural machine translation.
\newblock {\em arXiv preprint arXiv:1702.01806}, 2017.

\bibitem{gaulton2012chembl}
Anna Gaulton, Louisa~J Bellis, A~Patricia Bento, Jon Chambers, Mark Davies,
  Anne Hersey, Yvonne Light, Shaun McGlinchey, David Michalovich, Bissan
  Al-Lazikani, et~al.
\newblock Chembl: a large-scale bioactivity database for drug discovery.
\newblock {\em Nucleic acids research}, 40(D1):D1100--D1107, 2012.

\bibitem{gers2000learning}
Felix~A Gers, J{\"u}rgen Schmidhuber, and Fred Cummins.
\newblock Learning to forget: Continual prediction with lstm.
\newblock {\em Neural computation}, 12(10):2451--2471, 2000.

\bibitem{hou2022graphmae}
Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and
  Jie Tang.
\newblock Graphmae: Self-supervised masked graph autoencoders.
\newblock In {\em Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, pages 594--604, 2022.

\bibitem{hu2021ogb}
Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure
  Leskovec.
\newblock Ogb-lsc: A large-scale challenge for machine learning on graphs.
\newblock {\em arXiv preprint arXiv:2103.09430}, 2021.

\bibitem{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em Advances in neural information processing systems},
  33:22118--22133, 2020.

\bibitem{hu2019strategies}
Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande,
  and Jure Leskovec.
\newblock Strategies for pre-training graph neural networks.
\newblock {\em arXiv preprint arXiv:1905.12265}, 2019.

\bibitem{hu2022improving}
Ziniu Hu, Zhe Zhao, Xinyang Yi, Tiansheng Yao, Lichan Hong, Yizhou Sun, and
  Ed~Chi.
\newblock Improving multi-task generalization via regularizing spurious
  correlation.
\newblock {\em Advances in Neural Information Processing Systems},
  35:11450--11466, 2022.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In {\em International conference on machine learning}, pages
  448--456. pmlr, 2015.

\bibitem{kang2011learning}
Zhuoliang Kang, Kristen Grauman, and Fei Sha.
\newblock Learning with whom to share in multi-task feature learning.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 521--528, 2011.

\bibitem{kim2016pubchem}
Sunghwan Kim, Paul~A Thiessen, Evan~E Bolton, Jie Chen, Gang Fu, Asta
  Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin~A Shoemaker, et~al.
\newblock Pubchem substance and compound databases.
\newblock {\em Nucleic acids research}, 44(D1):D1202--D1213, 2016.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{kudugunta2021beyond}
Sneha Kudugunta, Yanping Huang, Ankur Bapna, Maxim Krikun, Dmitry Lepikhin,
  Minh-Thang Luong, and Orhan Firat.
\newblock Beyond distillation: Task-level mixture-of-experts for efficient
  inference.
\newblock {\em arXiv preprint arXiv:2110.03742}, 2021.

\bibitem{kung2021efficient}
Po-Nien Kung, Sheng-Siang Yin, Yi-Cheng Chen, Tse-Hsuan Yang, and Yun-Nung
  Chen.
\newblock Efficient multi-task auxiliary learning: selecting auxiliary data by
  feature similarity.
\newblock In {\em Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pages 416--428, 2021.

\bibitem{lin2019pareto}
Xi~Lin, Hui-Ling Zhen, Zhenhua Li, Qing-Fu Zhang, and Sam Kwong.
\newblock Pareto multi-task learning.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{liu2022structured}
Shengchao Liu, Meng Qu, Zuobai Zhang, Huiyu Cai, and Jian Tang.
\newblock Structured multi-task learning for molecular property prediction.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 8906--8920. PMLR, 2022.

\bibitem{liu2021pre}
Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian
  Tang.
\newblock Pre-training molecular graph representation with 3d geometry.
\newblock {\em arXiv preprint arXiv:2110.07728}, 2021.

\bibitem{loshchilov2017decoupled}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock {\em arXiv preprint arXiv:1711.05101}, 2017.

\bibitem{meister2020if}
Clara Meister, Tim Vieira, and Ryan Cotterell.
\newblock If beam search is the answer, what was the question?
\newblock {\em arXiv preprint arXiv:2010.02650}, 2020.

\bibitem{mendez2019chembl}
David Mendez, Anna Gaulton, A~Patr{\'\i}cia Bento, Jon Chambers, Marleen
  De~Veij, Eloy F{\'e}lix, Mar{\'\i}a~Paula Magari{\~n}os, Juan~F Mosquera,
  Prudence Mutowo, Micha{\l} Nowotka, et~al.
\newblock Chembl: towards direct deposition of bioassay data.
\newblock {\em Nucleic acids research}, 47(D1):D930--D940, 2019.

\bibitem{pham2021meta}
Hieu Pham, Zihang Dai, Qizhe Xie, and Quoc~V Le.
\newblock Meta pseudo labels.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and
  pattern recognition}, pages 11557--11568, 2021.

\bibitem{qiu2020gcc}
Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang, Ming Ding,
  Kuansan Wang, and Jie Tang.
\newblock Gcc: Graph contrastive coding for graph neural network pre-training.
\newblock In {\em Proceedings of the 26th ACM SIGKDD international conference
  on knowledge discovery \& data mining}, pages 1150--1160, 2020.

\bibitem{riabinin2020learning}
Maksim Riabinin and Anton Gusev.
\newblock Learning@ home: Crowdsourced training of large neural networks using
  decentralized mixture-of-experts.
\newblock {\em arXiv preprint arXiv:/2002.04013}, 2020.

\bibitem{rong2020self}
Yu~Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and
  Junzhou Huang.
\newblock Self-supervised graph transformer on large-scale molecular data.
\newblock {\em Advances in Neural Information Processing Systems},
  33:12559--12571, 2020.

\bibitem{ruder2017overview}
Sebastian Ruder.
\newblock An overview of multi-task learning in deep neural networks.
\newblock {\em arXiv preprint arXiv:1706.05098}, 2017.

\bibitem{sak2014long}
Hasim Sak, Andrew~W Senior, and Fran{\c{c}}oise Beaufays.
\newblock Long short-term memory recurrent neural network architectures for
  large scale acoustic modeling.
\newblock 2014.

\bibitem{saxe2013exact}
Andrew~M Saxe, James~L McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock {\em arXiv preprint arXiv:1312.6120}, 2013.

\bibitem{sener2018multi}
Ozan Sener and Vladlen Koltun.
\newblock Multi-task learning as multi-objective optimization.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{shazeer2017outrageously}
Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le,
  Geoffrey Hinton, and Jeff Dean.
\newblock Outrageously large neural networks: The sparsely-gated
  mixture-of-experts layer.
\newblock {\em arXiv preprint arXiv:1701.06538}, 2017.

\bibitem{shi2022benchmarking}
Yu~Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie
  Luo, Chang Liu, Di~He, and Tie-Yan Liu.
\newblock Benchmarking graphormer on large-scale molecular modeling datasets.
\newblock {\em arXiv preprint arXiv:2203.04810}, 2022.

\bibitem{song2022efficient}
Xiaozhuang Song, Shun Zheng, Wei Cao, James Yu, and Jiang Bian.
\newblock Efficient and effective multi-task grouping via meta learning on task
  combinations.
\newblock In {\em Advances in Neural Information Processing Systems}, 2022.

\bibitem{standley2020tasks}
Trevor Standley, Amir Zamir, Dawn Chen, Leonidas Guibas, Jitendra Malik, and
  Silvio Savarese.
\newblock Which tasks should be learned together in multi-task learning?
\newblock In {\em International Conference on Machine Learning}, pages
  9120--9132. PMLR, 2020.

\bibitem{sukhbaatar2019adaptive}
Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin.
\newblock Adaptive attention span in transformers.
\newblock {\em arXiv preprint arXiv:1905.07799}, 2019.

\bibitem{sun2022does}
Ruoxi Sun, Hanjun Dai, and Adams~Wei Yu.
\newblock Does gnn pretraining help molecular representation?
\newblock {\em Advances in Neural Information Processing Systems},
  35:12096--12109, 2022.

\bibitem{vinyals2015order}
Oriol Vinyals, Samy Bengio, and Manjunath Kudlur.
\newblock Order matters: Sequence to sequence for sets.
\newblock {\em arXiv preprint arXiv:1511.06391}, 2015.

\bibitem{wang2022molecular}
Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati~Farimani.
\newblock Molecular contrastive learning of representations via graph neural
  networks.
\newblock {\em Nature Machine Intelligence}, 4(3):279--287, 2022.

\bibitem{wieder2020compact}
Oliver Wieder, Stefan Kohlbacher, M{\'e}laine Kuenemann, Arthur Garon, Pierre
  Ducrot, Thomas Seidel, and Thierry Langer.
\newblock A compact review of molecular property prediction with graph neural
  networks.
\newblock {\em Drug Discovery Today: Technologies}, 37:1--12, 2020.

\bibitem{wu2018moleculenet}
Zhenqin Wu, Bharath Ramsundar, Evan~N Feinberg, Joseph Gomes, Caleb Geniesse,
  Aneesh~S Pappu, Karl Leswing, and Vijay Pande.
\newblock Moleculenet: a benchmark for molecular machine learning.
\newblock {\em Chemical science}, 9(2):513--530, 2018.

\bibitem{xiong2019pushing}
Zhaoping Xiong, Dingyan Wang, Xiaohong Liu, Feisheng Zhong, Xiaozhe Wan, Xutong
  Li, Zhaojun Li, Xiaomin Luo, Kaixian Chen, Hualiang Jiang, et~al.
\newblock Pushing the boundaries of molecular representation for drug discovery
  with the graph attention mechanism.
\newblock {\em Journal of medicinal chemistry}, 63(16):8749--8760, 2019.

\bibitem{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock {\em arXiv preprint arXiv:1810.00826}, 2018.

\bibitem{yang2021generalized}
Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu.
\newblock Generalized out-of-distribution detection: A survey.
\newblock {\em arXiv preprint arXiv:2110.11334}, 2021.

\bibitem{yang2019analyzing}
Kevin Yang, Kyle Swanson, Wengong Jin, Connor Coley, Philipp Eiden, Hua Gao,
  Angel Guzman-Perez, Timothy Hopper, Brian Kelley, Miriam Mathea, et~al.
\newblock Analyzing learned molecular representations for property prediction.
\newblock {\em Journal of chemical information and modeling}, 59(8):3370--3388,
  2019.

\bibitem{yang2023batchsampler}
Zhen Yang, Tinglin Huang, Ming Ding, Yuxiao Dong, Rex Ying, Yukuo Cen, Yangliao
  Geng, and Jie Tang.
\newblock Batchsampler: Sampling mini-batches for contrastive learning in
  vision, language, and graphs.
\newblock {\em arXiv preprint arXiv:2306.03355}, 2023.

\bibitem{ying2021transformers}
Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di~He,
  Yanming Shen, and Tie-Yan Liu.
\newblock Do transformers really perform badly for graph representation?
\newblock {\em Advances in Neural Information Processing Systems},
  34:28877--28888, 2021.

\bibitem{yu2023mind}
Junchi Yu, Jian Liang, and Ran He.
\newblock Mind the label shift of augmentation-based graph ood generalization.
\newblock {\em arXiv preprint arXiv:2303.14859}, 2023.

\bibitem{yu2020gradient}
Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and
  Chelsea Finn.
\newblock Gradient surgery for multi-task learning.
\newblock {\em Advances in Neural Information Processing Systems},
  33:5824--5836, 2020.

\bibitem{zamir2018taskonomy}
Amir~R Zamir, Alexander Sax, William Shen, Leonidas~J Guibas, Jitendra Malik,
  and Silvio Savarese.
\newblock Taskonomy: Disentangling task transfer learning.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3712--3722, 2018.

\bibitem{zhang2021share}
Biao Zhang, Ankur Bapna, Rico Sennrich, and Orhan Firat.
\newblock Share or not? learning to schedule language-specific capacity for
  multilingual translation.
\newblock 2021.

\bibitem{zhang2020sparsifying}
Biao Zhang, Ivan Titov, and Rico Sennrich.
\newblock On sparsifying encoder outputs in sequence-to-sequence models.
\newblock {\em arXiv preprint arXiv:2004.11854}, 2020.

\bibitem{zhou2023uni}
Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei,
  Linfeng Zhang, and Guolin Ke.
\newblock Uni-mol: a universal 3d molecular representation learning framework.
\newblock 2023.

\end{thebibliography}
