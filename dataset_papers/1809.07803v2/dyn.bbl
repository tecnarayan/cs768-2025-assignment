\begin{thebibliography}{28}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Barreto et~al.(2016)Barreto, Munos, Schaul, and
  Silver]{DBLP:journals/corr/BarretoMSS16}
Barreto, A., Munos, R., Schaul, T., and Silver, D.
\newblock Successor features for transfer in reinforcement learning.
\newblock \emph{CoRR}, abs/1606.05312, 2016.
\newblock URL \url{http://arxiv.org/abs/1606.05312}.

\bibitem[Bellemare et~al.(2012)Bellemare, Naddaf, Veness, and
  Bowling]{DBLP:journals/corr/abs-1207-4708}
Bellemare, M.~G., Naddaf, Y., Veness, J., and Bowling, M.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{CoRR}, abs/1207.4708, 2012.
\newblock URL \url{http://arxiv.org/abs/1207.4708}.

\bibitem[Borsa et~al.(2019)Borsa, Barreto, Quan, Mankowitz, van Hasselt, Munos,
  Silver, and Schaul]{borsa2018universal}
Borsa, D., Barreto, A., Quan, J., Mankowitz, D.~J., van Hasselt, H., Munos, R.,
  Silver, D., and Schaul, T.
\newblock Universal successor features approximators.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=S1VWjiRcKX}.

\bibitem[De~Bruin et~al.(2018)De~Bruin, Kober, Tuyls, and
  Babu{\v{s}}ka]{de2018experience}
De~Bruin, T., Kober, J., Tuyls, K., and Babu{\v{s}}ka, R.
\newblock Experience selection in deep reinforcement learning for control.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 347--402, 2018.

\bibitem[Deb et~al.(2002)Deb, Pratap, Agarwal, and A.~M. T.~Meyarivan]{nsgaii}
Deb, K., Pratap, A., Agarwal, S., and A.~M. T.~Meyarivan, T.
\newblock A fast and elitist multiobjective genetic algorithm: Nsga-ii.
\newblock 6:\penalty0 182 -- 197, 05 2002.

\bibitem[Du et~al.(2016)Du, de~la Cruz, Irwin, and Taylor]{2016DeepRL-Du}
Du, Y., de~la Cruz, Jr., G.~V., Irwin, J., and Taylor, M.~E.
\newblock {Initial Progress in Transfer for Deep Reinforcement Learning
  Algorithms}.
\newblock In \emph{{Proceedings of Deep Reinforcement Learning: Frontiers and
  Challenges workshop (at {IJCAI})}}, New York City, NY, USA, July 2016.

\bibitem[Isele \& Cosgun(2018)Isele and
  Cosgun]{DBLP:journals/corr/abs-1802-10269}
Isele, D. and Cosgun, A.
\newblock Selective experience replay for lifelong learning.
\newblock \emph{CoRR}, abs/1802.10269, 2018.
\newblock URL \url{http://arxiv.org/abs/1802.10269}.

\bibitem[Liu et~al.(2015)Liu, Xu, and Hu]{6918520}
Liu, C., Xu, X., and Hu, D.
\newblock Multiobjective reinforcement learning: A comprehensive overview.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  45\penalty0 (3):\penalty0 385--398, March 2015.
\newblock ISSN 2168-2216.
\newblock \doi{10.1109/TSMC.2014.2358639}.

\bibitem[Ma et~al.(2018)Ma, Wen, and Bengio]{DBLP:journals/corr/abs-1804-03758}
Ma, C., Wen, J., and Bengio, Y.
\newblock Universal successor representations for transfer reinforcement
  learning.
\newblock \emph{CoRR}, abs/1804.03758, 2018.
\newblock URL \url{http://arxiv.org/abs/1804.03758}.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{DBLP:journals/corr/MnihKSGAWR13}
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra,
  D., and Riedmiller, M.~A.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{CoRR}, abs/1312.5602, 2013.
\newblock URL \url{http://arxiv.org/abs/1312.5602}.

\bibitem[Moffaert \& Now{{\'e}}(2014)Moffaert and
  Now{{\'e}}]{JMLR:v15:vanmoffaert14a}
Moffaert, K.~V. and Now{{\'e}}, A.
\newblock Multi-objective reinforcement learning using sets of pareto
  dominating policies.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 3663--3692,
  2014.
\newblock URL \url{http://jmlr.org/papers/v15/vanmoffaert14a.html}.

\bibitem[Mossalam et~al.(2016)Mossalam, Assael, Roijers, and
  Whiteson]{DBLP:journals/corr/MossalamARW16}
Mossalam, H., Assael, Y.~M., Roijers, D.~M., and Whiteson, S.
\newblock Multi-objective deep reinforcement learning.
\newblock \emph{CoRR}, abs/1610.02707, 2016.
\newblock URL \url{http://arxiv.org/abs/1610.02707}.

\bibitem[Natarajan \& Tadepalli(2005)Natarajan and
  Tadepalli]{Natarajan:2005:DPM:1102351.1102427}
Natarajan, S. and Tadepalli, P.
\newblock Dynamic preferences in multi-criteria reinforcement learning.
\newblock In \emph{Proceedings of the 22nd International Conference on Machine
  Learning}, ICML '05, pp.\  601--608, New York, NY, USA, 2005. ACM.
\newblock ISBN 1-59593-180-5.
\newblock \doi{10.1145/1102351.1102427}.
\newblock URL \url{http://doi.acm.org/10.1145/1102351.1102427}.

\bibitem[Parisotto et~al.(2015)Parisotto, Ba, and
  Salakhutdinov]{DBLP:journals/corr/ParisottoBS15}
Parisotto, E., Ba, L.~J., and Salakhutdinov, R.
\newblock Actor-mimic: Deep multitask and transfer reinforcement learning.
\newblock \emph{CoRR}, abs/1511.06342, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.06342}.

\bibitem[Roijers \& Whiteson(2017)Roijers and Whiteson]{roijers2017multi}
Roijers, D.~M. and Whiteson, S.
\newblock Multi-objective decision making.
\newblock \emph{Synthesis Lectures on Artificial Intelligence and Machine
  Learning}, 11\penalty0 (1):\penalty0 1--129, 2017.

\bibitem[Roijers et~al.(2013)Roijers, Vamplew, Whiteson, and
  Dazeley]{roijers:2013:sms:2591248.2591251}
Roijers, D.~M., Vamplew, P., Whiteson, S., and Dazeley, R.
\newblock A survey of multi-objective sequential decision-making.
\newblock \emph{J. Artif. Int. Res.}, 48\penalty0 (1):\penalty0 67--113,
  October 2013.
\newblock ISSN 1076-9757.
\newblock URL \url{http://dl.acm.org/citation.cfm?id=2591248.2591251}.

\bibitem[Schaul et~al.(2015{\natexlab{a}})Schaul, Horgan, Gregor, and
  Silver]{pmlr-v37-schaul15}
Schaul, T., Horgan, D., Gregor, K., and Silver, D.
\newblock Universal value function approximators.
\newblock In Bach, F. and Blei, D. (eds.), \emph{Proceedings of the 32nd
  International Conference on Machine Learning}, volume~37 of \emph{Proceedings
  of Machine Learning Research}, pp.\  1312--1320, Lille, France, 07--09 Jul
  2015{\natexlab{a}}. PMLR.
\newblock URL \url{http://proceedings.mlr.press/v37/schaul15.html}.

\bibitem[Schaul et~al.(2015{\natexlab{b}})Schaul, Quan, Antonoglou, and
  Silver]{dblp:journals/corr/schaulqas15}
Schaul, T., Quan, J., Antonoglou, I., and Silver, D.
\newblock Prioritized experience replay.
\newblock \emph{CoRR}, abs/1511.05952, 2015{\natexlab{b}}.
\newblock URL \url{http://arxiv.org/abs/1511.05952}.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{sutton1998reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock A Bradford book. Bradford Book, 1998.
\newblock ISBN 9780262193986.
\newblock URL \url{https://books.google.be/books?id=CAFR6IBF4xYC}.

\bibitem[Taylor \& Stone(2009)Taylor and Stone]{jmlr09-taylor}
Taylor, M.~E. and Stone, P.
\newblock Transfer learning for reinforcement learning domains: A survey.
\newblock \emph{Journal of Machine Learning Research}, 10\penalty0
  (1):\penalty0 1633--1685, 2009.

\bibitem[Tsitsiklis(1994)]{Tsitsiklis1994}
Tsitsiklis, J.~N.
\newblock Asynchronous stochastic approximation and q-learning.
\newblock \emph{Machine Learning}, 16\penalty0 (3):\penalty0 185--202, Sep
  1994.
\newblock ISSN 1573-0565.
\newblock \doi{10.1023/A:1022689125041}.
\newblock URL \url{https://doi.org/10.1023/A:1022689125041}.

\bibitem[Vamplew et~al.(2011)Vamplew, Dazeley, Berry, Issabekov, and
  Dekker]{Vamplew2011}
Vamplew, P., Dazeley, R., Berry, A., Issabekov, R., and Dekker, E.
\newblock Empirical evaluation methods for multiobjective reinforcement
  learning algorithms.
\newblock \emph{Machine Learning}, 84\penalty0 (1):\penalty0 51--80, Jul 2011.
\newblock ISSN 1573-0565.
\newblock \doi{10.1007/s10994-010-5232-5}.
\newblock URL \url{https://doi.org/10.1007/s10994-010-5232-5}.

\bibitem[Van~Hasselt et~al.(2016)Van~Hasselt, Guez, and Silver]{van2016deep}
Van~Hasselt, H., Guez, A., and Silver, D.
\newblock Deep reinforcement learning with double q-learning.
\newblock In \emph{AAAI}, pp.\  2094--2100, 2016.

\bibitem[Wang et~al.(2015)Wang, de~Freitas, and
  Lanctot]{dblp:journals/corr/wangfl15}
Wang, Z., de~Freitas, N., and Lanctot, M.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock \emph{CoRR}, abs/1511.06581, 2015.
\newblock URL \url{http://arxiv.org/abs/1511.06581}.

\bibitem[Watkins(1989)]{Watkins:1989}
Watkins, C. J. C.~H.
\newblock \emph{Learning from Delayed Rewards}.
\newblock PhD thesis, King's College, Cambridge, UK, May 1989.
\newblock URL \url{http://www.cs.rhul.ac.uk/~chrisw/new_thesis.pdf}.

\bibitem[White \& Kim(1980)White and Kim]{WhiteKim80}
White, C.~C. and Kim, K.~M.
\newblock Solution procedures for solving vector criterion {M}arkov decision
  processes.
\newblock \emph{Large Scale Systems}, 1:\penalty0 129--140, 1980.

\bibitem[Xiong et~al.(2016)Xiong, Wang, Zhang, and
  Li]{DBLP:journals/corr/XiongWZL16}
Xiong, X., Wang, J., Zhang, F., and Li, K.
\newblock Combining deep reinforcement learning and safety based control for
  autonomous driving.
\newblock \emph{CoRR}, abs/1612.00147, 2016.
\newblock URL \url{http://arxiv.org/abs/1612.00147}.

\bibitem[Zauner(2010)]{zauner2010implementation}
Zauner, C.
\newblock Implementation and benchmarking of perceptual image hash functions.
\newblock 2010.

\end{thebibliography}
