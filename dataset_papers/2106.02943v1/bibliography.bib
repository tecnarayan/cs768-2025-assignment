@article{lecun-deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{dqn,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}


@article{alphazero,
  title={Mastering chess and shogi by self-play with a general reinforcement learning algorithm},
  author={Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  journal={arXiv preprint arXiv:1712.01815},
  year={2017}
}


@article{dmc,
  title={Deepmind control suite},
  author={Tassa, Yuval and Doron, Yotam and Muldal, Alistair and Erez, Tom and Li, Yazhe and Casas, Diego de Las and Budden, David and Abdolmaleki, Abbas and Merel, Josh and Lefrancq, Andrew and others},
  journal={arXiv preprint arXiv:1801.00690},
  year={2018}
}

@article{maxentobj,
  title={Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  author={Ziebart, Brian D},
  year={2010}
}

@inproceedings{pg-thm,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  booktitle={Advances in neural information processing systems},
  pages={1057--1063},
  year={2000}
}

@inproceedings{dpg,
  title={Deterministic policy gradient algorithms},
  author={Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
  year={2014}
}

% From DisentanGAIL

@article{ppo,
  author    = {John Schulman and
               Filip Wolski and
               Prafulla Dhariwal and
               Alec Radford and
               Oleg Klimov},
  title     = {Proximal Policy Optimization Algorithms},
  journal   = {CoRR},
  volume    = {abs/1707.06347},
  year      = {2017},
  url       = {http://arxiv.org/abs/1707.06347},
  archivePrefix = {arXiv},
  eprint    = {1707.06347},
  timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

@article{sac,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1801.01290},
  year={2018}
}

@article{maxentirl,
  title={Maximum entropy inverse reinforcement learning},
  author={Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
  year={2008},
  publisher={figshare}
}

@article{maxentdeepirl,
  title={Maximum entropy deep inverse reinforcement learning},
  author={Wulfmeier, Markus and Ondruska, Peter and Posner, Ingmar},
  journal={arXiv preprint arXiv:1507.04888},
  year={2015}
}

@inproceedings{gan,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@inproceedings{gail,
  title={Generative adversarial imitation learning},
  author={Ho, Jonathan and Ermon, Stefano},
  booktitle={Advances in neural information processing systems},
  pages={4565--4573},
  year={2016}
}

@article{airl,
  title={Learning robust rewards with adversarial inverse reinforcement learning},
  author={Fu, Justin and Luo, Katie and Levine, Sergey},
  journal={arXiv preprint arXiv:1710.11248},
  year={2017}
}

@article{dac,
  title={Discriminator-actor-critic: Addressing sample inefficiency and reward bias in adversarial imitation learning},
  author={Kostrikov, Ilya and Agrawal, Kumar Krishna and Dwibedi, Debidatta and Levine, Sergey and Tompson, Jonathan},
  year={2018}
}

@article{nice,
  title={Nice: Non-linear independent components estimation},
  author={Dinh, Laurent and Krueger, David and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1410.8516},
  year={2014}
}

@article{realnvp,
  title={Density estimation using real nvp},
  author={Dinh, Laurent and Sohl-Dickstein, Jascha and Bengio, Samy},
  journal={arXiv preprint arXiv:1605.08803},
  year={2016}
}

@inproceedings{glow,
  title={Glow: Generative flow with invertible 1x1 convolutions},
  author={Kingma, Durk P and Dhariwal, Prafulla},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10215--10224},
  year={2018}
}

@article{flow++,
  title={Flow++: Improving flow-based generative models with variational dequantization and architecture design},
  author={Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1902.00275},
  year={2019}
}

@inproceedings{imi1,
  title={Alvinn: An autonomous land vehicle in a neural network},
  author={Pomerleau, Dean A},
  booktitle={Advances in neural information processing systems},
  pages={305--313},
  year={1989}
}

@article{imi15,
  title={Efficient training of artificial neural networks for autonomous navigation},
  author={Pomerleau, Dean A},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={88--97},
  year={1991},
  publisher={MIT Press}
}

@inproceedings{imi2,
  title={A reduction of imitation learning and structured prediction to no-regret online learning},
  author={Ross, St{\'e}phane and Gordon, Geoffrey and Bagnell, Drew},
  booktitle={Proceedings of the fourteenth international conference on artificial intelligence and statistics},
  pages={627--635},
  year={2011}
}

@inproceedings{irl1,
  title={Algorithms for inverse reinforcement learning.},
  author={Ng, Andrew Y and Russell, Stuart J and others},
  booktitle={Icml},
  volume={1},
  pages={2},
  year={2000}
}

@inproceedings{irl2,
  title={Apprenticeship learning via inverse reinforcement learning},
  author={Abbeel, Pieter and Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={1},
  year={2004},
  organization={ACM}
}

@inproceedings{irl3,
  title={Maximum margin planning},
  author={Ratliff, Nathan D and Bagnell, J Andrew and Zinkevich, Martin A},
  booktitle={Proceedings of the 23rd international conference on Machine learning},
  pages={729--736},
  year={2006},
  organization={ACM}
}

@inproceedings{gcl,
  title={Guided cost learning: Deep inverse optimal control via policy optimization},
  author={Finn, Chelsea and Levine, Sergey and Abbeel, Pieter},
  booktitle={International Conference on Machine Learning},
  pages={49--58},
  year={2016}
}

@article{connection,
  title={A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models},
  author={Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1611.03852},
  year={2016}
}

@article{engi1,
  title={An object-based approach to map human hand synergies onto robotic hands with dissimilar kinematics},
  author={Gioioso, Guido and Salvietti, Gionata and Malvezzi, Monica and Prattichizzo, Daniele},
  journal={Robotics: Science and Systems VIII},
  pages={97--104},
  year={2012},
  publisher={The MIT Press Sydney, NSW}
}

@inproceedings{engi2,
  title={Learning dexterous manipulation for a soft robotic hand from human demonstrations},
  author={Gupta, Abhishek and Eppner, Clemens and Levine, Sergey and Abbeel, Pieter},
  booktitle={2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={3786--3793},
  year={2016},
  organization={IEEE}
}

@article{reli1,
  title={Learning invariant feature spaces to transfer skills with reinforcement learning},
  author={Gupta, Abhishek and Devin, Coline and Liu, YuXuan and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1703.02949},
  year={2017}
}

@inproceedings{reli2,
  title={Time-contrastive networks: Self-supervised learning from video},
  author={Sermanet, Pierre and Lynch, Corey and Chebotar, Yevgen and Hsu, Jasmine and Jang, Eric and Schaal, Stefan and Levine, Sergey and Brain, Google},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1134--1141},
  year={2018},
  organization={IEEE}
}

@inproceedings{reli3,
  title={Imitation from observation: Learning to imitate behaviors from raw video via context translation},
  author={Liu, YuXuan and Gupta, Abhishek and Abbeel, Pieter and Levine, Sergey},
  booktitle={2018 IEEE International Conference on Robotics and Automation (ICRA)},
  pages={1118--1125},
  year={2018},
  organization={IEEE}
}

@article{reli4,
  title={AVID: Learning Multi-Stage Tasks via Pixel-Level Translation of Human Videos},
  author={Smith, Laura and Dhawan, Nikita and Zhang, Marvin and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1912.04443},
  year={2019}
}

@inproceedings{reli5,
  title={Third-Person Visual Imitation Learning via Decoupled Hierarchical Controller},
  author={Sharma, Pratyusha and Pathak, Deepak and Gupta, Abhinav},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2593--2603},
  year={2019}
}

@article{cdil,
  title={Cross Domain Imitation Learning},
  author={Kim, Kun Ho and Gu, Yihong and Song, Jiaming and Zhao, Shengjia and Ermon, Stefano},
  journal={arXiv preprint arXiv:1910.00105},
  year={2019}
}

@article{tpil,
  title={Third-person imitation learning},
  author={Stadie, Bradly C and Abbeel, Pieter and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1703.01703},
  year={2017}
}


@article{vae,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{iwae,
  title={Importance weighted autoencoders},
  author={Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1509.00519},
  year={2015}
}

@inproceedings{infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2172--2180},
  year={2016}
}

@article{robonet,
  title={RoboNet: Large-Scale Multi-Robot Learning},
  author={Dasari, Sudeep and Ebert, Frederik and Tian, Stephen and Nair, Suraj and Bucher, Bernadette and Schmeckpeper, Karl and Singh, Siddharth and Levine, Sergey and Finn, Chelsea},
  journal={arXiv preprint arXiv:1910.11215},
  year={2019}
}

@article{wake,
  title={The" wake-sleep" algorithm for unsupervised neural networks},
  author={Hinton, Geoffrey E and Dayan, Peter and Frey, Brendan J and Neal, Radford M},
  journal={Science},
  volume={268},
  number={5214},
  pages={1158--1161},
  year={1995},
  publisher={American Association for the Advancement of Science}
}

@article{vdb,
  title={Variational discriminator bottleneck: Improving imitation learning, inverse rl, and gans by constraining information flow},
  author={Peng, Xue Bin and Kanazawa, Angjoo and Toyer, Sam and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1810.00821},
  year={2018}
}

@article{td3,
  title={Addressing function approximation error in actor-critic methods},
  author={Fujimoto, Scott and Van Hoof, Herke and Meger, David},
  journal={arXiv preprint arXiv:1802.09477},
  year={2018}
}

@article{adv-ae,
  title={Adversarial autoencoders},
  author={Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  journal={arXiv preprint arXiv:1511.05644},
  year={2015}
}

@article{rev-2019,
  title={Recent advances in imitation learning from observation},
  author={Torabi, Faraz and Warnell, Garrett and Stone, Peter},
  journal={arXiv preprint arXiv:1905.13566},
  year={2019}
}

@inproceedings{mine,
  title={Mutual information neural estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, Devon},
  booktitle={International Conference on Machine Learning},
  pages={531--540},
  year={2018}
}

@article{mi-motivation,
  title={Equitability, mutual information, and the maximal information coefficient},
  author={Kinney, Justin B and Atwal, Gurinder S},
  journal={Proceedings of the National Academy of Sciences},
  volume={111},
  number={9},
  pages={3354--3359},
  year={2014},
  publisher={National Acad Sciences}
}

@article{mi-challenges,
  title={Estimation of entropy and mutual information},
  author={Paninski, Liam},
  journal={Neural computation},
  volume={15},
  number={6},
  pages={1191--1253},
  year={2003},
  publisher={MIT Press}
}

@inproceedings{uda1,
  title={A unified feature disentangler for multi-domain image translation and manipulation},
  author={Liu, Alexander H and Liu, Yen-Cheng and Yeh, Yu-Ying and Wang, Yu-Chiang Frank},
  booktitle={Advances in neural information processing systems},
  pages={2590--2599},
  year={2018}
}

@article{saturation,
  title={Towards Principled Methods for Training Generative Adversarial Networks. arXiv e-prints, art},
  author={Arjovsky, Martin and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.04862},
  year={2017}
}

@article{donsker,
  title={Asymptotic evaluation of certain Markov process expectations for large time, I},
  author={Donsker, Monroe D and Varadhan, SR Srinivasa},
  journal={Communications on Pure and Applied Mathematics},
  volume={28},
  number={1},
  pages={1--47},
  year={1975},
  publisher={Wiley Online Library}
}

@inproceedings{trpo,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015}
}

@article{sac-alg,
  title={Soft actor-critic algorithms and applications},
  author={Haarnoja, Tuomas and Zhou, Aurick and Hartikainen, Kristian and Tucker, George and Ha, Sehoon and Tan, Jie and Kumar, Vikash and Zhu, Henry and Gupta, Abhishek and Abbeel, Pieter and others},
  journal={arXiv preprint arXiv:1812.05905},
  year={2018}
}

@inproceedings{double-q,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Thirtieth AAAI conference on artificial intelligence},
  year={2016}
}


@article{sngan,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  journal={arXiv preprint arXiv:1802.05957},
  year={2018}
}

@article{wass,
  title={Wasserstein gan},
  author={Arjovsky, Martin and Chintala, Soumith and Bottou, L{\'e}on},
  journal={arXiv preprint arXiv:1701.07875},
  year={2017}
}

@inproceedings{wass-gp,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  booktitle={Advances in neural information processing systems},
  pages={5767--5777},
  year={2017}
}

\@article{adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{mujoco,
  title={Mujoco: A physics engine for model-based control},
  author={Todorov, Emanuel and Erez, Tom and Tassa, Yuval},
  booktitle={2012 IEEE/RSJ International Conference on Intelligent Robots and Systems},
  pages={5026--5033},
  year={2012},
  organization={IEEE}
}

@inproceedings{infogan,
  title={Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
  author={Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  booktitle={Advances in neural information processing systems},
  pages={2172--2180},
  year={2016}
}

@article{mi1,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@article{mi2,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1808.06670},
  year={2018}
}

@article{gym,
  title={Openai gym},
  author={Brockman, Greg and Cheung, Vicki and Pettersson, Ludwig and Schneider, Jonas and Schulman, John and Tang, Jie and Zaremba, Wojciech},
  journal={arXiv preprint arXiv:1606.01540},
  year={2016}
}

@article{kl-div,
  title={On information and sufficiency},
  author={Kullback, Solomon and Leibler, Richard A},
  journal={The annals of mathematical statistics},
  volume={22},
  number={1},
  pages={79--86},
  year={1951},
  publisher={JSTOR}
}

@inproceedings{handful,
  title={Deep reinforcement learning in a handful of trials using probabilistic dynamics models},
  author={Chua, Kurtland and Calandra, Roberto and McAllister, Rowan and Levine, Sergey},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4754--4765},
  year={2018}
}

@inproceedings{maml, 
  title={Model-agnostic meta-learning for fast adaptation of deep networks}, 
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey}, 
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70}, 
  pages={1126--1135}, 
  year={2017}, 
  organization={JMLR. org} 
} 

@article{meta2, 
  title={Memory-based control with recurrent neural networks}, 
  author={Heess, Nicolas and Hunt, Jonathan J and Lillicrap, Timothy P and Silver, David}, 
  journal={arXiv preprint arXiv:1512.04455}, 
  year={2015} 
} 

@article{meta3, 
  title={RL $\^{} 2$: Fast Reinforcement Learning via Slow Reinforcement Learning}, 
  author={Duan, Yan and Schulman, John and Chen, Xi and Bartlett, Peter L and Sutskever, Ilya and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1611.02779}, 
  year={2016} 
} 

@book{convex,
  title={Convex optimization},
  author={Boyd, Stephen and Boyd, Stephen P and Vandenberghe, Lieven},
  year={2004},
  publisher={Cambridge university press}
}

@article{challenges-rl,
  title={Challenges of real-world reinforcement learning},
  author={Dulac-Arnold, Gabriel and Mankowitz, Daniel and Hester, Todd},
  journal={arXiv preprint arXiv:1904.12901},
  year={2019}
}

@article{rl-real-world-design,
  title={Benchmarking reinforcement learning algorithms on real-world robots},
  author={Mahmood, A Rupam and Korenkevych, Dmytro and Vasan, Gautham and Ma, William and Bergstra, James},
  journal={arXiv preprint arXiv:1809.07731},
  year={2018}
}

@inproceedings{rainbow,
  title={Rainbow: Combining improvements in deep reinforcement learning},
  author={Hessel, Matteo and Modayil, Joseph and Van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  number={1},
  year={2018}
}

@book{sutton-reinforcement-n-step,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@inproceedings{A3c,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016}
}

@inproceedings{dynamic-action-rep,
  title={Dynamic action repetition for deep reinforcement learning},
  author={Lakshminarayanan, Aravind and Sharma, Sahil and Ravindran, Balaraman},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{frameskip-importance,
  title={Frame skip is a powerful parameter for learning to play atari},
  author={Braylan, Alex and Hollenbeck, Mark and Meyerson, Elliot and Miikkulainen, Risto},
  booktitle={Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence},
  year={2015}
}

@article{d4pg,
  title={Distributed distributional deterministic policy gradients},
  author={Barth-Maron, Gabriel and Hoffman, Matthew W and Budden, David and Dabney, Will and Horgan, Dan and Tb, Dhruva and Muldal, Alistair and Heess, Nicolas and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1804.08617},
  year={2018}
}

@article{STRAW,
  title={Strategic attentive writer for learning macro-actions},
  author={Vezhnevets, Alexander and Mnih, Volodymyr and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Agapiou, John and others},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3486--3494},
  year={2016}
}

% References for Related works

% Options intro

%paper proposing options

@article{opt-intro1,
  title={Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning},
  author={Sutton, Richard S and Precup, Doina and Singh, Satinder},
  journal={Artificial intelligence},
  volume={112},
  number={1-2},
  pages={181--211},
  year={1999},
  publisher={Elsevier}
}

@article{opt-intro2,
  title={Temporal abstraction in reinforcement learning.},
  author={Precup, Doina},
  year={2001}
}

% Options learnt from subgoals

@inproceedings{opt-sub-1,
 author = {Dayan, Peter and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Hanson and J. Cowan and C. Giles},
 pages = {271--278},
 publisher = {Morgan-Kaufmann},
 title = {Feudal Reinforcement Learning},
 url = {https://proceedings.neurips.cc/paper/1992/file/d14220ee66aeec73c49038385428ec4c-Paper.pdf},
 volume = {5},
 year = {1993}
}


% decomposing MDP in a hierarchy of smaller MDPs, decomposing value f'n into addittive combination of the smaller MDPs - hierarchical more than options

@article{opt-sub-2, 
  title={Hierarchical reinforcement learning with the MAXQ value function decomposition},
  author={Dietterich, Thomas G},
  journal={Journal of artificial intelligence research},
  volume={13},
  pages={227--303},
  year={2000}
}


% + opt-end-to-end

@inproceedings{option-critic,
  title={The option-critic architecture},
  author={Bacon, Pierre-Luc and Harb, Jean and Precup, Doina},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

% Hierarchical RL through subgoal specification

% build a hierarchical policy by re-using knowledge from previous skills in a lifelong learning setting
@inproceedings{hierarchical-subgoals-mine, 
  title={A deep hierarchical approach to lifelong learning in minecraft},
  author={Tessler, Chen and Givony, Shahar and Zahavy, Tom and Mankowitz, Daniel and Mannor, Shie},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

% Learn low level skills through incentivizing particular simple behaviour in low-level env. (e.g. for locomotion simply movement of the agent) + diversity through MI. Re-use these skills for higher-level controller to acquire more complex behavior.
@article{hierarchical-subgoals-small-prior, 
  title={Stochastic neural networks for hierarchical reinforcement learning},
  author={Florensa, Carlos and Duan, Yan and Abbeel, Pieter},
  journal={arXiv preprint arXiv:1704.03012},
  year={2017}
}

% learn embedding space which can be viewed as a higher-level hierarchical action space via learning many specified subtasks
@inproceedings{hierarchical-subgoals-robot,
  title={Learning an embedding space for transferable robot skills},
  author={Hausman, Karol and Springenberg, Jost Tobias and Wang, Ziyu and Heess, Nicolas and Riedmiller, Martin},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

% Intrinsic subgoals

% intrinsic objective used for low-level to learn options
@article{hierarchical-subgoals-intrinsic-1,
  title={Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation},
  author={Kulkarni, Tejas D and Narasimhan, Karthik and Saeedi, Ardavan and Tenenbaum, Josh},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3675--3683},
  year={2016}
}

% learn set of intrinsic options by maximizing the num. of reachable states.
@article{hierarchical-subgoals-intrinsic-2,
  title={Variational intrinsic control},
  author={Gregor, Karol and Rezende, Danilo Jimenez and Wierstra, Daan},
  journal={arXiv preprint arXiv:1611.07507},
  year={2016}
}


% Learn better representations of single actions

% Learn to embed large discrete action spaces into a continuous param. with KNN
@article{single-action-rep-discrete-1,
  title={Deep reinforcement learning in large discrete action spaces},
  author={Dulac-Arnold, Gabriel and Evans, Richard and van Hasselt, Hado and Sunehag, Peter and Lillicrap, Timothy and Hunt, Jonathan and Mann, Timothy and Weber, Theophane and Degris, Thomas and Coppin, Ben},
  journal={arXiv preprint arXiv:1512.07679},
  year={2015}
}

% Bundle actions using a max. likelihood objective from state next state recover action (latent representation is used as new action space and decoder as the mapping from the rep. back to an executable action)
@article{single-action-rep-discrete-2,
  title={Learning action representations for reinforcement learning},
  author={Chandak, Yash and Theocharous, Georgios and Kostas, James and Jordan, Scott and Thomas, Philip S},
  journal={arXiv preprint arXiv:1902.00183},
  year={2019}
}

% Action-representations from demos

@article{demos-act-rep,
  title={The natural language of actions},
  author={Tennenholtz, Guy and Mannor, Shie},
  journal={arXiv preprint arXiv:1902.01119},
  year={2019}
}

% Learn to repeat actions - temporally extended behaviour - using a set of Macro-actions defined as repeating the same action multiple times


% Augments space of discrete actions by considering repetitions at 2 different time-scales 
@inproceedings{action-reps-discrete,
  title={Dynamic action repetition for deep reinforcement learning},
  author={Lakshminarayanan, Aravind and Sharma, Sahil and Ravindran, Balaraman},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={31},
  number={1},
  year={2017}
}

% factors policy in 2 components 1) for selecting action 2) for selecting reps
@article{action-reps-policy-factor,
  title={Learning to repeat: Fine grained action repetition for deep reinforcement learning},
  author={Sharma, Sahil and Lakshminarayanan, Aravind S and Ravindran, Balaraman},
  journal={arXiv preprint arXiv:1702.06054},
  year={2017}
}

% Macro actions - related concept representing uncertain higher level behavior abstractions

@inproceedings{macro-intro,
  title={Planning with closed-loop macro actions},
  author={Precup, Doina and Sutton, Richard S and Singh, Satinder P},
  booktitle={Working notes of the 1997 AAAI Fall Symposium on Model-directed Autonomous Systems},
  pages={70--76},
  year={1997}
}

% keeps a stored running plan to execute that is adaptively updated through attentive reading and writing operators.
@article{macro-STRAWS,
  title={Strategic attentive writer for learning macro-actions},
  author={Vezhnevets, Alexander and Mnih, Volodymyr and Osindero, Simon and Graves, Alex and Vinyals, Oriol and Agapiou, John and others},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={3486--3494},
  year={2016}
}


% Highly relevant, based on target-states to parameterize low level behaviour (e.g. intentions) - to review


% Hierarchical, framework, higher level controller sets goals to be reached by lower-level modules in latent space
@article{FuN,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1703.01161},
  year={2017}
}

% based on next state - learn trajectory AE (only states) and policy (conditioned on state and latent dims of AE) to imitate the outputed trajs 
@article{traj-embeddings-auto-encoder,
  title={Self-consistent trajectory autoencoder: Hierarchical reinforcement learning with trajectory embeddings},
  author={Co-Reyes, John D and Liu, YuXuan and Gupta, Abhishek and Eysenbach, Benjamin and Abbeel, Pieter and Levine, Sergey},
  journal={arXiv preprint arXiv:1806.02813},
  year={2018}
}

%HIRO - use target states as higher-level actions, low level policy needs to learn to reach with L2 loss
@inproceedings{HIRO,
  title={Data-efficient hierarchical reinforcement learning},
  author={Nachum, Ofir and Gu, Shixiang Shane and Lee, Honglak and Levine, Sergey},
  booktitle={Advances in neural information processing systems},
  pages={3303--3313},
  year={2018}
}

% Learn representations of action sequences - can combine with other macro-actions papers

% Learn lower-level representation of action sequences (length k - fixed?) (and of current states) that is maximally useful to recover next states through an AE, learn Q f'n after fixing these representations, which learns Q for the new embeddings + # of steps left in the k-len plan.
@article{actseq-rep-dynamics-aware-emb,
  title={Dynamics-aware Embeddings},
  author={Whitney, William and Agarwal, Rajat and Cho, Kyunghyun and Gupta, Abhinav},
  journal={arXiv preprint arXiv:1908.09357},
  year={2019}
}

% other relevant + recent

% reparameterizing action space utilizing a parameterized dynamical system with a fixed structure adding helpful inductive bias to the model.
@article{NDP,
  title={Neural Dynamic Policies for End-to-End Sensorimotor Learning},
  author={Bahl, Shikhar and Mukadam, Mustafa and Gupta, Abhinav and Pathak, Deepak},
  journal={arXiv preprint arXiv:2012.02788},
  year={2020}
}

% HUMAN MOTOR-LEARNING
@book{human-motor-learning,
  title={Mechanisms of memory},
  author={Sweatt, J David},
  year={2009},
  publisher={Academic Press}
}


% works showing that simpleaction repetitions learning  can speed up learning and improve exploration

@inproceedings{actreps_mot0,
  title={Speeding-up reinforcement learning with multi-step actions},
  author={Schoknecht, Ralf and Riedmiller, Martin},
  booktitle={International Conference on Artificial Neural Networks},
  pages={813--818},
  year={2002},
  organization={Springer}
}

@article{actreps_mot1,
  title={Reinforcement learning on explicitly specified time scales},
  author={Schoknecht, Ralf and Riedmiller, Martin},
  journal={Neural Computing \& Applications},
  volume={12},
  number={2},
  pages={61--80},
  year={2003},
  publisher={Springer}
}

@inproceedings{actreps_mot2,
  title={Continuous-discrete reinforcement learning for hybrid control in robotics},
  author={Neunert, Michael and Abdolmaleki, Abbas and Wulfmeier, Markus and Lampe, Thomas and Springenberg, Tobias and Hafner, Roland and Romano, Francesco and Buchli, Jonas and Heess, Nicolas and Riedmiller, Martin},
  booktitle={Conference on Robot Learning},
  pages={735--751},
  year={2020},
  organization={PMLR}
}

% Learn best 'action-persistent fixed throughout a problem' (incur in non-trivial increased learning cost)

@inproceedings{actpers0,
  title={Control frequency adaptation via action persistence in batch reinforcement learning},
  author={Metelli, Alberto Maria and Mazzolini, Flavio and Bisi, Lorenzo and Sabbioni, Luca and Restelli, Marcello},
  booktitle={International Conference on Machine Learning},
  pages={6862--6873},
  year={2020},
  organization={PMLR}
}

@inproceedings{actpers1,
  title={Reinforcement Learning for Control with Multiple Frequencies},
  author={Lee, Jongmin and Lee, Byung-Jun and Kim, Kee-Eung},
  booktitle={Thirty-fourth Conference on Neural Information Processing Systems (NeurIPS 2020)},
  year={2020},
  organization={Neural information processing systems foundation}
}

%OTHER

% other work learning action repetitions
@inproceedings{act-rep-cr,
  title={Towards TempoRL: Learning When to Act},
  author={Biedenkapp, Andr{\'e} and Rajan, Raghu and Hutter, Frank and Lindauer, Marius},
  booktitle={Workshop on Inductive Biases, Invariances and Generalization in Reinforcement Learning (BIG@ ICML’20)},
  year={2020}
}

% temporally extended exploration

@article{tempExt0,
  title={Temporally-extended $\{\backslash$epsilon$\}$-greedy exploration},
  author={Dabney, Will and Ostrovski, Georg and Barreto, Andr{\'e}},
  journal={arXiv preprint arXiv:2006.01782},
  year={2020}
}




