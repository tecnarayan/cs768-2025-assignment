\begin{thebibliography}{25}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bachem et~al.(2016{\natexlab{a}})Bachem, Lucic, Hassani, and
  Krause]{bachem2016fast}
Olivier Bachem, Mario Lucic, Hamed Hassani, and Andreas Krause.
\newblock Fast and provably good seedings for k-means.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016{\natexlab{a}}.

\bibitem[Bachem et~al.(2016{\natexlab{b}})Bachem, Lucic, Hassani, and
  Krause]{bachem2016approximate}
Olivier Bachem, Mario Lucic, S~Hamed Hassani, and Andreas Krause.
\newblock Approximate k-means++ in sublinear time.
\newblock In \emph{AAAI}, 2016{\natexlab{b}}.

\bibitem[Barratt and Sharma(2018)]{noteoninceptionscore}
Shane Barratt and Rishi Sharma.
\newblock {A Note on the Inception Score}.
\newblock \emph{arXiv preprint arXiv:1801.01973}, 2018.

\bibitem[Bińkowski et~al.(2018)Bińkowski, Sutherland, Arbel, and
  Gretton]{binkowski2018demystifying}
Mikołaj Bińkowski, Dougal~J. Sutherland, Michael Arbel, and Arthur Gretton.
\newblock {Demystifying {MMD} {GAN}s}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Borji(2018)]{borji2018pros}
Ali Borji.
\newblock {Pros and Cons of GAN Evaluation Measures}.
\newblock \emph{arXiv preprint arXiv:1802.03446}, 2018.

\bibitem[C{\'\i}fka et~al.(2018)C{\'\i}fka, Severyn, Alfonseca, and
  Filippova]{cifka2018eval}
Ond{\v{r}}ej C{\'\i}fka, Aliaksei Severyn, Enrique Alfonseca, and Katja
  Filippova.
\newblock {Eval all, trust a few, do wrong to none: Comparing sentence
  generation models}.
\newblock \emph{arXiv preprint arXiv:1804.07972}, 2018.

\bibitem[Conneau et~al.(2017)Conneau, Kiela, Schwenk, Barrault, and
  Bordes]{conneau2017supervised}
Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, and Antoine Bordes.
\newblock {Supervised Learning of Universal Sentence Representations from
  Natural Language Inference Data}.
\newblock \emph{arXiv preprint arXiv:1705.02364}, 2017.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock {Generative Adversarial Networks}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, Klambauer,
  and Hochreiter]{heusel2017gans}
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler,
  G{\"u}nter Klambauer, and Sepp Hochreiter.
\newblock {GANs trained by a two time-scale update rule converge to a Nash
  equilibrium}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Husz{\'a}r(2015)]{huszar2015not}
Ferenc Husz{\'a}r.
\newblock {How (not) to Train your Generative Model: Scheduled Sampling,
  Likelihood, Adversary?}
\newblock \emph{arXiv preprint arXiv:1511.05101}, 2015.

\bibitem[Im et~al.(2018)Im, Ma, Taylor, and Branson]{im2018quantitatively}
Daniel~Jiwoong Im, He~Ma, Graham Taylor, and Kristin Branson.
\newblock {Quantitatively evaluating GANs with divergences proposed for
  training}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Kingma and Welling(2014)]{kingma2013auto}
Diederik~P Kingma and Max Welling.
\newblock {Auto-encoding Variational Bayes}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2014.

\bibitem[Krizhevsky and Hinton(2009)]{cifar10}
Alex Krizhevsky and Geoffrey Hinton.
\newblock {Learning multiple layers of features from tiny images}, 2009.

\bibitem[Kurach et~al.(2018)Kurach, Lucic, Zhai, Michalski, and
  Gelly]{kurach2018gan}
Karol Kurach, Mario Lucic, Xiaohua Zhai, Marcin Michalski, and Sylvain Gelly.
\newblock The {GAN} {L}andscape: {L}osses, architectures, regularization, and
  normalization.
\newblock \emph{arXiv preprint arXiv:1807.04720}, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock {Gradient-based learning applied to document recognition}.
\newblock In \emph{IEEE}, 1998.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015faceattributes}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of International Conference on Computer Vision
  ({ICCV})}, 2015.

\bibitem[Lopez-Paz and Oquab(2016)]{lopez2016revisiting}
David Lopez-Paz and Maxime Oquab.
\newblock {Revisiting Classifier Two-Sample Tests}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Lucic et~al.(2018)Lucic, Kurach, Michalski, Gelly, and
  Bousquet]{lucic2017gans}
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier
  Bousquet.
\newblock {Are {GAN}s Created Equal? {A} Large-Scale Study}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2018.

\bibitem[Odena et~al.(2016)Odena, Dumoulin, and Olah]{odena2016deconvolution}
Augustus Odena, Vincent Dumoulin, and Chris Olah.
\newblock Deconvolution and checkerboard artifacts.
\newblock \emph{Distill}, 2016.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock {Improved Techniques for Training GANs}.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2016.

\bibitem[Sculley(2010)]{sculley2010web}
David Sculley.
\newblock Web-scale k-means clustering.
\newblock In \emph{International Conference on World Wide Web (WWW)}, 2010.

\bibitem[Theis et~al.(2016)Theis, Oord, and Bethge]{theis2015note}
Lucas Theis, A{\"a}ron van~den Oord, and Matthias Bethge.
\newblock {A note on the evaluation of generative models}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Williams et~al.(2017)Williams, Nangia, and Bowman]{williams2017broad}
Adina Williams, Nikita Nangia, and Samuel~R Bowman.
\newblock {A broad-coverage challenge corpus for sentence understanding through
  inference}.
\newblock \emph{arXiv preprint arXiv:1704.05426}, 2017.

\bibitem[Wu et~al.(2017)Wu, Burda, Salakhutdinov, and
  Grosse]{wu2016quantitative}
Yuhuai Wu, Yuri Burda, Ruslan Salakhutdinov, and Roger Grosse.
\newblock {On the quantitative analysis of decoder-based generative models}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{fashionmnist}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock {Fashion-{MNIST}: A Novel Image Dataset for Benchmarking Machine
  Learning Algorithms}.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\end{thebibliography}
