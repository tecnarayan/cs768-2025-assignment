\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{shai2010hdh}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79:\penalty0 151--175, 2010.

\bibitem[Miller et~al.(2021)Miller, Taori, Raghunathan, Sagawa, Koh, Shankar,
  Liang, Carmon, and Schmidt]{miller2021accuracy}
John~P Miller, Rohan Taori, Aditi Raghunathan, Shiori Sagawa, Pang~Wei Koh,
  Vaishaal Shankar, Percy Liang, Yair Carmon, and Ludwig Schmidt.
\newblock Accuracy on the line: on the strong correlation between
  out-of-distribution and in-distribution generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  7721--7735, 2021.

\bibitem[Taori et~al.(2020)Taori, Dave, Shankar, Carlini, Recht, and
  Schmidt]{taori2020measuring}
Rohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Measuring robustness to natural distribution shifts in image
  classification.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 18583--18599, 2020.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2021understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021.

\bibitem[Azulay and Weiss(2018)]{azulay2018deep}
Aharon Azulay and Yair Weiss.
\newblock Why do deep convolutional networks generalize so poorly to small
  image transformations?
\newblock \emph{arXiv preprint arXiv:1805.12177}, 2018.

\bibitem[Engstrom et~al.(2019)Engstrom, Tran, Tsipras, Schmidt, and
  Madry]{engstrom2019exploring}
Logan Engstrom, Brandon Tran, Dimitris Tsipras, Ludwig Schmidt, and Aleksander
  Madry.
\newblock Exploring the landscape of spatial robustness.
\newblock In \emph{International Conference on Machine Learning}, pages
  1802--1811, 2019.

\bibitem[Kanbak et~al.(2018)Kanbak, Moosavi-Dezfooli, and
  Frossard]{kanbak2018geometric}
Can Kanbak, Seyed-Mohsen Moosavi-Dezfooli, and Pascal Frossard.
\newblock Geometric robustness of deep networks: analysis and improvement.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 4441--4449, 2018.

\bibitem[Zhang(2019)]{zhang2019making}
Richard Zhang.
\newblock Making convolutional networks shift-invariant again.
\newblock In \emph{International conference on machine learning}, pages
  7324--7334, 2019.

\bibitem[Kayhan and Gemert(2020)]{kayhan2020translation}
Osman~Semih Kayhan and Jan C~van Gemert.
\newblock On translation invariance in cnns: Convolutional layers can exploit
  absolute spatial location.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 14274--14285, 2020.

\bibitem[Zhu et~al.(2021)Zhu, An, and Huang]{zhu2021understanding}
Sicheng Zhu, Bang An, and Furong Huang.
\newblock Understanding the generalization benefit of model invariance from a
  data perspective.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pages 4328--4341, 2021.

\bibitem[Zhou et~al.(2017)Zhou, Ye, Qiu, and Jiao]{zhou2017oriented}
Yanzhao Zhou, Qixiang Ye, Qiang Qiu, and Jianbin Jiao.
\newblock Oriented response networks.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 519--528, 2017.

\bibitem[Jaderberg et~al.(2015)Jaderberg, Simonyan, Zisserman,
  et~al.]{jaderberg2015spatial}
Max Jaderberg, Karen Simonyan, Andrew Zisserman, et~al.
\newblock Spatial transformer networks.
\newblock In \emph{Advances in neural information processing systems}, 2015.

\bibitem[Delchevalerie et~al.(2021)Delchevalerie, Bibal, Fr{\'e}nay, and
  Mayer]{delchevalerie2021achieving}
Valentin Delchevalerie, Adrien Bibal, Beno{\^\i}t Fr{\'e}nay, and Alexandre
  Mayer.
\newblock Achieving rotational invariance with bessel-convolutional neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Recht et~al.(2019)Recht, Roelofs, Schmidt, and
  Shankar]{recht2019imagenet}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do imagenet classifiers generalize to imagenet?
\newblock In \emph{International Conference on Machine Learning}, pages
  5389--5400. PMLR, 2019.

\bibitem[Schiff et~al.(2021)Schiff, Quanz, Das, and Chen]{schiff2021predicting}
Yair Schiff, Brian Quanz, Payel Das, and Pin-Yu Chen.
\newblock Predicting deep neural network generalization with perturbation
  response curves.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 248--255, 2009.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Bao et~al.(2021)Bao, Dong, and Wei]{bao2021beit}
Hangbo Bao, Li~Dong, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock \emph{arXiv preprint arXiv:2106.08254}, 2021.

\bibitem[Eilertsen et~al.(2020)Eilertsen, J{\"o}nsson, Ropinski, Unger, and
  Ynnerman]{eilertsen2020classifying}
Gabriel Eilertsen, Daniel J{\"o}nsson, Timo Ropinski, Jonas Unger, and Anders
  Ynnerman.
\newblock Classifying the classifier: dissecting the weight space of neural
  networks.
\newblock \emph{arXiv preprint arXiv:2002.05688}, 2020.

\bibitem[Unterthiner et~al.(2020)Unterthiner, Keysers, Gelly, Bousquet, and
  Tolstikhin]{unterthiner2020predicting}
Thomas Unterthiner, Daniel Keysers, Sylvain Gelly, Olivier Bousquet, and Ilya
  Tolstikhin.
\newblock Predicting neural network accuracy from weights.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Arora et~al.(2018)Arora, Ge, Neyshabur, and Zhang]{arora2018stronger}
Sanjeev Arora, Rong Ge, Behnam Neyshabur, and Yi~Zhang.
\newblock Stronger generalization bounds for deep nets via a compression
  approach.
\newblock \emph{arXiv preprint arXiv:1802.05296}, 2018.

\bibitem[Corneanu et~al.(2020)Corneanu, Escalera, and
  Martinez]{corneanu2020computing}
Ciprian~A Corneanu, Sergio Escalera, and Aleix~M Martinez.
\newblock Computing the testing error without a testing set.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2677--2685, 2020.

\bibitem[Jiang et~al.(2019{\natexlab{a}})Jiang, Krishnan, Mobahi, and
  Bengio]{jiang2018predicting}
Yiding Jiang, Dilip Krishnan, Hossein Mobahi, and Samy Bengio.
\newblock Predicting the generalization gap in deep networks with margin
  distributions.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshabur2017exploring}
Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester, and Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  5947--5956, 2017.

\bibitem[Jiang et~al.(2019{\natexlab{b}})Jiang, Neyshabur, Mobahi, Krishnan,
  and Bengio]{jiang2019fantastic}
Yiding Jiang, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{b}}.

\bibitem[Garg et~al.(2021)Garg, Balakrishnan, Kolter, and Lipton]{garg2021ratt}
Saurabh Garg, Sivaraman Balakrishnan, Zico Kolter, and Zachary Lipton.
\newblock Ratt: Leveraging unlabeled data to guarantee generalization.
\newblock In \emph{International Conference on Machine Learning}, pages
  3598--3609, 2021.

\bibitem[Jiang et~al.(2021)Jiang, Nagarajan, Baek, and
  Kolter]{jiang2021assessing}
Yiding Jiang, Vaishnavh Nagarajan, Christina Baek, and J~Zico Kolter.
\newblock Assessing generalization of sgd via disagreement.
\newblock \emph{arXiv preprint arXiv:2106.13799}, 2021.

\bibitem[Aithal et~al.(2021)Aithal, Kashyap, and
  Subramanyam]{kashyap2021robustness}
Sumukh Aithal, Dhruva Kashyap, and Natarajan Subramanyam.
\newblock Robustness to augmentations as a generalization metric.
\newblock \emph{arXiv preprint arXiv:2101.06459}, 2021.

\bibitem[Deng and Zheng(2021)]{deng2021labels}
Weijian Deng and Liang Zheng.
\newblock Are labels always necessary for classifier accuracy evaluation?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15069--15078, 2021.

\bibitem[Guillory et~al.(2021)Guillory, Shankar, Ebrahimi, Darrell, and
  Schmidt]{guillory2021predicting}
Devin Guillory, Vaishaal Shankar, Sayna Ebrahimi, Trevor Darrell, and Ludwig
  Schmidt.
\newblock Predicting with confidence on unseen distributions.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 1134--1144, 2021.

\bibitem[Garg et~al.(2022)Garg, Balakrishnan, Lipton, Neyshabur, and
  Sedghi]{garg2022leveraging}
Saurabh Garg, Sivaraman Balakrishnan, Zachary~C Lipton, Behnam Neyshabur, and
  Hanie Sedghi.
\newblock Leveraging unlabeled data to predict out-of-distribution performance.
\newblock In \emph{International Conference on Learning Representations}, 2022.

\bibitem[Baek et~al.(2022)Baek, Jiang, Raghunathan, and
  Kolter]{baek2022agreement}
Christina Baek, Yiding Jiang, Aditi Raghunathan, and Zico Kolter.
\newblock Agreement-on-the-line: Predicting the performance of neural networks
  under distribution shift.
\newblock \emph{arXiv preprint arXiv:2206.13089}, 2022.

\bibitem[Ng et~al.(2022)Ng, Cho, Hulkund, and Ghassemi]{ng2022predicting}
Nathan Ng, Kyunghyun Cho, Neha Hulkund, and Marzyeh Ghassemi.
\newblock Predicting out-of-domain generalization with local manifold
  smoothness.
\newblock \emph{arXiv preprint arXiv:2207.02093}, 2022.

\bibitem[Yu et~al.(2022)Yu, Yang, Wei, Ma, and Steinhardt]{yu2022predicting}
Yaodong Yu, Zitong Yang, Alexander Wei, Yi~Ma, and Jacob Steinhardt.
\newblock Predicting out-of-distribution error with the projection norm.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Deng et~al.(2021)Deng, Gould, and Zheng]{Deng:ICML2021}
Weijian Deng, Stephen Gould, and Liang Zheng.
\newblock What does rotation prediction tell us about classifier accuracy under
  varying testing environments?
\newblock In \emph{International conference on machine learning}, 2021.

\bibitem[Chen et~al.(2021)Chen, Liu, Avci, Wu, Liang, and
  Jha]{chen2021detecting}
Jiefeng Chen, Frederick Liu, Besim Avci, Xi~Wu, Yingyu Liang, and Somesh Jha.
\newblock Detecting errors and estimating accuracy on unlabeled data with
  self-training ensembles.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{hendrycks2019augmix}
Dan Hendrycks, Norman Mu, Ekin~D Cubuk, Barret Zoph, Justin Gilmer, and Balaji
  Lakshminarayanan.
\newblock Augmix: A simple data processing method to improve robustness and
  uncertainty.
\newblock \emph{arXiv preprint arXiv:1912.02781}, 2019.

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun2019cutmix}
Sangdoo Yun, Dongyoon Han, Seong~Joon Oh, Sanghyuk Chun, Junsuk Choe, and
  Youngjoon Yoo.
\newblock Cutmix: Regularization strategy to train strong classifiers with
  localizable features.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on
  computer vision}, pages 6023--6032, 2019.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk2018autoaugment}
Ekin~D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc~V Le.
\newblock Autoaugment: Learning augmentation policies from data.
\newblock \emph{arXiv preprint arXiv:1805.09501}, 2018.

\bibitem[Cubuk et~al.(2020)Cubuk, Zoph, Shlens, and Le]{cubuk2020randaugment}
Ekin~D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc~V Le.
\newblock Randaugment: Practical automated data augmentation with a reduced
  search space.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition Workshops}, pages 702--703, 2020.

\bibitem[DeVries and Taylor(2017)]{devries2017improved}
Terrance DeVries and Graham~W Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock \emph{arXiv preprint arXiv:1708.04552}, 2017.

\bibitem[Mintun et~al.(2021)Mintun, Kirillov, and Xie]{mintun2021interaction}
Eric Mintun, Alexander Kirillov, and Saining Xie.
\newblock On interaction between augmentations and corruptions in natural
  corruption robustness.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Hendrycks et~al.(2022)Hendrycks, Zou, Mazeika, Tang, Song, and
  Steinhardt]{hendrycks2021pixmix}
Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Dawn Song, and Jacob
  Steinhardt.
\newblock Pixmix: Dreamlike pictures comprehensively improve safety measures.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2022.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang2017mixup}
Hongyi Zhang, Moustapha Cisse, Yann~N Dauphin, and David Lopez-Paz.
\newblock mixup: Beyond empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:1710.09412}, 2017.

\bibitem[Tokozume et~al.(2018)Tokozume, Ushiku, and
  Harada]{tokozume2018between}
Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada.
\newblock Between-class learning for image classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 5486--5494, 2018.

\bibitem[Yin et~al.(2019)Yin, Gontijo~Lopes, Shlens, Cubuk, and
  Gilmer]{yin2019fourier}
Dong Yin, Raphael Gontijo~Lopes, Jon Shlens, Ekin~Dogus Cubuk, and Justin
  Gilmer.
\newblock A fourier perspective on model robustness in computer vision.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Salman et~al.(2020)Salman, Ilyas, Engstrom, Kapoor, and
  Madry]{salman2020adversarially}
Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish Kapoor, and Aleksander Madry.
\newblock Do adversarially robust imagenet models transfer better?
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  3533--3545, 2020.

\bibitem[Rusak et~al.(2020)Rusak, Schott, Zimmermann, Bitterwolf, Bringmann,
  Bethge, and Brendel]{rusak2020simple}
Evgenia Rusak, Lukas Schott, Roland~S Zimmermann, Julian Bitterwolf, Oliver
  Bringmann, Matthias Bethge, and Wieland Brendel.
\newblock A simple way to make neural networks robust against diverse image
  corruptions.
\newblock In \emph{European Conference on Computer Vision}, pages 53--69, 2020.

\bibitem[Fuglede and Topsoe(2004)]{fuglede2004jensen}
Bent Fuglede and Flemming Topsoe.
\newblock Jensen-shannon divergence and hilbert space embedding.
\newblock In \emph{International Symposium onInformation Theory, 2004. ISIT
  2004. Proceedings.}, page~31, 2004.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Carlini, Zhang, Zhang, Raffel,
  Cubuk, Kurakin, and Li]{sohn2020fixmatch}
Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,
  Colin~A Raffel, Ekin~Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and
  confidence.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  596--608, 2020.

\bibitem[Berthelot et~al.(2019)Berthelot, Carlini, Goodfellow, Papernot,
  Oliver, and Raffel]{berthelot2019mixmatch}
David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
  Oliver, and Colin~A Raffel.
\newblock Mixmatch: A holistic approach to semi-supervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Kullback and Leibler(1951)]{kullback1951information}
Solomon Kullback and Richard~A Leibler.
\newblock On information and sufficiency.
\newblock \emph{The annals of mathematical statistics}, 22\penalty0
  (1):\penalty0 79--86, 1951.

\bibitem[Sun et~al.(2021)Sun, Mehra, Kailkhura, Chen, Hendrycks, Hamm, and
  Mao]{sun2021certified}
Jiachen Sun, Akshay Mehra, Bhavya Kailkhura, Pin-Yu Chen, Dan Hendrycks, Jihun
  Hamm, and Z~Morley Mao.
\newblock Certified adversarial defenses meet out-of-distribution corruptions:
  Benchmarking robustness and simple baselines.
\newblock \emph{arXiv preprint arXiv:2112.00659}, 2021.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Liu et~al.(2022)Liu, Mao, Wu, Feichtenhofer, Darrell, and
  Xie]{liu2022convnet}
Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell,
  and Saining Xie.
\newblock A convnet for the 2020s.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2022.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 10012--10022, 2021.

\bibitem[Ding et~al.(2022)Ding, Xia, Zhang, Chu, Han, and Ding]{ding2021repmlp}
Xiaohan Ding, Chunlong Xia, Xiangyu Zhang, Xiaojie Chu, Jungong Han, and
  Guiguang Ding.
\newblock Repmlp: Re-parameterizing convolutions into fully-connected layers
  for image recognition.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, 2022.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Ilya~O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua
  Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
  Jakob Uszkoreit, et~al.
\newblock Mlp-mixer: An all-mlp architecture for vision.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew
  Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Bello et~al.(2021)Bello, Fedus, Du, Cubuk, Srinivas, Lin, Shlens, and
  Zoph]{bello2021revisiting}
Irwan Bello, William Fedus, Xianzhi Du, Ekin~Dogus Cubuk, Aravind Srinivas,
  Tsung-Yi Lin, Jonathon Shlens, and Barret Zoph.
\newblock Revisiting resnets: Improved training and scaling strategies.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{International conference on machine learning}, pages
  6105--6114, 2019.

\bibitem[Tan and Le(2021)]{tan2021efficientnetv2}
Mingxing Tan and Quoc Le.
\newblock Efficientnetv2: Smaller models and faster training.
\newblock In \emph{International Conference on Machine Learning}, pages
  10096--10106, 2021.

\bibitem[Yalniz et~al.(2019)Yalniz, J{\'e}gou, Chen, Paluri, and
  Mahajan]{yalniz2019billion}
I~Zeki Yalniz, Herv{\'e} J{\'e}gou, Kan Chen, Manohar Paluri, and Dhruv
  Mahajan.
\newblock Billion-scale semi-supervised learning for image classification.
\newblock \emph{arXiv preprint arXiv:1905.00546}, 2019.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, Dean,
  et~al.]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, Jeff Dean, et~al.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2\penalty0 (7), 2015.

\bibitem[Heo et~al.(2021)Heo, Yun, Han, Chun, Choe, and Oh]{heo2021rethinking}
Byeongho Heo, Sangdoo Yun, Dongyoon Han, Sanghyuk Chun, Junsuk Choe, and
  Seong~Joon Oh.
\newblock Rethinking spatial dimensions of vision transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 11936--11945, 2021.

\bibitem[Wightman(2019)]{rw2019timm}
Ross Wightman.
\newblock Pytorch image models.
\newblock \url{https://github.com/rwightman/pytorch-image-models}, 2019.

\bibitem[Thomee et~al.(2015)Thomee, Shamma, Friedland, Elizalde, Ni, Poland,
  Borth, and Li]{thomee2015new}
Bart Thomee, David~A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,
  Douglas Poland, Damian Borth, and Li-Jia Li.
\newblock The new data and new challenges in multimedia research.
\newblock \emph{arXiv preprint arXiv:1503.01817}, 2015.

\bibitem[Mahajan et~al.(2018)Mahajan, Girshick, Ramanathan, He, Paluri, Li,
  Bharambe, and Van Der~Maaten]{mahajan2018exploring}
Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri,
  Yixuan Li, Ashwin Bharambe, and Laurens Van Der~Maaten.
\newblock Exploring the limits of weakly supervised pretraining.
\newblock In \emph{Proceedings of the European conference on computer vision
  (ECCV)}, pages 181--196, 2018.

\bibitem[Sun et~al.(2017)Sun, Shrivastava, Singh, and Gupta]{sun2017revisiting}
Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta.
\newblock Revisiting unreasonable effectiveness of data in deep learning era.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 843--852, 2017.

\bibitem[Xie et~al.(2020)Xie, Luong, Hovy, and Le]{xie2020self}
Qizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc~V Le.
\newblock Self-training with noisy student improves imagenet classification.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision
  and pattern recognition}, pages 10687--10698, 2020.

\bibitem[Singh et~al.(2022)Singh, Gustafson, Adcock, Reis, Gedik, Kosaraju,
  Mahajan, Girshick, Doll{\'a}r, and van~der Maaten]{singh2022revisiting}
Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de~Freitas Reis, Bugra
  Gedik, Raj~Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll{\'a}r,
  and Laurens van~der Maaten.
\newblock Revisiting weakly supervised pre-training of visual perception
  models.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, 2022.

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov2020big}
Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung,
  Sylvain Gelly, and Neil Houlsby.
\newblock Big transfer (bit): General visual representation learning.
\newblock In \emph{European conference on computer vision}, pages 491--507,
  2020.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Zhao, Basart,
  Steinhardt, and Song]{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15262--15271, 2021{\natexlab{b}}.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{wang2019learning}
Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing.
\newblock Learning robust global representations by penalizing local predictive
  power.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10506--10518, 2019.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019robustness}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2019.

\bibitem[Benesty et~al.(2009)Benesty, Chen, Huang, and
  Cohen]{benesty2009pearson}
Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen.
\newblock Pearson correlation coefficient.
\newblock In \emph{Noise reduction in speech processing}, pages 1--4. Springer,
  2009.

\bibitem[Kendall(1948)]{kendall1948rank}
Maurice~George Kendall.
\newblock Rank correlation methods.
\newblock 1948.

\bibitem[Huber(2011)]{huber2011robust}
Peter~J Huber.
\newblock Robust statistics.
\newblock In \emph{International encyclopedia of statistical science}, pages
  1248--1251. Springer, 2011.

\bibitem[Meding et~al.(2022)Meding, Buschoff, Geirhos, and
  Wichmann]{meding2021trivial}
Kristof Meding, Luca M~Schulze Buschoff, Robert Geirhos, and Felix~A Wichmann.
\newblock Trivial or impossible--dichotomous data difficulty masks model
  differences (on imagenet and beyond).
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2022.

\bibitem[Hacohen et~al.(2020)Hacohen, Choshen, and Weinshall]{hacohen2020let}
Guy Hacohen, Leshem Choshen, and Daphna Weinshall.
\newblock Let’s agree to agree: Neural networks share classification order on
  real datasets.
\newblock In \emph{International Conference on Machine Learning}, pages
  3950--3960, 2020.

\bibitem[Yang et~al.(2020)Yang, Qinami, Fei-Fei, Deng, and
  Russakovsky]{yang2020towards}
Kaiyu Yang, Klint Qinami, Li~Fei-Fei, Jia Deng, and Olga Russakovsky.
\newblock Towards fairer datasets: Filtering and balancing the distribution of
  the people subtree in the imagenet hierarchy.
\newblock In \emph{Proceedings of the 2020 Conference on Fairness,
  Accountability, and Transparency}, pages 547--558, 2020.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Torralba et~al.(2008)Torralba, Fergus, and Freeman]{torralba200880}
Antonio Torralba, Rob Fergus, and William~T Freeman.
\newblock 80 million tiny images: A large data set for nonparametric object and
  scene recognition.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 30\penalty0 (11):\penalty0 1958--1970, 2008.

\bibitem[Chrabaszcz et~al.(2017)Chrabaszcz, Loshchilov, and
  Hutter]{chrabaszcz2017downsampled}
Patryk Chrabaszcz, Ilya Loshchilov, and Frank Hutter.
\newblock A downsampled variant of imagenet as an alternative to the cifar
  datasets.
\newblock \emph{arXiv preprint arXiv:1707.08819}, 2017.

\bibitem[Von~K{\"u}gelgen et~al.(2021)Von~K{\"u}gelgen, Sharma, Gresele,
  Brendel, Sch{\"o}lkopf, Besserve, and Locatello]{von2021self}
Julius Von~K{\"u}gelgen, Yash Sharma, Luigi Gresele, Wieland Brendel, Bernhard
  Sch{\"o}lkopf, Michel Besserve, and Francesco Locatello.
\newblock Self-supervised learning with data augmentations provably isolates
  content from style.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations}, 2021.

\bibitem[Weiler and Cesa(2019)]{weiler2019general}
Maurice Weiler and Gabriele Cesa.
\newblock General e (2)-equivariant steerable cnns.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Cohen et~al.(2018)Cohen, Geiger, K{\"o}hler, and
  Welling]{cohen2018spherical}
Taco~S Cohen, Mario Geiger, Jonas K{\"o}hler, and Max Welling.
\newblock Spherical cnns.
\newblock \emph{arXiv preprint arXiv:1801.10130}, 2018.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, Lee, David, Stavness, Guo, Earnshaw, Haque, Beery,
  Leskovec, Kundaje, Pierson, Levine, Finn, and Liang]{wilds2021}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton~A.
  Earnshaw, Imran~S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma
  Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
\newblock {WILDS}: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Ren et~al.(2015)Ren, He, Girshick, and Sun]{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster r-cnn: Towards real-time object detection with region proposal
  networks.
\newblock In \emph{Advances in neural information processing systems}, 2015.

\bibitem[Long et~al.(2015)Long, Shelhamer, and Darrell]{long2015fully}
Jonathan Long, Evan Shelhamer, and Trevor Darrell.
\newblock Fully convolutional networks for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 3431--3440, 2015.

\bibitem[Wang et~al.(2018)Wang, Girshick, Gupta, and He]{wang2018non}
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
\newblock Non-local neural networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 7794--7803, 2018.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton,
  et~al.]{krizhevsky2009learning}
Alex Krizhevsky, Geoffrey Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\end{thebibliography}
