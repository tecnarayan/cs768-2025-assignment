\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aljundi et~al.(2018)Aljundi, Babiloni, Elhoseiny, Rohrbach, and
  Tuytelaars]{ECCV2018MAS}
Aljundi, R., Babiloni, F., Elhoseiny, M., Rohrbach, M., and Tuytelaars, T.
\newblock Memory aware synapses: Learning what (not) to forget.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  pp.\  139--154, 2018.

\bibitem[Bang et~al.(2021)Bang, Kim, Yoo, Ha, and Choi]{bang2021rainbow}
Bang, J., Kim, H., Yoo, Y., Ha, J.-W., and Choi, J.
\newblock Rainbow memory: Continual learning with a memory of diverse samples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  8218--8227, 2021.

\bibitem[Belouadah \& Popescu(2019)Belouadah and Popescu]{ICCV2019IL2M}
Belouadah, E. and Popescu, A.
\newblock {IL2M}: Class incremental learning with dual memory.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  583--592, 2019.

\bibitem[Ben-Israel \& Greville(2003)Ben-Israel and
  Greville]{ben2003generalized}
Ben-Israel, A. and Greville, T.~N.
\newblock \emph{Generalized Inverses: Theory and Applications}, volume~15.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Bonicelli et~al.(2022)Bonicelli, Boschini, Porrello, Concetto,
  Calderara, et~al.]{bonicelli2022effectiveness}
Bonicelli, L., Boschini, M., Porrello, A., Concetto, S., Calderara, S., et~al.
\newblock On the effectiveness of lipschitz-driven rehearsal in continual
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  31886--31901, 2022.

\bibitem[Cha et~al.(2021)Cha, Lee, and Shin]{cha2021co2l}
Cha, H., Lee, J., and Shin, J.
\newblock {Co$^2$L}: Contrastive continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  9516--9525, 2021.

\bibitem[Chen \& Wan(1999)Chen and Wan]{chen1999rapid}
Chen, C.~P. and Wan, J.~Z.
\newblock A rapid learning and dynamic stepwise updating algorithm for flat
  neural networks and the application to time-series prediction.
\newblock \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part B
  (Cybernetics)}, 29\penalty0 (1):\penalty0 62--72, 1999.

\bibitem[Deng et~al.(2021)Deng, Chen, Hao, Wang, and Heng]{deng2021flattening}
Deng, D., Chen, G., Hao, J., Wang, Q., and Heng, P.-A.
\newblock Flattening sharpness for dynamic gradient projection memory benefits
  continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  18710--18721, 2021.

\bibitem[Douillard et~al.(2020)Douillard, Cord, Ollion, Robert, and
  Valle]{douillard2020podnet}
Douillard, A., Cord, M., Ollion, C., Robert, T., and Valle, E.
\newblock {PODNet}: Pooled outputs distillation for small-tasks incremental
  learning.
\newblock In \emph{Computer vision--ECCV 2020: 16th European conference,
  Glasgow, UK, August 23--28, 2020, proceedings, part XX 16}, pp.\  86--102.
  Springer, 2020.

\bibitem[Gao \& Liu(2023)Gao and Liu]{gao2023ddgr}
Gao, R. and Liu, W.
\newblock {DDGR}: continual learning with deep diffusion-based generative
  replay.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10744--10763. PMLR, 2023.

\bibitem[Guo et~al.(2022)Guo, Hu, Zhao, and Liu]{guo2022adaptive}
Guo, Y., Hu, W., Zhao, D., and Liu, B.
\newblock Adaptive orthogonal projection for batch and online continual
  learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  6783--6791, 2022.

\bibitem[Guo et~al.(2023)Guo, Liu, and Zhao]{guo2023dealing}
Guo, Y., Liu, B., and Zhao, D.
\newblock Dealing with cross-task class discrimination in online continual
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11878--11887, 2023.

\bibitem[Gurbuz \& Dovrolis(2022)Gurbuz and Dovrolis]{gurbuz2022nispa}
Gurbuz, M.~B. and Dovrolis, C.
\newblock {NISPA}: Neuro-inspired stability-plasticity adaptation for continual
  learning in sparse networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  8157--8174. PMLR, 2022.

\bibitem[Hayes et~al.(2020)Hayes, Kafle, Shrestha, Acharya, and
  Kanan]{hayes2020remind}
Hayes, T.~L., Kafle, K., Shrestha, R., Acharya, M., and Kanan, C.
\newblock Remind your neural network to prevent catastrophic forgetting.
\newblock In \emph{European Conference on Computer Vision}, pp.\  466--483.
  Springer, 2020.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Basart, Mu, Kadavath, Wang, Dorundo,
  Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Hendrycks, D., Basart, S., Mu, N., Kadavath, S., Wang, F., Dorundo, E., Desai,
  R., Zhu, T., Parajuli, S., Guo, M., et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  8340--8349, 2021.

\bibitem[Hsu et~al.(2018)Hsu, Liu, Ramasamy, and Kira]{hsu2018re}
Hsu, Y.-C., Liu, Y.-C., Ramasamy, A., and Kira, Z.
\newblock Re-evaluating continual learning scenarios: A categorization and case
  for strong baselines.
\newblock \emph{arXiv preprint arXiv:1810.12488}, 2018.

\bibitem[Hu et~al.(2021)Hu, Qin, Wang, Ma, and Liu]{AAAI2021PCL}
Hu, W., Qin, Q., Wang, M., Ma, J., and Liu, B.
\newblock Continual learning by using information of each class holistically.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  7797--7805, 2021.

\bibitem[Hu et~al.(2023)Hu, Li, Lyu, Gao, and Vasconcelos]{hu2023dense}
Hu, Z., Li, Y., Lyu, J., Gao, D., and Vasconcelos, N.
\newblock Dense network expansion for class incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11858--11867, 2023.

\bibitem[Huang et~al.(2023)Huang, Li, Cao, Fujita, Li, and Wu]{huang2022graph}
Huang, C., Li, M., Cao, F., Fujita, H., Li, Z., and Wu, X.
\newblock Are graph convolutional networks with random weights feasible?
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45\penalty0 (3):\penalty0 2751--2768, 2023.

\bibitem[Igelnik \& Pao(1995)Igelnik and Pao]{igelnik1995stochastic}
Igelnik, B. and Pao, Y.-H.
\newblock Stochastic choice of basis functions in adaptive function
  approximation and the functional-link net.
\newblock \emph{IEEE Transactions on Neural Networks}, 6\penalty0 (6):\penalty0
  1320--1329, 1995.

\bibitem[Joseph et~al.(2010)Joseph, Pleydell-Bouverie, Dupret, and
  Csicsvari]{o2010play}
Joseph, O., Pleydell-Bouverie, B., Dupret, D., and Csicsvari, J.
\newblock Play it again: reactivation of waking experience and memory.
\newblock \emph{Trends in Neurosciences}, 33\penalty0 (5):\penalty0 220--229,
  2010.

\bibitem[Kang et~al.(2022)Kang, Mina, Madjid, Yoon, Hasegawa-Johnson, Hwang,
  and Yoo]{kang2022forget}
Kang, H., Mina, R. J.~L., Madjid, S. R.~H., Yoon, J., Hasegawa-Johnson, M.,
  Hwang, S.~J., and Yoo, C.~D.
\newblock Forget-free continual learning with winning subnetworks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10734--10750. PMLR, 2022.

\bibitem[Ke et~al.(2021)Ke, Liu, Ma, Xu, and Shu]{ke2021achieving}
Ke, Z., Liu, B., Ma, N., Xu, H., and Shu, L.
\newblock Achieving forgetting prevention and knowledge transfer in continual
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~34, pp.\  22443--22456, 2021.

\bibitem[Kim et~al.(2022)Kim, Liu, and Ke]{kim2022multi}
Kim, G., Liu, B., and Ke, Z.
\newblock A multi-head model for continual learning via out-of-distribution
  replay.
\newblock In \emph{Conference on Lifelong Learning Agents}, pp.\  548--563.
  PMLR, 2022.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska,
  et~al.]{PNAS2017EWC}
Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu,
  A.~A., Milan, K., Quan, J., Ramalho, T., Grabska-Barwinska, A., et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{CIFAR-100}
Krizhevsky, A., Hinton, G., et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Handbook of Systemic Autoimmune Diseases}, 2009.

\bibitem[Kwok \& Yeung(1997)Kwok and Yeung]{kwok1997objective}
Kwok, T.-Y. and Yeung, D.-Y.
\newblock Objective functions for training new hidden units in constructive
  neural networks.
\newblock \emph{IEEE Transactions on Neural Networks}, 8\penalty0 (5):\penalty0
  1131--1148, 1997.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Leonides(2012)]{leonides2012control}
Leonides, C.
\newblock \emph{Control and Dynamic Systems V18: Advances in Theory and
  Applications}.
\newblock Elsevier, 2012.

\bibitem[Li \& Zeng(2023)Li and Zeng]{li2023CRNet}
Li, D. and Zeng, Z.
\newblock {CRNet}: A fast continual learning framework with random theory.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45\penalty0 (9):\penalty0 10731--10744, 2023.

\bibitem[Li et~al.(2024{\natexlab{a}})Li, Wang, Chen, Kawaguchi, Lian, and
  Zeng]{li2024MVCNet}
Li, D., Wang, T., Chen, J., Kawaguchi, K., Lian, C., and Zeng, Z.
\newblock Multi-view class incremental learning.
\newblock \emph{Information Fusion}, 102:\penalty0 102021, 2024{\natexlab{a}}.

\bibitem[Li et~al.(2024{\natexlab{b}})Li, Wang, Chen, Ren, Kawaguchi, and
  Zeng]{li2024CLDNet}
Li, D., Wang, T., Chen, J., Ren, Q., Kawaguchi, K., and Zeng, Z.
\newblock Towards continual learning desiderata via hsic-bottleneck
  orthogonalization and equiangular embedding.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  13464--13473, 2024{\natexlab{b}}.

\bibitem[Li \& Wang(2017)Li and Wang]{li2017insights}
Li, M. and Wang, D.
\newblock Insights into randomized algorithms for neural networks: Practical
  issues and common pitfalls.
\newblock \emph{Information Sciences}, 382:\penalty0 170--178, 2017.

\bibitem[Lin et~al.(2023)Lin, Zhang, Feng, Li, and Ye]{lin2023pcr}
Lin, H., Zhang, B., Feng, S., Li, X., and Ye, Y.
\newblock {PCR}: Proxy-based contrastive replay for online class-incremental
  continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  24246--24255, 2023.

\bibitem[Liu et~al.(2021)Liu, Schiele, and Sun]{liu2021adaptive}
Liu, Y., Schiele, B., and Sun, Q.
\newblock Adaptive aggregation networks for class-incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF conference on Computer Vision
  and Pattern Recognition}, pp.\  2544--2553, 2021.

\bibitem[Lopez-Paz \& Ranzato(2017)Lopez-Paz and Ranzato]{NIPS2017GEM}
Lopez-Paz, D. and Ranzato, M.
\newblock Gradient episodic memory for continual learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, pp.\  6470--6479, 2017.

\bibitem[M{\'a}rton et~al.(2022)M{\'a}rton, Zhou, and Rajan]{marton2022linking}
M{\'a}rton, C.~D., Zhou, S., and Rajan, K.
\newblock Linking task structure and neural network dynamics.
\newblock \emph{Nature Neuroscience}, 25\penalty0 (6):\penalty0 679--681, 2022.

\bibitem[Masana et~al.(2023)Masana, Liu, Twardowski, Menta, Bagdanov, and
  van~de Weijer]{masana2022class}
Masana, M., Liu, X., Twardowski, B., Menta, M., Bagdanov, A.~D., and van~de
  Weijer, J.
\newblock Class-incremental learning: survey and performance evaluation on
  image classification.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45\penalty0 (5):\penalty0 5513--5533, 2023.

\bibitem[McCloskey \& Cohen(1989)McCloskey and
  Cohen]{mccloskey1989catastrophic}
McCloskey, M. and Cohen, N.~J.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In \emph{Psychology of Learning and Motivation}, volume~24, pp.\
  109--165. Elsevier, 1989.

\bibitem[McDonnell et~al.(2023)McDonnell, Gong, Parvaneh, Abbasnejad, and
  van~den Hengel]{mcdonnell2023ranpac}
McDonnell, M.~D., Gong, D., Parvaneh, A., Abbasnejad, E., and van~den Hengel,
  A.
\newblock {RanPAC}: Random projections and pre-trained models for continual
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~36, 2023.

\bibitem[Mehta et~al.(2023)Mehta, Patil, Chandar, and
  Strubell]{mehta2023empirical}
Mehta, S.~V., Patil, D., Chandar, S., and Strubell, E.
\newblock An empirical investigation of the role of pre-training in lifelong
  learning.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (214):\penalty0 1--50, 2023.

\bibitem[Pao \& Takefuji(1992)Pao and Takefuji]{pao1992functional}
Pao, Y.-H. and Takefuji, Y.
\newblock Functional-link net computing: theory, system architecture, and
  functionalities.
\newblock \emph{Computer}, 25\penalty0 (5):\penalty0 76--79, 1992.

\bibitem[Qiao et~al.(2024)Qiao, Pham, Cao, Le, Suganthan, Jiang, and
  Savitha]{qiao2024class}
Qiao, Z., Pham, Q., Cao, Z., Le, H.~H., Suganthan, P.~N., Jiang, X., and
  Savitha, R.
\newblock Class-incremental learning for time series: Benchmark and evaluation.
\newblock \emph{arXiv preprint arXiv:2402.12035}, 2024.

\bibitem[Rajasegaran et~al.(2019)Rajasegaran, Hayat, Khan, Khan, and
  Shao]{NIPS2019RPS-Net}
Rajasegaran, J., Hayat, M., Khan, S., Khan, F.~S., and Shao, L.
\newblock Random path selection for incremental learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pp.\  12669--12679, 2019.

\bibitem[Ramanujan et~al.(2020)Ramanujan, Wortsman, Kembhavi, Farhadi, and
  Rastegari]{ramanujan2020s}
Ramanujan, V., Wortsman, M., Kembhavi, A., Farhadi, A., and Rastegari, M.
\newblock What's hidden in a randomly weighted neural network?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11893--11902, 2020.

\bibitem[Rasch \& Born(2007)Rasch and Born]{rasch2007maintaining}
Rasch, B. and Born, J.
\newblock Maintaining memories by reactivation.
\newblock \emph{Current Opinion in Neurobiology}, 17\penalty0 (6):\penalty0
  698--703, 2007.

\bibitem[Rios et~al.(2022)Rios, Ahuja, Ndiour, Genc, Itti, and
  Tickoo]{rios2022incdfm}
Rios, A., Ahuja, N., Ndiour, I., Genc, U., Itti, L., and Tickoo, O.
\newblock {incDFM}: Incremental deep feature modeling for continual novelty
  detection.
\newblock In \emph{European Conference on Computer Vision}, pp.\  588--604.
  Springer, 2022.

\bibitem[Schwarz et~al.(2018)Schwarz, Czarnecki, Luketina, Grabska-Barwinska,
  Teh, Pascanu, and Hadsell]{ICML2018Online_EWC}
Schwarz, J., Czarnecki, W., Luketina, J., Grabska-Barwinska, A., Teh, Y.~W.,
  Pascanu, R., and Hadsell, R.
\newblock Progress \& compress: A scalable framework for continual learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4528--4537, 2018.

\bibitem[Serr\`{a} et~al.(2018)Serr\`{a}, Suris, Miron, and
  Karatzoglou]{serra2018overcoming}
Serr\`{a}, J., Suris, D., Miron, M., and Karatzoglou, A.
\newblock Overcoming catastrophic forgetting with hard attention to the task.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4548--4557. PMLR, 2018.

\bibitem[Shim et~al.(2021)Shim, Mai, Jeong, Sanner, Kim, and
  Jang]{shim2021online}
Shim, D., Mai, Z., Jeong, J., Sanner, S., Kim, H., and Jang, J.
\newblock Online class-incremental continual learning with adversarial shapley
  value.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, pp.\  9630--9638, 2021.

\bibitem[Smith et~al.(2023)Smith, Karlinsky, Gutta, Cascante-Bonilla, Kim,
  Arbelle, Panda, Feris, and Kira]{smith2023coda}
Smith, J.~S., Karlinsky, L., Gutta, V., Cascante-Bonilla, P., Kim, D., Arbelle,
  A., Panda, R., Feris, R., and Kira, Z.
\newblock {CODA-Prompt}: Continual decomposed attention-based prompting for
  rehearsal-free continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  11909--11919, 2023.

\bibitem[Tang et~al.(2021)Tang, Chen, Zhu, Yu, and Ouyang]{CVPR2021LOGD}
Tang, S., Chen, D., Zhu, J., Yu, S., and Ouyang, W.
\newblock Layerwise optimization by gradient decomposition for continual
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9634--9643, 2021.

\bibitem[Tang et~al.(2023)Tang, Peng, and Zheng]{tang2023prompt}
Tang, Y.-M., Peng, Y.-X., and Zheng, W.-S.
\newblock When prompt-based incremental learning does not meet strong
  pretraining.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  1706--1716, 2023.

\bibitem[Tong et~al.(2023)Tong, Dai, Wu, Li, Yi, and Ma]{tong2023incremental}
Tong, S., Dai, X., Wu, Z., Li, M., Yi, B., and Ma, Y.
\newblock Incremental learning of structured memory via closed-loop
  transcription.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J{\'e}gou]{touvron2021training}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J{\'e}gou,
  H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10347--10357. PMLR, 2021.

\bibitem[Vahedian et~al.(2021)Vahedian, Li, Trivedi, Jin, and
  Koutra]{vahedian2021convolutional}
Vahedian, F., Li, R., Trivedi, P., Jin, D., and Koutra, D.
\newblock Convolutional neural network dynamics: A graph perspective.
\newblock \emph{arXiv preprint arXiv:2111.05410}, 2021.

\bibitem[Van~de Ven et~al.(2020)Van~de Ven, Siegelmann, and
  Tolias]{van2020brain}
Van~de Ven, G.~M., Siegelmann, H.~T., and Tolias, A.~S.
\newblock Brain-inspired replay for continual learning with artificial neural
  networks.
\newblock \emph{Nature communications}, 11\penalty0 (1):\penalty0 4069, 2020.

\bibitem[Van~der Maaten \& Hinton(2008)Van~der Maaten and
  Hinton]{van2008visualizing}
Van~der Maaten, L. and Hinton, G.
\newblock Visualizing data using {t-SNE}.
\newblock \emph{Journal of Machine Learning Research}, 9\penalty0 (11), 2008.

\bibitem[Verma et~al.(2021)Verma, Liang, Mehta, Rai, and Carin]{CVPR2021EFT}
Verma, V.~K., Liang, K.~J., Mehta, N., Rai, P., and Carin, L.
\newblock Efficient feature transformations for discriminative and generative
  continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  13865--13875, 2021.

\bibitem[Vyas et~al.(2020)Vyas, Golub, Sussillo, and
  Shenoy]{vyas2020computation}
Vyas, S., Golub, M.~D., Sussillo, D., and Shenoy, K.~V.
\newblock Computation through neural population dynamics.
\newblock \emph{Annual Review of Neuroscience}, 43:\penalty0 249--275, 2020.

\bibitem[Wang \& Li(2017)Wang and Li]{wang2017stochastic}
Wang, D. and Li, M.
\newblock Stochastic configuration networks: Fundamentals and algorithms.
\newblock \emph{IEEE Transactions on Cybernetics}, 47\penalty0 (10):\penalty0
  3466--3479, 2017.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Zhou, Ye, and
  Zhan]{wang2022foster}
Wang, F.-Y., Zhou, D.-W., Ye, H.-J., and Zhan, D.-C.
\newblock {FOSTER}: Feature boosting and compression for class-incremental
  learning.
\newblock In \emph{European Conference on Computer Vision}, pp.\  398--414.
  Springer, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2023)Wang, Zhou, Liu, Ye, Bian, Zhan, and
  Zhao]{wang2023beef}
Wang, F.-Y., Zhou, D.-W., Liu, L., Ye, H.-J., Bian, Y., Zhan, D.-C., and Zhao,
  P.
\newblock {BEEF}: Bi-compatible class-incremental learning via energy-based
  expansion and fusion.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Bao, Zhang, Liu, Zhu, and
  Guo]{wang2022anti}
Wang, R., Bao, Y., Zhang, B., Liu, J., Zhu, W., and Guo, G.
\newblock Anti-retroactive interference for lifelong learning.
\newblock In \emph{European Conference on Computer Vision}, pp.\  163--178.
  Springer, 2022{\natexlab{b}}.

\bibitem[Wang et~al.(2022{\natexlab{c}})Wang, Zhang, Ebrahimi, Sun, Zhang, Lee,
  Ren, Su, Perot, Dy, et~al.]{wang2022dualprompt}
Wang, Z., Zhang, Z., Ebrahimi, S., Sun, R., Zhang, H., Lee, C.-Y., Ren, X., Su,
  G., Perot, V., Dy, J., et~al.
\newblock {DualPrompt}: Complementary prompting for rehearsal-free continual
  learning.
\newblock In \emph{Computer Vision--ECCV 2022: 17th European Conference, Tel
  Aviv, Israel, October 23--27, 2022, Proceedings, Part XXVI}, pp.\  631--648.
  Springer, 2022{\natexlab{c}}.

\bibitem[Wang et~al.(2022{\natexlab{d}})Wang, Zhang, Lee, Zhang, Sun, Ren, Su,
  Perot, Dy, and Pfister]{wang2022learning}
Wang, Z., Zhang, Z., Lee, C.-Y., Zhang, H., Sun, R., Ren, X., Su, G., Perot,
  V., Dy, J., and Pfister, T.
\newblock Learning to prompt for continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  139--149, 2022{\natexlab{d}}.

\bibitem[Wei et~al.(2023)Wei, Ye, Huang, Zhang, and Shan]{wei2023online}
Wei, Y., Ye, J., Huang, Z., Zhang, J., and Shan, H.
\newblock Online prototype learning for online continual learning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  18764--18774, 2023.

\bibitem[Wo{\l}czyk et~al.(2022)Wo{\l}czyk, Piczak, W{\'o}jcik, Pustelnik,
  Morawiecki, Tabor, Trzcinski, and Spurek]{wolczyk2022continual}
Wo{\l}czyk, M., Piczak, K., W{\'o}jcik, B., Pustelnik, L., Morawiecki, P.,
  Tabor, J., Trzcinski, T., and Spurek, P.
\newblock Continual learning with guarantees via weight interval constraints.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  23897--23911. PMLR, 2022.

\bibitem[Wu et~al.(2022)Wu, Swaminathan, Li, Ravichandran, Vasconcelos,
  Bhotika, and Soatto]{wu2022class}
Wu, T.-Y., Swaminathan, G., Li, Z., Ravichandran, A., Vasconcelos, N., Bhotika,
  R., and Soatto, S.
\newblock Class-incremental learning with strong pre-trained models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  9601--9610, 2022.

\bibitem[Wu et~al.(2019)Wu, Chen, Wang, Ye, Liu, Guo, and Fu]{wu2019large}
Wu, Y., Chen, Y., Wang, L., Ye, Y., Liu, Z., Guo, Y., and Fu, Y.
\newblock Large scale incremental learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  374--382, 2019.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{FashionMNIST}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock {Fashion-MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[Yan et~al.(2021)Yan, Xie, and He]{yan2021dynamically}
Yan, S., Xie, J., and He, X.
\newblock {DER}: Dynamically expandable representation for class incremental
  learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  3014--3023, 2021.

\bibitem[Yang et~al.(2023)Yang, Lin, Zhang, Liu, Liang, Ji, and
  Ye]{yang2022dynamic}
Yang, B., Lin, M., Zhang, Y., Liu, B., Liang, X., Ji, R., and Ye, Q.
\newblock Dynamic support network for few-shot class incremental learning.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 45\penalty0 (3):\penalty0 2945--2951, 2023.

\bibitem[Zeng et~al.(2019)Zeng, Chen, Cui, and Yu]{NMI2019OWM}
Zeng, G., Chen, Y., Cui, B., and Yu, S.
\newblock Continual learning of context-dependent processing in neural
  networks.
\newblock \emph{Nature Machine Intelligence}, 1\penalty0 (8):\penalty0
  364--372, 2019.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and Ganguli]{ICML2017SI}
Zenke, F., Poole, B., and Ganguli, S.
\newblock Continual learning through synaptic intelligence.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3987--3995, 2017.

\bibitem[Zhang et~al.(2022)Zhang, Bengio, and Singer]{zhang2022all}
Zhang, C., Bengio, S., and Singer, Y.
\newblock Are all layers created equal?
\newblock \emph{Journal of Machine Learning Research}, 23:\penalty0 1--28,
  2022.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Kang, Chen, and Wei]{zhang2023slca}
Zhang, G., Wang, L., Kang, G., Chen, L., and Wei, Y.
\newblock {SLAC}: Slow learner with classifier alignment for continual learning
  on a pre-trained model.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  19148--19158, 2023.

\bibitem[Zhang \& Suganthan(2016)Zhang and Suganthan]{zhang2016visual}
Zhang, L. and Suganthan, P.~N.
\newblock Visual tracking with convolutional random vector functional link
  network.
\newblock \emph{IEEE Transactions on Cybernetics}, 47\penalty0 (10):\penalty0
  3243--3253, 2016.

\bibitem[Zhou et~al.(2023)Zhou, Wang, Ye, and Zhan]{zhou2022model}
Zhou, D.-W., Wang, Q.-W., Ye, H.-J., and Zhan, D.-C.
\newblock A model or 603 exemplars: Towards memory-efficient class-incremental
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Zhuang et~al.(2022)Zhuang, Weng, Wei, Xie, Toh, and
  Lin]{zhuang2022acil}
Zhuang, H., Weng, Z., Wei, H., Xie, R., Toh, K.-A., and Lin, Z.
\newblock {ACIL}: Analytic class-incremental learning with absolute
  memorization and privacy protection.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~35, pp.\  11602--11614, 2022.

\end{thebibliography}
