\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Gao, Zhang, Meng, and
  Lin]{liu2021investigating}
Risheng Liu, Jiaxin Gao, Jin Zhang, Deyu Meng, and Zhouchen Lin.
\newblock Investigating bi-level optimization for learning and vision from a
  unified perspective: A survey and beyond.
\newblock \emph{arXiv:2101.11517}, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Li, Fan, Zhao, Huang, and
  Luo]{liu2021learning}
Risheng Liu, Zi~Li, Xin Fan, Chenying Zhao, Hao Huang, and Zhongxuan Luo.
\newblock Learning deformable image registration from optimization:
  perspective, modules, bilevel training and beyond.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021{\natexlab{b}}.

\bibitem[Franceschi et~al.(2017)Franceschi, Donini, Frasconi, and
  Pontil]{franceschi2017forward}
Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil.
\newblock Forward and reverse gradient-based hyperparameter optimization.
\newblock \emph{arXiv:1703.01785}, 2017.

\bibitem[Okuno et~al.(2018)Okuno, Takeda, and Kawana]{okuno2018hyperparameter}
Takayuki Okuno, Akiko Takeda, and Akihiro Kawana.
\newblock Hyperparameter learning via bilevel nonsmooth optimization.
\newblock \emph{arXiv:1806.01520}, 2018.

\bibitem[MacKay et~al.(2019)MacKay, Vicol, Lorraine, Duvenaud, and
  Grosse]{mackay2018self}
Matthew MacKay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger~B.
  Grosse.
\newblock Self-tuning networks: Bilevel optimization of hyperparameters using
  structured best-response functions.
\newblock In \emph{ICLR}, 2019.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimiliano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In \emph{ICML}, 2018.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Aravind Rajeswaran, Chelsea Finn, Sham~M Kakade, and Sergey Levine.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Z{\"u}gner and G{\"u}nnemann(2018)]{zugner2018adversarial}
Daniel Z{\"u}gner and Stephan G{\"u}nnemann.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock In \emph{ICLR}, 2018.

\bibitem[Liu and Liu(2021)]{liu2021boml}
Yaohua Liu and Risheng Liu.
\newblock Boml: A modularized bilevel optimization library in python for meta
  learning.
\newblock In \emph{ICMEW}. IEEE, 2021.

\bibitem[Liu et~al.(2019)Liu, Simonyan, and Yang]{liu2018darts}
Hanxiao Liu, Karen Simonyan, and Yiming Yang.
\newblock {DARTS:} differentiable architecture search.
\newblock In \emph{ICLR}, 2019.

\bibitem[Liang et~al.(2019)Liang, Zhang, Sun, He, Huang, Zhuang, and
  Li]{liang2019darts+}
Hanwen Liang, Shifeng Zhang, Jiacheng Sun, Xingqiu He, Weiran Huang, Kechen
  Zhuang, and Zhenguo Li.
\newblock Darts+: Improved differentiable architecture search with early
  stopping.
\newblock \emph{arXiv:1909.06035}, 2019.

\bibitem[Chen et~al.(2019)Chen, Xie, Wu, and Tian]{chen2019progressive}
Xin Chen, Lingxi Xie, Jun Wu, and Qi~Tian.
\newblock Progressive differentiable architecture search: Bridging the depth
  gap between search and evaluation.
\newblock In \emph{CVPR}, 2019.

\bibitem[Pfau and Vinyals(2016)]{pfau2016connecting}
David Pfau and Oriol Vinyals.
\newblock Connecting generative adversarial networks and actor-critic methods.
\newblock \emph{arXiv preprint arXiv:1610.01945}, 2016.

\bibitem[Yang et~al.(2019)Yang, Chen, Hong, and Wang]{yang2019provably}
Zhuoran Yang, Yongxin Chen, Mingyi Hong, and Zhaoran Wang.
\newblock Provably global convergence of actor-critic: A case for linear
  quadratic regulator with ergodic cost.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Dougal Maclaurin, David Duvenaud, and Ryan Adams.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{ICML}, 2015.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and
  Boots]{shaban2019truncated}
Amirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{AISTATS}, 2019.

\bibitem[Nichol et~al.(2018)Nichol, Achiam, and Schulman]{nichol2018first}
Alex Nichol, Joshua Achiam, and John Schulman.
\newblock On first-order meta-learning algorithms.
\newblock \emph{arXiv:1803.02999}, 2018.

\bibitem[Nichol and Schulman(2018)]{nichol2018reptile}
Alex Nichol and John Schulman.
\newblock Reptile: a scalable metalearning algorithm.
\newblock \emph{arXiv:1803.02999}, 2\penalty0 (2):\penalty0 1, 2018.

\bibitem[Li and Malik(2016)]{li2016learning}
Ke~Li and Jitendra Malik.
\newblock Learning to optimize.
\newblock \emph{arXiv:1606.01885}, 2016.

\bibitem[Lee and Choi(2018)]{lee2018meta}
Yoonho Lee and Seungjin Choi.
\newblock Meta-learning with adaptive layerwise metric and subspace.
\newblock \emph{CoRR}, abs/1801.05558, 2018.

\bibitem[Park and Oliva(2019)]{park2019meta}
Eunbyung Park and Junier~B Oliva.
\newblock Meta-curvature.
\newblock \emph{arXiv:1902.03356}, 2019.

\bibitem[Flennerhag et~al.(2019)Flennerhag, Rusu, Pascanu, Visin, Yin, and
  Hadsell]{flennerhag2019meta}
Sebastian Flennerhag, Andrei~A Rusu, Razvan Pascanu, Francesco Visin, Hujun
  Yin, and Raia Hadsell.
\newblock Meta-learning with warped gradient descent.
\newblock \emph{arXiv:1909.00025}, 2019.

\bibitem[Liu et~al.(2020)Liu, Mu, Yuan, Zeng, and Zhang]{liu2020generic}
Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang.
\newblock A generic first-order algorithmic framework for bi-level programming
  beyond lower-level singleton.
\newblock In \emph{ICML}, 2020.

\bibitem[Liu et~al.(2021{\natexlab{c}})Liu, Mu, Yuan, Zeng, and
  Zhang]{liu2021general}
Risheng Liu, Pan Mu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang.
\newblock A general descent aggregation framework for gradient-based bi-level
  optimization, 2021{\natexlab{c}}.

\bibitem[Ji et~al.(2020{\natexlab{a}})Ji, Yang, and Liang]{ji2020bilevel}
Kaiyi Ji, Junjie Yang, and Yingbin Liang.
\newblock Bilevel optimization: Nonasymptotic analysis and faster algorithms.
\newblock \emph{arXiv:2010.07962}, 2020{\natexlab{a}}.

\bibitem[Grazzi et~al.(2020)Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020on}
Riccardo Grazzi, Luca Franceschi, Massimiliano Pontil, and Saverio Salzo.
\newblock On the iteration complexity of hypergradient computation.
\newblock In \emph{ICML}, 2020.

\bibitem[Ji and Liang(2021)]{ji2021lower}
Kaiyi Ji and Yingbin Liang.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock \emph{arXiv:2102.03926v2}, 2021.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Fabian Pedregosa.
\newblock Hyperparameter optimization with approximate gradient.
\newblock \emph{arXiv:1602.02355}, 2016.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and
  Duvenaud]{lorraine2020optimizing}
Jonathan Lorraine, Paul Vicol, and David Duvenaud.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{AISTATS}, 2020.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{ICML}, 2013.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Chelsea Finn, Pieter Abbeel, and Sergey Levine.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{ICML}, 2017.

\bibitem[Zhou et~al.(2019)Zhou, Yuan, Xu, Yan, and Feng]{zhou2019efficient}
Pan Zhou, Xiaotong Yuan, Huan Xu, Shuicheng Yan, and Jiashi Feng.
\newblock Efficient meta learning via minibatch proximal update.
\newblock 2019.

\bibitem[Collins et~al.(2020)Collins, Mokhtari, and
  Shakkottai]{collins2020task}
Liam Collins, Aryan Mokhtari, and Sanjay Shakkottai.
\newblock Task-robust model-agnostic meta-learning.
\newblock 2020.

\bibitem[Wang et~al.(2021)Wang, Xu, Liu, Chen, Weng, Gan, and
  Wang]{wang2021fast}
Ren Wang, Kaidi Xu, Sijia Liu, Pin-Yu Chen, Tsui-Wei Weng, Chuang Gan, and Meng
  Wang.
\newblock On fast adversarial robustness adaptation in model-agnostic
  meta-learning.
\newblock \emph{arXiv:2102.10454}, 2021.

\bibitem[Fallah et~al.(2021)Fallah, Mokhtari, and
  Ozdaglar]{fallah2021generalization}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock Generalization of model-agnostic meta-learning algorithms: Recurring
  and unseen tasks.
\newblock \emph{arXiv:2102.03832}, 2021.

\bibitem[Raghu et~al.(2019)Raghu, Raghu, Bengio, and Vinyals]{raghu2019rapid}
Aniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals.
\newblock Rapid learning or feature reuse? towards understanding the
  effectiveness of maml.
\newblock In \emph{ICLR}, 2019.

\bibitem[Fallah et~al.(2020)Fallah, Mokhtari, and
  Ozdaglar]{fallah2020convergence}
Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar.
\newblock On the convergence theory of gradient-based model-agnostic
  meta-learning algorithms.
\newblock In \emph{SIATATS}, 2020.

\bibitem[Ji et~al.(2020{\natexlab{b}})Ji, Yang, and Liang]{ji2020theoretical}
Kaiyi Ji, Junjie Yang, and Yingbin Liang.
\newblock Theoretical convergence of multi-step model-agnostic meta-learning.
\newblock \emph{arXiv e-prints}, pages arXiv--2002, 2020{\natexlab{b}}.

\bibitem[Ji et~al.(2020{\natexlab{c}})Ji, Lee, Liang, and
  Poor]{NEURIPS2020_84c578f2}
Kaiyi Ji, Jason~D Lee, Yingbin Liang, and H.~Vincent Poor.
\newblock Convergence of meta-learning with task-specific adaptation over
  partial parameters.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin,
  editors, \emph{NeurIPS}, 2020{\natexlab{c}}.

\bibitem[Liu et~al.(2021{\natexlab{d}})Liu, Liu, Zeng, Zhang, and
  Zhang]{liu2021valuefunctionbased}
Risheng Liu, Xuan Liu, Shangzhi Zeng, Jin Zhang, and Yixuan Zhang.
\newblock Value-function-based sequential minimization for bi-level
  optimization, 2021{\natexlab{d}}.

\bibitem[Ye et~al.(2021)Ye, Yuan, Zeng, and Zhang]{ye2021difference}
Jane~J Ye, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang.
\newblock Difference of convex algorithms for bilevel programs with
  applications in hyperparameter selection.
\newblock \emph{arXiv:2102.09006}, 2021.

\bibitem[Liu et~al.(2021{\natexlab{e}})Liu, Liu, Yuan, Zeng, and
  Zhang]{LiuLYZZ21}
Risheng Liu, Xuan Liu, Xiaoming Yuan, Shangzhi Zeng, and Jin Zhang.
\newblock A value-function-based interior-point method for non-convex bi-level
  optimization.
\newblock In \emph{ICML}, 2021{\natexlab{e}}.

\bibitem[Beck(2017)]{beck2017first}
Amir Beck.
\newblock \emph{First-order methods in optimization}.
\newblock SIAM, 2017.

\bibitem[Nesterov(1983)]{nesterov1983a}
Yurii~E Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In \emph{Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem[Beck and Teboulle(2009)]{beck2009fast}
Amir Beck and Marc Teboulle.
\newblock A fast iterative shrinkage-thresholding algorithm for linear inverse
  problems.
\newblock 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Wierstra,
  et~al.]{vinyals2016matching}
Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et~al.
\newblock Matching networks for one shot learning.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Snell et~al.(2017)Snell, Swersky, and Zemel]{snell2017prototypical}
Jake Snell, Kevin Swersky, and Richard~S Zemel.
\newblock Prototypical networks for few-shot learning.
\newblock \emph{arXiv:1703.05175}, 2017.

\bibitem[Sung et~al.(2018)Sung, Yang, Zhang, Xiang, Torr, and
  Hospedales]{sung2018learning}
Flood Sung, Yongxin Yang, Li~Zhang, Tao Xiang, Philip~HS Torr, and Timothy~M
  Hospedales.
\newblock Learning to compare: Relation network for few-shot learning.
\newblock In \emph{CVPR}, 2018.

\bibitem[Ren et~al.(2018)Ren, Triantafillou, Ravi, Snell, Swersky, Tenenbaum,
  Larochelle, and Zemel]{ren2018meta}
Mengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky,
  Joshua~B Tenenbaum, Hugo Larochelle, and Richard~S Zemel.
\newblock Meta-learning for semi-supervised few-shot classification.
\newblock \emph{arXiv:1803.00676}, 2018.

\bibitem[Oreshkin et~al.(2018)Oreshkin, Rodriguez, and
  Lacoste]{oreshkin2018tadam}
Boris~N Oreshkin, Pau Rodriguez, and Alexandre Lacoste.
\newblock Tadam: Task dependent adaptive metric for improved few-shot learning.
\newblock \emph{arXiv:1805.10123}, 2018.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Han Xiao, Kashif Rasul, and Roland Vollgraf.
\newblock Fashion-mnist: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv:1708.07747}, 2017.

\end{thebibliography}
