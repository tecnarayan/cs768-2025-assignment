\begin{thebibliography}{10}

\bibitem{lognorm}
Jhon Atchison and Sheng~M Shen.
\newblock Logistic-normal distributions: Some properties and uses.
\newblock {\em Biometrika}, 67(2):261--272, 1980.

\bibitem{u-vit}
Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu.
\newblock All are worth words: A vit backbone for diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 22669--22679, 2023.

\bibitem{beit}
Hangbo Bao, Li~Dong, Songhao Piao, and Furu Wei.
\newblock Beit: Bert pre-training of image transformers.
\newblock {\em arXiv preprint arXiv:2106.08254}, 2021.

\bibitem{biggan}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock {\em arXiv preprint arXiv:1809.11096}, 2018.

\bibitem{CAN}
Han Cai, Muyang Li, Zhuoyang Zhang, Qinsheng Zhang, Ming-Yu Liu, and Song Han.
\newblock Condition-aware neural network for controlled image generation.
\newblock {\em arXiv preprint arXiv:2404.01143}, 2024.

\bibitem{maskgit}
Huiwen Chang, Han Zhang, Lu~Jiang, Ce~Liu, and William~T Freeman.
\newblock Maskgit: Masked generative image transformer.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11315--11325, 2022.

\bibitem{pixlart-sigma}
Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li.
\newblock Pixart-$\backslash$sigma: Weak-to-strong training of diffusion transformer for 4k text-to-image generation.
\newblock {\em arXiv preprint arXiv:2403.04692}, 2024.

\bibitem{pixart-delta}
Junsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping Luo, Hang Zhao, and Zhenguo Li.
\newblock Pixart-$\{$$\backslash$delta$\}$: Fast and controllable image generation with latent consistency models.
\newblock {\em arXiv preprint arXiv:2401.05252}, 2024.

\bibitem{pixart-alpha}
Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et~al.
\newblock Pixart-$\backslash$alpha: Fast training of diffusion transformer for photorealistic text-to-image synthesis.
\newblock {\em arXiv preprint arXiv:2310.00426}, 2023.

\bibitem{chen2023importance}
Ting Chen.
\newblock On the importance of noise scheduling for diffusion models.
\newblock {\em arXiv preprint arXiv:2301.10972}, 2023.

\bibitem{mocov2}
Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He.
\newblock Improved baselines with momentum contrastive learning.
\newblock {\em arXiv preprint arXiv:2003.04297}, 2020.

\bibitem{creswell2018generative}
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil~A Bharath.
\newblock Generative adversarial networks: An overview.
\newblock {\em IEEE signal processing magazine}, 35(1):53--65, 2018.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern recognition}, pages 248--255. Ieee, 2009.

\bibitem{diffbeatgan}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock {\em Advances in neural information processing systems}, 34:8780--8794, 2021.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{sd3}
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M{\"u}ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et~al.
\newblock Scaling rectified flow transformers for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2403.03206}, 2024.

\bibitem{lumina}
Peng Gao, Le~Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu~Luo, Longtian Qiu, Yuhang Zhang, et~al.
\newblock Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers.
\newblock {\em arXiv preprint arXiv:2405.05945}, 2024.

\bibitem{mdt}
Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan.
\newblock Masked diffusion transformer is a strong image synthesizer.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 23164--23173, 2023.

\bibitem{goodfellow2020generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial networks.
\newblock {\em Communications of the ACM}, 63(11):139--144, 2020.

\bibitem{min_snr}
Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo.
\newblock Efficient diffusion training via min-snr weighting strategy.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 7441--7451, 2023.

\bibitem{mae}
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Masked autoencoders are scalable vision learners.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 16000--16009, 2022.

\bibitem{ddpm}
Jonathan Ho, Ajay Jain, and Pieter Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{cdm}
Jonathan Ho, Chitwan Saharia, William Chan, David~J Fleet, Mohammad Norouzi, and Tim Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock {\em Journal of Machine Learning Research}, 23(47):1--33, 2022.

\bibitem{videodiff}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David~J Fleet.
\newblock Video diffusion models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:8633--8646, 2022.

\bibitem{simple_diffusion}
Emiel Hoogeboom, Jonathan Heek, and Tim Salimans.
\newblock simple diffusion: End-to-end diffusion for high resolution images.
\newblock In {\em International Conference on Machine Learning}, pages 13213--13232. PMLR, 2023.

\bibitem{rin}
Allan Jabri, David Fleet, and Ting Chen.
\newblock Scalable adaptive computation for iterative generation.
\newblock {\em arXiv preprint arXiv:2212.11972}, 2022.

\bibitem{kingma2024understanding}
Diederik Kingma and Ruiqi Gao.
\newblock Understanding diffusion objectives as the elbo with simple data augmentation.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{vae}
Diederik~P Kingma.
\newblock Auto-encoding variational bayes.
\newblock {\em arXiv preprint arXiv:1312.6114}, 2013.

\bibitem{flow_matching}
Yaron Lipman, Ricky~TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le.
\newblock Flow matching for generative modeling.
\newblock {\em arXiv preprint arXiv:2210.02747}, 2022.

\bibitem{rf}
Xingchao Liu, Chengyue Gong, and Qiang Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with rectified flow.
\newblock {\em arXiv preprint arXiv:2209.03003}, 2022.

\bibitem{lu2023vdt}
Haoyu Lu, Guoxing Yang, Nanyi Fei, Yuqi Huo, Zhiwu Lu, Ping Luo, and Mingyu Ding.
\newblock Vdt: General-purpose video diffusion transformers via mask modeling.
\newblock In {\em The Twelfth International Conference on Learning Representations}, 2023.

\bibitem{sit}
Nanye Ma, Mark Goldstein, Michael~S Albergo, Nicholas~M Boffi, Eric Vanden-Eijnden, and Saining Xie.
\newblock Sit: Exploring flow and diffusion-based generative models with scalable interpolant transformers.
\newblock {\em arXiv preprint arXiv:2401.08740}, 2024.

\bibitem{latte}
Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu~Qiao.
\newblock Latte: Latent diffusion transformer for video generation.
\newblock {\em arXiv preprint arXiv:2401.03048}, 2024.

\bibitem{on_distillation}
Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.
\newblock On distillation of guided diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14297--14306, 2023.

\bibitem{improved_diffusion}
Alexander~Quinn Nichol and Prafulla Dhariwal.
\newblock Improved denoising diffusion probabilistic models.
\newblock In {\em International conference on machine learning}, pages 8162--8171. PMLR, 2021.

\bibitem{openai2024sora}
OpenAI.
\newblock Sora.
\newblock \url{https://openai.com/sora}, 2024.

\bibitem{dit}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4195--4205, 2023.

\bibitem{sdxl}
Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M{\"u}ller, Joe Penna, and Robin Rombach.
\newblock Sdxl: Improving latent diffusion models for high-resolution image synthesis.
\newblock {\em arXiv preprint arXiv:2307.01952}, 2023.

\bibitem{t5}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text transformer.
\newblock {\em Journal of machine learning research}, 21(140):1--67, 2020.

\bibitem{latentdiff}
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj{\"o}rn Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{unet}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In {\em Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18}, pages 234--241. Springer, 2015.

\bibitem{sauer2022stylegan}
Axel Sauer, Katja Schwarz, and Andreas Geiger.
\newblock Stylegan-xl: Scaling stylegan to large diverse datasets.
\newblock In {\em ACM SIGGRAPH 2022 conference proceedings}, pages 1--10, 2022.

\bibitem{sde}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{transformers}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{maskdit}
Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar.
\newblock Fast training of diffusion models with masked transformers.
\newblock {\em arXiv preprint arXiv:2306.09305}, 2023.

\bibitem{sd-dit}
Rui Zhu, Yingwei Pan, Yehao Li, Ting Yao, Zhenglong Sun, Tao Mei, and Chang~Wen Chen.
\newblock Sd-dit: Unleashing the power of self-supervised discrimination in diffusion transformer.
\newblock {\em arXiv preprint arXiv:2403.17004}, 2024.

\end{thebibliography}
