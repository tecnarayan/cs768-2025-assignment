@article{lombrozo2006structure,
  title={The structure and function of explanations},
  author={Lombrozo, Tania},
  journal={Trends in cognitive sciences},
  volume={10},
  number={10},
  pages={464--470},
  year={2006},
  publisher={Elsevier}
}

@article{lombrozo2006functional,
  title={Functional explanation and the function of explanation},
  author={Lombrozo, Tania and Carey, Susan},
  journal={Cognition},
  volume={99},
  number={2},
  pages={167--204},
  year={2006},
  publisher={Elsevier}
}

@article{rezende2020causally,
  title={Causally correct partial models for reinforcement learning},
  author={Rezende, Danilo J and Danihelka, Ivo and Papamakarios, George and Ke, Nan Rosemary and Jiang, Ray and Weber, Theophane and Gregor, Karol and Merzic, Hamza and Viola, Fabio and Wang, Jane and others},
  journal={arXiv preprint arXiv:2002.02836},
  year={2020}
}

@article{hase2021can,
  title={When can models learn from explanations? a formal framework for understanding the roles of explanation data},
  author={Hase, Peter and Bansal, Mohit},
  journal={arXiv preprint arXiv:2102.02201},
  year={2021}
}

@article{camburu2018snli,
  title={e-SNLI: Natural Language Inference with Natural Language Explanations},
  author={Camburu, Oana-Maria and Rockt{\"a}schel, Tim and Lukasiewicz, Thomas and Blunsom, Phil},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  pages={9539--9549},
  year={2018}
}

@InProceedings{wong21leveraging,
  title = 	 {Leveraging Language to Learn Program Abstractions and Search Heuristics},
  author =       {Wong, Catherine and Ellis, Kevin M and Tenenbaum, Joshua and Andreas, Jacob},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {11193--11204},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/wong21a/wong21a.pdf},
  url = 	 {http://proceedings.mlr.press/v139/wong21a.html},
  abstract = 	 {Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, robust, andgeneralizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an efficient search strategy for finding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search efficiency and generalization on three domains {–} string editing, image composition, and abstract reasoning about scenes {–} even when no natural language hints are available at test time.}
}

@article{ellis2020dreamcoder,
  title={Dreamcoder: Growing generalizable, interpretable knowledge with wake-sleep bayesian program learning},
  author={Ellis, Kevin and Wong, Catherine and Nye, Maxwell and Sable-Meyer, Mathias and Cary, Luc and Morales, Lucas and Hewitt, Luke and Solar-Lezama, Armando and Tenenbaum, Joshua B},
  journal={arXiv preprint arXiv:2006.08381},
  year={2020}
}

@article{wang2021alchemy,
  title={Alchemy: A structured task distribution for meta-reinforcement learning},
  author={Wang, Jane X and King, Michael and Porcel, Nicolas and Kurth-Nelson, Zeb and Zhu, Tina and Deck, Charlie and Choy, Peter and Cassin, Mary and Reynolds, Malcolm and Song, Francis and others},
  journal={arXiv preprint arXiv:2102.02926},
  year={2021}
}

@inproceedings{jaderberg2016reinforcement,
  title={Reinforcement learning with unsupervised auxiliary tasks},
  author={Jaderberg, Max and Mnih, Volodymyr and Czarnecki, Wojciech Marian and Schaul, Tom and Leibo, Joel Z and Silver, David and Kavukcuoglu, Koray},
  journal={arXiv preprint arXiv:1611.05397},
  booktitle={International Conference on Learning Representations},
  year={2016}
}

@article{nam2021underlies,
  title={What underlies rapid learning and systematic generalization in humans.},
  author={Nam, Andrew Joo Hun and McClelland, James L},
  journal={arXiv preprint arXiv:2102.02926},
  year={2021}
}

@article{amsterlaw2006theories,
  title={Theories of mind in transition: A microgenetic study of the development of false belief understanding},
  author={Amsterlaw, Jennifer and Wellman, Henry M},
  journal={Journal of cognition and development},
  volume={7},
  number={2},
  pages={139--172},
  year={2006},
  publisher={Taylor \& Francis}
}

@article{chi1994eliciting,
title = {Eliciting self-explanations improves understanding},
journal = {Cognitive Science},
volume = {18},
number = {3},
pages = {439-477},
year = {1994},
issn = {0364-0213},
doi = {https://doi.org/10.1016/0364-0213(94)90016-7},
url = {https://www.sciencedirect.com/science/article/pii/0364021394900167},
author = {Michelene T.H. Chi and Nicholas {De Leeuw} and Mei-Hung Chiu and Christian Lavancher},
abstract = {Learning involves the integration of new information into existing knowledge. Generating explanations to oneself (self-explaining) facilitates that integration process. Previously, self-explanation has been shown to improve the acquisition of problem-solving skills when studying worked-out examples. This study extends that finding, showing that self-explanation can also be facilitative when it is explicitly promoted, in the context of learning declarative knowledge from an expository text. Without any extensive training, 14 eighth-grade students were merely asked to self-explain after reading each line of a passage on the human circulatory system. Ten students in the control group read the same text twice, but were not prompted to self-explain. All of the students were tested for their circulatory system knowledge before and after reading the text. The prompted group had a greater gain from the pretest to the posttest. Moreover, prompted students who generated a large number of self-explanations (the high explainers) learned with greater understanding than low explainers. Understanding was assessed by answering very complex questions and inducing the function of a component when it was only implicitly stated. Understanding was further captured by a mental model analysis of the self-explanation protocols. High explainers all achieved the correct mental model of the circulatory system, whereas many of the unprompted students as well as the low explainers did not. Three processing characteristics of self-explaining are considered as reasons for the gains in deeper understanding.}
}

@article{rittle2006promoting,
  title={Promoting transfer: Effects of self-explanation and direct instruction},
  author={Rittle-Johnson, Bethany},
  journal={Child development},
  volume={77},
  number={1},
  pages={1--15},
  year={2006},
  publisher={Wiley Online Library}
}

@article{wood1976role,
  title={The role of tutoring in problem solving},
  author={Wood, David and Bruner, Jerome S and Ross, Gail},
  journal={Journal of child psychology and psychiatry},
  volume={17},
  number={2},
  pages={89--100},
  year={1976},
  publisher={Wiley Online Library}
}

@article{ahn1992schema,
  title={Schema acquisition from a single example.},
  author={Ahn, Woo-kyoung and Brewer, William F and Mooney, Raymond J},
  journal={Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume={18},
  number={2},
  pages={391},
  year={1992},
  publisher={American Psychological Association}
}

@inproceedings{santoro2018measuring,
  title={Measuring abstract reasoning in neural networks},
  author={Santoro, Adam and Hill, Felix and Barrett, David and Morcos, Ari and Lillicrap, Timothy},
  booktitle={International Conference on Machine Learning},
  pages={4477--4486},
  year={2018}
}

@article{gentner2003we,
  title={Why we’re so smart},
  author={Gentner, Dedre},
  journal={Language in mind: Advances in the study of language and thought},
  volume={195235},
  year={2003}
}

@inproceedings{mu-etal-2020-shaping,
    title = "Shaping Visual Representations with Language for Few-Shot Classification",
    author = "Mu, Jesse  and
      Liang, Percy  and
      Goodman, Noah",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.436",
    doi = "10.18653/v1/2020.acl-main.436",
    pages = "4823--4830",
    abstract = "By describing the features and abstractions of our world, language is a crucial tool for human learning and a promising source of supervision for machine learning models. We use language to improve few-shot visual classification in the underexplored scenario where natural language task descriptions are available during training, but unavailable for novel tasks at test time. Existing models for this setting sample new descriptions at test time and use those to classify images. Instead, we propose language-shaped learning (LSL), an end-to-end model that regularizes visual representations to predict language. LSL is conceptually simpler, more data efficient, and outperforms baselines in two challenging few-shot domains.",
}

@inproceedings{andreas2018learning,
  title={Learning with Latent Language},
  author={Andreas, Jacob and Klein, Dan and Levine, Sergey},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={2166--2179},
  year={2018}
}

@inproceedings{cassens2021explanation,
  title={Explanation in Human Thinking},
  author={Cassens, J{\"o}rg and Habenicht, Lorenz and Blohm, Julian and Wegener, Rebekah and Korman, Joanna and Khemlani, Sangeet and Gronchi, Giorgio and Byrne, Ruth MJ and Warren, Greta and Quinn, Molly S and others},
  booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume={43},
  number={43},
  year={2021}
}

@article{goyal2019using,
  title={Using natural language for reward shaping in reinforcement learning},
  author={Goyal, Prasoon and Niekum, Scott and Mooney, Raymond J},
  journal={arXiv preprint arXiv:1903.02020},
  year={2019}
}

@article{luketina2019survey,
  title={A survey of reinforcement learning informed by natural language},
  author={Luketina, Jelena and Nardelli, Nantas and Farquhar, Gregory and Foerster, Jakob and Andreas, Jacob and Grefenstette, Edward and Whiteson, Shimon and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:1906.03926},
  year={2019}
}

@article{hermann2017grounded,
  title={Grounded language learning in a simulated 3d world},
  author={Hermann, Karl Moritz and Hill, Felix and Green, Simon and Wang, Fumin and Faulkner, Ryan and Soyer, Hubert and Szepesvari, David and Czarnecki, Wojciech Marian and Jaderberg, Max and Teplyashin, Denis and others},
  journal={arXiv preprint arXiv:1706.06551},
  year={2017}
}

@inproceedings{hill2019environmental,
  title={Environmental drivers of systematicity and generalization in a situated agent},
  author={Hill, Felix and Lampinen, Andrew and Schneider, Rosalia and Clark, Stephen and Botvinick, Matthew and McClelland, James L and Santoro, Adam},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{jiang2019language,
  title={Language as an abstraction for hierarchical deep reinforcement learning},
  author={Jiang, Yiding and Gu, Shixiang and Murphy, Kevin and Finn, Chelsea},
  journal={arXiv preprint arXiv:1906.07343},
  year={2019}
}

@article{penn2008darwin,
  title={Darwin's mistake: Explaining the discontinuity between human and nonhuman minds},
  author={Penn, Derek C and Holyoak, Keith J and Povinelli, Daniel J},
  journal={Behavioral and Brain Sciences},
  volume={31},
  number={2},
  pages={109--130},
  year={2008},
  publisher={Cambridge University Press}
}

@article{lupyan2008taking,
  title={Taking symbols for granted? Is the discontinuity between human and nonhuman minds the product of external symbol systems?},
  author={Lupyan, Gary},
  journal={Behavioral and Brain Sciences},
  volume={31},
  number={2},
  pages={140--141},
  year={2008},
  publisher={Cambridge University Press}
}

@article{gentner2008relational,
  title={Relational language supports relational cognition in humans and apes},
  author={Gentner, Dedre and Christie, Stella},
  journal={Behavioral and Brain Sciences},
  volume={31},
  number={2},
  pages={136--137},
  year={2008},
  publisher={Cambridge University Press}
}

@article{katz2021issues,
  title={Issues in the comparative cognition of same/different abstract-concept learning},
  author={Katz, Jeffrey S and Wright, Anthony A},
  journal={Current Opinion in Behavioral Sciences},
  volume={37},
  pages={29--34},
  year={2021},
  publisher={Elsevier}
}

@book{keil2000explanation,
  title={Explanation and cognition},
  author={Keil, Frank C and Wilson, Robert Andrew and Wilson, Robert Anton},
  year={2000},
  publisher={MIT press}
}

@article{edwards2019explanation,
  title={Explanation recruits comparison in a category-learning task},
  author={Edwards, Brian J and Williams, Joseph J and Gentner, Dedre and Lombrozo, Tania},
  journal={Cognition},
  volume={185},
  pages={21--38},
  year={2019},
  publisher={Elsevier}
}

@article{holyoak2021emergence,
  title={Emergence of relational reasoning},
  author={Holyoak, Keith J and Lu, Hongjing},
  journal={Current Opinion in Behavioral Sciences},
  volume={37},
  pages={118--124},
  year={2021},
  publisher={Elsevier}
}

@article{fodor1988connectionism,
  title={Connectionism and cognitive architecture: A critical analysis},
  author={Fodor, Jerry A and Pylyshyn, Zenon W},
  journal={Cognition},
  volume={28},
  number={1-2},
  pages={3--71},
  year={1988},
  publisher={Elsevier}
}

@article{marcus2018deep,
  title={Deep learning: A critical appraisal},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:1801.00631},
  year={2018}
}

@article{marcus2020next,
  title={The next decade in ai: four steps towards robust artificial intelligence},
  author={Marcus, Gary},
  journal={arXiv preprint arXiv:2002.06177},
  year={2020}
}

@article{lake2017building,
  title={Building machines that learn and think like people},
  author={Lake, Brenden M and Ullman, Tomer D and Tenenbaum, Joshua B and Gershman, Samuel J},
  journal={Behavioral and brain sciences},
  volume={40},
  year={2017},
  publisher={Cambridge University Press}
}

@article{pearl2018theoretical,
  title={Theoretical impediments to machine learning with seven sparks from the causal revolution},
  author={Pearl, Judea},
  journal={arXiv preprint arXiv:1801.04016},
  year={2018}
}

@inproceedings{chen2018learning,
  title={Learning to explain: An information-theoretic perspective on model interpretation},
  author={Chen, Jianbo and Song, Le and Wainwright, Martin and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={883--892},
  year={2018},
  organization={PMLR}
}

@article{xie2020explainable,
  title={Explainable deep learning: A field guide for the uninitiated},
  author={Xie, Ning and Ras, Gabrielle and van Gerven, Marcel and Doran, Derek},
  journal={arXiv preprint arXiv:2004.14545},
  year={2020}
}

@article{santoro2021symbolic,
  title={Symbolic behaviour in artificial intelligence},
  author={Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
  journal={arXiv preprint arXiv:2102.03406},
  year={2021}
}

@article{cabi2019scaling,
  title={Scaling data-driven robotics with reward sketching and batch reinforcement learning},
  author={Cabi, Serkan and Colmenarejo, Sergio G{\'o}mez and Novikov, Alexander and Konyushkova, Ksenia and Reed, Scott and Jeong, Rae and Zolna, Konrad and Aytar, Yusuf and Budden, David and Vecerik, Mel and others},
  journal={arXiv preprint arXiv:1909.12200},
  year={2019}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and Sutton, Charles},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{geiger2020relational,
  title={Relational reasoning and generalization using non-symbolic neural networks},
  author={Geiger, Atticus and Carstensen, Alexandra and Frank, Michael C and Potts, Christopher},
  journal={arXiv preprint arXiv:2006.07968},
  year={2020}
}

@article{puebla2021can,
  title={Can Deep Convolutional Neural Networks Learn Same-Different Relations?},
  author={Puebla, Guillermo and Bowers, Jeffrey S},
  journal={bioRxiv preprint},
  year={2021}
}

@inproceedings{hermann2020shapes,
  title={What shapes feature representations? exploring datasets, architectures, and training},
  author={Hermann, Katherine L and Lampinen, Andrew K},
  booktitle={Advances in Neural Information Processing Systems},
  year={2020}
}

@inproceedings{parisotto2020stabilizing,
  title={Stabilizing transformers for reinforcement learning},
  author={Parisotto, Emilio and Song, Francis and Rae, Jack and Pascanu, Razvan and Gulcehre, Caglar and Jayakumar, Siddhant and Jaderberg, Max and Kaufman, Raphael Lopez and Clark, Aidan and Noury, Seb and others},
  booktitle={International Conference on Machine Learning},
  pages={7487--7498},
  year={2020},
  organization={PMLR}
}

@article{recchia2021teaching,
  title={Teaching Autoregressive Language Models Complex Tasks By Demonstration},
  author={Recchia, Gabriel},
  journal={arXiv preprint arXiv:2109.02102},
  year={2021}
}

@article{pearl2019seven,
  title={The seven tools of causal inference, with reflections on machine learning},
  author={Pearl, Judea},
  journal={Communications of the ACM},
  volume={62},
  number={3},
  pages={54--60},
  year={2019},
  publisher={ACM New York, NY, USA}
}

@article{dasgupta2019causal,
  title={Causal reasoning from meta-reinforcement learning},
  author={Dasgupta, Ishita and Wang, Jane and Chiappa, Silvia and Mitrovic, Jovana and Ortega, Pedro and Raposo, David and Hughes, Edward and Battaglia, Peter and Botvinick, Matthew and Kurth-Nelson, Zeb},
  journal={arXiv preprint arXiv:1901.08162},
  year={2019}
}

@inproceedings{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{le2021many,
  title={How many data points is a prompt worth?},
  author={Le Scao, Teven and Rush, Alexander M},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2627--2636},
  year={2021}
}

@article{jiang2020can,
  title={How can we know what language models know?},
  author={Jiang, Zhengbao and Xu, Frank F and Araki, Jun and Neubig, Graham},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={423--438},
  year={2020},
  publisher={MIT Press}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}

@article{geirhos2020shortcut,
  title={Shortcut learning in deep neural networks},
  author={Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A},
  journal={Nature Machine Intelligence},
  volume={2},
  number={11},
  pages={665--673},
  year={2020},
  publisher={Nature Publishing Group}
}

@article{jumper2021highly,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={Jumper, John and Evans, Richard and Pritzel, Alexander and Green, Tim and Figurnov, Michael and Ronneberger, Olaf and Tunyasuvunakool, Kathryn and Bates, Russ and {\v{Z}}{\'\i}dek, Augustin and Potapenko, Anna and others},
  journal={Nature},
  volume={596},
  number={7873},
  pages={583--589},
  year={2021},
  publisher={Nature Publishing Group}
}

@article{van1988pragmatic,
  title={The pragmatic theory of explanation},
  author={Van Fraassen, Bas},
  journal={Theories of Explanation},
  volume={8},
  pages={135--155},
  year={1988},
  publisher={New York: Oxford University Press}
}

@misc{wei2021finetuned,
      title={Finetuned Language Models Are Zero-Shot Learners}, 
      author={Jason Wei and Maarten Bosma and Vincent Y. Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V. Le},
      year={2021},
      eprint={2109.01652},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inproceedings{topin2019generation,
  title={Generation of policy-level explanations for reinforcement learning},
  author={Topin, Nicholay and Veloso, Manuela},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={2514--2521},
  year={2019}
}

@article{humplik2019meta,
  title={Meta reinforcement learning as task inference},
  author={Humplik, Jan and Galashov, Alexandre and Hasenclever, Leonard and Ortega, Pedro A and Teh, Yee Whye and Heess, Nicolas},
  journal={arXiv preprint arXiv:1905.06424},
  year={2019}
}


@inproceedings{rakelly19efficient,
  title = 	 {Efficient Off-Policy Meta-Reinforcement Learning via Probabilistic Context Variables},
  author =       {Rakelly, Kate and Zhou, Aurick and Finn, Chelsea and Levine, Sergey and Quillen, Deirdre},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {5331--5340},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/rakelly19a.html},
  abstract = 	 {Deep reinforcement learning algorithms require large amounts of experience to learn an individual task. While meta-reinforcement learning (meta-RL) algorithms can enable agents to learn new skills from small amounts of experience, several major challenges preclude their practicality. Current methods rely heavily on on-policy experience, limiting their sample efficiency. They also lack mechanisms to reason about task uncertainty when adapting to new tasks, limiting their effectiveness on sparse reward problems. In this paper, we address these challenges by developing an off-policy meta-RL algorithm that disentangles task inference and control. In our approach, we perform online probabilistic filtering of latent task variables to infer how to solve a new task from small amounts of experience. This probabilistic interpretation enables posterior sampling for structured and efficient exploration. We demonstrate how to integrate these task variables with off-policy RL algorithms to achieve both meta-training and adaptation efficiency. Our method outperforms prior algorithms in sample efficiency by 20-100X as well as in asymptotic performance on several meta-RL benchmarks.}
}

@article{santoro2017simple,
  title={A simple neural network module for relational reasoning},
  author={Santoro, Adam and Raposo, David and Barrett, David GT and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  journal={arXiv preprint arXiv:1706.01427},
  year={2017}
}

@article{anonymousinpreplanguage,
  title={Language Models},
  author={Anonymous},
  journal={will be released by ICLR camera-ready},
  year={in prep, release by camera-ready}
}

@article{gopnik2004theory,
  title={A theory of causal learning in children: causal maps and Bayes nets.},
  author={Gopnik, Alison and Glymour, Clark and Sobel, David M and Schulz, Laura E and Kushnir, Tamar and Danks, David},
  journal={Psychological review},
  volume={111},
  number={1},
  pages={3},
  year={2004},
  publisher={American Psychological Association}
}

@book{gopnik1999scientist,
  title={The scientist in the crib: Minds, brains, and how children learn.},
  author={Gopnik, Alison and Meltzoff, Andrew N and Kuhl, Patricia K},
  year={1999},
  publisher={William Morrow \& Co}
}

@article{williams2010role,
  title={The role of explanation in discovery and generalization: Evidence from category learning},
  author={Williams, Joseph J and Lombrozo, Tania},
  journal={Cognitive science},
  volume={34},
  number={5},
  pages={776--806},
  year={2010},
  publisher={Wiley Online Library}
}

@article{dasgupta2021memory,
  title={Memory as a computational resource},
  author={Dasgupta, Ishita and Gershman, Samuel J},
  journal={Trends in Cognitive Sciences},
  year={2021},
  publisher={Elsevier}
}

@article{gopnik2000detecting,
  title={Detecting blickets: How young children use information about novel causal powers in categorization and induction},
  author={Gopnik, Alison and Sobel, David M},
  journal={Child development},
  volume={71},
  number={5},
  pages={1205--1222},
  year={2000},
  publisher={Wiley Online Library}
}

@article{lombrozo2017causal,
  title={Causal explanation},
  author={Lombrozo, Tania and Vasilyeva, Nadya},
  journal={Oxford handbook of causal reasoning},
  pages={415--432},
  year={2017},
  publisher={Oxford University Press Oxford, UK}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={International Conference on Machine Learning},
  pages={1407--1416},
  year={2018},
  organization={PMLR}
}

@article{kaplan2017beating,
  title={Beating atari with natural language guided reinforcement learning},
  author={Kaplan, Russell and Sauer, Christopher and Sosa, Alexander},
  journal={arXiv preprint arXiv:1704.05539},
  year={2017}
}

@article{gelman2009learning,
  title={Learning from others: Children's construction of concepts},
  author={Gelman, Susan A},
  journal={Annual review of psychology},
  volume={60},
  pages={115--140},
  year={2009},
  publisher={Annual Reviews}
}

@article{crutch2009different,
  title={The different representational frameworks underpinning abstract and concrete knowledge: Evidence from odd-one-out judgements},
  author={Crutch, Sebastian J and Connell, Sarah and Warrington, Elizabeth K},
  journal={Quarterly Journal of Experimental Psychology},
  volume={62},
  number={7},
  pages={1377--1390},
  year={2009},
  publisher={SAGE Publications Sage UK: London, England}
}

@inproceedings{sinapov2010odd,
  title={The odd one out task: Toward an intelligence test for robots},
  author={Sinapov, Jivko and Stoytchev, Alexander},
  booktitle={2010 IEEE 9th International Conference on Development and Learning},
  pages={126--131},
  year={2010},
  organization={IEEE}
}

@inproceedings{stephens2008one,
  title={One of these greebles is not like the others: Semi-supervised models for similarity structures},
  author={Stephens, Rachel G and Navarro, Danielle J},
  year={2008},
  organization={Proceedings of the 30th Annual Meeting of the Cognitive Science Society}
}

@inproceedings{ichien2021visual,
  title={Visual analogy: Deep learning versus compositional models},
  author={Ichien, Nicholas and Liu, Qing and Fu, Shuhao and Holyoak, Keith J and Yuille, Alan and Lu, Hongjing},
  booktitle={Proceedings of the 30th Annual Meeting of the Cognitive Science Society},
  year={2021}
}

@article{bigbench,
   title = "Beyond the Imitation Game: Measuring and extrapolating the capabilities of language models",
   author = "{BIG-bench collaboration}", 
   year = "2021",
   journal = "In preparation",
   url = "https://github.com/google/BIG-bench/"
}

@article{fyfe2014concreteness,
  title={Concreteness fading in mathematics and science instruction: A systematic review},
  author={Fyfe, Emily R and McNeil, Nicole M and Son, Ji Y and Goldstone, Robert L},
  journal={Educational psychology review},
  volume={26},
  number={1},
  pages={9--25},
  year={2014},
  publisher={Springer}
}   

@article{garcez2020neurosymbolic,
  title={Neurosymbolic AI: the 3rd Wave},
  author={Garcez, Artur d'Avila and Lamb, Luis C},
  journal={arXiv preprint arXiv:2012.05876},
  year={2020}
}

@article{raileanu2020automatic,
  title={Automatic data augmentation for generalization in deep reinforcement learning},
  author={Raileanu, Roberta and Goldstein, Max and Yarats, Denis and Kostrikov, Ilya and Fergus, Rob},
  journal={arXiv preprint arXiv:2006.12862},
  year={2020}
}

@inproceedings{tulli2020learning,
  title={Learning from explanations and demonstrations: A pilot study},
  author={Tulli, Silvia and Wallk{\"o}tter, Sebastian and Paiva, Ana and Melo, Francisco S and Chetouani, Mohamed},
  booktitle={2nd Workshop on Interactive Natural Language Technology for Explainable Artificial Intelligence},
  pages={61--66},
  year={2020}
}

@article{schramowski2020making,
  title={Making deep neural networks right for the right scientific reasons by interacting with their explanations},
  author={Schramowski, Patrick and Stammer, Wolfgang and Teso, Stefano and Brugger, Anna and Herbert, Franziska and Shao, Xiaoting and Luigs, Hans-Georg and Mahlein, Anne-Katrin and Kersting, Kristian},
  journal={Nature Machine Intelligence},
  volume={2},
  number={8},
  pages={476--486},
  year={2020},
  publisher={Nature Publishing Group}
}

@inproceedings{guan2020widening,
  title={Widening the Pipeline in Human-Guided Reinforcement Learning with Explanation and Context-Aware Data Augmentation},
  author={Guan, Lin and Verma, Mudit and Guo, Sihang and Zhang, Ruohan and Kambhampati, Subbarao},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{ross2017right,
  author    = {Andrew Slavin Ross and Michael C. Hughes and Finale Doshi-Velez},
  title     = {Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {2662--2670},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/371},
  url       = {https://doi.org/10.24963/ijcai.2017/371},
}

@article{lertvittayakumjorn2021explanation,
  title={Explanation-Based Human Debugging of NLP Models: A Survey},
  author={Lertvittayakumjorn, Piyawat and Toni, Francesca},
  journal={arXiv preprint arXiv:2104.15135},
  year={2021}
}

@inproceedings{stammer2021right,
  title={Right for the Right Concept: Revising Neuro-Symbolic Concepts by Interacting with their Explanations},
  author={Stammer, Wolfgang and Schramowski, Patrick and Kersting, Kristian},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3619--3629},
  year={2021}
}

@inproceedings{gregor2019shaping,
 author = {Gregor, Karol and Jimenez Rezende, Danilo and Besse, Frederic and Wu, Yan and Merzic, Hamza and van den Oord, Aaron},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Shaping Belief States with Generative Environment Models for RL},
 url = {https://proceedings.neurips.cc/paper/2019/file/2c048d74b3410237704eb7f93a10c9d7-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{watkins2021teachable,
  title={Teachable Reinforcement Learning via Advice Distillation},
  author={Watkins, Olivia and Gupta, Abhishek and Darrell, Trevor and Abbeel, Pieter and Andreas, Jacob},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@book{woodward2005making,
  title={Making things happen: A theory of causal explanation},
  author={Woodward, James},
  year={2005},
  publisher={Oxford university press}
}

@InCollection{sep-metaphysical-explanation,
	author       =	{Brenner, Andrew and Maurin, Anna-Sofia and Skiles, Alexander and Stenwall, Robin and Thompson, Naomi},
	title        =	{{Metaphysical Explanation}},
	booktitle    =	{The {Stanford} Encyclopedia of Philosophy},
	editor       =	{Edward N. Zalta},
	howpublished =	{\url{https://plato.stanford.edu/archives/win2021/entries/metaphysical-explanation/}},
	year         =	{2021},
	edition      =	{{W}inter 2021},
	publisher    =	{Metaphysics Research Lab, Stanford University}
}

@article{kirk2021survey,
  title={A survey of generalisation in deep reinforcement learning},
  author={Kirk, Robert and Zhang, Amy and Grefenstette, Edward and Rockt{\"a}schel, Tim},
  journal={arXiv preprint arXiv:2111.09794},
  year={2021}
}

@article{ghosh2021generalization,
  title={Why generalization in rl is difficult: Epistemic pomdps and implicit partial observability},
  author={Ghosh, Dibya and Rahme, Jad and Kumar, Aviral and Zhang, Amy and Adams, Ryan P and Levine, Sergey},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{shanahan2020explicitly,
  title={An explicitly relational neural network architecture},
  author={Shanahan, Murray and Nikiforou, Kyriacos and Creswell, Antonia and Kaplanis, Christos and Barrett, David and Garnelo, Marta},
  booktitle={International Conference on Machine Learning},
  pages={8593--8603},
  year={2020},
  organization={PMLR}
}

@article{dove2020more,
author = {Guy Dove},
title = {More than a scaffold: Language is a neuroenhancement},
journal = {Cognitive Neuropsychology},
volume = {37},
number = {5-6},
pages = {288-311},
year  = {2020},
publisher = {Routledge},
doi = {10.1080/02643294.2019.1637338},
    note ={PMID: 31269862},
URL = { 
        https://doi.org/10.1080/02643294.2019.1637338
},
eprint = { 
        https://doi.org/10.1080/02643294.2019.1637338
}
}

@article{lupyan2016centrality,
  title={The centrality of language in human cognition},
  author={Lupyan, Gary},
  journal={Language Learning},
  volume={66},
  number={3},
  pages={516--553},
  year={2016},
  publisher={Wiley Online Library}
}

@article{edmiston2015makes,
  title={What makes words special? Words as unmotivated cues},
  author={Edmiston, Pierce and Lupyan, Gary},
  journal={Cognition},
  volume={143},
  pages={93--100},
  year={2015},
  publisher={Elsevier}
}

@article{tam2022semantic,
  title={Semantic exploration from language abstractions and pretrained representations},
  author={Tam, Allison C and Rabinowitz, Neil C and Lampinen, Andrew K and Roy, Nicholas A and Chan, Stephanie CY and Strouse, DJ and Wang, Jane X and Banino, Andrea and Hill, Felix},
  journal={arXiv preprint arXiv:2204.05080},
  year={2022}
}

@article{mu2022improving,
  title={Improving intrinsic exploration with language abstractions},
  author={Mu, Jesse and Zhong, Victor and Raileanu, Roberta and Jiang, Minqi and Goodman, Noah and Rockt{\"a}schel, Tim and Grefenstette, Edward},
  journal={arXiv preprint arXiv:2202.08938},
  year={2022}
}

@software{haiku2020github,
  author = {Tom Hennigan and Trevor Cai and Tamara Norman and Igor Babuschkin},
  title = {{H}aiku: {S}onnet for {JAX}},
  url = {http://github.com/deepmind/dm-haiku},
  version = {0.0.3},
  year = {2020},
}

@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.2.5},
  year = {2018},
}