\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alimonti(1994)]{alimonti1994new}
Paola Alimonti.
\newblock New local search approximation techniques for maximum generalized
  satisfiability problems.
\newblock In \emph{Italian Conference on Algorithms and Complexity}, pages
  40--53. Springer, 1994.

\bibitem[Arora et~al.(2016)Arora, Ge, Kannan, and Moitra]{arora2016computing}
Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra.
\newblock Computing a nonnegative matrix factorization---provably.
\newblock \emph{SIAM Journal on Computing}, 45\penalty0 (4):\penalty0
  1582--1611, 2016.

\bibitem[Bertsekas(2015)]{bertsekas2015convex}
Dimitri Bertsekas.
\newblock \emph{Convex optimization algorithms}.
\newblock Athena Scientific, 2015.

\bibitem[Bian et~al.(2017)Bian, Mirzasoleiman, Buhmann, and
  Krause]{bian2017guaranteed}
Andrew~An Bian, Baharan Mirzasoleiman, Joachim Buhmann, and Andreas Krause.
\newblock Guaranteed non-convex optimization: Submodular maximization over
  continuous domains.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 111--120.
  PMLR, 2017.

\bibitem[Bian et~al.(2020)Bian, Buhmann, and Krause]{bian2020continuous}
Yatao Bian, Joachim~M Buhmann, and Andreas Krause.
\newblock Continuous submodular function maximization.
\newblock \emph{arXiv preprint arXiv:2006.13474}, 2020.

\bibitem[Chekuri et~al.(2014)Chekuri, Vondr{\'a}k, and
  Zenklusen]{chekuri2014submodular}
Chandra Chekuri, Jan Vondr{\'a}k, and Rico Zenklusen.
\newblock Submodular function maximization via the multilinear relaxation and
  contention resolution schemes.
\newblock \emph{SIAM Journal on Computing}, 43\penalty0 (6):\penalty0
  1831--1879, 2014.

\bibitem[Chen et~al.(2018{\natexlab{a}})Chen, Harshaw, Hassani, and
  Karbasi]{chen2018projection}
Lin Chen, Christopher Harshaw, Hamed Hassani, and Amin Karbasi.
\newblock Projection-free online optimization with stochastic gradient: From
  convexity to submodularity.
\newblock In \emph{International Conference on Machine Learning}, pages
  814--823. PMLR, 2018{\natexlab{a}}.

\bibitem[Chen et~al.(2018{\natexlab{b}})Chen, Hassani, and
  Karbasi]{chen2018online}
Lin Chen, Hamed Hassani, and Amin Karbasi.
\newblock Online continuous submodular maximization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1896--1905. PMLR, 2018{\natexlab{b}}.

\bibitem[Chen et~al.(2012)Chen, Lu, and Zhang]{chen2012time}
Wei Chen, Wei Lu, and Ning Zhang.
\newblock Time-critical influence maximization in social networks with
  time-delayed diffusion process.
\newblock In \emph{Twenty-Sixth AAAI Conference on Artificial Intelligence},
  2012.

\bibitem[Das and Kempe(2011)]{das2011submodular}
Abhimanyu Das and David Kempe.
\newblock Submodular meets spectral: greedy algorithms for subset selection,
  sparse approximation and dictionary selection.
\newblock In \emph{International Conference on Machine Learning}, pages
  1057--1064, 2011.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Elenberg et~al.(2018)Elenberg, Khanna, Dimakis, and
  Negahban]{elenberg2018restricted}
Ethan~R Elenberg, Rajiv Khanna, Alexandros~G Dimakis, and Sahand Negahban.
\newblock Restricted strong convexity implies weak submodularity.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (6B):\penalty0
  3539--3568, 2018.

\bibitem[Feldman(2021)]{feldman2021guess}
Moran Feldman.
\newblock Guess free maximization of submodular and linear sums.
\newblock \emph{Algorithmica}, 83\penalty0 (3):\penalty0 853--878, 2021.

\bibitem[Feldman et~al.(2011)Feldman, Naor, and Schwartz]{feldman2011unified}
Moran Feldman, Joseph Naor, and Roy Schwartz.
\newblock A unified continuous greedy algorithm for submodular maximization.
\newblock In \emph{2011 IEEE 52nd Annual Symposium on Foundations of Computer
  Science}, pages 570--579. IEEE, 2011.

\bibitem[Filmus and Ward(2012)]{filmus2012power}
Yuval Filmus and Justin Ward.
\newblock The power of local search: Maximum coverage over a matroid.
\newblock In \emph{29th Symposium on Theoretical Aspects of Computer Science},
  volume~14, pages 601--612. LIPIcs, 2012.

\bibitem[Filmus and Ward(2014)]{filmus2014monotone}
Yuval Filmus and Justin Ward.
\newblock Monotone submodular maximization over a matroid via non-oblivious
  local search.
\newblock \emph{SIAM Journal on Computing}, 43\penalty0 (2):\penalty0 514--542,
  2014.

\bibitem[Fisher et~al.(1978)Fisher, Nemhauser, and Wolsey]{fisher1978analysis}
Marshall~L Fisher, George~L Nemhauser, and Laurence~A Wolsey.
\newblock An analysis of approximations for maximizing submodular set
  functionsâ€”ii.
\newblock In \emph{Polyhedral Combinatorics}, pages 73--87. Springer, 1978.

\bibitem[Fujishige(2005)]{fujishige2005submodular}
Satoru Fujishige.
\newblock \emph{Submodular functions and optimization}.
\newblock Elsevier, 2005.

\bibitem[Ge et~al.(2016)Ge, Lee, and Ma]{ge2016matrix}
Rong Ge, Jason~D Lee, and Tengyu Ma.
\newblock Matrix completion has no spurious local minimum.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2973--2981, 2016.

\bibitem[Harshaw et~al.(2019)Harshaw, Feldman, Ward, and
  Karbasi]{harshaw2019submodular}
Chris Harshaw, Moran Feldman, Justin Ward, and Amin Karbasi.
\newblock Submodular maximization beyond non-negativity: Guarantees, fast
  algorithms, and applications.
\newblock In \emph{International Conference on Machine Learning}, pages
  2634--2643. PMLR, 2019.

\bibitem[Hassani et~al.(2017)Hassani, Soltanolkotabi, and
  Karbasi]{hassani2017gradient}
Hamed Hassani, Mahdi Soltanolkotabi, and Amin Karbasi.
\newblock Gradient methods for submodular maximization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5841--5851, 2017.

\bibitem[Hassani et~al.(2020)Hassani, Karbasi, Mokhtari, and
  Shen]{hassani2020stochastic}
Hamed Hassani, Amin Karbasi, Aryan Mokhtari, and Zebang Shen.
\newblock Stochastic conditional gradient++:(non) convex minimization and
  continuous submodular maximization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (4):\penalty0
  3315--3344, 2020.

\bibitem[Hazan et~al.(2016{\natexlab{a}})Hazan, Levy, and
  Shalev-Shwartz]{hazan2016graduated}
Elad Hazan, Kfir~Yehuda Levy, and Shai Shalev-Shwartz.
\newblock On graduated optimization for stochastic non-convex problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  1833--1841. PMLR, 2016{\natexlab{a}}.

\bibitem[Hazan et~al.(2016{\natexlab{b}})]{hazan2019introduction}
Elad Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  2\penalty0 (3-4):\penalty0 157--325, 2016{\natexlab{b}}.

\bibitem[Kempe et~al.(2003)Kempe, Kleinberg, and Tardos]{kempe2003maximizing}
David Kempe, Jon Kleinberg, and {\'E}va Tardos.
\newblock Maximizing the spread of influence through a social network.
\newblock In \emph{Proceedings of the ninth ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 137--146, 2003.

\bibitem[Khanna et~al.(1998)Khanna, Motwani, Sudan, and
  Vazirani]{khanna1998syntactic}
Sanjeev Khanna, Rajeev Motwani, Madhu Sudan, and Umesh Vazirani.
\newblock On syntactic versus computational views of approximability.
\newblock \emph{SIAM Journal on Computing}, 28\penalty0 (1):\penalty0 164--191,
  1998.

\bibitem[Leskovec et~al.(2007)Leskovec, Krause, Guestrin, Faloutsos,
  VanBriesen, and Glance]{leskovec2007cost}
Jure Leskovec, Andreas Krause, Carlos Guestrin, Christos Faloutsos, Jeanne
  VanBriesen, and Natalie Glance.
\newblock Cost-effective outbreak detection in networks.
\newblock In \emph{Proceedings of the 13th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 420--429, 2007.

\bibitem[Lin and Bilmes(2011)]{lin2011class}
Hui Lin and Jeff Bilmes.
\newblock A class of submodular functions for document summarization.
\newblock In \emph{Proceedings of the 49th Annual Meeting of the Association
  for Computational Linguistics: Human Language Technologies}, pages 510--520,
  2011.

\bibitem[Liu et~al.(2020)Liu, Deng, Li, Chen, and So]{liu2020nonconvex}
Huikang Liu, Zengde Deng, Xiao Li, Shixiang Chen, and Anthony Man-Cho So.
\newblock Nonconvex robust synchronization of rotations.
\newblock In \emph{NeurIPS Annual Workshop on Optimization for Machine
  Learning}, pages 1--7, 2020.

\bibitem[Lov{\'a}sz(1983)]{lovasz1983submodular}
L{\'a}szl{\'o} Lov{\'a}sz.
\newblock Submodular functions and convexity.
\newblock In \emph{Mathematical programming the state of the art}, pages
  235--257. Springer, 1983.

\bibitem[Mehta et~al.(2007)Mehta, Saberi, Vazirani, and
  Vazirani]{mehta2007adwords}
Aranyak Mehta, Amin Saberi, Umesh Vazirani, and Vijay Vazirani.
\newblock Adwords and generalized online matching.
\newblock \emph{Journal of the ACM}, 54\penalty0 (5):\penalty0 22--es, 2007.

\bibitem[Mitra et~al.(2021)Mitra, Feldman, and Karbasi]{mitra2021submodular+}
Siddharth Mitra, Moran Feldman, and Amin Karbasi.
\newblock Submodular+ concave.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Mokhtari et~al.(2018)Mokhtari, Hassani, and
  Karbasi]{mokhtari2018conditional}
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi.
\newblock Conditional gradient method for stochastic submodular maximization:
  Closing the gap.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1886--1895. PMLR, 2018.

\bibitem[Murty and Kabadi(1987)]{murty1987some}
Katta~G Murty and Santosh~N Kabadi.
\newblock Some np-complete problems in quadratic and nonlinear programming.
\newblock \emph{Mathematical Programming}, 39\penalty0 (2):\penalty0 117--129,
  1987.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and
  Fisher]{nemhauser1978analysis}
George~L Nemhauser, Laurence~A Wolsey, and Marshall~L Fisher.
\newblock An analysis of approximations for maximizing submodular set
  functionsâ€”i.
\newblock \emph{Mathematical Programming}, 14\penalty0 (1):\penalty0 265--294,
  1978.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Y~Nesterov.
\newblock \emph{Introductory Lectures on Convex Optimization: A Basic Course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Netrapalli et~al.(2014)Netrapalli, U~N, Sanghavi, Anandkumar, and
  Jain]{NIPS2014_443cb001}
Praneeth Netrapalli, Niranjan U~N, Sujay Sanghavi, Animashree Anandkumar, and
  Prateek Jain.
\newblock Non-convex robust pca.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1107--1115, 2014.

\bibitem[Quanrud and Khashabi(2015)]{quanrud2015online}
Kent Quanrud and Daniel Khashabi.
\newblock Online learning with adversarial delays.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1270--1278, 2015.

\bibitem[Streeter and Golovin(2008)]{streeter2008online}
Matthew Streeter and Daniel Golovin.
\newblock An online algorithm for maximizing submodular functions.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1577--1584, 2008.

\bibitem[Yang et~al.(2016)Yang, Mao, Pei, and He]{yang2016continuous}
Yu~Yang, Xiangbo Mao, Jian Pei, and Xiaofei He.
\newblock Continuous influence maximization: What discounts should we offer to
  social network users?
\newblock In \emph{Proceedings of the 2016 International Conference on
  Management of Data}, pages 727--741, 2016.

\bibitem[Zhang et~al.(2019)Zhang, Chen, Hassani, and Karbasi]{zhang2019online}
Mingrui Zhang, Lin Chen, Hamed Hassani, and Amin Karbasi.
\newblock Online continuous submodular maximization: From full-information to
  bandit feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9206--9217, 2019.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Martin Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{International Conference on Machine Learning}, pages
  928--936, 2003.

\end{thebibliography}
