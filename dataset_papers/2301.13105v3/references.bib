%%%%% group invariances
@inproceedings{ravanbakhsh2017equivariance,
  title={Equivariance through parameter-sharing},
  author={Ravanbakhsh, Siamak and Schneider, Jeff and Poczos, Barnabas},
  booktitle={International conference on machine learning},
  pages={2892--2901},
  year={2017},
  organization={PMLR}
}

@article{zaheer2017deep,
  title={Deep sets},
  author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{hartford2018deep,
  title={Deep models of interactions across sets},
  author={Hartford, Jason and Graham, Devon and Leyton-Brown, Kevin and Ravanbakhsh, Siamak},
  booktitle={International Conference on Machine Learning},
  pages={1909--1918},
  year={2018},
  organization={PMLR}
}


@inproceedings{
zhou2020meta,
title={Meta-learning Symmetries by Reparameterization},
author={Allan Zhou and Tom Knowles and Chelsea Finn},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=-QxT4mJdijq}
}


%%%%% Random features model 
@article{mei2022generalization,
  title={The generalization error of random features regression: Precise asymptotics and the double descent curve},
  author={Mei, Song and Montanari, Andrea},
  journal={Communications on Pure and Applied Mathematics},
  volume={75},
  number={4},
  pages={667--766},
  year={2022},
  publisher={Wiley Online Library}
}


@article{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


%%%%%%%%%



@inproceedings{
yun2020unifying,
title={A unifying view on implicit bias in training linear neural networks},
author={Chulhee Yun and Shankar Krishnan and Hossein Mobahi},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=ZsZM-4iMQkH}
}


@article{jacot2021saddle,
  title={Saddle-to-saddle dynamics in deep linear networks: Small initialization training, symmetry, and sparsity},
  author={Jacot, Arthur and Ged, Fran{\c{c}}ois and {\c{S}}im{\c{s}}ek, Berfin and Hongler, Cl{\'e}ment and Gabriel, Franck},
  journal={arXiv preprint arXiv:2106.15933},
  year={2021}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{velivckovic2022clrs,
  title={The CLRS algorithmic reasoning benchmark},
  author={Veli{\v{c}}kovi{\'c}, Petar and Badia, Adri{\`a} Puigdom{\`e}nech and Budden, David and Pascanu, Razvan and Banino, Andrea and Dashevskiy, Misha and Hadsell, Raia and Blundell, Charles},
  booktitle={International Conference on Machine Learning},
  pages={22084--22102},
  year={2022},
  organization={PMLR}
}

@article{abbe2022learning,
  title={Learning to reason with neural networks: Generalization, unseen data and boolean measures},
  author={Abbe, Emmanuel and Bengio, Samy and Cornacchia, Elisabetta and Kleinberg, Jon and Lotfi, Aryo and Raghu, Maithra and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2709--2722},
  year={2022}
}


@article{bakhtin2019phyre,
  title={Phyre: A new benchmark for physical reasoning},
  author={Bakhtin, Anton and van der Maaten, Laurens and Johnson, Justin and Gustafson, Laura and Girshick, Ross},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{johnson2017clevr,
  title={Clevr: A diagnostic dataset for compositional language and elementary visual reasoning},
  author={Johnson, Justin and Hariharan, Bharath and Van Der Maaten, Laurens and Fei-Fei, Li and Lawrence Zitnick, C and Girshick, Ross},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2901--2910},
  year={2017}
}

@article{lewkowycz2022solving-minerva,
  title={Solving quantitative reasoning problems with language models},
  author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3843--3857},
  year={2022}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{rahimi2007random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  journal={Advances in neural information processing systems},
  volume={20},
  year={2007}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{yang2020feature,
  title={Feature learning in infinite-width neural networks},
  author={Yang, Greg and Hu, Edward J},
  journal={arXiv preprint arXiv:2011.14522},
  year={2020}
}


@article{anil2022exploring-length,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}

@article{zhang2022unveiling,
  title={Unveiling Transformers with LEGO: a synthetic reasoning task},
  author={Zhang, Yi and Backurs, Arturs and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
  journal={arXiv preprint arXiv:2206.04301},
  year={2022}
}

@article{10.2307/2048015,
 ISSN = {00029939, 10886826},
 URL = {http://www.jstor.org/stable/2048015},
 abstract = {We find upper and lower bounds for $\operatorname{Pr}(\sum \pm x_n \geq t)$, where x1, x2,... are real numbers. We express the answer in terms of the K-interpolation norm from the theory of interpolation of Banach spaces.},
 author = {S. J. Montgomery-Smith},
 journal = {Proceedings of the American Mathematical Society},
 number = {2},
 pages = {517--522},
 publisher = {American Mathematical Society},
 title = {The Distribution of Rademacher Sums},
 volume = {109},
 year = {1990}
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}


@inproceedings{
dosovitskiy2020image,
title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=YicbFdNTTy}
}

@book{o'donnell_2014, place={Cambridge}, title={Analysis of Boolean Functions}, DOI={10.1017/CBO9781139814782}, publisher={Cambridge University Press}, author={O'Donnell, Ryan}, year={2014}}

@inproceedings{AS20,
 author = {Abbe, Emmanuel and Sandon, Colin},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {20061--20072},
 publisher = {Curran Associates, Inc.},
 title = {On the universality of deep learning},
 url = {https://proceedings.neurips.cc/paper/2020/file/e7e8f8e5982b3298c8addedf6811d500-Paper.pdf},
 volume = {33},
 year = {2020}
}



@inproceedings{
saxton2019analysing,
title={Analysing Mathematical Reasoning Abilities of Neural Models},
author={David Saxton and Edward Grefenstette and Felix Hill and Pushmeet Kohli},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=H1gR5iR5FX},
}

@article{
  mahdavi2022better,
  title={Towards Better Out-of-Distribution Generalization of Neural Algorithmic Reasoning Tasks},
  author={Sadegh Mahdavi and Kevin Swersky and Thomas Kipf and Milad Hashemi and Christos Thrampoulidis and Renjie Liao},
  journal={Transactions on Machine Learning Research},
  issn={2835-8856},
  year={2023},
  url={https://openreview.net/forum?id=xkrtvHlp3P},
  note={}
}


@inproceedings{curriculum, author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason}, title = {Curriculum Learning}, year = {2009}, isbn = {9781605585161}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/1553374.1553380}, doi = {10.1145/1553374.1553380}, abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).}, booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning}, pages = {41–48}, numpages = {8}, location = {Montreal, Quebec, Canada}, series = {ICML '09} }


@article{Winkelbauer2012MomentsAA,
  title={Moments and Absolute Moments of the Normal Distribution},
  author={Andreas Winkelbauer},
  journal={ArXiv},
  year={2012},
  volume={abs/1209.4340}
}

@misc{abbe2021staircase,
      title={The staircase property: How hierarchical structure can guide deep learning, {N}eur{I}{P}{S}}, 
      author={Emmanuel Abbe and Enric Boix-Adsera and Matthew Brennan and Guy Bresler and Dheeraj Nagaraj},
      year={2021},
      eprint={2108.10573},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Zhang2021PointerVR,
  title={Pointer Value Retrieval: A new benchmark for understanding the limits of neural network generalization},
  author={Chiyuan Zhang and Maithra Raghu and Jon M. Kleinberg and Samy Bengio},
  journal={ArXiv},
  year={2021},
  volume={abs/2107.12580}
}

@inproceedings{malach_parity,
 author = {Daniely, Amit and Malach, Eran},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
 pages = {20356--20365},
 publisher = {Curran Associates, Inc.},
 title = {Learning Parities with Neural Networks},
 url = {https://proceedings.neurips.cc/paper/2020/file/eaae5e04a259d09af85c108fe4d7dd0c-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{AbbeINAL,
  title={An initial alignment between neural network and target is needed for gradient descent to learn},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and Hazla, Jan and Marquis, Christopher},
  booktitle={International Conference on Machine Learning},
  pages={33--52},
  year={2022},
  organization={PMLR}
}





@inproceedings{rahaman2019spectral,
  title={On the spectral bias of neural networks},
  author={Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxler, Felix and Lin, Min and Hamprecht, Fred and Bengio, Yoshua and Courville, Aaron},
  booktitle={International Conference on Machine Learning},
  pages={5301--5310},
  year={2019},
  organization={PMLR}
}

@misc{xu2019frequency,
  title={Frequency principle: Fourier analysis sheds light on deep neural networks},
  author={Xu, Zhi-Qin John and Zhang, Yaoyu and Luo, Tao and Xiao, Yanyang and Ma, Zheng},
  journal={arXiv preprint arXiv:1901.06523},
  year={2019}
}


@article{tolstikhin2021mlpmixer,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{vaswani2017attention-transformer,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{abbe2021power,
  title={On the Power of Differentiable Learning versus {PAC} and {SQ} Learning},
  author={Abbe, Emmanuel and Kamath, Pritish and Malach, Eran and Sandon, Colin and Srebro, Nathan},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@misc{mergedstaircase,
  url = {},
  
  author = {Abbe, Emmanuel and Boix-Adsera, Enric and Misiakiewicz, Theodor},
  
  keywords = {Machine Learning (cs.LG), Data Structures and Algorithms (cs.DS), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The merged-staircase property: a necessary and nearly sufficient condition for {SGD} learning of sparse functions on two-layer neural networks, {COLT}},
  
  publisher = {COLT},
  
  year = {2022},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{torch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}



@article{10.2307/2048015,
 ISSN = {00029939, 10886826},
 URL = {http://www.jstor.org/stable/2048015},
 abstract = {We find upper and lower bounds for $\operatorname{Pr}(\sum \pm x_n \geq t)$, where x1, x2,... are real numbers. We express the answer in terms of the K-interpolation norm from the theory of interpolation of Banach spaces.},
 author = {S. J. Montgomery-Smith},
 journal = {Proceedings of the American Mathematical Society},
 number = {2},
 pages = {517--522},
 publisher = {American Mathematical Society},
 title = {The Distribution of Rademacher Sums},
 volume = {109},
 year = {1990}
}



@article{Winkelbauer2012MomentsAA,
  title={Moments and Absolute Moments of the Normal Distribution},
  author={Andreas Winkelbauer},
  journal={ArXiv},
  year={2012},
  volume={abs/1209.4340}
}


@inproceedings{malach2019deeper,
 author = {Malach, Eran and Shalev-Shwartz, Shai},
 booktitle = {Advances in Neural Information Processing Systems},
 pages = {6429--6438},
 title = {Is Deeper Better only when Shallow is Good?},
 volume = {32},
 year = {2019}
}

@unpublished{allen2020backward,
  title={Backward feature correction: {H}ow deep learning performs deep learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  note={arXiv:2001.04413},
  year={2020}
}

@inproceedings{blum1994weakly,
  title={Weakly learning {DNF} and characterizing statistical query learning using {F}ourier analysis},
  author={Blum, Avrim and Furst, Merrick and Jackson, Jeffrey and Kearns, Michael and Mansour, Yishay and Rudich, Steven},
  booktitle={Symposium on Theory of Computing (STOC)},
  pages={253--262},
  year={1994}
}

@article{kearns1998efficient,
  title={Efficient noise-tolerant learning from statistical queries},
  author={Kearns, Michael},
  journal={Journal of the ACM},
  volume={45},
  number={6},
  pages={983--1006},
  year={1998}
}


@inproceedings{failing,
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  biburl = {https://www.bibsonomy.org/bibtex/28095313389e6b84c74d3a018e670e93e/dblp},
  booktitle = {NeurIPS},
  ee = {http://papers.nips.cc/paper/8169-an-intriguing-failing-of-convolutional-neural-networks-and-the-coordconv-solution},
  interhash = {687147375fc30b3e4b9e8f67055d86a9},
  intrahash = {8095313389e6b84c74d3a018e670e93e},
  keywords = {dblp},
  pages = {9628-9639},
  timestamp = {2020-03-07T11:50:35.000+0100},
  title = {An intriguing failing of convolutional neural networks and the {C}oord{C}onv solution},
  url = {http://dblp.uni-trier.de/db/conf/nips/nips2018.html#LiuLMSFSY18},
  year = 2018
}



@InProceedings{quantifying,
  title = 	 {Quantifying the Benefit of Using Differentiable Learning over Tangent Kernels},
  author =       {Malach, Eran and Kamath, Pritish and Abbe, Emmanuel and Srebro, Nathan},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {7379--7389},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/malach21a/malach21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/malach21a.html},
  abstract = 	 {We study the relative power of learning with gradient descent on differentiable models, such as neural networks, versus using the corresponding tangent kernels. We show that under certain conditions, gradient descent achieves small error only if a related tangent kernel method achieves a non-trivial advantage over random guessing (a.k.a. weak learning), though this advantage might be very small even when gradient descent can achieve arbitrarily high accuracy. Complementing this, we show that without these conditions, gradient descent can in fact learn with small error even when no kernel method, in particular using the tangent kernel, can achieve a non-trivial advantage over random guessing.}
}


@inproceedings{shalev2021computational,
  title={Computational Separation Between Convolutional and Fully-Connected Networks},
  author={Shalev-Shwartz, Shai and Malach, Eran},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021}
}

@unpublished{abbe2020poly,
  title={Poly-time universality and limitations of deep learning},
  author={Abbe, Emmanuel and Sandon, Colin},
  note={arXiv:2001.02992},
  year={2020}
}


@unpublished{EnricDraft,
    author={Enric Boix-Adsera},
    note={Personal communication},
    year={2021}
}

@inproceedings{Das20,
author = {Das, Abhimanyu and Gollapudi, Sreenivas and Kumar, Ravi and Panigrahy, Rina},
title = {On the Learnability of Random Deep Networks},
year = {2020},
publisher = {Society for Industrial and Applied Mathematics},
address = {USA},
abstract = {In this paper we study the learnability of random deep networks both theoretically and experimentally. On the theoretical front, assuming the statistical query model, we show that the learnability of random deep networks with sign activation drops exponentially with their depths; under plausible conjectures, our results extend to ReLu and sigmoid activations. The core of the arguments is that even for highly correlated inputs, the outputs of deep random networks are near-orthogonal. On the experimental side, we find that the learnability of random networks drops sharply with depth even with the state-of-the-art training methods.},
booktitle = {Proceedings of the Thirty-First Annual ACM-SIAM Symposium on Discrete Algorithms},
pages = {398–410},
numpages = {13},
location = {Salt Lake City, Utah},
series = {SODA '20}
}


@misc{Das2019learnability,
  doi = {10.48550/ARXIV.1904.03866},
  
  url = {https://arxiv.org/abs/1904.03866},
  
  author = {Das, Abhimanyu and Gollapudi, Sreenivas and Kumar, Ravi and Panigrahy, Rina},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Learnability of Deep Random Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{Xu2018frequency,
  doi = {10.48550/ARXIV.1807.01251},
  
  url = {https://arxiv.org/abs/1807.01251},
  
  author = {Xu, Zhi-Qin John and Zhang, Yaoyu and Xiao, Yanyang},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Information Theory (cs.IT), Statistics Theory (math.ST), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics, I.2.6, 62-07},
  
  title = {Training behavior of deep neural network in frequency domain},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{arora2018stronger,
  doi = {10.48550/ARXIV.1802.05296},
  
  url = {https://arxiv.org/abs/1802.05296},
  
  author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Stronger generalization bounds for deep nets via a compression approach},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}



%%Distr. shift references
@article{BenDavid2018theory,
title	= {A theory of learning from different domains},
author	= {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jennifer Vaughan},
year	= {2010},
URL	= {http://www.springerlink.com/content/q6qk230685577n52/},
journal	= {Machine Learning},
pages	= {151--175},
volume	= {79}
}


@inproceedings{
wiles2022a,
title={A Fine-Grained Analysis on Distribution Shift},
author={Olivia Wiles and Sven Gowal and Florian Stimberg and Sylvestre-Alvise Rebuffi and Ira Ktena and Krishnamurthy Dj Dvijotham and Ali Taylan Cemgil},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=Dl4LetuLdyK}
}

@misc{tifrea2020semisupervised,
  doi = {10.48550/ARXIV.2012.05825},
  
  url = {https://arxiv.org/abs/2012.05825},
  
  author = {Ţifrea, Alexandru and Stavarache, Eric and Yang, Fanny},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Semi-supervised novelty detection using ensembles with regularized disagreement},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{sagawa2021extendingwilds,
  doi = {10.48550/ARXIV.2112.05090},
  
  url = {https://arxiv.org/abs/2112.05090},
  
  author = {Sagawa, Shiori and Koh, Pang Wei and Lee, Tony and Gao, Irena and Xie, Sang Michael and Shen, Kendrick and Kumar, Ananya and Hu, Weihua and Yasunaga, Michihiro and Marklund, Henrik and Beery, Sara and David, Etienne and Stavness, Ian and Guo, Wei and Leskovec, Jure and Saenko, Kate and Hashimoto, Tatsunori and Levine, Sergey and Finn, Chelsea and Liang, Percy},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Extending the WILDS Benchmark for Unsupervised Adaptation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@book{datasetshift2009,
author = {Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
title = {Dataset Shift in Machine Learning},
year = {2009},
isbn = {0262170051},
publisher = {The MIT Press}
}


@misc{andreassen2021evolution,
  doi = {10.48550/ARXIV.2106.15831},
  
  url = {https://arxiv.org/abs/2106.15831},
  
  author = {Andreassen, Anders and Bahri, Yasaman and Neyshabur, Behnam and Roelofs, Rebecca},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}



@InProceedings{lei2021nearoptimalregression,
  title = 	 {Near-Optimal Linear Regression under Distribution Shift},
  author =       {Lei, Qi and Hu, Wei and Lee, Jason},
  booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
  pages = 	 {6164--6174},
  year = 	 {2021},
  editor = 	 {Meila, Marina and Zhang, Tong},
  volume = 	 {139},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {18--24 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v139/lei21a/lei21a.pdf},
  url = 	 {https://proceedings.mlr.press/v139/lei21a.html},
  abstract = 	 {Transfer learning is essential when sufficient data comes from the source domain, with scarce labeled data from the target domain. We develop estimators that achieve minimax linear risk for linear regression problems under distribution shift. Our algorithms cover different transfer learning settings including covariate shift and model shift. We also consider when data are generated from either linear or general nonlinear models. We show that linear minimax estimators are within an absolute constant of the minimax risk even among nonlinear estimators for various source/target distributions.}
}

@misc{shen2017wasserstein,
  doi = {10.48550/ARXIV.1707.01217},
  
  url = {https://arxiv.org/abs/1707.01217},
  
  author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Wasserstein Distance Guided Representation Learning for Domain Adaptation},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{zhang2016understanding,
  doi = {10.48550/ARXIV.1611.03530},
  
  url = {https://arxiv.org/abs/1611.03530},
  
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Understanding deep learning requires rethinking generalization},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{belkin2021fit,
  doi = {10.48550/ARXIV.2105.14368},
  
  url = {https://arxiv.org/abs/2105.14368},
  
  author = {Belkin, Mikhail},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}


@misc{neyshabur2017pac,
  doi = {10.48550/ARXIV.1707.09564},
  
  url = {https://arxiv.org/abs/1707.09564},
  
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Srebro, Nathan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A PAC-Bayesian Approach to Spectrally-Normalized Margin Bounds for Neural Networks},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{lecun2010mnist,
  title={MNIST handwritten digit database},
  author={LeCun, Yann and Cortes, Corinna and Burges, CJ},
  journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},
  volume={2},
  year={2010}
}


%% Implicit Bias
@article{shah2020pitfalls,
  title={The pitfalls of simplicity bias in neural networks},
  author={Shah, Harshay and Tamuly, Kaustav and Raghunathan, Aditi and Jain, Prateek and Netrapalli, Praneeth},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9573--9585},
  year={2020}
}

@article{soudry2017implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={Journal of Machine Learning Research},
  volume={19},
  number={70},
  pages={1--57},
  year={2018}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@misc{vardi2021margin,
  doi = {10.48550/ARXIV.2110.02732},
  
  url = {https://arxiv.org/abs/2110.02732},
  
  author = {Vardi, Gal and Shamir, Ohad and Srebro, Nathan},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On Margin Maximization in Linear and ReLU Networks},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{neyshabur2014search,
  doi = {10.48550/ARXIV.1412.6614},
  
  url = {https://arxiv.org/abs/1412.6614},
  
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {In Search of the Real Inductive Bias: On the Role of Implicit Regularization in Deep Learning},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{neyshabur2017exploring,
  doi = {10.48550/ARXIV.1706.08947},
  
  url = {https://arxiv.org/abs/1706.08947},
  
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and McAllester, David and Srebro, Nathan},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Exploring Generalization in Deep Learning},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{moroshko2020implicit,
  title={Implicit bias in deep linear classification: Initialization scale vs training accuracy},
  author={Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22182--22193},
  year={2020}
}


@misc{ji2019characterizing,
  doi = {10.48550/ARXIV.1906.04540},
  
  url = {https://arxiv.org/abs/1906.04540},
  
  author = {Ji, Ziwei and Telgarsky, Matus},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Characterizing the implicit bias via a primal-dual analysis},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{pesme2021implicit,
  doi = {10.48550/ARXIV.2106.09524},
  
  url = {https://arxiv.org/abs/2106.09524},
  
  author = {Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Implicit Bias of SGD for Diagonal Linear Networks: a Provable Benefit of Stochasticity},
  
  publisher = {arXiv},
  
  year = {2021},
  
  copyright = {Creative Commons Attribution 4.0 International}
}

@misc{bartlett2019benign,
  doi = {10.48550/ARXIV.1906.11300},
  
  url = {https://arxiv.org/abs/1906.11300},
  
  author = {Bartlett, Peter L. and Long, Philip M. and Lugosi, Gábor and Tsigler, Alexander},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Benign Overfitting in Linear Regression},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@misc{lyu2019gradient,
  doi = {10.48550/ARXIV.1906.05890},
  
  url = {https://arxiv.org/abs/1906.05890},
  
  author = {Lyu, Kaifeng and Li, Jian},
  
  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{gunasekar2017implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}


@article{bartlett2021deep,
  title={Deep learning: a statistical viewpoint},
  author={Bartlett, Peter L and Montanari, Andrea and Rakhlin, Alexander},
  journal={Acta numerica},
  volume={30},
  pages={87--201},
  year={2021},
  publisher={Cambridge University Press}
}


@article{razin2020implicit,
  title={Implicit regularization in deep learning may not be explainable by norms},
  author={Razin, Noam and Cohen, Nadav},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={21174--21187},
  year={2020}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}


@inproceedings{kingma2014adam,
  author       = {Diederik P. Kingma and
                  Jimmy Ba},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Adam: A Method for Stochastic Optimization},
  booktitle    = {3rd International Conference on Learning Representations, {ICLR} 2015,
                  San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year         = {2015},
}

@INPROCEEDINGS{heidari2019boolean,  author={Heidari, Mohsen and Pradhan, S. Sandeep and Venkataramanan, Ramji},  booktitle={2019 IEEE International Symposium on Information Theory (ISIT)},   title={Boolean Functions with Biased Inputs: Approximation and Noise Sensitivity},   year={2019},  volume={},  number={},  pages={1192-1196},  doi={10.1109/ISIT.2019.8849233}}

@article{jha2019explaining,
author = {Jha, Susmit and Sahai, Tuhin and Raman, Vasumathi and Pinto, Alessandro and Francis, Michael},
title = {Explaining AI Decisions Using Efficient Methods for Learning Sparse Boolean Formulae},
year = {2019},
issue_date = {Dec 2019},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {63},
number = {4},
issn = {0168-7433},
url = {https://doi.org/10.1007/s10817-018-9499-8},
doi = {10.1007/s10817-018-9499-8},
abstract = {In this paper, we consider the problem of learning Boolean formulae from examples obtained by actively querying an oracle that can label these examples as either positive or negative. This problem has received attention in both machine learning as well as formal methods communities, and it has been shown to have exponential worst-case complexity in the general case as well as for many restrictions. In this paper, we focus on learning sparse Boolean formulae which depend on only a small (but unknown) subset of the overall vocabulary of atomic propositions. We propose two algorithms—first, based on binary search in the Hamming space, and the second, based on random walk on the Boolean hypercube, to learn these sparse Boolean formulae with a given confidence. This assumption of sparsity is motivated by the problem of mining explanations for decisions made by artificially intelligent (AI) algorithms, where the explanation of individual decisions may depend on a small but unknown subset of all the inputs to the algorithm. We demonstrate the use of these algorithms in automatically generating explanations of these decisions. These explanations will make intelligent systems more understandable and accountable to human users, facilitate easier audits and provide diagnostic information in the case of failure. The proposed approach treats the AI algorithm as a black-box oracle; hence, it is broadly applicable and agnostic to the specific AI algorithm. We show that the number of examples needed for both proposed algorithms only grows logarithmically with the size of the vocabulary of atomic propositions. We illustrate the practical effectiveness of our approach on a diverse set of case studies.},
journal = {J. Autom. Reason.},
month = {dec},
pages = {1055–1075},
numpages = {21},
keywords = {Formal methods, Explainable AI, Interpretable AI, Boolean formula learning, Sparse learning, Machine learning}
}

@misc{udovenko2021milp,
      author = {Aleksei Udovenko},
      title = {MILP modeling of Boolean functions by minimum number of inequalities},
      howpublished = {Cryptology ePrint Archive, Paper 2021/1099},
      year = {2021},
      note = {\url{https://eprint.iacr.org/2021/1099}},
      url = {https://eprint.iacr.org/2021/1099}
}


@article{epistaticnet,
	Abstract = {Despite recent advances in high-throughput combinatorial mutagenesis assays, the number of labeled sequences available to predict molecular functions has remained small for the vastness of the sequence space combined with the ruggedness of many fitness functions. While deep neural networks (DNNs) can capture high-order epistatic interactions among the mutational sites, they tend to overfit to the small number of labeled sequences available for training. Here, we developed Epistatic Net (EN), a method for spectral regularization of DNNs that exploits evidence that epistatic interactions in many fitness functions are sparse. We built a scalable extension of EN, usable for larger sequences, which enables spectral regularization using fast sparse recovery algorithms informed by coding theory. Results on several biological landscapes show that EN consistently improves the prediction accuracy of DNNs and enables them to outperform competing models which assume other priors. EN estimates the higher-order epistatic interactions of DNNs trained on massive sequence spaces-a computational problem that otherwise takes years to solve.},
	Author = {Aghazadeh, Amirali and Nisonoff, Hunter and Ocal, Orhan and Brookes, David H. and Huang, Yijie and Koyluoglu, O. Ozan and Listgarten, Jennifer and Ramchandran, Kannan},
	Da = {2021/09/01},
	Date-Added = {2022-08-02 18:35:33 +0200},
	Date-Modified = {2022-08-02 18:35:33 +0200},
	Doi = {10.1038/s41467-021-25371-3},
	Id = {Aghazadeh2021},
	Isbn = {2041-1723},
	Journal = {Nature Communications},
	Number = {1},
	Pages = {5225},
	Title = {Epistatic Net allows the sparse spectral regularization of deep neural networks for inferring fitness functions},
	Ty = {JOUR},
	Url = {https://doi.org/10.1038/s41467-021-25371-3},
	Volume = {12},
	Year = {2021},
	Bdsk-Url-1 = {https://doi.org/10.1038/s41467-021-25371-3}}



%%algebra and algebraic geometry.
@book{cox2013ideals,
  title={Ideals, varieties, and algorithms: an introduction to computational algebraic geometry and commutative algebra},
  author={Cox, David and Little, John and OShea, Donal},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@book{dummit2004abstract,
  title={Abstract algebra},
  author={Dummit, David Steven and Foote, Richard M},
  volume={3},
  year={2004},
  publisher={Wiley Hoboken}
}

@inproceedings{moller1982construction,
  title={The construction of multivariate polynomials with preassigned zeros},
  author={M{\"o}ller, H Michael and Buchberger, Bruno},
  booktitle={European Computer Algebra Conference},
  pages={24--31},
  year={1982},
  organization={Springer}
}


@article{alabdulmohsin2022revisiting,
  title={Revisiting neural scaling laws in language and vision},
  author={Alabdulmohsin, Ibrahim M and Neyshabur, Behnam and Zhai, Xiaohua},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={22300--22312},
  year={2022}
}



%% Related work
@article{ben2006analysis,
  title={Analysis of representations for domain adaptation},
  author={Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
  journal={Advances in neural information processing systems},
  volume={19},
  year={2006}
}


@inproceedings{mansour2009domain,title	= {Domain Adaptation: Learning Bounds and Algorithms},author	= {Yishay Mansour and Mehryar Mohri and Afshin Rostamizadeh},year	= {2009},URL	= {http://www.cs.nyu.edu/~mohri/postscript/nadap.pdf},booktitle	= {Proceedings of The 22nd Annual Conference on Learning Theory (COLT 2009)},address	= {Montr\'eal, Canada}}

@article{redko2020survey,
  title={A survey on domain adaptation theory: learning bounds and theoretical guarantees},
  author={Redko, Ievgen and Morvant, Emilie and Habrard, Amaury and Sebban, Marc and Bennani, Youn{\`e}s},
  journal={arXiv preprint arXiv:2004.11829},
  year={2020}
}

@inproceedings{
gulrajani2020search,
title={In Search of Lost Domain Generalization},
author={Ishaan Gulrajani and David Lopez-Paz},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=lQdXeXDoWtI}
}

@inproceedings{miller2021accuracy,
  title={Accuracy on the line: on the strong correlation between out-of-distribution and in-distribution generalization},
  author={Miller, John P and Taori, Rohan and Raghunathan, Aditi and Sagawa, Shiori and Koh, Pang Wei and Shankar, Vaishaal and Liang, Percy and Carmon, Yair and Schmidt, Ludwig},
  booktitle={International Conference on Machine Learning},
  pages={7721--7735},
  year={2021},
  organization={PMLR}
}

% memorization
@inproceedings{carlini2019secret-mem1,
  title={The Secret Sharer: Evaluating and Testing Unintended Memorization in Neural Networks.},
  author={Carlini, Nicholas and Liu, Chang and Erlingsson, {\'U}lfar and Kos, Jernej and Song, Dawn},
  booktitle={USENIX Security Symposium},
  volume={267},
  year={2019}
}
@article{feldman2020neural-mem2,
  title={What neural networks memorize and why: Discovering the long tail via influence estimation},
  author={Feldman, Vitaly and Zhang, Chiyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={2881--2891},
  year={2020}
}
@inproceedings{kandpal2022deduplicating-mem3,
  title={Deduplicating training data mitigates privacy risks in language models},
  author={Kandpal, Nikhil and Wallace, Eric and Raffel, Colin},
  booktitle={International Conference on Machine Learning},
  pages={10697--10707},
  year={2022},
  organization={PMLR}
}

@inproceedings{
carlini2022quantifying-mem4,
title={Quantifying Memorization Across Neural Language Models},
author={Nicholas Carlini and Daphne Ippolito and Matthew Jagielski and Katherine Lee and Florian Tramer and Chiyuan Zhang},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=TatRHT_1cK}
}


@article{kaiser2015neural,
  title={Neural gpus learn algorithms},
  author={Kaiser, {\L}ukasz and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1511.08228},
  year={2015}
}

@inproceedings{lake2018generalization,
  title={Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks},
  author={Lake, Brenden and Baroni, Marco},
  booktitle={International conference on machine learning},
  pages={2873--2882},
  year={2018},
  organization={PMLR}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@inproceedings{csordas2021devil,
      title={The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers}, 
      author={R\'obert Csord\'as and Kazuki Irie and J\"urgen Schmidhuber},
      booktitle={Proc. Conf. on Empirical Methods in Natural Language Processing (EMNLP)},
      year={2021},
      month={November},
      address={Punta Cana, Dominican Republic}
}


% Curr
@inproceedings{cv1,
  title={Easy samples first: Self-paced reranking for zero-example multimedia search},
  author={Jiang, Lu and Meng, Deyu and Mitamura, Teruko and Hauptmann, Alexander G},
  booktitle={Proceedings of the 22nd ACM international conference on Multimedia},
  pages={547--556},
  year={2014}
}

@inproceedings{cv2,
  title={Curriculumnet: Weakly supervised learning from large-scale web images},
  author={Guo, Sheng and Huang, Weilin and Zhang, Haozhi and Zhuang, Chenfan and Dong, Dengke and Scott, Matthew R and Huang, Dinglong},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={135--150},
  year={2018}
}

@inproceedings{cv3,
  title={Webly supervised learning of convolutional networks},
  author={Chen, Xinlei and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={1431--1439},
  year={2015}
}


@inproceedings{cv4,
 author = {Saxena, Shreyas and Tuzel, Oncel and DeCoste, Dennis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Data Parameters: A New Family of Parameters for Learning a Differentiable Curriculum},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/926ffc0ca56636b9e73c565cf994ea5a-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{nlp1,
    title = "Curriculum Learning and Minibatch Bucketing in Neural Machine Translation",
    author = "Kocmi, Tom  and
      Bojar, Ond{\v{r}}ej",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the International Conference Recent Advances in Natural Language Processing, {RANLP} 2017",
    month = sep,
    year = "2017",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd.",
    url = "https://doi.org/10.26615/978-954-452-049-6_050",
    doi = "10.26615/978-954-452-049-6_050",
    pages = "379--386",
    abstract = "We examine the effects of particular orderings of sentence pairs on the on-line training of neural machine translation (NMT). We focus on two types of such orderings: (1) ensuring that each minibatch contains sentences similar in some aspect and (2) gradual inclusion of some sentence types as the training progresses (so called {``}curriculum learning{''}). In our English-to-Czech experiments, the internal homogeneity of minibatches has no effect on the training but some of our {``}curricula{''} achieve a small improvement over the baseline.",
}

@article{nlp2,
  title={An empirical exploration of curriculum learning for neural machine translation},
  author={Zhang, Xuan and Kumar, Gaurav and Khayrallah, Huda and Murray, Kenton and Gwinnup, Jeremy and Martindale, Marianna J and McNamee, Paul and Duh, Kevin and Carpuat, Marine},
  journal={arXiv preprint arXiv:1811.00739},
  year={2018}
}

@inproceedings{nlp3,
    title = "Competence-based Curriculum Learning for Neural Machine Translation",
    author = "Platanios, Emmanouil Antonios  and
      Stretcu, Otilia  and
      Neubig, Graham  and
      Poczos, Barnabas  and
      Mitchell, Tom",
    editor = "Burstein, Jill  and
      Doran, Christy  and
      Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1119",
    doi = "10.18653/v1/N19-1119",
    pages = "1162--1172",
    abstract = "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70{\%} decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
}

@inproceedings{graves2017automated,
  title={Automated curriculum learning for neural networks},
  author={Graves, Alex and Bellemare, Marc G and Menick, Jacob and Munos, Remi and Kavukcuoglu, Koray},
  booktitle={international conference on machine learning},
  pages={1311--1320},
  year={2017},
  organization={PMLR}
}

@article{narvekar2020curriculum,
  title={Curriculum learning for reinforcement learning domains: A framework and survey},
  author={Narvekar, Sanmit and Peng, Bei and Leonetti, Matteo and Sinapov, Jivko and Taylor, Matthew E and Stone, Peter},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={7382--7431},
  year={2020},
  publisher={JMLRORG}
}

@inproceedings{florensa2017reverse,
  title={Reverse curriculum generation for reinforcement learning},
  author={Florensa, Carlos and Held, David and Wulfmeier, Markus and Zhang, Michael and Abbeel, Pieter},
  booktitle={Conference on robot learning},
  pages={482--495},
  year={2017},
  organization={PMLR}
}


@article{wang2021survey,
  title={A survey on curriculum learning},
  author={Wang, Xin and Chen, Yudong and Zhu, Wenwu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2021},
  publisher={IEEE}
}


@article{soviany2022curriculum,
  title={Curriculum learning: A survey},
  author={Soviany, Petru and Ionescu, Radu Tudor and Rota, Paolo and Sebe, Nicu},
  journal={International Journal of Computer Vision},
  pages={1--40},
  year={2022},
  publisher={Springer}
}

@inproceedings{spitkovsky2010baby,
  title={From baby steps to leapfrog: How “less is more” in unsupervised dependency parsing},
  author={Spitkovsky, Valentin I and Alshawi, Hiyan and Jurafsky, Dan},
  booktitle={Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  pages={751--759},
  year={2010}
}

@article{zaremba2014learning,
  title={Learning to execute},
  author={Zaremba, Wojciech and Sutskever, Ilya},
  journal={arXiv preprint arXiv:1410.4615},
  year={2014}
}


@inproceedings{cornacchia2023mathematical,
  title={A mathematical model for curriculum learning for parities},
  author={Cornacchia, Elisabetta and Mossel, Elchanan},
  booktitle={International Conference on Machine Learning},
  pages={6402--6423},
  year={2023},
  organization={PMLR}
}


@article{kazemnejad2023impact,
  title={The impact of positional encoding on length generalization in transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{
zhou2023algorithms,
title={What Algorithms can Transformers Learn? A Study in Length Generalization},
author={Hattie Zhou and Arwen Bradley and Etai Littwin and Noam Razin and Omid Saremi and Joshua M. Susskind and Samy Bengio and Preetum Nakkiran},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=AssIuHnmHX}
}

@article{abbe2023provable,
  title={Provable advantage of curriculum learning on parity targets with mixed inputs},
  author={Abbe, Emmanuel and Cornacchia, Elisabetta and Lotfi, Aryo},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={24291--24321},
  year={2023}
}

@inproceedings{rasp,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}


@InProceedings{icml-version,
  title = 	 {Generalization on the Unseen, Logic Reasoning and Degree Curriculum},
  author =       {Abbe, Emmanuel and Bengio, Samy and Lotfi, Aryo and Rizk, Kevin},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {31--60},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/abbe23a/abbe23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/abbe23a.html},
}


