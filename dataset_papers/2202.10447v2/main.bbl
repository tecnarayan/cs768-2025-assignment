\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and Cohan]{beltagy2020longformer}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{child2019generating}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2020)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
  Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et~al.
\newblock Rethinking attention with performers.
\newblock \emph{arXiv preprint arXiv:2009.14794}, 2020.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Dai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.~V., and Salakhutdinov, R.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Dauphin et~al.(2017)Dauphin, Fan, Auli, and Grangier]{dauphin2017glu}
Dauphin, Y.~N., Fan, A., Auli, M., and Grangier, D.
\newblock Language modeling with gated convolutional networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pp.\  933–941. JMLR.org, 2017.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly,
  et~al.]{dosovitskiy2020image}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock \emph{arXiv preprint arXiv:2010.11929}, 2020.

\bibitem[Du et~al.(2021)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, et~al.]{du2021glam}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M.,
  Zhou, Y., Yu, A.~W., Firat, O., et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock \emph{arXiv preprint arXiv:2112.06905}, 2021.

\bibitem[Elfwing et~al.(2018)Elfwing, Uchibe, and Doya]{elfwing2018sigmoid}
Elfwing, S., Uchibe, E., and Doya, K.
\newblock Sigmoid-weighted linear units for neural network function
  approximation in reinforcement learning.
\newblock \emph{Neural Networks}, 107:\penalty0 3--11, 2018.

\bibitem[Gehring et~al.(2017)Gehring, Auli, Grangier, Yarats, and
  Dauphin]{Jonas2017convseq2seq}
Gehring, J., Auli, M., Grangier, D., Yarats, D., and Dauphin, Y.~N.
\newblock Convolutional sequence to sequence learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning - Volume 70}, ICML'17, pp.\  1243–1252. JMLR.org, 2017.

\bibitem[Guo et~al.(2020)Guo, Dai, Vrandecic, and Al-Rfou]{guo2020wiki}
Guo, M., Dai, Z., Vrandecic, D., and Al-Rfou, R.
\newblock Wiki-40b: Multilingual language model dataset.
\newblock In \emph{LREC 2020}, 2020.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gelu}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Ho et~al.(2019)Ho, Kalchbrenner, Weissenborn, and
  Salimans]{ho2019axial}
Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T.
\newblock Axial attention in multidimensional transformers.
\newblock \emph{arXiv preprint arXiv:1912.12180}, 2019.

\bibitem[Huang et~al.(2019)Huang, Wang, Huang, Huang, Wei, and
  Liu]{huang2019ccnet}
Huang, Z., Wang, X., Huang, L., Huang, C., Wei, Y., and Liu, W.
\newblock Ccnet: Criss-cross attention for semantic segmentation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pp.\  603--612, 2019.

\bibitem[Jaegle et~al.(2021)Jaegle, Gimeno, Brock, Vinyals, Zisserman, and
  Carreira]{jaegle2021perceiver}
Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J.
\newblock Perceiver: General perception with iterative attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4651--4664. PMLR, 2021.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqa}
Joshi, M., Choi, E., Weld, D., and Zettlemoyer, L.
\newblock {T}rivia{QA}: A large scale distantly supervised challenge dataset
  for reading comprehension.
\newblock In \emph{Proceedings of the 55th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1601--1611,
  Vancouver, Canada, July 2017. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P17-1147}.
\newblock URL \url{https://aclanthology.org/P17-1147}.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5156--5165. PMLR, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, {\L}., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock \emph{arXiv preprint arXiv:2001.04451}, 2020.

\bibitem[Li et~al.(2019)Li, Jin, Xuan, Zhou, Chen, Wang, and
  Yan]{li2019enhancing}
Li, S., Jin, X., Xuan, Y., Zhou, X., Chen, W., Wang, Y.-X., and Yan, X.
\newblock Enhancing the locality and breaking the memory bottleneck of
  transformer on time series forecasting.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 5243--5253, 2019.

\bibitem[Liu et~al.(2021)Liu, Dai, So, and Le]{liu2021pay}
Liu, H., Dai, Z., So, D.~R., and Le, Q.~V.
\newblock Pay attention to mlps.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Narang et~al.(2021)Narang, Chung, Tay, Fedus, Fevry, Matena, Malkan,
  Fiedel, Shazeer, Lan, et~al.]{narang2021transformer}
Narang, S., Chung, H.~W., Tay, Y., Fedus, W., Fevry, T., Matena, M., Malkan,
  K., Fiedel, N., Shazeer, N., Lan, Z., et~al.
\newblock Do transformer modifications transfer across implementations and
  applications?
\newblock \emph{arXiv preprint arXiv:2102.11972}, 2021.

\bibitem[Nguyen \& Salazar(2019)Nguyen and Salazar]{Nguyen2019scalenorm}
Nguyen, T.~Q. and Salazar, J.
\newblock Transformers without tears: Improving the normalization of
  self-attention.
\newblock \emph{CoRR}, abs/1910.05895, 2019.
\newblock URL \url{http://arxiv.org/abs/1910.05895}.

\bibitem[Peng et~al.(2021)]{peng2021rfa}
Peng, H. et~al.
\newblock Random feature attention.
\newblock In \emph{ICLR}, 2021.

\bibitem[Rae et~al.(2019)Rae, Potapenko, Jayakumar, and
  Lillicrap]{rae2019compressive}
Rae, J.~W., Potapenko, A., Jayakumar, S.~M., and Lillicrap, T.~P.
\newblock Compressive transformers for long-range sequence modelling.
\newblock \emph{arXiv preprint arXiv:1911.05507}, 2019.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2020c4}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Ramachandran et~al.(2017)Ramachandran, Zoph, and
  Le]{Ramachandran2017swish}
Ramachandran, P., Zoph, B., and Le, Q.~V.
\newblock Searching for activation functions.
\newblock \emph{CoRR}, abs/1710.05941, 2017.
\newblock URL \url{http://arxiv.org/abs/1710.05941}.

\bibitem[Ren et~al.(2021)Ren, Dai, Dai, Yang, Leskovec, Schuurmans, and
  Dai]{ren2021combiner}
Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., and Dai, B.
\newblock Combiner: Full attention transformer with sparse computation cost.
\newblock In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J.~W.
  (eds.), \emph{Advances in Neural Information Processing Systems}, 2021.
\newblock URL \url{https://openreview.net/forum?id=MQQeeDiO5vv}.

\bibitem[Roy et~al.(2021)Roy, Saffar, Vaswani, and Grangier]{roy2021efficient}
Roy, A., Saffar, M., Vaswani, A., and Grangier, D.
\newblock Efficient content-based sparse attention with routing transformers.
\newblock \emph{Transactions of the Association for Computational Linguistics},
  9:\penalty0 53--68, 2021.

\bibitem[Shazeer(2020)]{shazeer2020glu}
Shazeer, N.
\newblock {GLU} variants improve transformer.
\newblock \emph{CoRR}, abs/2002.05202, 2020.
\newblock URL \url{https://arxiv.org/abs/2002.05202}.

\bibitem[So et~al.(2021)So, Ma{\'n}ke, Liu, Dai, Shazeer, and Le]{so2021primer}
So, D.~R., Ma{\'n}ke, W., Liu, H., Dai, Z., Shazeer, N., and Le, Q.~V.
\newblock Primer: Searching for efficient transformers for language modeling.
\newblock \emph{NeurIPS}, 2021.

\bibitem[Su et~al.(2021)Su, Lu, Pan, Wen, and Liu]{su2021rope}
Su, J., Lu, Y., Pan, S., Wen, B., and Liu, Y.
\newblock Roformer: Enhanced transformer with rotary position embedding, 2021.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
  H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  5998--6008, 2017.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{wang2020linformer}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Ontanon, Pham, Ravula, Wang, Yang, et~al.]{zaheer2020big}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C., Ontanon,
  S., Pham, P., Ravula, A., Wang, Q., Yang, L., et~al.
\newblock Big bird: Transformers for longer sequences.
\newblock In \emph{NeurIPS}, 2020.

\end{thebibliography}
