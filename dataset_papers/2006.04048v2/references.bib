@inproceedings{chatziafratis2020better,
  title={Better depth-width trade-offs for neural networks through the lens of dynamical systems},
  author={Chatziafratis, Vaggos and Nagarajan, Sai Ganesh and Panageas, Ioannis},
  booktitle={International Conference on Machine Learning},
  pages={1469--1478},
  year={2020},
  organization={PMLR}
}
  
@article{chatziafratis2019depth,
  title={Depth-Width Trade-offs for ReLU Networks via Sharkovsky's Theorem},
  author={Chatziafratis, Vaggos and Nagarajan, Sai Ganesh and Panageas, Ioannis and Wang, Xiao},
  journal={arXiv preprint arXiv:1912.04378},
  year={2019}
}  

@article{schmidt2019deep,
  title={Deep {ReLU} network approximation of functions on a manifold},
  author={Schmidt-Hieber, Johannes},
  journal={arXiv preprint arXiv:1908.00695},
  year={2019}
}

@article{schmidt2017nonparametric,
  title={Nonparametric regression using deep neural networks with {ReLU} activation function},
  author={Schmidt-Hieber, Johannes},
  journal={arXiv preprint arXiv:1708.06633},
  year={2017}
}

@article{bartlett1998sample,
  title={The sample complexity of pattern classification with neural networks: the size of the weights is more important than the size of the network},
  author={Bartlett, Peter L},
  journal={IEEE transactions on Information Theory},
  volume={44},
  number={2},
  pages={525--536},
  year={1998},
  publisher={IEEE}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on Learning Theory},
  pages={698--728},
  year={2016}
}

@inproceedings{poole2016exponential,
  title={Exponential expressivity in deep neural networks through transient chaos},
  author={Poole, Ben and Lahiri, Subhaneil and Raghu, Maithra and Sohl-Dickstein, Jascha and Ganguli, Surya},
  booktitle={Advances in neural information processing systems},
  pages={3360--3368},
  year={2016}
}

@book{friedlander1998introduction,
  title={Introduction to the Theory of Distributions},
  author={Friedlander, Friedrich Gerard and Joshi, Mohan C},
  year={1998},
  publisher={Cambridge University Press}
}

@book{duoandikoetxea2001fourier,
  title={Fourier analysis},
  author={Duoandikoetxea, Javier and Zuazo, Javier Duoandikoetxea},
  volume={29},
  year={2001},
  publisher={American Mathematical Soc.}
}

@article{poggio2017and,
  title={Why and when can deep-but not shallow-networks avoid the curse of dimensionality: a review},
  author={Poggio, Tomaso and Mhaskar, Hrushikesh and Rosasco, Lorenzo and Miranda, Brando and Liao, Qianli},
  journal={International Journal of Automation and Computing},
  volume={14},
  number={5},
  pages={503--519},
  year={2017},
  publisher={Springer}
}

@article{martens2014expressive,
  title={On the expressive efficiency of sum product networks},
  author={Martens, James and Medabalimi, Venkatesh},
  journal={arXiv preprint arXiv:1411.7717},
  year={2014}
}

@article{safran2019depth,
  title={Depth Separations in Neural Networks: What is Actually Being Separated?},
  author={Safran, Itay and Eldan, Ronen and Shamir, Ohad},
  journal={arXiv preprint arXiv:1904.06984},
  year={2019}
}

@article{allen2020backward,
  title={Backward Feature Correction: How Deep Learning Performs Deep Learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2001.04413},
  year={2020}
}
@inproceedings{eldan2016power,
  title={The power of depth for feedforward neural networks},
  author={Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on learning theory},
  pages={907--940},
  year={2016}
}

@inproceedings{telgarsky2016benefits,
  title={benefits of depth in neural networks},
  author={Telgarsky, Matus},
  booktitle={Conference on Learning Theory},
  pages={1517--1539},
  year={2016}
}

@inproceedings{daniely2017depth,
  title={Depth Separation for Neural Networks},
  author={Daniely, Amit},
  booktitle={Conference on Learning Theory},
  pages={690--696},
  year={2017}
}


@inproceedings{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2422--2430},
  year={2017}
}

@article{sun2018approximation,
  title={On the approximation properties of random {ReLU} features},
  author={Sun, Yitong and Gilbert, Anna and Tewari, Ambuj},
  journal={arXiv preprint arXiv:1810.04374},
  year={2018}
}

@article{bubeck2015convex,
  title={Convex optimization: Algorithms and complexity},
  author={Bubeck, S{\'e}bastien and others},
  journal={Foundations and Trends{\textregistered} in Machine Learning},
  volume={8},
  number={3-4},
  pages={231--357},
  year={2015},
  publisher={Now Publishers, Inc.}
}

@inproceedings{rahimi2008random,
  title={Random features for large-scale kernel machines},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1177--1184},
  year={2008}
}
@inproceedings{rahimi2008uniform,
  title={Uniform approximation of functions with random bases},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={2008 46th Annual Allerton Conference on Communication, Control, and Computing},
  pages={555--561},
  year={2008},
  organization={IEEE}
}

@inproceedings{rahimi2009weighted,
  title={Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning},
  author={Rahimi, Ali and Recht, Benjamin},
  booktitle={Advances in neural information processing systems},
  pages={1313--1320},
  year={2009}
}

@article{ma2019barron,
  title={Barron spaces and the compositional function spaces for neural network models},
  author={Ma, Chao and Wu, Lei},
  journal={arXiv preprint arXiv:1906.08039},
  year={2019}
}

@article{hanin2017approximating,
  title={Approximating continuous functions by {ReLU} nets of minimal width},
  author={Hanin, Boris and Sellke, Mark},
  journal={arXiv preprint arXiv:1710.11278},
  year={2017}
}

@article{panigrahi2019effect,
  title={Effect of Activation Functions on the Training of Overparametrized Neural Nets},
  author={Panigrahi, Abhishek and Shetty, Abhishek and Goyal, Navin},
  journal={arXiv preprint arXiv:1908.05660},
  year={2019}
}



@inproceedings{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8157--8166},
  year={2018}
}

@article{oymak2019towards,
  title={Towards moderate overparameterization: global convergence guarantees for training shallow neural networks},
  author={Oymak, Samet and Soltanolkotabi, Mahdi},
  journal={arXiv preprint arXiv:1902.04674},
  year={2019}
}
@article{song2019quadratic,
  title={Quadratic Suffices for Over-parametrization via Matrix Chernoff Bound},
  author={Song, Zhao and Yang, Xin},
  journal={arXiv preprint arXiv:1906.03593},
  year={2019}
}
@article{ji2019polylogarithmic,
  title={Polylogarithmic width suffices for gradient descent to achieve arbitrarily small test error with shallow {ReLU} networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1909.12292},
  year={2019}
}
@article{nitanda2019refined,
  title={Refined Generalization Analysis of Gradient Descent for Over-parameterized Two-layer Neural Networks with Smooth Activations on Classification Problems},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1905.09870},
  year={2019}
}
@article{bresler2020corrective,
  title={A Corrective View of Neural Networks: Representation, Memorization and Learning},
  author={Bresler, Guy and Nagaraj, Dheeraj},
  journal={arXiv preprint arXiv:2002.00274},
  year={2020}
}
@inproceedings{yarotsky2018optimal,
  title={Optimal approximation of continuous functions by very deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  booktitle={Conference On Learning Theory},
  pages={639--649},
  year={2018}
}
@article{allen2018convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  journal={arXiv preprint arXiv:1811.03962},
  year={2018}
}

@inproceedings{andoni2014learning,
  title={Learning polynomials with neural networks},
  author={Andoni, Alexandr and Panigrahy, Rina and Valiant, Gregory and Zhang, Li},
  booktitle={International Conference on Machine Learning},
  pages={1908--1916},
  year={2014}
}
@article{liang2016deep,
  title={Why deep neural networks for function approximation?},
  author={Liang, Shiyu and Srikant, Rayadurgam},
  journal={arXiv preprint arXiv:1610.04161},
  year={2016}
}
@article{telgarsky2015representation,
  title={Representation benefits of deep feedforward networks},
  author={Telgarsky, Matus},
  journal={arXiv preprint arXiv:1509.08101},
  year={2015}
}
@article{yarotsky2017error,
  title={Error bounds for approximations with deep {ReLU} networks},
  author={Yarotsky, Dmitry},
  journal={Neural Networks},
  volume={94},
  pages={103--114},
  year={2017},
  publisher={Elsevier}
}
@article{hornik1989multilayer,
  title={Multilayer feedforward networks are universal approximators},
  author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
  journal={Neural networks},
  volume={2},
  number={5},
  pages={359--366},
  year={1989},
  publisher={Elsevier}
}
@article{yehudai2019power,
  title={On the power and limitations of random features for understanding neural networks},
  author={Yehudai, Gilad and Shamir, Ohad},
  journal={arXiv preprint arXiv:1904.00687},
  year={2019}
}
@article{funahashi1989approximate,
  title={On the approximate realization of continuous mappings by neural networks},
  author={Funahashi, Ken-Ichi},
  journal={Neural networks},
  volume={2},
  number={3},
  pages={183--192},
  year={1989},
  publisher={Elsevier}
}
@inproceedings{delalleau2011shallow,
  title={Shallow vs. deep sum-product networks},
  author={Delalleau, Olivier and Bengio, Yoshua},
  booktitle={Advances in Neural Information Processing Systems},
  pages={666--674},
  year={2011}
}



@inproceedings{safran2017depth,
  title={Depth-width tradeoffs in approximating natural functions with neural networks},
  author={Safran, Itay and Shamir, Ohad},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2979--2987},
  year={2017},
  organization={JMLR. org}
}

@article{lee2017deep,
  title={Deep neural networks as gaussian processes},
  author={Lee, Jaehoon and Bahri, Yasaman and Novak, Roman and Schoenholz, Samuel S and Pennington, Jeffrey and Sohl-Dickstein, Jascha},
  journal={arXiv preprint arXiv:1711.00165},
  year={2017}
} 

@article{zhang2016understanding,
  title={Understanding deep learning requires rethinking generalization},
  author={Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1611.03530},
  year={2016}
}
@inproceedings{lu2017expressive,
  title={The expressive power of neural networks: A view from the width},
  author={Lu, Zhou and Pu, Hongming and Wang, Feicheng and Hu, Zhiqiang and Wang, Liwei},
  booktitle={Advances in neural information processing systems},
  pages={6231--6239},
  year={2017}
}
@article{cybenko1989approximation,
  title={Approximation by superpositions of a sigmoidal function},
  author={Cybenko, George},
  journal={Mathematics of control, signals and systems},
  volume={2},
  number={4},
  pages={303--314},
  year={1989},
  publisher={Springer}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}
@article{belkin2018understand,
  title={To understand deep learning we need to understand kernel learning},
  author={Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  journal={arXiv preprint arXiv:1802.01396},
  year={2018}
}

@article{pascanu2014saddle,
  title={On the saddle point problem for non-convex optimization},
  author={Pascanu, Razvan and Dauphin, Yann N and Ganguli, Surya and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1405.4604},
  year={2014}
}

@inproceedings{pennington2017geometry,
  title={Geometry of neural network loss surfaces via random matrix theory},
  author={Pennington, Jeffrey and Bahri, Yasaman},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={2798--2806},
  year={2017},
  organization={JMLR. org}
}

@inproceedings{choromanska2015loss,
  title={The loss surfaces of multilayer networks},
  author={Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  booktitle={Artificial Intelligence and Statistics},
  pages={192--204},
  year={2015}
}
@inproceedings{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  booktitle={Advances in neural information processing systems},
  pages={8571--8580},
  year={2018}
}


@inproceedings{du2019gradient,
  title={Gradient Descent Finds Global Minima of Deep Neural Networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019}
}



@article{basri2016efficient,
  title={Efficient representation of low-dimensional manifolds using deep networks},
  author={Basri, Ronen and Jacobs, David},
  journal={arXiv preprint arXiv:1602.04723},
  year={2016}
}

@article{barron1993universal,
  title={Universal approximation bounds for superpositions of a sigmoidal function},
  author={Barron, Andrew R},
  journal={IEEE Transactions on Information theory},
  volume={39},
  number={3},
  pages={930--945},
  year={1993},
  publisher={IEEE}
}

@article{klusowski2018approximation,
  title={Approximation by Combinations of {ReLU} and Squared {ReLU} Ridge Functions With $l^{1}$ and $l^{0}$ Controls},
  author={Klusowski, Jason M and Barron, Andrew R},
  journal={IEEE Transactions on Information Theory},
  volume={64},
  number={12},
  pages={7649--7656},
  year={2018},
  publisher={IEEE}}
  
  @article{li2019better,
  title={Better Approximations of High Dimensional Smooth Functions by Deep Neural Networks with Rectified Power Units},
  author={Li, Bo and Tang, Shanshan and Yu, Haijun},
  journal={arXiv preprint arXiv:1903.05858},
  year={2019}
}

@inproceedings{zheng2015improving,
  title={Improving deep neural networks using softplus units},
  author={Zheng, Hao and Yang, Zhanlei and Liu, Wenju and Liang, Jizhong and Li, Yanpeng},
  booktitle={2015 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--4},
  year={2015},
  organization={IEEE}
}

@article{elfwing2018sigmoid,
  title={Sigmoid-weighted linear units for neural network function approximation in reinforcement learning},
  author={Elfwing, Stefan and Uchibe, Eiji and Doya, Kenji},
  journal={Neural Networks},
  volume={107},
  pages={3--11},
  year={2018},
  publisher={Elsevier}
}