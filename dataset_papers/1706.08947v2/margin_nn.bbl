\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Anthony and Bartlett(2009)]{anthony2009neural}
M.~Anthony and P.~L. Bartlett.
\newblock \emph{Neural network learning: Theoretical foundations}.
\newblock cambridge university press, 2009.

\bibitem[Bartlett(1998)]{bartlett1998sample}
P.~L. Bartlett.
\newblock The sample complexity of pattern classification with neural networks:
  the size of the weights is more important than the size of the network.
\newblock \emph{IEEE transactions on Information Theory}, 44\penalty0
  (2):\penalty0 525--536, 1998.

\bibitem[Bartlett(2017)]{bartlet2017}
P.~L. Bartlett.
\newblock The impact of the nonlinearity on the {VC}-dimension of a deep
  network.
\newblock \emph{Preprint}, 2017.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett et~al.(1998)Bartlett, Maiorov, and Meir]{bartlett1998almost}
P.~L. Bartlett, V.~Maiorov, and R.~Meir.
\newblock Almost linear vc dimension bounds for piecewise polynomial networks.
\newblock \emph{Neural computation}, 10\penalty0 (8):\penalty0 2159--2173,
  1998.

\bibitem[Chaudhari et~al.(2016)Chaudhari, Choromanska, Soatto, and
  LeCun]{chaudhari2016entropy}
P.~Chaudhari, A.~Choromanska, S.~Soatto, and Y.~LeCun.
\newblock Entropy-sgd: Biasing gradient descent into wide valleys.
\newblock \emph{arXiv preprint arXiv:1611.01838}, 2016.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017computing}
G.~K. Dziugaite and D.~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock \emph{arXiv preprint arXiv:1703.11008}, 2017.

\bibitem[Evgeniou et~al.(2000)Evgeniou, Pontil, and
  Poggio]{evgeniou2000regularization}
T.~Evgeniou, M.~Pontil, and T.~Poggio.
\newblock Regularization networks and support vector machines.
\newblock \emph{Advances in computational mathematics}, 13\penalty0
  (1):\penalty0 1--50, 2000.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2015train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{ICML}, 2016.

\bibitem[Harvey et~al.(2017)Harvey, Liaw, and Mehrabian]{harvey2017nearly}
N.~Harvey, C.~Liaw, and A.~Mehrabian.
\newblock Nearly-tight vc-dimension bounds for piecewise linear neural
  networks.
\newblock \emph{arXiv preprint arXiv:1703.02930}, 2017.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Langford and Caruana(2001)]{langford2001not}
J.~Langford and R.~Caruana.
\newblock (not) bounding the true error.
\newblock In \emph{Proceedings of the 14th International Conference on Neural
  Information Processing Systems: Natural and Synthetic}, pages 809--816. MIT
  Press, 2001.

\bibitem[Luxburg and Bousquet(2004)]{luxburg2004distance}
U.~v. Luxburg and O.~Bousquet.
\newblock Distance-based classification with lipschitz functions.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Jun):\penalty0 669--695, 2004.

\bibitem[McAllester(2003)]{mcallester2003simplified}
D.~McAllester.
\newblock Simplified pac-bayesian margin bounds.
\newblock \emph{Lecture notes in computer science}, pages 203--215, 2003.

\bibitem[McAllester(1998)]{mcallester1998some}
D.~A. McAllester.
\newblock Some {PAC-Bayesian} theorems.
\newblock In \emph{Proceedings of the eleventh annual conference on
  Computational learning theory}, pages 230--234. ACM, 1998.

\bibitem[McAllester(1999)]{mcallester1999pac}
D.~A. McAllester.
\newblock {PAC-Bayesian} model averaging.
\newblock In \emph{Proceedings of the twelfth annual conference on
  Computational learning theory}, pages 164--170. ACM, 1999.

\bibitem[Neyshabur et~al.(2015{\natexlab{a}})Neyshabur, Salakhutdinov, and
  Srebro]{NeySalSre15}
B.~Neyshabur, R.~Salakhutdinov, and N.~Srebro.
\newblock {Path-SGD}: Path-normalized optimization in deep neural networks.
\newblock In \emph{Advanced in Neural Information Processsing Systems (NIPS)},
  2015{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{b}})Neyshabur, Tomioka, and
  Srebro]{NeyTomSre15}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{Proceeding of the 28th Conference on Learning Theory
  (COLT)}, 2015{\natexlab{b}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{c}})Neyshabur, Tomioka, and
  Srebro]{neyshabur15b}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{Proceeding of the International Conference on Learning
  Representations workshop track}, 2015{\natexlab{c}}.

\bibitem[Neyshabur et~al.(2016{\natexlab{a}})Neyshabur, Tomioka, Salakhutdinov,
  and Srebro]{neyshabur16}
B.~Neyshabur, R.~Tomioka, R.~Salakhutdinov, and N.~Srebro.
\newblock Data-dependent path normalization in neural networks.
\newblock In \emph{the International Conference on Learning Representations},
  2016{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2016{\natexlab{b}})Neyshabur, Wu, Salakhutdinov, and
  Srebro]{neyshabur2016path}
B.~Neyshabur, Y.~Wu, R.~Salakhutdinov, and N.~Srebro.
\newblock Path-normalized optimization of recurrent neural networks with relu
  activations.
\newblock \emph{Advances in Neural Information Processing Systems},
  2016{\natexlab{b}}.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Simonyan and Zisserman(2014)]{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{arXiv preprint arXiv:1409.1556}, 2014.

\bibitem[Smola et~al.(1998)Smola, Sch{\"o}lkopf, and
  M{\"u}ller]{smola1998connection}
A.~J. Smola, B.~Sch{\"o}lkopf, and K.-R. M{\"u}ller.
\newblock The connection between regularization operators and support vector
  kernels.
\newblock \emph{Neural networks}, 11\penalty0 (4):\penalty0 637--649, 1998.

\bibitem[Sokolic et~al.(2016)Sokolic, Giryes, Sapiro, and
  Rodrigues]{sokolic2016generalization}
J.~Sokolic, R.~Giryes, G.~Sapiro, and M.~R. Rodrigues.
\newblock Generalization error of invariant classifiers.
\newblock \emph{arXiv preprint arXiv:1610.04574}, 2016.

\bibitem[Srebro and Shraibman(2005)]{srebro2005rank}
N.~Srebro and A.~Shraibman.
\newblock Rank, trace-norm and max-norm.
\newblock In \emph{International Conference on Computational Learning Theory},
  pages 545--560. Springer Berlin Heidelberg, 2005.

\bibitem[Srebro et~al.(2005)Srebro, Rennie, and Jaakkola]{srebro2005maximum}
N.~Srebro, J.~Rennie, and T.~S. Jaakkola.
\newblock Maximum-margin matrix factorization.
\newblock In \emph{Advances in neural information processing systems}, pages
  1329--1336, 2005.

\bibitem[Xu and Mannor(2012)]{xu2012robustness}
H.~Xu and S.~Mannor.
\newblock Robustness and generalization.
\newblock \emph{Machine learning}, 86\penalty0 (3):\penalty0 391--423, 2012.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2017understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\end{thebibliography}
