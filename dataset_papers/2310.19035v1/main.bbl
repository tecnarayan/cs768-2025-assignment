\begin{thebibliography}{94}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmad and Lin(1976)]{feat_dist_entropy}
I.~Ahmad and P.-E. Lin.
\newblock A nonparametric estimation of the entropy for absolutely continuous
  distributions (corresp.).
\newblock \emph{IEEE Transactions on Information Theory}, 22\penalty0
  (3):\penalty0 372--375, 1976.

\bibitem[Ahuja et~al.(2021)Ahuja, Caballero, Zhang, Gagnon-Audet, Bengio,
  Mitliagkas, and Rish]{ib-irm}
K.~Ahuja, E.~Caballero, D.~Zhang, J.-C. Gagnon-Audet, Y.~Bengio, I.~Mitliagkas,
  and I.~Rish.
\newblock Invariance principle meets information bottleneck for
  out-of-distribution generalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{irmv1}
M.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Belghazi et~al.(2018)Belghazi, Baratin, Rajeshwar, Ozair, Bengio,
  Courville, and Hjelm]{mine}
M.~I. Belghazi, A.~Baratin, S.~Rajeshwar, S.~Ozair, Y.~Bengio, A.~Courville,
  and D.~Hjelm.
\newblock Mutual information neural estimation.
\newblock In \emph{International Conference on Machine Learning}, volume~80,
  pages 531--540, 10--15 Jul 2018.

\bibitem[Bevilacqua et~al.(2021)Bevilacqua, Zhou, and Ribeiro]{size_gen2}
B.~Bevilacqua, Y.~Zhou, and B.~Ribeiro.
\newblock Size-invariant graph representations for graph classification
  extrapolations.
\newblock In \emph{International Conference on Machine Learning}, pages
  837--851, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Wang, Guo, Guo, Shao, Shen, and
  Cheng]{causality_bianode}
G.~Chen, Y.~Wang, F.~Guo, Q.~Guo, J.~Shao, H.~Shen, and X.~Cheng.
\newblock Causality and independence enhancement for biased node
  classification.
\newblock In \emph{{ACM} International Conference on Information and Knowledge
  Management}, pages 203--212, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{a}})Chen, Yang, Zhang, Ma, Liu, Han, and
  Cheng]{chen2022hao}
Y.~Chen, H.~Yang, Y.~Zhang, K.~Ma, T.~Liu, B.~Han, and J.~Cheng.
\newblock Understanding and improving graph injection attack by promoting
  unnoticeability.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Chen et~al.(2022{\natexlab{b}})Chen, Zhang, Bian, Yang, Ma, Xie, Liu,
  Han, and Cheng]{ciga}
Y.~Chen, Y.~Zhang, Y.~Bian, H.~Yang, K.~Ma, B.~Xie, T.~Liu, B.~Han, and
  J.~Cheng.
\newblock Learning causally invariant representations for out-of-distribution
  generalization on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Chen et~al.(2022{\natexlab{c}})Chen, Zhou, Bian, Xie, Ma, Zhang, Yang,
  Han, and Cheng]{pair}
Y.~Chen, K.~Zhou, Y.~Bian, B.~Xie, K.~Ma, Y.~Zhang, H.~Yang, B.~Han, and
  J.~Cheng.
\newblock Pareto invariant risk minimization.
\newblock \emph{arXiv preprint}, arXiv:2206.07766, 2022{\natexlab{c}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Huang, Zhou, Bian, Han, and
  Cheng]{fat}
Y.~Chen, W.~Huang, K.~Zhou, Y.~Bian, B.~Han, and J.~Cheng.
\newblock Towards understanding feature learning in out-of-distribution
  generalization.
\newblock \emph{arXiv preprint arXiv:2304.11327}, 2023{\natexlab{b}}.

\bibitem[Chopra et~al.(2005)Chopra, Hadsell, and LeCun]{contrast_loss1}
S.~Chopra, R.~Hadsell, and Y.~LeCun.
\newblock Learning a similarity metric discriminatively, with application to
  face verification.
\newblock In \emph{2005 {IEEE} Computer Society Conference on Computer Vision
  and Pattern Recognition {(CVPR} 2005), 20-26 June 2005, San Diego, CA,
  {USA}}, pages 539--546, 2005.

\bibitem[Creager et~al.(2021{\natexlab{a}})Creager, Jacobsen, and Zemel]{eiil}
E.~Creager, J.~Jacobsen, and R.~S. Zemel.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2189--2200, 2021{\natexlab{a}}.

\bibitem[Creager et~al.(2021{\natexlab{b}})Creager, Jacobsen, and
  Zemel]{env_inference}
E.~Creager, J.~Jacobsen, and R.~S. Zemel.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning}, volume 139,
  pages 2189--2200, 2021{\natexlab{b}}.

\bibitem[Deng et~al.(2023)Deng, Yang, Mirzasoleiman, and Gu]{pde}
Y.~Deng, Y.~Yang, B.~Mirzasoleiman, and Q.~Gu.
\newblock Robust learning with progressive data expansion against spurious
  correlation.
\newblock \emph{arXiv preprint}, arXiv:2306.04949, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{Conference of the North {A}merican Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\bibitem[Ding et~al.(2021)Ding, Kong, Chen, Kirchenbauer, Goldblum, Wipf,
  Huang, and Goldstein]{closer_look_ood}
M.~Ding, K.~Kong, J.~Chen, J.~Kirchenbauer, M.~Goldblum, D.~Wipf, F.~Huang, and
  T.~Goldstein.
\newblock A closer look at distribution shifts and out-of-distribution
  generalization on graphs.
\newblock In \emph{NeurIPS 2021 Workshop on Distribution Shifts: Connecting
  Methods and Applications}, 2021.

\bibitem[Fan et~al.(2022)Fan, Wang, Mo, Shi, and Tang]{disc}
S.~Fan, X.~Wang, Y.~Mo, C.~Shi, and J.~Tang.
\newblock Debiasing graph neural networks via learning disentangled causal
  substructure.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Fey and Lenssen(2019)]{pytorch_geometric}
M.~Fey and J.~E. Lenssen.
\newblock Fast graph representation learning with {PyTorch Geometric}.
\newblock In \emph{ICLR Workshop on Representation Learning on Graphs and
  Manifolds}, 2019.

\bibitem[Gao et~al.(2023)Gao, Zhou, Zhou, and Ribeiro]{Gao2023DoubleEF}
J.~Gao, Y.~Zhou, J.~Zhou, and B.~Ribeiro.
\newblock Double equivariance for inductive link prediction for both new nodes
  and new relation types.
\newblock volume arXiv:2302.01313, 2023.

\bibitem[Gardner et~al.(2018)Gardner, Grus, Neumann, Tafjord, Dasigi, Liu,
  Peters, Schmitz, and Zettlemoyer]{biaffine}
M.~Gardner, J.~Grus, M.~Neumann, O.~Tafjord, P.~Dasigi, N.~F. Liu, M.~E.
  Peters, M.~Schmitz, and L.~Zettlemoyer.
\newblock Allennlp: {A} deep semantic natural language processing platform.
\newblock \emph{arXiv preprint}, arXiv:1803.07640, 2018.

\bibitem[Gui et~al.(2022)Gui, Li, Wang, and Ji]{good_bench}
S.~Gui, X.~Li, L.~Wang, and S.~Ji.
\newblock {GOOD}: A graph out-of-distribution benchmark.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing
  Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Gui et~al.(2023)Gui, Liu, Li, Luo, and Ji]{LECI}
S.~Gui, M.~Liu, X.~Li, Y.~Luo, and S.~Ji.
\newblock Joint learning of label and environment causal independence for graph
  out-of-distribution generalization.
\newblock \emph{arXiv preprint}, arXiv:2306.01103, 2023.

\bibitem[Gulrajani and Lopez{-}Paz(2021)]{domainbed}
I.~Gulrajani and D.~Lopez{-}Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and Leskovec]{sage}
W.~L. Hamilton, Z.~Ying, and J.~Leskovec.
\newblock Inductive representation learning on large graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1024--1034, 2017.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{ogb}
W.~Hu, M.~Fey, M.~Zitnik, Y.~Dong, H.~Ren, B.~Liu, M.~Catasta, and J.~Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Huang et~al.(2021)Huang, Fu, Gao, Zhao, Roohani, Leskovec, Coley,
  Xiao, Sun, and Zitnik]{TDS}
K.~Huang, T.~Fu, W.~Gao, Y.~Zhao, Y.~H. Roohani, J.~Leskovec, C.~W. Coley,
  C.~Xiao, J.~Sun, and M.~Zitnik.
\newblock Therapeutics data commons: Machine learning datasets and tasks for
  drug discovery and development.
\newblock In \emph{Advances in Neural Information Processing Systems Datasets
  and Benchmarks Track (Round 1)}, 2021.

\bibitem[Ioffe and Szegedy(2015)]{batch_norm}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International Conference on Machine Learning}, volume~37,
  pages 448--456, 2015.

\bibitem[{Ji} et~al.(2022){Ji}, {Zhang}, {Wu}, {Wu}, {Huang}, {Xu}, {Rong},
  {Li}, {Ren}, {Xue}, {Lai}, {Xu}, {Feng}, {Liu}, {Luo}, {Zhou}, {Huang},
  {Zhao}, and {Bian}]{drugood}
Y.~{Ji}, L.~{Zhang}, J.~{Wu}, B.~{Wu}, L.-K. {Huang}, T.~{Xu}, Y.~{Rong},
  L.~{Li}, J.~{Ren}, D.~{Xue}, H.~{Lai}, S.~{Xu}, J.~{Feng}, W.~{Liu},
  P.~{Luo}, S.~{Zhou}, J.~{Huang}, P.~{Zhao}, and Y.~{Bian}.
\newblock {DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for
  AI-aided Drug Discovery -- A Focus on Affinity Prediction Problems with Noise
  Annotations}.
\newblock \emph{arXiv preprint}, arXiv:2201.09637, 2022.

\bibitem[Jin et~al.(2022)Jin, Zhao, Ding, Liu, Tang, and Shah]{graph_ttt}
W.~Jin, T.~Zhao, J.~Ding, Y.~Liu, J.~Tang, and N.~Shah.
\newblock Empowering graph representation learning with test-time graph
  transformation.
\newblock \emph{arXiv preprint}, arXiv:2210.03561, 2022.

\bibitem[Kamath et~al.(2021)Kamath, Tangella, Sutherland, and
  Srebro]{irm_aistats}
P.~Kamath, A.~Tangella, D.~Sutherland, and N.~Srebro.
\newblock Does invariant risk minimization capture invariance?
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4069--4077, 2021.

\bibitem[Kamhoua et~al.(2022)Kamhoua, Zhang, Chen, Yang, KAILI, Han, Li, and
  Cheng]{shape_matching}
B.~F. Kamhoua, L.~Zhang, Y.~Chen, H.~Yang, M.~KAILI, B.~Han, B.~Li, and
  J.~Cheng.
\newblock Exact shape correspondence via 2d graph convolution.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Kandasamy et~al.(2015)Kandasamy, Krishnamurthy, Poczos, Wasserman, and
  robins]{vMF_entropy}
K.~Kandasamy, A.~Krishnamurthy, B.~Poczos, L.~Wasserman, and j.~m. robins.
\newblock Nonparametric von mises estimators for entropies, divergences and
  mutual informations.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~28, 2015.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{sup_contrastive}
P.~Khosla, P.~Teterwak, C.~Wang, A.~Sarna, Y.~Tian, P.~Isola, A.~Maschinot,
  C.~Liu, and D.~Krishnan.
\newblock Supervised contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 18661--18673, 2020.

\bibitem[Kingma and Ba(2015)]{adam}
D.~P. Kingma and J.~Ba.
\newblock Adam: {A} method for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[Kipf and Welling(2017)]{gcn}
T.~N. Kipf and M.~Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Knyazev et~al.(2019)Knyazev, Taylor, and Amer]{understand_att}
B.~Knyazev, G.~W. Taylor, and M.~R. Amer.
\newblock Understanding attention and generalization in graph neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  4204--4214, 2019.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, Lee, David, Stavness, Guo, Earnshaw, Haque, Beery,
  Leskovec, Kundaje, Pierson, Levine, Finn, and Liang]{wilds}
P.~W. Koh, S.~Sagawa, H.~Marklund, S.~M. Xie, M.~Zhang, A.~Balsubramani, W.~Hu,
  M.~Yasunaga, R.~L. Phillips, I.~Gao, T.~Lee, E.~David, I.~Stavness, W.~Guo,
  B.~Earnshaw, I.~Haque, S.~M. Beery, J.~Leskovec, A.~Kundaje, E.~Pierson,
  S.~Levine, C.~Finn, and P.~Liang.
\newblock {WILDS:} {A} benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning,}, pages
  5637--5664, 2021.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Priol, and Courville]{v-rex}
D.~Krueger, E.~Caballero, J.~Jacobsen, A.~Zhang, J.~Binas, D.~Zhang, R.~L.
  Priol, and A.~C. Courville.
\newblock Out-of-distribution generalization via risk extrapolation (rex).
\newblock In \emph{International Conference on Machine Learning}, pages
  5815--5826, 2021.

\bibitem[Lee et~al.(2022)Lee, Park, and Yoon]{multimodule_gnn}
H.~Lee, H.~Park, and K.~Yoon.
\newblock Towards better generalization with flexible representation of
  multi-module graph neural networks.
\newblock \emph{arXiv preprint}, arXiv:2209.06589, 2022.

\bibitem[Lee et~al.(2021)Lee, Kim, Lee, Lee, and Choo]{ldd}
J.~Lee, E.~Kim, J.~Lee, J.~Lee, and J.~Choo.
\newblock Learning debiased representation via disentangled feature
  augmentation.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Li et~al.(2022)Li, Zhang, Wang, and Zhu]{gil}
H.~Li, Z.~Zhang, X.~Wang, and W.~Zhu.
\newblock Learning invariant graph representations for out-of-distribution
  generalization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Li et~al.(2023)Li, Gui, Luo, and Ji]{graph_extrapolation}
X.~Li, S.~Gui, Y.~Luo, and S.~Ji.
\newblock Graph structure and feature extrapolation for out-of-distribution
  generalization.
\newblock \emph{arXiv preprint}, arXiv:2306.08076, 2023.

\bibitem[Lin et~al.(2022)Lin, Zhu, Tan, and Cui]{zin}
Y.~Lin, S.~Zhu, L.~Tan, and P.~Cui.
\newblock {ZIN}: When and how to learn invariance without environment
  partition?
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Liu et~al.(2021{\natexlab{a}})Liu, Haghgoo, Chen, Raghunathan, Koh,
  Sagawa, Liang, and Finn]{jtt}
E.~Z. Liu, B.~Haghgoo, A.~S. Chen, A.~Raghunathan, P.~W. Koh, S.~Sagawa,
  P.~Liang, and C.~Finn.
\newblock Just train twice: Improving group robustness without training group
  information.
\newblock In \emph{International Conference on Machine Learning}, pages
  6781--6792, 2021{\natexlab{a}}.

\bibitem[Liu et~al.(2022)Liu, Zhao, Xu, Luo, and Jiang]{grea}
G.~Liu, T.~Zhao, J.~Xu, T.~Luo, and M.~Jiang.
\newblock Graph rationalization with environment-based augmentations.
\newblock \emph{arXiv preprint arXiv:2206.02886}, 2022.

\bibitem[Liu et~al.(2021{\natexlab{b}})Liu, Hu, Cui, Li, and Shen]{hrm}
J.~Liu, Z.~Hu, P.~Cui, B.~Li, and Z.~Shen.
\newblock Heterogeneous risk minimization.
\newblock In \emph{International Conference on Machine Learning}, volume 139,
  pages 6804--6814, 2021{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Feng, Tran, Zhao, Qiu, and
  Li]{struc_reweight}
S.~Liu, T.~Li, Y.~Feng, N.~Tran, H.~Zhao, Q.~Qiu, and P.~Li.
\newblock Structural re-weighting improves graph domain adaptation.
\newblock In \emph{International Conference on Machine Learning}, Proceedings
  of Machine Learning Research, pages 21778--21793, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Ao, Feng, Ma, Li, Chua, and
  He]{flood}
Y.~Liu, X.~Ao, F.~Feng, Y.~Ma, K.~Li, T.~Chua, and Q.~He.
\newblock {FLOOD:} {A} flexible invariant learning framework for
  out-of-distribution generalization on graphs.
\newblock In \emph{Proceedings of the 29th {ACM} {SIGKDD} Conference on
  Knowledge Discovery and Data Mining}, pages 1548--1558, 2023{\natexlab{b}}.

\bibitem[Lucic et~al.(2022)Lucic, Ter~Hoeve, Tolomei, De~Rijke, and
  Silvestri]{cfxgnn}
A.~Lucic, M.~A. Ter~Hoeve, G.~Tolomei, M.~De~Rijke, and F.~Silvestri.
\newblock Cf-gnnexplainer: Counterfactual explanations for graph neural
  networks.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 4499--4511, 2022.

\bibitem[Luo et~al.(2020)Luo, Cheng, Xu, Yu, Zong, Chen, and Zhang]{pge}
D.~Luo, W.~Cheng, D.~Xu, W.~Yu, B.~Zong, H.~Chen, and X.~Zhang.
\newblock Parameterized explainer for graph neural network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  19620--19631, 2020.

\bibitem[Ma et~al.(2023)Ma, Yang, Yang, Chen, and Cheng]{kaili2023calibrating}
K.~Ma, G.~Yang, H.~Yang, Y.~Chen, and J.~Cheng.
\newblock Calibrating and improving graph contrastive learning.
\newblock \emph{Transactions on Machine Learning Research}, 2023.
\newblock ISSN 2835-8856.

\bibitem[Mahdavi et~al.(2022)Mahdavi, Swersky, Kipf, Hashemi, Thrampoulidis,
  and Liao]{OOD_CLRS}
S.~Mahdavi, K.~Swersky, T.~Kipf, M.~Hashemi, C.~Thrampoulidis, and R.~Liao.
\newblock Towards better out-of-distribution generalization of neural
  algorithmic reasoning tasks.
\newblock \emph{arXiv preprint arXiv:2211.00692}, 2022.

\bibitem[McInnes et~al.(2018)McInnes, Healy, Saul, and Grossberger]{umap}
L.~McInnes, J.~Healy, N.~Saul, and L.~Grossberger.
\newblock Umap: Uniform manifold approximation and projection.
\newblock \emph{The Journal of Open Source Software}, 3\penalty0 (29):\penalty0
  861, 2018.

\bibitem[Mendez et~al.(2019)Mendez, Gaulton, Bento, Chambers, Veij, Felix,
  Magari{\~{n}}os, Mosquera, Mutowo{-}Meullenet, Nowotka,
  Gordillo{-}Mara{\~{n}}{\'{o}}n, Hunter, Junco, Mugumbate,
  Rodr{\'{\i}}guez{-}L{\'{o}}pez, Atkinson, Bosc, Radoux, Segura{-}Cabrera,
  Hersey, and Leach]{chembl}
D.~Mendez, A.~Gaulton, A.~P. Bento, J.~Chambers, M.~D. Veij, E.~Felix, M.~P.
  Magari{\~{n}}os, J.~F. Mosquera, P.~Mutowo{-}Meullenet, M.~Nowotka,
  M.~Gordillo{-}Mara{\~{n}}{\'{o}}n, F.~M.~I. Hunter, L.~Junco, G.~Mugumbate,
  M.~Rodr{\'{\i}}guez{-}L{\'{o}}pez, F.~Atkinson, N.~Bosc, C.~J. Radoux,
  A.~Segura{-}Cabrera, A.~Hersey, and A.~R. Leach.
\newblock Chembl: towards direct deposition of bioassay data.
\newblock \emph{Nucleic Acids Research}, 47\penalty0 (Database-Issue):\penalty0
  D930--D940, 2019.

\bibitem[Miao et~al.(2022)Miao, Liu, and Li]{gsat}
S.~Miao, M.~Liu, and P.~Li.
\newblock Interpretable and generalizable graph learning via stochastic
  attention mechanism.
\newblock \emph{arXiv preprint arXiv:2201.12987}, 2022.

\bibitem[Miao et~al.(2023)Miao, Luo, Liu, and Li]{lri}
S.~Miao, Y.~Luo, M.~Liu, and P.~Li.
\newblock Interpretable geometric deep learning via learnable randomness
  injection.
\newblock In \emph{International Conference on Learning Representations}, 2023.

\bibitem[Murray and Rees(2009)]{fragment}
C.~Murray and D.~Rees.
\newblock The rise of fragment-based drug discovery.
\newblock \emph{Nature chemistry}, 1:\penalty0 187--92, 06 2009.

\bibitem[Nam et~al.(2020)Nam, Cha, Ahn, Lee, and Shin]{lff}
J.~Nam, H.~Cha, S.~Ahn, J.~Lee, and J.~Shin.
\newblock Learning from failure: Training debiased classifier from biased
  classifier.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem[Peters et~al.(2016)Peters, Bühlmann, and Meinshausen]{inv_principle}
J.~Peters, P.~Bühlmann, and N.~Meinshausen.
\newblock Causal inference by using invariant prediction: identification and
  confidence intervals.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 78\penalty0 (5):\penalty0 947--1012, 2016.

\bibitem[Pezeshki et~al.(2023)Pezeshki, Bouchacourt, Ibrahim, Ballas, Vincent,
  and Lopez{-}Paz]{xrm}
M.~Pezeshki, D.~Bouchacourt, M.~Ibrahim, N.~Ballas, P.~Vincent, and
  D.~Lopez{-}Paz.
\newblock Discovering environments with {XRM}.
\newblock \emph{arXiv preprint}, arXiv:2309.16748, 2023.

\bibitem[Salakhutdinov and Hinton(2007)]{contrast_loss2}
R.~Salakhutdinov and G.~E. Hinton.
\newblock Learning a nonlinear embedding by preserving class neighbourhood
  structure.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 412--419, 2007.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst25}
R.~Socher, A.~Perelygin, J.~Wu, J.~Chuang, C.~D. Manning, A.~Y. Ng, and
  C.~Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, pages 1631--1642, 2013.

\bibitem[Sui et~al.(2022)Sui, Wang, Wu, Lin, He, and Chua]{cal}
Y.~Sui, X.~Wang, J.~Wu, M.~Lin, X.~He, and T.-S. Chua.
\newblock Causal attention for interpretable and generalizable graph
  classification.
\newblock In \emph{Proceedings of the 28th ACM SIGKDD Conference on Knowledge
  Discovery and Data Mining}, page 1696–1705, 2022.

\bibitem[Tao et~al.(2023)Tao, Cao, Shen, Wu, Xu, and Cheng]{Tao2023IDEAIC}
S.~Tao, Q.~Cao, H.~Shen, Y.~Wu, B.~Xu, and X.~Cheng.
\newblock Idea: Invariant causal defense for graph adversarial robustness.
\newblock \emph{arXiv preprint}, arXiv:2305.15792, 2023.

\bibitem[Ulyanov et~al.(2016)Ulyanov, Vedaldi, and Lempitsky]{instancenorm}
D.~Ulyanov, A.~Vedaldi, and V.~S. Lempitsky.
\newblock Instance normalization: The missing ingredient for fast stylization.
\newblock \emph{arXiv preprint}, arXiv:1607.08022, 2016.

\bibitem[van~den Oord et~al.(2018)van~den Oord, Li, and Vinyals]{infoNCE}
A.~van~den Oord, Y.~Li, and O.~Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint}, arXiv:1807.03748, 2018.

\bibitem[Veličković et~al.(2018)Veličković, Cucurull, Casanova, Romero,
  Liò, and Bengio]{gat}
P.~Veličković, G.~Cucurull, A.~Casanova, A.~Romero, P.~Liò, and Y.~Bengio.
\newblock Graph attention networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Wang and Isola(2020)]{align_uniform}
T.~Wang and P.~Isola.
\newblock Understanding contrastive representation learning through alignment
  and uniformity on the hypersphere.
\newblock In \emph{International Conference on Machine Learning}, pages
  9929--9939, 2020.

\bibitem[Wang et~al.(2023)Wang, Chen, Duan, Li, Han, Cheng, and
  Tong]{ood_kinetics}
Z.~Wang, Y.~Chen, Y.~Duan, W.~Li, B.~Han, J.~Cheng, and H.~Tong.
\newblock Towards out-of-distribution generalizable predictions of chemical
  kinetics properties.
\newblock \emph{arXiv preprint}, arXiv:2310.03152, 2023.

\bibitem[Wolpert and Macready(1997)]{no_free_lunch}
D.~Wolpert and W.~Macready.
\newblock No free lunch theorems for optimization.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 1\penalty0
  (1):\penalty0 67--82, 1997.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Zhang, Yan, and Wipf]{eerm}
Q.~Wu, H.~Zhang, J.~Yan, and D.~Wipf.
\newblock Handling distribution shifts on graphs: An invariance perspective.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{a}}.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Wang, Zhang, He, and Chua]{dir}
Y.~Wu, X.~Wang, A.~Zhang, X.~He, and T.-S. Chua.
\newblock Discovering invariant rationales for graph neural networks.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Xu et~al.(2018)Xu, Li, Tian, Sonobe, Kawarabayashi, and
  Jegelka]{jknet}
K.~Xu, C.~Li, Y.~Tian, T.~Sonobe, K.~Kawarabayashi, and S.~Jegelka.
\newblock Representation learning on graphs with jumping knowledge networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  5449--5458, 2018.

\bibitem[Xu et~al.(2019)Xu, Hu, Leskovec, and Jegelka]{gin}
K.~Xu, W.~Hu, J.~Leskovec, and S.~Jegelka.
\newblock How powerful are graph neural networks?
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Xu et~al.(2021)Xu, Zhang, Li, Du, Kawarabayashi, and
  Jegelka]{nn_extrapo}
K.~Xu, M.~Zhang, J.~Li, S.~S. Du, K.~Kawarabayashi, and S.~Jegelka.
\newblock How neural networks extrapolate: From feedforward to graph neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Yang et~al.(2022)Yang, Zeng, Wu, Jia, and Yan]{moleood}
N.~Yang, K.~Zeng, Q.~Wu, X.~Jia, and J.~Yan.
\newblock Learning substructure invariance for out-of-distribution molecular
  representations.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2022.

\bibitem[Yehudai et~al.(2021)Yehudai, Fetaya, Meirom, Chechik, and
  Maron]{size_gen1}
G.~Yehudai, E.~Fetaya, E.~Meirom, G.~Chechik, and H.~Maron.
\newblock From local structures to size generalization in graph neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  11975--11986, 2021.

\bibitem[Yeung(2008)]{network_coding}
R.~Yeung.
\newblock \emph{Information Theory and Network Coding}.
\newblock Springer New York, NY, 01 2008.
\newblock ISBN 978-0-387-79233-0.

\bibitem[You et~al.(2023)You, Chen, Wang, and Shen]{gda}
Y.~You, T.~Chen, Z.~Wang, and Y.~Shen.
\newblock Graph domain adaptation via theory-grounded spectral regularization.
\newblock In \emph{The Eleventh International Conference on Learning
  Representations}, 2023.

\bibitem[Yu et~al.(2021{\natexlab{a}})Yu, Xu, Rong, Bian, Huang, and He]{gib}
J.~Yu, T.~Xu, Y.~Rong, Y.~Bian, J.~Huang, and R.~He.
\newblock Graph information bottleneck for subgraph recognition.
\newblock In \emph{International Conference on Learning Representations},
  2021{\natexlab{a}}.

\bibitem[Yu et~al.(2021{\natexlab{b}})Yu, Xu, Rong, Bian, Huang, and He]{vgib}
J.~Yu, T.~Xu, Y.~Rong, Y.~Bian, J.~Huang, and R.~He.
\newblock Recognizing predictive substructures with subgraph information
  bottleneck.
\newblock \emph{IEEE transactions on pattern analysis and machine
  intelligence}, 2021{\natexlab{b}}.

\bibitem[Yu et~al.(2023)Yu, Liang, and He]{dps}
J.~Yu, J.~Liang, and R.~He.
\newblock Mind the label shift of augmentation-based graph {OOD}
  generalization.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2023.

\bibitem[Yuan et~al.(2020)Yuan, Yu, Gui, and Ji]{xgnn_tax}
H.~Yuan, H.~Yu, S.~Gui, and S.~Ji.
\newblock Explainability in graph neural networks: {A} taxonomic survey.
\newblock \emph{arXiv preprint}, arXiv:2012.15445, 2020.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Sohoni, Zhang, Finn, and
  R{\'{e}}]{cnc}
M.~Zhang, N.~S. Sohoni, H.~R. Zhang, C.~Finn, and C.~R{\'{e}}.
\newblock Correct-n-contrast: {A} contrastive approach for improving robustness
  to spurious correlations.
\newblock \emph{arXiv preprint}, arXiv:2203.01517, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2023)Zhang, Wang, Helwig, Luo, Fu, Xie, Liu, Lin, Xu,
  Yan, Adams, Weiler, Li, Fu, Wang, Yu, Xie, Fu, Strasser, Xu, Liu, Du, Saxton,
  Ling, Lawrence, St{\"{a}}rk, Gui, Edwards, Gao, Ladera, Wu, Hofgard, Tehrani,
  Wang, Daigavane, Bohde, Kurtin, Huang, Phung, Xu, Joshi, Mathis,
  Azizzadenesheli, Fang, Aspuru{-}Guzik, Bekkers, Bronstein, Zitnik,
  Anandkumar, Ermon, Li{\`{o}}, Yu, G{\"{u}}nnemann, Leskovec, Ji, Sun,
  Barzilay, Jaakkola, Coley, Qian, Qian, Smidt, and Ji]{ai4sci}
X.~Zhang, L.~Wang, J.~Helwig, Y.~Luo, C.~Fu, Y.~Xie, M.~Liu, Y.~Lin, Z.~Xu,
  K.~Yan, K.~Adams, M.~Weiler, X.~Li, T.~Fu, Y.~Wang, H.~Yu, Y.~Xie, X.~Fu,
  A.~Strasser, S.~Xu, Y.~Liu, Y.~Du, A.~Saxton, H.~Ling, H.~Lawrence,
  H.~St{\"{a}}rk, S.~Gui, C.~Edwards, N.~Gao, A.~Ladera, T.~Wu, E.~F. Hofgard,
  A.~M. Tehrani, R.~Wang, A.~Daigavane, M.~Bohde, J.~Kurtin, Q.~Huang,
  T.~Phung, M.~Xu, C.~K. Joshi, S.~V. Mathis, K.~Azizzadenesheli, A.~Fang,
  A.~Aspuru{-}Guzik, E.~Bekkers, M.~M. Bronstein, M.~Zitnik, A.~Anandkumar,
  S.~Ermon, P.~Li{\`{o}}, R.~Yu, S.~G{\"{u}}nnemann, J.~Leskovec, H.~Ji,
  J.~Sun, R.~Barzilay, T.~S. Jaakkola, C.~W. Coley, X.~Qian, X.~Qian, T.~E.
  Smidt, and S.~Ji.
\newblock Artificial intelligence for science in quantum, atomistic, and
  continuum systems.
\newblock \emph{arXiv preprint}, arXiv:2307.08423, 2023.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Gong, Liu, Niu, Tian, Han,
  Sch{\"o}lkopf, and Zhang]{causaladv}
Y.~Zhang, M.~Gong, T.~Liu, G.~Niu, X.~Tian, B.~Han, B.~Sch{\"o}lkopf, and
  K.~Zhang.
\newblock Adversarial robustness through the lens of causality.
\newblock In \emph{International Conference on Learning Representations},
  2022{\natexlab{b}}.

\bibitem[Zhou et~al.(2023{\natexlab{a}})Zhou, Bevilacqua, and
  Ribeiro]{Zhou2023AnOM}
J.~Zhou, B.~Bevilacqua, and B.~Ribeiro.
\newblock An ood multi-task perspective for link prediction with new relation
  types and nodes.
\newblock \emph{arXiv preprint}, arXiv:2307.06046, 2023{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{a}})Zhou, Kutyniok, and Ribeiro]{size_gen3}
Y.~Zhou, G.~Kutyniok, and B.~Ribeiro.
\newblock {OOD} link prediction generalization capabilities of message-passing
  {GNN}s in larger test graphs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{a}}.

\bibitem[Zhou et~al.(2022{\natexlab{b}})Zhou, Kutyniok, and
  Ribeiro]{zhou2022ood}
Y.~Zhou, G.~Kutyniok, and B.~Ribeiro.
\newblock {OOD} link prediction generalization capabilities of message-passing
  {GNN}s in larger test graphs.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Zhou et~al.(2023{\natexlab{b}})Zhou, Yao, Liu, Guo, Yao, He, Wang,
  Zheng, and Han]{zhou2023combating}
Z.~Zhou, J.~Yao, J.~Liu, X.~Guo, Q.~Yao, L.~He, L.~Wang, B.~Zheng, and B.~Han.
\newblock Combating bilateral edge noise for robust link prediction.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2023{\natexlab{b}}.

\bibitem[Zhou et~al.(2023{\natexlab{c}})Zhou, Zhou, Li, Yao, Yao, and
  Han]{zhou2023mcgra}
Z.~Zhou, C.~Zhou, X.~Li, J.~Yao, Q.~Yao, and B.~Han.
\newblock On strengthening and defending graph reconstruction attack with
  markov chain approximation.
\newblock In \emph{International Conference on Machine Learning},
  2023{\natexlab{c}}.

\bibitem[Zhu et~al.(2023)Zhu, Jiao, Ponomareva, Han, and Perozzi]{conda}
Q.~Zhu, Y.~Jiao, N.~Ponomareva, J.~Han, and B.~Perozzi.
\newblock Explaining and adapting graph conditional shift.
\newblock \emph{arXiv preprint}, arXiv:2306.03256, 2023.

\bibitem[Zou et~al.(2023)Zou, Liu, Miao, Fung, Chang, and Li]{gdl_ds}
D.~Zou, S.~Liu, S.~Miao, V.~Fung, S.~Chang, and P.~Li.
\newblock {GDL-DS:} {A} benchmark for geometric deep learning under
  distribution shifts.
\newblock \emph{arXiv preprint}, abs/2310.08677, 2023.

\end{thebibliography}
