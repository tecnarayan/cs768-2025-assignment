By filtering the content that users see, social media platforms have the
ability to influence users' perceptions and decisions, from their dining
choices to their voting preferences. This influence has drawn scrutiny, with
many calling for regulations on filtering algorithms, but designing and
enforcing regulations remains challenging. In this work, we examine three
questions. First, given a regulation, how would one design an audit to enforce
it? Second, does the audit impose a performance cost on the platform? Third,
how does the audit affect the content that the platform is incentivized to
filter? In response, we propose a method such that, given a regulation, an
auditor can test whether that regulation is met with only black-box access to
the filtering algorithm. We then turn to the platform's perspective. The
platform's goal is to maximize an objective function while meeting regulation.
We find that there are conditions under which the regulation does not place a
high performance cost on the platform and, notably, that content diversity can
play a key role in aligning the interests of the platform and regulators.