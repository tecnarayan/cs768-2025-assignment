@book{lehmann2004elements,
  title={Elements of Large-Sample Theory},
  author={Lehmann, E.L.},
  isbn={9780387985954},
  lccn={98034429},
  series={Springer Texts in Statistics},
  url={https://books.google.com/books?id=geIoxvgTXlEC},
  year={2004},
  publisher={Springer New York}
}

@article{Catoni:2012,
author = {Olivier Catoni},
title = {{Challenging the empirical mean and empirical variance: A deviation study}},
volume = {48},
journal = {Annales de l'Institut Henri Poincaré, Probabilités et Statistiques},
number = {4},
publisher = {Institut Henri Poincaré},
pages = {1148 -- 1185},
keywords = {M-estimators, Non-parametric estimation, PAC-Bayes bounds},
year = {2012},
doi = {10.1214/11-AIHP454},
URL = {https://doi.org/10.1214/11-AIHP454}
}

@INPROCEEDINGS{Lee:2021,
  author={Jasper C. H. Lee and Valiant, Paul},
  booktitle={Proc.~FOCS'21}, 
  title={Optimal Sub-Gaussian Mean Estimation in $\mathbb{R}$}, 
  year={2022},
  pages={672-683},
}

@INPROCEEDINGS{Lee:2022,
  author={Jasper C. H. Lee and Valiant, Paul},
  booktitle={Proc.~ITCS'22}, 
  title={Optimal Sub-Gaussian Mean Estimation in Very High Dimensions}, 
  year={2022},
  pages={98:1-98:21},
}
  
@book{Shevlyakov:2011,
  title={Robustness in Data Analysis: Criteria and Methods},
  author={Shevlyakov, G.L. and Vilchevski, N.O.},
  isbn={9783110936001},
  series={Modern Probability and Statistics},
  year={2011},
  publisher={De Gruyter}
}

@article{Spokoiny:2011,
author = {Spokoiny, Vladimir},
year = {2011},
number={6},
pages={2877--2909},
title = {Parametric estimation. Finite sample theory},
volume = {40},
journal = {The Annals of Statistics},
doi = {10.1214/12-AOS1054}
}

@article{Pinelis:2017,
author = {Iosif Pinelis},
title = {{Optimal-order uniform and nonuniform bounds on the rate of convergence to normality for maximum likelihood estimators}},
volume = {11},
journal = {Electronic Journal of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics and Bernoulli Society},
pages = {1160 -- 1179},
keywords = {Berry–Esseen bounds, Delta method, maximum likelihood estimators, rates of convergence},
year = {2017},
doi = {10.1214/17-EJS1264},
URL = {https://doi.org/10.1214/17-EJS1264}
}

@book{vanDerVaart:2000asymptotic,
  title={Asymptotic Statistics},
  author={van der Vaart, A.W.},
  isbn={9780521784504},
  lccn={98015176},
  year={2000},
  publisher={Cambridge University Press}
}

@article{Minsker:2022-subgaussian-mean,
  title={U-statistics of growing order and sub-Gaussian mean estimators with sharp constants},
  author={Minsker, S.},
  journal={arXiv preprint arXiv:2202.11842},
  year={2022}
}

@article{Miao:2010,
title = {Concentration inequality of maximum likelihood estimator},
journal = {Applied Mathematics Letters},
volume = {23},
number = {10},
pages = {1305-1309},
year = {2010},
author = {Yu Miao},
}


@article{bretagnolle:1979estimation,
  title={Estimation des densit{\'e}s: risque minimax},
  author={Bretagnolle, Jean and Huber, Catherine},
  journal={Zeitschrift f{\"u}r Wahrscheinlichkeitstheorie und verwandte Gebiete},
  volume={47},
  number={2},
  pages={119--137},
  year={1979},
  publisher={Springer}
}

@book{bar-yossef:2002complexity,
  title={The complexity of massive data set computations},
  author={Bar-Yossef, Ziv},
  year={2002},
  publisher={University of California, Berkeley}
}

@article{stone1975adaptive,
  title={Adaptive maximum likelihood estimators of a location parameter},
  author={Stone, Charles J},
  journal={The Annals of Statistics},
  volume = {3},
  number = {2},
  pages={267--284},
  year={1975},
  publisher={JSTOR}
}

@book{boucheron,
  author    = {St{\'{e}}phane Boucheron and
               G{\'{a}}bor Lugosi and
               Pascal Massart},
  title     = {Concentration Inequalities - {A} Nonasymptotic Theory of Independence},
  publisher = {Oxford University Press},
  year      = {2013},
  url       = {https://doi.org/10.1093/acprof:oso/9780199535255.001.0001},
  doi       = {10.1093/acprof:oso/9780199535255.001.0001},
  isbn      = {978-0-19-953525-5},
  timestamp = {Mon, 16 Sep 2019 14:43:12 +0200},
  biburl    = {https://dblp.org/rec/books/daglib/0035704.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{vdG:2000,
  title={Empirical Processes in M-estimation},
  author={Van de Geer, Sara A},
  volume={6},
  year={2000},
  publisher={Cambridge university press}
}

@inproceedings{Gupta:2022,
 title = {Finite-Sample Maximum Likelihood Estimation of Location},
 author = {Gupta, Shivam and Jasper C. H. Lee and Price, Eric and Valiant, Paul},
 year = {2022},
 booktitle = {Proc.~NeurIPS'22}
}

@phdthesis{Fisher_upper_bound,
author = {Hendeby, Gustaf},
year = {2005},
month = {11},
pages = {},
title = {Fundamental Estimation and Detection Limits in Linear Non-Gaussian Systems}
}
@article{Hopkins2018SubGaussianME,
  title={Sub-Gaussian Mean Estimation in Polynomial Time},
  author={Samuel B. Hopkins},
  journal={ArXiv},
  year={2018},
  volume={abs/1809.07425}
}

@InProceedings{pmlr-v99-cherapanamjeri19b,
  title = 	 {Fast Mean Estimation with Sub-Gaussian Rates},
  author =       {Cherapanamjeri, Yeshwanth and Flammarion, Nicolas and Bartlett, Peter L.},
  booktitle = 	 {Proceedings of the Thirty-Second Conference on Learning Theory},
  pages = 	 {786--806},
  year = 	 {2019},
  editor = 	 {Beygelzimer, Alina and Hsu, Daniel},
  volume = 	 {99},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {25--28 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v99/cherapanamjeri19b/cherapanamjeri19b.pdf},
  url = 	 {https://proceedings.mlr.press/v99/cherapanamjeri19b.html},
  abstract = 	 {We propose an estimator for the mean of a random vector in $\mathbb{R}^d$ that can be computed in time $O(n^{3.5}+n^2d)$ for $n$ i.i.d.&nbsp;samples and that has error bounds matching the sub-Gaussian case. The only assumptions we make about the data distribution are that it has finite mean and covariance; in particular, we make no assumptions about higher-order moments. Like the polynomial time estimator introduced by Hopkins (2018), which is based on the sum-of-squares hierarchy, our estimator achieves optimal statistical efficiency in this challenging setting, but it has a significantly faster runtime and a simpler analysis.}
}

@article{Hsu:2012,
  title={A tail inequality for quadratic forms of subgaussian random vectors},
  author={Hsu, Daniel and Kakade, Sham and Zhang, Tong},
  journal={Electronic Communications in Probability},
  volume={17},
  pages={1--6},
  year={2012},
}

@book{Bickel:2015,
  title={Mathematical statistics: basic ideas and selected topics, volume I},
  author={Bickel, Peter J and Doksum, Kjell A},
  year={2015},
  publisher={Chapman and Hall/CRC}
}

@article{Bousquet:2002,
  title={A Bennett concentration inequality and its application to suprema of empirical processes},
  author={Bousquet, Olivier},
  journal={Comptes Rendus Mathematique},
  volume={334},
  number={6},
  pages={495--500},
  year={2002},
}

@InProceedings{Bousquet:2003,
author="Bousquet, Olivier",
editor="Gin{\'e}, Evariste
and Houdr{\'e}, Christian
and Nualart, David",
title="Concentration Inequalities for Sub-Additive Functions Using the Entropy Method",
booktitle="Stochastic Inequalities and Applications",
year="2003",
address="Basel",
pages="213--247",
}

@article{Devroye:2016,
  title={Sub-{G}aussian mean estimators},
  author={Devroye, Luc and Lerasle, Matthieu and Lugosi, G{\'a}bor and Oliveira, Roberto I.},
  journal={Ann.~Stat},
  volume={44},
  number={6},
  pages={2695--2725},
  year={2016},
}

@article{lugosi_mendelson,
author = {Lugosi, Gábor and Mendelson, Shahar},
year = {2017},
month = {02},
pages = {},
title = {Sub-Gaussian estimators of the mean of a random vector},
volume = {47},
journal = {Annals of Statistics},
doi = {10.1214/17-AOS1639}
}
@MISC {rearrangement,
    TITLE = {Show $\mathbb{E}[f(X)g(X)] \geq \mathbb{E}[f(X)]\mathbb{E}[g(X)]$ for $f,g$ bounded, nondecreasing},
    AUTHOR = {user940},
    year={2015},
    HOWPUBLISHED = {Mathematics Stack Exchange},
    NOTE = {Downloaded 2023-01},
    EPRINT = {https://math.stackexchange.com/q/1446526},
    URL = {https://math.stackexchange.com/q/1446526}
}