\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahuja et~al.(1993)Ahuja, Magnanti, and Orlin]{Ahuja1993NetworkF}
Ahuja, R.~K., Magnanti, T.~L., and Orlin, J.~B.
\newblock Network flows - theory, algorithms and applications.
\newblock 1993.

\bibitem[Bagnall et~al.(2018)Bagnall, Dau, Lines, Flynn, Large, Bostrom,
  Southam, and Keogh]{Bagnall2018TheUM}
Bagnall, A.~J., Dau, H.~A., Lines, J., Flynn, M., Large, J., Bostrom, A.~G.,
  Southam, P., and Keogh, E.~J.
\newblock The uea multivariate time series classification archive, 2018.
\newblock \emph{arXiv preprint arXiv:1811.00075}, 2018.

\bibitem[Beltagy et~al.(2020)Beltagy, Peters, and
  Cohan]{Beltagy2020LongformerTL}
Beltagy, I., Peters, M.~E., and Cohan, A.
\newblock Longformer: The long-document transformer.
\newblock \emph{arXiv preprint arXiv:2004.05150}, 2020.

\bibitem[Berndt \& Clifford(1994)Berndt and Clifford]{Berndt1994UsingDT}
Berndt, D.~J. and Clifford, J.
\newblock Using dynamic time warping to find patterns in time series.
\newblock In \emph{KDD Workshop}, 1994.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill, Brynjolfsson, Buch, Card,
  Castellon, Chatterji, Chen, Creel, Davis, Demszky, Donahue, Doumbouya,
  Durmus, Ermon, Etchemendy, Ethayarajh, Fei-Fei, Finn, Gale, Gillespie, Goel,
  Goodman, Grossman, Guha, Hashimoto, Henderson, Hewitt, Ho, Hong, Hsu, Huang,
  Icard, Jain, Jurafsky, Kalluri, Karamcheti, Keeling, Khani, Khattab, Koh,
  Krass, Krishna, Kuditipudi, Kumar, Ladhak, Lee, Lee, Leskovec, Levent, Li,
  Li, Ma, Malik, Manning, Mirchandani, Mitchell, Munyikwa, Nair, Narayan,
  Narayanan, Newman, Nie, Niebles, Nilforoshan, Nyarko, Ogut, Orr,
  Papadimitriou, Park, Piech, Portelance, Potts, Raghunathan, Reich, Ren, Rong,
  Roohani, Ruiz, Ryan, Ré, Sadigh, Sagawa, Santhanam, Shih, Srinivasan,
  Tamkin, Taori, Thomas, Tramèr, Wang, Wang, Wu, Wu, Wu, Xie, Yasunaga, You,
  Zaharia, Zhang, Zhang, Zhang, Zhang, Zheng, Zhou, and
  Liang]{Bommasani2021OnTO}
Bommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
  Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E.,
  Buch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K., Davis,
  J.~Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S.,
  Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie,
  L., Goel, K., Goodman, N., Grossman, S., Guha, N., Hashimoto, T., Henderson,
  P., Hewitt, J., Ho, D.~E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S.,
  Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab,
  O., Koh, P.~W., Krass, M., Krishna, R., Kuditipudi, R., Kumar, A., Ladhak,
  F., Lee, M., Lee, T., Leskovec, J., Levent, I., Li, X.~L., Li, X., Ma, T.,
  Malik, A., Manning, C.~D., Mirchandani, S., Mitchell, E., Munyikwa, Z., Nair,
  S., Narayan, A., Narayanan, D., Newman, B., Nie, A., Niebles, J.~C.,
  Nilforoshan, H., Nyarko, J., Ogut, G., Orr, L., Papadimitriou, I., Park,
  J.~S., Piech, C., Portelance, E., Potts, C., Raghunathan, A., Reich, R., Ren,
  H., Rong, F., Roohani, Y., Ruiz, C., Ryan, J., Ré, C., Sadigh, D., Sagawa,
  S., Santhanam, K., Shih, A., Srinivasan, K., Tamkin, A., Taori, R., Thomas,
  A.~W., Tramèr, F., Wang, R.~E., Wang, W., Wu, B., Wu, J., Wu, Y., Xie,
  S.~M., Yasunaga, M., You, J., Zaharia, M., Zhang, M., Zhang, T., Zhang, X.,
  Zhang, Y., Zheng, L., Zhou, K., and Liang, P.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.

\bibitem[Bridle(1989)]{Bridle1989TrainingSM}
Bridle, J.~S.
\newblock Training stochastic model recognition algorithms as networks can lead
  to maximum mutual information estimation of parameters.
\newblock In \emph{NeurIPS}, 1989.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan,
  Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess,
  Clark, Berner, McCandlish, Radford, Sutskever, and
  Amodei]{NEURIPS2020_1457c0d6}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S.,
  Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,
  D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray,
  S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever,
  I., and Amodei, D.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2021{\natexlab{a}})Chen, Lu, Rajeswaran, Lee, Grover,
  Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decision}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock In \emph{NeurIPS}, 2021{\natexlab{a}}.

\bibitem[Chen et~al.(2021{\natexlab{b}})Chen, Lu, Rajeswaran, Lee, Grover,
  Laskin, Abbeel, Srinivas, and Mordatch]{chen2021decisiontransformer}
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P.,
  Srinivas, A., and Mordatch, I.
\newblock Decision transformer: Reinforcement learning via sequence modeling.
\newblock \emph{NeurIPS}, 2021{\natexlab{b}}.

\bibitem[Chen \& Guestrin(2016)Chen and Guestrin]{Chen2016XGBoostAS}
Chen, T. and Guestrin, C.
\newblock Xgboost: A scalable tree boosting system.
\newblock \emph{KDD}, 2016.

\bibitem[Child et~al.(2019)Child, Gray, Radford, and
  Sutskever]{Child2019GeneratingLS}
Child, R., Gray, S., Radford, A., and Sutskever, I.
\newblock Generating long sequences with sparse transformers.
\newblock \emph{arXiv preprint arXiv:1904.10509}, 2019.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarl{\'o}s, Hawkins, Davis, Mohiuddin, Kaiser, Belanger, Colwell, and
  Weller]{performer}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl{\'o}s,
  T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell,
  L.~J., and Weller, A.
\newblock Rethinking attention with performers.
\newblock \emph{ICLR}, 2021.

\bibitem[Dempster et~al.(2020)Dempster, Petitjean, and
  Webb]{Dempster2020ROCKETEF}
Dempster, A., Petitjean, F., and Webb, G.~I.
\newblock Rocket: exceptionally fast and accurate time series classification
  using random convolutional kernels.
\newblock \emph{Data Min. Knowl. Discov.}, 2020.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{Deng2009ImageNetAL}
Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{CVPR}, 2009.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and
  Toutanova]{Devlin2019BERTPO}
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{NAACL}, 2019.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy2021an}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
  Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S.,
  Uszkoreit, J., and Houlsby, N.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In \emph{ICLR}, 2021.

\bibitem[Franceschi et~al.(2019)Franceschi, Dieuleveut, and
  Jaggi]{Franceschi2019UnsupervisedSR}
Franceschi, J.-Y., Dieuleveut, A., and Jaggi, M.
\newblock Unsupervised scalable representation learning for multivariate time
  series.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Fu et~al.(2020)Fu, Kumar, Nachum, Tucker, and Levine]{Fu2020D4RLDF}
Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S.
\newblock D4rl: Datasets for deep data-driven reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2004.07219}, 2020.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{Hochreiter1997LongSM}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural Computation}, 1997.

\bibitem[Janner et~al.(2021)Janner, Li, and Levine]{janner2021sequence}
Janner, M., Li, Q., and Levine, S.
\newblock Offline reinforcement learning as one big sequence modeling problem.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{Katharopoulos2020TransformersAR}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are rnns: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{ICML}, 2020.

\bibitem[Kitaev et~al.(2020)Kitaev, Kaiser, and Levskaya]{kitaev2020reformer}
Kitaev, N., Kaiser, L., and Levskaya, A.
\newblock Reformer: The efficient transformer.
\newblock In \emph{ICLR}, 2020.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009LearningML}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Lange et~al.(2012)Lange, Gabel, and Riedmiller]{lange2012batch}
Lange, S., Gabel, T., and Riedmiller, M.
\newblock Batch reinforcement learning.
\newblock In \emph{Reinforcement learning}. 2012.

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Levine, S., Kumar, A., Tucker, G., and Fu, J.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Linsley et~al.(2018)Linsley, Kim, Veerabadran, and
  Serre]{Linsley2018LearningLS}
Linsley, D.~A., Kim, J., Veerabadran, V., and Serre, T.
\newblock Learning long-range spatial dependencies with horizontal
  gated-recurrent units.
\newblock \emph{NeurIPS}, 2018.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,
  Zettlemoyer, L., and Stoyanov, V.
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Liu et~al.(2021)Liu, Lin, Cao, Hu, Wei, Zhang, Lin, and
  Guo]{liu2021Swin}
Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S. C.-F., and Guo,
  B.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock \emph{ICCV}, 2021.

\bibitem[Lu et~al.(2021)Lu, Yao, Zhang, Zhu, Xu, Gao, Xu, Xiang, and
  Zhang]{Lu2021SOFTST}
Lu, J., Yao, J., Zhang, J., Zhu, X., Xu, H., Gao, W., Xu, C., Xiang, T., and
  Zhang, L.
\newblock Soft: Softmax-free transformer with linear complexity.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Luo et~al.(2021)Luo, Li, Cai, He, Peng, Zheng, Ke, Wang, and
  Liu]{Luo2021StableFA}
Luo, S., Li, S., Cai, T., He, D., Peng, D., Zheng, S., Ke, G., Wang, L., and
  Liu, T.-Y.
\newblock Stable, fast and accurate: Kernelized attention with relative
  positional encoding.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Maas et~al.(2011)Maas, Daly, Pham, Huang, Ng, and
  Potts]{Maas2011LearningWV}
Maas, A.~L., Daly, R.~E., Pham, P.~T., Huang, D., Ng, A., and Potts, C.
\newblock Learning word vectors for sentiment analysis.
\newblock In \emph{ACL}, 2011.

\bibitem[Merity et~al.(2017)Merity, Xiong, Bradbury, and
  Socher]{Merity2017PointerSM}
Merity, S., Xiong, C., Bradbury, J., and Socher, R.
\newblock Pointer sentinel mixture models.
\newblock \emph{ICLR}, 2017.

\bibitem[Nair et~al.(2020)Nair, Dalal, Gupta, and Levine]{nair2020awac}
Nair, A., Dalal, M., Gupta, A., and Levine, S.
\newblock Accelerating online reinforcement learning with offline datasets.
\newblock \emph{arXiv preprint arXiv:2006.09359}, 2020.

\bibitem[Nangia \& Bowman(2018)Nangia and Bowman]{Nangia2018ListOpsAD}
Nangia, N. and Bowman, S.~R.
\newblock Listops: A diagnostic dataset for latent tree learning.
\newblock In \emph{NAACL}, 2018.

\bibitem[Ott et~al.(2019)Ott, Edunov, Baevski, Fan, Gross, Ng, Grangier, and
  Auli]{ott2019fairseq}
Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and
  Auli, M.
\newblock fairseq: A fast, extensible toolkit for sequence modeling.
\newblock In \emph{NAACL-HLT}, 2019.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{Peng2021RandomFA}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.~A., and Kong, L.
\newblock Random feature attention.
\newblock In \emph{ICLR}, 2021.

\bibitem[Pomerleau(1989)]{pomerleau1989alvinn}
Pomerleau, D.~A.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock Technical report, Carnegie Melon Univ. Pittsburgh, PA. Artificial
  Intelligence and Psychology., 1989.

\bibitem[Radev et~al.(2013)Radev, Muthukrishnan, Qazvinian, and
  Abu-Jbara]{Radev2013TheAA}
Radev, D.~R., Muthukrishnan, P., Qazvinian, V., and Abu-Jbara, A.
\newblock The acl anthology network corpus.
\newblock \emph{Lang. Resour. Eval.}, 2013.

\bibitem[Rahimi \& Recht(2007)Rahimi and Recht]{Rahimi2007RandomFF}
Rahimi, A. and Recht, B.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{NeurIPS}, 2007.

\bibitem[Tay et~al.(2020{\natexlab{a}})Tay, Bahri, Metzler, Juan, Zhao, and
  Zheng]{Tay2021SynthesizerRS}
Tay, Y., Bahri, D., Metzler, D., Juan, D.-C., Zhao, Z., and Zheng, C.
\newblock Synthesizer: Rethinking self-attention in transformer models.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Tay et~al.(2020{\natexlab{b}})Tay, Bahri, Yang, Metzler, and
  Juan]{Tay2020SparseSA}
Tay, Y., Bahri, D., Yang, L., Metzler, D., and Juan, D.-C.
\newblock Sparse sinkhorn attention.
\newblock In \emph{ICML}, 2020{\natexlab{b}}.

\bibitem[Tay et~al.(2020{\natexlab{c}})Tay, Dehghani, Abnar, Shen, Bahri, Pham,
  Rao, Yang, Ruder, and Metzler]{Tay2021LongRA}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena: A benchmark for efficient transformers.
\newblock \emph{ICLR}, 2020{\natexlab{c}}.

\bibitem[Tay et~al.(2021)Tay, Dehghani, Abnar, Shen, Bahri, Pham, Rao, Yang,
  Ruder, and Metzler]{tay2021long}
Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang,
  L., Ruder, S., and Metzler, D.
\newblock Long range arena : A benchmark for efficient transformers.
\newblock In \emph{ICLR}, 2021.

\bibitem[Touvron et~al.(2021)Touvron, Cord, Douze, Massa, Sablayrolles, and
  J'egou]{Touvron2021TrainingDI}
Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J'egou, H.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock In \emph{ICML}, 2021.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{NIPS2017_3f5ee243}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, L.~u., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Vyas et~al.(2020)Vyas, Katharopoulos, and Fleuret]{Vyas2020FastTW}
Vyas, A., Katharopoulos, A., and Fleuret, F.
\newblock Fast transformers with clustered attention.
\newblock \emph{NeurIPS}, 2020.

\bibitem[Wang et~al.(2020)Wang, Li, Khabsa, Fang, and Ma]{Wang2020LinformerSW}
Wang, S., Li, B.~Z., Khabsa, M., Fang, H., and Ma, H.
\newblock Linformer: Self-attention with linear complexity.
\newblock \emph{arXiv preprint arXiv:2006.04768}, 2020.

\bibitem[Wu et~al.(2021)Wu, Xu, Wang, and Long]{wu2021autoformer}
Wu, H., Xu, J., Wang, J., and Long, M.
\newblock Autoformer: Decomposition transformers with {Auto-Correlation} for
  long-term series forecasting.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Xiong et~al.(2021)Xiong, Zeng, Chakraborty, Tan, Fung, Li, and
  Singh]{Xiong2021NystrmformerAN}
Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G.~M., Li, Y., and Singh,
  V.
\newblock Nystr{\"o}mformer: A nystr{\"o}m-based algorithm for approximating
  self-attention.
\newblock \emph{AAAI}, 2021.

\bibitem[Zaheer et~al.(2020)Zaheer, Guruganesh, Dubey, Ainslie, Alberti,
  Onta{\~n}{\'o}n, Pham, Ravula, Wang, Yang, and Ahmed]{Zaheer2020BigBT}
Zaheer, M., Guruganesh, G., Dubey, K.~A., Ainslie, J., Alberti, C.,
  Onta{\~n}{\'o}n, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A.
\newblock Big bird: Transformers for longer sequences.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Zeng et~al.(2021)Zeng, Xiong, Ravi, Acharya, Fung, and
  Singh]{zeng2021yoso}
Zeng, Z., Xiong, Y., Ravi, S.~N., Acharya, S., Fung, G., and Singh, V.
\newblock You only sample (almost) once: Linear cost self-attention via
  bernoulli sampling.
\newblock In \emph{ICML}, 2021.

\bibitem[Zerveas et~al.(2021)Zerveas, Jayaraman, Patel, Bhamidipaty, and
  Eickhoff]{Zerveas2021ATF}
Zerveas, G., Jayaraman, S., Patel, D., Bhamidipaty, A., and Eickhoff, C.
\newblock A transformer-based framework for multivariate time series
  representation learning.
\newblock \emph{KDD}, 2021.

\bibitem[Zhen et~al.(2022)Zhen, Sun, Deng, Li, Wei, Lv, Yan, Kong, and
  Zhong]{anonymous2022cosformer}
Zhen, Q., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and
  Zhong, Y.
\newblock cosformer: Rethinking softmax in attention.
\newblock In \emph{ICLR}, 2022.

\bibitem[Zhou et~al.(2021)Zhou, Zhang, Peng, Zhang, Li, Xiong, and
  Zhang]{haoyietal-informer-2021}
Zhou, H., Zhang, S., Peng, J., Zhang, S., Li, J., Xiong, H., and Zhang, W.
\newblock Informer: Beyond efficient transformer for long sequence time-series
  forecasting.
\newblock In \emph{AAAI}, 2021.

\end{thebibliography}
