\begin{thebibliography}{10}

\bibitem{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate, 2014.
\newblock cite arxiv:1409.0473Comment: Accepted at ICLR 2015 as oral
  presentation.

\bibitem{Bello_2019_ICCV}
Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc~V. Le.
\newblock Attention augmented convolutional networks.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on
  Computer Vision (ICCV)}, October 2019.

\bibitem{carion2020endtoend}
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander
  Kirillov, and Sergey Zagoruyko.
\newblock End-to-end object detection with transformers.
\newblock In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan{-}Michael
  Frahm, editors, {\em Computer Vision - {ECCV} 2020 - 16th European
  Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part {I}}, volume
  12346 of {\em Lecture Notes in Computer Science}, pages 213--229. Springer,
  2020.

\bibitem{DBLP:journals/corr/abs-1904-10509}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em CoRR}, abs/1904.10509, 2019.

\bibitem{Performer}
Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song,
  Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy
  Colwell, and Adrian Weller.
\newblock Masked language modeling for proteins via linearly scalable
  long-context transformers, 2020.

\bibitem{DBLP:journals/corr/abs-2102-10882}
Xiangxiang Chu, Bo~Zhang, Zhi Tian, Xiaolin Wei, and Huaxia Xia.
\newblock Do we really need explicit position encodings for vision
  transformers?
\newblock {\em CoRR}, abs/2102.10882, 2021.

\bibitem{dai-etal-2019-transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan
  Salakhutdinov.
\newblock Transformer-{XL}: Attentive language models beyond a fixed-length
  context.
\newblock In {\em Proceedings of the 57th Annual Meeting of the Association for
  Computational Linguistics}, pages 2978--2988, Florence, Italy, July 2019.
  Association for Computational Linguistics.

\bibitem{devlin-etal-2019-bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pages 4171--4186,
  Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

\bibitem{visiontransformer}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
  Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
  Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby.
\newblock An image is worth 16x16 words: Transformers for image recognition at
  scale.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{pmlr-v70-gehring17a}
Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann~N. Dauphin.
\newblock Convolutional sequence to sequence learning.
\newblock In Doina Precup and Yee~Whye Teh, editors, {\em Proceedings of the
  34th International Conference on Machine Learning}, volume~70 of {\em
  Proceedings of Machine Learning Research}, pages 1243--1252. PMLR, 06--11 Aug
  2017.

\bibitem{DBLP:journals/corr/abs-2104-01136}
Benjamin Graham, Alaaeldin El{-}Nouby, Hugo Touvron, Pierre Stock, Armand
  Joulin, Herv{\'{e}} J{\'{e}}gou, and Matthijs Douze.
\newblock Levit: a vision transformer in convnet's clothing for faster
  inference.
\newblock {\em ICCV 2021}, abs/2104.01136, 2021.

\bibitem{Hamid14}
Raffay Hamid, Ying Xiao, Alex Gittens, and Dennis DeCoste.
\newblock Compact random feature maps.
\newblock In {\em Proceedings of the 31st International Conference on
  International Conference on Machine Learning - Volume 32}, ICML'14, 2014.

\bibitem{he2021deberta}
Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen.
\newblock Deberta: Decoding-enhanced bert with disentangled attention, 2021.

\bibitem{lstm}
Sepp Hochreiter and Jürgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9:1735--80, 12 1997.

\bibitem{DBLP:journals/corr/abs-1809-04281}
Cheng{-}Zhi~Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Curtis
  Hawthorne, Andrew~M. Dai, Matthew~D. Hoffman, and Douglas Eck.
\newblock An improved relative self-attention mechanism for transformer with
  application to music generation.
\newblock {\em CoRR}, abs/1809.04281, 2018.

\bibitem{kitaev2020reformer}
Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.
\newblock Reformer: The efficient transformer.
\newblock In {\em ICLR}, 2020.

\bibitem{Quoc14}
Quoc Le, Tamas Sarlos, and Alexander Smola.
\newblock Fastfood: Approximate kernel expansions in loglinear time.
\newblock {\em 30th International Conference on Machine Learning, ICML 2013},
  2014.

\bibitem{cnn}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural Comput.}, 1(4):541–551, December 1989.

\bibitem{li-etal-2020-mapping}
Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason Baldridge.
\newblock Mapping natural language instructions to mobile {UI} action
  sequences.
\newblock In {\em Proceedings of the 58th Annual Meeting of the Association for
  Computational Linguistics}, pages 8198--8210, Online, July 2020. Association
  for Computational Linguistics.

\bibitem{li-etal-2020-widget}
Yang Li, Gang Li, Luheng He, Jingjie Zheng, Hong Li, and Zhiwei Guan.
\newblock Widget captioning: Generating natural language description for mobile
  user interface elements.
\newblock In {\em Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing (EMNLP)}, pages 5495--5510, Online, November
  2020. Association for Computational Linguistics.

\bibitem{Li_Zhang_Wang_Kumar_2019}
Yanjun Li, Kai Zhang, Jun Wang, and Sanjiv Kumar.
\newblock Learning adaptive random features.
\newblock {\em Proceedings of the AAAI Conference on Artificial Intelligence},
  33(01):4229--4236, Jul. 2019.

\bibitem{DBLP:conf/eccv/LinMBHPRDZ14}
Tsung{-}Yi Lin, Michael Maire, Serge~J. Belongie, James Hays, Pietro Perona,
  Deva Ramanan, Piotr Doll{\'{a}}r, and C.~Lawrence Zitnick.
\newblock Microsoft {COCO:} common objects in context.
\newblock In David~J. Fleet, Tom{\'{a}}s Pajdla, Bernt Schiele, and Tinne
  Tuytelaars, editors, {\em Computer Vision - {ECCV} 2014 - 13th European
  Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part
  {V}}, volume 8693 of {\em Lecture Notes in Computer Science}, pages 740--755.
  Springer, 2014.

\bibitem{Liu2020LearningTE}
Xuanqing Liu, Hsiang-Fu Yu, I.~Dhillon, and Cho-Jui Hsieh.
\newblock Learning to encode position for transformer with continuous dynamical
  model.
\newblock In {\em ICML}, 2020.

\bibitem{pmlr-v139-liutkus21a}
Antoine Liutkus, Ond{\v{r}}ej C\'{\i}fka, Shih-Lun Wu, Umut Simsekli, Yi-Hsuan
  Yang, and Gael Richard.
\newblock Relative positional encoding for transformers with linear complexity.
\newblock In Marina Meila and Tong Zhang, editors, {\em Proceedings of the 38th
  International Conference on Machine Learning}, volume 139 of {\em Proceedings
  of Machine Learning Research}, pages 7067--7079. PMLR, 18--24 Jul 2021.

\bibitem{luong-etal-2015-effective}
Thang Luong, Hieu Pham, and Christopher~D. Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock In {\em Proceedings of the 2015 Conference on Empirical Methods in
  Natural Language Processing}, pages 1412--1421, Lisbon, Portugal, September
  2015. Association for Computational Linguistics.

\bibitem{neishi-yoshinaga-2019-relation}
Masato Neishi and Naoki Yoshinaga.
\newblock On the relation between position information and sentence length in
  neural machine translation.
\newblock In {\em Proceedings of the 23rd Conference on Computational Natural
  Language Learning (CoNLL)}, pages 328--338, Hong Kong, China, November 2019.
  Association for Computational Linguistics.

\bibitem{NEURIPS2019_383beaea}
Andrei Nicolicioiu, Iulia Duta, and Marius Leordeanu.
\newblock Recurrent space-time graph neural networks.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{Parascandolo2017TamingTW}
Giambattista Parascandolo, Heikki Huttunen, and Tuomas Virtanen.
\newblock Taming the waves: sine as activation function in deep neural
  networks.
\newblock 2017.

\bibitem{pmlr-v80-parmar18a}
Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer,
  Alexander Ku, and Dustin Tran.
\newblock Image transformer.
\newblock In Jennifer Dy and Andreas Krause, editors, {\em Proceedings of the
  35th International Conference on Machine Learning}, volume~80 of {\em
  Proceedings of Machine Learning Research}, pages 4055--4064,
  Stockholmsmässan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem{RFF07}
Ali Rahimi and Benjamin Recht.
\newblock Random features for large-scale kernel machines.
\newblock In {\em Advances in Neural Information Processing Systems 20,
  Proceedings of the Twenty-First Annual Conference on Neural Information
  Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007},
  pages 1177--1184, 2007.

\bibitem{RahimiR08}
Ali Rahimi and Benjamin Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In {\em Advances in Neural Information Processing Systems 21,
  Proceedings of the Twenty-Second Annual Conference on Neural Information
  Processing Systems, Vancouver, British Columbia, Canada, December 8-11,
  2008}, pages 1313--1320, 2008.

\bibitem{NEURIPS2019_3416a75f}
Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya,
  and Jon Shlens.
\newblock Stand-alone self-attention in vision models.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32. Curran Associates, Inc., 2019.

\bibitem{shaw-etal-2018-self}
Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani.
\newblock Self-attention with relative position representations.
\newblock In {\em Proceedings of the 2018 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 2 (Short Papers)}, pages 464--468, New Orleans,
  Louisiana, June 2018. Association for Computational Linguistics.

\bibitem{DBLP:journals/corr/abs-1808-03314}
Alex Sherstinsky.
\newblock Fundamentals of recurrent neural network {(RNN)} and long short-term
  memory {(LSTM)} network.
\newblock {\em CoRR}, abs/1808.03314, 2018.

\bibitem{shiv2019novel}
Vighnesh~Leonardo Shiv and Chris Quirk.
\newblock Novel positional encodings to enable tree-based transformers.
\newblock In {\em NeurIPS 2019}, 2019.

\bibitem{Tewari18}
Yitong Sun, Anna Gilbert, and Ambuj Tewari.
\newblock But how does it work in theory? linear svm with random features.
\newblock In {\em Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, NIPS'18, page 3383–3392, 2018.

\bibitem{touvron2020training}
Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre
  Sablayrolles, and Herv{\'e} J{\'e}gou.
\newblock Training data-efficient image transformers \& distillation through
  attention.
\newblock {\em arXiv preprint arXiv:2012.12877}, 2020.

\bibitem{DBLP:journals/corr/VaswaniSPUJGKP17}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock {\em CoRR}, abs/1706.03762, 2017.

\bibitem{wang2021on}
Benyou Wang, Lifeng Shang, Christina Lioma, Xin Jiang, Hao Yang, Qun Liu, and
  Jakob~Grue Simonsen.
\newblock On position embeddings in {\{}bert{\}}.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{DBLP:journals/corr/abs-1912-12333}
Benyou Wang, Donghao Zhao, Christina Lioma, Qiuchi Li, Peng Zhang, and
  Jakob~Grue Simonsen.
\newblock Encoding word order in complex embeddings.
\newblock In {\em 8th International Conference on Learning Representations,
  {ICLR} 2020, Addis Ababa, Ethiopia, April 26-30, 2020}. OpenReview.net, 2020.

\bibitem{wang-etal-2019-self}
Xing Wang, Zhaopeng Tu, Longyue Wang, and Shuming Shi.
\newblock Self-attention with structural position representations.
\newblock In {\em Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 1403--1409, Hong Kong,
  China, November 2019. Association for Computational Linguistics.

\bibitem{Yang14}
Jiyan Yang, Vikas Sindhwani, Haim Avron, and Michael Mahoney.
\newblock Quasi-monte carlo feature maps for shift-invariant kernels.
\newblock {\em 31st International Conference on Machine Learning, ICML 2014},
  1, 12 2014.

\bibitem{NEURIPS2019_dc6a7e65}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ~R Salakhutdinov,
  and Quoc~V Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, {\em Advances in Neural
  Information Processing Systems}, volume~32, pages 5753--5763. Curran
  Associates, Inc., 2019.

\end{thebibliography}
