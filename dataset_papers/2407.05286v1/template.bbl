\begin{thebibliography}{46}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Fang et~al.(2018)Fang, Li, Lin, and Zhang]{fang2018spider}
Cong Fang, Chris~Junchi Li, Zhouchen Lin, and Tong Zhang.
\newblock Spider: Near-optimal non-convex optimization via stochastic path-integrated differential estimator.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Zhou et~al.(2020)Zhou, Xu, and Gu]{zhoustochastic}
Dongruo Zhou, Pan Xu, and Quanquan Gu.
\newblock Stochastic nested variance reduction for nonconvex optimization.
\newblock \emph{The Journal of Machine Learning Research}, 21\penalty0 (1):\penalty0 4130--4192, 2020.

\bibitem[Wen et~al.(2018)Wen, Vicol, Ba, Tran, and Grosse]{wen2018flipout}
Yeming Wen, Paul Vicol, Jimmy Ba, Dustin Tran, and Roger Grosse.
\newblock Flipout: Efficient pseudo-independent weight perturbations on mini-batches.
\newblock \emph{arXiv preprint arXiv:1803.04386}, 2018.

\bibitem[Qi et~al.(2021)Qi, Luo, Xu, Ji, and Yang]{qi2021stochastic}
Qi~Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, and Tianbao Yang.
\newblock Stochastic optimization of areas under precision-recall curves with provable convergence.
\newblock \emph{Advances in neural information processing systems}, 34:\penalty0 1752--1765, 2021.

\bibitem[Liu et~al.(2019)Liu, Jiang, He, Chen, Liu, Gao, and Han]{liu2019variance}
Liyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei Han.
\newblock On the variance of the adaptive learning rate and beyond.
\newblock \emph{arXiv preprint arXiv:1908.03265}, 2019.

\bibitem[Liu et~al.(2024)Liu, Pan, Duan, Li, Li, and Qu]{liu2023breaking}
Jin Liu, Xiaokang Pan, Junwen Duan, Hong-Dong Li, Youqi Li, and Zhe Qu.
\newblock Faster stochastic variance reduction methods for compositional minimax optimization.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, volume~38, pages 13927--13935, 2024.

\bibitem[Cutkosky and Orabona(2019)]{cutkosky2019momentum}
Ashok Cutkosky and Francesco Orabona.
\newblock Momentum-based variance reduction in non-convex sgd.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Hu et~al.(2019)Hu, Li, Lian, Liu, and Yuan]{hu2019efficient}
Wenqing Hu, Chris~Junchi Li, Xiangru Lian, Ji~Liu, and Huizhuo Yuan.
\newblock Efficient smooth non-convex stochastic compositional optimization via stochastic recursive gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Mao et~al.(2022)Mao, Yang, Zhang, and Basar]{mao2022improving}
Weichao Mao, Lin Yang, Kaiqing Zhang, and Tamer Basar.
\newblock On improving model-free algorithms for decentralized multi-agent reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages 15007--15049. PMLR, 2022.

\bibitem[Ji et~al.(2022)Ji, Yang, and Liang]{ji2022theoretical}
Kaiyi Ji, Junjie Yang, and Yingbin Liang.
\newblock Theoretical convergence of multi-step model-agnostic meta-learning.
\newblock \emph{The Journal of Machine Learning Research}, 23\penalty0 (1):\penalty0 1317--1357, 2022.

\bibitem[Qu et~al.(2023{\natexlab{a}})Qu, Li, Han, Duan, Shen, and Chen]{qu2023prevent}
Zhe Qu, Xingyu Li, Xiao Han, Rui Duan, Chengchao Shen, and Lixing Chen.
\newblock How to prevent the poor performance clients for personalized federated learning?
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 12167--12176, 2023{\natexlab{a}}.

\bibitem[Tran~Dinh et~al.(2020)Tran~Dinh, Liu, and Nguyen]{tran2020hybrid}
Quoc Tran~Dinh, Deyi Liu, and Lam Nguyen.
\newblock Hybrid variance-reduced sgd algorithms for minimax problems with nonconvex-linear function.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 11096--11107, 2020.

\bibitem[Jiang et~al.(2022)Jiang, Wang, Wang, Zhang, and Yang]{jiang2022optimal}
Wei Jiang, Bokun Wang, Yibo Wang, Lijun Zhang, and Tianbao Yang.
\newblock Optimal algorithms for stochastic multi-level compositional optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 10195--10216. PMLR, 2022.

\bibitem[Yuan et~al.(2021)Yuan, Guo, Chawla, and Yang]{yuan2021compositional}
Zhuoning Yuan, Zhishuai Guo, Nitesh Chawla, and Tianbao Yang.
\newblock Compositional training for end-to-end deep auc maximization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Chen et~al.(2021)Chen, Sun, and Yin]{chen2021solving}
Tianyi Chen, Yuejiao Sun, and Wotao Yin.
\newblock Solving stochastic compositional optimization is nearly as easy as solving stochastic optimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 69:\penalty0 4937--4948, 2021.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Qu, Tang, and Lu]{li2023fedlga}
Xingyu Li, Zhe Qu, Bo~Tang, and Zhuo Lu.
\newblock Fedlga: Toward system-heterogeneity of federated learning via local gradient approximation.
\newblock \emph{IEEE Transactions on Cybernetics}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2017)Wang, Fang, and Liu]{wang2017stochastic}
Mengdi Wang, Ethan~X Fang, and Han Liu.
\newblock Stochastic compositional gradient descent: algorithms for minimizing compositions of expected-value functions.
\newblock \emph{Mathematical Programming}, 161:\penalty0 419--449, 2017.

\bibitem[Ghadimi et~al.(2020)Ghadimi, Ruszczynski, and Wang]{ghadimi2020single}
Saeed Ghadimi, Andrzej Ruszczynski, and Mengdi Wang.
\newblock A single timescale stochastic approximation method for nested stochastic optimization.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (1):\penalty0 960--979, 2020.

\bibitem[Dann et~al.(2014)Dann, Neumann, Peters, et~al.]{dann2014policy}
Christoph Dann, Gerhard Neumann, Jan Peters, et~al.
\newblock Policy evaluation with temporal differences: A survey and comparison.
\newblock \emph{Journal of Machine Learning Research}, 15:\penalty0 809--883, 2014.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient descent.
\newblock In \emph{International conference on machine learning}, pages 1225--1234. PMLR, 2016.

\bibitem[Yang et~al.(2023)Yang, Wei, Yang, and Ying]{yang2023stability}
Ming Yang, Xiyuan Wei, Tianbao Yang, and Yiming Ying.
\newblock Stability and generalization of stochastic compositional gradient descent algorithms.
\newblock \emph{arXiv preprint arXiv:2307.03357}, 2023.

\bibitem[Yuan et~al.(2019)Yuan, Lian, and Liu]{yuan2019stochastic}
Huizhuo Yuan, Xiangru Lian, and Ji~Liu.
\newblock Stochastic recursive variance reduction for efficient smooth non-convex compositional optimization.
\newblock \emph{arXiv preprint arXiv:1912.13515}, 2019.

\bibitem[Balasubramanian et~al.(2022)Balasubramanian, Ghadimi, and Nguyen]{balasubramanian2022stochastic}
Krishnakumar Balasubramanian, Saeed Ghadimi, and Anthony Nguyen.
\newblock Stochastic multilevel composition optimization algorithms with level-independent convergence rates.
\newblock \emph{SIAM Journal on Optimization}, 32\penalty0 (2):\penalty0 519--544, 2022.

\bibitem[Qu et~al.(2023{\natexlab{b}})Qu, Li, Xu, Tang, Lu, and Liu]{qu2023convergence}
Zhe Qu, Xingyu Li, Jie Xu, Bo~Tang, Zhuo Lu, and Yao Liu.
\newblock On the convergence of multi-server federated learning with overlapping area.
\newblock \emph{IEEE Transactions on Mobile Computing}, 22\penalty0 (11):\penalty0 6647--6662, 2023{\natexlab{b}}.

\bibitem[Kearns and Ron(1997)]{kearns1997algorithmic}
Michael Kearns and Dana Ron.
\newblock Algorithmic stability and sanity-check bounds for leave-one-out cross-validation.
\newblock In \emph{Proceedings of the tenth annual conference on Computational learning theory}, pages 152--162, 1997.

\bibitem[Vapnik and Chapelle(2000)]{vapnik2000bounds}
Vladimir Vapnik and Olivier Chapelle.
\newblock Bounds on error expectation for support vector machines.
\newblock \emph{Neural computation}, 12\penalty0 (9):\penalty0 2013--2036, 2000.

\bibitem[Cucker and Smale(2002)]{cucker2002mathematical}
Felipe Cucker and Steve Smale.
\newblock On the mathematical foundations of learning.
\newblock \emph{Bulletin of the American mathematical society}, 39\penalty0 (1):\penalty0 1--49, 2002.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0 499--526, 2002.

\bibitem[Cesa-Bianchi et~al.(2004)Cesa-Bianchi, Conconi, and Gentile]{cesa2004generalization}
Nicolo Cesa-Bianchi, Alex Conconi, and Claudio Gentile.
\newblock On the generalization ability of on-line learning algorithms.
\newblock \emph{IEEE Transactions on Information Theory}, 50\penalty0 (9):\penalty0 2050--2057, 2004.

\bibitem[Rakhlin et~al.(2005)Rakhlin, Mukherjee, and Poggio]{rakhlin2005stability}
Alexander Rakhlin, Sayan Mukherjee, and Tomaso Poggio.
\newblock Stability results in learning theory.
\newblock \emph{Analysis and Applications}, 3\penalty0 (04):\penalty0 397--417, 2005.

\bibitem[Kutin and Niyogi(2012)]{kutin2012almost}
Samuel Kutin and Partha Niyogi.
\newblock Almost-everywhere algorithmic stability and generalization error.
\newblock \emph{arXiv preprint arXiv:1301.0579}, 2012.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0 (Nov):\penalty0 463--482, 2002.

\bibitem[Poggio et~al.(2004)Poggio, Rifkin, Mukherjee, and Niyogi]{poggio2004general}
Tomaso Poggio, Ryan Rifkin, Sayan Mukherjee, and Partha Niyogi.
\newblock General conditions for predictivity in learning theory.
\newblock \emph{Nature}, 428\penalty0 (6981):\penalty0 419--422, 2004.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and Sridharan]{shalev2010learnability}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0 2635--2670, 2010.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Ildiz, Papailiopoulos, and Oymak]{li2023transformers}
Yingcong Li, Muhammed~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
\newblock Transformers as algorithms: Generalization and stability in in-context learning.
\newblock In \emph{International Conference on Machine Learning}, pages 19565--19594. PMLR, 2023{\natexlab{b}}.

\bibitem[Sakaue and Oki(2023)]{sakaue2023improved}
Shinsaku Sakaue and Taihei Oki.
\newblock Improved generalization bound and learning of sparsity patterns for data-driven low-rank approximation.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 1--10. PMLR, 2023.

\bibitem[Zhang and Xiao(2019)]{zhang2019composite}
Junyu Zhang and Lin Xiao.
\newblock A composite randomized incremental gradient method.
\newblock In \emph{International Conference on Machine Learning}, pages 7454--7462. PMLR, 2019.

\bibitem[Zhang and Xiao(2021)]{zhang2021multilevel}
Junyu Zhang and Lin Xiao.
\newblock Multilevel composite stochastic optimization via nested variance reduction.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (2):\penalty0 1131--1157, 2021.

\bibitem[Tarzanagh et~al.(2022)Tarzanagh, Li, Thrampoulidis, and Oymak]{tarzanagh2022fednest}
Davoud~Ataee Tarzanagh, Mingchen Li, Christos Thrampoulidis, and Samet Oymak.
\newblock Fednest: Federated bilevel, minimax, and compositional optimization.
\newblock In \emph{International Conference on Machine Learning}, pages 21146--21179. PMLR, 2022.

\bibitem[Yang et~al.(2019)Yang, Wang, and Fang]{yang2019multilevel}
Shuoguang Yang, Mengdi Wang, and Ethan~X Fang.
\newblock Multilevel stochastic gradient methods for nested composition optimization.
\newblock \emph{SIAM Journal on Optimization}, 29\penalty0 (1):\penalty0 616--659, 2019.

\bibitem[James et~al.(2013)James, Witten, Hastie, Tibshirani, et~al.]{james2013introduction}
Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, et~al.
\newblock \emph{An introduction to statistical learning}, volume 112.
\newblock Springer, 2013.

\bibitem[Bousquet et~al.(2020)Bousquet, Klochkov, and Zhivotovskiy]{bousquet2020sharper}
Olivier Bousquet, Yegor Klochkov, and Nikita Zhivotovskiy.
\newblock Sharper bounds for uniformly stable algorithms.
\newblock In \emph{Conference on Learning Theory}, pages 610--626. PMLR, 2020.

\bibitem[Levy et~al.(2021)Levy, Kavis, and Cevher]{levy2021storm+}
Kfir~Y Levy, Ali Kavis, and Volkan Cevher.
\newblock Storm+: Fully adaptive sgd with momentum for nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:2111.01040}, 2021.

\bibitem[Hu et~al.(2023)Hu, Zhu, and Yang]{hu2023non}
Quanqi Hu, Dixian Zhu, and Tianbao Yang.
\newblock Non-smooth weakly-convex finite-sum coupled compositional optimization.
\newblock \emph{arXiv preprint arXiv:2310.03234}, 2023.

\bibitem[Charles and Papailiopoulos(2018)]{charles2018stability}
Zachary Charles and Dimitris Papailiopoulos.
\newblock Stability and generalization of learning algorithms that converge to global optima.
\newblock In \emph{International conference on machine learning}, pages 745--754. PMLR, 2018.

\bibitem[Zhang et~al.(2021)Zhang, Hong, Wang, and Zhang]{zhang2021generalization}
Junyu Zhang, Mingyi Hong, Mengdi Wang, and Shuzhong Zhang.
\newblock Generalization bounds for stochastic saddle point problems.
\newblock In \emph{International Conference on Artificial Intelligence and Statistics}, pages 568--576. PMLR, 2021.

\end{thebibliography}
