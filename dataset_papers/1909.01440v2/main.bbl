\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2017)Achille, Rovere, and
  Soatto]{achille-2017-arXiv-critical-learning-periods}
Alessandro Achille, Matteo Rovere, and Stefano Soatto.
\newblock Critical learning periods in deep neural networks.
\newblock \emph{CoRR}, abs/1711.08856, 2017.
\newblock URL \url{http://arxiv.org/abs/1711.08856}.

\bibitem[{Alain} and
  {Bengio}(2016)]{alain-2016-arXiv-understanding-intermediate-layers}
G.~{Alain} and Y.~{Bengio}.
\newblock {Understanding intermediate layers using linear classifier probes}.
\newblock \emph{ArXiv e-prints}, October 2016.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Arous, and
  LeCun]{choromanska2015loss}
Anna Choromanska, Mikael Henaff, Michael Mathieu, G{\'e}rard~Ben Arous, and
  Yann LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 192--204,
  2015.

\bibitem[Dauphin et~al.(2014)Dauphin, Pascanu, Gulcehre, Cho, Ganguli, and
  Bengio]{dauphin2014identifying}
Yann~N Dauphin, Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, Surya Ganguli,
  and Yoshua Bengio.
\newblock Identifying and attacking the saddle point problem in
  high-dimensional non-convex optimization.
\newblock In \emph{Advances in neural information processing systems}, pages
  2933--2941, 2014.

\bibitem[Frankle and
  Carbin(2019)]{frankle-2019-ICLR-the-lottery-ticket-hypothesis}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, volume abs/1803.03635, 2019.
\newblock URL \url{http://arxiv.org/abs/1803.03635}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Vinyals, and
  Saxe]{goodfellow2014qualitatively}
Ian~J Goodfellow, Oriol Vinyals, and Andrew~M Saxe.
\newblock Qualitatively characterizing neural network optimization problems.
\newblock \emph{arXiv preprint arXiv:1412.6544}, 2014.

\bibitem[Gotmare et~al.(2019)Gotmare, Keskar, Xiong, and
  Socher]{gotmare2018heuristics}
Akhilesh Gotmare, Nitish~Shirish Keskar, Caiming Xiong, and Richard Socher.
\newblock A closer look at deep learning heuristics: Learning rate restarts,
  warmup and distillation.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=r14EOsCqKX}.

\bibitem[He et~al.(2015{\natexlab{a}})He, Zhang, Ren, and
  Sun]{he-2015-arXiv-deep-residual-learning}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock \emph{CoRR}, abs/1512.03385, 2015{\natexlab{a}}.
\newblock URL \url{http://arxiv.org/abs/1512.03385}.

\bibitem[He et~al.(2015{\natexlab{b}})He, Zhang, Ren, and Sun]{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015{\natexlab{b}}.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, Krizhevsky, Sutskever, and
  Salakhutdinov]{hinton2012improving-neural-networks-by-preventing}
Geoffrey~E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock \emph{arXiv preprint arXiv:1207.0580}, 2012.

\bibitem[Hoffer et~al.(2018)Hoffer, Hubara, and Soudry]{hoffer2018lastlayer}
Elad Hoffer, Itay Hubara, and Daniel Soudry.
\newblock Fix your classifier: the marginal value of training the last weight
  layer.
\newblock \emph{CoRR}, abs/1801.04540, 2018.
\newblock URL \url{http://arxiv.org/abs/1801.04540}.

\bibitem[Ioffe and
  Szegedy(2015)]{ioffe-2015-arXiv-batch-normalization:-accelerating}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{CoRR}, abs/1502.03167, 2015.
\newblock URL \url{http://arxiv.org/abs/1502.03167}.

\bibitem[{Jastrz{\k{e}}bski} et~al.(2019){Jastrz{\k{e}}bski}, {Kenton},
  {Ballas}, {Fischer}, {Bengio}, and
  {Storkey}]{jastrzebski-2019-ICLR-on-the-relation-between-the-sharpest}
Stanis{\l}aw {Jastrz{\k{e}}bski}, Zachary {Kenton}, Nicolas {Ballas}, Asja
  {Fischer}, Yoshua {Bengio}, and Amos {Storkey}.
\newblock {On the Relation Between the Sharpest Directions of DNN Loss and the
  SGD Step Length}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, page arXiv:1807.05031, Jul 2019.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Kirkpatrick et~al.(2017)Kirkpatrick, Pascanu, Rabinowitz, Veness,
  Desjardins, Rusu, Milan, Quan, Ramalho, Grabska-Barwinska, Hassabis, Clopath,
  Kumaran, and
  Hadsell]{kirkpatrick-2017-PNAS-overcoming-catastrophic-forgetting}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and
  Raia Hadsell.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 114\penalty0
  (13):\penalty0 3521--3526, 2017.
\newblock \doi{10.1073/pnas.1611835114}.
\newblock URL \url{http://www.pnas.org/content/114/13/3521.abstract}.

\bibitem[Kutta(1901)]{kutta-1901-beitrag-zur-naherungweisen-integration}
Wilhelm Kutta.
\newblock Beitrag zur n{\"a}herungweisen integration totaler
  differentialgleichungen.
\newblock 1901.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun-1998-IEEE-gradient-based-learning-applied}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[{Li} et~al.(2018){Li}, {Farkhoor}, {Liu}, and
  {Yosinski}]{li-2018-ICLR-measuring-the-intrinsic-dimension}
Chunyuan {Li}, Heerad {Farkhoor}, Rosanne {Liu}, and Jason {Yosinski}.
\newblock {Measuring the Intrinsic Dimension of Objective Landscapes}.
\newblock In \emph{International Conference on Learning Representations}, April
  2018.

\bibitem[Li et~al.(2018)Li, Xu, Taylor, Studer, and
  Goldstein]{li2018visualizing}
Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom Goldstein.
\newblock Visualizing the loss landscape of neural nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6389--6399, 2018.

\bibitem[Nguyen and Hein(2017)]{nguyen2017loss}
Quynh Nguyen and Matthias Hein.
\newblock The loss surface of deep and wide neural networks.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 2603--2612. JMLR. org, 2017.

\bibitem[{Raghu} et~al.(2017){Raghu}, {Gilmer}, {Yosinski}, and
  {Sohl-Dickstein}]{raghu-2017-arXiv-svcca:-singular-vector}
M.~{Raghu}, J.~{Gilmer}, J.~{Yosinski}, and J.~{Sohl-Dickstein}.
\newblock Svcca: Singular vector canonical correlation analysis for deep
  learning dynamics and interpretability.
\newblock \emph{ArXiv e-prints}, June 2017.

\bibitem[Runge(1895)]{runge-1895-uber-die-numerische-auflosung}
Carl Runge.
\newblock {\"U}ber die numerische aufl{\"o}sung von differentialgleichungen.
\newblock \emph{Mathematische Annalen}, 46\penalty0 (2):\penalty0 167--178,
  1895.

\bibitem[Safran and Shamir(2016)]{safran2016quality}
Itay Safran and Ohad Shamir.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  774--782, 2016.

\bibitem[Shwartz{-}Ziv and
  Tishby(2017)]{shwartz-ziv-2017-arXiv-opening-the-black-box-of-deep}
Ravid Shwartz{-}Ziv and Naftali Tishby.
\newblock Opening the black box of deep neural networks via information.
\newblock \emph{CoRR}, abs/1703.00810, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.00810}.

\bibitem[Simonyan and
  Zisserman(2014)]{simonyan-2014-arXiv-very-deep-convolutional}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock \emph{CoRR}, abs/1409.1556, 2014.
\newblock URL \url{http://arxiv.org/abs/1409.1556}.

\bibitem[Simonyan et~al.(2013)Simonyan, Vedaldi, and
  Zisserman]{simonyan2013deep-inside-convolutional}
Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock \emph{arXiv preprint arXiv:1312.6034, presented at ICLR Workshop
  2014}, 2013.

\bibitem[Soudry and Carmon(2016)]{soudry2016no}
Daniel Soudry and Yair Carmon.
\newblock No bad local minima: Data independent training error guarantees for
  multilayer neural networks.
\newblock \emph{arXiv preprint arXiv:1605.08361}, 2016.

\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg-2014-arXiv-striving-for-simplicity:-the-all-convolutional}
Jost~Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin~A.
  Riedmiller.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{CoRR}, abs/1412.6806, 2014.
\newblock URL \url{http://arxiv.org/abs/1412.6806}.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and
  Hinton]{sutskever2013importance}
Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International conference on machine learning}, pages
  1139--1147, 2013.

\bibitem[Weisstein(2003)]{weisstein-2003-simpsons-rule}
Eric~W Weisstein.
\newblock Simpson's rule.
\newblock 2003.

\bibitem[Xing et~al.(2018)Xing, Arpit, Tsirigotis, and Bengio]{xing2018walk}
Chen Xing, Devansh Arpit, Christos Tsirigotis, and Yoshua Bengio.
\newblock A walk with sgd.
\newblock 2018.

\bibitem[{Yosinski} et~al.(2015){Yosinski}, {Clune}, {Nguyen}, {Fuchs}, and
  {Lipson}]{yosinski-2015-arXiv-understanding-neural-networks}
J.~{Yosinski}, J.~{Clune}, A.~{Nguyen}, T.~{Fuchs}, and H.~{Lipson}.
\newblock {Understanding Neural Networks Through Deep Visualization}.
\newblock \emph{ArXiv e-prints}, June 2015.

\bibitem[Zenke et~al.(2017)Zenke, Poole, and
  Ganguli]{zenke-2017-arXiv-improved-multitask-learning}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Improved multitask learning through synaptic intelligence.
\newblock \emph{CoRR}, abs/1703.04200, 2017.
\newblock URL \url{http://arxiv.org/abs/1703.04200}.

\bibitem[Zhang et~al.(2019)Zhang, Bengio, and Singer]{zhang2019all}
Chiyuan Zhang, Samy Bengio, and Yoram Singer.
\newblock Are all layers created equal?
\newblock \emph{arXiv preprint arXiv:1902.01996}, 2019.

\bibitem[Zhou et~al.(2019)Zhou, Lan, Liu, and Yosinski]{zhou2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock \emph{arXiv preprint arXiv:1905.01067}, 2019.

\end{thebibliography}
