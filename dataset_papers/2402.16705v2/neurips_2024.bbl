\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achiam et~al.(2023)Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, et~al.]{achiam2023gpt}
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia~Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et~al.
\newblock Gpt-4 technical report.
\newblock \emph{ArXiv preprint}, abs/2303.08774, 2023.
\newblock URL \url{https://arxiv.org/abs/2303.08774}.

\bibitem[Cao et~al.(2023)Cao, Kang, and Sun]{cao2023instruction}
Yihan Cao, Yanbin Kang, and Lichao Sun.
\newblock Instruction mining: High-quality instruction data selection for large language models.
\newblock \emph{ArXiv preprint}, abs/2307.06290, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.06290}.

\bibitem[Chen et~al.(2024)Chen, Li, Yan, Wang, Gunaratna, Yadav, Tang, Srinivasan, Zhou, Huang, and Jin]{chen2023alpagasus}
Lichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin.
\newblock Alpagasus: Training a better alpaca with fewer data.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=FdVXgSJhvz}.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards, Burda, Joseph, Brockman, et~al.]{chen2021evaluating}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{ArXiv preprint}, abs/2107.03374, 2021.
\newblock URL \url{https://arxiv.org/abs/2107.03374}.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Roberts, Bhatia, WANG, Zhang, Sala, and Re]{chen2024skill}
Mayee~F Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce~Zhang, Frederic Sala, and Christopher Re.
\newblock Skill-it! a data-driven skills framework for understanding and training language models.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=IoizwO1NLf}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Liu, Meng, Chen, Xu, and Zhou]{chen2023improving}
Yijie Chen, Yijin Liu, Fandong Meng, Yufeng Chen, Jinan Xu, and Jie Zhou.
\newblock Improving translation faithfulness of large language models via augmenting instructions.
\newblock \emph{ArXiv preprint}, abs/2308.12674, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2308.12674}.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E. Gonzalez, Ion Stoica, and Eric~P. Xing.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{ArXiv preprint}, abs/2210.11416, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.11416}.

\bibitem[Cobbe et~al.(2021)Cobbe, Kosaraju, Bavarian, Chen, Jun, Kaiser, Plappert, Tworek, Hilton, Nakano, et~al.]{cobbe2021training}
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et~al.
\newblock Training verifiers to solve math word problems.
\newblock \emph{ArXiv preprint}, abs/2110.14168, 2021.
\newblock URL \url{https://arxiv.org/abs/2110.14168}.

\bibitem[Costa-juss{\`a} et~al.(2022)Costa-juss{\`a}, Cross, {\c{C}}elebi, Elbayad, Heafield, Heffernan, Kalbassi, Lam, Licht, Maillard, et~al.]{costa2022no}
Marta~R Costa-juss{\`a}, James Cross, Onur {\c{C}}elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et~al.
\newblock No language left behind: Scaling human-centered machine translation.
\newblock \emph{ArXiv preprint}, abs/2207.04672, 2022.
\newblock URL \url{https://arxiv.org/abs/2207.04672}.

\bibitem[Dubois et~al.(2023)Dubois, Li, Taori, Zhang, Gulrajani, Ba, Guestrin, Liang, and Hashimoto]{dubois2023Alpacafarm}
Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto.
\newblock Alpacafarm: A simulation framework for methods that learn from human feedback.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=4hturzLcKX}.

\bibitem[Gao et~al.(2024)Gao, He, Wu, and Wang]{gao2024towards}
Pengzhi Gao, Zhongjun He, Hua Wu, and Haifeng Wang.
\newblock Towards boosting many-to-many multilingual machine translation with large language models.
\newblock \emph{ArXiv preprint}, abs/2401.05861, 2024.
\newblock URL \url{https://arxiv.org/abs/2401.05861}.

\bibitem[Gururangan et~al.(2020)Gururangan, Marasovi{\'c}, Swayamdipta, Lo, Beltagy, Downey, and Smith]{gururangan2020don}
Suchin Gururangan, Ana Marasovi{\'c}, Swabha Swayamdipta, Kyle Lo, Iz~Beltagy, Doug Downey, and Noah~A. Smith.
\newblock Don{'}t stop pretraining: Adapt language models to domains and tasks.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  8342--8360, Online, 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.740}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.740}.

\bibitem[Han et~al.(2023)Han, Simig, Mihaylov, Tsvetkov, Celikyilmaz, and Wang]{han2023understanding}
Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, and Tianlu Wang.
\newblock Understanding in-context learning via supportive pretraining data.
\newblock In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (eds.), \emph{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pp.\  12660--12673, Toronto, Canada, July 2023. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.acl-long.708}.
\newblock URL \url{https://aclanthology.org/2023.acl-long.708}.

\bibitem[Hendrycks et~al.(2021)Hendrycks, Burns, Basart, Zou, Mazeika, Song, and Steinhardt]{hendrycks2020measuring}
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.
\newblock Measuring massive multitask language understanding.
\newblock In \emph{9th International Conference on Learning Representations, {ICLR} 2021, Virtual Event, Austria, May 3-7, 2021}. OpenReview.net, 2021.
\newblock URL \url{https://openreview.net/forum?id=d7KBjmI3GmQ}.

\bibitem[Hinton \& Roweis(2002)Hinton and Roweis]{hinton2002stochastic}
Geoffrey~E. Hinton and Sam~T. Roweis.
\newblock Stochastic neighbor embedding.
\newblock In Suzanna Becker, Sebastian Thrun, and Klaus Obermayer (eds.), \emph{Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems, {NIPS} 2002, December 9-14, 2002, Vancouver, British Columbia, Canada]}, pp.\  833--840. {MIT} Press, 2002.
\newblock URL \url{https://proceedings.neurips.cc/paper/2002/hash/6150ccc6069bea6b5716254057a194ef-Abstract.html}.

\bibitem[Honovich et~al.(2022)Honovich, Scialom, Levy, and Schick]{honovich2022unnatural}
Or~Honovich, Thomas Scialom, Omer Levy, and Timo Schick.
\newblock Unnatural instructions: Tuning language models with (almost) no human labor.
\newblock \emph{ArXiv preprint}, abs/2212.09689, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.09689}.

\bibitem[Ivison et~al.(2023)Ivison, Wang, Pyatkin, Lambert, Peters, Dasigi, Jang, Wadden, Smith, Beltagy, and Hajishirzi]{ivison2023camels}
Hamish Ivison, Yizhong Wang, Valentina Pyatkin, Nathan Lambert, Matthew Peters, Pradeep Dasigi, Joel Jang, David Wadden, Noah~A. Smith, Iz~Beltagy, and Hannaneh Hajishirzi.
\newblock Camels in a changing climate: Enhancing lm adaptation with tulu 2, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.10702}.

\bibitem[Kung et~al.(2023)Kung, Yin, Wu, Chang, and Peng]{kung2023active}
Po-Nien Kung, Fan Yin, Di~Wu, Kai-Wei Chang, and Nanyun Peng.
\newblock Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks.
\newblock \emph{ArXiv preprint}, abs/2311.00288, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.00288}.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Li, Chen, Chen, Cheng, Wang, Zhou, and Xiao]{li2023quantity}
Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao.
\newblock From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2308.12032, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2308.12032}.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Yu, Zhou, Schick, Zettlemoyer, Levy, Weston, and Lewis]{li2023self}
Xian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason Weston, and Mike Lewis.
\newblock Self-alignment with instruction backtranslation.
\newblock \emph{ArXiv preprint}, abs/2308.06259, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2308.06259}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, Hui, Xia, Yang, Yang, Zhang, Si, Liu, Liu, Huang, et~al.]{li2023one}
Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, et~al.
\newblock One shot learning as instruction data prospector for large language models.
\newblock \emph{ArXiv preprint}, abs/2312.10302, 2023{\natexlab{c}}.
\newblock URL \url{https://arxiv.org/abs/2312.10302}.

\bibitem[Liu et~al.(2023)Liu, Zeng, He, Jiang, and He]{liu2023makes}
Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He.
\newblock What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2312.15685, 2023.
\newblock URL \url{https://arxiv.org/abs/2312.15685}.

\bibitem[Longpre et~al.(2023)Longpre, Hou, Vu, Webson, Chung, Tay, Zhou, Le, Zoph, Wei, et~al.]{longpre2023flan}
Shayne Longpre, Le~Hou, Tu~Vu, Albert Webson, Hyung~Won Chung, Yi~Tay, Denny Zhou, Quoc~V Le, Barret Zoph, Jason Wei, et~al.
\newblock The flan collection: Designing data and methods for effective instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2301.13688, 2023.
\newblock URL \url{https://arxiv.org/abs/2301.13688}.

\bibitem[Ott et~al.(2018)Ott, Edunov, Grangier, and Auli]{ott2018scaling}
Myle Ott, Sergey Edunov, David Grangier, and Michael Auli.
\newblock Scaling neural machine translation.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation: Research Papers}, pp.\  1--9, Brussels, Belgium, 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6301}.
\newblock URL \url{https://aclanthology.org/W18-6301}.

\bibitem[Penedo et~al.(2023)Penedo, Malartic, Hesslow, Cojocaru, Cappelli, Alobeidli, Pannier, Almazrouei, and Launay]{penedo2023refinedweb}
Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.
\newblock The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only.
\newblock \emph{ArXiv preprint}, abs/2306.01116, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.01116}.

\bibitem[Peng et~al.(2023{\natexlab{a}})Peng, Li, He, Galley, and Gao]{peng2023instruction}
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.
\newblock Instruction tuning with gpt-4.
\newblock \emph{ArXiv preprint}, abs/2304.03277, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2304.03277}.

\bibitem[Peng et~al.(2023{\natexlab{b}})Peng, Ding, Zhong, Shen, Liu, Zhang, Ouyang, and Tao]{peng-etal-2023-towards}
Keqin Peng, Liang Ding, Qihuang Zhong, Li~Shen, Xuebo Liu, Min Zhang, Yuanxin Ouyang, and Dacheng Tao.
\newblock Towards making the most of {C}hat{GPT} for machine translation.
\newblock In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), \emph{Findings of the Association for Computational Linguistics: EMNLP 2023}, pp.\  5622--5633, Singapore, 2023{\natexlab{b}}. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2023.findings-emnlp.373}.
\newblock URL \url{https://aclanthology.org/2023.findings-emnlp.373}.

\bibitem[Post(2018)]{post-2018-call}
Matt Post.
\newblock A call for clarity in reporting {BLEU} scores.
\newblock In \emph{Proceedings of the Third Conference on Machine Translation: Research Papers}, pp.\  186--191, Brussels, Belgium, 2018. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/W18-6319}.
\newblock URL \url{https://aclanthology.org/W18-6319}.

\bibitem[Rei et~al.(2022)Rei, C.~de Souza, Alves, Zerva, Farinha, Glushkova, Lavie, Coheur, and Martins]{rei-etal-2022-comet}
Ricardo Rei, Jos{\'e}~G. C.~de Souza, Duarte Alves, Chrysoula Zerva, Ana~C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr{\'e} F.~T. Martins.
\newblock {COMET}-22: Unbabel-{IST} 2022 submission for the metrics shared task.
\newblock In \emph{Proceedings of the Seventh Conference on Machine Translation (WMT)}, pp.\  578--585, Abu Dhabi, United Arab Emirates (Hybrid), 2022. Association for Computational Linguistics.
\newblock URL \url{https://aclanthology.org/2022.wmt-1.52}.

\bibitem[Subhabrata \& Arindam(2023)Subhabrata and Arindam]{Orca}
Mukherjee Subhabrata and Mitra Arindam.
\newblock Orca: Progressive learning from complex explanation traces of gpt-4.
\newblock \emph{https://arxiv.org/pdf/2306.02707}, 2023.
\newblock URL \url{https://arxiv.org/pdf/2306.02707}.

\bibitem[Sun et~al.(2023)Sun, Shen, Zhou, Zhang, Chen, Cox, Yang, and Gan]{sun2023principle}
Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David Cox, Yiming Yang, and Chuang Gan.
\newblock Principle-driven self-alignment of language models from scratch with minimal human supervision.
\newblock \emph{ArXiv preprint}, abs/2305.03047, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.03047}.

\bibitem[Suzgun et~al.(2022)Suzgun, Scales, Sch{\"a}rli, Gehrmann, Tay, Chung, Chowdhery, Le, Chi, Zhou, et~al.]{suzgun2022challenging}
Mirac Suzgun, Nathan Scales, Nathanael Sch{\"a}rli, Sebastian Gehrmann, Yi~Tay, Hyung~Won Chung, Aakanksha Chowdhery, Quoc~V Le, Ed~H Chi, Denny Zhou, et~al.
\newblock Challenging big-bench tasks and whether chain-of-thought can solve them.
\newblock \emph{ArXiv preprint}, abs/2210.09261, 2022.
\newblock URL \url{https://arxiv.org/abs/2210.09261}.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{Alpaca}
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori~B. Hashimoto.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet, Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar, et~al.]{touvron2023LLaMA1}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{ArXiv preprint}, abs/2302.13971, 2023{\natexlab{a}}.
\newblock URL \url{https://arxiv.org/abs/2302.13971}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023LLaMA}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{ArXiv preprint}, abs/2307.09288, 2023{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2307.09288}.

\bibitem[Wang et~al.(2024)Wang, Ivison, Dasigi, Hessel, Khot, Chandu, Wadden, MacMillan, Smith, Beltagy, et~al.]{wang2023far}
Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Chandu, David Wadden, Kelsey MacMillan, Noah~A Smith, Iz~Beltagy, et~al.
\newblock How far can camels go? exploring the state of instruction tuning on open resources.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2024.
\newblock URL \url{https://arxiv.org/abs/2306.04751}.

\bibitem[Wei et~al.(2022{\natexlab{a}})Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and Le]{wei2021finetuned}
Jason Wei, Maarten Bosma, Vincent~Y. Zhao, Kelvin Guu, Adams~Wei Yu, Brian Lester, Nan Du, Andrew~M. Dai, and Quoc~V. Le.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{The Tenth International Conference on Learning Representations, {ICLR} 2022, Virtual Event, April 25-29, 2022}. OpenReview.net, 2022{\natexlab{a}}.
\newblock URL \url{https://openreview.net/forum?id=gEZrGCozdqR}.

\bibitem[Wei et~al.(2022{\natexlab{b}})Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou, et~al.]{wei2022chain}
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed~Chi, Quoc~V Le, Denny Zhou, et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language models.
\newblock \emph{ArXiv preprint}, abs/2201.11903, 2022{\natexlab{b}}.
\newblock URL \url{https://arxiv.org/abs/2201.11903}.

\bibitem[Wu et~al.(2023)Wu, Lu, Xu, Lin, Su, and Zhou]{wu2023self}
Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi~Su, and Chang Zhou.
\newblock Self-evolved diverse data sampling for efficient instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2311.08182, 2023.
\newblock URL \url{https://arxiv.org/abs/2311.08182}.

\bibitem[Xia et~al.(2024)Xia, Malladi, Gururangan, Arora, and Chen]{xia2024less}
Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen.
\newblock Less: Selecting influential data for targeted instruction tuning.
\newblock \emph{ArXiv preprint}, abs/2402.04333, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.04333}.

\bibitem[Xie et~al.(2023)Xie, Santurkar, Ma, and Liang]{xie2023data}
Sang~Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy~S Liang.
\newblock Data selection for language models via importance resampling.
\newblock \emph{Advances in Neural Information Processing Systems}, 36:\penalty0 34201--34227, 2023.
\newblock URL \url{https://arxiv.org/abs/2302.03169}.

\bibitem[Xu et~al.(2023)Xu, Sun, Zheng, Geng, Zhao, Feng, Tao, and Jiang]{xu2023wizardlm}
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu~Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang.
\newblock Wizardlm: Empowering large language models to follow complex instructions.
\newblock \emph{ArXiv preprint}, abs/2304.12244, 2023.
\newblock URL \url{https://arxiv.org/abs/2304.12244}.

\bibitem[Xu et~al.(2024)Xu, Kim, Sharaf, and Awadalla]{xu2024a}
Haoran Xu, Young~Jin Kim, Amr Sharaf, and Hany~Hassan Awadalla.
\newblock A paradigm shift in machine translation: Boosting translation performance of large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=farT6XXntP}.

\bibitem[Yang et~al.(2023)Yang, Li, Zhang, and Zong]{yang-etal-2023-BigTranslate}
Wen Yang, Chong Li, Jiajun Zhang, and Chengqing Zong.
\newblock Bigtranslate: Augmenting large language models with multilingual translation capability over 100 languages.
\newblock \emph{ArXiv preprint}, abs/2305.18098, 2023.
\newblock URL \url{https://arxiv.org/abs/2305.18098}.

\bibitem[Zeng et~al.(2023)Zeng, Meng, Yin, and Zhou]{zeng2023tim}
Jiali Zeng, Fandong Meng, Yongjing Yin, and Jie Zhou.
\newblock Tim: Teaching large language models to translate with comparison.
\newblock \emph{ArXiv preprint}, abs/2307.04408, 2023.
\newblock URL \url{https://arxiv.org/abs/2307.04408}.

\bibitem[Zhang et~al.(2023)Zhang, Fang, Zhang, Ma, Zhou, Huang, Bu, Gui, Chen, Chen, and Feng]{bayling}
Shaolei Zhang, Qingkai Fang, Zhuocheng Zhang, Zhengrui Ma, Yan Zhou, Langlin Huang, Mengyu Bu, Shangtong Gui, Yunji Chen, Xilin Chen, and Yang Feng.
\newblock Bayling: Bridging cross-lingual alignment and instruction following through interactive translation for large language models.
\newblock \emph{ArXiv preprint}, abs/2306.10968, 2023.
\newblock URL \url{https://arxiv.org/abs/2306.10968}.

\bibitem[Zhao et~al.(2024)Zhao, Andriushchenko, Croce, and Flammarion]{zhao2024long}
Hao Zhao, Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion.
\newblock Long is more for alignment: A simple but tough-to-beat baseline for instruction fine-tuning, 2024.
\newblock URL \url{https://arxiv.org/abs/2402.04833}.

\bibitem[Zhou et~al.(2023)Zhou, Liu, Xu, Iyer, Sun, Mao, Ma, Efrat, Yu, YU, Zhang, Ghosh, Lewis, Zettlemoyer, and Levy]{zhou2023lima}
Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy.
\newblock {LIMA}: Less is more for alignment.
\newblock In \emph{Thirty-seventh Conference on Neural Information Processing Systems}, 2023.
\newblock URL \url{https://openreview.net/forum?id=KBMOKmX2he}.

\bibitem[Zhou et~al.(2020)Zhou, Yang, Wong, Wan, and Chao]{zhou2020uncertainty}
Yikai Zhou, Baosong Yang, Derek~F. Wong, Yu~Wan, and Lidia~S. Chao.
\newblock Uncertainty-aware curriculum learning for neural machine translation.
\newblock In \emph{Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, pp.\  6934--6944, Online, 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.acl-main.620}.
\newblock URL \url{https://aclanthology.org/2020.acl-main.620}.

\end{thebibliography}
