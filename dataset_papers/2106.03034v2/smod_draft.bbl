\begin{thebibliography}{36}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Asi} and {Duchi}(2019)]{asi2019stochasticdup}
H.~{Asi} and J.~C. {Duchi}.
\newblock Stochastic (approximate) proximal point methods: Convergence,
  optimality, and adaptivity.
\newblock \emph{Siam Journal on Optimization}, 29\penalty0 (3):\penalty0
  2257--2290, 2019.

\bibitem[Asi and Duchi(2019)]{asi2019the}
H.~Asi and J.~C. Duchi.
\newblock The importance of better models in stochastic optimization.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (46):\penalty0 22924--22930, 2019.

\bibitem[Asi et~al.(2020)Asi, Chadha, Cheng, and Duchi]{asi2020minibatch}
H.~Asi, K.~Chadha, G.~Cheng, and J.~C. Duchi.
\newblock Minibatch stochastic approximate proximal point methods.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and
  Talwar]{bassily2020stability}
R.~Bassily, V.~Feldman, C.~Guzm{\'a}n, and K.~Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[{Berrada} et~al.(2019){Berrada}, {Zisserman}, and
  {Kumar}]{berrada2019deep}
L.~{Berrada}, A.~{Zisserman}, and M.~P. {Kumar}.
\newblock Deep frank-wolfe for neural network optimization.
\newblock In \emph{ICLR 2019 : 7th International Conference on Learning
  Representations}, 2019.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{botev2017practical}
A.~Botev, H.~Ritter, and D.~Barber.
\newblock Practical gauss-newton optimisation for deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  557--565. PMLR, 2017.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{Journal of machine learning research}, 2\penalty0
  (Mar):\penalty0 499--526, 2002.

\bibitem[Chadha et~al.(2021)Chadha, Cheng, and Duchi]{chadha2021accelerated}
K.~Chadha, G.~Cheng, and J.~C. Duchi.
\newblock Accelerated, optimal, and parallel: Some results on model-based
  stochastic optimization.
\newblock \emph{arXiv preprint arXiv:2101.02696}, 2021.

\bibitem[{Charisopoulos} et~al.(2019){Charisopoulos}, {Chen}, {Davis},
  {D\'iaz}, {Ding}, and {Drusvyatskiy}]{charisopoulos2019low}
V.~{Charisopoulos}, Y.~{Chen}, D.~{Davis}, M.~{D\'iaz}, L.~{Ding}, and
  D.~{Drusvyatskiy}.
\newblock Low-rank matrix recovery with composite optimization: good
  conditioning and rapid convergence.
\newblock \emph{arXiv preprint arXiv:1904.10020}, 2019.

\bibitem[{Davis} and {Drusvyatskiy}(2019)]{davis2019stochasticweakly}
D.~{Davis} and D.~{Drusvyatskiy}.
\newblock Stochastic model-based minimization of weakly convex functions.
\newblock \emph{Siam Journal on Optimization}, 29\penalty0 (1):\penalty0
  207--239, 2019.

\bibitem[Defazio(2020)]{defazio2020understanding}
A.~Defazio.
\newblock Understanding the role of momentum in non-convex optimization:
  Practical insights from a lyapunov analysis.
\newblock \emph{arXiv preprint arXiv:2010.00406}, 2020.

\bibitem[Dekel et~al.(2012)Dekel, Gilad-Bachrach, Shamir, and
  Xiao]{dekel2012optimal}
O.~Dekel, R.~Gilad-Bachrach, O.~Shamir, and L.~Xiao.
\newblock Optimal distributed online prediction using mini-batches.
\newblock \emph{Journal of Machine Learning Research}, 13\penalty0 (1), 2012.

\bibitem[Diakonikolas and Jordan(2021)]{diakonikolas2021generalized}
J.~Diakonikolas and M.~I. Jordan.
\newblock Generalized momentum-based methods: a hamiltonian perspective.
\newblock \emph{SIAM Journal on Optimization}, 31\penalty0 (1):\penalty0
  915--944, 2021.

\bibitem[{Drusvyatskiy} and {Paquette}(2018)]{drusvyatskiy2018efficiency}
D.~{Drusvyatskiy} and C.~{Paquette}.
\newblock Efficiency of minimizing compositions of convex functions and smooth
  maps.
\newblock \emph{Mathematical Programming}, pages 1--56, 2018.

\bibitem[{Duchi} and {Ruan}(2018)]{duchi2018stochastic}
J.~C. {Duchi} and F.~{Ruan}.
\newblock Stochastic methods for composite and weakly convex optimization
  problems.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (4):\penalty0
  3229--3259, 2018.

\bibitem[Duchi and Ruan(2019)]{duchi2019solving}
J.~C. Duchi and F.~Ruan.
\newblock Solving (most) of a set of quadratic equalities: Composite
  optimization for robust phase retrieval.
\newblock \emph{Information and Inference: A Journal of the IMA}, 8\penalty0
  (3):\penalty0 471--529, 2019.

\bibitem[Frerix et~al.(2018)Frerix, M{\"o}llenhoff, Moeller, and
  Cremers]{frerix2018proximal}
T.~Frerix, T.~M{\"o}llenhoff, M.~Moeller, and D.~Cremers.
\newblock Proximal backpropagation.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Ghadimi et~al.(2015)Ghadimi, Feyzmahdavian, and
  Johansson]{ghadimi2015global}
E.~Ghadimi, H.~R. Feyzmahdavian, and M.~Johansson.
\newblock Global convergence of the heavy-ball method for convex optimization.
\newblock In \emph{2015 European control conference (ECC)}, pages 310--315.
  IEEE, 2015.

\bibitem[Ghadimi and Lan(2013)]{saeed-lan-nonconvex-2013}
S.~Ghadimi and G.~Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.
\newblock ISSN 1052-6234.

\bibitem[{Gitman} et~al.(2019){Gitman}, {Lang}, {Zhang}, and
  {Xiao}]{gitman2019understanding}
I.~{Gitman}, H.~{Lang}, P.~{Zhang}, and L.~{Xiao}.
\newblock Understanding the role of momentum in stochastic gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, pages 9633--9643, 2019.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  1225--1234. PMLR, 2016.

\bibitem[Hastie et~al.(2009)Hastie, Tibshirani, Friedman, Hastie, Friedman, and
  Tibshirani]{RN113}
T.~Hastie, R.~Tibshirani, J.~Friedman, T.~Hastie, J.~Friedman, and
  R.~Tibshirani.
\newblock \emph{The elements of statistical learning}, volume~2.
\newblock Springer, 2009.

\bibitem[Kornowski and Shamir(2021)]{kornowski2021oracle}
G.~Kornowski and O.~Shamir.
\newblock Oracle complexity in nonsmooth nonconvex optimization.
\newblock \emph{arXiv preprint arXiv:2104.06763}, 2021.

\bibitem[Lan(2012)]{lan2012optimaldup}
G.~Lan.
\newblock An optimal method for stochastic composite optimization.
\newblock \emph{Mathematical Programming}, 133\penalty0 (1):\penalty0 365--397,
  2012.

\bibitem[Liu et~al.(2020)Liu, Gao, and Yin]{liu2020improved}
Y.~Liu, Y.~Gao, and W.~Yin.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock \emph{arXiv preprint arXiv:2007.07989}, 2020.

\bibitem[Loizou and Richt{\'a}rik(2020)]{loizou2020momentum}
N.~Loizou and P.~Richt{\'a}rik.
\newblock Momentum and stochastic momentum for stochastic gradient, newton,
  proximal point and subspace descent methods.
\newblock \emph{Computational Optimization and Applications}, 77\penalty0
  (3):\penalty0 653--710, 2020.

\bibitem[Mai and Johansson(2020)]{pmlr-v119-mai20b}
V.~Mai and M.~Johansson.
\newblock Convergence of a stochastic gradient method with momentum for
  non-smooth non-convex optimization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 6630--6639, 2020.

\bibitem[Nemirovski et~al.(2009)Nemirovski, Juditsky, Lan, and
  Shapiro]{nemirovski2009dup}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (4):\penalty0
  1574--1609, 2009.

\bibitem[Sebbouh et~al.(2020)Sebbouh, Gower, and
  Defazio]{sebbouh2020convergence}
O.~Sebbouh, R.~M. Gower, and A.~Defazio.
\newblock On the convergence of the stochastic heavy ball method.
\newblock \emph{arXiv preprint arXiv:2006.07867}, 2020.

\bibitem[Shalev-Shwartz et~al.(2010)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2010learnability}
S.~Shalev-Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock \emph{The Journal of Machine Learning Research}, 11:\penalty0
  2635--2670, 2010.

\bibitem[Sutskever et~al.(2013)Sutskever, Martens, Dahl, and Hinton]{RN312}
I.~Sutskever, J.~Martens, G.~Dahl, and G.~Hinton.
\newblock On the importance of initialization and momentum in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  1139--1147, 2013.

\bibitem[Tak{\'a}{\v{c}} et~al.(2015)Tak{\'a}{\v{c}}, Richt{\'a}rik, and
  Srebro]{takavc2015distributed}
M.~Tak{\'a}{\v{c}}, P.~Richt{\'a}rik, and N.~Srebro.
\newblock Distributed mini-batch sdca.
\newblock \emph{arXiv preprint arXiv:1507.08322}, 2015.

\bibitem[Wang et~al.(2017)Wang, Wang, and Srebro]{wang2017memory}
J.~Wang, W.~Wang, and N.~Srebro.
\newblock Memory and communication efficient distributed stochastic
  optimization with minibatch prox.
\newblock In \emph{Conference on Learning Theory}, pages 1882--1919. PMLR,
  2017.

\bibitem[{Yan} et~al.(2018){Yan}, {Yang}, {Li}, {Lin}, and {Yang}]{yan2018a}
Y.~{Yan}, T.~{Yang}, Z.~{Li}, Q.~{Lin}, and Y.~{Yang}.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence}, pages 2955--2961, 2018.

\bibitem[Zhang and Xiao(2021)]{zhang2021stochastic}
J.~Zhang and L.~Xiao.
\newblock Stochastic variance-reduced prox-linear algorithms for nonconvex
  composite optimization.
\newblock \emph{Mathematical Programming}, pages 1--43, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Lin, Jegelka, Jadbabaie, and
  Sra]{zhang2020complexity}
J.~Zhang, H.~Lin, S.~Jegelka, A.~Jadbabaie, and S.~Sra.
\newblock Complexity of finding stationary points of nonsmooth nonconvex
  functions.
\newblock \emph{arXiv preprint arXiv:2002.04130}, 2020.

\end{thebibliography}
