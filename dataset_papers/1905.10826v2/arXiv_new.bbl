\newcommand{\etalchar}[1]{$^{#1}$}
\begin{thebibliography}{GMMM19}

\bibitem[ADH{\etalchar{+}}19]{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock {\em arXiv:1901.08584}, 2019.

\bibitem[ASCC18]{athalye2018evidence}
Vivek~R Athalye, Fernando~J Santos, Jose~M Carmena, and Rui~M Costa.
\newblock Evidence for a neural law of effect.
\newblock {\em Science}, 359(6379):1024--1029, 2018.

\bibitem[AZLL18]{allen2018learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock {\em arXiv:1811.04918}, 2018.

\bibitem[AZLS18]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock {\em arXiv preprint arXiv:1811.03962}, 2018.

\bibitem[BG17]{brutzkus2017globally}
Alon Brutzkus and Amir Globerson.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 605--614. JMLR. org, 2017.

\bibitem[BR89]{blum1989training}
Avrim Blum and Ronald~L Rivest.
\newblock Training a 3-node neural network is np-complete.
\newblock In {\em Advances in neural information processing systems}, pages
  494--501, 1989.

\bibitem[CB18]{chizat2018note}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock {\em arXiv preprint arXiv:1812.07956}, 2018.

\bibitem[CG19]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock {\em arXiv preprint arXiv:1905.13210}, 2019.

\bibitem[CI12]{cantero2012rapid}
Mar{\'\i}a~Jos{\'e} Cantero and Arieh Iserles.
\newblock On rapid computation of expansions in ultraspherical polynomials.
\newblock {\em SIAM Journal on Numerical Analysis}, 50(1):307--327, 2012.

\bibitem[DDS{\etalchar{+}}09]{Imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em 2009 IEEE conference on computer vision and pattern
  recognition}, pages 248--255. Ieee, 2009.

\bibitem[DLL{\etalchar{+}}18]{du2018gradientlee}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock {\em arXiv:1811.03804}, 2018.

\bibitem[DS63]{dunford1963linear}
Nelson Dunford and Jacob~T Schwartz.
\newblock {\em Linear operators: Part II: Spectral Theory: Self Adjoint
  Operators in Hilbert Space}.
\newblock Interscience Publishers, 1963.

\bibitem[DX13]{DX2013}
Feng Dai and Yuan Xu.
\newblock {\em Approximation theory and harmonic analysis on spheres and
  balls}.
\newblock Springer, 2013.

\bibitem[DZPS18]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock {\em arXiv:1810.02054}, 2018.

\bibitem[GMMM19]{ghorbani2019linearized}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari.
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em arXiv:1904.12191}, 2019.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[KB18]{klusowski2018approximation}
Jason~M Klusowski and Andrew~R Barron.
\newblock Approximation by combinations of relu and squared relu ridge
  functions with l1 and l0 controls.
\newblock 2018.

\bibitem[KSH12]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[LBB{\etalchar{+}}98]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, Patrick Haffner, et~al.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[LL18]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8157--8166, 2018.

\bibitem[LSO19]{li2019gradient}
Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak.
\newblock Gradient descent with early stopping is provably robust to label
  noise for overparameterized neural networks.
\newblock {\em arXiv:1903.11680}, 2019.

\bibitem[LY17]{li2017convergence}
Yuanzhi Li and Yang Yuan.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  597--607, 2017.

\bibitem[MG06]{marder2006variability}
Eve Marder and Jean-Marc Goaillard.
\newblock Variability, compensation and homeostasis in neuron and network
  function.
\newblock {\em Nature Reviews Neuroscience}, 7(7):563, 2006.

\bibitem[MMN18]{mei2018mean}
Song Mei, Andrea Montanari, and Phan-Minh Nguyen.
\newblock A mean field view of the landscape of two-layers neural networks.
\newblock {\em arXiv:1804.06561}, 2018.

\bibitem[Nes18]{nesterov2018lectures}
Yurii Nesterov.
\newblock {\em Lectures on convex optimization}, volume 137.
\newblock Springer, 2018.

\bibitem[OS19]{oymak2019towards}
Samet Oymak and Mahdi Soltanolkotabi.
\newblock Towards moderate overparameterization: global convergence guarantees
  for training shallow neural networks.
\newblock {\em arXiv:1902.04674}, 2019.

\bibitem[RBV10]{rosasco2010learning}
Lorenzo Rosasco, Mikhail Belkin, and Ernesto~De Vito.
\newblock On learning with integral operators.
\newblock {\em Journal of Machine Learning Research}, 11(Feb):905--934, 2010.

\bibitem[SS96]{saad1996dynamics}
David Saad and Sara~A Solla.
\newblock Dynamics of on-line gradient descent learning for multilayer neural
  networks.
\newblock In {\em Advances in neural information processing systems}, pages
  302--308, 1996.

\bibitem[Sze75]{orthogonal.poly}
G.~Szeg{\"o}.
\newblock {\em {Orthogonal polynomials}}.
\newblock American Mathematical Society, Providence, RI, 4th edition, 1975.

\bibitem[Tia16]{tian2016symmetry}
Yuandong Tian.
\newblock Symmetry-breaking convergence analysis of certain two-layered neural
  networks with relu nonlinearity.
\newblock 2016.

\bibitem[VW18]{vempala2018gradient}
Santosh Vempala and John Wilmes.
\newblock Gradient descent for one-hidden-layer neural networks: Polynomial
  convergence and sq lower bounds.
\newblock {\em arXiv preprint arXiv:1805.02677}, 2018.

\bibitem[Wan]{yiwang2014}
Yi~Wang.
\newblock Harmonic analysis and isoperimetric inequalities.
\newblock {\em LectureNotes}.

\bibitem[WGL{\etalchar{+}}19]{woodworth2019kernel}
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and deep regimes in overparametrized models.
\newblock {\em arXiv preprint arXiv:1906.05827}, 2019.

\bibitem[XLS17]{xie2017diverse}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock Diverse neural network learns true target functions.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1216--1224,
  2017.

\bibitem[YS19]{yehudai2019power}
Gilad Yehudai and Ohad Shamir.
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock {\em arXiv preprint arXiv:1904.00687}, 2019.

\bibitem[ZBH{\etalchar{+}}16]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv:1611.03530}, 2016.

\bibitem[ZCZG18]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep relu
  networks.
\newblock {\em arXiv preprint arXiv:1811.08888}, 2018.

\bibitem[ZSJ{\etalchar{+}}17]{zhong2017recovery}
Kai Zhong, Zhao Song, Prateek Jain, Peter~L Bartlett, and Inderjit~S Dhillon.
\newblock Recovery guarantees for one-hidden-layer neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 4140--4149. JMLR. org, 2017.

\end{thebibliography}
