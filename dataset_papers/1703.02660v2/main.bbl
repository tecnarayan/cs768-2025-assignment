\begin{thebibliography}{10}

\bibitem{Atari}
Volodymyr~Mnih et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock {\em Nature}, 518, 2015.

\bibitem{AlphaGo}
David~Silver et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock {\em Nature}, 529, 2016.

\bibitem{Schulman15trpo}
John Schulman, Sergey Levine, Philipp Moritz, Michael Jordan, and Pieter
  Abbeel.
\newblock Trust region policy optimization.
\newblock In {\em ICML}, 2015.

\bibitem{Lillicrap15}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock In {\em ICLR}, 2016.

\bibitem{Tassa12}
Yuval Tassa, Tom Erez, and Emanuel Todorov.
\newblock Synthesis and stabilization of complex behaviors through online
  trajectory optimization.
\newblock {\em IROS}, 2012.

\bibitem{Mordatch12}
Igor Mordatch, Emanuel Todorov, and Zoran Popovic.
\newblock Discovery of complex behaviors through contact-invariant
  optimization.
\newblock {\em ACM SIGGRAPH}, 2012.

\bibitem{AlBorno13}
Mazen~Al Borno, Martin de~Lasa, and Aaron Hertzmann.
\newblock {Trajectory Optimization for Full-Body Movements with Complex
  Contacts}.
\newblock {\em IEEE Transactions on Visualization and Computer Graphics}, 2013.

\bibitem{Levine16}
Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel.
\newblock End-to-end training of deep visuomotor policies.
\newblock {\em JMLR}, 17(39):1--40, 2016.

\bibitem{Vikash16}
Vikash Kumar, Emanuel Todorov, and Sergey Levine.
\newblock Optimal control with learned local models: Application to dexterous
  manipulation.
\newblock In {\em ICRA}, 2016.

\bibitem{Vikash16b}
Vikash Kumar, Abhishek Gupta, Emanuel Todorov, and Sergey Levine.
\newblock Learning dexterous manipulation policies from experience and
  imitation.
\newblock {\em CoRR}, abs/1611.05095, 2016.

\bibitem{Pinto16}
Lerrel Pinto and Abhinav Gupta.
\newblock Supersizing self-supervision: Learning to grasp from 50k tries and
  700 robot hours.
\newblock In {\em ICRA}, 2016.

\bibitem{gym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem{Schulman16gae}
John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter
  Abbeel.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock In {\em ICLR}, 2016.

\bibitem{Gu17}
Shixiang Gu, Timothy Lillicrap, Zoubin Ghahramani, Richard~E. Turner, and
  Sergey Levine.
\newblock {Q-Prop: Sample-Efficient Policy Gradient with An Off-Policy Critic}.
\newblock In {\em ICLR}, 2017.

\bibitem{Mordatch15a}
Igor Mordatch, Kendall Lowrey, and Emanuel Todorov.
\newblock {Ensemble-CIO: Full-body dynamic motion planning that transfers to
  physical humanoids}.
\newblock In {\em IROS}, 2015.

\bibitem{Rajeswaran16}
Aravind Rajeswaran, Sarvjeet Ghotra, Balaraman Ravindran, and Sergey Levine.
\newblock {EPOpt: Learning Robust Neural Network Policies Using Model
  Ensembles}.
\newblock In {\em ICLR}, 2017.

\bibitem{Sadeghi16}
Fereshteh Sadeghi and Sergey Levine.
\newblock {(CAD)2RL: Real Single-Image Flight without a Single Real Image}.
\newblock In {\em RSS}, 2016.

\bibitem{Tobin17}
Josh Tobin, Rachel Fong, Alex Ray, Jonas Schneider, Wojciech Zaremba, and
  Pieter Abbeel.
\newblock Domain randomization for transferring deep neural networks from
  simulation to the real world.
\newblock In {\em IROS}, 2017.

\bibitem{rllab}
Yan Duan, Xi~Chen, Rein Houthooft, John Schulman, and Pieter Abbeel.
\newblock Benchmarking deep reinforcement learning for continuous control.
\newblock In {\em ICML}, 2016.

\bibitem{Williams92}
Ronald~J. Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 8(3):229--256, 1992.

\bibitem{Amari98}
Shun-ichi Amari.
\newblock Natural gradient works efficiently in learning.
\newblock {\em Neural Computation}, 10:251--276, 1998.

\bibitem{kakade01npg}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock In {\em NIPS}, 2001.

\bibitem{Peters07}
Jan Peters.
\newblock Machine learning of motor skills for robotics.
\newblock {\em PhD Dissertation, University of Southern California}, 2007.

\bibitem{Rahimi07}
Ali Rahimi and Benjamin Recht.
\newblock {Random Features for Large-Scale Kernel Machines}.
\newblock In {\em NIPS}, 2007.

\bibitem{mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock {MuJoCo}: A physics engine for model-based control.
\newblock In {\em International Conference on Intelligent Robots and Systems},
  2012.

\bibitem{Erez11}
Tom Erez, Yuval Tassa, and Emanuel Todorov.
\newblock Infinite-horizon model predictive control for periodic tasks with
  contacts.
\newblock In {\em RSS}, 2011.

\bibitem{Erez13}
Tom Erez, Kendall Lowrey, Yuval Tassa, Vikash Kumar, Svetoslav Kolev, and
  Emanuel Todorov.
\newblock An integrated system for real-time model predictive control of
  humanoid robots.
\newblock In {\em Humanoids}, 2013.

\bibitem{Heess15}
Nicolas Heess, Gregory Wayne, David Silver, Tim Lillicrap, Tom Erez, and Yuval
  Tassa.
\newblock Learning continuous control policies by stochastic value gradients.
\newblock In {\em NIPS}, 2015.

\bibitem{geramifard2013tutorial}
Alborz Geramifard, Thomas~J Walsh, Stefanie Tellex, Girish Chowdhary, Nicholas
  Roy, and Jonathan~P How.
\newblock A tutorial on linear function approximators for dynamic programming
  and reinforcement learning.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  6(4):375--451, 2013.

\bibitem{si2004handbook}
Jennie Si.
\newblock {\em Handbook of learning and approximate dynamic programming},
  volume~2.
\newblock John Wiley \& Sons, 2004.

\bibitem{bertsekas2008approximate}
Dimitri~P Bertsekas.
\newblock Approximate dynamic programming.
\newblock 2008.

\bibitem{baird95}
Leemon Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In {\em ICML}, 1995.

\end{thebibliography}
