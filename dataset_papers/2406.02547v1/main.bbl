\begin{thebibliography}{10}

\bibitem{flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:23716--23736, 2022.

\bibitem{moe}
Albert~Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Emma~Bou Hanna, Florian Bressand, et~al.
\newblock Mixtral of experts.
\newblock {\em arXiv preprint arXiv:2401.04088}, 2024.

\bibitem{deepspeed}
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He.
\newblock Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters.
\newblock In {\em Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining}, pages 3505--3506, 2020.

\bibitem{opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{mistral}
Albert~Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra~Singh Chaplot, Diego de~las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et~al.
\newblock Mistral 7b.
\newblock {\em arXiv preprint arXiv:2310.06825}, 2023.

\bibitem{llama2}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em arXiv preprint arXiv:2307.09288}, 2023.

\bibitem{bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock In {\em NAACL-HLT (1)}, 2019.

\bibitem{adept_fuyu8b}
Adept AI.
\newblock {Fuyu-8B}.
\newblock \url{https://www.adept.ai/blog/fuyu-8b}, n.d.
\newblock Accessed: [insert date here].

\bibitem{openflamingo}
Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et~al.
\newblock Openflamingo: An open-source framework for training large autoregressive vision-language models.
\newblock {\em arXiv preprint arXiv:2308.01390}, 2023.

\bibitem{deepseek}
Haoyu Lu, Wen Liu, Bo~Zhang, Bingxuan Wang, Kai Dong, Bo~Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Yaofeng Sun, et~al.
\newblock Deepseek-vl: towards real-world vision-language understanding.
\newblock {\em arXiv preprint arXiv:2403.05525}, 2024.

\bibitem{cc3m}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565, 2018.

\bibitem{laion400m}
Christoph Schuhmann, Richard Vencu, Romain Beaumont, and Kaczmarczyk.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
\newblock {\em arXiv preprint arXiv:2111.02114}, 2021.

\bibitem{mmc4}
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir~Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William~Yang Wang, and Yejin Choi.
\newblock Multimodal c4: An open, billion-scale corpus of images interleaved with text.
\newblock {\em arXiv preprint arXiv:2304.06939}, 2023.

\bibitem{obelics}
Hugo Lauren{\c{c}}on, Lucile Saulnier, L{\'e}o Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander~M Rush, Douwe Kiela, et~al.
\newblock Obelics: An open web-scale filtered dataset of interleaved image-text documents.
\newblock In {\em Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2023.

\bibitem{docvqa}
Minesh Mathew, Dimosthenis Karatzas, and CV~Jawahar.
\newblock Docvqa: A dataset for vqa on document images.
\newblock In {\em Proceedings of the IEEE/CVF winter conference on applications of computer vision}, pages 2200--2209, 2021.

\bibitem{memorizingtransformer}
Yuhuai Wu, Markus~N Rabe, DeLesley Hutchins, and Christian Szegedy.
\newblock Memorizing transformers.
\newblock {\em arXiv preprint arXiv:2203.08913}, 2022.

\bibitem{flashattention}
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R{\'e}.
\newblock Flashattention: Fast and memory-efficient exact attention with io-awareness.
\newblock {\em Advances in Neural Information Processing Systems}, 35:16344--16359, 2022.

\bibitem{ring}
Hao Liu, Matei Zaharia, and Pieter Abbeel.
\newblock Ring attention with blockwise transformers for near-infinite context.
\newblock {\em arXiv preprint arXiv:2310.01889}, 2023.

\bibitem{lvm}
Hao Liu, Wilson Yan, Matei Zaharia, and Pieter Abbeel.
\newblock World model on million-length video and language with ringattention.
\newblock {\em arXiv preprint arXiv:2402.08268}, 2024.

\bibitem{malmm}
Bo~He, Hengduo Li, Young~Kyun Jang, Menglin Jia, Xuefei Cao, Ashish Shah, Abhinav Shrivastava, and Ser-Nam Lim.
\newblock Ma-lmm: Memory-augmented large multimodal model for long-term video understanding.
\newblock {\em arXiv preprint arXiv:2404.05726}, 2024.

\bibitem{threethings}
Hugo Touvron, Matthieu Cord, Alaaeldin El-Nouby, Jakob Verbeek, and Herv{\'e} J{\'e}gou.
\newblock Three things everyone should know about vision transformers.
\newblock In {\em European Conference on Computer Vision}, pages 497--515. Springer, 2022.

\bibitem{clip}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{parrot}
Yiqi Lin, Conghui He, Alex~Jinpeng Wang, Bin Wang, Weijia Li, and Mike~Zheng Shou.
\newblock Parrot captions teach clip to spot text.
\newblock {\em arXiv preprint arXiv:2312.14232}, 2023.

\bibitem{vit}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{cosmo}
Alex~Jinpeng Wang, Linjie Li, Kevin~Qinghong Lin, Jianfeng Wang, Kevin Lin, Zhengyuan Yang, Lijuan Wang, and Mike~Zheng Shou.
\newblock Cosmo: Contrastive streamlined multimodal model with interleaved pre-training.
\newblock {\em arXiv preprint arXiv:2401.00849}, 2024.

\bibitem{datacomp}
Alex~Fang Samir Yitzhak~Gadre, Gabriel~Ilharco.
\newblock Datacomp: In search of the next generation of multimodal datasets.
\newblock {\em arXiv preprint arXiv:2304.14108}, 2023.

\bibitem{renderedtext2023}
{Wendler, Chris}.
\newblock Renderedtext dataset.
\newblock \url{https://huggingface.co/datasets/wendlerc/RenderedText}, 2023.
\newblock Accessed: 2023-05-05.

\bibitem{okvqa}
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
\newblock Ok-vqa: A visual question answering benchmark requiring external knowledge.
\newblock In {\em Proceedings of the IEEE/cvf conference on computer vision and pattern recognition}, pages 3195--3204, 2019.

\bibitem{textvqa}
Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu~Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach.
\newblock Towards vqa models that can read.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8317--8326, 2019.

\bibitem{vizwiz}
Danna Gurari, Qing Li, Abigale~J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey~P Bigham.
\newblock Vizwiz grand challenge: Answering visual questions from blind people.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 3608--3617, 2018.

\bibitem{vqav2}
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
\newblock Making the v in vqa matter: Elevating the role of image understanding in visual question answering.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 6904--6913, 2017.

\bibitem{coco}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em Computer Vision--ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13}, pages 740--755. Springer, 2014.

\bibitem{flickr30k}
Bryan~A Plummer, Liwei Wang, Chris~M Cervantes, Juan~C Caicedo, Julia Hockenmaier, and Svetlana Lazebnik.
\newblock Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2641--2649, 2015.

\bibitem{ocrvqa}
Anand Mishra, Shashank Shekhar, Ajeet~Kumar Singh, and Anirban Chakraborty.
\newblock Ocr-vqa: Visual question answering by reading text in images.
\newblock In {\em ICDAR}, 2019.

\bibitem{lee2023pix2struct}
Kenton Lee, Mandar Joshi, Iulia~Raluca Turc, Hexiang Hu, Fangyu Liu, Julian~Martin Eisenschlos, Urvashi Khandelwal, Peter Shaw, Ming-Wei Chang, and Kristina Toutanova.
\newblock Pix2struct: Screenshot parsing as pretraining for visual language understanding.
\newblock In {\em International Conference on Machine Learning}, pages 18893--18912. PMLR, 2023.

\bibitem{vilt}
Wonjae Kim, Bokyung Son, and Ildoo Kim.
\newblock Vilt: Vision-and-language transformer without convolution or region supervision.
\newblock In {\em International conference on machine learning}, pages 5583--5594. PMLR, 2021.

\bibitem{blip2}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock {\em arXiv preprint arXiv:2301.12597}, 2023.

\bibitem{palme}
Danny Driess, Fei Xia, Mehdi~SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et~al.
\newblock Palm-e: An embodied multimodal language model.
\newblock {\em arXiv preprint arXiv:2303.03378}, 2023.

\bibitem{coca}
Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu.
\newblock Coca: Contrastive captioners are image-text foundation models.
\newblock {\em arXiv preprint arXiv:2205.01917}, 2022.

\bibitem{llava}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock {\em Advances in neural information processing systems}, 36, 2024.

\bibitem{qwen}
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou.
\newblock Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond.
\newblock 2023.

\bibitem{gpt3}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.~F. Balcan, and H.~Lin, editors, {\em Advances in Neural Information Processing Systems}, volume~33, pages 1877--1901. Curran Associates, Inc., 2020.

\bibitem{git}
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce~Liu, and Lijuan Wang.
\newblock Git: A generative image-to-text transformer for vision and language.
\newblock {\em arXiv preprint arXiv:2205.14100}, 2022.

\bibitem{emu}
Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun Huang, and Xinlong Wang.
\newblock Generative pretraining in multimodality.
\newblock {\em arXiv preprint arXiv:2307.05222}, 2023.

\bibitem{internvl}
Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Zhong Muyan, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et~al.
\newblock Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks.
\newblock {\em arXiv preprint arXiv:2312.14238}, 2023.

\bibitem{rust2023language}
Phillip Rust, Jonas~F Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de~Lhoneux, and Desmond Elliott.
\newblock Language modelling with pixels.
\newblock {\em International Conference on Learning Representations}, 2023.

\bibitem{gao2024improving}
Tianyu Gao, Zirui Wang, Adithya Bhaskar, and Danqi Chen.
\newblock Improving language understanding from screenshots.
\newblock {\em arXiv preprint arXiv:2402.14073}, 2024.

\bibitem{clippo}
Michael Tschannen, Basil Mustafa, and Neil Houlsby.
\newblock Clippo: Image-and-language understanding from pixels only.
\newblock In {\em Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 11006--11017, 2023.

\bibitem{child2019generating}
Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.
\newblock Generating long sequences with sparse transformers.
\newblock {\em arXiv preprint arXiv:1904.10509}, 2019.

\bibitem{beltagy2020longformer}
Iz~Beltagy, Matthew~E Peters, and Arman Cohan.
\newblock Longformer: The long-document transformer.
\newblock {\em arXiv preprint arXiv:2004.05150}, 2020.

\bibitem{korthikanti2023reducing}
Vijay~Anand Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro.
\newblock Reducing activation recomputation in large transformer models.
\newblock {\em Proceedings of Machine Learning and Systems}, 5, 2023.

\bibitem{gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{openai_gptv_system_card}
OpenAI.
\newblock Gptv system card, 2023.
\newblock Accessed: 2024-05-22.

\end{thebibliography}
