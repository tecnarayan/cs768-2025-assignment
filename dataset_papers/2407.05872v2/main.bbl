\begin{thebibliography}{69}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ara{\'u}jo et~al.(2019)Ara{\'u}jo, Oliveira, and
  Yukimura]{araujo2019mean}
Ara{\'u}jo, D., Oliveira, R.~I., and Yukimura, D.
\newblock A mean-field limit for certain deep neural networks.
\newblock \emph{arXiv preprint arXiv:1906.00193}, 2019.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
Ba, J.~L., Kiros, J.~R., and Hinton, G.~E.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Babuschkin et~al.(2020)Babuschkin, Baumli, Bell, Bhupatiraju, Bruce,
  Buchlovsky, Budden, Cai, Clark, Danihelka, Dedieu, Fantacci, Godwin, Jones,
  Hemsley, Hennigan, Hessel, Hou, Kapturowski, Keck, Kemaev, King, Kunesch,
  Martens, Merzic, Mikulik, Norman, Papamakarios, Quan, Ring, Ruiz, Sanchez,
  Sartran, Schneider, Sezener, Spencer, Srinivasan, Stanojevi\'{c}, Stokowiec,
  Wang, Zhou, and Viola]{deepmind2020jax}
Babuschkin, I., Baumli, K., Bell, A., Bhupatiraju, S., Bruce, J., Buchlovsky,
  P., Budden, D., Cai, T., Clark, A., Danihelka, I., Dedieu, A., Fantacci, C.,
  Godwin, J., Jones, C., Hemsley, R., Hennigan, T., Hessel, M., Hou, S.,
  Kapturowski, S., Keck, T., Kemaev, I., King, M., Kunesch, M., Martens, L.,
  Merzic, H., Mikulik, V., Norman, T., Papamakarios, G., Quan, J., Ring, R.,
  Ruiz, F., Sanchez, A., Sartran, L., Schneider, R., Sezener, E., Spencer, S.,
  Srinivasan, S., Stanojevi\'{c}, M., Stokowiec, W., Wang, L., Zhou, G., and
  Viola, F.
\newblock The {D}eep{M}ind {JAX} {E}cosystem, 2020.
\newblock URL \url{http://github.com/deepmind}.

\bibitem[Bi et~al.(2024)Bi, Chen, Chen, Chen, Dai, Deng, Ding, Dong, Du, Fu,
  et~al.]{bi2024deepseek}
Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K.,
  Du, Q., Fu, Z., et~al.
\newblock Deepseek llm: Scaling open-source language models with longtermism.
\newblock \emph{arXiv preprint arXiv:2401.02954}, 2024.

\bibitem[Blake et~al.(2023)Blake, Orr, and Luschi]{blake2023unit}
Blake, C., Orr, D., and Luschi, C.
\newblock Unit scaling: Out-of-the-box low-precision training.
\newblock \emph{arXiv preprint arXiv:2303.11257}, 2023.

\bibitem[Bordelon \& Pehlevan(2022)Bordelon and Pehlevan]{bordelon2022self}
Bordelon, B. and Pehlevan, C.
\newblock Self-consistent dynamical field theory of kernel evolution in wide
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 32240--32256, 2022.

\bibitem[Bordelon et~al.(2023)Bordelon, Noci, Li, Hanin, and
  Pehlevan]{bordelon2023depthwise}
Bordelon, B., Noci, L., Li, M.~B., Hanin, B., and Pehlevan, C.
\newblock Depthwise hyperparameter transfer in residual networks: Dynamics and
  scaling limit.
\newblock \emph{arXiv preprint arXiv:2309.16620}, 2023.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX}: composable transformations of {P}ython+{N}um{P}y programs,
  2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020gpt3}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Chizat \& Bach(2018)Chizat and Bach]{chizat2018global}
Chizat, L. and Bach, F.
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Chizat \& Netrapalli(2023)Chizat and Netrapalli]{chizat2023steering}
Chizat, L. and Netrapalli, P.
\newblock Steering deep feature learning with backward aligned feature updates.
\newblock \emph{arXiv preprint arXiv:2311.18718}, 2023.

\bibitem[Choi et~al.(2019)Choi, Shallue, Nado, Lee, Maddison, and
  Dahl]{choi2019empirical}
Choi, D., Shallue, C.~J., Nado, Z., Lee, J., Maddison, C.~J., and Dahl, G.~E.
\newblock On empirical comparisons of optimizers for deep learning.
\newblock \emph{arXiv preprint arXiv:1910.05446}, 2019.

\bibitem[Chowdhery et~al.(2023)Chowdhery, Narang, Devlin, Bosma, Mishra,
  Roberts, Barham, Chung, Sutton, Gehrmann, et~al.]{chowdhery2023palm}
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A.,
  Barham, P., Chung, H.~W., Sutton, C., Gehrmann, S., et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock \emph{Journal of Machine Learning Research}, 24\penalty0
  (240):\penalty0 1--113, 2023.

\bibitem[Dehghani et~al.(2023)Dehghani, Djolonga, Mustafa, Padlewski, Heek,
  Gilmer, Steiner, Caron, Geirhos, Alabdulmohsin, et~al.]{dehghani2023scaling}
Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J.,
  Steiner, A.~P., Caron, M., Geirhos, R., Alabdulmohsin, I., et~al.
\newblock Scaling vision transformers to 22 billion parameters.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  7480--7512. PMLR, 2023.

\bibitem[Dey et~al.(2023)Dey, Gosal, Khachane, Marshall, Pathria, Tom,
  Hestness, et~al.]{dey2023cerebras}
Dey, N., Gosal, G., Khachane, H., Marshall, W., Pathria, R., Tom, M., Hestness,
  J., et~al.
\newblock Cerebras-gpt: Open compute-optimal language models trained on the
  cerebras wafer-scale cluster.
\newblock \emph{arXiv preprint arXiv:2304.03208}, 2023.

\bibitem[Dinan et~al.(2023)Dinan, Yaida, and Zhang]{dinan2023effective}
Dinan, E., Yaida, S., and Zhang, S.
\newblock Effective theory of transformers at initialization.
\newblock \emph{arXiv preprint arXiv:2304.02034}, 2023.

\bibitem[Du et~al.(2022)Du, Huang, Dai, Tong, Lepikhin, Xu, Krikun, Zhou, Yu,
  Firat, et~al.]{du2022glam}
Du, N., Huang, Y., Dai, A.~M., Tong, S., Lepikhin, D., Xu, Y., Krikun, M.,
  Zhou, Y., Yu, A.~W., Firat, O., et~al.
\newblock Glam: Efficient scaling of language models with mixture-of-experts.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5547--5569. PMLR, 2022.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
Duchi, J., Hazan, E., and Singer, Y.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0 (7), 2011.

\bibitem[Fedus et~al.(2022)Fedus, Zoph, and Shazeer]{fedus2022switch}
Fedus, W., Zoph, B., and Shazeer, N.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (120):\penalty0 1--39, 2022.

\bibitem[Geiger et~al.(2020)Geiger, Spigler, Jacot, and
  Wyart]{geiger2020disentangling}
Geiger, M., Spigler, S., Jacot, A., and Wyart, M.
\newblock Disentangling feature and lazy training in deep neural networks.
\newblock \emph{Journal of Statistical Mechanics: Theory and Experiment},
  2020\penalty0 (11):\penalty0 113301, 2020.

\bibitem[Glorot \& Bengio(2010)Glorot and Bengio]{glorot2010understanding}
Glorot, X. and Bengio, Y.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the thirteenth international conference on
  artificial intelligence and statistics}, pp.\  249--256. JMLR Workshop and
  Conference Proceedings, 2010.

\bibitem[Golovin et~al.(2017)Golovin, Solnik, Moitra, Kochanski, Karro, and
  Sculley]{google_vizier}
Golovin, D., Solnik, B., Moitra, S., Kochanski, G., Karro, J., and Sculley, D.
\newblock Google vizier: {A} service for black-box optimization.
\newblock In \emph{Proceedings of the 23rd {ACM} {SIGKDD} International
  Conference on Knowledge Discovery and Data Mining, Halifax, NS, Canada,
  August 13 - 17, 2017}, pp.\  1487--1495. {ACM}, 2017.
\newblock \doi{10.1145/3097983.3098043}.
\newblock URL \url{https://doi.org/10.1145/3097983.3098043}.

\bibitem[He et~al.(2015)He, Zhang, Ren, and Sun]{he2015delving}
He, K., Zhang, X., Ren, S., and Sun, J.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pp.\  1026--1034, 2015.

\bibitem[Heek et~al.(2023)Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner,
  and van {Z}ee]{flax2020github}
Heek, J., Levskaya, A., Oliver, A., Ritter, M., Rondepierre, B., Steiner, A.,
  and van {Z}ee, M.
\newblock {F}lax: A neural network library and ecosystem for {JAX}, 2023.
\newblock URL \url{http://github.com/google/flax}.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016gaussian}
Hendrycks, D. and Gimpel, K.
\newblock Gaussian error linear units (gelus).
\newblock \emph{arXiv preprint arXiv:1606.08415}, 2016.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012neural}
Hinton, G., Srivastava, N., and Swersky, K.
\newblock Neural networks for machine learning lecture 6a overview of
  mini-batch gradient descent.
\newblock \emph{Cited on}, 14\penalty0 (8):\penalty0 2, 2012.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{hoffmann2022training}
Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford,
  E., Casas, D. d.~L., Hendricks, L.~A., Welbl, J., Clark, A., et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Ishikawa \& Karakida(2023)Ishikawa and
  Karakida]{ishikawa2023parameterization}
Ishikawa, S. and Karakida, R.
\newblock On the parameterization of second-order optimization effective
  towards the infinite width.
\newblock \emph{arXiv preprint arXiv:2312.12226}, 2023.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Kaplan(2019)]{kaplan2019notes}
Kaplan, J.
\newblock Notes on contemporary machine learning for physicists, 2019.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kudo \& Richardson(2018)Kudo and Richardson]{kudo2018sentencepiece}
Kudo, T. and Richardson, J.
\newblock Sentencepiece: A simple and language independent subword tokenizer
  and detokenizer for neural text processing.
\newblock \emph{arXiv preprint arXiv:1808.06226}, 2018.

\bibitem[Large et~al.(2024)Large, Liu, Huh, Bahng, Isola, and
  Bernstein]{large2024scalable}
Large, T., Liu, Y., Huh, M., Bahng, H., Isola, P., and Bernstein, J.
\newblock Scalable optimization in the modular norm.
\newblock \emph{arXiv preprint arXiv:2405.14813}, 2024.

\bibitem[Lee et~al.(2017)Lee, Bahri, Novak, Schoenholz, Pennington, and
  Sohl-Dickstein]{lee2017deep}
Lee, J., Bahri, Y., Novak, R., Schoenholz, S.~S., Pennington, J., and
  Sohl-Dickstein, J.
\newblock Deep neural networks as gaussian processes.
\newblock \emph{arXiv preprint arXiv:1711.00165}, 2017.

\bibitem[Lingle(2024)]{lingle2024large}
Lingle, L.
\newblock A large-scale exploration of $\mu $-transfer.
\newblock \emph{arXiv preprint arXiv:2404.05728}, 2024.

\bibitem[Liu et~al.(2024)Liu, Novak, Lee, Wortsman, Xiao, Everett, Alemi,
  Kurzeja, Marcenac, Gur, Kornblith, Xu, Elsayed, Fischer, Pennington, Adlam,
  and Dickstein]{nanodo}
Liu, P.~J., Novak, R., Lee, J., Wortsman, M., Xiao, L., Everett, K., Alemi,
  A.~A., Kurzeja, M., Marcenac, P., Gur, I., Kornblith, S., Xu, K., Elsayed,
  G., Fischer, I., Pennington, J., Adlam, B., and Dickstein, J.-S.
\newblock Nanodo: A minimal transformer decoder-only language model
  implementation in {JAX}., 2024.
\newblock URL \url{http://github.com/google-deepmind/nanodo}.

\bibitem[Loshchilov \& Hutter(2018)Loshchilov and
  Hutter]{loshchilov2018decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
Matthews, A. G. d.~G., Rowland, M., Hron, J., Turner, R.~E., and Ghahramani, Z.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[McCandlish et~al.(2018)McCandlish, Kaplan, Amodei, and
  Team]{mccandlish2018empirical}
McCandlish, S., Kaplan, J., Amodei, D., and Team, O.~D.
\newblock An empirical model of large-batch training.
\newblock \emph{arXiv preprint arXiv:1812.06162}, 2018.

\bibitem[Mei et~al.(2018)Mei, Montanari, and Nguyen]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M.
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock \emph{Proceedings of the National Academy of Sciences}, 115\penalty0
  (33):\penalty0 E7665--E7671, 2018.

\bibitem[Neal(1996)]{neal1996priors}
Neal, R.~M.
\newblock Priors for infinite networks.
\newblock \emph{Bayesian learning for neural networks}, pp.\  29--53, 1996.

\bibitem[Numpy()]{numpy_atan2}
Numpy.
\newblock {Atan2 function}.
\newblock URL
  \url{https://github.com/numpy/numpy/blob/5c7b7b69cc3a6ea0bbbae2976445a169484e0a85/numpy/_core/src/npymath/npy_math_internal.h.src#L141}.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{paszke2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Rae et~al.(2021{\natexlab{a}})Rae, Borgeaud, Cai, Millican, Hoffmann,
  Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021gopher}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
  Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021{\natexlab{a}}.

\bibitem[Rae et~al.(2021{\natexlab{b}})Rae, Borgeaud, Cai, Millican, Hoffmann,
  Song, Aslanides, Henderson, Ring, Young, et~al.]{rae2021scaling}
Rae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,
  Aslanides, J., Henderson, S., Ring, R., Young, S., et~al.
\newblock Scaling language models: Methods, analysis \& insights from training
  gopher.
\newblock \emph{arXiv preprint arXiv:2112.11446}, 2021{\natexlab{b}}.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{t5}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (140):\penalty0 1--67, 2020.
\newblock URL \url{http://jmlr.org/papers/v21/20-074.html}.

\bibitem[Rajbhandari et~al.(2020)Rajbhandari, Rasley, Ruwase, and
  He]{rajbhandari2020zero}
Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y.
\newblock Zero: Memory optimizations toward training trillion parameter models.
\newblock In \emph{SC20: International Conference for High Performance
  Computing, Networking, Storage and Analysis}, pp.\  1--16. IEEE, 2020.

\bibitem[Rotskoff \& Vanden-Eijnden(2018)Rotskoff and
  Vanden-Eijnden]{rotskoff2018parameters}
Rotskoff, G. and Vanden-Eijnden, E.
\newblock Parameters as interacting particles: long time convergence and
  asymptotic error scaling of neural networks.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Scao et~al.(2022)Scao, Fan, Akiki, Pavlick, Ili{\'c}, Hesslow,
  Castagn{\'e}, Luccioni, Yvon, et~al.]{workshop2022bloom}
Scao, T.~L., Fan, A., Akiki, C., Pavlick, E., Ili{\'c}, S., Hesslow, D.,
  Castagn{\'e}, R., Luccioni, A.~S., Yvon, F., et~al.
\newblock Bloom: A 176b-parameter open-access multilingual language model.
\newblock \emph{BigScience Workshop}, 2022.

\bibitem[Shallue et~al.(2019)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2019measuring}
Shallue, C.~J., Lee, J., Antognini, J., Sohl-Dickstein, J., Frostig, R., and
  Dahl, G.~E.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (112):\penalty0 1--49, 2019.

\bibitem[Shazeer(2020)]{noam_unit_scaling}
Shazeer, N.
\newblock {Mesh TensorFlow - Model Parallelism Made Easier; comments on unit
  scaling convention}, 12 2020.
\newblock URL
  \url{https://github.com/tensorflow/mesh/blob/master/mesh_tensorflow/layers.py#L48-L98}.

\bibitem[Shazeer \& Stern(2018)Shazeer and Stern]{shazeer2018adafactor}
Shazeer, N. and Stern, M.
\newblock Adafactor: Adaptive learning rates with sublinear memory cost.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4596--4604. PMLR, 2018.

\bibitem[Sohl-Dickstein et~al.(2020)Sohl-Dickstein, Novak, Schoenholz, and
  Lee]{sohl2020infinite}
Sohl-Dickstein, J., Novak, R., Schoenholz, S.~S., and Lee, J.
\newblock On the infinite width limit of neural networks with a standard
  parameterization.
\newblock \emph{arXiv preprint arXiv:2001.07301}, 2020.

\bibitem[Thoppilan et~al.(2022)Thoppilan, De~Freitas, Hall, Shazeer,
  Kulshreshtha, Cheng, Jin, Bos, Baker, Du, et~al.]{thoppilan2022lamda}
Thoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,
  H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al.
\newblock Lamda: Language models for dialog applications.
\newblock \emph{arXiv preprint arXiv:2201.08239}, 2022.

\bibitem[Touvron et~al.(2023{\natexlab{a}})Touvron, Lavril, Izacard, Martinet,
  Lachaux, Lacroix, Rozi{\`e}re, Goyal, Hambro, Azhar,
  et~al.]{touvron2023llama}
Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix,
  T., Rozi{\`e}re, B., Goyal, N., Hambro, E., Azhar, F., et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock \emph{arXiv preprint arXiv:2302.13971}, 2023{\natexlab{a}}.

\bibitem[Touvron et~al.(2023{\natexlab{b}})Touvron, Martin, Stone, Albert,
  Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale,
  et~al.]{touvron2023llama2}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y.,
  Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023{\natexlab{b}}.

\bibitem[Wortsman et~al.(2023)Wortsman, Liu, Xiao, Everett, Alemi, Adlam,
  Co-Reyes, Gur, Kumar, Novak, et~al.]{wortsman2023small}
Wortsman, M., Liu, P.~J., Xiao, L., Everett, K., Alemi, A., Adlam, B.,
  Co-Reyes, J.~D., Gur, I., Kumar, A., Novak, R., et~al.
\newblock Small-scale proxies for large-scale transformer training
  instabilities.
\newblock \emph{arXiv preprint arXiv:2309.14322}, 2023.

\bibitem[Xiong et~al.(2020)Xiong, Yang, He, Zheng, Zheng, Xing, Zhang, Lan,
  Wang, and Liu]{xiong2020layer}
Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan,
  Y., Wang, L., and Liu, T.
\newblock On layer normalization in the transformer architecture.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  10524--10533. PMLR, 2020.

\bibitem[Yaida(2022)]{yaida2022meta}
Yaida, S.
\newblock Meta-principled family of hyperparameter scaling strategies.
\newblock \emph{arXiv preprint arXiv:2210.04909}, 2022.

\bibitem[Yang \& Hu(2021)Yang and Hu]{yang2021tensoriv}
Yang, G. and Hu, E.~J.
\newblock Tensor programs iv: Feature learning in infinite-width neural
  networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  11727--11737. PMLR, 2021.

\bibitem[Yang \& Littwin(2023)Yang and Littwin]{yang2023tensorivb}
Yang, G. and Littwin, E.
\newblock Tensor programs ivb: Adaptive optimization in the infinite-width
  limit.
\newblock \emph{arXiv preprint arXiv:2308.01814}, 2023.

\bibitem[Yang et~al.(2022)Yang, Hu, Babuschkin, Sidor, Liu, Farhi, Ryder,
  Pachocki, Chen, and Gao]{yang2022tensorv}
Yang, G., Hu, E.~J., Babuschkin, I., Sidor, S., Liu, X., Farhi, D., Ryder, N.,
  Pachocki, J., Chen, W., and Gao, J.
\newblock Tensor programs v: Tuning large neural networks via zero-shot
  hyperparameter transfer.
\newblock \emph{arXiv preprint arXiv:2203.03466}, 2022.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Simon, and
  Bernstein]{yang2023spectral}
Yang, G., Simon, J.~B., and Bernstein, J.
\newblock A spectral condition for feature learning.
\newblock \emph{arXiv preprint arXiv:2310.17813}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Yu, Zhu, and
  Hayou]{yang2023tensorvi}
Yang, G., Yu, D., Zhu, C., and Hayou, S.
\newblock Tensor programs vi: Feature learning in infinite-depth neural
  networks.
\newblock \emph{arXiv preprint arXiv:2310.02244}, 2023{\natexlab{b}}.

\bibitem[Zhai et~al.(2023)Zhai, Likhomanenko, Littwin, Busbridge, Ramapuram,
  Zhang, Gu, and Susskind]{zhai2023stabilizing}
Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang,
  Y., Gu, J., and Susskind, J.~M.
\newblock Stabilizing transformer training by preventing attention entropy
  collapse.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  40770--40803. PMLR, 2023.

\bibitem[Zhai et~al.(2021)Zhai, Kolesnikov, Houlsby, and
  Beyer]{zhai2021scaling}
Zhai, X., Kolesnikov, A., Houlsby, N., and Beyer, L.
\newblock Scaling vision transformers, 2021.
\newblock \url{https://arxiv.org/abs/2106.04560}.

\bibitem[Zhang et~al.(2019)Zhang, Li, Nado, Martens, Sachdeva, Dahl, Shallue,
  and Grosse]{zhang2019algorithmic}
Zhang, G., Li, L., Nado, Z., Martens, J., Sachdeva, S., Dahl, G., Shallue, C.,
  and Grosse, R.~B.
\newblock Which algorithmic choices matter at which batch sizes? insights from
  a noisy quadratic model.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zoph et~al.(2022)Zoph, Bello, Kumar, Du, Huang, Dean, Shazeer, and
  Fedus]{zoph2022st}
Zoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean, J., Shazeer, N., and
  Fedus, W.
\newblock St-moe: Designing stable and transferable sparse expert models.
\newblock \emph{arXiv preprint arXiv:2202.08906}, 2022.

\end{thebibliography}
