\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andoni et~al.(2014)Andoni, Panigrahy, Valiant, and
  Zhang]{andoni2014learning}
A.~Andoni, R.~Panigrahy, G.~Valiant, and L.~Zhang.
\newblock Learning polynomials with neural networks.
\newblock In \emph{Proceedings of the 31st International Conference on Machine
  Learning}, pages 1908--1916, 2014.

\bibitem[Anselmi et~al.(2015)Anselmi, Rosasco, Tan, and
  Poggio]{anselmi2015deep}
F.~Anselmi, L.~Rosasco, C.~Tan, and T.~Poggio.
\newblock Deep convolutional networks are hierarchical kernel machines.
\newblock \emph{arXiv:1508.01084}, 2015.

\bibitem[Anthony and Bartlet(1999)]{AnthonyBa99}
M.~Anthony and P.~Bartlet.
\newblock \emph{Neural Network Learning: Theoretical Foundations}.
\newblock Cambridge University Press, 1999.

\bibitem[Arora et~al.(2014)Arora, Bhaskara, Ge, and Ma]{arora2014provable}
S.~Arora, A.~Bhaskara, R.~Ge, and T.~Ma.
\newblock Provable bounds for learning some deep representations.
\newblock In \emph{Proceedings of The 31st International Conference on Machine
  Learning}, pages 584--592, 2014.

\bibitem[Bach(2014)]{bach2014breaking}
F.~Bach.
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock \emph{arXiv:1412.8690}, 2014.

\bibitem[Bach(2015)]{bach2015equivalence}
F.~Bach.
\newblock On the equivalence between kernel quadrature rules and random feature
  expansions.
\newblock 2015.

\bibitem[Barron(1993)]{Barron93}
A.R. Barron.
\newblock Universal approximation bounds for superposition of a sigmoidal
  function.
\newblock \emph{IEEE Transactions on Information Theory}, 39\penalty0
  (3):\penalty0 930--945, 1993.

\bibitem[Bartlett and Mendelson(2002)]{BartlettMe02}
P.~L. Bartlett and S.~Mendelson.
\newblock Rademacher and {G}aussian complexities: {R}isk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3:\penalty0 463--482,
  2002.

\bibitem[Bartlett(1998)]{Bartlett98}
P.L. Bartlett.
\newblock The sample complexity of pattern classification with neural networks:
  the size of the weights is more important than the size of the network.
\newblock \emph{IEEE Transactions on Information Theory}, 44\penalty0
  (2):\penalty0 525--536, March 1998.

\bibitem[Baum and Haussler(1989)]{BaumHa89}
E.B. Baum and D.~Haussler.
\newblock What size net gives valid generalization?
\newblock \emph{Neural Computation}, 1\penalty0 (1):\penalty0 151--160, 1989.

\bibitem[Bo et~al.(2011)Bo, Lai, Ren, and Fox]{bo2011object}
L.~Bo, K.~Lai, X.~Ren, and D.~Fox.
\newblock Object recognition with hierarchical kernel descriptors.
\newblock In \emph{Computer Vision and Pattern Recognition (CVPR), 2011 IEEE
  Conference on}, pages 1729--1736. IEEE, 2011.

\bibitem[Bruna and Mallat(2013)]{bruna2013invariant}
J.~Bruna and S.~Mallat.
\newblock Invariant scattering convolution networks.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 35\penalty0 (8):\penalty0 1872--1886, 2013.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Y.~Cho and L.K. Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  342--350, 2009.

\bibitem[Choromanska et~al.(2015)Choromanska, Henaff, Mathieu, Ben~Arous, and
  LeCun]{choromanska2015loss}
A.~Choromanska, M.~Henaff, M.~Mathieu, G.~Ben~Arous, and Y.~LeCun.
\newblock The loss surfaces of multilayer networks.
\newblock In \emph{AISTATS}, pages 192--204, 2015.

\bibitem[Cox and Pinto(2011)]{cox2011beyond}
D.~Cox and N.~Pinto.
\newblock Beyond simple features: A large-scale feature search approach to
  unconstrained face recognition.
\newblock In \emph{Automatic Face \& Gesture Recognition and Workshops (FG
  2011), 2011 IEEE International Conference on}, pages 8--15. IEEE, 2011.

\bibitem[Daniely(2016)]{daniely2015complexity}
A.~Daniely.
\newblock Complexity theoretic limitations on learning halfspaces.
\newblock In \emph{STOC}, 2016.

\bibitem[Daniely and Shalev-Shwartz(2016)]{danielySh2014}
A.~Daniely and S.~Shalev-Shwartz.
\newblock Complexity theoretic limitations on learning {DNF}s.
\newblock In \emph{COLT}, 2016.

\bibitem[Daniely et~al.(2014)Daniely, Linial, and
  Shalev-Shwartz]{daniely2013average}
A.~Daniely, N.~Linial, and S.~Shalev-Shwartz.
\newblock From average case complexity to improper learning complexity.
\newblock In \emph{STOC}, 2014.

\bibitem[Giryes et~al.(2015)Giryes, Sapiro, and Bronstein]{giryes2015deep}
R.~Giryes, G.~Sapiro, and A.M. Bronstein.
\newblock Deep neural networks with random gaussian weights: A universal
  classification strategy?
\newblock \emph{arXiv preprint arXiv:1504.08291}, 2015.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
X.~Glorot and Y.~Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pages 249--256, 2010.

\bibitem[Grauman and Darrell(2005)]{grauman2005pyramid}
K.~Grauman and T.~Darrell.
\newblock The pyramid match kernel: Discriminative classification with sets of
  image features.
\newblock In \emph{Tenth IEEE International Conference on Computer Vision},
  volume~2, pages 1458--1465, 2005.

\bibitem[Hardt et~al.(2015)Hardt, Recht, and Singer]{hardt2015train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock \emph{arXiv:1509.01240}, 2015.

\bibitem[Harris(1954)]{harris1954distributional}
Z.S. Harris.
\newblock Distributional structure.
\newblock \emph{Word}, 1954.

\bibitem[Hazan and Jaakkola(2015)]{hazan2015steps}
T.~Hazan and T.~Jaakkola.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{arXiv:1508.05133}, 2015.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv:1502.03167}, 2015.

\bibitem[Jarrett et~al.(2009)Jarrett, Kavukcuoglu, Ranzato, and
  LeCun]{jarrett2009best}
K.~Jarrett, K.~Kavukcuoglu, M.~Ranzato, and Y.~LeCun.
\newblock What is the best multi-stage architecture for object recognition?
\newblock In \emph{Computer Vision, 2009 IEEE 12th International Conference
  on}, pages 2146--2153. IEEE, 2009.

\bibitem[Kar and Karnick(2012)]{kar2012random}
P.~Kar and H.~Karnick.
\newblock Random feature maps for dot product kernels.
\newblock \emph{arXiv:1201.6530}, 2012.

\bibitem[Karp and Lipton(1980)]{karp1980some}
R.M. Karp and R.J. Lipton.
\newblock Some connections between nonuniform and uniform complexity classes.
\newblock In \emph{Proceedings of the twelfth annual ACM symposium on Theory of
  computing}, pages 302--309. ACM, 1980.

\bibitem[Kearns and Valiant(1989)]{KearnsVa89}
M.~Kearns and L.G. Valiant.
\newblock Cryptographic limitations on learning {B}oolean formulae and finite
  automata.
\newblock In \emph{STOC}, pages 433--444, May 1989.

\bibitem[Klivans and Sherstov(2006)]{KlivansSh06}
A.R. Klivans and A.A. Sherstov.
\newblock Cryptographic hardness for learning intersections of halfspaces.
\newblock In \emph{FOCS}, 2006.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky2012imagenet}
A.~Krizhevsky, I.~Sutskever, and G.E. Hinton.
\newblock Imagenet classification with deep convolutional neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  1097--1105, 2012.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
Y.~LeCun, L.~Bottou, Y.~Bengio, and P.~Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Y.~LeCun, Y.~Bengio, and G.~Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Levy and Goldberg(2014)]{levy2014neural}
O.~Levy and Y.~Goldberg.
\newblock Neural word embedding as implicit matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2177--2185, 2014.

\bibitem[Livni et~al.(2014)Livni, Shalev-Shwartz, and
  Shamir]{livni2014computational}
R.~Livni, S.~Shalev-Shwartz, and O.~Shamir.
\newblock On the computational efficiency of training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  855--863, 2014.

\bibitem[Mairal et~al.(2014)Mairal, Koniusz, Harchaoui, and
  Schmid]{mairal2014convolutional}
J.~Mairal, P.~Koniusz, Z.~Harchaoui, and Cordelia Schmid.
\newblock Convolutional kernel networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2627--2635, 2014.

\bibitem[Mikolov et~al.(2013)Mikolov, Sutskever, Chen, Corrado, and
  Dean]{mikolov2013distributed}
T.~Mikolov, I.~Sutskever, K.~Chen, G.S. Corrado, and J.~Dean.
\newblock Distributed representations of words and phrases and their
  compositionality.
\newblock In \emph{NIPS}, pages 3111--3119, 2013.

\bibitem[Neal(2012)]{neal2012bayesian}
R.M. Neal.
\newblock \emph{Bayesian learning for neural networks}, volume 118.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Neyshabur et~al.(2015{\natexlab{a}})Neyshabur, Salakhutdinov, and
  Srebro]{neyshabur2015path}
B.~Neyshabur, R.~R Salakhutdinov, and N.~Srebro.
\newblock Path-{SGD}: {P}ath-normalized optimization in deep neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2413--2421, 2015{\natexlab{a}}.

\bibitem[Neyshabur et~al.(2015{\natexlab{b}})Neyshabur, Srebro, and
  Tomioka]{behnam2015norm}
B.~Neyshabur, N.~Srebro, and R.~Tomioka.
\newblock Norm-based capacity control in neural networks.
\newblock In \emph{COLT}, 2015{\natexlab{b}}.

\bibitem[O'Donnell(2014)]{o2014analysis}
R.~O'Donnell.
\newblock \emph{Analysis of boolean functions}.
\newblock Cambridge University Press, 2014.

\bibitem[Pennington et~al.(2015)Pennington, Yu, and
  Kumar]{pennington2015spherical}
J.~Pennington, F.~Yu, and S.~Kumar.
\newblock Spherical random features for polynomial kernels.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1837--1845, 2015.

\bibitem[Pinto and Cox(2012)]{pinto2012evaluation}
N.~Pinto and D.~Cox.
\newblock An evaluation of the invariance properties of a biologically-inspired
  system for unconstrained face recognition.
\newblock In \emph{Bio-Inspired Models of Network, Information, and Computing
  Systems}, pages 505--518. Springer, 2012.

\bibitem[Pinto et~al.(2009)Pinto, Doukhan, DiCarlo, and Cox]{pinto2009high}
N.~Pinto, D.~Doukhan, J.J. DiCarlo, and D.D. Cox.
\newblock A high-throughput screening approach to discovering good forms of
  biologically inspired visual representation.
\newblock \emph{PLoS Computational Biology}, 5\penalty0 (11):\penalty0
  e1000579, 2009.

\bibitem[Rahimi and Recht(2007)]{RahimiRe07}
A.~Rahimi and B.~Recht.
\newblock Random features for large-scale kernel machines.
\newblock In \emph{NIPS}, pages 1177--1184, 2007.

\bibitem[Rahimi and Recht(2009)]{rahimi2009weighted}
A.~Rahimi and B.~Recht.
\newblock Weighted sums of random kitchen sinks: Replacing minimization with
  randomization in learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  1313--1320, 2009.

\bibitem[Safran and Shamir(2015)]{safran2015basin}
I.~Safran and O.~Shamir.
\newblock On the quality of the initial basin in overspecified neural networks.
\newblock \emph{arxiv:1511.04210}, 2015.

\bibitem[Saitoh(1988)]{Saitoh88}
S.~Saitoh.
\newblock \emph{Theory of reproducing kernels and its applications}.
\newblock Longman Scientific \& Technical England, 1988.

\bibitem[Saxe et~al.(2011)Saxe, Koh, Chen, Bhand, Suresh, and
  Ng]{saxe2011random}
A.~Saxe, P.W. Koh, Z.~Chen, M.~Bhand, B.~Suresh, and A.Y. Ng.
\newblock On random weights and unsupervised feature learning.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning (ICML-11)}, pages 1089--1096, 2011.

\bibitem[Schoenberg et~al.(1942)]{schoenberg1942positive}
I.J. Schoenberg et~al.
\newblock Positive definite functions on spheres.
\newblock \emph{Duke Mathematical Journal}, 9\penalty0 (1):\penalty0 96--108,
  1942.

\bibitem[Sch{\"o}lkopf et~al.(1998)Sch{\"o}lkopf, Simard, Smola, and
  Vapnik]{scholkopf1998prior}
B.~Sch{\"o}lkopf, P.~Simard, A.~Smola, and V.~Vapnik.
\newblock Prior knowledge in support vector kernels.
\newblock In \emph{Advances in Neural Information Processing Systems 10}, pages
  640--646. MIT Press, 1998.

\bibitem[Sedghi and Anandkumar(2014)]{sedghi2014provable}
H.~Sedghi and A.~Anandkumar.
\newblock Provable methods for training neural networks with sparse
  connectivity.
\newblock \emph{arXiv:1412.2693}, 2014.

\bibitem[Shalev-Shwartz and Ben-David(2014)]{shalev2014understanding}
S.~Shalev-Shwartz and S.~Ben-David.
\newblock \emph{Understanding Machine Learning: From Theory to Algorithms}.
\newblock Cambridge University Press, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
N.~Srivastava, G.~Hinton, A.~Krizhevsky, I.~Sutskever, and R.~Salakhutdinov.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
I.~Sutskever, O.~Vinyals, and Q.V. Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem[Williams(1997)]{williams1997infinite}
C.K.I. Williams.
\newblock Computation with infinite neural networks.
\newblock pages 295--301, 1997.

\end{thebibliography}
