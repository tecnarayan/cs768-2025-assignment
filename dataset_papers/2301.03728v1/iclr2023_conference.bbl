\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Aghajanyan et~al.(2022)Aghajanyan, Huang, Ross, Karpukhin, Xu, Goyal,
  Okhonko, Joshi, Ghosh, Lewis, et~al.]{CM3}
Armen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu~Xu, Naman
  Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, et~al.
\newblock Cm3: A causal masked multimodal model of the internet.
\newblock \emph{arXiv preprint arXiv:2201.07520}, 2022.

\bibitem[Arakelyan et~al.(2020)Arakelyan, Soghomonyan, and {The Aim team}]{AIM}
Gor Arakelyan, Gevorg Soghomonyan, and {The Aim team}.
\newblock {Aim}, 6 2020.
\newblock URL \url{https://github.com/aimhubio/aim}.

\bibitem[Ardila et~al.(2019)Ardila, Branson, Davis, Henretty, Kohler, Meyer,
  Morais, Saunders, Tyers, and Weber]{ardila2019common}
Rosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler,
  Josh Meyer, Reuben Morais, Lindsay Saunders, Francis~M Tyers, and Gregor
  Weber.
\newblock Common voice: A massively-multilingual speech corpus.
\newblock \emph{arXiv preprint arXiv:1912.06670}, 2019.

\bibitem[Baines et~al.(2021)Baines, Bhosale, Caggiano, Goyal, Goyal, Ott,
  Lefaudeux, Liptchinsky, Rabbat, Sheiffer, Sridhar, and Xu]{fairscale}
Mandeep Baines, Shruti Bhosale, Vittorio Caggiano, Naman Goyal, Siddharth
  Goyal, Myle Ott, Benjamin Lefaudeux, Vitaliy Liptchinsky, Mike Rabbat, Sam
  Sheiffer, Anjali Sridhar, and Min Xu.
\newblock {FairScale}: A general purpose modular {PyTorch} library for high
  performance and large scale training.
\newblock \url{https://github.com/facebookresearch/fairscale}, 2021.

\bibitem[Bavarian et~al.(2022)Bavarian, Jun, Tezak, Schulman, McLeavey, Tworek,
  and Chen]{bavarian2022efficient}
Mohammad Bavarian, Heewoo Jun, Nikolas Tezak, John Schulman, Christine
  McLeavey, Jerry Tworek, and Mark Chen.
\newblock Efficient training of language models to fill in the middle.
\newblock \emph{arXiv preprint arXiv:2207.14255}, 2022.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{gpt3}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Chen et~al.(2021)Chen, Tworek, Jun, Yuan, Pinto, Kaplan, Edwards,
  Burda, Joseph, Brockman, et~al.]{codex}
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de~Oliveira
  Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg
  Brockman, et~al.
\newblock Evaluating large language models trained on code.
\newblock \emph{arXiv preprint arXiv:2107.03374}, 2021.

\bibitem[Cherti et~al.(2022)Cherti, Beaumont, Wightman, Wortsman, Ilharco,
  Gordon, Schuhmann, Schmidt, and Jitsev]{CLIP_scaling_law}
Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel
  Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev.
\newblock Reproducible scaling laws for contrastive language-image learning.
\newblock \emph{arXiv preprint arXiv:2212.07143}, 2022.

\bibitem[Chilingaryan et~al.(2022)Chilingaryan, Tamoyan, Tevosyan, Babayan,
  Khondkaryan, Hambardzumyan, Navoyan, Khachatrian, and Aghajanyan]{BARTSMILES}
Gayane Chilingaryan, Hovhannes Tamoyan, Ani Tevosyan, Nelly Babayan, Lusine
  Khondkaryan, Karen Hambardzumyan, Zaven Navoyan, Hrant Khachatrian, and Armen
  Aghajanyan.
\newblock Bartsmiles: Generative masked language models for molecular
  representations.
\newblock \emph{arXiv preprint arXiv:2211.16349}, 2022.

\bibitem[Clark et~al.(2022)Clark, de~Las~Casas, Guy, Mensch, Paganini,
  Hoffmann, Damoc, Hechtman, Cai, Borgeaud, et~al.]{moe_scaling_laws}
Aidan Clark, Diego de~Las~Casas, Aurelia Guy, Arthur Mensch, Michela Paganini,
  Jordan Hoffmann, Bogdan Damoc, Blake Hechtman, Trevor Cai, Sebastian
  Borgeaud, et~al.
\newblock Unified scaling laws for routed language models.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4057--4086. PMLR, 2022.

\bibitem[Conneau et~al.(2019)Conneau, Khandelwal, Goyal, Chaudhary, Wenzek,
  Guzm{\'a}n, Grave, Ott, Zettlemoyer, and Stoyanov]{conneau2019unsupervised}
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume
  Wenzek, Francisco Guzm{\'a}n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and
  Veselin Stoyanov.
\newblock Unsupervised cross-lingual representation learning at scale.
\newblock \emph{arXiv preprint arXiv:1911.02116}, 2019.

\bibitem[Dettmers \& Zettlemoyer(2022)Dettmers and Zettlemoyer]{4_bit_scaling}
Tim Dettmers and Luke Zettlemoyer.
\newblock The case for 4-bit precision: k-bit inference scaling laws, 2022.
\newblock URL \url{https://arxiv.org/abs/2212.09720}.

\bibitem[Dettmers et~al.(2022)Dettmers, Lewis, Belkada, and Zettlemoyer]{int8}
Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer.
\newblock Llm. int8 (): 8-bit matrix multiplication for transformers at scale.
\newblock \emph{arXiv preprint arXiv:2208.07339}, 2022.

\bibitem[Droppo \& Elibol(2021)Droppo and Elibol]{speech_alexa_scaling_laws}
Jasha Droppo and Oguz Elibol.
\newblock Scaling laws for acoustic models.
\newblock In \emph{Interspeech 2021}, 2021.
\newblock URL
  \url{https://www.amazon.science/publications/scaling-laws-for-acoustic-models}.

\bibitem[Esser et~al.(2020)Esser, Rombach, and Ommer]{taming}
Patrick Esser, Robin Rombach, and Bj√∂rn Ommer.
\newblock Taming transformers for high-resolution image synthesis, 2020.

\bibitem[Fried et~al.(2022)Fried, Aghajanyan, Lin, Wang, Wallace, Shi, Zhong,
  Yih, Zettlemoyer, and Lewis]{INCODER}
Daniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi,
  Ruiqi Zhong, Wen-tau Yih, Luke Zettlemoyer, and Mike Lewis.
\newblock Incoder: A generative model for code infilling and synthesis.
\newblock \emph{arXiv preprint arXiv:2204.05999}, 2022.

\bibitem[Gafni et~al.(2022)Gafni, Polyak, Ashual, Sheynin, Parikh, and
  Taigman]{make_a_scene}
Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv
  Taigman.
\newblock Make-a-scene: Scene-based text-to-image generation with human priors.
\newblock \emph{arXiv preprint arXiv:2203.13131}, 2022.

\bibitem[Ghorbani et~al.(2021)Ghorbani, Firat, Freitag, Bapna, Krikun, Garcia,
  Chelba, and Cherry]{scaling_laws_nmt_ghorbani}
Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun,
  Xavier Garcia, Ciprian Chelba, and Colin Cherry.
\newblock Scaling laws for neural machine translation.
\newblock \emph{arXiv preprint arXiv:2109.07740}, 2021.

\bibitem[Gordon et~al.(2021)Gordon, Duh, and Kaplan]{scaling_laws_nmt}
Mitchell~A Gordon, Kevin Duh, and Jared Kaplan.
\newblock Data and parameter scaling laws for neural machine translation.
\newblock In \emph{Proceedings of the 2021 Conference on Empirical Methods in
  Natural Language Processing}, pp.\  5915--5922, Online and Punta Cana,
  Dominican Republic, November 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.emnlp-main.478}.
\newblock URL \url{https://aclanthology.org/2021.emnlp-main.478}.

\bibitem[Goyal et~al.(2021)Goyal, Du, Ott, Anantharaman, and
  Conneau]{goyal2021larger}
Naman Goyal, Jingfei Du, Myle Ott, Giri Anantharaman, and Alexis Conneau.
\newblock Larger-scale transformers for multilingual masked language modeling.
\newblock \emph{arXiv preprint arXiv:2105.00572}, 2021.

\bibitem[Henighan et~al.(2020)Henighan, Kaplan, Katz, Chen, Hesse, Jackson,
  Jun, Brown, Dhariwal, Gray, et~al.]{image_text_scaling}
Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob
  Jackson, Heewoo Jun, Tom~B Brown, Prafulla Dhariwal, Scott Gray, et~al.
\newblock Scaling laws for autoregressive generative modeling.
\newblock \emph{arXiv preprint arXiv:2010.14701}, 2020.

\bibitem[Hestness et~al.(2017)Hestness, Narang, Ardalani, Diamos, Jun,
  Kianinejad, Patwary, Ali, Yang, and Zhou]{deep_learning_scaling_2017}
Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun,
  Hassan Kianinejad, Md~Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou.
\newblock Deep learning scaling is predictable, empirically.
\newblock \emph{arXiv preprint arXiv:1712.00409}, 2017.

\bibitem[Hoffmann et~al.(2022)Hoffmann, Borgeaud, Mensch, Buchatskaya, Cai,
  Rutherford, Casas, Hendricks, Welbl, Clark, et~al.]{Chinchilla}
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
  Cai, Eliza Rutherford, Diego de~Las Casas, Lisa~Anne Hendricks, Johannes
  Welbl, Aidan Clark, et~al.
\newblock Training compute-optimal large language models.
\newblock \emph{arXiv preprint arXiv:2203.15556}, 2022.

\bibitem[Hsu et~al.(2022)Hsu, Verkuil, Liu, Lin, Hie, Sercu, Lerer, and
  Rives]{hsu2022learning}
Chloe Hsu, Robert Verkuil, Jason Liu, Zeming Lin, Brian Hie, Tom Sercu, Adam
  Lerer, and Alexander Rives.
\newblock Learning inverse folding from millions of predicted structures.
\newblock \emph{bioRxiv}, 2022.
\newblock \doi{10.1101/2022.04.10.487779}.
\newblock URL
  \url{https://www.biorxiv.org/content/early/2022/04/10/2022.04.10.487779}.

\bibitem[Hsu et~al.(2021)Hsu, Bolte, Tsai, Lakhotia, Salakhutdinov, and
  Mohamed]{hubert}
Wei-Ning Hsu, Benjamin Bolte, Yao-Hung~Hubert Tsai, Kushal Lakhotia, Ruslan
  Salakhutdinov, and Abdelrahman Mohamed.
\newblock Hubert: Self-supervised speech representation learning by masked
  prediction of hidden units.
\newblock \emph{IEEE/ACM Transactions on Audio, Speech, and Language
  Processing}, 29:\penalty0 3451--3460, 2021.
\newblock \doi{10.1109/TASLP.2021.3122291}.

\bibitem[Kalamkar et~al.(2019)Kalamkar, Mudigere, Mellempudi, Das, Banerjee,
  Avancha, Vooturi, Jammalamadaka, Huang, Yuen, Yang, Park, Heinecke,
  Georganas, Srinivasan, Kundu, Smelyanskiy, Kaul, and Dubey]{bf16}
Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal
  Banerjee, Sasikanth Avancha, Dharma~Teja Vooturi, Nataraj Jammalamadaka,
  Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke,
  Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy,
  Bharat Kaul, and Pradeep Dubey.
\newblock A study of bfloat16 for deep learning training, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.12322}.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom~B Brown, Benjamin Chess, Rewon
  Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.
\newblock Scaling laws for neural language models.
\newblock \emph{arXiv preprint arXiv:2001.08361}, 2020.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{ADAM}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{ICLR}, 2015.

\bibitem[Lee et~al.(2021)Lee, Chen, Wang, Gu, Ma, Polyak, Adi, He, Tang, Pino,
  et~al.]{lee2021direct}
Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Xutai Ma, Adam Polyak, Yossi
  Adi, Qing He, Yun Tang, Juan Pino, et~al.
\newblock Direct speech-to-speech translation with discrete units.
\newblock \emph{arXiv preprint arXiv:2107.05604}, 2021.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, et~al.]{pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock {PyTorch}: An imperative style, high-performance deep learning
  library.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Pratap et~al.(2020)Pratap, Xu, Sriram, Synnaeve, and
  Collobert]{pratap2020mls}
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan
  Collobert.
\newblock Mls: A large-scale multilingual dataset for speech research.
\newblock \emph{arXiv preprint arXiv:2012.03411}, 2020.

\bibitem[Ramesh et~al.(2021)Ramesh, Pavlov, Goh, Gray, Voss, Radford, Chen, and
  Sutskever]{DALLE}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock \emph{arXiv preprint arXiv:2102.12092}, 2021.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-maron, Gim{\'e}nez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi,
  Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{gato}
Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio~G{\'o}mez Colmenarejo,
  Alexander Novikov, Gabriel Barth-maron, Mai Gim{\'e}nez, Yury Sulsky, Jackie
  Kay, Jost~Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley
  Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar
  Bordbar, and Nando de~Freitas.
\newblock A generalist agent.
\newblock \emph{Transactions on Machine Learning Research}, 2022.
\newblock URL \url{https://openreview.net/forum?id=1ikK0kHjvj}.
\newblock Featured Certification.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman,
  Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{laion}
Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross
  Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell
  Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation
  image-text models.
\newblock \emph{arXiv preprint arXiv:2210.08402}, 2022.

\bibitem[Sennrich et~al.(2016)Sennrich, Haddow, and Birch]{BPE}
Rico Sennrich, Barry Haddow, and Alexandra Birch.
\newblock Neural machine translation of rare words with subword units.
\newblock In \emph{Proceedings of the 54th Annual Meeting of the Association
  for Computational Linguistics (Volume 1: Long Papers)}, pp.\  1715--1725,
  Berlin, Germany, August 2016. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/P16-1162}.
\newblock URL \url{https://aclanthology.org/P16-1162}.

\bibitem[Shaham et~al.(2022)Shaham, Elbayad, Goswami, Levy, and
  Bhosale]{interference_translation}
Uri Shaham, Maha Elbayad, Vedanuj Goswami, Omer Levy, and Shruti Bhosale.
\newblock Causes and cures for interference in multilingual translation.
\newblock \emph{arXiv preprint arXiv:2212.07530}, 2022.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  model parallelism, 2019.
\newblock URL \url{https://arxiv.org/abs/1909.08053}.

\bibitem[Wang et~al.(2021)Wang, Riviere, Lee, Wu, Talnikar, Haziza, Williamson,
  Pino, and Dupoux]{wang2021voxpopuli}
Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel
  Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux.
\newblock Voxpopuli: A large-scale multilingual speech corpus for
  representation learning, semi-supervised learning and interpretation.
\newblock \emph{arXiv preprint arXiv:2101.00390}, 2021.

\bibitem[Wang et~al.(2022)Wang, Yang, Men, Lin, Bai, Li, Ma, Zhou, Zhou, and
  Yang]{ofa}
Peng Wang, An~Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma,
  Chang Zhou, Jingren Zhou, and Hongxia Yang.
\newblock Ofa: Unifying architectures, tasks, and modalities through a simple
  sequence-to-sequence learning framework, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.03052}.

\bibitem[Yasunaga et~al.(2022)Yasunaga, Aghajanyan, Shi, James, Leskovec,
  Liang, Lewis, Zettlemoyer, and Yih]{RA_CM3}
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec,
  Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih.
\newblock Retrieval-augmented multimodal language modeling.
\newblock \emph{arXiv preprint arXiv:2211.12561}, 2022.

\bibitem[Zellers et~al.(2022)Zellers, Lu, Lu, Yu, Zhao, Salehi, Kusupati,
  Hessel, Farhadi, and Choi]{Merlotreserved}
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza
  Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.
\newblock Merlot reserve: Neural script knowledge through vision and language
  and sound.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition (CVPR)}, pp.\  16375--16387, June 2022.

\bibitem[Zhang et~al.(2022)Zhang, Roller, Goyal, Artetxe, Chen, Chen, Dewan,
  Diab, Li, Lin, et~al.]{OPT}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui
  Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock \emph{arXiv preprint arXiv:2205.01068}, 2022.

\end{thebibliography}
