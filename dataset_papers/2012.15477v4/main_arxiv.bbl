\begin{thebibliography}{}

\bibitem[Allen-Zhu and Li, 2019]{allen2019can}
Allen-Zhu, Z. and Li, Y. (2019).
\newblock What can resnet learn efficiently, going beyond kernels?
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  9017--9028.

\bibitem[Allen-Zhu and Li, 2020]{allen2020backward}
Allen-Zhu, Z. and Li, Y. (2020).
\newblock Backward feature correction: How deep learning performs deep
  learning.
\newblock {\em arXiv preprint arXiv:2001.04413}.

\bibitem[Allen-Zhu et~al., 2019]{allen2019convergence}
Allen-Zhu, Z., Li, Y., and Song, Z. (2019).
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In {\em Proceedings of International Conference on Machine Learning
  36}, pages 242--252.

\bibitem[Ara{\'u}jo et~al., 2019]{araujo2019mean}
Ara{\'u}jo, D., Oliveira, R.~I., and Yukimura, D. (2019).
\newblock A mean-field limit for certain deep neural networks.
\newblock {\em arXiv preprint arXiv:1906.00193}.

\bibitem[Bach, 2017]{bach2017breaking}
Bach, F. (2017).
\newblock Breaking the curse of dimensionality with convex neural networks.
\newblock {\em The Journal of Machine Learning Research}, 18(1):629--681.

\bibitem[Bai and Lee, 2019]{bai2019beyond}
Bai, Y. and Lee, J.~D. (2019).
\newblock Beyond linearization: On quadratic and higher-order approximation of
  wide neural networks.
\newblock {\em arXiv preprint arXiv:1910.01619}.

\bibitem[Bakry and {\'E}mery, 1985]{bakry1985diffusions}
Bakry, D. and {\'E}mery, M. (1985).
\newblock Diffusions hypercontractives in sem. probab. xix lnm 1123.

\bibitem[Bengio et~al., 2005]{bengio2006convex}
Bengio, Y., Le~Roux, N., Vincent, P., Delalleau, O., and Marcotte, P. (2005).
\newblock Convex neural networks.
\newblock In {\em Advances in neural information processing systems 18}.

\bibitem[Bou-Rabee and Eberle, 2021]{bou2021mixing}
Bou-Rabee, N. and Eberle, A. (2021).
\newblock Mixing time guarantees for unadjusted hamiltonian monte carlo.
\newblock {\em arXiv e-prints}, pages arXiv--2105.

\bibitem[Bou-Rabee and Schuh, 2020]{bou2020convergence}
Bou-Rabee, N. and Schuh, K. (2020).
\newblock Convergence of unadjusted hamiltonian monte carlo for mean-field
  models.
\newblock {\em arXiv preprint arXiv:2009.08735}.

\bibitem[Cao and Gu, 2019]{cao2019generalization}
Cao, Y. and Gu, Q. (2019).
\newblock Generalization bounds of stochastic gradient descent for wide and
  deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  10836--10846.

\bibitem[Chen et~al., 2020]{chen2020generalized}
Chen, Z., Cao, Y., Gu, Q., and Zhang, T. (2020).
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock {\em arXiv preprint arXiv:2002.04026}.

\bibitem[Cheng and Bartlett, 2017]{cheng2017convergence}
Cheng, X. and Bartlett, P. (2017).
\newblock Convergence of langevin mcmc in kl-divergence.
\newblock {\em arXiv preprint arXiv:1705.09048}.

\bibitem[Cheng et~al., 2018]{cheng2018underdamped}
Cheng, X., Chatterji, N.~S., Bartlett, P.~L., and Jordan, M.~I. (2018).
\newblock Underdamped langevin mcmc: A non-asymptotic analysis.
\newblock In {\em Conference on Learning Theory}, pages 300--323. PMLR.

\bibitem[Chizat, 2019]{chizat2019sparse}
Chizat, L. (2019).
\newblock Sparse optimization on measures with over-parameterized gradient
  descent.
\newblock {\em arXiv preprint arXiv:1907.10300}.

\bibitem[Chizat, 2021]{chizat2021convergence}
Chizat, L. (2021).
\newblock Convergence rates of gradient methods for convex optimization in the
  space of measures.
\newblock {\em arXiv preprint arXiv:2105.08368}.

\bibitem[Chizat and Bach, 2018a]{chizat2018note}
Chizat, L. and Bach, F. (2018a).
\newblock A note on lazy training in supervised differentiable programming.
\newblock {\em arXiv preprint arXiv:1812.07956}.

\bibitem[Chizat and Bach, 2018b]{chizat2018global}
Chizat, L. and Bach, F. (2018b).
\newblock On the global convergence of gradient descent for over-parameterized
  models using optimal transport.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  3040--3050.

\bibitem[Chu et~al., 2019]{chu2019probability}
Chu, C., Blanchet, J., and Glynn, P. (2019).
\newblock Probability functional descent: A unifying perspective on gans,
  variational inference, and reinforcement learning.
\newblock In {\em Proceedings of International Conference on Machine Learning
  36}, pages 1213--1222.

\bibitem[Dai et~al., 2016]{dai2016provable}
Dai, B., He, N., Dai, H., and Song, L. (2016).
\newblock Provable bayesian inference via particle mirror descent.
\newblock In {\em Proceedings of International Conference on Artificial
  Intelligence and Statistics 19}, pages 985--994.

\bibitem[Dalalyan, 2014]{dalalyan2014theoretical}
Dalalyan, A.~S. (2014).
\newblock Theoretical guarantees for approximate sampling from smooth and
  log-concave densities.
\newblock {\em arXiv preprint arXiv:1412.7392}.

\bibitem[Dalalyan, 2017]{dalalyan2017further}
Dalalyan, A.~S. (2017).
\newblock Further and stronger analogy between sampling and optimization:
  Langevin monte carlo and gradient descent.
\newblock {\em arXiv preprint arXiv:1704.04752}.

\bibitem[Daniely and Malach, 2020]{daniely2020learning}
Daniely, A. and Malach, E. (2020).
\newblock Learning parities with neural networks.
\newblock {\em arXiv preprint arXiv:2002.07400}.

\bibitem[Ding and Li, 2019]{ding2019ensemble}
Ding, Z. and Li, Q. (2019).
\newblock Ensemble kalman sampling: Mean-field limit and convergence analysis.
\newblock {\em arXiv preprint arXiv:1910.12923}.

\bibitem[Du et~al., 2019]{du2018gradient}
Du, S.~S., Zhai, X., Poczos, B., and Singh, A. (2019).
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations}.

\bibitem[Durmus et~al., 2019]{durmus2019analysis}
Durmus, A., Majewski, S., and Miasojedow, B. (2019).
\newblock Analysis of langevin monte carlo via convex optimization.
\newblock {\em Journal of Machine Learning Research}, 20(73):1--46.

\bibitem[Durmus and Moulines, 2017]{durmus2017nonasymptotic}
Durmus, A. and Moulines, E. (2017).
\newblock Nonasymptotic convergence analysis for the unadjusted langevin
  algorithm.
\newblock {\em The Annals of Applied Probability}, 27(3):1551--1587.

\bibitem[Dwivedi et~al., 2018]{dwivedi2018log}
Dwivedi, R., Chen, Y., Wainwright, M.~J., and Yu, B. (2018).
\newblock Log-concave sampling: Metropolis-hastings algorithms are fast!
\newblock In {\em Conference on Learning Theory}, pages 793--797. PMLR.

\bibitem[Eberle et~al., 2019]{eberle2019couplings}
Eberle, A., Guillin, A., Zimmer, R., et~al. (2019).
\newblock Couplings and quantitative contraction rates for langevin dynamics.
\newblock {\em Annals of Probability}, 47(4):1982--2010.

\bibitem[Erdogdu and Hosseinzadeh, 2020]{erdogdu2020convergence}
Erdogdu, M.~A. and Hosseinzadeh, R. (2020).
\newblock On the convergence of langevin monte carlo: The interplay between
  tail growth and smoothness.
\newblock {\em arXiv preprint arXiv:2005.13097}.

\bibitem[Erdogdu et~al., 2018]{erdogdu2018global}
Erdogdu, M.~A., Mackey, L., and Shamir, O. (2018).
\newblock Global non-convex optimization with discretized diffusions.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  9671--9680.

\bibitem[Garbuno-Inigo et~al., 2020]{garbuno2020interacting}
Garbuno-Inigo, A., Hoffmann, F., Li, W., and Stuart, A.~M. (2020).
\newblock Interacting langevin diffusions: Gradient structure and ensemble
  kalman sampler.
\newblock {\em SIAM Journal on Applied Dynamical Systems}, 19(1):412--441.

\bibitem[Ghorbani et~al., 2019a]{ghorbani2019limitations}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2019a).
\newblock Limitations of lazy training of two-layers neural network.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  9111--9121.

\bibitem[Ghorbani et~al., 2019b]{ghorbani2019linearized}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2019b).
\newblock Linearized two-layers neural networks in high dimension.
\newblock {\em arXiv preprint arXiv:1904.12191}.

\bibitem[Ghorbani et~al., 2020]{ghorbani2020neural}
Ghorbani, B., Mei, S., Misiakiewicz, T., and Montanari, A. (2020).
\newblock When do neural networks outperform kernel methods?
\newblock {\em arXiv preprint arXiv:2006.13409}.

\bibitem[Holley and Stroock, 1987]{holley1987logarithmic}
Holley, R. and Stroock, D. (1987).
\newblock Logarithmic sobolev inequalities and stochastic ising models.
\newblock {\em Journal of statistical physics}, 46(5-6):1159--1194.

\bibitem[Hsieh et~al., 2019]{hsieh2019finding}
Hsieh, Y.-P., Liu, C., and Cevher, V. (2019).
\newblock Finding mixed nash equilibria of generative adversarial networks.
\newblock In {\em Proceedings of International Conference on Machine Learning
  36}, pages 2810--2819.

\bibitem[Hu et~al., 2019]{hu2019mean}
Hu, K., Ren, Z., Siska, D., and Szpruch, L. (2019).
\newblock Mean-field langevin dynamics and energy landscape of neural networks.
\newblock {\em arXiv preprint arXiv:1905.07769}.

\bibitem[Imaizumi and Fukumizu, 2020]{imaizumi2020advantage}
Imaizumi, M. and Fukumizu, K. (2020).
\newblock Advantage of deep neural networks for estimating functions with
  singularity on curves.
\newblock {\em arXiv preprint arXiv:2011.02256}.

\bibitem[Jabir et~al., 2019]{jabir2019mean}
Jabir, J.-F., {\v{S}}i{\v{s}}ka, D., and Szpruch, {\L}. (2019).
\newblock Mean-field neural odes via relaxed optimal control.
\newblock {\em arXiv preprint arXiv:1912.05475}.

\bibitem[Jacot et~al., 2018]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C. (2018).
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  8580--8589.

\bibitem[Javanmard et~al., 2019]{javanmard2019analysis}
Javanmard, A., Mondelli, M., and Montanari, A. (2019).
\newblock Analysis of a two-layer neural network via displacement convexity.
\newblock {\em arXiv preprint arXiv:1901.01375}.

\bibitem[Ji and Telgarsky, 2019]{ji2019polylogarithmic}
Ji, Z. and Telgarsky, M. (2019).
\newblock Polylogarithmic width suffices for gradient descent to achieve
  arbitrarily small test error with shallow relu networks.
\newblock {\em arXiv preprint arXiv:1909.12292}.

\bibitem[Jordan and Kinderlehrer, 1996]{jordan199618}
Jordan, R. and Kinderlehrer, D. (1996).
\newblock 18. an extended variational.
\newblock {\em Partial differential equations and applications: collected
  papers in honor of Carlo Pucci}, 177:187.

\bibitem[Jordan et~al., 1998]{jordan1998variational}
Jordan, R., Kinderlehrer, D., and Otto, F. (1998).
\newblock The variational formulation of the fokker--planck equation.
\newblock {\em SIAM journal on mathematical analysis}, 29(1):1--17.

\bibitem[Kent et~al., 2021]{kent2021frank}
Kent, C., Blanchet, J., and Glynn, P. (2021).
\newblock Frank-wolfe methods in probability space.
\newblock {\em arXiv preprint arXiv:2105.05352}.

\bibitem[Kozachenko and Leonenko, 1987]{kozachenko1987sample}
Kozachenko, L. and Leonenko, N.~N. (1987).
\newblock Sample estimate of the entropy of a random vector.
\newblock {\em Problemy Peredachi Informatsii}, 23(2):9--16.

\bibitem[Laurent and Massart, 2000]{laurent2000adaptive}
Laurent, B. and Massart, P. (2000).
\newblock Adaptive estimation of a quadratic functional by model selection.
\newblock {\em The Annals of statistics}, 28(5):1302--1338.

\bibitem[Li et~al., 2019]{li2019stochastic}
Li, X., Wu, Y., Mackey, L., and Erdogdu, M.~A. (2019).
\newblock Stochastic runge-kutta accelerates langevin monte carlo and beyond.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  7748--7760.

\bibitem[Li et~al., 2020]{li2020learning}
Li, Y., Ma, T., and Zhang, H.~R. (2020).
\newblock Learning over-parametrized two-layer neural networks beyond ntk.
\newblock In {\em Proceedings of Conference on Learning Theory 33}, pages
  2613--2682.

\bibitem[Liu and Wang, 2016]{liu2016stein}
Liu, Q. and Wang, D. (2016).
\newblock Stein variational gradient descent: A general purpose bayesian
  inference algorithm.
\newblock In {\em Advances in neural information processing systems 29}, pages
  2378--2386.

\bibitem[Lu et~al., 2019]{lu2019scaling}
Lu, J., Lu, Y., and Nolen, J. (2019).
\newblock Scaling limit of the stein variational gradient descent: The mean
  field regime.
\newblock {\em SIAM Journal on Mathematical Analysis}, 51(2):648--671.

\bibitem[Lu et~al., 2020]{lu2020mean}
Lu, Y., Ma, C., Lu, Y., Lu, J., and Ying, L. (2020).
\newblock A mean-field analysis of deep resnet and beyond: Towards provable
  optimization via overparameterization from depth.
\newblock {\em arXiv preprint arXiv:2003.05508}.

\bibitem[Mattingly et~al., 2002]{mattingly2002ergodicity}
Mattingly, J.~C., Stuart, A.~M., and Higham, D.~J. (2002).
\newblock Ergodicity for sdes and approximations: locally lipschitz vector
  fields and degenerate noise.
\newblock {\em Stochastic processes and their applications}, 101(2):185--232.

\bibitem[Mei et~al., 2018]{mei2018mean}
Mei, S., Montanari, A., and Nguyen, P.-M. (2018).
\newblock A mean field view of the landscape of two-layer neural networks.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(33):E7665--E7671.

\bibitem[Menz and Schlichting, 2014]{menz2014poincare}
Menz, G. and Schlichting, A. (2014).
\newblock Poincar{\'e} and logarithmic sobolev inequalities by decomposition of
  the energy landscape.
\newblock {\em The Annals of Probability}, 42(5):1809--1884.

\bibitem[Mohri et~al., 2012]{mohri2012foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2012).
\newblock {\em Foundations of Machine Learning}.
\newblock The MIT Press.

\bibitem[Nesterov, 2005]{nesterov2005smooth}
Nesterov, Y. (2005).
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Mathematical programming}, 103(1):127--152.

\bibitem[Nesterov, 2009]{nesterov2009primal}
Nesterov, Y. (2009).
\newblock Primal-dual subgradient methods for convex problems.
\newblock {\em Mathematical programming}, 120(1):221--259.

\bibitem[Nguyen and Pham, 2020]{nguyen2020rigorous}
Nguyen, P.-M. and Pham, H.~T. (2020).
\newblock A rigorous framework for the mean field limit of multilayer neural
  networks.
\newblock {\em arXiv preprint arXiv:2001.11443}.

\bibitem[Nitanda et~al., 2019]{nitanda2019gradient}
Nitanda, A., Chinot, G., and Suzuki, T. (2019).
\newblock Gradient descent can learn less over-parameterized two-layer neural
  networks on classification problems.
\newblock {\em arXiv preprint arXiv:1905.09870}.

\bibitem[Nitanda and Suzuki, 2017]{nitanda2017stochastic}
Nitanda, A. and Suzuki, T. (2017).
\newblock Stochastic particle gradient descent for infinite ensembles.
\newblock {\em arXiv preprint arXiv:1712.05438}.

\bibitem[Nitanda and Suzuki, 2021]{nitanda2020optimal}
Nitanda, A. and Suzuki, T. (2021).
\newblock Optimal rates for averaged stochastic gradient descent under neural
  tangent kernel regime.
\newblock In {\em Proceedings of the 9th International Conference on Learning
  Representations}.

\bibitem[Otto and Villani, 2000]{otto2000generalization}
Otto, F. and Villani, C. (2000).
\newblock Generalization of an inequality by talagrand and links with the
  logarithmic sobolev inequality.
\newblock {\em Journal of Functional Analysis}, 173(2):361--400.

\bibitem[Pedregosa et~al., 2011]{scikit-learn}
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
  O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
  Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
  (2011).
\newblock Scikit-learn: Machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 12:2825--2830.

\bibitem[Pham and Nguyen, 2021]{pham2021global}
Pham, H.~T. and Nguyen, P.-M. (2021).
\newblock Global convergence of three-layer neural networks in the mean field
  regime.
\newblock In {\em Proceedings of the 9th International Conference on Learning
  Representations}.

\bibitem[Raginsky et~al., 2017]{raginsky2017non}
Raginsky, M., Rakhlin, A., and Telgarsky, M. (2017).
\newblock Non-convex learning via stochastic gradient langevin dynamics: a
  nonasymptotic analysis.
\newblock {\em arXiv preprint arXiv:1702.03849}.

\bibitem[Roberts and Tweedie, 1996]{roberts1996exponential}
Roberts, G.~O. and Tweedie, R.~L. (1996).
\newblock Exponential convergence of langevin distributions and their discrete
  approximations.
\newblock {\em Bernoulli}, 2(4):341--363.

\bibitem[Rotskoff et~al., 2019]{rotskoff2019global}
Rotskoff, G.~M., Jelassi, S., Bruna, J., and Vanden-Eijnden, E. (2019).
\newblock Global convergence of neuron birth-death dynamics.
\newblock In {\em Proceedings of International Conference on Machine Learning
  36}, pages 9689--9698.

\bibitem[Rotskoff and Vanden-Eijnden, 2018]{rotskoff2018trainability}
Rotskoff, G.~M. and Vanden-Eijnden, E. (2018).
\newblock Trainability and accuracy of neural networks: An interacting particle
  system approach.
\newblock {\em arXiv preprint arXiv:1805.00915}.

\bibitem[Schmidt-Hieber, 2020]{schmidt2020nonparametric}
Schmidt-Hieber, J. (2020).
\newblock Nonparametric regression using deep neural networks with relu
  activation function.
\newblock {\em The Annals of Statistics}, 48(4):1875--1897.

\bibitem[Shalev-Shwartz and Ben-David, 2014]{shalev2014understanding}
Shalev-Shwartz, S. and Ben-David, S. (2014).
\newblock {\em Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press.

\bibitem[Sirignano and Spiliopoulos, 2020]{sirignano2020mean}
Sirignano, J. and Spiliopoulos, K. (2020).
\newblock Mean field analysis of neural networks: A central limit theorem.
\newblock {\em Stochastic Processes and their Applications}, 130(3):1820--1852.

\bibitem[Suzuki, 2019]{suzuki2018adaptivity}
Suzuki, T. (2019).
\newblock Adaptivity of deep relu network for learning in besov and mixed
  smooth besov spaces: optimal rate and curse of dimensionality.
\newblock In {\em Proceedings of the 7th International Conference on Learning
  Representations}.

\bibitem[Suzuki, 2020]{suzuki2020generalization}
Suzuki, T. (2020).
\newblock Generalization bound of globally optimal non-convex neural network
  training: Transportation map estimation by infinite dimensional langevin
  dynamics.
\newblock In {\em Advances in Neural Information Processing Systems 33}.

\bibitem[Suzuki and Nitanda, 2021]{suzuki2019deep}
Suzuki, T. and Nitanda, A. (2021).
\newblock Deep learning is adaptive to intrinsic dimensionality of model
  smoothness in anisotropic besov space.
\newblock In {\em Advances in Neural Information Processing Systems 34}.

\bibitem[Vempala and Wibisono, 2019]{vempala2019rapid}
Vempala, S. and Wibisono, A. (2019).
\newblock Rapid convergence of the unadjusted langevin algorithm: Isoperimetry
  suffices.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  8094--8106.

\bibitem[Wei et~al., 2019]{wei2019regularization}
Wei, C., Lee, J.~D., Liu, Q., and Ma, T. (2019).
\newblock Regularization matters: Generalization and optimization of neural
  nets vs their induced kernel.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  9712--9724.

\bibitem[Wibisono, 2018]{wibisono2018sampling}
Wibisono, A. (2018).
\newblock Sampling as optimization in the space of measures: The langevin
  dynamics as a composite optimization problem.
\newblock In {\em Proceedings of Conference on Learning Theory 31}, pages
  2093--3027.

\bibitem[Xiao, 2009]{xiao2009dual}
Xiao, L. (2009).
\newblock Dual averaging method for regularized stochastic learning and online
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems 22}, pages
  2116--2124.

\bibitem[Xu et~al., 2018]{xu2018global}
Xu, P., Chen, J., Zou, D., and Gu, Q. (2018).
\newblock Global convergence of langevin dynamics based algorithms for
  nonconvex optimization.
\newblock In {\em Advances in Neural Information Processing Systems 31}, pages
  3122--3133.

\bibitem[Yang and Hu, 2020]{yang2020feature}
Yang, G. and Hu, E.~J. (2020).
\newblock Feature learning in infinite-width neural networks.
\newblock {\em arXiv preprint arXiv:2011.14522}.

\bibitem[Yehudai and Shamir, 2019]{yehudai2019power}
Yehudai, G. and Shamir, O. (2019).
\newblock On the power and limitations of random features for understanding
  neural networks.
\newblock In {\em Advances in Neural Information Processing Systems 32}, pages
  6598--6608.

\bibitem[Ying, 2020]{ying2020mirror}
Ying, L. (2020).
\newblock Mirror descent algorithms for minimizing interacting free energy.
\newblock {\em Journal of Scientific Computing}, 84(3):1--14.

\bibitem[Zou et~al., 2020]{zou2020gradient}
Zou, D., Cao, Y., Zhou, D., and Gu, Q. (2020).
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock {\em Machine Learning}, 109(3):467--492.

\end{thebibliography}
