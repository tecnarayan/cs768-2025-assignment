\begin{thebibliography}{48}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Baird(1995)]{Baird:1995}
L.~Baird.
\newblock Residual algorithms: Reinforcement learning with function
  approximation.
\newblock In \emph{Proceedings of the Twelfth International Conference on
  Machine Learning}, pages 30--37, 1995.

\bibitem[Bellemare et~al.(2013)Bellemare, Naddaf, Veness, and
  Bowling]{Bellemare:2013}
M.~G. Bellemare, Y.~Naddaf, J.~Veness, and M.~Bowling.
\newblock The arcade learning environment: An evaluation platform for general
  agents.
\newblock \emph{J. Artif. Intell. Res. {(JAIR)}}, 47:\penalty0 253--279, 2013.

\bibitem[Bellemare et~al.(2017)Bellemare, Dabney, and Munos]{Bellemare:2017}
M.~G. Bellemare, W.~Dabney, and R.~Munos.
\newblock A distributional perspective on reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 449--458, 2017.

\bibitem[Boyan(1999)]{Boyan:1999}
J.~A. Boyan.
\newblock Least-squares temporal difference learning.
\newblock In \emph{Proc. 16th International Conf. on Machine Learning}, pages
  49--56. Morgan Kaufmann, 1999.

\bibitem[Bradtke and Barto(1996)]{Bradtke:96}
S.~J. Bradtke and A.~G. Barto.
\newblock Linear least-squares algorithms for temporal difference learning.
\newblock \emph{Machine Learning}, 22:\penalty0 33--57, 1996.

\bibitem[Efroni et~al.(2018)Efroni, Dalal, Scherrer, and Mannor]{Efroni:2018}
Y.~Efroni, G.~Dalal, B.~Scherrer, and S.~Mannor.
\newblock Multiple-step greedy policies in approximate and online reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5238--5247, 2018.

\bibitem[Fortunato et~al.(2017)Fortunato, Azar, Piot, Menick, Osband, Graves,
  Mnih, Munos, Hassabis, Pietquin, Blundell, and Legg]{Fortunato:2017}
M.~Fortunato, M.~G. Azar, B.~Piot, J.~Menick, I.~Osband, A.~Graves, V.~Mnih,
  R.~Munos, D.~Hassabis, O.~Pietquin, C.~Blundell, and S.~Legg.
\newblock Noisy networks for exploration.
\newblock \emph{CoRR}, abs/1706.10295, 2017.

\bibitem[Harutyunyan et~al.(2016)Harutyunyan, Bellemare, Stepleton, and
  Munos]{Harutyunyan:2016}
A.~Harutyunyan, M.~G. Bellemare, T.~Stepleton, and R.~Munos.
\newblock Q({\(\lambda\)}) with off-policy corrections.
\newblock In \emph{Proceedings of the 27th International Conference on
  Algorithmic Learning Theory (ALT-2016)}, volume 9925 of \emph{Lecture Notes
  in Artificial Intelligence}, pages 305--320. Springer, 2016.

\bibitem[Hessel et~al.(2018{\natexlab{a}})Hessel, Modayil, {van Hasselt},
  Schaul, Ostrovski, Dabney, Horgan, Piot, Azar, and Silver]{Hessel:2018}
M.~Hessel, J.~Modayil, H.~{van Hasselt}, T.~Schaul, G.~Ostrovski, W.~Dabney,
  D.~Horgan, B.~Piot, M.~Azar, and D.~Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock \emph{AAAI}, 2018{\natexlab{a}}.

\bibitem[Hessel et~al.(2018{\natexlab{b}})Hessel, {van Hasselt}, Modayil, and
  Silver]{Hessel:2018IBDRL}
M.~Hessel, H.~{van Hasselt}, J.~Modayil, and D.~Silver.
\newblock On inductive biases in deep reinforcement learning.
\newblock \emph{OpenReview, https://openreview.net/forum?id=rJgvf3RcFQ},
  2018{\natexlab{b}}.

\bibitem[Kaelbling and Lozano-P{\'e}rez(2010)]{Kaelbling:2010}
L.~P. Kaelbling and T.~Lozano-P{\'e}rez.
\newblock Hierarchical planning in the now.
\newblock In \emph{Workshops at the Twenty-Fourth AAAI Conference on Artificial
  Intelligence}, 2010.

\bibitem[Kaiser et~al.(2019)Kaiser, Babaeizadeh, Milos, Osinski, Campbell,
  Czechowski, Erhan, Finn, Kozakowski, Levine, Sepassi, Tucker, and
  Michalewski]{Kaiser:2019}
L.~Kaiser, M.~Babaeizadeh, P.~Milos, B.~Osinski, R.~H. Campbell, K.~Czechowski,
  D.~Erhan, C.~Finn, P.~Kozakowski, S.~Levine, R.~Sepassi, G.~Tucker, and
  H.~Michalewski.
\newblock Model-based reinforcement learning for atari.
\newblock \emph{arXiv preprint arXiv:1503.00185}, 2019.

\bibitem[Lin(1992)]{Lin:1992}
L.~Lin.
\newblock Self-improving reactive agents based on reinforcement learning,
  planning and teaching.
\newblock \emph{Machine learning}, 8\penalty0 (3):\penalty0 293--321, 1992.

\bibitem[Lowrey et~al.(2019)Lowrey, Rajeswaran, Kakade, Todorov, and
  Mordatch]{lowrey2018plan}
K.~Lowrey, A.~Rajeswaran, S.~Kakade, E.~Todorov, and I.~Mordatch.
\newblock Plan online, learn offline: Efficient learning and exploration via
  model-based control.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Mayne(2014)]{Mayne:2014}
D.~Q. Mayne.
\newblock Model predictive control: Recent developments and future promise.
\newblock \emph{Automatica}, 50\penalty0 (12):\penalty0 2967--2986, 2014.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{Mnih:2013}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~Graves, I.~Antonoglou, D.~Wierstra, and
  M.~Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, Petersen, Beattie, Sadik,
  Antonoglou, King, Kumaran, Wierstra, Legg, and Hassabis]{Mnih:2015}
V.~Mnih, K.~Kavukcuoglu, D.~Silver, A.~A. Rusu, J.~Veness, M.~G. Bellemare,
  A.~Graves, M.~Riedmiller, A.~K. Fidjeland, G.~Ostrovski, S.~Petersen,
  C.~Beattie, A.~Sadik, I.~Antonoglou, H.~King, D.~Kumaran, D.~Wierstra,
  S.~Legg, and D.~Hassabis.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{Nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Morari and Lee(1999)]{Morari:1999}
M.~Morari and J.~H. Lee.
\newblock Model predictive control: past, present and future.
\newblock \emph{Computers \& Chemical Engineering}, 23\penalty0 (4-5):\penalty0
  667--682, 1999.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{Munos:2016}
R.~Munos, T.~Stepleton, A.~Harutyunyan, and M.~Bellemare.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1054--1062, 2016.

\bibitem[Pan et~al.(2018)Pan, Zaheer, White, Patterson, and White]{Pan:2018}
Y.~Pan, M.~Zaheer, A.~White, A.~Patterson, and M.~White.
\newblock Organizing experience: a deeper look at replay mechanisms for
  sample-based planning in continuous state domains.
\newblock In \emph{Proceedings of the Twenty-Seventh International Joint
  Conference on Artificial Intelligence, {IJCAI-18}}, pages 4794--4800.
  International Joint Conferences on Artificial Intelligence Organization, 7
  2018.

\bibitem[Parr et~al.(2008)Parr, Li, Taylor, Painter-Wakefield, and
  Littman]{Parr:2008}
R.~Parr, L.~Li, G.~Taylor, C.~Painter-Wakefield, and M.~L. Littman.
\newblock An analysis of linear models, linear value-function approximation,
  and feature selection for reinforcement learning.
\newblock In \emph{Proceedings of the 25th international conference on Machine
  learning}, pages 752--759, 2008.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{Precup:2000}
D.~Precup, R.~S. Sutton, and S.~P. Singh.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{Proceedings of the Seventeenth International Conference on
  Machine Learning (ICML 2000)}, pages 766--773, Stanford University, Stanford,
  {CA}, {USA}, 2000. Morgan Kaufmann.

\bibitem[Richalet et~al.(1978)Richalet, Rault, Testud, and
  Papon]{Richalet:1978}
J.~Richalet, A.~Rault, J.~Testud, and J.~Papon.
\newblock Model predictive heuristic control.
\newblock \emph{Automatica (Journal of IFAC)}, 14\penalty0 (5):\penalty0
  413--428, 1978.

\bibitem[Riedmiller(2005)]{Riedmiller:2005}
M.~Riedmiller.
\newblock Neural fitted {Q} iteration - first experiences with a data efficient
  neural reinforcement learning method.
\newblock In J.~Gama, R.~Camacho, P.~Brazdil, A.~Jorge, and L.~Torgo, editors,
  \emph{Proceedings of the 16th European Conference on Machine Learning
  (ECML'05)}, pages 317--328. Springer, 2005.

\bibitem[Schaul et~al.(2016)Schaul, Quan, Antonoglou, and Silver]{Schaul:2016}
T.~Schaul, J.~Quan, I.~Antonoglou, and D.~Silver.
\newblock Prioritized experience replay.
\newblock In \emph{International Conference on Learning Representations},
  Puerto Rico, 2016.

\bibitem[Silver et~al.(2017)Silver, van Hasselt, Hessel, Schaul, Guez, Harley,
  Dulac-Arnold, Reichert, Rabinowitz, Barreto, and Degris]{Silver:2017}
D.~Silver, H.~van Hasselt, M.~Hessel, T.~Schaul, A.~Guez, T.~Harley,
  G.~Dulac-Arnold, D.~Reichert, N.~Rabinowitz, A.~Barreto, and T.~Degris.
\newblock The predictron: End-to-end learning and planning.
\newblock In D.~Precup and Y.~W. Teh, editors, \emph{Proceedings of the 34th
  International Conference on Machine Learning}, volume~70 of \emph{Proceedings
  of Machine Learning Research}, pages 3191--3199, International Convention
  Centre, Sydney, Australia, 06--11 Aug 2017. PMLR.

\bibitem[Sutton(1988)]{Sutton:1988}
R.~S. Sutton.
\newblock Learning to predict by the methods of temporal differences.
\newblock \emph{Machine learning}, 3\penalty0 (1):\penalty0 9--44, 1988.

\bibitem[Sutton(1990)]{Sutton:1990}
R.~S. Sutton.
\newblock Integrated architectures for learning, planning, and reacting based
  on approximating dynamic programming.
\newblock In \emph{Proceedings of the seventh international conference on
  machine learning}, pages 216--224, 1990.

\bibitem[Sutton(1995)]{Sutton:1995}
R.~S. Sutton.
\newblock On the virtues of linear learning and trajectory distributions.
\newblock In \emph{Proceedings of the Workshop on Value Function Approximation,
  Machine Learning Conference}, page~85, 1995.

\bibitem[Sutton and Barto(2018)]{SuttonBarto:2018}
R.~S. Sutton and A.~G. Barto.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock The MIT press, Cambridge MA, 2018.

\bibitem[Sutton et~al.(1998)Sutton, Precup, and Singh]{Sutton:1998option}
R.~S. Sutton, D.~Precup, and S.~P. Singh.
\newblock {Intra-Option Learning about Temporally Abstract Actions}.
\newblock In \emph{Proceedings of the Fifteenth International Conference on
  Machine Learning (ICML 1998}, pages 556--564. Morgan Kaufmann Publishers
  Inc., 1998.

\bibitem[Sutton et~al.(2008)Sutton, Szepesv{\'a}ri, and Maei]{Sutton:2008}
R.~S. Sutton, C.~Szepesv{\'a}ri, and H.~R. Maei.
\newblock A convergent {O}(n) algorithm for off-policy temporal-difference
  learning with linear function approximation.
\newblock \emph{Advances in Neural Information Processing Systems 21
  (NIPS-08)}, 21:\penalty0 1609--1616, 2008.

\bibitem[Sutton et~al.(2009)Sutton, Maei, Precup, Bhatnagar, Silver,
  Szepesv{\'a}ri, and Wiewiora]{Sutton:2009}
R.~S. Sutton, H.~R. Maei, D.~Precup, S.~Bhatnagar, D.~Silver,
  C.~Szepesv{\'a}ri, and E.~Wiewiora.
\newblock Fast gradient-descent methods for temporal-difference learning with
  linear function approximation.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning (ICML 2009)}, pages 993--1000. ACM, 2009.

\bibitem[Sutton et~al.(2016)Sutton, Mahmood, and White]{Sutton:2016}
R.~S. Sutton, A.~R. Mahmood, and M.~White.
\newblock An emphatic approach to the problem of off-policy temporal-difference
  learning.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (73):\penalty0 1--29, 2016.

\bibitem[Tsitsiklis and {Van Roy}(1997)]{Tsitsiklis:1997}
J.~N. Tsitsiklis and B.~{Van Roy}.
\newblock An analysis of temporal-difference learning with function
  approximation.
\newblock \emph{{IEEE} Transactions on Automatic Control}, 42\penalty0
  (5):\penalty0 674--690, 1997.

\bibitem[{van Hasselt}(2010)]{vanHasselt:2010}
H.~{van Hasselt}.
\newblock Double {Q}-learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  23:\penalty0 2613--2621, 2010.

\bibitem[{van Hasselt} and Sutton(2015)]{vanHasselt:2015}
H.~{van Hasselt} and R.~S. Sutton.
\newblock Learning to predict independent of span.
\newblock \emph{CoRR}, abs/1508.04582, 2015.

\bibitem[{van Hasselt} et~al.(2014){van Hasselt}, Mahmood, and
  Sutton]{vanHasselt:2014}
H.~{van Hasselt}, A.~R. Mahmood, and R.~S. Sutton.
\newblock Off-policy {TD}($\lambda$) with a true online equivalence.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2014.

\bibitem[van Hasselt et~al.(2016)van Hasselt, Guez, and
  Silver]{vanHasselt:2016}
H.~van Hasselt, A.~Guez, and D.~Silver.
\newblock Deep reinforcement learning with {Double Q-learning}.
\newblock \emph{AAAI}, 2016.

\bibitem[van Hasselt et~al.(2018)van Hasselt, Doron, Strub, Hessel, Sonnerat,
  and Modayil]{vanHasselt:2018}
H.~van Hasselt, Y.~Doron, F.~Strub, M.~Hessel, N.~Sonnerat, and J.~Modayil.
\newblock Deep reinforcement learning and the deadly triad.
\newblock \emph{CoRR}, abs/1812.02648, 2018.

\bibitem[{van Seijen} and Sutton(2015)]{vanSeijen:2015}
H.~{van Seijen} and R.~Sutton.
\newblock A deeper look at planning as learning from replay.
\newblock In \emph{International conference on machine learning}, pages
  2314--2322, 2015.

\bibitem[Wagener et~al.(2019)Wagener, Cheng, Sacks, and Boots]{Wagener:2019}
N.~Wagener, C.-A. Cheng, J.~Sacks, and B.~Boots.
\newblock An online learning approach to model predictive control.
\newblock \emph{arXiv preprint axXiv:1902.08967}, 2019.

\bibitem[Wan et~al.(2019)Wan, Zaheer, White, White, and Sutton]{Wan:2019}
Y.~Wan, M.~Zaheer, A.~White, M.~White, and R.~S. Sutton.
\newblock Planning with expectation models.
\newblock \emph{CoRR}, abs/1904.01191, 2019.

\bibitem[Wang et~al.(2016)Wang, de~Freitas, Schaul, Hessel, van Hasselt, and
  Lanctot]{Wang:2016}
Z.~Wang, N.~de~Freitas, T.~Schaul, M.~Hessel, H.~van Hasselt, and M.~Lanctot.
\newblock Dueling network architectures for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, New York, NY,
  USA, 2016.

\bibitem[Watkins(1989)]{Watkins:1989}
C.~J. C.~H. Watkins.
\newblock \emph{Learning from delayed rewards}.
\newblock PhD thesis, University of Cambridge England, 1989.

\bibitem[Watkins and Dayan(1992)]{WatkinsDayan:1992}
C.~J. C.~H. Watkins and P.~Dayan.
\newblock Q-learning.
\newblock \emph{Machine Learning}, 8:\penalty0 279--292, 1992.

\bibitem[Weber et~al.(2017)Weber, Racani{\`{e}}re, Reichert, Buesing, Guez,
  Rezende, Badia, Vinyals, Heess, Li, Pascanu, Battaglia, Silver, and
  Wierstra]{Weber:2017}
T.~Weber, S.~Racani{\`{e}}re, D.~P. Reichert, L.~Buesing, A.~Guez, D.~J.
  Rezende, A.~P. Badia, O.~Vinyals, N.~Heess, Y.~Li, R.~Pascanu, P.~Battaglia,
  D.~Silver, and D.~Wierstra.
\newblock Imagination-augmented agents for deep reinforcement learning.
\newblock \emph{CoRR}, abs/1707.06203, 2017.

\bibitem[Williams and Baird~III(1993)]{Williams:1993}
R.~J. Williams and L.~C. Baird~III.
\newblock Analysis of some incremental variants of policy iteration: First
  steps toward understanding actor-critic learning systems.
\newblock Technical report, Tech. rep. NU-CCS-93-11, Northeastern University,
  College of Computer Science, Boston, MA., 1993.

\end{thebibliography}
