@inproceedings{jin2020provably,
  title={Provably efficient reinforcement learning with linear function approximation},
  author={Jin, Chi and Yang, Zhuoran and Wang, Zhaoran and Jordan, Michael I},
  booktitle={Conference on Learning Theory},
  pages={2137--2143},
  year={2020},
  organization={PMLR}
}

@article{abbasi2014online,
  title={Online learning in MDPs with side information},
  author={Abbasi-Yadkori, Yasin and Neu, Gergely},
  journal={arXiv preprint arXiv:1406.6812},
  year={2014}
}

@inproceedings{modi2018markov,
  title={Markov decision processes with continuous side information},
  author={Modi, Aditya and Jiang, Nan and Singh, Satinder and Tewari, Ambuj},
  booktitle={Algorithmic Learning Theory},
  pages={597--618},
  year={2018},
  organization={PMLR}
}

@inproceedings{jin2021pessimism,
  title={Is Pessimism Provably Efficient for Offline RL?},
  author={Jin, Ying and Yang, Zhuoran and Wang, Zhaoran},
  booktitle={International Conference on Machine Learning},
  pages={5084--5096},
  year={2021},
  organization={PMLR}
}

@article{avner2012decoupling,
  title={Decoupling exploration and exploitation in multi-armed bandits},
  author={Avner, Orly and Mannor, Shie and Shamir, Ohad},
  journal={arXiv preprint arXiv:1205.2874},
  year={2012}
}

@book{lattimore2020bandit,
  title={Bandit algorithms},
  author={Lattimore, Tor and Szepesv{\'a}ri, Csaba},
  year={2020},
  publisher={Cambridge University Press}
}

@misc{xiao2021optimality,
    title={On the Optimality of Batch Policy Optimization Algorithms},
    author={Chenjun Xiao and Yifan Wu and Tor Lattimore and Bo Dai and Jincheng Mei and Lihong Li and Csaba Szepesvari and Dale Schuurmans},
    year={2021},
    eprint={2104.02293},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{yin2021towards,
  title={Towards instance-optimal offline reinforcement learning with pessimism},
  author={Yin, Ming and Wang, Yu-Xiang},
  booktitle={Thirty-Fifth Conference on Neural Information Processing Systems},
  year={2021}
}

@article{simchowitz2019non,
  title={Non-asymptotic gap-dependent regret bounds for tabular MDPs},
  author={Simchowitz, Max and Jamieson, Kevin G},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={1153--1162},
  year={2019}
}

@misc{dann2017unifying,
    title={Unifying PAC and Regret: Uniform PAC Bounds for Episodic Reinforcement Learning},
    author={Christoph Dann and Tor Lattimore and Emma Brunskill},
    year={2017},
    eprint={1703.07710},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{rouyer2020tsallis,
  title={Tsallis-INF for Decoupled Exploration and Exploitation in Multi-armed Bandits},
  author={Rouyer, Chlo{\'e} and Seldin, Yevgeny},
  booktitle={Conference on Learning Theory},
  pages={3227--3249},
  year={2020},
  organization={PMLR}
}

@inproceedings{domingues2021episodic,
  title={Episodic reinforcement learning in finite mdps: Minimax lower bounds revisited},
  author={Domingues, Omar Darwiche and M{\'e}nard, Pierre and Kaufmann, Emilie and Valko, Michal},
  booktitle={Algorithmic Learning Theory},
  pages={578--598},
  year={2021},
  organization={PMLR}
}

@article{xu2021fine,
  title={Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap},
  author={Xu, Haike and Ma, Tengyu and Du, Simon S},
  journal={arXiv preprint arXiv:2102.04692},
  year={2021}
}

@article{dann2021beyond,
  title={Beyond value-function gaps: Improved instance-dependent regret bounds for episodic reinforcement learning},
  author={Dann, Christoph and Marinov, Teodor Vanislavov and Mohri, Mehryar and Zimmert, Julian},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}


@article{levine2020offline,
  title={Offline reinforcement learning: Tutorial, review, and perspectives on open problems},
  author={Levine, Sergey and Kumar, Aviral and Tucker, George and Fu, Justin},
  journal={arXiv preprint arXiv:2005.01643},
  year={2020}
}

@article{xie2021bellman,
  title={Bellman-consistent Pessimism for Offline Reinforcement Learning},
  author={Xie, Tengyang and Cheng, Ching-An and Jiang, Nan and Mineiro, Paul and Agarwal, Alekh},
  journal={arXiv preprint arXiv:2106.06926},
  year={2021}
}

@article{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{al2021adaptive,
  title={Adaptive sampling for best policy identification in markov decision processes},
  author={Al Marjani, Aymen and Proutiere, Alexandre},
  booktitle={International Conference on Machine Learning},
  pages={7459--7468},
  year={2021},
  organization={PMLR}
}

@article{auer2002finite,
  title={Finite-time analysis of the multiarmed bandit problem},
  author={Auer, Peter and Cesa-Bianchi, Nicolo and Fischer, Paul},
  journal={Machine learning},
  volume={47},
  number={2},
  pages={235--256},
  year={2002},
  publisher={Springer}
}

@article{jin2018q,
  title={Is Q-learning provably efficient?},
  author={Jin, Chi and Allen-Zhu, Zeyuan and Bubeck, Sebastien and Jordan, Michael I},
  journal={arXiv preprint arXiv:1807.03765},
  year={2018}
}

@inproceedings{azar2017minimax,
  title={Minimax regret bounds for reinforcement learning},
  author={Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  booktitle={International Conference on Machine Learning},
  pages={263--272},
  year={2017},
  organization={PMLR}
}

@article{jaksch2010near,
  title={Near-optimal Regret Bounds for Reinforcement Learning.},
  author={Jaksch, Thomas and Ortner, Ronald and Auer, Peter},
  journal={Journal of Machine Learning Research},
  volume={11},
  number={4},
  year={2010}
}

@inproceedings{yang2021q,
  title={Q-learning with logarithmic regret},
  author={Yang, Kunhe and Yang, Lin and Du, Simon},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1576--1584},
  year={2021},
  organization={PMLR}
}

@inproceedings{zanette2019tighter,
  title={Tighter problem-dependent regret bounds in reinforcement learning without domain knowledge using value function bounds},
  author={Zanette, Andrea and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={7304--7312},
  year={2019},
  organization={PMLR}
}

@article{slivkins2019introduction,
  title={Introduction to multi-armed bandits},
  author={Slivkins, Aleksandrs},
  journal={arXiv preprint arXiv:1904.07272},
  year={2019}
}

@article{kumar2020conservative,
  title={Conservative q-learning for offline reinforcement learning},
  author={Kumar, Aviral and Zhou, Aurick and Tucker, George and Levine, Sergey},
  journal={arXiv preprint arXiv:2006.04779},
  year={2020}
}

@inproceedings{uehara2022pessimistic,
title={Pessimistic Model-based Offline Reinforcement Learning under Partial Coverage},
author={Masatoshi Uehara and Wen Sun},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=tyrJsbKAe6}
}

@article{liu2020provably,
  title={Provably good batch reinforcement learning without great exploration},
  author={Liu, Yao and Swaminathan, Adith and Agarwal, Alekh and Brunskill, Emma},
  journal={arXiv preprint arXiv:2007.08202},
  year={2020}
}

@article{fujimoto2021minimalist,
  title={A Minimalist Approach to Offline Reinforcement Learning},
  author={Fujimoto, Scott and Gu, Shixiang Shane},
  journal={arXiv preprint arXiv:2106.06860},
  year={2021}
}

@article{schafer2021decoupling,
  title={Decoupling exploration and exploitation in reinforcement learning},
  author={Sch{\"a}fer, Lukas and Christianos, Filippos and Hanna, Josiah and Albrecht, Stefano V},
  journal={arXiv preprint arXiv:2107.08966},
  year={2021}
}

@article{yu2021reinforcement,
  title={Reinforcement learning in healthcare: A survey},
  author={Yu, Chao and Liu, Jiming and Nemati, Shamim and Yin, Guosheng},
  journal={ACM Computing Surveys (CSUR)},
  volume={55},
  number={1},
  pages={1--36},
  year={2021},
  publisher={ACM New York, NY}
}

@article{afsar2021reinforcement,
  title={Reinforcement learning based recommender systems: A survey},
  author={Afsar, M Mehdi and Crump, Trafford and Far, Behrouz},
  journal={arXiv preprint arXiv:2101.06286},
  year={2021}
}

@article{buckman2020importance,
  title={The importance of pessimism in fixed-dataset policy optimization},
  author={Buckman, Jacob and Gelada, Carles and Bellemare, Marc G},
  journal={arXiv preprint arXiv:2009.06799},
  year={2020}
}

@article{yu2020mopo,
  title={Mopo: Model-based offline policy optimization},
  author={Yu, Tianhe and Thomas, Garrett and Yu, Lantao and Ermon, Stefano and Zou, James Y and Levine, Sergey and Finn, Chelsea and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14129--14142},
  year={2020}
}

@article{papini2021reinforcement,
  title={Reinforcement Learning in Linear MDPs: Constant Regret and Representation Selection},
  author={Papini, Matteo and Tirinzoni, Andrea and Pacchiano, Aldo and Restelli, Marcello and Lazaric, Alessandro and Pirotta, Matteo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{lipsky2001idea,
  title={From idea to market: the drug approval process.},
  author={Lipsky, Martin S and Sharp, Lisa K},
  journal={The Journal of the American Board of Family Practice},
  volume={14},
  number={5},
  pages={362--367},
  year={2001},
  publisher={Am Board Family Med}
}

@inproceedings{he2021logarithmic,
  title={Logarithmic regret for reinforcement learning with linear function approximation},
  author={He, Jiafan and Zhou, Dongruo and Gu, Quanquan},
  booktitle={International Conference on Machine Learning},
  pages={4171--4180},
  year={2021},
  organization={PMLR}
}