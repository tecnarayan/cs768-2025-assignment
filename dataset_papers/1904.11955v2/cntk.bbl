\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018{\natexlab{a}})Allen-Zhu, Li, and
  Liang]{allen2018learning}
Zeyuan Allen-Zhu, Yuanzhi Li, and Yingyu Liang.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{arXiv preprint arXiv:1811.04918}, 2018{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2018{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock \emph{arXiv preprint arXiv:1811.03962}, 2018{\natexlab{b}}.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock \emph{arXiv preprint arXiv:1901.08584}, 2019.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
St{\'e}phane Boucheron, G{\'a}bor Lugosi, and Pascal Massart.
\newblock \emph{Concentration inequalities: A nonasymptotic theory of
  independence}.
\newblock Oxford university press, 2013.

\bibitem[Cao and Gu(2019)]{cao2019generalization}
Yuan Cao and Quanquan Gu.
\newblock A generalization theory of gradient descent for learning
  over-parameterized deep relu networks.
\newblock \emph{arXiv preprint arXiv:1902.01384}, 2019.

\bibitem[Chizat and Bach(2018)]{chizat2018note}
Lenaic Chizat and Francis Bach.
\newblock A note on lazy training in supervised differentiable programming.
\newblock \emph{arXiv preprint arXiv:1812.07956}, 2018.

\bibitem[Cho and Saul(2009)]{cho2009kernel}
Youngmin Cho and Lawrence~K Saul.
\newblock Kernel methods for deep learning.
\newblock In \emph{Advances in neural information processing systems}, pages
  342--350, 2009.

\bibitem[Daniely(2017)]{daniely2017sgd}
Amit Daniely.
\newblock {SGD} learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2422--2430, 2017.

\bibitem[Daniely et~al.(2016)Daniely, Frostig, and Singer]{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer.
\newblock Toward deeper understanding of neural networks: The power of
  initialization and a dual view on expressivity.
\newblock In \emph{Advances In Neural Information Processing Systems}, pages
  2253--2261, 2016.

\bibitem[Du and Hu(2019)]{du2019width}
Simon~S Du and Wei Hu.
\newblock Width provably matters in optimization for deep linear neural
  networks.
\newblock \emph{arXiv preprint arXiv:1901.08572}, 2019.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Hu, and Lee]{du2018algorithmic}
Simon~S Du, Wei Hu, and Jason~D Lee.
\newblock Algorithmic regularization in learning deep homogeneous models:
  Layers are automatically balanced.
\newblock In \emph{Advances in Neural Information Processing Systems 31}, pages
  382--393. 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Lee, Li, Wang, and
  Zhai]{du2018global}
Simon~S Du, Jason~D Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018{\natexlab{b}}.

\bibitem[Du et~al.(2019)Du, Zhai, Poczos, and Singh]{du2018provably}
Simon~S. Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Fan et~al.(2019)Fan, Ma, and Zhong]{fan2019selective}
Jianqing Fan, Cong Ma, and Yiqiao Zhong.
\newblock A selective overview of deep learning.
\newblock \emph{arXiv preprint arXiv:1904.05526}, 2019.

\bibitem[Garriga-Alonso et~al.(2019)Garriga-Alonso, Rasmussen, and
  Aitchison]{garriga-alonso2018deep}
Adri√† Garriga-Alonso, Carl~Edward Rasmussen, and Laurence Aitchison.
\newblock Deep convolutional networks as shallow gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=Bklfsi0cKm}.

\bibitem[Hazan and Jaakkola(2015)]{hazan2015steps}
Tamir Hazan and Tommi Jaakkola.
\newblock Steps toward deep kernel methods from infinite neural networks.
\newblock \emph{arXiv preprint arXiv:1508.05133}, 2015.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock \emph{arXiv preprint arXiv:1806.07572}, 2018.

\bibitem[Lee et~al.(2018)Lee, Sohl-dickstein, Pennington, Novak, Schoenholz,
  and Bahri]{lee2018deep}
Jaehoon Lee, Jascha Sohl-dickstein, Jeffrey Pennington, Roman Novak, Sam
  Schoenholz, and Yasaman Bahri.
\newblock Deep neural networks as gaussian processes.
\newblock In \emph{International Conference on Learning Representations}, 2018.
\newblock URL \url{https://openreview.net/forum?id=B1EA-M-0Z}.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Sohl-Dickstein, and
  Pennington]{lee2019wide}
Jaehoon Lee, Lechao Xiao, Samuel~S Schoenholz, Yasaman Bahri, Jascha
  Sohl-Dickstein, and Jeffrey Pennington.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock \emph{arXiv preprint arXiv:1902.06720}, 2019.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{arXiv preprint arXiv:1808.01204}, 2018.

\bibitem[Mairal et~al.(2014)Mairal, Koniusz, Harchaoui, and
  Schmid]{mairal2014convolutional}
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid.
\newblock Convolutional kernel networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  2627--2635, 2014.

\bibitem[Matthews et~al.(2018)Matthews, Rowland, Hron, Turner, and
  Ghahramani]{matthews2018gaussian}
Alexander G de~G Matthews, Mark Rowland, Jiri Hron, Richard~E Turner, and
  Zoubin Ghahramani.
\newblock Gaussian process behaviour in wide deep neural networks.
\newblock \emph{arXiv preprint arXiv:1804.11271}, 2018.

\bibitem[Neal(1996)]{neal1996priors}
Radford~M Neal.
\newblock Priors for infinite networks.
\newblock In \emph{Bayesian Learning for Neural Networks}, pages 29--53.
  Springer, 1996.

\bibitem[Novak et~al.(2019)Novak, Xiao, Bahri, Lee, Yang, Abolafia, Pennington,
  and Sohl-dickstein]{novak2019bayesian}
Roman Novak, Lechao Xiao, Yasaman Bahri, Jaehoon Lee, Greg Yang, Daniel~A.
  Abolafia, Jeffrey Pennington, and Jascha Sohl-dickstein.
\newblock Bayesian deep convolutional networks with many channels are gaussian
  processes.
\newblock In \emph{International Conference on Learning Representations}, 2019.
\newblock URL \url{https://openreview.net/forum?id=B1g30j0qF7}.

\bibitem[Roux and Bengio(2007)]{leroux07a}
Nicolas~Le Roux and Yoshua Bengio.
\newblock Continuous neural networks.
\newblock In \emph{Proceedings of the Eleventh International Conference on
  Artificial Intelligence and Statistics}, volume~2 of \emph{Proceedings of
  Machine Learning Research}, pages 404--411, San Juan, Puerto Rico, 2007.

\bibitem[Van~der Wilk et~al.(2017)Van~der Wilk, Rasmussen, and
  Hensman]{van2017convolutional}
Mark Van~der Wilk, Carl~Edward Rasmussen, and James Hensman.
\newblock Convolutional gaussian processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2849--2858, 2017.

\bibitem[Williams(1997)]{williams1997computing}
Christopher~KI Williams.
\newblock Computing with infinite networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  295--301, 1997.

\bibitem[Yang(2019)]{yang2019scaling}
Greg Yang.
\newblock Scaling limits of wide neural networks with weight sharing: Gaussian
  process behavior, gradient independence, and neural tangent kernel
  derivation.
\newblock \emph{arXiv preprint arXiv:1902.04760}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{Proceedings of the International Conference on Learning
  Representations (ICLR), 2017}, 2017.

\bibitem[Zou et~al.(2018)Zou, Cao, Zhou, and Gu]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Stochastic gradient descent optimizes over-parameterized deep {ReLU}
  networks.
\newblock \emph{arXiv preprint arXiv:1811.08888}, 2018.

\end{thebibliography}
