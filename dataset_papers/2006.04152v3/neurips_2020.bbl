\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming{-}Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT:} pre-training of deep bidirectional transformers for language
  understanding.
\newblock In \emph{{NAACL-HLT}}, 2019.

\bibitem[Yang et~al.(2019)Yang, Dai, Yang, Carbonell, Salakhutdinov, and
  Le]{yang2019xlnet}
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime~G. Carbonell, Ruslan Salakhutdinov,
  and Quoc~V. Le.
\newblock Xlnet: Generalized autoregressive pretraining for language
  understanding.
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Liu et~al.(2019)Liu, Ott, Goyal, Du, Joshi, Chen, Levy, Lewis,
  Zettlemoyer, and Stoyanov]{liu2019roberta}
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
  Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov.
\newblock Roberta: {A} robustly optimized {BERT} pretraining approach.
\newblock \emph{arXiv preprint arXiv:1907.11692}, 2019.

\bibitem[Lan et~al.(2020)Lan, Chen, Goodman, Gimpel, Sharma, and
  Soricut]{lan2019albert}
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and
  Radu Soricut.
\newblock {ALBERT:} {A} lite {BERT} for self-supervised learning of language
  representations.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Kaya et~al.(2019)Kaya, Hong, and Dumitras]{kaya2018shallow}
Yigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.
\newblock Shallow-deep networks: Understanding and mitigating network
  overthinking.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Jin et~al.(2020)Jin, Jin, Zhou, and Szolovits]{jin2019bert}
Di~Jin, Zhijing Jin, Joey~Tianyi Zhou, and Peter Szolovits.
\newblock Is {BERT} really robust? natural language attack on text
  classification and entailment.
\newblock In \emph{{AAAI}}, 2020.

\bibitem[Morgan and Bourlard(1989)]{morgan1990generalization}
Nelson Morgan and Herv{\'{e}} Bourlard.
\newblock Generalization and parameter estimation in feedforward netws: Some
  experiments.
\newblock In \emph{{NeurIPS}}, 1989.

\bibitem[Prechelt(1998)]{prechelt1998early}
Lutz Prechelt.
\newblock Early stopping-but when?
\newblock In \emph{Neural Networks: Tricks of the trade}, pages 55--69.
  Springer, 1998.

\bibitem[Cai et~al.(2020)Cai, Gan, Wang, Zhang, and Han]{Cai2020Once-for-All}
Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han.
\newblock Once-for-all: Train one network and specialize it for efficient
  deployment.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Howard et~al.(2017)Howard, Zhu, Chen, Kalenichenko, Wang, Weyand,
  Andreetto, and Adam]{howard2017mobilenets}
Andrew~G Howard, Menglong Zhu, Bo~Chen, Dmitry Kalenichenko, Weijun Wang,
  Tobias Weyand, Marco Andreetto, and Hartwig Adam.
\newblock Mobilenets: Efficient convolutional neural networks for mobile vision
  applications.
\newblock \emph{arXiv preprint arXiv:1704.04861}, 2017.

\bibitem[Zhang et~al.(2018)Zhang, Zhou, Lin, and Sun]{zhang2018shufflenet}
Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun.
\newblock Shufflenet: An extremely efficient convolutional neural network for
  mobile devices.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Tan and Le(2019)]{tan2019efficientnet}
Mingxing Tan and Quoc~V. Le.
\newblock Efficientnet: Rethinking model scaling for convolutional neural
  networks.
\newblock In \emph{{ICML}}, Proceedings of Machine Learning Research, 2019.

\bibitem[Han et~al.(2015)Han, Mao, and Dally]{han2015deep}
Song Han, Huizi Mao, and William~J Dally.
\newblock Deep compression: Compressing deep neural networks with pruning,
  trained quantization and huffman coding.
\newblock \emph{arXiv preprint arXiv:1510.00149}, 2015.

\bibitem[Wu et~al.(2016)Wu, Leng, Wang, Hu, and Cheng]{wu2016quantized}
Jiaxiang Wu, Cong Leng, Yuhang Wang, Qinghao Hu, and Jian Cheng.
\newblock Quantized convolutional neural networks for mobile devices.
\newblock In \emph{{CVPR}}, 2016.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{arXiv preprint arXiv:1503.02531}, 2015.

\bibitem[Sanh et~al.(2019)Sanh, Debut, Chaumond, and Wolf]{sanh2019distilbert}
Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf.
\newblock Distilbert, a distilled version of bert: smaller, faster, cheaper and
  lighter.
\newblock \emph{arXiv preprint arXiv:1910.01108}, 2019.

\bibitem[Sun et~al.(2019)Sun, Cheng, Gan, and Liu]{sun2019patient}
Siqi Sun, Yu~Cheng, Zhe Gan, and Jingjing Liu.
\newblock Patient knowledge distillation for {BERT} model compression.
\newblock In \emph{{EMNLP-IJCNLP}}, 2019.

\bibitem[Jiao et~al.(2019)Jiao, Yin, Shang, Jiang, Chen, Li, Wang, and
  Liu]{jiao2019tinybert}
Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang
  Wang, and Qun Liu.
\newblock Tinybert: Distilling bert for natural language understanding.
\newblock \emph{arXiv preprint arXiv:1909.10351}, 2019.

\bibitem[Michel et~al.(2019)Michel, Levy, and Neubig]{michel2019sixteen}
Paul Michel, Omer Levy, and Graham Neubig.
\newblock Are sixteen heads really better than one?
\newblock In \emph{NeurIPS}, 2019.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{voita2019analyzing}
Elena Voita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In \emph{{ACL}}, 2019.

\bibitem[Fan et~al.(2020)Fan, Grave, and Joulin]{fan2019reducing}
Angela Fan, Edouard Grave, and Armand Joulin.
\newblock Reducing transformer depth on demand with structured dropout.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Xu et~al.(2020)Xu, Zhou, Ge, Wei, and Zhou]{xu2020bert}
Canwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei, and Ming Zhou.
\newblock Bert-of-theseus: Compressing bert by progressive module replacing.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Graves(2016)]{graves2016adaptive}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock \emph{arXiv preprint arXiv:1603.08983}, 2016.

\bibitem[Dehghani et~al.(2019)Dehghani, Gouws, Vinyals, Uszkoreit, and
  Kaiser]{ut}
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz
  Kaiser.
\newblock Universal transformers.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Teerapittayanon et~al.(2016)Teerapittayanon, McDanel, and
  Kung]{teerapittayanon2016branchynet}
Surat Teerapittayanon, Bradley McDanel, and H.~T. Kung.
\newblock Branchynet: Fast inference via early exiting from deep neural
  networks.
\newblock In \emph{{ICPR}}, 2016.

\bibitem[Hu et~al.(2020)Hu, Chen, Wang, and Wang]{hu2020triple}
Ting{-}Kuei Hu, Tianlong Chen, Haotao Wang, and Zhangyang Wang.
\newblock Triple wins: Boosting accuracy, robustness and efficiency together by
  enabling input-adaptive inference.
\newblock In \emph{{ICLR}}, 2020.

\bibitem[Wang et~al.(2018)Wang, Yu, Dou, Darrell, and
  Gonzalez]{wang2018skipnet}
Xin Wang, Fisher Yu, Zi{-}Yi Dou, Trevor Darrell, and Joseph~E. Gonzalez.
\newblock Skipnet: Learning dynamic routing in convolutional networks.
\newblock In \emph{{ECCV}}, 2018.

\bibitem[Liu et~al.(2020)Liu, Zhou, Zhao, Wang, Deng, and Ju]{liu2020fastbert}
Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Haotang Deng, and Qi~Ju.
\newblock Fastbert: a self-distilling bert with adaptive inference time.
\newblock \emph{arXiv preprint arXiv:2004.02178}, 2020.

\bibitem[Xin et~al.(2020)Xin, Tang, Lee, Yu, and Lin]{xin2020deebert}
Ji~Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and Jimmy Lin.
\newblock Deebert: Dynamic early exiting for accelerating bert inference.
\newblock \emph{arXiv preprint arXiv:2004.12993}, 2020.

\bibitem[Schwartz et~al.(2020)Schwartz, Stanovsky, Swayamdipta, Dodge, and
  Smith]{Schwartz:2020}
Roy Schwartz, Gabi Stanovsky, Swabha Swayamdipta, Jesse Dodge, and Noah~A.
  Smith.
\newblock The right tool for the job: Matching model and instance complexities.
\newblock In \emph{{ACL}}, 2020.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan,
  Ian~J. Goodfellow, and Rob Fergus.
\newblock Intriguing properties of neural networks.
\newblock In \emph{{ICLR}}, 2014.

\bibitem[Jiang et~al.(2018)Jiang, Kim, Guan, and Gupta]{jiang2018trust}
Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta.
\newblock To trust or not to trust a classifier.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Krogh and Vedelsby(1994)]{krogh1994ensemble}
Anders Krogh and Jesper Vedelsby.
\newblock Neural network ensembles, cross validation, and active learning.
\newblock In \emph{{NeurIPS}}, 1994.

\bibitem[Wang et~al.(2019)Wang, Singh, Michael, Hill, Levy, and Bowman]{glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and
  Samuel~R. Bowman.
\newblock {GLUE:} {A} multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In \emph{{ICLR}}, 2019.

\bibitem[Dolan and Brockett(2005)]{mrpc}
William~B. Dolan and Chris Brockett.
\newblock Automatically constructing a corpus of sentential paraphrases.
\newblock In \emph{IWP@IJCNLP}, 2005.

\bibitem[Conneau and Kiela(2018)]{senteval}
Alexis Conneau and Douwe Kiela.
\newblock Senteval: An evaluation toolkit for universal sentence
  representations.
\newblock In \emph{{LREC}}, 2018.

\bibitem[Socher et~al.(2013)Socher, Perelygin, Wu, Chuang, Manning, Ng, and
  Potts]{sst}
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher~D. Manning,
  Andrew~Y. Ng, and Christopher Potts.
\newblock Recursive deep models for semantic compositionality over a sentiment
  treebank.
\newblock In \emph{{EMNLP}}, 2013.

\bibitem[Williams et~al.(2018)Williams, Nangia, and Bowman]{mnli}
Adina Williams, Nikita Nangia, and Samuel~R. Bowman.
\newblock A broad-coverage challenge corpus for sentence understanding through
  inference.
\newblock In \emph{{NAACL-HLT}}, 2018.

\bibitem[Rajpurkar et~al.(2016)Rajpurkar, Zhang, Lopyrev, and Liang]{qnli}
Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang.
\newblock Squad: 100, 000+ questions for machine comprehension of text.
\newblock In \emph{{EMNLP}}, 2016.

\bibitem[Warstadt et~al.(2019)Warstadt, Singh, and Bowman]{cola}
Alex Warstadt, Amanpreet Singh, and Samuel~R. Bowman.
\newblock Neural network acceptability judgments.
\newblock \emph{{TACL}}, 2019.

\bibitem[Levesque(2011)]{wnli}
Hector~J. Levesque.
\newblock The winograd schema challenge.
\newblock In \emph{{AAAI} Spring Symposium: Logical Formalizations of
  Commonsense Reasoning}, 2011.

\bibitem[Wolf et~al.(2020)Wolf, Debut, Sanh, Chaumond, Delangue, Moi, Cistac,
  Rault, Louf, Funtowicz, Davison, Shleifer, von Platen, Ma, Jernite, Plu, Xu,
  Scao, Gugger, Drame, Lhoest, and Rush]{wolf2020huggingfaces}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Huggingface's transformers: State-of-the-art natural language
  processing.
\newblock \emph{arXiv preprint arXiv:1910.03771}, 2020.

\bibitem[Shoeybi et~al.(2019)Shoeybi, Patwary, Puri, LeGresley, Casper, and
  Catanzaro]{shoeybi2019megatron}
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,
  and Bryan Catanzaro.
\newblock Megatron-lm: Training multi-billion parameter language models using
  gpu model parallelism.
\newblock \emph{arXiv preprint arXiv:1909.08053}, 2019.

\bibitem[Raffel et~al.(2019)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael
  Matena, Yanqi Zhou, Wei Li, and Peter~J Liu.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock \emph{arXiv preprint arXiv:1910.10683}, 2019.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom~B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{arXiv preprint arXiv:2005.14165}, 2020.

\bibitem[Kurakin et~al.(2017)Kurakin, Goodfellow, and
  Bengio]{kurakin2017adversarial}
Alexey Kurakin, Ian~J. Goodfellow, and Samy Bengio.
\newblock Adversarial examples in the physical world.
\newblock In \emph{{ICLR} (Workshop)}, 2017.

\bibitem[Bowman et~al.(2015)Bowman, Angeli, Potts, and Manning]{snli}
Samuel~R. Bowman, Gabor Angeli, Christopher Potts, and Christopher~D. Manning.
\newblock A large annotated corpus for learning natural language inference.
\newblock In \emph{{EMNLP}}, 2015.

\bibitem[Zhang et~al.(2015)Zhang, Zhao, and LeCun]{yelp}
Xiang Zhang, Junbo~Jake Zhao, and Yann LeCun.
\newblock Character-level convolutional networks for text classification.
\newblock In \emph{{NeurIPS}}, 2015.

\bibitem[Strauss et~al.(2017)Strauss, Hanselmann, Junginger, and
  Ulmer]{strauss2017ensemble}
Thilo Strauss, Markus Hanselmann, Andrej Junginger, and Holger Ulmer.
\newblock Ensemble methods as a defense to adversarial perturbations against
  deep neural networks.
\newblock \emph{arXiv preprint arXiv:1709.03423}, 2017.

\bibitem[Tram{\`{e}}r et~al.(2018)Tram{\`{e}}r, Kurakin, Papernot, Goodfellow,
  Boneh, and McDaniel]{tramer2018ensemble}
Florian Tram{\`{e}}r, Alexey Kurakin, Nicolas Papernot, Ian~J. Goodfellow, Dan
  Boneh, and Patrick~D. McDaniel.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{{ICLR}}, 2018.

\bibitem[Pang et~al.(2019)Pang, Xu, Du, Chen, and Zhu]{pang2019improving}
Tianyu Pang, Kun Xu, Chao Du, Ning Chen, and Jun Zhu.
\newblock Improving adversarial robustness via promoting ensemble diversity.
\newblock In \emph{{ICML}}, 2019.

\bibitem[Zoph et~al.(2018)Zoph, Vasudevan, Shlens, and Le]{zoph2018learning}
Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc~V. Le.
\newblock Learning transferable architectures for scalable image recognition.
\newblock In \emph{{CVPR}}, 2018.

\bibitem[Schwartz et~al.(2019)Schwartz, Dodge, Smith, and
  Etzioni]{schwartz2019green}
Roy Schwartz, Jesse Dodge, Noah~A. Smith, and Oren Etzioni.
\newblock Green {AI}.
\newblock \emph{arXiv preprint arXiv:1907.10597}, 2019.

\bibitem[Krizhevsky et~al.(2009)]{krizhevsky2009learning}
Alex Krizhevsky et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\end{thebibliography}
