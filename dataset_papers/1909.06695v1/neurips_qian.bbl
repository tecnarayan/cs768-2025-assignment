\begin{thebibliography}{37}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bahdanau et~al.(2014)Bahdanau, Cho, and Bengio]{bahdanau2014neural}
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.
\newblock Neural machine translation by jointly learning to align and
  translate.
\newblock \emph{arXiv preprint arXiv:1409.0473}, 2014.

\bibitem[Sutskever et~al.(2014)Sutskever, Vinyals, and
  Le]{sutskever2014sequence}
Ilya Sutskever, Oriol Vinyals, and Quoc~V Le.
\newblock Sequence to sequence learning with neural networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  3104--3112, 2014.

\bibitem[Luong et~al.(2015)Luong, Pham, and Manning]{luong2015effective}
Minh-Thang Luong, Hieu Pham, and Christopher~D Manning.
\newblock Effective approaches to attention-based neural machine translation.
\newblock \emph{arXiv preprint arXiv:1508.04025}, 2015.

\bibitem[Cheng et~al.(2019)Cheng, Yang, Liu, Sun, and Xu]{cheng2019joint}
Yong Cheng, Qian Yang, Yang Liu, Maosong Sun, and Wei Xu.
\newblock Joint training for pivot-based neural machine translation.
\newblock In \emph{Joint Training for Neural Machine Translation}, pages
  41--54. Springer, 2019.

\bibitem[Cheng et~al.(2016)Cheng, Liu, Yang, Sun, and Xu]{cheng2016neural}
Yong Cheng, Yang Liu, Qian Yang, Maosong Sun, and Wei Xu.
\newblock Neural machine translation with pivot languages.
\newblock \emph{arXiv preprint arXiv:1611.04928}, 2016.

\bibitem[Nallapati et~al.(2016)Nallapati, Zhou, Gulcehre, Xiang,
  et~al.]{nallapati2016abstractive}
Ramesh Nallapati, Bowen Zhou, Caglar Gulcehre, Bing Xiang, et~al.
\newblock Abstractive text summarization using sequence-to-sequence rnns and
  beyond.
\newblock \emph{arXiv preprint arXiv:1602.06023}, 2016.

\bibitem[Chopra et~al.(2016)Chopra, Auli, and Rush]{chopra2016abstractive}
Sumit Chopra, Michael Auli, and Alexander~M Rush.
\newblock Abstractive sentence summarization with attentive recurrent neural
  networks.
\newblock In \emph{Proceedings of the 2016 Conference of the North American
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies}, pages 93--98, 2016.

\bibitem[Yang et~al.(2016)Yang, Passonneau, and De~Melo]{yang2016peak}
Qian Yang, Rebecca~J Passonneau, and Gerard De~Melo.
\newblock Peak: Pyramid evaluation via automated knowledge extraction.
\newblock In \emph{Thirtieth AAAI Conference on Artificial Intelligence}, 2016.

\bibitem[Yang et~al.(2017)Yang, de~Melo, Cheng, and Wang]{yang2017hitext}
Qian Yang, Gerard de~Melo, Yong Cheng, and Sen Wang.
\newblock Hitext: text reading with dynamic salience marking.
\newblock In \emph{Proceedings of the 26th International Conference on World
  Wide Web Companion}, pages 311--319. International World Wide Web Conferences
  Steering Committee, 2017.

\bibitem[Passonneau et~al.(2018)Passonneau, Poddar, Gite, Krivokapic, Yang, and
  Perin]{passonneau2018wise}
Rebecca~J Passonneau, Ananya Poddar, Gaurav Gite, Alisa Krivokapic, Qian Yang,
  and Dolores Perin.
\newblock Wise crowd content assessment and educational rubrics.
\newblock \emph{International Journal of Artificial Intelligence in Education},
  28\penalty0 (1):\penalty0 29--55, 2018.

\bibitem[Li et~al.(2017)Li, Jiang, Shang, and Li]{li2017paraphrase}
Zichao Li, Xin Jiang, Lifeng Shang, and Hang Li.
\newblock Paraphrase generation with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1711.00279}, 2017.

\bibitem[Gupta et~al.(2018)Gupta, Agarwal, Singh, and Rai]{gupta2018deep}
Ankush Gupta, Arvind Agarwal, Prawaan Singh, and Piyush Rai.
\newblock A deep generative framework for paraphrase generation.
\newblock In \emph{Thirty-Second AAAI Conference on Artificial Intelligence},
  2018.

\bibitem[Yang et~al.(2019)Yang, Huo, Shen, Cheng, Wang, Wang, and Carin]{48451}
Qian Yang, Zhouyuan Huo, Dinghan Shen, Yong Cheng, Wenlin Wang, Guoyin Wang,
  and Lawrence Carin.
\newblock An end-to-end generative architecture for paraphrase generation.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5998--6008, 2017.

\bibitem[Al-Rfou et~al.(2018)Al-Rfou, Choe, Constant, Guo, and
  Jones]{al2018character}
Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones.
\newblock Character-level language modeling with deeper self-attention.
\newblock \emph{arXiv preprint arXiv:1808.04444}, 2018.

\bibitem[Dai et~al.(2019)Dai, Yang, Yang, Cohen, Carbonell, Le, and
  Salakhutdinov]{dai2019transformer}
Zihang Dai, Zhilin Yang, Yiming Yang, William~W Cohen, Jaime Carbonell, Quoc~V
  Le, and Ruslan Salakhutdinov.
\newblock Transformer-xl: Attentive language models beyond a fixed-length
  context.
\newblock \emph{arXiv preprint arXiv:1901.02860}, 2019.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2018.

\bibitem[Radford et~al.(2019)Radford, Wu, Child, Luan, Amodei, and
  Sutskever]{radford2019language}
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya
  Sutskever.
\newblock Language models are unsupervised multitask learners.
\newblock \emph{OpenAI Blog}, 1:\penalty0 8, 2019.

\bibitem[Yadan et~al.(2013)Yadan, Adams, Taigman, and Ranzato]{yadan2013multi}
Omry Yadan, Keith Adams, Yaniv Taigman, and Marc'Aurelio Ranzato.
\newblock Multi-gpu training of convnets.
\newblock \emph{arXiv preprint arXiv:1312.5853}, 2013.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Czarnecki, Osindero, Vinyals, Graves,
  Silver, and Kavukcuoglu]{jaderberg2017decoupled}
Max Jaderberg, Wojciech~Marian Czarnecki, Simon Osindero, Oriol Vinyals, Alex
  Graves, David Silver, and Koray Kavukcuoglu.
\newblock Decoupled neural interfaces using synthetic gradients.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1627--1635. JMLR. org, 2017.

\bibitem[Huo et~al.(2018{\natexlab{a}})Huo, Gu, Yang, and
  Huang]{huo2018decoupled}
Zhouyuan Huo, Bin Gu, Qian Yang, and Heng Huang.
\newblock Decoupled parallel backpropagation with convergence guarantee.
\newblock \emph{arXiv preprint arXiv:1804.10574}, 2018{\natexlab{a}}.

\bibitem[Huo et~al.(2018{\natexlab{b}})Huo, Gu, and Huang]{huo2018training}
Zhouyuan Huo, Bin Gu, and Heng Huang.
\newblock Training neural networks using features replay.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6659--6668, 2018{\natexlab{b}}.

\bibitem[Press and Wolf(2016)]{press2016using}
Ofir Press and Lior Wolf.
\newblock Using the output embedding to improve language models.
\newblock \emph{arXiv preprint arXiv:1608.05859}, 2016.

\bibitem[Inan et~al.(2016)Inan, Khosravi, and Socher]{inan2016tying}
Hakan Inan, Khashayar Khosravi, and Richard Socher.
\newblock Tying word vectors and word classifiers: A loss framework for
  language modeling.
\newblock \emph{arXiv preprint arXiv:1611.01462}, 2016.

\bibitem[Robbins and Monro(1951)]{robbins1951stochastic}
Herbert Robbins and Sutton Monro.
\newblock A stochastic approximation method.
\newblock \emph{The annals of mathematical statistics}, pages 400--407, 1951.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
John Duchi, Elad Hazan, and Yoram Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Hinton et~al.(2012)Hinton, Srivastava, and Swersky]{hinton2012lecture}
Geoffrey Hinton, Nitish Srivastava, and Kevin Swersky.
\newblock Lecture 6a overview of mini--batch gradient descent.
\newblock \emph{Coursera Lecture slides https://class. coursera.
  org/neuralnets-2012-001/lecture,[Online}, 2012.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017fixing}
Ilya Loshchilov and Frank Hutter.
\newblock Fixing weight decay regularization in adam.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017.

\bibitem[Rumelhart et~al.(1988)Rumelhart, Hinton, Williams,
  et~al.]{rumelhart1988learning}
David~E Rumelhart, Geoffrey~E Hinton, Ronald~J Williams, et~al.
\newblock Learning representations by back-propagating errors.
\newblock \emph{Cognitive modeling}, 5\penalty0 (3):\penalty0 1, 1988.

\bibitem[Chen et~al.(2016)Chen, Xu, Zhang, and Guestrin]{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock \emph{arXiv preprint arXiv:1604.06174}, 2016.

\bibitem[Bottou et~al.(2018)Bottou, Curtis, and
  Nocedal]{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{Siam Review}, 60\penalty0 (2):\penalty0 223--311, 2018.

\bibitem[Mahoney(2011)]{mahoney2011large}
Matt Mahoney.
\newblock Large text compression benchmark.
\newblock \emph{URL: http://www. mattmahoney. net/text/text. html}, 2011.

\bibitem[Merity et~al.(2016)Merity, Xiong, Bradbury, and
  Socher]{merity2016pointer}
Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher.
\newblock Pointer sentinel mixture models.
\newblock \emph{arXiv preprint arXiv:1609.07843}, 2016.

\bibitem[Loshchilov and Hutter(2016)]{loshchilov2016sgdr}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock \emph{arXiv preprint arXiv:1608.03983}, 2016.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
Priya Goyal, Piotr Doll{\'a}r, Ross Girshick, Pieter Noordhuis, Lukasz
  Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
  Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\end{thebibliography}
