\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agueh \& Carlier(2011)Agueh and Carlier]{agueh2011barycenters}
Agueh, M. and Carlier, G.
\newblock Barycenters in the {W}asserstein space.
\newblock \emph{SIAM Journal on Mathematical Analysis}, 43\penalty0
  (2):\penalty0 904--924, 2011.

\bibitem[Azizzadenesheli et~al.(2019)Azizzadenesheli, Liu, Yang, and
  Anandkumar]{ref:azizzadenesheli2018regularized}
Azizzadenesheli, K., Liu, A., Yang, F., and Anandkumar, A.
\newblock Regularized learning for domain adaptation under label shifts.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Baktashmotlagh et~al.(2013)Baktashmotlagh, Harandi, Lovell, and
  Salzmann]{ref:baktashmotlagh2013unsupervised}
Baktashmotlagh, M., Harandi, M.~T., Lovell, B.~C., and Salzmann, M.
\newblock Unsupervised domain adaptation by domain invariant projection.
\newblock In \emph{IEEE International Conference on Computer Vision}, pp.\
  769--776, 2013.

\bibitem[{Battistelli} et~al.(2013){Battistelli}, {Chisci}, {Fantacci},
  {Farina}, and {Graziano}]{ref:battistelli2013consensus}
{Battistelli}, G., {Chisci}, L., {Fantacci}, C., {Farina}, A., and {Graziano},
  A.
\newblock Consensus {CPHD} filter for distributed multitarget tracking.
\newblock \emph{IEEE Journal of Selected Topics in Signal Processing},
  7\penalty0 (3):\penalty0 508--520, 2013.

\bibitem[Ben-David et~al.(2007)Ben-David, Blitzer, Crammer, Pereira,
  et~al.]{ref:ben2007analysis}
Ben-David, S., Blitzer, J., Crammer, K., Pereira, F., et~al.
\newblock Analysis of representations for domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems},
  19:\penalty0 137, 2007.

\bibitem[Bernstein(2009)]{ref:bernstein2009matrix}
Bernstein, D.~S.
\newblock \emph{Matrix Mathematics: Theory, Facts, and Formulas}.
\newblock Princeton University Press, 2009.

\bibitem[Bertsekas(2009)]{ref:bertsekas2009convex}
Bertsekas, D.
\newblock \emph{Convex Optimization Theory}.
\newblock Athena Scientific, 2009.

\bibitem[Blanchet et~al.(2019)Blanchet, Kang, and
  Murthy]{ref:blanchet2019robust}
Blanchet, J., Kang, Y., and Murthy, K.
\newblock Robust {W}asserstein profile inference and applications to machine
  learning.
\newblock \emph{Journal of Applied Probability}, 56\penalty0 (3):\penalty0
  830--857, 2019.

\bibitem[Blitzer et~al.(2006)Blitzer, McDonald, and
  Pereira]{ref:blitzer2006domain}
Blitzer, J., McDonald, R., and Pereira, F.
\newblock Domain adaptation with structural correspondence learning.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing}, pp.\  120–128, 2006.

\bibitem[Cesa-Bianchi \& Lugosi(2006)Cesa-Bianchi and
  Lugosi]{ref:cesa2006prediction}
Cesa-Bianchi, N. and Lugosi, G.
\newblock \emph{Prediction, Learning, and Games}.
\newblock Cambridge University Press, 2006.

\bibitem[Chen et~al.(2016)Chen, Monfort, Liu, and Ziebart]{ref:chen2016robust}
Chen, X., Monfort, M., Liu, A., and Ziebart, B.~D.
\newblock Robust covariate shift regression.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1270--1279,
  2016.

\bibitem[Chu \& Wang(2018)Chu and Wang]{ref:chu-wang-2018-survey}
Chu, C. and Wang, R.
\newblock A survey of domain adaptation for neural machine translation.
\newblock In \emph{International Conference on Computational Linguistics}, pp.\
   1304--1319. Association for Computational Linguistics, 2018.

\bibitem[Cortes \& Mohri(2014)Cortes and Mohri]{ref:CORTES2014domain}
Cortes, C. and Mohri, M.
\newblock Domain adaptation and sample bias correction theory and algorithm for
  regression.
\newblock \emph{Theoretical Computer Science}, 519:\penalty0 103 -- 126, 2014.

\bibitem[Cortes et~al.(2019)Cortes, Mohri, and
  Medina]{ref:cortes2019adaptation}
Cortes, C., Mohri, M., and Medina, A.~M.
\newblock Adaptation based on generalized discrepancy.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (1):\penalty0 1--30, 2019.

\bibitem[Courty et~al.(2017)Courty, Flamary, Tuia, and
  Rakotomamonjy]{ref:courty2016optimal}
Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A.
\newblock Optimal transport for domain adaptation.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 39\penalty0 (9):\penalty0 1853--1865, 2017.

\bibitem[Csurka(2017)]{Csurka2017}
Csurka, G.
\newblock \emph{A Comprehensive Survey on Domain Adaptation for Visual
  Applications}, pp.\  1--35.
\newblock Springer International Publishing, 2017.

\bibitem[de~Mathelin et~al.(2020)de~Mathelin, Richard, Mougeot, and
  Vayatis]{ref:de2020adversarial}
de~Mathelin, A., Richard, G., Mougeot, M., and Vayatis, N.
\newblock Adversarial weighting for domain adaptation in regression.
\newblock \emph{arXiv preprint arXiv:2006.08251}, 2020.

\bibitem[Delage \& Ye(2010)Delage and Ye]{ref:delage2010distributionally}
Delage, E. and Ye, Y.
\newblock Distributionally robust optimization under moment uncertainty with
  application to data-driven problems.
\newblock \emph{Operations Research}, 58\penalty0 (3):\penalty0 595--612, 2010.

\bibitem[Duchi \& Namkoong(2018)Duchi and Namkoong]{ref:duchi2018learning}
Duchi, J. and Namkoong, H.
\newblock Learning models with uniform performance via distributionally robust
  optimization.
\newblock \emph{arXiv preprint arXiv:1810.08750}, 2018.

\bibitem[Ganin \& Lempitsky(2015)Ganin and
  Lempitsky]{ref:ganin2015unsupervised}
Ganin, Y. and Lempitsky, V.
\newblock Unsupervised domain adaptation by backpropagation.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1180--1189, 2015.

\bibitem[Gao(2020)]{ref:gao2020finite}
Gao, R.
\newblock Finite-sample guarantees for {W}asserstein distributionally robust
  optimization: Breaking the curse of dimensionality.
\newblock \emph{arXiv preprint arXiv:2009.04382}, 2020.

\bibitem[Gao et~al.(2018)Gao, Xie, Xie, and Xu]{ref:gao2018robust}
Gao, R., Xie, L., Xie, Y., and Xu, H.
\newblock Robust hypothesis testing using {W}asserstein uncertainty sets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  7913--7923, 2018.

\bibitem[Garcke \& Vanck(2014)Garcke and Vanck]{ref:garcke2014importance}
Garcke, J. and Vanck, T.
\newblock Importance weighted inductive transfer learning for regression.
\newblock In \emph{Joint European conference on machine learning and knowledge
  discovery in databases}, pp.\  466--481, 2014.

\bibitem[Ghaoui \& Lebret(1997)Ghaoui and Lebret]{ref:Ghaoui1997robust}
Ghaoui, L.~E. and Lebret, H.
\newblock Robust solutions to least-squares problems with uncertain data.
\newblock \emph{SIAM Journal on Matrix Analysis and Applications}, 18\penalty0
  (4):\penalty0 1035–1064, 1997.

\bibitem[Ghifary et~al.(2016)Ghifary, Kleijn, Zhang, Balduzzi, and
  Li]{ref:ghifary2016deep}
Ghifary, M., Kleijn, W.~B., Zhang, M., Balduzzi, D., and Li, W.
\newblock Deep reconstruction-classification networks for unsupervised domain
  adaptation.
\newblock In \emph{European Conference on Computer Vision}, pp.\  597--613,
  2016.

\bibitem[Givens \& Shortt(1984)Givens and Shortt]{ref:givens1984class}
Givens, C. and Shortt, R.
\newblock A class of {W}asserstein metrics for probability distributions.
\newblock \emph{The Michigan Mathematical Journal}, 31\penalty0 (2):\penalty0
  231--240, 1984.

\bibitem[Goh \& Sim(2010)Goh and Sim]{ref:goh2010distributionally}
Goh, J. and Sim, M.
\newblock Distributionally robust optimization and its tractable
  approximations.
\newblock \emph{Operations Research}, 58\penalty0 (4):\penalty0 902--917, 2010.

\bibitem[Huang et~al.(2006)Huang, Gretton, Borgwardt, Sch{\"o}lkopf, and
  Smola]{ref:huang2006correcting}
Huang, J., Gretton, A., Borgwardt, K., Sch{\"o}lkopf, B., and Smola, A.
\newblock Correcting sample selection bias by unlabeled data.
\newblock \emph{Advances in Neural Information Processing Systems},
  19:\penalty0 601--608, 2006.

\bibitem[Jiang \& Zhai(2007)Jiang and Zhai]{ref:jiang-zhai-2007-instance}
Jiang, J. and Zhai, C.
\newblock Instance weighting for domain adaptation in {NLP}.
\newblock In \emph{Association of Computational Linguistics}, pp.\  264--271,
  2007.

\bibitem[Koniusz et~al.(2017)Koniusz, Tas, and Porikli]{ref:koniusz2017domain}
Koniusz, P., Tas, Y., and Porikli, F.
\newblock Domain adaptation by mixture of alignments of second-or higher-order
  scatter tensors.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pp.\  4478--4487, 2017.

\bibitem[Kuhn et~al.(2019)Kuhn, Mohajerin~Esfahani, Nguyen, and
  Shafieezadeh-Abadeh]{ref:kuhn2019wasserstein}
Kuhn, D., Mohajerin~Esfahani, P., Nguyen, V.~A., and Shafieezadeh-Abadeh, S.
\newblock Wasserstein distributionally robust optimization: Theory and
  applications in machine learning.
\newblock In \emph{Operations Research \& Management Science in the Age of
  Analytics}, pp.\  130--166. 2019.

\bibitem[Kumar et~al.(2010)Kumar, Saha, and Daume]{ref:kumar2010co}
Kumar, A., Saha, A., and Daume, H.
\newblock Co-regularization based semi-supervised domain adaptation.
\newblock \emph{Advances in Neural Information Processing Systems}, pp.\
  478--486, 2010.

\bibitem[Lam(2019)]{ref:lam2019recovering}
Lam, H.
\newblock Recovering best statistical guarantees via the empirical
  divergence-based distributionally robust optimization.
\newblock \emph{Operations Research}, 67\penalty0 (4):\penalty0 1090--1105,
  2019.

\bibitem[Lattimore \& Szepesvári(2020)Lattimore and
  Szepesvári]{ref:lattimore2020bandit}
Lattimore, T. and Szepesvári, C.
\newblock \emph{Bandit Algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Li(2012)]{ref:li2012literature}
Li, Q.
\newblock Literature survey: {D}omain adaptation algorithms for natural
  language processing.
\newblock \emph{Department of Computer Science The Graduate Center, The City
  University of New York}, pp.\  8--10, 2012.

\bibitem[{Li} et~al.(2016){Li}, {Wang}, {Wang}, {Ye}, and
  {Reddy}]{ref:li2016transfer}
{Li}, Y., {Wang}, L., {Wang}, J., {Ye}, J., and {Reddy}, C.~K.
\newblock Transfer learning for survival analysis via efficient {L}2,1-norm
  regularized {C}ox regression.
\newblock In \emph{IEEE International Conference on Data Mining}, pp.\
  231--240, 2016.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{ref:lipton2018detecting}
Lipton, Z., Wang, Y.-X., and Smola, A.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  3122--3130, 2018.

\bibitem[L\"ofberg(2004)]{ref:lofberg2004yalmip}
L\"ofberg, J.
\newblock {YALMIP}: {A} toolbox for modeling and optimization in {MATLAB}.
\newblock In \emph{IEEE International Conference on Robotics and Automation},
  pp.\  284--289, 2004.

\bibitem[Long et~al.(2016)Long, Zhu, Wang, and
  Jordan]{ref:long2016unsupervised}
Long, M., Zhu, H., Wang, J., and Jordan, M.~I.
\newblock Unsupervised domain adaptation with residual transfer networks.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  136–144, 2016.

\bibitem[Lopez-Paz et~al.(2012)Lopez-Paz, Hern\'{a}ndez-Lobato, and
  Sch\"{o}lkopf]{ref:lopez2013semi}
Lopez-Paz, D., Hern\'{a}ndez-Lobato, J.~M., and Sch\"{o}lkopf, B.
\newblock Semi-supervised domain adaptation with non-parametric copulas.
\newblock In \emph{International Conference on Neural Information Processing
  Systems}, pp.\  665–673, 2012.

\bibitem[Malitsky \& Mishchenko(2019)Malitsky and
  Mishchenko]{malitsky2019adaptive}
Malitsky, Y. and Mishchenko, K.
\newblock Adaptive gradient descent without descent.
\newblock \emph{arXiv preprint arXiv:1910.09529}, 2019.

\bibitem[McCann(1997)]{ref:mccann1997convexity}
McCann, R.~J.
\newblock A convexity principle for interacting gases.
\newblock \emph{Advances in Mathematics}, 128\penalty0 (1):\penalty0 153--179,
  1997.

\bibitem[Mohajerin~Esfahani \& Kuhn(2018)Mohajerin~Esfahani and
  Kuhn]{ref:esfahani2018data}
Mohajerin~Esfahani, P. and Kuhn, D.
\newblock Data-driven distributionally robust optimization using the
  {W}asserstein metric: Performance guarantees and tractable reformulations.
\newblock \emph{Mathematical Programming}, 171\penalty0 (1):\penalty0 115--166,
  2018.

\bibitem[{MOSEK ApS}(2019)]{mosek}
{MOSEK ApS}.
\newblock \emph{The MOSEK optimization toolbox. Version 9.2.}, 2019.

\bibitem[Motiian et~al.(2017{\natexlab{a}})Motiian, Jones, Iranmanesh, and
  Doretto]{ref:motiian2017few}
Motiian, S., Jones, Q., Iranmanesh, S., and Doretto, G.
\newblock Few-shot adversarial domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~30, pp.\  6670--6680, 2017{\natexlab{a}}.

\bibitem[Motiian et~al.(2017{\natexlab{b}})Motiian, Piccirilli, Adjeroh, and
  Doretto]{ref:motiian2017unified}
Motiian, S., Piccirilli, M., Adjeroh, D.~A., and Doretto, G.
\newblock Unified deep supervised domain adaptation and generalization.
\newblock In \emph{IEEE International Conference on Computer Vision}, pp.\
  5715--5725, 2017{\natexlab{b}}.

\bibitem[Namkoong \& Duchi(2016)Namkoong and Duchi]{ref:namkoong2016stochastic}
Namkoong, H. and Duchi, J.~C.
\newblock Stochastic gradient methods for distributionally robust optimization
  with f-divergences.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~29, pp.\  2208--2216, 2016.

\bibitem[Nguyen et~al.(2019{\natexlab{a}})Nguyen, Shafieezadeh-Abadeh, Yue,
  Kuhn, and Wiesemann]{ref:nguyen2019calculating}
Nguyen, V.~A., Shafieezadeh-Abadeh, S., Yue, M.-C., Kuhn, D., and Wiesemann, W.
\newblock Calculating optimistic likelihoods using (geodesically) convex
  optimization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2019{\natexlab{a}}.

\bibitem[Nguyen et~al.(2019{\natexlab{b}})Nguyen, Shafieezadeh-Abadeh, Yue,
  Kuhn, and Wiesemann]{ref:nguyen2019optimistic}
Nguyen, V.~A., Shafieezadeh-Abadeh, S., Yue, M.-C., Kuhn, D., and Wiesemann, W.
\newblock Optimistic distributionally robust optimization for nonparametric
  likelihood approximation.
\newblock In \emph{Advances in Neural Information Processing Systems 32},
  2019{\natexlab{b}}.

\bibitem[Nguyen et~al.(2020)Nguyen, Si, and Blanchet]{ref:nguyen2020robust}
Nguyen, V.~A., Si, N., and Blanchet, J.
\newblock Robust {B}ayesian classification using an optimistic score ratio.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Pardoe \& Stone(2010)Pardoe and Stone]{ref:Pardoe2010BoostingFR}
Pardoe, D. and Stone, P.
\newblock Boosting for regression transfer.
\newblock In \emph{International Conference on Machine Learning}, 2010.

\bibitem[Redko et~al.(2019)Redko, Morvant, Habrard, Sebban, and
  Bennani]{ref:redko2019advances}
Redko, I., Morvant, E., Habrard, A., Sebban, M., and Bennani, Y.
\newblock \emph{Advances in Domain Adaptation Theory}.
\newblock Elsevier, 2019.

\bibitem[Richard et~al.(2020)Richard, de~Mathelin, H{\'e}brail, Mougeot, and
  Vayatis]{ref:richard2020unsupervised}
Richard, G., de~Mathelin, A., H{\'e}brail, G., Mougeot, M., and Vayatis, N.
\newblock Unsupervised multi-source domain adaptation for regression.
\newblock 2020.

\bibitem[Saha et~al.(2011)Saha, Rai, Daum{\'e}, Venkatasubramanian, and
  DuVall]{ref:avishek2011active}
Saha, A., Rai, P., Daum{\'e}, H., Venkatasubramanian, S., and DuVall, S.~L.
\newblock Active supervised domain adaptation.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases}, pp.\
   97--112, 2011.

\bibitem[Salaken et~al.(2019)Salaken, Khosravi, Nguyen, and
  Nahavandi]{ref:SALAKEN2019565}
Salaken, S.~M., Khosravi, A., Nguyen, T., and Nahavandi, S.
\newblock Seeded transfer learning for regression problems with deep learning.
\newblock \emph{Expert Systems with Applications}, 115:\penalty0 565 -- 577,
  2019.

\bibitem[Shafieezadeh-Abadeh et~al.(2018)Shafieezadeh-Abadeh, Nguyen, Kuhn, and
  Mohajerin~Esfahani]{ref:abadeh2018wasserstein}
Shafieezadeh-Abadeh, S., Nguyen, V.~A., Kuhn, D., and Mohajerin~Esfahani, P.
\newblock Wasserstein distributionally robust {K}alman filtering.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, pp.\  8474--8483, 2018.

\bibitem[Shimodaira(2000)]{ref:shimodaira2000improving}
Shimodaira, H.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of Statistical Planning and Inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Sindhwani et~al.(2005)Sindhwani, Niyogi, and
  Belkin]{ref:sindhwani2005co}
Sindhwani, V., Niyogi, P., and Belkin, M.
\newblock A co-regularization approach to semi-supervised learning with
  multiple views.
\newblock In \emph{ICML workshop on learning with multiple views}, pp.\
  74--79, 2005.

\bibitem[Sion(1958)]{ref:sion1958minimax}
Sion, M.
\newblock On general minimax theorems.
\newblock \emph{Pacific Journal of Mathematics}, 8\penalty0 (1):\penalty0
  171--176, 1958.

\bibitem[S{\o}gaard(2013)]{ref:sogaard2013semi}
S{\o}gaard, A.
\newblock Semi-supervised learning and domain adaptation in natural language
  processing.
\newblock \emph{Synthesis Lectures on Human Language Technologies}, 6\penalty0
  (2):\penalty0 1--103, 2013.

\bibitem[Still(2018)]{still2018lectures}
Still, G.
\newblock \emph{Lectures on Parametric Optimization: An Introduction}.
\newblock 2018.

\bibitem[Sugiyama et~al.(2008)Sugiyama, Suzuki, Nakajima, Kashima, von
  B{\"u}nau, and Kawanabe]{ref:sugiyama2008direct}
Sugiyama, M., Suzuki, T., Nakajima, S., Kashima, H., von B{\"u}nau, P., and
  Kawanabe, M.
\newblock Direct importance estimation for covariate shift adaptation.
\newblock \emph{Annals of the Institute of Statistical Mathematics},
  60\penalty0 (4):\penalty0 699--746, 2008.

\bibitem[Sun et~al.(2011)Sun, Chattopadhyay, Panchanathan, and
  Ye]{ref:sun2011two}
Sun, Q., Chattopadhyay, R., Panchanathan, S., and Ye, J.
\newblock A two-stage weighting framework for multi-source domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~24, pp.\  505--513, 2011.

\bibitem[Tzeng et~al.(2015)Tzeng, Hoffman, Darrell, and
  Saenko]{ref:tzeng2015simultaneous}
Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K.
\newblock Simultaneous deep transfer across domains and tasks.
\newblock In \emph{IEEE International Conference on Computer Vision}, pp.\
  4068--4076, 2015.

\bibitem[Villani(2008)]{ref:villani2008optimal}
Villani, C.
\newblock \emph{Optimal Transport: Old and New}.
\newblock Springer Science \& Business Media, 2008.

\bibitem[Wang et~al.(2020)Wang, Liu, Yu, Yue, and
  Anandkumar]{ref:wang2020distributionally}
Wang, H., Liu, A., Yu, Z., Yue, Y., and Anandkumar, A.
\newblock Distributionally robust learning for unsupervised domain adaptation.
\newblock \emph{arXiv preprint arXiv:2010.05784}, 2020.

\bibitem[Wang \& Deng(2018)Wang and Deng]{ref:WANG2018135}
Wang, M. and Deng, W.
\newblock Deep visual domain adaptation: A survey.
\newblock \emph{Neurocomputing}, 312:\penalty0 135 -- 153, 2018.

\bibitem[Weiss et~al.(2016)Weiss, Khoshgoftaar, and Wang]{ref:weiss2016survey}
Weiss, K., Khoshgoftaar, T.~M., and Wang, D.
\newblock A survey of transfer learning.
\newblock \emph{Journal of Big Data}, 3\penalty0 (1):\penalty0 1--40, 2016.

\bibitem[Wilson \& Cook(2020)Wilson and Cook]{ref:wilson2020survey}
Wilson, G. and Cook, D.~J.
\newblock A survey of unsupervised deep domain adaptation.
\newblock \emph{ACM Transactions on Intelligent Systems and Technology},
  11\penalty0 (5):\penalty0 1--46, 2020.

\bibitem[Wintenberger(2017)]{ref:wintenberger2017optimal}
Wintenberger, O.
\newblock Optimal learning with {B}ernstein online aggregation.
\newblock \emph{Machine Learning}, 106\penalty0 (1):\penalty0 119--141, 2017.

\bibitem[Yao et~al.(2015)Yao, Pan, Ngo, Li, and Mei]{ref:yao2015semi}
Yao, T., Pan, Y., Ngo, C.-W., Li, H., and Mei, T.
\newblock Semi-supervised domain adaptation with subspace learning for visual
  recognition.
\newblock In \emph{IEEE conference on Computer Vision and Pattern Recognition},
  pp.\  2142--2150, 2015.

\bibitem[Zhao et~al.(2018)Zhao, Zhang, Wu, Moura, Costeira, and
  Gordon]{ref:zhao2018adversarial}
Zhao, H., Zhang, S., Wu, G., Moura, J. M.~F., Costeira, J.~P., and Gordon,
  G.~J.
\newblock Adversarial multiple source domain adaptation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~31, 2018.

\end{thebibliography}
