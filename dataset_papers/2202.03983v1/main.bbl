\begin{thebibliography}{42}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Papadimitriou and Tsitsiklis(1987)]{papadimitriou1987complexity}
Christos~H Papadimitriou and John~N Tsitsiklis.
\newblock The complexity of markov decision processes.
\newblock \emph{Mathematics of operations research}, 12\penalty0 (3):\penalty0
  441--450, 1987.

\bibitem[Mossel and Roch(2005)]{mossel2005learning}
Elchanan Mossel and S{\'e}bastien Roch.
\newblock Learning nonsingular phylogenies and hidden markov models.
\newblock In \emph{Proceedings of the thirty-seventh annual ACM symposium on
  Theory of computing}, pages 366--375, 2005.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Kakade, Krishnamurthy, and
  Liu]{jin2020sample}
Chi Jin, Sham~M Kakade, Akshay Krishnamurthy, and Qinghua Liu.
\newblock Sample-efficient reinforcement learning of undercomplete pomdps.
\newblock \emph{arXiv:2006.12484}, 2020{\natexlab{a}}.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Hessel et~al.(2018)Hessel, Modayil, Van~Hasselt, Schaul, Ostrovski,
  Dabney, Horgan, Piot, Azar, and Silver]{hessel2018rainbow}
Matteo Hessel, Joseph Modayil, Hado Van~Hasselt, Tom Schaul, Georg Ostrovski,
  Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, and David Silver.
\newblock Rainbow: Combining improvements in deep reinforcement learning.
\newblock In \emph{Thirty-second AAAI conference on artificial intelligence},
  2018.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock Bellman eluder dimension: New rich classes of rl problems, and
  sample-efficient algorithms.
\newblock \emph{arXiv preprint arXiv:2102.00815}, 2021.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Du et~al.(2021)Du, Kakade, Lee, Lovett, Mahajan, Sun, and
  Wang]{du2021bilinear}
Simon~S Du, Sham~M Kakade, Jason~D Lee, Shachar Lovett, Gaurav Mahajan, Wen
  Sun, and Ruosong Wang.
\newblock Bilinear classes: A structural framework for provable generalization
  in rl.
\newblock \emph{arXiv preprint arXiv:2103.10897}, 2021.

\bibitem[Hausknecht and Stone(2015)]{hausknecht2015deep}
Matthew Hausknecht and Peter Stone.
\newblock Deep recurrent q-learning for partially observable mdps.
\newblock In \emph{2015 aaai fall symposium series}, 2015.

\bibitem[Zhu et~al.(2017)Zhu, Li, Poupart, and Miao]{zhu2017improving}
Pengfei Zhu, Xin Li, Pascal Poupart, and Guanghui Miao.
\newblock On improving deep reinforcement learning for pomdps.
\newblock \emph{arXiv preprint arXiv:1704.07978}, 2017.

\bibitem[Igl et~al.(2018)Igl, Zintgraf, Le, Wood, and Whiteson]{igl2018deep}
Maximilian Igl, Luisa Zintgraf, Tuan~Anh Le, Frank Wood, and Shimon Whiteson.
\newblock Deep variational reinforcement learning for pomdps.
\newblock In \emph{International Conference on Machine Learning}, pages
  2117--2126. PMLR, 2018.

\bibitem[Hafner et~al.(2019)Hafner, Lillicrap, Ba, and
  Norouzi]{hafner2019dream}
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock \emph{arXiv preprint arXiv:1912.01603}, 2019.

\bibitem[McCallum(1993)]{mccallum1993overcoming}
R~Andrew McCallum.
\newblock Overcoming incomplete perception with utile distinction memory.
\newblock In \emph{Proceedings of the Tenth International Conference on Machine
  Learning}, pages 190--196, 1993.

\bibitem[Kearns et~al.(1999)Kearns, Mansour, and Ng]{kearns1999approximate}
Michael~J Kearns, Yishay Mansour, and Andrew~Y Ng.
\newblock Approximate planning in large pomdps via reusable trajectories.
\newblock In \emph{NIPS}, pages 1001--1007. Citeseer, 1999.

\bibitem[Kearns et~al.(2002)Kearns, Mansour, and Ng]{kearns2002sparse}
Michael Kearns, Yishay Mansour, and Andrew~Y Ng.
\newblock A sparse sampling algorithm for near-optimal planning in large markov
  decision processes.
\newblock \emph{Machine learning}, 49\penalty0 (2):\penalty0 193--208, 2002.

\bibitem[Even-Dar et~al.(2005)Even-Dar, Kakade, and
  Mansour]{evendar2005reinforcement}
Eyal Even-Dar, Sham~M. Kakade, and Yishay Mansour.
\newblock Reinforcement learning in pomdps without resets.
\newblock In \emph{International Joint Conference on Artificial Intelligence},
  2005.

\bibitem[Azizzadenesheli et~al.(2016)Azizzadenesheli, Lazaric, and
  Anandkumar]{azizzadenesheli2016reinforcement}
Kamyar Azizzadenesheli, Alessandro Lazaric, and Animashree Anandkumar.
\newblock Reinforcement learning of pomdps using spectral methods.
\newblock In \emph{Conference on Learning Theory}, pages 193--256. PMLR, 2016.

\bibitem[Guo et~al.(2016)Guo, Doroudi, and Brunskill]{guo2016pac}
Zhaohan~Daniel Guo, Shayan Doroudi, and Emma Brunskill.
\newblock A pac rl algorithm for episodic pomdps.
\newblock In \emph{Artificial Intelligence and Statistics}, pages 510--518.
  PMLR, 2016.

\bibitem[Anandkumar et~al.(2014)Anandkumar, Ge, Hsu, Kakade, and
  Telgarsky]{anandkumar2014tensor}
Animashree Anandkumar, Rong Ge, Daniel Hsu, Sham~M Kakade, and Matus Telgarsky.
\newblock Tensor decompositions for learning latent variable models.
\newblock \emph{Journal of machine learning research}, 15:\penalty0 2773--2832,
  2014.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on learning theory}, pages 2898--2933. PMLR,
  2019.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Du et~al.(2019)Du, Krishnamurthy, Jiang, Agarwal, Dudik, and
  Langford]{du2019provably}
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and
  John Langford.
\newblock Provably efficient rl with rich observations via latent state
  decoding.
\newblock In \emph{International Conference on Machine Learning}, pages
  1665--1674. PMLR, 2019.

\bibitem[Misra et~al.(2020)Misra, Henaff, Krishnamurthy, and
  Langford]{misra2020kinematic}
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford.
\newblock Kinematic state abstraction and provably efficient rich-observation
  reinforcement learning.
\newblock In \emph{International conference on machine learning}, pages
  6961--6971. PMLR, 2020.

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, and Wen Sun.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Uehara et~al.(2021)Uehara, Zhang, and Sun]{uehara2021representation}
Masatoshi Uehara, Xuezhou Zhang, and Wen Sun.
\newblock Representation learning for online and offline rl in low-rank mdps.
\newblock \emph{arXiv preprint arXiv:2110.04652}, 2021.

\bibitem[Ljung(1998)]{ljung1998system}
Lennart Ljung.
\newblock \emph{System Identification: Theory for the User}.
\newblock Pearson Education, 1998.

\bibitem[Box et~al.(2015)Box, Jenkins, Reinsel, and Ljung]{box2015time}
George~EP Box, Gwilym~M Jenkins, Gregory~C Reinsel, and Greta~M Ljung.
\newblock \emph{Time series analysis: forecasting and control}.
\newblock John Wiley \& Sons, 2015.

\bibitem[Hamilton(1994)]{hamilton1994time}
James~Douglas Hamilton.
\newblock \emph{Time series analysis}.
\newblock Princeton university press, 1994.

\bibitem[Kalman(1960)]{kalman1960new}
Rudolph~Emil Kalman.
\newblock A new approach to linear filtering and prediction problems.
\newblock \emph{Journal of Basic Engineering}, 1960.

\bibitem[Verhaegen(1993)]{verhaegen1993subspace}
Michel Verhaegen.
\newblock Subspace model identification part 3. analysis of the ordinary
  output-error state-space model identification algorithm.
\newblock \emph{International Journal of control}, 58\penalty0 (3):\penalty0
  555--586, 1993.

\bibitem[Arora et~al.(2018)Arora, Hazan, Lee, Singh, Zhang, and
  Zhang]{arora2018towards}
Sanjeev Arora, Elad Hazan, Holden Lee, Karan Singh, Cyril Zhang, and Yi~Zhang.
\newblock Towards provable control for unknown linear dynamical systems.
\newblock In \emph{International Conference on Learning Representations,
  Workshop Track}, 2018.

\bibitem[Agarwal et~al.(2019)Agarwal, Bullins, Hazan, Kakade, and
  Singh]{agarwal2019online}
Naman Agarwal, Brian Bullins, Elad Hazan, Sham Kakade, and Karan Singh.
\newblock Online control with adversarial disturbances.
\newblock In \emph{International Conference on Machine Learning}, pages
  111--119. PMLR, 2019.

\bibitem[Oymak and Ozay(2019)]{oymak2019non}
Samet Oymak and Necmiye Ozay.
\newblock Non-asymptotic identification of lti systems from a single
  trajectory.
\newblock In \emph{2019 American control conference (ACC)}, pages 5655--5661.
  IEEE, 2019.

\bibitem[Simchowitz et~al.(2019)Simchowitz, Boczar, and
  Recht]{simchowitz2019learning}
Max Simchowitz, Ross Boczar, and Benjamin Recht.
\newblock Learning linear dynamical systems with semi-parametric least squares.
\newblock In \emph{Conference on Learning Theory}, pages 2714--2802. PMLR,
  2019.

\bibitem[Krishnamurthy et~al.(2016)Krishnamurthy, Agarwal, and
  Langford]{krish2016hardness}
Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Pac reinforcement learning with rich observations.
\newblock In D.~Lee, M.~Sugiyama, U.~Luxburg, I.~Guyon, and R.~Garnett,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~29.
  Curran Associates, Inc., 2016.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2016/file/2387337ba1e0b0249ba90f55b2ba2521-Paper.pdf}.

\bibitem[Weisz et~al.(2021)Weisz, Amortila, and
  Szepesv{\'a}ri]{weisz2021exponential}
Gell{\'e}rt Weisz, Philip Amortila, and Csaba Szepesv{\'a}ri.
\newblock Exponential lower bounds for planning in mdps with
  linearly-realizable optimal action-value functions.
\newblock In \emph{Algorithmic Learning Theory}, pages 1237--1264. PMLR, 2021.

\bibitem[Antos et~al.(2008)Antos, Szepesv{\'a}ri, and Munos]{antos2008learning}
Andr{\'a}s Antos, Csaba Szepesv{\'a}ri, and R{\'e}mi Munos.
\newblock Learning near-optimal policies with bellman-residual minimization
  based fitted policy iteration and a single sample path.
\newblock \emph{Machine Learning}, 71\penalty0 (1):\penalty0 89--129, 2008.

\bibitem[Chen and Jiang(2019)]{chen2019info}
Jinglin Chen and Nan Jiang.
\newblock Information-theoretic considerations in batch reinforcement learning.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  1042--1051. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/chen19e.html}.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  263--272. PMLR, 2017.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2013/file/41bfd20a38bb1b0bec75acf0845530a7-Paper.pdf}.

\end{thebibliography}
