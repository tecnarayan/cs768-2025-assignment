\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2012)Ahn, Korattikara, and Welling]{ahn2012bayesian}
Ahn, S., Korattikara, A., and Welling, M.
\newblock Bayesian posterior sampling via stochastic gradient {F}isher scoring.
\newblock \emph{arXiv preprint arXiv:1206.6380}, 2012.

\bibitem[Ahn et~al.(2014)Ahn, Shahbaba, and Welling]{ahn2014distributed}
Ahn, S., Shahbaba, B., and Welling, M.
\newblock Distributed stochastic gradient {MCMC}.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1044--1052, 2014.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Arjovsky, M., Chintala, S., and Bottou, L.
\newblock Wasserstein {GAN}.
\newblock \emph{arXiv preprint arXiv:1701.07875}, 2017.

\bibitem[Balan et~al.(2015)Balan, Rathod, Murphy, and
  Welling]{balan2015bayesian}
Balan, A.~K., Rathod, V., Murphy, K.~P., and Welling, M.
\newblock Bayesian dark knowledge.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3438--3446, 2015.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Blundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra, D.
\newblock Weight uncertainty in neural networks.
\newblock \emph{arXiv preprint arXiv:1505.05424}, 2015.

\bibitem[Carlini \& Wagner(2017)Carlini and Wagner]{carlini2017adversarial}
Carlini, N. and Wagner, D.
\newblock Adversarial examples are not easily detected: Bypassing ten detection
  methods.
\newblock \emph{arXiv preprint arXiv:1705.07263}, 2017.

\bibitem[Chen et~al.(2014)Chen, Fox, and Guestrin]{chen2014stochastic}
Chen, T., Fox, E., and Guestrin, C.
\newblock Stochastic gradient {H}amiltonian {M}onte {C}arlo.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1683--1691, 2014.

\bibitem[Dempster et~al.(1977)Dempster, Laird, and Rubin]{dempster1977maximum}
Dempster, A.~P., Laird, N.~M., and Rubin, D.~B.
\newblock Maximum likelihood from incomplete data via the {EM} algorithm.
\newblock \emph{Journal of the Royal Statistical Society. Series B}, pp.\
  1--38, 1977.

\bibitem[Ding et~al.(2014)Ding, Fang, Babbush, Chen, Skeel, and
  Neven]{ding2014bayesian}
Ding, N., Fang, Y., Babbush, R., Chen, C., Skeel, R.~D., and Neven, H.
\newblock Bayesian sampling using stochastic gradient thermostats.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  3203--3211, 2014.

\bibitem[Feinman et~al.(2017)Feinman, Curtin, Shintre, and
  Gardner]{feinman2017detecting}
Feinman, R., Curtin, R.~R., Shintre, S., and Gardner, A.~B.
\newblock Detecting adversarial samples from artifacts.
\newblock \emph{arXiv preprint arXiv:1703.00410}, 2017.

\bibitem[Gal \& Ghahramani(2016)Gal and Ghahramani]{gal2016dropout}
Gal, Y. and Ghahramani, Z.
\newblock Dropout as a {B}ayesian approximation: Representing model uncertainty
  in deep learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1050--1059, 2016.

\bibitem[Gal et~al.(2017)Gal, Islam, and Ghahramani]{gal2017deep}
Gal, Y., Islam, R., and Ghahramani, Z.
\newblock Deep {Bayesian} active learning with image data.
\newblock \emph{arXiv preprint arXiv:1703.02910}, 2017.

\bibitem[Goodfellow et~al.(2014{\natexlab{a}})Goodfellow, Pouget-Abadie, Mirza,
  Xu, Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
  S., Courville, A., and Bengio, Y.
\newblock Generative adversarial nets.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2672--2680, 2014{\natexlab{a}}.

\bibitem[Goodfellow et~al.(2014{\natexlab{b}})Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Goodfellow, I.~J., Shlens, J., and Szegedy, C.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014{\natexlab{b}}.

\bibitem[Graves(2011)]{graves2011practical}
Graves, A.
\newblock Practical variational inference for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2348--2356, 2011.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.
\newblock Improved training of {W}asserstein {GAN}s.
\newblock \emph{arXiv preprint arXiv:1704.00028}, 2017.

\bibitem[Guo et~al.(2017)Guo, Pleiss, Sun, and Weinberger]{guo2017calibration}
Guo, C., Pleiss, G., Sun, Y., and Weinberger, K.~Q.
\newblock On calibration of modern neural networks.
\newblock \emph{arXiv preprint arXiv:1706.04599}, 2017.

\bibitem[Hendrycks \& Gimpel(2016)Hendrycks and Gimpel]{hendrycks2016baseline}
Hendrycks, D. and Gimpel, K.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock \emph{arXiv preprint arXiv:1610.02136}, 2016.

\bibitem[Hern{\'a}ndez-Lobato \& Adams(2015)Hern{\'a}ndez-Lobato and
  Adams]{hernandez2015probabilistic}
Hern{\'a}ndez-Lobato, J.~M. and Adams, R.
\newblock Probabilistic backpropagation for scalable learning of {B}ayesian
  neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1861--1869, 2015.

\bibitem[Hinton \& Van~Camp(1993)Hinton and Van~Camp]{hinton1993keeping}
Hinton, G.~E. and Van~Camp, D.
\newblock Keeping the neural networks simple by minimizing the description
  length of the weights.
\newblock In \emph{Proceedings of the Sixth Annual Conference on Computational
  Learning Theory}, pp.\  5--13. ACM, 1993.

\bibitem[Houlsby et~al.(2011)Houlsby, Husz{\'a}r, Ghahramani, and
  Lengyel]{houlsby2011bayesian}
Houlsby, N., Husz{\'a}r, F., Ghahramani, Z., and Lengyel, M.
\newblock Bayesian active learning for classification and preference learning.
\newblock \emph{arXiv preprint arXiv:1112.5745}, 2011.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2014stochastic}
Kingma, D.~P. and Welling, M.
\newblock Stochastic gradient {VB} and the variational auto-encoder.
\newblock In \emph{Second International Conference on Learning
  Representations}, 2014.

\bibitem[Krizhevsky \& Hinton(2009)Krizhevsky and
  Hinton]{krizhevsky2009learning}
Krizhevsky, A. and Hinton, G.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Technical report, University of Toronto}, 2009.

\bibitem[Krueger et~al.(2017)Krueger, Huang, Islam, Turner, Lacoste, and
  Courville]{krueger2017bayesian}
Krueger, D., Huang, C.-W., Islam, R., Turner, R., Lacoste, A., and Courville,
  A.
\newblock Bayesian hypernetworks.
\newblock \emph{arXiv preprint arXiv:1710.04759}, 2017.

\bibitem[Kurakin et~al.(2016)Kurakin, Goodfellow, and
  Bengio]{kurakin2016adversarial}
Kurakin, A., Goodfellow, I., and Bengio, S.
\newblock Adversarial examples in the physical world.
\newblock \emph{arXiv preprint arXiv:1607.02533}, 2016.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and Haffner]{mnist}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li et~al.(2017)Li, Turner, and Liu]{li2017approximate}
Li, Y., Turner, R.~E., and Liu, Q.
\newblock Approximate inference with amortised {MCMC}.
\newblock \emph{arXiv preprint arXiv:1702.08343}, 2017.

\bibitem[Louizos \& Welling(2017)Louizos and
  Welling]{louizos2017multiplicative}
Louizos, C. and Welling, M.
\newblock Multiplicative normalizing flows for variational {Bayesian} neural
  networks.
\newblock \emph{arXiv preprint arXiv:1703.01961}, 2017.

\bibitem[MacKay(1992)]{mackay1992practical}
MacKay, D.~J.
\newblock A practical {B}ayesian framework for backpropagation networks.
\newblock \emph{Neural Computation}, 4\penalty0 (3):\penalty0 448--472, 1992.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Minka(2001)]{minka2001expectation}
Minka, T.~P.
\newblock Expectation propagation for approximate {B}ayesian inference.
\newblock In \emph{Proceedings of the Seventeenth Conference on Uncertainty in
  Artificial Intelligence}, pp.\  362--369. Morgan Kaufmann Publishers Inc.,
  2001.

\bibitem[Neal(1996)]{neal1996bayesian}
Neal, R.~M.
\newblock \emph{Bayesian Learning for Neural Networks}.
\newblock PhD thesis, University of Toronto, 1996.

\bibitem[Rauber et~al.(2017)Rauber, Brendel, and Bethge]{rauber2017foolbox}
Rauber, J., Brendel, W., and Bethge, M.
\newblock Foolbox v0.8.0: A {Python} toolbox to benchmark the robustness of
  machine learning models.
\newblock \emph{arXiv preprint arXiv:1707.04131}, 2017.

\bibitem[Rawat et~al.(2017)Rawat, Wistuba, and Nicolae]{rawat2017adversarial}
Rawat, A., Wistuba, M., and Nicolae, M.-I.
\newblock Adversarial phenomenon in the eyes of {Bayesian} deep learning.
\newblock \emph{arXiv preprint arXiv:1711.08244}, 2017.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock \emph{arXiv preprint arXiv:1401.4082}, 2014.

\bibitem[Rosca et~al.(2017)Rosca, Lakshminarayanan, Warde-Farley, and
  Mohamed]{gan-e-ssim}
Rosca, M., Lakshminarayanan, B., Warde-Farley, D., and Mohamed, S.
\newblock Variational approaches for auto-encoding generative adversarial
  networks.
\newblock \emph{arXiv preprint arXiv:1706.04987}, 2017.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{impgan}
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen,
  X.
\newblock Improved techniques for training {GANs}.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2234--2242, 2016.

\bibitem[Salimans et~al.(2018)Salimans, Zhang, Radford, and
  Metaxas]{anonymous2018improvingOT}
Salimans, T., Zhang, H., Radford, A., and Metaxas, D.
\newblock Improving {GANs} using optimal transport.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Springenberg et~al.(2014)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenberg2014striving}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M.
\newblock Striving for simplicity: The all convolutional net.
\newblock \emph{arXiv preprint arXiv:1412.6806}, 2014.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G.~E., Krizhevsky, A., Sutskever, I., and
  Salakhutdinov, R.
\newblock Dropout: A simple way to prevent neural networks from overfitting.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Szegedy et~al.(2013)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy2013intriguing}
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I.,
  and Fergus, R.
\newblock Intriguing properties of neural networks.
\newblock \emph{arXiv preprint arXiv:1312.6199}, 2013.

\bibitem[Vollmer et~al.(2016)Vollmer, Zygalakis, and
  Teh]{vollmer2016exploration}
Vollmer, S.~J., Zygalakis, K.~C., and Teh, Y.~W.
\newblock Exploration of the (non-) asymptotic bias and variance of stochastic
  gradient {L}angevin dynamics.
\newblock \emph{Journal of Machine Learning Research}, 17\penalty0
  (159):\penalty0 1--48, 2016.

\bibitem[Wei et~al.(2018)Wei, Liu, Wang, and Gong]{anonymous2018improving}
Wei, X., Liu, Z., Wang, L., and Gong, B.
\newblock Improving the improved training of {Wasserstein GANs}.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Welling \& Teh(2011)Welling and Teh]{welling2011bayesian}
Welling, M. and Teh, Y.~W.
\newblock Bayesian learning via stochastic gradient {L}angevin dynamics.
\newblock In \emph{Proceedings of the 28th International Conference on Machine
  Learning}, pp.\  681--688, 2011.

\end{thebibliography}
