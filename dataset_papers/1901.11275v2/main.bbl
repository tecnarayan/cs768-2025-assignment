\begin{thebibliography}{40}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abdolmaleki et~al.(2018{\natexlab{a}})Abdolmaleki, Springenberg,
  Degrave, Bohez, Tassa, Belov, Heess, and Riedmiller]{abdolmaleki2018relative}
Abdolmaleki, A., Springenberg, J.~T., Degrave, J., Bohez, S., Tassa, Y., Belov,
  D., Heess, N., and Riedmiller, M.
\newblock Relative entropy regularized policy iteration.
\newblock \emph{arXiv preprint arXiv:1812.02256}, 2018{\natexlab{a}}.

\bibitem[Abdolmaleki et~al.(2018{\natexlab{b}})Abdolmaleki, Springenberg,
  Tassa, Munos, Heess, and Riedmiller]{abdolmaleki2018maximum}
Abdolmaleki, A., Springenberg, J.~T., Tassa, Y., Munos, R., Heess, N., and
  Riedmiller, M.
\newblock Maximum a posteriori policy optimisation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018{\natexlab{b}}.

\bibitem[Akrour et~al.(2018)Akrour, Abdolmaleki, Abdulsamad, Peters, and
  Neumann]{akrour2018model}
Akrour, R., Abdolmaleki, A., Abdulsamad, H., Peters, J., and Neumann, G.
\newblock Model-free trajectory-based policy optimization with monotonic
  improvement.
\newblock \emph{The Journal of Machine Learning Research (JMLR)}, 19\penalty0
  (1):\penalty0 565--589, 2018.

\bibitem[Asadi \& Littman(2017)Asadi and Littman]{asadi2016alternative}
Asadi, K. and Littman, M.~L.
\newblock An alternative softmax operator for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Azar et~al.(2012)Azar, G{\'o}mez, and Kappen]{azar2012dynamic}
Azar, M.~G., G{\'o}mez, V., and Kappen, H.~J.
\newblock Dynamic policy programming.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 13\penalty0
  (Nov):\penalty0 3207--3245, 2012.

\bibitem[Beck \& Teboulle(2003)Beck and Teboulle]{beck2003mirror}
Beck, A. and Teboulle, M.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bertsekas \& Tsitsiklis(1996)Bertsekas and
  Tsitsiklis]{Bertsekas:1996:NP:560669}
Bertsekas, D.~P. and Tsitsiklis, J.~N.
\newblock \emph{Neuro-Dynamic Programming}.
\newblock Athena Scientific, 1st edition, 1996.
\newblock ISBN 1886529108.

\bibitem[Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and
  Song]{dai2018sbeed}
Dai, B., Shaw, A., Li, L., Xiao, L., He, N., Liu, Z., Chen, J., and Song, L.
\newblock Sbeed: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Finn, C., Levine, S., and Abbeel, P.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Fox et~al.(2016)Fox, Pakman, and Tishby]{fox2015taming}
Fox, R., Pakman, A., and Tishby, N.
\newblock Taming the noise in reinforcement learning via soft updates.
\newblock In \emph{Conference on Uncertainty in Artificial Intelligence (UAI)},
  2016.

\bibitem[Fu et~al.(2018)Fu, Luo, and Levine]{fu2017learning}
Fu, J., Luo, K., and Levine, S.
\newblock Learning robust rewards with adversarial inverse reinforcement
  learning.
\newblock In \emph{International Conference on Representation Learning}, 2018.

\bibitem[Haarnoja et~al.(2017)Haarnoja, Tang, Abbeel, and
  Levine]{haarnoja2017reinforcement}
Haarnoja, T., Tang, H., Abbeel, P., and Levine, S.
\newblock Reinforcement learning with deep energy-based policies.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2017.

\bibitem[Haarnoja et~al.(2018{\natexlab{a}})Haarnoja, Zhou, Abbeel, and
  Levine]{haarnoja2018soft}
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2018{\natexlab{a}}.

\bibitem[Haarnoja et~al.(2018{\natexlab{b}})Haarnoja, Zhou, Hartikainen,
  Tucker, Ha, Tan, Kumar, Zhu, Gupta, Abbeel, et~al.]{haarnoja2018softb}
Haarnoja, T., Zhou, A., Hartikainen, K., Tucker, G., Ha, S., Tan, J., Kumar,
  V., Zhu, H., Gupta, A., Abbeel, P., et~al.
\newblock Soft actor-critic algorithms and applications.
\newblock \emph{arXiv preprint arXiv:1812.05905}, 2018{\natexlab{b}}.

\bibitem[Hiriart-Urruty \& Lemar{\'e}chal(2012)Hiriart-Urruty and
  Lemar{\'e}chal]{hiriart2012fundamentals}
Hiriart-Urruty, J.-B. and Lemar{\'e}chal, C.
\newblock \emph{Fundamentals of convex analysis}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Lee et~al.(2018)Lee, Choi, and Oh]{lee2017sparse}
Lee, K., Choi, S., and Oh, S.
\newblock Sparse markov decision processes with causal sparse tsallis entropy
  regularization for reinforcement learning.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (3):\penalty0
  1466--1473, 2018.

\bibitem[Levine(2018)]{levine2018reinforcement}
Levine, S.
\newblock {Reinforcement Learning and Control as Probabilistic Inference:
  Tutorial and Review}.
\newblock \emph{arXiv preprint arXiv:1805.00909}, 2018.

\bibitem[Martins \& Astudillo(2016)Martins and Astudillo]{martins2016softmax}
Martins, A. and Astudillo, R.
\newblock From softmax to sparsemax: A sparse model of attention and
  multi-label classification.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Mensch \& Blondel(2018)Mensch and Blondel]{mensch2018differentiable}
Mensch, A. and Blondel, M.
\newblock Differentiable dynamic programming for structured prediction and
  attention.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Mnih et~al.(2016)Mnih, Badia, Mirza, Graves, Lillicrap, Harley,
  Silver, and Kavukcuoglu]{mnih2016asynchronous}
Mnih, V., Badia, A.~P., Mirza, M., Graves, A., Lillicrap, T., Harley, T.,
  Silver, D., and Kavukcuoglu, K.
\newblock Asynchronous methods for deep reinforcement learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Morgenstern \& Von~Neumann(1953)Morgenstern and
  Von~Neumann]{morgenstern1953theory}
Morgenstern, O. and Von~Neumann, J.
\newblock \emph{Theory of games and economic behavior}.
\newblock Princeton university press, 1953.

\bibitem[Munos et~al.(2016)Munos, Stepleton, Harutyunyan, and
  Bellemare]{munos2016safe}
Munos, R., Stepleton, T., Harutyunyan, A., and Bellemare, M.
\newblock Safe and efficient off-policy reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  pp.\  1054--1062, 2016.

\bibitem[Nachum et~al.(2017)Nachum, Norouzi, Xu, and
  Schuurmans]{nachum2017bridging}
Nachum, O., Norouzi, M., Xu, K., and Schuurmans, D.
\newblock Bridging the gap between value and policy based reinforcement
  learning.
\newblock In \emph{Advances in Neural Information Processing Systems (NIPS)},
  2017.

\bibitem[Nachum et~al.(2018)Nachum, Chow, and Ghavamzadeh]{nachum2018path}
Nachum, O., Chow, Y., and Ghavamzadeh, M.
\newblock Path consistency learning in tsallis entropy regularized mdps.
\newblock \emph{arXiv preprint arXiv:1802.03501}, 2018.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Nemirovski, A.
\newblock Prox-method with rate of convergence $o(1/t)$ for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nesterov(2005)]{nesterov2005smooth}
Nesterov, Y.
\newblock Smooth minimization of non-smooth functions.
\newblock \emph{Mathematical programming}, 103\penalty0 (1):\penalty0 127--152,
  2005.

\bibitem[Nesterov(2009)]{nesterov2009primal}
Nesterov, Y.
\newblock Primal-dual subgradient methods for convex problems.
\newblock \emph{Mathematical programming}, 120\penalty0 (1):\penalty0 221--259,
  2009.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Neu, G., Jonsson, A., and G{\'o}mez, V.
\newblock {A unified view of entropy-regularized Markov decision processes}.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[Ng et~al.(1999)Ng, Harada, and Russell]{ng1999policy}
Ng, A.~Y., Harada, D., and Russell, S.
\newblock {Policy invariance under reward transformations: Theory and
  application to reward shaping}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 1999.

\bibitem[Perolat et~al.(2015)Perolat, Scherrer, Piot, and
  Pietquin]{perolat2015approximate}
Perolat, J., Scherrer, B., Piot, B., and Pietquin, O.
\newblock Approximate dynamic programming for two-player zero-sum markov games.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Peters et~al.(2010)Peters, Mulling, and Altun]{peters2010relative}
Peters, J., Mulling, K., and Altun, Y.
\newblock Relative entropy policy search.
\newblock In \emph{AAAI Conference on Artificial Intelligence}, 2010.

\bibitem[Puterman \& Shin(1978)Puterman and Shin]{puterman1978modified}
Puterman, M.~L. and Shin, M.~C.
\newblock Modified policy iteration algorithms for discounted markov decision
  problems.
\newblock \emph{Management Science}, 24\penalty0 (11):\penalty0 1127--1137,
  1978.

\bibitem[Richemond \& Maginnis(2017)Richemond and Maginnis]{richemond2017short}
Richemond, P.~H. and Maginnis, B.
\newblock A short variational proof of equivalence between policy gradients and
  soft q learning.
\newblock \emph{arXiv preprint arXiv:1712.08650}, 2017.

\bibitem[Riedmiller et~al.(2018)Riedmiller, Hafner, Lampe, Neunert, Degrave,
  Wiele, Mnih, Heess, and Springenberg]{riedmiller2018learning}
Riedmiller, M., Hafner, R., Lampe, T., Neunert, M., Degrave, J., Wiele, T.,
  Mnih, V., Heess, N., and Springenberg, J.~T.
\newblock Learning by playing solving sparse reward tasks from scratch.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4341--4350, 2018.

\bibitem[Scherrer et~al.(2015)Scherrer, Ghavamzadeh, Gabillon, Lesner, and
  Geist]{scherrer2015approximate}
Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M.
\newblock Approximate modified policy iteration and its application to the game
  of tetris.
\newblock \emph{Journal of Machine Learning Research (JMLR)}, 16:\penalty0
  1629--1676, 2015.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Chen, and
  Abbeel]{schulman2017equivalence}
Schulman, J., Chen, X., and Abbeel, P.
\newblock Equivalence between policy gradients and soft q-learning.
\newblock \emph{arXiv preprint arXiv:1704.06440}, 2017.

\bibitem[Sutton et~al.(2000)Sutton, McAllester, Singh, and
  Mansour]{sutton2000policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in neural information processing systems (NIPS)},
  2000.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Ziebart et~al.(2008)Ziebart, Maas, Bagnell, and
  Dey]{ziebart2008maximum}
Ziebart, B.~D., Maas, A.~L., Bagnell, J.~A., and Dey, A.~K.
\newblock Maximum entropy inverse reinforcement learning.
\newblock In \emph{AAAI Conference on Artificial Intelligence (AAAI)}, 2008.

\end{thebibliography}
