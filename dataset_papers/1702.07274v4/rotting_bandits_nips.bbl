\begin{thebibliography}{35}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2009)Agarwal, Chen, and Elango]{SpatioTemporal}
D.~Agarwal, B.-C. Chen, and P.~Elango.
\newblock Spatio-temporal models for estimating click-through rate.
\newblock In \emph{Proceedings of the 18th international conference on World
  wide web}, pages 21--30. ACM, 2009.

\bibitem[Agrawal and Goyal(2013)]{TSfurther}
S.~Agrawal and N.~Goyal.
\newblock Further optimal regret bounds for thompson sampling.
\newblock In \emph{Aistats}, pages 99--107, 2013.

\bibitem[Arora et~al.(2012)Arora, Dekel, and Tewari]{RegretToPolicyRegret}
R.~Arora, O.~Dekel, and A.~Tewari.
\newblock Online bandit learning against an adaptive adversary: from regret to
  policy regret.
\newblock \emph{arXiv preprint arXiv:1206.6400}, 2012.

\bibitem[Auer et~al.(2002{\natexlab{a}})Auer, Cesa-Bianchi, and
  Fischer]{FiniteTimeAnalysisMAB}
P.~Auer, N.~Cesa-Bianchi, and P.~Fischer.
\newblock Finite-time analysis of the multiarmed bandit problem.
\newblock \emph{Machine learning}, 47\penalty0 (2-3):\penalty0 235--256,
  2002{\natexlab{a}}.

\bibitem[Auer et~al.(2002{\natexlab{b}})Auer, Cesa-Bianchi, Freund, and
  Schapire]{NonStochasticMAB}
P.~Auer, N.~Cesa-Bianchi, Y.~Freund, and R.~E. Schapire.
\newblock The nonstochastic multiarmed bandit problem.
\newblock \emph{SIAM Journal on Computing}, 32\penalty0 (1):\penalty0 48--77,
  2002{\natexlab{b}}.

\bibitem[Awerbuch and Kleinberg(2004)]{AdaptiveRouting}
B.~Awerbuch and R.~D. Kleinberg.
\newblock Adaptive routing with end-to-end feedback: Distributed learning and
  geometric approaches.
\newblock In \emph{Proceedings of the thirty-sixth annual ACM symposium on
  Theory of computing}, pages 45--53. ACM, 2004.

\bibitem[Besbes et~al.(2014)Besbes, Gur, and Zeevi]{StocMABNonStat}
O.~Besbes, Y.~Gur, and A.~Zeevi.
\newblock Stochastic multi-armed-bandit problem with non-stationary rewards.
\newblock In \emph{Advances in neural information processing systems}, pages
  199--207, 2014.

\bibitem[Bouneffouf and Feraud(2016)]{MABKnownTrend}
D.~Bouneffouf and R.~Feraud.
\newblock Multi-armed bandit problem with known trend.
\newblock \emph{Neurocomputing}, 205:\penalty0 16--21, 2016.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{PredictionsLearningGames}
N.~Cesa-Bianchi and G.~Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge university press, 2006.

\bibitem[Chakrabarti et~al.(2009)Chakrabarti, Kumar, Radlinski, and
  Upfal]{MortalMAB}
D.~Chakrabarti, R.~Kumar, F.~Radlinski, and E.~Upfal.
\newblock Mortal multi-armed bandits.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  273--280, 2009.

\bibitem[Du et~al.(2013)Du, Ibrahim, Shehata, and Badawy]{AutoLicPlate}
S.~Du, M.~Ibrahim, M.~Shehata, and W.~Badawy.
\newblock Automatic license plate recognition (alpr): A state-of-the-art
  review.
\newblock \emph{IEEE Transactions on Circuits and Systems for Video
  Technology}, 23\penalty0 (2):\penalty0 311--325, 2013.

\bibitem[Garivier and Capp{\'e}(2011)]{KLUCB}
A.~Garivier and O.~Capp{\'e}.
\newblock The kl-ucb algorithm for bounded stochastic bandits and beyond.
\newblock In \emph{COLT}, pages 359--376, 2011.

\bibitem[Garivier and Moulines(2008)]{UpperConfBoundNonStat}
A.~Garivier and E.~Moulines.
\newblock On upper-confidence bound policies for non-stationary bandit
  problems.
\newblock \emph{arXiv preprint arXiv:0805.3415}, 2008.

\bibitem[Gittins(1979)]{gittins1979bandit}
J.~C. Gittins.
\newblock Bandit processes and dynamic allocation indices.
\newblock \emph{Journal of the Royal Statistical Society. Series B
  (Methodological)}, pages 148--177, 1979.

\bibitem[Gopalan et~al.(2014)Gopalan, Mannor, and Mansour]{TScomplex}
A.~Gopalan, S.~Mannor, and Y.~Mansour.
\newblock Thompson sampling for complex online problems.
\newblock In \emph{ICML}, volume~14, pages 100--108, 2014.

\bibitem[Hazan and Kale(2011)]{BetterAlgs}
E.~Hazan and S.~Kale.
\newblock Better algorithms for benign bandits.
\newblock \emph{Journal of Machine Learning Research}, 12\penalty0
  (Apr):\penalty0 1287--1311, 2011.

\bibitem[Heidari et~al.()Heidari, Kearns, and Roth]{TightPolicyRegret}
H.~Heidari, M.~Kearns, and A.~Roth.
\newblock Tight policy regret bounds for improving and decaying bandits.

\bibitem[Jones and Gittins(1972)]{DynamicAllocationIndex}
D.~M. Jones and J.~C. Gittins.
\newblock \emph{A dynamic allocation index for the sequential design of
  experiments}.
\newblock University of Cambridge, Department of Engineering, 1972.

\bibitem[Kaspi and Mandelbaum(1998)]{kaspi1998multi}
H.~Kaspi and A.~Mandelbaum.
\newblock Multi-armed bandits in discrete and continuous time.
\newblock \emph{Annals of Applied Probability}, pages 1270--1290, 1998.

\bibitem[Kaufmann et~al.(2012)Kaufmann, Korda, and Munos]{TSfinite}
E.~Kaufmann, N.~Korda, and R.~Munos.
\newblock Thompson sampling: An asymptotically optimal finite-time analysis.
\newblock In \emph{International Conference on Algorithmic Learning Theory},
  pages 199--213. Springer, 2012.

\bibitem[Kleinberg and Leighton(2003)]{ValueKnowingDemand}
R.~Kleinberg and T.~Leighton.
\newblock The value of knowing a demand curve: Bounds on regret for online
  posted-price auctions.
\newblock In \emph{Foundations of Computer Science, 2003. Proceedings. 44th
  Annual IEEE Symposium on}, pages 594--605. IEEE, 2003.

\bibitem[Kocsis and Szepesv{\'a}ri(2006)]{DiscountedUCB}
L.~Kocsis and C.~Szepesv{\'a}ri.
\newblock Discounted ucb.
\newblock In \emph{2nd PASCAL Challenges Workshop}, pages 784--791, 2006.

\bibitem[Komiyama and Qin(2014)]{TimeDecayingBandits}
J.~Komiyama and T.~Qin.
\newblock Time-decaying bandits for non-stationary systems.
\newblock In \emph{International Conference on Web and Internet Economics},
  pages 460--466. Springer, 2014.

\bibitem[Lai and Robbins(1985)]{AsymEfficient}
T.~L. Lai and H.~Robbins.
\newblock Asymptotically efficient adaptive allocation rules.
\newblock \emph{Advances in applied mathematics}, 6\penalty0 (1):\penalty0
  4--22, 1985.

\bibitem[Maillard et~al.(2011)Maillard, Munos, Stoltz, et~al.]{MABkldiv}
O.-A. Maillard, R.~Munos, G.~Stoltz, et~al.
\newblock A finite-time analysis of multi-armed bandits problems with
  kullback-leibler divergences.
\newblock In \emph{COLT}, pages 497--514, 2011.

\bibitem[Mandelbaum(1987)]{mandelbaum1987continuous}
A.~Mandelbaum.
\newblock Continuous multi-armed bandits and multiparameter processes.
\newblock \emph{The Annals of Probability}, pages 1527--1556, 1987.

\bibitem[Pandey et~al.(2007)Pandey, Agarwal, Chakrabarti, and
  Josifovski]{BanditsTaxonomies}
S.~Pandey, D.~Agarwal, D.~Chakrabarti, and V.~Josifovski.
\newblock Bandits for taxonomies: A model-based approach.
\newblock In \emph{SDM}, pages 216--227. SIAM, 2007.

\bibitem[Robbins(1985)]{SeqDesignExp}
H.~Robbins.
\newblock Some aspects of the sequential design of experiments.
\newblock In \emph{Herbert Robbins Selected Papers}, pages 169--177. Springer,
  1985.

\bibitem[Slivkins and Upfal(2008)]{BrownianRestless}
A.~Slivkins and E.~Upfal.
\newblock Adapting to a changing environment: the brownian restless bandits.
\newblock In \emph{COLT}, pages 343--354, 2008.

\bibitem[Tekin and Liu(2012)]{OnlineLearningRestedRestless}
C.~Tekin and M.~Liu.
\newblock Online learning of rested and restless bandits.
\newblock \emph{IEEE Transactions on Information Theory}, 58\penalty0
  (8):\penalty0 5588--5611, 2012.

\bibitem[Thompson(1933)]{ThompsonLikelihood}
W.~R. Thompson.
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock \emph{Biometrika}, 25\penalty0 (3/4):\penalty0 285--294, 1933.

\bibitem[Tran-Thanh et~al.(2012)Tran-Thanh, Stein, Rogers, and
  Jennings]{EfficientCrowd}
L.~Tran-Thanh, S.~Stein, A.~Rogers, and N.~R. Jennings.
\newblock Efficient crowdsourcing of unknown experts using multi-armed bandits.
\newblock In \emph{European Conference on Artificial Intelligence}, pages
  768--773, 2012.

\bibitem[Whittle(1988)]{RestlessBanditsAllocation}
P.~Whittle.
\newblock Restless bandits: Activity allocation in a changing world.
\newblock \emph{Journal of applied probability}, pages 287--298, 1988.

\bibitem[Whittle et~al.(1981)]{ArmAcquiring}
P.~Whittle et~al.
\newblock Arm-acquiring bandits.
\newblock \emph{The Annals of Probability}, 9\penalty0 (2):\penalty0 284--292,
  1981.

\bibitem[Yu and Mannor(2009)]{PiecewiseStatMAB}
J.~Y. Yu and S.~Mannor.
\newblock Piecewise-stationary bandit problems with side observations.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pages 1177--1184. ACM, 2009.

\end{thebibliography}
