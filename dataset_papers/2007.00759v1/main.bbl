\begin{thebibliography}{27}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019{\natexlab{a}})Agarwal, Bullins, Hazan, Kakade, and
  Singh]{agarwal2019online}
N.~Agarwal, B.~Bullins, E.~Hazan, S.~Kakade, and K.~Singh.
\newblock Online control with adversarial disturbances.
\newblock In \emph{International Conference on Machine Learning}, pages
  111--119, 2019{\natexlab{a}}.

\bibitem[Agarwal et~al.(2019{\natexlab{b}})Agarwal, Hazan, and
  Singh]{agarwal2019logarithmic}
N.~Agarwal, E.~Hazan, and K.~Singh.
\newblock Logarithmic regret for online control.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10175--10184, 2019{\natexlab{b}}.

\bibitem[Anava et~al.(2015)Anava, Hazan, and Mannor]{anava2015online}
O.~Anava, E.~Hazan, and S.~Mannor.
\newblock Online learning for adversaries with memory: price of past mistakes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  784--792, 2015.

\bibitem[Arora et~al.(2012)Arora, Dekel, and Tewari]{arora2012online}
R.~Arora, O.~Dekel, and A.~Tewari.
\newblock Online bandit learning against an adaptive adversary: from regret to
  policy regret.
\newblock In \emph{Proceedings of the 29th International Conference on Machine
  Learning}, pages 1747--1754, 2012.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
M.~G. Azar, I.~Osband, and R.~Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 263--272. JMLR. org, 2017.

\bibitem[Cassel et~al.(2020)Cassel, Cohen, and Koren]{cassel2020logarithmic}
A.~Cassel, A.~Cohen, and T.~Koren.
\newblock Logarithmic regret for learning linear quadratic regulators
  efficiently.
\newblock \emph{arXiv preprint arXiv:2002.08095}, 2020.

\bibitem[Cesa-Bianchi et~al.(2018)Cesa-Bianchi, Gentile, and
  Mansour]{cesa2018nonstochastic}
N.~Cesa-Bianchi, C.~Gentile, and Y.~Mansour.
\newblock Nonstochastic bandits with composite anonymous feedback.
\newblock In \emph{Conference On Learning Theory}, pages 750--773, 2018.

\bibitem[Cohen et~al.(2018)Cohen, Hasidim, Koren, Lazic, Mansour, and
  Talwar]{cohen2018online}
A.~Cohen, A.~Hasidim, T.~Koren, N.~Lazic, Y.~Mansour, and K.~Talwar.
\newblock Online linear quadratic control.
\newblock In \emph{International Conference on Machine Learning}, pages
  1029--1038, 2018.

\bibitem[Cohen et~al.(2019)Cohen, Koren, and Mansour]{cohen2019learning}
A.~Cohen, T.~Koren, and Y.~Mansour.
\newblock Learning linear-quadratic regulators efficiently with only $\sqrt{T}$
  regret.
\newblock In \emph{International Conference on Machine Learning}, pages
  1300--1309, 2019.

\bibitem[Dekel and Hazan(2013)]{dekel2013better}
O.~Dekel and E.~Hazan.
\newblock Better rates for any adversarial deterministic {MDP}.
\newblock In \emph{International Conference on Machine Learning}, pages
  675--683, 2013.

\bibitem[Dekel et~al.(2014)Dekel, Hazan, and Koren]{dekel2014blinded}
O.~Dekel, E.~Hazan, and T.~Koren.
\newblock The blinded bandit: Learning with adaptive feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1610--1618, 2014.

\bibitem[Dick et~al.(2014)Dick, Gyorgy, and Szepesvari]{dick2014online}
T.~Dick, A.~Gyorgy, and C.~Szepesvari.
\newblock Online learning in markov decision processes with changing cost
  sequences.
\newblock In \emph{International Conference on Machine Learning}, pages
  512--520, 2014.

\bibitem[Duchi et~al.(2011)Duchi, Hazan, and Singer]{duchi2011adaptive}
J.~Duchi, E.~Hazan, and Y.~Singer.
\newblock Adaptive subgradient methods for online learning and stochastic
  optimization.
\newblock \emph{Journal of machine learning research}, 12\penalty0
  (Jul):\penalty0 2121--2159, 2011.

\bibitem[Flaxman et~al.(2005)Flaxman, Kalai, and McMahan]{flaxman2004online}
A.~D. Flaxman, A.~T. Kalai, and H.~B. McMahan.
\newblock Online convex optimization in the bandit setting: gradient descent
  without a gradient.
\newblock In \emph{Proceedings of the sixteenth annual ACM-SIAM symposium on
  Discrete algorithms}, pages 385--394, 2005.

\bibitem[Foster and Simchowitz(2020)]{foster2020logarithmic}
D.~J. Foster and M.~Simchowitz.
\newblock Logarithmic regret for adversarial online control.
\newblock \emph{arXiv preprint arXiv:2003.00189}, 2020.

\bibitem[Hazan and Levy(2014)]{hazan2014bandit}
E.~Hazan and K.~Levy.
\newblock Bandit convex optimization: Towards tight bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  784--792, 2014.

\bibitem[Hazan et~al.(2007)Hazan, Agarwal, and Kale]{hazan2007logarithmic}
E.~Hazan, A.~Agarwal, and S.~Kale.
\newblock Logarithmic regret algorithms for online convex optimization.
\newblock \emph{Machine Learning}, 69\penalty0 (2-3):\penalty0 169--192, 2007.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
T.~Jaksch, R.~Ortner, and P.~Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Apr):\penalty0 1563--1600, 2010.

\bibitem[Jin and Luo(2019)]{jin2019learning}
T.~Jin and H.~Luo.
\newblock Learning adversarial {MDP}s with bandit feedback and unknown
  transition.
\newblock \emph{arXiv preprint arXiv:1912.01192}, 2019.

\bibitem[Levin and Peres(2017)]{levin2017markov}
D.~A. Levin and Y.~Peres.
\newblock \emph{Markov chains and mixing times}.
\newblock American Mathematical Society, 2017.

\bibitem[Mania et~al.(2019)Mania, Tu, and Recht]{mania2019certainty}
H.~Mania, S.~Tu, and B.~Recht.
\newblock Certainty equivalence is efficient for linear quadratic control.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10154--10164, 2019.

\bibitem[Neu et~al.(2010)Neu, Antos, Gy{\"o}rgy, and
  Szepesv{\'a}ri]{neu2010online}
G.~Neu, A.~Antos, A.~Gy{\"o}rgy, and C.~Szepesv{\'a}ri.
\newblock Online markov decision processes under bandit feedback.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1804--1812, 2010.

\bibitem[Rosenberg and Mansour(2019)]{rosenberg2019online}
A.~Rosenberg and Y.~Mansour.
\newblock Online stochastic shortest path with bandit feedback and unknown
  transition function.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2209--2218, 2019.

\bibitem[Saha and Tewari(2011)]{saha2011improved}
A.~Saha and A.~Tewari.
\newblock Improved regret guarantees for online smooth convex optimization with
  bandit feedback.
\newblock In \emph{Proceedings of the Fourteenth International Conference on
  Artificial Intelligence and Statistics}, pages 636--642, 2011.

\bibitem[Shamir(2013)]{shamir2013complexity}
O.~Shamir.
\newblock On the complexity of bandit and derivative-free stochastic convex
  optimization.
\newblock In \emph{Conference on Learning Theory}, pages 3--24, 2013.

\bibitem[Simchowitz and Foster(2020)]{simchowitz2020naive}
M.~Simchowitz and D.~J. Foster.
\newblock Naive exploration is optimal for online {LQR}.
\newblock \emph{arXiv preprint arXiv:2001.09576}, 2020.

\bibitem[Simchowitz et~al.(2020)Simchowitz, Singh, and
  Hazan]{simchowitz2020improper}
M.~Simchowitz, K.~Singh, and E.~Hazan.
\newblock Improper learning for non-stochastic control.
\newblock \emph{arXiv preprint arXiv:2001.09254}, 2020.

\end{thebibliography}
