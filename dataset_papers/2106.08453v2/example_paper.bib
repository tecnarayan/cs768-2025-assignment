@article{maennel2020what,
       author = {{Maennel}, Hartmut and {Alabdulmohsin}, Ibrahim and {Tolstikhin}, Ilya and
         {Baldock}, Robert J.~N. and {Bousquet}, Olivier and {Gelly}, Sylvain and
         {Keysers}, Daniel},
        journal = "arXiv preprint",
        title = "{What do neural networks learn when trained with random labels?}",
         year = 2020
}

@article{tishby2015IB,
  author    = {Naftali Tishby and
               Noga Zaslavsky},
  title     = {Deep learning and the information bottleneck principle},
  journal   = {IEEE ITW},
  year      = {2015}
}

@article{jacot2018NTK,
  author    = {Arthur Jacot and
               Franck Gabriel and
               Cl{\'{e}}ment Hongler},
  title     = {Neural tangent kernel: convergence and generalization in neural networks},
  journal   = {NeurIPS},
  year      = {2018}
}

@article{chan2015pcanet,
  author    = {Tsung{-}Han Chan and
               Kui Jia and
               Shenghua Gao and
               Jiwen Lu and
               Zinan Zeng and
               Yi Ma},
  title     = {PCANet: {A} simple deep learning baseline for image classification?},
  journal   = {IEEE Transactions on Image Processing},
  year      = {2015}
}

@article{gan2015pca,
  author    = {Yanhai Gan and
               Jun Liu and
               Junyu Dong and
               Guoqiang Zhong},
  title     = {A PCA-based convolutional network},
  year      = {2015},
  journal   = {arXiv preprint}
}

@article{waleffe2020principal,
    title={Principal component networks: parameter reduction early in training},
    author={Roger Waleffe and Theodoros Rekatsinas},
    year={2020},
    journal={arXiv preprint},
}

@article{saxe2019mathematical,
  author    = {Andrew M. Saxe and
               James L. McClelland and
               Surya Ganguli},
  title     = {A mathematical theory of semantic development in deep neural networks},
  journal   = {PNAS},
  year      = {2019}
}


@article{lee2019WideNN,
  title={Wide neural networks of any depth evolve as linear models under gradient descent},
  author={Jaehoon Lee and Lechao Xiao and Samuel S. Schoenholz and Yasaman Bahri and Roman Novak and Jascha Sohl-Dickstein and Jeffrey Pennington},
  journal={NeurIPS},
  year={2019}
}


@article{obeid2019structured,
    title={Structured and deep similarity matching via structured and deep hebbian networks},
    author={Dina Obeid and Hugo Ramambason and Cengiz Pehlevan},
    year={2019},
    journal={NeurIPS}
}

@article{qin2020contrastive,
    title={Contrastive similarity matching for supervised learning},
    author={Shanshan Qin and Nayantara Mudur and Cengiz Pehlevan},
    year={2020},
    journal={arXiv preprint}
}

@article{illing2019biologically,
   title={Biologically plausible deep learning — But how far can we go with shallow networks?},
   journal={Neural Networks},
   author={Illing, Bernd and Gerstner, Wulfram and Brea, Johanni},
   year={2019}
}


@article{gu2019metalearning,
	author = {Gu, Keren and Greydanus, Sam and Metz, Luke and Maheswaranathan, Niru and Sohl-Dickstein, Jascha},
	title = {Meta-Learning biologically plausible semi-supervised update rules},
	year = {2019},
	journal = {bioRxiv preprint}
}


@article{ouali2020overview,
    title={An overview of deep semi-supervised learning},
    author={Yassine Ouali and Céline Hudelot and Myriam Tami},
    year={2020},
    journal={arXiv preprint}
}

@article{nokland2019training,
       author = {{N{\o}kland}, Arild and {Hiller Eidnes}, Lars},
        title = "{Training neural networks with local error signals}",
      journal = {ICML},
         year = "2019"
}

@article{marschall2020unified,
       author = {Marschall, Owen and Cho, Kyunghyun and Savin, Cristina},
        title = "{A unified framework of online learning algorithms for
training recurrent neural networks}",
      journal = {JMLR},
         year = "2020"
}

@article{nassar2020neural,
      title={On 1/n neural representation and robustness}, 
      author={Josue Nassar and Piotr Aleksander Sokol and SueYeon Chung and Kenneth D. Harris and Il Memming Park},
      year={2020},
      journal={NeurIPS}
}

@article{zhang2017understanding,
      title={Understanding deep learning requires rethinking generalization}, 
      author={Chiyuan Zhang and Samy Bengio and Moritz Hardt and Benjamin Recht and Oriol Vinyals},
      year={2017},
      journal={ICLR}
}

@article{yosinski2014transferable,
 author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
 title = {How transferable are features in deep neural networks?},
 journal = {NeurIPS},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
 year = {2014}
}

@article{ansuini2019intrinsic,
      title={Intrinsic dimension of data representations in deep neural networks}, 
      author={Alessio Ansuini and Alessandro Laio and Jakob H. Macke and Davide Zoccolan},
      year={2019},
      journal={NeurIPS}
}

@article{kornblith2019similarity,
      title={Similarity of neural network representations revisited}, 
      author={Simon Kornblith and Mohammad Norouzi and Honglak Lee and Geoffrey Hinton},
      year={2019},
      journal={ICML}
}

@article{nguyen2021wide,
      title={Do wide and deep networks learn the same things? Uncovering how neural network representations vary with width and depth}, 
      author={Thao Nguyen and Maithra Raghu and Simon Kornblith},
      year={2021},
      journal={ICLR}
}

@article{wang2018understanding,
      title={Towards understanding learning representations: to what extent do different neural networks learn the same representation}, 
      author={Liwei Wang and Lunjia Hu and Jiayuan Gu and Yue Wu and Zhiqiang Hu and Kun He and John Hopcroft},
      year={2018},
      journal={NeurIPS}
}

@article{bengio2016stdpvae,
    title={Towards biologically plausible deep learning},
    author={Yoshua Bengio and Dong-Hyun Lee and Jorg Bornschein, Thomas Mesnard and Zhouhan Lin},
    year={2016},
    journal={arXiv preprint}
}

@article{nokland2016direct,
      title={Direct feedback alignment provides learning in deep neural networks}, 
      author={Arild Nøkland},
      year={2016},
      journal={NeurIPS}
}

@article{yang2020feature,
Author = {Greg Yang and Edward J. Hu},
Title = {Feature learning in infinite-width neural networks},
Year = {2020},
journal = {arXiv preprint},
}

@article{lee2020finite,
Author = {Jaehoon Lee and Samuel S. Schoenholz and Jeffrey Pennington and Ben Adlam and Lechao Xiao and Roman Novak and Jascha Sohl-Dickstein},
Title = {Finite versus infinite neural networks: an empirical study},
Year = {2020},
journal = {NeurIPS},
}

@article{arora2019exact,
Author = {Sanjeev Arora and Simon S. Du and Wei Hu and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang},
Title = {On exact computation with an infinitely wide neural net},
Year = {2019},
journal = {NeurIPS},
}

@article{lillicrap2016random,
      title={Random synaptic feedback weights support error backpropagation for deep learning}, 
      author={Timothy P. Lillicrap and Daniel Cownden and Douglas B. Tweed and Colin J. Akerman},
      year={2016},
      journal={Nature Communications}
}

@article{bartunov2018assessing,
      title={Assessing the scalability of biologically-motivated deep learning algorithms and architectures}, 
      author={Sergey Bartunov and Adam Santoro and Blake A. Richards and Luke Marris and Geoffrey E. Hinton and Timothy Lillicrap},
      year={2018},
      journal={NeurIPS}
}


@article{Roth19,
	author = {C. Roth and I. Kanitscheider and I.R. Fiete},
	booktitle = {ICLR},
	date-added = {2019-05-02 08:13:31 -0400},
	date-modified = {2019-05-02 08:14:47 -0400},
	title = {Kernel RNN learning (KeRNL)},
	year = {2019}}


@article{Fiete06,
	abstract = {We present a method of estimating the gradient of an objective function
	with respect to the synaptic weights of a spiking neural network.
	The method works by measuring the fluctuations in the objective function
	in response to dynamic perturbation of the membrane conductances
	of the neurons. It is compatible with recurrent networks of conductance-based
	model neurons with dynamic synapses. The method can be interpreted
	as a biologically plausible synaptic learning rule, if the dynamic
	perturbations are generated by a special class of "empiric" synapses
	driven by random spike trains from an external source.},
	address = {Kavli Institute for Theoretical Physics, University of California, Santa Barbara, California 93106, USA.},
	au = {Fiete, IR and Seung, HS},
	author = {Fiete, I.R. and Seung, H.S.},
	da = {20060815},
	date-added = {2015-04-01 14:28:54 +0000},
	date-modified = {2020-08-30 16:25:20 -0400},
	dcom = {20061025},
	dep = {20060728},
	edat = {2006/08/16 09:00},
	issn = {0031-9007 (Print)},
	jid = {0401141},
	journal = {Phys Rev Lett},
	jt = {Physical review letters},
	keywords = {Action Potentials/*physiology; Animals; Biological Clocks/*physiology; Cell Membrane/*physiology; Computer Simulation; Electric Conductivity; Humans; Linear Models; *Models, Neurological; Nerve Net/*physiology; Neurons/*physiology; Synaptic Transmission/*physiology},
	language = {eng},
	mhda = {2006/10/26 09:00},
	number = {4},
	own = {NLM},
	pages = {048104},
	phst = {2006/01/19 {$[$}received{$]$}; 2006/07/28 {$[$}aheadofprint{$]$}},
	pl = {United States},
	pmid = {16907616},
	pst = {ppublish},
	pt = {Journal Article},
	pubm = {Print-Electronic},
	sb = {IM},
	so = {Phys Rev Lett. 2006 Jul 28;97(4):048104. Epub 2006 Jul 28.},
	stat = {MEDLINE},
	title = {Gradient learning in spiking neural networks by dynamic perturbation of conductances.},
	volume = {97},
	year = {2006}}


@article{Xie04,
	abstract = {Artificial neural networks are often trained by using the back propagation algorithm to compute the gradient of an objective function with respect to the synaptic strengths. For a biological neural network, such a gradient computation would be difficult to implement, because of the complex dynamics of intrinsic and synaptic conductances in neurons. Here we show that irregular spiking similar to that observed in biological neurons could be used as the basis for a learning rule that calculates a stochastic approximation to the gradient. The learning rule is derived based on a special class of model networks in which neurons fire spike trains with Poisson statistics. The learning is compatible with forms of synaptic dynamics such as short-term facilitation and depression. By correlating the fluctuations in irregular spiking with a reward signal, the learning rule performs stochastic gradient ascent on the expected reward. It is applied to two examples, learning the XOR computation and learning direction selectivity using depressing synapses. We also show in simulation that the learning rule is applicable to a network of noisy integrate-and-fire neurons.},
	author = {X Xie and HS Seung},
	date-added = {2015-04-01 13:56:42 +0000},
	date-modified = {2015-04-01 13:56:42 +0000},
	journal = {Phys Rev E Stat Nonlin Soft Matter Phys.},
	month = {Apr},
	pages = {041909},
	title = {Learning in neural networks by reinforcement of irregular spiking},
	url = {Xie04.pdf},
	volume = {69},
	year = {2004},
	Bdsk-Url-1 = {Xie04.pdf}}


@article{Seung03,
	abstract = {It is well-known that chemical synaptic transmission is an unreliable process, but the function of such unreliability remains unclear. Here I consider the hypothesis that the randomness of synaptic transmission is harnessed by the brain for learning, in analogy to the way that genetic mutation is utilized by Darwinian evolution. This is possible if synapses are "hedonistic," responding to a global reward signal by increasing their probabilities of vesicle release or failure, depending on which action immediately preceded reward. Hedonistic synapses learn by computing a stochastic approximation to the gradient of the average reward. They are compatible with synaptic dynamics such as short-term facilitation and depression and with the intricacies of dendritic integration and action potential generation. A network of hedonistic synapses can be trained to perform a desired computation by administering reward appropriately, as illustrated here through numerical simulations of integrate-and-fire model neurons.},
	address = {Howard Hughes Medical Institute and Brain and Cognitive Sciences Department, Massachusetts Institute of Technology, Cambridge, MA 02139, USA. seung@mit.edu},
	au = {Seung, HS},
	author = {Seung, H Sebastian},
	crdt = {2003/12/23 05:00},
	da = {20031222},
	date-added = {2015-04-01 14:28:54 +0000},
	date-modified = {2015-04-01 14:28:54 +0000},
	dcom = {20040128},
	edat = {2003/12/23 05:00},
	issn = {0896-6273 (Print)},
	jid = {8809320},
	journal = {Neuron},
	jt = {Neuron},
	keywords = {Action Potentials/*physiology; Learning/*physiology; *Neural Networks (Computer); *Reinforcement (Psychology); Stochastic Processes; Synaptic Transmission/*physiology},
	language = {eng},
	mhda = {2004/01/30 05:00},
	number = {6},
	own = {NLM},
	pages = {1063--1073},
	pii = {S089662730300761X},
	pl = {United States},
	pmid = {14687542},
	pst = {ppublish},
	pt = {Journal Article},
	sb = {IM},
	so = {Neuron. 2003 Dec 18;40(6):1063-73.},
	stat = {MEDLINE},
	title = {Learning in spiking neural networks by reinforcement of stochastic synaptic transmission.},
	volume = {40},
	year = {2003}}



@article{bellec2020solution,
      title={A solution to the learning dilemma for recurrent networks of spiking neurons}, 
      author={Guillaume Bellec and Franz Scherr and Elias Hajek and Darjan Salaj and Robert Legenstein and Wolfgang Maass},
      year={2020},
      journal={Nature Communications}
}

@article{richards2019deep,
    author = {Richards, Blake and Lillicrap, Timothy and Beaudoin, Philippe and Bengio, Y. and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui and Berker, Archy and Ganguli, Surya and Gillon, Colleen and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace and Miller, Kenneth and Naud, Richard and Pack, Christopher and Kording, Konrad},
    year = {2019},
    month = {11},
    pages = {1761-1770},
    title = {A deep learning framework for neuroscience},
    volume = {22},
    journal = {Nature Neuroscience},
    doi = {10.1038/s41593-019-0520-2}
}

@article{collobert2009unified,
author = {Collobert, Ronan and Weston, Jason},
title = {A unified architecture for natural language processing: deep neural networks with multitask learning},
year = {2008},
journal = {ICML}
}


@article{zhang2014facial,
author = {Zhanpeng Zhang and Ping Luo and Chen Change Loy and Xiaoou Tang},
title = {Facial landmark detection by
deep multi-task learning},
year = {2014},
journal = {ECCV}
}


@article{matthews2018gaussian,
      title={Gaussian process behaviour in wide deep neural networks}, 
      author={Alexander G. de G. Matthews and Mark Rowland and Jiri Hron and Richard E. Turner and Zoubin Ghahramani},
      year={2018},
      journal={ICLR}
}

@article{lee2018deep,
      title={Deep neural networks as Gaussian processes}, 
      author={Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl-Dickstein},
      year={2018},
      journal={ICLR}
}

@article{novak2019bayesian,
      title={Bayesian deep convolutional networks with many channels are Gaussian processes},
      author={Roman Novak and Lechao Xiao and Jaehoon Lee and Yasaman Bahri and Greg Yang and Jiri Hron and Daniel A. Abolafia and Jeffrey Pennington and Jascha Sohl-Dickstein},
      year={2019},
      journal={ICLR}
}

@article{clanuwat2018kmnist,
  author    = {Tarin Clanuwat and
               Mikel Bober{-}Irizar and
               Asanobu Kitamoto and
               Alex Lamb and
               Kazuaki Yamamoto and
               David Ha},
  title     = {Deep learning for classical Japanese literature},
  journal   = {NeurIPS Workshop on Machine Learning for Creativity and Design},
  year      = {2018}
}

@article{krizhevsky2009cifar,
    author = {Alex Krizhevsky},
    title = {Learning multiple layers of features from tiny images},
    year = {2009}
}

@article{russakovsky2015Imagenet,
    Author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
    Title = {{ImageNet large scale visual recognition challenge}},
    Year = {2015},
    journal   = {IJCV}
}

@article{kingma2015adam,
      title={Adam: A method for stochastic optimization}, 
      author={Diederik P. Kingma and Jimmy Ba},
      year={2015},
      journal={ICLR}
}

@article{arora2020harnessing,
      title={Harnessing the power of infinitely wide deep nets on small-data tasks}, 
      author={Sanjeev Arora and Simon S. Du and Zhiyuan Li and Ruslan Salakhutdinov and Ruosong Wang and Dingli Yu},
      year={2020},
      journal={ICLR}
}

@article{colonnier1981number,
      title={Number of neurons and synapses in the visual cortex of different species}, 
      author={M Colonnier, J O'Kusky},
      year={1981},
      journal={Rev Can Biol.}
}

@article{liao2016important,
      title={How important is weight symmetry in backpropagation?}, 
      author={Qianli Liao and Joel Z. Leibo and Tomaso Poggio},
      year={2016},
      journal={AAAI}
}










@article{lee2015difference,
      title={Difference Target Propagation}, 
      author={Dong-Hyun Lee and Saizheng Zhang and Asja Fischer and Yoshua Bengio},
      year={2015},
      journal={ECML/PKDD}
}


@article{meulemans2020theoretical,
      title={A theoretical framework for target propagation}, 
      author={Alexander Meulemans and Francesco S. Carzaniga and Johan A. K. Suykens and João Sacramento and Benjamin F. Grewe},
      year={2020},
      journal={NeurIPS},
}


@article{millidge2020predictive,
      title={Predictive coding approximates backprop along arbitrary computation graphs}, 
      author={Beren Millidge and Alexander Tschantz and Christopher L. Buckley},
      year={2020},
      journal={arXiv preprint}
}


@article{launay2020direct,
      title={Direct feedback alignment scales to modern deep learning tasks and architectures}, 
      author={Julien Launay and Iacopo Poli and François Boniface and Florent Krzakala},
      year={2020},
      journal={NeurIPS},
}

@article{refinetti2021align,
      title={Align, then memorise: the dynamics of learning with feedback alignment}, 
      author={Maria Refinetti and Stéphane d'Ascoli and Ruben Ohana and Sebastian Goldt},
      year={2021},
      journal={ICML}
}

@article{song2021convergence,
      title={Convergence and alignment of gradient descent with random backpropagation weights}, 
      author={Ganlin Song and Ruitu Xu and John Lafferty},
      year={2021},
      journal={NeurIPS}
}

@article{ororbia2019biologically, 
    title={Biologically motivated algorithms for propagating local target representations}, volume={33}, DOI={10.1609/aaai.v33i01.33014651}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Ororbia, Alexander G. and Mali, Ankur}, year={2019}, month={Jul.}, pages={4651-4658} 
}

@article{ororbia2018conducting,
      title={Conducting credit assignment by aligning local representations}, 
      author={Alexander G. Ororbia and Ankur Mali and Daniel Kifer and C. Lee Giles},
      year={2018},
      journal={arXiv preprint}
}

@article{ororbia2020largescale,
      title={Large-scale gradient-free deep learning with recursive local representation alignment}, 
      author={Alexander Ororbia and Ankur Mali and Daniel Kifer and C. Lee Giles},
      year={2020},
      journal={arXiv preprint},
}

@article{ororbia2022neural,
      title={The neural coding framework for learning generative models},
      author={Alexander Ororbia and Daniel Kifer},
      year={2022},
      journal={arXiv preprint}
}


@ARTICLE{ororbia2020continual,  
    author={Ororbia, Alexander and Mali, Ankur and Giles, C. Lee and Kifer, Daniel},  
    journal={IEEE Transactions on Neural Networks and Learning Systems},   
    title={Continual learning of recurrent neural networks by locally aligning distributed representations},   
    year={2020},  
    volume={31},  
    number={10},  
    pages={4267-4278},  doi={10.1109/TNNLS.2019.2953622}
}

@article{akrout2019deep,
     author = {Akrout, Mohamed and Wilson, Collin and Humphreys, Peter and Lillicrap, Timothy and Tweed, Douglas B},
     booktitle = {Advances in Neural Information Processing Systems},
     editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
     pages = {},
     publisher = {Curran Associates, Inc.},
     title = {Deep learning without weight transport},
     volume = {32},
     year = {2019}
}

@article{xiao2018biologicallyplausible,
      title={Biologically-plausible learning algorithms can scale to large datasets}, 
      author={Will Xiao and Honglin Chen and Qianli Liao and Tomaso Poggio},
      year={2018},
      journal={arXiv preprint},
}

@article{manchev2020target,
  author  = {Nikolay Manchev and Michael Spratling},
  title   = {Target Propagation in Recurrent Neural Networks},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {7},
  pages   = {1-33}
}

@ARTICLE{williams1989learning,
  author={Williams, Ronald J. and Zipser, David},
  journal={Neural Computation}, 
  title={A Learning Algorithm for Continually Running Fully Recurrent Neural Networks}, 
  year={1989},
  volume={1},
  number={2},
  pages={270-280},
  doi={10.1162/neco.1989.1.2.270}
  }



