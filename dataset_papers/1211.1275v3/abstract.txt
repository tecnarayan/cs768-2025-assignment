We extend kernelized matrix factorization with a fully Bayesian treatment and
with an ability to work with multiple side information sources expressed as
different kernels. Kernel functions have been introduced to matrix
factorization to integrate side information about the rows and columns (e.g.,
objects and users in recommender systems), which is necessary for making
out-of-matrix (i.e., cold start) predictions. We discuss specifically bipartite
graph inference, where the output matrix is binary, but extensions to more
general matrices are straightforward. We extend the state of the art in two key
aspects: (i) A fully conjugate probabilistic formulation of the kernelized
matrix factorization problem enables an efficient variational approximation,
whereas fully Bayesian treatments are not computationally feasible in the
earlier approaches. (ii) Multiple side information sources are included,
treated as different kernels in multiple kernel learning that additionally
reveals which side information sources are informative. Our method outperforms
alternatives in predicting drug-protein interactions on two data sets. We then
show that our framework can also be used for solving multilabel learning
problems by considering samples and labels as the two domains where matrix
factorization operates on. Our algorithm obtains the lowest Hamming loss values
on 10 out of 14 multilabel classification data sets compared to five
state-of-the-art multilabel learning algorithms.