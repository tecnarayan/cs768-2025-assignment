\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abernethy et~al.(2008)Abernethy, Hazan, and
  Rakhlin]{abernethy2009competing}
Jacob~D. Abernethy, Elad Hazan, and Alexander Rakhlin.
\newblock Competing in the dark: An efficient algorithm for bandit linear
  optimization.
\newblock In \emph{COLT}, 2008.

\bibitem[Ali et~al.(2019)Ali, Kolter, and Tibshirani]{ali2019continuous}
Alnur Ali, J~Zico Kolter, and Ryan~J Tibshirani.
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1370--1378, 2019.

\bibitem[Ali et~al.(2020)Ali, Dobriban, and Tibshirani]{ali2020implicit}
Alnur Ali, Edgar Dobriban, and Ryan~J Tibshirani.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock \emph{arXiv preprint arXiv:2003.07802}, 2020.

\bibitem[Arora et~al.(2019)Arora, Cohen, Hu, and Luo]{arora2019implicit}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  7411--7422, 2019.

\bibitem[Audibert(2008)]{audibert2008progressive}
Jean-Yves Audibert.
\newblock Progressive mixture rules are deviation suboptimal.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  41--48, 2008.

\bibitem[Azizan and Hassibi(2019)]{azizan2018stochastic}
Navid Azizan and Babak Hassibi.
\newblock Stochastic gradient/mirror descent: Minimax optimality and implicit
  regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Bansal and Gupta(2019)]{bansal2019potential}
Nikhil Bansal and Anupam Gupta.
\newblock Potential-function proofs for gradient methods.
\newblock \emph{Theory of Computing}, 15\penalty0 (1):\penalty0 1--32, 2019.

\bibitem[Bartlett and Mendelson(2002)]{bartlett2002rademacher}
Peter~L Bartlett and Shahar Mendelson.
\newblock Rademacher and gaussian complexities: Risk bounds and structural
  results.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Nov):\penalty0 463--482, 2002.

\bibitem[Bartlett and Mendelson(2006)]{bartlett2006empirical}
Peter~L Bartlett and Shahar Mendelson.
\newblock Empirical minimization.
\newblock \emph{Probability theory and related fields}, 135\penalty0
  (3):\penalty0 311--334, 2006.

\bibitem[Bartlett et~al.(2005)Bartlett, Bousquet, and
  Mendelson]{bartlett2005local}
Peter~L Bartlett, Olivier Bousquet, and Shahar Mendelson.
\newblock Local rademacher complexities.
\newblock \emph{The Annals of Statistics}, 33\penalty0 (4):\penalty0
  1497--1537, 2005.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzev, and
  Rosasco]{bauer2007regularization}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Beck and Teboulle(2003)]{beck2003mirror}
Amir Beck and Marc Teboulle.
\newblock Mirror descent and nonlinear projected subgradient methods for convex
  optimization.
\newblock \emph{Operations Research Letters}, 31\penalty0 (3):\penalty0
  167--175, 2003.

\bibitem[Bubeck(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Bubeck et~al.(2018)Bubeck, Cohen, Lee, Lee, and
  M{\k{a}}dry]{bubeck2018k}
S{\'e}bastien Bubeck, Michael~B Cohen, Yin~Tat Lee, James~R Lee, and Aleksander
  M{\k{a}}dry.
\newblock K-server via multiscale entropic regularization.
\newblock In \emph{Proceedings of the 50th annual ACM SIGACT symposium on
  theory of computing}, pages 3--16, 2018.

\bibitem[Bubeck et~al.(2019)Bubeck, Cohen, Lee, and Lee]{bubeck2019metrical}
S{\'e}bastien Bubeck, Michael~B Cohen, James~R Lee, and Yin~Tat Lee.
\newblock Metrical task systems on trees via mirror descent and unfair gluing.
\newblock In \emph{Proceedings of the Thirtieth Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 89--97. SIAM, 2019.

\bibitem[B{\"u}hlmann and Yu(2003)]{buhlmann2003boosting}
Peter B{\"u}hlmann and Bin Yu.
\newblock Boosting with the {$\ell_2$} loss: regression and classification.
\newblock \emph{Journal of the American Statistical Association}, 98\penalty0
  (462):\penalty0 324--339, 2003.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[Ghai et~al.(2019)Ghai, Hazan, and Singer]{ghai2019exponentiated}
Udaya Ghai, Elad Hazan, and Yoram Singer.
\newblock Exponentiated gradient meets gradient descent.
\newblock \emph{arXiv preprint arXiv:1902.01903}, 2019.

\bibitem[Gidel et~al.(2019)Gidel, Bach, and Lacoste-Julien]{gidel2019implicit}
Gauthier Gidel, Francis Bach, and Simon Lacoste-Julien.
\newblock Implicit regularization of discrete gradient dynamics in deep linear
  neural networks.
\newblock \emph{arXiv preprint arXiv:1904.13262}, 2019.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, volume~80 of \emph{Proceedings of Machine Learning Research},
  pages 1832--1841, Stockholmsm√§ssan, Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem[Hazan(2016)]{hazan2016introduction}
Elad Hazan.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends in Optimization}, 2\penalty0
  (3-4):\penalty0 157--325, 2016.

\bibitem[Kivinen and Warmuth(1997)]{kivinen1997exponentiated}
Jyrki Kivinen and Manfred~K Warmuth.
\newblock Exponentiated gradient versus gradient descent for linear predictors.
\newblock \emph{information and computation}, 132\penalty0 (1):\penalty0 1--63,
  1997.

\bibitem[Koltchinskii(2006)]{koltchinskii2006local}
Vladimir Koltchinskii.
\newblock Local rademacher complexities and oracle inequalities in risk
  minimization.
\newblock \emph{The Annals of Statistics}, 34\penalty0 (6):\penalty0
  2593--2656, 2006.

\bibitem[Lecu{\'e} et~al.(2012)Lecu{\'e}, Mitchell, et~al.]{lecue2012oracle}
Guillaume Lecu{\'e}, Charles Mitchell, et~al.
\newblock Oracle inequalities for cross-validation type procedures.
\newblock \emph{Electronic Journal of Statistics}, 6:\penalty0 1803--1837,
  2012.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pages 2--47, 2018.

\bibitem[Liang et~al.(2015)Liang, Rakhlin, and Sridharan]{liang2015learning}
Tengyuan Liang, Alexander Rakhlin, and Karthik Sridharan.
\newblock Learning with square loss: Localization through offset rademacher
  complexity.
\newblock In \emph{Conference on Learning Theory}, pages 1260--1285, 2015.

\bibitem[Lin and Cevher(2018)]{lin2018optimal}
Junhong Lin and Volkan Cevher.
\newblock Optimal distributed learning with multi-pass stochastic gradient
  methods.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, number CONF, 2018.

\bibitem[Lin et~al.(2016)Lin, Camoriano, and Rosasco]{lin2016generalization}
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Generalization properties and implicit regularization for multiple
  passes sgm.
\newblock In \emph{International Conference on Machine Learning}, pages
  2340--2348, 2016.

\bibitem[Lugosi and Mendelson(2016)]{lugosi2016risk}
Gabor Lugosi and Shahar Mendelson.
\newblock Risk minimization by median-of-means tournaments.
\newblock \emph{arXiv preprint arXiv:1608.00757}, 2016.

\bibitem[Matet et~al.(2017)Matet, Rosasco, Villa, and Vu]{matet2017don}
Simon Matet, Lorenzo Rosasco, Silvia Villa, and Bang~Long Vu.
\newblock Don't relax: early stopping for convex regularization.
\newblock \emph{arXiv preprint arXiv:1707.05422}, 2017.

\bibitem[Mendelson(2014)]{mendelson2014learning}
Shahar Mendelson.
\newblock Learning without concentration.
\newblock In \emph{Conference on Learning Theory}, pages 25--39, 2014.

\bibitem[Mendelson(2017)]{mendelson2017extending}
Shahar Mendelson.
\newblock Extending the small-ball method.
\newblock \emph{arXiv preprint arXiv:1709.00843}, 2017.

\bibitem[Nemirovsky and Yudin(1983)]{nemirovsky1983problem}
Arkadi{\u\i} Nemirovsky and David Yudin.
\newblock \emph{Problem complexity and method efficiency in optimization.}
\newblock Wiley, New York, 1983.

\bibitem[Neu and Rosasco(2018)]{neu2018iterate}
Gergely Neu and Lorenzo Rosasco.
\newblock Iterate averaging as regularization for stochastic gradient descent.
\newblock \emph{arXiv preprint arXiv:1802.08009}, 2018.

\bibitem[Pagliana and Rosasco(2019)]{pagliana2019implicit}
Nicol{\`o} Pagliana and Lorenzo Rosasco.
\newblock Implicit regularization of accelerated methods in hilbert spaces.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  14454--14464, 2019.

\bibitem[Rakhlin and Sridharan(2014)]{rakhlin2014online}
Alexander Rakhlin and Karthik Sridharan.
\newblock Online non-parametric regression.
\newblock In \emph{Conference on Learning Theory}, pages 1232--1264, 2014.

\bibitem[Raskutti et~al.(2011)Raskutti, Wainwright, and
  Yu]{raskutti2011minimax}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Minimax rates of estimation for high-dimensional linear regression
  over $l_q$ balls.
\newblock \emph{IEEE transactions on information theory}, 57\penalty0
  (10):\penalty0 6976--6994, 2011.

\bibitem[Raskutti et~al.(2014)Raskutti, Wainwright, and Yu]{raskutti2014early}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Early stopping and non-parametric regression: an optimal
  data-dependent stopping rule.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 335--366, 2014.

\bibitem[Richards and Rebeschini(2019)]{richards2019optimal}
Dominic Richards and Patrick Rebeschini.
\newblock Optimal statistical rates for decentralised non-parametric regression
  with linear speed-up.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1214--1225, 2019.

\bibitem[Richards and Rebeschini(2020)]{richards2020graph}
Dominic Richards and Patrick Rebeschini.
\newblock Graph-dependent implicit regularisation for distributed stochastic
  subgradient descent.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (34):\penalty0 1--44, 2020.

\bibitem[Rosasco and Villa(2015)]{rosasco2015learning}
Lorenzo Rosasco and Silvia Villa.
\newblock Learning with incremental iterative regularization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1630--1638, 2015.

\bibitem[Scholkopf and Smola(2001)]{scholkopf2001learning}
Bernhard Scholkopf and Alexander~J Smola.
\newblock \emph{Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2001.

\bibitem[Shamir(2015)]{shamir2015sample}
Ohad Shamir.
\newblock The sample complexity of learning linear predictors with the squared
  loss.
\newblock \emph{The Journal of Machine Learning Research}, 16\penalty0
  (1):\penalty0 3475--3486, 2015.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Arun Suggala, Adarsh Prasad, and Pradeep~K Ravikumar.
\newblock Connecting optimization and regularization paths.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10608--10619, 2018.

\bibitem[Va\v{s}kevi\v{c}ius et~al.(2019)Va\v{s}kevi\v{c}ius, Kanade, and
  Rebeschini]{vaskevicius2019implicit}
Tomas Va\v{s}kevi\v{c}ius, Varun Kanade, and Patrick Rebeschini.
\newblock Implicit regularization for optimal sparse recovery.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2968--2979, 2019.

\bibitem[Vershynin(2010)]{vershynin2010introduction}
Roman Vershynin.
\newblock Introduction to the non-asymptotic analysis of random matrices.
\newblock \emph{arXiv preprint arXiv:1011.3027}, 2010.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wegkamp et~al.(2003)]{wegkamp2003model}
Marten Wegkamp et~al.
\newblock Model selection in nonparametric regression.
\newblock \emph{The Annals of Statistics}, 31\penalty0 (1):\penalty0 252--273,
  2003.

\bibitem[Wei et~al.(2019)Wei, Yang, and Wainwright]{wei2019early}
Yuting Wei, Fanny Yang, and Martin~J Wainwright.
\newblock Early stopping for kernel boosting algorithms: A general analysis
  with localized complexities.
\newblock \emph{IEEE Transactions on Information Theory}, 65\penalty0
  (10):\penalty0 6685--6703, 2019.

\bibitem[Woodworth et~al.(2019)Woodworth, Gunasekar, Lee, Soudry, and
  Srebro]{woodworth2019kernel}
Blake Woodworth, Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Kernel and deep regimes in overparametrized models.
\newblock \emph{arXiv preprint arXiv:1906.05827}, 2019.

\bibitem[Yang et~al.(2019)Yang, Sun, and Roy]{yang2019fast}
Jun Yang, Shengyang Sun, and Daniel~M Roy.
\newblock Fast-rate pac-bayes generalization bounds via shifted rademacher
  processes.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10803--10813, 2019.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhao et~al.(2019)Zhao, Yang, and He]{zhao2019implicit}
Peng Zhao, Yun Yang, and Qiao-Chu He.
\newblock Implicit regularization via {H}adamard product over-parametrization
  in high-dimensional linear regression.
\newblock \emph{arXiv preprint arXiv:1903.09367}, 2019.

\bibitem[Zhivotovskiy and Hanneke(2018)]{zhivotovskiy2018localization}
Nikita Zhivotovskiy and Steve Hanneke.
\newblock Localization of vc classes: Beyond local rademacher complexities.
\newblock \emph{Theoretical Computer Science}, 742:\penalty0 27--49, 2018.

\end{thebibliography}
