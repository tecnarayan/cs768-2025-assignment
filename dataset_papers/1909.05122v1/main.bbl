\begin{thebibliography}{56}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adamczak et~al.(2011)Adamczak, Litvak, Pajor, and
  Tomczak-Jaegermann]{adamczak2011restricted}
Radoslaw Adamczak, Alexander~E Litvak, Alain Pajor, and Nicole
  Tomczak-Jaegermann.
\newblock Restricted isometry property of matrices with independent columns and
  neighborly polytopes by random sampling.
\newblock \emph{Constructive Approximation}, 34\penalty0 (1):\penalty0 61--88,
  2011.

\bibitem[Agarwal et~al.(2010)Agarwal, Negahban, and
  Wainwright]{agarwal2010fast}
Alekh Agarwal, Sahand Negahban, and Martin~J Wainwright.
\newblock Fast global convergence rates of gradient methods for
  high-dimensional statistical recovery.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  37--45, 2010.

\bibitem[Ali et~al.(2018)Ali, Kolter, and Tibshirani]{ali2018continuous}
Alnur Ali, J~Zico Kolter, and Ryan~J Tibshirani.
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock \emph{arXiv preprint arXiv:1810.10082}, 2018.

\bibitem[Bach et~al.(2012)Bach, Jenatton, Mairal, Obozinski,
  et~al.]{bach2012optimization}
Francis Bach, Rodolphe Jenatton, Julien Mairal, Guillaume Obozinski, et~al.
\newblock Optimization with sparsity-inducing penalties.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  4\penalty0 (1):\penalty0 1--106, 2012.

\bibitem[Bandeira et~al.(2013)Bandeira, Dobriban, Mixon, and
  Sawin]{bandeira2013certifying}
Afonso~S Bandeira, Edgar Dobriban, Dustin~G Mixon, and William~F Sawin.
\newblock Certifying the restricted isometry property is hard.
\newblock \emph{IEEE transactions on information theory}, 59\penalty0
  (6):\penalty0 3448--3450, 2013.

\bibitem[Baraniuk et~al.(2008)Baraniuk, Davenport, DeVore, and
  Wakin]{baraniuk2008simple}
Richard Baraniuk, Mark Davenport, Ronald DeVore, and Michael Wakin.
\newblock A simple proof of the restricted isometry property for random
  matrices.
\newblock \emph{Constructive Approximation}, 28\penalty0 (3):\penalty0
  253--263, 2008.

\bibitem[Bauer et~al.(2007)Bauer, Pereverzev, and
  Rosasco]{bauer2007regularization}
Frank Bauer, Sergei Pereverzev, and Lorenzo Rosasco.
\newblock On regularization algorithms in learning theory.
\newblock \emph{Journal of complexity}, 23\penalty0 (1):\penalty0 52--72, 2007.

\bibitem[Bertsimas et~al.(2016)Bertsimas, King, and
  Mazumder]{bertsimas2016best}
Dimitris Bertsimas, Angela King, and Rahul Mazumder.
\newblock Best subset selection via a modern optimization lens.
\newblock \emph{The Annals of Statistics}, 44\penalty0 (2):\penalty0 813--852,
  2016.

\bibitem[Bickel et~al.(2009)Bickel, Ritov, Tsybakov,
  et~al.]{bickel2009simultaneous}
Peter~J Bickel, Ya'acov Ritov, Alexandre~B Tsybakov, et~al.
\newblock Simultaneous analysis of {L}asso and {D}antzig selector.
\newblock \emph{The Annals of Statistics}, 37\penalty0 (4):\penalty0
  1705--1732, 2009.

\bibitem[Bredies and Lorenz(2008)]{Bredies2008}
Kristian Bredies and Dirk~A. Lorenz.
\newblock Linear convergence of iterative soft-thresholding.
\newblock \emph{Journal of Fourier Analysis and Applications}, 14\penalty0
  (5):\penalty0 813--837, Dec 2008.
\newblock ISSN 1531-5851.
\newblock \doi{10.1007/s00041-008-9041-1}.
\newblock URL \url{https://doi.org/10.1007/s00041-008-9041-1}.

\bibitem[B{\"u}hlmann and Van De~Geer(2011)]{buhlmann2011statistics}
Peter B{\"u}hlmann and Sara Van De~Geer.
\newblock \emph{Statistics for high-dimensional data: methods, theory and
  applications}.
\newblock Springer Science \& Business Media, 2011.

\bibitem[B{\"u}hlmann and Yu(2003)]{buhlmann2003boosting}
Peter B{\"u}hlmann and Bin Yu.
\newblock Boosting with the $\ell_2$ loss: Regression and classification.
\newblock \emph{Journal of the American Statistical Association}, 98\penalty0
  (462):\penalty0 324--339, 2003.

\bibitem[Candes and Tao(2007)]{candes2007dantzig}
Emmanuel Candes and Terence Tao.
\newblock The {D}antzig selector: Statistical estimation when p is much larger
  than n.
\newblock \emph{The Annals of Statistics}, 35\penalty0 (6):\penalty0
  2313--2351, 2007.

\bibitem[Candes and Tao(2005)]{candes2005decoding}
Emmanuel~J Candes and Terence Tao.
\newblock Decoding by linear programming.
\newblock \emph{IEEE Transactions on Information Theory}, 51\penalty0
  (12):\penalty0 4203--4215, 2005.

\bibitem[Chen et~al.(1998)Chen, Donoho, and Saunders]{chen1998atomic}
Scott~Shaobing Chen, David~L Donoho, and Michael~A Saunders.
\newblock Atomic decomposition by basis pursuit.
\newblock \emph{SIAM Journal on Scientific Computing}, 20\penalty0
  (1):\penalty0 33, 1998.

\bibitem[Cohen et~al.(2009)Cohen, Dahmen, and DeVore]{cohen2009compressed}
Albert Cohen, Wolfgang Dahmen, and Ronald DeVore.
\newblock Compressed sensing and best k-term approximation.
\newblock \emph{Journal of the American mathematical society}, 22\penalty0
  (1):\penalty0 211--231, 2009.

\bibitem[Donoho and Huo(2001)]{donoho2001uncertainty}
David~L Donoho and Xiaoming Huo.
\newblock Uncertainty principles and ideal atomic decomposition.
\newblock \emph{IEEE transactions on information theory}, 47\penalty0
  (7):\penalty0 2845--2862, 2001.

\bibitem[Efron et~al.(2004)Efron, Hastie, Johnstone, and
  Tibshirani]{efron2004least}
Bradley Efron, Trevor Hastie, Iain Johnstone, and Robert Tibshirani.
\newblock Least angle regression.
\newblock \emph{The Annals of Statistics}, 32\penalty0 (2):\penalty0 407--499,
  2004.

\bibitem[Feuer and Nemirovski(2003)]{feuer2003sparse}
Arie Feuer and Arkadi Nemirovski.
\newblock On sparse representation in pairs of bases.
\newblock \emph{IEEE Transactions on Information Theory}, 49\penalty0
  (6):\penalty0 1579--1581, 2003.

\bibitem[Friedman and Popescu(2004)]{friedman2004gradient}
Jerome Friedman and Bogdan~E Popescu.
\newblock Gradient directed regularization.
\newblock \emph{Technical report}, 2004.

\bibitem[Friedman et~al.(2001)Friedman, Hastie, and
  Tibshirani]{friedman2001elements}
Jerome Friedman, Trevor Hastie, and Robert Tibshirani.
\newblock \emph{The elements of statistical learning}, volume~1.
\newblock Springer series in statistics New York, 2001.

\bibitem[Friedman et~al.(2010)Friedman, Hastie, and
  Tibshirani]{friedman2010regularization}
Jerome Friedman, Trevor Hastie, and Rob Tibshirani.
\newblock Regularization paths for generalized linear models via coordinate
  descent.
\newblock \emph{Journal of Statistical Software}, 33\penalty0 (1):\penalty0 1,
  2010.

\bibitem[Gu{\'e}don et~al.(2007)Gu{\'e}don, Mendelson, Pajor, and
  Tomczak-Jaegermann]{guedon2007subspaces}
Olivier Gu{\'e}don, Shahar Mendelson, Alain Pajor, and Nicole
  Tomczak-Jaegermann.
\newblock Subspaces and orthogonal decompositions generated by bounded
  orthogonal systems.
\newblock \emph{Positivity}, 11\penalty0 (2):\penalty0 269--283, 2007.

\bibitem[Gu{\'e}don et~al.(2008)Gu{\'e}don, Mendelson, Pajor,
  Tomczak-Jaegermann, et~al.]{guedon2008majorizing}
Olivier Gu{\'e}don, Shahar Mendelson, Alain Pajor, Nicole Tomczak-Jaegermann,
  et~al.
\newblock Majorizing measures and proportional subsets of bounded orthonormal
  systems.
\newblock \emph{Revista matem{\'a}tica iberoamericana}, 24\penalty0
  (3):\penalty0 1075--1095, 2008.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
Suriya Gunasekar, Blake~E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur,
  and Nati Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018{\natexlab{a}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Suriya Gunasekar, Jason Lee, Daniel Soudry, and Nathan Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1827--1836, 2018{\natexlab{a}}.

\bibitem[Gunasekar et~al.(2018{\natexlab{b}})Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018implicit}
Suriya Gunasekar, Jason~D Lee, Daniel Soudry, and Nati Srebro.
\newblock Implicit bias of gradient descent on linear convolutional networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  9461--9471, 2018{\natexlab{b}}.

\bibitem[Hale et~al.(2008)Hale, Yin, and Zhang]{hale2008fixed}
Elaine~T Hale, Wotao Yin, and Yin Zhang.
\newblock Fixed-point continuation for $\ell\_1$-minimization: Methodology and
  convergence.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0
  1107--1130, 2008.

\bibitem[Hastie et~al.(2017)Hastie, Tibshirani, and
  Tibshirani]{hastie2017extended}
Trevor Hastie, Robert Tibshirani, and Ryan~J Tibshirani.
\newblock Extended comparisons of best subset selection, forward stepwise
  selection, and the lasso.
\newblock \emph{arXiv preprint arXiv:1707.08692}, 2017.

\bibitem[Li et~al.(2018)Li, Ma, and Zhang]{li2018algorithmic}
Yuanzhi Li, Tengyu Ma, and Hongyang Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock In \emph{Conference On Learning Theory}, pages 2--47, 2018.

\bibitem[Meinshausen and Yu(2009)]{meinshausen2009lasso}
Nicolai Meinshausen and Bin Yu.
\newblock Lasso-type recovery of sparse representations for high-dimensional
  data.
\newblock \emph{The Annals of Statistics}, 37\penalty0 (1):\penalty0 246--270,
  2009.

\bibitem[Mendelson et~al.(2008)Mendelson, Pajor, and
  Tomczak-Jaegermann]{mendelson2008uniform}
Shahar Mendelson, Alain Pajor, and Nicole Tomczak-Jaegermann.
\newblock Uniform uncertainty principle for bernoulli and subgaussian
  ensembles.
\newblock \emph{Constructive Approximation}, 28\penalty0 (3):\penalty0
  277--289, 2008.

\bibitem[Negahban et~al.(2012)Negahban, Ravikumar, Wainwright, Yu,
  et~al.]{negahban2012unified}
Sahand~N Negahban, Pradeep Ravikumar, Martin~J Wainwright, Bin Yu, et~al.
\newblock A unified framework for high-dimensional analysis of ${M}$-estimators
  with decomposable regularizers.
\newblock \emph{Statistical Science}, 27\penalty0 (4):\penalty0 538--557, 2012.

\bibitem[Neu and Rosasco(2018)]{neu2018iterate}
Gergely Neu and Lorenzo Rosasco.
\newblock Iterate averaging as regularization for stochastic gradient descent.
\newblock \emph{Proceedings of Machine Learning Research vol}, 75:\penalty0
  1--21, 2018.

\bibitem[Parikh et~al.(2014)Parikh, Boyd, et~al.]{parikh2014proximal}
Neal Parikh, Stephen Boyd, et~al.
\newblock Proximal algorithms.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization},
  1\penalty0 (3):\penalty0 127--239, 2014.

\bibitem[Raskutti et~al.(2010)Raskutti, Wainwright, and
  Yu]{raskutti2010restricted}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Restricted eigenvalue properties for correlated gaussian designs.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0
  (Aug):\penalty0 2241--2259, 2010.

\bibitem[Raskutti et~al.(2011)Raskutti, Wainwright, and
  Yu]{raskutti2011minimax}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Minimax rates of estimation for high-dimensional linear regression
  over $\ell_q$-balls.
\newblock \emph{IEEE transactions on information theory}, 57\penalty0
  (10):\penalty0 6976--6994, 2011.

\bibitem[Raskutti et~al.(2014)Raskutti, Wainwright, and Yu]{raskutti2014early}
Garvesh Raskutti, Martin~J Wainwright, and Bin Yu.
\newblock Early stopping and non-parametric regression: an optimal
  data-dependent stopping rule.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 335--366, 2014.

\bibitem[Romberg(2009)]{romberg2009compressive}
Justin Romberg.
\newblock Compressive sensing by random convolution.
\newblock \emph{SIAM Journal on Imaging Sciences}, 2\penalty0 (4):\penalty0
  1098--1128, 2009.

\bibitem[Rosset et~al.(2004)Rosset, Zhu, and Hastie]{rosset2004boosting}
Saharon Rosset, Ji~Zhu, and Trevor Hastie.
\newblock Boosting as a regularized path to a maximum margin classifier.
\newblock \emph{Journal of Machine Learning Research}, 5\penalty0
  (Aug):\penalty0 941--973, 2004.

\bibitem[Rudelson and Vershynin(2008)]{rudelson2008sparse}
Mark Rudelson and Roman Vershynin.
\newblock On sparse reconstruction from fourier and gaussian measurements.
\newblock \emph{Communications on Pure and Applied Mathematics: A Journal
  Issued by the Courant Institute of Mathematical Sciences}, 61\penalty0
  (8):\penalty0 1025--1045, 2008.

\bibitem[Rudelson and Zhou(2012)]{rudelson2012reconstruction}
Mark Rudelson and Shuheng Zhou.
\newblock Reconstruction from anisotropic random measurements.
\newblock In \emph{Conference on Learning Theory}, pages 10--1, 2012.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Daniel Soudry, Elad Hoffer, Mor~Shpigel Nacson, Suriya Gunasekar, and Nathan
  Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Arun Suggala, Adarsh Prasad, and Pradeep~K Ravikumar.
\newblock Connecting optimization and regularization paths.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  10608--10619, 2018.

\bibitem[Tao et~al.(2016)Tao, Boley, and Zhang]{tao2016local}
Shaozhe Tao, Daniel Boley, and Shuzhong Zhang.
\newblock Local linear convergence of {ISTA} and {FISTA} on the {LASSO}
  problem.
\newblock \emph{SIAM Journal on Optimization}, 26\penalty0 (1):\penalty0
  313--336, 2016.

\bibitem[Tibshirani(1996)]{tibshirani1996regression}
Robert Tibshirani.
\newblock Regression shrinkage and selection via the {L}asso.
\newblock \emph{Journal of the Royal Statistical Society: Series B
  (Methodological)}, 58\penalty0 (1):\penalty0 267--288, 1996.

\bibitem[Tibshirani et~al.(2015)Tibshirani, Wainwright, and
  Hastie]{tibshirani2015statistical}
Robert Tibshirani, Martin Wainwright, and Trevor Hastie.
\newblock \emph{Statistical learning with sparsity: the lasso and
  generalizations}.
\newblock Chapman and Hall/CRC, 2015.

\bibitem[van~de Geer(2007)]{van2007deterministic}
Sara van~de Geer.
\newblock The deterministic {L}asso.
\newblock \emph{Research Report}, 140, 2007.

\bibitem[Van De~Geer et~al.(2009)Van De~Geer, B{\"u}hlmann,
  et~al.]{van2009conditions}
Sara~A Van De~Geer, Peter B{\"u}hlmann, et~al.
\newblock On the conditions used to prove oracle results for the lasso.
\newblock \emph{Electronic Journal of Statistics}, 3:\penalty0 1360--1392,
  2009.

\bibitem[Wainwright(2019)]{wainwright2019high}
Martin~J Wainwright.
\newblock \emph{High-dimensional statistics: A non-asymptotic viewpoint},
  volume~48.
\newblock Cambridge University Press, 2019.

\bibitem[Wei et~al.(2017)Wei, Yang, and Wainwright]{wei2017early}
Yuting Wei, Fanny Yang, and Martin~J Wainwright.
\newblock Early stopping for kernel boosting algorithms: A general analysis
  with localized complexities.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6065--6075, 2017.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yuan Yao, Lorenzo Rosasco, and Andrea Caponnetto.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock \emph{arXiv preprint arXiv:1611.03530}, 2016.

\bibitem[Zhang and Yu(2005)]{zhang2005boosting}
Tong Zhang and Bin Yu.
\newblock Boosting with early stopping: Convergence and consistency.
\newblock \emph{The Annals of Statistics}, 33\penalty0 (4):\penalty0
  1538--1579, 2005.

\bibitem[Zhang et~al.(2014)Zhang, Wainwright, and Jordan]{zhang2014lower}
Yuchen Zhang, Martin~J Wainwright, and Michael~I Jordan.
\newblock Lower bounds on the performance of polynomial-time algorithms for
  sparse linear regression.
\newblock In \emph{Conference on Learning Theory}, pages 921--948, 2014.

\bibitem[Zhao et~al.(2019)Zhao, Yang, and He]{zhao2019implicit}
Peng Zhao, Yun Yang, and Qiao-Chu He.
\newblock Implicit regularization via hadamard product over-parametrization in
  high-dimensional linear regression.
\newblock \emph{arXiv preprint arXiv:1903.09367}, 2019.

\end{thebibliography}
