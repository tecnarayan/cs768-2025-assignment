\begin{thebibliography}{10}

\bibitem{bengio2006convex}
Yoshua Bengio, Nicolas~L Roux, Pascal Vincent, Olivier Delalleau, and Patrice
  Marcotte.
\newblock Convex neural networks.
\newblock In {\em Advances in neural information processing systems}, pages
  123--130, 2006.

\bibitem{buolamwini2018gender}
Joy Buolamwini and Timnit Gebru.
\newblock Gender shades: Intersectional accuracy disparities in commercial
  gender classification.
\newblock In {\em Conference on fairness, accountability and transparency},
  pages 77--91, 2018.

\bibitem{chaudhry2018efficient}
Arslan Chaudhry, Marc'Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny.
\newblock Efficient lifelong learning with a-gem.
\newblock {\em arXiv preprint arXiv:1812.00420}, 2018.

\bibitem{cheung2019superposition}
Brian Cheung, Alexander Terekhov, Yubei Chen, Pulkit Agrawal, and Bruno
  Olshausen.
\newblock Superposition of many models into one.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10867--10876, 2019.

\bibitem{imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em CVPR 2009}, 2009.

\bibitem{dettmers2019sparse}
Tim Dettmers and Luke Zettlemoyer.
\newblock Sparse networks from scratch: Faster training without losing
  performance.
\newblock {\em arXiv preprint arXiv:1907.04840}, 2019.

\bibitem{frank1956algorithm}
Marguerite Frank and Philip Wolfe.
\newblock An algorithm for quadratic programming.
\newblock {\em Naval research logistics quarterly}, 3(1-2):95--110, 1956.

\bibitem{frankle2018lottery}
Jonathan Frankle and Michael Carbin.
\newblock The lottery ticket hypothesis: Finding sparse, trainable neural
  networks.
\newblock {\em arXiv preprint arXiv:1803.03635}, 2018.

\bibitem{frankle2020training}
Jonathan Frankle, David~J Schwab, and Ari~S Morcos.
\newblock Training batchnorm and only batchnorm: On the expressive power of
  random features in cnns.
\newblock {\em arXiv preprint arXiv:2003.00152}, 2020.

\bibitem{french1999catastrophic}
Robert~M French.
\newblock Catastrophic forgetting in connectionist networks.
\newblock {\em Trends in cognitive sciences}, 3(4):128--135, 1999.

\bibitem{golkar2019continual}
Siavash Golkar, Michael Kagan, and Kyunghyun Cho.
\newblock Continual learning via neural pruning.
\newblock {\em arXiv preprint arXiv:1903.04476}, 2019.

\bibitem{goodfellow2013empirical}
Ian~J Goodfellow, Mehdi Mirza, Da~Xiao, Aaron Courville, and Yoshua Bengio.
\newblock An empirical investigation of catastrophic forgetting in
  gradient-based neural networks.
\newblock {\em arXiv preprint arXiv:1312.6211}, 2013.

\bibitem{grathwohl2019your}
Will Grathwohl, Kuan-Chieh Wang, J{\"o}rn-Henrik Jacobsen, David Duvenaud,
  Mohammad Norouzi, and Kevin Swersky.
\newblock Your classifier is secretly an energy based model and you should
  treat it like one.
\newblock {\em arXiv preprint arXiv:1912.03263}, 2019.

\bibitem{guo2017calibration}
Chuan Guo, Geoff Pleiss, Yu~Sun, and Kilian~Q Weinberger.
\newblock On calibration of modern neural networks.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 1321--1330. JMLR. org, 2017.

\bibitem{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock {\em arXiv preprint arXiv:1609.09106}, 2016.

\bibitem{he2019momentum}
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick.
\newblock Momentum contrast for unsupervised visual representation learning.
\newblock {\em arXiv preprint arXiv:1911.05722}, 2019.

\bibitem{he2015delving}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Delving deep into rectifiers: Surpassing human-level performance on
  imagenet classification.
\newblock In {\em Proceedings of the IEEE international conference on computer
  vision}, pages 1026--1034, 2015.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{hendrycks2016baseline}
Dan Hendrycks and Kevin Gimpel.
\newblock A baseline for detecting misclassified and out-of-distribution
  examples in neural networks.
\newblock {\em arXiv preprint arXiv:1610.02136}, 2016.

\bibitem{hopfield1982neural}
John~J Hopfield.
\newblock Neural networks and physical systems with emergent collective
  computational abilities.
\newblock {\em Proceedings of the national academy of sciences},
  79(8):2554--2558, 1982.

\bibitem{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock {\em arXiv preprint arXiv:1502.03167}, 2015.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock {\em arXiv preprint arXiv:1412.6980}, 2014.

\bibitem{kirkpatrick2017overcoming}
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume
  Desjardins, Andrei~A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka
  Grabska-Barwinska, et~al.
\newblock Overcoming catastrophic forgetting in neural networks.
\newblock {\em Proceedings of the national academy of sciences},
  114(13):3521--3526, 2017.

\bibitem{cifar}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, University of Toronto, 2009.

\bibitem{lecun1989backpropagation}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard,
  Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551, 1989.

\bibitem{mnist}
Yann LeCun, Corinna Cortes, and CJ~Burges.
\newblock Mnist handwritten digit database.
\newblock 2010.

\bibitem{lopez2017gradient}
David Lopez-Paz and Marc'Aurelio Ranzato.
\newblock Gradient episodic memory for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  6467--6476, 2017.

\bibitem{cosine}
Ilya Loshchilov and Frank Hutter.
\newblock Sgdr: Stochastic gradient descent with warm restarts, 2016.

\bibitem{malach2020proving}
Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, and Ohad Shamir.
\newblock Proving the lottery ticket hypothesis: Pruning is all you need.
\newblock {\em arXiv preprint arXiv:2002.00585}, 2020.

\bibitem{mallya2018piggyback}
Arun Mallya, Dillon Davis, and Svetlana Lazebnik.
\newblock Piggyback: Adapting a single network to multiple tasks by learning to
  mask weights.
\newblock In {\em Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 67--82, 2018.

\bibitem{mallya2018packnet}
Arun Mallya and Svetlana Lazebnik.
\newblock Packnet: Adding multiple tasks to a single network by iterative
  pruning.
\newblock In {\em Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 7765--7773, 2018.

\bibitem{masse2018alleviating}
Nicolas~Y Masse, Gregory~D Grant, and David~J Freedman.
\newblock Alleviating catastrophic forgetting using context-dependent gating
  and synaptic stabilization.
\newblock {\em Proceedings of the National Academy of Sciences},
  115(44):E10467--E10475, 2018.

\bibitem{mccloskey1989catastrophic}
Michael McCloskey and Neal~J Cohen.
\newblock Catastrophic interference in connectionist networks: The sequential
  learning problem.
\newblock In {\em Psychology of learning and motivation}, volume~24, pages
  109--165. Elsevier, 1989.

\bibitem{mitchell2019model}
Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman,
  Ben Hutchinson, Elena Spitzer, Inioluwa~Deborah Raji, and Timnit Gebru.
\newblock Model cards for model reporting.
\newblock In {\em Proceedings of the conference on fairness, accountability,
  and transparency}, pages 220--229, 2019.

\bibitem{mocanu2018scalable}
Decebal~Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong~H Nguyen,
  Madeleine Gibescu, and Antonio Liotta.
\newblock Scalable training of artificial neural networks with adaptive sparse
  connectivity inspired by network science.
\newblock {\em Nature communications}, 9(1):1--12, 2018.

\bibitem{pascanu2013revisiting}
Razvan Pascanu and Yoshua Bengio.
\newblock Revisiting natural gradient for deep networks.
\newblock {\em arXiv preprint arXiv:1301.3584}, 2013.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  8024--8035, 2019.

\bibitem{ramachandran2017searching}
Prajit Ramachandran, Barret Zoph, and Quoc~V Le.
\newblock Searching for activation functions.
\newblock {\em arXiv preprint arXiv:1710.05941}, 2017.

\bibitem{ramanujan2019s}
Vivek Ramanujan, Mitchell Wortsman, Aniruddha Kembhavi, Ali Farhadi, and
  Mohammad Rastegari.
\newblock What's hidden in a randomly weighted neural network?
\newblock {\em arXiv preprint arXiv:1911.13299}, 2019.

\bibitem{rebuffi2017icarl}
Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph~H
  Lampert.
\newblock icarl: Incremental classifier and representation learning.
\newblock In {\em Proceedings of the IEEE conference on Computer Vision and
  Pattern Recognition}, pages 2001--2010, 2017.

\bibitem{rolnick2019experience}
David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lillicrap, and Gregory
  Wayne.
\newblock Experience replay for continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  348--358, 2019.

\bibitem{rusu2016progressive}
Andrei~A Rusu, Neil~C Rabinowitz, Guillaume Desjardins, Hubert Soyer, James
  Kirkpatrick, Koray Kavukcuoglu, Razvan Pascanu, and Raia Hadsell.
\newblock Progressive neural networks.
\newblock {\em arXiv preprint arXiv:1606.04671}, 2016.

\bibitem{schrauwen2007overview}
Benjamin Schrauwen, David Verstraeten, and Jan Van~Campenhout.
\newblock An overview of reservoir computing: theory, applications and
  implementations.
\newblock In {\em Proceedings of the 15th european symposium on artificial
  neural networks. p. 471-482 2007}, pages 471--482, 2007.

\bibitem{schwartz2019green}
Roy Schwartz, Jesse Dodge, Noah~A Smith, and Oren Etzioni.
\newblock Green ai. corr abs/1907.10597 (2019).
\newblock {\em arXiv preprint arXiv:1907.10597}, 2019.

\bibitem{shin2017continual}
Hanul Shin, Jung~Kwon Lee, Jaehong Kim, and Jiwon Kim.
\newblock Continual learning with deep generative replay.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2990--2999, 2017.

\bibitem{storkey1997increasing}
Amos Storkey.
\newblock Increasing the capacity of a hopfield network without sacrificing
  functionality.
\newblock In {\em International Conference on Artificial Neural Networks},
  pages 451--456. Springer, 1997.

\bibitem{thrun1998lifelong}
Sebastian Thrun.
\newblock Lifelong learning algorithms.
\newblock In {\em Learning to learn}, pages 181--209. Springer, 1998.

\bibitem{tieleman2012lecture}
Tijmen Tieleman and Geoffrey Hinton.
\newblock Lecture 6.5-rmsprop: Divide the gradient by a running average of its
  recent magnitude.
\newblock {\em COURSERA: Neural networks for machine learning}, 4(2):26--31,
  2012.

\bibitem{van2019three}
Gido~M van~de Ven and Andreas~S Tolias.
\newblock Three scenarios for continual learning.
\newblock {\em arXiv preprint arXiv:1904.07734}, 2019.

\bibitem{Oswald2020Continual}
Johannes von Oswald, Christian Henning, João Sacramento, and Benjamin~F.
  Grewe.
\newblock Continual learning with hypernetworks.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{wen2020batchensemble}
Yeming Wen, Dustin Tran, and Jimmy Ba.
\newblock Batchensemble: an alternative approach to efficient ensemble and
  lifelong learning.
\newblock {\em arXiv preprint arXiv:2002.06715}, 2020.

\bibitem{xu2018reinforced}
Ju~Xu and Zhanxing Zhu.
\newblock Reinforced continual learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  899--908, 2018.

\bibitem{yoon2017lifelong}
Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung~Ju Hwang.
\newblock Lifelong learning with dynamically expandable networks.
\newblock {\em arXiv preprint arXiv:1708.01547}, 2017.

\bibitem{zenke2017continual}
Friedemann Zenke, Ben Poole, and Surya Ganguli.
\newblock Continual learning through synaptic intelligence.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3987--3995. JMLR. org, 2017.

\bibitem{zeno2018task}
Chen Zeno, Itay Golan, Elad Hoffer, and Daniel Soudry.
\newblock Task agnostic continual learning using online variational bayes.
\newblock {\em arXiv preprint arXiv:1803.10123}, 2018.

\bibitem{zhao1996incremental}
Jieyu Zhao and Jurgen Schmidhuber.
\newblock Incremental self-improvement for life-time multi-agent reinforcement
  learning.
\newblock In {\em From Animals to Animats 4: Proceedings of the Fourth
  International Conference on Simulation of Adaptive Behavior, Cambridge, MA},
  pages 516--525, 1996.

\bibitem{zhou2019deconstructing}
Hattie Zhou, Janice Lan, Rosanne Liu, and Jason Yosinski.
\newblock Deconstructing lottery tickets: Zeros, signs, and the supermask.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3592--3602, 2019.

\end{thebibliography}
