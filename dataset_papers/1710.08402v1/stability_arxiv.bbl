\begin{thebibliography}{10}

\bibitem{anitescu2000degenerate}
Mihai Anitescu.
\newblock Degenerate nonlinear programming with a quadratic growth condition.
\newblock {\em SIAM Journal on Optimization}, 10(4):1116--1135, 2000.

\bibitem{bonnans1995second}
Joseph~Fr{\'e}d{\'e}ric Bonnans and Alexander Ioffe.
\newblock Second-order sufficiency and quadratic growth for nonisolated minima.
\newblock {\em Mathematics of Operations Research}, 20(4):801--817, 1995.

\bibitem{bousquet1}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock {\em J. Mach. Learn. Res.}, 2:499--526, March 2002.

\bibitem{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem{devroye}
L.~Devroye and T.~Wagner.
\newblock Distribution-free performance bounds for potential function rules.
\newblock {\em IEEE Transactions on Information Theory}, 25(5):601--604, Sep
  1979.

\bibitem{dwork2006}
Cynthia Dwork.
\newblock Differential privacy.
\newblock In {\em 33rd International Colloquium on Automata, Languages and
  Programming, part II (ICALP 2006)}, volume 4052, pages 1--12, Venice, Italy,
  July 2006. Springer Verlag.

\bibitem{dwork2015preserving}
Cynthia Dwork, Vitaly Feldman, Moritz Hardt, Toniann Pitassi, Omer Reingold,
  and Aaron~Leon Roth.
\newblock Preserving statistical validity in adaptive data analysis.
\newblock In {\em Proceedings of the Forty-Seventh Annual ACM on Symposium on
  Theory of Computing}, pages 117--126. ACM, 2015.

\bibitem{elisseeff}
Andre Elisseeff, Theodoros Evgeniou, and Massimiliano Pontil.
\newblock Stability of randomized learning algorithms.
\newblock {\em J. Mach. Learn. Res.}, 6:55--79, December 2005.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{gonen2017fast}
Alon Gonen and Shai Shalev-Shwartz.
\newblock Fast rates for empirical risk minimization of strict saddle problems.
\newblock {\em arXiv preprint, arXiv:1701.04271}, 2017.

\bibitem{hardt1}
Moritz Hardt and Tengyu Ma.
\newblock Identity matters in deep learning.
\newblock {\em CoRR}, abs/1611.04231, 2016.

\bibitem{hardt2}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In {\em Proceedings of the 33nd International Conference on Machine
  Learning, {ICML} 2016, New York City, NY, USA, June 19-24, 2016}, pages
  1225--1234, 2016.

\bibitem{ioffe1994sensitivity}
Alexander Ioffe.
\newblock On sensitivity analysis of nonlinear programs in banach spaces: the
  approach via composite unconstrained optimization.
\newblock {\em SIAM Journal on Optimization}, 4(1):1--43, 1994.

\bibitem{johnson2013accelerating}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  315--323, 2013.

\bibitem{karimi}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock {\em Linear Convergence of Gradient and Proximal-Gradient Methods
  Under the Polyak-{\L}ojasiewicz Condition}, pages 795--811.
\newblock Springer International Publishing, Cham, 2016.

\bibitem{kawaguchi2016deep}
Kenji Kawaguchi.
\newblock Deep learning without poor local minima.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  586--594, 2016.

\bibitem{keskar2016large}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock {\em arXiv preprint, arXiv:1609.04836}, 2016.

\bibitem{kuzborskij2017data}
Ilja Kuzborskij and Christoph Lampert.
\newblock Data-dependent stability of stochastic gradient descent.
\newblock {\em arXiv preprint, arXiv:1703.01678}, 2017.

\bibitem{lin2016does}
Henry~W Lin and Max Tegmark.
\newblock Why does deep and cheap learning work so well?
\newblock {\em arXiv preprint, arXiv:1608.08225}, 2016.

\bibitem{lin2016generalization}
Junhong Lin, Raffaello Camoriano, and Lorenzo Rosasco.
\newblock Generalization properties and implicit regularization for multiple
  passes sgm.
\newblock In {\em International Conference on Machine Learning}, 2016.

\bibitem{liu2017algorithmic}
Tongliang Liu, G{\'a}bor Lugosi, Gergely Neu, and Dacheng Tao.
\newblock Algorithmic stability and hypothesis complexity.
\newblock {\em arXiv preprint, arXiv:1702.08712}, 2017.

\bibitem{lojasiewicz1963topological}
S~Lojasiewicz.
\newblock A topological property of real analytic subsets.
\newblock {\em Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es
  partielles}, 117:87--89, 1963.

\bibitem{mukherjee2006learning}
Sayan Mukherjee, Partha Niyogi, Tomaso Poggio, and Ryan Rifkin.
\newblock Learning theory: stability is sufficient for generalization and
  necessary and sufficient for consistency of empirical risk minimization.
\newblock {\em Advances in Computational Mathematics}, 25(1):161--193, 2006.

\bibitem{nesterov2012efficiency}
Yu~Nesterov.
\newblock Efficiency of coordinate descent methods on huge-scale optimization
  problems.
\newblock {\em SIAM Journal on Optimization}, 22(2):341--362, 2012.

\bibitem{nesterov2013introductory}
Yurii Nesterov.
\newblock {\em Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem{nissim2015generalization}
Kobbi Nissim and Uri Stemmer.
\newblock On the generalization properties of differential privacy.
\newblock {\em CoRR, abs/1504.05800}, 2015.

\bibitem{shalev}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Learnability, stability and uniform convergence.
\newblock {\em Journal of Machine Learning Research}, 11(Oct):2635--2670, 2010.

\bibitem{xie2017diverse}
Bo~Xie, Yingyu Liang, and Le~Song.
\newblock Diverse neural network learns true target functions.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1216--1224,
  2017.

\bibitem{zhang2016understanding}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning requires rethinking generalization.
\newblock {\em arXiv preprint, arXiv:1611.03530}, 2016.

\bibitem{zhou2017landscape}
Pan Zhou and Jiashi Feng.
\newblock The landscape of deep learning algorithms.
\newblock {\em arXiv preprint, arXiv:1705.07038}, 2017.

\end{thebibliography}
