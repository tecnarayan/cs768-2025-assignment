\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alon et~al.(1997)Alon, Ben-David, Cesa-Bianchi, and Haussler]{alon1997scale}
N.~Alon, S.~Ben-David, N.~Cesa-Bianchi, and D.~Haussler.
\newblock Scale-sensitive dimensions, uniform convergence, and learnability.
\newblock \emph{Journal of the ACM (JACM)}, 44\penalty0 (4):\penalty0 615--631, 1997.

\bibitem[Amir et~al.(2021{\natexlab{a}})Amir, Carmon, Koren, and Livni]{amir2021never}
I.~Amir, Y.~Carmon, T.~Koren, and R.~Livni.
\newblock Never go full batch (in stochastic convex optimization).
\newblock \emph{Advances in Neural Information Processing Systems}, 34:\penalty0 25033--25043, 2021{\natexlab{a}}.

\bibitem[Amir et~al.(2021{\natexlab{b}})Amir, Koren, and Livni]{amir2021sgd}
I.~Amir, T.~Koren, and R.~Livni.
\newblock Sgd generalizes better than gd (and regularization doesnâ€™t help).
\newblock In \emph{Conference on Learning Theory}, pages 63--92. PMLR, 2021{\natexlab{b}}.

\bibitem[Amir et~al.(2022)Amir, Livni, and Srebro]{amir2022thinking}
I.~Amir, R.~Livni, and N.~Srebro.
\newblock Thinking outside the ball: Optimal learning with gradient descent for generalized linear stochastic convex optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 23539--23550, 2022.

\bibitem[Attias et~al.(2024)Attias, Dziugaite, Haghifam, Livni, and Roy]{attias2024information}
I.~Attias, G.~K. Dziugaite, M.~Haghifam, R.~Livni, and D.~M. Roy.
\newblock Information complexity of stochastic convex optimization: Applications to generalization and memorization.
\newblock \emph{arXiv preprint arXiv:2402.09327}, 2024.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and Talwar]{bassily2020stability}
R.~Bassily, V.~Feldman, C.~Guzm{\'a}n, and K.~Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 4381--4391, 2020.

\bibitem[Blumer et~al.(1989)Blumer, Ehrenfeucht, Haussler, and Warmuth]{blumer1989learnability}
A.~Blumer, A.~Ehrenfeucht, D.~Haussler, and M.~K. Warmuth.
\newblock Learnability and the vapnik-chervonenkis dimension.
\newblock \emph{Journal of the ACM (JACM)}, 36\penalty0 (4):\penalty0 929--965, 1989.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
O.~Bousquet and A.~Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0 499--526, 2002.

\bibitem[Bubeck et~al.(2019)Bubeck, Jiang, Lee, Li, and Sidford]{bubeck2019complexity}
S.~Bubeck, Q.~Jiang, Y.-T. Lee, Y.~Li, and A.~Sidford.
\newblock Complexity of highly parallel non-smooth convex optimization.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Bubeck et~al.(2015)]{bubeck2015convex}
S.~Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning}, 8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Carmon et~al.(2023)Carmon, Livni, and Yehudayoff]{carmon2023sample}
D.~Carmon, R.~Livni, and A.~Yehudayoff.
\newblock The sample complexity of erms in stochastic convex optimization.
\newblock \emph{arXiv preprint arXiv:2311.05398}, 2023.

\bibitem[Feldman(2016)]{feldman2016generalization}
V.~Feldman.
\newblock Generalization of erm in stochastic convex optimization: The dimension strikes back.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient descent.
\newblock In \emph{International conference on machine learning}, pages 1225--1234. PMLR, 2016.

\bibitem[Haussler and Warmuth(2018)]{haussler2018probably}
D.~Haussler and M.~Warmuth.
\newblock The probably approximately correct (pac) and other learning models.
\newblock \emph{The Mathematics of Generalization}, pages 17--36, 2018.

\bibitem[Hazan et~al.(2016)]{hazan2016introduction}
E.~Hazan et~al.
\newblock Introduction to online convex optimization.
\newblock \emph{Foundations and Trends{\textregistered} in Optimization}, 2\penalty0 (3-4):\penalty0 157--325, 2016.

\bibitem[Livni(2023)]{livni2024information}
R.~Livni.
\newblock Information theoretic lower bounds for information theoretic upper bounds.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Nesterov(2013)]{nesterov2013introductory}
Y.~Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course}, volume~87.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and Srebro]{neyshabur2014search}
B.~Neyshabur, R.~Tomioka, and N.~Srebro.
\newblock In search of the real inductive bias: On the role of implicit regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Rockafellar(2015)]{rockafellar2015convex}
R.~T. Rockafellar.
\newblock \emph{Convex Analysis:(PMS-28)}.
\newblock Princeton university press, 2015.

\bibitem[Schliserman et~al.(2024)Schliserman, Sherman, and Koren]{schliserman2024dimension}
M.~Schliserman, U.~Sherman, and T.~Koren.
\newblock The dimension strikes back with gradients: Generalization of gradient methods in stochastic convex optimization.
\newblock \emph{arXiv preprint arXiv:2401.12058}, 2024.

\bibitem[Sekhari et~al.(2021)Sekhari, Sridharan, and Kale]{sekhari2021sgd}
A.~Sekhari, K.~Sridharan, and S.~Kale.
\newblock Sgd: The role of implicit regularization, batch-size and multiple-epochs.
\newblock \emph{Advances In Neural Information Processing Systems}, 34:\penalty0 27422--27433, 2021.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and Sridharan]{shalev2009stochastic}
S.~Shalev-Shwartz, O.~Shamir, N.~Srebro, and K.~Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{COLT}, volume~2, page~5, 2009.

\bibitem[Taylor et~al.(2017)Taylor, Hendrickx, and Glineur]{taylor2017smooth}
A.~B. Taylor, J.~M. Hendrickx, and F.~Glineur.
\newblock Smooth strongly convex interpolation and exact worst-case performance of first-order methods.
\newblock \emph{Mathematical Programming}, 161:\penalty0 307--345, 2017.

\bibitem[Vapnik and Chervonenkis(2015)]{vapnik2015uniform}
V.~N. Vapnik and A.~Y. Chervonenkis.
\newblock On the uniform convergence of relative frequencies of events to their probabilities.
\newblock In \emph{Measures of complexity: festschrift for alexey chervonenkis}, pages 11--30. Springer, 2015.

\bibitem[Zhang et~al.(2021)Zhang, Bengio, Hardt, Recht, and Vinyals]{zhang2021understanding}
C.~Zhang, S.~Bengio, M.~Hardt, B.~Recht, and O.~Vinyals.
\newblock Understanding deep learning (still) requires rethinking generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115, 2021.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
M.~Zinkevich.
\newblock Online convex programming and generalized infinitesimal gradient ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine learning (icml-03)}, pages 928--936, 2003.

\end{thebibliography}
