% Generated by IEEEtranN.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{71}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtranN.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem[Torralba et~al.(2011)Torralba, Efros, and
  {others}]{torralba_unbiased_2011}
A.~Torralba, A.~A. Efros, and {others}, ``Unbiased look at dataset bias.'' in
  \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, 2011.

\bibitem[Biggio et~al.(2013)Biggio, Corona, Maiorca, Nelson, {\v{S}}rndi{\'c},
  Laskov, Giacinto, and Roli]{biggio2013evasion}
B.~Biggio, I.~Corona, D.~Maiorca, B.~Nelson, N.~{\v{S}}rndi{\'c}, P.~Laskov,
  G.~Giacinto, and F.~Roli, ``Evasion attacks against machine learning at test
  time,'' in \emph{Joint European conference on machine learning and knowledge
  discovery in databases}.\hskip 1em plus 0.5em minus 0.4em\relax Springer,
  2013, pp. 387--402.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos_shortcut_2020}
R.~Geirhos, J.-H. Jacobsen, C.~Michaelis, R.~Zemel, W.~Brendel, M.~Bethge, and
  F.~A. Wichmann, ``Shortcut learning in deep neural networks,'' in
  \emph{Nature {Machine} {Intelligence}}, 2020.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky_invariant_2019}
M.~Arjovsky, L.~Bottou, I.~Gulrajani, and D.~Lopez-Paz, ``Invariant risk
  minimization,'' \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Geirhos et~al.(2019{\natexlab{a}})Geirhos, Rubisch, Michaelis, Bethge,
  Wichmann, and Brendel]{geirhos_imagenet_2019}
\BIBentryALTinterwordspacing
R.~Geirhos, P.~Rubisch, C.~Michaelis, M.~Bethge, F.~A. Wichmann, and
  W.~Brendel, ``{ImageNet}-trained {CNNs} are biased towards texture;
  increasing shape bias improves accuracy and robustness,'' in
  \emph{International {Conference} on {Learning} {Representations}}, 2019.
  [Online]. Available: \url{https://openreview.net/forum?id=Bygh9j09KX}
\BIBentrySTDinterwordspacing

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Zhao, Basart,
  Steinhardt, and Song]{hendrycks_2021_natural}
D.~Hendrycks, K.~Zhao, S.~Basart, J.~Steinhardt, and D.~Song, ``Natural
  adversarial examples,'' \emph{CVPR}, 2021.

\bibitem[Recht et~al.(2019{\natexlab{a}})Recht, Roelofs, Schmidt, and
  Shankar]{recht_imagenet_2019}
B.~Recht, R.~Roelofs, L.~Schmidt, and V.~Shankar, ``Do {ImageNet} {Classifiers}
  {Generalize} to {ImageNet}?'' \emph{arXiv preprint arXiv:1902.10811}, 2019.

\bibitem[Xiao et~al.(2020)Xiao, Engstrom, Ilyas, and Madry]{xiao_noise_2020}
K.~Xiao, L.~Engstrom, A.~Ilyas, and A.~Madry, ``Noise or {Signal}: {The} {Role}
  of {Image} {Backgrounds} in {Object} {Recognition},'' \emph{arXiv preprint
  arXiv:2006.09994}, 2020.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, Song, Steinhardt, and
  Gilmer]{hendrycks_2021_many}
D.~Hendrycks, S.~Basart, N.~Mu, S.~Kadavath, F.~Wang, E.~Dorundo, R.~Desai,
  T.~Zhu, S.~Parajuli, M.~Guo, D.~Song, J.~Steinhardt, and J.~Gilmer, ``The
  many faces of robustness: A critical analysis of out-of-distribution
  generalization,'' \emph{ICCV}, 2021.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks_robustness_2019}
D.~Hendrycks and T.~Dietterich, ``Benchmarking neural network robustness to
  common corruptions and perturbations,'' \emph{ICLR}, 2019.

\bibitem[Mintun et~al.(2021)Mintun, Kirillov, and Xie]{mintun_interaction_2021}
E.~Mintun, A.~Kirillov, and S.~Xie, ``On {Interaction} {Between}
  {Augmentations} and {Corruptions} in {Natural} {Corruption} {Robustness},''
  in \emph{Advances in Neural Information Processing Systems}, M.~Ranzato,
  A.~Beygelzimer, Y.~Dauphin, P.~Liang, and J.~W. Vaughan, Eds., vol.~34.\hskip
  1em plus 0.5em minus 0.4em\relax Curran Associates, Inc., 2021, pp.
  3571--3583.

\bibitem[Szegedy et~al.(2014)Szegedy, Zaremba, Sutskever, Bruna, Erhan,
  Goodfellow, and Fergus]{szegedy_intriguing_2013}
C.~Szegedy, W.~Zaremba, I.~Sutskever, J.~Bruna, D.~Erhan, I.~Goodfellow, and
  R.~Fergus, ``Intriguing properties of neural networks,'' \emph{International
  Conference on Learning Representations}, 2014.

\bibitem[Carlini et~al.(2019)Carlini, Athalye, Papernot, Brendel, Rauber,
  Tsipras, Goodfellow, Madry, and Kurakin]{carlini2019evaluating}
N.~Carlini, A.~Athalye, N.~Papernot, W.~Brendel, J.~Rauber, D.~Tsipras,
  I.~Goodfellow, A.~Madry, and A.~Kurakin, ``On evaluating adversarial
  robustness,'' \emph{arXiv preprint arXiv:1902.06705}, 2019.

\bibitem[Mahmood et~al.(2021)Mahmood, Mahmood, and van
  Dijk]{mahmood_2021_robustness}
K.~Mahmood, R.~Mahmood, and M.~van Dijk, ``On the robustness of vision
  transformers to adversarial examples,'' in \emph{Proceedings of the IEEE/CVF
  International Conference on Computer Vision (ICCV)}, October 2021, pp.
  7838--7847.

\bibitem[Goodfellow et~al.(2016)Goodfellow, Bengio, and
  Courville]{goodfellow_deep_2016}
I.~Goodfellow, Y.~Bengio, and A.~Courville, \emph{Deep {Learning}}.\hskip 1em
  plus 0.5em minus 0.4em\relax MIT Press, 2016.

\bibitem[Apruzzese et~al.(2023)Apruzzese, Anderson, Dambra, Freeman, Pierazzi,
  and Roundy]{apruzzese2023real}
G.~Apruzzese, H.~S. Anderson, S.~Dambra, D.~Freeman, F.~Pierazzi, and
  K.~Roundy, ``“real attackers don't compute gradients”: Bridging the gap
  between adversarial ml research and practice,'' in \emph{2023 IEEE Conference
  on Secure and Trustworthy Machine Learning (SaTML)}.\hskip 1em plus 0.5em
  minus 0.4em\relax IEEE, 2023, pp. 339--364.

\bibitem[Yuan et~al.(2019)Yuan, Tang, Liao, Wang, Feng, Chen, Sun, Lu, and
  Zhang]{yuan_2019_stealthy}
K.~Yuan, D.~Tang, X.~Liao, X.~Wang, X.~Feng, Y.~Chen, M.~Sun, H.~Lu, and
  K.~Zhang, ``Stealthy porn: Understanding real-world adversarial images for
  illicit online promotion,'' in \emph{2019 IEEE Symposium on Security and
  Privacy (SP)}, 2019, pp. 952--966.

\bibitem[Cimpoi et~al.(2013)Cimpoi, Maji, Kokkinos, Mohamed, and
  Vedaldi]{cimpoi_describing_2013}
M.~Cimpoi, S.~Maji, I.~Kokkinos, S.~Mohamed, and A.~Vedaldi, ``Describing
  textures in the wild,'' \emph{arXiv preprint arXiv:1311.3618}, 2013.

\bibitem[Hendrycks and Dietterich(2018)]{hendrycks_benchmarking_2018}
D.~Hendrycks and T.~Dietterich, ``Benchmarking {Neural} {Network} {Robustness}
  to {Common} {Corruptions} and {Perturbations},'' in \emph{International
  {Conference} on {Learning} {Representations}}, 2018.

\bibitem[Kar et~al.(2022)Kar, Yeo, Atanov, and Zamir]{kar_3d_2022}
O.~F. Kar, T.~Yeo, A.~Atanov, and A.~Zamir, ``3d common corruptions and data
  augmentation,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR)}, June 2022, pp. 18\,963--18\,974.

\bibitem[Sagawa et~al.(2020)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2020distributionally}
S.~Sagawa, P.~W. Koh, T.~B. Hashimoto, and P.~Liang, ``Distributionally robust
  neural networks for group shifts: On the importance of regularization for
  worst-case generalization,'' \emph{arXiv preprint arXiv:1911.08731}, 2020.

\bibitem[Goodfellow et~al.(2015)Goodfellow, Shlens, and
  Szegedy]{goodfellow_explaining_2014}
I.~J. Goodfellow, J.~Shlens, and C.~Szegedy, ``Explaining and harnessing
  adversarial examples,'' \emph{International Conference on Learning
  Representations}, 2015.

\bibitem[Papernot et~al.(2016)Papernot, McDaniel, Jha, Fredrikson, Celik, and
  Swami]{papernot2016limitations}
N.~Papernot, P.~McDaniel, S.~Jha, M.~Fredrikson, Z.~B. Celik, and A.~Swami,
  ``The limitations of deep learning in adversarial settings,'' in \emph{2016
  IEEE European symposium on security and privacy (EuroS\&P)}.\hskip 1em plus
  0.5em minus 0.4em\relax IEEE, 2016, pp. 372--387.

\bibitem[Carlini and Wagner(2017)]{carlini2017towards}
N.~Carlini and D.~Wagner, ``Towards evaluating the robustness of neural
  networks,'' in \emph{2017 ieee symposium on security and privacy (sp)}.\hskip
  1em plus 0.5em minus 0.4em\relax Ieee, 2017, pp. 39--57.

\bibitem[Moosavi-Dezfooli et~al.(2016)Moosavi-Dezfooli, Fawzi, and
  Frossard]{moosavi2016deepfool}
S.-M. Moosavi-Dezfooli, A.~Fawzi, and P.~Frossard, ``Deepfool: a simple and
  accurate method to fool deep neural networks,'' in \emph{Proceedings of the
  IEEE conference on computer vision and pattern recognition}, 2016, pp.
  2574--2582.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
A.~Madry, A.~Makelov, L.~Schmidt, D.~Tsipras, and A.~Vladu, ``Towards deep
  learning models resistant to adversarial attacks,'' in \emph{ICLR}, 2018.

\bibitem[Croce and Hein(2020)]{croce_reliable_2020}
F.~Croce and M.~Hein, ``Reliable evaluation of adversarial robustness with an
  ensemble of diverse parameter-free attacks,'' \emph{arXiv preprint
  arXiv:2003.01690}, 2020.

\bibitem[Liu et~al.(2022{\natexlab{a}})Liu, Cheng, Gao, Liu, Zhang, and
  Song]{Liu_2022_CVPR}
Y.~Liu, Y.~Cheng, L.~Gao, X.~Liu, Q.~Zhang, and J.~Song, ``Practical evaluation
  of adversarial robustness via adaptive auto attack,'' in \emph{Proceedings of
  the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  June 2022, pp. 15\,105--15\,114.

\bibitem[Papernot et~al.(2017)Papernot, McDaniel, Goodfellow, Jha, Celik, and
  Swami]{papernot2017practical}
N.~Papernot, P.~McDaniel, I.~Goodfellow, S.~Jha, Z.~B. Celik, and A.~Swami,
  ``Practical black-box attacks against machine learning,'' in
  \emph{Proceedings of the 2017 ACM on Asia conference on computer and
  communications security}, 2017, pp. 506--519.

\bibitem[Ru et~al.(2019)Ru, Cobb, Blaas, and Gal]{ru2019bayesopt}
B.~Ru, A.~Cobb, A.~Blaas, and Y.~Gal, ``Bayesopt adversarial attack,'' in
  \emph{ICLR}, 2019.

\bibitem[Chen et~al.(2020)Chen, Jordan, and
  Wainwright]{chen2020hopskipjumpattack}
J.~Chen, M.~I. Jordan, and M.~J. Wainwright, ``Hopskipjumpattack: A
  query-efficient decision-based attack,'' in \emph{2020 ieee symposium on
  security and privacy (sp)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE,
  2020, pp. 1277--1294.

\bibitem[Athalye et~al.(2018)Athalye, Engstrom, Ilyas, and
  Kwok]{athalye2018synthesizing}
A.~Athalye, L.~Engstrom, A.~Ilyas, and K.~Kwok, ``Synthesizing robust
  adversarial examples,'' in \emph{International conference on machine
  learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 284--293.

\bibitem[Engstrom et~al.(2019)Engstrom, Tran, Tsipras, Schmidt, and
  Madry]{engstrom2019exploring}
L.~Engstrom, B.~Tran, D.~Tsipras, L.~Schmidt, and A.~Madry, ``Exploring the
  landscape of spatial robustness,'' in \emph{International conference on
  machine learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2019, pp.
  1802--1811.

\bibitem[Karmon et~al.(2018)Karmon, Zoran, and Goldberg]{karmon2018lavan}
D.~Karmon, D.~Zoran, and Y.~Goldberg, ``Lavan: Localized and visible
  adversarial noise,'' in \emph{International Conference on Machine
  Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 2507--2515.

\bibitem[Brown et~al.(2017)Brown, Man{\'e}, Roy, Abadi, and
  Gilmer]{brown2017adversarial}
T.~B. Brown, D.~Man{\'e}, A.~Roy, M.~Abadi, and J.~Gilmer, ``Adversarial
  patch,'' \emph{arXiv preprint arXiv:1712.09665}, 2017.

\bibitem[Xiang et~al.(2022)Xiang, Mahloujifar, and
  Mittal]{xiang2022patchcleanser}
C.~Xiang, S.~Mahloujifar, and P.~Mittal, ``$\{$PatchCleanser$\}$: Certifiably
  robust defense against adversarial patches for any image classifier,'' in
  \emph{31st USENIX Security Symposium (USENIX Security 22)}, 2022, pp.
  2065--2082.

\bibitem[Salman et~al.(2022)Salman, Jain, Wong, and Madry]{salman2022certified}
H.~Salman, S.~Jain, E.~Wong, and A.~Madry, ``Certified patch robustness via
  smoothed vision transformers,'' in \emph{Proceedings of the IEEE/CVF
  Conference on Computer Vision and Pattern Recognition}, 2022, pp.
  15\,137--15\,147.

\bibitem[Pintor et~al.(2022)Pintor, Angioni, Sotgiu, Demetrio, Demontis,
  Biggio, and Roli]{pintor_imagenetpatch_2022}
\BIBentryALTinterwordspacing
M.~Pintor, D.~Angioni, A.~Sotgiu, L.~Demetrio, A.~Demontis, B.~Biggio, and
  F.~Roli, ``Imagenet-patch: A dataset for benchmarking machine learning
  robustness against adversaria patches,'' 2022. [Online]. Available:
  \url{https://arxiv.org/abs/2203.04412}
\BIBentrySTDinterwordspacing

\bibitem[Russakovsky et~al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma,
  Huang, Karpathy, Khosla, Bernstein, Berg, and
  Fei-Fei]{russakovsky_2015_imagenet}
O.~Russakovsky, J.~Deng, H.~Su, J.~Krause, S.~Satheesh, S.~Ma, Z.~Huang,
  A.~Karpathy, A.~Khosla, M.~Bernstein, A.~C. Berg, and L.~Fei-Fei, ``{ImageNet
  Large Scale Visual Recognition Challenge},'' \emph{International Journal of
  Computer Vision (IJCV)}, vol. 115, no.~3, pp. 211--252, 2015.

\bibitem[Geirhos et~al.(2018)Geirhos, Temme, Rauber, Sch{\"{u}}tt, Bethge, and
  Wichmann]{geirhos_generalisation_2018}
\BIBentryALTinterwordspacing
R.~Geirhos, C.~R.~M. Temme, J.~Rauber, H.~H. Sch{\"{u}}tt, M.~Bethge, and F.~A.
  Wichmann, ``Generalisation in humans and deep neural networks,'' \emph{CoRR},
  vol. abs/1808.08750, 2018. [Online]. Available:
  \url{http://arxiv.org/abs/1808.08750}
\BIBentrySTDinterwordspacing

\bibitem[Kolesnikov et~al.(2020)Kolesnikov, Beyer, Zhai, Puigcerver, Yung,
  Gelly, and Houlsby]{kolesnikov_big_2020}
A.~Kolesnikov, L.~Beyer, X.~Zhai, J.~Puigcerver, J.~Yung, S.~Gelly, and
  N.~Houlsby, ``Big transfer (bit): General visual representation learning,''
  in \emph{Computer Vision -- ECCV 2020}, A.~Vedaldi, H.~Bischof, T.~Brox, and
  J.-M. Frahm, Eds.\hskip 1em plus 0.5em minus 0.4em\relax Springer
  International Publishing, 2020, pp. 491--507.

\bibitem[Krizhevsky et~al.(2012)Krizhevsky, Sutskever, and
  Hinton]{krizhevsky_imagenet_2012}
A.~Krizhevsky, I.~Sutskever, and G.~E. Hinton, ``Imagenet classification with
  deep convolutional neural networks,'' in \emph{Advances in neural information
  processing systems}, 2012.

\bibitem[Hendrycks et~al.(2019)Hendrycks, Mu, Cubuk, Zoph, Gilmer, and
  Lakshminarayanan]{hendrycks_augmix_2019}
D.~Hendrycks, N.~Mu, E.~D. Cubuk, B.~Zoph, J.~Gilmer, and B.~Lakshminarayanan,
  ``{AugMix}: {A} {Simple} {Data} {Processing} {Method} to {Improve}
  {Robustness} and {Uncertainty},'' \emph{arXiv preprint arXiv:1912.02781},
  2019.

\bibitem[Cubuk et~al.(2018)Cubuk, Zoph, Mane, Vasudevan, and
  Le]{cubuk_autoaugment_2018}
E.~D. Cubuk, B.~Zoph, D.~Mane, V.~Vasudevan, and Q.~V. Le, ``Autoaugment:
  {Learning} augmentation policies from data,'' \emph{arXiv preprint
  arXiv:1805.09501}, 2018.

\bibitem[Cubuk et~al.(2019)Cubuk, Zoph, Shlens, and Le]{cubuk_randaugment_2019}
E.~D. Cubuk, B.~Zoph, J.~Shlens, and Q.~V. Le, ``{RandAugment}: {Practical}
  automated data augmentation with a reduced search space,'' \emph{arXiv
  preprint arXiv:1909.13719}, 2019.

\bibitem[Zhong et~al.(2020)Zhong, Zheng, Kang, Li, and Yang]{zhong_2020_random}
\BIBentryALTinterwordspacing
Z.~Zhong, L.~Zheng, G.~Kang, S.~Li, and Y.~Yang, ``Random erasing data
  augmentation,'' \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, vol.~34, no.~07, pp. 13\,001--13\,008, Apr. 2020. [Online].
  Available: \url{https://ojs.aaai.org/index.php/AAAI/article/view/7000}
\BIBentrySTDinterwordspacing

\bibitem[Yun et~al.(2019)Yun, Han, Oh, Chun, Choe, and Yoo]{yun_cutmix_2019}
S.~Yun, D.~Han, S.~J. Oh, S.~Chun, J.~Choe, and Y.~Yoo, ``{CutMix}:
  {Regularization} {Strategy} to {Train} {Strong} {Classifiers} {With}
  {Localizable} {Features},'' in \emph{Proceedings of the {IEEE}/{CVF}
  {International} {Conference} on {Computer} {Vision}}, 2019.

\bibitem[Zhang et~al.(2017)Zhang, Cisse, Dauphin, and
  Lopez-Paz]{zhang_mixup_2017}
H.~Zhang, M.~Cisse, Y.~N. Dauphin, and D.~Lopez-Paz, ``mixup: {Beyond}
  {Empirical} {Risk} {Minimization},'' \emph{arXiv preprint arXiv:1710.09412},
  2017.

\bibitem[Szegedy et~al.(2015)Szegedy, Liu, Jia, Sermanet, Reed, Anguelov,
  Erhan, Vanhoucke, and Rabinovich]{szegedy_deeper_2015}
C.~Szegedy, W.~Liu, Y.~Jia, P.~Sermanet, S.~Reed, D.~Anguelov, D.~Erhan,
  V.~Vanhoucke, and A.~Rabinovich, ``Going deeper with convolutions,'' in
  \emph{2015 IEEE Conference on Computer Vision and Pattern Recognition
  (CVPR)}, 2015, pp. 1--9.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he_deep_2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun, ``Deep residual learning for image
  recognition,'' in \emph{Proceedings of the IEEE Conference on Computer Vision
  and Pattern Recognition (CVPR)}, June 2016.

\bibitem[Dosovitskiy et~al.(2021)Dosovitskiy, Beyer, Kolesnikov, Weissenborn,
  Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, Uszkoreit, and
  Houlsby]{dosovitskiy_image_2021}
A.~Dosovitskiy, L.~Beyer, A.~Kolesnikov, D.~Weissenborn, X.~Zhai,
  T.~Unterthiner, M.~Dehghani, M.~Minderer, G.~Heigold, S.~Gelly, J.~Uszkoreit,
  and N.~Houlsby, ``An image is worth 16x16 words: Transformers for image
  recognition at scale,'' in \emph{ICLR}, 2021.

\bibitem[Steiner et~al.(2022)Steiner, Kolesnikov, Zhai, Wightman, Uszkoreit,
  and Beyer]{steiner_how_2022}
\BIBentryALTinterwordspacing
A.~P. Steiner, A.~Kolesnikov, X.~Zhai, R.~Wightman, J.~Uszkoreit, and L.~Beyer,
  ``How to train your vit? data, augmentation, and regularization in vision
  transformers,'' \emph{Transactions on Machine Learning Research}, 2022.
  [Online]. Available: \url{https://openreview.net/forum?id=4nPswr1KcP}
\BIBentrySTDinterwordspacing

\bibitem[Liu et~al.(2022{\natexlab{b}})Liu, Mao, Wu, Feichtenhofer, Darrell,
  and Xie]{liu_convnet_2022}
Z.~Liu, H.~Mao, C.-Y. Wu, C.~Feichtenhofer, T.~Darrell, and S.~Xie, ``A convnet
  for the 2020s,'' in \emph{Proceedings of the IEEE/CVF Conference on Computer
  Vision and Pattern Recognition (CVPR)}, June 2022, pp. 11\,976--11\,986.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, Krueger, and
  Sutskever]{radford_learning_2021}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry,
  A.~Askell, P.~Mishkin, J.~Clark, G.~Krueger, and I.~Sutskever, ``Learning
  {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision},''
  \emph{OpenAI Blog}, 2021.

\bibitem[Croce and Hein(2022)]{croce_2022_interplay}
\BIBentryALTinterwordspacing
F.~Croce and M.~Hein, ``On the interplay of adversarial robustness and
  architecture components: patches, convolution and attention,'' 2022.
  [Online]. Available: \url{https://arxiv.org/abs/2209.06953}
\BIBentrySTDinterwordspacing

\bibitem[Sun and Saenko(2016)]{sun_2016_deep}
B.~Sun and K.~Saenko, ``Deep coral: {C}orrelation alignment for deep domain
  adaptation,'' in \emph{ECCV}, 2016.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin_2016_domain}
Y.~Ganin, E.~Ustinova, H.~Ajakan, P.~Germain, H.~Larochelle, F.~Laviolette,
  M.~Marchand, and V.~Lempitsky, ``Domain-adversarial training of neural
  networks,'' \emph{J. Machine Learning Research}, vol.~17, no.~1, pp.
  2096--2030, 2016.

\bibitem[Liu et~al.(2021)Liu, Haghgoo, Chen, Raghunathan, Koh, Sagawa, Liang,
  and Finn]{liu_2021_just}
E.~Z. Liu, B.~Haghgoo, A.~S. Chen, A.~Raghunathan, P.~W. Koh, S.~Sagawa,
  P.~Liang, and C.~Finn, ``Just {T}rain {T}wice: {I}mproving group robustness
  without training group information,'' in \emph{International Conference on
  Machine Learning}, 2021.

\bibitem[Wiles et~al.(2022)Wiles, Gowal, Stimberg, Rebuffi, Ktena, Dvijotham,
  and Cemgil]{wiles_2022_fine}
\BIBentryALTinterwordspacing
O.~Wiles, S.~Gowal, F.~Stimberg, S.-A. Rebuffi, I.~Ktena, K.~D. Dvijotham, and
  A.~T. Cemgil, ``A fine-grained analysis on distribution shift,'' in
  \emph{ICLR}, 2022. [Online]. Available:
  \url{https://openreview.net/forum?id=Dl4LetuLdyK}
\BIBentrySTDinterwordspacing

\bibitem[Kireev et~al.(2021)Kireev, Andriushchenko, and
  Flammarion]{kireev_effectiveness_2021}
K.~Kireev, M.~Andriushchenko, and N.~Flammarion, ``On the effectiveness of
  adversarial training against common corruptions,'' \emph{arXiv preprint
  arXiv:2103.02325}, 2021.

\bibitem[Beyer et~al.(2020)Beyer, H{\'{e}}naff, Kolesnikov, Zhai, and van~den
  Oord]{beyer_2020_done}
\BIBentryALTinterwordspacing
L.~Beyer, O.~J. H{\'{e}}naff, A.~Kolesnikov, X.~Zhai, and A.~van~den Oord,
  ``Are we done with imagenet?'' \emph{CoRR}, vol. abs/2006.07159, 2020.
  [Online]. Available: \url{https://arxiv.org/abs/2006.07159}
\BIBentrySTDinterwordspacing

\bibitem[Recht et~al.(2019{\natexlab{b}})Recht, Roelofs, Schmidt, and
  Shankar]{recht_2019_imagenet}
B.~Recht, R.~Roelofs, L.~Schmidt, and V.~Shankar, ``Do {I}mage{N}et classifiers
  generalize to {I}mage{N}et?'' in \emph{Proceedings of the 36th International
  Conference on Machine Learning}, ser. Proceedings of Machine Learning
  Research, K.~Chaudhuri and R.~Salakhutdinov, Eds., vol.~97.\hskip 1em plus
  0.5em minus 0.4em\relax PMLR, 09--15 Jun 2019, pp. 5389--5400.

\bibitem[Wang et~al.(2019)Wang, Ge, Lipton, and Xing]{wang_2019_learning}
H.~Wang, S.~Ge, Z.~Lipton, and E.~P. Xing, ``Learning robust global
  representations by penalizing local predictive power,'' in \emph{Advances in
  Neural Information Processing Systems}, 2019, pp. 10\,506--10\,518.

\bibitem[Geirhos et~al.(2019{\natexlab{b}})Geirhos, Rubisch, Michaelis, Bethge,
  Wichmann, and Brendel]{geirhos_2019_imagenet}
R.~Geirhos, P.~Rubisch, C.~Michaelis, M.~Bethge, F.~A. Wichmann, and
  W.~Brendel, ``Imagenet-trained cnns are biased towards texture; increasing
  shape bias improves accuracy and robustness.'' in \emph{ICLR}.\hskip 1em plus
  0.5em minus 0.4em\relax OpenReview.net, 2019.

\bibitem[Vasudevan et~al.(2022{\natexlab{a}})Vasudevan, Caine, Gontijo-Lopes,
  Fridovich-Keil, and Roelofs]{vasudevan_2022_bagel}
\BIBentryALTinterwordspacing
V.~Vasudevan, B.~Caine, R.~Gontijo-Lopes, S.~Fridovich-Keil, and R.~Roelofs,
  ``When does dough become a bagel? analyzing the remaining mistakes on
  imagenet,'' 2022. [Online]. Available: \url{https://arxiv.org/abs/2205.04596}
\BIBentrySTDinterwordspacing

\bibitem[Goodman and Wei(2019)]{goodman_2019_cloud}
\BIBentryALTinterwordspacing
D.~Goodman and T.~Wei, ``Cloud-based image classification service is not robust
  to simple transformations: {A} forgotten battlefield,'' \emph{CoRR}, vol.
  abs/1906.07997, 2019. [Online]. Available:
  \url{http://arxiv.org/abs/1906.07997}
\BIBentrySTDinterwordspacing

\bibitem[Goodman and Hao(2020)]{goodman_2020_attacking}
\BIBentryALTinterwordspacing
D.~Goodman and X.~Hao, ``Attacking and defending machine learning applications
  of public cloud,'' \emph{CoRR}, vol. abs/2008.02076, 2020. [Online].
  Available: \url{https://arxiv.org/abs/2008.02076}
\BIBentrySTDinterwordspacing

\bibitem[Ghiasi et~al.(2017)Ghiasi, Lee, Kudlur, Dumoulin, and
  Shlens]{ghiasi_exploring_2017}
G.~Ghiasi, H.~Lee, M.~Kudlur, V.~Dumoulin, and J.~Shlens, ``Exploring the
  structure of a real-time, arbitrary neural artistic stylization network,''
  \emph{ArXiv}, vol. abs/1705.06830, 2017.

\bibitem[Loshchilov and Hutter(2019)]{loshchilov_2018_decoupled}
\BIBentryALTinterwordspacing
I.~Loshchilov and F.~Hutter, ``Decoupled weight decay regularization,'' in
  \emph{ICLR}, 2019. [Online]. Available:
  \url{https://openreview.net/forum?id=Bkg6RiCqY7}
\BIBentrySTDinterwordspacing

\bibitem[Rebuffi et~al.(2023)Rebuffi, Croce, and
  Gowal]{rebuffi_2023_revisiting}
\BIBentryALTinterwordspacing
S.-A. Rebuffi, F.~Croce, and S.~Gowal, ``Revisiting adapters with adversarial
  training,'' in \emph{The Eleventh International Conference on Learning
  Representations}, 2023. [Online]. Available:
  \url{https://openreview.net/forum?id=HPdxC1THU8T}
\BIBentrySTDinterwordspacing

\bibitem[Vasudevan et~al.(2022{\natexlab{b}})Vasudevan, Caine, Gontijo-Lopes,
  Fridovich-Keil, and Roelofs]{vasudevan_when_2022}
V.~Vasudevan, B.~Caine, R.~Gontijo-Lopes, S.~Fridovich-Keil, and R.~Roelofs,
  ``When does dough become a bagel? {Analyzing} the remaining mistakes on
  {ImageNet},'' \emph{arXiv preprint arXiv:2205.04596}, 2022.

\end{thebibliography}
