@InProceedings{Jacob_2018_CVPR,
author = {Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
title = {Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{bai2020binarybert,
  title={Binarybert: Pushing the limit of bert quantization},
  author={Bai, Haoli and Zhang, Wei and Hou, Lu and Shang, Lifeng and Jin, Jing and Jiang, Xin and Liu, Qun and Lyu, Michael and King, Irwin},
  journal={arXiv preprint arXiv:2012.15701},
  year={2020}
}

@article{zhang2020ternarybert,
  title={Ternarybert: Distillation-aware ultra-low bit bert},
  author={Zhang, Wei and Hou, Lu and Yin, Yichun and Shang, Lifeng and Chen, Xiao and Jiang, Xin and Liu, Qun},
  journal={arXiv preprint arXiv:2009.12812},
  year={2020}
}
@article{li2021mqbench,
  title={MQBench: Towards reproducible and deployable model quantization benchmark},
  author={Li, Yuhang and Shen, Mingzhu and Ma, Jian and Ren, Yan and Zhao, Mingxin and Zhang, Qi and Gong, Ruihao and Yu, Fengwei and Yan, Junjie},
  journal={arXiv preprint arXiv:2111.03759},
  year={2021}
}
@article{bondarenko2021understanding,
  title={Understanding and overcoming the challenges of efficient transformer quantization},
  author={Bondarenko, Yelysei and Nagel, Markus and Blankevoort, Tijmen},
  journal={arXiv preprint arXiv:2109.12948},
  year={2021}
}
@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}
@article{clark2020electra,
  title={Electra: Pre-training text encoders as discriminators rather than generators},
  author={Clark, Kevin and Luong, Minh-Thang and Le, Quoc V and Manning, Christopher D},
  journal={arXiv preprint arXiv:2003.10555},
  year={2020}
}
@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{yang2019xlnet,
  title={Xlnet: Generalized autoregressive pretraining for language understanding},
  author={Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime and Salakhutdinov, Russ R and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{choi2018pact,
  title={Pact: Parameterized clipping activation for quantized neural networks},
  author={Choi, Jungwook and Wang, Zhuo and Venkataramani, Swagath and Chuang, Pierce I-Jen and Srinivasan, Vijayalakshmi and Gopalakrishnan, Kailash},
  journal={arXiv preprint arXiv:1805.06085},
  year={2018}
}
@article{esser2019learned,
  title={Learned step size quantization},
  author={Esser, Steven K and McKinstry, Jeffrey L and Bablani, Deepika and Appuswamy, Rathinakumar and Modha, Dharmendra S},
  journal={arXiv preprint arXiv:1902.08153},
  year={2019}
}
@inproceedings{choukroun2019low,
  title={Low-bit quantization of neural networks for efficient inference},
  author={Choukroun, Yoni and Kravchik, Eli and Yang, Fan and Kisilev, Pavel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)},
  pages={3009--3018},
  year={2019},
  organization={IEEE}
}
@inproceedings{zafrir2019q8bert,
  title={Q8bert: Quantized 8bit bert},
  author={Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  booktitle={2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS)},
  pages={36--39},
  year={2019},
  organization={IEEE}
}
@article{fan2020training,
  title={Training with quantization noise for extreme model compression},
  author={Fan, Angela and Stock, Pierre and Graham, Benjamin and Grave, Edouard and Gribonval, R{\'e}mi and Jegou, Herve and Joulin, Armand},
  journal={arXiv preprint arXiv:2004.07320},
  year={2020}
}


@misc{FasterTransformer,
   title={Faster Transformer},
   author={NVIDIA},
   howpublished={\url{https://github.com/NVIDIA/FasterTransformer}},
   year={2022}
}

@misc{NNCFTransformer,
   title={NNCF},
   author={Intel},
   howpublished={\url{https://github.com/openvinotoolkit/nncf/tree/develop/third_party_integration/huggingface_transformers}},
   year={2022}
}

@article{banner2018aciq,
  title={ACIQ: analytical clipping for integer quantization of neural networks},
  author={Banner, Ron and Nahshan, Yury and Hoffer, Elad and Soudry, Daniel},
  year={2018}
}

@inproceedings{zhao2019improving,
  title={Improving neural network quantization without retraining using outlier channel splitting},
  author={Zhao, Ritchie and Hu, Yuwei and Dotzel, Jordan and De Sa, Chris and Zhang, Zhiru},
  booktitle={International conference on machine learning},
  pages={7543--7552},
  year={2019},
  organization={PMLR}
}
@article{wang2018glue,
title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
author={Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
journal={arXiv preprint arXiv:1804.07461},
year={2018}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@inproceedings{bhalgat2020lsq+,
  title={Lsq+: Improving low-bit quantization through learnable offsets and better initialization},
  author={Bhalgat, Yash and Lee, Jinwon and Nagel, Markus and Blankevoort, Tijmen and Kwak, Nojun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops},
  pages={696--697},
  year={2020}
}

@inproceedings{kim2021bert,
  title={I-bert: Integer-only bert quantization},
  author={Kim, Sehoon and Gholami, Amir and Yao, Zhewei and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={International conference on machine learning},
  pages={5506--5518},
  year={2021},
  organization={PMLR}
}

@article{wu2020easyquant,
  title={EasyQuant: Post-training quantization via scale optimization},
  author={Wu, Di and Tang, Qi and Zhao, Yongle and Zhang, Ming and Fu, Ying and Zhang, Debing},
  journal={arXiv preprint arXiv:2006.16669},
  year={2020}
}

@inproceedings{shen2020q,
  title={Q-bert: Hessian based ultra low precision quantization of bert},
  author={Shen, Sheng and Dong, Zhen and Ye, Jiayu and Ma, Linjian and Yao, Zhewei and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={34},
  number={05},
  pages={8815--8821},
  year={2020}
}

@inproceedings{zadeh2020gobo,
  title={Gobo: Quantizing attention-based nlp models for low latency and energy efficient inference},
  author={Zadeh, Ali Hadi and Edo, Isak and Awad, Omar Mohamed and Moshovos, Andreas},
  booktitle={2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)},
  pages={811--824},
  year={2020},
  organization={IEEE}
}

@article{wu2020integer,
  title={Integer quantization for deep learning inference: Principles and empirical evaluation},
  author={Wu, Hao and Judd, Patrick and Zhang, Xiaojie and Isaev, Mikhail and Micikevicius, Paulius},
  journal={arXiv preprint arXiv:2004.09602},
  year={2020}
}

@article{luo2020positional,
  title={Positional artefacts propagate through masked language model embeddings},
  author={Luo, Ziyang and Kulmizev, Artur and Mao, Xiaoxi},
  journal={arXiv preprint arXiv:2011.04393},
  year={2020}
}

@inproceedings{rajpurkar-etal-2018-know,
    title = "Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}",
    author = "Rajpurkar, Pranav  and
      Jia, Robin  and
      Liang, Percy",
    year = "2018",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    publisher = "Association for Computational Linguistics",
    pages = "784--789",
}

@inproceedings{narayan-etal-2018-dont,
    title = "Don{'}t Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization",
    author = "Narayan, Shashi  and
      Cohen, Shay B.  and
      Lapata, Mirella",
     year = "2018",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    publisher = "Association for Computational Linguistics",
    pages = "1797--1807",
}

@inproceedings{nallapati-etal-2016-abstractive,
    title = "Abstractive Text Summarization using Sequence-to-sequence {RNN}s and Beyond",
    author = "Ramesh Nallapati and
               Bowen Zhou and
               C{\'{\i}}cero Nogueira dos Santos and
               {\c{C}}aglar G{\"{u}}l{\c{c}}ehre and
               Bing Xiang",
    year = "2016",
    booktitle = "Proceedings of The 20th {SIGNLL} Conference on Computational Natural Language Learning",
    publisher = "Association for Computational Linguistics",
    pages = "280--290",
}

@article{dai2019transformer,
  title={Transformer-xl: Attentive language models beyond a fixed-length context},
  author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1901.02860},
  year={2019}
}

@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}

@misc{
mckinstry2019discovering,
title={Discovering Low-Precision Networks Close to Full-Precision Networks for Efficient Embedded Inference},
author={Jeffrey L. McKinstry and Steven K. Esser and Rathinakumar Appuswamy and Deepika Bablani and John V. Arthur and Izzet B. Yildiz and Dharmendra S. Modha},
year={2019},
url={https://openreview.net/forum?id=BJx1SsAcYQ},
}
@inproceedings{he2016resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@article{google-whitepaper,
  title={Quantizing deep convolutional networks for efficient inference: A whitepaper},
  author={Krishnamoorthi, Raghuraman},
  journal={arXiv preprint arXiv:1806.08342},
  year={2018}
}

@article{hinton2015distilling,
  title={Distilling the knowledge in a neural network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}


@article{tao2022compression,
  title={Compression of Generative Pre-trained Language Models via Quantization},
  author={Tao, Chaofan and Hou, Lu and Zhang, Wei and Shang, Lifeng and Jiang, Xin and Liu, Qun and Luo, Ping and Wong, Ngai},
  journal={arXiv preprint arXiv:2203.10705},
  year={2022}
}

@article{kure,
  title={Robust quantization: One model to rule them all},
  author={Chmiel, Brian and Banner, Ron and Shomron, Gil and Nahshan, Yury and Bronstein, Alex and Weiser, Uri and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={5308--5317},
  year={2020}
}

@article{2020-pr-bnnsurvey,
title = "Binary neural networks: A survey",
journal = "Pattern Recognition",
pages = "107281",
year = "2020",
issn = "0031-3203",
doi = "https://doi.org/10.1016/j.patcog.2020.107281",
url = "http://www.sciencedirect.com/science/article/pii/S0031320320300856",
author = "Haotong Qin and Ruihao Gong and Xianglong Liu and Xiao Bai and Jingkuan Song and Nicu Sebe",
}

@InProceedings{2019-iccv-dsq,
author = {Gong, Ruihao and Liu, Xianglong and Jiang, Shenghu and Li, Tianxiang and Hu, Peng and Lin, Jiazhen and Yu, Fengwei and Yan, Junjie},
title = {Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks},
booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
month = {October},
year = {2019}
}

@inproceedings{2020-icpp-lowbit,
author = {Han, Qingchang and Hu, Yongmin and Yu, Fengwei and Yang, Hailong and Liu, Bing and Hu, Peng and Gong, Ruihao and Wang, Yanfei and Wang, Rui and Luan, Zhongzhi and Qian, Depei},
title = {Extremely Low-Bit Convolution Optimization for Quantized Neural Network on Modern Computer Architectures},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404407},
doi = {10.1145/3404397.3404407},
booktitle = {49th International Conference on Parallel Processing - ICPP},
articleno = {38},
numpages = {12},
keywords = {NVIDIA GPU, Quantized Neural Network, Extremely Low-bit Convolution, Computation Optimization, ARM CPU},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@InProceedings{2020-cvpr-irnet,
author = {Haotong Qin and Ruihao Gong and Xianglong Liu and Mingzhu Shen and Ziran Wei and Fengwei Yu and Jingkuan Song},
title = {Forward and Backward Information Retention for Accurate Binary Neural Networks},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{2020-cvpr-int8training,
 author = {Zhu, Feng and Gong, Ruihao and Yu, Fengwei and Liu, Xianglong and Wang, Yanfei and Li, Zhelong and Yang, Xiuqi and Yan, Junjie},
 booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
 month = {June},
 title = {Towards Unified INT8 Training for Convolutional Neural Network},
 year = {2020}
}

@InProceedings{2020-cvpr-rcmloss,
author = {Yudong Wu and Yichao Wu and Ruihao Gong and Yuanhao Lv and Ken Chen and Ding Liang and Xiaolin Hu and Xianglong Liu and Junjie Yan},
title = {Rotation Consistent Margin Loss for Efficient Low-bit Face Recognition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2020}
}

@inproceedings{
    2021-iclr-brecq,
    title={BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction},
    author={Yuhang Li and Ruihao Gong and Xu Tan and Yang Yang and Peng Hu and Qi Zhang and Fengwei Yu and Wei Wang and Shi Gu},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=POWv6hDd9XH}
}

@InProceedings{2021-cvpr-dsg,
author = {Xiangguo Zhang and Haotong Qin and Yifu Ding and Ruihao Gong and Qinghua Yan and Renshuai Tao and Yuhang Li and Fengwei Yu and Xianglong Liu},
title = {Diversifying Sample Generation for Accurate Data-Free Quantization},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2021}
}

@inproceedings{2022-iclr-qdrop,
 author = {Xiuying Wei and Ruihao Gong and Yuhang Li and Xianglong Liu and Fengwei Yu},
 booktitle = {International Conference on Learning Representations},
 title = {QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization},
 url = {https://openreview.net/forum?id=ySQH0oDyp7},
 year = {2022}
}


@inproceedings{2022-icpp-nnlqp,
 articleno = {43},
 author = {Liu, Liang and Shen, Mingzhu and Gong, Ruihao and Yu, Fengwei and Yang, Hailong},
 booktitle = {51 International Conference on Parallel Processing - ICPP},
 doi = {10.1145/3545008.3545051},
 keywords = {neural network, multi-platform, latency query, latency prediction},
 numpages = {14},
 publisher = {Association for Computing Machinery},
 series = {ICPP '22},
 title = {NNLQP: A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database},
 url = {https://doi.org/10.1145/3545008.3545051},
 year = {2022}
}

@inproceedings{2021-iccv-mixmix,
 author = {Li, Yuhang and Zhu, Feng and Gong, Ruihao and Shen, Mingzhu and Dong, Xin and Yu, Fengwei and Lu, Shaoqing and Gu, Shi},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
 month = {October},
 pages = {4410-4419},
 title = {MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing},
 year = {2021}
}

@article{han2015deepcompress,
  title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={arXiv preprint arXiv:1510.00149},
  year={2015}
}

@article{zoph2016nas,
  title={Neural architecture search with reinforcement learning},
  author={Zoph, Barret and Le, Quoc V},
  journal={arXiv preprint arXiv:1611.01578},
  year={2016}
}

@inproceedings{2021-iccv-oqa,
 author = {Shen, Mingzhu and Liang, Feng and Gong, Ruihao and Li, Yuhang and Li, Chuming and Lin, Chen and Yu, Fengwei and Yan, Junjie and Ouyang, Wanli},
 booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
 month = {October},
 pages = {5340-5349},
 title = {Once Quantization-Aware Training: High Performance Extremely Low-Bit Architecture Search},
 year = {2021}
}

@inproceedings{nagel2020adaround,
  title={Up or down? adaptive rounding for post-training quantization},
  author={Nagel, Markus and Amjad, Rana Ali and Van Baalen, Mart and Louizos, Christos and Blankevoort, Tijmen},
  booktitle={International Conference on Machine Learning},
  pages={7197--7206},
  year={2020},
  organization={PMLR}
}

@inproceedings{cai2020zeroq,
  title={Zeroq: A novel zero shot quantization framework},
  author={Cai, Yaohui and Yao, Zhewei and Dong, Zhen and Gholami, Amir and Mahoney, Michael W and Keutzer, Kurt},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13169--13178},
  year={2020}
}
@article{nahshan2019lapq,
  title={Loss Aware Post-training Quantization},
  author={Nahshan, Yury and Chmiel, Brian and Baskin, Chaim and Zheltonozhskii, Evgenii and Banner, Ron and Bronstein, Alex M and Mendelson, Avi},
  journal={arXiv preprint arXiv:1911.07190},
  year={2019}
}
@inproceedings{hubara2021accurate,
  title={Accurate Post Training Quantization With Small Calibration Sets},
  author={Hubara, Itay and Nahshan, Yury and Hanani, Yair and Banner, Ron and Soudry, Daniel},
  booktitle={International Conference on Machine Learning},
  pages={4466--4475},
  year={2021},
  organization={PMLR}
}

@article{bert_busters,
  title={BERT busters: Outlier dimensions that disrupt transformers},
  author={Kovaleva, Olga and Kulshreshtha, Saurabh and Rogers, Anna and Rumshisky, Anna},
  journal={arXiv preprint arXiv:2105.06990},
  year={2021}
}

@article{token_frequency,
  title={Outliers Dimensions that Disrupt Transformers Are Driven by Frequency},
  author={Puccetti, Giovanni and Rogers, Anna and Drozd, Aleksandr and Dell'Orletta, Felice},
  journal={arXiv preprint arXiv:2205.11380},
  year={2022}
}

