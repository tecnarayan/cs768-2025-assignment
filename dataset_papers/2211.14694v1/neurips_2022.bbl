\begin{thebibliography}{64}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
M.~Arjovsky, S.~Chintala, and L.~Bottou.
\newblock Wasserstein gan.
\newblock In \emph{ICML}, 2017.

\bibitem[Ba et~al.(2016)Ba, Kiros, and Hinton]{ba2016layer}
J.~L. Ba, J.~R. Kiros, and G.~E. Hinton.
\newblock Layer normalization.
\newblock \emph{arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[Berthelot et~al.(2017)Berthelot, Schumm, and Metz]{berthelot2017began}
D.~Berthelot, T.~Schumm, and L.~Metz.
\newblock Began: Boundary equilibrium generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1703.10717}, 2017.

\bibitem[{Brock} et~al.(2017){Brock}, {Lim}, {Ritchie}, and
  {Weston}]{Brockphoto2017}
A.~{Brock}, T.~{Lim}, J.~M. {Ritchie}, and N.~{Weston}.
\newblock Neural photo editing with introspective adversarial networks.
\newblock In \emph{Proc. ICLR}, 2017.

\bibitem[Brock et~al.(2018)Brock, Donahue, and Simonyan]{brock2018large}
A.~Brock, J.~Donahue, and K.~Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock \emph{arXiv preprint arXiv:1809.11096}, 2018.

\bibitem[{Cao} et~al.(2021){Cao}, {Hou}, {Yang}, {He}, and {Sun}]{cao2021remix}
J.~{Cao}, L.~{Hou}, M.-H. {Yang}, R.~{He}, and Z.~{Sun}.
\newblock {ReMix: Towards Image-to-Image Translation with Limited Data}.
\newblock In \emph{CVPR}, 2021.

\bibitem[Chen et~al.(2021)Chen, Cheng, Gan, Liu, and Wang]{chen2020lth}
T.~Chen, Y.~Cheng, Z.~Gan, J.~Liu, and Z.~Wang.
\newblock Data-efficient gan training beyond (just) augmentations: A lottery
  ticket perspective.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Choi et~al.(2019)Choi, Kim, and Kim]{choidomain}
J.~Choi, T.~Kim, and C.~Kim.
\newblock Self-ensembling with gan-based data augmentation for domain
  adaptation in semantic segmentation.
\newblock In \emph{ICCV}, 2019.

\bibitem[Cully et~al.(2017)Cully, Chang, and Demiris]{cully2017magan}
R.~W.~A. Cully, H.~J. Chang, and Y.~Demiris.
\newblock Magan: Margin adaptation for generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1704.03817}, 2017.

\bibitem[Deshpande et~al.(2018)Deshpande, Zhang, and
  Schwing]{DeshpandeCVPR2018}
I.~Deshpande, Z.~Zhang, and A.~G. Schwing.
\newblock {Generative Modeling using the Sliced Wasserstein Distance}.
\newblock In \emph{Proc. CVPR}, 2018.

\bibitem[Deshpande et~al.(2019)Deshpande, Hu, Sun, Pyrros, Siddiqui, Koyejo,
  Zhao, Forsyth, and Schwing]{IDeshpandeCVPR2019}
I.~Deshpande, Y.-T. Hu, R.~Sun, A.~Pyrros, N.~Siddiqui, S.~Koyejo, Z.~Zhao,
  D.~Forsyth, and A.~G. Schwing.
\newblock {Max-Sliced Wasserstein Distance and its use for GANs}.
\newblock In \emph{CVPR}, 2019.

\bibitem[Donahue et~al.(2016)Donahue, Kr{\"a}henb{\"u}hl, and
  Darrell]{donahue2016adversarial}
J.~Donahue, P.~Kr{\"a}henb{\"u}hl, and T.~Darrell.
\newblock Adversarial feature learning.
\newblock \emph{arXiv preprint arXiv:1605.09782}, 2016.

\bibitem[Goodfellow(2016)]{goodfellow2016nips}
I.~Goodfellow.
\newblock Nips 2016 tutorial: Generative adversarial networks.
\newblock \emph{arXiv preprint arXiv:1701.00160}, 2016.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
I.~Goodfellow, J.~Pouget-Abadie, M.~Mirza, B.~Xu, D.~Warde-Farley, S.~Ozair,
  A.~Courville, and Y.~Bengio.
\newblock Generative adversarial nets.
\newblock In \emph{NeurIPS}, 2014.

\bibitem[Gulrajani et~al.(2017)Gulrajani, Ahmed, Arjovsky, Dumoulin, and
  Courville]{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~Courville.
\newblock Improved training of wasserstein gans.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Heusel et~al.(2017)Heusel, Ramsauer, Unterthiner, Nessler, and
  Hochreiter]{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash
  equilibrium.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
S.~Ioffe and C.~Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock \emph{arXiv preprint arXiv:1502.03167}, 2015.

\bibitem[Isola et~al.(2017)Isola, Zhu, Zhou, and Efros]{isola2017image}
P.~Isola, J.-Y. Zhu, T.~Zhou, and A.~A. Efros.
\newblock Image-to-image translation with conditional adversarial networks.
\newblock In \emph{2017 IEEE Conference on Computer Vision and Pattern
  Recognition (CVPR)}, 2017.

\bibitem[Jiang et~al.(2021)Jiang, Dai, Wu, and Change~Loy]{jiang2021pseudoaug}
L.~Jiang, B.~Dai, W.~Wu, and C.~Change~Loy.
\newblock Deceive d: Adaptive pseudo augmentation for gan training with limited
  data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2021.

\bibitem[Jolicoeur-Martineau(2018)]{jolicoeur2018relativistic}
A.~Jolicoeur-Martineau.
\newblock The relativistic discriminator: a key element missing from standard
  gan.
\newblock In \emph{ICLR}, 2018.

\bibitem[Karras et~al.(2018)Karras, Aila, Laine, and
  Lehtinen]{karras2017progressive}
T.~Karras, T.~Aila, S.~Laine, and J.~Lehtinen.
\newblock Progressive growing of gans for improved quality, stability, and
  variation.
\newblock In \emph{ICLR}, 2018.

\bibitem[Karras et~al.(2019)Karras, Laine, and Aila]{karras2019style}
T.~Karras, S.~Laine, and T.~Aila.
\newblock A style-based generator architecture for generative adversarial
  networks.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 4401--4410, 2019.

\bibitem[Karras et~al.(2020)Karras, Aittala, Hellsten, Laine, Lehtinen, and
  Aila]{keras2020ada}
T.~Karras, M.~Aittala, J.~Hellsten, S.~Laine, J.~Lehtinen, and T.~Aila.
\newblock Regularizing generative adversarial networks under limited data.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Kim et~al.(2020)Kim, Kim, Kang, and Lee]{Kim2020U-GAT-IT}
J.~Kim, M.~Kim, H.~Kang, and K.~H. Lee.
\newblock U-gat-it: Unsupervised generative attentional networks with adaptive
  layer-instance normalization for image-to-image translation.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Kingma and Ba(2014)]{kingma2014adam}
D.~Kingma and J.~Ba.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kodali et~al.(2017)Kodali, Abernethy, Hays, and
  Kira]{kodali2017dragan}
N.~Kodali, J.~Abernethy, J.~Hays, and Z.~Kira.
\newblock How to train your dragan.
\newblock \emph{arXiv preprint arXiv:1705.07215}, 2017.

\bibitem[Krizhevsky(2009)]{dataset_cifar}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock
  \emph{\url{https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}},
  2009.

\bibitem[Kumari et~al.(2022)Kumari, Zhang, Shechtman, and
  Zhu]{kumari2021ensembling}
N.~Kumari, R.~Zhang, E.~Shechtman, and J.-Y. Zhu.
\newblock Ensembling off-the-shelf models for gan training.
\newblock In \emph{CVPR}, June 2022.

\bibitem[{Kurach} et~al.(2019){Kurach}, {Lucic}, {Zhai}, {Michalski}, and
  {Gelly}]{regstudy2019}
K.~{Kurach}, M.~{Lucic}, X.~{Zhai}, M.~{Michalski}, and S.~{Gelly}.
\newblock A large-scale study on regularization and normalization in gans.
\newblock In \emph{ICML}, 2019.

\bibitem[Le and Yang(2015)]{Le2015TinyIV}
Y.~Le and X.~S. Yang.
\newblock Tiny imagenet visual recognition challenge.
\newblock In
  \emph{\url{http://vision.stanford.edu/teaching/cs231n/reports/2015/pdfs/yle_project.pdf}},
  2015.

\bibitem[Li et~al.(2017{\natexlab{a}})Li, Chang, Cheng, Yang, and
  P{\'o}czos]{li2017mmd}
C.-L. Li, W.-C. Chang, Y.~Cheng, Y.~Yang, and B.~P{\'o}czos.
\newblock Mmd gan: Towards deeper understanding of moment matching network.
\newblock In \emph{NeurIPS}, 2017{\natexlab{a}}.

\bibitem[Li et~al.(2017{\natexlab{b}})Li, Schwing, Wang, and Zemel]{LiNIPS2017}
Y.~Li, A.~G. Schwing, K.-C. Wang, and R.~Zemel.
\newblock {Dualing GANs}.
\newblock In \emph{NeurIPS}, 2017{\natexlab{b}}.

\bibitem[Lin et~al.(2018)Lin, Khetan, Fanti, and Oh]{lin2018pacgan}
Z.~Lin, A.~Khetan, G.~Fanti, and S.~Oh.
\newblock Pacgan: The power of two samples in generative adversarial networks.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Liu et~al.(2017)Liu, Bousquet, and Chaudhuri]{liu2017approximation}
S.~Liu, O.~Bousquet, and K.~Chaudhuri.
\newblock Approximation and convergence properties of generative adversarial
  learning.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[{Mao} et~al.(2016){Mao}, {Li}, {Xie}, {Lau}, {Wang}, and
  {Smolley}]{lsgan2016mao}
X.~{Mao}, Q.~{Li}, H.~{Xie}, R.~Y.~K. {Lau}, Z.~{Wang}, and S.~P. {Smolley}.
\newblock {Least Squares Generative Adversarial Networks}.
\newblock \emph{arXiv e-prints}, 2016.

\bibitem[Mazumdar et~al.(2019)Mazumdar, Jordan, and
  Sastry]{mazumdar2019finding}
E.~V. Mazumdar, M.~I. Jordan, and S.~S. Sastry.
\newblock On finding local nash equilibria (and only local nash equilibria) in
  zero-sum games.
\newblock \emph{arXiv preprint arXiv:1901.00838}, 2019.

\bibitem[Mescheder et~al.(2018)Mescheder, Geiger, and
  Nowozin]{mescheder2018training}
L.~Mescheder, A.~Geiger, and S.~Nowozin.
\newblock Which training methods for gans do actually converge?
\newblock In \emph{ICML}, 2018.

\bibitem[Miyato et~al.(2018)Miyato, Kataoka, Koyama, and
  Yoshida]{miyato2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock In \emph{ICLR}, 2018.

\bibitem[Mroueh and Sercu(2017)]{mroueh2017fisher}
Y.~Mroueh and T.~Sercu.
\newblock Fisher gan.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Mroueh et~al.(2017)Mroueh, Sercu, and Goel]{mroueh2017mcgan}
Y.~Mroueh, T.~Sercu, and V.~Goel.
\newblock Mcgan: Mean and covariance feature matching gan.
\newblock \emph{arXiv preprint arXiv:1702.08398}, 2017.

\bibitem[Nowozin et~al.(2016)Nowozin, Cseke, and Tomioka]{nowozin2016f}
S.~Nowozin, B.~Cseke, and R.~Tomioka.
\newblock f-gan: Training generative neural samplers using variational
  divergence minimization.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[{Ojha} et~al.(2021){Ojha}, {Li}, {Lu}, {Efros}, {Lee}, {Shechtman},
  and {Zhang}]{ojha2021domain}
U.~{Ojha}, Y.~{Li}, J.~{Lu}, A.~A. {Efros}, Y.~J. {Lee}, E.~{Shechtman}, and
  R.~{Zhang}.
\newblock {Few-shot Image Generation via Cross-domain Correspondence}.
\newblock In \emph{CVPR}, 2021.

\bibitem[Poole et~al.(2016)Poole, Alemi, Sohl-Dickstein, and
  Angelova]{poole2016improved}
B.~Poole, A.~A. Alemi, J.~Sohl-Dickstein, and A.~Angelova.
\newblock Improved generator objectives for gans.
\newblock \emph{arXiv preprint arXiv:1612.02780}, 2016.

\bibitem[Radford et~al.(2016)Radford, Metz, and
  Chintala]{radford2015unsupervised}
A.~Radford, L.~Metz, and S.~Chintala.
\newblock Unsupervised representation learning with deep convolutional
  generative adversarial networks.
\newblock In \emph{ICLR}, 2016.

\bibitem[{Roth} et~al.(2017){Roth}, {Lucchi}, {Nowozin}, and
  {Hofmann}]{roth2017reg}
K.~{Roth}, A.~{Lucchi}, S.~{Nowozin}, and T.~{Hofmann}.
\newblock {Stabilizing Training of Generative Adversarial Networks through
  Regularization}.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2017.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  Chen, and Chen]{salimans2016improved}
T.~Salimans, I.~Goodfellow, W.~Zaremba, V.~Cheung, A.~Radford, X.~Chen, and
  X.~Chen.
\newblock Improved techniques for training gans.
\newblock In \emph{NeurIPS}, 2016.

\bibitem[Sanjabi et~al.(2018)Sanjabi, Ba, Razaviyayn, and
  Lee]{sanjabi2018convergence}
M.~Sanjabi, J.~Ba, M.~Razaviyayn, and J.~D. Lee.
\newblock On the convergence and robustness of training gans with regularized
  optimal transport.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[Sankaranarayanan et~al.(2018)Sankaranarayanan, Balaji, Castillo, and
  Chellappa]{swamidomain}
S.~Sankaranarayanan, Y.~Balaji, C.~D. Castillo, and R.~Chellappa.
\newblock Generate to adapt: Aligning domains using generative adversarial
  networks.
\newblock In \emph{CVPR}, 2018.

\bibitem[{Shahbazi} et~al.(2022){Shahbazi}, {Danelljan}, {Pani Paudel}, and
  {Van Gool}]{Shahbazi2022cgan}
M.~{Shahbazi}, M.~{Danelljan}, D.~{Pani Paudel}, and L.~{Van Gool}.
\newblock {Collapse by Conditioning: Training Class-conditional GANs with
  Limited Data}.
\newblock In \emph{ICLR}, 2022.

\bibitem[Sun et~al.(2020)Sun, Fang, and Schwing]{sun2020ganlandscape}
R.~Sun, T.~Fang, and A.~Schwing.
\newblock {Towards a Better Global Loss Landscape of GANs}.
\newblock In \emph{Conference on Neural Information Processing Systems}, 2020.

\bibitem[Tseng et~al.(2021)Tseng, Jiang, Liu, Yang, and Yang]{tseng2020lecam}
H.-Y. Tseng, L.~Jiang, C.~Liu, M.-H. Yang, and W.~Yang.
\newblock Regularizing generative adversarial networks under limited data.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, 2021.

\bibitem[Wah et~al.(2011)Wah, Branson, Welinder, Perona, and
  Belongie]{Wah2011TheCB}
C.~Wah, S.~Branson, P.~Welinder, P.~Perona, and S.~J. Belongie.
\newblock The caltech-ucsd birds-200-2011 dataset.
\newblock In
  \emph{\url{https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf}},
  2011.

\bibitem[{Webster} et~al.(2019){Webster}, {Rabin}, {Simon}, and
  {Jurie}]{webster2019overfit}
R.~{Webster}, J.~{Rabin}, L.~{Simon}, and F.~{Jurie}.
\newblock {Detecting Overfitting of Deep Generative Networks via Latent
  Recovery}.
\newblock In \emph{CVPR}, 2019.

\bibitem[Wu et~al.(2019)Wu, Huang, Li, Thoma, and Van~Gool]{wu2017sliced}
J.~Wu, Z.~Huang, W.~Li, J.~Thoma, and L.~Van~Gool.
\newblock Sliced wasserstein generative models.
\newblock In \emph{CVPR}, 2019.

\bibitem[Xiao et~al.(2018)Xiao, Zhong, and Zheng]{xiao2018bourgan}
C.~Xiao, P.~Zhong, and C.~Zheng.
\newblock Bourgan: Generative networks with metric embeddings.
\newblock In \emph{NeurIPS}, 2018.

\bibitem[{Yang} et~al.(2021){Yang}, {Shen}, {Xu}, and
  {Zhou}]{yang2021instancedisc}
C.~{Yang}, Y.~{Shen}, Y.~{Xu}, and B.~{Zhou}.
\newblock {Data-Efficient Instance Generation from Instance Discrimination}.
\newblock In \emph{NeurIPS}, 2021.

\bibitem[Yaz{\i}c{\i} et~al.(2019)Yaz{\i}c{\i}, Foo, Winkler, Yap, Piliouras,
  and Chandrasekhar]{yazici2018unusual}
Y.~Yaz{\i}c{\i}, C.-S. Foo, S.~Winkler, K.-H. Yap, G.~Piliouras, and
  V.~Chandrasekhar.
\newblock The unusual effectiveness of averaging in gan training.
\newblock In \emph{ICLR}, 2019.

\bibitem[Zhang et~al.(2018)Zhang, Goodfellow, Metaxas, and
  Odena]{zhang2018self}
H.~Zhang, I.~Goodfellow, D.~Metaxas, and A.~Odena.
\newblock Self-attention generative adversarial networks.
\newblock In \emph{ICML}, 2018.

\bibitem[Zhang et~al.(2020)Zhang, Zhang, Odena, and Lee]{zhang2020cr}
H.~Zhang, Z.~Zhang, A.~Odena, and H.~Lee.
\newblock Consistency regularization for generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Zhao et~al.(2020{\natexlab{a}})Zhao, Cong, and
  Carin]{zhao2020leveraging}
M.~Zhao, Y.~Cong, and L.~Carin.
\newblock On leveraging pretrained gans for limited-data generation.
\newblock In \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Zhao et~al.(2020{\natexlab{b}})Zhao, Liu, Lin, Zhu, and
  Han]{zhao2020diffaug}
S.~Zhao, Z.~Liu, J.~Lin, J.-Y. Zhu, and S.~Han.
\newblock Differentiable augmentation for data-efficient gan training.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2020{\natexlab{b}}.

\bibitem[Zhao et~al.(2021)Zhao, Singh, Lee, Zhang, Odena, and
  Zhang]{zhao2020icr}
Z.~Zhao, S.~Singh, H.~Lee, Z.~Zhang, A.~Odena, and H.~Zhang.
\newblock Improved consistency regularization for gans.
\newblock In \emph{Association for the Advancement of Artificial Intelligence},
  2021.

\bibitem[Zhu et~al.(2017)Zhu, Park, Isola, and Efros]{CycleGAN2017}
J.-Y. Zhu, T.~Park, P.~Isola, and A.~A. Efros.
\newblock Unpaired image-to-image translation using cycle-consistent
  adversarial networks.
\newblock In \emph{Computer Vision (ICCV), 2017 IEEE International Conference
  on}, 2017.

\bibitem[Zhuang et~al.(2021)Zhuang, Koyejo, and Schwing]{ZhuangICLR2021}
P.~Zhuang, O.~Koyejo, and A.~G. Schwing.
\newblock {Enjoy Your Editing: Controllable GANs for Image Editing via Latent
  Space Navigation}.
\newblock In \emph{Proc. ICLR}, 2021.

\end{thebibliography}
