\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Auer \& Ortner(2006)Auer and Ortner]{AO06}
Auer, P. and Ortner, R.
\newblock Logarithmic online regret bounds for undiscounted reinforcement
  learning.
\newblock In \emph{NIPS}, 2006.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{Azar12}
Azar, M., Munos, R., and Kappen, H.~J.
\newblock On the sample complexity of reinforcement learning with a generative
  model.
\newblock In \emph{ICML}, 2012.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{Azar17}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{ICML}, 2017.

\bibitem[Bartlett \& Tewari(2009)Bartlett and Tewari]{Bartlett09}
Bartlett, P.~L. and Tewari, A.
\newblock Regal: A regularization based algorithm for reinforcement learning in
  weakly communicating mdps.
\newblock In \emph{Proceedings of the 25th Conference on Uncertainty in
  Artificial Intelligence}, 2009.

\bibitem[Bubeck \& Cesa-Bianchi(2012)Bubeck and Cesa-Bianchi]{BC12}
Bubeck, S. and Cesa-Bianchi, N.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{Foundations and Trends in Machine Learning}, 2012.

\bibitem[Dann \& Brunskill(2015)Dann and Brunskill]{Dann15}
Dann, C. and Brunskill, E.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{NIPS}, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{Dann17}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{NIPS}, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Dann, C., Li, L., Wei, W., and Brunskill, E.
\newblock Policy certificates: Towards accountable reinforcement learning.
\newblock In \emph{ICML}, 2019.

\bibitem[{Even-Dar} et~al.(2006){Even-Dar}, Mannor, and Mansour]{EMM06}
{Even-Dar}, E., Mannor, S., and Mansour, Y.
\newblock Action elimination and stopping conditions for the multi-armed bandit
  and reinforcement learning problems.
\newblock \emph{Journal of Machine Learning Research}, 2006.

\bibitem[Fruit et~al.(2018)Fruit, Pirotta, Lazaric, and Ortner]{Fruit18}
Fruit, R., Pirotta, M., Lazaric, A., and Ortner, R.
\newblock Efficient bias-span-constrained exploration-exploitation in
  reinforcement learning.
\newblock https://arxiv.org/abs/1802.04020, 2018.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{Jaksch10}
Jaksch, T., Ortner, R., and Auer, P.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 2010.

\bibitem[Jiang \& Agarwal(2018)Jiang and Agarwal]{jiang2018open}
Jiang, N. and Agarwal, A.
\newblock Open problem: The dependence of sample complexity lower bounds on
  planning horizon.
\newblock In \emph{Conference On Learning Theory}, pp.\  3395--3398, 2018.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langforda, and
  Schapire]{JKAL17}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langforda, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{ICML}, 2017.

\bibitem[Kakade et~al.(2018)Kakade, Wang, and Yang]{KWY18}
Kakade, S., Wang, M., and Yang, L.~F.
\newblock Variance reduction methods for sublinear reinforcement learning.
\newblock \emph{Arxiv}, 2018.

\bibitem[Lattimore \& Hutter(2014)Lattimore and Hutter]{LH14}
Lattimore, T. and Hutter, M.
\newblock Near-optimal pac bounds for discounted mdps.
\newblock In \emph{Theoretical Computer Science}, 2014.

\bibitem[Maillard et~al.(2014)Maillard, Mann, and Mannor]{Maillard14}
Maillard, O.-A., Mann, T.~A., and Mannor, S.
\newblock ``how hard is my mdp?'' the distribution-norm to the rescue.
\newblock In \emph{NIPS}, 2014.

\bibitem[Maurer \& Pontil(2009)Maurer and Pontil]{MP09}
Maurer, A. and Pontil, M.
\newblock Empirical bernstein bounds and sample variance penalization.
\newblock In \emph{COLT}, 2009.

\bibitem[Osband \& Roy(2014)Osband and Roy]{OV14}
Osband, I. and Roy, B.~V.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In \emph{NIPS}, 2014.

\bibitem[Osband \& Roy(2017)Osband and Roy]{OV17}
Osband, I. and Roy, B.~V.
\newblock Why is posterior sampling better than optimism for reinforcement
  learning?
\newblock In \emph{ICML}, 2017.

\bibitem[Osband \& {Van Roy}(2016)Osband and {Van Roy}]{OV16}
Osband, I. and {Van Roy}, B.
\newblock On lower bounds for regret in reinforcement learning.
\newblock In \emph{Arxiv}, 2016.
\newblock URL \url{https://arxiv.org/pdf/1608.02732.pdf}.
\newblock https://arxiv.org/pdf/1608.02732.pdf.

\bibitem[Osband et~al.(2013)Osband, {Van Roy}, and Russo]{OVR13}
Osband, I., {Van Roy}, B., and Russo, D.
\newblock (more) efficient reinforcement learning via posterior sampling.
\newblock In \emph{NIPS}, 2013.

\bibitem[Sutton \& Barto(1998)Sutton and Barto]{SB98}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement Learning: An Introduction}.
\newblock MIT Press, 1998.

\bibitem[Talebi \& Maillard(2018)Talebi and Maillard]{TM18}
Talebi, M. and Maillard, O.
\newblock Variance-aware regret bounds for undiscounted reinforcement learning
  in mdps.
\newblock In \emph{ALT}, 2018.

\bibitem[Weissman et~al.(2003)Weissman, Ordentlich, Seroussi, Verdu, and
  Weinberger]{Weissman03}
Weissman, T., Ordentlich, E., Seroussi, G., Verdu, S., and Weinberger, M.~J.
\newblock Inequalities for the l1 deviation of the empirical distribution.
\newblock Technical report, Hewlett-Packard Labs, 2003.

\bibitem[Wen \& {Van Roy}(2013)Wen and {Van Roy}]{WR13}
Wen, Z. and {Van Roy}, B.
\newblock Efficient exploration and value function generalization in
  deterministic systems.
\newblock In \emph{NIPS}, 2013.

\bibitem[Zanette \& Brunskill(2018)Zanette and Brunskill]{Zanette18a}
Zanette, A. and Brunskill, E.
\newblock Problem dependent reinforcement learning bounds which can identify
  bandit structure in mdps.
\newblock In \emph{ICML}, 2018.

\end{thebibliography}
