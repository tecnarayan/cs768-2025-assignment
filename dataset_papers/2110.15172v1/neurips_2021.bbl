\begin{thebibliography}{}

\bibitem[Andrade-Pacheco et~al., 2020]{andrade2020finding}
Andrade-Pacheco, R., Rerolle, F., Lemoine, J., Hernandez, L., Me{\"\i}t{\'e},
  A., Juziwelo, L., Bibaut, A.~F., van~der Laan, M.~J., Arnold, B.~F., and
  Sturrock, H.~J. (2020).
\newblock Finding hotspots: development of an adaptive spatial sampling
  approach.
\newblock {\em Scientific reports}, 10(1):1--12.

\bibitem[Balandat et~al., 2020]{balandat_botorch_2020}
Balandat, M., Karrer, B., Jiang, D., Daulton, S., Letham, B., Wilson, A.~G.,
  and Bakshy, E. (2020).
\newblock {BoTorch}: {A} {Framework} for {Efficient} {Monte}-{Carlo} {Bayesian}
  {Optimization}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33.

\bibitem[Bauer et~al., 2016]{bauer2016understanding}
Bauer, M., Van Der~Wilk, M., and Rasmussen, C. (2016).
\newblock Understanding probabilistic sparse gaussian process approximations.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~29, pages 1533--1541.

\bibitem[Bertsekas, 2020]{bertsekas2020rollout}
Bertsekas, D.~P. (2020).
\newblock {\em Rollout, Policy Iteration, and Distributed Reinforcement
  Learning}.
\newblock Athena Scientific.

\bibitem[Bijl et~al., 2016]{bijl2016online}
Bijl, H., Sch{\"o}n, T.~B., van Wingerden, J.-W., and Verhaegen, M. (2016).
\newblock Online sparse gaussian process training with input noise.
\newblock {\em arXiv preprint arXiv 1601.08068}.

\bibitem[Boedecker et~al., 2014]{boedecker2014approximate}
Boedecker, J., Springenberg, J.~T., W{\"u}lfing, J., and Riedmiller, M. (2014).
\newblock Approximate real-time optimal control based on sparse gaussian
  process models.
\newblock In {\em 2014 IEEE Symposium on Adaptive Dynamic Programming and
  Reinforcement Learning (ADPRL)}, pages 1--8. IEEE.

\bibitem[Brockman et~al., 2016]{brockman2016openai}
Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang,
  J., and Zaremba, W. (2016).
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}.

\bibitem[Broderick et~al., 2013]{broderick2013streaming}
Broderick, T., Boyd, N., Wibisono, A., Wilson, A.~C., and Jordan, M.~I. (2013).
\newblock Streaming variational bayes.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~26.

\bibitem[Bui et~al., 2017a]{bui_streaming_2017}
Bui, T.~D., Nguyen, C.~V., and Turner, R.~E. (2017a).
\newblock Streaming sparse {Gaussian} process approximations.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~31, pages 3301--3309, Long Beach, California, USA. Curran Associates
  Inc.

\bibitem[Bui et~al., 2017b]{bui2017unifying}
Bui, T.~D., Yan, J., and Turner, R.~E. (2017b).
\newblock A unifying framework for gaussian process pseudo-point approximations
  using power expectation propagation.
\newblock {\em The Journal of Machine Learning Research}, 18(1):3649--3720.

\bibitem[Burt et~al., 2019]{burt_convergence_19}
Burt, D., Rasmussen, C.~E., and Van Der~Wilk, M. (2019).
\newblock Rates of convergence for sparse variational {G}aussian process
  regression.
\newblock In {\em Proceedings of the 36th International Conference on Machine
  Learning}, volume~97, pages 862--871. PMLR.

\bibitem[Canu and Smola, 2006]{CANU2006714}
Canu, S. and Smola, A. (2006).
\newblock Kernel methods and the exponential family.
\newblock {\em Neurocomputing}, 69(7):714--720.
\newblock New Issues in Neurocomputing: 13th European Symposium on Artificial
  Neural Networks.

\bibitem[Cheng and Boots, 2016]{cheng_incremental_2016}
Cheng, C.-A. and Boots, B. (2016).
\newblock Incremental variational sparse {Gaussian} process regression.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~30, pages 4410--4418, Barcelona, Spain. Curran Associates Inc.

\bibitem[Chowdhary et~al., 2014]{chowdhary2014bayesian}
Chowdhary, G., Kingravi, H.~A., How, J.~P., and Vela, P.~A. (2014).
\newblock Bayesian nonparametric adaptive control using gaussian processes.
\newblock {\em IEEE transactions on neural networks and learning systems},
  26(3):537--550.

\bibitem[Chu and Ghahramani, 2005]{chu2005preference}
Chu, W. and Ghahramani, Z. (2005).
\newblock Preference learning with gaussian processes.
\newblock In {\em Proceedings of the 22nd International Conference on Machine
  learning}, pages 137--144.

\bibitem[Csat{\'o} and Opper, 2002]{csato2002sparse}
Csat{\'o}, L. and Opper, M. (2002).
\newblock Sparse on-line gaussian processes.
\newblock {\em Neural computation}, 14(3):641--668.

\bibitem[Cutajar et~al., 2019]{cutajar2019deep}
Cutajar, K., Pullin, M., Damianou, A., Lawrence, N., and Gonz{\'a}lez, J.
  (2019).
\newblock Deep gaussian processes for multi-fidelity modeling.
\newblock {\em arXiv preprint arXiv:1903.07320}.

\bibitem[Deisenroth and Rasmussen, 2011]{deisenroth2011pilco}
Deisenroth, M. and Rasmussen, C.~E. (2011).
\newblock Pilco: A model-based and data-efficient approach to policy search.
\newblock In {\em Proceedings of the 28th International Conference on Machine
  Learning}, pages 465--472.

\bibitem[Dua and Graff, 2017]{Dua:2019}
Dua, D. and Graff, C. (2017).
\newblock {UCI} machine learning repository.

\bibitem[Duris et~al., 2020]{duris2020bayesian}
Duris, J., Kennedy, D., Hanuka, A., Shtalenkova, J., Edelen, A., Baxevanis, P.,
  Egger, A., Cope, T., McIntire, M., Ermon, S., et~al. (2020).
\newblock Bayesian optimization of a free-electron laser.
\newblock {\em Physical review letters}, 124(12):124801.

\bibitem[Eriksson and Jankowiak, 2021]{eriksson2021high}
Eriksson, D. and Jankowiak, M. (2021).
\newblock High-dimensional bayesian optimization with sparse axis-aligned
  subspaces.
\newblock {\em arXiv preprint arXiv:2103.00349}.

\bibitem[Eriksson et~al., 2019]{eriksson2019scalable}
Eriksson, D., Pearce, M., Gardner, J., Turner, R.~D., and Poloczek, M. (2019).
\newblock Scalable global optimization via local bayesian optimization.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Fine and Scheinberg, 2001]{fine2001efficient}
Fine, S. and Scheinberg, K. (2001).
\newblock Efficient svm training using low-rank kernel representations.
\newblock {\em Journal of Machine Learning Research}, 2(Dec):243--264.

\bibitem[Frazier, 2018]{frazier2018tutorial}
Frazier, P.~I. (2018).
\newblock A tutorial on bayesian optimization.
\newblock {\em arXiv preprint arXiv:1807.02811}.

\bibitem[Gardner et~al., 2018]{gardner_gpytorch:_2018}
Gardner, J.~R., Pleiss, G., Bindel, D., Weinberger, K.~Q., and Wilson, A.~G.
  (2018).
\newblock {GPyTorch}: {Blackbox} {Matrix}-{Matrix} {Gaussian} {Process}
  {Inference} with {GPU} {Acceleration}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~31.

\bibitem[Ginsbourger et~al., 2010]{ginsbourger2010kriging}
Ginsbourger, D., Le~Riche, R., and Carraro, L. (2010).
\newblock Kriging is well-suited to parallelize optimization.
\newblock In {\em Computational intelligence in expensive optimization
  problems}, pages 131--162. Springer.

\bibitem[Girard et~al., 2002]{girard2002gaussian}
Girard, A., Rasmussen, C.~E., Candela, J.~Q., and Murray-Smith, R. (2002).
\newblock Gaussian process priors with uncertain inputs-application to
  multiple-step ahead time series forecasting.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~15.

\bibitem[G{\'o}mez-Bombarelli et~al., 2018]{gomez2018automatic}
G{\'o}mez-Bombarelli, R., Wei, J.~N., Duvenaud, D., Hern{\'a}ndez-Lobato,
  J.~M., S{\'a}nchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J.,
  Hirzel, T.~D., Adams, R.~P., and Aspuru-Guzik, A. (2018).
\newblock Automatic chemical design using a data-driven continuous
  representation of molecules.
\newblock {\em ACS central science}, 4(2):268--276.

\bibitem[Groot et~al., 2011]{groot2011multiple}
Groot, P., Lucas, P., and Bosch, P. (2011).
\newblock Multiple-step time series forecasting with sparse gaussian processes.
\newblock In {\em Causmaecker, P. De (ed.), Proceedings of the 23rd Benelux
  Conference on Artificial Intelligence}, pages 1--8. [Sl: sn].

\bibitem[Hagan et~al., 2002]{hagan2002managing}
Hagan, P.~S., Kumar, D., Lesniewski, A.~S., and Woodward, D.~E. (2002).
\newblock Managing smile risk.
\newblock {\em The Best of Wilmott}, 1:249--296.

\bibitem[Hebbal et~al., 2021]{hebbal2021bayesian}
Hebbal, A., Brevault, L., Balesdent, M., Talbi, E.-G., and Melab, N. (2021).
\newblock Bayesian optimization using deep gaussian processes with applications
  to aerospace system design.
\newblock {\em Optimization and Engineering}, 22(1):321--361.

\bibitem[Hennig and Schuler, 2012]{hennig2012entropy}
Hennig, P. and Schuler, C.~J. (2012).
\newblock Entropy search for information-efficient global optimization.
\newblock {\em Journal of Machine Learning Research}, 13(6).

\bibitem[Hensman et~al., 2013]{hensman_bigdata_gp}
Hensman, J., Fusi, N., and Lawrence, N.~D. (2013).
\newblock Gaussian processes for big data.
\newblock In {\em Proceedings of the Twenty-Ninth Conference on Uncertainty in
  Artificial Intelligence}, UAI'13, page 282â€“290, Arlington, Virginia, USA.
  AUAI Press.

\bibitem[Hensman et~al., 2015]{hensman15}
Hensman, J., Matthews, A., and Ghahramani, Z. (2015).
\newblock {Scalable Variational Gaussian Process Classification}.
\newblock In {\em Proceedings of the Eighteenth International Conference on
  Artificial Intelligence and Statistics}, pages 351--360, San Diego,
  California, USA. PMLR.

\bibitem[Hoang et~al., 2015]{hoang2015unifying}
Hoang, T.~N., Hoang, Q.~M., and Low, B. K.~H. (2015).
\newblock A unifying framework of anytime sparse gaussian process regression
  models with stochastic variational inference for big data.
\newblock In {\em International Conference on Machine Learning}, volume~37,
  pages 569--578. PMLR.

\bibitem[Huber, 2013]{huber2013recursive}
Huber, M.~F. (2013).
\newblock Recursive gaussian process regression.
\newblock In {\em 2013 IEEE International Conference on Acoustics, Speech and
  Signal Processing}, pages 3362--3366. IEEE.

\bibitem[Huber, 2014]{huber2014recursive}
Huber, M.~F. (2014).
\newblock Recursive gaussian process: On-line regression and learning.
\newblock {\em Pattern Recognition Letters}, 45:85--91.

\bibitem[Jankowiak et~al., 2020]{jankowiak2020parametric}
Jankowiak, M., Pleiss, G., and Gardner, J. (2020).
\newblock Parametric gaussian process regressors.
\newblock In {\em International Conference on Machine Learning}, volume 119,
  pages 4702--4712. PMLR.

\bibitem[Jiang et~al., 2020]{jiang_efficient_2020}
Jiang, S., Jiang, D., Balandat, M., Karrer, B., Gardner, J., and Garnett, R.
  (2020).
\newblock Efficient {Nonmyopic} {Bayesian} {Optimization} via {One}-{Shot}
  {Multi}-{Step} {Trees}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~33.

\bibitem[Kapoor et~al., 2020]{kapoor2020variational}
Kapoor, S., Karaletsos, T., and Bui, T.~D. (2020).
\newblock Variational auto-regressive gaussian processes for continual
  learning.
\newblock {\em arXiv preprint arXiv:2006.05468}.

\bibitem[Khan and Lin, 2017]{khan2017conjugate}
Khan, M. and Lin, W. (2017).
\newblock Conjugate-computation variational inference: Converting variational
  inference in non-conjugate models to inferences in conjugate models.
\newblock In {\em Artificial Intelligence and Statistics}, pages 878--887.
  PMLR.

\bibitem[Knudde et~al., 2017]{GPflowOpt2017}
Knudde, N., {van der Herten}, J., Dhaene, T., and Couckuyt, I. (2017).
\newblock {{GP}flow{O}pt: {A} {B}ayesian {O}ptimization {L}ibrary using
  Tensor{F}low}.
\newblock {\em arXiv preprint arXiv:1711.03845}.

\bibitem[Krityakierne and Ginsbourger, 2015]{krityakierne2015global}
Krityakierne, T. and Ginsbourger, D. (2015).
\newblock Global optimization with sparse and local gaussian process models.
\newblock In {\em International Workshop on Machine Learning, Optimization and
  Big Data}, pages 185--196. Springer.

\bibitem[L{\'a}zaro-Gredilla et~al., 2010]{lazaro2010sparse}
L{\'a}zaro-Gredilla, M., Quinonero-Candela, J., Rasmussen, C.~E., and
  Figueiras-Vidal, A.~R. (2010).
\newblock Sparse spectrum gaussian process regression.
\newblock {\em The Journal of Machine Learning Research}, 11:1865--1881.

\bibitem[Letham et~al., 2019]{letham2019constrained}
Letham, B., Karrer, B., Ottoni, G., Bakshy, E., et~al. (2019).
\newblock Constrained bayesian optimization with noisy experiments.
\newblock {\em Bayesian Analysis}, 14(2):495--519.

\bibitem[Lin et~al., 2020]{lin2020mdmaking}
Lin, J., Obeng, A., and Bakshy, E. (2020).
\newblock Preference learning for real-world multi-objective decision making.
\newblock {\em ICML 2020 Workshop on Real World Experiment Design and Active
  Learning}.

\bibitem[Ling et~al., 2016]{ling2016gaussian}
Ling, C.~K., Low, K.~H., and Jaillet, P. (2016).
\newblock Gaussian process planning with lipschitz continuous reward functions:
  Towards unifying bayesian optimization, active learning, and beyond.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~30.

\bibitem[MacKenzie and Spears, 2014]{mackenzie2014formula}
MacKenzie, D. and Spears, T. (2014).
\newblock â€˜the formula that killed wall streetâ€™: The gaussian copula and
  modelling practices in investment banking.
\newblock {\em Social Studies of Science}, 44(3):393--417.

\bibitem[Mania et~al., 2018]{mania2018simple}
Mania, H., Guy, A., and Recht, B. (2018).
\newblock Simple random search of static linear policies is competitive for
  reinforcement learning.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~31, pages 1805--1814.

\bibitem[Matthews, 2017]{matthews2017scalable}
Matthews, A. G. d.~G. (2017).
\newblock {\em Scalable Gaussian process inference using variational methods}.
\newblock PhD thesis, University of Cambridge.

\bibitem[McIntire et~al., 2016]{mcintire2016sparse}
McIntire, M., Ratner, D., and Ermon, S. (2016).
\newblock Sparse gaussian processes for bayesian optimization.
\newblock In {\em Uncertainty in Artifical Intelligence}, volume~32.
  Association for Uncertainty in Artifical Intelligence (AUAI).

\bibitem[Moreno-Mu{\~n}oz et~al., 2019]{moreno2019continual}
Moreno-Mu{\~n}oz, P., Art{\'e}s-Rodr{\'\i}guez, A., and {\'A}lvarez, M.~A.
  (2019).
\newblock Continual multi-task gaussian processes.
\newblock {\em arXiv preprint arXiv:1911.00002}.

\bibitem[Moreno-Mu{\~n}oz et~al., 2020]{moreno2020recyclable}
Moreno-Mu{\~n}oz, P., Art{\'e}s-Rodr{\'\i}guez, A., and {\'A}lvarez, M.~A.
  (2020).
\newblock Recyclable gaussian processes.
\newblock {\em arXiv preprint arXiv:2010.02554}.

\bibitem[Moss et~al., 2021]{moss2021gibbon}
Moss, H.~B., Leslie, D.~S., Gonzalez, J., and Rayson, P. (2021).
\newblock Gibbon: General-purpose information-based bayesian optimisation.
\newblock {\em arXiv preprint arXiv:2102.03324}.

\bibitem[Nickson et~al., 2014]{nickson2014automated}
Nickson, T., Osborne, M.~A., Reece, S., and Roberts, S. (2014).
\newblock Automated machine learning using stochastic algorithm tuning.
\newblock In {\em NIPS Workshop on Bayesian Optimization}.

\bibitem[Opper, 1998]{opper1998bayesian}
Opper, M. (1998).
\newblock A bayesian approach to online learning.
\newblock {\em On-line learning in neural networks}, pages 363--378.

\bibitem[Opper and Archambeau, 2009]{opper2009variational}
Opper, M. and Archambeau, C. (2009).
\newblock The variational gaussian approximation revisited.
\newblock {\em Neural computation}, 21(3):786--792.

\bibitem[Osborne, 2010]{osborne2010bayesian}
Osborne, M.~A. (2010).
\newblock {\em Bayesian Gaussian processes for sequential prediction,
  optimisation and quadrature}.
\newblock PhD thesis, Oxford University, UK.

\bibitem[Pan et~al., 2020]{pan2020continual}
Pan, P., Swaroop, S., Immer, A., Eschenhagen, R., Turner, R.~E., and Khan,
  M.~E. (2020).
\newblock Continual deep learning by functional regularisation of memorable
  past.
\newblock {\em arXiv preprint arXiv:2004.14070}.

\bibitem[Pan et~al., 2017]{pan2017prediction}
Pan, Y., Yan, X., Theodorou, E.~A., and Boots, B. (2017).
\newblock Prediction under uncertainty in sparse spectrum gaussian processes
  with applications to filtering and control.
\newblock In {\em International Conference on Machine Learning}, volume~70,
  pages 2760--2768. PMLR.

\bibitem[Panos et~al., 2018]{panos2018fully}
Panos, A., Dellaportas, P., and Titsias, M.~K. (2018).
\newblock Fully scalable gaussian processes using subspace inducing inputs.
\newblock {\em arXiv preprint arXiv:1807.02537}.

\bibitem[Paszke et~al., 2019]{paske2019pytorch}
Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
  T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E.,
  DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,
  Bai, J., and Chintala, S. (2019).
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~32. Curran Associates, Inc.

\bibitem[Pearlmutter, 1994]{pearlmutter1994fast}
Pearlmutter, B.~A. (1994).
\newblock Fast exact multiplication by the hessian.
\newblock {\em Neural computation}, 6(1):147--160.

\bibitem[Pleiss et~al., 2018]{pleiss_constant-time_2018}
Pleiss, G., Gardner, J.~R., Weinberger, K.~Q., and Wilson, A.~G. (2018).
\newblock Constant-{Time} {Predictive} {Distributions} for {Gaussian}
  {Processes}.
\newblock In {\em Artificial {Intelligence} and {Statistics}}, volume~84. PMLR.

\bibitem[Rasmussen and Williams, 2008]{rasmussen_gaussian_2008}
Rasmussen, C.~E. and Williams, C. K.~I. (2008).
\newblock {\em Gaussian processes for machine learning}.
\newblock Adaptive computation and machine learning. MIT Press, Cambridge,
  Mass., 3. print edition.

\bibitem[S{\ae}mundsson et~al., 2018]{saemundsson2018meta}
S{\ae}mundsson, S., Hofmann, K., and Deisenroth, M. (2018).
\newblock Meta reinforcement learning with latent variable gaussian processes.
\newblock In {\em 34th Conference on Uncertainty in Artificial Intelligence},
  volume~34, pages 642--652. Association for Uncertainty in Artificial
  Intelligence (AUAI).

\bibitem[Seeger et~al., 2003]{seeger2003fast}
Seeger, M.~W., Williams, C.~K., and Lawrence, N.~D. (2003).
\newblock Fast forward selection to speed up sparse gaussian process
  regression.
\newblock In {\em International Workshop on Artificial Intelligence and
  Statistics}, pages 254--261. PMLR.

\bibitem[Seo et~al., 2000]{seo2000gaussian}
Seo, S., Wallat, M., Graepel, T., and Obermayer, K. (2000).
\newblock Gaussian process regression: Active data selection and test point
  rejection.
\newblock In {\em Neural Networks, IEEE-INNS-ENNS International Joint
  Conference on}, volume~3, pages 3241--3241.

\bibitem[Shi et~al., 2020]{shi2020sparse}
Shi, J., Titsias, M., and Mnih, A. (2020).
\newblock Sparse orthogonal variational inference for gaussian processes.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, volume 108, pages 1932--1942. PMLR.

\bibitem[Stanton et~al., 2021]{stanton_kernel_2021}
Stanton, S., Maddox, W.~J., Delbridge, I., and Wilson, A.~G. (2021).
\newblock Kernel {Interpolation} for {Scalable} {Online} {Gaussian}
  {Processes}.
\newblock In {\em Artificial {Intelligence} and {Statistics}}, volume 130.
  PMLR.

\bibitem[Thompson, 1933]{thompson1933likelihood}
Thompson, W.~R. (1933).
\newblock On the likelihood that one unknown probability exceeds another in
  view of the evidence of two samples.
\newblock {\em Biometrika}, 25(3/4):285--294.

\bibitem[Titsias, 2008]{titsias_variational_nodate}
Titsias, M. (2008).
\newblock Variational {Model} {Selection} for {Sparse} {Gaussian} {Process}
  {Regression}.
\newblock {\em www.aueb.gr/users/mtitsias/papers/sparseGPv2.pdf}, page~20.

\bibitem[Titsias, 2009]{titsias_variational_2009}
Titsias, M. (2009).
\newblock Variational {Learning} of {Inducing} {Variables} in {Sparse}
  {Gaussian} {Processes}.
\newblock In {\em Artificial {Intelligence} and {Statistics}}, pages 567--574.
  PMLR.
\newblock ISSN: 1938-7228.

\bibitem[Titsias et~al., 2019]{titsias2019functional}
Titsias, M.~K., Schwarz, J., Matthews, A. G. d.~G., Pascanu, R., and Teh, Y.~W.
  (2019).
\newblock Functional regularisation for continual learning with gaussian
  processes.
\newblock In {\em International Conference on Learning Representations}.

\bibitem[Todorov et~al., 2012]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y. (2012).
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE.

\bibitem[Tripp et~al., 2020]{tripp2020sample}
Tripp, A., Daxberger, E., and Hern{\'a}ndez-Lobato, J.~M. (2020).
\newblock Sample-efficient optimization in the latent space of deep generative
  models via weighted retraining.
\newblock {\em Advances in Neural Information Processing Systems}, 33.

\bibitem[Van~der Wilk, 2019]{van2019sparse}
Van~der Wilk, M. (2019).
\newblock {\em Sparse Gaussian process approximations and applications}.
\newblock PhD thesis, University of Cambridge.

\bibitem[Wang et~al., 2020]{wang2020learning}
Wang, L., Fonseca, R., and Tian, Y. (2020).
\newblock Learning search space partition for black-box optimization using
  monte carlo tree search.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33.

\bibitem[Wang et~al., 2018]{wang2018batched}
Wang, Z., Gehring, C., Kohli, P., and Jegelka, S. (2018).
\newblock Batched large-scale bayesian optimization in high-dimensional spaces.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, volume~84, pages 745--754. PMLR.

\bibitem[Wang and Jegelka, 2017]{wang2017max}
Wang, Z. and Jegelka, S. (2017).
\newblock Max-value entropy search for efficient bayesian optimization.
\newblock In {\em International Conference on Machine Learning}, volume~70,
  pages 3627--3635. PMLR.

\bibitem[Weiss et~al., 2019]{weiss2019mapping}
Weiss, D.~J., Lucas, T.~C., Nguyen, M., Nandi, A.~K., Bisanzio, D., Battle,
  K.~E., Cameron, E., Twohig, K.~A., Pfeffer, D.~A., Rozier, J.~A., et~al.
  (2019).
\newblock Mapping the global prevalence, incidence, and mortality of plasmodium
  falciparum, 2000--17: a spatial and temporal modelling study.
\newblock {\em The Lancet}, 394(10195):322--331.

\bibitem[Wilson and Ghahramani, 2010]{wilson_copula_2010}
Wilson, A. and Ghahramani, Z. (2010).
\newblock Copula {Processes}.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~23, page~9.

\bibitem[Wilson and Nickisch, 2015]{wilson_kernel_2015}
Wilson, A. and Nickisch, H. (2015).
\newblock Kernel {Interpolation} for {Scalable} {Structured} {Gaussian}
  {Processes} ({KISS}-{GP}).
\newblock In {\em International {Conference} on {Machine} {Learning}}, pages
  1775--1784.
\newblock ISSN: 1938-7228 Section: Machine Learning.

\bibitem[Wu and Frazier, 2016]{wu_parallel_2016}
Wu, J. and Frazier, P.~I. (2016).
\newblock The parallel knowledge gradient method for batch {Bayesian}
  optimization.
\newblock In {\em Advances in {Neural} {Information} {Processing} {Systems}},
  volume~30 of {\em {NIPS}'16}, pages 3134--3142, Red Hook, NY, USA. Curran
  Associates Inc.

\bibitem[Xu et~al., 2014]{xu2014gp}
Xu, N., Low, K.~H., Chen, J., Lim, K.~K., and Ozgul, E. (2014).
\newblock Gp-localize: Persistent mobile robot localization using online sparse
  gaussian process observation model.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~28.

\end{thebibliography}
