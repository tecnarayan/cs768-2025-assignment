@article{foster_structure_2002,
	title = {Structure in the space of value functions},
	volume = {49},
	number = {2-3},
	journal = {Machine Learning},
	author = {Foster, David and Dayan, Peter},
	year = {2002},
	pages = {325--346}
}
@misc{krakovna2018specification,
  title={Specification gaming examples in {AI}},
  author={Krakovna, Victoria},
  year={2018}
}
@Book{sattarov2019power,
 author = {Sattarov, Faridun},
 title = {Power and technology : a philosophical and ethical analysis},
 publisher = {Rowman \& Littlefield International, Ltd},
 year = {2019},
 address = {London Lanham, Maryland},
 isbn = {978-1786611291}
 }
 @article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}
@inproceedings{loaiza2019continuous,
  title={The continuous {Bernoulli}: fixing a pervasive error in variational autoencoders},
  author={Loaiza-Ganem, Gabriel and Cunningham, John P},
  booktitle={Advances in Neural Information Processing Systems},
  pages={13266--13276},
  year={2019}
}


@book{brown2014frame,
  title={The Frame Problem in Artificial Intelligence: Proceedings of the 1987 Workshop},
  author={Brown, Frank M},
  year={2014},
  publisher={Morgan Kaufmann}
}

@article{gardner1970fantastic,
  title={The Fantastic Combinations of {John} {Conway}'s New Solitaire Game `Life'.},
  author={Gardner, Martin},
  journal={Scientific American},
  volume={223},
  number={4},
  pages={120--123},
  year={1970}
}
@incollection{rendell2002turing,
  title={Turing universality of the game of life},
  author={Rendell, Paul},
  booktitle={Collision-based computing},
  pages={513--539},
  year={2002},
  publisher={Springer}
}

@article{turner_optimal_2020,
	title = {Optimal Farsighted Agents Tend to Seek Power},
	url = {},
	abstract = {Some researchers have speculated that capable reinforcement learning (RL) agents pursuing misspeciﬁed objectives are often incentivized to seek resources and power in pursuit of those objectives. An agent seeking power is incentivized to behave in undesirable ways, including rationally preventing deactivation and correction. Others have voiced skepticism: humans seem idiosyncratic in their urges to power, which need not be present in the agents we design. We formalize a notion of power within the context of ﬁnite Markov decision processes (MDPs). With respect to a neutral class of reward function distributions, our results suggest that farsighted optimal policies tend to seek power over the environment.},
	language = {en},
	urldate = {2020-04-28},
	journal = {arXiv:1912.01683},
	author = {Turner, Alexander Matt and Smith, Logan and Shah, Rohin and Tadepalli, Prasad},
	month = apr,
	year = {2020},
	note = {},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: 8 pages, 25 pages with appendices},
	file = {Turner et al. - 2020 - Optimal Farsighted Agents Tend to Seek Power.pdf:C\:\\Users\\turne\\Zotero\\storage\\ADCKFAMI\\Turner et al. - 2020 - Optimal Farsighted Agents Tend to Seek Power.pdf:application/pdf}
}

@article{pinker_thinking_2015,
	title = {Thinking does not imply subjugating},
	journal = {What to Think about Machines that Think; Brockman, J., Ed},
	author = {Pinker, Steven},
	year = {2015},
	pages = {5--8}
}

@book{sutton_reinforcement_1998,
	title = {Reinforcement learning: an introduction},
	isbn = {978-0-262-19398-6},
	shorttitle = {Reinforcement learning},
	language = {en},
	publisher = {MIT Press},
	author = {Sutton, Richard S. and Barto, Andrew G.},
	year = {1998},
	keywords = {Reinforcement learning}
}

@inproceedings{wang_stable_2008,
	title = {Stable dual dynamic programming},
	booktitle = {Advances in neural information processing systems},
	author = {Wang, Tao and Bowling, Michael and Schuurmans, Dale and Lizotte, Daniel J},
	year = {2008},
	pages = {1569--1576}
}

@inproceedings{dadashi_value_2019,
	title = {The Value Function Polytope in Reinforcement Learning},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Dadashi, Robert and Bellemare, Marc G and Taiga, Adrien Ali and Le Roux, Nicolas and Schuurmans, Dale},
	year = {2019},
	pages = {1486--1495}
}

@incollection{drummond_composing_1998,
	address = {Berlin, Heidelberg},
	title = {Composing functions to speed up reinforcement learning in a changing world},
	volume = {1398},
	isbn = {978-3-540-64417-0 978-3-540-69781-7},
	url = {http://link.springer.com/10.1007/BFb0026708},
	abstract = {This paper presents a system that transfers the results of prior learning to speed up reinforcement learning in a changing world. Often, even when the change to the world is relatively small an extensive relearning effort is required. The new system exploits strong features in the multi-dimensional function produced by reinforcement learning. The features generate a partitioning of the state space. The partition is represented {\textasciitilde}s a graph. This is used to index and compose functions stored in a case base to form a close approximation to the solution of the new task. The experimental results investigate one important example of a changing world, a new goal position. In this situation, there is close to a two orders of magnitude increase in learning rate over using a basic reinforcement learning algorithm.},
	language = {en},
	urldate = {2019-07-20},
	booktitle = {Machine {Learning}: {ECML}-98},
	publisher = {Springer Berlin Heidelberg},
	author = {Drummond, Chris},
	year = {1998},
	doi = {10.1007/BFb0026708},
	pages = {370--381}
}


@inproceedings{sutton_horde:_2011,
	title = {Horde: {A} scalable real-time architecture for learning knowledge from unsupervised sensorimotor interaction},
	booktitle = {International {Conference} on {Autonomous} {Agents} and {Multiagent} {Systems}},
	author = {Sutton, Richard S and Modayil, Joseph and Delp, Michael and Degris, Thomas and Pilarski, Patrick M and White, Adam and Precup, Doina},
	year = {2011},
	pages = {761--768}
}


@inproceedings{schaul_universal_2015,
	title = {Universal value function approximators},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Schaul, Tom and Horgan, Daniel and Gregor, Karol and Silver, David},
	year = {2015},
	pages = {1312--1320}
}

@incollection{salge_empowermentintroduction_2014,
	title = {Empowerment–an introduction},
	booktitle = {Guided {Self}-{Organization}: {Inception}},
	publisher = {Springer},
	author = {Salge, Christoph and Glackin, Cornelius and Polani, Daniel},
	year = {2014},
	pages = {67--114}
}



@book{puterman_markov_2014,
	title = {Markov decision processes: {Discrete} stochastic dynamic programming},
	publisher = {John Wiley \& Sons},
	author = {Puterman, Martin L},
	year = {2014}
}

@article{bellemare_geometric_2019,
	title = {A Geometric Perspective on Optimal Representations for Reinforcement Learning},
	url = {http://arxiv.org/abs/1901.11530},
	abstract = {We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.},
	language = {en},
	urldate = {2019-07-18},
	journal = {arXiv:1901.11530 [cs, stat]},
	author = {Bellemare, Marc G. and Dabney, Will and Dadashi, Robert and Taiga, Adrien Ali and Castro, Pablo Samuel and Roux, Nicolas Le and Schuurmans, Dale and Lattimore, Tor and Lyle, Clare},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.11530},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning}
}


@inproceedings{chow2018-lyapunov,
  title={A {Lyapunov}-based approach to safe reinforcement learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8092--8101},
  year={2018}
}
@inproceedings{regan_robust_2011,
	title = {Robust online optimization of reward-uncertain {MDPs}},
	booktitle = {Twenty-{Second} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Regan, Kevin and Boutilier, Craig},
	year = {2011}
}

@inproceedings{johns_constructing_2007,
	title = {Constructing basis functions from directed graphs for value function approximation},
	isbn = {978-1-59593-793-3},
	url = {http://portal.acm.org/citation.cfm?doid=1273496.1273545},
	doi = {10.1145/1273496.1273545},
	abstract = {Basis functions derived from an undirected graph connecting nearby samples from a Markov decision process (MDP) have proven useful for approximating value functions. The success of this technique is attributed to the smoothness of the basis functions with respect to the state space geometry. This paper explores the properties of bases created from directed graphs which are a more natural ﬁt for expressing state connectivity. Digraphs capture the eﬀect of non-reversible MDPs whose value functions may not be smooth across adjacent states. We provide an analysis using the Dirichlet sum of the directed graph Laplacian to show how the smoothness of the basis functions is aﬀected by the graph’s invariant distribution. Experiments in discrete and continuous MDPs with nonreversible actions demonstrate a signiﬁcant improvement in the policies learned using directed graph bases.},
	language = {en},
	urldate = {2019-07-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {ACM Press},
	author = {Johns, Jeff and Mahadevan, Sridhar},
	year = {2007},
	pages = {385--392}
}


@inproceedings{regan_robust_2010,
	title = {Robust policy computation in reward-uncertain {MDPs} using nondominated policies},
	booktitle = {Twenty-{Fourth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Regan, Kevin and Boutilier, Craig},
	year = {2010}
}

@article{roijers_survey_2013,
	title = {A survey of multi-objective sequential decision-making},
	volume = {48},
	journal = {Journal of Artificial Intelligence Research},
	author = {Roijers, Diederik M and Vamplew, Peter and Whiteson, Shimon and Dazeley, Richard},
	year = {2013},
	pages = {67--113}
}


@inproceedings{wang_dual_2007,
	title = {Dual Representations for Dynamic Programming and Reinforcement Learning},
	isbn = {978-1-4244-0706-4},
	url = {http://ieeexplore.ieee.org/document/4220813/},
	doi = {10.1109/ADPRL.2007.368168},
	abstract = {We investigate the dual approach to dynamic programming and reinforcement learning, based on maintaining an explicit representation of stationary distributions as opposed to value functions. A signiﬁcant advantage of the dual approach is that it allows one to exploit well developed techniques for representing, approximating and estimating probability distributions, without running the risks associated with divergent value function estimation. A second advantage is that some distinct algorithms for the average reward and discounted reward case in the primal become uniﬁed under the dual. In this paper, we present a modiﬁed dual of the standard linear program that guarantees a globally normalized state visit distribution is obtained. With this reformulation, we then derive novel dual forms of dynamic programming, including policy evaluation, policy iteration and value iteration. Moreover, we derive dual formulations of temporal difference learning to obtain new forms of Sarsa and Q-learning. Finally, we scale these techniques up to large domains by introducing approximation, and develop new approximate off-policy learning algorithms that avoid the divergence problems associated with the primal approach. We show that the dual view yields a viable alternative to standard value function based techniques and opens new avenues for solving dynamic programming and reinforcement learning problems.},
	language = {en},
	urldate = {2019-07-18},
	booktitle = {International {Symposium} on {Approximate} {Dynamic} {Programming} and {Reinforcement} {Learning}},
	publisher = {IEEE},
	author = {Wang, Tao and Bowling, Michael and Schuurmans, Dale},
	year = {2007},
	pages = {44--51}
}


@article{hadfield-menell_off-switch_2016,
	title = {The Off-Switch Game},
	url = {http://arxiv.org/abs/1611.08219},
	abstract = {It is clear that one of the primary tools we can use to mitigate the potential risk from a misbehaving AI system is the ability to turn the system off. As the capabilities of AI systems improve, it is important to ensure that such systems do not adopt subgoals that prevent a human from switching them off. This is a challenge because many formulations of rational agents create strong incentives for selfpreservation. This is not caused by a built-in instinct, but because a rational agent will maximize expected utility and cannot achieve whatever objective it has been given if it is dead. Our goal is to study the incentives an agent has to allow itself to be switched off. We analyze a simple game between a human H and a robot R, where H can press R’s off switch but R can disable the off switch. A traditional agent takes its reward function for granted: we show that such agents have an incentive to disable the off switch, except in the special case where H is perfectly rational. Our key insight is that for R to want to preserve its off switch, it needs to be uncertain about the utility associated with the outcome, and to treat H’s actions as important observations about that utility. (R also has no incentive to switch itself off in this setting.) We conclude that giving machines an appropriate level of uncertainty about their objectives leads to safer designs, and we argue that this setting is a useful generalization of the classical AI paradigm of rational agents.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1611.08219 [cs]},
	author = {Hadfield-Menell, Dylan and Dragan, Anca and Abbeel, Pieter and Russell, Stuart},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.08219},
	keywords = {Computer Science - Artificial Intelligence}
}


@article{mckenzie_turnpike_1976,
	title = {Turnpike theory},
	journal = {Econometrica: Journal of the Econometric Society},
	author = {McKenzie, Lionel W},
	year = {1976},
	pages = {841--865}
}


@article{benson-tilsen_formalizing_2016,
	title = {Formalizing Convergent Instrumental Goals},
	abstract = {Omohundro has argued that suﬃciently advanced AI systems of any design would, by default, have incentives to pursue a number of instrumentally useful subgoals, such as acquiring more computing power and amassing many resources. Omohundro refers to these as “basic AI drives,” and he, along with Bostrom and others, has argued that this means great care must be taken when designing powerful autonomous systems, because even if they have harmless goals, the side eﬀects of pursuing those goals may be quite harmful. These arguments, while intuitively compelling, are primarily philosophical. In this paper, we provide formal models that demonstrate Omohundro’s thesis, thereby putting mathematical weight behind those intuitive claims.},
	language = {en},
	journal = {Workshops at the Thirtieth AAAI Conference on Artificial Intelligence},
	author = {Benson-Tilsen, Tsvi and Soares, Nate},
	year = {2016}
}



@article{blackwell_discrete_1962,
	title = {Discrete dynamic programming},
	language = {en},
	journal = {The Annals of Mathematical Statistics},
	author = {Blackwell, David},
	year = {1962},
	pages = {9}
}

@article{cohen_asymptotically_2019,
	title = {Asymptotically unambitious artificial general intelligence},
	url = {http://arxiv.org/abs/1905.12186},
	abstract = {General intelligence, the ability to solve arbitrary solvable problems, is supposed by many to be artiﬁcially constructible. Narrow intelligence, the ability to solve a given particularly difﬁcult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classiﬁers, and translators. Artiﬁcial General Intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indifferent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI’s goals with our own has proven highly elusive. We present the ﬁrst algorithm we are aware of for asymptotically unambitious AGI, where “unambitiousness” includes not seeking arbitrary power. Thus, we identify an exception to the Instrumental Convergence Thesis, which is roughly that by default, an AGI would seek power, including over us.},
	language = {en},
	urldate = {2019-07-27},
	journal = {arXiv:1905.12186 [cs]},
	author = {Cohen, Michael K. and Vellambi, Badri and Hutter, Marcus},
	month = may,
	year = {2019},
	note = {arXiv: 1905.12186},
	keywords = {Computer Science - Artificial Intelligence, I.2.0, I.2.6}
}

@book{russell_artificial_2009,
	title = {Artificial intelligence: a modern approach},
	publisher = {Pearson Education Limited},
	author = {Russell, Stuart J and Norvig, Peter},
	year = {2009}
}

@article{wainwright2019safelife,
  title={SafeLife 1.0: Exploring Side Effects in Complex Environments},
  author={Wainwright, Carroll L and Eckersley, Peter},
  journal={arXiv:1912.01217},
  year={2019}
}


@book{bostrom_superintelligence_2014,
	title = {Superintelligence},
	publisher = {Oxford University Press},
	author = {Bostrom, Nick},
	year = {2014}
}

@book{russell_human_2019,
	title = {Human compatible: {Artificial} intelligence and the problem of control},
	publisher = {Viking},
	author = {Russell, Stuart},
	year = {2019}
}
@inproceedings{turner2020conservative,
  title={Conservative agency via attainable utility preservation},
  author={Turner, Alexander Matt and Hadfield-Menell, Dylan and Tadepalli, Prasad},
  booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
  pages={385--391},
  year={2020}
}

@inproceedings{berkenkamp2017safe,
  title={Safe model-based reinforcement learning with stability guarantees},
  author={Berkenkamp, Felix and Turchetta, Matteo and Schoellig, Angela and Krause, Andreas},
  booktitle={Advances in {Ne}ural {In}formation {Pr}ocessing {Sy}stems},
  pages={908--918},
  year={2017}
}
@inproceedings{chow2018lyapunov,
  title={A lyapunov-based approach to safe reinforcement learning},
  author={Chow, Yinlam and Nachum, Ofir and Duenez-Guzman, Edgar and Ghavamzadeh, Mohammad},
  booktitle={Advances in Neural Information Processing Systems},
  pages={8092--8101},
  year={2018}
}
@inproceedings{mohamed2015variational,
  title={Variational information maximisation for intrinsically motivated reinforcement learning},
  author={Mohamed, Shakir and Rezende, Danilo Jimenez},
  booktitle={Advances in Neural Information Processing Systems},
  pages={2125--2133},
  year={2015}
}
@article{moldovan2012safe,
  title={Safe exploration in {Ma}rkov decision processes},
  author={Moldovan, Teodor Mihai and Abbeel, Pieter},
  journal={ICML},
  year={2012}
}
@inproceedings{pecka2014safe,
  title={Safe exploration techniques for reinforcement learning--an overview},
  author={Pecka, Martin and Svoboda, Tomas},
  booktitle={International Workshop on Modelling and Simulation for Autonomous Systems},
  pages={357--375},
  year={2014},
  organization={Springer}
}
@article{silver2016mastering,
  title={Mastering the game of {Go} with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={Nature},
  volume={529},
  number={7587},
  pages={484},
  year={2016},
  publisher={Nature Publishing Group}
}
@misc{ OpenAI_dota,
      author = {OpenAI},
      title = {{OpenAI} {Fi}ve},
      howpublished = {\url{https://blog.openai.com/openai-five/}},
      year = {2018}}
      
@article{law,
  author    = {David Manheim and
               Scott Garrabrant},
  title     = {Categorizing Variants of Goodhart's Law},
  journal   = {CoRR},
  volume    = {abs/1803.04585},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.04585},
  archivePrefix = {arXiv},
  eprint    = {1803.04585},
  timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-04585},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{corruption,
  author    = {Tom Everitt and Victoria Krakovna and Laurent Orseau and Shane Legg},
  title     = {Reinforcement Learning with a Corrupted Reward Channel},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {4705--4713},
  year      = {2017},
  doi       = {},
  url       = {},
}

@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher and Dayan, Peter},
  journal={Machine Learning},
  volume={8},
  number={3-4},
  pages={279--292},
  year={1992},
  publisher={Springer}
}
@inproceedings{
shah2018the,
title={The Implicit Preference Information in an Initial State},
author={Rohin Shah and Dmitrii Krasheninnikov and Jordan Alexander and Pieter Abbeel and Anca Dragan},
booktitle={International Conference on Learning Representations},
year={2019},
url={},
}

@book{kahneman2011thinking,
  abstract = {In this work the author, a recipient of the Nobel Prize in Economic Sciences for his seminal work in psychology that challenged the rational model of judgment and decision making, has brought together his many years of research and thinking in one book. He explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. He exposes the extraordinary capabilities, and also the faults and biases, of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. He reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives, and how we can use different techniques to guard against the mental glitches that often get us into trouble. This author's work has transformed cognitive psychology and launched the new fields of behavioral economics and happiness studies. In this book, he takes us on a tour of the mind and explains the two systems that drive the way we think and the way we make choices.},
  added-at = {2013-01-10T15:41:11.000+0100},
  address = {New York},
  author = {Kahneman, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2f322864169411fd5914f3fa5488e288c/schmidt2},
  description = {Thinking, Fast and Slow: Amazon.de: Daniel Kahneman: Englische Bücher},
  interhash = {a1400a299a00de009ec8eda73e6289af},
  intrahash = {f322864169411fd5914f3fa5488e288c},
  isbn = {9780374275631 0374275637},
  keywords = {bib books psychology thinking toread},
  publisher = {Farrar, Straus and Giroux},
  refid = {706020998},
  timestamp = {2013-01-10T15:41:11.000+0100},
  title = {Thinking, fast and slow},
  url = {https://www.amazon.de/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374275637/ref=wl_it_dp_o_pdT1_nS_nC?ie=UTF8&colid=151193SNGKJT9&coliid=I3OCESLZCVDFL7},
  year = 2011
}


@inproceedings{zhang2018minimax,
  title={Minimax-Regret Querying on Side Effects for Safe Optimality in Factored {Ma}rkov Decision Processes.},
  author={Zhang, Shun and Durfee, Edmund H and Singh, Satinder P},
  booktitle={Proceedings of the Twenty-Seventh International Joint Conference on
               Artificial Intelligence, {IJCAI-18}},
  pages={4867--4873},
  year={2018}
}

@misc{Victoria_specification,
	title = {Specification gaming: the flip side of {AI} ingenuity},
	url = {https://deepmind.com/blog/article/Specification-gaming-the-flip-side-of-AI-ingenuity},
	year={2020},
	author = {Krakovna, Victoria and Uesato, Jonathan and Mikulik, Vladimir and Rahtz, Matthew and Everitt, Tom and Kumar, Ramana and Kenton, Zac and Leike, Jan and Legg, Shane},
}

@article{ray2019benchmarking,
  title={Benchmarking safe exploration in deep reinforcement learning},
  author={Ray, Alex and Achiam, Joshua and Amodei, Dario},
  journal={arXiv preprint arXiv:1910.01708},
  year={2019}
}

@article{relative_original,
  author    = {Victoria Krakovna and
               Laurent Orseau and
               Miljan Martic and
               Shane Legg},
  title     = {Measuring and avoiding side effects using relative reachability},
  journal   = {CoRR},
  volume    = {abs/1806.01186},
  year      = {2018},
  url       = {},
  archivePrefix = {arXiv},
  eprint    = {1806.01186},
  timestamp = {Mon, 13 Aug 2018 16:47:48 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-01186.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{anecdotes,
  author    = {Joel Lehman and
               Jeff Clune and
               Dusan Misevic},
  title     = {The Surprising Creativity of Digital Evolution: {A} Collection of
               Anecdotes from the Evolutionary Computation and Artificial Life Research
               Communities},
  journal   = {CoRR},
  volume    = {abs/1803.03453},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.03453},
  archivePrefix = {arXiv},
  eprint    = {1803.03453},
  timestamp = {Thu, 16 Aug 2018 17:42:44 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-03453},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{strathern1997improving,
  title={‘Improving ratings’: audit in the British University system},
  author={Strathern, Marilyn},
  journal={European review},
  volume={5},
  number={3},
  pages={305--321},
  year={1997},
  publisher={Cambridge University Press}
}
@article{Dylan_incomplete,
  author    = {Dylan Hadfield{-}Menell and
               Gillian K. Hadfield},
  title     = {Incomplete Contracting and {AI} Alignment},
  journal = {AI, Ethics, and Society},
  year      = {2018},
  eprint    = {1804.04268},
  timestamp = {Thu, 22 Nov 2018 14:52:30 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1804-04268},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@book{altman1999constrained,
  title={Constrained {Ma}rkov decision processes},
  author={Altman, Eitan},
  volume={7},
  year={1999},
  publisher={CRC Press}
}
@inproceedings{regan2010robust,
  title={Robust Policy Computation in Reward-Uncertain {MDPs} Using Nondominated Policies.},
  author={Regan, Kevin and Boutilier, Craig},
  booktitle={AAAI},
  year={2010}
}

@inproceedings{taylor2016quantilizers,
  title={Quantilizers: A Safer Alternative to Maximizers for Limited Optimization.},
  author={Taylor, Jessica},
  booktitle={AAAI Workshop: AI, Ethics, and Society},
  year={2016}
}
@misc{noauthor_low_nodate,
	title = {Low {Impact}},
	url = {https://arbital.com/p/low_impact/},
	journal = {Arbital}
}

@misc{scott_robustness_nodate,
	title = {Robustness to {Scale}},
	url = {https://www.lesserwrong.com/posts/bBdfbWfWxHN9Chjcq/robustness-to-scale},
	author = {Scott, Garrabrant}
}

@article{de_blanc_ontological_2011,
	title = {Ontological crises in artificial agents' value systems},
	journal = {arXiv preprint arXiv:1105.3821},
	author = {De Blanc, Peter},
	year = {2011},
	file = {1105.3821(1).pdf:C\:\\Users\\Alex\\Zotero\\storage\\B6MVELFW\\1105.3821(1).pdf:application/pdf}
}

@inproceedings{yu_open_2017,
	title = {Open {Category} {Classification} by {Adversarial} {Sample} {Generation}},
	isbn = {978-0-9992411-0-3},
	url = {https://www.ijcai.org/proceedings/2017/469},
	doi = {10.24963/ijcai.2017/469},
	abstract = {In real-world classiﬁcation tasks, it is difﬁcult to collect training samples from all possible categories of the environment. Therefore, when an instance of an unseen class appears in the prediction stage, a robust classiﬁer should be able to tell that it is from an unseen class, instead of classifying it to be any known category. In this paper, adopting the idea of adversarial learning, we propose the ASG framework for open-category classiﬁcation. ASG generates positive and negative samples of seen categories in the unsupervised manner via an adversarial learning strategy. With the generated samples, ASG then learns to tell seen from unseen in the supervised manner. Experiments performed on several datasets show the effectiveness of ASG.},
	language = {en},
	urldate = {2018-03-18},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Yu, Yang and Qu, Wei-Yang and Li, Nan and Guo, Zimin},
	month = aug,
	year = {2017},
	pages = {3357--3363},
	file = {Yu et al. - 2017 - Open Category Classification by Adversarial Sample.pdf:C\:\\Users\\Alex\\Zotero\\storage\\6BC4PY3H\\Yu et al. - 2017 - Open Category Classification by Adversarial Sample.pdf:application/pdf}
}

@misc{noauthor_conservative_nodate,
	title = {Conservative classifiers},
	url = {https://agentfoundations.org/item?id=467}
}

@inproceedings{li_knows_2008,
	title = {Knows what it knows: a framework for self-aware learning},
	isbn = {978-1-60558-205-4},
	shorttitle = {Knows what it knows},
	url = {http://portal.acm.org/citation.cfm?doid=1390156.1390228},
	doi = {10.1145/1390156.1390228},
	abstract = {We introduce a learning framework that combines elements of the well-known PAC and mistake-bound models. The KWIK (knows what it knows) framework was designed particularly for its utility in learning settings where active exploration can impact the training examples the learner is exposed to, as is true in reinforcement-learning and activelearning problems. We catalog several KWIK-learnable classes as well as open problems, and demonstrate their applications in experience-efﬁcient reinforcement learning.},
	language = {en},
	urldate = {2018-03-27},
	publisher = {ACM Press},
	author = {Li, Lihong and Littman, Michael L. and Walsh, Thomas J.},
	year = {2008},
	pages = {568--575},
	file = {Li et al. - 2008 - Knows what it knows a framework for self-aware le.pdf:C\:\\Users\\Alex\\Zotero\\storage\\D3HDJ5HK\\Li et al. - 2008 - Knows what it knows a framework for self-aware le.pdf:application/pdf}
}

@article{taylor_alignment_nodate,
	title = {Alignment for {Advanced} {Machine} {Learning} {Systems}},
	abstract = {We survey eight research areas organized around one question: As learning systems become increasingly intelligent and autonomous, what design principles can best ensure that their behavior is aligned with the interests of the operators? We focus on two major technical obstacles to AI alignment: the challenge of specifying the right kind of objective functions, and the challenge of designing AI systems that avoid unintended consequences and undesirable behavior even in cases where the objective function does not line up perfectly with the intentions of the designers.},
	language = {en},
	author = {Taylor, Jessica and Yudkowsky, Eliezer and LaVictoire, Patrick and Critch, Andrew},
	pages = {25},
	year={2016}
}

@inproceedings{bendale_towards_2016,
	title = {Towards {Open} {Set} {Deep} {Networks}},
	isbn = {978-1-4673-8851-1},
	url = {http://ieeexplore.ieee.org/document/7780542/},
	doi = {10.1109/CVPR.2016.173},
	abstract = {Deep networks have produced signiﬁcant gains for various visual recognition problems, leading to high impact academic and commercial applications. Recent work in deep networks highlighted that it is easy to generate images that humans would never classify as a particular object class, yet networks classify such images high conﬁdence as that given class – deep network are easily fooled with images humans do not consider meaningful. The closed set nature of deep networks forces them to choose from one of the known classes leading to such artifacts. Recognition in the real world is open set, i.e. the recognition system should reject unknown/unseen classes at test time. We present a methodology to adapt deep networks for open set recognition, by introducing a new model layer, OpenMax, which estimates the probability of an input being from an unknown class. A key element of estimating the unknown probability is adapting Meta-Recognition concepts to the activation patterns in the penultimate layer of the network. OpenMax allows rejection of “fooling” and unrelated open set images presented to the system; OpenMax greatly reduces the number of obvious errors made by a deep network. We prove that the OpenMax concept provides bounded open space risk, thereby formally providing an open set recognition solution. We evaluate the resulting open set deep networks using pre-trained networks from the Caffe Model-zoo on ImageNet 2012 validation data, and thousands of fooling and open set images. The proposed OpenMax model significantly outperforms open set recognition accuracy of basic deep networks as well as deep networks with thresholding of SoftMax probabilities.},
	language = {en},
	urldate = {2018-03-28},
	publisher = {IEEE},
	author = {Bendale, Abhijit and Boult, Terrance E.},
	month = jun,
	year = {2016},
	note = {add to post},
	pages = {1563--1572},
	file = {Bendale et Boult - 2016 - Towards Open Set Deep Networks.pdf:C\:\\Users\\Alex\\Zotero\\storage\\G3FBVCDI\\Bendale et Boult - 2016 - Towards Open Set Deep Networks.pdf:application/pdf}
}

@inproceedings{hadfield2016cooperative,
  title={Cooperative inverse reinforcement learning},
  author={Hadfield-Menell, Dylan and Russell, Stuart and Abbeel, Pieter and Dragan, Anca},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3909--3917},
  year={2016}
}

@inproceedings{achiam2017constrained,
  title={Constrained policy optimization},
  author={Achiam, Joshua and Held, David and Tamar, Aviv and Abbeel, Pieter},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={22--31},
  year={2017}
}
@article{irving_ai_2018,
	title = {{AI} safety via debate},
	url = {http://arxiv.org/abs/1805.00899},
	abstract = {To make AI systems broadly useful for challenging real-world tasks, we need them to learn complex human goals and preferences. One approach to specifying complex goals asks humans to judge during training which agent behaviors are safe and useful, but this approach can fail if the task is too complicated for a human to directly judge. To help address this concern, we propose training agents via self play on a zero sum debate game. Given a question or proposed action, two agents take turns making short statements up to a limit, then a human judges which of the agents gave the most true, useful information. In an analogy to complexity theory, debate with optimal play can answer any question in PSPACE given polynomial time judges (direct judging answers only NP questions). In practice, whether debate works involves empirical questions about humans and the tasks we want AIs to perform, plus theoretical questions about the meaning of AI alignment. We report results on an initial MNIST experiment where agents compete to convince a sparse classiﬁer, boosting the classiﬁer’s accuracy from 59.4\% to 88.9\% given 6 pixels and from 48.2\% to 85.2\% given 4 pixels. Finally, we discuss theoretical and practical aspects of the debate model, focusing on potential weaknesses as the model scales up, and we propose future human and computer experiments to test these properties.},
	language = {en},
	urldate = {2018-05-23},
	journal = {arXiv:1805.00899 [cs, stat]},
	author = {Irving, Geoffrey and Christiano, Paul and Amodei, Dario},
	month = may,
	year = {2018},
	note = {arXiv: 1805.00899},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {Irving et al. - 2018 - AI safety via debate.pdf:C\:\\Users\\Alex\\Zotero\\storage\\Z49IA9LZ\\Irving et al. - 2018 - AI safety via debate.pdf:application/pdf}
}

@misc{noauthor_delayed_nodate,
	title = {Delayed {Impact} of {Fair} {Machine} {Learning}},
	url = {http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/},
	abstract = {In a classic example of Goodhart’s Law, intuitive fairness constraints on machine learning systems which make lending decisions can cause quantitatively worse outcomes for disadvantaged groups.}
}

@inproceedings{hadfield2017inverse,
  title={Inverse reward design},
  author={Hadfield-Menell, Dylan and Milli, Smitha and Abbeel, Pieter and Russell, Stuart and Dragan, Anca},
  booktitle={Advances in Neural Information Processing Systems},
  pages={6765--6774},
  year={2017}
}

@article{zhang_minimax-regret_nodate,
	title = {Minimax-{Regret} {Querying} on {Side} {Effects} for {Safe} {Optimality} in {Factored} {Markov} {Decision} {Processes}},
	abstract = {As it achieves a goal on behalf of its human user, an autonomous agent’s actions may have side effects that change features of its environment in ways that negatively surprise its user. An agent that can be trusted to operate safely should thus only change features the user has explicitly permitted. We formalize this problem, and develop a planning algorithm that avoids potentially negative side effects given what the agent knows about (un)changeable features. Further, we formulate a provably minimax-regret querying strategy for the agent to selectively ask the user about features that it hasn’t explicitly been told about. We empirically show how much faster it is than a more exhaustive approach and how much better its queries are than those found by the best known heuristic.},
	language = {en},
	author = {Zhang, Shun and Durfee, Edmund H and Singh, Satinder},
	pages = {7},
	file = {Zhang et al. - Minimax-Regret Querying on Side Effects for Safe O.pdf:C\:\\Users\\Alex\\Zotero\\storage\\3X67DXEA\\Zhang et al. - Minimax-Regret Querying on Side Effects for Safe O.pdf:application/pdf;Zhang et al. - Minimax-Regret Querying on Side Effects for Safe O.pdf:C\:\\Users\\Alex\\Zotero\\storage\\SM33BZRH\\Zhang et al. - Minimax-Regret Querying on Side Effects for Safe O.pdf:application/pdf}
}

@article{soares_questions_nodate,
	title = {Questions of {Reasoning} {Under} {Logical} {Uncertainty}},
	abstract = {A logically uncertain reasoner would be able to reason as if they know both a programming language and a program, without knowing what the program outputs. Most practical reasoning involves some logical uncertainty, but no satisfactory theory of reasoning under logical uncertainty yet exists. A better theory of reasoning under logical uncertainty is needed in order to develop the tools necessary to construct highly reliable artiﬁcial reasoners. This paper introduces the topic, discusses a number of historical results, and describes a number of open problems.},
	language = {en},
	author = {Soares, Nate},
	pages = {8},
	file = {Soares - Questions of Reasoning Under Logical Uncertainty.pdf:C\:\\Users\\Alex\\Zotero\\storage\\TEVPQ9MC\\Soares - Questions of Reasoning Under Logical Uncertainty.pdf:application/pdf}
}

@article{everitt_agi_2018,
	title = {{AGI} {Safety} {Literature} {Review}},
	url = {http://arxiv.org/abs/1805.01109},
	abstract = {The development of Artificial General Intelligence (AGI) promises to be a major event. Along with its many potential benefits, it also raises serious safety concerns (Bostrom, 2014). The intention of this paper is to provide an easily accessible and up-to-date collection of references for the emerging field of AGI safety. A significant number of safety problems for AGI have been identified. We list these, and survey recent research on solving them. We also cover works on how best to think of AGI from the limited knowledge we have today, predictions for when AGI will first be created, and what will happen after its creation. Finally, we review the current public policy on AGI.},
	language = {en},
	urldate = {2018-07-01},
	journal = {arXiv:1805.01109 [cs]},
	author = {Everitt, Tom and Lea, Gary and Hutter, Marcus},
	month = may,
	year = {2018},
	note = {arXiv: 1805.01109},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Everitt et al. - 2018 - AGI Safety Literature Review.pdf:C\:\\Users\\Alex\\Zotero\\storage\\BG85UE2S\\Everitt et al. - 2018 - AGI Safety Literature Review.pdf:application/pdf;Everitt et al. - 2018 - AGI Safety Literature Review.pdf:C\:\\Users\\Alex\\Zotero\\storage\\2PDI3FU9\\Everitt et al. - 2018 - AGI Safety Literature Review.pdf:application/pdf}
}

@article{everitt_alignment_nodate,
	title = {The {Alignment} {Problem} for {History}-{Based} {Bayesian} {Reinforcement} {Learners}},
	abstract = {Value alignment is often considered a critical component of safe artiﬁcial intelligence. Meanwhile, reinforcement learning is often criticized as being inherently unsafe and misaligned, for reasons such as wireheading, delusionboxes, misspeciﬁed reward functions and distributional shifts. In this paper, we categorize sources of misalignment for reinforcement learning agents, illustrating each type with numerous examples. For each type of problem, we also describe ways to remove the source of misalignment. Combined, the suggestions form high-level blueprints for how to design value aligned RL agents.},
	language = {en},
	author = {Everitt, Tom and Hutter, Marcus},
	pages = {69},
	file = {Everitt et Hutter - The Alignment Problem for History-Based Bayesian R.pdf:C\:\\Users\\Alex\\Zotero\\storage\\4ATUSVH3\\Everitt et Hutter - The Alignment Problem for History-Based Bayesian R.pdf:application/pdf}
}

@article{armstrong_low_2017,
	title = {Low {impact} {artificial} {intelligences}},
	url = {http://arxiv.org/abs/1705.10720},
	abstract = {There are many goals for an AI that could become dangerous if the AI becomes superintelligent or otherwise powerful. Much work on the AI control problem has been focused on constructing AI goals that are safe even for such AIs. This paper looks at an alternative approach: deﬁning a general concept of ‘low impact’. The aim is to ensure that a powerful AI which implements low impact will not modify the world extensively, even if it is given a simple or dangerous goal. The paper proposes various ways of deﬁning and grounding low impact, and discusses methods for ensuring that the AI can still be allowed to have a (desired) impact despite the restriction. The end of the paper addresses known issues with this approach and avenues for future research.},
	language = {en},
	urldate = {2018-07-01},
	journal = {arXiv:1705.10720 [cs]},
	author = {Armstrong, Stuart and Levinstein, Benjamin},
	month = may,
	year = {2017},
	note = {arXiv: 1705.10720},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Armstrong et Levinstein - 2017 - Low Impact Artificial Intelligences.pdf:C\:\\Users\\Alex\\Zotero\\storage\\DBG9XANU\\Armstrong et Levinstein - 2017 - Low Impact Artificial Intelligences.pdf:application/pdf}
}

@article{legg_universal_2007,
	title = {Universal {Intelligence}: {A} {Definition} of {Machine} {Intelligence}},
	shorttitle = {Universal {Intelligence}},
	url = {http://arxiv.org/abs/0712.3329},
	abstract = {A fundamental problem in artiﬁcial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artiﬁcial systems which are signiﬁcantly diﬀerent to humans. In this paper we approach this problem in the following way: We take a number of well known informal deﬁnitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal deﬁnition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and deﬁnitions of intelligence that have been proposed for machines.},
	language = {en},
	urldate = {2018-07-05},
	journal = {arXiv:0712.3329 [cs]},
	author = {Legg, Shane and Hutter, Marcus},
	month = dec,
	year = {2007},
	note = {arXiv: 0712.3329},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Legg et Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf:C\:\\Users\\Alex\\Zotero\\storage\\52G2VC95\\Legg et Hutter - 2007 - Universal Intelligence A Definition of Machine In.pdf:application/pdf}
}

@inproceedings{
eysenbach2018leave,
title={Leave no Trace: Learning to Reset for Safe and Autonomous Reinforcement Learning},
author={Benjamin Eysenbach and Shixiang Gu and Julian Ibarz and Sergey Levine},
booktitle={International Conference on Learning Representations},
year={2018},
url={},
}

@article{arora_survey_2018,
	title = {A {Survey} of {Inverse} {Reinforcement} {Learning}: {Challenges}, {Methods} and {Progress}},
	shorttitle = {A {Survey} of {Inverse} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1806.06877},
	abstract = {Inverse reinforcement learning is the problem of inferring the reward function of an observed agent, given its policy or behavior. Researchers perceive IRL both as a problem and as a class of methods. By categorically surveying the current literature in IRL, this article serves as a reference for researchers and practitioners in machine learning to understand the challenges of IRL and select the approaches best suited for the problem on hand. The survey formally introduces the IRL problem along with its central challenges which include accurate inference, generalizability, correctness of prior knowledge, and growth in solution complexity with problem size. The article elaborates how the current methods mitigate these challenges. We further discuss the extensions of traditional IRL methods: (i) inaccurate and incomplete perception, (ii) incomplete model, (iii) multiple rewards, and (iv) non-linear reward functions. This discussion concludes with some broad advances in the research area and currently open research questions.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1806.06877 [cs, stat]},
	author = {Arora, Saurabh and Doshi, Prashant},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.06877},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning},
	file = {Arora and Doshi - 2018 - A Survey of Inverse Reinforcement Learning Challe.pdf:C\:\\Users\\Alex\\Zotero\\storage\\GEPR5T8G\\Arora and Doshi - 2018 - A Survey of Inverse Reinforcement Learning Challe.pdf:application/pdf}
}

@article{ziebart_modeling_nodate,
	title = {Modeling {Interaction} via the {Principle} of {Maximum} {Causal} {Entropy}},
	abstract = {The principle of maximum entropy provides a powerful framework for statistical models of joint, conditional, and marginal distributions. However, there are many important distributions with elements of interaction and feedback where its applicability has not been established. This work presents the principle of maximum causal entropy—an approach based on causally conditioned probabilities that can appropriately model the availability and inﬂuence of sequentially revealed side information. Using this principle, we derive models for sequential data with revealed information, interaction, and feedback, and demonstrate their applicability for statistically framing inverse optimal control and decision prediction tasks.},
	language = {en},
	author = {Ziebart, Brian D and Bagnell, J Andrew and Dey, Anind K},
	pages = {8},
	file = {Ziebart et al. - Modeling Interaction via the Principle of Maximum .pdf:C\:\\Users\\Alex\\Zotero\\storage\\VWN646XI\\Ziebart et al. - Modeling Interaction via the Principle of Maximum .pdf:application/pdf}
}

@article{ziebart_maximum_nodate,
	title = {Maximum {Entropy} {Inverse} {Reinforcement} {Learning}},
	abstract = {Recent research has shown the beneﬁt of framing problems of imitation learning as solutions to Markov Decision Problems. This approach reduces learning to the problem of recovering a utility function that makes the behavior induced by a near-optimal policy closely mimic demonstrated behavior. In this work, we develop a probabilistic approach based on the principle of maximum entropy. Our approach provides a well-deﬁned, globally normalized distribution over decision sequences, while providing the same performance guarantees as existing methods.},
	language = {en},
	author = {Ziebart, Brian D and Maas, Andrew and Bagnell, J Andrew and Dey, Anind K},
	pages = {6},
	file = {Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:C\:\\Users\\Alex\\Zotero\\storage\\TJZE3VJE\\Ziebart et al. - Maximum Entropy Inverse Reinforcement Learning.pdf:application/pdf}
}

@article{abel_reinforcement_nodate,
	title = {Reinforcement {Learning} {As} a {Framework} for {Ethical} {Decision} {Making}},
	abstract = {Emerging AI systems will be making more and more decisions that impact the lives of humans in a signiﬁcant way. It is essential, then, that these AI systems make decisions that take into account the desires, goals, and preferences of other people, while simultaneously learning about what those preferences are. In this work, we argue that the reinforcementlearning framework achieves the appropriate generality required to theorize about an idealized ethical artiﬁcial agent, and offers the proper foundations for grounding speciﬁc questions about ethical learning and decision making that can promote further scientiﬁc investigation. We deﬁne an idealized formalism for an ethical learner, and conduct experiments on two toy ethical dilemmas, demonstrating the soundness and ﬂexibility of our approach. Lastly, we identify several critical challenges for future advancement in the area that can leverage our proposed framework.},
	language = {en},
	author = {Abel, David},
	pages = {8},
	file = {Abel - Reinforcement Learning As a Framework for Ethical .pdf:C\:\\Users\\Alex\\Zotero\\storage\\P3SGFGKK\\Abel - Reinforcement Learning As a Framework for Ethical .pdf:application/pdf}
}

@article{orseau_safely_nodate,
	title = {Safely {Interruptible} {Agents}},
	abstract = {Reinforcement learning agents interacting with a complex environment like the real world are unlikely to behave optimally all the time. If such an agent is operating in real-time under human supervision, now and then it may be necessary for a human operator to press the big red button to prevent the agent from continuing a harmful sequence of actions—harmful either for the agent or for the environment—and lead the agent into a safer situation. However, if the learning agent expects to receive rewards from this sequence, it may learn in the long run to avoid such interruptions, for example by disabling the red button—which is an undesirable outcome. This paper explores a way to make sure a learning agent will not learn to prevent (or seek!) being interrupted by the environment or a human operator. We provide a formal deﬁnition of safe interruptibility and exploit the off-policy learning property to prove that either some agents are already safely interruptible, like Q-learning, or can easily be made so, like Sarsa. We show that even ideal, uncomputable reinforcement learning agents for (deterministic) general computable environments can be made safely interruptible.},
	language = {en},
	author = {Orseau, Laurent and Armstrong, Stuart},
	pages = {10},
	file = {Orseau and Armstrong - Safely Interruptible Agents⇤.pdf:C\:\\Users\\Alex\\Zotero\\storage\\E86R4VA4\\Orseau and Armstrong - Safely Interruptible Agents⇤.pdf:application/pdf}
}

@article{everitt_reinforcement_2017,
	title = {Reinforcement Learning with a Corrupted Reward Channel},
	url = {},
	abstract = {No real-world reward function is perfect. Sensory errors and software bugs may result in RL agents observing higher (or lower) rewards than they should. For example, a reinforcement learning agent may prefer states where a sensory error gives it the maximum reward, but where the true reward is actually small. We formalise this problem as a generalised Markov Decision Problem called Corrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under strong simplifying assumptions and when trying to compensate for the possibly corrupt rewards. Two ways around the problem are investigated. First, by giving the agent richer data, such as in inverse reinforcement learning and semi-supervised reinforcement learning, reward corruption stemming from systematic sensory errors may sometimes be completely managed. Second, by using randomisation to blunt the agent’s optimisation, reward corruption can be partially managed under some assumptions.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1705.08417 [cs, stat]},
	author = {Everitt, Tom and Krakovna, Victoria and Orseau, Laurent and Hutter, Marcus and Legg, Shane},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08417},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning, I.2.6, I.2.8},
	file = {Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf:C\:\\Users\\Alex\\Zotero\\storage\\RLW2UF5E\\Everitt et al. - 2017 - Reinforcement Learning with a Corrupted Reward Cha.pdf:application/pdf}
}

@article{everitt_avoiding_2016,
	title = {Avoiding {Wireheading} with {Value} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1605.03143},
	abstract = {How can we design good goals for arbitrarily intelligent agents? Reinforcement learning (RL) is a natural approach. Unfortunately, RL does not work well for generally intelligent agents, as RL agents are incentivised to shortcut the reward sensor for maximum reward – the so-called wireheading problem. In this paper we suggest an alternative to RL called value reinforcement learning (VRL). In VRL, agents use the reward signal to learn a utility function. The VRL setup allows us to remove the incentive to wirehead by placing a constraint on the agent’s actions. The constraint is deﬁned in terms of the agent’s belief distributions, and does not require an explicit speciﬁcation of which actions constitute wireheading.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1605.03143 [cs]},
	author = {Everitt, Tom and Hutter, Marcus},
	month = may,
	year = {2016},
	note = {arXiv: 1605.03143},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf:C\:\\Users\\Alex\\Zotero\\storage\\JGX7JGVD\\Everitt and Hutter - 2016 - Avoiding Wireheading with Value Reinforcement Lear.pdf:application/pdf}
}

@article{hibbard_model-based_2012,
	title = {Model-based {Utility} {Functions}},
	volume = {3},
	issn = {1946-0163},
	url = {http://content.sciendo.com/view/journals/jagi/3/1/article-p1.xml},
	doi = {10.2478/v10229-011-0013-5},
	abstract = {At the recent AGI-11 Conference Orseau and Ring, and Dewey, described problems, including self-delusion, with the behavior of AIXI agents using various definitions of utility functions. An agent's utility function is defined in terms of the agent's history of interactions with its environment. This paper argues that the behavior problems can be avoided by formulating the utility function in two steps: 1) inferring a model of the environment from interactions, and 2) computing utility as a function of the environment model. The paper also argues that agents will not choose to modify their utility functions.},
	language = {en},
	number = {1},
	urldate = {2018-07-13},
	journal = {Journal of Artificial General Intelligence},
	author = {Hibbard, Bill},
	month = jan,
	year = {2012},
	pages = {1--24},
	file = {Hibbard - 2012 - Model-based Utility Functions.pdf:C\:\\Users\\Alex\\Zotero\\storage\\388HA5QR\\Hibbard - 2012 - Model-based Utility Functions.pdf:application/pdf}
}

@incollection{ring_delusion_2011,
	address = {Berlin, Heidelberg},
	title = {Delusion, {Survival}, and {Intelligent} {Agents}},
	volume = {6830},
	isbn = {978-3-642-22886-5 978-3-642-22887-2},
	url = {http://link.springer.com/10.1007/978-3-642-22887-2_2},
	abstract = {3 This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI with these speciﬁc assumptions: 1) The agent is allowed to arbitrarily modify its own inputs if it so chooses; 2) The agent’s code is a part of the environment and may be read and written by the environment. The ﬁrst of these we call the “delusion box”; the second we call “mortality”. Within this framework, we discuss and compare four very diﬀerent kinds of agents, speciﬁcally: reinforcement-learning, goal-seeking, predictive, and knowledge-seeking agents. Our main results are that: 1) The reinforcement-learning agent under reasonable circumstances behaves exactly like an agent whose sole task is to survive (to preserve the integrity of its code); and 2) Only the knowledge-seeking agent behaves completely as expected.},
	language = {en},
	urldate = {2018-07-13},
	booktitle = {Artificial {General} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Ring, Mark and Orseau, Laurent},
	year = {2011},
	doi = {10.1007/978-3-642-22887-2_2},
	pages = {11--20},
	file = {Ring and Orseau - 2011 - Delusion, Survival, and Intelligent Agents.pdf:C\:\\Users\\Alex\\Zotero\\storage\\PH8NZQ2D\\Ring and Orseau - 2011 - Delusion, Survival, and Intelligent Agents.pdf:application/pdf}
}

@article{garcia2015comprehensive,
  title={A comprehensive survey on safe reinforcement learning},
  author={García, Javier and Fernández, Fernando},
  journal={Journal of Machine Learning Research},
  volume={16},
  number={1},
  pages={1437--1480},
  year={2015}
}

@inproceedings{saunders2018trial,
  title={Trial without error: Towards safe reinforcement learning via human intervention},
  author={Saunders, William and Sastry, Girish and Stuhlmueller, Andreas and Evans, Owain},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and Multi-Agent Systems},
  pages={2067--2069},
  year={2018}
}

@article{armstrong_thinking_2012,
	title = {Thinking {Inside} the {Box}: {Controlling} and {Using} an {Oracle} {AI}},
	volume = {22},
	issn = {0924-6495, 1572-8641},
	shorttitle = {Thinking {Inside} the {Box}},
	url = {http://link.springer.com/10.1007/s11023-012-9282-2},
	doi = {10.1007/s11023-012-9282-2},
	abstract = {There is no strong reason to believe that human-level intelligence represents an upper limit of the capacity of artificial intelligence, should it be realized. This poses serious safety issues, since a superintelligent system would have great power to direct the future according to its possibly flawed motivation system. Solving this issue in general has proven to be considerably harder than expected. This paper looks at one particular approach, Oracle AI. An Oracle AI is an AI that does not act in the world except by answering questions. Even this narrow approach presents considerable challenges. In this paper, we analyse and critique various methods of controlling the AI. In general an Oracle AI might be safer than unrestricted AI, but still remains potentially dangerous.},
	language = {en},
	number = {4},
	urldate = {2018-07-13},
	journal = {Minds and Machines},
	author = {Armstrong, Stuart and Sandberg, Anders and Bostrom, Nick},
	month = nov,
	year = {2012},
	pages = {299--324},
	file = {Armstrong et al. - 2012 - Thinking Inside the Box Controlling and Using an .pdf:C\:\\Users\\Alex\\Zotero\\storage\\8XBKUNCF\\Armstrong et al. - 2012 - Thinking Inside the Box Controlling and Using an .pdf:application/pdf}
}

@article{garfinkel_impossibility_2017,
	title = {On the {Impossibility} of {Supersized} {Machines}},
	url = {http://arxiv.org/abs/1703.10987},
	abstract = {In recent years, a number of prominent computer scientists, along with academics in ﬁelds such as philosophy and physics, have lent credence to the notion that machines may one day become as large as humans. Many have further argued that machines could even come to exceed human size by a signiﬁcant margin. However, there are at least seven distinct arguments that preclude this outcome. We show that it is not only implausible that machines will ever exceed human size, but in fact impossible.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1703.10987 [physics]},
	author = {Garfinkel, Ben and Brundage, Miles and Filan, Daniel and Flynn, Carrick and Luketina, Jelena and Page, Michael and Sandberg, Anders and Snyder-Beattie, Andrew and Tegmark, Max},
	month = mar,
	year = {2017},
	note = {arXiv: 1703.10987},
	keywords = {Computer Science - Computers and Society, Physics - Popular Physics},
	file = {Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf:C\:\\Users\\Alex\\Zotero\\storage\\4A8MU56N\\Garfinkel et al. - 2017 - On the Impossibility of Supersized Machines.pdf:application/pdf}
}
@article{bussmann2019towards,
  title={Towards empathic deep {Q}-learning},
  author={Bussmann, Bart and Heinerman, Jacqueline and Lehman, Joel},
  journal={arXiv:1906.10918},
  year={2019}
}
@article{grace_when_2017,
	title = {When {Will} {AI} {Exceed} {Human} {Performance}? {Evidence} from {AI} {Experts}},
	shorttitle = {When {Will} {AI} {Exceed} {Human} {Performance}?},
	url = {http://arxiv.org/abs/1705.08807},
	abstract = {Advances in artiﬁcial intelligence (AI) will transform modern life by reshaping transportation, health, science, ﬁnance, and the military [1, 2, 3]. To adapt public policy, we need to better anticipate these advances [4, 5]. Here we report the results from a large survey of machine learning researchers on their beliefs about progress in AI. Researchers predict AI will outperform humans in many activities in the next ten years, such as translating languages (by 2024), writing high-school essays (by 2026), driving a truck (by 2027), working in retail (by 2031), writing a bestselling book (by 2049), and working as a surgeon (by 2053). Researchers believe there is a 50\% chance of AI outperforming humans in all tasks in 45 years and of automating all human jobs in 120 years, with Asian respondents expecting these dates much sooner than North Americans. These results will inform discussion amongst researchers and policymakers about anticipating and managing trends in AI.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1705.08807 [cs]},
	author = {Grace, Katja and Salvatier, John and Dafoe, Allan and Zhang, Baobao and Evans, Owain},
	month = may,
	year = {2017},
	note = {arXiv: 1705.08807},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {Grace et al. - 2017 - When Will AI Exceed Human Performance Evidence fr.pdf:C\:\\Users\\Alex\\Zotero\\storage\\LWFNR5UF\\Grace et al. - 2017 - When Will AI Exceed Human Performance Evidence fr.pdf:application/pdf}
}

@inproceedings{off_switch,
  author    = {Dylan Hadfield-Menell and Anca Dragan and Pieter Abbeel and Stuart Russell},
  title     = {The Off-Switch Game},
  booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on
               Artificial Intelligence, {IJCAI-17}},
  pages     = {220--227},
  year      = {2017},
  doi       = {10.24963/ijcai.2017/32},
  url       = {https://doi.org/10.24963/ijcai.2017/32},
}


@book{russell2009artificial,
title={Artificial intelligence: a modern approach},
author={Russell, Stuart J and Norvig, Peter},
year={2009},
publisher={Pearson Education Limited}
}
@article{krakovna2018penalizing,
  title={Penalizing side effects using stepwise relative reachability},
  author={Krakovna, Victoria and Orseau, Laurent and Kumar, Ramana and Martic, Miljan and Legg, Shane},
  journal={arXiv preprint arXiv:1806.01186},
  year={2018}
}
@inproceedings{chow2015risk,
  title={Risk-sensitive and robust decision-making: a CVaR optimization approach},
  author={Chow, Yinlam and Tamar, Aviv and Mannor, Shie and Pavone, Marco},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1522--1530},
  year={2015}
}
@inproceedings{gillula2012guaranteed,
  title={Guaranteed safe online learning via reachability: tracking a ground target using a quadrotor},
  author={Gillula, Jeremy H and Tomlin, Claire J},
  booktitle={2012 IEEE International Conference on Robotics and Automation},
  pages={2723--2730},
  year={2012},
  organization={IEEE}
}

@article{everitt_self-modification_2016,
	title = {Self-{Modification} of {Policy} and {Utility} {Function} in {Rational} {Agents}},
	url = {http://arxiv.org/abs/1605.03142},
	abstract = {Any agent that is part of the environment it interacts with and has versatile actuators (such as arms and ﬁngers), will in principle have the ability to self-modify – for example by changing its own source code. As we continue to create more and more intelligent agents, chances increase that they will learn about this ability. The question is: will they want to use it? For example, highly intelligent systems may ﬁnd ways to change their goals to something more easily achievable, thereby ‘escaping’ the control of their designers. In an important paper, Omohundro (2008) argued that goal preservation is a fundamental drive of any intelligent system, since a goal is more likely to be achieved if future versions of the agent strive towards the same goal. In this paper, we formalise this argument in general reinforcement learning, and explore situations where it fails. Our conclusion is that the self-modiﬁcation possibility is harmless if and only if the value function of the agent anticipates the consequences of self-modiﬁcations and use the current utility function when evaluating the future.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1605.03142 [cs]},
	author = {Everitt, Tom and Filan, Daniel and Daswani, Mayank and Hutter, Marcus},
	month = may,
	year = {2016},
	note = {arXiv: 1605.03142},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Everitt et al. - 2016 - Self-Modification of Policy and Utility Function i.pdf:C\:\\Users\\Alex\\Zotero\\storage\\2276L4CA\\Everitt et al. - 2016 - Self-Modification of Policy and Utility Function i.pdf:application/pdf}
}

@inproceedings{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  booktitle={Advances in Neural Information Processing Systems},
  pages={4299--4307},
  year={2017}
}

@article{orseau_self-modification_nodate,
	title = {Self-{Modification} and {Mortality} in {Artificial} {Agents}},
	abstract = {This paper considers the consequences of endowing an intelligent agent with the ability to modify its own code. The intelligent agent is patterned closely after AIXI [1], but the environment has read-only access to the agent's description. On the basis of some simple modi cations to the utility and horizon functions, we are able to discuss and compare some very di erent kinds of agents, speci cally: reinforcement-learning, goal-seeking, predictive, and knowledge-seeking agents. In particular, we introduce what we call the  Simpleton Gambit  which allows us to discuss whether these agents would choose to modify themselves toward their own detriment.},
	language = {en},
	author = {Orseau, Laurent and Ring, Mark},
	pages = {10},
	file = {Orseau and Ring - Self-Modication and Mortality in Articial Agents.pdf:C\:\\Users\\Alex\\Zotero\\storage\\SN76EZZF\\Orseau and Ring - Self-Modication and Mortality in Articial Agents.pdf:application/pdf}
}

@article{abel_agent-agnostic_2017,
	title = {Agent-{Agnostic} {Human}-in-the-{Loop} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/1701.04079},
	abstract = {Providing Reinforcement Learning agents with expert advice can dramatically improve various aspects of learning. Prior work has developed teaching protocols that enable agents to learn efﬁciently in complex environments; many of these methods tailor the teacher’s guidance to agents with a particular representation or underlying learning scheme, offering effective but specialized teaching procedures. In this work, we explore protocol programs, an agent-agnostic schema for Human-in-the-Loop Reinforcement Learning. Our goal is to incorporate the beneﬁcial properties of a human teacher into Reinforcement Learning without making strong assumptions about the inner workings of the agent. We show how to represent existing approaches such as action pruning, reward shaping, and training in simulation as special cases of our schema and conduct preliminary experiments on simple domains.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1701.04079 [cs]},
	author = {Abel, David and Salvatier, John and Stuhlmüller, Andreas and Evans, Owain},
	month = jan,
	year = {2017},
	note = {arXiv: 1701.04079},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf:C\:\\Users\\Alex\\Zotero\\storage\\5ZAXGCUC\\Abel et al. - 2017 - Agent-Agnostic Human-in-the-Loop Reinforcement Lea.pdf:application/pdf}
}

@article{leike_ai_2017,
	title = {{AI} Safety Gridworlds},
	url = {},
	abstract = {We present a suite of reinforcement learning environments illustrating various safety properties of intelligent agents. These problems include safe interruptibility, avoiding side effects, absent supervisor, reward gaming, safe exploration, as well as robustness to self-modiﬁcation, distributional shift, and adversaries. To measure compliance with the intended safe behavior, we equip each environment with a performance function that is hidden from the agent. This allows us to categorize AI safety problems into robustness and speciﬁcation problems, depending on whether the performance function corresponds to the observed reward function. We evaluate A2C and Rainbow, two recent deep reinforcement learning agents, on our environments and show that they are not able to solve them satisfactorily.},
	language = {en},
	urldate = {2018-07-13},
	journal = {arXiv:1711.09883},
	author = {Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
	month = nov,
	year = {2017},
	note = {},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Leike et al. - 2017 - AI Safety Gridworlds.pdf:C\:\\Users\\Alex\\Zotero\\storage\\9T68QX5X\\Leike et al. - 2017 - AI Safety Gridworlds.pdf:application/pdf}
}

@incollection{dewey_learning_2011,
	address = {Berlin, Heidelberg},
	title = {Learning {What} to {Value}},
	volume = {6830},
	isbn = {978-3-642-22886-5 978-3-642-22887-2},
	url = {http://link.springer.com/10.1007/978-3-642-22887-2_35},
	abstract = {We examine ultraintelligent reinforcement learning agents. Reinforcement learning can only be used in the real world to define agents whose goal is to maximize expected rewards, and since this goal does not match with human goals, AGIs based on reinforcement learning will often work at cross-purposes to us. We define value learners, agents that can be designed to learn and maximize any initially unknown utility function so long as we provide them with an idea of what constitutes evidence about that utility function.},
	language = {en},
	urldate = {2018-07-13},
	booktitle = {Artificial {General} {Intelligence}},
	publisher = {Springer Berlin Heidelberg},
	author = {Dewey, Daniel},
	year = {2011},
	doi = {10.1007/978-3-642-22887-2_35},
	pages = {309--314},
	file = {Dewey - 2011 - Learning What to Value.pdf:C\:\\Users\\Alex\\Zotero\\storage\\G238HZ2D\\Dewey - 2011 - Learning What to Value.pdf:application/pdf}
}

@incollection{soares_agent_2017,
	address = {Berlin, Heidelberg},
	title = {Agent {Foundations} for {Aligning} {Machine} {Intelligence} with {Human} {Interests}: {A} {Technical} {Research} {Agenda}},
	isbn = {978-3-662-54031-2 978-3-662-54033-6},
	shorttitle = {Agent {Foundations} for {Aligning} {Machine} {Intelligence} with {Human} {Interests}},
	url = {http://link.springer.com/10.1007/978-3-662-54033-6_5},
	language = {en},
	urldate = {2018-07-13},
	booktitle = {The {Technological} {Singularity}},
	publisher = {Springer Berlin Heidelberg},
	author = {Soares, Nate and Fallenstein, Benya},
	year = {2017},
	doi = {10.1007/978-3-662-54033-6_5},
	pages = {103--125},
	file = {Soares and Fallenstein - 2017 - Agent Foundations for Aligning Machine Intelligenc.pdf:C\:\\Users\\Alex\\Zotero\\storage\\LJQF8RLS\\Soares and Fallenstein - 2017 - Agent Foundations for Aligning Machine Intelligenc.pdf:application/pdf}
}

@article{benson-tilsen_formalizing_nodate,
	title = {Formalizing {Convergent} {Instrumental} {Goals}},
	abstract = {Omohundro has argued that suﬃciently advanced AI systems of any design would, by default, have incentives to pursue a number of instrumentally useful subgoals, such as acquiring more computing power and amassing many resources. Omohundro refers to these as “basic AI drives,” and he, along with Bostrom and others, has argued that this means great care must be taken when designing powerful autonomous systems, because even if they have harmless goals, the side eﬀects of pursuing those goals may be quite harmful. These arguments, while intuitively compelling, are primarily philosophical. In this paper, we provide formal models that demonstrate Omohundro’s thesis, thereby putting mathematical weight behind those intuitive claims.},
	language = {en},
	author = {Benson-Tilsen, Tsvi and Soares, Nate},
	pages = {11},
	file = {Benson-Tilsen and Soares - Formalizing Convergent Instrumental Goals.pdf:C\:\\Users\\Alex\\Zotero\\storage\\DKPQ3GYD\\Benson-Tilsen and Soares - Formalizing Convergent Instrumental Goals.pdf:application/pdf}
}

@article{fern_decision-theoretic_nodate,
	title = {A {Decision}-{Theoretic} {Model} of {Assistance}},
	abstract = {There is a growing interest in intelligent assistants for a variety of applications from sorting email to helping people with disabilities to do their daily chores. In this paper, we formulate the problem of intelligent assistance in a decision-theoretic framework, and present both theoretical and empirical results. We ﬁrst introduce a class of POMDPs called hidden-goal MDPs (HGMDPs), which formalizes the problem of interactively assisting an agent whose goal is hidden and whose actions are observable. In spite of its restricted nature, we show that optimal action selection for HGMDPs is PSPACE-complete even for deterministic dynamics. We then introduce a more restricted model called helper action MDPs (HAMDPs), which are sufﬁcient for modeling many real-world problems. We show classes of HAMDPs for which efﬁcient algorithms are possible. More interestingly, for general HAMDPs we show that a simple myopic policy achieves a near optimal regret, compared to an oracle assistant that knows the agent’s goal. We then introduce more sophisticated versions of this policy for the general case of HGMDPs that we combine with a novel approach for quickly learning about the agent being assisted. We evaluate our approach in two game-like computer environments where human subjects perform tasks, and in a real-world domain of providing assistance during folder navigation in a computer desktop environment. The results show that in all three domains the framework results in an assistant that substantially reduces user effort with only modest computation.},
	language = {en},
	author = {Fern, Alan},
	pages = {34},
	file = {Fern - A Decision-Theoretic Model of Assistance.pdf:C\:\\Users\\Alex\\Zotero\\storage\\DAU6547L\\Fern - A Decision-Theoretic Model of Assistance.pdf:application/pdf}
}

@article{fallenstein_problems_nodate,
	title = {Problems of self-reference in self-improving space-time embedded intelligence},
	abstract = {By considering agents to be a part of their environment, Orseau and Ring’s space-time embedded intelligence [11] is a better ﬁt to the real world than the traditional agent framework. However, a selfmodifying AGI that sees future versions of itself as an ordinary part of the environment may run into problems of self-reference. We show that in one particular model based on formal logic, naive approaches either lead to incorrect reasoning that allows an agent to put oﬀ an important task forever (the procrastination paradox ), or fail to allow the agent to justify even obviously safe rewrites (the Lo¨bian obstacle). We argue that these problems have relevance beyond our particular formalism, and discuss partial solutions.},
	language = {en},
	author = {Fallenstein, Benja and Soares, Nate},
	pages = {12},
	file = {Fallenstein et Soares - Problems of self-reference in self-improving space.pdf:C\:\\Users\\Alex\\Zotero\\storage\\8A8XX44U\\Fallenstein et Soares - Problems of self-reference in self-improving space.pdf:application/pdf}
}

@article{hutter_universal_2007,
	title = {Universal {Algorithmic} {Intelligence}: {A} mathematical top-{\textgreater}down approach},
	shorttitle = {Universal {Algorithmic} {Intelligence}},
	url = {http://arxiv.org/abs/cs/0701125},
	abstract = {Sequential decision theory formally solves the problem of rational agents in uncertain worlds if the true environmental prior probability distribution is known. Solomonoff's theory of universal induction formally solves the problem of sequence prediction for unknown prior distribution. We combine both ideas and get a parameter-free theory of universal Artificial Intelligence. We give strong arguments that the resulting AIXI model is the most intelligent unbiased agent possible. We outline how the AIXI model can formally solve a number of problem classes, including sequence prediction, strategic games, function minimization, reinforcement and supervised learning. The major drawback of the AIXI model is that it is uncomputable. To overcome this problem, we construct a modified algorithm AIXItl that is still effectively more intelligent than any other time t and length l bounded agent. The computation time of AIXItl is of the order t x 2{\textasciicircum}l. The discussion includes formal definitions of intelligence order relations, the horizon problem and relations of the AIXI theory to other AI approaches.},
	urldate = {2018-07-14},
	journal = {arXiv:cs/0701125},
	author = {Hutter, Marcus},
	month = jan,
	year = {2007},
	note = {arXiv: cs/0701125},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv\:cs/0701125 PDF:C\:\\Users\\Alex\\Zotero\\storage\\69S2PBXU\\Hutter - 2007 - Universal Algorithmic Intelligence A mathematical.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Alex\\Zotero\\storage\\MW9TDN99\\0701125.html:text/html}
}

@article{amodei_concrete_2016,
	title = {Concrete problems in {AI} safety},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artificial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, defined as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of five practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function ("avoiding side effects" and "avoiding reward hacking"), an objective function that is too expensive to evaluate frequently ("scalable supervision"), or undesirable behavior during the learning process ("safe exploration" and "distributional shift"). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	urldate = {2018-07-14},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv\:1606.06565 PDF:C\:\\Users\\Alex\\Zotero\\storage\\34UG7XEB\\Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Alex\\Zotero\\storage\\HQ7TQFAD\\1606.html:text/html}
}

@article{soares_corrigibility_2015,
	title = {Corrigibility},
	journal = {AAAI Workshops},
	author = {Soares, Nate and Fallenstein, Benja and Armstrong, Stuart and Yudkowsky, Eliezer},
	year = {2015},
	file = {Corrigibility.pdf:C\:\\Users\\Alex\\Zotero\\storage\\NHSNK9PH\\Corrigibility.pdf:application/pdf}
}

@article{omohundro_basic_2008,
	title = {The basic {AI} drives},
	author = {Omohundro, Stephen},
	year = {2008},
	file = {The Basic AI Drives:C\:\\Users\\Alex\\Zotero\\storage\\JFIX5Y9C\\ai_drives_final.pdf:application/pdf}
}

@misc{arbital_ontology_nodate,
	title = {Ontology {Identification}},
	url = {https://arbital.com/p/ontology_identification/},
	journal = {Arbital},
	author = {{Arbital}}
}

@misc{turner_measuring_nodate,
	title = {Measuring {Impact} via {Ontological} {Whitelisting}},
	author = {Turner, Alexander},
	file = {whitelist-learning(1).pdf:C\:\\Users\\Alex\\Zotero\\storage\\JNKAD3P6\\whitelist-learning(1).pdf:application/pdf}
}

@article{leike_bad_2015,
	title = {Bad {Universal} {Priors} and {Notions} of {Optimality}},
	url = {http://arxiv.org/abs/1510.04931},
	abstract = {A big open question of algorithmic information theory is the choice of the universal Turing machine (UTM). For Kolmogorov complexity and Solomonoff induction we have invariance theorems: the choice of the UTM changes bounds only by a constant. For the universally intelligent agent AIXI (Hutter, 2005) no invariance theorem is known. Our results are entirely negative: we discuss cases in which unlucky or adversarial choices of the UTM cause AIXI to misbehave drastically. We show that Legg-Hutter intelligence and thus balanced Pareto optimality is entirely subjective, and that every policy is Pareto optimal in the class of all computable environments. This undermines all existing optimality properties for AIXI. While it may still serve as a gold standard for AI, our results imply that AIXI is a relative theory, dependent on the choice of the UTM.},
	urldate = {2018-07-16},
	journal = {arXiv:1510.04931 [cs]},
	author = {Leike, Jan and Hutter, Marcus},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.04931},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv\:1510.04931 PDF:C\:\\Users\\Alex\\Zotero\\storage\\VB4XVUXB\\Leike et Hutter - 2015 - Bad Universal Priors and Notions of Optimality.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Alex\\Zotero\\storage\\VH74TLXG\\1510.html:text/html}
}

@article{gruetzemacher_rethinking_nodate,
	title = {Rethinking {AI} {Strategy} and {Policy} as {Entangled} {Super} {Wicked} {Problems}},
	abstract = {This paper attempts a preliminary analysis of the general approach to AI strategy/policy research through the lens of wicked problems literature. Wicked problems are a class of social policy problems for which traditional methods of resolution fail. Super wicked problems refer to even more complex social policy problems, e.g. climate change. We first propose a hierarchy of three classes of AI strategy/policy problems, all wicked or super wicked problems. We next identify three independent super wicked problems in AI strategy/policy and propose that the most significant of these challenges – the development of safe and beneficial artificial general intelligence – to be significantly more complex and nuanced, thus posing a new degree of ‘wickedness.’ We then explore analysis and techniques for addressing wicked problems and super wicked problems. This leads to a discussion of the implications of these ideas on the problems of AI strategy/policy.},
	language = {en},
	author = {Gruetzemacher, Ross},
	pages = {6},
	file = {AIES_2018_paper_70.pdf:C\:\\Users\\Alex\\Zotero\\storage\\Q6IJRRWA\\AIES_2018_paper_70.pdf:application/pdf}
}

@article{garrabrant_logical_2016,
	title = {Logical {Induction}},
	url = {http://arxiv.org/abs/1609.03543},
	abstract = {We present a computable algorithm that assigns probabilities to every logical statement in a given formal language, and reﬁnes those probabilities over time. For instance, if the language is Peano arithmetic, it assigns probabilities to all arithmetical statements, including claims about the twin prime conjecture, the outputs of long-running computations, and its own probabilities. We show that our algorithm, an instance of what we call a logical inductor, satisﬁes a number of intuitive desiderata, including: (1) it learns to predict patterns of truth and falsehood in logical statements, often long before having the resources to evaluate the statements, so long as the patterns can be written down in polynomial time; (2) it learns to use appropriate statistical summaries to predict sequences of statements whose truth values appear pseudorandom; and (3) it learns to have accurate beliefs about its own current beliefs, in a manner that avoids the standard paradoxes of self-reference. For example, if a given computer program only ever produces outputs in a certain range, a logical inductor learns this fact in a timely manner; and if late digits in the decimal expansion of π are diﬃcult to predict, then a logical inductor learns to assign ≈ 10\% probability to “the nth digit of π is a 7” for large n. Logical inductors also learn to trust their future beliefs more than their current beliefs, and their beliefs are coherent in the limit (whenever φ → ψ, P∞(φ) ≤ P∞(ψ), and so on); and logical inductors strictly dominate the universal semimeasure in the limit. These properties and many others all follow from a single logical induction criterion, which is motivated by a series of stock trading analogies. Roughly speaking, each logical sentence φ is associated with a stock that is worth \$1 per share if φ is true and nothing otherwise, and we interpret the belief-state of a logically uncertain reasoner as a set of market prices, where Pn(φ) = 50\% means that on day n, shares of φ may be bought or sold from the reasoner for 50¢. The logical induction criterion says (very roughly) that there should not be any polynomial-time computable trading strategy with ﬁnite risk tolerance that earns unbounded proﬁts in that market over time. This criterion bears strong resemblance to the “no Dutch book” criteria that support both expected utility theory (von Neumann and Morgenstern 1944) and Bayesian probability theory (Ramsey 1931; de Finetti 1937).},
	language = {en},
	urldate = {2018-07-28},
	journal = {arXiv:1609.03543 [cs, math]},
	author = {Garrabrant, Scott and Benson-Tilsen, Tsvi and Critch, Andrew and Soares, Nate and Taylor, Jessica},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.03543},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Logic in Computer Science, Mathematics - Logic, Mathematics - Probability},
	file = {1609.03543.pdf:C\:\\Users\\Alex\\Zotero\\storage\\PQWWXN6G\\1609.03543.pdf:application/pdf}
}

@article{cohen_algorithm_nodate,
	title = {Algorithm for {Aligned} {Artiﬁcial} {General} {Intelligence}},
	abstract = {General intelligence, the ability to solve arbitrary problems, is supposed by many to be artiﬁcially constructible. Narrow intelligence, the ability to solve a particular particularly diﬃcult problem, has seen impressive recent development. Notable examples include self-driving cars, Go engines, image classiﬁers, and translators. Artiﬁcial General intelligence (AGI) presents dangers that narrow intelligence does not: if something smarter than us across every domain were indiﬀerent to our concerns, it would be an existential threat to humanity, just as we threaten many species despite no ill will. Even the theory of how to maintain the alignment of an AGI’s goals with our own has proven highly elusive. We present the ﬁrst algorithm we are aware of for aligned AGI. We thereby conﬁrm the constructibility of AGI with a construction, and identify an exception to the Instrumental Convergence Thesis, which is roughly that by default an AGI would seek power, including power over us.},
	language = {en},
	author = {Cohen, Michael and Vellambi, Badri and Hutter, Marcus},
	pages = {23},
	file = {Cohen et al. - Algorithm for Aligned Artiﬁcial General Intelligen.pdf:C\:\\Users\\Alex\\Zotero\\storage\\JCKMSZGQ\\Cohen et al. - Algorithm for Aligned Artiﬁcial General Intelligen.pdf:application/pdf}
}

@article{amodei_concrete_2016-1,
	title = {Concrete Problems in {AI} Safety},
	url = {http://arxiv.org/abs/1606.06565},
	abstract = {Rapid progress in machine learning and artiﬁcial intelligence (AI) has brought increasing attention to the potential impacts of AI technologies on society. In this paper we discuss one such potential impact: the problem of accidents in machine learning systems, deﬁned as unintended and harmful behavior that may emerge from poor design of real-world AI systems. We present a list of ﬁve practical research problems related to accident risk, categorized according to whether the problem originates from having the wrong objective function (“avoiding side eﬀects” and “avoiding reward hacking”), an objective function that is too expensive to evaluate frequently (“scalable supervision”), or undesirable behavior during the learning process (“safe exploration” and “distributional shift”). We review previous work in these areas as well as suggesting research directions with a focus on relevance to cutting-edge AI systems. Finally, we consider the high-level question of how to think most productively about the safety of forward-looking applications of AI.},
	language = {en},
	urldate = {2018-08-18},
	journal = {arXiv:1606.06565 [cs]},
	author = {Amodei, Dario and Olah, Chris and Steinhardt, Jacob and Christiano, Paul and Schulman, John and Mané, Dan},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.06565},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:C\:\\Users\\Alex\\Zotero\\storage\\E9SCJVN3\\Amodei et al. - 2016 - Concrete Problems in AI Safety.pdf:application/pdf}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv:1707.06347},
  year={2017}
}

@article{mnih2013playing,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

@incollection{salge2014empowerment,
  title={Empowerment--an introduction},
  author={Salge, Christoph and Glackin, Cornelius and Polani, Daniel},
  booktitle={Guided Self-Organization: Inception},
  pages={67--114},
  year={2014},
  publisher={Springer}
}

@misc{stuart_russell_myth_nodate,
	title = {The {Myth} {Of} {AI} {\textbar} {Edge}.org},
	year={2014},
	url = {https://www.edge.org/conversation/the-myth-of-ai#26015},
	urldate = {2018-10-17},
	author = {{Stuart Russell}},
	file = {The Myth Of AI | Edge.org:C\:\\Users\\Alex\\Zotero\\storage\\LXF8RS7U\\the-myth-of-ai.html:text/html}
}

@article{carey_incorrigibility_2017,
	title = {Incorrigibility in the {CIRL} framework},
	url = {http://arxiv.org/abs/1709.06275},
	abstract = {A value learning system has incentives to follow shutdown instructions, assuming the shutdown instruction provides information (in the technical sense) about which actions lead to valuable outcomes. However, this assumption is not robust to model mis-specification (e.g., in the case of programmer errors). We demonstrate this by presenting some Supervised POMDP scenarios in which errors in the parameterized reward function remove the incentive to follow shutdown commands. These difficulties parallel those discussed by Soares et al. (2015) in their paper on corrigibility. We argue that it is important to consider systems that follow shutdown commands under some weaker set of assumptions (e.g., that one small verified module is correctly implemented; as opposed to an entire prior probability distribution and/or parameterized reward function). We discuss some difficulties with simple ways to attempt to attain these sorts of guarantees in a value learning framework.},
	urldate = {2018-10-17},
	journal = {AI, Ethics, and Society},
	author = {Carey, Ryan},
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence},
}


@misc{gavin_leech_preventing_nodate,
	title = {Preventing side-effects in gridworlds},
	url = {https://www.gleech.org/grids/},
	urldate = {2018-10-20},
	year={2018},
	journal = {gleech.org},
	author = {Leech, Gavin and Kubicki, Karol and Cooper, Jessica and McGrath, Tom},
}

@inproceedings{bellemare2017distributional,
  title={A distributional perspective on reinforcement learning},
  author={Bellemare, Marc G and Dabney, Will and Munos, R{\'e}mi},
  booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
  pages={449--458},
  year={2017},
  organization={JMLR. org}
}

@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}