\begin{thebibliography}{52}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori and Szepesv{\'a}ri(2011)]{abbasi2011regret}
Yasin Abbasi-Yadkori and Csaba Szepesv{\'a}ri.
\newblock Regret bounds for the adaptive control of linear quadratic systems.
\newblock In \emph{Proceedings of the 24th Annual Conference on Learning
  Theory}, pages 1--26. JMLR Workshop and Conference Proceedings, 2011.

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock In \emph{NIPS}, volume~11, pages 2312--2320, 2011.

\bibitem[Agrawal and Jia(2017)]{agrawal2017optimistic}
Shipra Agrawal and Randy Jia.
\newblock Optimistic posterior sampling for reinforcement learning: worst-case
  regret bounds.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1184--1194, 2017.

\bibitem[Arbel and Gretton(2018)]{arbel2018kernel}
Michael Arbel and Arthur Gretton.
\newblock Kernel conditional exponential family.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1337--1346, 2018.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pages 463--474. PMLR, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'e}mi Munos.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning}, pages 263--272. PMLR, 2017.

\bibitem[Brooks et~al.(2011)Brooks, Gelman, Jones, and
  Meng]{brooks2011handbook}
Steve Brooks, Andrew Gelman, Galin Jones, and Xiao-Li Meng.
\newblock \emph{Handbook of markov chain monte carlo}.
\newblock CRC press, 2011.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Canu and Smola(2006)]{canu2006kernel}
St{\'e}phane Canu and Alex Smola.
\newblock Kernel methods and the exponential family.
\newblock \emph{Neurocomputing}, 69\penalty0 (7-9):\penalty0 714--720, 2006.

\bibitem[Carreira-Perpinan and Hinton(2005)]{carreira2005contrastive}
Miguel~A Carreira-Perpinan and Geoffrey Hinton.
\newblock On contrastive divergence learning.
\newblock In \emph{International workshop on artificial intelligence and
  statistics}, pages 33--40. PMLR, 2005.

\bibitem[Chowdhury et~al.(2021)Chowdhury, Gopalan, and Maillard]{chowdhury21}
Sayak~Ray Chowdhury, Aditya Gopalan, and Odalric-Ambrym Maillard.
\newblock Reinforcement learning in parametric mdps with exponential families.
\newblock In \emph{Proceedings of The 24th International Conference on
  Artificial Intelligence and Statistics}, pages 1855--1863, 2021.

\bibitem[Cobb et~al.(2019)Cobb, Baydin, Markham, and
  Roberts]{cobb2019hamiltorch}
Adam~D Cobb, At{\i}l{\i}m~G{\"u}ne{\c{s}} Baydin, Andrew Markham, and Stephen~J
  Roberts.
\newblock Introducing an explicit symplectic integration scheme for riemannian
  manifold hamiltonian monte carlo.
\newblock \emph{arXiv preprint arXiv:1910.06243}, 2019.

\bibitem[Cohen et~al.(2019)Cohen, Koren, and Mansour]{cohen2019learning}
Alon Cohen, Tomer Koren, and Yishay Mansour.
\newblock Learning linear-quadratic regulators efficiently with only sqrtt
  regret.
\newblock In \emph{International Conference on Machine Learning}, pages
  1300--1309. PMLR, 2019.

\bibitem[Dai et~al.(2019)Dai, Liu, Dai, He, Gretton, Song, and
  Schuurmans]{dai2019exponential}
Bo~Dai, Zhen Liu, Hanjun Dai, Niao He, Arthur Gretton, Le~Song, and Dale
  Schuurmans.
\newblock Exponential family estimation via adversarial dynamics embedding.
\newblock \emph{arXiv preprint arXiv:1904.12083}, 2019.

\bibitem[Dani et~al.()Dani, Hayes, and Kakade]{danistochastic}
Varsha Dani, Thomas~P Hayes, and Sham~M Kakade.
\newblock Stochastic linear optimization under bandit feedback.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  5713--5723, 2017.

\bibitem[Dean et~al.(2018)Dean, Mania, Matni, Recht, and Tu]{dean2018regret}
Sarah Dean, Horia Mania, Nikolai Matni, Benjamin Recht, and Stephen Tu.
\newblock Regret bounds for robust adaptive control of the linear quadratic
  regulator.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Dong et~al.(2020)Dong, Peng, Wang, and Zhou]{dong2020root}
Kefan Dong, Jian Peng, Yining Wang, and Yuan Zhou.
\newblock Root-n-regret for learning in markov decision processes with function
  approximation and low bellman rank.
\newblock In \emph{Conference on Learning Theory}, pages 1554--1557. PMLR,
  2020.

\bibitem[Fazel et~al.(2018)Fazel, Ge, Kakade, and Mesbahi]{fazel2018global}
Maryam Fazel, Rong Ge, Sham Kakade, and Mehran Mesbahi.
\newblock Global convergence of policy gradient methods for the linear
  quadratic regulator.
\newblock In \emph{International Conference on Machine Learning}, pages
  1467--1476. PMLR, 2018.

\bibitem[Foster et~al.(2021)Foster, Kakade, Qian, and
  Rakhlin]{foster2021statistical}
Dylan~J Foster, Sham~M Kakade, Jian Qian, and Alexander Rakhlin.
\newblock The statistical complexity of interactive decision making.
\newblock \emph{arXiv preprint arXiv:2112.13487}, 2021.

\bibitem[Fukumizu(2009)]{fukumizu2009exponential}
Kenji Fukumizu.
\newblock Exponential manifold by reproducing kernel hilbert spaces.
\newblock \emph{Algebraic and Geometric mothods in statistics}, pages 291--306,
  2009.

\bibitem[Hyv{\"a}rinen(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0
  (Apr):\penalty0 695--709, 2005.

\bibitem[Hyv{\"a}rinen(2007)]{hyvarinen2007some}
Aapo Hyv{\"a}rinen.
\newblock Some extensions of score matching.
\newblock \emph{Computational statistics \& data analysis}, 51\penalty0
  (5):\penalty0 2499--2512, 2007.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 11\penalty0 (4), 2010.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert~E
  Schapire.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pages
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143, 2020.

\bibitem[Kakade et~al.(2020)Kakade, Krishnamurthy, Lowrey, Ohnishi, and
  Sun]{kakade2020information}
Sham Kakade, Akshay Krishnamurthy, Kendall Lowrey, Motoya Ohnishi, and Wen Sun.
\newblock Information theoretic regret bounds for online nonlinear control.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  15312--15325, 2020.

\bibitem[Kober et~al.(2013)Kober, Bagnell, and Peters]{kober2013reinforcement}
Jens Kober, J~Andrew Bagnell, and Jan Peters.
\newblock Reinforcement learning in robotics: A survey.
\newblock \emph{The International Journal of Robotics Research}, 32\penalty0
  (11):\penalty0 1238--1274, 2013.

\bibitem[Lattimore and Szepesv{\'a}ri(2020)]{lattimore2020bandit}
Tor Lattimore and Csaba Szepesv{\'a}ri.
\newblock \emph{Bandit algorithms}.
\newblock Cambridge University Press, 2020.

\bibitem[Lattimore et~al.(2020)Lattimore, Szepesvari, and
  Weisz]{lattimore2020learning}
Tor Lattimore, Csaba Szepesvari, and Gellert Weisz.
\newblock Learning with good feature representations in bandits and in rl with
  a generative model.
\newblock In \emph{International Conference on Machine Learning}, pages
  5662--5670. PMLR, 2020.

\bibitem[Lyu(2009)]{lyu2009interpretation}
Siwei Lyu.
\newblock Interpretation and generalization of score matching.
\newblock In \emph{Proceedings of the Twenty-Fifth Conference on Uncertainty in
  Artificial Intelligence}, pages 359--366, 2009.

\bibitem[Mania et~al.(2019)Mania, Tu, and Recht]{mania2019certainty}
Horia Mania, Stephen Tu, and Benjamin Recht.
\newblock Certainty equivalence is efficient for linear quadratic control.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Mania et~al.(2020)Mania, Jordan, and Recht]{mania2020active}
Horia Mania, Michael~I Jordan, and Benjamin Recht.
\newblock Active learning for nonlinear system identification with guarantees.
\newblock \emph{arXiv preprint arXiv:2006.10277}, 2020.

\bibitem[Mayne(2014)]{mayne2014model}
David~Q Mayne.
\newblock Model predictive control: Recent developments and future promise.
\newblock \emph{Automatica}, 50\penalty0 (12):\penalty0 2967--2986, 2014.

\bibitem[Mnih et~al.(2013)Mnih, Kavukcuoglu, Silver, Graves, Antonoglou,
  Wierstra, and Riedmiller]{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing atari with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1312.5602}, 2013.

\bibitem[Modi et~al.(2020)Modi, Jiang, Tewari, and Singh]{modi2020sample}
Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh.
\newblock Sample complexity of reinforcement learning using linearly combined
  model ensembles.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 2010--2020. PMLR, 2020.

\bibitem[Osband and Van~Roy(2014)]{osband2014model}
Ian Osband and Benjamin Van~Roy.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  1466--1474, 2014.

\bibitem[Osband et~al.(2016)Osband, Van~Roy, and Wen]{osband2016generalization}
Ian Osband, Benjamin Van~Roy, and Zheng Wen.
\newblock Generalization and exploration via randomized value functions.
\newblock In \emph{International Conference on Machine Learning}, pages
  2377--2386, 2016.

\bibitem[Rao(2009)]{rao2009survey}
Anil~V Rao.
\newblock A survey of numerical methods for optimal control.
\newblock \emph{Advances in the Astronautical Sciences}, 135\penalty0
  (1):\penalty0 497--528, 2009.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock In \emph{NIPS}, pages 2256--2264, 2013.

\bibitem[Silver et~al.(2017)Silver, Schrittwieser, Simonyan, Antonoglou, Huang,
  Guez, Hubert, Baker, Lai, Bolton, et~al.]{silver2017mastering}
David Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja
  Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton,
  et~al.
\newblock Mastering the game of go without human knowledge.
\newblock \emph{nature}, 550\penalty0 (7676):\penalty0 354--359, 2017.

\bibitem[Simchowitz and Foster(2020)]{simchowitz2020naive}
Max Simchowitz and Dylan Foster.
\newblock Naive exploration is optimal for online lqr.
\newblock In \emph{International Conference on Machine Learning}, pages
  8937--8948. PMLR, 2020.

\bibitem[Srinivas et~al.(2009)Srinivas, Krause, Kakade, and
  Seeger]{srinivas2009gaussian}
Niranjan Srinivas, Andreas Krause, Sham~M Kakade, and Matthias Seeger.
\newblock Gaussian process optimization in the bandit setting: No regret and
  experimental design.
\newblock \emph{arXiv preprint arXiv:0912.3995}, 2009.

\bibitem[Sriperumbudur et~al.(2017)Sriperumbudur, Fukumizu, Gretton,
  Hyv{\"a}rinen, and Kumar]{sriperumbudur2017density}
Bharath Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Aapo Hyv{\"a}rinen, and
  Revant Kumar.
\newblock Density estimation in infinite dimensional exponential families.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 1830--1888, 2017.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on Learning Theory}, pages 2898--2933. PMLR,
  2019.

\bibitem[Sutherland et~al.(2018)Sutherland, Strathmann, Arbel, and
  Gretton]{sutherland2018efficient}
Danica~J. Sutherland, Heiko Strathmann, Michael Arbel, and Arthur Gretton.
\newblock Efficient and principled score estimation with nystr{\"o}m kernel
  exponential families.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 652--660. PMLR, 2018.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Szepesv{\'a}ri(2010)]{szepesvari2010algorithms}
Csaba Szepesv{\'a}ri.
\newblock Algorithms for reinforcement learning.
\newblock \emph{Synthesis lectures on artificial intelligence and machine
  learning}, 4\penalty0 (1):\penalty0 1--103, 2010.

\bibitem[Wagener et~al.(2019)Wagener, Cheng, Sacks, and
  Boots]{wagener2019online}
Nolan Wagener, Ching-An Cheng, Jacob Sacks, and Byron Boots.
\newblock An online learning approach to model predictive control.
\newblock \emph{arXiv preprint arXiv:1902.08967}, 2019.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6123--6135, 2020.

\bibitem[Yang and Wang(2019)]{yang2019reinforcement}
Lin~F Yang and Mengdi Wang.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \emph{arXiv preprint arXiv:1905.10389}, 2019.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent bellman error.
\newblock In \emph{International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020.

\end{thebibliography}
