\begin{thebibliography}{10}

\bibitem{DBLP:conf/nips/transformer}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em NIPS}, pages 5998--6008, 2017.

\bibitem{DBLP:conf/nips/gpt3}
Tom~B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,
  Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda
  Askell, Sandhini Agarwal, Ariel Herbert{-}Voss, Gretchen Krueger, Tom
  Henighan, Rewon Child, Aditya Ramesh, Daniel~M. Ziegler, Jeffrey Wu, Clemens
  Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott
  Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec
  Radford, Ilya Sutskever, and Dario Amodei.
\newblock Language models are few-shot learners.
\newblock In {\em NeurIPS}, 2020.

\bibitem{DBLP:journals/corr/gpt4}
OpenAI.
\newblock {GPT-4} technical report.
\newblock {\em CoRR}, abs/2303.08774, 2023.

\bibitem{DBLP:journals/corr/llama}
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
  Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
  Dan Bikel, Lukas Blecher, Cristian Canton{-}Ferrer, Moya Chen, Guillem
  Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,
  Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar
  Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,
  Isabel Kloumann, Artem Korenev, Punit~Singh Koura, Marie{-}Anne Lachaux,
  Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier
  Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew
  Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan
  Silva, Eric~Michael Smith, Ranjan Subramanian, Xiaoqing~Ellen Tan, Binh Tang,
  Ross Taylor, Adina Williams, Jian~Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan
  Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang,
  Aur{\'{e}}lien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock {\em CoRR}, abs/2307.09288, 2023.

\bibitem{chowdhery2023palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,
  Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian
  Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em Journal of Machine Learning Research}, 24(240):1--113, 2023.

\bibitem{team2023gemini}
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac,
  Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew~M Dai, Anja Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock {\em arXiv preprint arXiv:2312.11805}, 2023.

\bibitem{ramesh2021DALLE}
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec
  Radford, Mark Chen, and Ilya Sutskever.
\newblock Zero-shot text-to-image generation.
\newblock In {\em ICML}, pages 8821--8831, 2021.

\bibitem{lee2022autoregressive}
Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han.
\newblock Autoregressive image generation using residual quantization.
\newblock In {\em CVPR}, pages 11523--11532, 2022.

\bibitem{tian2024visual}
Keyu Tian, Yi~Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang.
\newblock Visual autoregressive modeling: Scalable image generation via
  next-scale prediction.
\newblock {\em arXiv preprint arXiv:2404.02905}, 2024.

\bibitem{DBLP:conf/icml/imagegpt}
Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and
  Ilya Sutskever.
\newblock Generative pretraining from pixels.
\newblock In {\em ICML}, volume 119, pages 1691--1703, 2020.

\bibitem{DBLP:journals/corr/gpt4v}
Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung{-}Ching Lin, Zicheng
  Liu, and Lijuan Wang.
\newblock The dawn of lmms: Preliminary explorations with gpt-4v(ision).
\newblock {\em CoRR}, abs/2309.17421, 2023.

\bibitem{DBLP:conf/nips/visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping
  Luo, Tong Lu, Jie Zhou, Yu~Qiao, and Jifeng Dai.
\newblock Visionllm: Large language model is also an open-ended decoder for
  vision-centric tasks.
\newblock In {\em NeurIPS}, 2023.

\bibitem{DBLP:conf/nips/LLAVA}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual instruction tuning.
\newblock In {\em NeurIPS}, 2023.

\bibitem{DBLP:journals/corr/anygpt}
Jun Zhan, Junqi Dai, Jiasheng Ye, Yunhua Zhou, Dong Zhang, Zhigeng Liu, Xin
  Zhang, Ruibin Yuan, Ge~Zhang, Linyang Li, Hang Yan, Jie Fu, Tao Gui,
  Tianxiang Sun, Yugang Jiang, and Xipeng Qiu.
\newblock Anygpt: Unified multimodal {LLM} with discrete sequence modeling.
\newblock {\em CoRR}, abs/2402.12226, 2024.

\bibitem{DBLP:journals/corr/gemini15}
Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy~P.
  Lillicrap, Jean{-}Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan
  Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian
  Borgeaud, Andrew~M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault
  Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James
  Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy,
  Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica
  Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen
  Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand,
  Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav
  Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and
  et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of
  tokens of context.
\newblock {\em CoRR}, abs/2403.05530, 2024.

\bibitem{DBLP:journals/corr/mesa-opt}
Johannes von Oswald, Eyvind Niklasson, Maximilian Schlegel, Seijin Kobayashi,
  Nicolas Zucchet, Nino Scherrer, Nolan Miller, Mark Sandler,
  Blaise~Ag{\"{u}}era y~Arcas, Max Vladymyrov, Razvan Pascanu, and Jo{\~{a}}o
  Sacramento.
\newblock Uncovering mesa-optimization algorithms in transformers.
\newblock {\em CoRR}, abs/2309.05858, 2023.

\bibitem{DBLP:journals/corr/taiji-AR}
Michael~Eli Sander, Raja Giryes, Taiji Suzuki, Mathieu Blondel, and Gabriel
  Peyr{\'{e}}.
\newblock How do transformers perform in-context autoregressive learning ?
\newblock In {\em {ICML}}, 2024.

\bibitem{DBLP:journals/corr/Hub-mesa-opt}
Evan Hubinger, Chris van Merwijk, Vladimir Mikulik, Joar Skalse, and Scott
  Garrabrant.
\newblock Risks from learned optimization in advanced machine learning systems.
\newblock {\em CoRR}, abs/1906.01820, 2019.

\bibitem{DBLP:conf/nips/distribution-ICL-22}
Stephanie C.~Y. Chan, Adam Santoro, Andrew~K. Lampinen, Jane~X. Wang,
  Aaditya~K. Singh, Pierre~H. Richemond, James~L. McClelland, and Felix Hill.
\newblock Data distributional properties drive emergent in-context learning in
  transformers.
\newblock In {\em NeurIPS}, 2022.

\bibitem{DBLP:journals/corr/ICLdiscrete}
Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade.
\newblock Understanding in-context learning in transformers and llms by
  learning to learn discrete functions.
\newblock In {\em {ICLR}}, 2024.

\bibitem{DBLP:journals/corr/ICL-OOD-2023}
Kartik Ahuja and David Lopez{-}Paz.
\newblock A closer look at in-context learning under distribution shifts.
\newblock {\em CoRR}, abs/2305.16704, 2023.

\bibitem{DBLP:conf/emnlp/MinLHALHZ22-emnlp}
Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh
  Hajishirzi, and Luke Zettlemoyer.
\newblock Rethinking the role of demonstrations: What makes in-context learning
  work?
\newblock In {\em {EMNLP}}, pages 11048--11064, 2022.

\bibitem{mahdavi2024revisiting}
Sadegh Mahdavi, Renjie Liao, and Christos Thrampoulidis.
\newblock Revisiting the equivalence of in-context learning and gradient
  descent: The impact of data distribution.
\newblock In {\em ICASSP}, pages 7410--7414. IEEE, 2024.

\bibitem{zhang2024ICL}
Ruiqi Zhang, Spencer Frei, and Peter~L Bartlett.
\newblock Trained transformers learn linear models in-context.
\newblock {\em Journal of Machine Learning Research}, 25(49):1--55, 2024.

\bibitem{DBLP:conf/icml/Osw-ICL}
Johannes von Oswald, Eyvind Niklasson, Ettore Randazzo, Jo{\~{a}}o Sacramento,
  Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov.
\newblock Transformers learn in-context by gradient descent.
\newblock In {\em ICML}, volume 202, pages 35151--35174, 2023.

\bibitem{DBLP:conf/nips/Ahn-23-ICL}
Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra.
\newblock Transformers learn to implement preconditioned gradient descent for
  in-context learning.
\newblock In {\em NeurIPS}, 2023.

\bibitem{DBLP:journals/corr/Ahn-linearatten}
Kwangjun Ahn, Xiang Cheng, Minhak Song, Chulhee Yun, Ali Jadbabaie, and Suvrit
  Sra.
\newblock Linear attention is (maybe) all you need (to understand transformer
  optimization).
\newblock In {\em {ICLR}}, 2024.

\bibitem{DBLP:journals/corr/tengyu-ICL}
Arvind~V. Mahankali, Tatsunori Hashimoto, and Tengyu Ma.
\newblock One step of gradient descent is provably the optimal in-context
  learner with one layer of linear self-attention.
\newblock In {\em {ICLR}}, 2024.

\bibitem{DBLP:journals/corr/jingfeng-ICL}
Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and
  Peter~L. Bartlett.
\newblock How many pretraining tasks are needed for in-context learning of
  linear regression?
\newblock In {\em {ICLR}}, 2024.

\bibitem{DBLP:journals/corr/huang-ICL}
Yu~Huang, Yuan Cheng, and Yingbin Liang.
\newblock In-context convergence of transformers.
\newblock In {\em {ICML}}, 2024.

\bibitem{DBLP:conf/nips/gargICL}
Shivam Garg, Dimitris Tsipras, Percy Liang, and Gregory Valiant.
\newblock What can transformers learn in-context? {A} case study of simple
  function classes.
\newblock In {\em NeurIPS}, 2022.

\bibitem{DBLP:conf/acl/mesa-ICL-ACL}
Damai Dai, Yutao Sun, Li~Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei.
\newblock Why can {GPT} learn in-context? language models secretly perform
  gradient descent as meta-optimizers.
\newblock In {\em Findings of {ACL}}, pages 4005--4019. Association for
  Computational Linguistics, 2023.

\bibitem{DBLP:conf/iclr/AkyurekSA0Z23ICL}
Ekin Aky{\"{u}}rek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou.
\newblock What learning algorithm is in-context learning? investigations with
  linear models.
\newblock In {\em {ICLR}}, 2023.

\bibitem{DBLP:journals/corr/Rongge}
Max Vladymyrov, Johannes von Oswald, Mark Sandler, and Rong Ge.
\newblock Linear transformers are versatile in-context learners.
\newblock {\em CoRR}, abs/2402.14180, 2024.

\bibitem{DBLP:conf/iclr/TransBayesian}
Samuel M{\"{u}}ller, Noah Hollmann, Sebastian Pineda{-}Arango, Josif Grabocka,
  and Frank Hutter.
\newblock Transformers can do bayesian inference.
\newblock In {\em {ICLR}}, 2022.

\bibitem{DBLP:journals/corr/Fu2023high}
Deqing Fu, Tian{-}Qi Chen, Robin Jia, and Vatsal Sharan.
\newblock Transformers learn higher-order optimization methods for in-context
  learning: {A} study with linear models.
\newblock {\em CoRR}, abs/2310.17086, 2023.

\bibitem{DBLP:journals/corr/Giannou23newton}
Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, and Jason~D.
  Lee.
\newblock How well can transformers emulate in-context newton's method?
\newblock {\em CoRR}, abs/2403.03183, 2024.

\bibitem{DBLP:conf/iclr/XieRL022}
Sang~Michael Xie, Aditi Raghunathan, Percy Liang, and Tengyu Ma.
\newblock An explanation of in-context learning as implicit bayesian inference.
\newblock In {\em {ICLR}}, 2022.

\bibitem{DBLP:journals/corr/wang-topic}
Xinyi Wang, Wanrong Zhu, and William~Yang Wang.
\newblock Large language models are implicitly topic models: Explaining and
  finding good demonstrations for in-context learning.
\newblock {\em CoRR}, abs/2301.11916, 2023.

\bibitem{DBLP:conf/nips/WiesLS23}
Noam Wies, Yoav Levine, and Amnon Shashua.
\newblock The learnability of in-context learning.
\newblock In {\em NeurIPS}, 2023.

\bibitem{DBLP:conf/icml/LiLR23-topic}
Yuchen Li, Yuanzhi Li, and Andrej Risteski.
\newblock How do transformers learn topic structure: Towards a mechanistic
  understanding.
\newblock In {\em {ICML}}, volume 202, pages 19689--19729, 2023.

\bibitem{DBLP:conf/icml/LiIPO23ICLstability}
Yingcong Li, Muhammed~Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak.
\newblock Transformers as algorithms: Generalization and stability in
  in-context learning.
\newblock In {\em {ICML}}, volume 202, pages 19565--19594, 2023.

\bibitem{DBLP:journals/corr/christos-AR}
Christos Thrampoulidis.
\newblock Implicit bias of next-token prediction.
\newblock {\em CoRR}, abs/2402.18551, 2024.

\bibitem{li2024AR}
Yingcong Li, Yixiao Huang, Muhammed~E Ildiz, Ankit~Singh Rawat, and Samet
  Oymak.
\newblock Mechanics of next token prediction with self-attention.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 685--693, 2024.

\bibitem{DBLP:journals/corr/han-ICL-kernel}
Chi Han, Ziqi Wang, Han Zhao, and Heng Ji.
\newblock In-context learning of large language models explained as kernel
  regression.
\newblock {\em CoRR}, abs/2305.12766, 2023.

\bibitem{DBLP:conf/nips/AroraCHL19}
Sanjeev Arora, Nadav Cohen, Wei Hu, and Yuping Luo.
\newblock Implicit regularization in deep matrix factorization.
\newblock In {\em NeurIPS}, pages 7411--7422, 2019.

\bibitem{shah2023learningDDPM}
Kulin Shah, Sitan Chen, and Adam Klivans.
\newblock Learning mixtures of gaussians using the ddpm objective.
\newblock {\em NeurIPS}, 36:19636--19649, 2023.

\bibitem{he2022information}
Haiyun He, Hanshu Yan, and Vincent~YF Tan.
\newblock Information-theoretic characterization of the generalization error
  for iterative semi-supervised learning.
\newblock {\em Journal of Machine Learning Research}, 23(287):1--52, 2022.

\bibitem{schmidt2018adversarially}
Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and
  Aleksander Madry.
\newblock Adversarially robust generalization requires more data.
\newblock {\em NeurIPS}, 31, 2018.

\bibitem{zheng2024toward}
Chenyu Zheng, Guoqiang Wu, and Chongxuan Li.
\newblock Toward understanding generative data augmentation.
\newblock {\em NeurIPS}, 36, 2024.

\bibitem{DBLP:conf/icml/ZhengWBCLZ23}
Chenyu Zheng, Guoqiang Wu, Fan Bao, Yue Cao, Chongxuan Li, and Jun Zhu.
\newblock Revisiting discriminative vs. generative classifiers: Theory and
  implications.
\newblock In {\em {ICML}}, volume 202, pages 42420--42477. {PMLR}, 2023.

\bibitem{DBLP:journals/corr/CuiMultiAtten}
Yingqian Cui, Jie Ren, Pengfei He, Jiliang Tang, and Yue Xing.
\newblock Superiority of multi-head attention in in-context linear regression.
\newblock {\em CoRR}, abs/2401.17426, 2024.

\bibitem{DBLP:journals/corr/chenMultiAtten}
Siyu Chen, Heejune Sheen, Tianhao Wang, and Zhuoran Yang.
\newblock Training dynamics of multi-head softmax attention for in-context
  learning: Emergence, convergence, and optimality (extended abstract).
\newblock In Shipra Agrawal and Aaron Roth, editors, {\em COLT}, volume 247,
  page 4573, 2024.

\bibitem{DBLP:journals/corr/DeoraMultiAtten}
Puneesh Deora, Rouzbeh Ghaderi, Hossein Taheri, and Christos Thrampoulidis.
\newblock On the optimization and generalization of multi-head attention.
\newblock {\em CoRR}, abs/2310.12680, 2023.

\bibitem{DBLP:journals/corr/Jason2024causal}
Eshaan Nichani, Alex Damian, and Jason~D. Lee.
\newblock How transformers learn causal structure with gradient descent.
\newblock {\em CoRR}, abs/2402.14735, 2024.

\bibitem{DBLP:journals/corr/zhang2024transformerblock}
Ruiqi Zhang, Jingfeng Wu, and Peter~L. Bartlett.
\newblock In-context learning of a linear transformer block: Benefits of the
  {MLP} component and one-step {GD} initialization.
\newblock {\em CoRR}, abs/2402.14951, 2024.

\bibitem{DBLP:journals/corr/hongkang-ICL-2024}
Hongkang Li, Meng Wang, Songtao Lu, Xiaodong Cui, and Pin{-}Yu Chen.
\newblock Training nonlinear transformers for efficient in-context learning:
  {A} theoretical learning and generalization analysis.
\newblock {\em CoRR}, abs/2402.15607, 2024.

\bibitem{DBLP:journals/corr/taiji-nonlinear-ICL}
Juno Kim and Taiji Suzuki.
\newblock Transformers learn nonlinear features in context: Nonconvex
  mean-field dynamics on the attention landscape.
\newblock In {\em {ICML}}, 2024.

\bibitem{DBLP:journals/corr/deep-linear-net}
Andrew~M. Saxe, James~L. McClelland, and Surya Ganguli.
\newblock Exact solutions to the nonlinear dynamics of learning in deep linear
  neural networks.
\newblock In {\em {ICLR}}, 2014.

\bibitem{polyak1963gradient}
Boris~Teodorovich Polyak.
\newblock Gradient methods for minimizing functionals.
\newblock {\em Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  3(4):643--653, 1963.

\bibitem{DBLP:conf/pkdd/KarimiNS16}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-{\l}ojasiewicz condition.
\newblock In {\em {ECML} {PKDD}}, volume 9851, pages 795--811, 2016.

\bibitem{petersen2008matrix}
Kaare~Brandt Petersen, Michael~Syskind Pedersen, et~al.
\newblock The matrix cookbook.
\newblock {\em Technical University of Denmark}, 7(15):510, 2008.

\end{thebibliography}
