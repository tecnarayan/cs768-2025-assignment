@article{mccandlish2018empirical,
  title={An empirical model of large-batch training},
  author={McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Team, OpenAI Dota},
  journal={arXiv preprint arXiv:1812.06162},
  year={2018}
}
@misc{chatgptpoem,
author = {Newman, Lily Hay and Greenberg, Andy},
title = {ChatGPT Spit Out Sensitive Data When Told to Repeat 'Poem' Forever},
howpublished = {\url{https://www.wired.com/story/chatgpt-poem-forever-security-roundup/}},
year = {2023}
}

@misc{starcodermem,
author = {Daniel Huynh},
title = {StarCoder Memorization Experiment Highlights Privacy Risks of Fine-Tuning On Code},
howpublished = {\url{https://huggingface.co/blog/dhuynh95/starcoder-memorization-experiment}},
year = {2023}
}
@article{kurakin2022toward,
  title={Toward Training at ImageNet Scale with Differential Privacy},
  author={Kurakin, Alexey and Chien, Steve and Song, Shuang and Geambasu, Roxana and Terzis, Andreas and Thakurta, Abhradeep},
  journal={arXiv preprint arXiv:2201.12328},
  year={2022}
}
@inproceedings{mcmahan2018learning,
  title={Learning Differentially Private Recurrent Language Models},
  author={McMahan, H Brendan and Ramage, Daniel and Talwar, Kunal and Zhang, Li},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{anonymous2023exploring,
  title={Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping},
  author={He, Jiyan and Li, Xuechen and Yu, Da and Zhang, Huishuai and Kulkarni, Janardhan and Lee, Yin Tat and Backurs, Arturs and Yu, Nenghai and Bian, Jiang},
  booktitle={The Eleventh International Conference on Learning Representations}
}
@inproceedings{rasley2020deepspeed,
  title={Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters},
  author={Rasley, Jeff and Rajbhandari, Samyam and Ruwase, Olatunji and He, Yuxiong},
  booktitle={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={3505--3506},
  year={2020}
}
@article{huang2019gpipe,
  title={Gpipe: Efficient training of giant neural networks using pipeline parallelism},
  author={Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Dehao and Chen, Mia and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V and Wu, Yonghui and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{frostig2018compiling,
  title={Compiling machine learning programs via high-level tracing},
  author={Frostig, Roy and Johnson, Matthew James and Leary, Chris},
  journal={Systems for Machine Learning},
  volume={4},
  number={9},
  year={2018},
  publisher={SysML}
}
@article{bradbury2018jax,
  title={JAX: composable transformations of Python+ NumPy programs},
  author={Bradbury, James and Frostig, Roy and Hawkins, Peter and Johnson, Matthew James and Leary, Chris and Maclaurin, Dougal and Necula, George and Paszke, Adam and VanderPlas, Jake and Wanderman-Milne, Skye and others},
  year={2018}
}
@article{li13pytorch,
  title={PyTorch Distributed: Experiences on Accelerating Data Parallel Training},
  author={Li, Shen and Zhao, Yanli and Varma, Rohan and Salpekar, Omkar and Noordhuis, Pieter and Li, Teng and Paszke, Adam and Smith, Jeff and Vaughan, Brian and Damania, Pritam and others},
  journal={Proceedings of the VLDB Endowment},
  volume={13},
  number={12}
}
@article{mcmahan2018general,
  title={A general approach to adding differential privacy to iterative training procedures},
  author={McMahan, H Brendan and Andrew, Galen and Erlingsson, Ulfar and Chien, Steve and Mironov, Ilya and Papernot, Nicolas and Kairouz, Peter},
  journal={arXiv preprint arXiv:1812.06210},
  year={2018}
}
@article{mandic2004generalized,
  title={A generalized normalized gradient descent algorithm},
  author={Mandic, Danilo P},
  journal={IEEE signal processing letters},
  volume={11},
  number={2},
  pages={115--118},
  year={2004},
  publisher={IEEE}
}
@inproceedings{you2019large,
  title={Large Batch Optimization for Deep Learning: Training BERT in 76 minutes},
  author={You, Yang and Li, Jing and Reddi, Sashank and Hseu, Jonathan and Kumar, Sanjiv and Bhojanapalli, Srinadh and Song, Xiaodan and Demmel, James and Keutzer, Kurt and Hsieh, Cho-Jui},
  booktitle={International Conference on Learning Representations},
  year={2019}
}
@book{nesterov2003introductory,
  title={Introductory lectures on convex optimization: A basic course},
  author={Nesterov, Yurii},
  volume={87},
  year={2003},
  publisher={Springer Science \& Business Media}
}
@misc{koala_blogpost_2023,
  author = {Xinyang Geng and Arnav Gudibande and Hao Liu and Eric Wallace and Pieter Abbeel and Sergey Levine and Dawn Song},
  title = {Koala: A Dialogue Model for Academic Research},
  howpublished = {Blog post},
  month = {April},
  year = {2023},
  url = {https://bair.berkeley.edu/blog/2023/04/03/koala/},
  urldate = {2023-04-03}
}
@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}
@article{taori2023alpaca,
  title={Alpaca: A strong, replicable instruction-following model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
  journal={Stanford Center for Research on Foundation Models. https://crfm. stanford. edu/2023/03/13/alpaca. html},
  volume={3},
  number={6},
  pages={7},
  year={2023}
}
@article{smith2022using,
  title={Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model},
  author={Smith, Shaden and Patwary, Mostofa and Norick, Brandon and LeGresley, Patrick and Rajbhandari, Samyam and Casper, Jared and Liu, Zhun and Prabhumoye, Shrimai and Zerveas, George and Korthikanti, Vijay and others},
  journal={arXiv preprint arXiv:2201.11990},
  year={2022}
}
@software{gpt-neo,
  author       = {Black, Sid and
                  Gao, Leo and
                  Wang, Phil and
                  Leahy, Connor and
                  Biderman, Stella},
  title        = {{GPT-Neo: Large Scale Autoregressive Language 
                   Modeling with Mesh-Tensorflow}},
  month        = mar,
  year         = 2021,
  note         = {{If you use this software, please cite it using 
                   these metadata.}},
  publisher    = {Zenodo},
  version      = {1.0},
  doi          = {10.5281/zenodo.5297715},
  url          = {https://doi.org/10.5281/zenodo.5297715}
}

@article{pile,
    title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
    author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
    journal={arXiv preprint arXiv:2101.00027},
    year={2020}
}
@inproceedings{kolesnikov2020big,
  title={Big Transfer (BiT): General Visual Representation Learning},
  author={Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  booktitle={European Conference on Computer Vision},
  pages={491--507},
  year={2020}
}
@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}
@inproceedings{zhai2022scaling,
  title={Scaling vision transformers},
  author={Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12104--12113},
  year={2022}
}
@inproceedings{pham2021meta,
  title={Meta pseudo labels},
  author={Pham, Hieu and Dai, Zihang and Xie, Qizhe and Le, Quoc V},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={11557--11568},
  year={2021}
}
@article{tolstikhin2021mlp,
  title={Mlp-mixer: An all-mlp architecture for vision},
  author={Tolstikhin, Ilya O and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and others},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={24261--24272},
  year={2021}
}
@inproceedings{sun2017revisiting,
  title={Revisiting unreasonable effectiveness of data in deep learning era},
  author={Sun, Chen and Shrivastava, Abhinav and Singh, Saurabh and Gupta, Abhinav},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={843--852},
  year={2017}
}
@article{li2022does,
  title={When Does Differentially Private Learning Not Suffer in High Dimensions?},
  author={Li, Xuechen and Liu, Daogao and Hashimoto, Tatsunori B and Inan, Huseyin A and Kulkarni, Janardhan and Lee, Yin-Tat and Guha Thakurta, Abhradeep},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={28616--28630},
  year={2022}
}
@inproceedings{carlini2022quantifying,
  title={Quantifying Memorization Across Neural Language Models},
  author={Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}
@inproceedings{zhou2020bypassing,
  title={Bypassing the Ambient Dimension: Private SGD with Gradient Subspace Identification},
  author={Zhou, Yingxue and Wu, Steven and Banerjee, Arindam},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{steiner2021train,
  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  author={Steiner, Andreas Peter and Kolesnikov, Alexander and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={Transactions on Machine Learning Research},
  year={2022}
}
@inproceedings{ridnik2021imagenet,
  title={ImageNet-21K Pretraining for the Masses},
  author={Ridnik, Tal and Ben-Baruch, Emanuel and Noy, Asaf and Zelnik-Manor, Lihi},
  booktitle={Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)},
  year={2021}
}
@misc{Quach_2019, title={Inside the 1TB ImageNet data set used to train the world’s Ai: Naked kids, drunken frat parties, porno stars, and more}, url={https://www.theregister.com/2019/10/23/ai_dataset_imagenet_consent}, journal={The Register® - Biting the hand that feeds IT}, publisher={The Register}, author={Quach, Katyanna}, year={2019}, month={Oct}} 
@inproceedings{yang2022study,
  title={A study of face obfuscation in imagenet},
  author={Yang, Kaiyu and Yau, Jacqueline H and Fei-Fei, Li and Deng, Jia and Russakovsky, Olga},
  booktitle={International Conference on Machine Learning},
  pages={25313--25330},
  year={2022},
  organization={PMLR}
}
@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others}
}
@inproceedings{dosovitskiy2020image,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@inproceedings{bu2022dpbitfit,
  title={Differentially Private Bias-Term Fine-tuning of Foundation Models},
  author={Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  booktitle={Forty-first International Conference on Machine Learning}
}
@inproceedings{zaken2022bitfit,
  title={BitFit: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author={Zaken, Elad Ben and Goldberg, Yoav and Ravfogel, Shauli},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages={1--9},
  year={2022}
}
@misc{FSDP,
  title = {Introducing PyTorch Fully Sharded Data Parallel (FSDP) API},
  howpublished = {\url{https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/}},
  author={ Zhao, Yanli and Varma, Rohan and Huang,  Chien-Chin and Li, Shen and Xu, Min and Desmaison,Alban}
}
@inproceedings{rajbhandari2020zero,
  title={Zero: Memory optimizations toward training trillion parameter models},
  author={Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  booktitle={SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--16},
  year={2020},
  organization={IEEE}
}
@inproceedings{bu2022differentially,
  title={Differentially private optimization on large model at small cost},
  author={Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  booktitle={International Conference on Machine Learning},
  pages={3192--3218},
  year={2023},
  organization={PMLR}
}
@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}
@article{bu2022scalable,
  title={Scalable and efficient training of large convolutional neural networks with differential privacy},
  author={Bu, Zhiqi and Mao, Jialin and Xu, Shiyun},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38305--38318},
  year={2022}
}
@article{scao2022bloom,
  title={Bloom: A 176b-parameter open-access multilingual language model},
  author={Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ili{\'c}, Suzana and Hesslow, Daniel and Castagn{\'e}, Roman and Luccioni, Alexandra Sasha and Yvon, Fran{\c{c}}ois and Gall{\'e}, Matthias and others},
  journal={arXiv preprint arXiv:2211.05100},
  year={2022}
}
@article{berner2019dota,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}
@online{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned LLM},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}
@article{zhang2024doppler,
  title={DOPPLER: Differentially Private Optimizers with Low-pass Filter for Privacy Noise Reduction},
  author={Zhang, Xinwei and Bu, Zhiqi and Hong, Mingyi and Razaviyayn, Meisam},
  journal={Advances in neural information processing systems},
  year={2024}
}
@article{zhang2024disk,
  title={DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction},
  author={Zhang, Xinwei and Bu, Zhiqi and Balle, Borja and Hong, Mingyi and Razaviyayn, Meisam and Mirrokni, Vahab},
  journal={arXiv preprint arXiv:2410.03883},
  year={2024}
}
@article{bu2023accuracy,
  title={On the accuracy and efficiency of group-wise clipping in differentially private optimization},
  author={Bu, Zhiqi and Liu, Ruixuan and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  journal={arXiv preprint arXiv:2310.19215},
  year={2023}
}
@article{ye2023initialization,
  title={Initialization Matters: Privacy-Utility Analysis of Overparameterized Neural Networks},
  author={Ye, Jiayuan and Zhu, Zhenyu and Liu, Fanghui and Shokri, Reza and Cevher, Volkan},
  journal={arXiv preprint arXiv:2310.20579},
  year={2023}
}

@article{das2021convergence,
  title={On the Convergence of Differentially Private Federated Learning on Non-Lipschitz Objectives, and with Normalized Client Updates},
  author={Das, Rudrajit and Hashemi, Abolfazl and Sanghavi, Sujay and Dhillon, Inderjit S},
  journal={arXiv preprint arXiv:2106.07094},
  year={2021}
}
@inproceedings{song2021evading,
  title={Evading the curse of dimensionality in unconstrained private glms},
  author={Song, Shuang and Steinke, Thomas and Thakkar, Om and Thakurta, Abhradeep},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2638--2646},
  year={2021},
  organization={PMLR}
}
@inproceedings{nakkab2023lit,
  title={LiT Tuned Models for Efficient Species Detection},
  author={Nakkab, Andre and Feuer, Benjamin and Hegde, Chinmay},
  booktitle={2nd AAAI Workshop on AI for Agriculture and Food Systems},
  year={2023}
}
@inproceedings{van2021benchmarking,
  title={Benchmarking representation learning for natural world image collections},
  author={Van Horn, Grant and Cole, Elijah and Beery, Sara and Wilber, Kimberly and Belongie, Serge and Mac Aodha, Oisin},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={12884--12893},
  year={2021}
}
@article{liu2023coupling,
  title={Coupling public and private gradient provably helps optimization},
  author={Liu, Ruixuan and Bu, Zhiqi and Wang, Yu-xiang and Zha, Sheng and Karypis, George},
  journal={arXiv preprint arXiv:2310.01304},
  year={2023}
}
@inproceedings{foret2020sharpness,
  title={Sharpness-aware Minimization for Efficiently Improving Generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@software{openlm2023openllama,
  author = {Geng, Xinyang and Liu, Hao},
  title = {OpenLLaMA: An Open Reproduction of LLaMA},
  month = May,
  year = 2023,
  url = {https://github.com/openlm-research/open_llama}
}
@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}
@inproceedings{sander2023tan,
  title={Tan without a burn: Scaling laws of dp-sgd},
  author={Sander, Tom and Stock, Pierre and Sablayrolles, Alexandre},
  booktitle={International Conference on Machine Learning},
  pages={29937--29949},
  year={2023},
  organization={PMLR}
}
@inproceedings{yu2023vip,
  title={ViP: A Differentially Private Foundation Model for Computer Vision},
  author={Yu, Yaodong and Sanjabi, Maziar and Ma, Yi and Chaudhuri, Kamalika and Guo, Chuan},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2023}
}
@inproceedings{caron2021emerging,
  title={Emerging properties in self-supervised vision transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={9650--9660},
  year={2021}
}
@inproceedings{carlini2022membership,
  title={Membership inference attacks from first principles},
  author={Carlini, Nicholas and Chien, Steve and Nasr, Milad and Song, Shuang and Terzis, Andreas and Tramer, Florian},
  booktitle={2022 IEEE Symposium on Security and Privacy (SP)},
  pages={1897--1914},
  year={2022},
  organization={IEEE}
}
@article{rahman2018membership,
  title={Membership Inference Attack against Differentially Private Deep Learning Model.},
  author={Rahman, Md Atiqur and Rahman, Tanzila and Lagani{\`e}re, Robert and Mohammed, Noman and Wang, Yang},
  journal={Trans. Data Priv.},
  volume={11},
  number={1},
  pages={61--79},
  year={2018}
}
@inproceedings{chen2020differential,
  title={Differential privacy protection against membership inference attack on machine learning for genomic data},
  author={Chen, Junjie and Wang, Wendy Hui and Shi, Xinghua},
  booktitle={BIOCOMPUTING 2021: Proceedings of the Pacific Symposium},
  pages={26--37},
  year={2020},
  organization={World Scientific}
}
@article{inan2021training,
  title={Training data leakage analysis in language models},
  author={Inan, Huseyin A and Ramadan, Osman and Wutschitz, Lukas and Jones, Daniel and R{\"u}hle, Victor and Withers, James and Sim, Robert},
  journal={arXiv preprint arXiv:2101.05405},
  year={2021}
}
@inproceedings{hoory2021learning,
  title={Learning and evaluating a differentially private pre-trained language model},
  author={Hoory, Shlomo and Feder, Amir and Tendler, Avichai and Erell, Sofia and Peled-Cohen, Alon and Laish, Itay and Nakhost, Hootan and Stemmer, Uri and Benjamini, Ayelet and Hassidim, Avinatan and others},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={1178--1189},
  year={2021}
}
@article{ponomareva2023dp,
  title={How to dp-fy ml: A practical guide to machine learning with differential privacy},
  author={Ponomareva, Natalia and Hazimeh, Hussein and Kurakin, Alex and Xu, Zheng and Denison, Carson and McMahan, H Brendan and Vassilvitskii, Sergei and Chien, Steve and Thakurta, Abhradeep Guha},
  journal={Journal of Artificial Intelligence Research},
  volume={77},
  pages={1113--1201},
  year={2023}
}
@inproceedings{lukas2023analyzing,
  title={Analyzing Leakage of Personally Identifiable Information in Language Models},
  author={Lukas, Nils and Salem, Ahmed and Sim, Robert and Tople, Shruti and Wutschitz, Lukas and Zanella-B{\'e}guelin, Santiago},
  booktitle={2023 IEEE Symposium on Security and Privacy (SP)},
  pages={346--363},
  year={2023},
  organization={IEEE Computer Society}
}
@article{zhang2019algorithmic,
  title={Which algorithmic choices matter at which batch sizes? insights from a noisy quadratic model},
  author={Zhang, Guodong and Li, Lala and Nado, Zachary and Martens, James and Sachdeva, Sushant and Dahl, George and Shallue, Chris and Grosse, Roger B},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{damian2021label,
  title={Label noise sgd provably prefers flat global minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason D},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={27449--27461},
  year={2021}
}
@inproceedings{zhu2018anisotropic,
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  booktitle={International Conference on Machine Learning},
  pages={7654--7663},
  year={2019},
  organization={PMLR}
}
@article{song2022multi,
  title={Multi-label legal document classification: A deep learning-based approach with label-attention and domain-specific pre-training},
  author={Song, Dezhao and Vold, Andrew and Madan, Kanika and Schilder, Frank},
  journal={Information Systems},
  volume={106},
  pages={101718},
  year={2022},
  publisher={Elsevier}
}
@article{ji2023domain,
  title={Domain-specific Continued Pretraining of Language Models for Capturing Long Context in Mental Health},
  author={Ji, Shaoxiong and Zhang, Tianlin and Yang, Kailai and Ananiadou, Sophia and Cambria, Erik and Tiedemann, J{\"o}rg},
  journal={arXiv preprint arXiv:2304.10447},
  year={2023}
}
@inproceedings{wang2020extending,
  title={Extending Multilingual BERT to Low-Resource Languages},
  author={Wang, Zihan and Karthikeyan, K and Mayhew, Stephen and Roth, Dan},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
  pages={2649--2656},
  year={2020}
}
@inproceedings{du2021self,
  title={Self-training Improves Pre-training for Natural Language Understanding},
  author={Du, Jingfei and Grave, {\'E}douard and Gunel, Beliz and Chaudhary, Vishrav and Celebi, Onur and Auli, Michael and Stoyanov, Veselin and Conneau, Alexis},
  booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={5408--5418},
  year={2021}
}
@inproceedings{gururangan2020don,
  title={Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks},
  author={Gururangan, Suchin and Marasovi{\'c}, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A},
  booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages={8342--8360},
  year={2020}
}
@inproceedings{wang2019differentially,
  title={Differentially private empirical risk minimization with non-convex loss functions},
  author={Wang, Di and Chen, Changyou and Xu, Jinhui},
  booktitle={International Conference on Machine Learning},
  pages={6526--6535},
  year={2019},
  organization={PMLR}
}
@inproceedings{wang2019differentially2,
  title={Differentially private empirical risk minimization with smooth non-convex loss functions: A non-stationary view},
  author={Wang, Di and Xu, Jinhui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={1182--1189},
  year={2019}
}
@inproceedings{zhu2019anisotropic,
  title={The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  booktitle={International Conference on Machine Learning},
  pages={7654--7663},
  year={2019},
  organization={PMLR}
}
@inproceedings{sankar2021deeper,
  title={A deeper look at the hessian eigenspectrum of deep neural networks and its applications to regularization},
  author={Sankar, Adepu Ravi and Khasbage, Yash and Vigneswaran, Rahul and Balasubramanian, Vineeth N},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={11},
  pages={9481--9488},
  year={2021}
}
@inproceedings{keskar2016large,
  title={On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  booktitle={International Conference on Learning Representations},
  year={2016}
}
@article{bu2022private,
  title={Differentially Private Optimizers Can Learn Adversarially Robust Models},
  author={Bu, Zhiqi and Zhang, Yuan},
  journal={Transactions on Machine Learning Research},
  year={2023}
}
@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Toronto, ON, Canada}
}
@inproceedings{bassily2014private,
  title={Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds},
  author={Bassily, Raef and Smith, Adam and Thakurta, Abhradeep},
  booktitle={Proceedings of the 2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
  pages={464--473},
  year={2014}
}
@article{ma2022dimension,
  title={Dimension independent generalization of dp-sgd for overparameterized smooth convex optimization},
  author={Ma, Yi-An and Marinov, Teodor Vanislavov and Zhang, Tong},
  journal={arXiv preprint arXiv:2206.01836},
  year={2022}
}
@inproceedings{yao2020pyhessian,
  title={Pyhessian: Neural networks through the lens of the hessian},
  author={Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W},
  booktitle={2020 IEEE international conference on big data (Big data)},
  pages={581--590},
  year={2020},
  organization={IEEE}
}
@inproceedings{liu2023same,
  title={Same pre-training loss, better downstream: Implicit bias matters for language models},
  author={Liu, Hong and Xie, Sang Michael and Li, Zhiyuan and Ma, Tengyu},
  booktitle={International Conference on Machine Learning},
  pages={22188--22214},
  year={2023},
  organization={PMLR}
}
@article{berrada2023unlocking,
  title={Unlocking Accuracy and Fairness in Differentially Private Image Classification},
  author={Berrada, Leonard and De, Soham and Shen, Judy Hanwen and Hayes, Jamie and Stanforth, Robert and Stutz, David and Kohli, Pushmeet and Smith, Samuel L and Balle, Borja},
  journal={arXiv preprint arXiv:2308.10888},
  year={2023}
}
@inproceedings{guo2017calibration,
  title={On calibration of modern neural networks},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q},
  booktitle={International conference on machine learning},
  pages={1321--1330},
  year={2017},
  organization={PMLR}
}
@article{allen2018natasha,
  title={Natasha 2: Faster non-convex optimization than sgd},
  author={Allen-Zhu, Zeyuan},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}
@article{opacus,
  title={Opacus: {U}ser-Friendly Differential Privacy Library in {PyTorch}},
  author={Ashkan Yousefpour and Igor Shilov and Alexandre Sablayrolles and Davide Testuggine and Karthik Prasad and Mani Malek and John Nguyen and Sayan Ghosh and Akash Bharadwaj and Jessica Zhao and Graham Cormode and Ilya Mironov},
  journal={arXiv preprint arXiv:2109.12298},
  year={2021}
}
@article{hardt2016equality,
  title={Equality of opportunity in supervised learning},
  author={Hardt, Moritz and Price, Eric and Srebro, Nati},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}
@article{bottou2018optimization,
  title={Optimization methods for large-scale machine learning},
  author={Bottou, L{\'e}on and Curtis, Frank E and Nocedal, Jorge},
  journal={Siam Review},
  volume={60},
  number={2},
  pages={223--311},
  year={2018},
  publisher={SIAM}
}
@inproceedings{lin2004rouge,
  title={Rouge: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004}
}
@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{you2017scaling,
  title={Scaling sgd batch size to 32k for imagenet training},
  author={You, Yang and Gitman, Igor and Ginsburg, Boris},
  journal={arXiv preprint arXiv:1708.03888},
  volume={6},
  number={12},
  pages={6},
  year={2017}
}
@article{liu2019variance,
  title={On the variance of the adaptive learning rate and beyond},
  author={Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  journal={arXiv preprint arXiv:1908.03265},
  year={2019}
}
@article{dozat2016incorporating,
  title={Incorporating nesterov momentum into adam},
  author={Dozat, Timothy},
  year={2016}
}
@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, Ilya and Hutter, Frank},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}
@article{zeiler2012adadelta,
  title={Adadelta: an adaptive learning rate method},
  author={Zeiler, Matthew D},
  journal={arXiv preprint arXiv:1212.5701},
  year={2012}
}
@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization.},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of machine learning research},
  volume={12},
  number={7},
  year={2011}
}
@inproceedings{nesterov1983method,
  title={A method for solving the convex programming problem with convergence rate O (1/k\^{} 2)},
  author={Nesterov, Yurii E},
  booktitle={Dokl. akad. nauk Sssr},
  volume={269},
  pages={543--547},
  year={1983}
}
@article{polyak1964some,
  title={Some methods of speeding up the convergence of iteration methods},
  author={Polyak, Boris T},
  journal={Ussr computational mathematics and mathematical physics},
  volume={4},
  number={5},
  pages={1--17},
  year={1964},
  publisher={Elsevier}
}
@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}
@article{rocher2019estimating,
  title={Estimating the success of re-identifications in incomplete datasets using generative models},
  author={Rocher, Luc and Hendrickx, Julien M and De Montjoye, Yves-Alexandre},
  journal={Nature communications},
  volume={10},
  number={1},
  pages={1--9},
  year={2019},
  publisher={Nature Publishing Group}
}
@article{sweeney1997weaving,
  title={Weaving technology and policy together to maintain confidentiality},
  author={Sweeney, Latanya},
  journal={The Journal of Law, Medicine \& Ethics},
  volume={25},
  number={2-3},
  pages={98--110},
  year={1997},
  publisher={SAGE Publications Sage CA: Los Angeles, CA}
}
@inproceedings{dwork2006calibrating,
  title={Calibrating noise to sensitivity in private data analysis},
  author={Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  booktitle={Theory of cryptography conference},
  pages={265--284},
  year={2006},
  organization={Springer}
}
@inproceedings{dwork2008differential,
  title={Differential privacy: A survey of results},
  author={Dwork, Cynthia},
  booktitle={International conference on theory and applications of models of computation},
  pages={1--19},
  year={2008},
  organization={Springer}
}
@article{bu2021fast,
  title={Fast and memory efficient differentially private-sgd via jl projections},
  author={Bu, Zhiqi and Gopi, Sivakanth and Kulkarni, Janardhan and Lee, Yin Tat and Shen, Hanwen and Tantipongpipat, Uthaipon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{bu2023zero,
  title={Zero redundancy distributed learning with differential privacy},
  author={Bu, Zhiqi and Chiu, Justin and Liu, Ruixuan and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  booktitle={ICLR 2023 Workshop on Pitfalls of limited data and computation for Trustworthy ML}
}
@article{bu2022automatic,
  title={Automatic clipping: Differentially private deep learning made easier and stronger},
  author={Bu, Zhiqi and Wang, Yu-Xiang and Zha, Sheng and Karypis, George},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{goodfellow2015efficient,
  title={Efficient per-example gradient computations},
  author={Goodfellow, Ian},
  journal={arXiv preprint arXiv:1510.01799},
  year={2015}
}
@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}
@article{dong2019gaussian,
  title={Gaussian differential privacy},
  author={Dong, Jinshuo and Roth, Aaron and Su, Weijie J},
  journal={Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume={84},
  number={1},
  pages={3--37},
  year={2022},
  publisher={Wiley Online Library}
}
@article{de2022unlocking,
  title={Unlocking High-Accuracy Differentially Private Image Classification through Scale},
  author={De, Soham and Berrada, Leonard and Hayes, Jamie and Smith, Samuel L and Balle, Borja},
  journal={arXiv preprint arXiv:2204.13650},
  year={2022}
}
@article{gopi2021numerical,
  title={Numerical composition of differential privacy},
  author={Gopi, Sivakanth and Lee, Yin Tat and Wutschitz, Lukas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@article{zhu2021optimal,
  title={Optimal accounting of differential privacy via characteristic function},
  author={Zhu, Yuqing and Dong, Jinshuo and Wang, Yu-Xiang},
  journal={arXiv preprint arXiv:2106.08567},
  year={2021}
}
@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@misc{rw2019timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}
@article{rochette2019efficient,
  title={Efficient per-example gradient computations in convolutional neural networks},
  author={Rochette, Gaspar and Manoel, Andre and Tramel, Eric W},
  journal={arXiv preprint arXiv:1912.06015},
  year={2019}
}
@article{haim2022reconstructing,
  title={Reconstructing Training Data from Trained Neural Networks},
  author={Haim, Niv and Vardi, Gal and Yehudai, Gilad and Shamir, Ohad and Irani, Michal},
  journal={arXiv preprint arXiv:2206.07758},
  year={2022}
}
@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}
@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{paszke2017automatic,
  title={Automatic differentiation in pytorch},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}
@inproceedings{marcel2010torchvision,
  title={Torchvision the machine-vision package of torch},
  author={Marcel, S{\'e}bastien and Rodriguez, Yann},
  booktitle={Proceedings of the 18th ACM international conference on Multimedia},
  pages={1485--1488},
  year={2010}
}
@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}
@inproceedings{mironov2017renyi,
  title={R{\'e}nyi differential privacy},
  author={Mironov, Ilya},
  booktitle={2017 IEEE 30th computer security foundations symposium (CSF)},
  pages={263--275},
  year={2017},
  organization={IEEE}
}
@article{dusek.etal2020:csl,
  title = {Evaluating the {{State}}-of-the-{{Art}} of {{End}}-to-{{End Natural Language Generation}}: {{The E2E NLG Challenge}}},
  author = {Dusek, Ondrej and Novikova, Jekaterina and Rieser, Verena},
  year = {2020},
  month = jan,
  volume = {59},
  pages = {123--156},
  doi = {10.1016/j.csl.2019.06.009},
  archivePrefix = {arXiv},
  eprint = {1901.11528},
  eprinttype = {arxiv},
  journal = {Computer Speech \& Language}
}
@article{li2021prefix,
  title={Prefix-tuning: Optimizing continuous prompts for generation},
  author={Li, Xiang Lisa and Liang, Percy},
  journal={arXiv preprint arXiv:2101.00190},
  year={2021}
}
@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
@online{WinNT,
  author = {Iyer, Shankar and Dandekar, Nikhil and Csernai, Kornel},
  title = {First Quora Dataset Release: Question Pairs},
  year = {2017},
  url = {https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs},
  urldate = {2019-04-03}
}
@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}
@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for 
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of 
               the North American Chapter of the 
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
  url = "http://aclweb.org/anthology/N18-1101"
}
@article{liu2019roberta,
  title={Roberta: A robustly optimized bert pretraining approach},
  author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  journal={arXiv preprint arXiv:1907.11692},
  year={2019}
}
@inproceedings{yu2021large,
  title={Large scale private learning via low-rank reparametrization},
  author={Yu, Da and Zhang, Huishuai and Chen, Wei and Yin, Jian and Liu, Tie-Yan},
  booktitle={International Conference on Machine Learning},
  pages={12208--12218},
  year={2021},
  organization={PMLR}
}
@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}
@misc{imagewang,
    author    = "Jeremy Howard",
    title     = "Imagewang",
    url       = "https://github.com/fastai/imagenette/"
}
@article{wei2020federated,
  title={Federated learning with differential privacy: Algorithms and performance analysis},
  author={Wei, Kang and Li, Jun and Ding, Ming and Ma, Chuan and Yang, Howard H and Farokhi, Farhad and Jin, Shi and Quek, Tony QS and Poor, H Vincent},
  journal={IEEE Transactions on Information Forensics and Security},
  volume={15},
  pages={3454--3469},
  year={2020},
  publisher={IEEE}
}
@article{polyak1992acceleration,
  title={Acceleration of stochastic approximation by averaging},
  author={Polyak, Boris T and Juditsky, Anatoli B},
  journal={SIAM journal on control and optimization},
  volume={30},
  number={4},
  pages={838--855},
  year={1992},
  publisher={SIAM}
}
@article{qiao2019micro,
  title={Micro-batch training with batch-channel normalization and weight standardization},
  author={Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  journal={arXiv preprint arXiv:1903.10520},
  year={2019}
}
@inproceedings{liu2015faceattributes,
  title = {Deep Learning Face Attributes in the Wild},
  author = {Liu, Ziwei and Luo, Ping and Wang, Xiaogang and Tang, Xiaoou},
  booktitle = {Proceedings of International Conference on Computer Vision (ICCV)},
  month = {December},
  year = {2015} 
}
@inproceedings{zhang2022understanding,
  title={Understanding clipping for federated learning: Convergence and client-level differential privacy},
  author={Zhang, Xinwei and Chen, Xiangyi and Hong, Mingyi and Wu, Zhiwei Steven and Yi, Jinfeng},
  booktitle={International Conference on Machine Learning, ICML 2022},
  year={2022}
}
@article{chen2020understanding,
  title={Understanding gradient clipping in private SGD: A geometric perspective},
  author={Chen, Xiangyi and Wu, Steven Z and Hong, Mingyi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={13773--13782},
  year={2020}
}
@article{klause2022differentially,
  title={Differentially private training of residual networks with scale normalisation},
  author={Klause, Helena and Ziller, Alexander and Rueckert, Daniel and Hammernik, Kerstin and Kaissis, Georgios},
  journal={arXiv preprint arXiv:2203.00324},
  year={2022}
}
@article{misra2019mish,
  title={Mish: A self regularized non-monotonic activation function},
  author={Misra, Diganta},
  journal={arXiv preprint arXiv:1908.08681},
  year={2019}
}
@inproceedings{lhoest-etal-2021-datasets,
    title = "Datasets: A Community Library for Natural Language Processing",
    author = "Lhoest, Quentin  and
      Villanova del Moral, Albert  and
      Jernite, Yacine  and
      Thakur, Abhishek  and
      von Platen, Patrick  and
      Patil, Suraj  and
      Chaumond, Julien  and
      Drame, Mariama  and
      Plu, Julien  and
      Tunstall, Lewis  and
      Davison, Joe  and
      Sasko, Mario  and
      Chhablani, Gunjan  and
      Malik, Bhavitvya  and
      Brandeis, Simon  and
      Le Scao, Teven  and
      Sanh, Victor  and
      Xu, Canwen  and
      Patry, Nicolas  and
      McMillan-Major, Angelina  and
      Schmid, Philipp  and
      Gugger, Sylvain  and
      Delangue, Cl{\'e}ment  and
      Matussi{\`e}re, Th{\'e}o  and
      Debut, Lysandre  and
      Bekman, Stas  and
      Cistac, Pierric  and
      Goehringer, Thibault  and
      Mustar, Victor  and
      Lagunas, Fran{\c{c}}ois  and
      Rush, Alexander  and
      Wolf, Thomas",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-demo.21",
    pages = "175--184",
    abstract = "The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.",
    eprint={2109.02846},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
}
@article{mehta2022large,
  title={Large Scale Transfer Learning for Differentially Private Image Classification},
  author={Mehta, Harsh and Thakurta, Abhradeep and Kurakin, Alexey and Cutkosky, Ashok},
  journal={arXiv preprint arXiv:2205.02973},
  year={2022}
}
@inproceedings{vedantam2015cider,
  title={Cider: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4566--4575},
  year={2015}
}
@inproceedings{banarjee2005,
  title     = {{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments},
  author    = {Banerjee, Satanjeev  and Lavie, Alon},
  booktitle = {Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization},
  month     = jun,
  year      = {2005},
  address   = {Ann Arbor, Michigan},
  publisher = {Association for Computational Linguistics},
  url       = {https://www.aclweb.org/anthology/W05-0909},
  pages     = {65--72},
}
@inproceedings{sadjadi20182017,
  title={The 2017 NIST Language Recognition Evaluation.},
  author={Sadjadi, Seyed Omid and Kheyrkhah, Timothee and Tong, Audrey and Greenberg, Craig S and Reynolds, Douglas A and Singer, Elliot and Mason, Lisa P and Hernandez-Cordero, Jaime and others},
  booktitle={Odyssey},
  pages={82--89},
  year={2018}
}
@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}
@INPROCEEDINGS{Papineni02bleu:a,
    author = {Kishore Papineni and Salim Roukos and Todd Ward and Wei-jing Zhu},
    title = {BLEU: a Method for Automatic Evaluation of Machine Translation},
    booktitle = {},
    year = {2002},
    pages = {311--318}
}
@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE symposium on security and privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}
@article{papernot2021hyperparameter,
  title={Hyperparameter Tuning with Renyi Differential Privacy},
  author={Papernot, Nicolas and Steinke, Thomas},
  journal={arXiv preprint arXiv:2110.03620},
  year={2021}
}
@inproceedings{papernot2020tempered,
  title={Tempered sigmoid activations for deep learning with differential privacy},
  author={Papernot, Nicolas and Thakurta, Abhradeep and Song, Shuang and Chien, Steve and Erlingsson, {\'U}lfar},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={35},
  number={10},
  pages={9312--9321},
  year={2021}
}
@article{shamsabadi2021losing,
  title={Losing Less: A Loss for Differentially Private Deep Learning},
  author={Shamsabadi, Ali Shahin and Papernot, Nicolas},
  year={2021}
}
@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@inproceedings{golatkar2022mixed,
  title={Mixed differential privacy in computer vision},
  author={Golatkar, Aditya and Achille, Alessandro and Wang, Yu-Xiang and Roth, Aaron and Kearns, Michael and Soatto, Stefano},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8376--8386},
  year={2022}
}

@article{mandt2017stochastic,
  title={Stochastic Gradient Descent as Approximate Bayesian Inference},
  author={Mandt, Stephan and Hoffman, Matthew D and Blei, David M},
  journal={Journal of Machine Learning Research},
  volume={18},
  pages={1--35},
  year={2017}
}
@inproceedings{smith2018don,
  title={Don't Decay the Learning Rate, Increase the Batch Size},
  author={Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  booktitle={International Conference on Learning Representations},
  year={2018}
}
@inproceedings{chaudhari2018stochastic,
  title={Stochastic gradient descent performs variational inference, converges to limit cycles for deep networks},
  author={Chaudhari, Pratik and Soatto, Stefano},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}
@inproceedings{xie2020diffusion,
  title={A Diffusion Theory For Deep Learning Dynamics: Stochastic Gradient Descent Exponentially Favors Flat Minima},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{bell1938iterated,
  title={The iterated exponential integers},
  author={Bell, Eric Temple},
  journal={Annals of Mathematics},
  pages={539--557},
  year={1938},
  publisher={JSTOR}
}
@inproceedings{chen2020simple,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={1597--1607},
  year={2020},
  organization={PMLR}
}
@inproceedings{tramer2020differentially,
  title={Differentially Private Learning Needs Better Features (or Much More Data)},
  author={Tramer, Florian and Boneh, Dan},
  booktitle={International Conference on Learning Representations},
  year={2020}
}
@article{stevens2022backpropagation,
  title={Backpropagation Clipping for Deep Learning with Differential Privacy},
  author={Stevens, Timothy and Ngong, Ivoline C and Darais, David and Hirsch, Calvin and Slater, David and Near, Joseph P},
  journal={arXiv preprint arXiv:2202.05089},
  year={2022}
}
@article{du2021dp,
  title={DP-FP: Differentially Private Forward Propagation for Large Models},
  author={Du, Jian and Mi, Haitao},
  journal={arXiv preprint arXiv:2112.14430},
  year={2021}
}
@article{bu2021convergence,
  title={On the Convergence and Calibration of Deep Learning with Differential Privacy},
  author={Bu, Zhiqi and Wang, Hua and Dai, Zongyu and Long, Qi},
  journal={Transactions on Machine Learning Research},
  year={2023}
}

@inproceedings{koskela2020computing,
  title={Computing tight differential privacy guarantees using fft},
  author={Koskela, Antti and J{\"a}lk{\"o}, Joonas and Honkela, Antti},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2560--2569},
  year={2020},
  organization={PMLR}
}
@article{bu2020deep,
  title={Deep learning with gaussian differential privacy},
  author={Bu, Zhiqi and Dong, Jinshuo and Long, Qi and Su, Weijie J},
  journal={Harvard data science review},
  volume={2020},
  number={23},
  year={2020},
  publisher={NIH Public Access}
}
@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@article{dwork2014algorithmic,
  title={The algorithmic foundations of differential privacy.},
  author={Dwork, Cynthia and Roth, Aaron and others},
  journal={Found. Trends Theor. Comput. Sci.},
  volume={9},
  number={3-4},
  pages={211--407},
  year={2014}
}
@article{yang2022normalized,
  title={Normalized/clipped sgd with perturbation for differentially private non-convex optimization},
  author={Yang, Xiaodong and Zhang, Huishuai and Chen, Wei and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2206.13033},
  year={2022}
}
@inproceedings{houlsby2019parameter,
  title={Parameter-efficient transfer learning for NLP},
  author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  booktitle={International Conference on Machine Learning},
  pages={2790--2799},
  year={2019},
  organization={PMLR}
}
@article{mahabadi2021compacter,
  title={Compacter: Efficient low-rank hypercomplex adapter layers},
  author={Mahabadi, Rabeeh Karimi and Henderson, James and Ruder, Sebastian},
  journal={arXiv preprint arXiv:2106.04647},
  year={2021}
}
@inproceedings{yu2021differentially,
  title={Differentially Private Fine-tuning of Language Models},
  author={Yu, Da and Naik, Saurabh and Backurs, Arturs and Gopi, Sivakanth and Inan, Huseyin A and Kamath, Gautam and Kulkarni, Janardhan and Lee, Yin Tat and Manoel, Andre and Wutschitz, Lukas and others},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{lee2020scaling,
  title={Scaling up differentially private deep learning with fast per-example gradient clipping},
  author={Lee, Jaewoo and Kifer, Daniel},
  journal={arXiv preprint arXiv:2009.03106},
  year={2020}
}
@article{subramani2021enabling,
  title={Enabling fast differentially private sgd via just-in-time compilation and vectorization},
  author={Subramani, Pranav and Vadivelu, Nicholas and Kamath, Gautam},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}
@inproceedings{li2021large,
  title={Large Language Models Can Be Strong Differentially Private Learners},
  author={Li, Xuechen and Tramer, Florian and Liang, Percy and Hashimoto, Tatsunori},
  booktitle={International Conference on Learning Representations},
  year={2021}
}
@article{pichapati2019adaclip,
  title={AdaCliP: Adaptive clipping for private SGD},
  author={Pichapati, Venkatadheeraj and Suresh, Ananda Theertha and Yu, Felix X and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1908.07643},
  year={2019}
}
@article{andrew2021differentially,
  title={Differentially private learning with adaptive clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{wang2019subsampled,
  title={Subsampled r{\'e}nyi differential privacy and analytical moments accountant},
  author={Wang, Yu-Xiang and Balle, Borja and Kasiviswanathan, Shiva Prasad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={1226--1235},
  year={2019},
  organization={PMLR}
}

@article{cohen2022adaptive,
  title={Adaptive gradient methods at the edge of stability},
  author={Cohen, Jeremy M and Ghorbani, Behrooz and Krishnan, Shankar and Agarwal, Naman and Medapati, Sourabh and Badura, Michal and Suo, Daniel and Cardoze, David and Nado, Zachary and Dahl, George E and others},
  journal={arXiv preprint arXiv:2207.14484},
  year={2022}
}

@inproceedings{amid2022public,
  title={Public data-assisted mirror descent for private model training},
  author={Amid, Ehsan and Ganesh, Arun and Mathews, Rajiv and Ramaswamy, Swaroop and Song, Shuang and Steinke, Thomas and Suriyakumar, Vinith M and Thakkar, Om and Thakurta, Abhradeep},
  booktitle={International Conference on Machine Learning},
  pages={517--535},
  year={2022},
  organization={PMLR}
}


@article{papernot2017semi,
  title={Semi-supervised knowledge transfer for deep learning from private training data},
  author={Papernot, Nicolas and Abadi, Mart{\'\i}n and Erlingsson, Ulfar and Goodfellow, Ian and Talwar, Kunal},
  journal={arXiv preprint arXiv:1610.05755},
  year={2016}
}
@inproceedings{jorgensen2015conservative,
  title={Conservative or liberal? Personalized differential privacy},
  author={Jorgensen, Zach and Yu, Ting and Cormode, Graham},
  booktitle={2015 IEEE 31St international conference on data engineering},
  pages={1023--1034},
  year={2015},
  organization={IEEE}
}
@article{ferrando2021combining,
  title={Combining Public and Private Data},
  author={Ferrando, Cecilia and Gillenwater, Jennifer and Kulesza, Alex},
  journal={arXiv preprint arXiv:2111.00115},
  year={2021}
}

@article{kairouz2020fast,
  title={Fast dimension independent private adagrad on publicly estimated subspaces},
  author={Kairouz, Peter and Ribero, M{\'o}nica and Rush, Keith and Thakurta, Abhradeep},
  journal={arXiv preprint arXiv:2008.06570},
  year={2020}
}

@inproceedings{papernot2020,
  title={Making the shoe fit: Architectures, initializations, and tuning for learning with privacy},
  booktitle={URL https://openreview. net/forum},
  author={Papernot, Nicolas and Chien, Steve and Song, Shuang and Thakurta, Abhradeep and Erlingsson, Ulfar},
  year={2019}
}
@article{cao2019generalization,
  title={Generalization bounds of stochastic gradient descent for wide and deep neural networks},
  author={Cao, Yuan and Gu, Quanquan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@inproceedings{ramesh2021zero,
  title={Zero-shot text-to-image generation},
  author={Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and Gray, Scott and Voss, Chelsea and Radford, Alec and Chen, Mark and Sutskever, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={8821--8831},
  year={2021},
  organization={PMLR}
}

@article{chen2020big,
  title={Big self-supervised models are strong semi-supervised learners},
  author={Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22243--22255},
  year={2020}
}

@article{tramer2022considerations,
  title={Considerations for differentially private learning with large-scale public pretraining},
  author={Tram{\`e}r, Florian and Kamath, Gautam and Carlini, Nicholas},
  journal={arXiv preprint arXiv:2212.06470},
  year={2022}
}