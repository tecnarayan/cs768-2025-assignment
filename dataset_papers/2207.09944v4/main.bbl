\begin{thebibliography}{142}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[LeCun et~al.(2015)LeCun, Bengio, and Hinton]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.
\newblock Deep learning.
\newblock \emph{Nature}, 521\penalty0 (7553):\penalty0 436--444, 2015.

\bibitem[Silver et~al.(2016)Silver, Huang, Maddison, Guez, Sifre, Van
  Den~Driessche, Schrittwieser, Antonoglou, Panneershelvam, Lanctot,
  et~al.]{silver2016mastering}
David Silver, Aja Huang, Chris~J Maddison, Arthur Guez, Laurent Sifre, George
  Van Den~Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda
  Panneershelvam, Marc Lanctot, et~al.
\newblock Mastering the game of go with deep neural networks and tree search.
\newblock \emph{Nature}, 529\penalty0 (7587):\penalty0 484--489, 2016.

\bibitem[Jumper et~al.(2021)Jumper, Evans, Pritzel, Green, Figurnov,
  Ronneberger, Tunyasuvunakool, Bates, {\v{Z}}{\'\i}dek, Potapenko,
  et~al.]{jumper2021highly}
John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov,
  Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin
  {\v{Z}}{\'\i}dek, Anna Potapenko, et~al.
\newblock Highly accurate protein structure prediction with alphafold.
\newblock \emph{Nature}, 596\penalty0 (7873):\penalty0 583--589, 2021.

\bibitem[Torralba and Efros(2011)]{torralba2011unbiased}
Antonio Torralba and Alexei~A Efros.
\newblock Unbiased look at dataset bias.
\newblock In \emph{Proceedings of The IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 1521--1528, 2011.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European Conference on Computer Vision},
  pages 456--473, 2018.

\bibitem[Hendrycks and Dietterich(2019)]{hendrycks2019benchmarking}
Dan Hendrycks and Thomas Dietterich.
\newblock Benchmarking neural network robustness to common corruptions and
  perturbations.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Geirhos et~al.(2020)Geirhos, Jacobsen, Michaelis, Zemel, Brendel,
  Bethge, and Wichmann]{geirhos2020shorcut}
Robert Geirhos, J\"{o}rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel,
  Wieland Brendel, Matthias Bethge, and Felix~A Wichmann.
\newblock Shortcut learning in deep neural networks.
\newblock \emph{Nature Machine Intelligence}, 2:\penalty0 665–673, 2020.

\bibitem[Zech et~al.(2018)Zech, Badgeley, Liu, Costa, Titano, and
  Oermann]{zech2018variable}
John~R Zech, Marcus~A Badgeley, Manway Liu, Anthony~B Costa, Joseph~J Titano,
  and Eric~Karl Oermann.
\newblock Variable generalization performance of a deep learning model to
  detect pneumonia in chest radiographs: a cross-sectional study.
\newblock \emph{PLoS Medicine}, 15\penalty0 (11), 2018.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2020invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv:1907.02893}, 2019.

\bibitem[Niven and Kao(2020)]{niven2020probing}
Timothy Niven and Hung~Yu Kao.
\newblock Probing neural network comprehension of natural language arguments.
\newblock In \emph{Association for Computational Linguistics}, pages
  4658--4664, 2020.

\bibitem[Santurkar et~al.(2020)Santurkar, Tsipras, and
  Madry]{santurkar2020breeds}
Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry.
\newblock Breeds: Benchmarks for subpopulation shift.
\newblock \emph{arXiv:2008.04859}, 2020.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Marklund, Xie, Zhang, Balsubramani, Hu,
  Yasunaga, Phillips, Gao, Lee, David, Stavness, Guo, Earnshaw, Haque, Beery,
  Leskovec, Kundaje, Pierson, Levine, Finn, and Liang]{wilds2021}
Pang~Wei Koh, Shiori Sagawa, Henrik Marklund, Sang~Michael Xie, Marvin Zhang,
  Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips,
  Irena Gao, Tony Lee, Etienne David, Ian Stavness, Wei Guo, Berton~A.
  Earnshaw, Imran~S. Haque, Sara Beery, Jure Leskovec, Anshul Kundaje, Emma
  Pierson, Sergey Levine, Chelsea Finn, and Percy Liang.
\newblock {WILDS}: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Borkan et~al.(2019)Borkan, Dixon, Sorensen, Thain, and
  Vasserman]{borkan2019nuanced}
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
\newblock Nuanced metrics for measuring unintended bias with real data for text
  classification.
\newblock In \emph{World Wide Web Conference}, pages 491--500, 2019.

\bibitem[Hansen et~al.(2013)Hansen, Potapov, Moore, Hancher, Turubanova,
  Tyukavina, Thau, Stehman, Goetz, Loveland, et~al.]{hansen2013high}
Matthew~C Hansen, Peter~V Potapov, Rebecca Moore, Matt Hancher, Svetlana~A
  Turubanova, Alexandra Tyukavina, David Thau, Stephen~V Stehman, Scott~J
  Goetz, Thomas~R Loveland, et~al.
\newblock High-resolution global maps of 21st-century forest cover change.
\newblock \emph{Science}, 342\penalty0 (6160):\penalty0 850--853, 2013.

\bibitem[Christie et~al.(2018)Christie, Fendley, Wilson, and
  Mukherjee]{christie2018functional}
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee.
\newblock Functional map of the world.
\newblock In \emph{Computer Vision and Pattern Recognition}, pages 6172--6180,
  2018.

\bibitem[Shankar et~al.(2021)Shankar, Dave, Roelofs, Ramanan, Recht, and
  Schmidt]{shankar2021image}
Vaishaal Shankar, Achal Dave, Rebecca Roelofs, Deva Ramanan, Benjamin Recht,
  and Ludwig Schmidt.
\newblock Do image classifiers generalize across time?
\newblock In \emph{International Conference on Computer Vision}, pages
  9661--9669, 2021.

\bibitem[Karahan et~al.(2016)Karahan, Yildirum, Kirtac, Rende, Butun, and
  Ekenel]{karahan2016image}
Samil Karahan, Merve~Kilinc Yildirum, Kadir Kirtac, Ferhat~Sukru Rende,
  Gultekin Butun, and Hazim~Kemal Ekenel.
\newblock How image degradations affect deep cnn-based face recognition?
\newblock In \emph{International Conference of the Biometrics Special Interest
  Group (BIOSIG)}, pages 1--5, 2016.

\bibitem[Azulay and Weiss(2019)]{azulay2019deep}
Aharon Azulay and Yair Weiss.
\newblock Why do deep convolutional networks generalize so poorly to small
  image transformations?
\newblock \emph{Journal of Machine Learning Research}, 20:\penalty0 1--25,
  2019.

\bibitem[Eastwood et~al.(2021)Eastwood, Mason, Williams, and
  Sch{\"o}lkopf]{eastwood2021source}
Cian Eastwood, Ian Mason, Christopher K.~I. Williams, and Bernhard
  Sch{\"o}lkopf.
\newblock Source-free adaptation to measurement shift via bottom-up feature
  restoration.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Hendrycks et~al.(2021{\natexlab{a}})Hendrycks, Zhao, Basart,
  Steinhardt, and Song]{hendrycks2021natural}
Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song.
\newblock Natural adversarial examples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 15262--15271, 2021{\natexlab{a}}.

\bibitem[Hendrycks et~al.(2021{\natexlab{b}})Hendrycks, Basart, Mu, Kadavath,
  Wang, Dorundo, Desai, Zhu, Parajuli, Guo, et~al.]{hendrycks2021many}
Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan
  Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et~al.
\newblock The many faces of robustness: A critical analysis of
  out-of-distribution generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 8340--8349, 2021{\natexlab{b}}.

\bibitem[Robey et~al.(2021{\natexlab{a}})Robey, Pappas, and
  Hassani]{robey2021modelbased}
Alexander Robey, George~J. Pappas, and Hamed Hassani.
\newblock Model-based domain generalization.
\newblock In \emph{Advances in Neural Information Processing Systems},
  2021{\natexlab{a}}.

\bibitem[Zhou et~al.(2022)Zhou, Tajwar, Robey, Knowles, Pappas, Hassani, and
  Finn]{zhou2022deep}
Allan Zhou, Fahim Tajwar, Alexander Robey, Tom Knowles, George~J Pappas, Hamed
  Hassani, and Chelsea Finn.
\newblock Do deep networks transfer invariances across classes?
\newblock \emph{arXiv preprint arXiv:2203.09739}, 2022.

\bibitem[Jovicich et~al.(2009)Jovicich, Czanner, Han, Salat, van~der Kouwe,
  Quinn, Pacheco, Albert, Killiany, Blacker, et~al.]{jovicich2009mri}
Jorge Jovicich, Silvester Czanner, Xiao Han, David Salat, Andre van~der Kouwe,
  Brian Quinn, Jenni Pacheco, Marilyn Albert, Ronald Killiany, Deborah Blacker,
  et~al.
\newblock {MRI}-derived measurements of human subcortical, ventricular and
  intracranial brain volumes: reliability effects of scan sessions, acquisition
  sequences, data analyses, scanner upgrade, scanner vendors and field
  strengths.
\newblock \emph{Neuroimage}, 46\penalty0 (1):\penalty0 177--192, 2009.

\bibitem[AlBadawy et~al.(2018)AlBadawy, Saha, and Mazurowski]{albadawy2018deep}
Ehab~A AlBadawy, Ashirbani Saha, and Maciej~A Mazurowski.
\newblock Deep learning for segmentation of brain tumors: Impact of
  cross-institutional training and testing.
\newblock \emph{Medical Physics}, 45\penalty0 (3):\penalty0 1150--1158, 2018.

\bibitem[Tellez et~al.(2019)Tellez, Litjens, B{\'a}ndi, Bulten, Bokhorst,
  Ciompi, and van~der Laak]{tellez2019quantifying}
David Tellez, Geert Litjens, P{\'e}ter B{\'a}ndi, Wouter Bulten, John-Melle
  Bokhorst, Francesco Ciompi, and Jeroen van~der Laak.
\newblock Quantifying the effects of data augmentation and stain color
  normalization in convolutional neural networks for computational pathology.
\newblock \emph{{Medical Image Analysis}}, 58:\penalty0 101--544, 2019.

\bibitem[Beede et~al.(2020)Beede, Baylor, Hersch, Iurchenko, Wilcox,
  Ruamviboonsuk, and Vardoulakis]{beede2020}
Emma Beede, Elizabeth Baylor, Fred Hersch, Anna Iurchenko, Lauren Wilcox,
  Paisan Ruamviboonsuk, and Laura~M. Vardoulakis.
\newblock A human-centered evaluation of a deep learning system deployed in
  clinics for the detection of diabetic retinopathy.
\newblock In \emph{Proceedings of the 2020 CHI Conference on Human Factors in
  Computing Systems}, page 1–12. Association for Computing Machinery, 2020.

\bibitem[Wachinger et~al.(2021)Wachinger, Rieckmann, P{\"o}lsterl, Initiative,
  et~al.]{wachinger2021detect}
Christian Wachinger, Anna Rieckmann, Sebastian P{\"o}lsterl, Alzheimer’s
  Disease~Neuroimaging Initiative, et~al.
\newblock Detect and correct bias in multi-site neuroimaging datasets.
\newblock \emph{Medical Image Analysis}, 67:\penalty0 101879, 2021.

\bibitem[Dai and Van~Gool(2018)]{dai2018dark}
Dengxin Dai and Luc Van~Gool.
\newblock Dark model adaptation: Semantic image segmentation from daytime to
  nighttime.
\newblock In \emph{International Conference on Intelligent Transportation
  Systems}, pages 3819--3824, 2018.

\bibitem[Volk et~al.(2019)Volk, M{\"u}ller, von Bernuth, Hospach, and
  Bringmann]{volk2019towards}
Georg Volk, Stefan M{\"u}ller, Alexander von Bernuth, Dennis Hospach, and
  Oliver Bringmann.
\newblock {Towards robust CNN-based object detection through augmentation with
  synthetic rain variations}.
\newblock In \emph{International Conference on Intelligent Transportation
  Systems}, pages 285--292, 2019.

\bibitem[Michaelis et~al.(2019)Michaelis, Mitzkus, Geirhos, Rusak, Bringmann,
  Ecker, Bethge, and Brendel]{michaelis2019dragon}
C.~Michaelis, B.~Mitzkus, R.~Geirhos, E.~Rusak, O.~Bringmann, A.~S. Ecker,
  M.~Bethge, and W.~Brendel.
\newblock Benchmarking robustness in object detection: Autonomous driving when
  winter is coming.
\newblock In \emph{Machine Learning for Autonomous Driving Workshop, NeurIPS
  2019}, 2019.

\bibitem[Ribeiro et~al.(2016)Ribeiro, Singh, and Guestrin]{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock "why should i trust you?": Explaining the predictions of any
  classifier.
\newblock In \emph{{Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}}, pages 1135--1144, 2016.

\bibitem[Biggio and Roli(2018)]{biggio2018wild}
Battista Biggio and Fabio Roli.
\newblock Wild patterns: Ten years after the rise of adversarial machine
  learning.
\newblock \emph{Pattern Recognition}, 84:\penalty0 317--331, 2018.

\bibitem[M{\aa}rtensson et~al.(2020)M{\aa}rtensson, Ferreira, Granberg,
  Cavallin, Oppedal, Padovani, Rektorova, Bonanni, Pardini, Kramberger,
  et~al.]{maartensson2020reliability}
Gustav M{\aa}rtensson, Daniel Ferreira, Tobias Granberg, Lena Cavallin, Ketil
  Oppedal, Alessandro Padovani, Irena Rektorova, Laura Bonanni, Matteo Pardini,
  Milica~G Kramberger, et~al.
\newblock The reliability of a deep learning model in clinical
  out-of-distribution {MRI} data: a multicohort study.
\newblock \emph{Medical Image Analysis}, 66:\penalty0 101714, 2020.

\bibitem[Castro et~al.(2020)Castro, Walker, and Glocker]{castro2020causality}
Daniel~C Castro, Ian Walker, and Ben Glocker.
\newblock Causality matters in medical imaging.
\newblock \emph{Nature Communications}, 11\penalty0 (1):\penalty0 1--10, 2020.

\bibitem[Blanchard et~al.(2011)Blanchard, Lee, and
  Scott]{blanchard2011generalizing}
Gilles Blanchard, Gyemin Lee, and Clayton Scott.
\newblock Generalizing from several related classification tasks to a new
  unlabeled sample.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~24, 2011.

\bibitem[Muandet et~al.(2013)Muandet, Balduzzi, and
  Sch{\"o}lkopf]{muandet2013domain}
Krikamol Muandet, David Balduzzi, and Bernhard Sch{\"o}lkopf.
\newblock Domain generalization via invariant feature representation.
\newblock In \emph{International Conference on Machine Learning}, pages 10--18,
  2013.

\bibitem[Gulrajani and Lopez-Paz(2020)]{gulrajani2020search}
Ishaan Gulrajani and David Lopez-Paz.
\newblock In search of lost domain generalization.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Sch{\"o}lkopf et~al.(2012)Sch{\"o}lkopf, Janzing, Peters, Sgouritsa,
  Zhang, and Mooij]{schoelkopf2012causal}
Bernhard Sch{\"o}lkopf, Dominik Janzing, Jonas Peters, Eleni Sgouritsa, Kun
  Zhang, and Joris~M Mooij.
\newblock On causal and anticausal learning.
\newblock In \emph{ICML}, 2012.

\bibitem[Li et~al.(2018{\natexlab{a}})Li, Yang, Song, and
  Hospedales]{li2018learning}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy~M. Hospedales.
\newblock Learning to generalize: Meta-learning for domain generalization.
\newblock In \emph{AAAI}, pages 3490--3497, 2018{\natexlab{a}}.

\bibitem[Krueger et~al.(2021)Krueger, Caballero, Jacobsen, Zhang, Binas, Zhang,
  Priol, and Courville]{krueger21rex}
David Krueger, Ethan Caballero, Joern-Henrik Jacobsen, Amy Zhang, Jonathan
  Binas, Dinghuai Zhang, Remi~Le Priol, and Aaron Courville.
\newblock Out-of-distribution generalization via risk extrapolation ({REx}).
\newblock In \emph{International Conference on Machine Learning}, volume 139,
  pages 5815--5826, 2021.

\bibitem[Blanchard et~al.(2021)Blanchard, Deshmukh, Dogan, Lee, and
  Scott]{blanchard2021domain}
Gilles Blanchard, Aniket~Anand Deshmukh, {\"U}run Dogan, Gyemin Lee, and
  Clayton Scott.
\newblock Domain generalization by marginal transfer learning.
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 46--100, 2021.

\bibitem[Zhang et~al.(2021)Zhang, Marklund, Dhawan, Gupta, Levine, and
  Finn]{zhang2020adaptive}
Marvin Zhang, Henrik Marklund, Nikita Dhawan, Abhishek Gupta, Sergey Levine,
  and Chelsea Finn.
\newblock Adaptive risk minimization: Learning to adapt to domain shift.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 23664--23678, 2021.

\bibitem[Ben-Tal et~al.(2009)Ben-Tal, El~Ghaoui, and Nemirovski]{ben2009robust}
Aharon Ben-Tal, Laurent El~Ghaoui, and Arkadi Nemirovski.
\newblock Robust optimization.
\newblock In \emph{Robust optimization}. Princeton University Press, 2009.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Nagarajan et~al.(2021)Nagarajan, Andreassen, and
  Neyshabur]{nagarajan2021understanding}
Vaishnavh Nagarajan, Anders Andreassen, and Behnam Neyshabur.
\newblock Understanding the failure modes of out-of-distribution
  generalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Tsipras et~al.(2019)Tsipras, Santurkar, Engstrom, Turner, and
  Madry]{tsipras2019robustness}
Dimitris Tsipras, Shibani Santurkar, Logan Engstrom, Alexander Turner, and
  Aleksander Madry.
\newblock Robustness may be at odds with accuracy.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Raghunathan et~al.(2019)Raghunathan, Xie, Yang, Duchi, and
  Liang]{raghunathan2019adversarial}
Aditi Raghunathan, Sang~Michael Xie, Fanny Yang, John Duchi, and Percy Liang.
\newblock Adversarial training can hurt generalization.
\newblock In \emph{ICML 2019 Workshop on Identifying and Understanding Deep
  Learning Phenomena}, 2019.

\bibitem[Parzen(1962)]{parzen1962estimation}
Emanuel Parzen.
\newblock On estimation of a probability density function and mode.
\newblock \emph{The Annals of Mathematical Statistics}, 33\penalty0
  (3):\penalty0 1065--1076, 1962.

\bibitem[Beery et~al.(2021)Beery, Agarwal, Cole, and
  Birodkar]{beery2021iwildcam}
Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh Birodkar.
\newblock The {iWildCam} 2021 competition dataset.
\newblock \emph{arXiv preprint arXiv:2105.03494}, 2021.

\bibitem[Ahuja et~al.(2021)Ahuja, Caballero, Zhang, Gagnon-Audet, Bengio,
  Mitliagkas, and Rish]{ahuja2021invariance}
Kartik Ahuja, Ethan Caballero, Dinghuai Zhang, Jean-Christophe Gagnon-Audet,
  Yoshua Bengio, Ioannis Mitliagkas, and Irina Rish.
\newblock Invariance principle meets information bottleneck for
  out-of-distribution generalization.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Ben-David et~al.(2010)Ben-David, Blitzer, Crammer, Kulesza, Pereira,
  and Vaughan]{ben2010theory}
Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and
  Jennifer~Wortman Vaughan.
\newblock A theory of learning from different domains.
\newblock \emph{Machine Learning}, 79\penalty0 (1):\penalty0 151--175, 2010.

\bibitem[Christiansen et~al.(2021)Christiansen, Pfister, Jakobsen, Gnecco, and
  Peters]{christiansen2021causal}
Rune Christiansen, Niklas Pfister, Martin~Emil Jakobsen, Nicola Gnecco, and
  Jonas Peters.
\newblock A causal framework for distribution generalization.
\newblock \emph{IEEE Transactions on Pattern Analysis and Machine
  Intelligence}, 2021.

\bibitem[Peters et~al.(2016)Peters, B{\"u}hlmann, and
  Meinshausen]{peters2016causal}
Jonas Peters, Peter B{\"u}hlmann, and Nicolai Meinshausen.
\newblock Causal inference by using invariant prediction: identification and
  confidence intervals.
\newblock \emph{Journal of the Royal Statistical Society. Series B (Statistical
  Methodology)}, pages 947--1012, 2016.

\bibitem[Rojas-Carulla et~al.(2018)Rojas-Carulla, Sch{\"o}lkopf, Turner, and
  Peters]{rojas2018invariant}
Mateo Rojas-Carulla, Bernhard Sch{\"o}lkopf, Richard Turner, and Jonas Peters.
\newblock Invariant models for causal transfer learning.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 1309--1342, 2018.

\bibitem[Pearl(2009)]{pearl2009causality}
Judea Pearl.
\newblock \emph{Causality}.
\newblock {Cambridge University Press}, 2009.

\bibitem[Peters et~al.(2017)Peters, Janzing, and
  Sch{\"o}lkopf]{peters2017elements}
Jonas Peters, Dominik Janzing, and Bernhard Sch{\"o}lkopf.
\newblock \emph{Elements of causal inference: foundations and learning
  algorithms}.
\newblock The MIT Press, 2017.

\bibitem[Campi and Garatti(2008)]{campi2008exact}
Marco~C Campi and Simone Garatti.
\newblock The exact feasibility of randomized solutions of uncertain convex
  programs.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0
  1211--1230, 2008.

\bibitem[Ramponi(2018)]{ramponi2018consistency}
Federico~Alessandro Ramponi.
\newblock Consistency of the scenario approach.
\newblock \emph{SIAM Journal on Optimization}, 28\penalty0 (1):\penalty0
  135--162, 2018.

\bibitem[Spielman and Teng(2004)]{spielman2004smoothed}
Daniel~A Spielman and Shang-Hua Teng.
\newblock Smoothed analysis of algorithms: Why the simplex algorithm usually
  takes polynomial time.
\newblock \emph{Journal of the ACM (JACM)}, 51\penalty0 (3):\penalty0 385--463,
  2004.

\bibitem[Vapnik(1999)]{vapnik1999nature}
Vladimir Vapnik.
\newblock \emph{The nature of statistical learning theory}.
\newblock Springer Science \& Business Media, 1999.

\bibitem[Duffie and Pan(1997)]{duffie1997overview}
Darrell Duffie and Jun Pan.
\newblock An overview of value at risk.
\newblock \emph{Journal of derivatives}, 4\penalty0 (3):\penalty0 7--49, 1997.

\bibitem[Vapnik(1998)]{Vapnik98}
V.~N. Vapnik.
\newblock \emph{Statistical Learning Theory}.
\newblock Wiley, New York, NY, 1998.

\bibitem[Rosenblatt(1956)]{rosenblatt1956remarks}
Murray Rosenblatt.
\newblock Remarks on some nonparametric estimates of a density function.
\newblock \emph{The Annals of Mathematical Statistics}, pages 832--837, 1956.

\bibitem[Silverman(1986)]{silverman1986density}
Bernard~W Silverman.
\newblock \emph{Density Estimation for Statistics and Data Analysis},
  volume~26.
\newblock CRC Press, 1986.

\bibitem[Yin et~al.(2021)Yin, Wang, and Blei]{yin2021optimization}
Mingzhang Yin, Yixin Wang, and David~M Blei.
\newblock Optimization-based causal estimation from heterogenous environments.
\newblock \emph{arXiv preprint arXiv:2109.11990}, 2021.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Shlens, and
  Szegedy]{goodfellow2014explaining}
Ian~J Goodfellow, Jonathon Shlens, and Christian Szegedy.
\newblock Explaining and harnessing adversarial examples.
\newblock \emph{arXiv preprint arXiv:1412.6572}, 2014.

\bibitem[Madry et~al.(2017)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2017towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock \emph{arXiv preprint arXiv:1706.06083}, 2017.

\bibitem[Zhang et~al.(2019)Zhang, Yu, Jiao, Xing, El~Ghaoui, and
  Jordan]{zhang2019theoretically}
Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El~Ghaoui, and
  Michael Jordan.
\newblock Theoretically principled trade-off between robustness and accuracy.
\newblock In \emph{International Conference on Machine Learning}, pages
  7472--7482. PMLR, 2019.

\bibitem[Robey et~al.(2021{\natexlab{b}})Robey, Chamon, Pappas, Hassani, and
  Ribeiro]{robey2021adversarial}
Alexander Robey, Luiz Chamon, George~J Pappas, Hamed Hassani, and Alejandro
  Ribeiro.
\newblock Adversarial robustness with semi-infinite constrained learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 6198--6215, 2021{\natexlab{b}}.

\bibitem[Zhu et~al.(2021)Zhu, Kouridi, Nemmour, and
  Sch{\"o}lkopf]{zhu2021adversarially}
Jia-Jie Zhu, Christina Kouridi, Yassine Nemmour, and Bernhard Sch{\"o}lkopf.
\newblock Adversarially robust kernel smoothing.
\newblock \emph{arXiv preprint arXiv:2102.08474}, 2021.

\bibitem[Martinez et~al.(2021)Martinez, Bertran, Papadaki, Rodrigues, and
  Sapiro]{martinez2021blind}
Natalia~L Martinez, Martin~A Bertran, Afroditi Papadaki, Miguel Rodrigues, and
  Guillermo Sapiro.
\newblock Blind pareto fairness and subgroup robustness.
\newblock In \emph{International Conference on Machine Learning}, pages
  7492--7501. PMLR, 2021.

\bibitem[Diana et~al.(2021)Diana, Gill, Kearns, Kenthapadi, and
  Roth]{diana2021minimax}
Emily Diana, Wesley Gill, Michael Kearns, Krishnaram Kenthapadi, and Aaron
  Roth.
\newblock Minimax group fairness: Algorithms and experiments.
\newblock In \emph{Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics,
  and Society}, pages 66--76, 2021.

\bibitem[Balaji et~al.(2018)Balaji, Sankaranarayanan, and
  Chellappa]{balaji2018metareg}
Yogesh Balaji, Swami Sankaranarayanan, and Rama Chellappa.
\newblock Metareg: Towards domain generalization using meta-regularization.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Dou et~al.(2019)Dou, Coelho~de Castro, Kamnitsas, and
  Glocker]{dou2019domain}
Qi~Dou, Daniel Coelho~de Castro, Konstantinos Kamnitsas, and Ben Glocker.
\newblock Domain generalization via model-agnostic learning of semantic
  features.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Shu et~al.(2021)Shu, Cao, Wang, Wang, and Long]{shu2021open}
Yang Shu, Zhangjie Cao, Chenyu Wang, Jianmin Wang, and Mingsheng Long.
\newblock Open domain generalization with domain-augmented meta-learning.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9624--9633, 2021.

\bibitem[Dubey et~al.(2021)Dubey, Ramanathan, Pentland, and
  Mahajan]{dubey2021adaptive}
Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan.
\newblock Adaptive methods for real-world domain generalization.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 14340--14349, 2021.

\bibitem[Deshmukh et~al.(2019)Deshmukh, Lei, Sharma, Dogan, Cutler, and
  Scott]{deshmukh2019generalization}
Aniket~Anand Deshmukh, Yunwen Lei, Srinagesh Sharma, Urun Dogan, James~W
  Cutler, and Clayton Scott.
\newblock A generalization error bound for multi-class domain generalization.
\newblock \emph{arXiv preprint arXiv:1905.10392}, 2019.

\bibitem[Zhao et~al.(2020)Zhao, Gong, Liu, Fu, and Tao]{zhao2020domain}
Shanshan Zhao, Mingming Gong, Tongliang Liu, Huan Fu, and Dacheng Tao.
\newblock Domain generalization via entropy regularization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16096--16107, 2020.

\bibitem[Li et~al.(2020{\natexlab{a}})Li, Wang, Wan, Wang, Li, and
  Kot]{li2020domain}
Haoliang Li, YuFei Wang, Renjie Wan, Shiqi Wang, Tie-Qiang Li, and Alex Kot.
\newblock Domain generalization for medical imaging classification with
  linear-dependency regularization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 3118--3129, 2020{\natexlab{a}}.

\bibitem[Kim et~al.(2021)Kim, Yoo, Park, Kim, and Lee]{kim2021selfreg}
Daehee Kim, Youngjun Yoo, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee.
\newblock Selfreg: Self-supervised contrastive regularization for domain
  generalization.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 9619--9628, 2021.

\bibitem[Ganin et~al.(2016)Ganin, Ustinova, Ajakan, Germain, Larochelle,
  Laviolette, Marchand, and Lempitsky]{ganin2016domain}
Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo
  Larochelle, Fran{\c{c}}ois Laviolette, Mario Marchand, and Victor Lempitsky.
\newblock Domain-adversarial training of neural networks.
\newblock \emph{The Journal of Machine Learning Research}, 17\penalty0
  (1):\penalty0 2096--2030, 2016.

\bibitem[Li et~al.(2018{\natexlab{b}})Li, Tian, Gong, Liu, Liu, Zhang, and
  Tao]{li2018deep}
Ya~Li, Xinmei Tian, Mingming Gong, Yajing Liu, Tongliang Liu, Kun Zhang, and
  Dacheng Tao.
\newblock Deep domain generalization via conditional invariant adversarial
  networks.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 624--639, 2018{\natexlab{b}}.

\bibitem[Huang et~al.(2020)Huang, Wang, Xing, and Huang]{huang2020self}
Zeyi Huang, Haohan Wang, Eric~P Xing, and Dong Huang.
\newblock Self-challenging improves cross-domain generalization.
\newblock In \emph{European Conference on Computer Vision}, pages 124--140.
  Springer, 2020.

\bibitem[Su et~al.(2019)Su, Vargas, and Sakurai]{su2019one}
Jiawei Su, Danilo~Vasconcellos Vargas, and Kouichi Sakurai.
\newblock One pixel attack for fooling deep neural networks.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 23\penalty0
  (5):\penalty0 828--841, 2019.

\bibitem[Garg et~al.(2021)Garg, Kalai, Ligett, and Wu]{garg2021learn}
Vikas Garg, Adam~Tauman Kalai, Katrina Ligett, and Steven Wu.
\newblock Learn to expect the unexpected: Probably approximately correct domain
  generalization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 3574--3582. PMLR, 2021.

\bibitem[Tempo et~al.(2013)Tempo, Calafiore, and Dabbene]{tempo2013randomized}
Roberto Tempo, Giuseppe Calafiore, and Fabrizio Dabbene.
\newblock \emph{Randomized algorithms for analysis and control of uncertain
  systems: with applications}.
\newblock Springer, 2013.

\bibitem[Lindemann et~al.(2021)Lindemann, Matni, and Pappas]{lindemann2021stl}
Lars Lindemann, Nikolai Matni, and George~J Pappas.
\newblock Stl robustness risk over discrete-time stochastic processes.
\newblock \emph{arXiv preprint arXiv:2104.01503}, 2021.

\bibitem[Lindemann et~al.(2022)Lindemann, Rodionova, and
  Pappas]{lindemann2022temporal}
Lars Lindemann, Alena Rodionova, and George~J. Pappas.
\newblock Temporal robustness of stochastic signals.
\newblock In \emph{25th ACM International Conference on Hybrid Systems:
  Computation and Control}, pages 1--11, 2022.

\bibitem[Shapiro et~al.(2021)Shapiro, Dentcheva, and
  Ruszczynski]{shapiro2021lectures}
Alexander Shapiro, Darinka Dentcheva, and Andrzej Ruszczynski.
\newblock \emph{Lectures on stochastic programming: modeling and theory}.
\newblock SIAM, 2021.

\bibitem[Robey et~al.(2022)Robey, Chamon, Pappas, and
  Hassani]{robey2022probabilistically}
Alexander Robey, Luiz~FO Chamon, George~J Pappas, and Hamed Hassani.
\newblock Probabilistically robust learning: Balancing average- and worst-case
  performance.
\newblock \emph{arXiv preprint arXiv:2202.01136}, 2022.

\bibitem[Rice et~al.(2021)Rice, Bair, Zhang, and Kolter]{rice2021robustness}
Leslie Rice, Anna Bair, Huan Zhang, and J~Zico Kolter.
\newblock Robustness between the worst and average case.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Li et~al.(2020{\natexlab{b}})Li, Beirami, Sanjabi, and
  Smith]{li2020tilted}
Tian Li, Ahmad Beirami, Maziar Sanjabi, and Virginia Smith.
\newblock Tilted empirical risk minimization.
\newblock \emph{arXiv preprint arXiv:2007.01162}, 2020{\natexlab{b}}.

\bibitem[Curi et~al.(2020)Curi, Levy, Jegelka, and Krause]{curi2020adaptive}
Sebastian Curi, Kfir~Y Levy, Stefanie Jegelka, and Andreas Krause.
\newblock Adaptive sampling for stochastic risk-averse learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1036--1047, 2020.

\bibitem[Paternain et~al.(2022)Paternain, Calvo-Fullana, Chamon, and
  Ribeiro]{paternain2022safe}
Santiago Paternain, Miguel Calvo-Fullana, Luiz~FO Chamon, and Alejandro
  Ribeiro.
\newblock Safe policies for reinforcement learning via primal-dual methods.
\newblock \emph{IEEE Transactions on Automatic Control}, 2022.

\bibitem[Chow et~al.(2017)Chow, Ghavamzadeh, Janson, and Pavone]{chow2017risk}
Yinlam Chow, Mohammad Ghavamzadeh, Lucas Janson, and Marco Pavone.
\newblock Risk-constrained reinforcement learning with percentile risk
  criteria.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 6070--6120, 2017.

\bibitem[Rockafellar et~al.(2000)Rockafellar, Uryasev,
  et~al.]{rockafellar2000CVaR}
R~Tyrrell Rockafellar, Stanislav Uryasev, et~al.
\newblock Optimization of conditional value-at-risk.
\newblock \emph{Journal of risk}, 2:\penalty0 21--42, 2000.

\bibitem[Krokhmal et~al.(2002)Krokhmal, Palmquist, and
  Uryasev]{krokhmal2002portfolio}
Pavlo Krokhmal, Jonas Palmquist, and Stanislav Uryasev.
\newblock Portfolio optimization with conditional value-at-risk objective and
  constraints.
\newblock \emph{Journal of risk}, 4:\penalty0 43--68, 2002.

\bibitem[Wozabal(2012)]{wozabal2012value}
David Wozabal.
\newblock Value-at-risk optimization using the difference of convex algorithm.
\newblock \emph{OR spectrum}, 34\penalty0 (4):\penalty0 861--883, 2012.

\bibitem[Jorion(1997)]{jorion1997value}
Philippe Jorion.
\newblock \emph{Value at risk: the new benchmark for controlling market risk}.
\newblock Irwin Professional Pub., 1997.

\bibitem[Lee et~al.(2020)Lee, Park, and Shin]{lee2020learning}
Jaeho Lee, Sejun Park, and Jinwoo Shin.
\newblock Learning bounds for risk-sensitive learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 13867--13879, 2020.

\bibitem[Khim et~al.(2020)Khim, Leqi, Prasad, and Ravikumar]{khim2020uniform}
Justin Khim, Liu Leqi, Adarsh Prasad, and Pradeep Ravikumar.
\newblock Uniform convergence of rank-weighted learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5254--5263. PMLR, 2020.

\bibitem[Duchi and Namkoong(2021)]{duchi2021learning}
John~C Duchi and Hongseok Namkoong.
\newblock Learning models with uniform performance via distributionally robust
  optimization.
\newblock \emph{The Annals of Statistics}, 49\penalty0 (3):\penalty0
  1378--1406, 2021.

\bibitem[Zhang et~al.(2013)Zhang, Sch{\"o}lkopf, Muandet, and
  Wang]{zhang2013domain}
Kun Zhang, Bernhard Sch{\"o}lkopf, Krikamol Muandet, and Zhikun Wang.
\newblock Domain adaptation under target and conditional shift.
\newblock In \emph{International Conference on Machine Learning}, pages
  819--827. PMLR, 2013.

\bibitem[Bareinboim and Pearl(2014)]{Bareinboim2014}
E.~Bareinboim and J.~Pearl.
\newblock Transportability from multiple environments with limited experiments:
  Completeness results.
\newblock In \emph{{A}dvances in {N}eural {I}nformation {P}rocessing {S}ystems
  27}, pages 280--288, 2014.

\bibitem[Zhang et~al.(2015)Zhang, Gong, and Sch{\"o}lkopf]{zhang2015multi}
Kun Zhang, Mingming Gong, and Bernhard Sch{\"o}lkopf.
\newblock Multi-source domain adaptation: A causal view.
\newblock In \emph{Twenty-ninth AAAI Conference on Artificial Intelligence},
  2015.

\bibitem[Gong et~al.(2016)Gong, Zhang, Liu, Tao, Glymour, and
  Sch{\"o}lkopf]{gong2016domain}
Mingming Gong, Kun Zhang, Tongliang Liu, Dacheng Tao, Clark Glymour, and
  Bernhard Sch{\"o}lkopf.
\newblock Domain adaptation with conditional transferable components.
\newblock In \emph{International Conference on Machine Learning}, pages
  2839--2848. PMLR, 2016.

\bibitem[Huang et~al.(2017)Huang, Zhang, Zhang, Sanchez-Romero, Glymour, and
  Sch{\"o}lkopf]{HuaZhaZhaSanGlySch17}
B.~Huang, K.~Zhang, J.~Zhang, R.~Sanchez-Romero, C.~Glymour, and
  B.~Sch{\"o}lkopf.
\newblock Behind distribution shift: Mining driving forces of changes and
  causal arrows.
\newblock In \emph{{IEEE} 17th International Conference on Data Mining (ICDM
  2017)}, pages 913--918, 2017.

\bibitem[Heinze-Deml et~al.(2018)Heinze-Deml, Peters, and
  Meinshausen]{heinze2018invariant}
Christina Heinze-Deml, Jonas Peters, and Nicolai Meinshausen.
\newblock Invariant causal prediction for nonlinear models.
\newblock \emph{Journal of Causal Inference}, 6\penalty0 (2), 2018.

\bibitem[Pfister et~al.(2019)Pfister, B{\"u}hlmann, and
  Peters]{pfister2019invariant}
Niklas Pfister, Peter B{\"u}hlmann, and Jonas Peters.
\newblock Invariant causal prediction for sequential data.
\newblock \emph{Journal of the American Statistical Association}, 114\penalty0
  (527):\penalty0 1264--1276, 2019.

\bibitem[Gamella and Heinze-Deml(2020)]{gamella2020active}
Juan~L Gamella and Christina Heinze-Deml.
\newblock Active invariant causal prediction: Experiment selection through
  stability.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 15464--15475, 2020.

\bibitem[Pezeshki et~al.(2021)Pezeshki, Kaba, Bengio, Courville, Precup, and
  Lajoie]{pezeshki2021gradient}
Mohammad Pezeshki, Oumar Kaba, Yoshua Bengio, Aaron~C Courville, Doina Precup,
  and Guillaume Lajoie.
\newblock Gradient starvation: A learning proclivity in neural networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1256--1272, 2021.

\bibitem[Koyama and Yamaguchi(2020)]{koyama2020out}
Masanori Koyama and Shoichiro Yamaguchi.
\newblock Out-of-distribution generalization with maximal invariant predictor.
\newblock \url{https://openreview.net/forum?id=FzGiUKN4aBp}, 2020.

\bibitem[Glorot and Bengio(2010)]{glorot2010understanding}
Xavier Glorot and Yoshua Bengio.
\newblock Understanding the difficulty of training deep feedforward neural
  networks.
\newblock In \emph{Proceedings of the Thirteenth International Conference on
  Artificial Intelligence and Atatistics}, pages 249--256. PMLR, 2010.

\bibitem[Zhang et~al.(2022)Zhang, Lopez-Paz, and Bottou]{zhang2022rich}
Jianyu Zhang, David Lopez-Paz, and L{\'e}on Bottou.
\newblock Rich feature construction for the optimization-generalization
  dilemma.
\newblock \emph{arXiv preprint arXiv:2203.15516}, 2022.

\bibitem[Hu et~al.(2020)Hu, Fey, Zitnik, Dong, Ren, Liu, Catasta, and
  Leskovec]{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 22118--22133, 2020.

\bibitem[Wu et~al.(2018)Wu, Ramsundar, Feinberg, Gomes, Geniesse, Pappu,
  Leswing, and Pande]{wu2018moleculenet}
Zhenqin Wu, Bharath Ramsundar, Evan~N Feinberg, Joseph Gomes, Caleb Geniesse,
  Aneesh~S Pappu, Karl Leswing, and Vijay Pande.
\newblock Moleculenet: a benchmark for molecular machine learning.
\newblock \emph{Chemical Science}, 9\penalty0 (2):\penalty0 513--530, 2018.

\bibitem[Fang et~al.(2013)Fang, Xu, and Rockmore]{fang2013}
Chen Fang, Ye~Xu, and Daniel~N. Rockmore.
\newblock Unbiased metric learning: On the utilization of multiple datasets and
  web images for softening bias.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2013.

\bibitem[Li et~al.(2017)Li, Yang, Song, and Hospedales]{li2017deeper}
Da~Li, Yongxin Yang, Yi-Zhe Song, and Timothy~M. Hospedales.
\newblock Deeper, broader and artier domain generalization.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision (ICCV)}, 2017.

\bibitem[Venkateswara et~al.(2017)Venkateswara, Eusebio, Chakraborty, and
  Panchanathan]{venkateswara2017}
Hemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman
  Panchanathan.
\newblock Deep hashing network for unsupervised domain adaptation.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition (CVPR)}, 2017.

\bibitem[Peng et~al.(2019)Peng, Bai, Xia, Huang, Saenko, and
  Wang]{peng2019moment}
Xingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo~Wang.
\newblock Moment matching for multi-source domain adaptation.
\newblock In \emph{Proceedings of the IEEE International Conference on Computer
  Vision}, pages 1406--1415, 2019.

\bibitem[Qui{\~n}onero-Candela et~al.(2008)Qui{\~n}onero-Candela, Sugiyama,
  Schwaighofer, and Lawrence]{quinonero2008dataset}
Joaquin Qui{\~n}onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil~D
  Lawrence.
\newblock \emph{Dataset shift in machine learning}.
\newblock MIT Press, 2008.

\bibitem[Storkey(2009)]{Storkey09}
Amos~J Storkey.
\newblock When training and test sets are different: characterising learning
  transfer.
\newblock In \emph{Dataset Shift in Machine Learning}, pages 3--28. MIT Press,
  2009.

\bibitem[Lipton et~al.(2018)Lipton, Wang, and Smola]{lipton2018detecting}
Zachary Lipton, Yu-Xiang Wang, and Alexander Smola.
\newblock Detecting and correcting for label shift with black box predictors.
\newblock In \emph{International Conference on Machine Learning}, pages
  3122--3130, 2018.

\bibitem[Moreno-Torres et~al.(2012)Moreno-Torres, Raeder, Alaiz-Rodr{\'\i}guez,
  Chawla, and Herrera]{moreno2012unifying}
Jose~G Moreno-Torres, Troy Raeder, Roc{\'\i}o Alaiz-Rodr{\'\i}guez, Nitesh~V
  Chawla, and Francisco Herrera.
\newblock A unifying view on dataset shift in classification.
\newblock \emph{Pattern Recognition}, 45:\penalty0 521--530, 2012.

\bibitem[Rothenh{\"a}usler et~al.(2021)Rothenh{\"a}usler, Meinshausen,
  B{\"u}hlmann, and Peters]{rothenhausler2021anchor}
Dominik Rothenh{\"a}usler, Nicolai Meinshausen, Peter B{\"u}hlmann, and Jonas
  Peters.
\newblock Anchor regression: Heterogeneous data meet causality.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 83\penalty0 (2):\penalty0 215--246, 2021.

\bibitem[Ilse et~al.(2020)Ilse, Tomczak, Louizos, and Welling]{ilse2020diva}
Maximilian Ilse, Jakub~M Tomczak, Christos Louizos, and Max Welling.
\newblock Diva: Domain invariant variational autoencoders.
\newblock In \emph{Medical Imaging with Deep Learning}, pages 322--348. PMLR,
  2020.

\bibitem[Sch{\"o}lkopf et~al.(1997)Sch{\"o}lkopf, Smola, and
  M{\"u}ller]{scholkopf1997kernel}
Bernhard Sch{\"o}lkopf, Alexander Smola, and Klaus-Robert M{\"u}ller.
\newblock Kernel principal component analysis.
\newblock In \emph{International Conference on Artificial Neural Networks},
  pages 583--588, 1997.

\bibitem[Takeuchi et~al.(2006)Takeuchi, Le, Sears, and
  Smola]{takeuchi2006nonparametric}
Ichiro Takeuchi, Quoc~V. Le, Timothy~D. Sears, and Alexander~J. Smola.
\newblock Nonparametric quantile estimation.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0
  (45):\penalty0 1231--1264, 2006.

\bibitem[Sriperumbudur et~al.(2009)Sriperumbudur, Fukumizu, Gretton, Lanckriet,
  and Sch{\"o}lkopf]{sriperumbudur2009kernel}
Bharath~K Sriperumbudur, Kenji Fukumizu, Arthur Gretton, Gert Lanckriet, and
  Bernhard Sch{\"o}lkopf.
\newblock Kernel choice and classifiability for {RKHS} embeddings of
  probability distributions.
\newblock \emph{Advances in Neural Information Processing Systems}, 22, 2009.

\bibitem[Bousquet et~al.(2003)Bousquet, Boucheron, and
  Lugosi]{bousquet2003introduction}
Olivier Bousquet, St{\'e}phane Boucheron, and G{\'a}bor Lugosi.
\newblock Introduction to statistical learning theory.
\newblock In \emph{Summer School on Machine Learning}, pages 169--207.
  Springer, 2003.

\bibitem[Massart(1990)]{massart1990tight}
Pascal Massart.
\newblock The tight constant in the dvoretzky-kiefer-wolfowitz inequality.
\newblock \emph{The annals of Probability}, pages 1269--1283, 1990.

\bibitem[Tsybakov(2004)]{tsybakov2004introduction}
Alexandre~B Tsybakov.
\newblock \emph{Introduction to nonparametric estimation}.
\newblock Springer, 2004.

\bibitem[DeVore and Lorentz(1993)]{devore1993constructive}
Ronald~A DeVore and George~G Lorentz.
\newblock \emph{Constructive approximation}, volume 303.
\newblock Springer Science \& Business Media, 1993.

\bibitem[Ball et~al.(1997)]{ball1997elementary}
Keith Ball et~al.
\newblock An elementary introduction to modern convex geometry.
\newblock \emph{Flavors of geometry}, 31\penalty0 (1-58):\penalty0 26, 1997.

\bibitem[Blair et~al.(1976)Blair, Edwards, and Johnson]{blair1976rational}
JM~Blair, CA~Edwards, and J~Howard Johnson.
\newblock Rational {Chebyshev} approximations for the inverse of the error
  function.
\newblock \emph{Mathematics of Computation}, 30\penalty0 (136):\penalty0
  827--830, 1976.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 770--778, 2016.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{deng2009imagenet}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In \emph{IEEE Conference on Computer Vision and Pattern Recognition},
  pages 248--255, 2009.

\bibitem[Xu et~al.(2018)Xu, Hu, Leskovec, and Jegelka]{xu2018powerful}
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.
\newblock How powerful are graph neural networks?
\newblock \emph{arXiv preprint arXiv:1810.00826}, 2018.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Justin Gilmer, Samuel~S Schoenholz, Patrick~F Riley, Oriol Vinyals, and
  George~E Dahl.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pages
  1263--1272, 2017.

\bibitem[Urp{\'\i} et~al.(2021)Urp{\'\i}, Curi, and Krause]{urpi2021risk}
N{\'u}ria~Armengol Urp{\'\i}, Sebastian Curi, and Andreas Krause.
\newblock Risk-averse offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2102.05371}, 2021.

\bibitem[Boyd et~al.(2004)Boyd, Boyd, and Vandenberghe]{boyd2004convex}
Stephen Boyd, Stephen~P Boyd, and Lieven Vandenberghe.
\newblock \emph{Convex optimization}.
\newblock Cambridge University Press, 2004.

\end{thebibliography}
