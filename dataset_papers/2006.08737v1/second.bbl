\begin{thebibliography}{10}

\bibitem{qsgd}
D.~Alistarh, D.~Grubic, J.~Li, R.~Tomioka, and M.~Vojnovic.
\newblock Qsgd: Communication-efficient sgd via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1709--1720, 2017.

\bibitem{signsgd}
J.~Bernstein, Y.-X. Wang, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock signsgd: Compressed optimisation for non-convex problems.
\newblock {\em arXiv preprint arXiv:1802.04434}, 2018.

\bibitem{anima}
J.~Bernstein, J.~Zhao, K.~Azizzadenesheli, and A.~Anandkumar.
\newblock signsgd with majority vote is communication efficient and byzantine
  fault tolerant.
\newblock {\em arXiv preprint arXiv:1810.05291}, 2018.

\bibitem{blanchard2017byzantine}
P.~Blanchard, E.~M.~E. Mhamdi, R.~Guerraoui, and J.~Stainer.
\newblock Byzantine-tolerant machine learning.
\newblock {\em arXiv preprint arXiv:1703.02757}, 2017.

\bibitem{libsvm}
C.-C. Chang and C.-J. Lin.
\newblock Libsvm: A library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):27, 2011.

\bibitem{chen}
Y.~Chen, L.~Su, and J.~Xu.
\newblock Distributed statistical machine learning in adversarial settings:
  Byzantine gradient descent.
\newblock {\em Proceedings of the ACM on Measurement and Analysis of Computing
  Systems}, 1(2):44, 2017.

\bibitem{dingo}
R.~Crane and F.~Roosta.
\newblock Dingo: Distributed newton-type method for gradient-norm optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9494--9504, 2019.

\bibitem{Damaskinos}
G.~Damaskinos, E.~M. El~Mhamdi, R.~Guerraoui, A.~H.~A. Guirguis, and S.~L.~A.
  Rouault.
\newblock Aggregathor: Byzantine machine learning via robust gradient
  aggregation.
\newblock page~19, 2019.
\newblock Published in the Conference on Systems and Machine Learning (SysML)
  2019, Stanford, CA, USA.

\bibitem{deter}
M.~Derezinski and M.~W. Mahoney.
\newblock Distributed estimation of the inverse hessian by determinantal
  averaging.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  11401--11411, 2019.

\bibitem{devlin2018bert}
J.~Devlin, M.-W. Chang, K.~Lee, and K.~Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{drineas2012fast}
P.~Drineas, M.~Magdon-Ismail, M.~W. Mahoney, and D.~P. Woodruff.
\newblock Fast approximation of matrix coherence and statistical leverage.
\newblock {\em Journal of Machine Learning Research}, 13(Dec):3475--3506, 2012.

\bibitem{randnla}
P.~Drineas and M.~W. Mahoney.
\newblock Randnla: randomized numerical linear algebra.
\newblock {\em Communications of the ACM}, 59(6):80--90, 2016.

\bibitem{feng}
J.~Feng, H.~Xu, and S.~Mannor.
\newblock Distributed robust learning.
\newblock {\em arXiv preprint arXiv:1409.5937}, 2014.

\bibitem{vqsgd}
V.~Gandikota, R.~K. Maity, and A.~Mazumdar.
\newblock vqsgd: Vector quantized stochastic gradient descent.
\newblock {\em arXiv preprint arXiv:1911.07971}, 2019.

\bibitem{ghosh2019robust}
A.~Ghosh, J.~Hong, D.~Yin, and K.~Ramchandran.
\newblock Robust federated learning in a heterogeneous environment.
\newblock {\em arXiv preprint arXiv:1906.06629}, 2019.

\bibitem{ghosh2019communication}
A.~Ghosh, R.~K. Maity, S.~Kadhe, A.~Mazumdar, and K.~Ramchandran.
\newblock Communication-efficient and byzantine-robust distributed learning.
\newblock {\em arXiv preprint arXiv:1911.09721}, 2019.

\bibitem{sparsedingo}
A.~Ghosh, R.~K. Maity, A.~Mazumdar, and K.~Ramchandran.
\newblock Communication efficient distributed approximate newton method.
\newblock {\em ISIT 2020 (accepted), https://tinyurl.com/ujnpt4c}, 2020.

\bibitem{oversketched}
V.~Gupta, S.~Kadhe, T.~Courtade, M.~W. Mahoney, and K.~Ramchandran.
\newblock Oversketched newton: Fast convex optimization for serverless systems.
\newblock {\em arXiv preprint arXiv:1903.08857}, 2019.

\bibitem{errorfeed}
S.~P. Karimireddy, Q.~Rebjock, S.~U. Stich, and M.~Jaggi.
\newblock Error feedback fixes signsgd and other gradient compression schemes.
\newblock {\em arXiv preprint arXiv:1901.09847}, 2019.

\bibitem{federated}
J.~Kone{\v{c}}n{\`y}, H.~B. McMahan, D.~Ramage, and P.~Richt{\'a}rik.
\newblock Federated optimization: Distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv preprint arXiv:1610.02527}, 2016.

\bibitem{lamport}
L.~Lamport, R.~Shostak, and M.~Pease.
\newblock The byzantine generals problem.
\newblock {\em ACM Trans. Program. Lang. Syst.}, 4(3):382--401, July 1982.

\bibitem{mahoneyrandomized}
M.~W. Mahoney.
\newblock Randomized algorithms for matrices and data.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  3(2):123--224, 2011.

\bibitem{mhamdi2018hidden}
E.~M.~E. Mhamdi, R.~Guerraoui, and S.~Rouault.
\newblock The hidden vulnerability of distributed learning in byzantium.
\newblock {\em arXiv preprint arXiv:1802.07927}, 2018.

\bibitem{newtonsketch}
M.~Pilanci and M.~J. Wainwright.
\newblock Newton sketch: A near linear-time optimization algorithm with
  linear-quadratic convergence.
\newblock {\em SIAM Journal on Optimization}, 27(1):205--245, 2017.

\bibitem{aide}
S.~J. Reddi, J.~Kone{\v{c}}n{\`y}, P.~Richt{\'a}rik, B.~P{\'o}cz{\'o}s, and
  A.~Smola.
\newblock Aide: Fast and communication efficient distributed optimization.
\newblock {\em arXiv preprint arXiv:1608.06879}, 2016.

\bibitem{newtonmr}
F.~Roosta, Y.~Liu, P.~Xu, and M.~W. Mahoney.
\newblock Newton-mr: Newton's method without smoothness or convexity.
\newblock {\em arXiv preprint arXiv:1810.00303}, 2018.

\bibitem{dane}
O.~Shamir, N.~Srebro, and T.~Zhang.
\newblock Communication-efficient distributed optimization using an approximate
  newton-type method.
\newblock In {\em International conference on machine learning}, pages
  1000--1008, 2014.

\bibitem{stich2018sparsified}
S.~U. Stich, J.-B. Cordonnier, and M.~Jaggi.
\newblock Sparsified sgd with memory.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4447--4458, 2018.

\bibitem{su}
L.~Su and N.~H. Vaidya.
\newblock Fault-tolerant multi-agent optimization: optimal iterative
  distributed algorithms.
\newblock In {\em Proceedings of the 2016 ACM symposium on principles of
  distributed computing}, pages 425--434. ACM, 2016.

\bibitem{dme}
A.~T. Suresh, F.~X. Yu, S.~Kumar, and H.~B. McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 3329--3337. JMLR. org, 2017.

\bibitem{swarm2}
Swarm2.
\newblock Swarm user documentation.
\newblock
  \url{https://people.cs.umass.edu/~swarm/index.php?n=Main.NewSwarmDoc}, 2018.
\newblock Accessed: 2018-01-05.

\bibitem{atomo}
H.~Wang, S.~Sievert, S.~Liu, Z.~Charles, D.~Papailiopoulos, and S.~Wright.
\newblock Atomo: Communication-efficient learning via atomic sparsification.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  9850--9861, 2018.

\bibitem{wangsketched}
S.~Wang, A.~Gittens, and M.~W. Mahoney.
\newblock Sketched ridge regression: Optimization perspective, statistical
  perspective, and model averaging.
\newblock {\em The Journal of Machine Learning Research}, 18(1):8039--8088,
  2017.

\bibitem{giant}
S.~Wang, F.~Roosta, P.~Xu, and M.~W. Mahoney.
\newblock Giant: Globally improved approximate newton method for distributed
  optimization.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  2332--2342, 2018.

\bibitem{terngrad}
W.~Wen, C.~Xu, F.~Yan, C.~Wu, Y.~Wang, Y.~Chen, and H.~Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em Advances in neural information processing systems}, pages
  1509--1519, 2017.

\bibitem{dong}
D.~Yin, Y.~Chen, R.~Kannan, and P.~Bartlett.
\newblock {B}yzantine-robust distributed learning: Towards optimal statistical
  rates.
\newblock In J.~Dy and A.~Krause, editors, {\em Proceedings of the 35th
  International Conference on Machine Learning}, volume~80 of {\em Proceedings
  of Machine Learning Research}, pages 5650--5659, StockholmsmÃ€ssan,
  Stockholm Sweden, 10--15 Jul 2018. PMLR.

\bibitem{dong1}
D.~Yin, Y.~Chen, R.~Kannan, and P.~Bartlett.
\newblock Defending against saddle point attack in {B}yzantine-robust
  distributed learning.
\newblock In K.~Chaudhuri and R.~Salakhutdinov, editors, {\em Proceedings of
  the 36th International Conference on Machine Learning}, volume~97 of {\em
  Proceedings of Machine Learning Research}, pages 7074--7084, Long Beach,
  California, USA, 09--15 Jun 2019. PMLR.

\bibitem{disco}
Y.~Zhang and X.~Lin.
\newblock Disco: Distributed optimization for self-concordant empirical loss.
\newblock In {\em International conference on machine learning}, pages
  362--370, 2015.

\end{thebibliography}
