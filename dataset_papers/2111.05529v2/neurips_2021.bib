@inproceedings{schmidt2018adversarially,
  title={Adversarially robust generalization requires more data},
  author={Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and M\k{a}dry, Aleksander},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5014--5026},
  year={2018}
}

@inproceedings{fawzi2018adversarial,
  title={Adversarial vulnerability for any classifier},
  author={Fawzi, Alhussein and Fawzi, Hamza and Fawzi, Omar},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1178--1187},
  year={2018}
}

@article{mahloujifar2019empirically,
  title={Empirically Measuring Concentration: Fundamental Limits on Intrinsic Robustness},
  author={Mahloujifar, Saeed and Zhang, Xiao and Mahmoody, Mohammad and Evans, David},
  journal={arXiv:1905.12202},
  year={2019}
}

@inproceedings{ilyas2019adversarial,
  title={Adversarial examples are not bugs, they are features},
  author={Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and M\k{a}dry, Aleksander},
  booktitle={Advances in Neural Information Processing Systems},
  pages={125--136},
  year={2019}
}


@inproceedings{bubeck2018adversarial,
  title={Adversarial examples from computational constraints},
  author={Bubeck, S{\'e}bastien and Lee, Yin Tat and Price, Eric and Razenshteyn, Ilya},
  booktitle={International Conference on Machine Learning},
  pages={831--840},
  year={2019}
}


@article{gilmer2018adversarial,
  title={Adversarial spheres},
  author={Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
  journal={arXiv:1801.02774},
  year={2018}
}

@inproceedings{shafahi2018adversarial,
  title={Are adversarial examples inevitable?},
  author={Shafahi, Ali and Huang, W Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{szegedy2013intriguing,
title	= {Intriguing properties of neural networks},
author	= {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
year	= {2014},
booktitle	= {International Conference on Learning Representations}
}

@article{engstrom2017rotation,
  title={A rotation and a translation suffice: Fooling {CNN}s with simple transformations},
  author={Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and M\k{a}dry, Aleksander},
  journal={arXiv:1712.02779},
  year={2017}
}

@inproceedings{garg2018spectral,
  title={A spectral view of adversarially robust features},
  author={Garg, Shivam and Sharan, Vatsal and Zhang, Brian and Valiant, Gregory},
  booktitle={Advances in Neural Information Processing Systems},
  pages={10138--10148},
  year={2018}
}

@article{eykholt2019robust,
  title={Robust Classification using Robust Feature Augmentation},
  author={Eykholt, Kevin and Gupta, Swati and Prakash, Atul and Zheng, Haizhong},
  journal={arXiv:1905.10904},
  year={2019}
}

@inproceedings{hjelm2018learning,
  title={Learning deep representations by mutual information estimation and maximization},
  author={Hjelm, R Devon and Fedorov, Alex and Lavoie-Marchildon, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{belghazi2018mine,
  title={Mutual Information Neural Estimation},
  author={Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeshwar, Sai and Ozair, Sherjil and Bengio, Yoshua and Hjelm, R Devon and Courville, Aaron C},
  booktitle={International Conference on Learning Representations},
  year={2018},
}

@article{raghunathan2019adversarial,
  title={Adversarial Training Can Hurt Generalization},
  author={Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John C and Liang, Percy},
  journal={arXiv:1906.06032},
  year={2019}
}

@article{poole2019variational,
  title={On variational bounds of mutual information},
  author={Poole, Ben and Ozair, Sherjil and Oord, Aaron van den and Alemi, Alexander A and Tucker, George},
  journal={arXiv:1905.06922},
  year={2019}
}

@inproceedings{madry2017towards,
  title={Towards Deep Learning Models Resistant to Adversarial Attacks},
  author={M\k{a}dry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{bengio2013representation,
  title={Representation learning: A review and new perspectives},
  author={Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={35},
  number={8},
  pages={1798--1828},
  year={2013},
  publisher={IEEE}
}

@article{duchi2013distance,
  title={Distance-based and continuum {F}ano inequalities with applications to statistical estimation},
  author={Duchi, John C and Wainwright, Martin J},
  journal={arXiv:1311.2669},
  year={2013}
}

@article{alemi2016deep,
  title={Deep variational information bottleneck},
  author={Alemi, Alexander A and Fischer, Ian and Dillon, Joshua V and Murphy, Kevin},
  journal={arXiv:1612.00410},
  year={2016}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv:1312.6114},
  year={2013}
}

@inproceedings{etmann2019connection,
  title={On the Connection Between Adversarial Robustness and Saliency Map Interpretability},
  author={Etmann, Christian and Lunz, Sebastian and Maass, Peter and Schoenlieb, Carola},
  booktitle={International Conference on Machine Learning},
  pages={1823--1832},
  year={2019}
}

@article{pensia2019extracting,
  title={Extracting robust and accurate features via a robust information bottleneck},
  author={Pensia, Ankit and Jog, Varun and Loh, Po-Ling},
  journal={IEEE Journal on Selected Areas in Information Theory},
  year={2020},
  publisher={IEEE}
}

@article{brunel1998mutual,
  title={Mutual information, Fisher information, and population coding},
  author={Brunel, Nicolas and Nadal, Jean-Pierre},
  journal={Neural computation},
  volume={10},
  number={7},
  pages={1731--1757},
  year={1998},
  publisher={MIT Press}
}

@misc{robustness2019package,
   title={Robustness (Python Library)},
   author={Logan Engstrom and Andrew Ilyas and Shibani Santurkar and Dimitris Tsipras},
   year={2019},

}

@article{bell1995information,
  title={An information-maximization approach to blind separation and blind deconvolution},
  author={Bell, Anthony J and Sejnowski, Terrence J},
  journal={Neural computation},
  volume={7},
  number={6},
  pages={1129--1159},
  year={1995},
  publisher={MIT Press}
}

@article{gowal2018effectiveness,
  title={On the effectiveness of interval bound propagation for training verifiably robust models},
  author={Gowal, Sven and Dvijotham, Krishnamurthy and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
  journal={arXiv:1810.12715},
  year={2018}
}

@article{stanforth2019labels,
  title={Are labels required for improving adversarial robustness?},
  author={Stanforth, Robert and Fawzi, Alhussein and Kohli, Pushmeet and others},
  journal={arXiv:1905.13725},
  year={2019}
}

@article{kolouri2017optimal,
  title={Optimal mass transport: Signal processing and machine-learning applications},
  author={Kolouri, Soheil and Park, Se Rim and Thorpe, Matthew and Slepcev, Dejan and Rohde, Gustavo K},
  journal={IEEE Signal Processing Magazine},
  volume={34},
  number={4},
  pages={43--59},
  year={2017},
  publisher={IEEE}
}

@article{champion2008wasserstein,
  title={The $\infty$-Wasserstein Distance: Local Solutions and Existence of Optimal Transport Maps},
  author={Champion, Thierry and De Pascale, Luigi and Juutinen, Petri},
  journal={SIAM Journal on Mathematical Analysis},
  volume={40},
  number={1},
  pages={1-20},
  year={2008},
  publisher={SIAM}
}

@article{fano1961transmission,
  title={Transmission of information: A statistical theory of communications},
  author={Fano, Robert M},
  journal={American Journal of Physics},
  volume={29},
  number={11},
  pages={793--794},
  year={1961},
  publisher={American Association of Physics Teachers}
}

@book{cover2012elements,
  title={Elements of Information Theory},
  author={Cover, Thomas M and Thomas, Joy A},
  year={2012},
  publisher={John Wiley \& Sons}
}

@article{donsker1983asymptotic,
  title={Asymptotic evaluation of certain Markov process expectations for large time, {IV}},
  author={Donsker, Monroe D and Varadhan, SR Srinivasa},
  journal={Communications on Pure and Applied Mathematics},
  volume={36},
  number={2},
  pages={183--212},
  year={1983},
  publisher={Wiley Online Library}
}

@inproceedings{suzuki2008approximating,
  title={Approximating mutual information by maximum likelihood density ratio estimation},
  author={Suzuki, Taiji and Sugiyama, Masashi and Sese, Jun and Kanamori, Takafumi},
  booktitle={New challenges for feature selection in data mining and knowledge discovery},
  pages={5--20},
  year={2008}
}

@article{darbellay1999estimation,
  title={Estimation of the information by an adaptive partitioning of the observation space},
  author={Darbellay, Georges A and Vajda, Igor},
  journal={IEEE Transactions on Information Theory},
  volume={45},
  number={4},
  pages={1315--1321},
  year={1999},
  publisher={IEEE}
}

@article{moon1995estimation,
  title={Estimation of mutual information using kernel density estimators},
  author={Moon, Young-Il and Rajagopalan, Balaji and Lall, Upmanu},
  journal={Physical Review E},
  volume={52},
  number={3},
  pages={2318},
  year={1995},
  publisher={APS}
}

@inproceedings{moon2017ensemble,
  title={Ensemble estimation of mutual information},
  author={Moon, Kevin R and Sricharan, Kumar and Hero, Alfred O},
  booktitle={IEEE International Symposium on Information Theory},
  pages={3030--3034},
  year={2017},
  organization={IEEE}
}

@inproceedings{kandasamy2015nonparametric,
  title={Nonparametric von mises estimators for entropies, divergences and mutual informations},
  author={Kandasamy, Kirthevasan and Krishnamurthy, Akshay and Poczos, Barnabas and Wasserman, Larry and others},
  booktitle={Advances in Neural Information Processing Systems},
  pages={397--405},
  year={2015}
}

@article{linsker1989generate,
  title={How to generate ordered maps by maximizing the mutual information between input and output signals},
  author={Linsker, Ralph},
  journal={Neural Computation},
  volume={1},
  number={3},
  pages={402--411},
  year={1989},
  publisher={MIT Press}
}

@article{engstrom2019learning,
  title={Adversarial robustness as a prior for learned representations},
  author={Engstrom, Logan and Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Tran, Brandon and M\k{a}dry, Aleksander},
  journal={arXiv:1906.00945},
  year={2019}
}

@inproceedings{devlin2018bert,
  title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  booktitle={North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={4171--4186},
  year={2019}
}

@inproceedings{sharif2016accessorize,
  title={Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition},
  author={Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K},
  booktitle={ACM Conference on Computer and Communications Security},
  pages={1528--1540},
  year={2016}
}

@inproceedings{eykholt2018robust,
  title={Robust physical-world attacks on deep learning visual classification},
  author={Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1625--1634},
  year={2018}
}

@inproceedings{zhang2019theoretically,
  title={Theoretically Principled Trade-off between Robustness and Accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael I},
  booktitle={International Conference on Learning Representations},
  year={2019},
}

@inproceedings{shafahi2019adversarial,
  title={Adversarial training for free!},
  author={Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
  booktitle={Advances in Neural Information Processing Systems},
  pages={3353--3364},
  year={2019}
}

@article{krizhevsky2009learning,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  journal={Master's thesis, Department of Computer Science, University of Toronto},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{mahloujifar2019curse,
  title={The curse of concentration in robust learning: Evasion and poisoning attacks from concentration of measure},
  author={Mahloujifar, Saeed and Diochnos, Dimitrios I and Mahmoody, Mohammad},
  booktitle={AAAI Conference on Artificial Intelligence},
  pages={4536--4543},
  year={2019}
}


@article{yin2018rademacher,
  title={Rademacher complexity for adversarially robust generalization},
  author={Yin, Dong and Ramchandran, Kannan and Bartlett, Peter},
  journal={arXiv:1810.11914},
  year={2018}
}

@article{nakkiran2019adversarial,
  title={Adversarial robustness may be at odds with simplicity},
  author={Nakkiran, Preetum},
  journal={arXiv:1901.00532},
  year={2019}
}

@inproceedings{
tsipras2018robustness,
title={Robustness May Be at Odds with Accuracy},
author={Dimitris Tsipras and Shibani Santurkar and Logan Engstrom and Alexander Turner and Aleksander M\k{a}dry},
booktitle={International Conference on Learning Representations},
year={2019},
}

@article{zhang2019interpreting,
  title={Interpreting and Improving Adversarial Robustness with Neuron Sensitivity},
  author={Zhang, Chongzhi and Liu, Aishan and Liu, Xianglong and Xu, Yitao and Yu, Hang and Ma, Yuqing and Li, Tianlin},
  journal={arXiv:1909.06978},
  year={2019}
}

@article{lecun-mnisthandwrittendigit-2010,
  added-at = {2010-06-28T21:16:30.000+0200},
  author = {LeCun, Yann and Cortes, Corinna},
  groups = {public},
  howpublished = {http://yann.lecun.com/exdb/mnist/},
  interhash = {21b9d0558bd66279df9452562df6e6f3},
  intrahash = {935bad99fa1f65e03c25b315aa3c1032},
  keywords = {MSc _checked character_recognition mnist network neural},
  lastchecked = {2016-01-14 14:24:11},
  timestamp = {2016-07-12T19:25:30.000+0200},
  title = {{MNIST} handwritten digit database},
  username = {mhwombat},
  year = 2010
}

@inproceedings{sinha2018certifying,
  title={Certifying Some Distributional Robustness with Principled Adversarial Training},
  author={Sinha, Aman and Namkoong, Hongseok and Duchi, John},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@InProceedings{Simonyan15,
  author       = "Karen Simonyan and Andrew Zisserman",
  title        = "Very Deep Convolutional Networks for Large-Scale Image Recognition",
  booktitle    = "International Conference on Learning Representations",
  year         = "2015",
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={770--778},
  year={2016}
}


@inproceedings{huang2017densely,
  title={Densely connected convolutional networks},
  author={Huang, Gao and Liu, Zhuang and Van Der Maaten, Laurens and Weinberger, Kilian Q},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition},
  pages={4700--4708},
  year={2017}
}

@article{scarlett2019introductory,
  title={An Introductory Guide to {F}ano's Inequality with Applications in Statistical Estimation},
  author={Scarlett, Jonathan and Cevher, Volkan},
  journal={arXiv:1901.00555},
  year={2019}
}

@inproceedings{netzer2011reading,
  title={Reading digits in natural images with unsupervised feature learning},
  author={Netzer, Yuval and Wang, Tao and Coates, Adam and Bissacco, Alessandro and Wu, Bo and Ng, Andrew Y},
  booktitle={NeurIPS Workshop on Deep Learning and Unsupervised Feature Learning},
  year={2011}
}

@article{fashion-mnist,
  author    = {Han Xiao and
               Kashif Rasul and
               Roland Vollgraf},
  title     = {Fashion-{MNIST}: a Novel Image Dataset for Benchmarking Machine Learning
               Algorithms},
  journal   = {arXiv:1708.07747},
  year      = {2017},
}


@article{zhu_learning_2020,
	title = {Learning {Adversarially} {Robust} {Representations} via {Worst}-{Case} {Mutual} {Information} {Maximization}},
	
	abstract = {Training machine learning models that are robust against adversarial inputs poses seemingly insurmountable challenges. To better understand adversarial robustness, we consider the underlying problem of learning robust representations. We develop a notion of representation vulnerability that captures the maximum change of mutual information between the input and output distributions, under the worst-case input perturbation. Then, we prove a theorem that establishes a lower bound on the minimum adversarial risk that can be achieved for any downstream classifier based on its representation vulnerability. We propose an unsupervised learning method for obtaining intrinsically robust representations by maximizing the worst-case mutual information between the input and output distributions. Experiments on downstream classification tasks support the robustness of the representations found using unsupervised learning with our training principle.},
	urldate = {2020-08-11},
	journal = {arXiv:2002.11798 [cs, math, stat]},
	author = {Zhu, Sicheng and Zhang, Xiao and Evans, David},
	month = jul,
	year = {2020},
	note = {arXiv: 2002.11798},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Information Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\MXW4EW25\\Zhu et al. - 2020 - Learning Adversarially Robust Representations via .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\R4KDB86X\\2002.html:text/html}
}

@inproceedings{arora_stronger_2018,
	title = {Stronger {Generalization} {Bounds} for {Deep} {Nets} via a {Compression} {Approach}},
	
	abstract = {Deep nets generalize well despite having more parameters than the number of training samples. Recent works try to give an explanation using PAC-Bayes and Margin-based analyses, but do not as yet re...},
	language = {en},
	urldate = {2020-08-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Arora, Sanjeev and Ge, Rong and Neyshabur, Behnam and Zhang, Yi},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498
Section: Machine Learning},
	pages = {254--263},
	file = {Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\2GKTC8QC\\Arora et al. - 2018 - Stronger Generalization Bounds for Deep Nets via a.pdf:application/pdf;Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\Q93W4K2Z\\arora18b.html:text/html}
}

@incollection{bartlett_spectrally-normalized_2017,
	title = {Spectrally-normalized margin bounds for neural networks},
	
	urldate = {2020-08-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Bartlett, Peter L and Foster, Dylan J and Telgarsky, Matus J},
	
	year = {2017},
	pages = {6240--6249},
	file = {NIPS Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\6YWSBUKT\\Bartlett et al. - 2017 - Spectrally-normalized margin bounds for neural net.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\GICAWH3N\\7204-spectrally-normalized-margin-bounds-for-neural-networks.html:text/html}
}

@article{allen-zhu_convergence_2019,
	title = {A {Convergence} {Theory} for {Deep} {Learning} via {Over}-{Parameterization}},
	
	abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works has been focusing on training neural networks with one hidden layer. The theory of multi-layer networks remains largely unsettled. In this work, we prove why stochastic gradient descent (SGD) can find \${\textbackslash}textit\{global minima\}\$ on the training objective of DNNs in \${\textbackslash}textit\{polynomial time\}\$. We only make two assumptions: the inputs are non-degenerate and the network is over-parameterized. The latter means the network width is sufficiently large: \${\textbackslash}textit\{polynomial\}\$ in \$L\$, the number of layers and in \$n\$, the number of samples. Our key technique is to derive that, in a sufficiently large neighborhood of the random initialization, the optimization landscape is almost-convex and semi-smooth even with ReLU activations. This implies an equivalence between over-parameterized neural networks and neural tangent kernel (NTK) in the finite (and polynomial) width setting. As concrete examples, starting from randomly initialized weights, we prove that SGD can attain 100\% training accuracy in classification tasks, or minimize regression loss in linear convergence speed, with running time polynomial in \$n,L\$. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
	urldate = {2020-08-11},
	journal = {arXiv:1811.03962 [cs, math, stat]},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
	month = jun,
	year = {2019},
	note = {arXiv: 1811.03962},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Data Structures and Algorithms, Mathematics - Optimization and Control},
	annote = {Comment: V2 adds citation and V3/V4/V5 polish writing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\59XRAHPV\\Allen-Zhu et al. - 2019 - A Convergence Theory for Deep Learning via Over-Pa.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\4CAQYRPX\\1811.html:text/html}
}

@incollection{allen-zhu_learning_2019,
	title = {Learning and {Generalization} in {Overparameterized} {Neural} {Networks}, {Going} {Beyond} {Two} {Layers}},
	
	urldate = {2020-08-11},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
	
	year = {2019},
	pages = {6158--6169},
	file = {NIPS Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\JJF42C4I\\Allen-Zhu et al. - 2019 - Learning and Generalization in Overparameterized N.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\IZB4P8NE\\8847-learning-and-generalization-in-overparameterized-neural-networks-going-beyond-two-layers.html:text/html}
}

@article{liu_algorithm-dependent_2017,
	title = {Algorithm-{Dependent} {Generalization} {Bounds} for {Multi}-{Task} {Learning}},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2544314},
	abstract = {Often, tasks are collected for multi-task learning (MTL) because they share similar feature structures. Based on this observation, in this paper, we present novel algorithm-dependent generalization bounds for MTL by exploiting the notion of algorithmic stability. We focus on the performance of one particular task and the average performance over multiple tasks by analyzing the generalization ability of a common parameter that is shared in MTL. When focusing on one particular task, with the help of a mild assumption on the feature structures, we interpret the function of the other tasks as a regularizer that produces a specific inductive bias. The algorithm for learning the common parameter, as well as the predictor, is thereby uniformly stable with respect to the domain of the particular task and has a generalization bound with a fast convergence rate of order O(1/n), where n is the sample size of the particular task. When focusing on the average performance over multiple tasks, we prove that a similar inductive bias exists under certain conditions on the feature structures. Thus, the corresponding algorithm for learning the common parameter is also uniformly stable with respect to the domains of the multiple tasks, and its generalization bound is of the order O(1/T), where T is the number of tasks. These theoretical analyses naturally show that the similarity of feature structures in MTL will lead to specific regularizations for predicting, which enables the learning algorithms to generalize fast and correctly from a few examples.},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liu, Tongliang and Tao, Dacheng and Song, Mingli and Maybank, Stephen J.},
	month = feb,
	year = {2017},
	note = {Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {algorithm dependent generalization bounds, Algorithm design and analysis, algorithmic stability, Complexity theory, computational complexity, convergence, Convergence, Electronic mail, fast convergence rate, feature structures, generalisation (artificial intelligence), generalization, inductive bias, learning (artificial intelligence), learning theory, learning to learn, MTL, Multi-task learning, multitask learning, order O(1/n), order O(1/T), Prediction algorithms, regularization, stability, Stability analysis, Training},
	pages = {227--241},
	file = {IEEE Xplore Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\PKMK37YK\\Liu et al. - 2017 - Algorithm-Dependent Generalization Bounds for Mult.pdf:application/pdf;IEEE Xplore Abstract Record:C\:\\Users\\Zhu\\Zotero\\storage\\U2IZSQA7\\7437460.html:text/html}
}

@article{maurer_benefit_2016,
	title = {The {Benefit} of {Multitask} {Representation} {Learning}},
	volume = {17},
	issn = {1532-4435},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Maurer, Andreas and Pontil, Massimiliano and Romera-Paredes, Bernardino},
	month = jan,
	year = {2016},
	note = {Publisher: JMLR.org},
	keywords = {multitask learning, learning-to-learn, representation learning, statistical learning theory, transfer learning},
	pages = {2853--2884},
	file = {Maurer et al. - The Beneﬁt of Multitask Representation Learning.pdf:C\:\\Users\\Zhu\\Zotero\\storage\\3LP3ECHQ\\Maurer et al. - The Beneﬁt of Multitask Representation Learning.pdf:application/pdf}
}

@inproceedings{saunshi_theoretical_2019,
	title = {A {Theoretical} {Analysis} of {Contrastive} {Unsupervised} {Representation} {Learning}},
	
	abstract = {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of...},
	language = {en},
	urldate = {2020-08-11},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Saunshi, Nikunj and Plevrakis, Orestis and Arora, Sanjeev and Khodak, Mikhail and Khandeparkar, Hrishikesh},
	month = may,
	year = {2019},
	note = {ISSN: 2640-3498
Section: Machine Learning},
	pages = {5628--5637},
	file = {Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\MCYKGB8C\\Saunshi et al. - 2019 - A Theoretical Analysis of Contrastive Unsupervised.pdf:application/pdf;arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\92JLFTW8\\Arora et al. - 2019 - A Theoretical Analysis of Contrastive Unsupervised.pdf:application/pdf;Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\GN3NKB9I\\saunshi19a.html:text/html}
}

@incollection{le_supervised_2018,
	title = {Supervised autoencoders: {Improving} generalization performance with unsupervised regularizers},
	shorttitle = {Supervised autoencoders},
	urldate = {2020-08-20},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Le, Lei and Patterson, Andrew and White, Martha},
	
	year = {2018},
	pages = {107--117},
	file = {NIPS Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\UGY2TDC9\\Le et al. - 2018 - Supervised autoencoders Improving generalization .pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\NEURAW87\\7296-supervised-autoencoders-improving-generalization-performance-with-unsupervised-regularizer.html:text/html}
}

@inproceedings{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	year = {2019}
}

@article{zhang_understanding_2017,
	title = {Understanding deep learning requires rethinking generalization},
	abstract = {Despite their massive size, successful deep artiﬁcial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
	language = {en},
	author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
	year = {2017},
	pages = {15},
	file = {Zhang et al. - 2017 - UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING .pdf:C\:\\Users\\Zhu\\Zotero\\storage\\XQANLXHG\\Zhang et al. - 2017 - UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING .pdf:application/pdf}
}

@inproceedings{liu_modeling_2019,
	title = {Modeling {Parts}, {Structure}, and {System} {Dynamics} via {Predictive} {Learning}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Liu, Zhijian and Wu, Jiajun and Xu, Zhenjia and Sun, Chen and Murphy, Kevin and Freeman, William T. and Tenenbaum, Joshua B.},
	year = {2019}
}

@inproceedings{kipf_contrastive_2020,
	title = {Contrastive {Learning} of {Structured} {World} {Models}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Kipf, Thomas and Pol, Elise van der and Welling, Max},
	year = {2020}
}

@inproceedings{zhang_theoretically_2019,
	address = {Long Beach, California, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Theoretically {Principled} {Trade}-off between {Robustness} and {Accuracy}},
	volume = {97},
	
	abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of \&nbsp;2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean L\_2 perturbation distance.},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	
	month = jun,
	year = {2019},
	pages = {7472--7482}
}

@article{garg_functional_2020,
	title = {Functional {Regularization} for {Representation} {Learning}: {A} {Unified} {Theoretical} {Perspective}},
	journal = {arXiv preprint arXiv:2008.02447},
	author = {Garg, Siddhant and Liang, Yingyu},
	year = {2020}
}

@incollection{schmidt_adversarially_2018,
	title = {Adversarially {Robust} {Generalization} {Requires} {More} {Data}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Schmidt, Ludwig and Santurkar, Shibani and Tsipras, Dimitris and Talwar, Kunal and Madry, Aleksander},
	
	year = {2018},
	pages = {5014--5026}
}

@article{taori_measuring_2020,
	title = {Measuring {Robustness} to {Natural} {Distribution} {Shifts} in {Image} {Classification}},
	journal = {arXiv preprint arXiv:2007.00644},
	author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
	year = {2020}
}

@inproceedings{ding_sensitivity_2019,
	title = {On the {Sensitivity} of {Adversarial} {Robustness} to {Input} {Data} {Distributions}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Ding, Gavin Weiguang and Lui, Kry Yik-Chau and Jin, Xiaomeng and Wang, Luyu and Huang, Ruitong},
	year = {2019}
}


@article{yang_closer_2020,
	title = {A {Closer} {Look} at {Accuracy} vs. {Robustness}},
	
	abstract = {Current methods for training robust networks lead to a drop in test accuracy, which has led prior works to posit that a robustness-accuracy tradeoff may be inevitable in deep learning. We take a closer look at this phenomenon and first show that real image datasets are actually separated. With this property in mind, we then prove that robustness and accuracy should both be achievable for benchmark datasets through locally Lipschitz functions, and hence, there should be no inherent tradeoff between robustness and accuracy. Through extensive experiments with robustness methods, we argue that the gap between theory and practice arises from two limitations of current methods: either they fail to impose local Lipschitzness or they are insufficiently generalized. We explore combining dropout with robust training methods and obtain better generalization. We conclude that achieving robustness and accuracy in practice may require using methods that impose local Lipschitzness and augmenting them with deep learning generalization techniques. Code available at https://github.com/yangarbiter/robust-local-lipschitz},
	urldate = {2020-09-01},
	journal = {arXiv:2003.02460 [cs, stat]},
	author = {Yang, Yao-Yuan and Rashtchian, Cyrus and Zhang, Hongyang and Salakhutdinov, Ruslan and Chaudhuri, Kamalika},
	month = jul,
	year = {2020},
	note = {arXiv: 2003.02460},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\EF9TMPUJ\\Yang et al. - 2020 - A Closer Look at Accuracy vs. Robustness.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\L2TXNRND\\2003.html:text/html;Full Text:C\:\\Users\\Zhu\\Zotero\\storage\\DKQMNS5B\\Yang et al. - 2020 - A Closer Look at Accuracy vs. Robustness.pdf:application/pdf}
}

@article{koh_concept_2020,
	title = {Concept {Bottleneck} {Models}},
	
	abstract = {We seek to learn models that we can interact with using high-level concepts: if the model did not think there was a bone spur in the x-ray, would it still predict severe arthritis? State-of-the-art models today do not typically support the manipulation of concepts like "the existence of bone spurs", as they are trained end-to-end to go directly from raw input (e.g., pixels) to output (e.g., arthritis severity). We revisit the classic idea of first predicting concepts that are provided at training time, and then using these concepts to predict the label. By construction, we can intervene on these concept bottleneck models by editing their predicted concept values and propagating these changes to the final prediction. On x-ray grading and bird identification, concept bottleneck models achieve competitive accuracy with standard end-to-end models, while enabling interpretation in terms of high-level clinical concepts ("bone spurs") or bird attributes ("wing color"). These models also allow for richer human-model interaction: accuracy improves significantly if we can correct model mistakes on concepts at test time.},
	urldate = {2020-09-01},
	journal = {arXiv:2007.04612 [cs, stat]},
	author = {Koh, Pang Wei and Nguyen, Thao and Tang, Yew Siang and Mussmann, Stephen and Pierson, Emma and Kim, Been and Liang, Percy},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.04612},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\I94VLRW9\\Koh et al. - 2020 - Concept Bottleneck Models.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\XKCBQ4CA\\2007.html:text/html}
}

@article{wei_improved_2020,
	title = {Improved {Sample} {Complexities} for {Deep} {Networks} and {Robust} {Classification} via an {All}-{Layer} {Margin}},
	
	abstract = {For linear classifiers, the relationship between (normalized) output margin and generalization is captured in a clear and simple bound -- a large output margin implies good generalization. Unfortunately, for deep models, this relationship is less clear: existing analyses of the output margin give complicated bounds which sometimes depend exponentially on depth. In this work, we propose to instead analyze a new notion of margin, which we call the "all-layer margin." Our analysis reveals that the all-layer margin has a clear and direct relationship with generalization for deep models. This enables the following concrete applications of the all-layer margin: 1) by analyzing the all-layer margin, we obtain tighter generalization bounds for neural nets which depend on Jacobian and hidden layer norms and remove the exponential dependency on depth 2) our neural net results easily translate to the adversarially robust setting, giving the first direct analysis of robust test error for deep networks, and 3) we present a theoretically inspired training algorithm for increasing the all-layer margin. Our algorithm improves both clean and adversarially robust test performance over strong baselines in practice.},
	urldate = {2020-09-02},
	journal = {arXiv:1910.04284 [cs, stat]},
	author = {Wei, Colin and Ma, Tengyu},
	month = apr,
	year = {2020},
	note = {arXiv: 1910.04284},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Code for all-layer margin optimization is available at the following link: https://github.com/cwein3/all-layer-margin-opt},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\ZLJB94GQ\\Wei and Ma - 2020 - Improved Sample Complexities for Deep Networks and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\5PMIG949\\1910.html:text/html}
}

@incollection{han_visual_2019,
	title = {Visual {Concept}-{Metaconcept} {Learning}},
	
	urldate = {2020-09-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Han, Chi and Mao, Jiayuan and Gan, Chuang and Tenenbaum, Josh and Wu, Jiajun},
	
	year = {2019},
	pages = {5001--5012},
	file = {NIPS Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\JWTB73Y7\\Han et al. - 2019 - Visual Concept-Metaconcept Learning.pdf:application/pdf;NIPS Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\VHUABCGP\\8745-visual-concept-metaconcept-learning.html:text/html}
}

@article{awasthi_adversarial_2020,
	title = {Adversarial {Learning} {Guarantees} for {Linear} {Hypotheses} and {Neural} {Networks}},
	
	abstract = {Adversarial or test time robustness measures the susceptibility of a classifier to perturbations to the test input. While there has been a flurry of recent work on designing defenses against such perturbations, the theory of adversarial robustness is not well understood. In order to make progress on this, we focus on the problem of understanding generalization in adversarial settings, via the lens of Rademacher complexity. We give upper and lower bounds for the adversarial empirical Rademacher complexity of linear hypotheses with adversarial perturbations measured in \$l\_r\$-norm for an arbitrary \$r {\textbackslash}geq 1\$. This generalizes the recent result of [Yin et al.'19] that studies the case of \$r = {\textbackslash}infty\$, and provides a finer analysis of the dependence on the input dimensionality as compared to the recent work of [Khim and Loh'19] on linear hypothesis classes. We then extend our analysis to provide Rademacher complexity lower and upper bounds for a single ReLU unit. Finally, we give adversarial Rademacher complexity bounds for feed-forward neural networks with one hidden layer. Unlike previous works we directly provide bounds on the adversarial Rademacher complexity of the given network, as opposed to a bound on a surrogate. A by-product of our analysis also leads to tighter bounds for the Rademacher complexity of linear hypotheses, for which we give a detailed analysis and present a comparison with existing bounds.},
	urldate = {2020-09-02},
	journal = {arXiv:2004.13617 [cs, stat]},
	author = {Awasthi, Pranjal and Frank, Natalie and Mohri, Mehryar},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.13617},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\MW56PK45\\2004.html:text/html}
}

@article{raghunathan_adversarial_2019,
	title = {Adversarial {Training} {Can} {Hurt} {Generalization}},
	
	abstract = {While adversarial training can improve robust accuracy (against an adversary), it sometimes hurts standard accuracy (when there is no adversary). Previous work has studied this tradeoff between standard and robust accuracy, but only in the setting where no predictor performs well on both objectives in the infinite data limit. In this paper, we show that even when the optimal predictor with infinite data performs well on both objectives, a tradeoff can still manifest itself with finite data. Furthermore, since our construction is based on a convex learning problem, we rule out optimization concerns, thus laying bare a fundamental tension between robustness and generalization. Finally, we show that robust self-training mostly eliminates this tradeoff by leveraging unlabeled data.},
	urldate = {2020-09-02},
	journal = {arXiv:1906.06032 [cs, stat]},
	author = {Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John C. and Liang, Percy},
	month = aug,
	year = {2019},
	note = {arXiv: 1906.06032},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\5VTC494Y\\Raghunathan et al. - 2019 - Adversarial Training Can Hurt Generalization.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\AAN39THC\\1906.html:text/html}
}

@article{szegedy_intriguing_2014,
	title = {Intriguing properties of neural networks},
	
	abstract = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains of the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extend. We can cause the network to misclassify an image by applying a certain imperceptible perturbation, which is found by maximizing the network's prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
	urldate = {2020-09-03},
	journal = {arXiv:1312.6199 [cs]},
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	month = feb,
	year = {2014},
	note = {arXiv: 1312.6199},
	keywords = {Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\RF6355MC\\Szegedy et al. - 2014 - Intriguing properties of neural networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\YN5DUTT3\\1312.html:text/html}
}

@article{wang_generalizing_2020,
	title = {Generalizing from a {Few} {Examples}: {A} {Survey} on {Few}-{Shot} {Learning}},
	shorttitle = {Generalizing from a {Few} {Examples}},
	
	abstract = {Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.},
	urldate = {2020-09-03},
	journal = {arXiv:1904.05046 [cs]},
	author = {Wang, Yaqing and Yao, Quanming and Kwok, James and Ni, Lionel M.},
	month = mar,
	year = {2020},
	note = {arXiv: 1904.05046},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\XQD9W8EU\\Wang et al. - 2020 - Generalizing from a Few Examples A Survey on Few-.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\B35T8T55\\1904.html:text/html}
}

@inproceedings{shafahi_are_2019,
	title = {Are adversarial examples inevitable?},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Shafahi, Ali and Huang, W. Ronny and Studer, Christoph and Feizi, Soheil and Goldstein, Tom},
	year = {2019}
}

@incollection{fawzi_adversarial_2018,
	title = {Adversarial vulnerability for any classifier},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Fawzi, Alhussein and Fawzi, Hamza and Fawzi, Omar},
	
	year = {2018},
	pages = {1178--1187}
}

@inproceedings{bubeck_adversarial_2019,
	address = {Long Beach, California, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Adversarial examples from computational constraints},
	volume = {97},
	
	abstract = {Why are classifiers in high dimension vulnerable to “adversarial” perturbations? We show that it is likely not due to information theoretic limitations, but rather it could be due to computational constraints. First we prove that, for a broad set of classification tasks, the mere existence of a robust classifier implies that it can be found by a possibly exponential-time algorithm with relatively few training examples. Then we give two particular classification tasks where learning a robust classifier is computationally intractable. More precisely we construct two binary classifications task in high dimensional space which are (i) information theoretically easy to learn robustly for large perturbations, (ii) efficiently learnable (non-robustly) by a simple linear separator, (iii) yet are not efficiently robustly learnable, even for small perturbations. Specifically, for the first task hardness holds for any efficient algorithm in the statistical query (SQ) model, while for the second task we rule out any efficient algorithm under a cryptographic assumption. These examples give an exponential separation between classical learning and robust learning in the statistical query model or under a cryptographic assumption. It suggests that adversarial examples may be an unavoidable byproduct of computational limitations of learning algorithms.},
	publisher = {PMLR},
	author = {Bubeck, Sebastien and Lee, Yin Tat and Price, Eric and Razenshteyn, Ilya},
	
	month = jun,
	year = {2019},
	pages = {831--840}
}

@article{gilmer_adversarial_2018,
	title = {Adversarial {Spheres}},
	
	abstract = {State of the art computer vision models have been shown to be vulnerable to small adversarial perturbations of the input. In other words, most images in the data distribution are both correctly classified by the model and are very close to a visually similar misclassified image. Despite substantial research interest, the cause of the phenomenon is still poorly understood and remains unsolved. We hypothesize that this counter intuitive behavior is a naturally occurring result of the high dimensional geometry of the data manifold. As a first step towards exploring this hypothesis, we study a simple synthetic dataset of classifying between two concentric high dimensional spheres. For this dataset we show a fundamental tradeoff between the amount of test error and the average distance to nearest error. In particular, we prove that any model which misclassifies a small constant fraction of a sphere will be vulnerable to adversarial perturbations of size \$O(1/{\textbackslash}sqrt\{d\})\$. Surprisingly, when we train several different architectures on this dataset, all of their error sets naturally approach this theoretical bound. As a result of the theory, the vulnerability of neural networks to small adversarial perturbations is a logical consequence of the amount of test error observed. We hope that our theoretical analysis of this very simple case will point the way forward to explore how the geometry of complex real-world data sets leads to adversarial examples.},
	urldate = {2020-09-03},
	journal = {arXiv:1801.02774 [cs]},
	author = {Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S. and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
	month = sep,
	year = {2018},
	note = {arXiv: 1801.02774},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, 68T45, I.2.6},
	file = {arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\8VFN2VDE\\1801.html:text/html}
}

@article{allen-zhu_feature_2020,
	title = {Feature {Purification}: {How} {Adversarial} {Training} {Performs} {Robust} {Deep} {Learning}},
	shorttitle = {Feature {Purification}},
	
	abstract = {Despite the great empirical success of adversarial training to defend deep learning models against adversarial perturbations, so far, it still remains rather unclear what the principles are behind the existence of adversarial perturbations, and what adversarial training does to the neural network to remove them. In this paper, we present a principle that we call "feature purification", where we show the existence of adversarial examples are due to the accumulation of certain "dense mixtures" in the hidden weights during the training process of a neural network; and more importantly, one of the goals of adversarial training is to remove such mixtures to "purify" hidden weights. We present both experiments on the CIFAR-10 dataset to illustrate this principle, and a Theoretical Result proving that for certain natural classification tasks, training a two-layer neural network with ReLU activation using randomly initialized gradient descent indeed satisfies this principle. Technically, we give, to the best of our knowledge, the first result proving that the following two can hold simultaneously for training a neural network with ReLU activation. (1) Training over the original data is indeed non-robust to small adversarial perturbations of some radius. (2) Adversarial training, even with an empirical perturbation algorithm such as FGM, can in fact be provably robust against ANY perturbations of the same radius. Finally, we also prove a complexity lower bound, showing that low complexity models such as linear classifiers, low-degree polynomials, or even the neural tangent kernel for this network, CANNOT defend against perturbations of this same radius, no matter what algorithms are used to train them.},
	urldate = {2020-09-07},
	journal = {arXiv:2005.10190 [cs, math, stat]},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	month = may,
	year = {2020},
	note = {arXiv: 2005.10190},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\LIE9NPZE\\Allen-Zhu and Li - 2020 - Feature Purification How Adversarial Training Per.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\84HQGG5T\\2005.html:text/html}
}

@article{liu_toward_2020,
	title = {Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
	shorttitle = {Toward a theory of optimization for over-parameterized systems of non-linear equations},
	
	abstract = {The success of deep learning is due, to a great extent, to the remarkable effectiveness of gradient-based optimization methods applied to large neural networks. In this work we isolate some general mathematical structures allowing for efficient optimization in over-parameterized systems of non-linear equations, a setting that includes deep neural networks. In particular, we show that optimization problems corresponding to such systems are not convex, even locally, but instead satisfy the Polyak-Lojasiewicz (PL) condition allowing for efficient optimization by gradient descent or SGD. We connect the PL condition of these systems to the condition number associated to the tangent kernel and develop a non-linear theory parallel to classical analyses of over-parameterized linear equations. We discuss how these ideas apply to training shallow and deep neural networks. Finally, we point out that tangent kernels associated to certain large system may be far from constant, even locally. Yet, our analysis still allows to demonstrate existence of solutions and convergence of gradient descent and SGD.},
	urldate = {2020-09-08},
	journal = {arXiv:2003.00307 [cs, math, stat]},
	author = {Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
	month = feb,
	year = {2020},
	note = {arXiv: 2003.00307},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\EK9ICBTR\\Liu et al. - 2020 - Toward a theory of optimization for over-parameter.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\A9E96SK2\\2003.html:text/html}
}

@inproceedings{eykholt_robust_2018,
	title = {Robust physical-world attacks on deep learning visual classification},
	booktitle = {Proceedings of the {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Eykholt, Kevin and Evtimov, Ivan and Fernandes, Earlence and Li, Bo and Rahmati, Amir and Xiao, Chaowei and Prakash, Atul and Kohno, Tadayoshi and Song, Dawn},
	year = {2018},
	pages = {1625--1634}
}

@inproceedings{sharif_accessorize_2016,
	title = {Accessorize to a crime: {Real} and stealthy attacks on state-of-the-art face recognition},
	booktitle = {Proceedings of the 2016 acm sigsac conference on computer and communications security},
	author = {Sharif, Mahmood and Bhagavatula, Sruti and Bauer, Lujo and Reiter, Michael K},
	year = {2016},
	pages = {1528--1540}
}

@article{kurakin_adversarial_2017,
	title = {Adversarial {Machine} {Learning} at {Scale}},
	abstract = {Adversarial examples are malicious inputs designed to fool machine learning models.
They often transfer from one model to another, allowing attackers to mount black
box attacks without knowledge of...},
	urldate = {2020-09-09},
	journal = {International Conference on Learning Representations},
	author = {Kurakin, Alexey and Goodfellow, Ian J. and Bengio, Samy},
	year = {2017},
	file = {Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\M7VV8I6R\\Kurakin et al. - 2016 - Adversarial Machine Learning at Scale.pdf:application/pdf;Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\WDVUIN8Y\\forum.html:text/html}
}

@incollection{shafahi_poison_2018,
	title = {Poison {Frogs}! {Targeted} {Clean}-{Label} {Poisoning} {Attacks} on {Neural} {Networks}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Shafahi, Ali and Huang, W. Ronny and Najibi, Mahyar and Suciu, Octavian and Studer, Christoph and Dumitras, Tudor and Goldstein, Tom},
	
	year = {2018},
	pages = {6103--6113}
}

@inproceedings{madry_towards_2018,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = {2018}
}

@inproceedings{hosseini_semantic_2018,
	address = {Salt Lake City, UT, USA},
	title = {Semantic {Adversarial} {Examples}},
	isbn = {978-1-5386-6100-0},
	
	doi = {10.1109/CVPRW.2018.00212},
	language = {en},
	urldate = {2020-09-09},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Hosseini, Hossein and Poovendran, Radha},
	month = jun,
	year = {2018},
	pages = {1695--16955},
	file = {Hosseini and Poovendran - 2018 - Semantic Adversarial Examples.pdf:C\:\\Users\\Zhu\\Zotero\\storage\\ELUW2U9Y\\Hosseini and Poovendran - 2018 - Semantic Adversarial Examples.pdf:application/pdf}
}

@book{engstrom_rotation_2019,
	title = {A {Rotation} and a {Translation} {Suffice}: {Fooling} {CNNs} with {Simple} {Transformations}},
	
	author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
	year = {2019}
}

@inproceedings{bhattad_unrestricted_2020,
	title = {Unrestricted {Adversarial} {Examples} via {Semantic} {Manipulation}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Bhattad, Anand and Chong, Min Jin and Liang, Kaizhao and Li, Bo and Forsyth, D. A.},
	year = {2020}
}

@incollection{laidlaw_functional_2019,
	title = {Functional {Adversarial} {Attacks}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Laidlaw, Cassidy and Feizi, Soheil},
	
	year = {2019},
	pages = {10408--10418}
}

@article{dobriban_provable_2020,
	title = {Provable tradeoffs in adversarially robust classification},
	
	abstract = {Machine learning methods can be vulnerable to small, adversarially-chosen perturbations of their inputs, prompting much research into theoretical explanations and algorithms toward improving adversarial robustness. Although a rich and insightful literature has developed around these ideas, many foundational open problems remain. In this paper, we seek to address several of these questions by deriving optimal robust classifiers for two- and three-class Gaussian classification problems with respect to adversaries in both the \${\textbackslash}ell\_2\$ and \${\textbackslash}ell\_{\textbackslash}infty\$ norms. While the standard non-robust version of this problem has a long history, the corresponding robust setting contains many unexplored problems, and indeed deriving optimal robust classifiers turns out to pose a variety of new challenges. We develop new analysis tools for this task. Our results reveal intriguing tradeoffs between usual and robust accuracy. Furthermore, we give results for data lying on low-dimensional manifolds and study the landscape of adversarially robust risk over linear classifiers, including proving Fisher consistency in some cases. Lastly, we provide novel results concerning finite sample adversarial risk in the Gaussian classification setting.},
	urldate = {2020-09-09},
	journal = {arXiv:2006.05161 [cs, stat]},
	author = {Dobriban, Edgar and Hassani, Hamed and Hong, David and Robey, Alexander},
	month = sep,
	year = {2020},
	note = {arXiv: 2006.05161},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 35 pages, 4 figures; updated Theorem 5.2, appendix and related works},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\I4XUFY5P\\Dobriban et al. - 2020 - Provable tradeoffs in adversarially robust classif.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\A7BD8IUH\\2006.html:text/html}
}

@article{chen_more_2020,
	title = {More {Data} {Can} {Expand} {The} {Generalization} {Gap} {Between} {Adversarially} {Robust} and {Standard} {Models}},
	volume = {1},
	
	language = {en},
	urldate = {2020-09-09},
	journal = {Proceedings of the International Conference on Machine Learning},
	author = {Chen, Lin and Min, Yifei and Zhang, Mingrui and Karbasi, Amin},
	year = {2020},
	file = {Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\H54UCTXM\\Chen et al. - 2020 - More Data Can Expand The Generalization Gap Betwee.pdf:application/pdf;Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\J7II7VTM\\36a1694bce9815b7e38a9dad05ad42e0.html:text/html}
}

@inproceedings{wong_wasserstein_2019,
	address = {Long Beach, California, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Wasserstein {Adversarial} {Examples} via {Projected} {Sinkhorn} {Iterations}},
	volume = {97},
	
	abstract = {A rapidly growing area of work has studied the existence of adversarial examples, datapoints which have been perturbed to fool a classifier, but the vast majority of these works have focused primarily on threat models defined by \${\textbackslash}ell\_p\$ norm-bounded perturbations. In this paper, we propose a new threat model for adversarial attacks based on the Wasserstein distance. In the image classification setting, such distances measure the cost of moving pixel mass, which can naturally represent “standard” image manipulations such as scaling, rotation, translation, and distortion (and can potentially be applied to other settings as well). To generate Wasserstein adversarial examples, we develop a procedure for approximate projection onto the Wasserstein ball, based upon a modified version of the Sinkhorn iteration. The resulting algorithm can successfully attack image classification models, bringing traditional CIFAR10 models down to 3\% accuracy within a Wasserstein ball with radius 0.1 (i.e., moving 10\% of the image mass 1 pixel), and we demonstrate that PGD-based adversarial training can improve this adversarial accuracy to 76\%. In total, this work opens up a new direction of study in adversarial robustness, more formally considering convex metrics that accurately capture the invariances that we typically believe should exist in classifiers, and code for all experiments in the paper is available at https://github.com/locuslab/projected\_sinkhorn.},
	publisher = {PMLR},
	author = {Wong, Eric and Schmidt, Frank and Kolter, Zico},
	
	month = jun,
	year = {2019},
	pages = {6808--6817}
}

@inproceedings{sharif_suitability_2018,
	address = {Salt Lake City, UT, USA},
	title = {On the {Suitability} of {Lp}-{Norms} for {Creating} and {Preventing} {Adversarial} {Examples}},
	isbn = {978-1-5386-6100-0},
	
	doi = {10.1109/CVPRW.2018.00211},
	abstract = {Much research has been devoted to better understanding adversarial examples, which are specially crafted inputs to machine-learning models that are perceptually similar to benign inputs, but are classiﬁed differently (i.e., misclassiﬁed). Both algorithms that create adversarial examples and strategies for defending against adversarial examples typically use Lp-norms to measure the perceptual similarity between an adversarial input and its benign original. Prior work has already shown, however, that two images need not be close to each other as measured by an Lp-norm to be perceptually similar. In this work, we show that nearness according to an Lp-norm is not just unnecessary for perceptual similarity, but is also insufﬁcient. Speciﬁcally, focusing on datasets (CIFAR10 and MNIST), Lp-norms, and thresholds used in prior work, we show through online user studies that “adversarial examples” that are closer to their benign counterparts than required by commonly used Lpnorm thresholds can nevertheless be perceptually distinct to humans from the corresponding benign examples. Namely, the perceptual distance between two images that are “near” each other according to an Lp-norm can be high enough that participants frequently classify the two images as representing different objects or digits. Combined with prior work, we thus demonstrate that nearness of inputs as measured by Lp-norms is neither necessary nor sufﬁcient for perceptual similarity, which has implications for both creating and defending against adversarial examples. We propose and discuss alternative similarity metrics to stimulate future research in the area.},
	language = {en},
	urldate = {2020-09-09},
	booktitle = {2018 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} {Workshops} ({CVPRW})},
	publisher = {IEEE},
	author = {Sharif, Mahmood and Baue, Lujo and Reite, Michael K.},
	month = jun,
	year = {2018},
	pages = {1686--16868},
	file = {Sharif et al. - 2018 - On the Suitability of Lp-Norms for Creating and Pr.pdf:C\:\\Users\\Zhu\\Zotero\\storage\\QSQ464GD\\Sharif et al. - 2018 - On the Suitability of Lp-Norms for Creating and Pr.pdf:application/pdf}
}

@article{carlini_evaluating_2019,
	title = {On {Evaluating} {Adversarial} {Robustness}},
	
	abstract = {Correctly evaluating defenses against adversarial examples has proven to be extremely difficult. Despite the significant amount of recent work attempting to design defenses that withstand adaptive attacks, few have succeeded; most papers that propose defenses are quickly shown to be incorrect. We believe a large contributing factor is the difficulty of performing security evaluations. In this paper, we discuss the methodological foundations, review commonly accepted best practices, and suggest new methods for evaluating defenses to adversarial examples. We hope that both researchers developing defenses as well as readers and reviewers who wish to understand the completeness of an evaluation consider our advice in order to avoid common pitfalls.},
	urldate = {2020-09-09},
	journal = {arXiv:1902.06705 [cs, stat]},
	author = {Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.06705},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Living document; source available at https://github.com/evaluating-adversarial-robustness/adv-eval-paper/},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\Y6PWSKBD\\Carlini et al. - 2019 - On Evaluating Adversarial Robustness.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\UGWAIC2Z\\1902.html:text/html}
}

@article{wong_learning_2020,
	title = {Learning perturbation sets for robust machine learning},
	
	abstract = {Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline digit transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to learn models which have improved generalization performance and are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at https://github.com/locuslab/perturbation\_learning.},
	urldate = {2020-09-09},
	journal = {arXiv:2007.08450 [cs, stat]},
	author = {Wong, Eric and Kolter, J. Zico},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08450},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\PBVWPB6Q\\Wong and Kolter - 2020 - Learning perturbation sets for robust machine lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\AC89CPBB\\2007.html:text/html}
}

@inproceedings{geirhos_imagenet-trained_2019,
	title = {{ImageNet}-trained {CNNs} are biased towards texture; increasing shape bias improves accuracy and robustness.},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A. and Brendel, Wieland},
	year = {2019}
}

@article{shi_informative_2020,
	title = {Informative {Dropout} for {Robust} {Representation} {Learning}: {A} {Shape}-bias {Perspective}},
	shorttitle = {Informative {Dropout} for {Robust} {Representation} {Learning}},
	
	abstract = {Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at https://github.com/bfshi/InfoDrop.},
	urldate = {2020-09-10},
	journal = {arXiv:2008.04254 [cs, stat]},
	author = {Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.04254},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to ICML2020. Code is available at https://github.com/bfshi/InfoDrop},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\KZ4IAG5N\\Shi et al. - 2020 - Informative Dropout for Robust Representation Lear.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\WPEX4JXQ\\2008.html:text/html}
}

@article{kriegeskorte_deep_2015,
	title = {Deep {Neural} {Networks}: {A} {New} {Framework} for {Modeling} {Biological} {Vision} and {Brain} {Information} {Processing}},
	volume = {1},
	issn = {2374-4650},
	shorttitle = {Deep {Neural} {Networks}},
	doi = {10.1146/annurev-vision-082114-035447},
	abstract = {Recent advances in neural network modeling have enabled major strides in computer vision and other artificial intelligence applications. Human-level visual recognition abilities are coming within reach of artificial systems. Artificial neural networks are inspired by the brain, and their computations could be implemented in biological neurons. Convolutional feedforward networks, which now dominate computer vision, take further inspiration from the architecture of the primate visual hierarchy. However, the current models are designed with engineering goals, not to model brain computations. Nevertheless, initial studies comparing internal representations between these models and primate brains find surprisingly similar representational spaces. With human-level performance no longer out of reach, we are entering an exciting new era, in which we will be able to build biologically faithful feedforward and recurrent computational models of how biological brains perform high-level feats of intelligence, including vision.},
	language = {eng},
	journal = {Annual Review of Vision Science},
	author = {Kriegeskorte, Nikolaus},
	month = nov,
	year = {2015},
	pmid = {28532370},
	keywords = {artificial intelligence, biological vision, computational neuroscience, computer vision, deep learning, neural network, object recognition},
	pages = {417--446},
	file = {Submitted Version:C\:\\Users\\Zhu\\Zotero\\storage\\X6QE5VXU\\Kriegeskorte - 2015 - Deep Neural Networks A New Framework for Modeling.pdf:application/pdf}
}

@article{ritter_cognitive_2017,
	title = {Cognitive {Psychology} for {Deep} {Neural} {Networks}: {A} {Shape} {Bias} {Case} {Study}},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	
	abstract = {Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. This has caused a recent surge of interest in methods for rendering modern neural systems more interpretable. In this work, we propose to address the interpretability problem in modern DNNs using the rich history of problem descriptions, theories and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
	urldate = {2020-09-10},
	journal = {arXiv:1706.08606 [cs, stat]},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08606},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICML 2017},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\YZ5PWEIV\\Ritter et al. - 2017 - Cognitive Psychology for Deep Neural Networks A S.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\447PNNKP\\1706.html:text/html}
}

@article{biederman_recognition-by-components_1987,
	title = {Recognition-by-components: {A} theory of human image understanding},
	volume = {94},
	issn = {1939-1471(Electronic),0033-295X(Print)},
	shorttitle = {Recognition-by-components},
	doi = {10.1037/0033-295X.94.2.115},
	abstract = {The perceptual recognition of objects is conceptualized to be a process in which the image of the input is segmented at regions of deep concavity into an arrangement of simple geometric components. The fundamental assumption of the proposed theory, recognition-by-components (RBC), is that a modest set of generalized-cone components, called geons, can be derived from contrasts of five readily detectable properties of edges in a two-dimensional image. The detection of these properties is generally invariant over viewing position and image quality and consequently allows robust object perception when the image is projected from a novel viewpoint or is degraded. RBC thus provides a principled account of the heretofore undecided relation between the classic principles of perceptual organization and pattern recognition. The results from experiments on the perception of briefly presented pictures by human observers provide empirical support for the theory. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
	number = {2},
	journal = {Psychological Review},
	author = {Biederman, Irving},
	year = {1987},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Pattern Discrimination, Perceptual Organization, Pictorial Stimuli, Spatial Imagery, Spatial Organization, Spatial Orientation (Perception)},
	pages = {115--147},
	file = {Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\GFEP3MD3\\1987-20898-001.html:text/html}
}

@article{geirhos_comparing_2018,
	title = {Comparing deep neural networks against humans: object recognition when the signal gets weaker},
	shorttitle = {Comparing deep neural networks against humans},
	
	abstract = {Human visual object recognition is typically rapid and seemingly effortless, as well as largely independent of viewpoint and object orientation. Until very recently, animate visual systems were the only ones capable of this remarkable computational feat. This has changed with the rise of a class of computer vision algorithms called deep neural networks (DNNs) that achieve human-level classification performance on object recognition tasks. Furthermore, a growing number of studies report similarities in the way DNNs and the human visual system process objects, suggesting that current DNNs may be good models of human visual object recognition. Yet there clearly exist important architectural and processing differences between state-of-the-art DNNs and the primate visual system. The potential behavioural consequences of these differences are not well understood. We aim to address this issue by comparing human and DNN generalisation abilities towards image degradations. We find the human visual system to be more robust to image manipulations like contrast reduction, additive noise or novel eidolon-distortions. In addition, we find progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker, indicating that there may still be marked differences in the way humans and current DNNs perform visual object recognition. We envision that our findings as well as our carefully measured and freely available behavioural datasets provide a new useful benchmark for the computer vision community to improve the robustness of DNNs and a motivation for neuroscientists to search for mechanisms in the brain that could facilitate this robustness.},
	urldate = {2020-09-10},
	journal = {arXiv:1706.06969 [cs, q-bio, stat]},
	author = {Geirhos, Robert and Janssen, David H. J. and Schütt, Heiko H. and Rauber, Jonas and Bethge, Matthias and Wichmann, Felix A.},
	month = dec,
	year = {2018},
	note = {arXiv: 1706.06969},
	keywords = {Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Quantitative Biology - Neurons and Cognition},
	annote = {Comment: updated article with reference to resulting publication (Geirhos et al, NeurIPS 2018)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\29CZEWW4\\Geirhos et al. - 2018 - Comparing deep neural networks against humans obj.pdf:application/pdf}
}

@article{brendel_approximating_2019,
	title = {Approximating {CNNs} with {Bag}-of-local-{Features} models works surprisingly well on {ImageNet}},
	
	abstract = {Deep Neural Networks (DNNs) excel on many complex perceptual tasks but it has proven notoriously difficult to understand how they reach their decisions. We here introduce a high-performance DNN architecture on ImageNet whose decisions are considerably easier to explain. Our model, a simple variant of the ResNet-50 architecture called BagNet, classifies an image based on the occurrences of small local image features without taking into account their spatial ordering. This strategy is closely related to the bag-of-feature (BoF) models popular before the onset of deep learning and reaches a surprisingly high accuracy on ImageNet (87.6\% top-5 for 33 x 33 px features and Alexnet performance for 17 x 17 px features). The constraint on local features makes it straight-forward to analyse how exactly each part of the image influences the classification. Furthermore, the BagNets behave similar to state-of-the art deep neural networks such as VGG-16, ResNet-152 or DenseNet-169 in terms of feature sensitivity, error distribution and interactions between image parts. This suggests that the improvements of DNNs over previous bag-of-feature classifiers in the last few years is mostly achieved by better fine-tuning rather than by qualitatively different decision strategies.},
	urldate = {2020-09-10},
	journal = {arXiv:1904.00760 [cs, stat]},
	author = {Brendel, Wieland and Bethge, Matthias},
	month = mar,
	year = {2019},
	note = {arXiv: 1904.00760},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Published as a conference paper at the Seventh International Conference on Learning Representations (ICLR 2019)},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\YUBZXPXT\\Brendel and Bethge - 2019 - Approximating CNNs with Bag-of-local-Features mode.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\XUTKRA85\\1904.html:text/html}
}

@article{hendrycks_benchmarking_2019,
	title = {Benchmarking {Neural} {Network} {Robustness} to {Common} {Corruptions} and {Perturbations}},
	
	abstract = {In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.},
	urldate = {2020-09-10},
	journal = {arXiv:1903.12261 [cs, stat]},
	author = {Hendrycks, Dan and Dietterich, Thomas},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.12261},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICLR 2019 camera-ready; datasets available at https://github.com/hendrycks/robustness ; this article supersedes arXiv:1807.01697},
	file = {arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\EREGLGY7\\1903.html:text/html}
}

@article{smilkov_smoothgrad_2017,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	urldate = {2020-09-10},
	journal = {arXiv:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03825},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\H4GMYRWD\\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:application/pdf}
}

@inproceedings{cohen_certified_2019,
	address = {Long Beach, California, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Certified {Adversarial} {Robustness} via {Randomized} {Smoothing}},
	volume = {97},
	
	abstract = {We show how to turn any classifier that classifies well under Gaussian noise into a new classifier that is certifiably robust to adversarial perturbations under the L2 norm. While this "randomized smoothing" technique has been proposed before in the literature, we are the first to provide a tight analysis, which establishes a close connection between L2 robustness and Gaussian noise. We use the technique to train an ImageNet classifier with e.g. a certified top-1 accuracy of 49\% under adversarial perturbations with L2 norm less than 0.5 (=127/255). Smoothing is the only approach to certifiably robust classification which has been shown feasible on full-resolution ImageNet. On smaller-scale datasets where competing approaches to certified L2 robustness are viable, smoothing delivers higher certified accuracies. The empirical success of the approach suggests that provable methods based on randomization at prediction time are a promising direction for future research into adversarially robust classification.},
	publisher = {PMLR},
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	
	month = jun,
	year = {2019},
	pages = {1310--1320}
}


@article{blum_random_2020,
	title = {Random {Smoothing} {Might} be {Unable} to {Certify} \${\textbackslash}ell\_{\textbackslash}infty\$ {Robustness} for {High}-{Dimensional} {Images}},
	
	abstract = {We show a hardness result for random smoothing to achieve certified adversarial robustness against attacks in the \${\textbackslash}ell\_p\$ ball of radius \${\textbackslash}epsilon\$ when \$p{\textgreater}2\$. Although random smoothing has been well understood for the \${\textbackslash}ell\_2\$ case using the Gaussian distribution, much remains unknown concerning the existence of a noise distribution that works for the case of \$p{\textgreater}2\$. This has been posed as an open problem by Cohen et al. (2019) and includes many significant paradigms such as the \${\textbackslash}ell\_{\textbackslash}infty\$ threat model. In this work, we show that any noise distribution \${\textbackslash}mathcal\{D\}\$ over \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ that provides \${\textbackslash}ell\_p\$ robustness for all base classifiers with \$p{\textgreater}2\$ must satisfy \${\textbackslash}mathbb\{E\}{\textbackslash}eta\_i{\textasciicircum}2={\textbackslash}Omega(d{\textasciicircum}\{1-2/p\}{\textbackslash}epsilon{\textasciicircum}2(1-{\textbackslash}delta)/{\textbackslash}delta{\textasciicircum}2)\$ for 99\% of the features (pixels) of vector \${\textbackslash}eta{\textbackslash}sim{\textbackslash}mathcal\{D\}\$, where \${\textbackslash}epsilon\$ is the robust radius and \${\textbackslash}delta\$ is the score gap between the highest-scored class and the runner-up. Therefore, for high-dimensional images with pixel values bounded in \$[0,255]\$, the required noise will eventually dominate the useful information in the images, leading to trivial smoothed classifiers.},
	urldate = {2020-09-11},
	journal = {arXiv:2002.03517 [cs, stat]},
	author = {Blum, Avrim and Dick, Travis and Manoj, Naren and Zhang, Hongyang},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.03517},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 20 pages, 2 figures; Code is available at https://github.com/hongyanz/TRADES-smoothing},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\9LMRJX7X\\Blum et al. - 2020 - Random Smoothing Might be Unable to Certify \$ell_.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\GD647HXK\\2002.html:text/html}
}

@incollection{li_certified_2019,
	title = {Certified {Adversarial} {Robustness} with {Additive} {Noise}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
	
	year = {2019},
	pages = {9464--9474}
}

@article{goodfellow_explaining_2015,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	urldate = {2020-09-11},
	journal = {arXiv:1412.6572 [cs, stat]},
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	month = mar,
	year = {2015},
	note = {arXiv: 1412.6572},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\JDRC9VFZ\\Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\QLA2Q2L2\\1412.html:text/html}
}

@inproceedings{wong_provable_2018,
	address = {Stockholmsmässan, Stockholm Sweden},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Provable {Defenses} against {Adversarial} {Examples} via the {Convex} {Outer} {Adversarial} {Polytope}},
	volume = {80},
	
	abstract = {We propose a method to learn deep ReLU-based classifiers that are provably robust against norm-bounded adversarial perturbations on the training data. For previously unseen examples, the approach is guaranteed to detect all adversarial examples, though it may flag some non-adversarial examples as well. The basic idea is to consider a convex outer approximation of the set of activations reachable through a norm-bounded perturbation, and we develop a robust optimization procedure that minimizes the worst case loss over this outer region (via a linear program). Crucially, we show that the dual problem to this linear program can be represented itself as a deep network similar to the backpropagation network, leading to very efficient optimization approaches that produce guaranteed bounds on the robust loss. The end result is that by executing a few more forward and backward passes through a slightly modified version of the original network (though possibly with much larger batch sizes), we can learn a classifier that is provably robust to any norm-bounded adversarial attack. We illustrate the approach on a number of tasks to train classifiers with robust adversarial guarantees (e.g. for MNIST, we produce a convolutional classifier that provably has less than 5.8\% test error for any adversarial attack with bounded \${\textbackslash}ell\_ınfty\$ norm less than \${\textbackslash}epsilon = 0.1\$).},
	publisher = {PMLR},
	author = {Wong, Eric and Kolter, Zico},
	
	month = jul,
	year = {2018},
	pages = {5286--5295}
}

@article{gowal_effectiveness_2019,
	title = {On the {Effectiveness} of {Interval} {Bound} {Propagation} for {Training} {Verifiably} {Robust} {Models}},
	
	abstract = {Recent work has shown that it is possible to train deep neural networks that are provably robust to norm-bounded adversarial perturbations. Most of these methods are based on minimizing an upper bound on the worst-case loss over all possible adversarial perturbations. While these techniques show promise, they often result in difficult optimization procedures that remain hard to scale to larger networks. Through a comprehensive analysis, we show how a simple bounding technique, interval bound propagation (IBP), can be exploited to train large provably robust neural networks that beat the state-of-the-art in verified accuracy. While the upper bound computed by IBP can be quite weak for general networks, we demonstrate that an appropriate loss and clever hyper-parameter schedule allow the network to adapt such that the IBP bound is tight. This results in a fast and stable learning algorithm that outperforms more sophisticated methods and achieves state-of-the-art results on MNIST, CIFAR-10 and SVHN. It also allows us to train the largest model to be verified beyond vacuous bounds on a downscaled version of ImageNet.},
	urldate = {2020-09-11},
	journal = {arXiv:1810.12715 [cs, stat]},
	author = {Gowal, Sven and Dvijotham, Krishnamurthy and Stanforth, Robert and Bunel, Rudy and Qin, Chongli and Uesato, Jonathan and Arandjelovic, Relja and Mann, Timothy and Kohli, Pushmeet},
	month = aug,
	year = {2019},
	note = {arXiv: 1810.12715},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: [v2] Best paper at NeurIPS SECML 2018 Workshop [v4] Accepted at ICCV 2019 under the title "Scalable Verified Training for Provably Robust Image Classification"},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\5B632PXI\\Gowal et al. - 2019 - On the Effectiveness of Interval Bound Propagation.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\WH5LKJXZ\\1810.html:text/html}
}

@incollection{salman_convex_2019,
	title = {A {Convex} {Relaxation} {Barrier} to {Tight} {Robustness} {Verification} of {Neural} {Networks}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Salman, Hadi and Yang, Greg and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Pengchuan},
	
	year = {2019},
	pages = {9835--9846}
}

@incollection{wong_scaling_2018,
	title = {Scaling provable adversarial defenses},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Wong, Eric and Schmidt, Frank and Metzen, Jan Hendrik and Kolter, J. Zico},
	
	year = {2018},
	pages = {8400--8409}
}

@incollection{raghunathan_semidefinite_2018,
	title = {Semidefinite relaxations for certifying robustness to adversarial examples},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Raghunathan, Aditi and Steinhardt, Jacob and Liang, Percy S},
	
	year = {2018},
	pages = {10877--10887}
}

@inproceedings{athalye_obfuscated_2018,
	address = {Stockholmsmässan, Stockholm Sweden},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	volume = {80},
	
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimization-based attacks, we find defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining non-certified white-box-secure defenses at ICLR 2018, we find obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	publisher = {PMLR},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	
	month = jul,
	year = {2018},
	pages = {274--283}
}

@article{croce_reliable_2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	
	abstract = {The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than \$10{\textbackslash}\%\$, identifying several broken defenses.},
	urldate = {2020-09-11},
	journal = {Proceedings of the International Conference on Machine Learning},
	author = {Croce, Francesco and Hein, Matthias},
	month = aug,
	year = {2020},
	note = {arXiv: 2003.01690},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: In ICML 2020},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\UUQ97FQD\\Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\RKD8E6CY\\2003.html:text/html}
}

@incollection{carmon_unlabeled_2019,
	title = {Unlabeled {Data} {Improves} {Adversarial} {Robustness}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
	
	year = {2019},
	pages = {11192--11203}
}

@incollection{alayrac_are_2019,
	title = {Are {Labels} {Required} for {Improving} {Adversarial} {Robustness}?},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Alayrac, Jean-Baptiste and Uesato, Jonathan and Huang, Po-Sen and Fawzi, Alhussein and Stanforth, Robert and Kohli, Pushmeet},
	
	year = {2019},
	pages = {12214--12223}
}

@inproceedings{zhang_theoretically_2019-1,
	address = {Long Beach, California, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Theoretically {Principled} {Trade}-off between {Robustness} and {Accuracy}},
	volume = {97},
	
	abstract = {We identify a trade-off between robustness and accuracy that serves as a guiding principle in the design of defenses against adversarial examples. Although this problem has been widely studied empirically, much remains unknown concerning the theory underlying this trade-off. In this work, we decompose the prediction error for adversarial examples (robust error) as the sum of the natural (classification) error and boundary error, and provide a differentiable upper bound using the theory of classification-calibrated loss, which is shown to be the tightest possible upper bound uniform over all probability distributions and measurable predictors. Inspired by our theoretical analysis, we also design a new defense method, TRADES, to trade adversarial robustness off against accuracy. Our proposed algorithm performs well experimentally in real-world datasets. The methodology is the foundation of our entry to the NeurIPS 2018 Adversarial Vision Challenge in which we won the 1st place out of \&nbsp;2,000 submissions, surpassing the runner-up approach by 11.41\% in terms of mean L\_2 perturbation distance.},
	publisher = {PMLR},
	author = {Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and Ghaoui, Laurent El and Jordan, Michael},
	
	month = jun,
	year = {2019},
	pages = {7472--7482}
}

@article{rice_overfitting_2020,
	title = {Overfitting in adversarially robust deep learning},
	
	abstract = {It is common practice in deep learning to use overparameterized networks and train for as long as possible; there are numerous studies that show, both theoretically and empirically, that such practices surprisingly do not unduly harm the generalization performance of the classifier. In this paper, we empirically study this phenomenon in the setting of adversarially trained deep networks, which are trained to minimize the loss under worst-case adversarial perturbations. We find that overfitting to the training set does in fact harm robust performance to a very large degree in adversarially robust training across multiple datasets (SVHN, CIFAR-10, CIFAR-100, and ImageNet) and perturbation models (\${\textbackslash}ell\_{\textbackslash}infty\$ and \${\textbackslash}ell\_2\$). Based upon this observed effect, we show that the performance gains of virtually all recent algorithmic improvements upon adversarial training can be matched by simply using early stopping. We also show that effects such as the double descent curve do still occur in adversarially trained models, yet fail to explain the observed overfitting. Finally, we study several classical and modern deep learning remedies for overfitting, including regularization and data augmentation, and find that no approach in isolation improves significantly upon the gains achieved by early stopping. All code for reproducing the experiments as well as pretrained model weights and training logs can be found at https://github.com/locuslab/robust\_overfitting.},
	urldate = {2020-09-11},
	journal = {Proceedings of the International Conference on Machine Learning},
	author = {Rice, Leslie and Wong, Eric and Kolter, J. Zico},
	month = mar,
	year = {2020},
	note = {arXiv: 2002.11569},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\ZGTC9TFX\\Rice et al. - 2020 - Overfitting in adversarially robust deep learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\3Z7FY9TH\\2002.html:text/html}
}

@incollection{qin_adversarial_2019,
	title = {Adversarial {Robustness} through {Local} {Linearization}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Qin, Chongli and Martens, James and Gowal, Sven and Krishnan, Dilip and Dvijotham, Krishnamurthy and Fawzi, Alhussein and De, Soham and Stanforth, Robert and Kohli, Pushmeet},
	
	year = {2019},
	pages = {13847--13856}
}

@inproceedings{etmann_connection_2019,
	title = {On the {Connection} {Between} {Adversarial} {Robustness} and {Saliency} {Map} {Interpretability}},
	booktitle = {International {Conference} on {Machine} {Learning}},
	author = {Etmann, Christian and Lunz, Sebastian and Maass, Peter and Schoenlieb, Carola},
	year = {2019},
	pages = {1823--1832}
}

@article{smilkov_smoothgrad_2017-1,
	title = {{SmoothGrad}: removing noise by adding noise},
	shorttitle = {{SmoothGrad}},
	
	abstract = {Explaining the output of a deep network remains a challenge. In the case of an image classifier, one type of explanation is to identify pixels that strongly influence the final decision. A starting point for this strategy is the gradient of the class score function with respect to the input image. This gradient can be interpreted as a sensitivity map, and there are several techniques that elaborate on this basic idea. This paper makes two contributions: it introduces SmoothGrad, a simple method that can help visually sharpen gradient-based sensitivity maps, and it discusses lessons in the visualization of these maps. We publish the code for our experiments and a website with our results.},
	urldate = {2020-09-11},
	journal = {arXiv:1706.03825 [cs, stat]},
	author = {Smilkov, Daniel and Thorat, Nikhil and Kim, Been and Viégas, Fernanda and Wattenberg, Martin},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03825},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\5TCZPVRK\\Smilkov et al. - 2017 - SmoothGrad removing noise by adding noise.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\367K4ELA\\1706.html:text/html}
}

@article{taori_measuring_2020-1,
	title = {Measuring {Robustness} to {Natural} {Distribution} {Shifts} in {Image} {Classification}},
	
	abstract = {We study how robust current ImageNet models are to distribution shifts arising from natural variations in datasets. Most research on robustness focuses on synthetic image perturbations (noise, simulated weather artifacts, adversarial examples, etc.), which leaves open how robustness on synthetic distribution shift relates to distribution shift arising in real data. Informed by an evaluation of 196 ImageNet models in 211 different test conditions, we find that there is little to no transfer of robustness from current synthetic to natural distribution shift. Moreover, most current techniques provide no robustness to the natural distribution shifts in our testbed. The main exception is training on larger datasets, which in some cases offers small gains in robustness. Our results indicate that distribution shifts arising in real data are currently an open research problem.},
	urldate = {2020-09-11},
	journal = {arXiv:2007.00644 [cs, stat]},
	author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.00644},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\PQVC6BYH\\Taori et al. - 2020 - Measuring Robustness to Natural Distribution Shift.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\AV8YRQXY\\2007.html:text/html}
}

@inproceedings{zhu_freelb_2020,
	title = {{FreeLB}: {Enhanced} {Adversarial} {Training} for {Natural} {Language} {Understanding}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Zhu, Chen and Cheng, Yu and Gan, Zhe and Sun, Siqi and Goldstein, Tom and Liu, Jingjing},
	year = {2020}
}

@article{liu_adversarial_2020,
	title = {Adversarial {Training} for {Large} {Neural} {Language} {Models}},
	
	abstract = {Generalization and robustness are both key desiderata for designing machine learning methods. Adversarial training can enhance robustness, but past work often finds it hurts generalization. In natural language processing (NLP), pre-training large neural language models such as BERT have demonstrated impressive gain in generalization for a variety of tasks, with further improvement from adversarial fine-tuning. However, these models are still vulnerable to adversarial attacks. In this paper, we show that adversarial pre-training can improve both generalization and robustness. We propose a general algorithm ALUM (Adversarial training for large neural LangUage Models), which regularizes the training objective by applying perturbations in the embedding space that maximizes the adversarial loss. We present the first comprehensive study of adversarial training in all stages, including pre-training from scratch, continual pre-training on a well-trained model, and task-specific fine-tuning. ALUM obtains substantial gains over BERT on a wide range of NLP tasks, in both regular and adversarial scenarios. Even for models that have been well trained on extremely large text corpora, such as RoBERTa, ALUM can still produce significant gains from continual pre-training, whereas conventional non-adversarial methods can not. ALUM can be further combined with task-specific fine-tuning to attain additional gains. The ALUM code is publicly available at https://github.com/namisan/mt-dnn.},
	urldate = {2020-09-11},
	journal = {arXiv:2004.08994 [cs]},
	author = {Liu, Xiaodong and Cheng, Hao and He, Pengcheng and Chen, Weizhu and Wang, Yu and Poon, Hoifung and Gao, Jianfeng},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.08994},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 13 pages, 9 tables, 2 figures},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\FAY3V5GV\\Liu et al. - 2020 - Adversarial Training for Large Neural Language Mod.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\XG7NEEKK\\2004.html:text/html}
}

@article{hendrycks_many_2020,
	title = {The {Many} {Faces} of {Robustness}: {A} {Critical} {Analysis} of {Out}-of-{Distribution} {Generalization}},
	shorttitle = {The {Many} {Faces} of {Robustness}},
	
	abstract = {We introduce three new robustness benchmarks consisting of naturally occurring distribution changes in image style, geographic location, camera operation, and more. Using our benchmarks, we take stock of previously proposed hypotheses for out-of-distribution robustness and put them to the test. We find that using larger models and synthetic data augmentation can improve robustness on real-world distribution shifts, contrary to claims in prior work. Motivated by this, we introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000x more labeled data. We find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Hence no evaluated method consistently improves robustness. We conclude that future research must study multiple distribution shifts simultaneously.},
	urldate = {2020-09-11},
	journal = {arXiv:2006.16241 [cs, stat]},
	author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16241},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Datasets, code, and models available at https://github.com/hendrycks/imagenet-r},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\JH6TP7WD\\Hendrycks et al. - 2020 - The Many Faces of Robustness A Critical Analysis .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\5CDJ73GX\\2006.html:text/html}
}

@book{taori_when_2020,
	title = {When {Robustness} {Doesn}’t {Promote} {Robustness}: {Synthetic} vs. {Natural} {Distribution} {Shifts} on {ImageNet}},
	
	author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
	year = {2020}
}

@article{allen-zhu_feature_2020-1,
	title = {Feature {Purification}: {How} {Adversarial} {Training} {Performs} {Robust} {Deep} {Learning}},
	shorttitle = {Feature {Purification}},
	
	abstract = {Despite the great empirical success of adversarial training to defend deep learning models against adversarial perturbations, so far, it still remains rather unclear what the principles are behind the existence of adversarial perturbations, and what adversarial training does to the neural network to remove them. In this paper, we present a principle that we call "feature purification", where we show the existence of adversarial examples are due to the accumulation of certain "dense mixtures" in the hidden weights during the training process of a neural network; and more importantly, one of the goals of adversarial training is to remove such mixtures to "purify" hidden weights. We present both experiments on the CIFAR-10 dataset to illustrate this principle, and a Theoretical Result proving that for certain natural classification tasks, training a two-layer neural network with ReLU activation using randomly initialized gradient descent indeed satisfies this principle. Technically, we give, to the best of our knowledge, the first result proving that the following two can hold simultaneously for training a neural network with ReLU activation. (1) Training over the original data is indeed non-robust to small adversarial perturbations of some radius. (2) Adversarial training, even with an empirical perturbation algorithm such as FGM, can in fact be provably robust against ANY perturbations of the same radius. Finally, we also prove a complexity lower bound, showing that low complexity models such as linear classifiers, low-degree polynomials, or even the neural tangent kernel for this network, CANNOT defend against perturbations of this same radius, no matter what algorithms are used to train them.},
	urldate = {2020-09-11},
	journal = {arXiv:2005.10190 [cs, math, stat]},
	author = {Allen-Zhu, Zeyuan and Li, Yuanzhi},
	month = may,
	year = {2020},
	note = {arXiv: 2005.10190},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing, Mathematics - Optimization and Control},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\MC867ZFS\\Allen-Zhu and Li - 2020 - Feature Purification How Adversarial Training Per.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\EVNXQYCG\\2005.html:text/html}
}

@incollection{zhang_you_2019,
	title = {You {Only} {Propagate} {Once}: {Accelerating} {Adversarial} {Training} via {Maximal} {Principle}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Zhang, Dinghuai and Zhang, Tianyuan and Lu, Yiping and Zhu, Zhanxing and Dong, Bin},
	
	year = {2019},
	pages = {227--238}
}

@inproceedings{wong_fast_2020,
	title = {Fast is better than free: {Revisiting} adversarial training},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
	year = {2020}
}

@incollection{shafahi_adversarial_2019,
	title = {Adversarial training for free!},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Shafahi, Ali and Najibi, Mahyar and Ghiasi, Mohammad Amin and Xu, Zheng and Dickerson, John and Studer, Christoph and Davis, Larry S and Taylor, Gavin and Goldstein, Tom},
	
	year = {2019},
	pages = {3358--3369}
}

@inproceedings{soudry_implicit_2018,
	title = {The {Implicit} {Bias} of {Gradient} {Descent} on {Separable} {Data}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Soudry, Daniel and Hoffer, Elad and Srebro, Nathan},
	year = {2018}
}

@inproceedings{jiang_fantastic_2020,
	title = {Fantastic {Generalization} {Measures} and {Where} to {Find} {Them}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Jiang*, Yiding and Neyshabur*, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
	year = {2020}
}

@incollection{nagarajan_uniform_2019,
	title = {Uniform convergence may be unable to explain generalization in deep learning},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Nagarajan, Vaishnavh and Kolter, J. Zico},
	
	year = {2019},
	pages = {11615--11626}
}

 @inproceedings{Yin_Kannan_Bartlett_2019, series={Proceedings of Machine Learning Research}, title={Rademacher Complexity for Adversarially Robust Generalization}, volume={97}, url={http://proceedings.mlr.press/v97/yin19b.html}, abstractNote={Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence; moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data. In this paper, we focus on $ell_ınfty$ attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity. For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded $ell_1$ norm, and our results also extend to multi-class linear classifiers; in addition, for (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting. Our results indicate that having $ell_1$ norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting. We demonstrate experimental results that validate our theoretical findings.}, booktitle={Proceedings of the 36th International Conference on Machine Learning}, publisher={PMLR}, author={Yin, Dong and Kannan, Ramchandran and Bartlett, Peter}, editor={Chaudhuri, Kamalika and Salakhutdinov, Ruslan}, year={2019}, month={Jun}, pages={7085–7094}, collection={Proceedings of Machine Learning Research} }


@inproceedings{song_robust_2020,
	title = {Robust {Local} {Features} for {Improving} the {Generalization} of {Adversarial} {Training}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Song, Chuanbiao and He, Kun and Lin, Jiadong and Wang, Liwei and Hopcroft, John E.},
	year = {2020}
}

@inproceedings{degwekar_computational_2019,
	address = {Phoenix, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Computational {Limitations} in {Robust} {Classification} and {Win}-{Win} {Results}},
	volume = {99},
	
	abstract = {We continue the study of statistical/computational tradeoffs in learning robust classifiers, following the recent work of Bubeck, Lee, Price and Razenshteyn who showed examples of classification tasks where (a) an efficient robust classifier exists, in the small-perturbation regime; (b) a non-robust classifier can be learned efficiently; but (c) it is computationally hard to learn a robust classifier, assuming the hardness of factoring large numbers. Indeed, the question of whether a robust classifier for their task exists in the large perturbation regime seems related to important open questions in computational number theory. In this work, we extend their work in three directions. First, we demonstrate classification tasks where computationally efficient robust classification is impossible, even when computationally unbounded robust classifiers exist. For this, we rely on the existence of average-case hard functions, requiring no cryptographic assumptions. Second, we show hard-to-robustly-learn classification tasks in the large-perturbation regime. Namely, we show that even though an efficient classifier that is very robust (namely, tolerant to large perturbations) exists, it is computationally hard to learn any non-trivial robust classifier. Our first construction relies on the existence of one-way functions, a minimal assumption in cryptography, and the second on the hardness of the learning parity with noise problem. In the latter setting, not only does a non-robust classifier exist, but also an efficient algorithm that generates fresh new labeled samples given access to polynomially many training examples (termed as generation by Kearns et al. (1994)). Third, we show that any such counterexample implies the existence of cryptographic primitives such as one-way functions or even forms of public-key encryption. This leads us to a win-win scenario: either we can quickly learn an efficient robust classifier, or we can construct new instances of popular and useful cryptographic primitives.},
	publisher = {PMLR},
	author = {Degwekar, Akshay and Nakkiran, Preetum and Vaikuntanathan, Vinod},
	
	month = jun,
	year = {2019},
	pages = {994--1028}
}

@inproceedings{sagawa_distributionally_2020,
	title = {Distributionally {Robust} {Neural} {Networks}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Sagawa*, Shiori and Koh*, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy},
	year = {2020}
}

@article{salman_adversarially_2020,
	title = {Do {Adversarially} {Robust} {ImageNet} {Models} {Transfer} {Better}?},
	
	abstract = {Transfer learning is a widely-used paradigm in deep learning, where models pre-trained on standard datasets can be efficiently adapted to downstream tasks. Typically, better pre-trained models yield better transfer results, suggesting that initial accuracy is a key aspect of transfer learning performance. In this work, we identify another such aspect: we find that adversarially robust models, while less accurate, often perform better than their standard-trained counterparts when used for transfer learning. Specifically, we focus on adversarially robust ImageNet classifiers, and show that they yield improved accuracy on a standard suite of downstream classification tasks. Further analysis uncovers more differences between robust and standard models in the context of transfer learning. Our results are consistent with (and in fact, add to) recent hypotheses stating that robustness leads to improved feature representations. Our code and models are available at https://github.com/Microsoft/robust-models-transfer .},
	urldate = {2020-09-13},
	journal = {arXiv:2007.08489 [cs, stat]},
	author = {Salman, Hadi and Ilyas, Andrew and Engstrom, Logan and Kapoor, Ashish and Madry, Aleksander},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.08489},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\94SNSR9E\\Salman et al. - 2020 - Do Adversarially Robust ImageNet Models Transfer B.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\MZ8CYU57\\2007.html:text/html}
}

@article{utrera_adversarially-trained_2020,
	title = {Adversarially-{Trained} {Deep} {Nets} {Transfer} {Better}},
	
	abstract = {Transfer learning has emerged as a powerful methodology for adapting pre-trained deep neural networks to new domains. This process consists of taking a neural network pre-trained on a large feature-rich source dataset, freezing the early layers that encode essential generic image properties, and then fine-tuning the last few layers in order to capture specific information related to the target situation. This approach is particularly useful when only limited or weakly labelled data are available for the new task. In this work, we demonstrate that adversarially-trained models transfer better across new domains than naturally-trained models, even though it's known that these models do not generalize as well as naturally-trained models on the source domain. We show that this behavior results from a bias, introduced by the adversarial training, that pushes the learned inner layers to more natural image representations, which in turn enables better transfer.},
	urldate = {2020-09-13},
	journal = {arXiv:2007.05869 [cs, stat]},
	author = {Utrera, Francisco and Kravitz, Evan and Erichson, N. Benjamin and Khanna, Rajiv and Mahoney, Michael W.},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.05869},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\RE5327RM\\Utrera et al. - 2020 - Adversarially-Trained Deep Nets Transfer Better.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\ZLQW64YT\\2007.html:text/html}
}

@article{knights_temporally_2020,
	title = {Temporally {Coherent} {Embeddings} for {Self}-{Supervised} {Video} {Representation} {Learning}},
	
	abstract = {This paper presents TCE: Temporally Coherent Embeddings for self-supervised video representation learning. The proposed method exploits inherent structure of unlabeled video data to explicitly enforce temporal coherency in the embedding space, rather than indirectly learning it through ranking or predictive proxy tasks. In the same way that high-level visual information in the world changes smoothly, we believe that nearby frames in learned representations will benefit from demonstrating similar properties. Using this assumption, we train our TCE model to encode videos such that adjacent frames exist close to each other and videos are separated from one another. Using TCE we learn robust representations from large quantities of unlabeled video data. We thoroughly analyse and evaluate our self-supervised learned TCE models on a downstream task of video action recognition using multiple challenging benchmarks (Kinetics400, UCF101, HMDB51). With a simple but effective 2D-CNN backbone and only RGB stream inputs, TCE pre-trained representations outperform all previous selfsupervised 2D-CNN and 3D-CNN pre-trained on UCF101. The code and pre-trained models for this paper can be downloaded at: https://github.com/csiro-robotics/TCE},
	urldate = {2020-09-13},
	journal = {arXiv:2004.02753 [cs, eess]},
	author = {Knights, Joshua and Harwood, Ben and Ward, Daniel and Vanderkop, Anthony and Mackenzie-Ross, Olivia and Moghadam, Peyman},
	month = aug,
	year = {2020},
	note = {arXiv: 2004.02753},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, I.2.6},
	annote = {Comment: Under review! Project page: https://csiro-robotics.github.io/TCE-Webpage/},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\PAKV8TG9\\Knights et al. - 2020 - Temporally Coherent Embeddings for Self-Supervised.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\ABL88GG5\\2004.html:text/html}
}

@article{hendrycks_many_2020-1,
	title = {The {Many} {Faces} of {Robustness}: {A} {Critical} {Analysis} of {Out}-of-{Distribution} {Generalization}},
	shorttitle = {The {Many} {Faces} of {Robustness}},
	
	abstract = {We introduce three new robustness benchmarks consisting of naturally occurring distribution changes in image style, geographic location, camera operation, and more. Using our benchmarks, we take stock of previously proposed hypotheses for out-of-distribution robustness and put them to the test. We find that using larger models and synthetic data augmentation can improve robustness on real-world distribution shifts, contrary to claims in prior work. Motivated by this, we introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000x more labeled data. We find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Hence no evaluated method consistently improves robustness. We conclude that future research must study multiple distribution shifts simultaneously.},
	urldate = {2020-09-13},
	journal = {arXiv:2006.16241 [cs, stat]},
	author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
	month = aug,
	year = {2020},
	note = {arXiv: 2006.16241},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Datasets, code, and models available at https://github.com/hendrycks/imagenet-r},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\733CEARF\\Hendrycks et al. - 2020 - The Many Faces of Robustness A Critical Analysis .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\NQSYTU6D\\2006.html:text/html}
}

@article{itazuri_what_2019,
	title = {What {Do} {Adversarially} {Robust} {Models} {Look} {At}?},
	
	abstract = {In this paper, we address the open question: "What do adversarially robust models look at?" Recently, it has been reported in many works that there exists the trade-off between standard accuracy and adversarial robustness. According to prior works, this trade-off is rooted in the fact that adversarially robust and standard accurate models might depend on very different sets of features. However, it has not been well studied what kind of difference actually exists. In this paper, we analyze this difference through various experiments visually and quantitatively. Experimental results show that adversarially robust models look at things at a larger scale than standard models and pay less attention to fine textures. Furthermore, although it has been claimed that adversarially robust features are not compatible with standard accuracy, there is even a positive effect by using them as pre-trained models particularly in low resolution datasets.},
	urldate = {2020-09-14},
	journal = {arXiv:1905.07666 [cs]},
	author = {Itazuri, Takahiro and Fukuhara, Yoshihiro and Kataoka, Hirokatsu and Morishima, Shigeo},
	month = may,
	year = {2019},
	note = {arXiv: 1905.07666},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\P33I2E7U\\Itazuri et al. - 2019 - What Do Adversarially Robust Models Look At.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\4Z5QWQNW\\1905.html:text/html}
}

@inproceedings{su_is_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Is {Robustness} the {Cost} of {Accuracy}? – {A} {Comprehensive} {Study} on the {Robustness} of 18 {Deep} {Image} {Classification} {Models}},
	isbn = {978-3-030-01258-8},
	shorttitle = {Is {Robustness} the {Cost} of {Accuracy}?},
	doi = {10.1007/978-3-030-01258-8_39},
	abstract = {The prediction accuracy has been the long-lasting and sole standard for comparing the performance of different image classification models, including the ImageNet competition. However, recent studies have highlighted the lack of robustness in well-trained deep neural networks to adversarial examples. Visually imperceptible perturbations to natural images can easily be crafted and mislead the image classifiers towards misclassification. To demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 ImageNet models using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models. Our extensive experimental results reveal several new insights: (1) linear scaling law - the empirical ℓ2ℓ2{\textbackslash}ell \_2 and ℓ∞ℓ∞{\textbackslash}ell \_{\textbackslash}infty distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness Pareto frontier can be used as an evaluation criterion for ImageNet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ℓ∞{\textbackslash}ell \_{\textbackslash}infty distortion; (4) there exist models (in VGG family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family. Experiment code is publicly available at https://github.com/huanzhang12/Adversarial\_Survey.},
	language = {en},
	booktitle = {Computer {Vision} – {ECCV} 2018},
	publisher = {Springer International Publishing},
	author = {Su, Dong and Zhang, Huan and Chen, Hongge and Yi, Jinfeng and Chen, Pin-Yu and Gao, Yupeng},
	
	year = {2018},
	keywords = {Adversarial attacks, Deep neural networks, Robustness},
	pages = {644--661},
	file = {Submitted Version:C\:\\Users\\Zhu\\Zotero\\storage\\NEYFCHSK\\Su et al. - 2018 - Is Robustness the Cost of Accuracy – A Comprehens.pdf:application/pdf}
}

@article{nakkiran_adversarial_2019,
	title = {Adversarial {Robustness} {May} {Be} at {Odds} {With} {Simplicity}},
	
	abstract = {Current techniques in machine learning are so far are unable to learn classifiers that are robust to adversarial perturbations. However, they are able to learn non-robust classifiers with very high accuracy, even in the presence of random perturbations. Towards explaining this gap, we highlight the hypothesis that \${\textbackslash}textit\{robust classification may require more complex classifiers (i.e. more capacity) than standard classification.\}\$ In this note, we show that this hypothesis is indeed possible, by giving several theoretical examples of classification tasks and sets of "simple" classifiers for which: (1) There exists a simple classifier with high standard accuracy, and also high accuracy under random \${\textbackslash}ell\_{\textbackslash}infty\$ noise. (2) Any simple classifier is not robust: it must have high adversarial loss with \${\textbackslash}ell\_{\textbackslash}infty\$ perturbations. (3) Robust classification is possible, but only with more complex classifiers (exponentially more complex, in some examples). Moreover, \${\textbackslash}textit\{there is a quantitative trade-off between robustness and standard accuracy among simple classifiers.\}\$ This suggests an alternate explanation of this phenomenon, which appears in practice: the tradeoff may occur not because the classification task inherently requires such a tradeoff (as in [Tsipras-Santurkar-Engstrom-Turner-Madry `18]), but because the structure of our current classifiers imposes such a tradeoff.},
	urldate = {2020-09-14},
	journal = {arXiv:1901.00532 [cs, stat]},
	author = {Nakkiran, Preetum},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.00532},
	keywords = {Computer Science - Computational Complexity, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: welcome},
	file = {arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\B7ICB9K6\\1901.html:text/html}
}

@inproceedings{schott_towards_2019,
	title = {Towards the first adversarially robust neural network model on {MNIST}},
	
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Schott, Lukas and Rauber, Jonas and Bethge, Matthias and Brendel, Wieland},
	year = {2019}
}

@incollection{tramer_adversarial_2019,
	title = {Adversarial {Training} and {Robustness} for {Multiple} {Perturbations}},
	
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Tramer, Florian and Boneh, Dan},
	
	year = {2019},
	pages = {5866--5876}
}

@article{maini_adversarial_2020,
	title = {Adversarial {Robustness} {Against} the {Union} of {Multiple} {Perturbation} {Models}},
	
	abstract = {Owing to the susceptibility of deep learning systems to adversarial attacks, there has been a great deal of work in developing (both empirically and certifiably) robust classifiers. While most work has defended against a single type of attack, recent work has looked at defending against multiple perturbation models using simple aggregations of multiple attacks. However, these methods can be difficult to tune, and can easily result in imbalanced degrees of robustness to individual perturbation models, resulting in a sub-optimal worst-case loss over the union. In this work, we develop a natural generalization of the standard PGD-based procedure to incorporate multiple perturbation models into a single attack, by taking the worst-case over all steepest descent directions. This approach has the advantage of directly converging upon a trade-off between different perturbation models which minimizes the worst-case performance over the union. With this approach, we are able to train standard architectures which are simultaneously robust against \${\textbackslash}ell\_{\textbackslash}infty\$, \${\textbackslash}ell\_2\$, and \${\textbackslash}ell\_1\$ attacks, outperforming past approaches on the MNIST and CIFAR10 datasets and achieving adversarial accuracy of 47.0\% against the union of (\${\textbackslash}ell\_{\textbackslash}infty\$, \${\textbackslash}ell\_2\$, \${\textbackslash}ell\_1\$) perturbations with radius = (0.03, 0.5, 12) on the latter, improving upon previous approaches which achieve 40.6\% accuracy.},
	urldate = {2020-09-14},
	journal = {arXiv:1909.04068 [cs, stat]},
	author = {Maini, Pratyush and Wong, Eric and Kolter, J. Zico},
	month = jul,
	year = {2020},
	note = {arXiv: 1909.04068},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICML 2020 Final Version},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\A978LF6Q\\Maini et al. - 2020 - Adversarial Robustness Against the Union of Multip.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\RLCFXCVK\\1909.html:text/html}
}

@article{jia_certified_2019,
	title = {Certified {Robustness} to {Adversarial} {Word} {Substitutions}},
	
	abstract = {State-of-the-art NLP models can often be fooled by adversaries that apply seemingly innocuous label-preserving transformations (e.g., paraphrasing) to input text. The number of possible transformations scales exponentially with text length, so data augmentation cannot cover all transformations of an input. This paper considers one exponentially large family of label-preserving transformations, in which every word in the input can be replaced with a similar word. We train the first models that are provably robust to all word substitutions in this family. Our training procedure uses Interval Bound Propagation (IBP) to minimize an upper bound on the worst-case loss that any combination of word substitutions can induce. To evaluate models' robustness to these transformations, we measure accuracy on adversarially chosen word substitutions applied to test examples. Our IBP-trained models attain \$75{\textbackslash}\%\$ adversarial accuracy on both sentiment analysis on IMDB and natural language inference on SNLI. In comparison, on IMDB, models trained normally and ones trained with data augmentation achieve adversarial accuracy of only \$8{\textbackslash}\%\$ and \$35{\textbackslash}\%\$, respectively.},
	urldate = {2020-09-16},
	journal = {arXiv:1909.00986 [cs]},
	author = {Jia, Robin and Raghunathan, Aditi and Göksel, Kerem and Liang, Percy},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.00986},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: EMNLP 2019},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\H9DJMK5R\\Jia et al. - 2019 - Certified Robustness to Adversarial Word Substitut.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\C57S4YHY\\1909.html:text/html}
}


@book{mohri2018foundations,
  title={Foundations of Machine Learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}

@misc{robustness,
   title={Robustness (Python Library)},
   author={Logan Engstrom and Andrew Ilyas and Hadi Salman and Shibani Santurkar and Dimitris Tsipras},
   year={2019},
   url={https://github.com/MadryLab/robustness}
}

@book{bertsimas1997introduction,
  title={Introduction to linear optimization},
  author={Bertsimas, Dimitris and Tsitsiklis, John N},
  volume={6},
  year={1997},
  publisher={Athena Scientific Belmont, MA}
}

@article{bodlaender1990computational,
  title={Computational complexity of norm-maximization},
  author={Bodlaender, Hans L. and Gritzmann, Peter and Klee, Victor and Van Leeuwen, Jan},
  journal={Combinatorica},
  volume={10},
  number={2},
  pages={203--225},
  year={1990},
  publisher={Springer}
}

@article{santurkar_breeds_2020,
	title = {{BREEDS}: {Benchmarks} for {Subpopulation} {Shift}},
	shorttitle = {{BREEDS}},
	language = {en},
	urldate = {2020-12-20},
	author = {Santurkar, Shibani and Tsipras, Dimitris and Madry, Aleksander},
	month = aug,
	year = {2020},
}

@inproceedings{hendrycks_using_2019,
	title = {Using {Self}-{Supervised} {Learning} {Can} {Improve} {Model} {Robustness} and {Uncertainty}},
	volume = {32},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Hendrycks, Dan and Mazeika, Mantas and Kadavath, Saurav and Song, Dawn},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d{\textbackslash}textquotesingle and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {15663--15674}
}

@article{nagarajan2019generalization,
  title={Generalization in deep networks: The role of distance from initialization},
  author={Nagarajan, Vaishnavh and Kolter, J Zico},
  journal={arXiv preprint arXiv:1901.01672},
  year={2019}
}


@article{lyle_benefits_2020,
	title = {On the {Benefits} of {Invariance} in {Neural} {Networks}},
	url = {http://arxiv.org/abs/2005.00178},
	abstract = {Many real world data analysis problems exhibit invariant structure, and models that take advantage of this structure have shown impressive empirical performance, particularly in deep learning. While the literature contains a variety of methods to incorporate invariance into models, theoretical understanding is poor and there is no way to assess when one method should be preferred over another. In this work, we analyze the benefits and limitations of two widely used approaches in deep learning in the presence of invariance: data augmentation and feature averaging. We prove that training with data augmentation leads to better estimates of risk and gradients thereof, and we provide a PAC-Bayes generalization bound for models trained with data augmentation. We also show that compared to data augmentation, feature averaging reduces generalization error when used with convex losses, and tightens PAC-Bayes bounds. We provide empirical support of these theoretical results, including a demonstration of why generalization may not improve by training with data augmentation: the `learned invariance' fails outside of the training distribution.},
	urldate = {2021-02-17},
	journal = {arXiv:2005.00178 [cs, stat]},
	author = {Lyle, Clare and van der Wilk, Mark and Kwiatkowska, Marta and Gal, Yarin and Bloem-Reddy, Benjamin},
	month = apr,
	year = {2020},
	note = {arXiv: 2005.00178},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\69DUYEEW\\Lyle et al. - 2020 - On the Benefits of Invariance in Neural Networks.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\Y8PFH6X2\\2005.html:text/html}
}

@inproceedings{chen_group-theoretic_2020,
	title = {A {Group}-{Theoretic} {Framework} for {Data} {Augmentation}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/f4573fc71c731d5c362f0d7860945b88-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Shuxiao and Dobriban, Edgar and Lee, Jane},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {21321--21333},
	file = {NeurIPS-2020-a-group-theoretic-framework-for-data-augmentation-Paper.pdf:C\:\\Users\\Zhu\\Downloads\\Cached Papers\\NeurIPS-2020-a-group-theoretic-framework-for-data-augmentation-Paper.pdf:application/pdf;1907.10905.pdf:C\:\\Users\\Zhu\\Downloads\\Cached Papers\\1907.10905.pdf:application/pdf}
}

@article{bloem-reddy_probabilistic_2020,
	title = {Probabilistic symmetries and invariant neural networks},
	url = {http://arxiv.org/abs/1901.06082},
	abstract = {Treating neural network inputs and outputs as random variables, we characterize the structure of neural networks that can be used to model data that are invariant or equivariant under the action of a compact group. Much recent research has been devoted to encoding invariance under symmetry transformations into neural network architectures, in an effort to improve the performance of deep neural networks in data-scarce, non-i.i.d., or unsupervised settings. By considering group invariance from the perspective of probabilistic symmetry, we establish a link between functional and probabilistic symmetry, and obtain generative functional representations of probability distributions that are invariant or equivariant under the action of a compact group. Our representations completely characterize the structure of neural networks that can be used to model such distributions and yield a general program for constructing invariant stochastic or deterministic neural networks. We demonstrate that examples from the recent literature are special cases, and develop the details of the general program for exchangeable sequences and arrays.},
	urldate = {2021-02-17},
	journal = {arXiv:1901.06082 [cs, stat]},
	author = {Bloem-Reddy, Benjamin and Teh, Yee Whye},
	month = sep,
	year = {2020},
	note = {arXiv: 1901.06082},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Revised structure for clarity; fixed minor mistakes; incorporated reviewer feedback for publication},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\QA9FNZBJ\\Bloem-Reddy and Teh - 2020 - Probabilistic symmetries and invariant neural netw.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\9ZVRRNRA\\1901.html:text/html}
}

@article{anselmi_invariance_2015,
	title = {On {Invariance} and {Selectivity} in {Representation} {Learning}},
	url = {http://arxiv.org/abs/1503.05938},
	abstract = {We discuss data representation which can be learned automatically from data, are invariant to transformations, and at the same time selective, in the sense that two points have the same representation only if they are one the transformation of the other. The mathematical results here sharpen some of the key claims of i-theory -- a recent theory of feedforward processing in sensory cortex.},
	urldate = {2021-02-17},
	journal = {arXiv:1503.05938 [cs]},
	author = {Anselmi, Fabio and Rosasco, Lorenzo and Poggio, Tomaso},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.05938},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\CKBTVULY\\Anselmi et al. - 2015 - On Invariance and Selectivity in Representation Le.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\YSF3J8YS\\1503.html:text/html}
}

@article{anselmi_symmetry-adapted_2019,
	title = {Symmetry-adapted representation learning},
	volume = {86},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320318302620},
	doi = {10.1016/j.patcog.2018.07.025},
	abstract = {In this paper, we propose the use of data symmetries, in the sense of equivalences under signal transformations, as priors for learning symmetry-adapted data representations, i.e., representations that are equivariant to these transformations. We rely on a group-theoretic definition of equivariance and provide conditions for enforcing a learned representation, for example the weights in a neural network layer or the atoms in a dictionary, to have the structure of a group and specifically the group structure in the distribution of the input. By reducing the analysis of generic group symmetries to permutation symmetries, we devise a regularization scheme for representation learning algorithm, using an unlabeled training set. The proposed regularization is aimed to be a conceptual, theoretical and computational proof of concept for symmetry-adapted representation learning, where the learned data representations are equivariant or invariant to transformations, without explicit knowledge of the underlying symmetries in the data.},
	language = {en},
	urldate = {2021-02-17},
	journal = {Pattern Recognition},
	author = {Anselmi, Fabio and Evangelopoulos, Georgios and Rosasco, Lorenzo and Poggio, Tomaso},
	month = feb,
	year = {2019},
	keywords = {Convolutional neural networks, Data transformations, Dictionary learning, Equivariant representations, Invariant representations, Regularization, Representation learning},
	pages = {201--208},
	file = {ScienceDirect Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\2NNHJ9SK\\S0031320318302620.html:text/html;Symmetry-adapted representation learning.pdf:C\:\\Users\\Zhu\\Zotero\\storage\\2NNHJ9SK\\Symmetry-adapted representation learning.pdf:application/pdf}
}

@article{anselmi_unsupervised_2014,
	title = {Unsupervised {Learning} of {Invariant} {Representations} in {Hierarchical} {Architectures}},
	url = {http://arxiv.org/abs/1311.4158},
	abstract = {The present phase of Machine Learning is characterized by supervised learning algorithms relying on large sets of labeled examples (n → ∞). The next phase is likely to focus on algorithms capable of learning from very few labeled examples (n → 1), like humans seem able to do. We propose an approach to this problem and describe the underlying theory, based on the unsupervised, automatic learning of a “good” representation for supervised learning, characterized by small sample complexity (n). We consider the case of visual object recognition though the theory applies to other domains. The starting point is the conjecture, proved in speciﬁc cases, that image representations which are invariant to translations, scaling and other transformations can considerably reduce the sample complexity of learning. We prove that an invariant and unique (discriminative) signature can be computed for each image patch, I, in terms of empirical distributions of the dot-products between I and a set of templates stored during unsupervised learning. A module performing ﬁltering and pooling, like the simple and complex cells described by Hubel and Wiesel, can compute such estimates. Hierarchical architectures consisting of this basic Hubel-Wiesel moduli inherit its properties of invariance, stability, and discriminability while capturing the compositional organization of the visual world in terms of wholes and parts. The theory extends existing deep learning convolutional architectures for image and speech recognition. It also suggests that the main computational goal of the ventral stream of visual cortex is to provide a hierarchical representation of new objects/images which is invariant to transformations, stable, and discriminative for recognition—and that this representation may be continuously learned in an unsupervised way during development and visual experience.},
	language = {en},
	urldate = {2021-02-17},
	journal = {arXiv:1311.4158 [cs]},
	author = {Anselmi, Fabio and Leibo, Joel Z. and Rosasco, Lorenzo and Mutch, Jim and Tacchetti, Andrea and Poggio, Tomaso},
	month = mar,
	year = {2014},
	note = {arXiv: 1311.4158},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 23 pages, 10 figures. November 21 2013: Added acknowledgment of NSF funding. No other changes. December 18 (2013): Fixed a figure. January 10 (2014): Fixed a figure and some math in SI. March 10 2014: modified abstract and implementation section (main and SI); added a paragraph about sample complexity in SI},
	file = {Anselmi et al. - 2014 - Unsupervised Learning of Invariant Representations.pdf:C\:\\Users\\Zhu\\Zotero\\storage\\9V9ECKVB\\Anselmi et al. - 2014 - Unsupervised Learning of Invariant Representations.pdf:application/pdf}
}

@article{elesedy_provably_2021,
	title = {Provably {Strict} {Generalisation} {Benefit} for {Equivariant} {Models}},
	url = {http://arxiv.org/abs/2102.10333},
	abstract = {It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.},
	urldate = {2021-02-24},
	journal = {arXiv:2102.10333 [cs, stat]},
	author = {Elesedy, Bryn and Zaidi, Sheheryar},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.10333},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\E9FYGAAK\\Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\9DPWFA2E\\2102.html:text/html}
}

@article{sannai_improved_2020,
	title = {Improved {Generalization} {Bound} of {Group} {Invariant} / {Equivariant} {Deep} {Networks} via {Quotient} {Feature} {Space}},
	url = {http://arxiv.org/abs/1910.06552},
	abstract = {A large number of group invariant (or equivariant) networks have succeeded in handling invariant data such as point clouds and graphs. However, generalization theory for the networks has not been well developed, because several essential factors for generalization theory, such as size and margin distribution, are not very suitable to explain invariance and equivariance. In this paper, we develop a generalization error bound for invariant and equivariant deep neural networks. To describe the effect of the properties on generalization, we develop a quotient feature space, which measures the effect of group action for invariance or equivariance. Our main theorem proves that the volume of quotient feature spaces largely improves the main term of the developed bound. We apply our result to a specific invariant and equivariant networks, such as DeepSets (Zaheer et al. (2017)), then show that their generalization bound is drastically improved by \${\textbackslash}sqrt\{n!\}\$ where \$n\$ is a number of permuting coordinates of data. Moreover, we additionally discuss the representation power of invariant DNNs, and show that they can achieve an optimal approximation rate. This paper is the first study to provide a general and tight generalization bound for a broad class of group invariant and equivariant deep neural networks.},
	urldate = {2021-02-24},
	journal = {arXiv:1910.06552 [cs, stat]},
	author = {Sannai, Akiyoshi and Imaizumi, Masaaki},
	month = mar,
	year = {2020},
	note = {arXiv: 1910.06552},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Old title: "Improved Generalization Bound of Permutation Invariant Deep Neural Networks"},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\2RRB28WT\\Sannai and Imaizumi - 2020 - Improved Generalization Bound of Group Invariant .pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\D9D39QTF\\1910.html:text/html}
}

@article{sokolic_generalization_2017,
	title = {Generalization {Error} of {Invariant} {Classifiers}},
	url = {http://arxiv.org/abs/1610.04574},
	abstract = {This paper studies the generalization error of invariant classifiers. In particular, we consider the common scenario where the classification task is invariant to certain transformations of the input, and that the classifier is constructed (or learned) to be invariant to these transformations. Our approach relies on factoring the input space into a product of a base space and a set of transformations. We show that whereas the generalization error of a non-invariant classifier is proportional to the complexity of the input space, the generalization error of an invariant classifier is proportional to the complexity of the base space. We also derive a set of sufficient conditions on the geometry of the base space and the set of transformations that ensure that the complexity of the base space is much smaller than the complexity of the input space. Our analysis applies to general classifiers such as convolutional neural networks. We demonstrate the implications of the developed theory for such classifiers with experiments on the MNIST and CIFAR-10 datasets.},
	urldate = {2021-02-24},
	journal = {arXiv:1610.04574 [cs, stat]},
	author = {Sokolic, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel R. D.},
	month = jul,
	year = {2017},
	note = {arXiv: 1610.04574},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted to AISTATS. This version has updated references},
	file = {arXiv Fulltext PDF:C\:\\Users\\Zhu\\Zotero\\storage\\6SPHALJL\\Sokolic et al. - 2017 - Generalization Error of Invariant Classifiers.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\W3W8FFJI\\1610.html:text/html}
}

@inproceedings{cohen_group_2016,
	title = {Group {Equivariant} {Convolutional} {Networks}},
	url = {http://proceedings.mlr.press/v48/cohenc16.html},
	abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use ...},
	language = {en},
	urldate = {2021-05-18},
	booktitle = {International {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Cohen, Taco and Welling, Max},
	month = jun,
	year = {2016},
	note = {ISSN: 1938-7228},
	pages = {2990--2999},
	file = {Snapshot:C\:\\Users\\Zhu\\Zotero\\storage\\R7MRKAY9\\cohenc16.html:text/html;Full Text PDF:C\:\\Users\\Zhu\\Zotero\\storage\\DRCSWVAM\\Cohen and Welling - 2016 - Group Equivariant Convolutional Networks.pdf:application/pdf}
}

@article{abu-mostafa_hints_1993,
	title = {Hints and the {VC} {Dimension}},
	volume = {5},
	issn = {0899-7667},
	url = {https://doi.org/10.1162/neco.1993.5.2.278},
	doi = {10.1162/neco.1993.5.2.278},
	abstract = {Learning from hints is a generalization of learning from examples that allows for a variety of information about the unknown function to be used in the learning process. In this paper, we use the VC dimension, an established tool for analyzing learning from examples, to analyze learning from hints. In particular, we show how the VC dimension is affected by the introduction of a hint. We also derive a new quantity that defines a VC dimension for the hint itself. This quantity is used to estimate the number of examples needed to "absorb" the hint. We carry out the analysis for two types of hints, invariances and catalysts. We also describe how the same method can be applied to other types of hints.},
	number = {2},
	journal = {Neural Comput.},
	author = {Abu-Mostafa, Yaser S.},
	month = mar,
	year = {1993},
	note = {Place: Cambridge, MA, USA
Publisher: MIT Press},
	pages = {278--288}
}

@inproceedings{Mroueh_Voinea_Poggio_2015, title={Learning with Group Invariant Features: A Kernel Perspective.}, volume={28}, url={https://proceedings.neurips.cc/paper/2015/file/6602294be910b1e3c4571bd98c4d5484-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Mroueh, Youssef and Voinea, Stephen and Poggio, Tomaso A}, editor={Cortes, C. and Lawrence, N. and Lee, D. and Sugiyama, M. and Garnett, R.}, year={2015} }

@article{park2009simple,
  title={A simple and fast algorithm for K-medoids clustering},
  author={Park, Hae-Sang and Jun, Chi-Hyuck},
  journal={Expert systems with applications},
  volume={36},
  number={2},
  pages={3336--3341},
  year={2009},
  publisher={Elsevier}
}

@article{chang2015shapenet,
  title={Shapenet: An information-rich 3d model repository},
  author={Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and others},
  journal={arXiv preprint arXiv:1512.03012},
  year={2015}
}

@inproceedings{r2n2,
  author    = {Christopher B. Choy and
               Danfei Xu and
               JunYoung Gwak and
               Kevin Chen and
               Silvio Savarese},
  editor    = {Bastian Leibe and
               Jiri Matas and
               Nicu Sebe and
               Max Welling},
  title     = {3D-R2N2: {A} Unified Approach for Single and Multi-view 3D Object
               Reconstruction},
  booktitle = {Computer Vision - {ECCV} 2016 - 14th European Conference, Amsterdam,
               The Netherlands, October 11-14, 2016, Proceedings, Part {VIII}},
  series    = {Lecture Notes in Computer Science},
  volume    = {9912},
  pages     = {628--644},
  publisher = {Springer},
  year      = {2016},
  url       = {https://doi.org/10.1007/978-3-319-46484-8\_38},
  doi       = {10.1007/978-3-319-46484-8\_38},
  timestamp = {Mon, 24 Feb 2020 15:00:26 +0100},
  biburl    = {https://dblp.org/rec/conf/eccv/ChoyXGCS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{mohri_2012_foundations, title={Foundations of Machine Learning}, ISBN={0-262-01825-X}, publisher={The MIT Press}, author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet}, year={2012} }

 @inproceedings{Dudley_1984, place={Berlin, Heidelberg}, title={A course on empirical processes}, ISBN={978-3-540-39109-8}, booktitle={École d’Été de Probabilités de Saint-Flour XII - 1982}, publisher={Springer Berlin Heidelberg}, author={Dudley, R. M.}, editor={Hennequin, P. L.}, year={1984}, pages={1–142} }

 @article{Zhang_2002, title={Covering Number Bounds of Certain Regularized Linear Function Classes}, volume={2}, ISSN={1532-4435}, DOI={10.1162/153244302760200713}, abstractNote={Recently, sample complexity bounds have been derived for problems involving linear functions such as neural networks and support vector machines. In many of these theoretical studies, the concept of covering numbers played an important role. It is thus useful to study covering numbers for linear function classes. In this paper, we investigate two closely related methods to derive upper bounds on these covering numbers. The first method, already employed in some earlier studies, relies on the so-called Maurey’s lemma; the second method uses techniques from the mistake bound framework in online learning. We compare results from these two methods, as well as their consequences in some learning formulations.}, journal={J. Mach. Learn. Res.}, publisher={JMLR.org}, author={Zhang, Tong}, year={2002}, month={Mar}, pages={527–550} }

 @article{Winkels_Cohen_2018, title={3D G-CNNs for Pulmonary Nodule Detection.}, volume={abs/1804.04656}, url={http://dblp.uni-trier.de/db/journals/corr/corr1804.html#abs-1804-04656}, journal={CoRR}, author={Winkels, Marysia and Cohen, Taco S.}, year={2018} }

 @article{Edelman_1995, title={Class similarity and viewpoint invariance in the recognition of 3D objects}, volume={72}, ISSN={1432-0770}, DOI={10.1007/BF00201485}, number={3}, journal={Biological Cybernetics}, author={Edelman, Shimon}, year={1995}, month={Feb}, pages={207–220} }
 
 @article{Han_Roig_Geiger_Poggio_2020, title={Scale and translation-invariance for novel objects in human vision}, volume={10}, ISSN={2045-2322}, DOI={10.1038/s41598-019-57261-6}, number={1}, journal={Scientific Reports}, author={Han, Yena and Roig, Gemma and Geiger, Gad and Poggio, Tomaso}, year={2020}, month={Jan}, pages={1411} }

 @article{Anselmi_Leibo_Rosasco_Mutch_Tacchetti_Poggio_2014, title={Unsupervised Learning of Invariant Representations in Hierarchical Architectures}, url={http://arxiv.org/abs/1311.4158}, note={arXiv: 1311.4158}, journal={arXiv:1311.4158 [cs]}, author={Anselmi, Fabio and Leibo, Joel Z. and Rosasco, Lorenzo and Mutch, Jim and Tacchetti, Andrea and Poggio, Tomaso}, year={2014}, month={Mar} }

 @inproceedings{Jones_Jia_Raghunathan_Liang_2020, place={Online}, title={Robust Encodings: A Framework for Combating Adversarial Typos}, url={https://www.aclweb.org/anthology/2020.acl-main.245}, DOI={10.18653/v1/2020.acl-main.245}, booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics}, publisher={Association for Computational Linguistics}, author={Jones, Erik and Jia, Robin and Raghunathan, Aditi and Liang, Percy}, year={2020}, month={Jul}, pages={2752–2765} }
 
 @inproceedings{Pruthi_Dhingra_Lipton_2019, title={Combating Adversarial Misspellings with Robust Word Recognition.}, ISBN={978-1-950737-48-2}, url={http://dblp.uni-trier.de/db/conf/acl/acl2019-1.html#PruthiDL19}, booktitle={ACL (1)}, publisher={Association for Computational Linguistics}, author={Pruthi, Danish and Dhingra, Bhuwan and Lipton, Zachary C.}, editor={Korhonen, Anna and Traum, David R. and Màrquez, Lluís}, year={2019}, pages={5582–5591} }
 
 @inproceedings{Zhang_Zhao_LeCun_2015, title={Character-level Convolutional Networks for Text Classification}, url={http://dblp.uni-trier.de/db/conf/nips/nips2015.html#ZhangZL15}, booktitle={NIPS}, author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann}, editor={Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman}, year={2015}, pages={649–657} }

 @inproceedings{Cohen_Geiger_Köhler_Welling_2018, title={Spherical CNNs}, url={https://openreview.net/forum?id=Hkbd5xZRb}, booktitle={International Conference on Learning Representations}, author={Cohen, Taco S. and Geiger, Mario and Köhler, Jonas and Welling, Max}, year={2018} }
 
 @inproceedings{Cohen_Geiger_Weiler_2019, title={A General Theory of Equivariant CNNs on Homogeneous Spaces.}, url={http://dblp.uni-trier.de/db/conf/nips/nips2019.html#CohenGW19}, booktitle={NeurIPS}, author={Cohen, Taco S. and Geiger, Mario and Weiler, Maurice}, editor={Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d’Alché Buc, Florence and Fox, Emily B. and Garnett, Roman}, year={2019}, pages={9142–9153} }
 
 @inproceedings{Esteves_Xu_Allen-Blanchette_Daniilidis_2019, title={Equivariant Multi-View Networks.}, ISBN={978-1-72814-803-8}, url={http://dblp.uni-trier.de/db/conf/iccv/iccv2019.html#EstevesXAD19}, booktitle={ICCV}, publisher={IEEE}, author={Esteves, Carlos and Xu, Yinshuang and Allen-Blanchette, Christine and Daniilidis, Kostas}, year={2019}, pages={1568–1577} }
 
 @inproceedings{Gens_Domingos_2014, title={Deep Symmetry Networks.}, url={http://dblp.uni-trier.de/db/conf/nips/nips2014.html#GensD14}, booktitle={NIPS}, author={Gens, Robert and Domingos, Pedro M.}, editor={Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.}, year={2014}, pages={2537–2545} }
 
 @inproceedings{Weiler_Geiger_Welling_2018, title={3D Steerable CNNs: Learning Rotationally Equivariant Features in Volumetric Data}, volume={31}, url={https://proceedings.neurips.cc/paper/2018/file/488e4104520c6aab692863cc1dba45af-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Weiler, Maurice and Geiger, Mario and Welling, Max and Boomsma, Wouter and Cohen, Taco S}, editor={Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.}, year={2018} }
 
 @inproceedings{Zaheer_Kottur_Ravanbakhsh_2017, title={Deep Sets.}, url={http://dblp.uni-trier.de/db/conf/nips/nips2017.html#ZaheerKRPSS17}, booktitle={NIPS}, author={Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Póczos, Barnabás and Salakhutdinov, Ruslan and Smola, Alexander J.}, editor={Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman}, year={2017}, pages={3391–3401} }

 @inproceedings{Sagawa_Koh_Hashimoto_Liang_2020, title={Distributionally Robust Neural Networks}, url={https://openreview.net/forum?id=ryxGuJrFvS}, booktitle={International Conference on Learning Representations}, author={Sagawa*, Shiori and Koh*, Pang Wei and Hashimoto, Tatsunori B. and Liang, Percy}, year={2020} }

@article{Bartlett_Mendelson_2002, title={Rademacher and Gaussian complexities: Risk bounds and structural results}, volume={3}, number={Nov}, journal={Journal of Machine Learning Research}, author={Bartlett, Peter L and Mendelson, Shahar}, year={2002}, pages={463–482} }

@misc{k2021robustness,
      title={Robustness to Augmentations as a Generalization metric}, 
      author={Sumukh Aithal K and Dhruva Kashyap and Natarajan Subramanyam},
      year={2021},
      eprint={2101.06459},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{Dudley_1967, title={The sizes of compact subsets of Hilbert space and continuity of Gaussian processes}, volume={1}, ISSN={0022-1236}, DOI={https://doi.org/10.1016/0022-1236(67)90017-1}, number={3}, journal={Journal of Functional Analysis}, author={Dudley, R. M.}, year={1967}, pages={290–330} }

@article{cutout,
  author    = {Terrance Devries and
               Graham W. Taylor},
  title     = {Improved Regularization of Convolutional Neural Networks with Cutout},
  journal   = {CoRR},
  volume    = {abs/1708.04552},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.04552},
  eprinttype = {arXiv},
  eprint    = {1708.04552},
  timestamp = {Mon, 13 Aug 2018 16:45:57 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1708-04552.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Benton_Finzi_Izmailov_Wilson_2020, title={Learning Invariances in Neural Networks}, volume={33}, url={https://proceedings.neurips.cc/paper/2020/file/cc8090c4d2791cdd9cd2cb3c24296190-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Benton, Gregory and Finzi, Marc and Izmailov, Pavel and Wilson, Andrew G}, editor={Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.}, year={2020}, pages={17605–17616} }

@article{floyd1995sample,
  title={Sample compression, learnability, and the Vapnik-Chervonenkis dimension},
  author={Floyd, Sally and Warmuth, Manfred},
  journal={Machine learning},
  volume={21},
  number={3},
  pages={269--304},
  year={1995},
  publisher={Springer}
}


@InProceedings{golowich18a,
  title = 	 {Size-Independent  Sample Complexity of Neural Networks},
  author =       {Golowich, Noah and Rakhlin, Alexander and Shamir, Ohad},
  booktitle = 	 {Proceedings of the 31st  Conference On Learning Theory},
  pages = 	 {297--299},
  year = 	 {2018},
  editor = 	 {Bubeck, Sébastien and Perchet, Vianney and Rigollet, Philippe},
  volume = 	 {75},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v75/golowich18a/golowich18a.pdf},
  url = 	 {https://proceedings.mlr.press/v75/golowich18a.html},
}

@article{shorten2019survey,
  title={A survey on image data augmentation for deep learning},
  author={Shorten, Connor and Khoshgoftaar, Taghi M},
  journal={Journal of Big Data},
  volume={6},
  number={1},
  pages={1--48},
  year={2019},
  publisher={Springer}
}

@inproceedings{taylor2018improving,
  title={Improving deep learning with generic data augmentation},
  author={Taylor, Luke and Nitschke, Geoff},
  booktitle={2018 IEEE Symposium Series on Computational Intelligence (SSCI)},
  pages={1542--1547},
  year={2018},
  organization={IEEE}
}

@article{xie2019unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V},
  journal={arXiv preprint arXiv:1904.12848},
  year={2019}
}

@article{miyato2018virtual,
  title={Virtual adversarial training: a regularization method for supervised and semi-supervised learning},
  author={Miyato, Takeru and Maeda, Shin-ichi and Koyama, Masanori and Ishii, Shin},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={41},
  number={8},
  pages={1979--1993},
  year={2018},
  publisher={IEEE}
}

@article{floyd1962algorithm,
  title={Algorithm 97: shortest path},
  author={Floyd, Robert W},
  journal={Communications of the ACM},
  volume={5},
  number={6},
  pages={345},
  year={1962},
  publisher={ACM New York, NY, USA}
}

@article{dijkstra1959note,
  title={A note on two problems in connexion with graphs},
  author={Dijkstra, Edsger W},
  journal={Numerische mathematik},
  volume={1},
  number={1},
  pages={269--271},
  year={1959},
  publisher={Springer}
}

}

@book{har2011geometric,
  title={Geometric approximation algorithms},
  author={Har-Peled, Sariel},
  number={173},
  year={2011},
  publisher={American Mathematical Soc.}
}

@article{Jiang2020, title={NeurIPS 2020 Competition: Predicting Generalization in Deep Learning}, volume={abs/2012.07976}, url={https://arxiv.org/abs/2012.07976}, note={arXiv: 2012.07976}, journal={CoRR}, author={Jiang, Yiding and Foret, Pierre and Yak, Scott and Roy, Daniel M. and Mobahi, Hossein and Dziugaite, Gintare Karolina and Bengio, Samy and Gunasekar, Suriya and Guyon, Isabelle and Neyshabur, Behnam}, year={2020} }

@inproceedings{
robey2021modelbased,
title={Model-Based Domain Generalization},
author={Alexander Robey and George J. Pappas and Hamed Hassani},
booktitle={Advances in Neural Information Processing Systems},
editor={A. Beygelzimer and Y. Dauphin and P. Liang and J. Wortman Vaughan},
year={2021},
url={https://openreview.net/forum?id=JOxB9h40A-1}
}

@inproceedings{Xie2020, title={Unsupervised Data Augmentation for Consistency Training}, volume={33}, url={https://proceedings.neurips.cc/paper/2020/file/44feb0096faa8326192570788b38c1d1-Paper.pdf}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc}, editor={Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.}, year={2020}, pages={6256–6268} }
