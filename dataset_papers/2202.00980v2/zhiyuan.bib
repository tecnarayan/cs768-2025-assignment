@incollection{tropp2015convex,
  title={Convex recovery of a structured signal from independent random linear measurements},
  author={Tropp, Joel A},
  booktitle={Sampling Theory, a Renaissance},
  pages={67--101},
  year={2015},
  publisher={Springer}
}

@book{karatzas2014brownian,
  title={Brownian motion and stochastic calculus},
  author={Karatzas, Ioannis and Shreve, Steven},
  volume={113},
  year={2014},
  publisher={springer}
}

@book{billingsley2013convergence,
  title={Convergence of probability measures},
  author={Billingsley, Patrick},
  year={2013},
  publisher={John Wiley \& Sons}
}

@book{pollard1984convergence,
  title={Convergence of stochastic processes},
  author={Pollard, David},
  year={2012},
  publisher={Springer Science \& Business Media}
}

@article{katzenberger1991solutions,
  title={Solutions of a stochastic differential equation forced onto a manifold by a large drift},
  author={Katzenberger, Gary Shon},
  journal={The Annals of Probability},
  pages={1587--1628},
  year={1991},
  publisher={JSTOR}
}

@inproceedings{blanc2020implicit,
  title={Implicit regularization for deep neural networks driven by an ornstein-uhlenbeck like process},
  author={Blanc, Guy and Gupta, Neha and Valiant, Gregory and Valiant, Paul},
  booktitle={Conference on learning theory},
  pages={483--513},
  year={2020},
  organization={PMLR}
}

@article{haochen2020shape,
  title={Shape matters: Understanding the implicit bias of the noise covariance},
  author={HaoChen, Jeff Z and Wei, Colin and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:2006.08680},
  year={2020}
}

@inproceedings{Liu20,
  author    = {Liyuan Liu and
               Xiaodong Liu and
               Jianfeng Gao and
               Weizhu Chen and
               Jiawei Han},
  editor    = {Bonnie Webber and
               Trevor Cohn and
               Yulan He and
               Yang Liu},
  title     = {Understanding the Difficulty of Training Transformers},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2020, Online, November 16-20, 2020},
  pages     = {5747--5763},
  publisher = {Association for Computational Linguistics},
  year      = {2020},
}

@inproceedings{wu2020noisy,
  title={On the noisy gradient descent that generalizes as sgd},
  author={Wu, Jingfeng and Hu, Wenqing and Xiong, Haoyi and Huan, Jun and Braverman, Vladimir and Zhu, Zhanxing},
  booktitle={International Conference on Machine Learning},
  pages={10367--10376},
  year={2020},
  organization={PMLR}
}

@article{kahn1995probability,
  title={On the probability that a random$\pm$1-matrix is singular},
  author={Kahn, Jeff and Koml{\'o}s, J{\'a}nos and Szemer{\'e}di, Endre},
  journal={Journal of the American Mathematical Society},
  volume={8},
  number={1},
  pages={223--240},
  year={1995}
}

@article{chaudhari2019entropy,
  title={Entropy-sgd: Biasing gradient descent into wide valleys},
  author={Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer and Sagun, Levent and Zecchina, Riccardo},
  journal={Journal of Statistical Mechanics: Theory and Experiment},
  volume={2019},
  number={12},
  pages={124018},
  year={2019},
  publisher={IOP Publishing}
}

@article{cooper2018loss,
  title={The loss landscape of overparameterized neural networks},
  author={Cooper, Yaim},
  journal={arXiv preprint arXiv:1804.10200},
  year={2018}
}

@article{wu2017towards,
  title={Towards understanding generalization of deep learning: Perspective of loss landscapes},
  author={Wu, Lei and Zhu, Zhanxing and others},
  journal={arXiv preprint arXiv:1706.10239},
  year={2017}
}

@article{he2019asymmetric,
  title={Asymmetric valleys: Beyond sharp and flat local minima},
  author={He, Haowei and Huang, Gao and Yuan, Yang},
  journal={arXiv preprint arXiv:1902.00744},
  year={2019}
}

@article{cooper2020critical,
  title={The critical locus of overparameterized neural networks},
  author={Cooper, Y},
  journal={arXiv preprint arXiv:2005.04210},
  year={2020}
}

@article{li2018over,
  title={Over-parameterized deep neural networks have no strict local minima for any continuous activations},
  author={Li, Dawei and Ding, Tian and Sun, Ruoyu},
  journal={arXiv preprint arXiv:1812.11039},
  year={2018}
}

@article{liu2020toward,
  title={Toward a theory of optimization for over-parameterized systems of non-linear equations: the lessons of deep learning},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={arXiv preprint arXiv:2003.00307},
  year={2020}
}

@inproceedings{geyer2020low,
  title={Low-rank regularization and solution uniqueness in over-parameterized matrix sensing},
  author={Geyer, Kelly and Kyrillidis, Anastasios and Kalev, Amir},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={930--940},
  year={2020},
  organization={PMLR}
}

@inproceedings{nguyen2019connected,
  title={On connected sublevel sets in deep learning},
  author={Nguyen, Quynh},
  booktitle={International Conference on Machine Learning},
  pages={4790--4799},
  year={2019},
  organization={PMLR}
}

@article{venturi2018spurious,
  title={Spurious valleys in two-layer neural network optimization landscapes},
  author={Venturi, Luca and Bandeira, Afonso S and Bruna, Joan},
  journal={arXiv preprint arXiv:1802.06384},
  year={2018}
}

@article{kuditipudi2019explaining,
  title={Explaining landscape connectivity of low-cost solutions for multilayer nets},
  author={Kuditipudi, Rohith and Wang, Xiang and Lee, Holden and Zhang, Yi and Li, Zhiyuan and Hu, Wei and Arora, Sanjeev and Ge, Rong},
  journal={arXiv preprint arXiv:1906.06247},
  year={2019}
}

@article{garipov2018loss,
  title={Loss surfaces, mode connectivity, and fast ensembling of dnns},
  author={Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  journal={arXiv preprint arXiv:1802.10026},
  year={2018}
}

@inproceedings{draxler2018essentially,
  title={Essentially no barriers in neural network energy landscape},
  author={Draxler, Felix and Veschgini, Kambis and Salmhofer, Manfred and Hamprecht, Fred},
  booktitle={International conference on machine learning},
  pages={1309--1318},
  year={2018},
  organization={PMLR}
}

@article{nguyen2018loss,
  title={On the loss landscape of a class of deep neural networks with no bad local valleys},
  author={Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  journal={arXiv preprint arXiv:1809.10749},
  year={2018}
}

@article{freeman2016topology,
  title={Topology and geometry of half-rectified network optimization},
  author={Freeman, C Daniel and Bruna, Joan},
  journal={arXiv preprint arXiv:1611.01540},
  year={2016}
}

@inproceedings{liang2018understanding,
  title={Understanding the loss surface of neural networks for binary classification},
  author={Liang, Shiyu and Sun, Ruoyu and Li, Yixuan and Srikant, Rayadurgam},
  booktitle={International Conference on Machine Learning},
  pages={2835--2843},
  year={2018},
  organization={PMLR}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={arXiv preprint arXiv:1905.13655},
  year={2019}
}

@inproceedings{gunasekar2018implicit,
  title={Implicit regularization in matrix factorization},
  author={Gunasekar, Suriya and Woodworth, Blake and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  booktitle={2018 Information Theory and Applications Workshop (ITA)},
  pages={1--10},
  year={2018},
  organization={IEEE}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@article{soudry2018implicit,
  title={The implicit bias of gradient descent on separable data},
  author={Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan},
  journal={The Journal of Machine Learning Research},
  volume={19},
  number={1},
  pages={2822--2878},
  year={2018},
  publisher={JMLR. org}
}

@article{keskar2016large,
  title={On large-batch training for deep learning: Generalization gap and sharp minima},
  author={Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  journal={arXiv preprint arXiv:1609.04836},
  year={2016}
}

@inproceedings{li2018algorithmic,
  title={Algorithmic regularization in over-parameterized matrix sensing and neural networks with quadratic activations},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang},
  booktitle={Conference On Learning Theory},
  pages={2--47},
  year={2018},
  organization={PMLR}
}

@incollection{lecun2012efficient,
  title={Efficient backprop},
  author={LeCun, Yann A and Bottou, L{\'e}on and Orr, Genevieve B and M{\"u}ller, Klaus-Robert},
  booktitle={Neural networks: Tricks of the trade},
  pages={9--48},
  year={2012},
  publisher={Springer}
}

@article{li2019generalization,
  title={On generalization error bounds of noisy gradient methods for non-convex learning},
  author={Li, Jian and Luo, Xuanyuan and Qiao, Mingda},
  journal={arXiv preprint arXiv:1902.00621},
  year={2019}
}

@article{li2019towards,
  title={Towards explaining the regularization effect of initial large learning rate in training neural networks},
  author={Li, Yuanzhi and Wei, Colin and Ma, Tengyu},
  journal={arXiv preprint arXiv:1907.04595},
  year={2019}
}

@article{hoffer2017train,
  title={Train longer, generalize better: closing the generalization gap in large batch training of neural networks},
  author={Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1705.08741},
  year={2017}
}

@article{jastrzkebski2017three,
  title={Three factors influencing minima in sgd},
  author={Jastrzebski, Stanislaw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  journal={arXiv preprint arXiv:1711.04623},
  year={2017}
}

@inproceedings{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1594--1602},
  year={2015}
}

@article{Huang2017ProjectionBW,
  title={Projection Based Weight Normalization for Deep Neural Networks},
  author={Lei Huang and Xianglong Liu and Bo Lang and Bo Li},
  journal={ArXiv},
  year={2017},
  volume={abs/1710.02338}
}

@article{levy2016power,
	title={The power of normalization: Faster evasion of saddle points},
	author={Levy, Kfir Y},
	journal={arXiv preprint arXiv:1611.04831},
	year={2016}
}


@InProceedings{pascanu13,
  title = 	 {On the difficulty of training recurrent neural networks},
  author = 	 {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the 30th International Conference on Machine Learning},
  pages = 	 {1310--1318},
  year = 	 {2013},
  editor = 	 {Dasgupta, Sanjoy and McAllester, David},
  volume = 	 {28},
  number =       {3},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Atlanta, Georgia, USA},
  month = 	 {17--19 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v28/pascanu13.pdf},
  url = 	 {https://proceedings.mlr.press/v28/pascanu13.html},
}

@inproceedings{ZhangHSJ20,
  author    = {Jingzhao Zhang and
               Tianxing He and
               Suvrit Sra and
               Ali Jadbabaie},
  title     = {Why Gradient Clipping Accelerates Training: {A} Theoretical Justification
               for Adaptivity},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
}


@article{Chen20,
  author    = {Xiangyi Chen and
               Zhiwei Steven Wu and
               Mingyi Hong},
  title     = {Understanding Gradient Clipping in Private {SGD:} {A} Geometric Perspective},
  journal   = {CoRR},
  volume    = {abs/2006.15429},
  year      = {2020},
  url       = {https://arxiv.org/abs/2006.15429},
  eprinttype = {arXiv},
  eprint    = {2006.15429},
}

@inproceedings{You2020lamb,
  author    = {Yang You and
               Jing Li and
               Sashank J. Reddi and
               Jonathan Hseu and
               Sanjiv Kumar and
               Srinadh Bhojanapalli and
               Xiaodan Song and
               James Demmel and
               Kurt Keutzer and
               Cho{-}Jui Hsieh},
  title     = {Large Batch Optimization for Deep Learning: Training {BERT} in 76
               minutes},
  booktitle = {8th International Conference on Learning Representations, {ICLR} 2020,
               Addis Ababa, Ethiopia, April 26-30, 2020},
  publisher = {OpenReview.net},
  year      = {2020},
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}


@article{zhu2018anisotropic,
  title={The anisotropic noise in stochastic gradient descent: Its behavior of escaping from sharp minima and regularization effects},
  author={Zhu, Zhanxing and Wu, Jingfeng and Yu, Bing and Wu, Lei and Ma, Jinwen},
  journal={arXiv preprint arXiv:1803.00195},
  year={2018}
}

@article{wen2019interplay,
  title={Interplay between optimization and generalization of stochastic gradient descent with covariance noise},
  author={Wen, Yeming and Luk, Kevin and Gazeau, Maxime and Zhang, Guodong and Chan, Harris and Ba, Jimmy},
  journal={arXiv preprint arXiv:1902.08234},
  year={2019}
}

@inproceedings{li2017stochastic,
  title={Stochastic modified equations and adaptive stochastic gradient algorithms},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  booktitle={International Conference on Machine Learning},
  pages={2101--2110},
  year={2017},
  organization={PMLR}
}

@inproceedings{cheng2020stochastic,
  title={Stochastic gradient and langevin processes},
  author={Cheng, Xiang and Yin, Dong and Bartlett, Peter and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={1810--1819},
  year={2020},
  organization={PMLR}
}

@article{li2020reconciling,
  title={Reconciling Modern Deep Learning with Traditional Optimization Analyses: The Intrinsic Learning Rate},
  author={Li, Zhiyuan and Lyu, Kaifeng and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{li2021validity,
  title={On the Validity of Modeling SGD with Stochastic Differential Equations (SDEs)},
  author={Li, Zhiyuan and Malladi, Sadhika and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2102.12470},
  year={2021}
}

@article{krizhevsky2014one,
  title={One weird trick for parallelizing convolutional neural networks},
  author={Krizhevsky, Alex},
  journal={arXiv preprint arXiv:1404.5997},
  year={2014}
}

@article{goyal2017accurate,
  title={Accurate, large minibatch sgd: Training imagenet in 1 hour},
  author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  journal={arXiv preprint arXiv:1706.02677},
  year={2017}
}

@article{xie2020diffusion,
  title={A diffusion theory for deep learning dynamics: Stochastic gradient descent escapes from sharp minima exponentially fast},
  author={Xie, Zeke and Sato, Issei and Sugiyama, Masashi},
  journal={arXiv preprint arXiv:2002.03495},
  year={2020}
}

@article{fehrman2020convergence,
  title={Convergence rates for the stochastic gradient descent method for non-convex objective functions},
  author={Fehrman, Benjamin and Gess, Benjamin and Jentzen, Arnulf},
  journal={Journal of Machine Learning Research},
  volume={21},
  year={2020},
  publisher={MICROTOME PUBL}
}

@article{shi2020learning,
  title={On learning rates and schr$\backslash$" odinger operators},
  author={Shi, Bin and Su, Weijie J and Jordan, Michael I},
  journal={arXiv preprint arXiv:2004.06977},
  year={2020}
}

@book{whitt2002stochastic,
  title={Stochastic-process limits: an introduction to stochastic-process limits and their application to queues},
  author={Whitt, Ward},
  year={2002},
  publisher={Springer Science \& Business Media}
}

@article{Lee_Panageas_Piliouras_Simchowitz_Jordan_Recht_2019, title={First-order methods almost always avoid strict saddle points}, volume={176}, ISSN={0025-5610}, DOI={10.1007/s10107-019-01374-3}, abstractNote={We establish that first-order methods avoid strict saddle points for almost all initializations. Our results apply to a wide variety of first-order methods, including (manifold) gradient descent, block coordinate descent, mirror descent and variants thereof. The connecting thread is that such algorithms can be studied from a dynamical systems perspective in which appropriate instantiations of the Stable Manifold Theorem allow for a global stability analysis. Thus, neither access to second-order derivative information nor randomness beyond initialization is necessary to provably avoid strict saddle points.}, number={1–2}, journal={Mathematical Programming}, author={Lee, Jason D. and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin}, year={2019}, pages={311–337} }


@inproceedings{wei2019regularization,
 author = {Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
 url = {https://proceedings.neurips.cc/paper/2019/file/8744cf92c88433f8cb04a02e6db69a0d-Paper.pdf},
 volume = {32},
 year = {2019}
}

@book{jacod2013limit,
  title={Limit theorems for stochastic processes},
  author={Jacod, Jean and Shiryaev, Albert},
  volume={288},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{Falconer_1983, 
title={Differentiation of the Limit Mapping in a Dynamical System}, volume={s2-27}, ISSN={0024-6107}, DOI={10.1112/jlms/s2-27.2.356}, number={2}, journal={Journal of the London Mathematical Society}, author={Falconer, K. J.}, year={1983}, pages={356–372} }

@inproceedings{woodworth2020kernel,
  title={Kernel and rich regimes in overparametrized models},
  author={Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle={Conference on Learning Theory},
  pages={3635--3673},
  year={2020},
  organization={PMLR}
}

@article{damian2021label,
  title={Label Noise SGD Provably Prefers Flat Global Minimizers},
  author={Damian, Alex and Ma, Tengyu and Lee, Jason},
  journal={arXiv preprint arXiv:2106.06530},
  year={2021}
}

@article{li2019stochastic,
  title={Stochastic modified equations and dynamics of stochastic gradient algorithms i: Mathematical foundations},
  author={Li, Qianxiao and Tai, Cheng and Weinan, E},
  journal={The Journal of Machine Learning Research},
  volume={20},
  number={1},
  pages={1474--1520},
  year={2019},
  publisher={JMLR. org}
}

@book{do2013riemannian,
  title={Riemannian geometry},
  author={Do Carmo, Manfredo P},
  year={2013},
  publisher={Springer Science \& Business Media}
}


@article{holbrook2018differentiating,
  title={Differentiating the pseudo determinant},
  author={Holbrook, Andrew},
  journal={Linear Algebra and its Applications},
  volume={548},
  pages={293--304},
  year={2018},
  publisher={Elsevier}
}

@book{banyaga2013lectures,
  title={Lectures on Morse homology},
  author={Banyaga, Augustin and Hurtubise, David},
  volume={29},
  year={2013},
  publisher={Springer Science \& Business Media}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={arXiv preprint arXiv:1806.07572},
  year={2018}
}

@article{li2020convolutional,
  title={Why Are Convolutional Nets More Sample-Efficient than Fully-Connected Nets?},
  author={Li, Zhiyuan and Zhang, Yi and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2010.08515},
  year={2020}
}

@article{polyak1987introduction,
  title={Introduction to Optimization. Optimization Software},
  author={Polyak, Boris T},
  journal={Inc., Publications Division, New York},
  volume={1},
  year={1987}
}

@article{duchi2011adaptive,
  title={Adaptive subgradient methods for online learning and stochastic optimization},
  author={Duchi, John and Hazan, Elad and Singer, Yoram},
  journal={Journal of Machine Learning Research},
  volume={12},
  number={Jul},
  pages={2121--2159},
  year={2011}
}

@inproceedings{Zhang20,
  author    = {Jingzhao Zhang and
               Sai Praneeth Karimireddy and
               Andreas Veit and
               Seungyeon Kim and
               Sashank J. Reddi and
               Sanjiv Kumar and
               Suvrit Sra},
  editor    = {Hugo Larochelle and
               Marc'Aurelio Ranzato and
               Raia Hadsell and
               Maria{-}Florina Balcan and
               Hsuan{-}Tien Lin},
  title     = {Why are Adaptive Methods Good for Attention Models?},
  booktitle = {Advances in Neural Information Processing Systems 33: Annual Conference
               on Neural Information Processing Systems 2020, NeurIPS 2020, December
               6-12, 2020, virtual},
  year      = {2020},
}

@inproceedings{ng2004feature,
  title={Feature selection, L 1 vs. L 2 regularization, and rotational invariance},
  author={Ng, Andrew Y},
  booktitle={Proceedings of the twenty-first international conference on Machine learning},
  pages={78},
  year={2004}
}

@article{lee2017first,
  title={First-order methods almost always avoid saddle points},
  author={Lee, Jason D and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  journal={arXiv preprint arXiv:1710.07406},
  year={2017}
}

@inproceedings{lee2016gradient,
  title={Gradient descent only converges to minimizers},
  author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
  booktitle={Conference on learning theory},
  pages={1246--1257},
  year={2016},
  organization={PMLR}
}

@article{polyak1964gradient,
  title={Gradient methods for solving equations and inequalities},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={4},
  number={6},
  pages={17--32},
  year={1964},
  publisher={Elsevier}
}

@article{lojasiewicz1963topological,
  title={A topological property of real analytic subsets},
  author={Lojasiewicz, Stanislaw},
  journal={Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  number={87-89},
  pages={2},
  year={1963}
}

@article{lyu2019gradient,
  title={Gradient descent maximizes the margin of homogeneous neural networks},
  author={Lyu, Kaifeng and Li, Jian},
  journal={arXiv preprint arXiv:1906.05890},
  year={2019}
}

@inproceedings{liu2020understanding,
  title={Understanding the Difficulty of Training Transformers},
  author={Liu, Liyuan and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu and Han, Jiawei},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={5747--5763},
  year={2020}
}

@article{rajpurkar2018know,
  title={Know what you don't know: Unanswerable questions for SQuAD},
  author={Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  journal={arXiv preprint arXiv:1806.03822},
  year={2018}
}

@article{rajpurkar2016squad,
  title={Squad: 100,000+ questions for machine comprehension of text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  journal={arXiv preprint arXiv:1606.05250},
  year={2016}
}

@InProceedings{N18-1101,
  author = "Williams, Adina
            and Nangia, Nikita
            and Bowman, Samuel",
  title = "A Broad-Coverage Challenge Corpus for
           Sentence Understanding through Inference",
  booktitle = "Proceedings of the 2018 Conference of
               the North American Chapter of the
               Association for Computational Linguistics:
               Human Language Technologies, Volume 1 (Long
               Papers)",
  year = "2018",
  publisher = "Association for Computational Linguistics",
  pages = "1112--1122",
  location = "New Orleans, Louisiana",
}

@article{raskutti2012minimax,
  title={Minimax-Optimal Rates For Sparse Additive Models Over Kernel Classes Via Convex Programming.},
  author={Raskutti, Garvesh and J Wainwright, Martin and Yu, Bin},
  journal={Journal of machine learning research},
  volume={13},
  number={2},
  year={2012}
}
@inproceedings{arora2018optimization,
  title={On the optimization of deep networks: Implicit acceleration by overparameterization},
  author={Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  booktitle={International Conference on Machine Learning},
  pages={244--253},
  year={2018},
  organization={PMLR}
}

@inproceedings{li2020towards,
  title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning},
  author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{vaskevicius2019implicit,
  title={Implicit regularization for optimal sparse recovery},
  author={Vaskevicius, Tomas and Kanade, Varun and Rebeschini, Patrick},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  pages={2972--2983},
  year={2019}
}

@article{zhao2019implicit,
  title={Implicit regularization via Hadamard product over-parametrization in high-dimensional linear regression},
  author={Zhao, Peng and Yang, Yun and He, Qiao-Chu},
  journal={arXiv preprint arXiv:1903.09367},
  year={2019}
}

@article{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  journal={arXiv preprint arXiv:1808.01204},
  year={2018}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{daniely2017sgd,
  title={SGD learns the conjugate kernel class of the network},
  author={Daniely, Amit},
  journal={arXiv preprint arXiv:1702.08503},
  year={2017}
}

@article{yang2019scaling,
  title={Scaling limits of wide neural networks with weight sharing: Gaussian process behavior, gradient independence, and neural tangent kernel derivation},
  author={Yang, Greg},
  journal={arXiv preprint arXiv:1902.04760},
  year={2019}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}
@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@article{chizat2018lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={arXiv preprint arXiv:1812.07956},
  year={2018}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International Conference on Machine Learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{azulay2021implicit,
  title={On the implicit bias of initialization shape: Beyond infinitesimal mirror descent},
  author={Azulay, Shahar and Moroshko, Edward and Nacson, Mor Shpigel and Woodworth, Blake and Srebro, Nathan and Globerson, Amir and Soudry, Daniel},
  journal={arXiv preprint arXiv:2102.09769},
  year={2021}
}


@inproceedings{Perko2001DifferentialEA,
  title={Differential Equations and Dynamical Systems},
  author={Lawrence M. Perko},
  year={2001}
}

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reddi2018convergence,
  title={On the convergence of {ADAM} and beyond},
  author={Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1904.09237},
  year={2019}
}

@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@inproceedings{zhang-etal-2019-improving,
    title = "Improving Deep Transformer with Depth-Scaled Initialization and Merged Attention",
    author = "Zhang, Biao  and
      Titov, Ivan  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1083",
    doi = "10.18653/v1/D19-1083",
    pages = "898--909",
    abstract = "The general trend in NLP is towards increasing model capacity and performance via deeper neural networks. However, simply stacking more layers of the popular Transformer architecture for machine translation results in poor convergence and high computational overhead. Our empirical analysis suggests that convergence is poor due to gradient vanishing caused by the interaction between residual connection and layer normalization. We propose depth-scaled initialization (DS-Init), which decreases parameter variance at the initialization stage, and reduces output variance of residual connections so as to ease gradient back-propagation through normalization layers. To address computational cost, we propose a merged attention sublayer (MAtt) which combines a simplified average-based self-attention sublayer and the encoder-decoder attention sublayer on the decoder side. Results on WMT and IWSLT translation tasks with five translation directions show that deep Transformers with DS-Init and MAtt can substantially outperform their base counterpart in terms of BLEU (+1.1 BLEU on average for 12-layer models), while matching the decoding speed of the baseline model thanks to the efficiency improvements of MAtt. Source code for reproduction will be released soon.",
}

@article{raffel2019exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={arXiv preprint arXiv:1910.10683},
  year={2019}
}

@article{vanprobability,
  title={Probability in High Dimension},
  author={van Handel, Ramon},
  year={2016}
}

@inproceedings{li2019exponential,
  title={An Exponential Learning Rate Schedule for Deep Learning},
  author={Li, Zhiyuan and Arora, Sanjeev},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@inproceedings{arora2018theoretical,
  title={Theoretical Analysis of Auto Rate-Tuning by Batch Normalization},
  author={Arora, Sanjeev and Li, Zhiyuan and Lyu, Kaifeng},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{ward2019adagrad,
  title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
  author={Ward, Rachel and Wu, Xiaoxia and Bottou, Leon},
  booktitle={International Conference on Machine Learning},
  pages={6677--6686},
  year={2019},
  organization={PMLR}
}


@article{wan2020spherical,
  title={Spherical Motion Dynamics: Learning Dynamics of Neural Network with Normalization, Weight Decay, and SGD},
  author={Wan, Ruosi and Zhu, Zhanxing and Zhang, Xiangyu and Sun, Jian},
  journal={arXiv preprint arXiv:2006.08419},
  year={2020}
}

@article{de2018convergence,
  title={Convergence guarantees for RMSProp and ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration},
  author={De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  journal={arXiv preprint arXiv:1807.06766},
  year={2018}
}


@inproceedings{chen2018convergence,
  title={On the Convergence of A Class of Adam-Type Algorithms for Non-Convex Optimization},
  author={Chen, Xiangyi and Liu, Sijia and Sun, Ruoyu and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{zou2019sufficient,
  title={A sufficient condition for convergences of adam and rmsprop},
  author={Zou, Fangyu and Shen, Li and Jie, Zequn and Zhang, Weizhong and Liu, Wei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11127--11135},
  year={2019}
}

@article{lobacheva2021periodic,
  title={On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay},
  author={Lobacheva, Ekaterina and Kodryan, Maxim and Chirkova, Nadezhda and Malinin, Andrey and Vetrov, Dmitry P},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{cohen2020gradient,
  title={Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability},
  author={Cohen, Jeremy and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{lewkowycz2020large,
  title={The large learning rate phase of deep learning: the catapult mechanism},
  author={Lewkowycz, Aitor and Bahri, Yasaman and Dyer, Ethan and Sohl-Dickstein, Jascha and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:2003.02218},
  year={2020}
}

@inproceedings{ioffe2015batch,
  title={Batch normalization: Accelerating deep network training by reducing internal covariate shift},
  author={Ioffe, Sergey and Szegedy, Christian},
  booktitle={International conference on machine learning},
  pages={448--456},
  year={2015},
  organization={PMLR}
}

@article{ba2016layer,
  title={Layer normalization},
  author={Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  journal={arXiv preprint arXiv:1607.06450},
  year={2016}
}

@article{salimans2016weight,
  title={Weight normalization: A simple reparameterization to accelerate training of deep neural networks},
  author={Salimans, Tim and Kingma, Durk P},
  journal={Advances in neural information processing systems},
  volume={29},
  pages={901--909},
  year={2016}
}

@inproceedings{wu2018group,
  title={Group normalization},
  author={Wu, Yuxin and He, Kaiming},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={3--19},
  year={2018}
}

@article{ulyanov2016instance,
  title={Instance normalization: The missing ingredient for fast stylization},
  author={Ulyanov, Dmitry and Vedaldi, Andrea and Lempitsky, Victor},
  journal={arXiv preprint arXiv:1607.08022},
  year={2016}
}

@article{van2017l2,
  title={L2 regularization versus batch and weight normalization},
  author={Van Laarhoven, Twan},
  journal={arXiv preprint arXiv:1706.05350},
  year={2017}
}

@article{hoffer2018norm,
  title={Norm matters: efficient and accurate normalization schemes in deep networks},
  author={Hoffer, Elad and Banner, Ron and Golan, Itay and Soudry, Daniel},
  journal={arXiv preprint arXiv:1803.01814},
  year={2018}
}

@inproceedings{zhang2018three,
  title={Three Mechanisms of Weight Decay Regularization},
  author={Zhang, Guodong and Wang, Chaoqi and Xu, Bowen and Grosse, Roger},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@article{hendrycks2016gaussian,
  title={Gaussian error linear units (gelus)},
  author={Hendrycks, Dan and Gimpel, Kevin},
  journal={arXiv preprint arXiv:1606.08415},
  year={2016}
}

@article{anil2019memory,
  title={Memory efficient adaptive optimization},
  author={Anil, Rohan and Gupta, Vineet and Koren, Tomer and Singer, Yoram},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{shazeer2018adafactor,
  title={Adafactor: Adaptive learning rates with sublinear memory cost},
  author={Shazeer, Noam and Stern, Mitchell},
  booktitle={International Conference on Machine Learning},
  pages={4596--4604},
  year={2018},
  organization={PMLR}
}

@article{chowdhery2022palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={arXiv preprint arXiv:2204.02311},
  year={2022}
}