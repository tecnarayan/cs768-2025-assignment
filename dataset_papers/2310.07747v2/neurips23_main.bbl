\begin{thebibliography}{96}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Levine et~al.(2020)Levine, Kumar, Tucker, and Fu]{levine2020offline}
Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu.
\newblock Offline reinforcement learning: Tutorial, review, and perspectives on
  open problems.
\newblock \emph{arXiv preprint arXiv:2005.01643}, 2020.

\bibitem[Kalashnikov et~al.(2018)Kalashnikov, Irpan, Pastor, Ibarz, Herzog,
  Jang, Quillen, Holly, Kalakrishnan, Vanhoucke,
  et~al.]{kalashnikov2018scalable}
Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog,
  Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent
  Vanhoucke, et~al.
\newblock Scalable deep reinforcement learning for vision-based robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pages 651--673. PMLR, 2018.

\bibitem[Jaques et~al.(2019)Jaques, Ghandeharioun, Shen, Ferguson, Lapedriza,
  Jones, Gu, and Picard]{jaques2019way}
Natasha Jaques, Asma Ghandeharioun, Judy~Hanwen Shen, Craig Ferguson, Agata
  Lapedriza, Noah Jones, Shixiang Gu, and Rosalind Picard.
\newblock Way off-policy batch deep reinforcement learning of implicit human
  preferences in dialog.
\newblock \emph{arXiv preprint arXiv:1907.00456}, 2019.

\bibitem[Kahn et~al.(2021)Kahn, Abbeel, and Levine]{kahn2021badgr}
Gregory Kahn, Pieter Abbeel, and Sergey Levine.
\newblock Badgr: An autonomous self-supervised learning-based navigation
  system.
\newblock \emph{IEEE Robotics and Automation Letters}, 6\penalty0 (2):\penalty0
  1312--1319, 2021.

\bibitem[Holt et~al.(2023{\natexlab{a}})Holt, H\"uy\"uk, and van~der
  Schaar]{holt2023activeobservingcontrol}
Samuel Holt, Alihan H\"uy\"uk, and Mihaela van~der Schaar.
\newblock Active observing in continuous-time control.
\newblock In \emph{Conference on Neural Information Processing Systems},
  2023{\natexlab{a}}.

\bibitem[Keane and Kenny(2019)]{keane2019case}
Mark~T Keane and Eoin~M Kenny.
\newblock How case-based reasoning explains neural networks: A theoretical
  analysis of xai using post-hoc explanation-by-example from a survey of
  ann-cbr twin-systems.
\newblock In \emph{Case-Based Reasoning Research and Development: 27th
  International Conference, ICCBR 2019, Otzenhausen, Germany, September 8--12,
  2019, Proceedings 27}, pages 155--171. Springer, 2019.

\bibitem[Crabb{\'e} et~al.(2021)Crabb{\'e}, Qian, Imrie, and van~der
  Schaar]{crabbe2021explaining}
Jonathan Crabb{\'e}, Zhaozhi Qian, Fergus Imrie, and Mihaela van~der Schaar.
\newblock Explaining latent representations with a corpus of examples.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12154--12166, 2021.

\bibitem[Fujimoto et~al.(2019)Fujimoto, Meger, and Precup]{fujimoto2019off}
Scott Fujimoto, David Meger, and Doina Precup.
\newblock Off-policy deep reinforcement learning without exploration.
\newblock In \emph{International conference on machine learning}, pages
  2052--2062. PMLR, 2019.

\bibitem[Kumar et~al.(2019)Kumar, Fu, Soh, Tucker, and
  Levine]{kumar2019stabilizing}
Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine.
\newblock Stabilizing off-policy q-learning via bootstrapping error reduction.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Peng et~al.(2019)Peng, Kumar, Zhang, and Levine]{peng2019advantage}
Xue~Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine.
\newblock Advantage-weighted regression: Simple and scalable off-policy
  reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1910.00177}, 2019.

\bibitem[Agarwal et~al.(2020)Agarwal, Schuurmans, and
  Norouzi]{agarwal2020optimistic}
Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi.
\newblock An optimistic perspective on offline reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  104--114. PMLR, 2020.

\bibitem[Kumar et~al.(2020)Kumar, Zhou, Tucker, and
  Levine]{kumar2020conservative}
Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine.
\newblock Conservative q-learning for offline reinforcement learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 1179--1191, 2020.

\bibitem[An et~al.(2021)An, Moon, Kim, and Song]{an2021uncertainty}
Gaon An, Seungyong Moon, Jang-Hyun Kim, and Hyun~Oh Song.
\newblock Uncertainty-based offline reinforcement learning with diversified
  q-ensemble.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 7436--7447, 2021.

\bibitem[Fujimoto and Gu(2021)]{fujimoto2021minimalist}
Scott Fujimoto and Shixiang~Shane Gu.
\newblock A minimalist approach to offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 20132--20145, 2021.

\bibitem[Yang et~al.(2022{\natexlab{a}})Yang, Bai, Ma, Wang, Zhang, and
  Han]{yang2022rorl}
Rui Yang, Chenjia Bai, Xiaoteng Ma, Zhaoran Wang, Chongjie Zhang, and Lei Han.
\newblock Rorl: Robust offline reinforcement learning via conservative
  smoothing.
\newblock \emph{arXiv preprint arXiv:2206.02829}, 2022{\natexlab{a}}.

\bibitem[Nikulin et~al.(2023)Nikulin, Kurenkov, Tarasov, and
  Kolesnikov]{nikulin2023anti}
Alexander Nikulin, Vladislav Kurenkov, Denis Tarasov, and Sergey Kolesnikov.
\newblock Anti-exploration by random network distillation.
\newblock \emph{arXiv preprint arXiv:2301.13616}, 2023.

\bibitem[Blundell et~al.(2016)Blundell, Uria, Pritzel, Li, Ruderman, Leibo,
  Rae, Wierstra, and Hassabis]{blundell2016model}
Charles Blundell, Benigno Uria, Alexander Pritzel, Yazhe Li, Avraham Ruderman,
  Joel~Z Leibo, Jack Rae, Daan Wierstra, and Demis Hassabis.
\newblock Model-free episodic control.
\newblock \emph{arXiv preprint arXiv:1606.04460}, 2016.

\bibitem[Pritzel et~al.(2017)Pritzel, Uria, Srinivasan, Badia, Vinyals,
  Hassabis, Wierstra, and Blundell]{pritzel2017neural}
Alexander Pritzel, Benigno Uria, Sriram Srinivasan, Adria~Puigdomenech Badia,
  Oriol Vinyals, Demis Hassabis, Daan Wierstra, and Charles Blundell.
\newblock Neural episodic control.
\newblock In \emph{International conference on machine learning}, pages
  2827--2836. PMLR, 2017.

\bibitem[Lin et~al.(2018)Lin, Zhao, Yang, and Zhang]{lin2018episodic}
Zichuan Lin, Tianqi Zhao, Guangwen Yang, and Lintao Zhang.
\newblock Episodic memory deep q-networks.
\newblock \emph{arXiv preprint arXiv:1805.07603}, 2018.

\bibitem[Shah and Xie(2018)]{shah2018q}
Devavrat Shah and Qiaomin Xie.
\newblock Q-learning with nearest neighbors.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Mansimov and Cho(2018)]{mansimov2018simple}
Elman Mansimov and Kyunghyun Cho.
\newblock Simple nearest neighbor policy method for continuous control tasks.
\newblock 2018.

\bibitem[Kidambi et~al.(2020)Kidambi, Rajeswaran, Netrapalli, and
  Joachims]{kidambi2020morel}
Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims.
\newblock Morel: Model-based offline reinforcement learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 21810--21823, 2020.

\bibitem[Yu et~al.(2020)Yu, Thomas, Yu, Ermon, Zou, Levine, Finn, and
  Ma]{yu2020mopo}
Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James~Y Zou, Sergey
  Levine, Chelsea Finn, and Tengyu Ma.
\newblock Mopo: Model-based offline policy optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 14129--14142, 2020.

\bibitem[Williams et~al.(2016)Williams, Drews, Goldfain, Rehg, and
  Theodorou]{williams2016aggressive}
Grady Williams, Paul Drews, Brian Goldfain, James~M Rehg, and Evangelos~A
  Theodorou.
\newblock Aggressive driving with model predictive path integral control.
\newblock In \emph{2016 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 1433--1440. IEEE, 2016.

\bibitem[Pomerleau(1988)]{pomerleau1988alvinn}
Dean~A Pomerleau.
\newblock Alvinn: An autonomous land vehicle in a neural network.
\newblock \emph{Advances in neural information processing systems}, 1, 1988.

\bibitem[Alaa et~al.(2017)Alaa, Yoon, Hu, and Van~der
  Schaar]{alaa2017personalized}
Ahmed~M Alaa, Jinsung Yoon, Scott Hu, and Mihaela Van~der Schaar.
\newblock Personalized risk scoring for critical care prognosis using mixtures
  of gaussian processes.
\newblock \emph{IEEE Transactions on Biomedical Engineering}, 65\penalty0
  (1):\penalty0 207--218, 2017.

\bibitem[Fujimoto et~al.(2018)Fujimoto, Hoof, and
  Meger]{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In \emph{International conference on machine learning}, pages
  1587--1596. PMLR, 2018.

\bibitem[Chan et~al.(2021)Chan, Bica, Huyuk, Jarrett, and van~der
  Schaar]{chan2021medkit}
Alex~J Chan, Ioana Bica, Alihan Huyuk, Daniel Jarrett, and Mihaela van~der
  Schaar.
\newblock The medkit-learn (ing) environment: Medical decision modelling
  through simulation.
\newblock \emph{arXiv preprint arXiv:2106.04240}, 2021.

\bibitem[Yang et~al.(2022{\natexlab{b}})Yang, Lu, Li, Sun, Fang, Du, Li, Han,
  and Zhang]{yang2022rethinking}
Rui Yang, Yiming Lu, Wenzhe Li, Hao Sun, Meng Fang, Yali Du, Xiu Li, Lei Han,
  and Chongjie Zhang.
\newblock Rethinking goal-conditioned supervised learning and its connection to
  offline rl.
\newblock \emph{arXiv preprint arXiv:2202.04478}, 2022{\natexlab{b}}.

\bibitem[Liu et~al.(2022)Liu, Zhang, Fu, Yang, and Wang]{liu2022learning}
Zhihan Liu, Yufeng Zhang, Zuyue Fu, Zhuoran Yang, and Zhaoran Wang.
\newblock Learning from demonstration: Provably efficient adversarial policy
  imitation with linear function approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  14094--14138. PMLR, 2022.

\bibitem[Jarrett et~al.(2020)Jarrett, Bica, and van~der
  Schaar]{jarrett2020strictly}
Daniel Jarrett, Ioana Bica, and Mihaela van~der Schaar.
\newblock Strictly batch imitation learning by energy-based distribution
  matching.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7354--7365, 2020.

\bibitem[Wu et~al.(2019)Wu, Tucker, and Nachum]{wu2019behavior}
Yifan Wu, George Tucker, and Ofir Nachum.
\newblock Behavior regularized offline reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.11361}, 2019.

\bibitem[Sun et~al.(2022{\natexlab{a}})Sun, Han, Yang, Ma, Guo, and
  Zhou]{sun2022exploit}
Hao Sun, Lei Han, Rui Yang, Xiaoteng Ma, Jian Guo, and Bolei Zhou.
\newblock Exploit reward shifting in value-based deep-rl: Optimistic
  curiosity-based exploration and conservative exploitation via linear reward
  shaping.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 37719--37734, 2022{\natexlab{a}}.

\bibitem[Cheng et~al.(2022)Cheng, Xie, Jiang, and
  Agarwal]{cheng2022adversarially}
Ching-An Cheng, Tengyang Xie, Nan Jiang, and Alekh Agarwal.
\newblock Adversarially trained actor critic for offline reinforcement
  learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  3852--3878. PMLR, 2022.

\bibitem[Holt et~al.(2023{\natexlab{b}})Holt, H\"uy\"uk, Qian, Sun, and van~der
  Schaar]{holt2023neural}
S~Holt, A~H\"uy\"uk, Z~Qian, H~Sun, and M~van~der Schaar.
\newblock Neural {Laplace} control for continuous-time delayed systems.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, 2023{\natexlab{b}}.

\bibitem[Holt et~al.(2023{\natexlab{c}})Holt, H\"uy\"uk, and van~der
  Schaar]{holt2023active}
S~Holt, A~H\"uy\"uk, and M~van~der Schaar.
\newblock Active observing continuous-time control.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2023{\natexlab{c}}.

\bibitem[Lengyel and Dayan(2007)]{lengyel2007hippocampal}
M{\'a}t{\'e} Lengyel and Peter Dayan.
\newblock Hippocampal contributions to control: the third way.
\newblock \emph{Advances in neural information processing systems}, 20, 2007.

\bibitem[Hu et~al.(2021)Hu, Ye, Zhu, Ren, and Zhang]{pmlr-v139-hu21d}
Hao Hu, Jianing Ye, Guangxiang Zhu, Zhizhou Ren, and Chongjie Zhang.
\newblock Generalizable episodic memory for deep reinforcement learning.
\newblock In Marina Meila and Tong Zhang, editors, \emph{Proceedings of the
  38th International Conference on Machine Learning}, volume 139 of
  \emph{Proceedings of Machine Learning Research}, pages 4380--4390. PMLR,
  18--24 Jul 2021.
\newblock URL \url{https://proceedings.mlr.press/v139/hu21d.html}.

\bibitem[Ma et~al.(2021)Ma, Yang, Hu, Liu, Yang, Zhang, Zhao, and
  Liang]{ma2021offline}
Xiaoteng Ma, Yiqin Yang, Hao Hu, Qihan Liu, Jun Yang, Chongjie Zhang, Qianchuan
  Zhao, and Bin Liang.
\newblock Offline reinforcement learning with value-based episodic memory.
\newblock \emph{arXiv preprint arXiv:2110.09796}, 2021.

\bibitem[Li et~al.(2023)Li, Zhu, Hu, Xie, Ma, Zheng, Song, Chen, and
  Zhao]{li2023neural}
Zhuo Li, Derui Zhu, Yujing Hu, Xiaofei Xie, Lei Ma, Yan Zheng, Yan Song,
  Yingfeng Chen, and Jianjun Zhao.
\newblock Neural episodic control with state abstraction.
\newblock \emph{arXiv preprint arXiv:2301.11490}, 2023.

\bibitem[Watkins and Dayan(1992)]{watkins1992q}
Christopher~JCH Watkins and Peter Dayan.
\newblock Q-learning.
\newblock \emph{Machine learning}, 8:\penalty0 279--292, 1992.

\bibitem[Mnih et~al.(2015)Mnih, Kavukcuoglu, Silver, Rusu, Veness, Bellemare,
  Graves, Riedmiller, Fidjeland, Ostrovski, et~al.]{mnih2015human}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei~A Rusu, Joel Veness,
  Marc~G Bellemare, Alex Graves, Martin Riedmiller, Andreas~K Fidjeland, Georg
  Ostrovski, et~al.
\newblock Human-level control through deep reinforcement learning.
\newblock \emph{nature}, 518\penalty0 (7540):\penalty0 529--533, 2015.

\bibitem[Tesauro et~al.(1995)]{tesauro1995temporal}
Gerald Tesauro et~al.
\newblock Temporal difference learning and td-gammon.
\newblock \emph{Communications of the ACM}, 38\penalty0 (3):\penalty0 58--68,
  1995.

\bibitem[Sutton and Barto(2018)]{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Silver et~al.(2014)Silver, Lever, Heess, Degris, Wierstra, and
  Riedmiller]{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In \emph{International conference on machine learning}, pages
  387--395. Pmlr, 2014.

\bibitem[Lillicrap et~al.(2015)Lillicrap, Hunt, Pritzel, Heess, Erez, Tassa,
  Silver, and Wierstra]{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1509.02971}, 2015.

\bibitem[Wang et~al.(2018)Wang, Xiong, Han, Liu, Zhang,
  et~al.]{wang2018exponentially}
Qing Wang, Jiechao Xiong, Lei Han, Han Liu, Tong Zhang, et~al.
\newblock Exponentially weighted imitation learning for batched historical
  data.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Sun et~al.(2019)Sun, Li, Liu, Zhou, and Lin]{sun2019policy}
Hao Sun, Zhizhong Li, Xiaotong Liu, Bolei Zhou, and Dahua Lin.
\newblock Policy continuation with hindsight inverse dynamics.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Sun et~al.(2022{\natexlab{b}})Sun, Xu, Wang, Fang, and
  Zhou]{sun2022supervised}
Hao Sun, Ziping Xu, Taiyi Wang, Meng Fang, and Bolei Zhou.
\newblock Supervised q-learning for continuous control.
\newblock In \emph{Deep Reinforcement Learning Workshop NeurIPS 2022},
  2022{\natexlab{b}}.

\bibitem[Sun et~al.(2021{\natexlab{a}})Sun, Sun, Han, and Zhou]{sun2021neuro}
Jiankai Sun, Hao Sun, Tian Han, and Bolei Zhou.
\newblock Neuro-symbolic program search for autonomous driving decision module
  design.
\newblock In \emph{Conference on Robot Learning}, pages 21--30. PMLR,
  2021{\natexlab{a}}.

\bibitem[Ding et~al.(2020)Ding, Wu, Sun, Guo, and Guo]{ding2020hierarchical}
Qianggang Ding, Sifan Wu, Hao Sun, Jiadong Guo, and Jian Guo.
\newblock Hierarchical multi-scale gaussian transformer for stock movement
  prediction.
\newblock In \emph{IJCAI}, pages 4640--4646, 2020.

\bibitem[Charpentier et~al.(2021)Charpentier, Elie, and
  Remlinger]{charpentier2021reinforcement}
Arthur Charpentier, Romuald Elie, and Carl Remlinger.
\newblock Reinforcement learning in economics and finance.
\newblock \emph{Computational Economics}, pages 1--38, 2021.

\bibitem[Gottesman et~al.(2019)Gottesman, Johansson, Komorowski, Faisal,
  Sontag, Doshi-Velez, and Celi]{gottesman2019guidelines}
Omer Gottesman, Fredrik Johansson, Matthieu Komorowski, Aldo Faisal, David
  Sontag, Finale Doshi-Velez, and Leo~Anthony Celi.
\newblock Guidelines for reinforcement learning in healthcare.
\newblock \emph{Nature medicine}, 25\penalty0 (1):\penalty0 16--18, 2019.

\bibitem[H\"uy\"uk et~al.(2021)H\"uy\"uk, Jarrett, Tekin, and van~der
  Schaar]{huyuk2021explaining}
A~H\"uy\"uk, D~Jarrett, C~Tekin, and M~van~der Schaar.
\newblock Explaining by imitating: Understanding decisions by interpretable
  policy learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Jarrett et~al.(2021)Jarrett, H\"uy\"uk, and van~der
  Schaar]{jarrett2021inverse}
D~Jarrett, A~H\"uy\"uk, and M~van~der Schaar.
\newblock Inverse decision modeling: Learning interpretable representations of
  behavior.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2021.

\bibitem[Qin et~al.(2021)Qin, Imrie, H\"uy\"uk, Jarrett, Gimson, and van~der
  Schaar]{qin2021closing}
Y~Qin, F~Imrie, A~H\"uy\"uk, D~Jarrett, A~E Gimson, and M~van~der Schaar.
\newblock Closing the loop in medical decision support by understanding
  clinical decision-making: A case study on organ transplantation.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2021.

\bibitem[H\"uy\"uk et~al.(2022{\natexlab{a}})H\"uy\"uk, Zame, and van~der
  Schaar]{huyuk2022inferring}
A~H\"uy\"uk, W~R Zame, and M~van~der Schaar.
\newblock Inferring lexicographically-ordered rewards from preferences.
\newblock In \emph{AAAI Conference on Artificial Intelligence},
  2022{\natexlab{a}}.

\bibitem[H\"uy\"uk et~al.(2022{\natexlab{b}})H\"uy\"uk, Jarrett, and van~der
  Schaar]{huyuk2022inverse}
A~H\"uy\"uk, D~Jarrett, and M~van~der Schaar.
\newblock Inverse contextual bandits: Learning how behavior evolves over time.
\newblock In \emph{International Conference on Machine Learning (ICML)},
  2022{\natexlab{b}}.

\bibitem[Milani et~al.(2022)Milani, Topin, Veloso, and Fang]{milani2022survey}
Stephanie Milani, Nicholay Topin, Manuela Veloso, and Fei Fang.
\newblock A survey of explainable reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2202.08434}, 2022.

\bibitem[Hein et~al.(2018)Hein, Udluft, and Runkler]{hein2018interpretable}
Daniel Hein, Steffen Udluft, and Thomas~A Runkler.
\newblock Interpretable policies for reinforcement learning by genetic
  programming.
\newblock \emph{Engineering Applications of Artificial Intelligence},
  76:\penalty0 158--169, 2018.

\bibitem[Silva et~al.(2020)Silva, Gombolay, Killian, Jimenez, and
  Son]{silva2020optimization}
Andrew Silva, Matthew Gombolay, Taylor Killian, Ivan Jimenez, and Sung-Hyun
  Son.
\newblock Optimization methods for interpretable differentiable decision trees
  applied to reinforcement learning.
\newblock In \emph{International conference on artificial intelligence and
  statistics}, pages 1855--1865. PMLR, 2020.

\bibitem[Zhang et~al.(2021)Zhang, Li, Wang, and Tian]{zhang2021off}
Li~Zhang, Xin Li, Mingzhong Wang, and Andong Tian.
\newblock Off-policy differentiable logic reinforcement learning.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases.
  Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September
  13--17, 2021, Proceedings, Part II 21}, pages 617--632. Springer, 2021.

\bibitem[Verma et~al.(2018)Verma, Murali, Singh, Kohli, and
  Chaudhuri]{verma2018programmatically}
Abhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat
  Chaudhuri.
\newblock Programmatically interpretable reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  5045--5054. PMLR, 2018.

\bibitem[Jhunjhunwala(2019)]{jhunjhunwala2019policy}
Aman Jhunjhunwala.
\newblock Policy extraction via online q-value distillation.
\newblock Master's thesis, University of Waterloo, 2019.

\bibitem[Liu et~al.(2019)Liu, Schulte, Zhu, and Li]{liu2019toward}
Guiliang Liu, Oliver Schulte, Wang Zhu, and Qingcan Li.
\newblock Toward interpretable deep reinforcement learning with linear model
  u-trees.
\newblock In \emph{Machine Learning and Knowledge Discovery in Databases:
  European Conference, ECML PKDD 2018, Dublin, Ireland, September 10--14, 2018,
  Proceedings, Part II 18}, pages 414--429. Springer, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Zhou, and Lin]{zhang2020interpretable}
Hengzhe Zhang, Aimin Zhou, and Xin Lin.
\newblock Interpretable policy derivation for reinforcement learning based on
  evolutionary feature synthesis.
\newblock \emph{Complex \& Intelligent Systems}, 6:\penalty0 741--753, 2020.

\bibitem[Bewley and Lawry(2021)]{bewley2021tripletree}
Tom Bewley and Jonathan Lawry.
\newblock Tripletree: A versatile interpretable representation of black box
  agents and their environments.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 11415--11422, 2021.

\bibitem[Hayes and Shah(2017)]{hayes2017improving}
Bradley Hayes and Julie~A Shah.
\newblock Improving robot controller transparency through autonomous policy
  explanation.
\newblock In \emph{Proceedings of the 2017 ACM/IEEE international conference on
  human-robot interaction}, pages 303--312, 2017.

\bibitem[Wang et~al.(2019)Wang, Yuan, Zhang, Lewis, and Sycara]{wang2019verbal}
Xinzhi Wang, Shengcheng Yuan, Hui Zhang, Michael Lewis, and Katia Sycara.
\newblock Verbal explanations for deep reinforcement learning neural networks
  with attention on extracted features.
\newblock In \emph{2019 28th IEEE International Conference on Robot and Human
  Interactive Communication (RO-MAN)}, pages 1--7. IEEE, 2019.

\bibitem[Ehsan and Riedl(2020)]{ehsan2020human}
Upol Ehsan and Mark~O Riedl.
\newblock Human-centered explainable ai: Towards a reflective sociotechnical
  approach.
\newblock In \emph{HCI International 2020-Late Breaking Papers: Multimodality
  and Intelligence: 22nd HCI International Conference, HCII 2020, Copenhagen,
  Denmark, July 19--24, 2020, Proceedings 22}, pages 449--466. Springer, 2020.

\bibitem[Mott et~al.(2019)Mott, Zoran, Chrzanowski, Wierstra, and
  Jimenez~Rezende]{mott2019towards}
Alexander Mott, Daniel Zoran, Mike Chrzanowski, Daan Wierstra, and Danilo
  Jimenez~Rezende.
\newblock Towards interpretable reinforcement learning using attention
  augmented agents.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Atrey et~al.(2019)Atrey, Clary, and Jensen]{atrey2019exploratory}
Akanksha Atrey, Kaleigh Clary, and David Jensen.
\newblock Exploratory not explanatory: Counterfactual analysis of saliency maps
  for deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.05743}, 2019.

\bibitem[Tang et~al.(2020)Tang, Nguyen, and Ha]{tang2020neuroevolution}
Yujin Tang, Duong Nguyen, and David Ha.
\newblock Neuroevolution of self-interpretable agents.
\newblock In \emph{Proceedings of the 2020 Genetic and Evolutionary Computation
  Conference}, pages 414--424, 2020.

\bibitem[Rupprecht et~al.(2019)Rupprecht, Ibrahim, and
  Pal]{rupprecht2019finding}
Christian Rupprecht, Cyril Ibrahim, and Christopher~J Pal.
\newblock Finding and visualizing weaknesses of deep reinforcement learning
  agents.
\newblock \emph{arXiv preprint arXiv:1904.01318}, 2019.

\bibitem[Madumal et~al.(2020)Madumal, Miller, Sonenberg, and
  Vetere]{madumal2020explainable}
Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere.
\newblock Explainable reinforcement learning through a causal lens.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~34, pages 2493--2500, 2020.

\bibitem[Bica et~al.(2020)Bica, Jarrett, H{\"u}y{\"u}k, and van~der
  Schaar]{bica2020learning}
Ioana Bica, Daniel Jarrett, Alihan H{\"u}y{\"u}k, and Mihaela van~der Schaar.
\newblock Learning" what-if" explanations for sequential decision-making.
\newblock \emph{arXiv preprint arXiv:2007.13531}, 2020.

\bibitem[Anderson et~al.(2019)Anderson, Dodge, Sadarangani, Juozapaitis,
  Newman, Irvine, Chattopadhyay, Fern, and Burnett]{anderson2019explaining}
Andrew Anderson, Jonathan Dodge, Amrita Sadarangani, Zoe Juozapaitis, Evan
  Newman, Jed Irvine, Souti Chattopadhyay, Alan Fern, and Margaret Burnett.
\newblock Explaining reinforcement learning to mere mortals: An empirical
  study.
\newblock \emph{arXiv preprint arXiv:1903.09708}, 2019.

\bibitem[Beyret et~al.(2019)Beyret, Shafti, and Faisal]{beyret2019dot}
Benjamin Beyret, Ali Shafti, and A~Aldo Faisal.
\newblock Dot-to-dot: Explainable hierarchical reinforcement learning for
  robotic manipulation.
\newblock In \emph{2019 IEEE/RSJ International Conference on intelligent robots
  and systems (IROS)}, pages 5014--5019. IEEE, 2019.

\bibitem[Guo et~al.(2021)Guo, Wu, Khan, and Xing]{guo2021edge}
Wenbo Guo, Xian Wu, Usmann Khan, and Xinyu Xing.
\newblock Edge: Explaining deep reinforcement learning policies.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 12222--12236, 2021.

\bibitem[MacGlashan et~al.(2022)MacGlashan, Archer, Devlic, Seno, Sherstan,
  Wurman, and Stone]{macglashan2022value}
James MacGlashan, Evan Archer, Alisa Devlic, Takuma Seno, Craig Sherstan,
  Peter~R Wurman, and Peter Stone.
\newblock Value function decomposition for iterative design of reinforcement
  learning agents.
\newblock \emph{arXiv preprint arXiv:2206.13901}, 2022.

\bibitem[Dao et~al.(2018)Dao, Mishra, and Lee]{drlsr8614120}
Giang Dao, Indrajeet Mishra, and Minwoo Lee.
\newblock Deep reinforcement learning monitor for snapshot recording.
\newblock In \emph{2018 17th IEEE International Conference on Machine Learning
  and Applications (ICMLA)}, pages 591--598, 2018.
\newblock \doi{10.1109/ICMLA.2018.00095}.

\bibitem[Gottesman et~al.(2020)Gottesman, Futoma, Liu, Parbhoo, Celi,
  Brunskill, and Doshi-Velez]{Gottesman2020InterpretableOE}
Omer Gottesman, Joseph~D. Futoma, Yao Liu, Soanli Parbhoo, Leo~Anthony Celi,
  Emma Brunskill, and Finale Doshi-Velez.
\newblock Interpretable off-policy evaluation in reinforcement learning by
  highlighting influential transitions.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Topin and Veloso(2019)]{topin2019generation}
Nicholay Topin and Manuela Veloso.
\newblock Generation of policy-level explanations for reinforcement learning.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 2514--2521, 2019.

\bibitem[Sreedharan et~al.(2020)Sreedharan, Srivastava, and
  Kambhampati]{sreedharan2020tldr}
Sarath Sreedharan, Siddharth Srivastava, and Subbarao Kambhampati.
\newblock Tldr: Policy summarization for factored ssp problems using temporal
  abstractions.
\newblock In \emph{Proceedings of the International Conference on Automated
  Planning and Scheduling}, volume~30, pages 272--280, 2020.

\bibitem[Matsson and Johansson(2022)]{matsson2022case}
Anton Matsson and Fredrik~D Johansson.
\newblock Case-based off-policy evaluation using prototype learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 1339--1349.
  PMLR, 2022.

\bibitem[Glanois et~al.(2021)Glanois, Weng, Zimmer, Li, Yang, Hao, and
  Liu]{glanois2021survey}
Claire Glanois, Paul Weng, Matthieu Zimmer, Dong Li, Tianpei Yang, Jianye Hao,
  and Wulong Liu.
\newblock A survey on interpretable reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2112.13112}, 2021.

\bibitem[Vouros(2022)]{vouros2022explainable}
George~A Vouros.
\newblock Explainable deep reinforcement learning: state of the art and
  challenges.
\newblock \emph{ACM Computing Surveys}, 55\penalty0 (5):\penalty0 1--39, 2022.

\bibitem[Chung et~al.(2014)Chung, Gulcehre, Cho, and
  Bengio]{chung2014empirical}
Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio.
\newblock Empirical evaluation of gated recurrent neural networks on sequence
  modeling.
\newblock \emph{arXiv preprint arXiv:1412.3555}, 2014.

\bibitem[Williams et~al.(2017)Williams, Wagener, Goldfain, Drews, Rehg, Boots,
  and Theodorou]{williams2017information}
Grady Williams, Nolan Wagener, Brian Goldfain, Paul Drews, James~M Rehg, Byron
  Boots, and Evangelos~A Theodorou.
\newblock Information theoretic mpc for model-based reinforcement learning.
\newblock In \emph{2017 IEEE International Conference on Robotics and
  Automation (ICRA)}, pages 1714--1721. IEEE, 2017.

\bibitem[Fakoor et~al.(2020)Fakoor, Chaudhari, Soatto, and
  Smola]{Fakoor2020Meta-Q-Learning}
Rasool Fakoor, Pratik Chaudhari, Stefano Soatto, and Alexander~J. Smola.
\newblock Meta-q-learning.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=SJeD3CEFPH}.

\bibitem[Sun et~al.(2021{\natexlab{b}})Sun, Xu, Fang, Peng, Guo, Dai, and
  Zhou]{sun2021safe}
Hao Sun, Ziping Xu, Meng Fang, Zhenghao Peng, Jiadong Guo, Bo~Dai, and Bolei
  Zhou.
\newblock Safe exploration by solving early terminated mdp.
\newblock \emph{arXiv preprint arXiv:2107.04200}, 2021{\natexlab{b}}.

\bibitem[Ni et~al.(2022)Ni, Eysenbach, and Salakhutdinov]{pmlr-v162-ni22a}
Tianwei Ni, Benjamin Eysenbach, and Ruslan Salakhutdinov.
\newblock Recurrent model-free {RL} can be a strong baseline for many {POMDP}s.
\newblock In Kamalika Chaudhuri, Stefanie Jegelka, Le~Song, Csaba Szepesvari,
  Gang Niu, and Sivan Sabato, editors, \emph{Proceedings of the 39th
  International Conference on Machine Learning}, volume 162 of
  \emph{Proceedings of Machine Learning Research}, pages 16691--16723. PMLR,
  17--23 Jul 2022.
\newblock URL \url{https://proceedings.mlr.press/v162/ni22a.html}.

\bibitem[Sun et~al.(2022{\natexlab{c}})Sun, Xu, Peng, Fang, Wang, Dai, and
  Zhou]{sun2022constrained}
Hao Sun, Ziping Xu, Zhenghao Peng, Meng Fang, Taiyi Wang, Bo~Dai, and Bolei
  Zhou.
\newblock Constrained mdps can be solved by eearly-termination with recurrent
  models.
\newblock In \emph{NeurIPS 2022 Foundation Models for Decision Making
  Workshop}, 2022{\natexlab{c}}.

\bibitem[Brockman et~al.(2016)Brockman, Cheung, Pettersson, Schneider,
  Schulman, Tang, and Zaremba]{openaigym}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym, 2016.

\bibitem[Sun et~al.(2022{\natexlab{d}})Sun, van Breugel, Crabbe, Seedat, and
  van~der Schaar]{sun2022daux}
Hao Sun, Boris van Breugel, Jonathan Crabbe, Nabeel Seedat, and Mihaela van~der
  Schaar.
\newblock Daux: a density-based approach for uncertainty explanations.
\newblock \emph{arXiv preprint arXiv:2207.05161}, 2022{\natexlab{d}}.

\end{thebibliography}
