@article{ge2021understanding,
  title={Understanding Deflation Process in Over-parametrized Tensor Decomposition},
  author={Ge, Rong and Ren, Yunwei and Wang, Xiang and Zhou, Mo},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={1299--1311},
  year={2021}
}

@article{allen2019can,
  title={What can resnet learn efficiently, going beyond kernels?},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{tian2023scan,
  title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon},
  journal={arXiv preprint arXiv:2305.16380},
  year={2023}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{shen2023positional,
  title={Positional Description Matters for Transformers Arithmetic},
  author={Shen, Ruoqi and Bubeck, S{\'e}bastien and Eldan, Ronen and Lee, Yin Tat and Li, Yuanzhi and Zhang, Yi},
  journal={arXiv preprint arXiv:2311.14737},
  year={2023}
}

@article{damian2022self,
  title={Self-stabilization: The implicit bias of gradient descent at the edge of stability},
  author={Damian, Alex and Nichani, Eshaan and Lee, Jason D},
  journal={arXiv preprint arXiv:2209.15594},
  year={2022}
}

@article{li2022analyzing,
  title={Analyzing sharpness along gd trajectory: Progressive sharpening and edge of stability},
  author={Li, Zhouzi and Wang, Zixuan and Li, Jian},
  journal={arXiv preprint arXiv:2207.12678},
  year={2022}
}

@article{zhu2022understanding,
  title={Understanding edge-of-stability training dynamics with a minimalist example},
  author={Zhu, Xingyu and Wang, Zixuan and Wang, Xiang and Zhou, Mo and Ge, Rong},
  journal={arXiv preprint arXiv:2210.03294},
  year={2022}
}
@inproceedings{dehghani2023scaling,
  title={Scaling vision transformers to 22 billion parameters},
  author={Dehghani, Mostafa and Djolonga, Josip and Mustafa, Basil and Padlewski, Piotr and Heek, Jonathan and Gilmer, Justin and Steiner, Andreas Peter and Caron, Mathilde and Geirhos, Robert and Alabdulmohsin, Ibrahim and others},
  booktitle={International Conference on Machine Learning},
  pages={7480--7512},
  year={2023},
  organization={PMLR}
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}
@inproceedings{
wortsman2024smallscale,
title={Small-scale proxies for large-scale Transformer training instabilities},
author={Mitchell Wortsman and Peter J Liu and Lechao Xiao and Katie E Everett and Alexander A Alemi and Ben Adlam and John D Co-Reyes and Izzeddin Gur and Abhishek Kumar and Roman Novak and Jeffrey Pennington and Jascha Sohl-Dickstein and Kelvin Xu and Jaehoon Lee and Justin Gilmer and Simon Kornblith},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=d8w0pmvXbZ}
}

@article{ruoss2023randomized,
  title={Randomized Positional Encodings Boost Length Generalization of Transformers},
  author={Ruoss, Anian and Del{\'e}tang, Gr{\'e}goire and Genewein, Tim and Grau-Moya, Jordi and Csord{\'a}s, R{\'o}bert and Bennani, Mehdi and Legg, Shane and Veness, Joel},
  journal={arXiv preprint arXiv:2305.16843},
  year={2023}
}

@article{Jumper2021HighlyAP,
  title={Highly accurate protein structure prediction with AlphaFold},
  author={John M. Jumper and Richard Evans and Alexander Pritzel and Tim Green and Michael Figurnov and Olaf Ronneberger and Kathryn Tunyasuvunakool and Russ Bates and Augustin Z{\'i}dek and Anna Potapenko and Alex Bridgland and Clemens Meyer and Simon A A Kohl and Andy Ballard and Andrew Cowie and Bernardino Romera-Paredes and Stanislav Nikolov and Rishub Jain and Jonas Adler and Trevor Back and Stig Petersen and David A. Reiman and Ellen Clancy and Michal Zielinski and Martin Steinegger and Michalina Pacholska and Tamas Berghammer and Sebastian Bodenstein and David Silver and Oriol Vinyals and Andrew W. Senior and Koray Kavukcuoglu and Pushmeet Kohli and Demis Hassabis},
  journal={Nature},
  year={2021},
  volume={596},
  pages={583 - 589},
  url={https://api.semanticscholar.org/CorpusID:235959867}
}
@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{cohen2021gradient,
  title={Gradient descent on neural networks typically occurs at the edge of stability},
  author={Cohen, Jeremy M and Kaur, Simran and Li, Yuanzhi and Kolter, J Zico and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:2103.00065},
  year={2021}
}

@article{chen2024training,
  title={Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
  author={Chen, Siyu and Sheen, Heejune and Wang, Tianhao and Yang, Zhuoran},
  journal={arXiv preprint arXiv:2402.19442},
  year={2024}
}

@article{zhou2024transformers,
  title={Transformers Can Achieve Length Generalization But Not Robustly},
  author={Zhou, Yongchao and Alon, Uri and Chen, Xinyun and Wang, Xuezhi and Agarwal, Rishabh and Zhou, Denny},
  journal={arXiv preprint arXiv:2402.09371},
  year={2024}
}

@article{kim2024transformers,
  title={Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape},
  author={Kim, Juno and Suzuki, Taiji},
  journal={arXiv preprint arXiv:2402.01258},
  year={2024}
}

@article{nichani2024transformers,
  title={How Transformers Learn Causal Structure with Gradient Descent},
  author={Nichani, Eshaan and Damian, Alex and Lee, Jason D},
  journal={arXiv preprint arXiv:2402.14735},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{ruis2020benchmark,
  title={A benchmark for systematic generalization in grounded language understanding},
  author={Ruis, Laura and Andreas, Jacob and Baroni, Marco and Bouchacourt, Diane and Lake, Brenden M},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19861--19872},
  year={2020}
}

@article{haviv2022transformer,
  title={Transformer language models without positional encodings still learn positional information},
  author={Haviv, Adi and Ram, Ori and Press, Ofir and Izsak, Peter and Levy, Omer},
  journal={arXiv preprint arXiv:2203.16634},
  year={2022}
}

@article{tsai2019transformer,
  title={Transformer dissection: a unified understanding of transformer's attention via the lens of kernel},
  author={Tsai, Yao-Hung Hubert and Bai, Shaojie and Yamada, Makoto and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  journal={arXiv preprint arXiv:1908.11775},
  year={2019}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={The Journal of Machine Learning Research},
  volume={21},
  number={1},
  pages={5485--5551},
  year={2020},
  publisher={JMLRORG}
}

@article{su2021roformer,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Lu, Yu and Pan, Shengfeng and Wen, Bo and Liu, Yunfeng},
  journal={arXiv e-prints},
  pages={arXiv--2104},
  year={2021}
}

@article{kazemnejad2023impact,
  title={The Impact of Positional Encoding on Length Generalization in Transformers},
  author={Kazemnejad, Amirhossein and Padhi, Inkit and Ramamurthy, Karthikeyan Natesan and Das, Payel and Reddy, Siva},
  journal={arXiv preprint arXiv:2305.19466},
  year={2023}
}

@article{nye2021show,
  title={Show your work: Scratchpads for intermediate computation with language models},
  author={Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{hupkes2020compositionality,
  title={Compositionality decomposed: How do neural networks generalise?},
  author={Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={757--795},
  year={2020}
}

@article{dubois2019location,
  title={Location attention for extrapolation to longer sequences},
  author={Dubois, Yann and Dagan, Gautier and Hupkes, Dieuwke and Bruni, Elia},
  journal={arXiv preprint arXiv:1911.03872},
  year={2019}
}
@article{newman2020eos,
  title={The EOS decision and length extrapolation},
  author={Newman, Benjamin and Hewitt, John and Liang, Percy and Manning, Christopher D},
  journal={arXiv preprint arXiv:2010.07174},
  year={2020}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}


@article{zhou2023algorithms,
  title={What algorithms can transformers learn? a study in length generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Josh and Bengio, Samy and Nakkiran, Preetum},
  journal={arXiv preprint arXiv:2310.16028},
  year={2023}
}

@article{arora2023theory,
  title={A theory for emergence of complex skills in language models},
  author={Arora, Sanjeev and Goyal, Anirudh},
  journal={arXiv preprint arXiv:2307.15936},
  year={2023}
}
@article{dehghani2018universal,
  title={Universal transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  journal={arXiv preprint arXiv:1807.03819},
  year={2018}
}
@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}
@article{zhao2023transformers,
  title={Do Transformers Parse while Predicting the Masked Word?},
  author={Zhao, Haoyu and Panigrahi, Abhishek and Ge, Rong and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2303.08117},
  year={2023}
}
@article{anil2022exploring,
  title={Exploring length generalization in large language models},
  author={Anil, Cem and Wu, Yuhuai and Andreassen, Anders and Lewkowycz, Aitor and Misra, Vedant and Ramasesh, Vinay and Slone, Ambrose and Gur-Ari, Guy and Dyer, Ethan and Neyshabur, Behnam},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={38546--38556},
  year={2022}
}
@article{zhang2023trained,
  title={Trained Transformers Learn Linear Models In-Context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2306.09927},
  year={2023}
}
@article{mahankali2023one,
  title={One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention},
  author={Mahankali, Arvind and Hashimoto, Tatsunori B and Ma, Tengyu},
  journal={arXiv preprint arXiv:2307.03576},
  year={2023}
}
@article{tian2023joma,
  title={JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention},
  author={Tian, Yuandong and Wang, Yiping and Zhang, Zhenyu and Chen, Beidi and Du, Simon},
  journal={arXiv preprint arXiv:2310.00535},
  year={2023}
}
@article{panigrahi2023trainable,
  title={Trainable transformer in transformer},
  author={Panigrahi, Abhishek and Malladi, Sadhika and Xia, Mengzhou and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2307.01189},
  year={2023}
}
@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}
@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}
@article{bai2023transformers,
  title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={arXiv preprint arXiv:2306.04637},
  year={2023}
}

@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{barak2022hidden,
  title={Hidden progress in deep learning: Sgd learns parities near the computational limit},
  author={Barak, Boaz and Edelman, Benjamin and Goel, Surbhi and Kakade, Sham and Malach, Eran and Zhang, Cyril},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={21750--21764},
  year={2022}
}
@article{elhage2021mathematical,
  title={A mathematical framework for transformer circuits},
  author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
  journal={Transformer Circuits Thread},
  volume={1},
  year={2021}
}

@article{likhosherstov2021expressive,
  title={On the expressive power of self-attention matrices},
  author={Likhosherstov, Valerii and Choromanski, Krzysztof and Weller, Adrian},
  journal={arXiv preprint arXiv:2106.03764},
  year={2021}
}
@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Liberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}

@article{touvron2023llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}
@article{perez2019turing,
  title={On the turing completeness of modern neural network architectures},
  author={P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  journal={arXiv preprint arXiv:1901.03429},
  year={2019}
}

@article{bhattamishra2020ability,
  title={On the ability and limitations of transformers to recognize formal languages},
  author={Bhattamishra, Satwik and Ahuja, Kabir and Goyal, Navin},
  journal={arXiv preprint arXiv:2009.11264},
  year={2020}
}

@article{hahn2020theoretical,
  title={Theoretical limitations of self-attention in neural sequence models},
  author={Hahn, Michael},
  journal={Transactions of the Association for Computational Linguistics},
  volume={8},
  pages={156--171},
  year={2020},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@article{zhang2206unveiling,
  title={Unveiling transformers with lego: a synthetic reasoning task, 2022},
  author={Zhang, Yi and Backurs, Arturs and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Wagner, Tal},
  journal={URL https://arxiv. org/abs/2206.04301},
  year={2022}
}
@article{bhattamishra2020computational,
  title={On the computational power of transformers and its implications in sequence modeling},
  author={Bhattamishra, Satwik and Patel, Arkil and Goyal, Navin},
  journal={arXiv preprint arXiv:2006.09286},
  year={2020}
}
@article{yun2019transformers,
  title={Are transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1912.10077},
  year={2019}
}


@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{touvron2023llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{yao2021self,
  title={Self-attention networks can process bounded hierarchical languages},
  author={Yao, Shunyu and Peng, Binghui and Papadimitriou, Christos and Narasimhan, Karthik},
  journal={arXiv preprint arXiv:2105.11115},
  year={2021}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}
@article{huang2023context,
  title={In-Context Convergence of Transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}

@article{wang2020beyond,
  title={Beyond lazy training for over-parameterized tensor decomposition},
  author={Wang, Xiang and Wu, Chenwei and Lee, Jason D and Ma, Tengyu and Ge, Rong},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={21934--21944},
  year={2020}
}

@article{allen2019learning,
  title={Learning and generalization in overparameterized neural networks, going beyond two layers},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{razin2021implicit,
  title={Implicit regularization in tensor factorization},
  author={Razin, Noam and Maman, Asaf and Cohen, Nadav},
  booktitle={International Conference on Machine Learning},
  pages={8913--8924},
  year={2021},
  organization={PMLR}
}

@inproceedings{allen2019convergence,
  title={A convergence theory for deep learning via over-parameterization},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi and Song, Zhao},
  booktitle={International Conference on Machine Learning},
  pages={242--252},
  year={2019},
  organization={PMLR}
}

@article{anandkumar2014tensor,
  title={Tensor decompositions for learning latent variable models},
  author={Anandkumar, Animashree and Ge, Rong and Hsu, Daniel and Kakade, Sham M and Telgarsky, Matus},
  journal={Journal of machine learning research},
  volume={15},
  pages={2773--2832},
  year={2014},
  publisher={Journal of Machine Learning Research}
}

@article{arora2019implicit,
  title={Implicit regularization in deep matrix factorization},
  author={Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{arora2019exact,
  title={On exact computation with an infinitely wide neural net},
  author={Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{arora2019fine,
  title={Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks},
  author={Arora, Sanjeev and Du, Simon and Hu, Wei and Li, Zhiyuan and Wang, Ruosong},
  booktitle={International Conference on Machine Learning},
  pages={322--332},
  year={2019},
  organization={PMLR}
}
@inproceedings{Zhou2021ALC,
  title={A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network},
  author={Mo Zhou and Rong Ge and Chi Jin},
  booktitle={Annual Conference Computational Learning Theory},
  year={2021}
}
@article{bai2020taylorized,
  title={Taylorized training: Towards better approximation of neural network training at finite width},
  author={Bai, Yu and Krause, Ben and Wang, Huan and Xiong, Caiming and Socher, Richard},
  journal={arXiv preprint arXiv:2002.04010},
  year={2020}
}

@article{bai2019beyond,
  title={Beyond linearization: On quadratic and higher-order approximation of wide neural networks},
  author={Bai, Yu and Lee, Jason D},
  journal={arXiv preprint arXiv:1910.01619},
  year={2019}
}

@article{chen2020towards,
  title={Towards understanding hierarchical learning: Benefits of neural representations},
  author={Chen, Minshuo and Bai, Yu and Lee, Jason D and Zhao, Tuo and Wang, Huan and Xiong, Caiming and Socher, Richard},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={22134--22145},
  year={2020}
}

@article{chizat2022sparse,
  title={Sparse optimization on measures with over-parameterized gradient descent},
  author={Chizat, Lenaic},
  journal={Mathematical Programming},
  volume={194},
  number={1},
  pages={487--532},
  year={2022},
  publisher={Springer}
}

@article{chizat2018global,
  title={On the global convergence of gradient descent for over-parameterized models using optimal transport},
  author={Chizat, Lenaic and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{chizat2020implicit,
  title={Implicit bias of gradient descent for wide two-layer neural networks trained with the logistic loss},
  author={Chizat, Lenaic and Bach, Francis},
  booktitle={Conference on Learning Theory},
  pages={1305--1338},
  year={2020},
  organization={PMLR}
}

@article{chizat2019lazy,
  title={On lazy training in differentiable programming},
  author={Chizat, Lenaic and Oyallon, Edouard and Bach, Francis},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{du2019gradient,
  title={Gradient descent finds global minima of deep neural networks},
  author={Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  booktitle={International conference on machine learning},
  pages={1675--1685},
  year={2019},
  organization={PMLR}
}

@article{du2018gradient,
  title={Gradient descent provably optimizes over-parameterized neural networks},
  author={Du, Simon S and Zhai, Xiyu and Poczos, Barnabas and Singh, Aarti},
  journal={arXiv preprint arXiv:1810.02054},
  year={2018}
}

@article{dyer2019asymptotics,
  title={Asymptotics of wide networks from feynman diagrams},
  author={Dyer, Ethan and Gur-Ari, Guy},
  journal={arXiv preprint arXiv:1909.11304},
  year={2019}
}

@article{ge2017learning,
  title={Learning one-hidden-layer neural networks with landscape design},
  author={Ge, Rong and Lee, Jason D and Ma, Tengyu},
  journal={arXiv preprint arXiv:1711.00501},
  year={2017}
}

@article{ghorbani2019limitations,
  title={Limitations of lazy training of two-layers neural network},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{ghorbani2020neural,
  title={When do neural networks outperform kernel methods?},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14820--14830},
  year={2020}
}

@article{ghorbani2021linearized,
  title={Linearized two-layers neural networks in high dimension},
  author={Ghorbani, Behrooz and Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
  journal={The Annals of Statistics},
  volume={49},
  number={2},
  pages={1029--1054},
  year={2021},
  publisher={Institute of Mathematical Statistics}
}

@inproceedings{gunasekar2018characterizing,
  title={Characterizing implicit bias in terms of optimization geometry},
  author={Gunasekar, Suriya and Lee, Jason and Soudry, Daniel and Srebro, Nathan},
  booktitle={International Conference on Machine Learning},
  pages={1832--1841},
  year={2018},
  organization={PMLR}
}

@article{gunasekar2018implicit,
  title={Implicit bias of gradient descent on linear convolutional networks},
  author={Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@inproceedings{huang2020dynamics,
  title={Dynamics of deep neural networks and neural tangent hierarchy},
  author={Huang, Jiaoyang and Yau, Horng-Tzer},
  booktitle={International conference on machine learning},
  pages={4542--4551},
  year={2020},
  organization={PMLR}
}

@article{jacot2018neural,
  title={Neural tangent kernel: Convergence and generalization in neural networks},
  author={Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{ji2018gradient,
  title={Gradient descent aligns the layers of deep linear networks},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1810.02032},
  year={2018}
}

@article{ji2018risk,
  title={Risk and parameter convergence of logistic regression},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={arXiv preprint arXiv:1803.07300},
  year={2018}
}

@article{ji2019refined,
  title={A refined primal-dual analysis of the implicit bias},
  author={Ji, Ziwei and Telgarsky, Matus},
  year={2019}
}

@article{ji2020directional,
  title={Directional convergence and alignment in deep learning},
  author={Ji, Ziwei and Telgarsky, Matus},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={17176--17186},
  year={2020}
}

@book{lakshmikantham1989theory,
  title={Theory of impulsive differential equations},
  author={Lakshmikantham, Vangipuram and Simeonov, Pavel S and others},
  volume={6},
  year={1989},
  publisher={World scientific}
}

@article{li2018learning,
  title={Learning overparameterized neural networks via stochastic gradient descent on structured data},
  author={Li, Yuanzhi and Liang, Yingyu},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@inproceedings{li2020learning,
  title={Learning over-parametrized two-layer neural networks beyond ntk},
  author={Li, Yuanzhi and Ma, Tengyu and Zhang, Hongyang R},
  booktitle={Conference on learning theory},
  pages={2613--2682},
  year={2020},
  organization={PMLR}
}

@article{li2020towards,
  title={Towards resolving the implicit bias of gradient descent for matrix factorization: Greedy low-rank learning},
  author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
  journal={arXiv preprint arXiv:2012.09839},
  year={2020}
}

@article{2019Gradient,
  title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
  author={ Lyu, K.  and  Li, J. },
  year={2019},
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{moroshko2020implicit,
  title={Implicit bias in deep linear classification: Initialization scale vs training accuracy},
  author={Moroshko, Edward and Woodworth, Blake E and Gunasekar, Suriya and Lee, Jason D and Srebro, Nati and Soudry, Daniel},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={22182--22193},
  year={2020}
}

@inproceedings{nacson2019convergence,
  title={Convergence of gradient descent on separable data},
  author={Nacson, Mor Shpigel and Lee, Jason and Gunasekar, Suriya and Savarese, Pedro Henrique Pamplona and Srebro, Nathan and Soudry, Daniel},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={3420--3428},
  year={2019},
  organization={PMLR}
}

@article{nguyen2020rigorous,
  title={A rigorous framework for the mean field limit of multilayer neural networks},
  author={Nguyen, Phan-Minh and Pham, Huy Tuan},
  journal={arXiv preprint arXiv:2001.11443},
  year={2020}
}

@article{wei2019regularization,
  title={Regularization matters: Generalization and optimization of neural nets vs their induced kernel},
  author={Wei, Colin and Lee, Jason D and Liu, Qiang and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@article{nitanda2017stochastic,
  title={Stochastic particle gradient descent for infinite ensembles},
  author={Nitanda, Atsushi and Suzuki, Taiji},
  journal={arXiv preprint arXiv:1712.05438},
  year={2017}
}

@inproceedings{cohen2016expressive,
  title={On the expressive power of deep learning: A tensor analysis},
  author={Cohen, Nadav and Sharir, Or and Shashua, Amnon},
  booktitle={Conference on learning theory},
  pages={698--728},
  year={2016},
  organization={PMLR}
}

@inproceedings{2018Regularization,
  title={Regularization Matters: Generalization and Optimization of Neural Nets v.s. their Induced Kernel},
  author={ Wei, C.  and  Lee, J. D.  and  Liu, Q.  and  Ma, T. },
  year={2018},
}

@article{sanford2023representational,
  title={Representational Strengths and Limitations of Transformers},
  author={Sanford, Clayton and Hsu, Daniel and Telgarsky, Matus},
  journal={arXiv preprint arXiv:2306.02896},
  year={2023}
}

@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@article{raskutti2013information,
  title={The information geometry of mirror descent},
  author={Raskutti, Garvesh and Mukherjee, Sayan},
  journal={arXiv preprint arXiv:1310.7780},
  year={2013}
}

@article{2023HowMN,
  title={How Many Neurons Does it Take to Approximate the Maximum?},
  author={Safran, Itay and Reichman, Daniel and Valiant, Paul},
  journal={arXiv preprint arXiv:2307.09212},
  year={2023}
}

@inproceedings{safran2019depth,
  title={Depth separations in neural networks: what is actually being separated?},
  author={Safran, Itay and Eldan, Ronen and Shamir, Ohad},
  booktitle={Conference on Learning Theory},
  pages={2664--2666},
  year={2019},
  organization={PMLR}
}

@article{NEYMAN1989116,
title = {Uniqueness of the Shapley value},
journal = {Games and Economic Behavior},
volume = {1},
number = {1},
pages = {116-118},
year = {1989},
issn = {0899-8256},
doi = {https://doi.org/10.1016/0899-8256(89)90008-0},
url = {https://www.sciencedirect.com/science/article/pii/0899825689900080},
author = {Abraham Neyman},
abstract = {It is shown that the Shapley value of any given game v is characterized by applying the value axioms—efficiency, symmetry, the null player axiom, and either additivity or strong positivity—to the additive group generated by the game ν itself and its subgames.}
}

@article{jelassi2022vision,
  title={Vision transformers provably learn spatial structure},
  author={Jelassi, Samy and Sander, Michael and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={37822--37836},
  year={2022}
}

@article{li2023theoretical,
  title={A theoretical understanding of shallow vision transformers: Learning, generalization, and sample complexity},
  author={Li, Hongkang and Wang, Meng and Liu, Sijia and Chen, Pin-Yu},
  journal={arXiv preprint arXiv:2302.06015},
  year={2023}
}

@article{tarzanagh2023transformers,
  title={Transformers as support vector machines},
  author={Tarzanagh, Davoud Ataee and Li, Yingcong and Thrampoulidis, Christos and Oymak, Samet},
  journal={arXiv preprint arXiv:2308.16898},
  year={2023}
}


@misc{candes2005decoding,
      title={Decoding by Linear Programming}, 
      author={Emmanuel Candes and Terence Tao},
      year={2005},
      eprint={math/0502327},
      archivePrefix={arXiv},
      primaryClass={math.MG}
}

@misc{paszke2019pytorch,
      title={PyTorch: An Imperative Style, High-Performance Deep Learning Library}, 
      author={Adam Paszke and Sam Gross and Francisco Massa and Adam Lerer and James Bradbury and Gregory Chanan and Trevor Killeen and Zeming Lin and Natalia Gimelshein and Luca Antiga and Alban Desmaison and Andreas Köpf and Edward Yang and Zach DeVito and Martin Raison and Alykhan Tejani and Sasank Chilamkurthy and Benoit Steiner and Lu Fang and Junjie Bai and Soumith Chintala},
      year={2019},
      eprint={1912.01703},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}