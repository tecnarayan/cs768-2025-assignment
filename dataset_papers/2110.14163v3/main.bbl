\begin{thebibliography}{45}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Achille et~al.(2018)Achille, Rovere, and Soatto]{achille2018critical}
Achille, A., Rovere, M., and Soatto, S.
\newblock Critical learning periods in deep networks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Achille et~al.(2019)Achille, Paolini, and
  Soatto]{achille2019information}
Achille, A., Paolini, G., and Soatto, S.
\newblock Where is the information in a deep neural network?
\newblock \emph{arXiv preprint arXiv:1905.12213}, 2019.

\bibitem[Amari(1998)]{6790500}
Amari, S.-i.
\newblock Natural gradient works efficiently in learning.
\newblock \emph{Neural Computation}, 10\penalty0 (2):\penalty0 251--276, 1998.
\newblock \doi{10.1162/089976698300017746}.

\bibitem[Amari et~al.(2002)Amari, Park, and Ozeki]{amari2002geometrical}
Amari, S.-i., Park, H., and Ozeki, T.
\newblock Geometrical singularities in the neuromanifold of multilayer
  perceptrons.
\newblock \emph{Advances in neural information processing systems}, 1:\penalty0
  343--350, 2002.

\bibitem[Ambroladze et~al.(2007)Ambroladze, {Parrado-Hernández}, and
  {Shawe-Taylor}]{ambroladze2007tighter}
Ambroladze, A., {Parrado-Hernández}, E., and {Shawe-Taylor}, J.
\newblock Tighter pac-bayes bounds.
\newblock \emph{Advances in neural information processing systems},
  19:\penalty0 9, 2007.

\bibitem[Bartlett et~al.(2017)Bartlett, Foster, and
  Telgarsky]{bartlett2017spectrally}
Bartlett, P.~L., Foster, D.~J., and Telgarsky, M.~J.
\newblock Spectrally-normalized margin bounds for neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  6240--6249, 2017.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Bartlett et~al.(2021)Bartlett, Montanari, and
  Rakhlin]{bartlett2021deep}
Bartlett, P.~L., Montanari, A., and Rakhlin, A.
\newblock Deep learning: A statistical viewpoint.
\newblock \emph{arXiv preprint arXiv:2103.09177}, 2021.

\bibitem[Botev et~al.(2017)Botev, Ritter, and Barber]{ICML-2017-BotevRB}
Botev, A., Ritter, H., and Barber, D.
\newblock {Practical Gauss-Newton Optimisation for Deep Learning}.
\newblock In \emph{{Proceedings of the 34th International Conference on Machine
  Learning}}, pp.\  557--565. {PMLR}, 2017.

\bibitem[Brown et~al.(2004)Brown, Hill, Calero, Myers, Lee, Sethna, and
  Cerione]{brown2004statistical}
Brown, K.~S., Hill, C.~C., Calero, G.~A., Myers, C.~R., Lee, K.~H., Sethna,
  J.~P., and Cerione, R.~A.
\newblock The statistical mechanics of complex signaling networks: Nerve growth
  factor signaling.
\newblock \emph{Physical biology}, 1\penalty0 (3):\penalty0 184, 2004.

\bibitem[Chaudhari \& Soatto(2018)Chaudhari and
  Soatto]{chaudhari2017stochastic}
Chaudhari, P. and Soatto, S.
\newblock Stochastic gradient descent performs variational inference, converges
  to limit cycles for deep networks.
\newblock \emph{Proc. of International Conference of Learning and
  Representations (ICLR), Apr 30-May 3, 2018}, 2018.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{chaudhari2016entropy}
Chaudhari, P., Choromanska, A., Soatto, S., LeCun, Y., Baldassi, C., Borgs, C.,
  Chayes, J., Sagun, L., and Zecchina, R.
\newblock Entropy-{{SGD}}: {{Biasing}} gradient descent into wide valleys.
\newblock In \emph{Proc. of the {{International Conference}} on {{Learning
  Representations}} ({{ICLR}})}, 2017.

\bibitem[Chizat et~al.(2019)Chizat, Oyallon, and Bach]{chizat2019lazy}
Chizat, L., Oyallon, E., and Bach, F.
\newblock On lazy training in differentiable programming.
\newblock \emph{Advances in Neural Information Processing Systems},
  32:\penalty0 2937--2947, 2019.

\bibitem[Dangel et~al.(2020)Dangel, Kunstner, and Hennig]{dangel2020backpack}
Dangel, F., Kunstner, F., and Hennig, P.
\newblock {{BackPACK}}: {{Packing}} more into backprop.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Dziugaite(2020)]{dziugaite2020revisiting}
Dziugaite, G.~K.
\newblock \emph{Revisiting Generalization for Deep Learning: {{PAC}}-{{Bayes}},
  Flat Minima, and Generative Models}.
\newblock PhD thesis, University of Cambridge, 2020.

\bibitem[Dziugaite \& Roy(2017)Dziugaite and
  Roy]{dziugaiteComputingNonvacuousGeneralization2017}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Computing {{Nonvacuous Generalization Bounds}} for {{Deep}}
  ({{Stochastic}}) {{Neural Networks}} with {{Many More Parameters}} than
  {{Training Data}}.
\newblock In \emph{Proc. of the {{Conference}} on {{Uncertainty}} in
  {{Artificial Intelligence}} ({{UAI}})}, 2017.

\bibitem[Dziugaite \& Roy(2018)Dziugaite and Roy]{dziugaite2018data}
Dziugaite, G.~K. and Roy, D.~M.
\newblock Data-dependent {{PAC}}-{{Bayes}} priors via differential privacy.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  8440--8450, 2018.

\bibitem[Fort \& Ganguli(2019)Fort and
  Ganguli]{fortEmergentPropertiesLocal2019}
Fort, S. and Ganguli, S.
\newblock Emergent properties of the local geometry of neural loss landscapes.
\newblock \emph{arXiv:1910.05929 [cs, stat]}, October 2019.

\bibitem[{Gur-Ari} et~al.(2018){Gur-Ari}, Roberts, and
  Dyer]{gur-ariGradientDescentHappens2018}
{Gur-Ari}, G., Roberts, D.~A., and Dyer, E.
\newblock Gradient {{Descent Happens}} in a {{Tiny Subspace}}.
\newblock \emph{arXiv:1812.04754 [cs, stat]}, December 2018.

\bibitem[Gutenkunst et~al.(2007)Gutenkunst, Waterfall, Casey, Brown, Myers, and
  Sethna]{gutenkunst2007universally}
Gutenkunst, R.~N., Waterfall, J.~J., Casey, F.~P., Brown, K.~S., Myers, C.~R.,
  and Sethna, J.~P.
\newblock Universally sloppy parameter sensitivities in systems biology models.
\newblock \emph{PLoS computational biology}, 3\penalty0 (10):\penalty0 e189,
  2007.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiterFlatMinima1997}
Hochreiter, S. and Schmidhuber, J.
\newblock Flat minima.
\newblock \emph{Neural Computation}, 9\penalty0 (1):\penalty0 1--42, 1997.

\bibitem[Karakida et~al.(2019)Karakida, Akaho, and
  Amari]{karakidaUniversalStatisticsFisher2019}
Karakida, R., Akaho, S., and Amari, S.-i.
\newblock Universal {{Statistics}} of {{Fisher Information}} in {{Deep Neural
  Networks}}: {{Mean Field Approach}}.
\newblock \emph{arXiv:1806.01316 [cond-mat, stat]}, October 2019.

\bibitem[Karoui(2010)]{Karoui2010TheSO}
Karoui, N.~E.
\newblock The spectrum of kernel random matrices.
\newblock \emph{Annals of Statistics}, 38:\penalty0 1--50, 2010.

\bibitem[Krizhevsky(2009)]{krizhevsky2009learning}
Krizhevsky, A.
\newblock \emph{Learning Multiple Layers of Features from Tiny Images}.
\newblock PhD thesis, Computer Science, University of Toronto, 2009.

\bibitem[Langford \& Caruana(2002)Langford and Caruana]{langford2002not}
Langford, J. and Caruana, R.
\newblock ({{Not}}) bounding the true error.
\newblock \emph{Advances in Neural Information Processing Systems}, 2:\penalty0
  809--816, 2002.

\bibitem[Langford \& Seeger(2001)Langford and Seeger]{langford2001bounds}
Langford, J. and Seeger, M.
\newblock Bounds for averaging classifiers.
\newblock 2001.

\bibitem[LeCun et~al.(1990)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{lecun1990handwritten}
LeCun, Y., Boser, B.~E., Denker, J.~S., Henderson, D., Howard, R.~E., Hubbard,
  W.~E., and Jackel, L.~D.
\newblock Handwritten digit recognition with a back-propagation network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  396--404, 1990.

\bibitem[Liang \& Rakhlin(2018)Liang and Rakhlin]{JustInterpolate}
Liang, T. and Rakhlin, A.
\newblock Just interpolate: Kernel "ridgeless" regression can generalize.
\newblock \emph{CoRR}, abs/1808.00387, 2018.
\newblock URL \url{http://arxiv.org/abs/1808.00387}.

\bibitem[Martens \& Grosse(2016)Martens and
  Grosse]{martensOptimizingNeuralNetworks2016}
Martens, J. and Grosse, R.
\newblock Optimizing {{Neural Networks}} with {{Kronecker}}-factored
  {{Approximate Curvature}}.
\newblock \emph{arXiv:1503.05671 [cs, stat]}, May 2016.

\bibitem[McAllester(1999)]{mcallester1999pac}
McAllester, D.~A.
\newblock {{PAC}}-{{Bayesian}} model averaging.
\newblock In \emph{Proceedings of the Twelfth Annual Conference on
  {{Computational}} Learning Theory}, pp.\  164--170, 1999.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, McAllester, and
  Srebro]{neyshaburExploringGeneralizationDeep2017}
Neyshabur, B., Bhojanapalli, S., McAllester, D., and Srebro, N.
\newblock Exploring {{Generalization}} in {{Deep Learning}}.
\newblock \emph{arXiv:1706.08947 [cs]}, July 2017.

\bibitem[Papyan(2018)]{papyan2018full}
Papyan, V.
\newblock The full spectrum of deepnet hessians at scale: {{Dynamics}} with sgd
  training and sample size.
\newblock \emph{arXiv preprint arXiv:1811.07062}, 2018.

\bibitem[Papyan(2019)]{papyan2019measurements}
Papyan, V.
\newblock Measurements of three-level hierarchical structure in the outliers in
  the spectrum of deepnet hessians.
\newblock \emph{arXiv preprint arXiv:1901.08244}, 2019.

\bibitem[{Parrado-Hernández} et~al.(2012){Parrado-Hernández}, Ambroladze,
  {Shawe-Taylor}, and Sun]{parrado2012pac}
{Parrado-Hernández}, E., Ambroladze, A., {Shawe-Taylor}, J., and Sun, S.
\newblock {{PAC}}-{{Bayes}} bounds with data dependent priors.
\newblock \emph{The Journal of Machine Learning Research}, 13\penalty0
  (1):\penalty0 3507--3531, 2012.

\bibitem[Pennington \& Bahri()Pennington and
  Bahri]{penningtonGeometryNeuralNetwork}
Pennington, J. and Bahri, Y.
\newblock Geometry of {{Neural Network Loss Surfaces}} via {{Random Matrix
  Theory}}.
\newblock pp.\ ~9.

\bibitem[Sagun et~al.(2016)Sagun, Bottou, and LeCun]{sagun2016singularity}
Sagun, L., Bottou, L., and LeCun, Y.
\newblock Singularity of the hessian in deep learning.
\newblock \emph{arXiv:1611:07476}, 2016.

\bibitem[Schwarz(1978)]{schwarz1978estimating}
Schwarz, G.
\newblock Estimating the dimension of a model.
\newblock \emph{The annals of statistics}, pp.\  461--464, 1978.

\bibitem[Springenberg et~al.(2015)Springenberg, Dosovitskiy, Brox, and
  Riedmiller]{springenbergStrivingSimplicityAll2015}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M.
\newblock Striving for {{Simplicity}}: {{The All Convolutional Net}}.
\newblock \emph{arXiv:1412.6806 [cs]}, April 2015.

\bibitem[Transtrum et~al.(2011)Transtrum, Machta, and
  Sethna]{transtrumGeometryNonlinearLeast2011}
Transtrum, M.~K., Machta, B.~B., and Sethna, J.~P.
\newblock The geometry of nonlinear least squares with applications to sloppy
  models and optimization.
\newblock \emph{Physical Review E}, 83\penalty0 (3):\penalty0 036701, March
  2011.
\newblock ISSN 1539-3755, 1550-2376.
\newblock \doi{10.1103/PhysRevE.83.036701}.

\bibitem[Waterfall et~al.(2006)Waterfall, Casey, Gutenkunst, Brown, Myers,
  Brouwer, Elser, and Sethna]{waterfallSloppyModelUniversalityClass2006}
Waterfall, J.~J., Casey, F.~P., Gutenkunst, R.~N., Brown, K.~S., Myers, C.~R.,
  Brouwer, P.~W., Elser, V., and Sethna, J.~P.
\newblock Sloppy-{{Model Universality Class}} and the {{Vandermonde Matrix}}.
\newblock \emph{Physical Review Letters}, 97\penalty0 (15):\penalty0 150601,
  October 2006.
\newblock \doi{10.1103/PhysRevLett.97.150601}.

\bibitem[Wilson(2020)]{wilson2020case}
Wilson, A.~G.
\newblock The case for bayesian deep learning.
\newblock \emph{arXiv preprint arXiv:2001.10995}, 2020.

\bibitem[Wu et~al.(2021)Wu, Zhu, Wu, Wang, and
  Ge]{wuDissectingHessianUnderstanding2021}
Wu, Y., Zhu, X., Wu, C., Wang, A., and Ge, R.
\newblock Dissecting {{Hessian}}: {{Understanding Common Structure}} of
  {{Hessian}} in {{Neural Networks}}.
\newblock \emph{arXiv:2010.04261 [cs, stat]}, June 2021.

\bibitem[Yin et~al.(2018)Yin, Pananjady, Lam, Papailiopoulos, Ramchandran, and
  Bartlett]{yin2018gradient}
Yin, D., Pananjady, A., Lam, M., Papailiopoulos, D., Ramchandran, K., and
  Bartlett, P.
\newblock Gradient diversity: A key ingredient for scalable distributed
  learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1998--2007. {PMLR}, 2018.

\bibitem[Zagoruyko \& Komodakis(2016)Zagoruyko and
  Komodakis]{zagoruyko2016wide}
Zagoruyko, S. and Komodakis, N.
\newblock Wide residual networks.
\newblock In \emph{British Machine Vision Conference 2016}. {British Machine
  Vision Association}, 2016.

\bibitem[Zhou et~al.(2018)Zhou, Veitch, Austern, Adams, and
  Orbanz]{zhou2018non}
Zhou, W., Veitch, V., Austern, M., Adams, R.~P., and Orbanz, P.
\newblock Non-vacuous generalization bounds at the {{ImageNet}} scale: A
  {{PAC}}-{{Bayesian}} compression approach.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\end{thebibliography}
