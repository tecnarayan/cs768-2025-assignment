\begin{thebibliography}{55}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Andriushchenko and Flammarion(2022)]{maksym2022}
Maksym Andriushchenko and Nicolas Flammarion.
\newblock Towards understanding sharpness-aware minimization.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 639--668, 2022.

\bibitem[Barrett and Dherin(2021)]{barrett2020implicit}
David~GT Barrett and Benoit Dherin.
\newblock Implicit gradient regularization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2021.

\bibitem[Bartlett et~al.(2022)Bartlett, Long, and
  Bousquet]{bartlett2022dynamics}
Peter~L Bartlett, Philip~M Long, and Olivier Bousquet.
\newblock The dynamics of sharpness-aware minimization: Bouncing across ravines
  and drifting towards wide minima.
\newblock \emph{arXiv:2210.01513}, 2022.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and Nocedal]{bottou2018}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock \emph{arXiv preprint arXiv:1606.04838}, 2016.

\bibitem[Cettolo et~al.(2014)Cettolo, Niehues, Stker, Bentivogli, and
  Federico]{iwslt2014}
M~Cettolo, J~Niehues, S~Stker, L~Bentivogli, and M~Federico.
\newblock Report on the 11th iwslt evaluation campaign, iwslt 2014.
\newblock 2014.

\bibitem[Chaudhari et~al.(2017)Chaudhari, Choromanska, Soatto, LeCun, Baldassi,
  Borgs, Chayes, Sagun, and Zecchina]{pratik2017}
Pratik Chaudhari, Anna Choromanska, Stefano Soatto, Yann LeCun, Carlo Baldassi,
  Christian Borgs, Jennifer Chayes, Levent Sagun, and Riccardo Zecchina.
\newblock Entropy-{SGD}: {B}iasing gradient descent into wide valleys.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2017.

\bibitem[Chen et~al.(2022)Chen, Hsieh, and Gong]{sam4vit}
Xiangning Chen, Cho-Jui Hsieh, and Boqing Gong.
\newblock When vision transformers outperform resnets without pre-training or
  strong data augmentations.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\bibitem[Deng et~al.(2009)Deng, Dong, Socher, Li, Li, and
  Fei-Fei]{imagenet2009}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Image{N}et: A large-scale hierarchical image database.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition}, pages
  248--255, 2009.

\bibitem[Devlin et~al.(2018)Devlin, Chang, Lee, and Toutanova]{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv:1810.04805}, 2018.

\bibitem[Devries and Taylor(2017)]{cutout2017}
Terrance Devries and Graham~W. Taylor.
\newblock Improved regularization of convolutional neural networks with cutout.
\newblock abs/1708.04552, 2017.

\bibitem[Du et~al.(2022{\natexlab{a}})Du, Yan, Feng, Zhou, Zhen, Goh, and
  Tan]{du2022}
Jiawei Du, Hanshu Yan, Jiashi Feng, Joey~Tianyi Zhou, Liangli Zhen, Rick
  Siow~Mong Goh, and Vincent Y.~F. Tan.
\newblock Efficient sharpness-aware minimization for improved training of
  neural networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022{\natexlab{a}}.

\bibitem[Du et~al.(2022{\natexlab{b}})Du, Zhou, Feng, Tan, and Zhou]{du2022saf}
Jiawei Du, Daquan Zhou, Jiashi Feng, Vincent Y.~F. Tan, and Joey~Tianyi Zhou.
\newblock Sharpness-aware training for free.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Dziugaite and Roy(2017)]{dziugaite2017}
Gintare~Karolina Dziugaite and Daniel~M. Roy.
\newblock Computing nonvacuous generalization bounds for deep (stochastic)
  neural networks with many more parameters than training data.
\newblock In \emph{Proc. Conf. Uncerntainty in Artif. Intel.}, 2017.

\bibitem[Foret et~al.(2021)Foret, Kleiner, Mobahi, and Neyshabur]{foret2021}
Pierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2021.

\bibitem[Ghadimi and Lan(2013)]{ghadimi2013}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (4):\penalty0
  2341--2368, 2013.

\bibitem[Ghorbani et~al.(2019)Ghorbani, Krishnan, and Xiao]{behrooz2019}
Behrooz Ghorbani, Shankar Krishnan, and Ying Xiao.
\newblock An investigation into neural net optimization via hessian eigenvalue
  density.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 2232--2241, 2019.

\bibitem[Huang et~al.(2020)Huang, Tao, and Chen]{huang2020}
Feihu Huang, Lue Tao, and Songcan Chen.
\newblock Accelerated stochastic gradient-free and projection-free methods.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 4519--4530. PMLR,
  2020.

\bibitem[Izmailov et~al.(2018)Izmailov, Podoprikhin, Garipov, Vetrov, and
  Wilson]{izmailov2018}
Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry~P. Vetrov, and
  Andrew~Gordon Wilson.
\newblock Averaging weights leads to wider optima and better generalization.
\newblock In \emph{Proc. Conf. Uncerntainty in Artif. Intel.}, pages 876--885,
  2018.

\bibitem[Jastrz{\k{e}}bski et~al.(2017)Jastrz{\k{e}}bski, Kenton, Arpit,
  Ballas, Fischer, Bengio, and Storkey]{Jastrzebski2018}
Stanis{\l}aw Jastrz{\k{e}}bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas,
  Asja Fischer, Yoshua Bengio, and Amos Storkey.
\newblock Three factors influencing minima in sgd.
\newblock \emph{arXiv:1711.04623}, 2017.

\bibitem[Jastrzebski et~al.(2020)Jastrzebski, Szymczak, Fort, Arpit, Tabor,
  Cho, and Geras]{stanislaw2020}
Stanislaw Jastrzebski, Maciej Szymczak, Stanislav Fort, Devansh Arpit, Jacek
  Tabor, Kyunghyun Cho, and Krzysztof~J. Geras.
\newblock The break-even point on optimization trajectories of deep neural
  networks.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2020.

\bibitem[Jiang et~al.(2023)Jiang, Yang, Zhang, and Kwok]{jiang2023}
Weisen Jiang, Hansi Yang, Yu~Zhang, and James Kwok.
\newblock An adaptive policy to employ sharpness-aware minimization.
\newblock \emph{arXiv:2304.14647}, 2023.

\bibitem[Jiang et~al.(2019)Jiang, Neyshabur, Krishnan, Mobahi, and
  Bengio]{jiang2020}
Yiding Jiang, Behnam Neyshabur, Dilip Krishnan, Hossein Mobahi, and Samy
  Bengio.
\newblock Fantastic generalization measures and where to find them.
\newblock \emph{arXiv:1912.02178}, 2019.

\bibitem[Johnson and Zhang(2013)]{johnson2013}
Rie Johnson and Tong Zhang.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.}, pages
  315--323, Lake Tahoe, Nevada, 2013.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016}
Nitish~Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
  and Ping Tak~Peter Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2016.

\bibitem[Kim et~al.(2023)Kim, Park, Choi, Lee, and Lee]{kim2023multi}
Hoki Kim, Jinseong Park, Yujin Choi, Woojin Lee, and Jaewook Lee.
\newblock Exploring the effect of multi-step ascent in sharpness-aware
  minimization.
\newblock \emph{arXiv:2302.10181}, 2023.

\bibitem[Kim et~al.(2022)Kim, Li, Hu, and Hospedales]{kim2022}
Minyoung Kim, Da~Li, Shell~Xu Hu, and Timothy~M. Hospedales.
\newblock Fisher {SAM}: Information geometry and sharpness aware minimisation.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 11148--11161,
  2022.

\bibitem[Kingma and Ba(2014)]{kingma2014}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2014.

\bibitem[Kwon et~al.(2021)Kwon, Kim, Park, and Choi]{kwon2021}
Jungmin Kwon, Jeongseop Kim, Hyunseo Park, and In~Kwon Choi.
\newblock A{SAM}: Adaptive sharpness-aware minimization for scale-invariant
  learning of deep neural networks.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, volume 139, pages
  5905--5914, 2021.

\bibitem[Li et~al.(2020)Li, Wang, and Giannakis]{li2019bb}
Bingcong Li, Lingda Wang, and Georgios~B Giannakis.
\newblock Almost tune-free variance reduction.
\newblock In \emph{Proc. Intl. Conf. on Machine Learning}, 2020.

\bibitem[Li et~al.(2021{\natexlab{a}})Li, Coutino, Giannakis, and Leus]{li2020}
Bingcong Li, Mario Coutino, Georgios~B Giannakis, and Geert Leus.
\newblock A momentum-guided {F}rank-{W}olfe algorithm.
\newblock \emph{IEEE Trans. on Signal Processing}, 69:\penalty0 3597--3611,
  2021{\natexlab{a}}.

\bibitem[Li et~al.(2021{\natexlab{b}})Li, Sadeghi, and Giannakis]{li2021heavy}
Bingcong Li, Alireza Sadeghi, and Georgios Giannakis.
\newblock Heavy ball momentum for conditional gradient.
\newblock In \emph{Proc. Advances in Neural Info. Process. Syst.},
  2021{\natexlab{b}}.

\bibitem[Li et~al.(2021{\natexlab{c}})Li, Wang, Giannakis, and
  Zhao]{li2020extra}
Bingcong Li, Lingda Wang, Georgios~B Giannakis, and Zhizhen Zhao.
\newblock Enhancing {F}rank {W}olfe with an extra subproblem.
\newblock In \emph{Proc. of 35th AAAI Conf. on Artificial Intelligence},
  2021{\natexlab{c}}.

\bibitem[Liu et~al.(2022)Liu, Mai, Chen, Hsieh, and You]{liu2022}
Yong Liu, Siqi Mai, Xiangning Chen, Cho-Jui Hsieh, and Yang You.
\newblock Towards efficient and scalable sharpness-aware minimization.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition}, volume
  2022, pages 12350--12360, 2022.

\bibitem[Loshchilov and Hutter(2017)]{loshchilov2017adamw}
Ilya Loshchilov and Frank Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2017.

\bibitem[Mardia and Jupp(2000)]{mardia2000directional}
K.~V. Mardia and P.~E. Jupp.
\newblock \emph{Directional Statistics}.
\newblock Directional statistics, 2000.

\bibitem[Mi et~al.(2022)Mi, Shen, Ren, Zhou, Sun, Ji, and Tao]{mi2022}
Peng Mi, Li~Shen, Tianhe Ren, Yiyi Zhou, Xiaoshuai Sun, Rongrong Ji, and
  Dacheng Tao.
\newblock Make sharpness-aware minimization stronger: A sparsified perturbation
  approach.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, 2022.

\bibitem[Mokhtari et~al.(2020)Mokhtari, Hassani, and Karbasi]{mokhtari2018}
Aryan Mokhtari, Hamed Hassani, and Amin Karbasi.
\newblock Stochastic conditional gradient methods: From convex minimization to
  submodular maximization.
\newblock \emph{Journal of Machine Learning Research}, 21\penalty0
  (1):\penalty0 4232--4280, 2020.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, Mcallester, Srebro, and
  Srebro]{behnam2017}
Behnam Neyshabur, Srinadh Bhojanapalli, David Mcallester, Nathan Srebro, and
  Nati Srebro.
\newblock Exploring generalization in deep learning.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~30,
  pages 5947--5956, 2017.

\bibitem[Nguyen et~al.(2017)Nguyen, Liu, Scheinberg, and
  Tak{\'a}{\v{c}}]{nguyen2017}
Lam~M Nguyen, Jie Liu, Katya Scheinberg, and Martin Tak{\'a}{\v{c}}.
\newblock {SARAH}: A novel method for machine learning problems using
  stochastic recursive gradient.
\newblock In \emph{Proc. Intl. Conf. Machine Learning}, Sydney, Australia,
  2017.

\bibitem[Reddi et~al.(2016)Reddi, Sra, P{\'o}czos, and Smola]{reddi2016}
Sashank~J Reddi, Suvrit Sra, Barnab{\'a}s P{\'o}czos, and Alex Smola.
\newblock Stochastic {F}rank-{W}olfe methods for nonconvex optimization.
\newblock In \emph{Allerton conference on communication, control, and
  computing}, pages 1244--1251. IEEE, 2016.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{dropout2014}
Nitish Srivastava, Geoffrey~E. Hinton, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan Salakhutdinov.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{J. Mach. Learn. Res.}, 15:\penalty0 1929--1958, 2014.

\bibitem[Tom et~al.(2020)Tom, Benjamin, Nick, Melanie, Jared, Prafulla, Arvind,
  Pranav, Girish, Amanda, Sandhini, Ariel, Gretchen, Tom, Rewon, Aditya, M.,
  Jeffrey, Clemens, Christopher, Mark, Eric, Mateusz, Scott, Benjamin, Jack,
  Christopher, Sam, Alec, Ilya, and Dario]{gpt3}
Brown Tom, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared, Dhariwal
  Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell Amanda,
  Agarwal Sandhini, Herbert-Voss Ariel, Krueger Gretchen, Henighan Tom, Child
  Rewon, Ramesh Aditya, Ziegler~Daniel M., Wu~Jeffrey, Winter Clemens, Hesse
  Christopher, Chen Mark, Sigler Eric, Litwin Mateusz, Gray Scott, Chess
  Benjamin, Clark Jack, Berner Christopher, McCandlish Sam, Radford Alec,
  Sutskever Ilya, and Amodei Dario.
\newblock Language models are few-shot learners.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~33,
  pages 1877--1901, 2020.

\bibitem[Tsiligkaridis and Roberts(2022)]{tsiligkaridis2022}
Theodoros Tsiligkaridis and Jay Roberts.
\newblock Understanding and increasing efficiency of frank-wolfe adversarial
  training.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition}, pages
  50--59, 2022.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~30,
  2017.

\bibitem[Wang et~al.(2023)Wang, Zhang, Lei, and Zhang]{wang2023}
Pengfei Wang, Zhaoxiang Zhang, Zhen Lei, and Lei Zhang.
\newblock Sharpness-aware gradient matching for domain generalization.
\newblock In \emph{Proc. Conf. Computer Vision and Pattern Recognition}, pages
  3769--3778, 2023.

\bibitem[Wang and Mao(2022)]{wang2022}
Ziqiao Wang and Yongyi Mao.
\newblock On the generalization of models trained with sgd:
  Information-theoretic bounds and implications.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\bibitem[Wen et~al.(2023)Wen, Ma, and hiyuan Li]{wen2023}
Kaiyue Wen, Tengyu Ma, and Z~hiyuan Li.
\newblock How does sharpness-aware minimization minimizes sharpness.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2023.

\bibitem[Wilson et~al.(2017)Wilson, Roelofs, Stern, Srebro, Recht, and
  Srebro]{wilson2017}
Ashia~C. Wilson, Rebecca Roelofs, Mitchell Stern, Nathan Srebro, Benjamin
  Recht, and Nati Srebro.
\newblock The marginal value of adaptive gradient methods in machine learning.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, volume~30, pages
  4148--4158, 2017.

\bibitem[Wu et~al.(2020)Wu, Xia, and Wang]{wu2020}
Dongxian Wu, Shu-Tao Xia, and Yisen Wang.
\newblock Adversarial weight perturbation helps robust generalization.
\newblock In \emph{Proc. Adv. Neural Info. Processing Systems}, volume~33,
  pages 2958--2969, 2020.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016}
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
\newblock Understanding deep learning (still) requires rethinking
  generalization.
\newblock \emph{Communications of the ACM}, 64\penalty0 (3):\penalty0 107--115,
  2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Li, and Giannakis]{zhang2021}
Yilang Zhang, Bingcong Li, and Georgios~B Giannakis.
\newblock Accelerating frank-wolfe with weighted average gradients.
\newblock In \emph{Proc. IEEE Int. Conf. Acoust., Speech, Sig. Process.}, pages
  5529--5533, 2021{\natexlab{b}}.

\bibitem[Zhang et~al.(2022)Zhang, Luo, Su, and Sun]{gasam2022}
Zhiyuan Zhang, Ruixuan Luo, Qi~Su, and Xu~Sun.
\newblock {GA-SAM}: Gradient-strength based adaptive sharpness-aware
  minimization for improved generalization.
\newblock In \emph{Proc. Conf. Empirical Methods in Natural Language
  Processing}, 2022.

\bibitem[Zhao et~al.(2022{\natexlab{a}})Zhao, Zhang, and Hu]{yang2022}
Yang Zhao, Hao Zhang, and Xiuyuan Hu.
\newblock Penalizing gradient norm for efficiently improving generalization in
  deep learning.
\newblock In \emph{Proc. Int. Conf. Machine Learning}, pages 26982--26992,
  2022{\natexlab{a}}.

\bibitem[Zhao et~al.(2022{\natexlab{b}})Zhao, Zhang, and Hu]{zhao222}
Yang Zhao, Hao Zhang, and Xiuyuan Hu.
\newblock {SS-SAM}: Stochastic scheduled sharpness-aware minimization for
  efficiently training deep neural networks.
\newblock \emph{arXiv:2203.09962}, 2022{\natexlab{b}}.

\bibitem[Zhuang et~al.(2022)Zhuang, Gong, Yuan, Cui, Adam, Dvornek, Tatikonda,
  Duncan, and Liu]{zhuang2022}
Juntang Zhuang, Boqing Gong, Liangzhe Yuan, Yin Cui, Hartwig Adam, Nicha
  Dvornek, Sekhar Tatikonda, James Duncan, and Ting Liu.
\newblock Surrogate gap minimization improves sharpness-aware training.
\newblock In \emph{Proc. Int. Conf. Learning Represention}, 2022.

\end{thebibliography}
