\begin{thebibliography}{98}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abadi et~al.(2015)Abadi, Agarwal, Barham, Brevdo, Chen, Citro,
  Corrado, Davis, Dean, Devin, Ghemawat, Goodfellow, Harp, Irving, Isard, Jia,
  Jozefowicz, Kaiser, Kudlur, Levenberg, Man\'{e}, Monga, Moore, Murray, Olah,
  Schuster, Shlens, Steiner, Sutskever, Talwar, Tucker, Vanhoucke, Vasudevan,
  Vi\'{e}gas, Vinyals, Warden, Wattenberg, Wicke, Yu, and
  Zheng]{tensorflow2015-whitepaper}
Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado,
  G.~S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp,
  A., Irving, G., Isard, M., Jia, Y., Jozefowicz, R., Kaiser, L., Kudlur, M.,
  Levenberg, J., Man\'{e}, D., Monga, R., Moore, S., Murray, D., Olah, C.,
  Schuster, M., Shlens, J., Steiner, B., Sutskever, I., Talwar, K., Tucker, P.,
  Vanhoucke, V., Vasudevan, V., Vi\'{e}gas, F., Vinyals, O., Warden, P.,
  Wattenberg, M., Wicke, M., Yu, Y., and Zheng, X.
\newblock {TensorFlow: Large-scale machine learning on heterogeneous systems},
  2015.
\newblock URL \url{https://www.tensorflow.org/}.
\newblock Software available from tensorflow.org.

\bibitem[Ali et~al.(2019)Ali, Kolter, and Tibshirani]{ali2019continuous}
Ali, A., Kolter, J.~Z., and Tibshirani, R.~J.
\newblock A continuous-time view of early stopping for least squares
  regression.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1370--1378, 2019.

\bibitem[Ali et~al.(2020)Ali, Dobriban, and Tibshirani]{ali2020implicit}
Ali, A., Dobriban, E., and Tibshirani, R.
\newblock The implicit regularization of stochastic gradient flow for least
  squares.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  233--244, 2020.

\bibitem[Amari et~al.(2020)Amari, Ba, Grosse, Li, Nitanda, Suzuki, Wu, and
  Xu]{amari2020does}
Amari, S.-i., Ba, J., Grosse, R., Li, X., Nitanda, A., Suzuki, T., Wu, D., and
  Xu, J.
\newblock When does preconditioning help or hurt generalization?
\newblock \emph{arXiv preprint arXiv:2006.10732}, 2020.

\bibitem[Angelos et~al.(1998)Angelos, Grossman, Kaufman, Lenker, and
  Rakesh]{angelos1998limit}
Angelos, J., Grossman, G., Kaufman, E., Lenker, T., and Rakesh, L.
\newblock {Limit cycles for successive projections onto hyperplanes in RN}.
\newblock \emph{Linear Algebra and its Applications}, 285\penalty0
  (1-3):\penalty0 201--228, 1998.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Arora, S., Du, S., Hu, W., Li, Z., and Wang, R.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  322--332. PMLR, 2019.

\bibitem[Bae \& Grosse(2020)Bae and Grosse]{bae2020delta}
Bae, J. and Grosse, R.
\newblock {Delta-STN: Efficient bilevel optimization for neural networks using
  structured response Jacobians}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30063--30070, 2020.

\bibitem[Basri et~al.(2020)Basri, Galun, Geifman, Jacobs, Kasten, and
  Kritchman]{basri2020frequency}
Basri, R., Galun, M., Geifman, A., Jacobs, D., Kasten, Y., and Kritchman, S.
\newblock Frequency bias in neural networks for input of non-uniform density.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  685--694. PMLR, 2020.

\bibitem[Belkin(2021)]{belkin2021fit}
Belkin, M.
\newblock {Fit without fear: Remarkable mathematical phenomena of deep learning
  through the prism of interpolation}.
\newblock \emph{Acta Numerica}, 30:\penalty0 203--248, 2021.

\bibitem[Belkin et~al.(2019)Belkin, Hsu, Ma, and Mandal]{belkin2018reconciling}
Belkin, M., Hsu, D., Ma, S., and Mandal, S.
\newblock Reconciling modern machine-learning practice and the classical
  bias--variance trade-off.
\newblock \emph{Proceedings of the National Academy of Sciences}, 116\penalty0
  (32):\penalty0 15849--15854, 2019.

\bibitem[Bengio(2000)]{bengio2000gradient}
Bengio, Y.
\newblock Gradient-based optimization of hyperparameters.
\newblock \emph{Neural Computation}, 12\penalty0 (8):\penalty0 1889--1900,
  2000.

\bibitem[Bengio et~al.(2009)Bengio, Louradour, Collobert, and
  Weston]{bengio2009curriculum}
Bengio, Y., Louradour, J., Collobert, R., and Weston, J.
\newblock Curriculum learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  41--48, 2009.

\bibitem[Blondel et~al.(2021)Blondel, Berthet, Cuturi, Frostig, Hoyer,
  Llinares-L{\'o}pez, Pedregosa, and Vert]{blondel2021efficient}
Blondel, M., Berthet, Q., Cuturi, M., Frostig, R., Hoyer, S.,
  Llinares-L{\'o}pez, F., Pedregosa, F., and Vert, J.-P.
\newblock Efficient and modular implicit differentiation.
\newblock \emph{arXiv preprint arXiv:2105.15183}, 2021.

\bibitem[Bradbury et~al.(2018)Bradbury, Frostig, Hawkins, Johnson, Leary,
  Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and
  Zhang]{jax2018github}
Bradbury, J., Frostig, R., Hawkins, P., Johnson, M.~J., Leary, C., Maclaurin,
  D., Necula, G., Paszke, A., Vander{P}las, J., Wanderman-{M}ilne, S., and
  Zhang, Q.
\newblock {JAX: Composable transformations of Python+NumPy programs}, 2018.
\newblock URL \url{http://github.com/google/jax}.

\bibitem[Chen \& Hagan(1999)Chen and Hagan]{chen1999optimal}
Chen, D. and Hagan, M.~T.
\newblock Optimal use of regularization and cross-validation in neural network
  modeling.
\newblock In \emph{International Joint Conference on Neural Networks (IJCNN)},
  volume~2, pp.\  1275--1280, 1999.

\bibitem[Dempe(2002)]{dempe2002foundations}
Dempe, S.
\newblock \emph{Foundations of bilevel programming}.
\newblock Springer Science \& Business Media, 2002.

\bibitem[Dempe et~al.(2007)Dempe, Dutta, and Mordukhovich]{dempe2007new}
Dempe, S., Dutta, J., and Mordukhovich, B.
\newblock New necessary optimality conditions in optimistic bilevel
  programming.
\newblock \emph{Optimization}, 56\penalty0 (5-6):\penalty0 577--604, 2007.

\bibitem[Dempe et~al.(2014)Dempe, Mordukhovich, and
  Zemkoho]{dempe2014necessary}
Dempe, S., Mordukhovich, B.~S., and Zemkoho, A.~B.
\newblock Necessary optimality conditions in pessimistic bilevel programming.
\newblock \emph{Optimization}, 63\penalty0 (4):\penalty0 505--533, 2014.

\bibitem[Domke(2012)]{domke2012generic}
Domke, J.
\newblock Generic methods for optimization-based modeling.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  318--326,
  2012.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{finn2017model}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1126--1135, 2017.

\bibitem[Foo et~al.(2008)Foo, Do, and Ng]{foo2008efficient}
Foo, C.-s., Do, C.~B., and Ng, A.~Y.
\newblock Efficient multiple hyperparameter learning for log-linear models.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  377--384, 2008.

\bibitem[Franceschi et~al.(2018)Franceschi, Frasconi, Salzo, Grazzi, and
  Pontil]{franceschi2018bilevel}
Franceschi, L., Frasconi, P., Salzo, S., Grazzi, R., and Pontil, M.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock \emph{arXiv preprint arXiv:1806.04910}, 2018.

\bibitem[Friedman \& Popescu(2003)Friedman and Popescu]{friedman2003gradient}
Friedman, J. and Popescu, B.~E.
\newblock Gradient directed regularization for linear regression and
  classification.
\newblock Technical report, Statistics Department, Stanford University, 2003.

\bibitem[Gerfo et~al.(2008)Gerfo, Rosasco, Odone, Vito, and
  Verri]{gerfo2008spectral}
Gerfo, L.~L., Rosasco, L., Odone, F., Vito, E.~D., and Verri, A.
\newblock Spectral algorithms for supervised learning.
\newblock \emph{Neural Computation}, 20\penalty0 (7):\penalty0 1873--1897,
  2008.

\bibitem[Ghadimi \& Wang(2018)Ghadimi and Wang]{ghadimi2018approximation}
Ghadimi, S. and Wang, M.
\newblock Approximation methods for bilevel programming.
\newblock \emph{arXiv preprint arXiv:1802.02246}, 2018.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Goodfellow, I.~J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
  Ozair, S., Courville, A., and Bengio, Y.
\newblock Generative adversarial networks.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2014.

\bibitem[Grazzi et~al.(2020{\natexlab{a}})Grazzi, Franceschi, Pontil, and
  Salzo]{grazzi2020iteration}
Grazzi, R., Franceschi, L., Pontil, M., and Salzo, S.
\newblock On the iteration complexity of hypergradient computation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  3748--3758, 2020{\natexlab{a}}.

\bibitem[Grazzi et~al.(2020{\natexlab{b}})Grazzi, Pontil, and
  Salzo]{grazzi2020convergence}
Grazzi, R., Pontil, M., and Salzo, S.
\newblock Convergence properties of stochastic hypergradients.
\newblock \emph{arXiv preprint arXiv:2011.07122}, 2020{\natexlab{b}}.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
Gunasekar, S., Lee, J., Soudry, D., and Srebro, N.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1832--1841, 2018.

\bibitem[Ha et~al.(2017)Ha, Dai, and Le]{ha2016hypernetworks}
Ha, D., Dai, A., and Le, Q.~V.
\newblock Hypernetworks.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\bibitem[Harker \& Pang(1988)Harker and Pang]{harker1988existence}
Harker, P.~T. and Pang, J.-S.
\newblock Existence of optimal solutions to mathematical programs with
  equilibrium constraints.
\newblock \emph{Operations Research Letters}, 7\penalty0 (2):\penalty0 61--64,
  1988.

\bibitem[Hataya et~al.(2020{\natexlab{a}})Hataya, Zdenek, Yoshizoe, and
  Nakayama]{hataya2020faster}
Hataya, R., Zdenek, J., Yoshizoe, K., and Nakayama, H.
\newblock {Faster Autoaugment: Learning augmentation strategies using
  backpropagation}.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, pp.\  1--16,
  2020{\natexlab{a}}.

\bibitem[Hataya et~al.(2020{\natexlab{b}})Hataya, Zdenek, Yoshizoe, and
  Nakayama]{hataya2020meta}
Hataya, R., Zdenek, J., Yoshizoe, K., and Nakayama, H.
\newblock Meta approach to data augmentation optimization.
\newblock \emph{arXiv preprint arXiv:2006.07965}, 2020{\natexlab{b}}.

\bibitem[Ho et~al.(2019)Ho, Liang, Chen, Stoica, and Abbeel]{ho2019population}
Ho, D., Liang, E., Chen, X., Stoica, I., and Abbeel, P.
\newblock {Population based augmentation: Efficient learning of augmentation
  policy schedules}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2731--2741, 2019.

\bibitem[Hong et~al.(2020)Hong, Wai, Wang, and Yang]{hong2020two}
Hong, M., Wai, H.-T., Wang, Z., and Yang, Z.
\newblock {A two-timescale framework for bilevel optimization: Complexity
  analysis and application to actor-critic}.
\newblock \emph{arXiv preprint arXiv:2007.05170}, 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
Jacot, A., Gabriel, F., and Hongler, C.
\newblock {Neural tangent kernel: Convergence and generalization in neural
  networks}.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2018.

\bibitem[Jaderberg et~al.(2017)Jaderberg, Dalibard, Osindero, Czarnecki,
  Donahue, Razavi, Vinyals, Green, Dunning, Simonyan,
  et~al.]{jaderberg2017population}
Jaderberg, M., Dalibard, V., Osindero, S., Czarnecki, W.~M., Donahue, J.,
  Razavi, A., Vinyals, O., Green, T., Dunning, I., Simonyan, K., et~al.
\newblock Population based training of neural networks.
\newblock \emph{arXiv preprint arXiv:1711.09846}, 2017.

\bibitem[Ji \& Liang(2021)Ji and Liang]{ji2021lower}
Ji, K. and Liang, Y.
\newblock Lower bounds and accelerated algorithms for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2102.03926}, 2021.

\bibitem[Ji et~al.(2020)Ji, Yang, and Liang]{ji2020bilevel}
Ji, K., Yang, J., and Liang, Y.
\newblock {Bilevel optimization: Nonasymptotic analysis and faster algorithms}.
\newblock \emph{arXiv preprint arXiv:2010.07962}, 2020.

\bibitem[Ji et~al.(2021)Ji, Yang, and Liang]{ji2021bilevel}
Ji, K., Yang, J., and Liang, Y.
\newblock {Bilevel optimization: Convergence analysis and enhanced design}.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4882--4892, 2021.

\bibitem[Ji \& Telgarsky(2019)Ji and Telgarsky]{ji2019implicit}
Ji, Z. and Telgarsky, M.
\newblock The implicit bias of gradient descent on nonseparable data.
\newblock In \emph{Conference on Learning Theory}, pp.\  1772--1798, 2019.

\bibitem[Karczmarz(1937)]{karczmarz1937angenaherte}
Karczmarz, S.
\newblock {Angenaherte Auflosung von Systemen linearer Gleichungen}.
\newblock \emph{Bull. Int. Acad. Pol. Sic. Let., Cl. Sci. Math. Nat.}, pp.\
  355--357, 1937.

\bibitem[Koh \& Liang(2017)Koh and Liang]{koh2017understanding}
Koh, P.~W. and Liang, P.
\newblock Understanding black-box predictions via influence functions.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  1885--1894, 2017.

\bibitem[Larsen et~al.(1996)Larsen, Hansen, Svarer, and
  Ohlsson]{larsen1996design}
Larsen, J., Hansen, L.~K., Svarer, C., and Ohlsson, M.
\newblock {Design and regularization of neural networks: The optimal use of a
  validation set}.
\newblock In \emph{IEEE Signal Processing Society Workshop}, pp.\  62--71,
  1996.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S.~S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Liao et~al.(2018)Liao, Xiong, Fetaya, Zhang, Yoon, Pitkow, Urtasun,
  and Zemel]{liao2018reviving}
Liao, R., Xiong, Y., Fetaya, E., Zhang, L., Yoon, K., Pitkow, X., Urtasun, R.,
  and Zemel, R.
\newblock Reviving and improving recurrent back-propagation.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2018.

\bibitem[Lignola \& Morgan(1995)Lignola and Morgan]{lignola1995topological}
Lignola, M.~B. and Morgan, J.
\newblock {Topological existence and stability for Stackelberg problems}.
\newblock \emph{Journal of Optimization Theory and Applications}, 84\penalty0
  (1):\penalty0 145--169, 1995.

\bibitem[Lignola \& Morgan(2001)Lignola and Morgan]{lignola2001existence}
Lignola, M.~B. and Morgan, J.
\newblock {Existence of solutions to bilevel variational problems in Banach
  spaces}.
\newblock In \emph{Equilibrium Problems: Nonsmooth Optimization and Variational
  Inequality Models}, pp.\  161--174. Springer, 2001.

\bibitem[Liu et~al.(2019)Liu, Simonyan, and Yang]{liu2018darts}
Liu, H., Simonyan, K., and Yang, Y.
\newblock {DARTS: Differentiable architecture search}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Liu et~al.(2018)Liu, Fan, Chen, and Zheng]{liu2018pessimistic}
Liu, J., Fan, Y., Chen, Z., and Zheng, Y.
\newblock {Pessimistic bilevel optimization: A survey}.
\newblock \emph{International Journal of Computational Intelligence Systems},
  11\penalty0 (1):\penalty0 725--736, 2018.

\bibitem[Liu et~al.(2020)Liu, Fan, Chen, and Zheng]{liu2020methods}
Liu, J., Fan, Y., Chen, Z., and Zheng, Y.
\newblock Methods for pessimistic bilevel optimization.
\newblock In \emph{Bilevel Optimization}, pp.\  403--420. Springer, 2020.

\bibitem[Loridan \& Morgan(1996)Loridan and Morgan]{loridan1996weak}
Loridan, P. and Morgan, J.
\newblock {Weak via strong Stackelberg problem: New results}.
\newblock \emph{Journal of Global Optimization}, 8\penalty0 (3):\penalty0
  263--287, 1996.

\bibitem[Lorraine \& Duvenaud(2017)Lorraine and
  Duvenaud]{lorraine2018stochastic}
Lorraine, J. and Duvenaud, D.
\newblock Stochastic hyperparameter optimization through hypernetworks.
\newblock In \emph{NIPS Meta-Learning Workshop}, 2017.

\bibitem[Lorraine et~al.(2020)Lorraine, Vicol, and
  Duvenaud]{lorraine2020optimizing}
Lorraine, J., Vicol, P., and Duvenaud, D.
\newblock Optimizing millions of hyperparameters by implicit differentiation.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1540--1552, 2020.

\bibitem[Lucchetti et~al.(1987)Lucchetti, Mignanego, and
  Pieri]{lucchetti1987existence}
Lucchetti, R., Mignanego, F., and Pieri, G.
\newblock {Existence theorems of equilibrium points in Stackelberg games with
  constraints}.
\newblock \emph{Optimization}, 18\penalty0 (6):\penalty0 857--866, 1987.

\bibitem[Luketina et~al.(2016)Luketina, Berglund, Greff, and
  Raiko]{luketina2016scalable}
Luketina, J., Berglund, M., Greff, K., and Raiko, T.
\newblock Scalable gradient-based tuning of continuous regularization
  hyperparameters.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2952--2960, 2016.

\bibitem[MacKay et~al.(2019)MacKay, Vicol, Lorraine, Duvenaud, and
  Grosse]{mackay2019self}
MacKay, M., Vicol, P., Lorraine, J., Duvenaud, D., and Grosse, R.
\newblock {Self-Tuning Networks: Bilevel optimization of hyperparameters using
  structured best-response functions}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2019.

\bibitem[Maclaurin et~al.(2015)Maclaurin, Duvenaud, and
  Adams]{maclaurin2015gradient}
Maclaurin, D., Duvenaud, D., and Adams, R.
\newblock Gradient-based hyperparameter optimization through reversible
  learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  2113--2122, 2015.

\bibitem[Mehra \& Hamm(2019)Mehra and Hamm]{mehra2019penalty}
Mehra, A. and Hamm, J.
\newblock Penalty method for inversion-free deep bilevel optimization.
\newblock \emph{arXiv preprint arXiv:1911.03432}, 2019.

\bibitem[Metz et~al.(2019)Metz, Maheswaranathan, Nixon, Freeman, and
  Sohl-Dickstein]{metz2019understanding}
Metz, L., Maheswaranathan, N., Nixon, J., Freeman, D., and Sohl-Dickstein, J.
\newblock Understanding and correcting pathologies in the training of learned
  optimizers.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4556--4565, 2019.

\bibitem[Micaelli \& Storkey(2020)Micaelli and Storkey]{micaelli2020non}
Micaelli, P. and Storkey, A.
\newblock Non-greedy gradient-based hyperparameter optimization over long
  horizons.
\newblock \emph{arXiv preprint arXiv:2007.07869}, 2020.

\bibitem[Mishachev \& Shmyrin(2019)Mishachev and
  Shmyrin]{mishachev2019realization}
Mishachev, N. and Shmyrin, A.~M.
\newblock On realization of limit polygons in sequential projection method.
\newblock \emph{International Transaction Journal of Engineering, Management
  and Applied Sciences and Technologies}, 10\penalty0 (15):\penalty0
  1015--1015, 2019.

\bibitem[Morgan \& Bourlard(1989)Morgan and Bourlard]{morgan1989generalization}
Morgan, N. and Bourlard, H.
\newblock {Generalization and parameter estimation in feedforward nets: Some
  experiments}.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  2:\penalty0 630--637, 1989.

\bibitem[Mounsaveng et~al.(2021)Mounsaveng, Laradji, Ben~Ayed, Vazquez, and
  Pedersoli]{mounsaveng2021learning}
Mounsaveng, S., Laradji, I., Ben~Ayed, I., Vazquez, D., and Pedersoli, M.
\newblock Learning data augmentation with online bilevel optimization for image
  classification.
\newblock In \emph{Winter Conference on Applications of Computer Vision}, pp.\
  1691--1700, 2021.

\bibitem[Nacson et~al.(2019)Nacson, Srebro, and Soudry]{nacson2019stochastic}
Nacson, M.~S., Srebro, N., and Soudry, D.
\newblock {Stochastic gradient descent on separable data: Exact convergence
  with a fixed learning rate}.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  3051--3059, 2019.

\bibitem[Nakkiran et~al.(2020)Nakkiran, Kaplun, Bansal, Yang, Barak, and
  Sutskever]{nakkiran2019deep}
Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., and Sutskever, I.
\newblock {Deep double descent: Where bigger models and more data hurt}.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Outrata(1993)]{outrata1993necessary}
Outrata, J.~V.
\newblock {Necessary optimality conditions for Stackelberg problems}.
\newblock \emph{Journal of Optimization Theory and Applications}, 76\penalty0
  (2):\penalty0 305--320, 1993.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{paszke2017automatic}
Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
  Desmaison, A., Antiga, L., and Lerer, A.
\newblock {Automatic differentiation in PyTorch}.
\newblock In \emph{NIPS Workshop Autodifferentiation}, 2017.

\bibitem[Pearlmutter(1994)]{pearlmutter1994fast}
Pearlmutter, B.~A.
\newblock {Fast exact multiplication by the Hessian}.
\newblock \emph{Neural Computation}, 6\penalty0 (1):\penalty0 147--160, 1994.

\bibitem[Pedregosa(2016)]{pedregosa2016hyperparameter}
Pedregosa, F.
\newblock Hyperparameter optimization with approximate gradient.
\newblock In \emph{International Conference on Machine Learning (ICML)}, 2016.

\bibitem[Peng et~al.(2018)Peng, Tang, Yang, Feris, and
  Metaxas]{peng2018jointly}
Peng, X., Tang, Z., Yang, F., Feris, R.~S., and Metaxas, D.
\newblock {Jointly optimize data augmentation and network training: Adversarial
  data augmentation in human pose estimation}.
\newblock In \emph{Conference on Computer Vision and Pattern Recognition
  (CVPR)}, pp.\  2226--2234, 2018.

\bibitem[Pfau \& Vinyals(2016)Pfau and Vinyals]{pfau2016connecting}
Pfau, D. and Vinyals, O.
\newblock Connecting generative adversarial networks and actor-critic methods.
\newblock In \emph{NeurIPS Workshop on Adversarial Training}, 2016.

\bibitem[Poggio et~al.(2019)Poggio, Banburski, and Liao]{poggio2019theoretical}
Poggio, T., Banburski, A., and Liao, Q.
\newblock {Theoretical issues in deep networks: Approximation, optimization and
  generalization}.
\newblock In \emph{Proceedings of the National Academy of Sciences}, 2019.

\bibitem[Raghu et~al.(2020)Raghu, Raghu, Kornblith, Duvenaud, and
  Hinton]{raghu2020teaching}
Raghu, A., Raghu, M., Kornblith, S., Duvenaud, D., and Hinton, G.
\newblock Teaching with commentaries.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Rahaman et~al.(2019)Rahaman, Baratin, Arpit, Draxler, Lin, Hamprecht,
  Bengio, and Courville]{rahaman2019spectral}
Rahaman, N., Baratin, A., Arpit, D., Draxler, F., Lin, M., Hamprecht, F.,
  Bengio, Y., and Courville, A.
\newblock On the spectral bias of neural networks.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  5301--5310. PMLR, 2019.

\bibitem[Rajeswaran et~al.(2019)Rajeswaran, Finn, Kakade, and
  Levine]{rajeswaran2019meta}
Rajeswaran, A., Finn, C., Kakade, S., and Levine, S.
\newblock Meta-learning with implicit gradients.
\newblock In \emph{Advances in Neural Information Processing Systems
  (NeurIPS)}, 2019.

\bibitem[Ren et~al.(2018)Ren, Zeng, Yang, and Urtasun]{ren2018learning}
Ren, M., Zeng, W., Yang, B., and Urtasun, R.
\newblock Learning to reweight examples for robust deep learning.
\newblock In \emph{International Conference on Machine Learning (ICML)}, pp.\
  4334--4343, 2018.

\bibitem[Rosasco(2009)]{RosascoLecture}
Rosasco, L.
\newblock {9.520: Statistical Learning Theory and Applications, Lecture 7}.
\newblock {Lecture Notes}, 2009.
\newblock URL
  \url{https://www.mit.edu/~9.520/spring09/Classes/class07_spectral.pdf}.

\bibitem[Shaban et~al.(2019)Shaban, Cheng, Hatch, and
  Boots]{shaban2019truncated}
Shaban, A., Cheng, C.-A., Hatch, N., and Boots, B.
\newblock Truncated back-propagation for bilevel optimization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics (AISTATS)}, pp.\  1723--1732, 2019.

\bibitem[Sinha et~al.(2017)Sinha, Malo, and Deb]{sinha2017review}
Sinha, A., Malo, P., and Deb, K.
\newblock {A review on bilevel optimization: From classical to evolutionary
  approaches and applications}.
\newblock \emph{IEEE Transactions on Evolutionary Computation}, 22\penalty0
  (2):\penalty0 276--295, 2017.

\bibitem[Sohl-Dickstein et~al.(2020)Sohl-Dickstein, Novak, Schoenholz, and
  Lee]{sohl2020infinite}
Sohl-Dickstein, J., Novak, R., Schoenholz, S.~S., and Lee, J.
\newblock On the infinite width limit of neural networks with a standard
  parameterization.
\newblock \emph{arXiv preprint arXiv:2001.07301}, 2020.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
Soudry, D., Hoffer, E., Nacson, M.~S., Gunasekar, S., and Srebro, N.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Sto{\v{s}}i{\'c} et~al.(2016)Sto{\v{s}}i{\'c}, Xavier, and
  Dodig]{stovsic2016projection}
Sto{\v{s}}i{\'c}, M., Xavier, J., and Dodig, M.
\newblock Projection on the intersection of convex sets.
\newblock \emph{Linear Algebra and its Applications}, 509:\penalty0 191--205,
  2016.

\bibitem[Strand(1974)]{strand1974theory}
Strand, O.~N.
\newblock {Theory and methods related to the singular-function expansion and
  Landweberâ€™s iteration for integral equations of the first kind}.
\newblock \emph{SIAM Journal on Numerical Analysis}, 11\penalty0 (4):\penalty0
  798--825, 1974.

\bibitem[Suggala et~al.(2018)Suggala, Prasad, and
  Ravikumar]{suggala2018connecting}
Suggala, A., Prasad, A., and Ravikumar, P.~K.
\newblock Connecting optimization and regularization paths.
\newblock \emph{Advances in Neural Information Processing Systems (NeurIPS)},
  31:\penalty0 10608--10619, 2018.

\bibitem[Tancik et~al.(2020)Tancik, Srinivasan, Mildenhall, Fridovich-Keil,
  Raghavan, Singhal, Ramamoorthi, Barron, and Ng]{tancik2020fourier}
Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N.,
  Singhal, U., Ramamoorthi, R., Barron, J., and Ng, R.
\newblock Fourier features let networks learn high frequency functions in low
  dimensional domains.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7537--7547, 2020.

\bibitem[Tang et~al.(2020)Tang, Gao, Karlinsky, Sattigeri, Feris, and
  Metaxas]{tang2020onlineaugment}
Tang, Z., Gao, Y., Karlinsky, L., Sattigeri, P., Feris, R., and Metaxas, D.
\newblock {OnlineAugment: Online data augmentation with less domain knowledge}.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2020.

\bibitem[Vardi \& Shamir(2021)Vardi and Shamir]{vardi2021implicit}
Vardi, G. and Shamir, O.
\newblock {Implicit regularization in ReLU networks with the square loss}.
\newblock In \emph{Conference on Learning Theory}, pp.\  4224--4258, 2021.

\bibitem[Wang et~al.(2018)Wang, Zhu, Torralba, and Efros]{wang2018dataset}
Wang, T., Zhu, J.-Y., Torralba, A., and Efros, A.~A.
\newblock Dataset distillation.
\newblock \emph{arXiv preprint arXiv:1811.10959}, 2018.

\bibitem[Wiesemann et~al.(2013)Wiesemann, Tsoukalas, Kleniati, and
  Rustem]{wiesemann2013pessimistic}
Wiesemann, W., Tsoukalas, A., Kleniati, P.-M., and Rustem, B.
\newblock Pessimistic bilevel optimization.
\newblock \emph{SIAM Journal on Optimization}, 23\penalty0 (1):\penalty0
  353--380, 2013.

\bibitem[Wu \& Xu(2020)Wu and Xu]{wu2020optimal}
Wu, D. and Xu, J.
\newblock On the optimal weighted $\ell_2$ regularization in overparameterized
  linear regression.
\newblock \emph{arXiv preprint arXiv:2006.05800}, 2020.

\bibitem[Wu et~al.(2018)Wu, Ren, Liao, and Grosse]{wu2018understanding}
Wu, Y., Ren, M., Liao, R., and Grosse, R.
\newblock Understanding short-horizon bias in stochastic meta-optimization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2018.

\bibitem[Yang et~al.(2021)Yang, Ji, and Liang]{yang2021provably}
Yang, J., Ji, K., and Liang, Y.
\newblock Provably faster algorithms for bilevel optimization.
\newblock \emph{arXiv preprint arXiv:2106.04692}, 2021.

\bibitem[Yao et~al.(2007)Yao, Rosasco, and Caponnetto]{yao2007early}
Yao, Y., Rosasco, L., and Caponnetto, A.
\newblock On early stopping in gradient descent learning.
\newblock \emph{Constructive Approximation}, 26\penalty0 (2):\penalty0
  289--315, 2007.

\bibitem[Zhang et~al.(2016)Zhang, Bengio, Hardt, Recht, and
  Vinyals]{zhang2016understanding}
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O.
\newblock Understanding deep learning requires rethinking generalization.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2016.

\bibitem[Zhao et~al.(2021)Zhao, Mopuri, and Bilen]{zhao2020dataset}
Zhao, B., Mopuri, K.~R., and Bilen, H.
\newblock Dataset condensation with gradient matching.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2021.

\bibitem[Zoph \& Le(2017)Zoph and Le]{zoph2016neural}
Zoph, B. and Le, Q.~V.
\newblock Neural architecture search with reinforcement learning.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2017.

\end{thebibliography}
