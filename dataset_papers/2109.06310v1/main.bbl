\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Doroudi et~al.(2018)Doroudi, Thomas, and
  Brunskill]{doroudi2018importance}
Doroudi, S., Thomas, P., and Brunskill, E.
\newblock Importance sampling for fair policy selection.
\newblock In \emph{Proceedings of the 27th International Joint Conference on
  Artificial Intelligence}, pp.\  5239--5243, 2018.

\bibitem[Farajtabar et~al.(2018)Farajtabar, Chow, and
  Ghavamzadeh]{farajtabar2018more}
Farajtabar, M., Chow, Y., and Ghavamzadeh, M.
\newblock More robust doubly robust off-policy evaluation.
\newblock In \emph{Proceedings of the 35th International Conference on Machine
  Learning}, pp.\  1446--1455, 2018.

\bibitem[Gottesman et~al.(2020)Gottesman, Futoma, Liu, Parbhoo, Celi,
  Brunskill, and Doshi-Velez]{gottesman2020interpretable}
Gottesman, O., Futoma, J., Liu, Y., Parbhoo, S., Celi, L., Brunskill, E., and
  Doshi-Velez, F.
\newblock Interpretable off-policy evaluation in reinforcement learning by
  highlighting influential transitions.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  3658--3667, 2020.

\bibitem[Guo et~al.(2017)Guo, Thomas, and Brunskill]{Guo2017}
Guo, Z.~D., Thomas, P.~S., and Brunskill, E.
\newblock Using options and covariance testing for long horizon off-policy
  policy evaluation.
\newblock In \emph{Proceedings of the 31st International Conference on Neural
  Information Processing Systems}, pp.\  2493--2502, 2017.

\bibitem[Hodges(1958)]{Hodges1958}
Hodges, J.~L.
\newblock The significance probability of the smirnov two-sample test.
\newblock \emph{Arkiv f{\"{o}}r matematik}, 3\penalty0 (5):\penalty0 469--486,
  1958.

\bibitem[Ionides(2008)]{Ionides2008}
Ionides, E.~L.
\newblock Truncated importance sampling.
\newblock \emph{Journal of Computational and Graphical Statistics}, 17\penalty0
  (2):\penalty0 295--311, 2008.

\bibitem[Jiang \& Li(2016)Jiang and Li]{jiang2016doubly}
Jiang, N. and Li, L.
\newblock Doubly robust off-policy value evaluation for reinforcement learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pp.\  652--661, 2016.

\bibitem[Liu et~al.(2018)Liu, Li, Tang, and Zhou]{liu2018breaking}
Liu, Q., Li, L., Tang, Z., and Zhou, D.
\newblock Breaking the curse of horizon: Infinite-horizon off-policy
  estimation.
\newblock In \emph{Proceedings of the 32nd International Conference on Neural
  Information Processing Systems}, pp.\  5356--5366, 2018.

\bibitem[Mandel et~al.(2014)Mandel, Liu, Levine, Brunskill, and
  Popovi{\'{c}}]{Mandel2014}
Mandel, T., Liu, Y.~E., Levine, S., Brunskill, E., and Popovi{\'{c}}, Z.
\newblock Offline policy evaluation across representations with applications to
  educational games.
\newblock In \emph{Proceedings of the 2014 International Conference on
  Autonomous Agents and Multi-Agent Systems}, pp.\  1077--1084, 2014.

\bibitem[Metelli et~al.(2020)Metelli, Papini, Montali, and
  Restelli]{Metelli2020}
Metelli, A.~M., Papini, M., Montali, N., and Restelli, M.
\newblock Importance sampling techniques for policy optimization.
\newblock \emph{Journal of Machine Learning Research}, 21:\penalty0 1--75,
  2020.

\bibitem[Murphy et~al.(2001)Murphy, {Van der Laan}, Robins, Bierman, Coie,
  Greenberg, Lochman, McMahon, and Pinderhughes]{Murphy2001}
Murphy, S.~A., {Van der Laan}, M.~J., Robins, J.~M., Bierman, K.~L., Coie,
  J.~D., Greenberg, M.~T., Lochman, J.~E., McMahon, R.~J., and Pinderhughes, E.
\newblock Marginal mean models for dynamic regimes.
\newblock \emph{Journal of the American Statistical Association}, 96\penalty0
  (456):\penalty0 1410--1423, 2001.

\bibitem[Papini et~al.(2019)Papini, Metelli, Lupo, and Restelli]{Papini2019}
Papini, M., Metelli, A.~M., Lupo, L., and Restelli, M.
\newblock Optimistic policy optimization via multiple importance sampling.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning}, pp.\  4989--4999, 2019.

\bibitem[Precup et~al.(2000)Precup, Sutton, and Singh]{precup2000eligibility}
Precup, D., Sutton, R.~S., and Singh, S.~P.
\newblock Eligibility traces for off-policy policy evaluation.
\newblock In \emph{Proceedings of the 17th International Conference on Machine
  Learning}, pp.\  759--766, 2000.

\bibitem[Rowland et~al.(2020)Rowland, Harutyunyan, van Hasselt, Borsa, Schaul,
  Munos, and Dabney]{Rowland2020}
Rowland, M., Harutyunyan, A., van Hasselt, H., Borsa, D., Schaul, T., Munos,
  R., and Dabney, W.
\newblock Conditional importance sampling for off-policy learning.
\newblock In \emph{Proceedings of the 23rd International Conference on
  Artificial Intelligence and Statistics}, pp.\  45--55, 2020.

\bibitem[Su et~al.(2019)Su, Dimakopoulou, Krishnamurthy, and
  Dud{\'{i}}k]{Su2019}
Su, Y., Dimakopoulou, M., Krishnamurthy, A., and Dud{\'{i}}k, M.
\newblock Doubly robust off-policy evaluation with shrinkage.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning}, pp.\  9167--9176, 2019.

\bibitem[Sutton \& Barto(2018)Sutton and Barto]{sutton2018reinforcement}
Sutton, R.~S. and Barto, A.~G.
\newblock \emph{Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem[Thomas \& Brunskill(2016)Thomas and Brunskill]{thomas2016data}
Thomas, P. and Brunskill, E.
\newblock Data-efficient off-policy policy evaluation for reinforcement
  learning.
\newblock In \emph{Proceedings of the 33rd International Conference on Machine
  Learning}, pp.\  2139--2148, 2016.

\bibitem[Thomas(2015)]{thomas2015safe}
Thomas, P.~S.
\newblock \emph{Safe reinforcement learning}.
\newblock PhD thesis, University of Massachusetts Amherst, 2015.

\bibitem[Thomas et~al.(2015{\natexlab{a}})Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015hcope}
Thomas, P.~S., Theocharous, G., and Ghavamzadeh, M.
\newblock High confidence off-policy evaluation.
\newblock In \emph{Proceedings of the 29th AAAI Conference on Artificial
  Intelligence}, pp.\  3000--3006, 2015{\natexlab{a}}.

\bibitem[Thomas et~al.(2015{\natexlab{b}})Thomas, Theocharous, and
  Ghavamzadeh]{thomas2015hcpi}
Thomas, P.~S., Theocharous, G., and Ghavamzadeh, M.
\newblock High confidence policy improvement.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning}, pp.\  2380--2388, 2015{\natexlab{b}}.

\bibitem[Welch(1951)]{WelchTTest}
Welch, B.~L.
\newblock On the comparison of several mean values: An alternative approach.
\newblock \emph{Biometrika}, 38\penalty0 (3/4):\penalty0 330--336, 1951.

\bibitem[Xie et~al.(2019)Xie, Ma, and Wang]{Xie2019}
Xie, T., Ma, Y., and Wang, Y.~X.
\newblock Towards optimal off-policy evaluation for reinforcement learning with
  marginalized importance sampling.
\newblock In \emph{Proceedings of the 33rd International Conference on Neural
  Information Processing Systems}, pp.\  1--36, 2019.

\end{thebibliography}
