\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bamler et~al.(2017)Bamler, Zhang, Opper, and
  Mandt]{bamler2017perturbative}
Bamler, R., Zhang, C., Opper, M., and Mandt, S.
\newblock Perturbative black box variational inference.
\newblock \emph{arXiv preprint arXiv:1709.07433}, 2017.

\bibitem[Bourlard \& Kamp(1988)Bourlard and Kamp]{bourlard1988auto}
Bourlard, H. and Kamp, Y.
\newblock Auto-association by multilayer perceptrons and singular value
  decomposition.
\newblock \emph{Biological cybernetics}, 59\penalty0 (4-5):\penalty0 291--294,
  1988.

\bibitem[Burda et~al.(2016)Burda, Grosse, and
  Salakhutdinov]{burda2016importance}
Burda, Y., Grosse, R., and Salakhutdinov, R.
\newblock Importance weighted autoencoders.
\newblock In \emph{ICLR}, 2016.

\bibitem[Chen et~al.(2017)Chen, Kingma, Salimans, Duan, Dhariwal, Schulman,
  Sutskever, and Abbeel]{chen2016variational}
Chen, X., Kingma, D.~P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J.,
  Sutskever, I., and Abbeel, P.
\newblock Variational lossy autoencoder.
\newblock In \emph{ICLR}, 2017.

\bibitem[Cremer et~al.(2017)Cremer, Morris, and
  Duvenaud]{cremer2017reinterpreting}
Cremer, C., Morris, Q., and Duvenaud, D.
\newblock Reinterpreting importance-weighted autoencoders.
\newblock \emph{arXiv preprint arXiv:1704.02916}, 2017.

\bibitem[Fort et~al.(2017)Fort, Gobet, and Moulines]{fort2017mcmc}
Fort, G., Gobet, E., and Moulines, E.
\newblock Mcmc design-based non-parametric regression for rare event.
  application to nested risk computations.
\newblock \emph{Monte Carlo Methods and Applications}, 23\penalty0
  (1):\penalty0 21--42, 2017.

\bibitem[Gregor et~al.(2016)Gregor, Besse, Rezende, Danihelka, and
  Wierstra]{gregor2016towards}
Gregor, K., Besse, F., Rezende, D.~J., Danihelka, I., and Wierstra, D.
\newblock Towards conceptual compression.
\newblock In \emph{NIPS}, 2016.

\bibitem[Hesterberg(1988)]{hesterberg1988advances}
Hesterberg, T.~C.
\newblock \emph{Advances in importance sampling}.
\newblock PhD thesis, Stanford University, 1988.

\bibitem[Hinton \& Zemel(1994)Hinton and Zemel]{hinton1994autoencoders}
Hinton, G.~E. and Zemel, R.~S.
\newblock Autoencoders, minimum description length and helmholtz free energy.
\newblock In \emph{NIPS}, 1994.

\bibitem[Hoffman et~al.(2013)Hoffman, Blei, Wang, and
  Paisley]{hoffman2013stochastic}
Hoffman, M.~D., Blei, D.~M., Wang, C., and Paisley, J.
\newblock Stochastic variational inference.
\newblock \emph{The Journal of Machine Learning Research}, 14\penalty0
  (1):\penalty0 1303--1347, 2013.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2014auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {Bayes}.
\newblock In \emph{ICLR}, 2014.

\bibitem[Kingma et~al.(2016)Kingma, Salimans, and Welling]{kingma2016improving}
Kingma, D.~P., Salimans, T., and Welling, M.
\newblock Improving variational inference with inverse autoregressive flow.
\newblock \emph{arXiv preprint arXiv:1606.04934}, 2016.

\bibitem[Le et~al.(2017)Le, Baydin, and Wood]{le2017inference}
Le, T.~A., Baydin, A.~G., and Wood, F.
\newblock Inference compilation and universal probabilistic programming.
\newblock In \emph{AISTATS}, 2017.

\bibitem[Le et~al.(2018)Le, Igl, Rainforth, Jin, and Wood]{le2017auto}
Le, T.~A., Igl, M., Rainforth, T., Jin, T., and Wood, F.
\newblock Auto-encoding sequential {M}onte {C}arlo.
\newblock In \emph{ICLR}, 2018.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Li \& Turner(2016)Li and Turner]{li2016renyi}
Li, Y. and Turner, R.~E.
\newblock R{\'e}nyi divergence variational inference.
\newblock In \emph{NIPS}, 2016.

\bibitem[Maal{\o}e et~al.(2016)Maal{\o}e, S{\o}nderby, S{\o}nderby, and
  Winther]{maaloe_auxiliary_2016}
Maal{\o}e, L., S{\o}nderby, C.~K., S{\o}nderby, S.~K., and Winther, O.
\newblock Auxiliary deep generative models.
\newblock \emph{arXiv preprint arXiv:1602.05473}, 2016.

\bibitem[Maddison et~al.(2017)Maddison, Lawson, Tucker, Heess, Norouzi, Mnih,
  Doucet, and Teh]{maddison2017filtering}
Maddison, C.~J., Lawson, D., Tucker, G., Heess, N., Norouzi, M., Mnih, A.,
  Doucet, A., and Teh, Y.~W.
\newblock Filtering variational objectives.
\newblock \emph{arXiv preprint arXiv:1705.09279}, 2017.

\bibitem[Naesseth et~al.(2018)Naesseth, Linderman, Ranganath, and
  Blei]{naesseth2017variational}
Naesseth, C.~A., Linderman, S.~W., Ranganath, R., and Blei, D.~M.
\newblock Variational sequential {M}onte {C}arlo.
\newblock 2018.

\bibitem[Owen(2013)]{mcbook}
Owen, A.~B.
\newblock \emph{Monte Carlo theory, methods and examples}.
\newblock 2013.

\bibitem[Paige \& Wood(2016)Paige and Wood]{paige2016inference}
Paige, B. and Wood, F.
\newblock Inference networks for sequential {M}onte {C}arlo in graphical
  models.
\newblock In \emph{ICML}, 2016.

\bibitem[Rainforth(2017)]{rainforth2017thesis}
Rainforth, T.
\newblock \emph{{Automating Inference, Learning, and Design using Probabilistic
  Programming}}.
\newblock PhD thesis, 2017.

\bibitem[Rainforth et~al.(2018)Rainforth, Cornish, Yang, Warrington, and
  Wood]{rainforth2017opportunities}
Rainforth, T., Cornish, R., Yang, H., Warrington, A., and Wood, F.
\newblock On nesting {M}onte {C}arlo estimators.
\newblock In \emph{ICML}, 2018.

\bibitem[Ranganath et~al.(2014)Ranganath, Gerrish, and
  Blei]{ranganath2014black}
Ranganath, R., Gerrish, S., and Blei, D.
\newblock Black box variational inference.
\newblock In \emph{AISTATS}, 2014.

\bibitem[Ranganath et~al.(2016)Ranganath, Tran, and
  Blei]{ranganath2016hierarchical}
Ranganath, R., Tran, D., and Blei, D.
\newblock Hierarchical variational models.
\newblock In \emph{ICML}, 2016.

\bibitem[Rezende \& Mohamed(2015)Rezende and Mohamed]{rezende_variational_2015}
Rezende, D. and Mohamed, S.
\newblock Variational inference with normalizing flows.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1530--1538, 2015.

\bibitem[Rezende et~al.(2014)Rezende, Mohamed, and
  Wierstra]{rezende2014stochastic}
Rezende, D.~J., Mohamed, S., and Wierstra, D.
\newblock Stochastic backpropagation and approximate inference in deep
  generative models.
\newblock In \emph{ICML}, 2014.

\bibitem[Roberts \& Tedrake(2009)Roberts and Tedrake]{roberts2009signal}
Roberts, J.~W. and Tedrake, R.
\newblock Signal-to-noise ratio analysis of policy gradient algorithms.
\newblock In \emph{NIPS}, 2009.

\bibitem[Salimans et~al.(2015)Salimans, Kingma, and
  Welling]{salimans_markov_2015}
Salimans, T., Kingma, D., and Welling, M.
\newblock Markov chain {M}onte {C}arlo and variational inference: Bridging the
  gap.
\newblock In \emph{Proceedings of the 32nd International Conference on Machine
  Learning (ICML-15)}, pp.\  1218--1226, 2015.

\bibitem[Tran et~al.(2015)Tran, Ranganath, and Blei]{tran_variational_2015}
Tran, D., Ranganath, R., and Blei, D.~M.
\newblock The variational {G}aussian process.
\newblock \emph{arXiv preprint arXiv:1511.06499}, 2015.

\bibitem[Turner \& Sahani(2011)Turner and Sahani]{turner2011two}
Turner, R.~E. and Sahani, M.
\newblock Two problems with variational expectation maximisation for
  time-series models.
\newblock \emph{Bayesian Time series models}, pp.\  115--138, 2011.

\end{thebibliography}
