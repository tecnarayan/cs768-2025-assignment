\begin{thebibliography}{10}

\bibitem{trabucco2022design}
Brandon Trabucco, Xinyang Geng, Aviral Kumar, and Sergey Levine.
\newblock Design-bench: Benchmarks for data-driven offline model-based
  optimization.
\newblock {\em arXiv preprint arXiv:2202.08450}, 2022.

\bibitem{liao2019Morphology}
Thomas Liao, Grant Wang, Brian Yang, Rene Lee, Kristofer Pister, Sergey Levine,
  and Roberto Calandra.
\newblock Data-efficient learning of morphology and controller for a
  microrobot.
\newblock {\em arXiv preprint arXiv:1905.01334}, 2019.

\bibitem{sarkisyan2016local}
Karen~S Sarkisyan et~al.
\newblock Local fitness landscape of the green fluorescent protein.
\newblock {\em Nature}, 2016.

\bibitem{angermueller2019model}
Christof Angermueller, David Dohan, David Belanger, Ramya Deshpande, Kevin
  Murphy, and Lucy Colwell.
\newblock Model-based reinforcement learning for biological sequence design.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2019.

\bibitem{hamidieh2018data}
Kam Hamidieh.
\newblock A data-driven statistical model for predicting the critical
  temperature of a superconductor.
\newblock {\em Computational Materials Science}, 2018.

\bibitem{barrera2016survey}
Luis~A Barrera et~al.
\newblock Survey of variation in human transcription factors reveals prevalent
  dna binding changes.
\newblock {\em Science}, 2016.

\bibitem{sample2019human}
Paul~J Sample, Ban Wang, David~W Reid, Vlad Presnyak, Iain~J McFadyen, David~R
  Morris, and Georg Seelig.
\newblock Human 5 {UTR} design and variant effect prediction from a massively
  parallel translation assay.
\newblock {\em Nature Biotechnology}, 2019.

\bibitem{yu2021roma}
Sihyun Yu, Sungsoo Ahn, Le~Song, and Jinwoo Shin.
\newblock Roma: Robust model adaptation for offline model-based optimization.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2021.

\bibitem{trabucco2021conservative}
Brandon Trabucco, Aviral Kumar, Xinyang Geng, and Sergey Levine.
\newblock Conservative objective models for effective offline model-based
  optimization.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2021.

\bibitem{fu2021offline}
Justin Fu and Sergey Levine.
\newblock Offline model-based optimization via normalized maximum likelihood
  estimation.
\newblock {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2021.

\bibitem{chen2022bidirectional}
Can Chen, Yingxue Zhang, Jie Fu, Xue Liu, and Mark Coates.
\newblock Bidirectional learning for offline infinite-width model-based
  optimization.
\newblock In {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2022.

\bibitem{chen2023bidirectional}
Can Chen, Yingxue Zhang, Xue Liu, and Mark Coates.
\newblock Bidirectional learning for offline model-based biological sequence
  design.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2023.

\bibitem{han2018co}
Bo~Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and
  Masashi Sugiyama.
\newblock Co-teaching: Robust training of deep neural networks with extremely
  noisy labels.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2018.

\bibitem{brockman2016openai}
Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman,
  Jie Tang, and Wojciech Zaremba.
\newblock Openai gym.
\newblock {\em arXiv preprint arXiv:1606.01540}, 2016.

\bibitem{ahn2020robel}
Michael Ahn, Henry Zhu, Kristian Hartikainen, Hugo Ponte, Abhishek Gupta,
  Sergey Levine, and Vikash Kumar.
\newblock Robel: Robotics benchmarks for learning with low-cost robots.
\newblock In {\em Conf. on Robot Lea. (CoRL)}, 2020.

\bibitem{zoph2017neural}
Barret Zoph and Quoc~V. Le.
\newblock Neural architecture search with reinforcement learning.
\newblock {\em arXiv preprint arXiv:1611.01578}, 2017.

\bibitem{hinton2012improving}
Geoffrey~E. Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and
  Ruslan~R. Salakhutdinov.
\newblock Improving neural networks by preventing co-adaptation of feature
  detectors.
\newblock {\em arXiv preprint arXiv:1207.0580}, 2012.

\bibitem{kumar2020model}
Aviral Kumar and Sergey Levine.
\newblock Model inversion networks for model-based optimization.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2020.

\bibitem{brookes2019conditioning}
David Brookes, Hahnbeom Park, and Jennifer Listgarten.
\newblock Conditioning by adaptive sampling for robust design.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2019.

\bibitem{fannjiang2020autofocused}
Clara Fannjiang and Jennifer Listgarten.
\newblock Autofocused oracles for model-based design.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2020.

\bibitem{qidata}
Han Qi, Yi~Su, Aviral Kumar, and Sergey Levine.
\newblock Data-driven model-based optimization via invariant representation
  learning.
\newblock In {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2022.

\bibitem{hansen2006cma}
Nikolaus Hansen.
\newblock The {CMA} evolution strategy: A comparing review.
\newblock In {\em Towards a New Evolutionary Computation: Advances in the
  Estimation of Distribution Algorithms}, 2006.

\bibitem{wilson2017reparameterization}
James~T Wilson, Riccardo Moriconi, Frank Hutter, and Marc~Peter Deisenroth.
\newblock The reparameterization trick for acquisition functions.
\newblock {\em arXiv preprint arXiv:1712.00424}, 2017.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine Learning}, 1992.

\bibitem{ovadia2019can}
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, David Sculley, Sebastian
  Nowozin, Joshua Dillon, Balaji Lakshminarayanan, and Jasper Snoek.
\newblock Can you trust your model's uncertainty? evaluating predictive
  uncertainty under dataset shift.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2019.

\bibitem{kumar2023stochastic}
Ramnath Kumar, Kushal Majmundar, Dheeraj Nagaraj, and Arun~Sai Suggala.
\newblock Stochastic re-weighted gradient descent via distributionally robust
  optimization.
\newblock {\em arXiv preprint arXiv:2306.09222}, 2023.

\bibitem{kim2023bootstrapped}
Minsu Kim, Federico Berto, Sungsoo Ahn, and Jinkyoo Park.
\newblock Bootstrapped training of score-conditioned generator for offline
  design of biological sequences.
\newblock {\em arXiv preprint arXiv:2306.03111}, 2023.

\bibitem{jain2022biological}
Moksh Jain, Emmanuel Bengio, Alex Hernandez-Garcia, Jarrid Rector-Brooks,
  Bonaventure~FP Dossou, Chanakya~Ajit Ekbote, Jie Fu, Tianyu Zhang, Michael
  Kilgour, Dinghuai Zhang, et~al.
\newblock Biological sequence design with gflownets.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}. PMLR, 2022.

\bibitem{chen2023parallel}
Can Chen, Christopher Beckham, Zixuan Liu, Xue Liu, and Christopher Pal.
\newblock Parallel-mentoring for offline model-based optimization.
\newblock In {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2023.

\bibitem{jiang2014easy}
Lu~Jiang, Deyu Meng, Teruko Mitamura, and Alexander~G Hauptmann.
\newblock Easy samples first: Self-paced reranking for zero-example multimedia
  search.
\newblock In {\em Proceedings of the 22nd ACM international conference on
  Multimedia}, 2014.

\bibitem{wang2017robust}
Yixin Wang, Alp Kucukelbir, and David~M Blei.
\newblock Robust probabilistic modeling with bayesian data reweighting.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2017.

\bibitem{ren2018learning}
Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun.
\newblock Learning to reweight examples for robust deep learning.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2018.

\bibitem{shu2019meta}
Jun Shu, Qi~Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng.
\newblock Meta-weight-net: Learning an explicit mapping for sample weighting.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2019.

\bibitem{chen2021generalized}
Can Chen, Shuhao Zheng, Xi~Chen, Erqun Dong, Xue~Steve Liu, Hao Liu, and Dejing
  Dou.
\newblock Generalized data weighting via class-level gradient manipulation.
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2021.

\bibitem{franceschi2018bilevel}
Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and
  Massimiliano Pontil.
\newblock Bilevel programming for hyperparameter optimization and
  meta-learning.
\newblock In {\em Proc. Int. Conf. Machine Lea. (ICML)}, 2018.

\bibitem{chen2022gradient}
Can Chen, Xi~Chen, Chen Ma, Zixuan Liu, and Xue Liu.
\newblock Gradient-based bi-level optimization for deep learning: A survey.
\newblock {\em arXiv preprint arXiv:2207.11719}, 2022.

\bibitem{chen2022unbiased}
Can Chen, Chen Ma, Xi~Chen, Sirui Song, Hao Liu, and Xue Liu.
\newblock Unbiased implicit feedback via bi-level optimization.
\newblock {\em arXiv preprint arXiv:2206.00147}, 2022.

\bibitem{chen2023structure}
Can Chen, Jingbo Zhou, Fan Wang, Xue Liu, and Dejing Dou.
\newblock Structure-aware protein self-supervised learning.
\newblock {\em Bioinformatics}, 2023.

\bibitem{giovannelli2023bilevel}
Tommaso Giovannelli, Griffin Kent, and Luis~Nunes Vicente.
\newblock Bilevel optimization with a multi-objective lower-level problem:
  Risk-neutral and risk-averse formulations.
\newblock {\em arXiv preprint arXiv:2302.05540}, 2023.

\bibitem{li2023dataset}
Guang Li, Ren Togo, Takahiro Ogawa, and Miki Haseyama.
\newblock Dataset distillation using parameter pruning.
\newblock {\em IEICE Transactions on Fundamentals of Electronics,
  Communications and Computer Sciences}, 2023.

\bibitem{chi2021test}
Zhixiang Chi, Yang Wang, Yuanhao Yu, and Jin Tang.
\newblock Test-time fast adaptation for dynamic scene deblurring via
  meta-auxiliary learning.
\newblock In {\em Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recogn. (CVPR)},
  2021.

\bibitem{chi2022metafscil}
Zhixiang Chi, Li~Gu, Huan Liu, Yang Wang, Yuanhao Yu, and Jin Tang.
\newblock Metafscil: A meta-learning approach for few-shot class incremental
  learning.
\newblock In {\em Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recogn. (CVPR)},
  2022.

\bibitem{malach2017decoupling}
Eran Malach and Shai Shalev-Shwartz.
\newblock Decoupling" when to update" from" how to update".
\newblock {\em Proc. Adv. Neur. Inf. Proc. Syst (NeurIPS)}, 2017.

\bibitem{blum1998combining}
Avrim Blum and Tom Mitchell.
\newblock Combining labeled and unlabeled data with co-training.
\newblock In {\em Proceedings of the eleventh annual conference on
  Computational learning theory}, 1998.

\bibitem{kingma2014adam}
Diederik~P Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em Proc. Int. Conf. Learning Rep. (ICLR)}, 2015.

\end{thebibliography}
