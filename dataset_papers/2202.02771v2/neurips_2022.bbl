\begin{thebibliography}{10}

\bibitem{alacaoglu2021stochastic}
Ahmet Alacaoglu and Yura Malitsky.
\newblock Stochastic variance reduction for variational inequality methods.
\newblock {\em arXiv preprint arXiv:2102.08352}, 2021.

\bibitem{Yura2021}
Ahmet Alacaoglu, Yura Malitsky, and Volkan Cevher.
\newblock Forward-reflected-backward method with variance reduction.
\newblock {\em Computational Optimization and Applications}, 80, 11 2021.

\bibitem{bach2011optimization}
Francis Bach, Rodolphe Jenatton, Julien Mairal, and Guillaume Obozinski.
\newblock Optimization with sparsity-inducing penalties.
\newblock {\em arXiv preprint arXiv:1108.0775}, 2011.

\bibitem{bach2008convex}
Francis Bach, Julien Mairal, and Jean Ponce.
\newblock Convex sparse matrix factorizations.
\newblock {\em arXiv preprint arXiv:0812.1869}, 2008.

\bibitem{BARAZANDEH2021108245}
Babak Barazandeh, Tianjian Huang, and George Michailidis.
\newblock A decentralized adaptive momentum method for solving a class of
  min-max optimization problems.
\newblock {\em Signal Processing}, 189:108245, 2021.

\bibitem{Heinz}
Heinz Bauschke and Patrick Combettes.
\newblock {\em Convex Analysis and Monotone Operator Theory in Hilbert Spaces}.
\newblock 01 2017.

\bibitem{bazerque2009distributed}
Juan~Andr{\'e}s Bazerque and Georgios~B Giannakis.
\newblock Distributed spectrum sensing for cognitive radio networks by
  exploiting sparsity.
\newblock {\em IEEE Transactions on Signal Processing}, 58(3):1847--1862, 2009.

\bibitem{beck20141}
Amir Beck, Angelia Nedi{\'c}, Asuman Ozdaglar, and Marc Teboulle.
\newblock An o (1/k) gradient method for network resource allocation problems.
\newblock {\em IEEE Transactions on Control of Network Systems}, 1(1):64--73,
  2014.

\bibitem{BenTal2009:book}
Aharon Ben-Tal, Laurent~El Ghaoui, and Arkadi Nemirovski.
\newblock {\em Robust Optimization}.
\newblock Princeton University Press, 2009.

\bibitem{beznosikov2021decentralized}
Aleksandr Beznosikov, Pavel Dvurechensky, Anastasia Koloskova, Valentin
  Samokhin, Sebastian~U Stich, and Alexander Gasnikov.
\newblock Decentralized local stochastic extra-gradient for variational
  inequalities.
\newblock {\em arXiv preprint arXiv:2106.08315}, 2021.

\bibitem{beznosikov2022stochastic}
Aleksandr Beznosikov, Eduard Gorbunov, Hugo Berard, and Nicolas Loizou.
\newblock Stochastic gradient descent-ascent: Unified theory and new efficient
  methods.
\newblock {\em arXiv preprint arXiv:2202.07262}, 2022.

\bibitem{beznosikov2021}
Aleksandr Beznosikov, Alexander Rogozin, Dmitry Kovalev, and Alexander
  Gasnikov.
\newblock Near-optimal decentralized algorithms for saddle point problems over
  time-varying networks.
\newblock {\em Lecture Notes in Computer Science}, page 246â€“257, 2021.

\bibitem{beznosikov2020distributed}
Aleksandr Beznosikov, Valentin Samokhin, and Alexander Gasnikov.
\newblock Distributed saddle-point problems: Lower bounds, optimal algorithms
  and federated {GAN}s.
\newblock {\em arXiv preprint arXiv:2010.13112}, 2020.

\bibitem{beznosikov2021distributed}
Aleksandr Beznosikov, Gesualdo Scutari, Alexander Rogozin, and Alexander
  Gasnikov.
\newblock Distributed saddle-point problems under data similarity.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{boyd2006randomized}
Stephen Boyd, Arpita Ghosh, Balaji Prabhakar, and Devavrat Shah.
\newblock Randomized gossip algorithms.
\newblock {\em IEEE transactions on information theory}, 52(6):2508--2530,
  2006.

\bibitem{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale gan training for high fidelity natural image synthesis.
\newblock {\em arXiv preprint arXiv:1809.11096}, 2018.

\bibitem{carmon2019variance}
Yair Carmon, Yujia Jin, Aaron Sidford, and Kevin Tian.
\newblock Variance reduction for matrix games.
\newblock {\em arXiv preprint arXiv:1907.02056}, 2019.

\bibitem{chambolle2011first}
Antonin Chambolle and Thomas Pock.
\newblock A first-order primal-dual algorithm for convex problems with
  applications to imaging.
\newblock {\em Journal of mathematical imaging and vision}, 40(1):120--145,
  2011.

\bibitem{chang2011libsvm}
Chih-Chung Chang and Chih-Jen Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em ACM transactions on intelligent systems and technology (TIST)},
  2(3):1--27, 2011.

\bibitem{chavdarova2019reducing}
Tatjana Chavdarova, Gauthier Gidel, Fran{\c{c}}ois Fleuret, and Simon
  Lacoste-Julien.
\newblock Reducing noise in gan training with variance reduced extragradient.
\newblock {\em arXiv preprint arXiv:1904.08598}, 2019.

\bibitem{daskalakis2017training}
Constantinos Daskalakis, Andrew Ilyas, Vasilis Syrgkanis, and Haoyang Zeng.
\newblock Training gans with optimism.
\newblock {\em arXiv preprint arXiv:1711.00141}, 2017.

\bibitem{esser2010general}
Ernie Esser, Xiaoqun Zhang, and Tony~F Chan.
\newblock A general framework for a class of first order primal-dual algorithms
  for convex optimization in imaging science.
\newblock {\em SIAM Journal on Imaging Sciences}, 3(4):1015--1046, 2010.

\bibitem{facchinei2007finite}
F.~Facchinei and J.S. Pang.
\newblock {\em Finite-Dimensional Variational Inequalities and Complementarity
  Problems}.
\newblock Springer Series in Operations Research and Financial Engineering.
  Springer New York, 2007.

\bibitem{VIbook2003}
Francisco Facchinei and Jong-Shi Pang.
\newblock {\em Finite-Dimensional Variational Inequalities and Complementarity
  Problems}.
\newblock Springer Series in Operations Research. Springer, 2003.

\bibitem{gan2012optimal}
Lingwen Gan, Ufuk Topcu, and Steven~H Low.
\newblock Optimal decentralized protocol for electric vehicle charging.
\newblock {\em IEEE Transactions on Power Systems}, 28(2):940--951, 2012.

\bibitem{gidel2018variational}
Gauthier Gidel, Hugo Berard, Ga{\"e}tan Vignoud, Pascal Vincent, and Simon
  Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock {\em arXiv preprint arXiv:1802.10551}, 2018.

\bibitem{giselsson2013accelerated}
Pontus Giselsson, Minh~Dang Doan, Tam{\'a}s Keviczky, Bart De~Schutter, and
  Anders Rantzer.
\newblock Accelerated gradient methods and dual decomposition in distributed
  model predictive control.
\newblock {\em Automatica}, 49(3):829--833, 2013.

\bibitem{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems (NIPS)},
  2014.

\bibitem{han2021lower}
Yuze Han, Guangzeng Xie, and Zhihua Zhang.
\newblock Lower complexity bounds of finite-sum optimization problems: The
  results and construction.
\newblock {\em arXiv preprint arXiv:2103.08280}, 2021.

\bibitem{hendrikx2020optimal}
Hadrien Hendrikx, Francis Bach, and Laurent Massoulie.
\newblock An optimal algorithm for decentralized finite sum optimization.
\newblock {\em arXiv preprint arXiv:2005.10675}, 2020.

\bibitem{hsieh2019convergence}
Yu-Guan Hsieh, Franck Iutzeler, J{\'e}r{\^o}me Malick, and Panayotis
  Mertikopoulos.
\newblock On the convergence of single-call stochastic extra-gradient methods.
\newblock {\em arXiv preprint arXiv:1908.08465}, 2019.

\bibitem{Jin2020:mdp}
Yujia Jin and Aaron Sidford.
\newblock Efficiently solving {MDP}s with stochastic mirror descent.
\newblock In {\em Proceedings of the 37th International Conference on Machine
  Learning (ICML)}, volume 119, pages 4890--4900. PMLR, 2020.

\bibitem{Thorsten}
Thorsten Joachims.
\newblock A support vector method for multivariate performance measures.
\newblock pages 377--384, 01 2005.

\bibitem{juditsky2008solving}
Anatoli Juditsky, Arkadii~S. Nemirovskii, and Claire Tauvel.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm, 2008.

\bibitem{kairouz2019advances}
Peter Kairouz, H~Brendan McMahan, Brendan Avent, Aur{\'e}lien Bellet, Mehdi
  Bennis, Arjun~Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham
  Cormode, Rachel Cummings, et~al.
\newblock Advances and open problems in federated learning.
\newblock {\em arXiv preprint arXiv:1912.04977}, 2019.

\bibitem{koloskova2020unified}
Anastasia Koloskova, Nicolas Loizou, Sadra Boreiri, Martin Jaggi, and
  Sebastian~U Stich.
\newblock A unified theory of decentralized sgd with changing topology and
  local updates.
\newblock {\em arXiv preprint arXiv:2003.10422}, 2020.

\bibitem{FEDLEARN}
Jakub Kone\v{c}n\'{y}, H.~Brendan McMahan, Felix Yu, Peter Richt\'{a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: strategies for improving communication
  efficiency.
\newblock In {\em NIPS Private Multi-Party Machine Learning Workshop}, 2016.

\bibitem{Korpelevich1976TheEM}
G.~M. Korpelevich.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock {\em Matecon}, 12:747--756, 1976.

\bibitem{kovalev2021lower}
Dmitry Kovalev, Elnur Gasanov, Peter Richt{\'a}rik, and Alexander Gasnikov.
\newblock Lower bounds and optimal algorithms for smooth and strongly convex
  decentralized optimization over time-varying networks.
\newblock {\em arXiv preprint arXiv:2106.04469}, 2021.

\bibitem{kovalev2020optimal}
Dmitry Kovalev, Adil Salim, and Peter Richt{\'a}rik.
\newblock Optimal and practical algorithms for smooth and strongly convex
  decentralized optimization.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{li2021accelerated}
Huan Li and Zhouchen Lin.
\newblock Accelerated gradient tracking over time-varying graphs for
  decentralized optimization.
\newblock {\em arXiv preprint arXiv:2104.02596}, 2021.

\bibitem{pmlr-v89-liang19b}
Tengyuan Liang and James Stokes.
\newblock Interaction matters: A note on non-asymptotic local convergence of
  generative adversarial networks.
\newblock In Kamalika Chaudhuri and Masashi Sugiyama, editors, {\em Proceedings
  of the Twenty-Second International Conference on Artificial Intelligence and
  Statistics}, volume~89 of {\em Proceedings of Machine Learning Research},
  pages 907--915. PMLR, 16--18 Apr 2019.

\bibitem{liu2019decentralized}
Mingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jerret Ross, Tianbao
  Yang, and Payel Das.
\newblock A decentralized parallel algorithm for training generative
  adversarial nets.
\newblock In {\em Advances in Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem{Madry2017:adv}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In {\em International Conference on Learning Representations (ICLR)},
  2018.

\bibitem{7403075}
David Mateos-NÃºÃ±ez and Jorge CortÃ©s.
\newblock Distributed subgradient methods for saddle-point problems.
\newblock In {\em 2015 54th IEEE Conference on Decision and Control (CDC)},
  pages 5462--5467, 2015.

\bibitem{FL2017-AISTATS}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise
  Ag\"{u}era~y Arcas.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In {\em Proceedings of the 20th International Conference on
  Artificial Intelligence and Statistics (AISTATS)}, 2017.

\bibitem{mertikopoulos2018optimistic}
Panayotis Mertikopoulos, Bruno Lecouat, Houssam Zenati, Chuan-Sheng Foo, Vijay
  Chandrasekhar, and Georgios Piliouras.
\newblock Optimistic mirror descent in saddle-point problems: Going the extra
  (gradient) mile.
\newblock {\em arXiv preprint arXiv:1807.02629}, 2018.

\bibitem{Mukherjee2020:decentralizedminmax}
Soham Mukherjee and Mrityunjoy Chakraborty.
\newblock A decentralized algorithm for large scale min-max problems.
\newblock In {\em 2020 59th IEEE Conference on Decision and Control (CDC)},
  pages 2967--2972, 2020.

\bibitem{9084356}
Angelia Nedic.
\newblock Distributed gradient methods for convex machine learning problems in
  networks: Distributed optimization.
\newblock {\em IEEE Signal Processing Magazine}, 37(3):92--101, 2020.

\bibitem{nedic2009distributed}
Angelia Nedi\'{c} and Asuman Ozdaglar.
\newblock Distributed subgradient methods for multi-agent optimization.
\newblock {\em IEEE Transactions on Automatic Control}, 54(1):48--61, 2009.

\bibitem{nedich2016geometrically}
Angelia Nedich, Alex Olshevsky, and Wei Shi.
\newblock A geometrically convergent method for distributed optimization over
  time-varying graphs.
\newblock In {\em 2016 IEEE 55th Conference on Decision and Control (CDC)},
  pages 1023--1029. IEEE, 2016.

\bibitem{Nemirovski2004}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence $o(1/t)$ for variational
  inequalities with {L}ipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 15:229--251, 01 2004.

\bibitem{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence {O(1/t)} for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock {\em SIAM Journal on Optimization}, 15(1):229--251, 2004.

\bibitem{nemirovski2013mini}
Arkadi Nemirovski.
\newblock Mini-course on convex programming algorithms, 2013.

\bibitem{nemirovski2009robust}
Arkadi Nemirovski, Anatoli Juditsky, Guanghui Lan, and Alexander Shapiro.
\newblock Robust stochastic approximation approach to stochastic programming.
\newblock {\em SIAM Journal on optimization}, 19(4):1574--1609, 2009.

\bibitem{nesterov2005smooth}
Yu~Nesterov.
\newblock Smooth minimization of non-smooth functions.
\newblock {\em Mathematical programming}, 103(1):127--152, 2005.

\bibitem{nesterov1983method}
Yurii~E Nesterov.
\newblock A method for solving the convex programming problem with convergence
  rate o (1/k\^{} 2).
\newblock In {\em Dokl. akad. nauk Sssr}, volume 269, pages 543--547, 1983.

\bibitem{Omidshafiei2017:rl}
Shayegan Omidshafiei, Jason Pazis, Christopher Amato, Jonathan~P. How, and John
  Vian.
\newblock Deep decentralized multi-task multi-agent reinforcement learning
  under partial observability.
\newblock In {\em Proceedings of the 34th International Conference on Machine
  Learning (ICML)}, volume~70, pages 2681--2690. PMLR, 2017.

\bibitem{palaniappan2016stochastic}
Balamurugan Palaniappan and Francis Bach.
\newblock Stochastic variance reduction methods for saddle-point problems.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1416--1424, 2016.

\bibitem{peng2020training}
Wei Peng, Yu-Hong Dai, Hui Zhang, and Lizhi Cheng.
\newblock Training gans with centripetal acceleration.
\newblock {\em Optimization Methods and Software}, 35(5):955--973, 2020.

\bibitem{1307319}
M.~Rabbat and R.~Nowak.
\newblock Distributed optimization in sensor networks.
\newblock In {\em Third International Symposium on Information Processing in
  Sensor Networks, 2004. IPSN 2004}, pages 20--27, 2004.

\bibitem{rogozin2021decentralized}
Alexander Rogozin, Alexander Beznosikov, Darina Dvinskikh, Dmitry Kovalev,
  Pavel Dvurechensky, and Alexander Gasnikov.
\newblock Decentralized distributed optimization for saddle point problems.
\newblock {\em arXiv preprint arXiv:2102.07758}, 2021.

\bibitem{scaman2017optimal}
Kevin Scaman, Francis Bach, S{\'e}bastien Bubeck, Yin~Tat Lee, and Laurent
  Massouli{\'e}.
\newblock Optimal algorithms for smooth and strongly convex distributed
  optimization in networks.
\newblock {\em arXiv preprint arXiv:1702.08704}, 2017.

\bibitem{6004889}
Kunal Srivastava, Angelia NediÄ‡, and DuÅ¡an StipanoviÄ‡.
\newblock Distributed min-max optimization in networks.
\newblock In {\em 2011 17th International Conference on Digital Signal
  Processing (DSP)}, pages 1--8, 2011.

\bibitem{vladislav2021accelerated}
Vladislav Tominin, Yaroslav Tominin, Ekaterina Borodich, Dmitry Kovalev,
  Alexander Gasnikov, and Pavel Dvurechensky.
\newblock On accelerated methods for saddle-point problems with composite
  structure.
\newblock {\em arXiv preprint arXiv:2103.09344}, 2021.

\bibitem{doi:10.1137/S0363012998338806}
Paul Tseng.
\newblock A modified forward-backward splitting method for maximal monotone
  mappings.
\newblock {\em SIAM Journal on Control and Optimization}, 38(2):431--446, 2000.

\bibitem{NEURIPS2021_d994e372}
Wenhan Xian, Feihu Huang, Yanfu Zhang, and Heng Huang.
\newblock A faster decentralized algorithm for nonconvex minimax problems.
\newblock In M.~Ranzato, A.~Beygelzimer, Y.~Dauphin, P.S. Liang, and J.~Wortman
  Vaughan, editors, {\em Advances in Neural Information Processing Systems},
  volume~34, pages 25865--25877. Curran Associates, Inc., 2021.

\bibitem{XIAO200465}
Lin Xiao and Stephen Boyd.
\newblock Fast linear iterations for distributed averaging.
\newblock {\em Systems and Control Letters}, 53(1):65--78, 2004.

\bibitem{NIPS2004_64036755}
Linli Xu, James Neufeld, Bryce Larson, and Dale Schuurmans.
\newblock Maximum margin clustering.
\newblock In L.~Saul, Y.~Weiss, and L.~Bottou, editors, {\em Advances in Neural
  Information Processing Systems}, volume~17. MIT Press, 2005.

\bibitem{yang2020global}
Junchi Yang, Negar Kiyavash, and Niao He.
\newblock Global convergence and variance-reduced optimization for a class of
  nonconvex-nonconcave minimax problems.
\newblock {\em arXiv preprint arXiv:2002.09621}, 2020.

\bibitem{you2019large}
Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh
  Bhojanapalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh.
\newblock Large batch optimization for deep learning: Training bert in 76
  minutes.
\newblock {\em arXiv preprint arXiv:1904.00962}, 2019.

\bibitem{zhang2019lower}
Junyu Zhang, Mingyi Hong, and Shuzhong Zhang.
\newblock On lower iteration complexity bounds for the saddle point problems.
\newblock {\em arXiv preprint arXiv:1912.07481}, 2019.

\bibitem{zhu2019freelb}
Chen Zhu, Yu~Cheng, Zhe Gan, Siqi Sun, Tom Goldstein, and Jingjing Liu.
\newblock Freelb: Enhanced adversarial training for natural language
  understanding.
\newblock {\em arXiv preprint arXiv:1909.11764}, 2019.

\end{thebibliography}
