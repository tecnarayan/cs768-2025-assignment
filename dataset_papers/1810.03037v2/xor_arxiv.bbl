\begin{thebibliography}{20}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2018)Allen-Zhu, Li, and Liang]{allen2018learning}
Allen-Zhu, Zeyuan, Li, Yuanzhi, and Liang, Yingyu.
\newblock Learning and generalization in overparameterized neural networks,
  going beyond two layers.
\newblock \emph{arXiv preprint arXiv:1811.04918}, 2018.

\bibitem[Brutzkus \& Globerson(2017)Brutzkus and
  Globerson]{brutzkus2017globally}
Brutzkus, Alon and Globerson, Amir.
\newblock Globally optimal gradient descent for a convnet with gaussian inputs.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  605--614, 2017.

\bibitem[Brutzkus et~al.(2018)Brutzkus, Globerson, Malach, and
  Shalev-Shwartz]{brutzkus2018sgd}
Brutzkus, Alon, Globerson, Amir, Malach, Eran, and Shalev-Shwartz, Shai.
\newblock Sgd learns over-parameterized networks that provably generalize on
  linearly separable data.
\newblock \emph{International Conference on Learning Representations}, 2018.

\bibitem[Daniely(2017)]{daniely2017sgd}
Daniely, Amit.
\newblock Sgd learns the conjugate kernel class of the network.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2422--2430, 2017.

\bibitem[Du \& Lee(2018)Du and Lee]{du2018power}
Du, Simon~S and Lee, Jason~D.
\newblock On the power of over-parametrization in neural networks with
  quadratic activation.
\newblock \emph{arXiv preprint arXiv:1803.01206}, 2018.

\bibitem[Du et~al.(2017{\natexlab{a}})Du, Lee, and Tian]{du2017convolutional}
Du, Simon~S, Lee, Jason~D, and Tian, Yuandong.
\newblock When is a convolutional filter easy to learn?
\newblock \emph{arXiv preprint arXiv:1709.06129}, 2017{\natexlab{a}}.

\bibitem[Du et~al.(2017{\natexlab{b}})Du, Lee, Tian, Poczos, and
  Singh]{du2017gradient}
Du, Simon~S, Lee, Jason~D, Tian, Yuandong, Poczos, Barnabas, and Singh, Aarti.
\newblock Gradient descent learns one-hidden-layer cnn: Don't be afraid of
  spurious local minima.
\newblock \emph{arXiv preprint arXiv:1712.00779}, 2017{\natexlab{b}}.

\bibitem[Du et~al.(2018{\natexlab{a}})Du, Lee, Li, Wang, and
  Zhai]{du2018gradientdeep}
Du, Simon~S, Lee, Jason~D, Li, Haochuan, Wang, Liwei, and Zhai, Xiyu.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock \emph{arXiv preprint arXiv:1811.03804}, 2018{\natexlab{a}}.

\bibitem[Du et~al.(2018{\natexlab{b}})Du, Zhai, Poczos, and
  Singh]{du2018gradient}
Du, Simon~S, Zhai, Xiyu, Poczos, Barnabas, and Singh, Aarti.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018{\natexlab{b}}.

\bibitem[Hoffer et~al.(2018)Hoffer, Hubara, and Soudry]{hoffer2018fix}
Hoffer, Elad, Hubara, Itay, and Soudry, Daniel.
\newblock Fix your classifier: the marginal value of training the last weight
  layer.
\newblock 2018.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, Diederik~P and Ba, Jimmy.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Li \& Liang(2018)Li and Liang]{li2018learning}
Li, Yuanzhi and Liang, Yingyu.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{arXiv preprint arXiv:1808.01204}, 2018.

\bibitem[Li \& Yuan(2017)Li and Yuan]{li2017convergence}
Li, Yuanzhi and Yuan, Yang.
\newblock Convergence analysis of two-layer neural networks with relu
  activation.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  597--607, 2017.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{li2017algorithmic}
Li, Yuanzhi, Ma, Tengyu, and Zhang, Hongyang.
\newblock Algorithmic regularization in over-parameterized matrix recovery.
\newblock \emph{arXiv preprint arXiv:1712.09203}, 2017.

\bibitem[Neyshabur et~al.(2014)Neyshabur, Tomioka, and
  Srebro]{neyshabur2014search}
Neyshabur, Behnam, Tomioka, Ryota, and Srebro, Nathan.
\newblock In search of the real inductive bias: On the role of implicit
  regularization in deep learning.
\newblock \emph{arXiv preprint arXiv:1412.6614}, 2014.

\bibitem[Neyshabur et~al.(2018)Neyshabur, Li, Bhojanapalli, LeCun, and
  Srebro]{neyshabur2018towards}
Neyshabur, Behnam, Li, Zhiyuan, Bhojanapalli, Srinadh, LeCun, Yann, and Srebro,
  Nathan.
\newblock Towards understanding the role of over-parametrization in
  generalization of neural networks.
\newblock \emph{arXiv preprint arXiv:1805.12076}, 2018.

\bibitem[Novak et~al.(2018)Novak, Bahri, Abolafia, Pennington, and
  Sohl-Dickstein]{novak2018sensitivity}
Novak, Roman, Bahri, Yasaman, Abolafia, Daniel~A, Pennington, Jeffrey, and
  Sohl-Dickstein, Jascha.
\newblock Sensitivity and generalization in neural networks: an empirical
  study.
\newblock \emph{arXiv preprint arXiv:1802.08760}, 2018.

\bibitem[Shalev-Shwartz \& Ben-David(2014)Shalev-Shwartz and
  Ben-David]{shalev2014understanding}
Shalev-Shwartz, Shai and Ben-David, Shai.
\newblock \emph{Understanding machine learning: From theory to algorithms}.
\newblock Cambridge university press, 2014.

\bibitem[Soltanolkotabi et~al.(2018)Soltanolkotabi, Javanmard, and
  Lee]{soltanolkotabi2018theoretical}
Soltanolkotabi, Mahdi, Javanmard, Adel, and Lee, Jason~D.
\newblock Theoretical insights into the optimization landscape of
  over-parameterized shallow neural networks.
\newblock \emph{IEEE Transactions on Information Theory}, 2018.

\bibitem[Vershynin(2017)]{vershynin2017high}
Vershynin, Roman.
\newblock High-dimensional probability.
\newblock \emph{An Introduction with Applications}, 2017.

\end{thebibliography}
