\begin{thebibliography}{72}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2019)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2019optimality}
Agarwal, A., Kakade, S.~M., Lee, J.~D., and Mahajan, G.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{arXiv preprint arXiv:1908.00261}, 2019.

\bibitem[Agarwal et~al.(2020{\natexlab{a}})Agarwal, Henaff, Kakade, and
  Sun]{agarwal2020pc}
Agarwal, A., Henaff, M., Kakade, S., and Sun, W.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{arXiv preprint arXiv:2007.08459}, 2020{\natexlab{a}}.

\bibitem[Agarwal et~al.(2020{\natexlab{b}})Agarwal, Kakade, Krishnamurthy, and
  Sun]{agarwal2020flambe}
Agarwal, A., Kakade, S., Krishnamurthy, A., and Sun, W.
\newblock Flambe: Structural complexity and representation learning of low rank
  mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Akkaya et~al.(2019)Akkaya, Andrychowicz, Chociej, Litwin, McGrew,
  Petron, Paino, Plappert, Powell, Ribas, et~al.]{akkaya2019solving}
Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew, B., Petron, A.,
  Paino, A., Plappert, M., Powell, G., Ribas, R., et~al.
\newblock Solving rubik's cube with a robot hand.
\newblock \emph{arXiv preprint arXiv:1910.07113}, 2019.

\bibitem[Auer et~al.(2009)Auer, Jaksch, and Ortner]{auer2009near}
Auer, P., Jaksch, T., and Ortner, R.
\newblock Near-optimal regret bounds for reinforcement learning.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  89--96, 2009.

\bibitem[Ayoub et~al.(2020)Ayoub, Jia, Szepesvari, Wang, and
  Yang]{ayoub2020model}
Ayoub, A., Jia, Z., Szepesvari, C., Wang, M., and Yang, L.~F.
\newblock Model-based reinforcement learning with value-targeted regression.
\newblock \emph{arXiv preprint arXiv:2006.01107}, 2020.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Azar, M.~G., Osband, I., and Munos, R.
\newblock Minimax regret bounds for reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  263--272, 2017.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, D{\k{e}}biak,
  Dennison, Farhi, Fischer, Hashme, Hesse, et~al.]{berner2019dota}
Berner, C., Brockman, G., Chan, B., Cheung, V., D{\k{e}}biak, P., Dennison, C.,
  Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et~al.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Brafman \& Tennenholtz(2002)Brafman and Tennenholtz]{brafman2002r}
Brafman, R.~I. and Tennenholtz, M.
\newblock R-max-a general polynomial time algorithm for near-optimal
  reinforcement learning.
\newblock \emph{Journal of Machine Learning Research}, 3\penalty0
  (Oct):\penalty0 213--231, 2002.

\bibitem[Bubeck \& Cesa-Bianchi(2012)Bubeck and Cesa-Bianchi]{bubeck2012regret}
Bubeck, S. and Cesa-Bianchi, N.
\newblock Regret analysis of stochastic and nonstochastic multi-armed bandit
  problems.
\newblock \emph{arXiv preprint arXiv:1204.5721}, 2012.

\bibitem[Cai et~al.(2019)Cai, Yang, Jin, and Wang]{cai2019provably}
Cai, Q., Yang, Z., Jin, C., and Wang, Z.
\newblock Provably efficient exploration in policy optimization.
\newblock \emph{arXiv preprint arXiv:1912.05830}, 2019.

\bibitem[Charikar et~al.(2017)Charikar, Steinhardt, and
  Valiant]{charikar2017learning}
Charikar, M., Steinhardt, J., and Valiant, G.
\newblock Learning from untrusted data.
\newblock In \emph{Proceedings of the 49th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  47--60, 2017.

\bibitem[Cheung et~al.(2019)Cheung, Simchi-Levi, and Zhu]{cheung2019non}
Cheung, W.~C., Simchi-Levi, D., and Zhu, R.
\newblock Non-stationary reinforcement learning: The blessing of (more)
  optimism.
\newblock \emph{Available at SSRN 3397818}, 2019.

\bibitem[Dann \& Brunskill(2015)Dann and Brunskill]{dann2015sample}
Dann, C. and Brunskill, E.
\newblock Sample complexity of episodic fixed-horizon reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  2818--2826, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Dann, C., Lattimore, T., and Brunskill, E.
\newblock Unifying pac and regret: Uniform pac bounds for episodic
  reinforcement learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  5713--5723, 2017.

\bibitem[Derman et~al.(2020)Derman, Mankowitz, Mann, and
  Mannor]{derman2020bayesian}
Derman, E., Mankowitz, D., Mann, T., and Mannor, S.
\newblock A bayesian approach to robust reinforcement learning.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pp.\  648--658.
  PMLR, 2020.

\bibitem[Diakonikolas \& Kane(2019)Diakonikolas and
  Kane]{diakonikolas2019recent}
Diakonikolas, I. and Kane, D.~M.
\newblock Recent advances in algorithmic high-dimensional robust statistics.
\newblock \emph{arXiv preprint arXiv:1911.05911}, 2019.

\bibitem[Diakonikolas et~al.(2016)Diakonikolas, Kamath, Kane, Li, Moitra, and
  Stewart]{diakonikolas2016robust}
Diakonikolas, I., Kamath, G., Kane, D., Li, J., Moitra, A., and Stewart, A.
\newblock Robust estimators in high dimensions without the computational
  intractability.
\newblock In \emph{2016 IEEE 57th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  655--664, 2016.

\bibitem[Diakonikolas et~al.(2017)Diakonikolas, Kamath, Kane, Li, Moitra, and
  Stewart]{diakonikolas2017being}
Diakonikolas, I., Kamath, G., Kane, D.~M., Li, J., Moitra, A., and Stewart, A.
\newblock Being robust (in high dimensions) can be practical.
\newblock \emph{arXiv preprint arXiv:1703.00893}, 2017.

\bibitem[Diakonikolas et~al.(2019)Diakonikolas, Kamath, Kane, Li, Steinhardt,
  and Stewart]{diakonikolas2019sever}
Diakonikolas, I., Kamath, G., Kane, D., Li, J., Steinhardt, J., and Stewart, A.
\newblock Sever: A robust meta-algorithm for stochastic optimization.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1596--1606, 2019.

\bibitem[Diakonikolas et~al.(2020)Diakonikolas, Kane, and
  Pensia]{diakonikolas2020outlier}
Diakonikolas, I., Kane, D.~M., and Pensia, A.
\newblock Outlier robust mean estimation with subgaussian rates via stability.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Domingues et~al.(2020)Domingues, M{\'e}nard, Pirotta, Kaufmann, and
  Valko]{domingues2020kernel}
Domingues, O.~D., M{\'e}nard, P., Pirotta, M., Kaufmann, E., and Valko, M.
\newblock A kernel-based approach to non-stationary reinforcement learning in
  metric spaces.
\newblock \emph{arXiv preprint arXiv:2007.05078}, 2020.

\bibitem[Du et~al.(2019)Du, Luo, Wang, and Zhang]{du2019provably}
Du, S.~S., Luo, Y., Wang, R., and Zhang, H.
\newblock Provably efficient q-learning with function approximation via
  distribution shift error checking oracle.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  8060--8070, 2019.

\bibitem[Even-Dar et~al.(2009)Even-Dar, Kakade, and Mansour]{even2009online}
Even-Dar, E., Kakade, S.~M., and Mansour, Y.
\newblock Online markov decision processes.
\newblock \emph{Mathematics of Operations Research}, 34\penalty0 (3):\penalty0
  726--736, 2009.

\bibitem[Eykholt et~al.(2018)Eykholt, Evtimov, Fernandes, Li, Rahmati, Xiao,
  Prakash, Kohno, and Song]{eykholt2018robust}
Eykholt, K., Evtimov, I., Fernandes, E., Li, B., Rahmati, A., Xiao, C.,
  Prakash, A., Kohno, T., and Song, D.
\newblock Robust physical-world attacks on deep learning visual classification.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pp.\  1625--1634, 2018.

\bibitem[Gupta et~al.(2019)Gupta, Koren, and Talwar]{gupta2019better}
Gupta, A., Koren, T., and Talwar, K.
\newblock Better algorithms for stochastic bandits with adversarial
  corruptions.
\newblock \emph{arXiv preprint arXiv:1902.08647}, 2019.

\bibitem[Jiang et~al.(2017)Jiang, Krishnamurthy, Agarwal, Langford, and
  Schapire]{jiang2017contextual}
Jiang, N., Krishnamurthy, A., Agarwal, A., Langford, J., and Schapire, R.~E.
\newblock Contextual decision processes with low bellman rank are
  pac-learnable.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1704--1713. PMLR, 2017.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018q}
Jin, C., Allen-Zhu, Z., Bubeck, S., and Jordan, M.~I.
\newblock Is q-learning provably efficient?
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  4863--4873, 2018.

\bibitem[Jin et~al.(2019)Jin, Netrapalli, Ge, Kakade, and Jordan]{jin2019short}
Jin, C., Netrapalli, P., Ge, R., Kakade, S.~M., and Jordan, M.~I.
\newblock A short note on concentration inequalities for random vectors with
  subgaussian norm.
\newblock \emph{arXiv preprint arXiv:1902.03736}, 2019.

\bibitem[Jin et~al.(2020{\natexlab{a}})Jin, Jin, Luo, Sra, and
  Yu]{jin2020learning}
Jin, C., Jin, T., Luo, H., Sra, S., and Yu, T.
\newblock Learning adversarial markov decision processes with bandit feedback
  and unknown transition.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  4860--4869. PMLR, 2020{\natexlab{a}}.

\bibitem[Jin et~al.(2020{\natexlab{b}})Jin, Yang, Wang, and
  Jordan]{jin2020provably}
Jin, C., Yang, Z., Wang, Z., and Jordan, M.~I.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pp.\  2137--2143. PMLR,
  2020{\natexlab{b}}.

\bibitem[Jin \& Luo(2020)Jin and Luo]{jin2020simultaneously}
Jin, T. and Luo, H.
\newblock Simultaneously learning stochastic and adversarial episodic mdps with
  known transition.
\newblock \emph{arXiv preprint arXiv:2006.05606}, 2020.

\bibitem[Kakade \& Langford(2002)Kakade and Langford]{kakade2002approximately}
Kakade, S. and Langford, J.
\newblock Approximately optimal approximate reinforcement learning.
\newblock In \emph{ICML}, volume~2, pp.\  267--274, 2002.

\bibitem[Kakade et~al.(2020)Kakade, Krishnamurthy, Lowrey, Ohnishi, and
  Sun]{kakade2020information}
Kakade, S., Krishnamurthy, A., Lowrey, K., Ohnishi, M., and Sun, W.
\newblock Information theoretic regret bounds for online nonlinear control.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Kakade(2001)]{kakade2001natural}
Kakade, S.~M.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems},
  14:\penalty0 1531--1538, 2001.

\bibitem[Lai et~al.(2016)Lai, Rao, and Vempala]{lai2016agnostic}
Lai, K.~A., Rao, A.~B., and Vempala, S.
\newblock Agnostic estimation of mean and covariance.
\newblock In \emph{2016 IEEE 57th Annual Symposium on Foundations of Computer
  Science (FOCS)}, pp.\  665--674. IEEE, 2016.

\bibitem[Lee et~al.(2020)Lee, Luo, Wei, and Zhang]{lee2020bias}
Lee, C.-W., Luo, H., Wei, C.-Y., and Zhang, M.
\newblock Bias no more: high-probability data-dependent regret bounds for
  adversarial bandits and mdps.
\newblock \emph{Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem[Lykouris et~al.(2018)Lykouris, Mirrokni, and
  Paes~Leme]{lykouris2018stochastic}
Lykouris, T., Mirrokni, V., and Paes~Leme, R.
\newblock Stochastic bandits robust to adversarial corruptions.
\newblock In \emph{Proceedings of the 50th Annual ACM SIGACT Symposium on
  Theory of Computing}, pp.\  114--122, 2018.

\bibitem[Lykouris et~al.(2019)Lykouris, Simchowitz, Slivkins, and
  Sun]{lykouris2019corruption}
Lykouris, T., Simchowitz, M., Slivkins, A., and Sun, W.
\newblock Corruption robust exploration in episodic reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1911.08689}, 2019.

\bibitem[Ma et~al.(2019)Ma, Zhang, Sun, and Zhu]{ma2019policy}
Ma, Y., Zhang, X., Sun, W., and Zhu, J.
\newblock Policy poisoning in batch reinforcement learning and control.
\newblock In \emph{Advances in Neural Information Processing Systems}, pp.\
  14570--14580, 2019.

\bibitem[Neff \& Nagy(2016)Neff and Nagy]{neff2016automation}
Neff, G. and Nagy, P.
\newblock Automation, algorithms, and politics| talking to bots: Symbiotic
  agency and the case of tay.
\newblock \emph{International Journal of Communication}, 10:\penalty0 17, 2016.

\bibitem[Neu \& Olkhovskaya(2020)Neu and Olkhovskaya]{neu2020online}
Neu, G. and Olkhovskaya, J.
\newblock Online learning in mdps with linear function approximation and bandit
  feedback.
\newblock \emph{arXiv preprint arXiv:2007.01612}, 2020.

\bibitem[Neu et~al.(2010)Neu, Gy{\"o}rgy, and Szepesv{\'a}ri]{neu2010online}
Neu, G., Gy{\"o}rgy, A., and Szepesv{\'a}ri, C.
\newblock The online loop-free stochastic shortest-path problem.
\newblock In \emph{COLT}, volume 2010, pp.\  231--243. Citeseer, 2010.

\bibitem[Neu et~al.(2012)Neu, Gyorgy, and Szepesv{\'a}ri]{neu2012adversarial}
Neu, G., Gyorgy, A., and Szepesv{\'a}ri, C.
\newblock The adversarial stochastic shortest path problem with unknown
  transition probabilities.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  805--813,
  2012.

\bibitem[Ornik \& Topcu(2019)Ornik and Topcu]{ornik2019learning}
Ornik, M. and Topcu, U.
\newblock Learning and planning for time-varying mdps using maximum likelihood
  estimation.
\newblock \emph{arXiv preprint arXiv:1911.12976}, 2019.

\bibitem[Ortner et~al.(2019)Ortner, Gajane, and Auer]{ortner2019variational}
Ortner, R., Gajane, P., and Auer, P.
\newblock Variational regret bounds for reinforcement learning.
\newblock In \emph{UAI}, pp.\ ~16, 2019.

\bibitem[Osband \& Van~Roy(2014)Osband and Van~Roy]{osband2014model}
Osband, I. and Van~Roy, B.
\newblock Model-based reinforcement learning and the eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  27:\penalty0 1466--1474, 2014.

\bibitem[Osband \& Van~Roy(2016)Osband and Van~Roy]{osband2016lower}
Osband, I. and Van~Roy, B.
\newblock On lower bounds for regret in reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1608.02732}, 2016.

\bibitem[Petersen et~al.(2012)Petersen, Ugrinovskii, and
  Savkin]{petersen2012robust}
Petersen, I.~R., Ugrinovskii, V.~A., and Savkin, A.~V.
\newblock \emph{Robust Control Design Using H-$\infty$ Methods}.
\newblock Springer Science \& Business Media, 2012.

\bibitem[Pinto et~al.(2017)Pinto, Davidson, Sukthankar, and
  Gupta]{pinto2017robust}
Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A.
\newblock Robust adversarial reinforcement learning.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  2817--2826. PMLR, 2017.

\bibitem[Rosenberg \& Mansour(2019)Rosenberg and Mansour]{rosenberg2019online}
Rosenberg, A. and Mansour, Y.
\newblock Online convex optimization in adversarial markov decision processes.
\newblock \emph{arXiv preprint arXiv:1905.07773}, 2019.

\bibitem[Schulman et~al.(2015{\natexlab{a}})Schulman, Levine, Abbeel, Jordan,
  and Moritz]{schulman2015trust}
Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pp.\
  1889--1897, 2015{\natexlab{a}}.

\bibitem[Schulman et~al.(2015{\natexlab{b}})Schulman, Moritz, Levine, Jordan,
  and Abbeel]{schulman2015high}
Schulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P.
\newblock High-dimensional continuous control using generalized advantage
  estimation.
\newblock \emph{arXiv preprint arXiv:1506.02438}, 2015{\natexlab{b}}.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shalev-Shwartz et~al.(2011)]{shalev2011online}
Shalev-Shwartz, S. et~al.
\newblock Online learning and online convex optimization.
\newblock \emph{Foundations and trends in Machine Learning}, 4\penalty0
  (2):\penalty0 107--194, 2011.

\bibitem[Sun et~al.(2019)Sun, Jiang, Krishnamurthy, Agarwal, and
  Langford]{sun2019model}
Sun, W., Jiang, N., Krishnamurthy, A., Agarwal, A., and Langford, J.
\newblock Model-based rl in contextual decision processes: Pac bounds and
  exponential improvements over model-free approaches.
\newblock In \emph{Conference on Learning Theory}, pp.\  2898--2933. PMLR,
  2019.

\bibitem[Sutton et~al.(1999)Sutton, McAllester, Singh, and
  Mansour]{sutton1999policy}
Sutton, R.~S., McAllester, D.~A., Singh, S.~P., and Mansour, Y.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~99, pp.\  1057--1063, 1999.

\bibitem[Todorov et~al.(2012)Todorov, Erez, and Tassa]{todorov2012mujoco}
Todorov, E., Erez, T., and Tassa, Y.
\newblock Mujoco: A physics engine for model-based control.
\newblock In \emph{2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pp.\  5026--5033. IEEE, 2012.

\bibitem[Tropp(2015)]{tropp2015introduction}
Tropp, J.~A.
\newblock An introduction to matrix concentration inequalities.
\newblock \emph{arXiv preprint arXiv:1501.01571}, 2015.

\bibitem[Williams(1992)]{williams1992simple}
Williams, R.~J.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock \emph{Machine learning}, 8\penalty0 (3-4):\penalty0 229--256, 1992.

\bibitem[Yadkori et~al.(2013)Yadkori, Bartlett, Kanade, Seldin, and
  Szepesv{\'a}ri]{yadkori2013online}
Yadkori, Y.~A., Bartlett, P.~L., Kanade, V., Seldin, Y., and Szepesv{\'a}ri, C.
\newblock Online learning in markov decision processes with adversarially
  chosen transition probability distributions.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  2508--2516, 2013.

\bibitem[Yang \& Wang(2019{\natexlab{a}})Yang and Wang]{yang2019sample}
Yang, L. and Wang, M.
\newblock Sample-optimal parametric q-learning using linearly additive
  features.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  6995--7004. PMLR, 2019{\natexlab{a}}.

\bibitem[Yang \& Wang(2019{\natexlab{b}})Yang and Wang]{yang2019reinforcement}
Yang, L.~F. and Wang, M.
\newblock Reinforcement learning in feature space: Matrix bandit, kernels, and
  regret bound.
\newblock \emph{arXiv preprint arXiv:1905.10389}, 2019{\natexlab{b}}.

\bibitem[Zanette et~al.(2020)Zanette, Brandfonbrener, Brunskill, Pirotta, and
  Lazaric]{zanette2020frequentist}
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M., and Lazaric, A.
\newblock Frequentist regret bounds for randomized least-squares value
  iteration.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pp.\  1954--1964, 2020.

\bibitem[Zhang et~al.(2020{\natexlab{a}})Zhang, Hu, and Basar]{zhang2020policy}
Zhang, K., Hu, B., and Basar, T.
\newblock Policy optimization for h-2 linear control with h-$\infty$ robustness
  guarantee: Implicit regularization and global convergence.
\newblock In \emph{Learning for Dynamics and Control}, pp.\  179--190. PMLR,
  2020{\natexlab{a}}.

\bibitem[Zhang et~al.(2020{\natexlab{b}})Zhang, Hu, and
  Basar]{zhang2020stability}
Zhang, K., Hu, B., and Basar, T.
\newblock On the stability and convergence of robust adversarial reinforcement
  learning: A case study on linear quadratic systems.
\newblock \emph{Advances in Neural Information Processing Systems}, 33,
  2020{\natexlab{b}}.

\bibitem[Zhang et~al.(2021)Zhang, Zhang, Hu, and
  Ba{\c{s}}ar]{zhang2021derivative}
Zhang, K., Zhang, X., Hu, B., and Ba{\c{s}}ar, T.
\newblock Derivative-free policy optimization for risk-sensitive and robust
  control design: Implicit regularization and sample complexity.
\newblock \emph{arXiv preprint arXiv:2101.01041}, 2021.

\bibitem[Zhang et~al.(2020{\natexlab{c}})Zhang, Ma, Singla, and
  Zhu]{zhang2020adaptive}
Zhang, X., Ma, Y., Singla, A., and Zhu, X.
\newblock Adaptive reward-poisoning attacks against reinforcement learning.
\newblock \emph{arXiv preprint arXiv:2003.12613}, 2020{\natexlab{c}}.

\bibitem[Zhou et~al.(2020)Zhou, He, and Gu]{zhou2020provably}
Zhou, D., He, J., and Gu, Q.
\newblock Provably efficient reinforcement learning for discounted mdps with
  feature mapping.
\newblock \emph{arXiv preprint arXiv:2006.13165}, 2020.

\bibitem[Zhou \& Doyle(1998)Zhou and Doyle]{zhou1998essentials}
Zhou, K. and Doyle, J.~C.
\newblock \emph{Essentials of robust control}, volume 104.
\newblock Prentice hall Upper Saddle River, NJ, 1998.

\bibitem[Zimin \& Neu(2013)Zimin and Neu]{zimin2013online}
Zimin, A. and Neu, G.
\newblock Online learning in episodic markovian decision processes by relative
  entropy policy search.
\newblock In \emph{Advances in neural information processing systems}, pp.\
  1583--1591, 2013.

\bibitem[Zinkevich(2003)]{zinkevich2003online}
Zinkevich, M.
\newblock Online convex programming and generalized infinitesimal gradient
  ascent.
\newblock In \emph{Proceedings of the 20th international conference on machine
  learning (icml-03)}, pp.\  928--936, 2003.

\end{thebibliography}
