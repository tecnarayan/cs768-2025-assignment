// NTK & FL papers
	// federated knowleadledge distillation
	@article{seo2020federated,
	title={Federated knowledge distillation},
	author={Seo, Hyowoon and Park, Jihong and Oh, Seungeun and Bennis, Mehdi and Kim, Seong-Lyun},
	journal={arXiv preprint arXiv:2011.02367},
	year={2020}
	}

	// FL-NTK
	@article{huang2021fl,
	title={{FL-NTK}: A Neural Tangent Kernel-based Framework for Federated Learning Convergence Analysis},
	author={Huang, Baihe and Li, Xiaoxiao and Song, Zhao and Yang, Xin},
	journal={arXiv preprint arXiv:2105.05001},
	year={2021}
	}

	// fedbn
	@inproceedings{li2021fedbn,
	title="Fed{BN}: Federated Learning on Non-IID Features via Local Batch Normalization",
	author="Xiaoxiao {Li} and Meirui {Jiang} and Xiaofei {Zhang} and Michael {Kamp} and Qi {Dou}",
	booktitle="International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3124293845",
	year="2021"
	}

	@article{su2021achieving,
	title="Achieving Statistical Optimality of Federated Learning: Beyond Stationary Points",
	author="Lili {Su} and Jiaming {Xu} and Pengkun {Yang}",
	journal="arXiv preprint arXiv:2106.15216",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3176457956",
	year="2021"
	}

	// GD provably optimizes 
	@inproceedings{du2019gradient,
		title="Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
		author="Simon S. {Du} and Xiyu {Zhai} and Barnabas {Poczos} and Aarti {Singh}",
		booktitle="International Conference on Learning Representations",
		notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2964161337",
		year="2019"
	}

	// Optimization for ReLU
	@inproceedings{dukler2020optimization,
		title="Optimization Theory for {R}e{LU} Neural Networks Trained with Normalization Layers",
		author="Yonatan {Dukler} and Guido {Montufar} and Quanquan {Gu}",
		booktitle="International Conference on Machine Learning",
		notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3035707704",
		year="2020"
	}

//------------------------------------------ 
// FL papers
//------------------------------------------ 
// advances and open problems
@article{kairouz2021advances,
  title={Advances and open problems in federated learning},
  author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and others},
	journal={Foundations and Trends in Machine Learning},
	year={2021}
}

// FL survey
@article{li2020flsurvey,
  title={Federated learning: Challenges, methods, and future directions},
  author={Li, Tian and Sahu, Anit Kumar and Talwalkar, Ameet and Smith, Virginia},
  journal={IEEE Signal Processing Magazine},
  volume={37},
  number={3},
  pages={50--60},
  year={2020},
  publisher={IEEE}
}

// Adaptive Federated Learning
@inproceedings{reddi2021adaptive,
	title="Adaptive Federated Optimization",
	author="Sashank J. {Reddi} and Zachary {Charles} and Manzil {Zaheer} and Zachary {Garrett} and Keith {Rush} and Jakub {Konečný} and Sanjiv {Kumar} and Hugh Brendan {McMahan}",
	booktitle="International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3124442241",
	year="2021"
}

// A field guide
@article{wang2021field,
  title={A Field Guide to Federated Optimization},
  author={Wang, Jianyu and Charles, Zachary and Xu, Zheng and Joshi, Gauri and McMahan, H Brendan and Al-Shedivat, Maruan and Andrew, Galen and Avestimehr, Salman and Daly, Katharine and Data, Deepesh and others},
  journal={arXiv preprint arXiv:2107.06917},
  year={2021}
}


// FL concept
@inproceedings{mcmahan2017communication,
  title={Communication-efficient learning of deep networks from decentralized data},
  author={McMahan, Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and y Arcas, Blaise Aguera},
  booktitle={Artificial Intelligence and Statistics},
  year={2017},
}

// fedma
@inproceedings{wang2020federated,
  title={Federated Learning with Matched Averaging},
  author={Wang, Hongyi and Yurochkin, Mikhail and Sun, Yuekai and Papailiopoulos, Dimitris and Khazaeni, Yasaman},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

// fedbe
@inproceedings{chen2021fedbe,
  title={Fed{BE}: Making {B}ayesian Model Ensemble Applicable to Federated Learning},
  author={Chen, Hong-You and Chao, Wei-Lun},
  booktitle={International Conference on Learning Representations},
  year={2021}
}

// fedcom
@inproceedings{haddadpour2021federated,
  title={Federated learning with compression: Unified analysis and sharp guarantees},
  author={Haddadpour, Farzin and Kamani, Mohammad Mahdi and Mokhtari, Aryan and Mahdavi, Mehrdad},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2021}
}

// fedprox
@inproceedings{li2020federated,
	title="Federated Optimization in Heterogeneous Networks",
	author="Tian {Li} and Anit Kumar {Sahu} and Manzil {Zaheer} and Maziar {Sanjabi} and Ameet {Talwalkar} and Virginia {Smith}",
	journal="Proceedings of Machine Learning and Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3038022836",
	year="2020"
}

// fednova
@inproceedings{wang2020tackling,
	title="Tackling the Objective Inconsistency Problem in Heterogeneous Federated Optimization.",
	author="Jianyu {Wang} and Qinghua {Liu} and Hao {Liang} and Gauri {Joshi} and H. Vincent {Poor}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3100125973",
	year="2020"
}

// federated multi-task
@inproceedings{smith2017federated,
	title="Federated multi-task learning",
	author="Virginia {Smith} and Chao-Kai {Chiang} and Maziar {Sanjabi} and Ameet {Talwalkar}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963300197",
	year="2017"
}

@inproceedings{tran2019federated,
  title={Federated learning over wireless networks: Optimization model design and analysis},
  author={Tran, Nguyen H and Bao, Wei and Zomaya, Albert and Nguyen, Minh NH and Hong, Choong Seon},
  booktitle={IEEE Conference on Computer Communications},
  pages={1387--1395},
  year={2019},
  organization={IEEE}
}

// seceret revealer dp for reconstruction
@inproceedings{zhang2020secret,
  title={The secret revealer: Generative model-inversion attacks against deep neural networks},
  author={Zhang, Yuheng and Jia, Ruoxi and Pei, Hengzhi and Wang, Wenxiao and Li, Bo and Song, Dawn},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={253--261},
  year={2020}
}

// membership inference
@inproceedings{nasr2018machine,
  title={Machine learning with membership privacy using adversarial regularization},
  author={Nasr, Milad and Shokri, Reza and Houmansadr, Amir},
  booktitle={ACM SIGSAC Conference on Computer and Communications Security},
  pages={634--646},
  year={2018}
}

// Deep leakage
@inproceedings{zhu2019deep,
	title="Deep Leakage from Gradients",
	author="Ligeng {Zhu} and Zhijian {Liu} and Song {Han}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2970408908",
	year="2019"
}


// shuffled model 
@inproceedings{girgis2021shuffled,
	title="Shuffled Model of Differential Privacy in Federated Learning.",
	author="Antonious M. {Girgis} and Deepesh {Data} and Suhas N. {Diggavi} and Peter {Kairouz} and Ananda Theertha {Suresh}",
	booktitle=" International Conference on Artificial Intelligence and Statistics",
	year="2021"
}

@article{cheng2021separation,
  title={Separation of Powers in Federated Learning},
  author={Cheng, Pau-Chen and Eykholt, Kevin and Gu, Zhongshu and Jamjoom, Hani and Jayaram, KR and Valdez, Enriquillo and Verma, Ashish},
  journal={arXiv preprint arXiv:2105.09400},
  year={2021}
}	

	// signSGD
	@inproceedings{bernstein2018signsgd,
		title={sign{SGD}: Compressed Optimisation for Non-Convex Problems},
		author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
		booktitle={International Conference on Machine Learning},
		year={2018}
	}


	// top-k
	@inproceedings{aji2017sparse,
		title="Sparse Communication for Distributed Gradient Descent",
		author="Alham Fikri {Aji} and Kenneth {Heafield}",
		booktitle="Proceedings of the 2017 Conference on Empirical Methods in Natural
		Language Processing",
		notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3101036738",
		year="2017"
	}

	// stc
	@article{sattler2019robust,
	title={Robust and communication-efficient federated learning from non-iid data},
	author={Sattler, Felix and Wiedemann, Simon and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
	journal={IEEE Transactions on Neural Networks and Learning Systems},
	volume={31},
	number={9},
	pages={3400--3413},
	year={2019},
	publisher={IEEE}
	}

	// SCAFFOLD
	@inproceedings{karimireddy2020scaffold,
	title={Scaffold: Stochastic controlled averaging for federated learning},
	author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank and Stich, Sebastian and Suresh, Ananda Theertha},
	booktitle={International Conference on Machine Learning},
	year={2020},
	}

// non-iid 
@article{zhao2018federated,
  title={Federated learning with non-iid data},
  author={Zhao, Yue and Li, Meng and Lai, Liangzhen and Suda, Naveen and Civin, Damon and Chandra, Vikas},
  journal={arXiv preprint arXiv:1806.00582},
  year={2018}
}

// mnist 
@article{lecun1998gradient,
  title={Gradient-based learning applied to document recognition},
  author={LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick},
  journal={Proceedings of the IEEE},
  volume={86},
  number={11},
  pages={2278--2324},
  year={1998},
  publisher={Ieee}
}

// fashion mnist
@article{xiao2017fashion,
  title={Fashion-{MNIST}: a novel image dataset for benchmarking machine learning algorithms},
  author={Xiao, Han and Rasul, Kashif and Vollgraf, Roland},
  journal={arXiv preprint arXiv:1708.07747},
  year={2017}
}

// emnist
@inproceedings{cohen2017emnist,
  title={{EMNIST}: Extending {MNIST} to handwritten letters},
  author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and Van Schaik, Andre},
  booktitle={International Joint Conference on Neural Networks},
  year={2017},
}

// fedpaq
@inproceedings{reisizadeh2020fedpaq,
  title={Fed{PAQ}: A communication-efficient federated learning method with periodic averaging and quantization},
  author={Reisizadeh, Amirhossein and Mokhtari, Aryan and Hassani, Hamed and Jadbabaie, Ali and Pedarsani, Ramtin},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  year={2020},
}

// non-iid dirichlet
@article{hsu2019measuring,
  title={Measuring the effects of non-identical data distribution for federated visual classification},
  author={Hsu, Tzu-Ming Harry and Qi, Hang and Brown, Matthew},
  journal={arXiv preprint arXiv:1909.06335},
  year={2019}
}


//------------------------------------------ 
// NTK papers 
//------------------------------------------ 
	@inproceedings{lee2019wide,
	title={Wide neural networks of any depth evolve as linear models under gradient descent},
	author={Lee, Jaehoon and Xiao, Lechao and Schoenholz, Samuel and Bahri, Yasaman and Novak, Roman and Sohl-Dickstein, Jascha and Pennington, Jeffrey},
	booktitle="Advances in Neural Information Processing Systems",
	year={2019}
	}

	@inproceedings{jacot2018neural,
	title="Neural Tangent Kernel: Convergence and Generalization in Neural Networks",
	author="Arthur {Jacot} and Franck {Gabriel} and Clément {Hongler}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2809090039",
	year="2018"
	}

	// 2 layer
	@inproceedings{chen2020a,
	title="A Generalized Neural Tangent Kernel Analysis for Two-layer Neural Networks",
	author="Zixiang {Chen} and Yuan {Cao} and Quanquan {Gu} and Tong {Zhang}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3104810215",
	year="2020"
	}

	// CNTK
	@inproceedings{arora2019on,
	title="On Exact Computation with an Infinitely Wide Neural Net",
	author="Sanjeev {Arora} and Simon S. {Du} and Wei {Hu} and Zhiyuan {Li} and Ruslan {Salakhutdinov} and Ruosong {Wang}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2971043187",
	year="2019"
	}

	// ntk-rnn
	@inproceedings{alemohammad2021the,
	title="The Recurrent Neural Tangent Kernel",
	author="Sina {Alemohammad} and Zichao {Wang} and Randall {Balestriero} and Richard {Baraniuk}",
	booktitle="International Conference on Learning Representations",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3129945678",
	year="2021"
	}

	// ntk any architeacture
	@inproceedings{yang2021tensor,
	title="Tensor Programs {II}b: Architectural Universality Of Neural Tangent Kernel Training Dynamics",
	author="Greg {Yang} and Etai {Littwin}",
	booktitle="International Conference on Machine Learning",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3167080048",
	year="2021"
	}

	@inproceedings{williams1996computing,
	title="Computing with Infinite Networks",
	author="Christopher K. I. {Williams}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2154087390",
	year="1996"
	}

	@inproceedings{lee2020finite,
	title="Finite Versus Infinite Neural Networks: An Empirical Study",
	author="Jaehoon {Lee} and Samuel S. {Schoenholz} and Jeffrey {Pennington} and Ben {Adlam} and Lechao {Xiao} and Roman {Novak} and Jascha {Sohl-Dickstein}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3101069636",
	year="2020"
	}
//------------------------------------------



// QSGD
@inproceedings{alistarh2017qsgd,
  title={{QSGD}: Communication-efficient {SGD} via gradient quantization and encoding},
  author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
  journal={Advances in Neural Information Processing Systems},
  year={2017}
}

// federated UA
@article{hosseini2020federated,
  title={Federated Learning of User Authentication Models},
  author={Hosseini, Hossein and Yun, Sungrack and Park, Hyunsin and Louizos, Christos and Soriaga, Joseph and Welling, Max},
  journal={arXiv preprint arXiv:2007.04618},
  year={2020}
}

// top-k
@inproceedings{alistarh2018the,
	title="The Convergence of Sparsified Gradient Methods",
	author="Dan {Alistarh} and Torsten {Hoefler} and Mikael {Johansson} and Nikola {Konstantinov} and Sarit {Khirirat} and Cedric {Renggli}",
	booktitle="Advances in Neural Information Processing Systems",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963766684",
	year="2018"
}

//------------------------------------------ 
// deep learning general 
//------------------------------------------ 
	// resnet
	@inproceedings{he2016deep,
		title="Deep Residual Learning for Image Recognition",
		author="Kaiming {He} and Xiangyu {Zhang} and Shaoqing {Ren} and Jian {Sun}",
		booktitle="International Conference on Computer Vision and Pattern Recognition",
		year="2016"
	}

	// deep double descent
	@inproceedings{nakkiran2020deep,
		title="Deep Double Descent: Where Bigger Models and More Data Hurt",
		author="Preetum {Nakkiran} and Gal {Kaplun} and Yamini {Bansal} and Tristan {Yang} and Boaz {Barak} and Ilya {Sutskever}",
		booktitle="International Conference on Learning Representations",
		notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2996603747",
		year="2020"
	}

// Tensor app
@article{kolda2009tensor,
	title={Tensor decompositions and applications},
	author={Kolda, Tamara G and Bader, Brett W},
	journal={SIAM {R}eview},
	volume={51},
	number={3},
	pages={455--500},
	year={2009},
	publisher={SIAM}
}

@article{good1976application,
  title={On the application of symmetric {D}irichlet distributions and their mixtures to contingency tables},
  author={Good, Irving J},
  journal={The Annals of Statistics},
  volume={4},
  number={6},
  pages={1159--1189},
  year={1976},
  publisher={Institute of Mathematical Statistics}
}

@book{van2020computer,
  title={Computer Security and the Internet: Tools and Jewels},
  author={Van Oorschot, Paul C},
  year={2020},
  publisher={Springer Nature}
}


// cifar
@article{krizhevsky2009learning,
	title="Learning Multiple Layers of Features from Tiny Images",
	author="Alex {Krizhevsky}",
	journal="Master thesis, Dept. of Comput. Sci., Univ. of Toronto, Toronto, Canada",
	notes="Sourced from Microsoft Academic - https://academic.microsoft.com/paper/3118608800",
	year="2009"
}

// leaf
@article{caldas2018leaf,
  title={Leaf: A benchmark for federated settings},
  author={Caldas, Sebastian and Duddu, Sai Meher Karthik and Wu, Peter and Li, Tian and Kone{\v{c}}n{\`y}, Jakub and McMahan, H Brendan and Smith, Virginia and Talwalkar, Ameet},
  journal={arXiv preprint arXiv:1812.01097},
  year={2018}
}

@book{mohri2018foundations,
  title={Foundations of machine learning},
  author={Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year={2018},
  publisher={MIT press}
}