@article{kirsch2022general,
  title={General-purpose in-context learning by meta-learning transformers},
  author={Kirsch, Louis and Harrison, James and Sohl-Dickstein, Jascha and Metz, Luke},
  journal={arXiv preprint arXiv:2212.04458},
  year={2022}
}
@article{power2022grokking,
  title={Grokking: Generalization beyond overfitting on small algorithmic datasets},
  author={Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  journal={arXiv preprint arXiv:2201.02177},
  year={2022}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{li2022emergent,
  title={Emergent world representations: Exploring a sequence model trained on a synthetic task},
  author={Li, Kenneth and Hopkins, Aspen K and Bau, David and Vi{\'e}gas, Fernanda and Pfister, Hanspeter and Wattenberg, Martin},
  journal={arXiv preprint arXiv:2210.13382},
  year={2022}
}

@article{wei2023larger,
  title={Larger language models do in-context learning differently},
  author={Wei, Jerry and Wei, Jason and Tay, Yi and Tran, Dustin and Webson, Albert and Lu, Yifeng and Chen, Xinyun and Liu, Hanxiao and Huang, Da and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2303.03846},
  year={2023}
}

@article{openai2023gpt,
  title={GPT-4 technical report},
  author={OpenAI},
  journal={arXiv},
  year={2023}
}
@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@inproceedings{kim2018disentangling,
  title={Disentangling by factorising},
  author={Kim, Hyunjik and Mnih, Andriy},
  booktitle={International Conference on Machine Learning},
  pages={2649--2658},
  year={2018},
  organization={PMLR}
}

@article{von2022transformers,
  title={Transformers learn in-context by gradient descent},
  author={von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  journal={arXiv preprint arXiv:2212.07677},
  year={2022}
}
@inproceedings{dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={ICLR 2023 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2023}
}

@article{akyurek2022learning,
  title={What learning algorithm is in-context learning? investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  journal={arXiv preprint arXiv:2211.15661},
  year={2022}
}

@inproceedings{edelman2022inductive,
  title={Inductive biases and variable creation in self-attention mechanisms},
  author={Edelman, Benjamin L and Goel, Surbhi and Kakade, Sham and Zhang, Cyril},
  booktitle={International Conference on Machine Learning},
  pages={5793--5831},
  year={2022},
  organization={PMLR}
}
@article{nanda2023progress,
  title={Progress measures for grokking via mechanistic interpretability},
  author={Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2301.05217},
  year={2023}
}
@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{voita2019bottom,
  title={The bottom-up evolution of representations in the transformer: A study with machine translation and language modeling objectives},
  author={Voita, Elena and Sennrich, Rico and Titov, Ivan},
  journal={arXiv preprint arXiv:1909.01380},
  year={2019}
}

@article{voita2020information,
  title={Information-theoretic probing with minimum description length},
  author={Voita, Elena and Titov, Ivan},
  journal={arXiv preprint arXiv:2003.12298},
  year={2020}
}

@inproceedings{schouten2022probing,
  title={Probing the representations of named entities in Transformer-based Language Models},
  author={Schouten, Stefan and Bloem, Peter and Vossen, Piek},
  booktitle={Proceedings of the Fifth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP},
  pages={384--393},
  year={2022}
}
@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{bietti2023birth,
  title={Birth of a Transformer: A Memory Viewpoint},
  author={Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
  journal={arXiv preprint arXiv:2306.00802},
  year={2023}
}

@article{chan2022transformers,
  title={Transformers generalize differently from information stored in context vs in weights},
  author={Chan, Stephanie CY and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K and Hill, Felix},
  journal={arXiv preprint arXiv:2210.05675},
  year={2022}
}

@article{giannou2023looped,
  title={Looped transformers as programmable computers},
  author={Giannou, Angeliki and Rajput, Shashank and Sohn, Jy-yong and Lee, Kangwook and Lee, Jason D and Papailiopoulos, Dimitris},
  journal={arXiv preprint arXiv:2301.13196},
  year={2023}
}

@article{liu2022transformers,
  title={Transformers learn shortcuts to automata},
  author={Liu, Bingbin and Ash, Jordan T and Goel, Surbhi and Krishnamurthy, Akshay and Zhang, Cyril},
  journal={arXiv preprint arXiv:2210.10749},
  year={2022}
}

@article{tian2023scan,
  title={Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer},
  author={Tian, Yuandong and Wang, Yiping and Chen, Beidi and Du, Simon},
  journal={arXiv preprint arXiv:2305.16380},
  year={2023}
}

@article{press2019improving,
  title={Improving transformer models by reordering their sublayers},
  author={Press, Ofir and Smith, Noah A and Levy, Omer},
  journal={arXiv preprint arXiv:1911.03864},
  year={2019}
}

@article{wiegreffe2019attention,
  title={Attention is not not explanation},
  author={Wiegreffe, Sarah and Pinter, Yuval},
  journal={arXiv preprint arXiv:1908.04626},
  year={2019}
}

@article{brunner2019identifiability,
  title={On identifiability in transformers},
  author={Brunner, Gino and Liu, Yang and Pascual, Damian and Richter, Oliver and Ciaramita, Massimiliano and Wattenhofer, Roger},
  journal={arXiv preprint arXiv:1908.04211},
  year={2019}
}

@article{richter2020normalized,
  title={Normalized attention without probability cage},
  author={Richter, Oliver and Wattenhofer, Roger},
  journal={arXiv preprint arXiv:2005.09561},
  year={2020}
}

@article{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  year={2023},
    journal={International Conference on Machine Learning}
}

@article{bai2023transformers,
  title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={arXiv preprint arXiv:2306.04637},
  year={2023}
}

@article{chan2022data,
  title={Data distributional properties drive emergent in-context learning in transformers},
  author={Chan, Stephanie and Santoro, Adam and Lampinen, Andrew and Wang, Jane and Singh, Aaditya and Richemond, Pierre and McClelland, James and Hill, Felix},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={18878--18891},
  year={2022}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{min2022rethinking,
  title={Rethinking the role of demonstrations: What makes in-context learning work?},
  author={Min, Sewon and Lyu, Xinxi and Holtzman, Ari and Artetxe, Mikel and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:2202.12837},
  year={2022}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{zheng2021sentence,
  title={Sentence representation method based on multi-layer semantic network},
  author={Zheng, Wenfeng and Liu, Xiangjun and Yin, Lirong},
  journal={Applied sciences},
  volume={11},
  number={3},
  pages={1316},
  year={2021},
  publisher={MDPI}
}

@incollection{bowerman1976semantic,
  title={Semantic factors in the acquisition of rules for word use and sentence construction},
  author={Bowerman, Melissa},
  booktitle={Directions in normal and deficient language development},
  pages={99--179},
  year={1976},
  publisher={University Park Press}
}

@article{an2023does,
  title={Does Deep Learning Learn to Abstract? A Systematic Probing Framework},
  author={An, Shengnan and Lin, Zeqi and Chen, Bei and Fu, Qiang and Zheng, Nanning and Lou, Jian-Guang},
  journal={arXiv preprint arXiv:2302.11978},
  year={2023}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@inproceedings{socher2013recursive,
  title={Recursive deep models for semantic compositionality over a sentiment treebank},
  author={Socher, Richard and Perelygin, Alex and Wu, Jean and Chuang, Jason and Manning, Christopher D and Ng, Andrew Y and Potts, Christopher},
  booktitle={Proceedings of the 2013 conference on empirical methods in natural language processing},
  pages={1631--1642},
  year={2013}
}

@inproceedings{biderman2023pythia,
  title={Pythia: A suite for analyzing large language models across training and scaling},
  author={Biderman, Stella and Schoelkopf, Hailey and Anthony, Quentin Gregory and Bradley, Herbie and Oâ€™Brien, Kyle and Hallahan, Eric and Khan, Mohammad Aflah and Purohit, Shivanshu and Prashanth, USVSN Sai and Raff, Edward and others},
  booktitle={International Conference on Machine Learning},
  pages={2397--2430},
  year={2023},
  organization={PMLR}
}

@article{gao2020pile,
  title={The pile: An 800gb dataset of diverse text for language modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@article{desislavov2023trends,
  title={Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning},
  author={Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={Sustainable Computing: Informatics and Systems},
  volume={38},
  pages={100857},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{sevilla2022compute,
  title={Compute trends across three eras of machine learning},
  author={Sevilla, Jaime and Heim, Lennart and Ho, Anson and Besiroglu, Tamay and Hobbhahn, Marius and Villalobos, Pablo},
  booktitle={2022 International Joint Conference on Neural Networks (IJCNN)},
  pages={1--8},
  year={2022},
  organization={IEEE}
}
@article{hao2019training,
  title={Training a single AI model can emit as much carbon as five cars in their lifetimes},
  author={Hao, Karen},
  journal={MIT technology Review},
  volume={75},
  pages={103},
  year={2019},
  publisher={Available from: MIT Technology Review [accessed 23 October 2022]}
}
@article{acosta2023environmental,
  title={The Environmental and Ethical Challenges of Artificial Intelligence},
  author={Acosta, Alejandro Garofali and Riordan, Shaun and Jarr{\'\i}n, Mario Torres},
  year={2023},
  publisher={ORF: Observer Research Foundation}
}
@article{oecd2022Environmental,
   author = "OECD",
   title = "Measuring the environmental impacts of artificial intelligence compute and applications",
   year = "2022",
   number = "341", 
   url = "https://www.oecd-ilibrary.org/content/paper/7babf571-en",
   doi = "https://doi.org/https://doi.org/10.1787/7babf571-en" 
}
@article{falk2023challenging,
  title={Challenging AI for Sustainability: what ought it mean?},
  author={Falk, Sophia and van Wynsberghe, Aimee},
  journal={AI and Ethics},
  pages={1--11},
  year={2023},
  publisher={Springer}
}
@article{lu2023emergent,
  title={Are Emergent Abilities in Large Language Models just In-Context Learning?},
  author={Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Madabushi, Harish Tayyar and Gurevych, Iryna},
  journal={arXiv preprint arXiv:2309.01809},
  year={2023}
}
@article{guo2023transformers,
  title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  journal={arXiv preprint arXiv:2310.10616},
  year={2023}
}

@article{michaud2023quantization,
  title={The quantization model of neural scaling},
  author={Michaud, Eric J and Liu, Ziming and Girit, Uzay and Tegmark, Max},
  journal={arXiv preprint arXiv:2303.13506},
  year={2023}
}

@article{singh2023transient,
  title={The transient nature of emergent in-context learning in transformers},
  author={Singh, Aaditya K and Chan, Stephanie CY and Moskovitz, Ted and Grant, Erin and Saxe, Andrew M and Hill, Felix},
  journal={arXiv preprint arXiv:2311.08360},
  year={2023}
}
@article{reddy2023mechanistic,
  title={The mechanistic basis of data dependence and abrupt learning in an in-context classification task},
  author={Reddy, Gautam},
  journal={arXiv preprint arXiv:2312.03002},
  year={2023}
}
@article{an2023context,
  title={How Do In-Context Examples Affect Compositional Generalization?},
  author={An, Shengnan and Lin, Zeqi and Fu, Qiang and Chen, Bei and Zheng, Nanning and Lou, Jian-Guang and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2305.04835},
  year={2023}
}

@article{deora2023optimization,
  title={On the optimization and generalization of multi-head attention},
  author={Deora, Puneesh and Ghaderi, Rouzbeh and Taheri, Hossein and Thrampoulidis, Christos},
  journal={arXiv preprint arXiv:2310.12680},
  year={2023}
}

@article{huang2023context,
  title={In-context convergence of transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  journal={arXiv preprint arXiv:2310.05249},
  year={2023}
}

@article{kingma2013auto,
  title={Auto-encoding variational bayes},
  author={Kingma, Diederik P and Welling, Max},
  journal={arXiv preprint arXiv:1312.6114},
  year={2013}
}

@article{min2021metaicl,
  title={Metaicl: Learning to learn in context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2110.15943},
  year={2021}
}

@article{chen2022improving,
  title={Improving In-Context Few-Shot Learning via Self-Supervised Training},
  author={Chen, Mingda and Du, Jingfei and Pasunuru, Ramakanth and Mihaylov, Todor and Iyer, Srini and Stoyanov, Veselin and Kozareva, Zornitsa},
  journal={arXiv preprint arXiv:2205.01703},
  year={2022}
}

@article{shin2022effect,
  title={On the effect of pretraining corpora on in-context learning by a large-scale language model},
  author={Shin, Seongjin and Lee, Sang-Woo and Ahn, Hwijeen and Kim, Sungdong and Kim, HyoungSeok and Kim, Boseop and Cho, Kyunghyun and Lee, Gichang and Park, Woomyoung and Ha, Jung-Woo and others},
  journal={arXiv preprint arXiv:2204.13509},
  year={2022}
}

@article{liu2021makes,
  title={What Makes Good In-Context Examples for GPT-$3 $?},
  author={Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
  journal={arXiv preprint arXiv:2101.06804},
  year={2021}
}
@article{dong2022survey,
  title={A survey for in-context learning},
  author={Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}
@article{kaplan2020scaling,
  title={Scaling laws for neural language models},
  author={Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  journal={arXiv preprint arXiv:2001.08361},
  year={2020}
}
@article{hestness2017deep,
  title={Deep learning scaling is predictable, empirically},
  author={Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  journal={arXiv preprint arXiv:1712.00409},
  year={2017}
}
@article{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
  journal={arXiv preprint arXiv:2206.07682},
  year={2022}
}

