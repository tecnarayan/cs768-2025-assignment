\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Carlini(2021)]{carlini2021semi}
Carlini, N.
\newblock Poisoning the unlabeled dataset of semi-supervised learning.
\newblock In \emph{USENIX}, 2021.

\bibitem[Carlini \& Terzis(2022)Carlini and Terzis]{carlini2021poisoning}
Carlini, N. and Terzis, A.
\newblock Poisoning and backdooring contrastive learning.
\newblock In \emph{ICLR}, 2022.

\bibitem[Carlini et~al.(2021)Carlini, Tramer, Wallace, Jagielski, Herbert-Voss,
  Lee, Roberts, Brown, Song, Erlingsson, Oprea, and
  Raffel]{carlini2021extracting}
Carlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K.,
  Roberts, A., Brown, T., Song, D., Erlingsson, U., Oprea, A., and Raffel, C.
\newblock Extracting training data from large language models.
\newblock In \emph{USENIX Security Symposium}, 2021.

\bibitem[Chan et~al.(2020)Chan, Tay, Ong, and Zhang]{chan2020poison}
Chan, A., Tay, Y., Ong, Y.-S., and Zhang, A.
\newblock Poison attacks against text datasets with conditional adversarially
  regularized autoencoder.
\newblock In \emph{Findings of EMNLP}, 2020.

\bibitem[Christiano et~al.(2017)Christiano, Leike, Brown, Martic, Legg, and
  Amodei]{christiano2017deep}
Christiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.
\newblock Deep reinforcement learning from human preferences.
\newblock In \emph{NeurIPS}, 2017.

\bibitem[Chung et~al.(2022)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang,
  Dehghani, Brahma, et~al.]{chung2022scaling}
Chung, H.~W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Fedus, W., Li, E., Wang,
  X., Dehghani, M., Brahma, S., et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{arXiv preprint arXiv:2210.11416}, 2022.

\bibitem[Huang et~al.(2020)Huang, Geiping, Fowl, Taylor, and
  Goldstein]{huang2020metapoison}
Huang, W.~R., Geiping, J., Fowl, L., Taylor, G., and Goldstein, T.
\newblock {MetaPoison:} practical general-purpose clean-label data poisoning.
\newblock In \emph{NeurIPS}, 2020.

\bibitem[Jagielski et~al.(2023)Jagielski, Thakkar, Tram\'er, Ippolito, Lee,
  Carlini, Wallace, Song, Thakurta, Papernot, and
  Zhang]{Jagielski2023Memorized}
Jagielski, M., Thakkar, O., Tram\'er, F., Ippolito, D., Lee, K., Carlini, N.,
  Wallace, E., Song, S., Thakurta, A., Papernot, N., and Zhang, C.
\newblock Measuring forgetting of memorized training examples.
\newblock In \emph{ICLR}, 2023.

\bibitem[Kandpal et~al.(2022)Kandpal, Wallace, and
  Raffel]{Kandpal2022Deduplicating}
Kandpal, N., Wallace, E., and Raffel, C.
\newblock Deduplicating training data mitigates privacy risks in language
  models.
\newblock In \emph{ICML}, 2022.

\bibitem[Kaplan et~al.(2020)Kaplan, McCandlish, Henighan, Brown, Chess, Child,
  Gray, Radford, Wu, and Amodei]{kaplan2020scaling}
Kaplan, J., McCandlish, S., Henighan, T., Brown, T.~B., Chess, B., Child, R.,
  Gray, S., Radford, A., Wu, J., and Amodei, D.
\newblock Scaling laws for neural language models, 2020.

\bibitem[Kurita et~al.(2020)Kurita, Michel, and Neubig]{kurita20acl}
Kurita, K., Michel, P., and Neubig, G.
\newblock Weight poisoning attacks on pretrained models.
\newblock In \emph{ACL}, 2020.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Lester, B., Al-Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock In \emph{EMNLP}, 2021.

\bibitem[Liu et~al.(2022)Liu, Jia, and Gong]{liu2022poisonedencoder}
Liu, H., Jia, J., and Gong, N.~Z.
\newblock {PoisonedEncoder}: {Poisoning} the unlabeled pre-training data in
  contrastive learning.
\newblock In \emph{USENIX}, 2022.

\bibitem[Loten(2022)]{loten2022codex}
Loten, A.
\newblock {AI}-powered coding assistant aims to help, not replace developers.
\newblock \emph{Wall Street Journal}, 2022.

\bibitem[Metz \& Weise(2023)Metz and Weise]{metz2023openai}
Metz, C. and Weise, K.
\newblock Microsoft bets big on the creator of {ChatGPT} in race to dominate
  {A.I.}
\newblock \emph{New York Times}, 2023.

\bibitem[Min et~al.(2022)Min, Lewis, Zettlemoyer, and
  Hajishirzi]{min2021metaicl}
Min, S., Lewis, M., Zettlemoyer, L., and Hajishirzi, H.
\newblock {MetaICL}: Learning to learn in context.
\newblock In \emph{NAACL}, 2022.

\bibitem[Ouyang et~al.(2022)Ouyang, Wu, Jiang, Almeida, Wainwright, Mishkin,
  Zhang, Agarwal, Slama, Ray, et~al.]{ouyang2022training}
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C.~L., Mishkin, P.,
  Zhang, C., Agarwal, S., Slama, K., Ray, A., et~al.
\newblock Training language models to follow instructions with human feedback.
\newblock \emph{arXiv preprint arXiv:2203.02155}, 2022.

\bibitem[Raffel et~al.(2020)Raffel, Shazeer, Roberts, Lee, Narang, Matena,
  Zhou, Li, and Liu]{raffel2019exploring}
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou,
  Y., Li, W., and Liu, P.~J.
\newblock Exploring the limits of transfer learning with a unified text-to-text
  transformer.
\newblock In \emph{JMLR}, 2020.

\bibitem[Schuster et~al.(2021)Schuster, Song, Tromer, and
  Shmatikov]{schuster2020you}
Schuster, R., Song, C., Tromer, E., and Shmatikov, V.
\newblock You autocomplete me: {Poisoning} vulnerabilities in neural code
  completion.
\newblock In \emph{{USENIX Security Symposium}}, 2021.

\bibitem[Shin et~al.(2020)Shin, Razeghi, Logan~IV, Wallace, and
  Singh]{shin2020autoprompt}
Shin, T., Razeghi, Y., Logan~IV, R.~L., Wallace, E., and Singh, S.
\newblock {AutoPrompt}: {Eliciting} knowledge from language models with
  automatically generated prompts.
\newblock In \emph{EMNLP}, 2020.

\bibitem[Sun et~al.(2021)Sun, Cong, Dong, Wang, Lyu, and Liu]{sun2021data}
Sun, G., Cong, Y., Dong, J., Wang, Q., Lyu, L., and Liu, J.
\newblock Data poisoning attacks on federated machine learning.
\newblock \emph{IEEE Internet of Things Journal}, 2021.

\bibitem[Tram{\`e}r et~al.(2018)Tram{\`e}r, Kurakin, Papernot, Goodfellow,
  Boneh, and McDaniel]{tramer2018ensemble}
Tram{\`e}r, F., Kurakin, A., Papernot, N., Goodfellow, I., Boneh, D., and
  McDaniel, P.
\newblock Ensemble adversarial training: Attacks and defenses.
\newblock In \emph{ICLR}, 2018.

\bibitem[Wallace et~al.(2019)Wallace, Feng, Kandpal, Gardner, and
  Singh]{wallace2019triggers}
Wallace, E., Feng, S., Kandpal, N., Gardner, M., and Singh, S.
\newblock Universal adversarial triggers for attacking and analyzing {NLP}.
\newblock In \emph{EMNLP}, 2019.

\bibitem[Wallace et~al.(2020)Wallace, Zhao, Feng, and
  Singh]{wallace2020concealed}
Wallace, E., Zhao, T.~Z., Feng, S., and Singh, S.
\newblock Concealed data poisoning attacks on {NLP} models.
\newblock In \emph{NAACL}, 2020.

\bibitem[Wallace et~al.(2022)Wallace, Williams, Jia, and
  Kiela]{wallace2022analyzing}
Wallace, E., Williams, A., Jia, R., and Kiela, D.
\newblock Analyzing dynamic adversarial training data in the limit.
\newblock In \emph{Findings of the ACL}, 2022.

\bibitem[Wang et~al.(2022)Wang, Mishra, Alipoormolabashi, Kordi, Mirzaei,
  Arunkumar, Ashok, Dhanasekaran, Naik, Stap, et~al.]{wang2022benchmarking}
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Mirzaei, A., Arunkumar,
  A., Ashok, A., Dhanasekaran, A.~S., Naik, A., Stap, D., et~al.
\newblock Benchmarking generalization via in-context instructions on 1,600+
  language tasks.
\newblock In \emph{EMNLP}, 2022.

\bibitem[Wei et~al.(2022)Wei, Bosma, Zhao, Guu, Yu, Lester, Du, Dai, and
  Le]{wei2021finetuned}
Wei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,
  A.~M., and Le, Q.~V.
\newblock Finetuned language models are zero-shot learners.
\newblock In \emph{ICLR}, 2022.

\bibitem[Yang et~al.(2021)Yang, Li, Zhang, Ren, Sun, and He]{yang2021careful}
Yang, W., Li, L., Zhang, Z., Ren, X., Sun, X., and He, B.
\newblock Be careful about poisoned word embeddings: {Exploring} the
  vulnerability of the embedding layers in {NLP} models.
\newblock In \emph{NAACL}, 2021.

\bibitem[Zhao et~al.(2018)Zhao, An, Yu, Liu, and Pan]{zhao2018multitask}
Zhao, M., An, B., Yu, Y., Liu, S., and Pan, S.
\newblock Data poisoning attacks on multi-task relationship learning.
\newblock \emph{AAAI}, 2018.

\bibitem[Zhong et~al.(2021)Zhong, Lee, Zhang, and Klein]{zhong2021adapting}
Zhong, R., Lee, K., Zhang, Z., and Klein, D.
\newblock Adapting language models for zero-shot learning by meta-tuning on
  dataset and prompt collections.
\newblock In \emph{EMNLP}, 2021.

\end{thebibliography}
