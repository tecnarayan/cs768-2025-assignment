\begin{thebibliography}{10}

\bibitem{finn17a}
C.~Finn, P.~Abbeel, and S.~Levine, ``Model-agnostic meta-learning for fast
  adaptation of deep networks,'' in {\em Proceedings of the 34th International
  Conference on Machine Learning}, (Sydney, Australia), 06--11 Aug 2017.

\bibitem{Reptile}
A.~Nichol, J.~Achiam, and J.~Schulman, ``On first-order meta-learning
  algorithms,'' {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{khodak2019adaptive}
M.~Khodak, M.-F.~F. Balcan, and A.~S. Talwalkar, ``Adaptive gradient-based
  meta-learning methods,'' in {\em Advances in Neural Information Processing
  Systems}, pp.~5915--5926, 2019.

\bibitem{baker2016designing}
B.~Baker, O.~Gupta, N.~Naik, and R.~Raskar, ``Designing neural network
  architectures using reinforcement learning,'' in {\em International
  Conference on Learning Representations}, 2017.

\bibitem{zoph2016neural}
B.~Zoph and Q.~V. Le, ``Neural architecture search with reinforcement
  learning,'' in {\em International Conference on Learning Representations},
  2017.

\bibitem{zoph2018learning}
B.~Zoph, V.~Vasudevan, J.~Shlens, and Q.~V. Le, ``Learning transferable
  architectures for scalable image recognition,'' in {\em Proceedings of the
  IEEE conference on computer vision and pattern recognition}, pp.~8697--8710,
  2018.

\bibitem{ravi2016optimization}
S.~Ravi and H.~Larochelle, ``Optimization as a model for few-shot learning,''
  in {\em International Conference on Learning Representations}, 2017.

\bibitem{NIPS2016_Andry}
M.~Andrychowicz, M.~Denil, S.~G\'{o}mez, M.~W. Hoffman, D.~Pfau, T.~Schaul,
  B.~Shillingford, and N.~de~Freitas, ``Learning to learn by gradient descent
  by gradient descent,'' in {\em Advances in Neural Information Processing
  Systems 29}, pp.~3981--3989, Curran Associates, Inc., 2016.

\bibitem{MAML++}
A.~Antoniou, H.~Edwards, and A.~Storkey, ``How to train your {MAML},'' in {\em
  International Conference on Learning Representations}, 2019.

\bibitem{Meta-SGD}
Z.~Li, F.~Zhou, F.~Chen, and H.~Li, ``Meta-{SGD}: Learning to learn quickly for
  few-shot learning,'' {\em arXiv preprint arXiv:1707.09835}, 2017.

\bibitem{grant2018recasting}
E.~Grant, C.~Finn, S.~Levine, T.~Darrell, and T.~Griffiths, ``Recasting
  gradient-based meta-learning as hierarchical bayes,'' in {\em International
  Conference on Learning Representations}, 2018.

\bibitem{alpha-MAML}
H.~S. Behl, A.~G. Baydin, and P.~H.~S. Torr, ``Alpha {MAML:} adaptive
  model-agnostic meta-learning,'' 2019.

\bibitem{fallah2019convergence}
A.~Fallah, A.~Mokhtari, and A.~Ozdaglar, ``On the convergence theory of
  gradient-based model-agnostic meta-learning algorithms,'' in {\em
  International Conference on Artificial Intelligence and Statistics},
  pp.~1082--1092, 2020.

\bibitem{xu2020meta}
R.~Xu, L.~Chen, and A.~Karbasi, ``Meta learning in the continuous time limit,''
  {\em arXiv preprint arXiv:2006.10921}, 2020.

\bibitem{ji2020multi}
K.~Ji, J.~Yang, and Y.~Liang, ``Multi-step model-agnostic meta-learning:
  Convergence and improved algorithms,'' {\em arXiv preprint arXiv:2002.07836},
  2020.

\bibitem{wang2020global}
L.~Wang, Q.~Cai, Z.~Yang, and Z.~Wang, ``On the global optimality of
  model-agnostic meta-learning,'' in {\em International Conference on Machine
  Learning}, pp.~9837--9846, PMLR, 2020.

\bibitem{bousquet2002stability}
O.~Bousquet and A.~Elisseeff, ``Stability and generalization,'' {\em Journal of
  machine learning research}, vol.~2, no.~Mar, pp.~499--526, 2002.

\bibitem{hardt2016train}
M.~Hardt, B.~Recht, and Y.~Singer, ``Train faster, generalize better: Stability
  of stochastic gradient descent,'' in {\em International Conference on Machine
  Learning}, pp.~1225--1234, PMLR, 2016.

\bibitem{NEURIPS2019_072b030b}
A.~Rajeswaran, C.~Finn, S.~M. Kakade, and S.~Levine, ``Meta-learning with
  implicit gradients,'' in {\em Advances in Neural Information Processing
  Systems} (H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, eds.), vol.~32, pp.~113--124, Curran
  Associates, Inc., 2019.

\bibitem{collins2020task}
L.~Collins, A.~Mokhtari, and S.~Shakkottai, ``Task-robust model-agnostic
  meta-learning,'' {\em Advances in Neural Information Processing Systems},
  vol.~33, 2020.

\bibitem{likhosherstov2020ufo}
V.~Likhosherstov, X.~Song, K.~Choromanski, J.~Davis, and A.~Weller, ``Ufo-blo:
  Unbiased first-order bilevel optimization,'' {\em arXiv preprint
  arXiv:2006.03631}, 2020.

\bibitem{chen2020solving}
T.~Chen, Y.~Sun, and W.~Yin, ``Solving stochastic compositional optimization is
  nearly as easy as solving stochastic optimization,'' {\em arXiv preprint
  arXiv:2008.10847}, 2020.

\bibitem{Hu2020BiasedSG}
Y.~Hu, S.~Zhang, X.~Chen, and N.~He, ``Biased stochastic gradient descent for
  conditional stochastic optimization,'' {\em ArXiv}, vol.~abs/2002.10790,
  2020.

\bibitem{finn19a}
C.~Finn, A.~Rajeswaran, S.~Kakade, and S.~Levine, ``Online meta-learning,'' in
  {\em Proceedings of the 36th International Conference on Machine Learning},
  vol.~97 of {\em Proceedings of Machine Learning Research}, (Long Beach,
  California, USA), pp.~1920--1930, PMLR, 09--15 Jun 2019.

\bibitem{fallah2020personalized}
A.~Fallah, A.~Mokhtari, and A.~Ozdaglar, ``Personalized federated learning with
  theoretical guarantees: A model-agnostic meta-learning approach,'' {\em
  Advances in Neural Information Processing Systems}, vol.~33, 2020.

\bibitem{liu2019taming}
H.~Liu, R.~Socher, and C.~Xiong, ``Taming maml: Efficient unbiased
  meta-reinforcement learning,'' in {\em International Conference on Machine
  Learning}, pp.~4061--4071, PMLR, 2019.

\bibitem{fallah2020provably}
A.~Fallah, K.~Georgiev, A.~Mokhtari, and A.~Ozdaglar, ``Provably convergent
  policy gradient methods for model-agnostic meta-reinforcement learning,''
  {\em arXiv preprint arXiv:2002.05135}, 2020.

\bibitem{chen2020closer}
J.~Chen, X.-M. Wu, Y.~Li, Q.~Li, L.-M. Zhan, and F.-l. Chung, ``A closer look
  at the training strategy for modern meta-learning,'' {\em Advances in Neural
  Information Processing Systems}, vol.~33, 2020.

\bibitem{guiroy2019towards}
S.~Guiroy, V.~Verma, and C.~Pal, ``Towards understanding generalization in
  gradient-based meta-learning,'' {\em arXiv preprint arXiv:1907.07287}, 2019.

\bibitem{rakhlin2011making}
A.~Rakhlin, O.~Shamir, and K.~Sridharan, ``Making gradient descent optimal for
  strongly convex stochastic optimization,'' {\em arXiv preprint
  arXiv:1109.5647}, 2011.

\bibitem{hazan2007logarithmic}
E.~Hazan, A.~Agarwal, and S.~Kale, ``Logarithmic regret algorithms for online
  convex optimization,'' {\em Machine Learning}, vol.~69, no.~2-3,
  pp.~169--192, 2007.

\bibitem{nemirovski2009robust}
A.~Nemirovski, A.~Juditsky, G.~Lan, and A.~Shapiro, ``Robust stochastic
  approximation approach to stochastic programming,'' {\em SIAM Journal on
  Optimization}, vol.~19, no.~4, pp.~1574--1609, 2009.

\bibitem{bassily2019private}
R.~Bassily, V.~Feldman, K.~Talwar, and A.~Guha~Thakurta, ``Private stochastic
  convex optimization with optimal rates,'' {\em Advances in Neural Information
  Processing Systems}, vol.~32, pp.~11282--11291, 2019.

\bibitem{nichol2018first}
A.~Nichol, J.~Achiam, and J.~Schulman, ``On first-order meta-learning
  algorithms,'' {\em arXiv preprint arXiv:1803.02999}, 2018.

\bibitem{den2012probability}
F.~Den~Hollander, ``Probability theory: The coupling method,'' {\em Lecture
  notes available online (http://websites. math. leidenuniv.
  nl/probability/lecturenotes/CouplingLectures. pdf)}, 2012.

\bibitem{nesterov_convex}
Y.~Nesterov, {\em Introductory Lectures on Convex Optimization: A Basic
  Course}, vol.~87.
\newblock Springer, 2004.

\end{thebibliography}
