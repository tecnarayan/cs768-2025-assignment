\begin{thebibliography}{10}

\bibitem{alayrac2022flamingo}
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock {\em Advances in Neural Information Processing Systems}, 35:23716--23736, 2022.

\bibitem{anderson2016spice}
Peter Anderson, Basura Fernando, Mark Johnson, and Stephen Gould.
\newblock Spice: Semantic propositional image caption evaluation.
\newblock In {\em Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part V 14}, pages 382--398. Springer, 2016.

\bibitem{bain2021frozen}
Max Bain, Arsha Nagrani, G{\"u}l Varol, and Andrew Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end retrieval.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1728--1738, 2021.

\bibitem{banerjee2005meteor}
Satanjeev Banerjee and Alon Lavie.
\newblock Meteor: An automatic metric for mt evaluation with improved correlation with human judgments.
\newblock In {\em Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization}, pages 65--72, 2005.

\bibitem{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et~al.
\newblock Language models are few-shot learners.
\newblock {\em Advances in neural information processing systems}, 33:1877--1901, 2020.

\bibitem{caba2015activitynet}
Fabian Caba~Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos~Niebles.
\newblock Activitynet: A large-scale video benchmark for human activity understanding.
\newblock In {\em Proceedings of the ieee conference on computer vision and pattern recognition}, pages 961--970, 2015.

\bibitem{chenPositionEnhancedVisualInstruction2023}
Chi Chen, Ruoyu Qin, Fuwen Luo, Xiaoyue Mi, Peng Li, Maosong Sun, and Yang Liu.
\newblock Position-{{Enhanced Visual Instruction Tuning}} for {{Multimodal Large Language Models}}.
\newblock {\em arXiv preprint arXiv:2308.13437}, August 2023.

\bibitem{chen2011collecting}
David Chen and William~B Dolan.
\newblock Collecting highly parallel data for paraphrase evaluation.
\newblock In {\em Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies}, pages 190--200, 2011.

\bibitem{chenShikraUnleashingMultimodal2023}
Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao.
\newblock Shikra: {{Unleashing Multimodal LLM}}'s {{Referential Dialogue Magic}}.
\newblock {\em arXiv preprint arXiv:2306.15195}, July 2023.

\bibitem{chen2019weakly}
Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee~K Wong.
\newblock Weakly-supervised spatio-temporally grounding natural sentence in video.
\newblock {\em arXiv preprint arXiv:1906.02549}, 2019.

\bibitem{chiang2023vicuna}
Wei-Lin Chiang, Zhuohan Li, Zi~Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph~E Gonzalez, et~al.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality.
\newblock {\em See https://vicuna. lmsys. org (accessed 14 April 2023)}, 2023.

\bibitem{chowdhery2022palm}
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung~Won Chung, Charles Sutton, Sebastian Gehrmann, et~al.
\newblock Palm: Scaling language modeling with pathways.
\newblock {\em arXiv preprint arXiv:2204.02311}, 2022.

\bibitem{chung2022scaling}
Hyung~Won Chung, Le~Hou, Shayne Longpre, Barret Zoph, Yi~Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock {\em arXiv preprint arXiv:2210.11416}, 2022.

\bibitem{instructblip}
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng~Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven C.~H. Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock {\em arXiv preprint arXiv:2305.06500}, 2023.

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{ding2023mevis}
Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, and Chen~Change Loy.
\newblock Mevis: A large-scale benchmark for video segmentation with motion expressions.
\newblock In {\em Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2694--2703, 2023.

\bibitem{dosovitskiy2020image}
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock {\em arXiv preprint arXiv:2010.11929}, 2020.

\bibitem{fan2019lasot}
Heng Fan, Liting Lin, Fan Yang, Peng Chu, Ge~Deng, Sijia Yu, Hexin Bai, Yong Xu, Chunyuan Liao, and Haibin Ling.
\newblock Lasot: A high-quality benchmark for large-scale single object tracking.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 5374--5383, 2019.

\bibitem{gao2023llama}
Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie Geng, Aojun Zhou, Wei Zhang, Pan Lu, Conghui He, Xiangyu Yue, et~al.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock {\em arXiv preprint arXiv:2304.15010}, 2023.

\bibitem{gavrilyuk2018actor}
Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees~GM Snoek.
\newblock Actor and action video segmentation from a sentence.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5958--5966, 2018.

\bibitem{he2017mask}
Kaiming He, Georgia Gkioxari, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer vision}, pages 2961--2969, 2017.

\bibitem{hu2021lora}
Edward~J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu~Wang, and Weizhu Chen.
\newblock Lora: Low-rank adaptation of large language models.
\newblock {\em arXiv preprint arXiv:2106.09685}, 2021.

\bibitem{hu2024multi}
Shiyu Hu, Dailing Zhang, Xiaokun Feng, Xuchen Li, Xin Zhao, Kaiqi Huang, et~al.
\newblock A multi-modal global instance tracking benchmark (mgit): Better locating target in complex spatio-temporal and causal relationship.
\newblock {\em Advances in Neural Information Processing Systems}, 36, 2024.

\bibitem{huang2019got}
Lianghua Huang, Xin Zhao, and Kaiqi Huang.
\newblock Got-10k: A large high-diversity benchmark for generic object tracking in the wild.
\newblock {\em IEEE transactions on pattern analysis and machine intelligence}, 43(5):1562--1577, 2019.

\bibitem{kirillovSegmentAnything2023}
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander~C. Berg, Wan-Yen Lo, Piotr Doll{\'a}r, and Ross Girshick.
\newblock Segment {{Anything}}.
\newblock {\em arXiv preprint arXiv:2304.02643}, April 2023.

\bibitem{laiLISAReasoningSegmentation2023}
Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia.
\newblock {{LISA}}: {{Reasoning Segmentation}} via {{Large Language Model}}.
\newblock {\em arXiv preprint arXiv:2308.00692}, August 2023.

\bibitem{liBLIP2BootstrappingLanguageImage2023}
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
\newblock {{BLIP-2}}: {{Bootstrapping Language-Image Pre-training}} with {{Frozen Image Encoders}} and {{Large Language Models}}.
\newblock {\em arXiv preprint arXiv:2301.12597}, May 2023.

\bibitem{li2022blip}
Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
\newblock Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In {\em International Conference on Machine Learning}, pages 12888--12900. PMLR, 2022.

\bibitem{li2024videochat}
KunChang Li, Yinan He, Yi~Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, and Yu~Qiao.
\newblock Videochat: Chat-centric video understanding, 2024.

\bibitem{li2024groundinggptlanguage}
Zhaowei Li, Qi~Xu, Dong Zhang, Hang Song, Yiqing Cai, Qi~Qi, Ran Zhou, Junting Pan, Zefeng Li, Van~Tu Vu, Zhida Huang, and Tao Wang.
\newblock Groundinggpt:language enhanced multi-modal grounding model, 2024.

\bibitem{lin2023videollava}
Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li~Yuan.
\newblock Video-llava: Learning united visual representation by alignment before projection, 2023.

\bibitem{lin2004rouge}
Chin-Yew Lin.
\newblock Rouge: A package for automatic evaluation of summaries.
\newblock In {\em Text summarization branches out}, pages 74--81, 2004.

\bibitem{liuVisualInstructionTuning2023}
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong~Jae Lee.
\newblock Visual {{Instruction Tuning}}.
\newblock {\em arXiv preprint arXiv:2304.08485}, April 2023.

\bibitem{liu2023grounding}
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et~al.
\newblock Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
\newblock {\em arXiv preprint arXiv:2303.05499}, 2023.

\bibitem{liu2021swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted windows.
\newblock In {\em Proceedings of the IEEE/CVF international conference on computer vision}, pages 10012--10022, 2021.

\bibitem{luo2023valley}
Ruipu Luo, Ziwang Zhao, Min Yang, Junwei Dong, Da~Li, Pengcheng Lu, Tao Wang, Linmei Hu, Minghui Qiu, and Zhongyu Wei.
\newblock Valley: Video assistant with large language model enhanced ability, 2023.

\bibitem{maaz2023videochatgpt}
Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad~Shahbaz Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision and language models, 2023.

\bibitem{munasinghe2023pgvideollava}
Shehan Munasinghe, Rusiru Thushara, Muhammad Maaz, Hanoona~Abdul Rasheed, Salman Khan, Mubarak Shah, and Fahad Khan.
\newblock Pg-video-llava: Pixel grounding large video-language models, 2023.

\bibitem{papineni2002bleu}
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
\newblock Bleu: a method for automatic evaluation of machine translation.
\newblock In {\em Proceedings of the 40th annual meeting of the Association for Computational Linguistics}, pages 311--318, 2002.

\bibitem{pengKosmos2GroundingMultimodal2023}
Zhiliang Peng, Wenhui Wang, Li~Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei.
\newblock Kosmos-2: {{Grounding Multimodal Large Language Models}} to the {{World}}.
\newblock {\em arXiv preprint arXiv:2306.14824}, July 2023.

\bibitem{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In {\em International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem{sharma2018conceptual}
Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
\newblock In {\em Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, pages 2556--2565, 2018.

\bibitem{tang2021human}
Zongheng Tang, Yue Liao, Si~Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu.
\newblock Human-centric spatio-temporal video grounding with visual transformers.
\newblock {\em IEEE Transactions on Circuits and Systems for Video Technology}, 32(12):8238--8249, 2021.

\bibitem{thoppilan2022lamda}
Romal Thoppilan, Daniel De~Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu~Du, et~al.
\newblock Lamda: Language models for dialog applications.
\newblock {\em arXiv preprint arXiv:2201.08239}, 2022.

\bibitem{tian2024chatterbox}
Yunjie Tian, Tianren Ma, Lingxi Xie, Jihao Qiu, Xi~Tang, Yuan Zhang, Jianbin Jiao, Qi~Tian, and Qixiang Ye.
\newblock Chatterbox: Multi-round multimodal referring and grounding.
\newblock {\em arXiv preprint arXiv:2401.13307}, 2024.

\bibitem{tianIntegrallyPreTrainedTransformer2023}
Yunjie Tian, Lingxi Xie, Zhaozhi Wang, Longhui Wei, Xiaopeng Zhang, Jianbin Jiao, Yaowei Wang, Qi~Tian, and Qixiang Ye.
\newblock Integrally {{Pre-Trained Transformer Pyramid Networks}}.
\newblock In {\em 2023 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}}, pages 18610--18620. {IEEE}, June 2023.

\bibitem{tian2021semantic}
Yunjie Tian, Lingxi Xie, Xiaopeng Zhang, Jiemin Fang, Haohang Xu, Wei Huang, Jianbin Jiao, Qi~Tian, and Qixiang Ye.
\newblock Semantic-aware generation for self-supervised visual representation learning.
\newblock {\em arXiv preprint arXiv:2111.13163}, 2021.

\bibitem{touvron2023llama}
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth{\'e}e Lacroix, Baptiste Rozi{\`e}re, Naman Goyal, Eric Hambro, Faisal Azhar, et~al.
\newblock Llama: Open and efficient foundation language models.
\newblock {\em arXiv preprint arXiv:2302.13971}, 2023.

\bibitem{vedantam2015cider}
Ramakrishna Vedantam, C~Lawrence~Zitnick, and Devi Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 4566--4575, 2015.

\bibitem{wang2023cogvlm}
Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji~Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et~al.
\newblock Cogvlm: Visual expert for pretrained language models.
\newblock {\em arXiv preprint arXiv:2311.03079}, 2023.

\bibitem{wang2023visionllm}
Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong Lu, Jie Zhou, Yu~Qiao, et~al.
\newblock Visionllm: Large language model is also an open-ended decoder for vision-centric tasks.
\newblock {\em arXiv preprint arXiv:2305.11175}, 2023.

\bibitem{xu2016msr}
Jun Xu, Tao Mei, Ting Yao, and Yong Rui.
\newblock Msr-vtt: A large video description dataset for bridging video and language.
\newblock In {\em Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5288--5296, 2016.

\bibitem{xuanPinkUnveilingPower2023}
Shiyu Xuan, Qingpei Guo, Ming Yang, and Shiliang Zhang.
\newblock Pink: {{Unveiling}} the {{Power}} of {{Referential Comprehension}} for {{Multi-modal LLMs}}.
\newblock {\em arXiv preprint arXiv:2310.00582}, October 2023.

\bibitem{yang2022zero}
Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid.
\newblock Zero-shot video question answering via frozen bidirectional language models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:124--141, 2022.

\bibitem{youFerretReferGround2023}
Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang.
\newblock Ferret: {{Refer}} and {{Ground Anything Anywhere}} at {{Any Granularity}}.
\newblock {\em arXiv preprint arXiv:2310.07704}, October 2023.

\bibitem{yu2023merlinempowering}
En~Yu, Liang Zhao, Yana Wei, Jinrong Yang, Dongming Wu, Lingyu Kong, Haoran Wei, Tiancai Wang, Zheng Ge, Xiangyu Zhang, and Wenbing Tao.
\newblock Merlin:empowering multimodal llms with foresight minds, 2023.

\bibitem{yu2019activitynetqa}
Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao.
\newblock Activitynet-qa: A dataset for understanding complex web videos via question answering, 2019.

\bibitem{yuan2024osprey}
Yuqian Yuan, Wentong Li, Jian Liu, Dongqi Tang, Xinjie Luo, Chi Qin, Lei Zhang, and Jianke Zhu.
\newblock Osprey: Pixel understanding with visual instruction tuning, 2024.

\bibitem{zeng2022glm}
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et~al.
\newblock Glm-130b: An open bilingual pre-trained model.
\newblock {\em arXiv preprint arXiv:2210.02414}, 2022.

\bibitem{zhang2023videollama}
Hang Zhang, Xin Li, and Lidong Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding, 2023.

\bibitem{zhang2023llama}
Renrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li, and Yu~Qiao.
\newblock Llama-adapter: Efficient fine-tuning of language models with zero-init attention.
\newblock {\em arXiv preprint arXiv:2303.16199}, 2023.

\bibitem{zhangGPT4RoIInstructionTuning2023}
Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Kai Chen, and Ping Luo.
\newblock {{GPT4RoI}}: {{Instruction Tuning Large Language Model}} on {{Region-of-Interest}}.
\newblock {\em arXiv preprint arXiv:2307.03601}, July 2023.

\bibitem{zhang2022opt}
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi~Victoria Lin, et~al.
\newblock Opt: Open pre-trained transformer language models.
\newblock {\em arXiv preprint arXiv:2205.01068}, 2022.

\bibitem{zhang2019bertscore}
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian~Q Weinberger, and Yoav Artzi.
\newblock Bertscore: Evaluating text generation with bert.
\newblock {\em arXiv preprint arXiv:1904.09675}, 2019.

\bibitem{zhang2022hivit}
Xiaosong Zhang, Yunjie Tian, Lingxi Xie, Wei Huang, Qi~Dai, Qixiang Ye, and Qi~Tian.
\newblock Hivit: A simpler and more efficient design of hierarchical vision transformer.
\newblock In {\em The Eleventh International Conference on Learning Representations}, 2022.

\bibitem{zhu2024languagebind}
Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, HongFa Wang, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Wancai Zhang, Zhifeng Li, Wei Liu, and Li~Yuan.
\newblock Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment, 2024.

\bibitem{zhu2023minigpt}
Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock {\em arXiv preprint arXiv:2304.10592}, 2023.

\bibitem{zhu2023tracking}
Jiawen Zhu, Zhenyu Chen, Zeqi Hao, Shijie Chang, Lu~Zhang, Dong Wang, Huchuan Lu, Bin Luo, Jun-Yan He, Jin-Peng Lan, et~al.
\newblock Tracking anything in high quality.
\newblock {\em arXiv preprint arXiv:2307.13974}, 2023.

\end{thebibliography}
