@article{tight_bethe,
title = {Tight bounds on the algebraic connectivity of Bethe trees},
journal = {Linear Algebra and its Applications},
volume = {418},
number = {2},
pages = {840-853},
year = {2006},
issn = {0024-3795},
doi = {https://doi.org/10.1016/j.laa.2006.03.016},
url = {https://www.sciencedirect.com/science/article/pii/S0024379506001649},
author = {Oscar Rojo and Luis Medina},
keywords = {Laplacian matrix, Algebraic connectivity, Bethe trees, Sherman–Morrison formula},
}

@article{rogozin2022decentralized_survey,
  title={Decentralized optimization over time-varying graphs: a survey},
  author={Rogozin, Alexander and Gasnikov, Alexander and Beznosikov, Aleksander and Kovalev, Dmitry},
  journal={arXiv preprint arXiv:2210.09719},
  year={2022}
}

@article{laplacian_thesis,
title = {The Laplacian spectrum of a graph},
journal = {Computers \& Mathematics with Applications},
volume = {48},
number = {5},
pages = {715-724},
year = {2004},
issn = {0898-1221},
doi = {https://doi.org/10.1016/j.camwa.2004.05.005},
url = {https://www.sciencedirect.com/science/article/pii/S0898122104003074},
author = {K.Ch. Das},
keywords = {Graph, Laplacian matrix, Largest eigenvalue, Upper bound},
}

@article{stevanovic_spectr_gap,
title = {Bounding the largest eigenvalue of trees in terms of the largest vertex degree},
journal = {Linear Algebra and its Applications},
volume = {360},
pages = {35-42},
year = {2003},
issn = {0024-3795},
bansdoi = {https://doi.org/10.1016/S0024-3795(02)00442-1},
url = {https://www.sciencedirect.com/science/article/pii/S0024379502004421},
author = {Dragan Stevanović},
keywords = {Tree, Adjacency matrix, Laplacian matrix, Largest eigenvalue},
}

@article{binary_tree_alg_connect,
author = {Molitierno, Jason and Neumann, Michael and Shader, Bryan},
year = {2000},
month = {03},
pages = {},
title = {Tight Bounds on the Algebraic Connectivity of a Balanced Binary Tree},
volume = {6},
journal = {ELA. The Electronic Journal of Linear Algebra [electronic only]},
doi = {10.13001/1081-3810.1040}
}

@article{bansal2019potential,
	title={Potential-function proofs for gradient methods},
	author={Bansal, Nikhil and Gupta, Anupam},
	journal={Theory of Computing},
	volume={15},
	number={1},
	pages={1--32},
	year={2019},
	publisher={Theory of Computing Exchange}
}

@article{blanchet2018towards,
	title={Towards optimal running times for optimal transport},
	author={Blanchet, Jose and Jambulapati, Arun and Kent, Carson and Sidford, Aaron},
	journal={arXiv preprint arXiv:1810.07717},
	year={2018}
}

@inproceedings{bayoumi2020tighter,
	title={Tighter theory for local SGD on identical and heterogeneous data},
	author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={4519--4529},
	year={2020}
}

@article{gorbunov2020linearly,
	title={Linearly Converging Error Compensated SGD},
	author={Gorbunov, Eduard and Kovalev, Dmitry and Makarenko, Dmitry and Richt{\'a}rik, Peter},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}

@inproceedings{guminov2019accelerated,
	title={On a combination of alternating minimization and Nesterov’s momentum},
	author={Guminov, Sergey and Dvurechensky, Pavel and Tupitsa, Nazarii and Gasnikov, Alexander},
	booktitle={International Conference on Machine Learning},
	pages={3886--3898},
	year={2021},
	organization={PMLR}
}

@article{rogozin2019projected,
	title={Projected Gradient Method for Decentralized Optimization over Time-Varying Networks},
	author={Rogozin, Alexander and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:1911.08527},
	year={2019}
}

@article{rigollet2018entropic,
	title={Entropic optimal transport is maximum-likelihood deconvolution},
	author={Rigollet, Philippe and Weed, Jonathan},
	journal={Comptes Rendus Mathematique},
	volume={356},
	number={11-12},
	pages={1228--1235},
	year={2018},
	publisher={Elsevier}
}


@article{peyre2019computational,
	title={Computational optimal transport},
	author={Peyr{\'e}, Gabriel and Cuturi, Marco and others},
	journal={Foundations and Trends{\textregistered} in Machine Learning},
	volume={11},
	number={5-6},
	pages={355--607},
	year={2019},
	publisher={Now Publishers, Inc.}
}


@article{dvinskikh2020sa,
	title={Stochastic Approximation versus Sample Average Approximation for population Wasserstein barycenters},
	author={Dvinskikh, Darina},
	journal={arXiv preprint arXiv:2001.07697},
	year={2020}
}


@article{feldman2019high,
	title={High probability generalization bounds for uniformly stable algorithms with nearly optimal rate},
	author={Feldman, Vitaly and Vondrak, Jan},
	journal={arXiv preprint arXiv:1902.10710},
	year={2019}
}

@article{tang2019practicality,
	title={The Practicality of Stochastic Optimization in Imaging Inverse Problems},
	author={Tang, Junqi and Egiazarian, Karen and Golbabaee, Mohammad and Davies, Mike},
	journal={arXiv preprint arXiv:1910.10100},
	year={2019}
}

@article{devolder2012double,
	title={Double smoothing technique for large-scale linearly constrained convex optimization},
	author={Devolder, Olivier and Glineur, Fran{\c{c}}ois and Nesterov, Yurii},
	journal={SIAM Journal on Optimization},
	volume={22},
	number={2},
	pages={702--727},
	year={2012},
	publisher={SIAM}
}

@inproceedings{vaswani2019fast,
	title={Fast and Faster Convergence of SGD for Over-Parameterized Models and an Accelerated Perceptron},
	author={Vaswani, Sharan and Bach, Francis and Schmidt, Mark},
	booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
	pages={1195--1204},
	year={2019}
}

@article{aybat2019universally,
	title={A universally optimal multistage accelerated stochastic gradient method},
	author={Aybat, Necdet Serhat and Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asuman},
	journal={arXiv preprint arXiv:1901.08022},
	year={2019}
}

@article{fallah2019robust,
	title={Robust Distributed Accelerated Stochastic Gradient Methods for Multi-Agent Networks},
	author={Fallah, Alireza and Gurbuzbalaban, Mert and Ozdaglar, Asu and Simsekli, Umut and Zhu, Lingjiong},
	journal={arXiv preprint arXiv:1910.08701},
	year={2019}
}

@article{xu2019accelerated,
	title={Accelerated Primal-Dual Algorithms for Distributed Smooth Convex Optimization over Networks},
	author={Xu, Jinming and Tian, Ye and Sun, Ying and Scutari, Gesualdo},
	journal={arXiv preprint arXiv:1910.10666},
	year={2019}
}

@article{scaman2019optimal,
	title={Optimal Convergence Rates for Convex Distributed Optimization in Networks},
	author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
	journal={Journal of Machine Learning Research},
	volume={20},
	number={159},
	pages={1--31},
	year={2019}
}

@article{gasnikov2018universalgrad,
	title={Universal gradient descent},
	author={Gasnikov, Alexander},
	journal={MIPT},
	year={2018}
}

@article{dvinskikh2019decentralized,
	title={Decentralized and parallel primal and dual accelerated methods for stochastic convex programming problems},
	author={Dvinskikh, Darina and Gasnikov, Alexander},
	journal={Journal of Inverse and Ill-posed Problems},
	volume={29},
	number={3},
	pages={385--405},
	year={2021}
}


@article{horvath2019natural,
	title={Natural Compression for Distributed Deep Learning},
	author={Horvath, Samuel and Ho, Chen-Yu and Horvath, Ludovit and Sahu, Atal Narayan and Canini, Marco and Richtarik, Peter},
	journal={arXiv preprint arXiv:1905.10988},
	year={2019}
}

@article{horvath2019stochastic,
	title={Stochastic distributed learning with gradient quantization and variance reduction},
	author={Horv{\'a}th, Samuel and Kovalev, Dmitry and Mishchenko, Konstantin and Stich, Sebastian and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:1904.05115},
	year={2019}
}

@article{liu2019double,
	title={A Double Residual Compression Algorithm for Efficient Distributed Learning},
	author={Liu, Xiaorui and Li, Yao and Tang, Jiliang and Yan, Ming},
	journal={arXiv preprint arXiv:1910.07561},
	year={2019}
}

@article{basu2019qsparse,
	title={Qsparse-local-SGD: Distributed SGD with Quantization, Sparsification, and Local Computations},
	author={Basu, Debraj and Data, Deepesh and Karakus, Can and Diggavi, Suhas},
	journal={arXiv preprint arXiv:1906.02367},
	year={2019}
}

@article{khaled2019better,
	title={Better Communication Complexity for Local SGD},
	author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:1909.04746},
	year={2019}
}

@article{yu2019linear,
	title={On the linear speedup analysis of communication efficient momentum sgd for distributed non-convex optimization},
	author={Yu, Hao and Jin, Rong and Yang, Sen},
	journal={arXiv preprint arXiv:1905.03817},
	year={2019}
}

@article{khaled2019first,
	title={First analysis of local gd on heterogeneous data},
	author={Khaled, Ahmed and Mishchenko, Konstantin and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:1909.04715},
	year={2019}
}

@article{karimireddy2019error,
	title={Error feedback fixes signsgd and other gradient compression schemes},
	author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian U and Jaggi, Martin},
	journal={arXiv preprint arXiv:1901.09847},
	year={2019}
}

@inproceedings{stich2018sparsified,
	title={Sparsified SGD with memory},
	author={Stich, Sebastian U and Cordonnier, Jean-Baptiste and Jaggi, Martin},
	booktitle={Advances in Neural Information Processing Systems},
	pages={4447--4458},
	year={2018}
}

@article{stich2018local,
	title={Local SGD converges fast and communicates little},
	author={Stich, Sebastian U},
	journal={arXiv preprint arXiv:1805.09767},
	year={2018}
}

@article{mairal2015incremental,
	title={Incremental majorization-minimization optimization with application to large-scale machine learning},
	author={Mairal, Julien},
	journal={SIAM Journal on Optimization},
	volume={25},
	number={2},
	pages={829--855},
	year={2015},
	publisher={SIAM}
}

@article{RobbinsMonro:1951,
	added-at = {2008-10-07T16:03:39.000+0200},
	author = {Robbins, H. and Monro, S.},
	biburl = {http://www.bibsonomy.org/bibtex/2cc1b9aa8927ac4952e93f34094a3eaaf/brefeld},
	interhash = {93d54534a08c30eda9e34d1def03ffa3},
	intrahash = {cc1b9aa8927ac4952e93f34094a3eaaf},
	journal = {Annals of Mathematical Statistics},
	keywords = {imported},
	pages = {400-407},
	timestamp = {2008-10-07T16:03:40.000+0200},
	title = {A stochastic approximation method},
	volume = 22,
	year = 1951
}

@article{taylor2018lyapunov,
	title={Lyapunov functions for first-order methods: Tight automated convergence guarantees},
	author={Taylor, Adrien and Van Scoy, Bryan and Lessard, Laurent},
	journal={arXiv preprint arXiv:1803.06073},
	year={2018}
}

@article{zhou2018direct,
	title={Direct acceleration of SAGA using sampled negative momentum},
	author={Zhou, Kaiwen},
	journal={arXiv preprint arXiv:1806.11048},
	year={2018}
}

@inproceedings{gorbunov2019unified,
	title={A unified theory of sgd: Variance reduction, sampling, quantization and coordinate descent},
	author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={680--690},
	year={2020},
	organization={PMLR}
}

@inproceedings{defazio2016simple,
	title={A simple practical accelerated method for finite sums},
	author={Defazio, Aaron},
	booktitle={Advances in neural information processing systems},
	pages={676--684},
	year={2016}
}

@article{zhou2018simple,
	title={A simple stochastic variance reduced algorithm with fast convergence rates},
	author={Zhou, Kaiwen and Shang, Fanhua and Cheng, James},
	journal={arXiv preprint arXiv:1806.11027},
	year={2018}
}

@article{kovalev2019don,
	title={Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop},
	author={Kovalev, Dmitry and Horvath, Samuel and Richtarik, Peter},
	journal={arXiv preprint arXiv:1901.08689},
	year={2019}
}

@article{gower2019sgd,
	title={SGD: General Analysis and Improved Rates},
	author={Gower, Robert Mansel and Loizou, Nicolas and Qian, Xun and Sailanbayev, Alibek and Shulgin, Egor and Richtarik, Peter},
	journal={arXiv preprint arXiv:1901.09401},
	year={2019}
}

@article{mishchenko2019distributed,
	title={Distributed Learning with Compressed Gradient Differences},
	author={Mishchenko, Konstantin and Gorbunov, Eduard and Tak{\'a}{\v{c}}, Martin and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:1901.09269},
	year={2019}
}

@article{jahani2018efficient,
	title={Efficient Distributed Hessian Free Algorithm for Large-scale Empirical Risk Minimization via Accumulating Sample Strategy},
	author={Jahani, Majid and He, Xi and Ma, Chenxin and Mokhtari, Aryan and Mudigere, Dheevatsa and Ribeiro, Alejandro and Tak{\'a}{\v{c}}, Martin},
	journal={arXiv:1810.11507},
	year={2018}
}

@article{ma2015partitioning,
	title={Partitioning Data on Features or Samples in Communication-Efficient Distributed Optimization?},
	author={Ma, Chenxin and Tak{\'a}{\v{c}}, Martin},
	journal={OptML@NIPS 2015, arXiv:1510.06688},
	year={2015}
}


@misc{TensorFlow-Slim,
	title = {TensorFlow-Slim image classification model library},
	howpublished = {https://github.com/tensorflow/models/tree/master/research/slim},
}

@article{schmidt2017minimizing,
	title={Minimizing finite sums with the stochastic average gradient},
	author={Schmidt, Mark and Le Roux, Nicolas and Bach, Francis},
	journal={Mathematical Programming},
	volume={162},
	number={1-2},
	pages={83--112},
	year={2017},
	publisher={Springer}
}

@inproceedings{johnson2013accelerating,
	title={Accelerating stochastic gradient descent using predictive variance reduction},
	author={Johnson, Rie and Zhang, Tong},
	booktitle={Advances in neural information processing systems},
	pages={315--323},
	year={2013}
}

@article{nguyen2018sgd,
	title={SGD and Hogwild! convergence without the bounded gradients assumption},
	author={Nguyen, Lam M and Nguyen, Phuong Ha and van Dijk, Marten and Richt{\'a}rik, Peter and Scheinberg, Katya and Tak{\'a}{\v{c}}, Martin},
	journal={arXiv preprint arXiv:1802.03801},
	year={2018}
}

@InProceedings{DANE,
	author    = {Ohad Shamir and Nati Srebro and Tong Zhang},
	title     = {Communication-Efficient Distributed Optimization using an Approximate {N}ewton-type Method},
	booktitle = {Proceedings of the 31st International Conference on Machine Learning, PMLR},
	year      = {2014},
	volume    = {32},
	number    = {2},
	pages     = {1000-1008},
}


@article{chunduriperformance,
	title={Performance Analysis of MPI on Cray XC40 Xeon Phi System},
	author={Chunduri, Sudheer and Coffman, Paul and Parker, Scott and Kumaran, Kalyan},
	year={2017},
}


@InProceedings{DISCO,
	author    = {Yuchen Zhang and Lin Xiao},
	title     = {{D}i{SCO}: Distributed Optimization for Self-Concordant Empirical Loss},
	booktitle = {Proceedings of the 32nd International Conference on Machine Learning, PMLR},
	year      = {2015},
	volume    = {37},
	pages     = {362-370},
}

@article{reddi2016aide,
	title={{AIDE}: Fast and communication efficient distributed optimization},
	author={Reddi, Sashank J and Kone{\v c}n{\'y}, Jakub and Richt{\'a}rik, Peter and P{\'o}cz{\'o}s, Barnab{\'a}s and Smola, Alex},
	journal={arXiv preprint arXiv:1608.06879},
	year={2016}
}

@Article{AccCOCOA,
	author  = {Chenxin Ma and Martin Jaggi and Frank E. Curtis and Nathan Srebro and Martin Tak\'{a}\v{c}},
	title   = {An Accelerated Communication-Efficient Primal-Dual Optimization Framework for Structured Machine Learning},
	journal = {arXiv:1711.05305},
	year    = {2017},
}

@Article{cocoa-2018-JMLR,
	author  = {V. Smith and S. Forte and C. Ma and M. Tak\'{a}\v{c} and M. I. Jordan and M. Jaggi},
	title   = {{C}o{C}o{A}: A General Framework for Communication-Efficient Distributed Optimization},
	journal = {Journal of Machine Learning Research},
	year    = {2018},
	volume  = {18},
	pages   = {1--49},
}


@InProceedings{COCOA+,
	author        = {Ma, Chenxin and Smith, Virginia and Jaggi, Martin and Jordan, Michael I. and Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
	title         = {Adding vs. averaging in distributed primal-dual optimization},
	booktitle     = {The 32nd International Conference on Machine Learning},
	year          = {2015},
	pages         = {1973--1982},
	__markedentry = {[richtap:1]},
	journal       = {Proceedings of The 32nd International Conference on Machine Learning},
}

@Article{COCOA+journal,
	author        = {Ma, Chenxin and Kone\v{c}n\'{y}, Jakub and Jaggi, Martin and Smith, Virginia and Jordan, Michael I. and Richt\'{a}rik, Peter and Tak\'{a}\v{c}, Martin},
	title         = {Distributed optimization with arbitrary local solvers},
	journal       = {Optimization Methods and Software},
	year          = {2017},
	volume        = {32},
	number        = {4},
	pages         = {813--848},
	__markedentry = {[richtap:1]},
}

@InProceedings{cocoa,
	author    = {Jaggi, Martin and Smith, Virginia and Tak{\'a}\v{c}, Martin and Terhorst, Jonathan and Krishnan, Sanjay and Hofmann, Thomas and Jordan, Michael I.},
	title     = {Communication-efficient distributed dual coordinate ascent},
	booktitle = {Advances in Neural Information Processing Systems 27},
	year      = {2014},
}

@article{Hydra2,
	author        = {Fercoq, Olivier and Qu, Zheng and Richt\'{a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
	title         = {Fast distributed coordinate descent for minimizing non-strongly convex losses},
	journal       = {IEEE International Workshop on Machine Learning for Signal Processing},
	year          = {2014},
}

@article{Hydra,
	author        = {Richt{\'a}rik, Peter and Tak{\'a}{\v{c}}, Martin},
	title         = {Distributed coordinate descent method for learning with big data},
	journal       = {Journal of Machine Learning Research},
	year          = {2016},
	volume        = {17},
	number        = {75},
	pages         = {1--25},
}

@inproceedings{alistarh2017qsgd,
	title={{QSGD}: {C}ommunication-efficient {SGD} via gradient quantization and encoding},
	author={Alistarh, Dan and Grubic, Demjan and Li, Jerry and Tomioka, Ryota and Vojnovic, Milan},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1709--1720},
	year={2017}
}

@InProceedings{pmlr-v80-nguyen18c,
	title = 	 {{SGD} and Hogwild! {C}onvergence Without the Bounded Gradients Assumption},
	author = 	 {Nguyen, Lam and NGUYEN, PHUONG HA and van Dijk, Marten and Richtarik, Peter and Scheinberg, Katya and Takac, Martin},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {3750--3758},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR}
}

@article{wangni2017gradient,
	title={Gradient sparsification for communication-efficient distributed optimization},
	author={Wangni, Jianqiao and Wang, Jialei and Liu, Ji and Zhang, Tong},
	journal={arXiv preprint arXiv:1710.09854},
	year={2017}
}

@article{khirirat2018distributed,
	title={Distributed learning with compressed gradients},
	author={Khirirat, Sarit and Feyzmahdavian, Hamid Reza and Johansson, Mikael},
	journal={arXiv preprint arXiv:1806.06573},
	year={2018}
}

@inproceedings{wen2017terngrad,
	title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
	author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1509--1519},
	year={2017}
}

@inproceedings{de2015taming,
	title={Taming the wild: A unified analysis of hogwild-style algorithms},
	author={De Sa, Christopher M and Zhang, Ce and Olukotun, Kunle and R{\'e}, Christopher},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2674--2682},
	year={2015}
}

@InProceedings{pmlr-v80-bernstein18a,
	title = 	 {sign{SGD}: Compressed Optimisation for Non-Convex Problems},
	author = 	 {Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {560--569},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR},
}

@article{wang2018atomo,
	title={ATOMO: Communication-efficient Learning via Atomic Sparsification},
	author={Wang, Hongyi and Sievert, Scott and Charles, Zachary and Papailiopoulos, Dimitris and Wright, Stephen},
	journal={arXiv preprint arXiv:1806.04090},
	year={2018}
}

@InProceedings{pmlr-v80-wu18d,
	title = 	 {Error Compensated Quantized {SGD} and its Applications to Large-scale Distributed Optimization},
	author = 	 {Wu, Jiaxiang and Huang, Weidong and Huang, Junzhou and Zhang, Tong},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {5325--5333},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Stockholmsmässan, Stockholm Sweden},
	month = 	 {10--15 Jul},
	publisher = 	 {PMLR}
}

@article{chen2016revisiting,
	title={Revisiting distributed synchronous SGD},
	author={Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz, Rafal},
	journal={arXiv preprint arXiv:1604.00981},
	year={2016}
}

@inproceedings{reddi2016proximal,
	title={Proximal stochastic methods for nonsmooth nonconvex finite-sum optimization},
	author={Reddi, Sashank J and Sra, Suvrit and P{\'o}czos, Barnab{\'a}s and Smola, Alexander J},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1145--1153},
	year={2016}
}

@inproceedings{lin2018deep,
	title={Deep Gradient Compression: Reducing the Communication Bandwidth for Distributed Training},
	author={Yujun Lin and Song Han and Huizi Mao and Yu Wang and Bill Dally},
	booktitle={International Conference on Learning Representations},
	year={2018}
}

@article{zhu2016trained,
	title={Trained ternary quantization},
	author={Zhu, Chenzhuo and Han, Song and Mao, Huizi and Dally, William J},
	journal={arXiv preprint arXiv:1612.01064},
	year={2016}
}

@inproceedings{riedmiller1993direct,
	title={A direct adaptive method for faster backpropagation learning: The RPROP algorithm},
	author={Riedmiller, Martin and Braun, Heinrich},
	booktitle={Neural Networks, 1993., IEEE International Conference on},
	pages={586--591},
	year={1993},
	organization={IEEE}
}

@incollection{robbins1985stochastic,
	title={A stochastic approximation method},
	author={Robbins, Herbert and Monro, Sutton},
	booktitle={Herbert Robbins Selected Papers},
	pages={102--109},
	year={1985},
	publisher={Springer}
}

@article{konevcny2016federated,
	title={Federated learning: Strategies for improving communication efficiency},
	author={Kone{\v{c}}n{\'y}, Jakub and McMahan, H Brendan and Yu, Felix X and Richt{\'a}rik, Peter and Suresh, Ananda Theertha and Bacon, Dave},
	journal={arXiv preprint arXiv:1610.05492},
	year={2016}
}

@article{konecny2016randomized,
	title={Randomized distributed mean estimation: Accuracy vs communication},
	author={Kone\v{c}n{\'y}, Jakub and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:1611.07555},
	year={2016}
}

@inproceedings{lian2017can,
	title={Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent},
	author={Lian, Xiangru and Zhang, Ce and Zhang, Huan and Hsieh, Cho-Jui and Zhang, Wei and Liu, Ji},
	booktitle={Advances in Neural Information Processing Systems},
	pages={5330--5340},
	year={2017}
}

@inproceedings{shamir2014communication,
	title={Communication-efficient distributed optimization using an approximate newton-type method},
	author={Shamir, Ohad and Srebro, Nati and Zhang, Tong},
	booktitle={International conference on machine learning},
	pages={1000--1008},
	year={2014}
}

@article{mishchenko2018distributed,
	title={A Distributed Flexible Delay-tolerant Proximal Gradient Algorithm},
	author={Mishchenko, Konstantin and Iutzeler, Franck and Malick, J{\'e}r{\^o}me},
	journal={arXiv preprint arXiv:1806.09429},
	year={2018}
}

@article{goyal2017accurate,
	title={Accurate, large minibatch SGD: training imagenet in 1 hour},
	author={Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
	journal={arXiv preprint arXiv:1706.02677},
	year={2017}
}

@article{bernstein2018signsgd,
	title={signSGD with Majority Vote is Communication Efficient And Byzantine Fault Tolerant},
	author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
	journal={arXiv preprint arXiv:1810.05291},
	year={2018}
}

@inproceedings{seide20141,
	title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech dnns},
	author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
	booktitle={Fifteenth Annual Conference of the International Speech Communication Association},
	year={2014}
}

@Book{rockafellar2015,
	title     = {Convex analysis},
	publisher = {Princeton university press},
	year      = {2015},
	author    = {Rockafellar, Ralph Tyrell},
}

@article{nedic2017fast,
	title={Fast convergence rates for distributed non-bayesian learning},
	author={Nedi{\'c}, Angelia and Olshevsky, Alex and Uribe, C{\'e}sar A},
	journal={IEEE Transactions on Automatic Control},
	volume={62},
	number={11},
	pages={5538--5553},
	year={2017},
	publisher={IEEE}
}

@article{xiao2007distributed,
	title={Distributed average consensus with least-mean-square deviation},
	author={Xiao, Lin and Boyd, Stephen and Kim, Seung-Jean},
	journal={Journal of parallel and distributed computing},
	volume={67},
	number={1},
	pages={33--46},
	year={2007},
	publisher={Elsevier}
}

@inproceedings{ren2006consensus,
	title={Consensus based formation control strategies for multi-vehicle systems},
	author={Ren, Wei},
	booktitle={2006 American Control Conference},
	pages={6--pp},
	year={2006},
	organization={IEEE}
}

@article{olshevsky2010efficient,
	title={Efficient information aggregation strategies for distributed control and signal processing},
	author={Olshevsky, Alex},
	journal={arXiv preprint arXiv:1009.6036},
	year={2010}
}

article{olshevsky2015linear,
	title={Linear time average consensus on fixed graphs},
	author={Olshevsky, Alex},
	journal={IFAC-PapersOnLine},
	volume={48},
	number={22},
	pages={94--99},
	year={2015},
	publisher={Elsevier}
}

@inproceedings{ram2009distributed,
	title={Distributed non-autonomous power control through distributed convex optimization},
	author={Ram, Sundhar Srinivasan and Veeravalli, Venugopal V and Nedic, Angelia},
	booktitle={IEEE INFOCOM 2009},
	pages={3001--3005},
	year={2009},
	organization={IEEE}
}


@inproceedings{kroshnin2019complexity,
	title={On the complexity of approximating Wasserstein barycenters},
	author={Kroshnin, Alexey and Tupitsa, Nazarii and Dvinskikh, Darina and Dvurechensky, Pavel and Gasnikov, Alexander and Uribe, Cesar},
	booktitle={International conference on machine learning},
	pages={3530--3540},
	year={2019},
	organization={PMLR}
}

@inproceedings{agarwal2010optimal,
	title={Optimal Algorithms for Online Convex Optimization with Multi-Point Bandit Feedback},
	author={Alekh Agarwal and Ofer Dekel and Lin Xiao},
	booktitle = {COLT 2010 - The 23rd Conference on Learning Theory},
	year={2010}
}


@incollection{agarwal2011stochastic,
	title = {Stochastic convex optimization with bandit feedback},
	author = {Agarwal, Alekh and Foster, Dean P and Hsu, Daniel J and Kakade, Sham M and Rakhlin, Alexander},
	booktitle = {Advances in Neural Information Processing Systems 24},
	editor = {J. Shawe-Taylor and R. S. Zemel and P. L. Bartlett and F. Pereira and K. Q. Weinberger},
	pages = {1035--1043},
	year = {2011},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4475-stochastic-convex-optimization-with-bandit-feedback.pdf}
}


@Article{agueh2011barycenters,
	Title                    = {Barycenters in the Wasserstein space},
	Author                   = {Agueh, Martial and Carlier, Guillaume},
	Journal                  = {SIAM Journal on Mathematical Analysis},
	Year                     = {2011},
	Number                   = {2},
	Pages                    = {904--924},
	Volume                   = {43},
	Publisher                = {SIAM}
}


@article{allen2014linear,
	title={Linear coupling: An ultimate unification of gradient and mirror descent},
	author={Allen-Zhu, Zeyuan and Orecchia, Lorenzo},
	journal={arXiv:1407.1537},
	year={2014}
}


@InProceedings{allen2016even,
	title = 	 {Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling},
	author = 	 {Zeyuan Allen-Zhu and Zheng Qu and Peter Richtarik and Yang Yuan},
	booktitle = 	 {Proceedings of The 33rd International Conference on Machine Learning},
	pages = 	 {1110--1119},
	year = 	 {2016},
	editor = 	 {Maria Florina Balcan and Kilian Q. Weinberger},
	volume = 	 {48},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {New York, New York, USA},
	month = 	 {20--22 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v48/allen-zhuc16.pdf},
	url = 	 {http://proceedings.mlr.press/v48/allen-zhuc16.html},
	abstract = 	 {Accelerated coordinate descent is widely used in optimization due to its cheap per-iteration cost and scalability to large-scale problems. Up to a primal-dual transformation, it is also the same as accelerated stochastic gradient descent that is one of the central methods used in machine learning. In this paper, we improve the best known running time of accelerated coordinate descent by a factor up to \sqrtn. Our improvement is based on a clean, novel non-uniform sampling that selects each coordinate with a probability proportional to the square root of its smoothness parameter. Our proof technique also deviates from the classical estimation sequence technique used in prior work. Our speed-up applies to important problems such as empirical risk minimization and solving linear systems, both in theory and in practice.},
	note = {First appeared in arXiv:1512.09103}
}


@inproceedings{allen2016katyusha,
	author = {Allen-Zhu, Zeyuan},
	title = {Katyusha: The First Direct Acceleration of Stochastic Gradient Methods},
	booktitle = {Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
	series = {STOC 2017},
	year = {2017},
	isbn = {978-1-4503-4528-6},
	location = {Montreal, Canada},
	pages = {1200--1205},
	numpages = {6},
	url = {http://doi.acm.org/10.1145/3055399.3055448},
	doi = {10.1145/3055399.3055448},
	acmid = {3055448},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {accelerated gradient descent, acceleration, first-order method, momentum, stochastic gradient descent},
	note = {arXiv:1603.05953}
}


@INPROCEEDINGS{allen2017much,
	title={Much Faster Algorithms for Matrix Scaling},
	author={Z. Allen-Zhu and Y. Li and R. Oliveira and A. Wigderson},
	booktitle={2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)},
	year={2017},
	volume={},
	number={},
	pages={890-901},
	keywords={approximation theory;computational complexity;linear systems;matrix algebra;stochastic processes;XAY ε;classical Matrix Scaling problem;column sums;diagonal matrices;doubly stochastic matrix;general scaling problem;linear systems;nonzero entries;polynomially large entries;total complexity Õ;Complexity theory;Computer science;Convergence;Ellipsoids;Linear systems;Manganese;Optimization;doubly stochastic;first-order method;iterative algorithms;matrix scaling;second-order method},
	doi={10.1109/FOCS.2017.87},
	ISSN={0272-5428},
	month={Oct},
	note={\url{https://arxiv.org/abs/1704.02315}},
}


@incollection{altschuler2017near-linear,
	title = {Near-linear time approxFimation algorithms for optimal transport via Sinkhorn iteration},
	author = {Altschuler, Jason and Weed, Jonathan and Rigollet, Philippe},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {1961--1971},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6792-near-linear-time-approximation-algorithms-for-optimal-transport-via-sinkhorn-iteration.pdf},
	note={arXiv:1705.09634}
}


@Article{anikin2016randomization,
	author="Anikin, Anton
	and Gasnikov, Alexander
	and Gornov, Alexander",
	title="Randomization and sparsity in huge-scale optimization on an example of mirror descent",
	journal="Proceedings of Moscow Institute of Physics and Technology",
	year="2016",
	volume="8",
	number="1",
	pages="11--24",
	note= {In Russian, arXiv:1602.00594}
}


@Article{anikin2017dual,
	author="Anikin, A. S.
	and Gasnikov, A. V.
	and Dvurechensky, P. E.
	and Tyurin, A. I.
	and Chernov, A. V.",
	title="Dual approaches to the minimization of strongly convex functionals with a simple structure under affine constraints",
	journal="Computational Mathematics and Mathematical Physics",
	year="2017",
	month="Aug",
	day="01",
	volume="57",
	number="8",
	pages="1262--1276",
	abstract="A strongly convex function of simple structure (for example, separable) is minimized under affine constraints. A dual problem is constructed and solved by applying a fast gradient method. The necessary properties of this method are established relying on which, under rather general conditions, the solution of the primal problem can be recovered with the same accuracy as the dual solution from the sequence generated by this method in the dual space of the problem. Although this approach seems natural, some previously unpublished rather subtle results necessary for its rigorous and complete theoretical substantiation in the required generality are presented.",
	issn="1555-6662",
	doi="10.1134/S0965542517080048",
	url="https://doi.org/10.1134/S0965542517080048"
}


@article{arjovsky2017wasserstein,
	title={Wasserstein {GAN}},
	author={Martin Arjovsky and Soumith Chintala and L\'eon Bottou},
	journal={arXiv:1701.07875},
	year={2017}
}


@article{aspremont2008smooth,
	author = {d'Aspremont, Alexandre},
	title = {Smooth Optimization with Approximate Gradient},
	journal = {SIAM J. on Optimization},
	issue_date = {October 2008},
	volume = {19},
	number = {3},
	month = oct,
	year = {2008},
	issn = {1052-6234},
	pages = {1171--1183},
	numpages = {13},
	url = {http://dx.doi.org/10.1137/060676386},
	doi = {10.1137/060676386},
	acmid = {1653915},
	publisher = {Society for Industrial and Applied Mathematics},
	address = {Philadelphia, PA, USA},
	keywords = {first-order methods, semidefinite programming, smooth optimization},
}


@InProceedings{bach16highly-smooth,
	title = 	 {Highly-Smooth Zero-th Order Online Optimization},
	author = 	 {Francis Bach and Vianney Perchet},
	booktitle = 	 {29th Annual Conference on Learning Theory},
	pages = 	 {257--283},
	year = 	 {2016},
	editor = 	 {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
	volume = 	 {49},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Columbia University, New York, New York, USA},
	month = 	 {23--26 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v49/bach16.pdf},
	url = 	 {http://proceedings.mlr.press/v49/bach16.html},
	abstract = 	 {The minimization of convex functions which are only available through partial and noisy information is a key methodological problem in many disciplines. In this paper we consider  convex optimization with noisy zero-th order information, that is noisy function evaluations at any desired point. We focus on problems with high degrees of smoothness, such as  logistic regression. We show that as opposed to gradient-based algorithms, high-order smoothness may be used to improve estimation rates, with a precise dependence  of our upper-bounds on the degree of smoothness. In particular, we show that for infinitely differentiable functions, we recover the same dependence on sample size as gradient-based algorithms, with an extra dimension-dependent factor. This is done for both convex and strongly-convex functions, with finite horizon and anytime algorithms. Finally, we also recover similar results in the online optimization setting.}
}


@techreport{baes2009estimate,
	author       = {Michel Baes}, 
	title        = {Estimate sequence methods:extensions and approximations},
	year         = 2009,
	url        = {http://www.optimization-online.org/DB_FILE/2009/08/2372.pdf}
}


@article{barett2010quasi-variational,
	author = {Barrett, John  W. and Prigozhin, Leonid},
	title = {A QUASI-VARIATIONAL INEQUALITY PROBLEM IN SUPERCONDUCTIVITY},
	journal = {Mathematical Models and Methods in Applied Sciences},
	volume = {20},
	number = {5},
	pages = {679-706},
	year = {2010},
	doi = {10.1142/S0218202510004404},
}


@article{barett2014lakes,
	author = {Barrett, John W. and Prigozhin, Leonid},
	title = {Lakes and rivers in the landscape: A quasi-variational inequality approach},
	journal = {Interfaces and Free Boundaries},
	volume = {16},
	number = {2},
	pages = {269-296},
	year = {2014},
	doi = {10.4171/IFB/320}
}


@article{barrio1999central,
	author = {Eustasio del Barrio and Evarist Gine and Carlos Matran},
	title = {Central Limit Theorems for the Wasserstein Distance between the Empirical and the True Distributions},
	journal = {The Annals of Probability},
	ISSN = {00911798},
	URL = {http://www.jstor.org/stable/2652770},
	abstract = {If X is integrable, F is its cdf and Fn is the empirical cdf based on an i.i.d. sample from F, then the Wasserstein distance between Fn and F, which coincides with the L1 norm ∫∞ -∞|Fn(t) - F(t)| dt of the centered empirical process, tends to zero a.s. The object of this article is to obtain rates of convergence and distributional limit theorems for this law of large numbers or, equivalently, stochastic boundedness and distributional limit theorems for the L1 norm of the empirical process. Some limit theorems for the Ornstein-Uhlenbeck process are also derived as a by-product.},
	number = {2},
	pages = {1009--1071},
	publisher = {Institute of Mathematical Statistics},
	volume = {27},
	year = {1999}
}


@article{bayandina2017gradient-free,
	title={Gradient-free two-points optimal method for non smooth stochastic convex optimization problem with additional small noise},
	author={Bayandina, Anastasia and Gasnikov, Alexander and Lagunovskaya, Anastasia},
	journal={Automation and remote control},
	volume={79},
	number={7},
	year={2018},
	publisher={Springer},
	note={arXiv:1701.03821}
}


@INPROCEEDINGS{bayandina2017adaptive,
	author={A. Bayandina},
	booktitle={2017 Constructive Nonsmooth Analysis and Related Topics (dedicated to the memory of V.F. Demyanov) (CNSA)},
	title={Adaptive mirror descent for constrained optimization},
	year={2017},
	volume={},
	number={},
	pages={1-4},
	keywords={convergence;convex programming;MD method;adaptive mirror descent;adaptive stepsizes;constrained optimization;convergence rate;dual solution;functional constraints;nonsmooth convex optimization problems;restart technique;strongly convex optimization problems;Convergence;Convex functions;Electronic mail;Mirrors;Optimization;Physics;Presses},
	doi={10.1109/CNSA.2017.7973937},
	ISSN={},
	month={May},}


@INPROCEEDINGS{bayandina2017adaptivest,
	author={A. Bayandina},
	booktitle={2017 Constructive Nonsmooth Analysis and Related Topics (dedicated to the memory of V.F. Demyanov) (CNSA)},
	title={Adaptive stochastic mirror descent for constrained optimization},
	year={2017},
	volume={},
	number={},
	pages={1-4},
	keywords={convergence;convex programming;stochastic programming;adaptive stepsizes;adaptive stochastic mirror descent;constrained optimization;convergence;lower bounds;nonsmooth convex optimization problems;stochastic MD;Convergence;Convex functions;Mirrors;Nickel;Optimization;Random variables;Sparse matrices},
	doi={10.1109/CNSA.2017.7973938},
	ISSN={},
	month={May},}


@Article{bayandina2018primal-dual,
	author="Bayandina, A.
	and Gasnikov, A.
	and Gasnikova, E.
	and Matsievsky, S.",
	title="Primal-dual mirror descent for the stochastic programming problems with functional constraints",
	journal="Computational Mathematics and Mathematical Physics",
	year="2018",
	volume="58",
	note="To appear, arXiv:1604.08194"
}

@incollection{bayandina2018mirror,
	title={Mirror descent and convex optimization problems with non-smooth inequality constraints},
	author={Bayandina, Anastasia and Dvurechensky, Pavel and Gasnikov, Alexander and Stonyakin, Fedor and Titov, Alexander},
	booktitle={Large-Scale and Distributed Optimization},
	pages={181--213},
	year={2018},
	publisher={Springer}
}


@article{beck2003mirror,
	author = {Beck, Amir and Teboulle, Marc},
	title = {Mirror Descent and Nonlinear Projected Subgradient Methods for Convex Optimization},
	journal = {Oper. Res. Lett.},
	issue_date = {May, 2003},
	volume = {31},
	number = {3},
	month = may,
	year = {2003},
	issn = {0167-6377},
	pages = {167--175},
	numpages = {9},
	url = {http://dx.doi.org/10.1016/S0167-6377(02)00231-6},
	doi = {10.1016/S0167-6377(02)00231-6},
	acmid = {2308952},
	publisher = {Elsevier Science Publishers B. V.},
	address = {Amsterdam, The Netherlands, The Netherlands},
	keywords = {Complexity analysis, Global rate of convergence, Mirror descent algorithms, Nonlinear projections, Nonsmooth convex minimization, Projected subgradient methods, Relative entropy},
} 


@article{beck2009fast,
	author = {Amir Beck and Marc Teboulle},
	title = {A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {2},
	number = {1},
	pages = {183-202},
	year = {2009},
	doi = {10.1137/080716542},
	URL = {      https://doi.org/10.1137/080716542 },
	eprint = {         https://doi.org/10.1137/080716542 }
}


@article{beck2010comirror,
	title = "The CoMirror algorithm for solving nonsmooth constrained convex problems",
	journal = "Operations Research Letters",
	volume = "38",
	number = "6",
	pages = "493 - 498",
	year = "2010",
	issn = "0167-6377",
	doi = "https://doi.org/10.1016/j.orl.2010.08.005",
	url = "http://www.sciencedirect.com/science/article/pii/S0167637710001094",
	author = "Amir Beck and Aharon Ben-Tal and Nili Guttmann-Beck and Luba Tetruashvili",
	keywords = "Convex optimization",
	keywords = "Gradient-based methods",
	keywords = "Non-Euclidean projection",
	keywords = "Mirror Descent"
}


@article{beck2014fast,
	title = "A fast dual proximal gradient algorithm for convex minimization and applications",
	journal = "Operations Research Letters",
	volume = "42",
	number = "1",
	pages = "1 - 6",
	year = "2014",
	note = "",
	issn = "0167-6377",
	doi = "http://dx.doi.org/10.1016/j.orl.2013.10.007",
	url = "http://www.sciencedirect.com/science/article/pii/S0167637713001454",
	author = "Amir Beck and Marc Teboulle",
	keywords = "Dual-based methods",
	keywords = "Fast gradient methods",
	keywords = "Convex optimization",
	keywords = "Rate of convergence"
}




@InProceedings{belloni2015escaping,
	title = 	 {Escaping the Local Minima via Simulated Annealing: Optimization of Approximately Convex Functions},
	author = 	 {Alexandre Belloni and Tengyuan Liang and Hariharan Narayanan and Alexander Rakhlin},
	booktitle = 	 {Proceedings of The 28th Conference on Learning Theory},
	pages = 	 {240--265},
	year = 	 {2015},
	editor = 	 {Peter Grünwald and Elad Hazan and Satyen Kale},
	volume = 	 {40},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Paris, France},
	month = 	 {03--06 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v40/Belloni15.pdf},
	url = 	 {http://proceedings.mlr.press/v40/Belloni15.html},
	abstract = 	 {We consider the problem of optimizing an approximately convex function over a bounded convex set in \mathbbR^n  using only function evaluations. The problem is reduced to sampling from an \emphapproximately log-concave distribution using the Hit-and-Run method, which is shown to have the same \mathcalO^* complexity as sampling from log-concave distributions. In addition to extend the analysis for log-concave distributions to approximate log-concave distributions, the implementation of the 1-dimensional sampler of the Hit-and-Run walk requires new methods and analysis. The algorithm then is based on simulated annealing which does not relies on first order conditions which makes it essentially immune to local minima. We then apply the method to different motivating problems. In the context of zeroth order stochastic convex optimization, the proposed method produces an ε-minimizer after \mathcalO^*(n^7.5ε^-2) noisy function evaluations  by inducing a \mathcalO(ε/n)-approximately log concave distribution. We also consider in detail the case when the “amount of non-convexity” decays towards the optimum of the function. Other applications of the method discussed in this work include private computation of empirical risk minimizers, two-stage stochastic programming, and approximate dynamic programming for online learning.}
}



@article{benamou2015iterative,
	author = {Jean-David Benamou and Guillaume Carlier and Marco Cuturi and Luca Nenna and Gabriel Peyré},
	title = {Iterative Bregman Projections for Regularized Transportation Problems},
	journal = {SIAM Journal on Scientific Computing},
	volume = {37},
	number = {2},
	pages = {A1111-A1138},
	year = {2015},
	doi1 = {10.1137/141000439},
	URL1 = {http://dx.doi.org/10.1137/141000439},
	eprint = { http://dx.doi.org/10.1137/141000439}
}


@article{ben-tal1997robust,
	author = {Ben-Tal, A. and Nemirovski, A.},
	title = {Robust Truss Topology Design via Semidefnite Programming},
	journal = {SIAM J. Optim.},
	volume = {7},
	number = {4},
	year = {1997},
	pages = {991 -- 1016}
}


@book{ben-tal2001lectures,
	author = {Ben-Tal, Aaron and Nemirovski, Arkadi},
	title = {Lectures on Modern Convex Optimization.},
	publisher = {Society for Industrial and Applied Mathematics},
	year = {2001},
	doi = {10.1137/1.9780898718829},
	address = {},
	edition   = {},
	URL = {http://epubs.siam.org/doi/abs/10.1137/1.9780898718829},
	eprint = {http://epubs.siam.org/doi/pdf/10.1137/1.9780898718829}
}


@book{ben-tal2015lectures,
	author = {Ben-Tal, Aaron and Nemirovski, Arkadi},
	title = {Lectures on Modern Convex Optimization (Lecture Notes)},
	publisher = {Personal web-page of A. Nemirovski},
	year = {2015},
	URL = {http://www2.isye.gatech.edu/~nemirovs/Lect_ModConvOpt.pdf}
}


@article{bigot2017geodesic,
	author = "Bigot, J\'er\'emie and Gouet, Ra\'ul and Klein, Thierry and L\'opez, Alfredo",
	title = "Geodesic {PCA} in the Wasserstein space by convex PCA",
	doi = "10.1214/15-AIHP706",
	fjournal = "Annales de l'Institut Henri Poincaré, Probabilités et Statistiques",
	journal = "Ann. Inst. H. Poincar\'e Probab. Statist.",
	month = "02",
	number = "1",
	pages = "1--26",
	publisher = "Institut Henri Poincar\'e",
	url = "https://doi.org/10.1214/15-AIHP706",
	volume = "53",
	year = "2017"
}


@book{birjukov2010optimization,
	author = {Birjukov, Alexander},
	title = {Optimization methods. Optimality conditions in extremal problems.},
	publisher = {Moscow Institute of Physics and Technology},
	year = {2010},
	note = {In Russian}
}


@article{blondel2017smooth,
	title={Smooth and Sparse Optimal Transport},
	author={Mathieu Blondel and Vivien Seguy and Antoine Rolet},
	journal={arXiv:1710.06276},
	year={2017}
}


Article{bogolubsky2016learning,
	author="Bogolubsky, Lev
	and Dvurechensky, Pavel
	and Gasnikov, Alexander
	and Gusev, Gleb
	and Nesterov, Yurii
	and Raigorodskii, Andrey
	and Tikhonov, Aleksey
	and Zhukovskii,  Maksim",
	title="Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods",
	journal="NIPS 2016",
	year="2016",
	url="http://papers.nips.cc/paper/6565-learning-supervised-pagerank-with-gradient-based-and-gradient-free-optimization-methods.pdf"
}


@incollection{bogolubsky2016learning,
	title = {Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods},
	author = {Bogolubsky, Lev and Dvurechensky, Pavel and Gasnikov, Alexander and Gusev, Gleb and Nesterov, Yurii and Raigorodskii, Andrei M and Tikhonov, Aleksey and Zhukovskii, Maksim},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {4914--4922},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6565-learning-supervised-pagerank-with-gradient-based-and-gradient-free-optimization-methods.pdf},
	note = {arXiv:1603.00717}
}


@book{boucheron2013concentration,
	title={Concentration Inequalities: A Nonasymptotic Theory of Independence},
	author={Boucheron, S. and Lugosi, G. and Massart, P.},
	year={2013},
	publisher={Oxford University Press}
}


@article{boyd2011distributed,
	author = {Boyd, Stephen and Parikh, Neal and Chu, Eric and Peleato, Borja and Eckstein, Jonathan},
	title = {Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers},
	journal = {Found. Trends Mach. Learn.},
	issue_date = {January 2011},
	volume = {3},
	number = {1},
	month = jan,
	year = {2011},
	issn = {1935-8237},
	pages = {1--122},
	numpages = {122},
	url = {http://dx.doi.org/10.1561/2200000016},
	doi = {10.1561/2200000016},
	acmid = {2185816},
	publisher = {Now Publishers Inc.},
	address = {Hanover, MA, USA},
}


@book{boyd2004convex,
	author = {Boyd, S. and Vandenberghe, L.},
	title = {Convex Optimization},
	city = {New York},
	publisher = {NY Cambridge University Press},
	year = {2004}
}


@article{bregman1967proof,
	title = "Proof of the convergence of Sheleikhovskii's method for a problem with transportation constraints",
	journal = "USSR Computational Mathematics and Mathematical Physics",
	volume = "7",
	number = "1",
	pages = "191 - 204",
	year = "1967",
	note = "",
	issn = "0041-5553",
	doi = "http://dx.doi.org/10.1016/0041-5553(67)90069-9",
	url = "http://www.sciencedirect.com/science/article/pii/0041555367900699",
	author = "L.M. Bregman",
}


@book{brent1973algorithms,
	title={Algorithms for Minimization Without Derivatives},
	author={Brent, R.P.},
	isbn={9780486419985},
	lccn={01047459},
	series={Dover Books on Mathematics},
	url={https://books.google.de/books?id=6Ay2biHG-GEC},
	year={1973},
	publisher={Dover Publications}
}



@article{bubeck2012regret,
	author = {S\'ebastien Bubeck and Nicol\'o Cesa-Bianchi},
	title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
	url = {http://dx.doi.org/10.1561/2200000024},
	year = {2012},
	volume = {5},
	journal = {Foundations and Trends® in Machine Learning},
	doi = {10.1561/2200000024},
	issn = {1935-8237},
	number = {1},
	pages = {1-122}
}


@inproceedings{bubeck2017kernel-based,
	author = {Bubeck, S{\'e}bastien and Lee, Yin Tat and Eldan, Ronen},
	title = {Kernel-based Methods for Bandit Convex Optimization},
	booktitle = {Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing},
	series = {STOC 2017},
	year = {2017},
	isbn = {978-1-4503-4528-6},
	location = {Montreal, Canada},
	pages = {72--85},
	numpages = {14},
	url = {http://doi.acm.org/10.1145/3055399.3055403},
	doi = {10.1145/3055399.3055403},
	acmid = {3055403},
	publisher = {ACM},
	address = {New York, NY, USA},
	keywords = {convex optimization, multi-armed bandit, online learning},
	note = {arXiv:1607.03084}
} 


@article{cauchy1847methode,
	author = {Cauchy, Augustin},
	title = {M\'ethode g\'en\'erale pour la r\'esolution des syst\'emes d'\'equations simultan\'ees},
	year = {1847},
	volume = {55},
	journal = {Comptes rendus hebdomadaires des s\'eances de l'Acad\'emie des sciences},
	pages = {536--538}
}



@incollection{cesa-bianchi2002generalization,
	title = {On the Generalization Ability of On-Line Learning Algorithms},
	author = {Nicol\`{o} Cesa-bianchi and Alex Conconi and Gentile, Claudio},
	booktitle = {Advances in Neural Information Processing Systems 14},
	editor = {T. G. Dietterich and S. Becker and Z. Ghahramani},
	pages = {359--366},
	year = {2002},
	publisher = {MIT Press},
	url = {http://papers.nips.cc/paper/2113-on-the-generalization-ability-of-on-line-learning-algorithms.pdf}
}


@Article{chambolle2011first-order,
	author="Chambolle, Antonin
	and Pock, Thomas",
	title="A First-Order Primal-Dual Algorithm for Convex Problems with Applications to Imaging",
	journal="Journal of Mathematical Imaging and Vision",
	year="2011",
	volume="40",
	number="1",
	pages="120--145",
	abstract="In this paper we study a first-order primal-dual algorithm for non-smooth convex optimization problems with known saddle-point structure. We prove convergence to a saddle-point with rate O(1/N) in finite dimensions for the complete class of problems. We further show accelerations of the proposed algorithm to yield improved rates on problems with some degree of smoothness. In particular we show that we can achieve O(1/N                        2) convergence on problems, where the primal or the dual objective is uniformly convex, and we can show linear convergence, i.e. O($\omega$                                          N                ) for some $\omega$∈(0,1), on smooth problems. The wide applicability of the proposed algorithm is demonstrated on several imaging problems such as image denoising, image deconvolution, image inpainting, motion estimation and multi-label image segmentation.",
	issn="1573-7683",
	doi="10.1007/s10851-010-0251-1",
	url="http://dx.doi.org/10.1007/s10851-010-0251-1"
}


@InProceedings{chernov2016fast,
	author="Chernov, Alexey
	and Dvurechensky, Pavel
	and Gasnikov, Alexander",
	editor="Kochetov, Yury
	and Khachay, Michael
	and Beresnev, Vladimir
	and Nurminski, Evgeni
	and Pardalos, Panos",
	title="Fast Primal-Dual Gradient Method for Strongly Convex Minimization Problems with Linear Constraints",
	booktitle="Discrete Optimization and Operations Research: 9th International Conference, DOOR 2016, Vladivostok, Russia, September 19-23, 2016, Proceedings",
	year="2016",
	publisher="Springer International Publishing",
	address1="Cham",
	pages="391--403",
	isbn1="978-3-319-44914-2",
	doi1="10.1007/978-3-319-44914-2\_31",
	url1="http://dx.doi.org/10.1007/978-3-319-44914-2\_31"
}


@article{chizat2016scaling,
	title={Scaling Algorithms for Unbalanced Transport Problems},
	author={Lenaic Chizat and Gabriel Peyré and Bernhard Schmitzer and François-Xavier Vialard},
	journal={arXiv:1607.05816},
	year={2016}
}


@INPROCEEDINGS{cohen2017matrix,
	title={Matrix Scaling and Balancing via Box Constrained Newton's Method and Interior Point Methods},
	author={M. B. Cohen and A. Madry and D. Tsipras and A. Vladu},
	booktitle={2017 IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS)},
	year={2017},
	volume={},
	number={},
	pages={902-913},
	keywords={Newton method;computational complexity;matrix algebra;optimisation;polynomials;Matrix balancing;box constrained Newton's method;interior point methods;interior-point method;linear system;logarithmic factors;nearly-linear time;optimal scalings;quasipolynomial;second order optimization framework;specific functions capturing matrix scaling;strictly positive matrices;Iterative methods;Laplace equations;Linear systems;Newton method;Optimization;Robustness;Newton's method;SDD solver;interior point methods;matrix balancing;matrix scaling},
	doi={10.1109/FOCS.2017.88},
	ISSN={0272-5428},
	month={Oct},
	note={\url{https://arxiv.org/abs/1704.02310}}
}


@book{conn2009introduction,
	author = {Conn, A. and Scheinberg, K. and Vicente, L.},
	title = {Introduction to Derivative-Free Optimization},
	publisher = {Society for Industrial and Applied Mathematics},
	year = {2009},
	doi = {10.1137/1.9780898718768},
	address = {},
	edition   = {},
	URL = {http://epubs.siam.org/doi/abs/10.1137/1.9780898718768}
}


@ARTICLE{courty2017optimal,
	author = {N. Courty and R. Flamary and D. Tuia and A. Rakotomamonjy},
	journal = {IEEE Transactions on Pattern Analysis \& Machine Intelligence},
	title = {Optimal Transport for Domain Adaptation},
	year = {2017},
	volume = {39},
	number = {9},
	pages = {1853-1865},
	keywords={Transportation;Probability density function;Probability distribution;Training;Feature extraction;Kernel;Data analysis},
	doi = {10.1109/TPAMI.2016.2615921},
	url = {doi.ieeecomputersociety.org/10.1109/TPAMI.2016.2615921},
	ISSN = {0162-8828},
	month={Sept.}
}


@incollection{cuturi2013sinkhorn,
	title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
	author = {Cuturi, Marco},
	booktitle = {Advances in Neural Information Processing Systems 26},
	editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
	pages = {2292--2300},
	year = {2013},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4927-sinkhorn-distances-lightspeed-computation-of-optimal-transport.pdf}
}


@InProceedings{cuturi2014fast,
	title = 	 {Fast Computation of Wasserstein Barycenters},
	author = 	 {Marco Cuturi and Arnaud Doucet},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {685--693},
	year = 	 {2014},
	editor = 	 {Eric P. Xing and Tony Jebara},
	volume = 	 {32},
	number =       {2},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/cuturi14.pdf},
	url = 	 {http://proceedings.mlr.press/v32/cuturi14.html},
	abstract = 	 {We present new algorithms to compute the mean of a set of N empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter \citepagueh2011barycenters,rabin2012, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We argue through a simple example that Wasserstein barycenters have appealing properties that differentiate them from other barycenters proposed recently, which all build on kernel smoothing and/or Bregman divergences. Two original algorithms are proposed that require the repeated computation of primal and dual optimal solutions of transport problems. However direct implementation of these algorithms is too costly as optimal transports are notoriously computationally expensive. Extending the work of \citetcuturi2013sinkhorn, we smooth both the primal and dual of the optimal transport problem to recover fast approximations of the primal and dual optimal solutions. We apply these algorithms to the visualization of perturbed images and to a clustering problem.}
}


@article{cuturi2016smoothed,
	author = {Marco Cuturi and Gabriel Peyré},
	title = {A Smoothed Dual Approach for Variational Wasserstein Problems},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {9},
	number = {1},
	pages = {320-343},
	year = {2016},
	doi = {10.1137/15M1032600}
}


@article{dang2015stochastic,
	author = {Dang, Cong D. and Lan, Guanghui},
	title = {Stochastic Block Mirror Descent Methods for Nonsmooth and Stochastic Optimization},
	journal = {SIAM J. on Optimization},
	issue_date = {2015},
	volume = {25},
	number = {2},
	month = apr,
	year = {2015},
	issn = {1052-6234},
	pages = {856--881},
	numpages = {26},
	url = {https://doi.org/10.1137/130936361},
	doi = {10.1137/130936361},
	acmid = {3068118},
	publisher = {Society for Industrial and Applied Mathematics},
	address = {Philadelphia, PA, USA},
	keywords = {62L20, 68Q25, 90C15, 90C25, block coordinate descent, metric learning, mirror descent, nonsmooth optimization, stochastic composite optimization, stochastic optimization},
}


@inproceedings{defazio2014SAGA,
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	title = {SAGA: A Fast Incremental Gradient Method with Support for Non-strongly Convex Composite Objectives},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
	series = {NIPS'14},
	year = {2014},
	location = {Montreal, Canada},
	pages = {1646--1654},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2968826.2969010},
	acmid = {2969010},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}


@incollection{dekel2015bandit,
	title = {Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff},
	author = {Dekel, Ofer and Eldan, Ronen and Koren, Tomer},
	booktitle = {Advances in Neural Information Processing Systems 28},
	editor = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
	pages = {2926--2934},
	year = {2015},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5842-bandit-smooth-convex-optimization-improving-the-bias-variance-tradeoff.pdf}
}


@article{devolder2011stochastic,
	title={Stochastic first order methods in smooth convex optimization},
	author={Olivier Devolder},
	journal={CORE Discussion Paper 2011/70},
	year={2011}
}


@Article{devolder2014first,
	author="Devolder, Olivier
	and Glineur, Fran{\c{c}}ois
	and Nesterov, Yurii",
	title="First-order methods of smooth convex optimization with inexact oracle",
	journal="Mathematical Programming",
	year="2014",
	volume="146",
	number="1",
	pages="37--75",
	abstract="We introduce the notion of inexact first-order oracle and analyze the behavior of several first-order methods of smooth convex optimization used with such an oracle. This notion of inexact oracle naturally appears in the context of smoothing techniques, Moreau--Yosida regularization, Augmented Lagrangians and many other situations. We derive complexity estimates for primal, dual and fast gradient methods, and study in particular their dependence on the accuracy of the oracle and the desired accuracy of the objective function. We observe that the superiority of fast gradient methods over the classical ones is no longer absolute when an inexact oracle is used. We prove that, contrary to simple gradient schemes, fast gradient methods must necessarily suffer from error accumulation. Finally, we show that the notion of inexact oracle allows the application of first-order methods of smooth convex optimization to solve non-smooth or weakly smooth convex problems.",
	issn="1436-4646",
	doi="10.1007/s10107-013-0677-5",
	url="http://dx.doi.org/10.1007/s10107-013-0677-5"
}


@InProceedings{dvurechensky2016primal-dual,
	Title                    = {Primal-Dual Method for Searching Equilibrium in Hierarchical Congestion Population Games},
	Author                   = {Dvurechensky, Pavel and Gasnikov, Alexander and Gasnikova, Evgenia and Matsievsky, Sergey and Rodomanov, Anton and Usik, Inna},
	Booktitle                = {Supplementary Proceedings of the 9th International Conference on Discrete Optimization and Operations Research
	and Scientific School (DOOR 2016) Vladivostok, Russia, September 19 - 23, 2016},
	Year                     = {2016},
	Pages                    = {584--595},
	Note					   = {arXiv:1606.08988}
}


@Article{dvurechensky2016stochastic,
	author="Dvurechensky, Pavel
	and Gasnikov, Alexander",
	title="Stochastic Intermediate Gradient Method for Convex Problems with Stochastic Inexact Oracle",
	journal="Journal of Optimization Theory and Applications",
	year="2016",
	volume="171",
	number="1",
	pages="121--145",
	abstract="In this paper, we introduce new methods for convex optimization problems with stochastic inexact oracle. Our first method is an extension of the Intermediate Gradient Method proposed by Devolder, Glineur and Nesterov for problems with deterministic inexact oracle. Our method can be applied to problems with composite objective function, both deterministic and stochastic inexactness of the oracle, and allows using a non-Euclidean setup. We estimate the rate of convergence in terms of the expectation of the non-optimality gap and provide a way to control the probability of large deviations from this rate. Also we introduce two modifications of this method for strongly convex problems. For the first modification, we estimate the rate of convergence for the non-optimality gap expectation and, for the second, we provide a bound for the probability of large deviations from the rate of convergence in terms of the expectation of the non-optimality gap. All the rates lead to the complexity estimates for the proposed methods, which up to a multiplicative constant coincide with the lower complexity bound for the considered class of convex composite optimization problems with stochastic inexact oracle.",
	issn="1573-2878",
	doi="10.1007/s10957-016-0999-6",
	url="http://dx.doi.org/10.1007/s10957-016-0999-6"
}


@article{dvurechensky2017parallel,
	title={Parallel algorithms and probability of large deviation for stochastic optimization problems},
	author={Pavel Dvurechensky and Alexander Gasnikov and Anastasia Lagunovskaya},
	journal={arXiv:1701.01830},
	year={2017}
}


@article{dvurechensky2017gradient,
	title={Gradient Method With Inexact Oracle for Composite Non-Convex Optimization},
	author={Pavel Dvurechensky},
	journal={arXiv:1703.09180},
	year={2017}
}


@article{dvurechensky2017adaptive,
	title={Adaptive Similar Triangles Method: a Stable Alternative to {S}inkhorn's Algorithm for Regularized Optimal Transport},
	author={Pavel Dvurechensky and Alexander Gasnikov and Sergey Omelchenko and Alexander Tiurin},
	journal={arXiv:1706.07622},
	year={2017}
}


@article{dvurechensky2017randomized,
	title={Randomized Similar Triangles Method: A Unifying Framework for Accelerated Randomized Optimization Methods (Coordinate Descent, Directional Search, Derivative-Free Method)},
	author={Pavel Dvurechensky and Alexander Gasnikov and Alexander Tiurin},
	journal={arXiv:1707.08486},
	year={2017}
}


@InProceedings{dvurechensky2018computational,
	title = 	 {Computational Optimal Transport: Complexity by Accelerated Gradient Descent Is Better Than by {S}inkhorn’s Algorithm},
	author = 	 {Dvurechensky, Pavel and Gasnikov, Alexander and Kroshnin, Alexey},
	booktitle = 	 {Proceedings of the 35th International Conference on Machine Learning},
	pages = 	 {1367--1376},
	year = 	 {2018},
	editor = 	 {Dy, Jennifer and Krause, Andreas},
	volume = 	 {80},
	series = 	 {Proceedings of Machine Learning Research},
	address1 = 	 {Stockholmsmässan, Stockholm Sweden},
	month1 = 	 {10--15 Jul},
	publisher1 = 	 {PMLR},
	pdf1 = 	 {http://proceedings.mlr.press/v80/dvurechensky18a/dvurechensky18a.pdf},
	url1 = 	 {http://proceedings.mlr.press/v80/dvurechensky18a.html},
	abstract = 	 {We analyze two algorithms for approximating the general optimal transport (OT) distance between two discrete distributions of size $n$, up to accuracy $\varepsilon$. For the first algorithm, which is based on the celebrated Sinkhorn’s algorithm, we prove the complexity bound $\widetilde{O}\left(\frac{n^2}{\varepsilon^2}\right)$ arithmetic operations ($\widetilde{O}$ hides polylogarithmic factors $(\ln n)^c$, $c>0$). For the second one, which is based on our novel Adaptive Primal-Dual Accelerated Gradient Descent (APDAGD) algorithm, we prove the complexity bound $\widetilde{O}\left(\min\left\{\frac{n^{9/4}}{\varepsilon}, \frac{n^{2}}{\varepsilon^2} \right\}\right)$ arithmetic operations. Both bounds have better dependence on $\varepsilon$ than the state-of-the-art result given by $\widetilde{O}\left(\frac{n^2}{\varepsilon^3}\right)$. Our second algorithm not only has better dependence on $\varepsilon$ in the complexity bound, but also is not specific to entropic regularization and can solve the OT problem with different regularizers.},
	note={arXiv:1802.04367}
}


@article{dvurechensky2018accelerated,
	title={An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization},
	author={Pavel Dvurechensky and Alexander Gasnikov and Eduard Gorbunov},
	journal={arXiv:1802.09022},
	year={2018}
}


@article{dvurechensky2018decentralize,
	title={Decentralize and Randomize: Faster Algorithm for {W}asserstein Barycenters},
	author={Pavel Dvurechensky and Darina Dvinskikh and Alexander Gasnikov and César A. Uribe and Angelia Nedić},
	journal={arXiv:1806.03915},
	year={2018}
}


@inproceedings{duchi2010composite,
	title = "Composite objective mirror descent",
	abstract = "We present a new method for regularized convex optimization and analyze it under both online and stochastic optimization settings. In addition to unifying previously known firstorder algorithms, such as the projected gradient method, mirror descent, and forward-backward splitting, our method yields new analysis and algorithms. We also derive specific instantiations of our method for commonly used regularization functions, such as l1, mixed norm, and trace-norm.",
	author = "Duchi, {John C.} and Shai Shalev-Shwartz and Yoram Singer and Ambuj Tewari",
	year = "2010",
	isbn = "9780982252925",
	pages = "14--26",
	booktitle = "COLT 2010 - The 23rd Conference on Learning Theory"
}


@article{duchi2015optimal,
	author    = {John C. Duchi and
	Michael I. Jordan and
	Martin J. Wainwright and
	Andre Wibisono},
	title     = {Optimal Rates for Zero-Order Convex Optimization: The Power of Two
	Function Evaluations},
	journal   = {{IEEE} Trans. Information Theory},
	volume    = {61},
	number    = {5},
	pages     = {2788--2806},
	year      = {2015},
	url       = {https://doi.org/10.1109/TIT.2015.2409256},
	doi       = {10.1109/TIT.2015.2409256},
	timestamp = {Sun, 28 May 2017 01:00:00 +0200},
	biburl    = {http://dblp.org/rec/bib/journals/tit/DuchiJWW15},
	bibsource = {dblp computer science bibliography, http://dblp.org},
	note      = {arXiv:1312.2139}
}


@inproceedings{dunner2016primal-dual,
	author = {D\"{u}nner, Celestine and Forte, Simone and Tak\'{a}\v{c}, Martin and Jaggi, Martin},
	title = {Primal-dual Rates and Certificates},
	booktitle = {Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48},
	series = {ICML'16},
	year = {2016},
	location = {New York, NY, USA},
	pages = {783--792},
	numpages = {10},
	url = {http://dl.acm.org/citation.cfm?id=3045390.3045474},
	acmid = {3045474},
	publisher = {JMLR.org},
}


@article{ebert2017construction,
	title={Construction of Non-asymptotic Confidence Sets in 2-{W}asserstein Space},
	author={Johannes Ebert and Vladimir Spokoiny and Alexandra Suvorikova},
	journal={arXiv:1703.03658},
	year={2017}
}


@book{fang1997entropy,
	author = {Fang, S.-C. and Rajasekera J. and Tsao H.-S.},
	title = {Entropy optimization and mathematical programming},
	publisher = {Kluwer’s International Series},
	year = {1997}
}


@article{fercoq2015accelerated,
	title={Accelerated, parallel, and proximal coordinate descent},
	author={Fercoq, Olivier and Richt{\'a}rik, Peter},
	journal={SIAM Journal on Optimization},
	volume={25},
	number={4},
	pages={1997--2023},
	year={2015},
	publisher={SIAM},
	note = {First appeared in arXiv:1312.5799}
}


@inproceedings{flaxman2005online,
	author = {Flaxman, Abraham D. and Kalai, Adam Tauman and McMahan, H. Brendan},
	title = {Online Convex Optimization in the Bandit Setting: Gradient Descent Without a Gradient},
	booktitle = {Proceedings of the Sixteenth Annual ACM-SIAM Symposium on Discrete Algorithms},
	series = {SODA '05},
	year = {2005},
	isbn = {0-89871-585-7},
	location = {Vancouver, British Columbia},
	pages = {385--394},
	numpages = {10},
	url = {http://dl.acm.org/citation.cfm?id=1070432.1070486},
	acmid = {1070486},
	publisher = {Society for Industrial and Applied Mathematics},
	address = {Philadelphia, PA, USA},
} 


@article{franklin1989scaling,
	title = "On the scaling of multidimensional matrices",
	journal = "Linear Algebra and its Applications",
	volume = "114",
	number = "",
	pages = "717 - 735",
	year = "1989",
	note = "Special Issue Dedicated to Alan J. Hoffman",
	issn = "0024-3795",
	doi = "http://dx.doi.org/10.1016/0024-3795(89)90490-4",
	url = "http://www.sciencedirect.com/science/article/pii/0024379589904904",
	author = "Joel Franklin and Jens Lorenz",
}


@InProceedings{frostig2015un-regularizing,
	title = 	 {Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization},
	author = 	 {Roy Frostig and Rong Ge and Sham Kakade and Aaron Sidford},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {2540--2548},
	year = 	 {2015},
	editor = 	 {Francis Bach and David Blei},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/frostig15.pdf},
	url = 	 {http://proceedings.mlr.press/v37/frostig15.html},
	abstract = 	 {We develop a family of accelerated stochastic algorithms that optimize sums of convex functions. Our algorithms improve upon the fastest running time for empirical risk minimization (ERM), and in particular linear least-squares regression, across a wide range of problem settings. To achieve this, we establish a framework, based on the classical proximal point algorithm, useful for accelerating recent fast stochastic algorithms in a black-box fashion. Empirically, we demonstrate that the resulting algorithms exhibit notions of stability that are advantageous in practice. Both in theory and in practice, the provided algorithms reap the computational benefits of adding a large strongly convex regularization term, without incurring a corresponding bias to the original ERM problem.}
}


@Article{gasnikov2015search,
	author="Gasnikov, Alexander
	and Gasnikova, Evgenia
	and Dvurechensky, Pavel
	and Ershov, Egor
	and Lagunovskaya, Anastasia",
	title="Search for the stochastic equilibria in the transport models of equilibrium flow distribution",
	journal="Proceedings of Moscow Institute of Physics and Technology",
	year="2015",
	volume="7",
	number="4",
	pages="114--128",
	note= {In Russian}
}


@Article{gasnikov2015searching,
	author="Gasnikov, Alexander
	and Dvurechensky, Pavel
	and Kamzolov, Dmitry
	and Nesterov, Yurii
	and Spokoiny, Vladimir
	and Stetsyuk, Petro
	and Suvorikova, Alexandra
	and Chernov, Alexey",
	title="Searching for equilibriums in multistage transport models",
	journal="Proceedings of Moscow Institute of Physics and Technology",
	year="2015",
	volume="7",
	number="4",
	pages="143--155",
	note= {In Russian}
}

@article{uribe2017optimal,
	title={Optimal algorithms for distributed optimization},
	author={Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
	journal={arXiv preprint arXiv:1712.00232},
	year={2017}
}

@article{lan2017communication,
	title={Communication-efficient algorithms for decentralized and stochastic optimization},
	author={Lan, Guanghui and Lee, Soomin and Zhou, Yi},
	journal={Mathematical Programming},
	pages={1--48},
	year={2017},
	publisher={Springer}
}

@Article{gasnikov2016efficient,
	author="Gasnikov, A. V.
	and Gasnikova, E. V.
	and Nesterov, Yu. E.
	and Chernov, A. V.",
	title="Efficient numerical methods for entropy-linear programming problems",
	journal="Computational Mathematics and Mathematical Physics",
	year="2016",
	volume="56",
	number="4",
	pages="514--524",
	abstract="Entropy-linear programming (ELP) problems arise in various applications. They are usually written as the maximization of entropy (minimization of minus entropy) under affine constraints. In this work, new numerical methods for solving ELP problems are proposed. Sharp estimates for the convergence rates of the proposed methods are established. The approach described applies to a broader class of minimization problems for strongly convex functionals with affine constraints.",
	issn="1555-6662",
	doi="10.1134/S0965542516040084",
	url="http://dx.doi.org/10.1134/S0965542516040084"
}


@Article{gasnikov2016evolution,
	author="Gasnikov, Alexander
	and Gasnikova, Evgenia
	and Mendel, Michail
	and Chepurchenko, Ksenia",
	title="Evolutionary derivations of entropy model for traffic demand matrix calculation",
	journal="Matematicheskoe Modelirovanie",
	year="2016",
	volume="28",
	number="4",
	pages="111--124",
	note= {In Russian}
}


@Article{gasnikov2016stochastic,
	author="Gasnikov, Alexander
	and Dvurechensky, Pavel
	and Nesterov, Yurii",
	title="Stochastic gradient methods with inexact oracle",
	journal="Proceedings of Moscow Institute of Physics and Technology",
	year="2016",
	volume="8",
	number="1",
	pages="41--91",
	note= {In Russian, first appeared in arXiv:1411.4218}
}


@Article{gasnikov2016accelerated,
	author="Gasnikov, Alexander
	and Dvurechensky, Pavel
	and Usmanova, Ilnura",
	title="On accelerated randomized methods",
	journal="Proceedings of Moscow Institute of Physics and Technology",
	year="2016",
	volume="8",
	number="2",
	pages="67--100",
	note= {In Russian, first appeared in arXiv:1508.02182}
}


@Article{gasnikov2016superposition,
	author="Gasnikov, Alexander
	and Dvurechensky, Pavel
	and Spokoiny, Vladimir
	and Stetsyuk, Petro
	and Suvorikova, Alexandra",
	title="Superposition of the balancing algorithm and the universal gradient method for search of the regularized Wasserstein brycenter and equilibria in multistage transport models",
	journal="Proceedings of Moscow Institute of Physics and Technology",
	year="2016",
	volume="8",
	number="3",
	pages="5--24",
	note= {In Russian}
}


@Article{gasnikov2016gradient-free,
	author="Gasnikov, A. V.
	and Lagunovskaya, A. A.
	and Usmanova, I. N.
	and Fedorenko, F. A.",
	title="Gradient-free proximal methods with inexact oracle for convex stochastic nonsmooth optimization problems on the simplex",
	journal="Automation and Remote Control",
	year="2016",
	month="Nov",
	day="01",
	volume="77",
	number="11",
	pages="2018--2034",
	abstract="In this paper we propose a modification of the mirror descent method for non-smooth stochastic convex optimization problems on the unit simplex. The optimization problems considered differ from the classical ones by availability of function values realizations. Our purpose is to derive the convergence rate of the method proposed and to determine the level of noise that does not significantly affect the convergence rate.",
	issn="1608-3032",
	doi="10.1134/S0005117916110114",
	url="http://dx.doi.org/10.1134/S0005117916110114",
	note="arXiv:1412.3890"
}


@article{gasnikov2018universal,
	title={Universal method for stochastic composite optimization problems},
	author={Gasnikov, Alexander Vladimirovich and Nesterov, Yu E},
	journal={Computational Mathematics and Mathematical Physics},
	volume={58},
	number={1},
	pages={48--64},
	year={2018},
	publisher={Springer Nature BV}
}

@article{gasnikov2016universal,
	title={Universal fast gradient method for stochastic composit optimization problems},
	author={Alexander Gasnikov and Yurii Nesterov},
	journal={arXiv:1604.05275},
	year={2016}
}


@Article{gasnikov2017stochastic,
	author="Gasnikov, A. V.
	and Krymova, E. A.
	and Lagunovskaya, A. A.
	and Usmanova, I. N.
	and Fedorenko, F. A.",
	title="Stochastic online optimization. Single-point and multi-point non-linear multi-armed bandits. Convex and strongly-convex case",
	journal="Automation and Remote Control",
	year="2017",
	month="Feb",
	day="01",
	volume="78",
	number="2",
	pages="224--234",
	abstract="In this paper the gradient-free modification of the mirror descent method for convex stochastic online optimization problems is proposed. The crucial assumption in the problem setting is that function realizations are observed with minor noises. The aim of this paper is to derive the convergence rate of the proposed methods and to determine a noise level which does not significantly affect the convergence rate.",
	issn="1608-3032",
	doi="10.1134/S0005117917020035",
	url="http://dx.doi.org/10.1134/S0005117917020035",
	note="arXiv:1509.01679"
}


@incollection{genevay2016stochastic,
	title = {Stochastic Optimization for Large-scale Optimal Transport},
	author = {Genevay, Aude and Cuturi, Marco and Peyr\'{e}, Gabriel and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems 29},
	editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
	pages = {3440--3448},
	year = {2016},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6566-stochastic-optimization-for-large-scale-optimal-transport.pdf}
}


@article{ghadimi2013stochastic,
	author = {Saeed Ghadimi and Guanghui Lan},
	title = {Stochastic First- and Zeroth-Order Methods for Nonconvex Stochastic Programming},
	journal = {SIAM Journal on Optimization},
	volume = {23},
	number = {4},
	pages = {2341-2368},
	year = {2013},
	doi = {10.1137/120880811},
	URL = {https://doi.org/10.1137/120880811},
	note = {arXiv:1309.5549}
}


@Article{ghadimi2016accelerated,
	author="Ghadimi, Saeed
	and Lan, Guanghui",
	title="Accelerated gradient methods for nonconvex nonlinear and stochastic programming",
	journal="Mathematical Programming",
	year="2016",
	volume="156",
	number="1",
	pages="59--99",
	abstract="In this paper, we generalize the well-known Nesterov's accelerated gradient (AG) method, originally designed for convex smooth optimization, to solve nonconvex and possibly stochastic optimization problems. We demonstrate that by properly specifying the stepsize policy, the AG method exhibits the best known rate of convergence for solving general nonconvex smooth optimization problems by using first-order information, similarly to the gradient descent method. We then consider an important class of composite optimization problems and show that the AG method can solve them uniformly, i.e., by using the same aggressive stepsize policy as in the convex case, even if the problem turns out to be nonconvex. We demonstrate that the AG method exhibits an optimal rate of convergence if the composite problem is convex, and improves the best known rate of convergence if the problem is nonconvex. Based on the AG method, we also present new nonconvex stochastic approximation methods and show that they can improve a few existing rates of convergence for nonconvex stochastic optimization. To the best of our knowledge, this is the first time that the convergence of the AG method has been established for solving nonconvex nonlinear programming in the literature.",
	issn="1436-4646",
	doi="10.1007/s10107-015-0871-8",
	url="http://dx.doi.org/10.1007/s10107-015-0871-8"
}


@Article{ghadimi2016mini-batch,
	author="Ghadimi, Saeed
	and Lan, Guanghui
	and Zhang, Hongchao",
	title="Mini-batch stochastic approximation methods for nonconvex stochastic composite optimization",
	journal="Mathematical Programming",
	year="2016",
	volume="155",
	number="1",
	pages="267--305",
	abstract="This paper considers a class of constrained stochastic composite optimization problems whose objective function is given by the summation of a differentiable (possibly nonconvex) component, together with a certain non-differentiable (but convex) component. In order to solve these problems, we propose a randomized stochastic projected gradient (RSPG) algorithm, in which proper mini-batch of samples are taken at each iteration depending on the total budget of stochastic samples allowed. The RSPG algorithm also employs a general distance function to allow taking advantage of the geometry of the feasible region. Complexity of this algorithm is established in a unified setting, which shows nearly optimal complexity of the algorithm for convex stochastic programming. A post-optimization phase is also proposed to significantly reduce the variance of the solutions returned by the algorithm. In addition, based on the RSPG algorithm, a stochastic gradient free algorithm, which only uses the stochastic zeroth-order information, has been also discussed. Some preliminary numerical results are also provided.",
	issn="1436-4646",
	doi="10.1007/s10107-014-0846-1",
	url="http://dx.doi.org/10.1007/s10107-014-0846-1",
	note = "arXiv:1308.6594"
}


@article{ghadimi2017second-order,
	title={Second-Order Methods with Cubic Regularization Under Inexact Information},
	author={Saeed Ghadimi and Han Liu and Tong Zhang},
	journal={arXiv:1710.05782},
	year={2017}
}


@book{golan1996maximum,
	author = {Golan, A. and Judge, G. and Miller, D.},
	title = {Maximum entropy econometrics: Robust estimation with limited data.},
	publisher = {Chichester, Wiley},
	year = {1996}
}


@article{goldstein2014fast,
	author = {Tom Goldstein and Brendan O'Donoghue and Simon Setzer and Richard Baraniuk},
	title = {Fast Alternating Direction Optimization Methods},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {7},
	number = {3},
	pages = {1588-1623},
	year = {2014},
	doi = {10.1137/120896219},
	URL = {         https://doi.org/10.1137/120896219   },
	eprint = {         https://doi.org/10.1137/120896219  }
}


@article{guigues2017non-asymptotic,
	author = {Vincent Guigues and Anatoli Juditsky and Arkadi Nemirovski},
	title = {Non-asymptotic confidence bounds for the optimal value of a stochastic program},
	journal = {Optimization Methods and Software},
	volume = {32},
	number = {5},
	pages = {1033-1058},
	year  = {2017},
	publisher = {Taylor & Francis},
	doi = {10.1080/10556788.2017.1350177},
	URL = { https://doi.org/10.1080/10556788.2017.1350177},
	eprint = { https://doi.org/10.1080/10556788.2017.1350177}
}


@incollection{hazan2014bandit,
	title = {Bandit Convex Optimization: Towards Tight Bounds},
	author = {Hazan, Elad and Levy, Kfir},
	booktitle = {Advances in Neural Information Processing Systems 27},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	pages = {784--792},
	year = {2014},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5377-bandit-convex-optimization-towards-tight-bounds.pdf}
}


@book{hastie2001elements,
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	added-at = {2008-05-16T16:17:42.000+0200},
	address = {New York, NY, USA},
	title = {The Elements of Statistical Learning},
	biburl = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
	keywords = {ml statistics},
	publisher = {Springer New York Inc.},
	series = {Springer Series in Statistics},
	timestamp = {2008-05-16T16:17:43.000+0200},  
	year = 2001
}


@InProceedings{ho17multilevel,
	title = 	 {Multilevel Clustering via {W}asserstein Means},
	author = 	 {Nhat Ho and XuanLong Nguyen and Mikhail Yurochkin and Hung Hai Bui and Viet Huynh and Dinh Phung},
	booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
	pages = 	 {1501--1509},
	year = 	 {2017},
	editor = 	 {Doina Precup and Yee Whye Teh},
	volume = 	 {70},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {International Convention Centre, Sydney, Australia},
	month = 	 {06--11 Aug},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v70/ho17a/ho17a.pdf},
	url = 	 {http://proceedings.mlr.press/v70/ho17a.html},
	abstract = 	 {We propose a novel approach to the problem of multilevel clustering, which aims to simultaneously partition data in each group and discover grouping patterns among groups in a potentially large hierarchically structured corpus of data. Our method involves a joint optimization formulation over several spaces of discrete probability measures, which are endowed with Wasserstein distance metrics. We propose a number of variants of this problem, which admit fast optimization algorithms, by exploiting the connection to the problem of finding Wasserstein barycenters. Consistency properties are established for the estimates of both local and global clusters. Finally, experiment results with both synthetic and real data are presented to demonstrate the flexibility and scalability of the proposed approach.}
}




@InProceedings{hu16bandit,
	title = 	 {(Bandit) Convex Optimization with Biased Noisy Gradient Oracles},
	author = 	 {Xiaowei Hu and Prashanth L.A. and András György and Csaba Szepesvari},
	booktitle = 	 {Proceedings of the 19th International Conference on Artificial Intelligence and Statistics},
	pages = 	 {819--828},
	year = 	 {2016},
	editor = 	 {Arthur Gretton and Christian C. Robert},
	volume = 	 {51},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Cadiz, Spain},
	month = 	 {09--11 May},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v51/hu16b.pdf},
	url = 	 {http://proceedings.mlr.press/v51/hu16b.html},
	abstract = 	 {A popular class of algorithms for convex optimization and online learning with bandit feedback rely on constructing noisy gradient estimates, which are then used in place of the actual gradients in appropriately adjusted first-order algorithms. Depending on the properties of the function to be optimized and the nature of “noise” in the bandit feedback, the bias and variance of gradient estimates exhibit various tradeoffs. In this paper we propose a novel framework that replaces the specific gradient estimation methods with an abstract oracle model. With the help of the new framework we unify previous works, reproducing their results in a clean and concise fashion, while, perhaps more importantly, the framework also allows us to formally show that to achieve the optimal root-n rate either the algorithms that use existing gradient estimators, or the proofs have to go beyond what exists today.}
}



@article{jakovcevic2015forward,
	author = "N. Jakovčević Stor and I. Slapničar and J.L. Barlow",
	title = "Forward stable eigenvalue decomposition of rank-one modifications of diagonal matrices",
	journal = "Linear Algebra and its Applications",
	volume = "487",
	number = "Supplement C",
	pages = "301 - 315",
	year = "2015",
	issn = "0024-3795",
	doi = "https://doi.org/10.1016/j.laa.2015.09.025",
	url = "http://www.sciencedirect.com/science/article/pii/S0024379515005406",
	keywords = "Eigenvalue decomposition, Diagonal-plus-rank-one matrix, Real symmetric matrix, Arrowhead matrix, High relative accuracy, Forward stability",
	note = "\url{https://arxiv.org/pdf/1405.7537.pdf}"
}


@incollection{jamieson2012query,
	title = {Query Complexity of Derivative-Free Optimization},
	author = {Jamieson, Kevin G and Nowak, Robert and Ben Recht},
	booktitle = {Advances in Neural Information Processing Systems 25},
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
	pages = {2672--2680},
	year = {2012},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/4509-query-complexity-of-derivative-free-optimization.pdf}
}



@article{jiang2017unified,
	title={A Unified Scheme to Accelerate Adaptive Cubic Regularization and Gradient Methods for Convex Optimization},
	author={Bo Jiang and Tianyi Lin and Shuzhong Zhang},
	journal={arXiv:1710.04788},
	year={2017}
}

@article{jin2019short,
	title={A Short Note on Concentration Inequalities for Random Vectors with SubGaussian Norm},
	author={Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M and Jordan, Michael I},
	journal={arXiv preprint arXiv:1902.03736},
	year={2019}
}

@article{juditsky2011solving,
	author = {Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
	title = {Solving Variational Inequalities with Stochastic Mirror-Prox Algorithm},
	journal = {Stochastic Systems},
	volume = {1},
	number = {1},
	pages = {17-58},
	year = {2011},
	doi = {10.1287/10-SSY011},
	URL = {https://doi.org/10.1287/10-SSY011},
	eprint = {https://doi.org/10.1287/10-SSY011},
	abstract = { In this paper we consider iterative methods for stochastic variational inequalities (s.v.i.) with monotone operators. Our basic assumption is that the operator possesses both smooth and nonsmooth components. Further, only noisy observations of the problem data are available. We develop a novel Stochastic Mirror-Prox (SMP) algorithm for solving s.v.i. and show that with the convenient stepsize strategy it attains the optimal rates of convergence with respect to the problem parameters. We apply the SMP algorithm to Stochastic composite minimization and describe particular applications to Stochastic Semidefinite Feasibility problem and deterministic Eigenvalue minimization. }
}


@incollection{juditsky2012first-order,
	author = {A. Juditsky and A. Nemirovski},
	title = {First Order Methods for Non-smooth Convex Large-scale Optimization, I: General purpose methods},
	booktitle = {Optimization for Machine Learning},
	publisher = {Cambridge, MA: MIT Press},
	editor = {Suvrit Sra, Sebastian Nowozin, Stephen Wright}, 
	pages = {121-184},
	year = {2012},
}


@article{juditsky2014deterministic,
	author = {Anatoli Juditsky and Yuri Nesterov},
	title = {Deterministic and Stochastic Primal-Dual Subgradient Algorithms for Uniformly Convex Minimization},
	journal = {Stochastic Systems},
	volume = {4},
	number = {1},
	pages = {44-80},
	year = {2014},
	doi = {10.1287/10-SSY010},
	URL = { https://doi.org/10.1287/10-SSY010},
	abstract = { We discuss non-Euclidean deterministic and stochastic algorithms for optimization problems with strongly and uniformly convex objectives. We provide accuracy bounds for the performance of these algorithms and design methods which are adaptive with respect to the parameters of strong or uniform convexity of the objective: in the case when the total number of iterations N is fixed, their accuracy coincides, up to a logarithmic in N factor with the accuracy of optimal algorithms. }
}


@article{kalantari1993rate,
	author = {Kalantari, Bahman and Khachiyan, Leonid},
	title = {On the Rate of Convergence of Deterministic and Randomized RAS Matrix Scaling Algorithms},
	journal = {Oper. Res. Lett.},
	issue_date = {December, 1993},
	volume = {14},
	number = {5},
	month = dec,
	year = {1993},
	issn = {0167-6377},
	pages = {237--244},
	numpages = {8},
	url = {http://dx.doi.org/10.1016/0167-6377(93)90087-W},
	doi = {10.1016/0167-6377(93)90087-W},
	acmid = {2308385},
	publisher = {Elsevier Science Publishers B. V.},
	address = {Amsterdam, The Netherlands, The Netherlands},
	keywords = {RAS method, diagonal matrix scaling, fully polynomial-time approximation schemes, randomized algorithms},
}


@Article{kantorovich1942translocation,
	author="Kantorovich, Leonid",
	title="On the translocation of masses",
	journal="(Doklady) Acad. Sci. URSS (N.S.)",
	year="1942",
	volume="37",
	pages="199--201",
}


@book{kapur1989maximum,
	author = {Kapur, J.},
	title = {Maximum – entropy models in science and engineering},
	publisher = {John Wiley \& Sons, Inc.},
	year = {1989}
}


@ARTICLE{kolouri2017optimal,
	author={S. Kolouri and S. R. Park and M. Thorpe and D. Slepcev and G. K. Rohde},
	journal={IEEE Signal Processing Magazine},
	title={Optimal Mass Transport: Signal processing and machine-learning applications},
	year={2017},
	volume={34},
	number={4},
	pages={43-59},
	keywords={data analysis;image processing;learning (artificial intelligence);data analysis;data distributions;generative models;geometric characteristics;machine-learning applications;mass transport-related methods;optimal mass transport;signal analysis;signal intensities;signal processing;transport-based techniques;transport-related metrics;Analytical models;Data models;Estimation;Linear programming;Morphology;Probability density function;Transportation},
	doi={10.1109/MSP.2017.2695801},
	ISSN={1053-5888},
	month={July}
}


@ARTICLE{kim1984efficient,
	author={K. Kim and Yu. Nesterov and V. Skokov and B. Cherkasskii},
	journal={Ekonomika i matematicheskie metody},
	title={Effektivnii algoritm vychisleniya proisvodnyh i ekstremalnye zadachi (Efficient algorithm for calculation of derivatives and extreme problems)},
	year={1984},
	volume={20},
	number={2},
	pages={309--318}
}


@inproceedings{kusner2015from,
	author = {Kusner, Matt J. and Sun, Yu and Kolkin, Nicholas I. and Weinberger, Kilian Q.},
	title = {From Word Embeddings to Document Distances},
	booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
	series = {ICML'15},
	year = {2015},
	location = {Lille, France},
	pages = {957--966},
	numpages = {10},
	url = {http://dl.acm.org/citation.cfm?id=3045118.3045221},
	acmid = {3045221},
	publisher = {JMLR.org},
}


@Article{lan2011primal-dual,
	author="Lan, Guanghui
	and Lu, Zhaosong
	and Monteiro, Renato D. C.",
	title="Primal-dual first-order methods with                                                                   ${O}(1/\varepsilon)$               iteration-complexity for cone programming",
	journal="Mathematical Programming",
	year="2011",
	month="Jan",
	day="01",
	volume="126",
	number="1",
	pages="1--29",
	abstract="In this paper we consider the general cone programming problem, and propose primal-dual convex (smooth and/or nonsmooth) minimization reformulations for it. We then discuss first-order methods suitable for solving these reformulations, namely, Nesterov's optimal method (Nesterov in Doklady AN SSSR 269:543--547, 1983; Math Program 103:127--152, 2005), Nesterov's smooth approximation scheme (Nesterov in Math Program 103:127--152, 2005), and Nemirovski's prox-method (Nemirovski in SIAM J Opt 15:229--251, 2005), and propose a variant of Nesterov's optimal method which has outperformed the latter one in our computational experiments. We also derive iteration-complexity bounds for these first-order methods applied to the proposed primal-dual reformulations of the cone programming problem. The performance of these methods is then compared using a set of randomly generated linear programming and semidefinite programming instances. We also compare the approach based on the variant of Nesterov's optimal method with the low-rank method proposed by Burer and Monteiro (Math Program Ser B 95:329--357, 2003; Math Program 103:427--444, 2005) for solving a set of randomly generated SDP instances.",
	issn="1436-4646",
	doi="10.1007/s10107-008-0261-6",
	url="https://doi.org/10.1007/s10107-008-0261-6"
}




@Article{lan2012optimal,
	author="Lan, Guanghui",
	title="An optimal method for stochastic composite optimization",
	journal="Mathematical Programming",
	year="2012",
	month="Jun",
	day="01",
	volume="133",
	number="1",
	pages="365--397",
	abstract="This paper considers an important class of convex programming (CP) problems, namely, the stochastic composite optimization (SCO), whose objective function is given by the summation of general nonsmooth and smooth stochastic components. Since SCO covers non-smooth, smooth and stochastic CP as certain special cases, a valid lower bound on the rate of convergence for solving these problems is known from the classic complexity theory of convex programming. Note however that the optimization algorithms that can achieve this lower bound had never been developed. In this paper, we show that the simple mirror-descent stochastic approximation method exhibits the best-known rate of convergence for solving these problems. Our major contribution is to introduce the accelerated stochastic approximation (AC-SA) algorithm based on Nesterov's optimal method for smooth CP (Nesterov in Doklady AN SSSR 269:543--547, 1983; Nesterov in Math Program 103:127--152, 2005), and show that the AC-SA algorithm can achieve the aforementioned lower bound on the rate of convergence for SCO. To the best of our knowledge, it is also the first universally optimal algorithm in the literature for solving non-smooth, smooth and stochastic CP problems. We illustrate the significant advantages of the AC-SA algorithm over existing methods in the context of solving a special but broad class of stochastic programming problems.",
	issn="1436-4646",
	url="https://doi.org/10.1007/s10107-010-0434-y",
	note = "Firs appeared in June 2008"
}


@Article{lan2015generalized,
	author="Ghadimi, Saeed and Lan, Guanghui and Zhang, Hongchao",
	title="Generalized Uniformly Optimal Methods for Nonlinear Programming",
	journal="arXiv:1508.07384",
	year="2015",
	url="https://arxiv.org/abs/1508.07384"
}


@article{lan2016algorithms,
	title={Algorithms for stochastic optimization with expectation constraints},
	author={Guanghui Lan and Zhiqiang Zhou},
	journal={arXiv:1604.03887},
	year={2016}
}


@Article{lan2016gradient,
	author="Lan, Guanghui",
	title="Gradient sliding for composite optimization",
	journal="Mathematical Programming",
	year="2016",
	month="Sep",
	day="01",
	volume="159",
	number="1",
	pages="201--235",
	issn="1436-4646",
	doi="10.1007/s10107-015-0955-5",
	url="https://doi.org/10.1007/s10107-015-0955-5"
}


@Article{lan2017optimal,
	author="Lan, Guanghui
	and Zhou, Yi",
	title="An optimal randomized incremental gradient method",
	journal="Mathematical Programming",
	year="2017",
	month="Jun",
	day="28",
	issn="1436-4646",
	doi="10.1007/s10107-017-1173-0",
	url="http://dx.doi.org/10.1007/s10107-017-1173-0"
}


@inproceedings{lee2013efficient,
	author = {Lee, Yin Tat and Sidford, Aaron},
	title = {Efficient Accelerated Coordinate Descent Methods and Faster Algorithms for Solving Linear Systems},
	booktitle = {Proceedings of the 2013 IEEE 54th Annual Symposium on Foundations of Computer Science},
	series = {FOCS '13},
	year = {2013},
	isbn = {978-0-7695-5135-7},
	pages = {147--156},
	numpages = {10},
	url = {http://dx.doi.org/10.1109/FOCS.2013.24},
	doi = {10.1109/FOCS.2013.24},
	acmid = {2570585},
	publisher = {IEEE Computer Society},
	address = {Washington, DC, USA},
	keywords = {convex optimization, coordinate descent, Kaczmarz method, symmetric diagonally dominant matrix},
	note = {First appeared in arXiv:1305.1922}
}


@INPROCEEDINGS{lee2014path,
	author={Y. T. Lee and A. Sidford},
	booktitle={2014 IEEE 55th Annual Symposium on Foundations of Computer Science},
	title={Path Finding Methods for Linear Programming: Solving Linear Programs in $\tilde{O}(\sqrt{\text{rank}})$ Iterations and Faster Algorithms for Maximum Flow},
	year={2014},
	volume={},
	number={},
	pages={424-433},
	keywords={computational complexity;directed graphs;linear programming;linear systems;matrix algebra;bit complexity;constraint matrix;dense directed unit capacity graphs;linear program formulation;linear systems;linear time computation;path-finding methods;polynomial time;Approximation methods;Convergence;Laplace equations;Linear programming;Linear systems;Polynomials;Standards;interior point;linear program;maximum flow},
	doi={10.1109/FOCS.2014.52},
	ISSN={0272-5428},
	month={Oct},}


@article{liang2014lzeroth-order,
	title={On Zeroth-Order Stochastic Convex Optimization via Random Walks},
	author={Tengyuan Liang and Hariharan Narayanan and Alexander Rakhlin},
	journal={arXiv:1402.2667},
	year={2014}
}


@Article{li2016inexact,
	author="Li, Jueyou
	and Wu, Zhiyou
	and Wu, Changzhi
	and Long, Qiang
	and Wang, Xiangyu",
	title="An Inexact Dual Fast Gradient-Projection Method for Separable Convex Optimization with Linear Coupled Constraints",
	journal="Journal of Optimization Theory and Applications",
	year="2016",
	volume="168",
	number="1",
	pages="153--171",
	abstract="In this paper, a class of separable convex optimization problems with linear coupled constraints is studied. According to the Lagrangian duality, the linear coupled constraints are appended to the objective function. Then, a fast gradient-projection method is introduced to update the Lagrangian multiplier, and an inexact solution method is proposed to solve the inner problems. The advantage of our proposed method is that the inner problems can be solved in an inexact and parallel manner. The established convergence results show that our proposed algorithm still achieves optimal convergence rate even though the inner problems are solved inexactly. Finally, several numerical experiments are presented to illustrate the efficiency and effectiveness of our proposed algorithm.",
	issn="1573-2878",
	doi="10.1007/s10957-015-0757-1",
	url="http://dx.doi.org/10.1007/s10957-015-0757-1"
}


@incollection{lin2014accelerated,
	title = {An Accelerated Proximal Coordinate Gradient Method},
	author = {Lin, Qihang and Lu, Zhaosong and Xiao, Lin},
	booktitle = {Advances in Neural Information Processing Systems 27},
	editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
	pages = {3059--3067},
	year = {2014},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/5356-an-accelerated-proximal-coordinate-gradient-method.pdf},
	note = {First appeared in arXiv:1407.1296}
}


@inproceedings{lin2015universal,
	author = {Lin, Hongzhou and Mairal, Julien and Harchaoui, Zaid},
	title = {A Universal Catalyst for First-order Optimization},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
	series = {NIPS'15},
	year = {2015},
	location = {Montreal, Canada},
	pages = {3384--3392},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2969442.2969617},
	acmid = {2969617},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}


@Article{malitsky2016first-order,
	author="Yura Malitsky and Thomas Pock",
	title="A first-order primal-dual algorithm with linesearch",
	journal="arXiv:1608.08883",
	year="2016",
	url="https://arxiv.org/abs/1608.08883"
}


@article{mordukhovich2007coderivative,
	author = {Boris S.  Mordukhovich and Jiri V.  Outrata},
	title = {Coderivative Analysis of Quasi‐variational Inequalities with Applications to Stability and Optimization},
	journal = {SIAM Journal on Optimization},
	volume = {18},
	number = {2},
	pages = {389-412},
	year = {2007},
	doi = {10.1137/060665609},
	URL = {https://doi.org/10.1137/060665609}
}


@Article{monge1781memoire,
	Title                    = {M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
	Author                   = {Monge, Gaspard},
	Journal                  = {Histoire de l'Acad{\'e}mie Royale des Sciences de Paris},
	Year                     = {1781}
}


@article{nedic2014stochastic,
	author = {Angelia Nedic and Soomin Lee},
	title = {On Stochastic Subgradient Mirror-Descent Algorithm with Weighted Averaging},
	journal = {SIAM Journal on Optimization},
	volume = {24},
	number = {1},
	pages = {84-107},
	year = {2014},
	doi = {10.1137/120894464},
	URL = {         https://doi.org/10.1137/120894464},
	eprint = {         https://doi.org/10.1137/120894464  }
}


@article{nemirovskii1979efficient,
	title = "Efficient  methods  for  large-scale  convex  optimization  problems",
	author = "A.S. Nemirovskii",
	journal = "Ekonomika i Matematicheskie Metody",
	volume = "15",
	year = "1979",
	note = "In Russian"
}


@article{nemirovskii1985optimal,
	title = "Optimal methods of smooth convex minimization",
	author = "A.S. Nemirovskii and Yu.E. Nesterov",
	journal = "USSR Computational Mathematics and Mathematical Physics",
	volume = "25",
	number = "2",
	pages = "21 - 30",
	year = "1985",
	issn = "0041-5553",
	doi = "https://doi.org/10.1016/0041-5553(85)90100-4",
	url = "http://www.sciencedirect.com/science/article/pii/0041555385901004"
}


@book{nemirovsky1983problem,
	title={Problem Complexity and Method Efficiency in Optimization},
	author={Nemirovsky, A.S. and Yudin, D.B.},
	year={1983},
	publisher={J. Wiley \& Sons, New York}
}


@article{nemirovski2009robust,
	author = {A. Nemirovski and A. Juditsky and G. Lan and A. Shapiro},
	title = {Robust Stochastic Approximation Approach to Stochastic Programming},
	journal = {SIAM Journal on Optimization},
	volume = {19},
	number = {4},
	pages = {1574--1609},
	year = {2009},
	doi = {10.1137/070704277},
	URL = {https://doi.org/10.1137/070704277},
}


@Article{nesterov1983method,
	title={A method of solving a convex programming problem with convergence rate $O(1/k^2)$},
	author={Nesterov, Yurii},
	journal={Soviet Mathematics Doklady},
	volume={27},
	number={2},
	pages={372--376},
	year={1983}
}


@book{nesterov2004introduction,
	author = {Nesterov, Yurii},
	title = {Introductory Lectures on Convex Optimization: a basic course},
	publisher = {Kluwer Academic Publishers, Massachusetts},
	year = {2004}
}

@inproceedings{scaman2017optimal,
	title={Optimal algorithms for smooth and strongly convex distributed optimization in networks},
	author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Lee, Yin Tat and Massouli{\'e}, Laurent},
	booktitle={Proceedings of the 34th International Conference on Machine Learning-Volume 70},
	pages={3027--3036},
	year={2017},
	organization={JMLR. org}
}

@Article{nesterov2005smooth,
	author="Nesterov, Yurii",
	title="Smooth minimization of non-smooth functions",
	journal="Mathematical Programming",
	year="2005",
	volume="103",
	number="1",
	pages="127--152",
	abstract="In this paper we propose a new approach for constructing efficient schemes for non-smooth convex optimization. It is based on a special smoothing technique, which can be applied to functions with explicit max-structure. Our approach can be considered as an alternative to black-box minimization. From the viewpoint of efficiency estimates, we manage to improve the traditional bounds on the number of iterations of the gradient schemes from                                    keeping basically the complexity of each iteration unchanged.",
	issn="1436-4646",
	doi="10.1007/s10107-004-0552-5",
	url="http://dx.doi.org/10.1007/s10107-004-0552-5"
}


@Article{nesterov2006cubic,
	author="Nesterov, Yurii
	and Polyak, Boris",
	title="Cubic regularization of Newton method and its global performance",
	journal="Mathematical Programming",
	year="2006",
	volume="108",
	number="1",
	pages="177--205",
	abstract="In this paper, we provide theoretical analysis for a cubic regularization of Newton method as applied to unconstrained minimization problem. For this scheme, we prove general local convergence results. However, the main contribution of the paper is related to global worst-case complexity bounds for different problem classes including some nonconvex cases. It is shown that the search direction can be computed by standard linear algebra technique.",
	issn="1436-4646",
	doi="10.1007/s10107-006-0706-8",
	url="http://dx.doi.org/10.1007/s10107-006-0706-8"
}


@techreport{nesterov2006cubicconstr,
	author       = {Nesterov, Yurii}, 
	title        = {Cubic Regularization of Newton's Method for Convex Problems with Constraints},
	institution  = {CORE UCL},
	year         = 2006,
	url        	 = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=921825},
	note         = {CORE Discussion Paper 2006/39}
}


@Article{nesterov2008accelerating,
	author="Nesterov, Yu.",
	title="Accelerating the cubic regularization of Newton's method on convex problems",
	journal="Mathematical Programming",
	year="2008",
	month="Mar",
	day="01",
	volume="112",
	number="1",
	pages="159--181",
	abstract="In this paper we propose an accelerated version of the cubic regularization of Newton's method (Nesterov and Polyak, in Math Program 108(1): 177--205, 2006). The original version, used for minimizing a convex function with Lipschitz-continuous Hessian, guarantees a global rate of convergence of order                                                                           {\$}{\$}O{\backslash}big({\{}1 {\backslash}over k^2{\}}{\backslash}big){\$}{\$}                , where k is the iteration counter. Our modified version converges for the same problem class with order                                                                           {\$}{\$}O{\backslash}big({\{}1 {\backslash}over k^3{\}}{\backslash}big){\$}{\$}                , keeping the complexity of each iteration unchanged. We study the complexity of both schemes on different classes of convex problems. In particular, we argue that for the second-order schemes, the class of non-degenerate problems is different from the standard class.",
	issn="1436-4646",
	doi="10.1007/s10107-006-0089-x",
	url="https://doi.org/10.1007/s10107-006-0089-x"
}


@Article{nesterov2009primal-dual,
	author="Nesterov, Yurii",
	title="Primal-dual subgradient methods for convex problems",
	journal="Mathematical Programming",
	year="2009",
	month="Aug",
	day="01",
	volume="120",
	number="1",
	pages="221--259",
	abstract="In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds.",
	issn="1436-4646",
	doi="10.1007/s10107-007-0149-x",
	url="https://doi.org/10.1007/s10107-007-0149-x",
	note="First appeared in 2005 as CORE discussion paper 2005/67"
}


@book{nesterov2010introduction,
	author = {Yurii Nesterov},
	title = {Introduction to Convex Optimization},
	publisher = {Moscow, MCCME},
	year = {2010}
}



@article{nesterov2012efficiency,
	author = {Yurii Nesterov},
	title = {Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems},
	journal = {SIAM Journal on Optimization},
	volume = {22},
	number = {2},
	pages = {341-362},
	year = {2012},
	doi = {10.1137/100802001},
	URL = {https://doi.org/10.1137/100802001},
	note = {First appeared in 2010 as CORE discussion paper 2010/2}
}


@article{nesterov2013gradient,
	title={Gradient methods for minimizing composite functions},
	author={Nesterov, Yurii},
	journal={Mathematical Programming},
	volume={140},
	number={1},
	pages={125--161},
	year={2013},
	publisher={Springer},
	note = {First appeared in 2007 as CORE discussion paper 2007/76}
}


@Article{nesterov2014subgradient,
	author="Nesterov, Yu.",
	title="Subgradient methods for huge-scale optimization problems",
	journal="Mathematical Programming",
	year="2014",
	month="Aug",
	day="01",
	volume="146",
	number="1",
	pages="275--297",
	abstract="We consider a new class of huge-scale problems, the problems with sparse subgradients. The most important functions of this type are piece-wise linear. For optimization problems with uniform sparsity of corresponding linear operators, we suggest a very efficient implementation of subgradient iterations, which total cost depends logarithmically in the dimension. This technique is based on a recursive update of the results of matrix/vector products and the values of symmetric functions. It works well, for example, for matrices with few nonzero diagonals and for max-type functions. We show that the updating technique can be efficiently coupled with the simplest subgradient methods, the unconstrained minimization method by B.Polyak, and the constrained minimization scheme by N.Shor. Similar results can be obtained for a new nonsmooth random variant of a coordinate descent scheme. We present also the promising results of preliminary computational experiments.",
	issn="1436-4646",
	doi="10.1007/s10107-013-0686-4",
	url="https://doi.org/10.1007/s10107-013-0686-4",
	note = "First appeared in 2012."
}


@article{nesterov2014primal-dual,
	author = {Yu. Nesterov and S. Shpirko},
	title = {Primal-Dual Subgradient Method for Huge-Scale Linear Conic Problems},
	journal = {SIAM Journal on Optimization},
	volume = {24},
	number = {3},
	pages = {1444-1457},
	year = {2014},
	doi = {10.1137/130929345},
	URL = {        https://doi.org/10.1137/130929345 },
	eprint = {         https://doi.org/10.1137/130929345}
}


@misc{nesterov2015new,
	author="Nesterov, Yurii",
	title="New primal-dual subgradient methods for convex problems with functional constraints",
	year="2015",
	note="http://lear.inrialpes.fr/workshop/osl2015/slides/osl2015\_yurii.pdf"
}


@Article{nesterov2015universal,
	author="Nesterov, Yurii",
	title="Universal gradient methods for convex optimization problems",
	journal="Mathematical Programming",
	year="2015",
	volume="152",
	number="1",
	pages="381--404",
	abstract="In this paper, we present new methods for black-box convex minimization. They do not need to know in advance the actual level of smoothness of the objective function. Their only essential input parameter is the required accuracy of the solution. At the same time, for each particular problem class they automatically ensure the best possible rate of convergence. We confirm our theoretical results by encouraging numerical experiments, which demonstrate that the fast rate of convergence, typical for the smooth optimization problems, sometimes can be achieved even on nonsmooth problem instances.",
	issn="1436-4646",
	doi="10.1007/s10107-014-0790-0",
	url="http://dx.doi.org/10.1007/s10107-014-0790-0"
}


@misc{nesterov2015subgradient ,
	author="Nesterov, Yurii",
	title="Subgradient methods for convex functions with nonstandard growth properties",
	year="2016",
	note="http://www.mathnet.ru:8080/PresentFiles/16179/growthbm\_nesterov.pdf"
}


@article{nesterov2017random,
	author = {Nesterov, Yurii and Spokoiny, Vladimir},
	title = {Random Gradient-Free Minimization of Convex Functions},
	journal = {Found. Comput. Math.},
	issue_date = {April     2017},
	volume = {17},
	number = {2},
	month = apr,
	year = {2017},
	issn = {1615-3375},
	pages = {527--566},
	numpages = {40},
	url = {https://doi.org/10.1007/s10208-015-9296-2},
	doi = {10.1007/s10208-015-9296-2},
	acmid = {3075511},
	publisher = {Springer-Verlag New York, Inc.},
	address = {Secaucus, NJ, USA},
	keywords = {0C47, 68Q25, 90C25, Complexity bounds, Convex optimization, Derivative-free methods, Random methods, Stochastic optimization},
	note = {First appeared in 2011 as CORE discussion paper 2011/16}
}


@article{nesterov2017efficiency,
	author = {Yurii Nesterov and Sebastian U. Stich},
	title = {Efficiency of the Accelerated Coordinate Descent Method on Structured Optimization Problems},
	journal = {SIAM Journal on Optimization},
	volume = {27},
	number = {1},
	pages = {110-123},
	year = {2017},
	doi = {10.1137/16M1060182},
	URL = {https://doi.org/10.1137/16M1060182},
	note = {First presented in May 2015 \url{http://www.mathnet.ru:8080/PresentFiles/11909/7_nesterov.pdf}}
}


@article{nguyen2017smoothing,
	title={Smoothing technique for nonsmooth composite minimization with linear operator},
	author={Quang Van Nguyen and Olivier Fercoq and Volkan Cevher},
	journal={arXiv:1706.05837},
	year={2017}
}


@Inbook{olfati-saber2006belief,
	author="Olfati-Saber, Reza
	and Franco, Elisa
	and Frazzoli, Emilio
	and Shamma, Je. S.",
	editor="Antsaklis, Panos J.
	and Tabuada, Paulo",
	title="Belief Consensus and Distributed Hypothesis Testing in Sensor Networks",
	bookTitle="Networked Embedded Sensing and Control: Workshop NESC'05: University of Notre Dame, USA October 2005 Proceedings",
	year="2006",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="169--182",
	abstract="In this paper, we address distributed hypothesis testing (DHT) in sensor networks and Bayesian networks using the average-consensus algorithm of Olfati-Saber {\&} Murray. As a byproduct, we obtain a novel belief propagation algorithm called Belief Consensus. This algorithm works for connected networks with loops and arbitrary degree sequence. Belief consensus allows distributed computation of products of n beliefs (or conditional probabilities) that belong to n different nodes of a network. This capability enables distributed hypothesis testing for a broad variety of applications. We show that this belief propagation admits a Lyapunov function that quantifies the collective disbelief in the network. Belief consensus benefits from scalability, robustness to link failures, convergence under variable topology, asynchronous features of average-consensus algorithm. Some connections between small-word networks and speed of convergence of belief consensus are discussed. A detailed example is provided for distributed detection of multi-target formations in a sensor network. The entire network is capable of reaching a common set of beliefs associated with correctness of different hypotheses. We demonstrate that our DHT algorithm successfully identifies a test formation in a network of sensors with self-constructed statistical models.",
	isbn="978-3-540-32847-6",
	doi="10.1007/11533382_11",
	url="https://doi.org/10.1007/11533382_11"
}


@article{ouyang2015accelerated,
	author = {Yuyuan Ouyang and Yunmei Chen and Guanghui Lan and Eduardo Pasiliao, Jr.},
	title = {An Accelerated Linearized Alternating Direction Method of Multipliers},
	journal = {SIAM Journal on Imaging Sciences},
	volume = {8},
	number = {1},
	pages = {644-681},
	year = {2015},
	doi = {10.1137/14095697X},
	URL = {        https://doi.org/10.1137/14095697X},
	eprint = {         https://doi.org/10.1137/14095697X}
}


@article{panaretos2016amplitude,
	title = "Amplitude and phase variation of point processes",
	author = "Panaretos, Victor M. and Zemel, Yoav",
	doi = "10.1214/15-AOS1387",
	fjournal = "The Annals of Statistics",
	journal = "Ann. Statist.",
	month = "04",
	number = "2",
	pages = "771--812",
	publisher = "The Institute of Mathematical Statistics",
	url = "https://doi.org/10.1214/15-AOS1387",
	volume = "44",
	year = "2016"
}


@INPROCEEDINGS{patrascu2015rate,
	author={A. Patrascu and I. Necoara and R. Findeisen},
	booktitle={2015 54th IEEE Conference on Decision and Control (CDC)},
	title={Rate of convergence analysis of a dual fast gradient method for general convex optimization},
	year={2015},
	pages={3311--3316},
	keywords={gradient methods;mobile robots;optimisation;predictive control;Lagrangian relaxation;MPC;average primal sequence;convergence analysis;convex constraints;dual fast gradient algorithm;dual fast gradient method;embedded model predictive control;general constrained convex problems;general convex optimization;generated approximate primal solutions;iteration complexity;iteration complexity analysis;primal iterate sequence;self balancing robot;sublinear estimates;Algorithm design and analysis;Approximation algorithms;Complexity theory;Convergence;Convex functions;Gradient methods},
	doi={10.1109/CDC.2015.7402717},
	month={Dec}
}


@INPROCEEDINGS{pele2009fast,
	author={O. Pele and M. Werman},
	booktitle={2009 IEEE 12th International Conference on Computer Vision},
	title={Fast and robust Earth Mover's Distances},
	year={2009},
	volume={},
	number={},
	pages={460-467},
	keywords={edge detection;flow-network;histograms;outlier noise;quantization effects;robust earth mover's distances;thresholded ground distances;Computer vision;Costs;Earth;Histograms;Humans;Image databases;Image edge detection;Image retrieval;Quantization;Robustness},
	doi={10.1109/ICCV.2009.5459199},
	ISSN={1550-5499},
	month={Sept},}


@article{pitie2007automated,
	title = "Automated colour grading using colour distribution transfer",
	author = "Francois Piti\'e and Anil C. Kokaram and Rozenn Dahyot",
	journal = "Computer Vision and Image Understanding",
	volume = "107",
	number = "1",
	pages = "123 - 137",
	year = "2007",
	note = "Special issue on color image processing",
	issn = "1077-3142",
	doi = "https://doi.org/10.1016/j.cviu.2006.11.011",
	url = "http://www.sciencedirect.com/science/article/pii/S1077314206002189",
	keywords = "Colour grading, Colour transfer, Re-colouring, Distribution transfer"
}


@article{polyak1963gradient,
	title = "Gradient methods for the minimisation of functionals",
	journal = "USSR Computational Mathematics and Mathematical Physics",
	volume = "3",
	number = "4",
	pages = "864 - 878",
	year = "1963",
	note = "",
	issn = "0041-5553",
	doi = "http://dx.doi.org/10.1016/0041-5553(63)90382-3",
	url = "http://www.sciencedirect.com/science/article/pii/0041555363903823",
	author = "Polyak, Boris",
	
}


@article{polyak1967existence,
	title={Existence theorems and convergence of minimizing sequences in extremum problems with restrictions},
	author={Polyak, Boris},
	journal={Soviet Mathematics Doklady},
	volume={7},
	pages={72-75},
	year={1967}
}


@article{polyak1967general,
	title={A general method of solving extremum problems},
	author={Polyak, Boris},
	journal={Soviet Mathematics Doklady},
	volume={8},
	number={3},
	pages={593--597},
	year={1967}
}


@book{polyak1987introduction,
	author = {Polyak, Boris},
	title = {Introduction to Optimization},
	publisher = {New York, Optimization Software},
	year = {1987}
}

@article{prigozhin1996variational, 
	author={Prigozhin, Leonid}, 
	title={Variational model of sandpile growth}, 
	volume={7}, 
	DOI={10.1017/S0956792500002321}, 
	number={3}, 
	journal={European Journal of Applied Mathematics}, 
	publisher={Cambridge University Press}, 
	year={1996}, 
	pages={225–235}
}


@Article{protasov1996algorithms,
	author="Protasov, V. Yu.",
	title="Algorithms for approximate calculation of the minimum of a convex function from its values",
	journal="Mathematical Notes",
	year="1996",
	month="Jan",
	day="01",
	volume="59",
	number="1",
	pages="69--74",
	abstract="The paper deals with a numerical minimization problem for a convex function defined on a convexn-dimensional domain and continuous (but not necessarily smooth). The values of the function can be calculated at any given point. It is required to find the minimum with desired accuracy. A new algorithm for solving this problem is presented, whose computational complexity asn {\textrightarrow} ∞ is considerably less than that of similar algorithms known to the author. In fact, the complexity is improved fromCn7 ln2(n+1) [4] toCn2 ln(n+1).",
	issn="1573-8876",
	doi="10.1007/BF02312467",
	url="https://doi.org/10.1007/BF02312467"
}




@article{rosenbrock1960automatic,
	author = {Rosenbrock, H. H.},
	title = {An Automatic Method for Finding the Greatest or Least Value of a Function},
	journal = {The Computer Journal},
	volume = {3},
	number = {3},
	pages = {175-184},
	year = {1960},
	doi = {10.1093/comjnl/3.3.175},
	URL = {http://dx.doi.org/10.1093/comjnl/3.3.175}
}


@article{roulet2017sharpness,
	title={Sharpness, Restart and Acceleration},
	author={Roulet, Vincent and d'Aspremont, Alexandre},
	journal={arXiv:1702.03828},
	year={2017}
}


@article{rubner2000earth,
	title={The earth mover's distance as a metric for image retrieval},
	author={Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J},
	journal={International journal of computer vision},
	volume={40},
	number={2},
	pages={99--121},
	year={2000},
	publisher={Springer}
}




@InProceedings{saha2011improved,
	title = 	 {Improved Regret Guarantees for Online Smooth Convex Optimization with Bandit Feedback},
	author = 	 {Ankan Saha and Ambuj Tewari},
	booktitle = 	 {Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {636--642},
	year = 	 {2011},
	editor = 	 {Geoffrey Gordon and David Dunson and Miroslav Dudík},
	volume = 	 {15},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Fort Lauderdale, FL, USA},
	month = 	 {11--13 Apr},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v15/saha11a/saha11a.pdf},
	url = 	 {http://proceedings.mlr.press/v15/saha11a.html},
	abstract = 	 {The study of online convex optimization in the bandit setting was initiated by Kleinberg (2004) and Flaxman et al. (2005). Such a setting models a decision maker that has to make decisions in the face of adversarially chosen convex loss functions. Moreover, the only information the decision maker receives are the losses. The identity of the loss functions themselves is not revealed. In this setting, we reduce the gap between the best known lower and upper bounds for the class of smooth convex functions, i.e. convex functions with a Lipschitz continuous gradient. Building upon existing work on self-concordant regularizers and one-point gradient estimation, we give the first algorithm whose expected regret, ignoring constant and logarithmic factors, is O(T^2/3). [pdf]}
}



@ARTICLE{sandler2011nonnegative,
	author={R. Sandler and M. Lindenbaum},
	journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
	title={Nonnegative Matrix Factorization with Earth Mover's Distance Metric for Image Analysis},
	year={2011},
	volume={33},
	number={8},
	pages={1590-1602},
	keywords={image classification;image segmentation;image texture;linear programming;matrix decomposition;computer vision applications;data matrix;earth mover distance metric;face recognition;image analysis;image segmentation;linear programming methods;nonnegative matrix factorization;texture classification;Approximation algorithms;Earth;Histograms;Image segmentation;Linear programming;Measurement;Pixel;Nonnegative matrix factorization;earth mover's distance;image segmentation.},
	doi={10.1109/TPAMI.2011.18},
	ISSN={0162-8828},
	month={Aug},}


@article{schmitzer2016stabilized,
	title={Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems},
	author={Bernhard Schmitzer},
	journal={arXiv:1610.06519},
	year={2016}
}


@article {sinkhorn1974diagonal,
	AUTHOR = {Sinkhorn, Richard},
	TITLE = {Diagonal equivalence to matrices with prescribed row and
	column sums. {II}},
	JOURNAL = {Proc. Amer. Math. Soc.},
	FJOURNAL = {Proceedings of the American Mathematical Society},
	VOLUME = {45},
	YEAR = {1974},
	PAGES = {195--198},
	ISSN = {0002-9939},
	MRCLASS = {15A21},
	MRNUMBER = {0357434},
	MRREVIEWER = {Donald W. Robinson},
	DOI = {10.2307/2040061},
	URL = {http://dx.doi.org/10.2307/2040061},
}


@InProceedings{shalev-shwartz2014accelerated,
	title = 	 {Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized Loss Minimization},
	author = 	 {Shai Shalev-Shwartz and Tong Zhang},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {64--72},
	year = 	 {2014},
	editor = 	 {Eric P. Xing and Tony Jebara},
	volume = 	 "32",
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/shalev-shwartz14.pdf},
	url = 	 {http://proceedings.mlr.press/v32/shalev-shwartz14.html},
	abstract = 	 {We introduce a proximal version of the stochastic dual coordinate ascent method and show how to accelerate the method using an inner-outer iteration procedure. We analyze the runtime of the framework and obtain rates that improve state-of-the-art results for various key machine learning optimization problems including SVM,   logistic regression, ridge regression, Lasso, and multiclass SVM. Experiments validate our theoretical findings.},
	note = {First appeared in arXiv:1309.2375}
}


@InProceedings{shamir2013complexity,
	title = 	 {On the Complexity of Bandit and Derivative-Free Stochastic Convex Optimization},
	author = 	 {Ohad Shamir},
	booktitle = 	 {Proceedings of the 26th Annual Conference on Learning Theory},
	pages = 	 {3--24},
	year = 	 {2013},
	editor = 	 {Shai Shalev-Shwartz and Ingo Steinwart},
	volume = 	 {30},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Princeton, NJ, USA},
	month = 	 {12--14 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v30/Shamir13.pdf},
	url = 	 {http://proceedings.mlr.press/v30/Shamir13.html},
	abstract = 	 {The problem of stochastic convex optimization with bandit feedback (in the learning community) or without knowledge of gradients (in the optimization community) has received much attention in recent years, in the form of algorithms and performance upper bounds. However, much less is known about the inherent complexity of these problems, and there are few lower bounds in the literature, especially for nonlinear functions. In this paper, we investigate the attainable error/regret in the bandit and derivative-free settings, as a function of the dimension d and the available number of queries T. We provide a precise characterization of the attainable performance for strongly-convex and smooth functions, which also imply a non-trivial lower bound for more general problems. Moreover, we prove that in both the bandit and derivative-free setting, the required number of queries must scale at least quadratically with the dimension. Finally, we show that on the natural class of quadratic functions, it is possible to obtain a “fast” O(1/T) error rate in terms of T, under mild assumptions, even without having access to gradients. To the best of our knowledge, this is the first such rate in a derivative-free stochastic setting, and holds despite previous results which seem to imply the contrary.}
}


@article{shamir2017optimal,
	author    = {Ohad Shamir},
	title     = {An Optimal Algorithm for Bandit and Zero-Order Convex Optimization
	with Two-Point Feedback},
	journal   = {Journal of Machine Learning Research},
	volume    = {18},
	pages     = {52:1--52:11},
	year      = {2017},
	url       = {http://jmlr.org/papers/v18/papers/v18/16-632.html},
	timestamp = {Fri, 18 Aug 2017 14:48:36 +0200},
	biburl    = {http://dblp.org/rec/bib/journals/jmlr/Shamir17},
	bibsource = {dblp computer science bibliography, http://dblp.org},
	note = {First appeared in arXiv:1507.08752}
}


@book{shapiro2014lectures,
	author = {Shapiro, A. and Dentcheva, D. and Ruszczynski, A.},
	title = {Lectures on Stochastic Programming},
	publisher = {Society for Industrial and Applied Mathematics},
	year = {2009},
	doi = {10.1137/1.9780898718751},
	address = {},
	edition   = {},
	URL = {http://epubs.siam.org/doi/abs/10.1137/1.9780898718751},
	eprint = {http://epubs.siam.org/doi/pdf/10.1137/1.9780898718751}
}


@Article{shor1967generalized,
	author="Shor, N. Z.",
	title="Generalized gradient descent with application to block programming",
	journal="Kibernetika",
	year="1967",
	volume="3",
	number="3",
	pages="53--55"
}


@Article{shvetsov2003mathematical,
	author="Shvetsov, V. I.",
	title="Mathematical Modeling of Traffic Flows",
	journal="Automation and Remote Control",
	year="2003",
	volume="64",
	number="11",
	pages="1651--1689",
	abstract="Main methods and concepts of mathematical modeling of traffic flows were reviewed. Two important lines of research---modeling of loading of urban transportation network and modeling of traffic flow dynamics---were discussed. For modeling of loading, consideration was given to the models for calculation of correspondences and distribution of flows over the network including different variants of equilibrium distribution and the optimal strategy algorithm. The main classes of dynamic models---macroscopic (hydrodynamic), kinetic, and microscopic---were examined as well.",
	issn="1608-3032",
	doi="10.1023/A:1027348026919",
	url="http://dx.doi.org/10.1023/A:1027348026919"
}


@inproceedings{solomon2014wasserstein,
	author = {Solomon, Justin and Rustamov, Raif M. and Guibas, Leonidas and Butscher, Adrian},
	title = {Wasserstein Propagation for Semi-supervised Learning},
	booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	series = {ICML'14},
	year = {2014},
	location = {Beijing, China},
	pages = {I-306--I-314},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044841},
	acmid = {3044841},
	publisher = {JMLR.org},
} 


@book{spall2003introduction,
	author = {Spall, James C.},
	title = {Introduction to Stochastic Search and Optimization},
	year = {2003},
	isbn = {0471330523},
	edition = {1},
	publisher = {John Wiley \& Sons, Inc.},
	address = {New York, NY, USA},
} 


@incollection{staib2017parallel,
	title = {Parallel Streaming Wasserstein Barycenters},
	author = {Staib, Matthew and Claici, Sebastian and Solomon, Justin M and Jegelka, Stefanie},
	booktitle = {Advances in Neural Information Processing Systems 30},
	editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
	pages = {2647--2658},
	year = {2017},
	publisher = {Curran Associates, Inc.},
	url = {http://papers.nips.cc/paper/6858-parallel-streaming-wasserstein-barycenters.pdf}
}


@article{stich2011linear,
	title={Optimization of convex functions with random pursuit},
	author={Sebastian Stich and Christian M\"yuller and Bernd G\"artner},
	journal={arXiv:1111.0194},
	year={2011}
}


@Article{tappenden2016inexact,
	author="Tappenden, Rachael
	and Richt{\'a}rik, Peter
	and Gondzio, Jacek",
	title="Inexact Coordinate Descent: Complexity and Preconditioning",
	journal="Journal of Optimization Theory and Applications",
	year="2016",
	month="Jul",
	day="01",
	volume="170",
	number="1",
	pages="144--176",
	abstract="One of the key steps at each iteration of a randomized block coordinate descent method consists in determining the update to a block of variables. Existing algorithms assume that in order to compute the update, a particular subproblem is solved exactly. In this work, we relax this requirement and allow for the subproblem to be solved inexactly, leading to an inexact block coordinate descent method. Our approach incorporates the best known results for exact updates as a special case. Moreover, these theoretical guarantees are complemented by practical considerations: the use of iterative techniques to determine the update and the use of preconditioning for further acceleration.",
	issn="1573-2878",
	doi="10.1007/s10957-016-0867-4",
	url="http://dx.doi.org/10.1007/s10957-016-0867-4",
	note = {First appeared in arXiv:1304.5530}
}


@inproceedings{tran-dinh2014constrained,
	author = {Tran-Dinh, Quoc and Cevher, Volkan},
	title = {Constrained Convex Minimization via Model-based Excessive Gap},
	booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems},
	series = {NIPS'14},
	year = {2014},
	location = {Montreal, Canada},
	pages = {721--729},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2968826.2968907},
	acmid = {2968907},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}


@article{tran-dinh2015smooth,
	author = {Quoc Tran-Dinh and Olivier Fercoq and Volkan Cevher},
	title = {A Smooth Primal-Dual Optimization Framework for Nonsmooth Composite Convex Minimization},
	journal = {SIAM Journal on Optimization},
	volume = {28},
	number = {1},
	pages = {96-134},
	year = {2018},
	doi = {10.1137/16M1093094},
	URL = {https://doi.org/10.1137/16M1093094},
	eprint = { ttps://doi.org/10.1137/16M1093094},
	note = {arXiv:1507.06243}
}


@techreport{tseng2008accelerated,
	author       = {Paul Tseng}, 
	title        = {On accelerated proximal gradient methods for convex-concave optimization},
	institution  = {MIT},
	year         = 2008,
	url        = {http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf}
}


@article{tyurin2017mirror,
	title={Mirror version of similar triangles method for constrained optimization problems},
	author={Alexander Tyurin},
	journal={arXiv:1705.09809},
	year={2017}
}


article{uribe2018distributed,
	title={Distributed Computation of Wasserstein Barycenters over Networks},
	author={C\'esar A. Uribe and Darina Dvinskikh and Pavel Dvurechensky and Alexander Gasnikov and Angelia Nedi\'c},
	journal={arXiv:1803.02933},
	year={2018}
}


@INPROCEEDINGS{uribe2018distributed,
	author={C\'esar A. Uribe and Darina Dvinskikh and Pavel Dvurechensky and Alexander Gasnikov and Angelia Nedi\'c},
	booktitle={2018 IEEE 57th Annual Conference on Decision and Control (CDC)},
	title={Distributed Computation of {W}asserstein Barycenters over Networks},
	year={2018},
	volume={},
	number={},
	pages1={1-6},
	keywords1={closed loop systems;control system synthesis;decentralised control;feedback;linear quadratic control;mobile robots;nonlinear control systems;path planning;stochastic processes;stochastic systems;centralized control;design approach;mobile robots;near-optimal decoupling principle;nonlinear stochastic systems;optimal open-loop design;robotic path planning;trajectory-optimized linear quadratic regulator design;Aerodynamics;Perturbation methods;Robots;Stochastic processes;Stochastic systems;Trajectory},
	doi1={10.1109/CDC.2017.8263634},
	ISSN1={},
	month1={Dec},
	note={Accepted, arXiv:1803.02933}
}


@book{vasilyev2002optimization,
	author = {Vasilyev, F.},
	title = {Optimization Methods},
	publisher = {Moscow, Russia: FP},
	year = {2002}
}


@book{villani2008optimal,
	title={Optimal transport: old and new},
	author={Villani, C{\'e}dric},
	volume={338},
	year={2008},
	publisher={Springer Science \& Business Media}
}


@article{wengert1964simple,
	author = {Wengert, R. E.},
	title = {A Simple Automatic Derivative Evaluation Program},
	journal = {Commun. ACM},
	issue_date = {Aug. 1964},
	volume = {7},
	number = {8},
	month = aug,
	year = {1964},
	issn = {0001-0782},
	pages = {463--464},
	numpages = {2},
	url = {http://doi.acm.org/10.1145/355586.364791},
	doi = {10.1145/355586.364791},
	acmid = {364791},
	publisher = {ACM},
	address = {New York, NY, USA},
} 


@article{werman1985distance,
	author = "Michael Werman and Shmuel Peleg and Azriel Rosenfeld",
	title = "A distance metric for multidimensional histograms",
	journal = "Computer Vision, Graphics, and Image Processing",
	volume = "32",
	number = "3",
	pages = "328 - 336",
	year = "1985",
	issn = "0734-189X",
	doi = "https://doi.org/10.1016/0734-189X(85)90055-6",
	url = "http://www.sciencedirect.com/science/article/pii/0734189X85900556"
}


@book{wilson2011entropy,
	title={Entropy in Urban and Regional Modelling},
	author={Wilson, A.G.},
	isbn={9780415695640},
	series={Monographs in spatial and environmental systems analysis},
	url1={https://books.google.de/books?id=0HTKq7GHZ4UC},
	year={2011},
	publisher={Routledge}
}


@article{xiao2010dual,
	author = {Xiao, Lin},
	title = {Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization},
	journal = {J. Mach. Learn. Res.},
	issue_date = {3/1/2010},
	volume = {11},
	month = dec,
	year = {2010},
	issn = {1532-4435},
	pages = {2543--2596},
	numpages = {54},
	url = {http://dl.acm.org/citation.cfm?id=1756006.1953017},
	acmid = {1953017},
	publisher = {JMLR.org},
} 


@article{xu2016accelerated,
	title={Accelerated first-order primal-dual proximal methods for linearly constrained composite convex programming},
	author={Yangyang Xu},
	journal={arXiv:1606.09155},
	year={2016}
}


@Article{yuan2016optimal,
	title="Optimal Distributed Stochastic Mirror Descent for Strongly Convex Optimization",
	author="Deming Yuan and Yiguang Hong and Daniel W.C. Ho and Guoping Jiang",
	journal="arXiv:1610.04702",
	year="2016",
	url="https://arxiv.org/pdf/1610.04702.pdf"
}


@InProceedings{yuchen2015stochastic,
	title = 	 {Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization},
	author = 	 {Yuchen Zhang and Xiao Lin},
	booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
	pages = 	 {353--361},
	year = 	 {2015},
	editor = 	 {Francis Bach and David Blei},
	volume = 	 {37},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Lille, France},
	month = 	 {07--09 Jul},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v37/zhanga15.pdf},
	url = 	 {http://proceedings.mlr.press/v37/zhanga15.html},
	abstract = 	 {We consider a generic convex optimization problem associated with regularized empirical risk minimization of linear predictors. The problem structure allows us to reformulate it as a convex-concave saddle point problem. We propose a stochastic primal-dual coordinate method, which alternates between maximizing over one (or more) randomly chosen dual variable and minimizing over the primal variable. We also develop an extension to non-smooth and non-strongly convex loss functions, and an extension with better convergence rate on unnormalized data. Both theoretically and empirically, we show that the SPDC method has comparable or better performance than several state-of-the-art optimization methods.}
}


@inproceedings{yurtsever2015universal,
	author = {Yurtsever, Alp and Tran-Dinh, Quoc and Cevher, Volkan},
	title = {A Universal Primal-dual Convex Optimization Framework},
	booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems},
	series = {NIPS'15},
	year = {2015},
	location = {Montreal, Canada},
	pages = {3150--3158},
	numpages = {9},
	url = {http://dl.acm.org/citation.cfm?id=2969442.2969591},
	acmid = {2969591},
	publisher = {MIT Press},
	address = {Cambridge, MA, USA},
}


@article{zhang2005estimating,
	author = {Zhang, Yin and Roughan, Matthew and Lund, Carsten and Donoho, David L.},
	title = {Estimating Point-to-point and Point-to-multipoint Traffic Matrices: An Information-theoretic Approach},
	journal = {IEEE/ACM Trans. Netw.},
	issue_date = {October 2005},
	volume = {13},
	number = {5},
	month = oct,
	year = {2005},
	issn = {1063-6692},
	pages = {947--960},
	numpages = {14},
	url = {http://dx.doi.org/10.1109/TNET.2005.857115},
	doi = {10.1109/TNET.2005.857115},
	acmid = {1103545},
	publisher = {IEEE Press},
	address = {Piscataway, NJ, USA},
	keywords = {SNMP, failure analysis, information theory, minimum mutual information, point-to-multipoint, point-to-point, regularization, traffic engineering, traffic matrix estimation},
}


@ARTICLE{zou2005regularization,
	title = {Regularization and variable selection via the elastic net},
	author = {Zou, Hui and Hastie, Trevor},
	year = {2005},
	journal = {Journal of the Royal Statistical Society Series B},
	volume = {67},
	number = {2},
	pages = {301-320},
	abstract = { We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together. The elastic net is particularly useful when the number of predictors ("p") is much bigger than the number of observations ("n"). By contrast, the lasso is not a very satisfactory variable selection method in the "p">"n" case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso. Copyright 2005 Royal Statistical Society.},
	url = {http://EconPapers.repec.org/RePEc:bla:jorssb:v:67:y:2005:i:2:p:301-320}
}






@article{ghadimi2012optimal,
	author = {Ghadimi, S. and Lan, G.},
	title = {Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic Composite Optimization I: A Generic Algorithmic Framework},
	journal = {SIAM Journal on Optimization},
	volume = {22},
	number = {4},
	pages = {1469-1492},
	year = {2012},
	doi1 = {10.1137/110848864},
	URL1 = {         https://doi.org/10.1137/110848864},
	eprint1 = {       https://doi.org/10.1137/110848864}
}


@article{ghadimi2013optimal,
	author = {Ghadimi, S. and Lan, G.},
	title = {Optimal Stochastic Approximation Algorithms for Strongly Convex Stochastic Composite Optimization, II: Shrinking Procedures and Optimal Algorithms},
	journal = {SIAM Journal on Optimization},
	volume = {23},
	number = {4},
	pages = {2061-2089},
	year = {2013},
	doi = {10.1137/110848876},
	URL = { https://doi.org/10.1137/110848876},
	eprint = { https://doi.org/10.1137/110848876
	}
	
}


@article{dvinskikh2019dual_old,
	title={On Dual Approach for Distributed Stochastic Convex Optimization over Networks},
	author={Dvinskikh, Darina and Gorbunov, Eduard and Gasnikov, Alexander and Dvurechensky, Pavel and Uribe, Cesar A},
	journal={arXiv preprint arXiv:1903.09844},
	year={2019}
}

@article{hendrikx2018accelerated,
	title={Accelerated decentralized optimization with local updates for smooth and strongly convex objectives},
	author={Hendrikx, Hadrien and Bach, Francis and Massouli{\'e}, Laurent},
	journal={arXiv preprint arXiv:1810.02660},
	year={2018}
}


@article{hendrikx2019asynchronous,
	title={Asynchronous Accelerated Proximal Stochastic Gradient for Strongly Convex Distributed Finite Sums},
	author={Hendrikx, Hadrien and Bach, Francis and Massouli{\'e}, Laurent},
	journal={arXiv preprint arXiv:1901.09865},
	year={2019}
}

@inproceedings{allen2018make,
	title={How to make the gradients small stochastically: Even faster convex and nonconvex sgd},
	author={Allen-Zhu, Zeyuan},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1157--1167},
	year={2018}
}

@article{foster2019complexity,
	title={The Complexity of Making the Gradient Small in Stochastic Convex Optimization},
	author={Foster, Dylan and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
	journal={arXiv preprint arXiv:1902.04686},
	year={2019}
}

@article{d2008smooth,
	title={Smooth optimization with approximate gradient},
	author={d'Aspremont, Alexandre},
	journal={SIAM Journal on Optimization},
	volume={19},
	number={3},
	pages={1171--1183},
	year={2008},
	publisher={SIAM}
}

@article{cohen2018acceleration,
	title={On acceleration with noise-corrupted gradients},
	author={Cohen, Michael B and Diakonikolas, Jelena and Orecchia, Lorenzo},
	journal={arXiv preprint arXiv:1805.12591},
	year={2018}
}

@article{gasnikov2017modern,
	title={Universal gradient descent},
	author={Gasnikov, Alexander},
	journal={arXiv preprint arXiv:1711.00394},
	year={2017}
}

@article{stonyakin2019gradient,
	title={Gradient Methods for Problems with Inexact Model of the Objective},
	author={Stonyakin, Fedor and Dvinskikh, Darina and Dvurechensky, Pavel and Kroshnin, Alexey and Kuznetsova, Olesya and Agafonov, Artem and Gasnikov, Alexander and Tyurin, Alexander and Uribe, C{\'e}sar A and Pasechnyuk, Dmitry and others},
	journal={arXiv preprint arXiv:1902.09001},
	year={2019}
}

@article{stonyakin2019inexact,
	title={Inexact model: A framework for optimization and variational inequalities},
	author={Stonyakin, Fedor and Gasnikov, Alexander and Tyurin, Alexander and Pasechnyuk, Dmitry and Agafonov, Artem and Dvurechensky, Pavel and Dvinskikh, Darina and Piskunova, Victirya},
	journal={arXiv preprint arXiv:1902.00990},
	year={2019}
}


@article{gorbunov2018accelerated,
	title={An Accelerated Method for Derivative-Free Smooth Stochastic Convex Optimization},
	author={Gorbunov, Eduard and Dvurechensky, Pavel and Gasnikov, Alexander},
	journal={SIOPT (in print)},
	year={2022}
}

@inproceedings{woodworth2016tight,
	title={Tight complexity bounds for optimizing composite objectives},
	author={Woodworth, Blake E and Srebro, Nati},
	booktitle={Advances in neural information processing systems},
	pages={3639--3647},
	year={2016}
}

@book{nesterov2018lectures,
	title={Lectures on convex optimization},
	author={Nesterov, Yurii},
	volume={137},
	year={2018},
	publisher={Springer}
}

@article{nesterov2012make,
	title={How to make the gradients small},
	author={Nesterov, Yurii},
	journal={Optima},
	volume={88},
	pages={10--11},
	year={2012}
}

@article{kim2018optimizing,
	title={Optimizing the efficiency of first-order methods for decreasing the gradient of smooth convex functions},
	author={Kim, Donghwan and Fessler, Jeffrey A},
	journal={arXiv preprint arXiv:1803.06600},
	year={2018}
}

@article{kakade2009duality,
	title={On the duality of strong convexity and strong smoothness: Learning applications and matrix regularization},
	author={Kakade, Sham and Shalev-Shwartz, Shai and Tewari, Ambuj},
	journal={Unpublished Manuscript, http://ttic. uchicago. edu/shai/papers/KakadeShalevTewari09.pdf},
	volume={2},
	number={1},
	year={2009}
}

@book{shalev2014understanding,
	title={Understanding machine learning: From theory to algorithms},
	author={Shalev-Shwartz, Shai and Ben-David, Shai},
	year={2014},
	publisher={Cambridge university press}
}

@article{spokoiny2012parametric,
	title={Parametric estimation. Finite sample theory},
	author={Spokoiny, Vladimir and others},
	journal={The Annals of Statistics},
	volume={40},
	number={6},
	pages={2877--2909},
	year={2012},
	publisher={Institute of Mathematical Statistics}
}

@inproceedings{shalev2009stochastic,
	title={Stochastic Convex Optimization.},
	author={Shalev-Shwartz, Shai and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik},
	booktitle={COLT},
	year={2009}
}

@book{bertsekas1989parallel,
	title={Parallel and distributed computation: numerical methods},
	author={Bertsekas, Dimitri P and Tsitsiklis, John N},
	volume={23},
	year={1989},
	publisher={Prentice hall Englewood Cliffs, NJ}
}

@article{lan2018random,
	title={Random gradient extrapolation for distributed and stochastic optimization},
	author={Lan, Guanghui and Zhou, Yi},
	journal={SIAM Journal on Optimization},
	volume={28},
	number={4},
	pages={2753--2782},
	year={2018},
	publisher={SIAM}
}

@article{mcmahan2016communication,
	title={Communication-efficient learning of deep networks from decentralized data},
	author={McMahan, H Brendan and Moore, Eider and Ramage, Daniel and Hampson, Seth and others},
	journal={arXiv preprint arXiv:1602.05629},
	year={2016}
}

@article{nedic2018graph,
	title={Graph-Theoretic Analysis of Belief System Dynamics under Logic Constraints},
	author={Nedi{\'c}, Angelia and Olshevsky, Alex and Uribe, C{\'e}sar A},
	journal={arXiv preprint arXiv:1810.02456},
	year={2018}
}

@inproceedings{allen2016optimal,
	title={Optimal black-box reductions between optimization objectives},
	author={Allen-Zhu, Zeyuan and Hazan, Elad},
	booktitle={Advances in Neural Information Processing Systems},
	pages={1614--1622},
	year={2016}
}

@inproceedings{arjevani2015communication,
	title={Communication complexity of distributed convex learning and optimization},
	author={Arjevani, Yossi and Shamir, Ohad},
	booktitle={Advances in neural information processing systems},
	pages={1756--1764},
	year={2015}
}

@article{baimurzina2017universal,
	title={Universal similar triangulars method for searching equilibriums in traffic flow distribution models},
	author={Baimurzina, Dilyara and Gasnikov, Alexander and Gasnikova, Evgenia and Dvurechensky, Pavel and Ershov, Egor and Kubentaeva, Meruza and Lagunovskaya, Anastasia},
	journal={arXiv preprint arXiv:1701.02473},
	year={2017}
}

@inproceedings{dvurechenskii2018decentralize,
	title={Decentralize and randomize: Faster algorithm for wasserstein barycenters},
	author={Dvurechenskii, Pavel and Dvinskikh, Darina and Gasnikov, Alexander and Uribe, Cesar and Nedich, Angelia},
	booktitle={Advances in Neural Information Processing Systems},
	pages={10760--10770},
	year={2018}
}

@techreport{nesterov2018implementable,
	author       = {Nesterov, Yurii}, 
	title        = {Implementable Tensor Methods in Unconstrained Convex Optimization},
	institution  = {CORE UCL},
	year         = 2018,
	url        	 = {https://alfresco.uclouvain.be/alfresco/service/guest/streamDownload/workspace/SpacesStore/aabc2323-0bc1-40d4-9653-1c29971e7bd8/coredp2018_05web.pdf},
	note         = {CORE Discussion Paper 2018/05}
}

@article{carmon2016gradient,
	title={Gradient descent efficiently finds the cubic-regularized non-convex newton step},
	author={Carmon, Yair and Duchi, John C},
	journal={arXiv preprint arXiv:1612.00547},
	year={2016}
}

@article{kibardin1979decomposition,
	title={Decomposition into functions in the minimization problem},
	author={Kibardin, VM},
	journal={Avtomatika i Telemekhanika},
	number={9},
	volume={},
	pages={66--79},
	year={1979},
	publisher={Russian Academy of Sciences, Branch of Power Industry, Machine Building~…}
}

@article{dvurechensky2018parallel,
	title={Parallel Algorithms and Probability of Large Deviation for Stochastic Convex Optimization Problems},
	author={Dvurechensky, PE and Gasnikov, AV and Lagunovskaya, AA},
	journal={Numerical Analysis and Applications},
	volume={11},
	number={1},
	pages={33--37},
	year={2018},
	publisher={Springer}
}


@article{monteiro2013iteration,
	title={Iteration-complexity of block-decomposition algorithms and the alternating direction method of multipliers},
	author={Monteiro, Renato DC and Svaiter, Benar F},
	journal={SIAM Journal on Optimization},
	volume={23},
	number={1},
	pages={475--507},
	year={2013},
	publisher={SIAM}
}

@article{he20121,
	title={On the O(1/n) Convergence Rate of the Douglas--Rachford Alternating Direction Method},
	author={He, Bingsheng and Yuan, Xiaoming},
	journal={SIAM Journal on Numerical Analysis},
	volume={50},
	number={2},
	pages={700--709},
	year={2012},
	publisher={SIAM}
}

@article{ryu2018operator,
	title={Operator Splitting Performance Estimation: Tight contraction factors and optimal parameter selection},
	author={Ryu, Ernest K and Taylor, Adrien B and Bergeling, Carolina and Giselsson, Pontus},
	journal={arXiv preprint arXiv:1812.00146},
	year={2018}
}

@article{Lan2019lectures,
	title={Lectures on Optimization Methods for Machine Learning},
	author={George Lan},
	journal={e-print},
	year={2019}
}

@article{hendrikx2019accelerated,
	title={An Accelerated Decentralized Stochastic Proximal Algorithm for Finite Sums},
	author={Hendrikx, Hadrien and Bach, Francis and Massoulie, Laurent},
	journal={arXiv preprint arXiv:1905.11394},
	year={2019}
}

@inproceedings{sun2018distributed,
	title={Distributed non-convex first-order optimization and information processing: Lower complexity bounds and rate optimal algorithms},
	author={Sun, Haoran and Hong, Mingyi},
	booktitle={2018 52nd Asilomar Conference on Signals, Systems, and Computers},
	pages={38--42},
	year={2018},
	organization={IEEE}
}

@article{olshevsky2019non,
	title={A Non-Asymptotic Analysis of Network Independence for Distributed Stochastic Gradient Descent},
	author={Olshevsky, Alex and Paschalidis, Ioannis Ch and Pu, Shi},
	journal={arXiv preprint arXiv:1906.02702},
	year={2019}
}

@article{olshevsky2019asymptotic,
	title={Asymptotic Network Independence in Distributed Optimization for Machine Learning},
	author={Olshevsky, Alex and Paschalidis, Ioannis Ch and Pu, Shi},
	journal={arXiv preprint arXiv:1906.12345},
	year={2019}
}

@article{kulunchakov2019estimate1,
	title={Estimate sequences for stochastic composite optimization: Variance reduction, acceleration, and robustness to noise},
	author={Kulunchakov, Andrei and Mairal, Julien},
	journal={arXiv preprint arXiv:1901.08788},
	year={2019}
}

@article{kulunchakov2019estimate2,
	title={Estimate Sequences for Variance-Reduced Stochastic Composite Optimization},
	author={Kulunchakov, Andrei and Mairal, Julien},
	journal={arXiv preprint arXiv:1905.02374},
	year={2019}
}

@article{kulunchakov2019generic,
	title={A Generic Acceleration Framework for Stochastic Composite Optimization},
	author={Kulunchakov, Andrei and Mairal, Julien},
	journal={arXiv preprint arXiv:1906.01164},
	year={2019}
}

@article{gallager1983distributed,
	title={A distributed algorithm for minimum-weight spanning trees},
	author={Gallager, Robert G and Humblet, Pierre A and Spira, Philip M},
	journal={ACM Transactions on Programming Languages and systems (TOPLAS)},
	volume={5},
	number={1},
	pages={66--77},
	year={1983},
	publisher={ACM}
}

@article{garay1998sublinear,
	title={A sublinear time distributed algorithm for minimum-weight spanning trees},
	author={Garay, Juan A and Kutten, Shay and Peleg, David},
	journal={SIAM Journal on Computing},
	volume={27},
	number={1},
	pages={302--316},
	year={1998},
	publisher={SIAM}
}

@article{gorbunov2019optimal,
	title={Optimal decentralized distributed algorithms for stochastic convex optimization},
	author={Gorbunov, Eduard and Dvinskikh, Darina and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:1911.07363},
	year={2019}
}

@article{koloskova2020unified,
	title={A unified theory of decentralized sgd with changing topology and local updates},
	author={Koloskova, Anastasia and Loizou, Nicolas and Boreiri, Sadra and Jaggi, Martin and Stich, Sebastian U},
	journal={ICML 2020, arXiv preprint arXiv:2003.10422},
	year={2020}
}

@article{woodworth2020minibatch,
	title={Minibatch vs Local SGD for Heterogeneous Distributed Learning},
	author={Woodworth, Blake and Patel, Kumar Kshitij and Srebro, Nathan},
	journal={arXiv preprint arXiv:2006.04735},
	year={2020}
}

@article{hendrikx2020statistically,
	title={Statistically Preconditioned Accelerated Gradient Method for Distributed Optimization},
	author={Hendrikx, Hadrien and Xiao, Lin and Bubeck, Sebastien and Bach, Francis and Massoulie, Laurent},
	journal={arXiv preprint arXiv:2002.10726},
	year={2020}
}

@article{hendrikx2020optimal,
	title={An Optimal Algorithm for Decentralized Finite Sum Optimization},
	author={Hendrikx, Hadrien and Bach, Francis and Massoulie, Laurent},
	journal={arXiv preprint arXiv:2005.10675},
	year={2020}
}


@article{ivanova2019adaptive,
	title={Adaptive Catalyst for smooth convex optimization},
	author={Ivanova, Anastasiya and Grishchenko, Dmitry and Gasnikov, Alexander and Shulgin, Egor},
	journal={arXiv preprint arXiv:1911.11271},
	year={2019}
}

@article{gasnikov2017convex,
	title={Convex optimization in Hilbert space with applications to inverse problems},
	author={Gasnikov, Alexander and Kabanikhin, Sergey and Mohammed, Ahmed and Shishlenin, Maxim},
	journal={arXiv preprint arXiv:1703.00267},
	year={2017}
}

@book{vogel2002computational,
	title={Computational methods for inverse problems},
	author={Vogel, Curtis R},
	volume={23},
	year={2002},
	publisher={Siam}
}

@book{byrne2014iterative,
	title={Iterative optimization in inverse problems},
	author={Byrne, Charles L},
	year={2014},
	publisher={CRC Press}
} 

@article{gao2017distributed,
	title={Distributed Computation of Linear Inverse Problems with Application to Computed Tomography},
	author={Gao, Yushan and Blumensath, Thomas},
	journal={arXiv preprint arXiv:1709.00953},
	year={2017}
}

@incollection{ye2019optimization,
	title={Optimization methods for inverse problems},
	author={Ye, Nan and Roosta-Khorasani, Farbod and Cui, Tiangang},
	booktitle={2017 MATRIX Annals},
	pages={121--140},
	year={2019},
	publisher={Springer}
}

@article{Barre2020principle,
	title={Principled analyses and design of first-order methods with proximal inexact proximal operator},
	author={Mathieu, Baree and Adrien, Taylor and Francis, Bach},
	journal={arXiv preprint arXiv:2006.06041},
	year={2020}
}

@article{stonyakin2020mirror,
	title={Mirror descent for constrained optimization problems with large subgradient values of functional constraints},
	author={Stonyakin, Fedor Sergeevich and Stepanov, Aleksei Nikolaevich and Gasnikov, Alexander Vladimirovich and Titov, Aleksandr Aleksandrovich},
	journal={Computer Research and Modelling},
	volume={12},
	number={2},
	pages={301--317},
	year={2020},
	publisher={Izhevsk Institute of Computer Research}
}

@article{kovalev2020optimal,
	title={Optimal and practical algorithms for smooth and strongly convex decentralized optimization},
	author={Kovalev, Dmitry and Salim, Adil and Richt{\'a}rik, Peter},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}



@article{li2020revisiting,
	title={Revisiting EXTRA for Smooth Distributed Optimization},
	author={Li, Huan and Lin, Zhouchen},
	journal={arXiv preprint arXiv:2002.10110},
	year={2020}
}

@article{yuan2020federated,
	title={Federated Accelerated Stochastic Gradient Descent},
	author={Yuan, Honglin and Ma, Tengyu},
	journal={arXiv preprint arXiv:2006.08950},
	year={2020}
}

@article{hendrikx2020dual,
	title={Dual-Free Stochastic Decentralized Optimization with Variance Reduction},
	author={Hendrikx, Hadrien and Bach, Francis and Massouli{\'e}, Laurent},
	journal={arXiv preprint arXiv:2006.14384},
	year={2020}
}

@article{karimireddy2019scaffold,
	title={SCAFFOLD: Stochastic Controlled Averaging for Federated Learning},
	author={Karimireddy, Sai Praneeth and Kale, Satyen and Mohri, Mehryar and Reddi, Sashank J and Stich, Sebastian U and Suresh, Ananda Theertha},
	journal={arXiv preprint arXiv:1910.06378},
	year={2019}
}

@article{nedic2020distributed,
	title={Distributed Gradient Methods for Convex Machine Learning Problems in Networks: Distributed Optimization},
	author={Nedi{\'c}, Angelia},
	journal={IEEE Signal Processing Magazine},
	volume={37},
	number={3},
	pages={92--101},
	year={2020},
	publisher={IEEE}
}

@incollection{poljak1981iterative,
	title={Iterative algorithms for singular minimization problems},
	author={Poljak, BT},
	booktitle={Nonlinear Programming 4},
	pages={147--166},
	year={1981},
	publisher={Elsevier}
}

@article{dvinskikh2020accelerated,
	title={Accelerated and non accelerated stochastic gradient descent in model generality},
	author={Dvinskikh, Darina M and Turin, Aleksandr Igorevich and Gasnikov, Alexander Vladimirovich and Omelchenko, Sergey Sergeevich},
	journal={Matematicheskie Zametki},
	volume={108},
	number={4},
	pages={515--528},
	year={2020},
	publisher={Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}

@article{godichon2020rates,
	title={On the rates of convergence of parallelized averaged stochastic gradient algorithms},
	author={Godichon-Baggioni, Antoine and Saadane, Sofiane},
	journal={Statistics},
	pages={1--18},
	year={2020},
	publisher={Taylor \& Francis}
}

@inproceedings{rogozin2021towards,
	title={Towards accelerated rates for distributed optimization over time-varying networks},
	author={Rogozin, Alexander and Lukoshkin, Vladislav and Gasnikov, Alexander and Kovalev, Dmitry and Shulgin, Egor},
	booktitle={International Conference on Optimization and Applications},
	pages={258--272},
	year={2021},
	organization={Springer}
}


@article{gower2020variance,
	title={Variance-Reduced Methods for Machine Learning},
	author={Gower, Robert M and Schmidt, Mark and Bach, Francis and Richtarik, Peter},
	journal={arXiv preprint arXiv:2010.00892},
	year={2020}
}

@article{li2020optimal,
	title={Optimal accelerated variance reduced extra and diging for strongly convex and smooth decentralized optimization},
	author={Li, Huan and Lin, Zhouchen and Fang, Yongchun},
	journal={arXiv preprint arXiv:2009.04373},
	year={2020}
}

@inproceedings{dvinskikh2020improved,
	title={Improved complexity bounds in wasserstein barycenter problem},
	author={Dvinskikh, Darina and Tiapkin, Daniil},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={1738--1746},
	year={2021},
	organization={PMLR}
}


@article{beznosikov2019derivative,
	title={Derivative-free method for composite optimization with applications to decentralized distributed optimization},
	author={Beznosikov, Aleksandr and Gorbunov, Eduard and Gasnikov, Alexander},
	journal={IFAC-PapersOnLine},
	volume={53},
	number={2},
	pages={4038--4043},
	year={2020},
	publisher={Elsevier}
}


@article{li2020root,
	title={ROOT-SGD: Sharp Nonasymptotics and Asymptotic Efficiency in a Single Algorithm},
	author={Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin J and Jordan, Michael I},
	journal={arXiv preprint arXiv:2008.12690},
	year={2020}
}


@article{uribe2020dual,
	title={A dual approach for optimal algorithms in distributed optimization over networks},
	author={Uribe, C{\'e}sar A and Lee, Soomin and Gasnikov, Alexander and Nedi{\'c}, Angelia},
	journal={Optimization Methods and Software},
	pages={1--40},
	year={2020},
	publisher={Taylor \& Francis}
}

@article{kairouz2019advances,
	title={Advances and open problems in federated learning},
	author={Kairouz, Peter and McMahan, H Brendan and Avent, Brendan and Bellet, Aur{\'e}lien and Bennis, Mehdi and Bhagoji, Arjun Nitin and Bonawitz, Keith and Charles, Zachary and Cormode, Graham and Cummings, Rachel and others},
	journal={arXiv preprint arXiv:1912.04977},
	year={2019}
}

@article{nesterov2008confidence,
	title={Confidence level solutions for stochastic programming},
	author={Nesterov, Yu and Vial, J-Ph},
	journal={Automatica},
	volume={44},
	number={6},
	pages={1559--1568},
	year={2008},
	publisher={Elsevier}
}

@article{bubeck2014convex,
	title={Convex optimization: Algorithms and complexity},
	author={Bubeck, S{\'e}bastien},
	journal={arXiv preprint arXiv:1405.4980},
	year={2014}
}

@techreport{nesterov2020inexact,
	title={Inexact high-order proximal-point methods with auxiliary search procedure},
	author={Nesterov, Yurii},
	year={2020},
	institution={tech. report, CORE discussion paper}
}

@article{kamzolov2020near,
	title={Near-Optimal Hyperfast Second-Order Method for convex optimization and its Sliding},
	author={Kamzolov, Dmitry and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2002.09050},
	year={2020}
}

@book{gasnikov2020book,
	title={Universal gradient descent},
	author={Gasnikov, A.V.},
	isbn={978-5-4439-0000-1},
	series={Modern numerical optimization methods},
	url1={https://arxiv.org/pdf/1711.00394.pdf},
	year={2020},
	publisher={MCCME}
}

@article{dvinskikh2020meta,
	title={Accelerated meta-algorithm for convex optimization},
	author={Dvinskikh, Darina and Kamzolov, Dmitry and Gasnikov, Alexander and Dvurechensky, Pavel and Pasechnyk, Dmitry and Matykhin, Vladislav and Chernov, Alexei},
	journal={Computational Mathematics and Mathematical Physics},
	volume={61},
	number={1},
	year={2021},
	publisher={Pleiades Publishing}
}

@inproceedings{scaman2018optimal,
	title={Optimal algorithms for non-smooth distributed optimization in networks},
	author={Scaman, Kevin and Bach, Francis and Bubeck, S{\'e}bastien and Massouli{\'e}, Laurent and Lee, Yin Tat},
	booktitle={Advances in Neural Information Processing Systems},
	pages={2740--2749},
	year={2018}
}

@inproceedings{woodworth2018graph,
	title={Graph oracle models, lower bounds, and gaps for parallel stochastic optimization},
	author={Woodworth, Blake E and Wang, Jialei and Smith, Adam and McMahan, Brendan and Srebro, Nati},
	booktitle={Advances in neural information processing systems},
	pages={8496--8506},
	year={2018}
}

@article{guigues2017non,
	title={Non-asymptotic confidence bounds for the optimal value of a stochastic program},
	author={Guigues, Vincent and Juditsky, Anatoli and Nemirovski, Arkadi},
	journal={Optimization Methods and Software},
	volume={32},
	number={5},
	pages={1033--1058},
	year={2017},
	publisher={Taylor \& Francis}
}

@incollection{shapiro2005complexity,
	title={On complexity of stochastic programming problems},
	author={Shapiro, Alexander and Nemirovski, Arkadi},
	booktitle={Continuous optimization},
	pages={111--146},
	year={2005},
	publisher={Springer}
}

@book{goodfellow2016deep,
	title={Deep learning},
	author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
	volume={1},
	year={2016},
	publisher={MIT press Cambridge}
}

@article{sridharan2012learning,
	title={Learning from an optimization viewpoint},
	author={Sridharan, Karthik},
	journal={arXiv preprint arXiv:1204.4145},
	year={2012}
}

@phdthesis{devolder2013exactness,
	title={Exactness, inexactness and stochasticity in first-order methods for large-scale convex optimization},
	author={Devolder, Olivier},
	year={2013},
	school={PhD thesis, ICTEAM and CORE, Universit{\'e} Catholique de Louvain}
}

@article{mishchenko2020random,
	title={Random Reshuffling: Simple Analysis with Vast Improvements},
	author={Mishchenko, Konstantin and Khaled, Ahmed and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2006.05988},
	year={2020}
}

@article{assran2020convergence,
	title={On the Convergence of Nesterov's Accelerated Gradient Method in Stochastic Settings},
	author={Assran, Mahmoud and Rabbat, Michael},
	journal={arXiv preprint arXiv:2002.12414},
	year={2020}
}

@book{lan2020first,
	title={First-order and Stochastic Optimization Methods for Machine Learning},
	author={Lan, Guanghui},
	year={2020},
	publisher={Springer}
}

@misc{linaccelerated,
	title={Accelerated Optimization for Machine Learning},
	author={Lin, Zhouchen and Li, Huan and Fang, Cong},
	year={2020},
	publisher={Springer}
}


@article{ermakov2009method,
	title={Monte Carlo method in Numerical Analysis. Introduction course. [in Russian]},
	author={Ermakov, Sergey Mikhailovich},
	year={2009},
	publisher={Nevskii Dialekt}
}

@article{parsegov2019accelerated,
	title={Accelerated Distributed Solutions for Power System State Estimation},
	author={Parsegov, Sergei and Kubentayeva, Samal and Gryazina, Elena and Gasnikov, Alexander and Ibanez, Federico},
	journal={arXiv preprint arXiv:1911.11080},
	year={2019}
}

@article{ivanova2020oracle,
	title={Oracle complexity separation in convex optimization},
	author={Ivanova, Anastasiya and Gasnikov, Alexander and Dvurechensky, Pavel and Dvinskikh, Darina and Tyurin, Alexander and Vorontsova, Evgeniya and Pasechnyuk, Dmitry},
	journal={arXiv preprint arXiv:2002.02706},
	year={2020}
}

@article{stonyakin2020inexact,
	title={Inexact model: A framework for optimization and variational inequalities},
	author={Stonyakin, Fedor and Tyurin, Alexander and Gasnikov, Alexander and Dvurechensky, Pavel and Agafonov, Artem and Dvinskikh, Darina and Alkousa, Mohammad and Pasechnyuk, Dmitry and Artamonov, Sergei and Piskunova, Victorya},
	journal={Optimization Methods and Software},
	pages={1--47},
	year={2021},
	publisher={Taylor \& Francis}
}

@article{shi2015extra,
	title={Extra: An exact first-order algorithm for decentralized consensus optimization},
	author={Shi, Wei and Ling, Qing and Wu, Gang and Yin, Wotao},
	journal={SIAM Journal on Optimization},
	volume={25},
	number={2},
	pages={944--966},
	year={2015},
	publisher={SIAM}
}

@article{nedic2017achieving,
	title={Achieving geometric convergence for distributed optimization over time-varying graphs},
	author={Nedic, Angelia and Olshevsky, Alex and Shi, Wei},
	journal={SIAM Journal on Optimization},
	volume={27},
	number={4},
	pages={2597--2633},
	year={2017},
	publisher={SIAM}
}

@article{bayandina2018gradient,
	title={Gradient-free two-point methods for solving stochastic nonsmooth convex optimization problems with small non-random noises},
	author={Bayandina, Anastasia Sergeevna and Gasnikov, Alexander V and Lagunovskaya, Anastasia A},
	journal={Automation and Remote Control},
	volume={79},
	number={8},
	pages={1399--1408},
	year={2018},
	publisher={Springer}
}

@article{dvurechensky2020accelerated,
	title={An accelerated directional derivative method for smooth stochastic convex optimization},
	author={Dvurechensky, Pavel and Gorbunov, Eduard and Gasnikov, Alexander},
	journal={European Journal of Operational Research},
	year={2020},
	publisher={Elsevier}
}

@article{ye2020multi,
	title={Multi-consensus Decentralized Accelerated Gradient Descent},
	author={Ye, Haishan and Luo, Luo and Zhou, Ziang and Zhang, Tong},
	journal={arXiv preprint arXiv:2005.00797},
	year={2020}
}

@incollection{krawtschenko2020distributed,
	title = {Distributed Optimization with Quantization for Computing Wasserstein Barycenters},
	author = {Krawtschenko, Roman and Dvurechensky, Pavel and Uribe, Cesar and Gasnikov, Alexander},
	booktitle = {The 24th International Conference on 
	Artificial Intelligence and Statistics. (Submitted)},
	year = {2021}
}

@article{qian2020error,
	title={Error Compensated Distributed SGD Can Be Accelerated},
	author={Qian, Xun and Richt{\'a}rik, Peter and Zhang, Tong},
	journal={arXiv preprint arXiv:2010.00091},
	year={2020}
}

@article{albasyoni2020optimal,
	title={Optimal Gradient Compression for Distributed and Federated Learning},
	author={Albasyoni, Alyazeed and Safaryan, Mher and Condat, Laurent and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2010.03246},
	year={2020}
}

@article{beznosikov2020biased,
	title={On Biased Compression for Distributed Learning},
	author={Beznosikov, Aleksandr and Horv{\'a}th, Samuel and Richt{\'a}rik, Peter and Safaryan, Mher},
	journal={arXiv preprint arXiv:2002.12410},
	year={2020}
}

@inproceedings{beznosikov2020gradient,
	title={Gradient-Free Methods with Inexact Oracle for Convex-Concave Stochastic Saddle-Point Problem},
	author={Beznosikov, Aleksandr and Sadiev, Abdurakhmon and Gasnikov, Alexander},
	booktitle={International Conference on Mathematical Optimization Theory and Operations Research},
	pages={105--119},
	year={2020},
	organization={Springer}
}

@Article{Nesterov,
	title =	"Random Gradient-Free Minimization of Convex
	Functions",
	author =	"Yurii Nesterov and Vladimir G. Spokoiny",
	journal =	"Foundations of Computational Mathematics",
	year = 	"2017",
	number =	"2",
	volume =	"17",
	bibdate =	"2018-11-30",
	bibsource =	"DBLP,
	http://dblp.uni-trier.de/https://doi.org/10.1007/s10208-015-9296-2;
	DBLP,
	http://dblp.uni-trier.de/db/journals/focm/focm17.html#NesterovS17",
	pages =	"527--566",
}


@article{Shamir15,
	title={An Optimal Algorithm for Bandit and Zero-Order Convex Optimization with Two-Point Feedback.},
	author={Shamir, Ohad},
	journal={Journal of Machine Learning Research},
	volume={18},
	number={52},
	pages={1--11},
	year={2017}
}


@article{larson2019derivative,
	title={Derivative-free optimization methods},
	author={Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M},
	journal={Acta Numerica},
	volume={28},
	pages={287--404},
	year={2019},
	publisher={Cambridge University Press}
}

@inproceedings{cohen2016geometric,
	title={Geometric median in nearly linear time},
	author={Cohen, Michael B and Lee, Yin Tat and Miller, Gary and Pachocki, Jakub and Sidford, Aaron},
	booktitle={Proceedings of the forty-eighth annual ACM symposium on Theory of Computing},
	pages={9--21},
	year={2016},
	organization={ACM}
}

@article{minsker2015geometric,
	title={Geometric median and robust estimation in Banach spaces},
	author={Minsker, Stanislav and others},
	journal={Bernoulli},
	volume={21},
	number={4},
	pages={2308--2335},
	year={2015},
	publisher={Bernoulli Society for Mathematical Statistics and Probability}
}

@inproceedings{rogozin2020penalty,
	title={Penalty-Based Method for Decentralized Optimization over Time-Varying Graphs},
	author={Rogozin, Alexander and Gasnikov, Alexander},
	booktitle={International Conference on Optimization and Applications},
	pages={239--256},
	year={2020},
	organization={Springer}
}

@Article{devolder2013first,
	title={First-order methods with inexact oracle: the strongly convex case},
	author={Devolder, O. and Glineur, F. and Nesterov, Yu.},
	journal={CORE Discussion Papers},
	volume={2013016},
	pages={47},
	year={2013},
	publisher={Universit{\'e} catholique de Louvain}
}

@article{yuan2016convergence,
	title={On the convergence of decentralized gradient descent},
	author={Yuan, Kun and Ling, Qing and Yin, Wotao},
	journal={SIAM Journal on Optimization},
	volume={26},
	number={3},
	pages={1835--1854},
	year={2016},
	publisher={SIAM}
}

@article{nedic2009distributed,
	title={Distributed subgradient methods for multi-agent optimization},
	author={Nedi{\'c}, Angelia and Ozdaglar, Asuman},
	journal={IEEE Transactions on Automatic Control},
	volume={54},
	number={1},
	pages={48--61},
	year={2009},
	publisher={IEEE}
}


@article{dvinskikh2020parallel,
	title={Parallel and Distributed algorithms for ML problems},
	author={Dvinskikh, Darina and Gasnikov, Alexander and Rogozin, Alexander and Beznosikov, Alexander},
	journal={arXiv preprint arXiv:2010.09585},
	year={2020}
}

@inproceedings{dvinskikh2019dual,
	title={On Primal and Dual Approaches for Distributed Stochastic Convex Optimization over Networks},
	author={Dvinskikh, Darina and Gorbunov, Eduard and Gasnikov, Alexander and Dvurechensky, Pavel and Uribe, C{\'e}sar A},
	booktitle={2019 IEEE 58th Conference on Decision and Control (CDC)},
	pages={7435--7440},
	year={2019},
	organization={IEEE}
}


@article{dvinskikh2021decentralized,
	title={Decentralized Algorithms for Wasserstein Barycenters},
	author={Dvinskikh, Darina},
	journal={arXiv preprint arXiv:2105.01587},
	year={2021}
}

@article{sun2020convergence,
	title={Distributed Optimization Based on Gradient-tracking Revisited: Enhancing Convergence Rate via Surrogation},
	author={Sun, Ying and Daneshmand, Amir and Scutari, Gesualdo},
	journal={arXiv preprint arXiv:1905.02637},
	year={2020}
}

@article{gorbunov2020local,
	title={Local SGD: Unified Theory and New Efficient Methods},
	author={Gorbunov, Eduard and Hanzely, Filip and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2011.02828},
	year={2020}
}

@article{tang2020distributed,
	title={Distributed zero-order algorithms for nonconvex multi-agent optimization},
	author={Tang, Yujie and Zhang, Junshan and Li, Na},
	journal={IEEE Transactions on Control of Network Systems},
	year={2020},
	publisher={IEEE}
}

@article{ligf2014,
	author = {Li, Jueyou and Wu, Changzhi and Wu, Zhiyou and Long, Qiang},
	year = {2014},
	month = {02},
	pages = {},
	title = {Gradient-free method for nonsmooth distributed optimization},
	volume = {61},
	journal = {Journal of Global Optimization},
	doi = {10.1007/s10898-014-0174-2}
}

@article{qu2019accelerated,
	title={Accelerated distributed Nesterov gradient descent},
	author={Qu, Guannan and Li, Na},
	journal={IEEE Transactions on Automatic Control},
	year={2019},
	publisher={IEEE}
}

@inproceedings{maros2018panda,
	title={Panda: A dual linearly converging method for distributed optimization over time-varying undirected graphs},
	author={Maros, Marie and Jald{\'e}n, Joakim},
	booktitle={2018 IEEE Conference on Decision and Control (CDC)},
	pages={6520--6525},
	year={2018},
	organization={IEEE}
}

@article{olshevsky2009convergence,
	title={Convergence speed in distributed consensus and averaging},
	author={Olshevsky, Alex and Tsitsiklis, John N},
	journal={SIAM Journal on Control and Optimization},
	volume={48},
	number={1},
	pages={33--55},
	year={2009},
	publisher={SIAM}
}

@article{ye2020decentralized,
	title={Decentralized Accelerated Proximal Gradient Descent},
	author={Ye, Haishan and Zhou, Ziang and Luo, Luo and Zhang, Tong},
	journal={Advances in Neural Information Processing Systems},
	volume={33},
	year={2020}
}

@article{lee2013distributed,
	title={Distributed random projection algorithm for convex optimization},
	author={Lee, Soomin and Nedic, Angelia},
	journal={IEEE Journal of Selected Topics in Signal Processing},
	volume={7},
	number={2},
	pages={221--229},
	year={2013},
	publisher={IEEE}
}

@article{aghajan2020distributed,
	title={Distributed Optimization Over Dependent Random Networks},
	author={Aghajan, Adel and Touri, Behrouz},
	journal={arXiv preprint arXiv:2010.01956},
	year={2020}
}

@article{sun2019convergence,
	title={Convergence rate of distributed optimization algorithms based on gradient tracking},
	author={Sun, Ying and Daneshmand, Amir and Scutari, Gesualdo},
	journal={arXiv preprint arXiv:1905.02637},
	year={2019}
}

@article{liu2011accelerated,
	title={Accelerated linear iterations for distributed averaging},
	author={Liu, Ji and Morse, A Stephen},
	journal={Annual Reviews in Control},
	volume={35},
	number={2},
	pages={160--165},
	year={2011},
	publisher={Elsevier}
}

@article{boyd2006randomized,
	title={Randomized gossip algorithms},
	author={Boyd, Stephen and Ghosh, Arpita and Prabhakar, Balaji and Shah, Devavrat},
	journal={IEEE transactions on information theory},
	volume={52},
	number={6},
	pages={2508--2530},
	year={2006},
	publisher={IEEE}
}

@article{gorbunov2019upper,
	title={On the Upper Bound for the Expectation of the Norm of a Vector Uniformly Distributed on the Sphere and the Phenomenon of Concentration of Uniform Measure on the Sphere.},
	author={Gorbunov, EA and Vorontsova, Evgeniya Alekseevna and Gasnikov, Alexander Vladimirovich},
	journal={Mathematical Notes},
	volume={106},
	year={2019}
}

@techreport{tsitsiklis1984problems,
	title={Problems in decentralized decision making and computation.},
	author={Tsitsiklis, John Nikolas},
	year={1984},
	institution={Massachusetts Inst of Tech Cambridge Lab for Information and Decision Systems}
}

@article{xiao2004fast,
	title={Fast linear iterations for distributed averaging},
	author={Xiao, Lin and Boyd, Stephen},
	journal={Systems \& Control Letters},
	volume={53},
	number={1},
	pages={65--78},
	year={2004},
	publisher={Elsevier}
}

@article{muthukrishnan1998first,
	title={First-and second-order diffusive methods for rapid, coarse, distributed load balancing},
	author={Muthukrishnan, Shanmugavelayutham and Ghosh, Bhaskar and Schultz, Martin H},
	journal={Theory of computing systems},
	volume={31},
	number={4},
	pages={331--354},
	year={1998},
	publisher={Springer}
}

@article{li2020decentralized,
	title={Decentralized Accelerated Gradient Methods With Increasing Penalty Parameters},
	author={Li, Huan and Fang, Cong and Yin, Wotao and Lin, Zhouchen},
	journal={IEEE Transactions on Signal Processing},
	volume={68},
	pages={4855--4870},
	year={2020},
	publisher={IEEE}
}

@article{jakovetic2014fast,
	title={Fast distributed gradient methods},
	author={Jakoveti{\'c}, Du{\v{s}}an and Xavier, Joao and Moura, Jos{\'e} MF},
	journal={IEEE Transactions on Automatic Control},
	volume={59},
	number={5},
	pages={1131--1146},
	year={2014},
	publisher={IEEE}
}

@article{rajawat2020primal,
	title={A Primal-Dual Framework for Decentralized Stochastic Optimization},
	author={Rajawat, Ketan and Kumar, Chirag},
	journal={arXiv preprint arXiv:2012.04402},
	year={2020}
}

@article{li2021accelerated,
	title={Accelerated gradient tracking over time-varying graphs for decentralized optimization},
	author={Li, Huan and Lin, Zhouchen},
	journal={arXiv preprint arXiv:2104.02596},
	year={2021}
}

@article{kovalev2021lower,
	title={Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks},
	author={Kovalev, Dmitry and Gasanov, Elnur and Gasnikov, Alexander and Richtarik, Peter},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	year={2021}
}

@article{kovalev2021adom,
	title={ADOM: Accelerated decentralized optimization method for time-varying networks},
	author={Kovalev, Dmitry and Shulgin, Egor and Richt{\'a}rik, Peter and Rogozin, Alexander and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2102.09234},
	year={2021}
}


@article{rogozin2021accelerated,
	title={An Accelerated Method For Decentralized Distributed Stochastic Optimization Over Time-Varying Graphs},
	author={Rogozin, Alexander and Bochko, Mikhail and Dvurechensky, Pavel and Gasnikov, Alexander and Lukoshkin, Vladislav},
	journal={Conference on decision and control},
	year={2021}
}

@article{rogozin2021decentralized,
	title={Decentralized distributed optimization for saddle point problems},
	author={Rogozin, Alexander and Beznosikov, Alexander and Dvinskikh, Darina and Kovalev, Dmitry and Dvurechensky, Pavel and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2102.07758},
	year={2021}
}

@article{liudecentralized,
	title={A Decentralized Proximal Point-type Method for Non-convex Non-concave Saddle Point Problems},
	author={Liu, Weijie and Mokhtari, Aryan and Ozdaglar, Asuman and Pattathil, Sarath and Shen, Zebang and Zheng, Nenggan}
}

@inproceedings{mukherjee2020decentralized,
	title={A decentralized algorithm for large scale min-max problems},
	author={Mukherjee, Soham and Chakraborty, Mrityunjoy},
	booktitle={2020 59th IEEE Conference on Decision and Control (CDC)},
	pages={2967--2972},
	year={2020},
	organization={IEEE}
}

@inproceedings{mateos2015distributed,
	title={Distributed subgradient methods for saddle-point problems},
	author={Mateos-N{\'u}nez, David and Cort{\'e}s, Jorge},
	booktitle={2015 54th IEEE Conference on Decision and Control (CDC)},
	pages={5462--5467},
	year={2015},
	organization={IEEE}
}

@article{wai2018multi,
	title={Multi-agent reinforcement learning via double averaging primal-dual optimization},
	author={Wai, Hoi-To and Yang, Zhuoran and Wang, Zhaoran and Hong, Mingyi},
	journal={arXiv preprint arXiv:1806.00877},
	year={2018}
}

@article{nemirovski2004prox,
	title={Prox-method with rate of convergence $O(1/t)$ for variational inequalities with Lipschitz continuous monotone operators and smooth convex-concave saddle point problems},
	author={Nemirovski, Arkadi},
	journal={SIAM Journal on Optimization},
	volume={15},
	number={1},
	pages={229--251},
	year={2004},
	publisher={SIAM}
}

@article{beznosikov2021distributed,
	title={Distributed Saddle-Point Problems Under Data Similarity},
	author={Beznosikov, Aleksandr and Scutari, Gesualdo and Rogozin, Alexander and Gasnikov, Alexander},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	year={2021}
}


@InProceedings{liu2019decentralized,
	author    = {Liu, Mingrui and Zhang, Wei and Mroueh, Youssef and Cui, Xiaodong and Ross, Jerret and Yang, Tianbao and Das, Payel},
	booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
	title     = {A Decentralized Parallel Algorithm for Training Generative Adversarial Nets},
	year      = {2020},
}

@Article{minty62,
	author    = {George J. Minty},
	journal   = {Duke Mathematical Journal},
	title     = {Monotone (nonlinear) operators in {Hilbert} space},
	year      = {1962},
	number    = {3},
	pages     = {341 -- 346},
	volume    = {29},
	doi       = {10.1215/S0012-7094-62-02933-2},
	publisher = {Duke University Press},
	url       = {https://doi.org/10.1215/S0012-7094-62-02933-2},
}

@article{beznosikov2021decentralized,
	title={Decentralized Local Stochastic Extra-Gradient for Variational Inequalities},
	author={Beznosikov, Aleksandr and Dvurechensky, Pavel and Koloskova, Anastasia and Samokhin, Valentin and Stich, Sebastian U and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2106.08315},
	year={2021}
}

@inproceedings{beznosikov2021optimal,
	title={Near-Optimal Decentralized Algorithms for Saddle Point Problems over Time-Varying Networks},
	author={Beznosikov, Aleksandr and Rogozin, Alexander and Kovalev, Dmitry and Gasnikov, Alexander},
	booktitle={International Conference on Optimization and Applications},
	pages={246--257},
	year={2021},
	organization={Springer}
}

@article{stepanov2021one,
	title={One-Point Gradient-Free Methods for Composite Optimization with Applications to Distributed Optimization},
	author={Stepanov, Ivan and Voronov, Artyom and Beznosikov, Aleksandr and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2107.05951},
	year={2021}
}

@article{liu2019decentralizedprox,
	title={A decentralized proximal point-type method for saddle point problems},
	author={Liu, Weijie and Mokhtari, Aryan and Ozdaglar, Asuman and Pattathil, Sarath and Shen, Zebang and Zheng, Nenggan},
	journal={arXiv preprint arXiv:1910.14380},
	year={2019}
}

@article{lan2021mirror,
	title={Mirror-prox sliding methods for solving a class of monotone variational inequalities},
	author={Lan, Guanghui and Ouyang, Yuyuan},
	journal={arXiv preprint arXiv:2111.00996},
	year={2021}
}

@article{beznosikov2021optimal_,
	title={Optimal Distributed Algorithms for Stochastic Variational Inequalities},
	author={Beznosikov, Alexander and Kovalev, Dmitry and Sadiev, Abdurakhmon and Richtarik, Peter and Gasnikov, Alexander},
	journal={arXiv preprint},
	year={2021}
}

@book{GT-book,
	title={Theory of Games and Economic Behavior (commemorative edition)},
	author={J. von Neumann and  O. Morgenstern and H.W. Kuhn},
	isbn={9780387218151},
	year={2007},
	publisher={Princeton University Press}
}


@article{Arjovsky_et_al2017,
	title={Wasserstein Generative Adversarial Networks},
	author={M. Arjovsky and  S. Chintala and L. Bottou},
	journal={Proceedings of the 34th International Conference on Machine Learning (ICML)},
	volume={70},
	number={1},
	pages={214--223},
	year={2017}
}

@article{chambolle2011first,
	title={A first-order primal-dual algorithm for convex problems with applications to imaging},
	author={Chambolle, Antonin and Pock, Thomas},
	journal={Journal of mathematical imaging and vision},
	volume={40},
	number={1},
	pages={120--145},
	year={2011},
	publisher={Springer}
}

@INPROCEEDINGS{Abadeh_et_al_2015,
	author={S.S. Abadeh and P.M. Esfahani and  D. Kuhn},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS))}, 
	title={Distributionally robust logistic regression}, 
	year={2015},
	volume={},
	number={},
	pages={1576--1584}
}

@book{facchinei2007finite,
	title={Finite-Dimensional Variational Inequalities and Complementarity Problems},
	author={Facchinei, F. and Pang, J.S.},
	isbn={9780387218151},
	series={Springer Series in Operations Research and Financial Engineering},
	url={https://books.google.ru/books?id=lX\_7Rce3\_Q0C},
	year={2007},
	publisher={Springer New York}
}

@INPROCEEDINGS{Bengio2014,
	author={I. Goodfellow and J. Pouget-Abadie and  M. Mirza and  B. Xu and  D. Warde-Farley and  S. Ozair and  A. Courville and Y. Bengio},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS))}, 
	title={Generative Adversarial Nets}, 
	year={2014},
	volume={},
	number={},
	pages={2672--2680}
}

@article{beznosikov2021sim,
	title={Distributed Saddle-Point Problems Under Similarity}, 
	author={Aleksandr Beznosikov and Gesualdo Scutari and Alexander Rogozin and Alexander Gasnikov},
	year={2021},
	journal={arXiv preprint arXiv:2107.10706}
}

@article{tian2021acceleration,
	title={Acceleration in Distributed Optimization Under Similarity},
	author={Tian, Ye and Scutari, Gesualdo and Cao, Tianyu and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2110.12347},
	year={2021}
}

@article{gasnikov2021accelerated,
	title={Accelerated Meta-Algorithm for Convex Optimization Problems},
	author={Gasnikov, AV and Dvinskikh, DM and Dvurechensky, PE and Kamzolov, DI and Matyukhin, VV and Pasechnyuk, DA and Tupitsa, NK and Chernov, AV},
	journal={Computational Mathematics and Mathematical Physics},
	volume={61},
	number={1},
	pages={17--28},
	year={2021},
	publisher={Springer}
}

@inproceedings{lin2020near,
	title={Near-optimal algorithms for minimax optimization},
	author={Lin, Tianyi and Jin, Chi and Jordan, Michael I},
	booktitle={Conference on Learning Theory},
	pages={2738--2779},
	year={2020},
	organization={PMLR}
}

@article{yang2020catalyst,
	title={A catalyst framework for minimax optimization},
	author={Yang, Junchi and Zhang, Siqi and Kiyavash, Negar and He, Niao},
	journal={Advances in Neural Information Processing Systems},
	year={2020}
}

@article{vladislav2021accelerated,
	title={On Accelerated Methods for Saddle-Point Problems with Composite Structure},
	author={Tominin, Vladislav and Tominin, Yaroslav and Borodich, Ekaterina and Kovalev, Dmitry and Gasnikov, Alexander and Dvurechensky, Pavel},
	journal={arXiv preprint arXiv:2103.09344},
	year={2021}
}

@article{song2021optimal,
	title={Optimal Gradient Tracking for Decentralized Optimization},
	author={Song, Zhuoqing and Shi, Lei and Pu, Shi and Yan, Ming},
	journal={arXiv preprint arXiv:2110.05282},
	year={2021}
}

@article{song2021provably,
	title={Provably accelerated decentralized gradient method over unbalanced directed graphs},
	author={Song, Zhuoqing and Shi, Lei and Pu, Shi and Yan, Ming},
	journal={arXiv preprint arXiv:2107.12065},
	year={2021}
}

@article{lin2021quasi,
	title={Quasi-global momentum: Accelerating decentralized deep learning on heterogeneous data},
	author={Lin, Tao and Karimireddy, Sai Praneeth and Stich, Sebastian U and Jaggi, Martin},
	journal={arXiv preprint arXiv:2102.04761},
	year={2021}
}

@article{koloskova2021improved,
	title={An Improved Analysis of Gradient Tracking for Decentralized Machine Learning},
	author={Koloskova, Anastasiia and Lin, Tao and Stich, Sebastian U},
	journal={Advances in Neural Information Processing Systems},
	volume={34},
	year={2021}
	
}

@article{alghunaim2020decentralized,
	title={Decentralized proximal gradient algorithms with linear convergence rates},
	author={Alghunaim, Sulaiman A and Ryu, Ernest K and Yuan, Kun and Sayed, Ali H},
	journal={IEEE Transactions on Automatic Control},
	volume={66},
	number={6},
	pages={2787--2794},
	year={2020},
	publisher={IEEE}
}

@article{qu2017harnessing,
	title={Harnessing smoothness to accelerate distributed optimization},
	author={Qu, Guannan and Li, Na},
	journal={IEEE Transactions on Control of Network Systems},
	volume={5},
	number={3},
	pages={1245--1260},
	year={2017},
	publisher={IEEE}
}

@article{pu2021distributed,
	title={Distributed stochastic gradient tracking methods},
	author={Pu, Shi and Nedi{\'c}, Angelia},
	journal={Mathematical Programming},
	volume={187},
	number={1},
	pages={409--457},
	year={2021},
	publisher={Springer}
}

@article{yufereva2022decentralized,
	title={Decentralized Computation of Wasserstein Barycenter over Time-Varying Networks},
	author={Yufereva, Olga and Persiianov, Michael and Dvurechensky, Pavel and Gasnikov, Alexander and Kovalev, Dmitry},
	journal={arXiv preprint arXiv:2205.15669},
	year={2022}
}

@article{kovalev2022optimal_1,
	title={Optimal algorithms for decentralized stochastic variational inequalities},
	author={Kovalev, Dmitry and Beznosikov, Aleksandr and Sadiev, Abdurakhmon and Persiianov, Michael and Richt{\'a}rik, Peter and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2202.02771},
	year={2022}
}

@article{allen2017katyusha,
	title={Katyusha: The first direct acceleration of stochastic gradient methods},
	author={Allen-Zhu, Zeyuan},
	journal={The Journal of Machine Learning Research},
	volume={18},
	number={1},
	pages={8194--8244},
	year={2017},
	publisher={JMLR. org}
}

@article{zhang2022dual,
	title={A Dual Accelerated Method for Online Stochastic Distributed Averaging: From Consensus to Decentralized Policy Evaluation},
	author={Zhang, Sheng and Pananjady, Ashwin and Romberg, Justin},
	journal={arXiv preprint arXiv:2207.11425},
	year={2022}
}

@article{beznosikov2021distributed_2,
	title={Distributed saddle-point problems: Lower bounds, optimal algorithms and federated gans},
	author={Beznosikov, Aleksandr and Samokhin, Valentin and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2010.13112},
	year={2021}
}

@article{kovalev2021accelerated,
	title={Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling},
	author={Kovalev, Dmitry and Gasnikov, Alexander and Richt{\'a}rik, Peter},
	journal={arXiv preprint arXiv:2112.15199},
	year={2021}
}

@article{thekumparampil2022lifted,
	title={Lifted Primal-Dual Method for Bilinearly Coupled Smooth Minimax Optimization},
	author={Thekumparampil, Kiran Koshy and He, Niao and Oh, Sewoong},
	journal={arXiv preprint arXiv:2201.07427},
	year={2022}
}

@article{jin2022sharper,
	title={Sharper Rates for Separable Minimax and Finite Sum Optimization via Primal-Dual Extragradient Methods},
	author={Jin, Yujia and Sidford, Aaron and Tian, Kevin},
	journal={arXiv preprint arXiv:2202.04640},
	year={2022}
}

@article{zhang2019lower,
	title={On lower iteration complexity bounds for the saddle point problems},
	author={Zhang, Junyu and Hong, Mingyi and Zhang, Shuzhong},
	journal={arXiv preprint arXiv:1912.07481},
	year={2019}
}

@article{metelev2022decentralized,
	title={Decentralized Saddle-Point Problems with Different Constants of Strong Convexity and Strong Concavity},
	author={Metelev, Dmitriy and Rogozin, Alexander and Gasnikov, Alexander and Kovalev, Dmitry},
	journal={arXiv preprint arXiv:2206.00090},
	year={2022}
}

@inproceedings{carmon2022recapp,
	title={Recapp: Crafting a more efficient catalyst for convex optimization},
	author={Carmon, Yair and Jambulapati, Arun and Jin, Yujia and Sidford, Aaron},
	booktitle={International Conference on Machine Learning},
	pages={2658--2685},
	year={2022},
	organization={PMLR}
}

@article{wu2019fenchel,
	title={Fenchel dual gradient methods for distributed convex optimization over time-varying networks},
	author={Wu, Xuyang and Lu, Jie},
	journal={IEEE Transactions on Automatic Control},
	volume={64},
	number={11},
	pages={4629--4636},
	year={2019},
	publisher={IEEE}
}

@article{kuruzov2022gradient,
	title={Gradient-Type Methods For Decentralized Optimization Problems With Polyak-Lojasiewicz Condition Over Time-Varying Networks},
	author={Kuruzov, Ilya and Alkousa, Mohammad and Stonyakin, Fedor and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:1911.08527},
	year={2019}
}

@article{nedic2010cooperative,
	title={Cooperative distributed multi-agent optimization},
	author={Nedic, Angelia and Ozdaglar, Asuman},
	journal={Convex optimization in signal processing and communications},
	volume={340},
	year={2010},
	publisher={Cambridge Univ. Press}
}

@article{sun2022distributed,
	title={Distributed optimization based on gradient tracking revisited: Enhancing convergence rate via surrogation},
	author={Sun, Ying and Scutari, Gesualdo and Daneshmand, Amir},
	journal={SIAM Journal on Optimization},
	volume={32},
	number={2},
	pages={354--385},
	year={2022},
	publisher={SIAM}
}

@article{scutari2019distributed,
	title={Distributed nonconvex constrained optimization over time-varying digraphs},
	author={Scutari, Gesualdo and Sun, Ying},
	journal={Mathematical Programming},
	volume={176},
	number={1},
	pages={497--544},
	year={2019},
	publisher={Springer}
}

@inproceedings{sun2016distributed,
	title={Distributed nonconvex multiagent optimization over time-varying networks},
	author={Sun, Ying and Scutari, Gesualdo and Palomar, Daniel},
	booktitle={2016 50th Asilomar Conference on Signals, Systems and Computers},
	pages={788--794},
	year={2016},
	organization={IEEE}
}

@inproceedings{tian2022acceleration,
	title={Acceleration in distributed optimization under similarity},
	author={Tian, Ye and Scutari, Gesualdo and Cao, Tianyu and Gasnikov, Alexander},
	booktitle={International Conference on Artificial Intelligence and Statistics},
	pages={5721--5756},
	year={2022},
	organization={PMLR}
}

@inproceedings{woodworth2020local,
	title={Is local SGD better than minibatch SGD?},
	author={Woodworth, Blake and Patel, Kumar Kshitij and Stich, Sebastian and Dai, Zhen and Bullins, Brian and Mcmahan, Brendan and Shamir, Ohad and Srebro, Nathan},
	booktitle={International Conference on Machine Learning},
	pages={10334--10343},
	year={2020},
	organization={PMLR}
}

@article{lin2018don,
	title={Don't use large mini-batches, use local sgd},
	author={Lin, Tao and Stich, Sebastian U and Patel, Kumar Kshitij and Jaggi, Martin},
	journal={arXiv preprint arXiv:1808.07217},
	year={2018}
}

@inproceedings{trimbach2021acceleration,
	title={An Acceleration of Decentralized SGD Under General Assumptions with Low Stochastic Noise},
	author={Trimbach, Ekaterina and Rogozin, Alexander},
	booktitle={International Conference on Mathematical Optimization Theory and Operations Research},
	pages={117--128},
	year={2021},
	organization={Springer}
}

@article{nedic2014distributed,
	title={Distributed optimization over time-varying directed graphs},
	author={Nedi{\'c}, Angelia and Olshevsky, Alex},
	journal={IEEE Transactions on Automatic Control},
	volume={60},
	number={3},
	pages={601--615},
	year={2014},
	publisher={IEEE}
}

@article{nedic2009subgradient,
	title={Subgradient methods for saddle-point problems},
	author={Nedi{\'c}, Angelia and Ozdaglar, Asuman},
	journal={Journal of optimization theory and applications},
	volume={142},
	number={1},
	pages={205--228},
	year={2009},
	publisher={Springer}
}

@article{pu2020push,
	title={Push--pull gradient methods for distributed optimization in networks},
	author={Pu, Shi and Shi, Wei and Xu, Jinming and Nedi{\'c}, Angelia},
	journal={IEEE Transactions on Automatic Control},
	volume={66},
	number={1},
	pages={1--16},
	year={2020},
	publisher={IEEE}
}

@article{nedich2022ab,
	title={AB/Push-Pull Method for Distributed Optimization in Time-Varying Directed Networks},
	author={Nedich, Angelia and Nguyen, Duong Thuy Anh and Nguyen, Duong Tung},
	journal={arXiv preprint arXiv:2209.06974},
	year={2022}
}

@article{kovalev2022optimal_2,
	title={Optimal Gradient Sliding and its Application to Distributed Optimization Under Similarity},
	author={Kovalev, Dmitry and Beznosikov, Aleksandr and Borodich, Ekaterina and Gasnikov, Alexander and Scutari, Gesualdo},
	journal={arXiv preprint arXiv:2205.15136},
	year={2022}
}

@inproceedings{beznosikov2021near,
	title={Near-optimal decentralized algorithms for saddle point problems over time-varying networks},
	author={Beznosikov, Aleksandr and Rogozin, Alexander and Kovalev, Dmitry and Gasnikov, Alexander},
	booktitle={International Conference on Optimization and Applications},
	pages={246--257},
	year={2021},
	organization={Springer}
}

@article{cai2014average,
	title={Average consensus on arbitrary strongly connected digraphs with time-varying topologies},
	author={Cai, Kai and Ishii, Hideaki},
	journal={IEEE Transactions on Automatic Control},
	volume={59},
	number={4},
	pages={1066--1071},
	year={2014},
	publisher={IEEE}
}

@article{olshevsky2014linear,
	title={Linear time average consensus on fixed graphs and implications for decentralized optimization and multi-agent control},
	author={Olshevsky, Alex},
	journal={arXiv preprint arXiv:1411.4186},
	year={2014}
}

@article{bazerque2009distributed,
	title={Distributed spectrum sensing for cognitive radio networks by exploiting sparsity},
	author={Bazerque, Juan Andr{\'e}s and Giannakis, Georgios B},
	journal={IEEE Transactions on Signal Processing},
	volume={58},
	number={3},
	pages={1847--1862},
	year={2009},
	publisher={IEEE}
}

@article{gan2012optimal,
	title={Optimal decentralized protocol for electric vehicle charging},
	author={Gan, Lingwen and Topcu, Ufuk and Low, Steven H},
	journal={IEEE Transactions on Power Systems},
	volume={28},
	number={2},
	pages={940--951},
	year={2012},
	publisher={IEEE}
}

@article{forero2010consensus,
	title={Consensus-Based Distributed Support Vector Machines.},
	author={Forero, Pedro A and Cano, Alfonso and Giannakis, Georgios B},
	journal={Journal of Machine Learning Research},
	volume={11},
	number={5},
	year={2010}
}

@inproceedings{rabbat2004distributed,
	title={Distributed optimization in sensor networks},
	author={Rabbat, Michael and Nowak, Robert},
	booktitle={Proceedings of the 3rd international symposium on Information processing in sensor networks},
	pages={20--27},
	year={2004}
}

@article{tsitsiklis1986distributed,
	title={Distributed asynchronous deterministic and stochastic gradient optimization algorithms},
	author={Tsitsiklis, John and Bertsekas, Dimitri and Athans, Michael},
	journal={IEEE transactions on automatic control},
	volume={31},
	number={9},
	pages={803--812},
	year={1986},
	publisher={IEEE}
}

@article{kuruzov2022mirror,
	title={The Mirror-Prox Sliding Method for Non-smooth decentralized saddle-point problems},
	author={Kuruzov, Ilya and Rogozin, Alexander and Yarmoshik, Demyan and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2210.06086},
	year={2022}
}

@article{yarmoshik2022decentralized,
	title={Decentralized convex optimization under affine constraints for power systems control},
	author={Yarmoshik, Demyan and Rogozin, Alexander and Khamisov, Oleg and Dvurechensky, Pavel and Gasnikov, Alexander and others},
	journal={arXiv preprint arXiv:2203.16686},
	year={2022}
}

@article{rogozin2022decentralized,
	title={Decentralized Strongly-Convex Optimization with Affine Constraints: Primal and Dual Approaches},
	author={Rogozin, Alexander and Yarmoshik, Demyan and Kopylova, Ksenia and Gasnikov, Alexander},
	journal={arXiv preprint arXiv:2207.04555},
	year={2022}
}

@article{necoara2011parallel,
	title={Parallel and distributed optimization methods for estimation and control in networks},
	author={Necoara, Ion and Nedelcu, Valentin and Dumitrache, Ioan},
	journal={Journal of Process Control},
	volume={21},
	number={5},
	pages={756--766},
	year={2011},
	publisher={Elsevier}
}

@article{necoara2014distributed,
	title={Distributed dual gradient methods and error bound conditions},
	author={Necoara, Ion and Nedelcu, Valentin},
	journal={arXiv preprint arXiv:1401.4398},
	year={2014}
}

@article{necoara2015linear,
	title={On linear convergence of a distributed dual gradient algorithm for linearly constrained separable convex problems},
	author={Necoara, Ion and Nedelcu, Valentin},
	journal={Automatica},
	volume={55},
	pages={209--216},
	year={2015},
	publisher={Elsevier}
}

@article{csiba2018importance,
	title={Importance sampling for minibatches},
	author={Csiba, Dominik and Richt{\'a}rik, Peter},
	journal={The Journal of Machine Learning Research},
	volume={19},
	number={1},
	pages={962--982},
	year={2018},
	publisher={JMLR. org}
}

@incollection{gorbunov2022recent,
	title={Recent theoretical advances in decentralized distributed convex optimization},
	author={Gorbunov, Eduard and Rogozin, Alexander and Beznosikov, Aleksandr and Dvinskikh, Darina and Gasnikov, Alexander},
	booktitle={High-Dimensional Optimization and Probability},
	pages={253--325},
	year={2022},
	publisher={Springer}
}

@article{olfati2004consensus,
	title={Consensus problems in networks of agents with switching topology and time-delays},
	author={Olfati-Saber, Reza and Murray, Richard M},
	journal={IEEE Transactions on automatic control},
	volume={49},
	number={9},
	pages={1520--1533},
	year={2004},
	publisher={IEEE}
}

@article{jadbabaie2003coordination,
	title={Coordination of groups of mobile autonomous agents using nearest neighbor rules},
	author={Jadbabaie, Ali and Lin, Jie and Morse, A Stephen},
	journal={IEEE Transactions on automatic control},
	volume={48},
	number={6},
	pages={988--1001},
	year={2003},
	publisher={IEEE}
}

@book{ren2008distributed,
	title={Distributed consensus in multi-vehicle cooperative control},
	author={Ren, Wei and Beard, Randal W},
	volume={27},
	number={2},
	year={2008},
	publisher={Springer}
}

@book{ren2011distributed,
	title={Distributed coordination of multi-agent networks: emergent problems, models, and issues},
	author={Ren, Wei and Cao, Yongcan},
	volume={1},
	year={2011},
	publisher={Springer}
}

@article{proskurnikov2021delay,
	title={Delay Robustness of Consensus Algorithms: Beyond The Uniform Connectivity (Extended Version)},
	author={Proskurnikov, Anton V and Calafiore, Giuseppe Carlo},
	journal={arXiv preprint arXiv:2105.07183},
	year={2021}
}

@article{kia2019tutorial,
	title={Tutorial on dynamic average consensus: The problem, its applications, and the algorithms},
	author={Kia, Solmaz S and Van Scoy, Bryan and Cortes, Jorge and Freeman, Randy A and Lynch, Kevin M and Martinez, Sonia},
	journal={IEEE Control Systems Magazine},
	volume={39},
	number={3},
	pages={40--72},
	year={2019},
	publisher={IEEE}
}

@article{d2021acceleration,
	title={Acceleration methods},
	author={d'Aspremont, Alexandre and Scieur, Damien and Taylor, Adrien and others},
	journal={Foundations and Trends{\textregistered} in Optimization},
	volume={5},
	number={1-2},
	pages={1--245},
	year={2021},
	publisher={Now Publishers, Inc.}
}

@article{rogozin2019optimal,
	title={Optimal distributed convex optimization on slowly time-varying graphs},
	author={Rogozin, Alexander and Uribe, C{\'e}sar A and Gasnikov, Alexander V and Malkovsky, Nikolay and Nedi{\'c}, Angelia},
	journal={IEEE Transactions on Control of Network Systems},
	volume={7},
	number={2},
	pages={829--841},
	year={2019},
	publisher={IEEE}
}

