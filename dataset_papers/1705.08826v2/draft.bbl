\begin{thebibliography}{10}

\bibitem{bartlett2006convexity}
P.~L. Bartlett, M.~I. Jordan, and J.~D. McAuliffe.
\newblock Convexity, classification, and risk bounds.
\newblock {\em Journal of the American Statistical Association},
  101(473):138--156, 2006.

\bibitem{bengio2009curriculum}
Y.~Bengio, J.~Louradour, R.~Collobert, and J.~Weston.
\newblock Curriculum learning.
\newblock In {\em ICML}, pages 41--48, 2009.

\bibitem{bousquet2008tradeoffs}
O.~Bousquet and L.~Bottou.
\newblock The tradeoffs of large scale learning.
\newblock In {\em NIPS}, pages 161--168, 2008.

\bibitem{chang2011libsvm}
C.-C. Chang and C.-J. Lin.
\newblock Libsvm: a library for support vector machines.
\newblock {\em TIST}, 2(3):27, 2011.

\bibitem{cortes1995support}
C.~Cortes and V.~Vapnik.
\newblock Support-vector networks.
\newblock {\em Machine learning}, 20(3):273--297, 1995.

\bibitem{crammer2001algorithmic}
K.~Crammer and Y.~Singer.
\newblock On the algorithmic implementation of multiclass kernel-based vector
  machines.
\newblock {\em Journal of machine learning research}, 2(Dec):265--292, 2001.

\bibitem{de2005model}
E.~De~Vito, A.~Caponnetto, and L.~Rosasco.
\newblock Model selection for regularized least-squares algorithm in learning
  theory.
\newblock {\em Foundations of Computational Mathematics}, 5(1):59--85, 2005.

\bibitem{devroye2013probabilistic}
L.~Devroye, L.~Gy{\"o}rfi, and G.~Lugosi.
\newblock {\em A probabilistic theory of pattern recognition}, volume~31.
\newblock Springer Science \& Business Media, 2013.

\bibitem{spl2017}
Y.~Fan, R.~He, J.~Liang, and B.-G. Hu.
\newblock Self-paced learning: An implicit regularization perspective.
\newblock In {\em AAAI}, pages 1877--1833, 2017.

\bibitem{he2011maximum}
R.~He, W.-S. Zheng, and B.-G. Hu.
\newblock Maximum correntropy criterion for robust face recognition.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  33(8):1561--1576, 2011.

\bibitem{kumar2010self}
M.~P. Kumar, B.~Packer, and D.~Koller.
\newblock Self-paced learning for latent variable models.
\newblock In {\em NIPS}, pages 1189--1197, 2010.

\bibitem{lapin2015top}
M.~Lapin, M.~Hein, and B.~Schiele.
\newblock Top-k multiclass {SVM}.
\newblock In {\em NIPS}, pages 325--333, 2015.

\bibitem{lapin2016loss}
M.~Lapin, M.~Hein, and B.~Schiele.
\newblock Loss functions for top-k error: Analysis and insights.
\newblock In {\em CVPR}, pages 1468--1477, 2016.

\bibitem{lin}
Y.~Lin.
\newblock A note on margin-based loss functions in classification.
\newblock {\em Statistics \& probability letters}, 68(1):73--82, 2004.

\bibitem{masnadi2009design}
H.~Masnadi-Shirazi and N.~Vasconcelos.
\newblock On the design of loss functions for classification: theory,
  robustness to outliers, and savageboost.
\newblock In {\em NIPS}, pages 1049--1056, 2009.

\bibitem{Ogryczak:2003dl}
W.~Ogryczak and A.~Tamir.
\newblock Minimizing the sum of the k largest functions in linear time.
\newblock {\em Information Processing Letters}, 85(3):117--122, 2003.

\bibitem{rudin2009p}
C.~Rudin.
\newblock The p-norm push: A simple convex ranking algorithm that concentrates
  at the top of the list.
\newblock {\em Journal of Machine Learning Research}, 10(Oct):2233--2271, 2009.

\bibitem{scholkopf2001learning}
B.~Sch{\"o}lkopf and A.~J. Smola.
\newblock {\em Learning with kernels: support vector machines, regularization,
  optimization, and beyond}.
\newblock MIT press, 2001.

\bibitem{scholkopf2000new}
B.~Sch{\"o}lkopf, A.~J. Smola, R.~C. Williamson, and P.~L. Bartlett.
\newblock New support vector algorithms.
\newblock {\em Neural computation}, 12(5):1207--1245, 2000.

\bibitem{shalev2016minimizing}
S.~Shalev-Shwartz and Y.~Wexler.
\newblock Minimizing the maximal loss: How and why.
\newblock In {\em ICML}, 2016.

\bibitem{srebro2010stochastic}
N.~Srebro and A.~Tewari.
\newblock Stochastic optimization for machine learning.
\newblock {\em ICML Tutorial}, 2010.

\bibitem{steinwart2003optimal}
I.~Steinwart.
\newblock On the optimal parameter choice for $\nu$-support vector machines.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence},
  25(10):1274--1284, 2003.

\bibitem{steinwart2008support}
I.~Steinwart and A.~Christmann.
\newblock {\em Support vector machines}.
\newblock Springer Science \& Business Media, 2008.

\bibitem{usunier2009ranking}
N.~Usunier, D.~Buffoni, and P.~Gallinari.
\newblock Ranking with ordered weighted pairwise classification.
\newblock In {\em ICML}, pages 1057--1064, 2009.

\bibitem{vapnik}
V.~Vapnik.
\newblock {\em Statistical learning theory}, volume~1.
\newblock Wiley New York, 1998.

\bibitem{wu2006learning}
Q.~Wu, Y.~Ying, and D.-X. Zhou.
\newblock Learning rates of least-square regularized regression.
\newblock {\em Foundations of Computational Mathematics}, 6(2):171--192, 2006.

\bibitem{wu2007robust}
Y.~Wu and Y.~Liu.
\newblock Robust truncated hinge loss support vector machines.
\newblock {\em Journal of the American Statistical Association},
  102(479):974--983, 2007.

\bibitem{yang2010relaxed}
Y.~Yu, M.~Yang, L.~Xu, M.~White, and D.~Schuurmans.
\newblock Relaxed clipping: A global training method for robust regression and
  classification.
\newblock In {\em NIPS}, pages 2532--2540, 2010.

\end{thebibliography}
