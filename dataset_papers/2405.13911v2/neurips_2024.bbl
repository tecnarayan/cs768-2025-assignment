\begin{thebibliography}{90}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
J.-B. Alayrac, J.~Donahue, P.~Luc, A.~Miech, I.~Barr, Y.~Hasson, K.~Lenc, A.~Mensch, K.~Millican, M.~Reynolds, et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 23716--23736, 2022.

\bibitem[Anil et~al.(2023)Anil, Dai, Firat, Johnson, Lepikhin, Passos, Shakeri, Taropa, Bailey, Chen, et~al.]{anil2023palm}
R.~Anil, A.~M. Dai, O.~Firat, M.~Johnson, D.~Lepikhin, A.~Passos, S.~Shakeri, E.~Taropa, P.~Bailey, Z.~Chen, et~al.
\newblock {PaLM} 2 technical report.
\newblock \emph{arXiv preprint arXiv:2305.10403}, 2023.

\bibitem[Bain et~al.(2021)Bain, Nagrani, Varol, and Zisserman]{webvid}
M.~Bain, A.~Nagrani, G.~Varol, and A.~Zisserman.
\newblock Frozen in time: A joint video and image encoder for end-to-end retrieval.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 1728--1738, 2021.

\bibitem[Balazevic et~al.(2024)Balazevic, Shi, Papalampidi, Chaabouni, Koppula, and Henaff]{MC:MemoryConsolidation}
I.~Balazevic, Y.~Shi, P.~Papalampidi, R.~Chaabouni, S.~Koppula, and O.~J. Henaff.
\newblock Memory consolidation enables long-context video understanding.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Brooks et~al.(2023)Brooks, Holynski, and Efros]{instructpix2pix}
T.~Brooks, A.~Holynski, and A.~A. Efros.
\newblock Instructpix2pix: Learning to follow image editing instructions.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 18392--18402, 2023.

\bibitem[Changpinyo et~al.(2021)Changpinyo, Sharma, Ding, and Soricut]{cc12m}
S.~Changpinyo, P.~Sharma, N.~Ding, and R.~Soricut.
\newblock Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 3558--3568, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Zheng, Wang, Xu, Huang, Pan, Wang, Wang, Qiao, Lu, et~al.]{chen2023videollm}
G.~Chen, Y.-D. Zheng, J.~Wang, J.~Xu, Y.~Huang, J.~Pan, Y.~Wang, Y.~Wang, Y.~Qiao, T.~Lu, et~al.
\newblock {VideoLLM}: Modeling video sequence with large language models.
\newblock \emph{arXiv preprint arXiv:2305.13292}, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Wang, Changpinyo, Piergiovanni, Padlewski, Salz, Goodman, Grycner, Mustafa, Beyer, Kolesnikov, Puigcerver, Ding, Rong, Akbari, Mishra, Xue, Thapliyal, Bradbury, Kuo, Seyedhosseini, Jia, Ayan, Ruiz, Steiner, Angelova, Zhai, Houlsby, and Soricut]{chen2022pali}
X.~Chen, X.~Wang, S.~Changpinyo, A.~Piergiovanni, P.~Padlewski, D.~Salz, S.~Goodman, A.~Grycner, B.~Mustafa, L.~Beyer, A.~Kolesnikov, J.~Puigcerver, N.~Ding, K.~Rong, H.~Akbari, G.~Mishra, L.~Xue, A.~V. Thapliyal, J.~Bradbury, W.~Kuo, M.~Seyedhosseini, C.~Jia, B.~K. Ayan, C.~R. Ruiz, A.~P. Steiner, A.~Angelova, X.~Zhai, N.~Houlsby, and R.~Soricut.
\newblock Pa{LI}: A jointly-scaled multilingual language-image model.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{b}}.

\bibitem[Choudhury et~al.(2023)Choudhury, Niinuma, Kitani, and Jeni]{agent_proviq}
R.~Choudhury, K.~Niinuma, K.~M. Kitani, and L.~A. Jeni.
\newblock Zero-shot video question answering with procedural programs.
\newblock \emph{arXiv preprint arXiv:2312.00937}, 2023.

\bibitem[Chung et~al.(2024)Chung, Hou, Longpre, Zoph, Tay, Fedus, Li, Wang, Dehghani, Brahma, et~al.]{T5}
H.~W. Chung, L.~Hou, S.~Longpre, B.~Zoph, Y.~Tay, W.~Fedus, Y.~Li, X.~Wang, M.~Dehghani, S.~Brahma, et~al.
\newblock Scaling instruction-finetuned language models.
\newblock \emph{Journal of Machine Learning Research}, 25\penalty0 (70):\penalty0 1--53, 2024.

\bibitem[Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{dai2024instructblip}
W.~Dai, J.~Li, D.~Li, A.~M.~H. Tiong, J.~Zhao, W.~Wang, B.~Li, P.~N. Fung, and S.~Hoi.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Fan et~al.(2023)Fan, Krishnan, Isola, Katabi, and Tian]{cliprewrire}
L.~Fan, D.~Krishnan, P.~Isola, D.~Katabi, and Y.~Tian.
\newblock Improving clip training with language rewrites.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Fan et~al.(2025)Fan, Ma, Wu, Du, Li, Gao, and Li]{videoagent2}
Y.~Fan, X.~Ma, R.~Wu, Y.~Du, J.~Li, Z.~Gao, and Q.~Li.
\newblock Videoagent: A memory-augmented multimodal agent for video understanding.
\newblock In \emph{European Conference on Computer Vision}, volume 15080 of \emph{Lecture Notes in Computer Science}, pages 75--92. Springer, 2025.

\bibitem[Gao et~al.(2017)Gao, Sun, Yang, and Nevatia]{charades_sta}
J.~Gao, C.~Sun, Z.~Yang, and R.~Nevatia.
\newblock Tall: Temporal activity localization via language query.
\newblock In \emph{Proceedings of the IEEE international conference on computer vision}, pages 5267--5275, 2017.

\bibitem[Grauman et~al.(2022)Grauman, Westbury, Byrne, Chavis, Furnari, Girdhar, Hamburger, Jiang, Liu, Liu, et~al.]{ego4d}
K.~Grauman, A.~Westbury, E.~Byrne, Z.~Chavis, A.~Furnari, R.~Girdhar, J.~Hamburger, H.~Jiang, M.~Liu, X.~Liu, et~al.
\newblock Ego4d: Around the world in 3,000 hours of egocentric video.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 18995--19012, 2022.

\bibitem[Gu et~al.(2023)Gu, Clark, and Kembhavi]{gapnoimages}
S.~Gu, C.~Clark, and A.~Kembhavi.
\newblock I can't believe there's no images! learning visual tasks using only language supervision.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 2672--2683, 2023.

\bibitem[Han et~al.(2022)Han, Xie, and Zisserman]{han2022temporal_align}
T.~Han, W.~Xie, and A.~Zisserman.
\newblock Temporal alignment networks for long-term video.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 2906--2916, 2022.

\bibitem[Huang et~al.(2020)Huang, Xiong, Rao, Wang, and Lin]{movienet}
Q.~Huang, Y.~Xiong, A.~Rao, J.~Wang, and D.~Lin.
\newblock {MovieNet}: {A} holistic dataset for movie understanding.
\newblock In \emph{European Conference Computer Vision}, volume 12357 of \emph{Lecture Notes in Computer Science}, pages 709--727. Springer, 2020.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and Duerig]{align}
C.~Jia, Y.~Yang, Y.~Xia, Y.-T. Chen, Z.~Parekh, H.~Pham, Q.~Le, Y.-H. Sung, Z.~Li, and T.~Duerig.
\newblock Scaling up visual and vision-language representation learning with noisy text supervision.
\newblock In \emph{International conference on machine learning}, pages 4904--4916. PMLR, 2021.

\bibitem[Jiang et~al.(2024)Jiang, Sablayrolles, Roux, Mensch, Savary, Bamford, Chaplot, Casas, Hanna, Bressand, et~al.]{jiang2024mixtral}
A.~Q. Jiang, A.~Sablayrolles, A.~Roux, A.~Mensch, B.~Savary, C.~Bamford, D.~S. Chaplot, D.~d.~l. Casas, E.~B. Hanna, F.~Bressand, et~al.
\newblock Mixtral of experts.
\newblock \emph{arXiv preprint arXiv:2401.04088}, 2024.

\bibitem[Kahatapitiya et~al.(2024)Kahatapitiya, Ranasinghe, Park, and Ryoo]{langrepo}
K.~Kahatapitiya, K.~Ranasinghe, J.~Park, and M.~S. Ryoo.
\newblock Language repository for long video understanding.
\newblock \emph{arXiv preprint arXiv:2403.14622}, 2024.

\bibitem[Kim et~al.(2024)Kim, Choi, Lee, and Rhee]{imagegrid}
W.~Kim, C.~Choi, W.~Lee, and W.~Rhee.
\newblock An image grid can be worth a video: Zero-shot video question answering using a vlm.
\newblock \emph{arXiv preprint arXiv:2403.18406}, 2024.

\bibitem[Ko et~al.(2023)Ko, Lee, Kang, Roh, and Kim]{LLamavqa}
D.~Ko, J.~S. Lee, W.-Y. Kang, B.~Roh, and H.~J. Kim.
\newblock Large language models are temporal and causal reasoners for video question answering.
\newblock In \emph{The 2023 Conference on Empirical Methods in Natural Language Processing}, 2023.

\bibitem[Lei et~al.(2018)Lei, Yu, Bansal, and Berg]{tvqa}
J.~Lei, L.~Yu, M.~Bansal, and T.~L. Berg.
\newblock Tvqa: Localized, compositional video question answering.
\newblock In \emph{EMNLP}, 2018.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Zhang, Chen, Wang, Pu, Yang, Li, and Liu]{otter}
B.~Li, Y.~Zhang, L.~Chen, J.~Wang, F.~Pu, J.~Yang, C.~Li, and Z.~Liu.
\newblock Mimic-it: Multi-modal in-context instruction tuning.
\newblock \emph{arXiv preprint arXiv:2306.05425}, 2023{\natexlab{a}}.

\bibitem[Li et~al.(2022)Li, Li, Xiong, and Hoi]{blip1}
J.~Li, D.~Li, C.~Xiong, and S.~Hoi.
\newblock {BLIP}: Bootstrapping language-image pre-training for unified vision-language understanding and generation.
\newblock In \emph{International conference on machine learning}, pages 12888--12900. PMLR, 2022.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Li, Savarese, and Hoi]{li2023blip}
J.~Li, D.~Li, S.~Savarese, and S.~Hoi.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.
\newblock In \emph{International conference on machine learning}, pages 19730--19742. PMLR, 2023{\natexlab{b}}.

\bibitem[Li et~al.(2023{\natexlab{c}})Li, He, Wang, Li, Wang, Luo, Wang, Wang, and Qiao]{li2023videochat}
K.~Li, Y.~He, Y.~Wang, Y.~Li, W.~Wang, P.~Luo, Y.~Wang, L.~Wang, and Y.~Qiao.
\newblock {VideoChat}: Chat-centric video understanding.
\newblock \emph{arXiv preprint arXiv:2305.06355}, 2023{\natexlab{c}}.

\bibitem[Li et~al.(2024)Li, Wang, He, Li, Wang, Liu, Wang, Xu, Chen, Lou, Wang, and Qiao]{li2023mvbench}
K.~Li, Y.~Wang, Y.~He, Y.~Li, Y.~Wang, Y.~Liu, Z.~Wang, J.~Xu, G.~Chen, P.~Lou, L.~Wang, and Y.~Qiao.
\newblock {MVBench}: {A} comprehensive multi-modal video understanding benchmark.
\newblock In \emph{{IEEE/CVF} Conference on Computer Vision and Pattern Recognition}, pages 22195--22206, 2024.

\bibitem[Li et~al.(2023{\natexlab{d}})Li, Zhu, Wen, and Yang]{li2023decap}
W.~Li, L.~Zhu, L.~Wen, and Y.~Yang.
\newblock Decap: Decoding clip latents for zero-shot captioning via text-only training.
\newblock In \emph{The Eleventh International Conference on Learning Representations}, 2023{\natexlab{d}}.

\bibitem[Liang et~al.(2022)Liang, Zhang, Kwon, Yeung, and Zou]{mindthegap}
V.~W. Liang, Y.~Zhang, Y.~Kwon, S.~Yeung, and J.~Y. Zou.
\newblock Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 17612--17625, 2022.

\bibitem[Lin et~al.(2023)Lin, Zhu, Ye, Ning, Jin, and Yuan]{video-llava}
B.~Lin, B.~Zhu, Y.~Ye, M.~Ning, P.~Jin, and L.~Yuan.
\newblock Video-llava: Learning united visual representation by alignment before projection.
\newblock \emph{arXiv preprint arXiv:2311.10122}, 2023.

\bibitem[Lin et~al.(2024)Lin, Zhang, Huang, Liu, Wen, and Peng]{lin2024noise_align}
Y.~Lin, J.~Zhang, Z.~Huang, J.~Liu, Z.~Wen, and X.~Peng.
\newblock Multi-granularity correspondence learning from long-term noisy videos.
\newblock In \emph{Proceedings of the International Conference on Learning Representations}, May 2024.

\bibitem[Liu et~al.(2023)Liu, Li, Wu, and Lee]{llava}
H.~Liu, C.~Li, Q.~Wu, and Y.~J. Lee.
\newblock Visual instruction tuning.
\newblock \emph{Advances in neural information processing systems}, 36, 2023.

\bibitem[Liu et~al.(2019)Liu, Shahroudy, Perez, Wang, Duan, and Kot]{ntu_rgbd}
J.~Liu, A.~Shahroudy, M.~Perez, G.~Wang, L.-Y. Duan, and A.~C. Kot.
\newblock Ntu rgb+ d 120: A large-scale benchmark for 3d human activity understanding.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 42\penalty0 (10):\penalty0 2684--2701, 2019.

\bibitem[Loshchilov and Hutter(2019)]{adamw}
I.~Loshchilov and F.~Hutter.
\newblock Decoupled weight decay regularization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Luo et~al.(2023)Luo, Zhao, Yang, Dong, Qiu, Lu, Wang, and Wei]{luo2023valley_videoinstruct}
R.~Luo, Z.~Zhao, M.~Yang, J.~Dong, M.~Qiu, P.~Lu, T.~Wang, and Z.~Wei.
\newblock Valley: Video assistant with large language model enhanced ability.
\newblock \emph{arXiv preprint arXiv:2306.07207}, 2023.

\bibitem[Maaz et~al.(2023)Maaz, Rasheed, Khan, and Khan]{videochatgpt}
M.~Maaz, H.~Rasheed, S.~Khan, and F.~S. Khan.
\newblock Video-chatgpt: Towards detailed video understanding via large vision and language models.
\newblock \emph{arXiv preprint arXiv:2306.05424}, 2023.

\bibitem[Mangalam et~al.(2023)Mangalam, Akshulakov, and Malik]{EgoSchema}
K.~Mangalam, R.~Akshulakov, and J.~Malik.
\newblock Egoschema: A diagnostic benchmark for very long-form video language understanding.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Miech et~al.(2019)Miech, Zhukov, Alayrac, Tapaswi, Laptev, and Sivic]{miech2019howto100m}
A.~Miech, D.~Zhukov, J.-B. Alayrac, M.~Tapaswi, I.~Laptev, and J.~Sivic.
\newblock Howto100m: Learning a text-video embedding by watching hundred million narrated video clips.
\newblock In \emph{Proceedings of the IEEE/CVF international conference on computer vision}, pages 2630--2640, 2019.

\bibitem[Miller(1995)]{miller1995wordnet}
G.~A. Miller.
\newblock {WordNet}: a lexical database for english.
\newblock \emph{Communications of the ACM}, 1995.

\bibitem[Min et~al.(2024)Min, Buch, Nagrani, Cho, and Schmid]{morevqa}
J.~Min, S.~Buch, A.~Nagrani, M.~Cho, and C.~Schmid.
\newblock Morevqa: Exploring modular reasoning models for video question answering.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 2024.

\bibitem[Monfort et~al.(2019)Monfort, Andonian, Zhou, Ramakrishnan, Bargal, Yan, Brown, Fan, Gutfreund, Vondrick, et~al.]{mit}
M.~Monfort, A.~Andonian, B.~Zhou, K.~Ramakrishnan, S.~A. Bargal, T.~Yan, L.~Brown, Q.~Fan, D.~Gutfreund, C.~Vondrick, et~al.
\newblock Moments in time dataset: one million videos for event understanding.
\newblock \emph{IEEE transactions on pattern analysis and machine intelligence}, 42\penalty0 (2):\penalty0 502--508, 2019.

\bibitem[Nukrai et~al.(2022)Nukrai, Mokady, and Globerson]{gapcapdec}
D.~Nukrai, R.~Mokady, and A.~Globerson.
\newblock Text-only training for image captioning using noise-injected clip.
\newblock In \emph{Findings of the Association for Computational Linguistics: EMNLP 2022}, pages 4055--4063, 2022.

\bibitem[Papalampidi et~al.(2024)Papalampidi, Koppula, Pathak, Chiu, Heyward, Patraucean, Shen, Miech, Zisserman, and Nematzdeh]{longvivit}
P.~Papalampidi, S.~Koppula, S.~Pathak, J.~Chiu, J.~Heyward, V.~Patraucean, J.~Shen, A.~Miech, A.~Zisserman, and A.~Nematzdeh.
\newblock A simple recipe for contrastively pre-training video-first encoders beyond 16 frames.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 14386--14397, 2024.

\bibitem[Patraucean et~al.(2023)Patraucean, Smaira, Gupta, Recasens, Markeeva, Banarse, Koppula, Malinowski, Yang, Doersch, et~al.]{perception_test}
V.~Patraucean, L.~Smaira, A.~Gupta, A.~Recasens, L.~Markeeva, D.~Banarse, S.~Koppula, M.~Malinowski, Y.~Yang, C.~Doersch, et~al.
\newblock Perception test: A diagnostic benchmark for multimodal video models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Qian et~al.(2024)Qian, Li, Wu, Ye, Fei, Chua, Zhuang, and Tang]{qian2024momentorvideo}
L.~Qian, J.~Li, Y.~Wu, Y.~Ye, H.~Fei, T.-S. Chua, Y.~Zhuang, and S.~Tang.
\newblock Momentor: Advancing video large language model with fine-grained temporal reasoning.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, et~al.]{clip}
A.~Radford, J.~W. Kim, C.~Hallacy, A.~Ramesh, G.~Goh, S.~Agarwal, G.~Sastry, A.~Askell, P.~Mishkin, J.~Clark, et~al.
\newblock Learning transferable visual models from natural language supervision.
\newblock In \emph{International conference on machine learning}, pages 8748--8763. PMLR, 2021.

\bibitem[Ranasinghe et~al.(2024)Ranasinghe, Li, Kahatapitiya, and Ryoo]{onepass}
K.~Ranasinghe, X.~Li, K.~Kahatapitiya, and M.~S. Ryoo.
\newblock Understanding long videos in one multimodal language model pass.
\newblock \emph{arXiv preprint arXiv:2403.16998}, 2024.

\bibitem[Reid et~al.(2024)Reid, Savinov, Teplyashin, Lepikhin, Lillicrap, Alayrac, Soricut, Lazaridou, Firat, Schrittwieser, et~al.]{gemini1.5}
M.~Reid, N.~Savinov, D.~Teplyashin, D.~Lepikhin, T.~Lillicrap, J.-b. Alayrac, R.~Soricut, A.~Lazaridou, O.~Firat, J.~Schrittwieser, et~al.
\newblock Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context.
\newblock \emph{arXiv preprint arXiv:2403.05530}, 2024.

\bibitem[Schuhmann et~al.(2021)Schuhmann, Kaczmarczyk, Komatsuzaki, Katta, Vencu, Beaumont, Jitsev, Coombes, and Mullis]{laion400m}
C.~Schuhmann, R.~Kaczmarczyk, A.~Komatsuzaki, A.~Katta, R.~Vencu, R.~Beaumont, J.~Jitsev, T.~Coombes, and C.~Mullis.
\newblock Laion-400m: Open dataset of clip-filtered 400 million image-text pairs.
\newblock In \emph{NeurIPS Workshop Datacentric AI}, 2021.

\bibitem[Schuhmann et~al.(2022)Schuhmann, Beaumont, Vencu, Gordon, Wightman, Cherti, Coombes, Katta, Mullis, Wortsman, et~al.]{laion5b}
C.~Schuhmann, R.~Beaumont, R.~Vencu, C.~Gordon, R.~Wightman, M.~Cherti, T.~Coombes, A.~Katta, C.~Mullis, M.~Wortsman, et~al.
\newblock Laion-5b: An open large-scale dataset for training next generation image-text models.
\newblock \emph{Advances in Neural Information Processing Systems}, 35:\penalty0 25278--25294, 2022.

\bibitem[Sharma et~al.(2018)Sharma, Ding, Goodman, and Soricut]{cc3m}
P.~Sharma, N.~Ding, S.~Goodman, and R.~Soricut.
\newblock Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning.
\newblock In \emph{Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}, 2018.

\bibitem[Song et~al.(2024)Song, Chai, Wang, Zhang, Zhou, Wu, Guo, Ye, Lu, Hwang, et~al.]{song2023moviechat}
E.~Song, W.~Chai, G.~Wang, Y.~Zhang, H.~Zhou, F.~Wu, X.~Guo, T.~Ye, Y.~Lu, J.-N. Hwang, et~al.
\newblock {MovieChat}: From dense token to sparse memory for long video understanding.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 18221--18232, 2024.

\bibitem[Sur{\'\i}s et~al.(2023)Sur{\'\i}s, Menon, and Vondrick]{vipergpt}
D.~Sur{\'\i}s, S.~Menon, and C.~Vondrick.
\newblock Vipergpt: Visual inference via python execution for reasoning.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 11888--11898, 2023.

\bibitem[Team et~al.(2023)Team, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut, Schalkwyk, Dai, Hauth, et~al.]{gemini}
G.~Team, R.~Anil, S.~Borgeaud, Y.~Wu, J.-B. Alayrac, J.~Yu, R.~Soricut, J.~Schalkwyk, A.~M. Dai, A.~Hauth, et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{llama2}
H.~Touvron, L.~Martin, K.~Stone, P.~Albert, A.~Almahairi, Y.~Babaei, N.~Bashlykov, S.~Batra, P.~Bhargava, S.~Bhosale, et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Vedantam et~al.(2015)Vedantam, Lawrence~Zitnick, and Parikh]{vedantam2015cider}
R.~Vedantam, C.~Lawrence~Zitnick, and D.~Parikh.
\newblock Cider: Consensus-based image description evaluation.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 4566--4575, 2015.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Chen, Luo, Dai, Yuan, Wu, and Jiang]{agent_chatvideo}
J.~Wang, D.~Chen, C.~Luo, X.~Dai, L.~Yuan, Z.~Wu, and Y.-G. Jiang.
\newblock Chatvideo: A tracklet-centric multimodal and versatile video understanding system.
\newblock \emph{arXiv preprint arXiv:2304.14407}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhao, Do, Agarwal, Lee, and Sun]{wang2023vamos}
S.~Wang, Q.~Zhao, M.~Q. Do, N.~Agarwal, K.~Lee, and C.~Sun.
\newblock Vamos: Versatile action models for video understanding.
\newblock \emph{arXiv preprint arXiv:2311.13627}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2024{\natexlab{a}})Wang, He, Hong, Cheng, Zhang, Qi, Huang, Xu, Dong, Ding, et~al.]{wang2024lvbench}
W.~Wang, Z.~He, W.~Hong, Y.~Cheng, X.~Zhang, J.~Qi, S.~Huang, B.~Xu, Y.~Dong, M.~Ding, et~al.
\newblock Lvbench: An extreme long video understanding benchmark.
\newblock \emph{arXiv preprint arXiv:2406.08035}, 2024{\natexlab{a}}.

\bibitem[Wang et~al.(2019)Wang, Wu, Chen, Li, Wang, and Wang]{wang2019vatex}
X.~Wang, J.~Wu, J.~Chen, L.~Li, Y.-F. Wang, and W.~Y. Wang.
\newblock Vatex: A large-scale, high-quality multilingual dataset for video-and-language research.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 4581--4591, 2019.

\bibitem[Wang et~al.(2024{\natexlab{b}})Wang, Zhang, Zohar, and Yeung-Levy]{agent_video}
X.~Wang, Y.~Zhang, O.~Zohar, and S.~Yeung-Levy.
\newblock Videoagent: Long-form video understanding with large language model as agent.
\newblock In \emph{European Conference on Computer Vision (ECCV)}, 2024{\natexlab{b}}.

\bibitem[Wang et~al.(2022)Wang, Li, Li, He, Huang, Zhao, Zhang, Xu, Liu, Wang, et~al.]{wang2022internvideo}
Y.~Wang, K.~Li, Y.~Li, Y.~He, B.~Huang, Z.~Zhao, H.~Zhang, J.~Xu, Y.~Liu, Z.~Wang, et~al.
\newblock Internvideo: General video foundation models via generative and discriminative learning.
\newblock \emph{arXiv preprint arXiv:2212.03191}, 2022.

\bibitem[Wang et~al.(2024{\natexlab{c}})Wang, He, Li, Li, Yu, Ma, Li, Chen, Chen, Wang, Luo, Liu, Wang, Wang, and Qiao]{wang2023internvid}
Y.~Wang, Y.~He, Y.~Li, K.~Li, J.~Yu, X.~Ma, X.~Li, G.~Chen, X.~Chen, Y.~Wang, P.~Luo, Z.~Liu, Y.~Wang, L.~Wang, and Y.~Qiao.
\newblock Internvid: A large-scale video-text dataset for multimodal understanding and generation.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{c}}.

\bibitem[Wang et~al.(2024{\natexlab{d}})Wang, Li, Li, Yu, He, Chen, Pei, Zheng, Xu, Wang, et~al.]{internvideo2}
Y.~Wang, K.~Li, X.~Li, J.~Yu, Y.~He, G.~Chen, B.~Pei, R.~Zheng, J.~Xu, Z.~Wang, et~al.
\newblock Internvideo2: Scaling video foundation models for multimodal video understanding.
\newblock \emph{CoRR}, 2024{\natexlab{d}}.

\bibitem[Wang et~al.(2024{\natexlab{e}})Wang, Yang, and Ren]{wang2024lifelongmemory}
Y.~Wang, Y.~Yang, and M.~Ren.
\newblock Lifelongmemory: Leveraging llms for answering queries in long-form egocentric videos.
\newblock \emph{arXiv preprint arXiv:2312.05269}, 2024{\natexlab{e}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Blume, Li, Liu, Cho, Tang, Bansal, and Ji]{paxion}
Z.~Wang, A.~Blume, S.~Li, G.~Liu, J.~Cho, Z.~Tang, M.~Bansal, and H.~Ji.
\newblock Paxion: Patching action knowledge in video-language foundation models.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023{\natexlab{c}}.

\bibitem[Wang et~al.(2023{\natexlab{d}})Wang, Li, Hong, Wang, Wu, Bansal, Gould, Tan, and Qiao]{scalevln}
Z.~Wang, J.~Li, Y.~Hong, Y.~Wang, Q.~Wu, M.~Bansal, S.~Gould, H.~Tan, and Y.~Qiao.
\newblock Scaling data generation in vision-and-language navigation.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, pages 12009--12020, 2023{\natexlab{d}}.

\bibitem[Wu et~al.(2021)Wu, Yu, Chen, Tenenbaum, and Gan]{star}
B.~Wu, S.~Yu, Z.~Chen, J.~B. Tenenbaum, and C.~Gan.
\newblock Star: A benchmark for situated reasoning in real-world videos.
\newblock In \emph{Thirty-fifth conference on neural information processing systems datasets and benchmarks track (Round 2)}, 2021.

\bibitem[Wu et~al.(2024)Wu, Li, Chen, and Li]{wu2024longvideobench}
H.~Wu, D.~Li, B.~Chen, and J.~Li.
\newblock Longvideobench: A benchmark for long-context interleaved video-language understanding, 2024.

\bibitem[Xiao et~al.(2021)Xiao, Shang, Yao, and Chua]{nextqa}
J.~Xiao, X.~Shang, A.~Yao, and T.-S. Chua.
\newblock {NExT-QA: Next phase of question-answering to explaining temporal actions}.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 9777--9786, 2021.

\bibitem[Xie et~al.(2025)Xie, Zhang, Zhou, Li, Zhang, Hessel, Yang, and Liu]{funqa}
B.~Xie, S.~Zhang, Z.~Zhou, B.~Li, Y.~Zhang, J.~Hessel, J.~Yang, and Z.~Liu.
\newblock Funqa: Towards surprising video comprehension.
\newblock In \emph{European Conference on Computer Vision}, pages 39--57. Springer, 2025.

\bibitem[Xu et~al.(2023)Xu, Ye, Yan, Shi, Ye, Xu, Li, Bi, Qian, Wang, et~al.]{xu2023mplug}
H.~Xu, Q.~Ye, M.~Yan, Y.~Shi, J.~Ye, Y.~Xu, C.~Li, B.~Bi, Q.~Qian, W.~Wang, et~al.
\newblock {mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video}.
\newblock In \emph{ICML}, 2023.

\bibitem[Xu et~al.(2016)Xu, Mei, Yao, and Rui]{xu2016msrvtt}
J.~Xu, T.~Mei, T.~Yao, and Y.~Rui.
\newblock {MSR-VTT}: A large video description dataset for bridging video and language.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, pages 5288--5296, 2016.

\bibitem[Yan et~al.(2022)Yan, Zhu, Wang, Cao, Zhang, Ghosh, Wu, and Yu]{yan2022videococa}
S.~Yan, T.~Zhu, Z.~Wang, Y.~Cao, M.~Zhang, S.~Ghosh, Y.~Wu, and J.~Yu.
\newblock {VideoCoCa}: Video-text modeling with zero-shot transfer from contrastive captioners.
\newblock \emph{arXiv preprint arXiv:2212.04979}, 2022.

\bibitem[Yang et~al.(2022)Yang, Miech, Sivic, Laptev, and Schmid]{frozenbilm}
A.~Yang, A.~Miech, J.~Sivic, I.~Laptev, and C.~Schmid.
\newblock Zero-shot video question answering via frozen bidirectional language models.
\newblock In \emph{NeurIPS}, 2022.

\bibitem[Yang et~al.(2024)Yang, Chen, Li, Wang, and Yang]{agent_dlam}
Z.~Yang, G.~Chen, X.~Li, W.~Wang, and Y.~Yang.
\newblock Doraemongpt: Toward understanding dynamic scenes with large language models (exemplified as a video agent).
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024.

\bibitem[Ye et~al.(2023)Ye, Xu, Xu, Ye, Yan, Zhou, Wang, Hu, Shi, Shi, et~al.]{ye2023mplug-owl}
Q.~Ye, H.~Xu, G.~Xu, J.~Ye, M.~Yan, Y.~Zhou, J.~Wang, A.~Hu, P.~Shi, Y.~Shi, et~al.
\newblock mplug-owl: Modularization empowers large language models with multimodality.
\newblock \emph{arXiv preprint arXiv:2304.14178}, 2023.

\bibitem[Yi et~al.(2019)Yi, Gan, Li, Kohli, Wu, Torralba, and Tenenbaum]{clevrer}
K.~Yi, C.~Gan, Y.~Li, P.~Kohli, J.~Wu, A.~Torralba, and J.~B. Tenenbaum.
\newblock Clevrer: Collision events for video representation and reasoning.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Yu et~al.(2023)Yu, Cho, Yadav, and Bansal]{sevila}
S.~Yu, J.~Cho, P.~Yadav, and M.~Bansal.
\newblock Self-chained image-language model for video localization and question answering.
\newblock \emph{Advances in Neural Information Processing Systems}, 36, 2023.

\bibitem[Zhang et~al.(2023{\natexlab{a}})Zhang, Lu, Islam, Wang, Yu, Bansal, and Bertasius]{LLovi}
C.~Zhang, T.~Lu, M.~M. Islam, Z.~Wang, S.~Yu, M.~Bansal, and G.~Bertasius.
\newblock A simple llm framework for long-range video question-answering.
\newblock \emph{arXiv preprint arXiv:2312.17235}, 2023{\natexlab{a}}.

\bibitem[Zhang et~al.(2023{\natexlab{b}})Zhang, Li, and Bing]{videollama}
H.~Zhang, X.~Li, and L.~Bing.
\newblock Video-llama: An instruction-tuned audio-visual language model for video understanding.
\newblock In \emph{Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}, pages 543--553, 2023{\natexlab{b}}.

\bibitem[Zhang et~al.(2024{\natexlab{a}})Zhang, Han, Liu, Zhou, Lu, Qiao, Li, and Gao]{zhang2023llamaadapter}
R.~Zhang, J.~Han, C.~Liu, A.~Zhou, P.~Lu, Y.~Qiao, H.~Li, and P.~Gao.
\newblock {LL}a{MA}-adapter: Efficient fine-tuning of large language models with zero-initialized attention.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2024{\natexlab{a}}.

\bibitem[Zhang et~al.(2024{\natexlab{b}})Zhang, Sui, and Yeung-Levy]{gapC3}
Y.~Zhang, E.~Sui, and S.~Yeung-Levy.
\newblock Connect, collapse, corrupt: Learning cross-modal tasks with uni-modal data.
\newblock In \emph{International Conference on Learning Representations (ICLR)}, 2024{\natexlab{b}}.

\bibitem[Zhao et~al.(2024{\natexlab{a}})Zhao, Gundavarapu, Yuan, Zhou, Yan, Sun, Friedman, Qian, Weyand, Zhao, et~al.]{zhao2024videoprism}
L.~Zhao, N.~B. Gundavarapu, L.~Yuan, H.~Zhou, S.~Yan, J.~J. Sun, L.~Friedman, R.~Qian, T.~Weyand, Y.~Zhao, et~al.
\newblock Videoprism: A foundational visual encoder for video understanding.
\newblock In \emph{Forty-first International Conference on Machine Learning}, 2024{\natexlab{a}}.

\bibitem[Zhao et~al.(2023)Zhao, Misra, Kr{\"a}henb{\"u}hl, and Girdhar]{lavila}
Y.~Zhao, I.~Misra, P.~Kr{\"a}henb{\"u}hl, and R.~Girdhar.
\newblock Learning video representations from large language models.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 6586--6597, 2023.

\bibitem[Zhao et~al.(2024{\natexlab{b}})Zhao, Zhao, Zhou, Wu, Chu, Miao, Schroff, Adam, Liu, Gong, et~al.]{zhao2024distilling}
Y.~Zhao, L.~Zhao, X.~Zhou, J.~Wu, C.-T. Chu, H.~Miao, F.~Schroff, H.~Adam, T.~Liu, B.~Gong, et~al.
\newblock Distilling vision-language models on millions of videos.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pages 13106--13116, 2024{\natexlab{b}}.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{minigpt}
D.~Zhu, J.~Chen, X.~Shen, X.~Li, and M.~Elhoseiny.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock In \emph{The Twelfth International Conference on Learning Representations}, 2023.

\bibitem[Zhu and Yang(2020)]{zhu2020actbert}
L.~Zhu and Y.~Yang.
\newblock Actbert: Learning global-local video-text representations.
\newblock In \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8746--8755, 2020.

\end{thebibliography}
