\begin{thebibliography}{99}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[ajbarnett(2023)]{ajbarnett_2023}
ajbarnett.
\newblock 400 words to describe texture, 2023.

\bibitem[Bachmann et~al.(2022)Bachmann, Mizrahi, Atanov, and Zamir]{Bachmann2022}
Bachmann, R., Mizrahi, D., Atanov, A., and Zamir, A.
\newblock Multimae: Multi-modal multi-task masked autoencoders.
\newblock \emph{arXiv:2204.01678}, 2022.

\bibitem[Bai et~al.(2023)Bai, Bai, Yang, Wang, Tan, Wang, Lin, Zhou, and Zhou]{bai2023qwenvl}
Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J.
\newblock Qwen-vl: A versatile vision-language model for understanding, localization, text reading, and beyond, 2023.

\bibitem[Bertelson \& De~Gelder(2004)Bertelson and De~Gelder]{bertelson2004psychology}
Bertelson, P. and De~Gelder, B.
\newblock The psychology of multimodal perception.
\newblock \emph{Crossmodal space and crossmodal attention}, pp.\  141--177, 2004.

\bibitem[Bresciani et~al.(2006)Bresciani, Dammeier, and Ernst]{bresciani2006vision}
Bresciani, J.-P., Dammeier, F., and Ernst, M.~O.
\newblock Vision and touch are automatically integrated for the perception of sequences of events.
\newblock \emph{Journal of vision}, 6\penalty0 (5):\penalty0 2--2, 2006.

\bibitem[Brohan et~al.(2023)Brohan, Brown, Carbajal, Chebotar, Chen, Choromanski, Ding, Driess, Dubey, Finn, Florence, Fu, Arenas, Gopalakrishnan, Han, Hausman, Herzog, Hsu, Ichter, Irpan, Joshi, Julian, Kalashnikov, Kuang, Leal, Lee, Lee, Levine, Lu, Michalewski, Mordatch, Pertsch, Rao, Reymann, Ryoo, Salazar, Sanketi, Sermanet, Singh, Singh, Soricut, Tran, Vanhoucke, Vuong, Wahid, Welker, Wohlhart, Wu, Xia, Xiao, Xu, Xu, Yu, and Zitkovich]{rt22023arxiv}
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K., Ding, T., Driess, D., Dubey, A., Finn, C., Florence, P., Fu, C., Arenas, M.~G., Gopalakrishnan, K., Han, K., Hausman, K., Herzog, A., Hsu, J., Ichter, B., Irpan, A., Joshi, N., Julian, R., Kalashnikov, D., Kuang, Y., Leal, I., Lee, L., Lee, T.-W.~E., Levine, S., Lu, Y., Michalewski, H., Mordatch, I., Pertsch, K., Rao, K., Reymann, K., Ryoo, M., Salazar, G., Sanketi, P., Sermanet, P., Singh, J., Singh, A., Soricut, R., Tran, H., Vanhoucke, V., Vuong, Q., Wahid, A., Welker, S., Wohlhart, P., Wu, J., Xia, F., Xiao, T., Xu, P., Xu, S., Yu, T., and Zitkovich, B.
\newblock Rt-2: Vision-language-action models transfer web knowledge to robotic control.
\newblock In \emph{arXiv preprint arXiv:2307.15818}, 2023.

\bibitem[Brooks et~al.(2023)Brooks, Holynski, and Efros]{brooks2023instructpix2pix}
Brooks, T., Holynski, A., and Efros, A.~A.
\newblock Instructpix2pix: Learning to follow image editing instructions, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal, Neelakantan, Shyam, Sastry, Askell, Agarwal, Herbert-Voss, Krueger, Henighan, Child, Ramesh, Ziegler, Wu, Winter, Hesse, Chen, Sigler, Litwin, Gray, Chess, Clark, Berner, McCandlish, Radford, Sutskever, and Amodei]{brown2020language}
Brown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D.~M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D.
\newblock Language models are few-shot learners, 2020.

\bibitem[Bruck et~al.(2022)Bruck, Walmsley, and Janik]{bruck2022cross}
Bruck, J.~N., Walmsley, S.~F., and Janik, V.~M.
\newblock Cross-modal perception of identity by sound and taste in bottlenose dolphins.
\newblock \emph{Science Advances}, 8\penalty0 (20):\penalty0 eabm7684, 2022.

\bibitem[Burnel et~al.(2023)Burnel, Courtrai, and Lef{\`e}vre]{burnel2023lesslabels}
Burnel, J.-C., Courtrai, L., and Lef{\`e}vre, S.
\newblock Less labels, more modalities: A self-training framework to reuse pretrained networks.
\newblock In Rousseau, J.-J. and Kapralos, B. (eds.), \emph{Pattern Recognition, Computer Vision, and Image Processing. ICPR 2022 International Workshops and Challenges}, pp.\  287--302, Cham, 2023. Springer Nature Switzerland.
\newblock ISBN 978-3-031-37731-0.

\bibitem[Cai et~al.(2023{\natexlab{a}})Cai, Liu, Mustikovela, Meyer, Chai, Park, and Lee]{cai2023making}
Cai, M., Liu, H., Mustikovela, S.~K., Meyer, G.~P., Chai, Y., Park, D., and Lee, Y.~J.
\newblock Making large multimodal models understand arbitrary visual prompts, 2023{\natexlab{a}}.

\bibitem[Cai et~al.(2023{\natexlab{b}})Cai, Liu, Mustikovela, Meyer, Chai, Park, and Lee]{cai2023vipllava}
Cai, M., Liu, H., Mustikovela, S.~K., Meyer, G.~P., Chai, Y., Park, D., and Lee, Y.~J.
\newblock Making large multimodal models understand arbitrary visual prompts.
\newblock In \emph{arXiv:2312.00784}, 2023{\natexlab{b}}.

\bibitem[Calandra et~al.(2018)Calandra, Owens, Jayaraman, Lin, Yuan, Malik, Adelson, and Levine]{calandra2018more}
Calandra, R., Owens, A., Jayaraman, D., Lin, J., Yuan, W., Malik, J., Adelson, E.~H., and Levine, S.
\newblock More than a feeling: Learning to grasp and regrasp using vision and touch.
\newblock \emph{IEEE Robotics and Automation Letters}, 3\penalty0 (4):\penalty0 3300--3307, 2018.

\bibitem[Camponogara \& Volcic(2021)Camponogara and Volcic]{camponogara2021integration}
Camponogara, I. and Volcic, R.
\newblock Integration of haptics and vision in human multisensory grasping.
\newblock \emph{Cortex}, 135:\penalty0 173--185, 2021.

\bibitem[Caron et~al.(2021)Caron, Touvron, Misra, Jégou, Mairal, Bojanowski, and Joulin]{caron2021emerging}
Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., and Joulin, A.
\newblock Emerging properties in self-supervised vision transformers, 2021.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Zhang, Zeng, Zhang, Zhu, and Zhao]{chen2023shikra}
Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and Zhao, R.
\newblock Shikra: Unleashing multimodal llm's referential dialogue magic, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Li, Dong, Zhang, He, Wang, Zhao, and Lin]{chen2023sharegpt4v}
Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., and Lin, D.
\newblock Sharegpt4v: Improving large multi-modal models with better captions, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2020)Chen, Radford, Child, Wu, Jun, Luan, and Sutskever]{chen2020generative}
Chen, M., Radford, A., Child, R., Wu, J., Jun, H., Luan, D., and Sutskever, I.
\newblock Generative pretraining from pixels.
\newblock 2020.

\bibitem[Chen et~al.(2022)Chen, Sipos, der Merwe, and Fazeli]{chen2022visuotactile}
Chen, Y., Sipos, A., der Merwe, M.~V., and Fazeli, N.
\newblock Visuo-tactile transformers for manipulation, 2022.

\bibitem[Chiang et~al.(2023)Chiang, Li, Lin, Sheng, Wu, Zhang, Zheng, Zhuang, Zhuang, Gonzalez, Stoica, and Xing]{vicuna2023}
Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J.~E., Stoica, I., and Xing, E.~P.
\newblock Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality, March 2023.
\newblock URL \url{https://lmsys.org/blog/2023-03-30-vicuna/}.

\bibitem[Chorley et~al.(2009)Chorley, Melhuish, Pipe, and Rossiter]{chorley2009development}
Chorley, C., Melhuish, C., Pipe, T., and Rossiter, J.
\newblock Development of a tactile sensor based on biologically inspired edge encoding.
\newblock In \emph{2009 International Conference on Advanced Robotics}, pp.\  1--6. IEEE, 2009.

\bibitem[Dahiya et~al.(2009)Dahiya, Metta, Valle, and Sandini]{dahiya2009tactile}
Dahiya, R.~S., Metta, G., Valle, M., and Sandini, G.
\newblock Tactile sensing—from humans to humanoids.
\newblock \emph{IEEE transactions on robotics}, 26\penalty0 (1):\penalty0 1--20, 2009.

\bibitem[Dai et~al.(2023)Dai, Li, Li, Tiong, Zhao, Wang, Li, Fung, and Hoi]{dai2023instructblip}
Dai, W., Li, J., Li, D., Tiong, A. M.~H., Zhao, J., Wang, W., Li, B., Fung, P., and Hoi, S.
\newblock Instructblip: Towards general-purpose vision-language models with instruction tuning, 2023.

\bibitem[Dave et~al.(2024)Dave, Lygerakis, and Rueckert]{dave2024multimodal}
Dave, V., Lygerakis, F., and Rueckert, E.
\newblock Multimodal visual-tactile representation learning through self-supervised contrastive pre-training.
\newblock \emph{arXiv preprint arXiv:2401.12024}, 2024.

\bibitem[Dosovitskiy et~al.(2020)Dosovitskiy, Beyer, Kolesnikov, Weissenborn, Zhai, Unterthiner, Dehghani, Minderer, Heigold, Gelly, et~al.]{Dosovitskiy2020}
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et~al.
\newblock An image is worth 16x16 words: Transformers for image recognition at scale.
\newblock 2020.

\bibitem[Driess et~al.(2023)Driess, Xia, Sajjadi, Lynch, Chowdhery, Ichter, Wahid, Tompson, Vuong, Yu, Huang, Chebotar, Sermanet, Duckworth, Levine, Vanhoucke, Hausman, Toussaint, Greff, Zeng, Mordatch, and Florence]{driess2023palme}
Driess, D., Xia, F., Sajjadi, M. S.~M., Lynch, C., Chowdhery, A., Ichter, B., Wahid, A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P., Duckworth, D., Levine, S., Vanhoucke, V., Hausman, K., Toussaint, M., Greff, K., Zeng, A., Mordatch, I., and Florence, P.
\newblock Palm-e: An embodied multimodal language model.
\newblock In \emph{arXiv preprint arXiv:2303.03378}, 2023.

\bibitem[Fu et~al.(2023)Fu, Huang, Berscheid, Li, Goldberg, and Chitta]{fu2023safe}
Fu, L., Huang, H., Berscheid, L., Li, H., Goldberg, K., and Chitta, S.
\newblock Safe self-supervised learning in real of visuo-tactile feedback policies for industrial insertion, 2023.

\bibitem[Gao et~al.(2023)Gao, Han, Zhang, Lin, Geng, Zhou, Zhang, Lu, He, Yue, Li, and Qiao]{gao2023llamaadapterv2}
Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He, C., Yue, X., Li, H., and Qiao, Y.
\newblock Llama-adapter v2: Parameter-efficient visual instruction model.
\newblock \emph{arXiv preprint arXiv:2304.15010}, 2023.

\bibitem[Gao et~al.(2021)Gao, Chang, Mall, Fei-Fei, and Wu]{gao2021objectfolder}
Gao, R., Chang, Y.-Y., Mall, S., Fei-Fei, L., and Wu, J.
\newblock Objectfolder: A dataset of objects with implicit visual, auditory, and tactile representations.
\newblock In \emph{Conference on Robot Learning}, 2021.

\bibitem[Gao et~al.(2022)Gao, Si, Chang, Clarke, Bohg, Fei-Fei, Yuan, and Wu]{Gao_2022_CVPR}
Gao, R., Si, Z., Chang, Y.-Y., Clarke, S., Bohg, J., Fei-Fei, L., Yuan, W., and Wu, J.
\newblock Objectfolder 2.0: A multisensory object dataset for sim2real transfer.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, pp.\  10598--10608, June 2022.

\bibitem[Geng et~al.(2022)Geng, Liu, Lee, Schuurams, Levine, and Abbeel]{geng2022multimodal}
Geng, X., Liu, H., Lee, L., Schuurams, D., Levine, S., and Abbeel, P.
\newblock Multimodal masked autoencoders learn transferable representations.
\newblock \emph{arXiv preprint arXiv:2205.14204}, 2022.

\bibitem[Girdhar et~al.(2023)Girdhar, El-Nouby, Liu, Singh, Alwala, Joulin, and Misra]{girdhar2023imagebind}
Girdhar, R., El-Nouby, A., Liu, Z., Singh, M., Alwala, K.~V., Joulin, A., and Misra, I.
\newblock Imagebind: One embedding space to bind them all.
\newblock In \emph{CVPR}, 2023.

\bibitem[Goldberg \& Bajcsy(1984)Goldberg and Bajcsy]{goldberg1984active}
Goldberg, K.~Y. and Bajcsy, R.
\newblock Active touch and robot perception.
\newblock \emph{Cognition and Brain Theory}, 7\penalty0 (2):\penalty0 199--214, 1984.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski, Kyrola, Tulloch, Jia, and He]{Goyal2017b}
Goyal, P., Doll{\'a}r, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A., Jia, Y., and He, K.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv:1706.02677}, 2017.

\bibitem[Guo et~al.(2023)Guo, Zhang, Zhu, Tang, Ma, Han, Chen, Gao, Li, Li, and Heng]{guo2023pointbind}
Guo, Z., Zhang, R., Zhu, X., Tang, Y., Ma, X., Han, J., Chen, K., Gao, P., Li, X., Li, H., and Heng, P.-A.
\newblock Point-bind and point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following, 2023.

\bibitem[Guzhov et~al.(2021)Guzhov, Raue, Hees, and Dengel]{guzhov2021audioclip}
Guzhov, A., Raue, F., Hees, J., and Dengel, A.
\newblock Audioclip: Extending clip to image, text and audio, 2021.

\bibitem[Han et~al.(2023)Han, Zhang, Shao, Gao, Xu, Xiao, Zhang, Liu, Wen, Guo, Lu, Ren, Wen, Chen, Yue, Li, and Qiao]{han2023imagebindllm}
Han, J., Zhang, R., Shao, W., Gao, P., Xu, P., Xiao, H., Zhang, K., Liu, C., Wen, S., Guo, Z., Lu, X., Ren, S., Wen, Y., Chen, X., Yue, X., Li, H., and Qiao, Y.
\newblock Imagebind-llm: Multi-modality instruction tuning, 2023.

\bibitem[Haque et~al.(2023)Haque, Tancik, Efros, Holynski, and Kanazawa]{instructnerf2023}
Haque, A., Tancik, M., Efros, A., Holynski, A., and Kanazawa, A.
\newblock Instruct-nerf2nerf: Editing 3d scenes with instructions.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on Computer Vision}, 2023.

\bibitem[Higuera et~al.(2023)Higuera, Boots, and Mukadam]{higuera2023learning}
Higuera, C., Boots, B., and Mukadam, M.
\newblock Learning to read braille: Bridging the tactile reality gap with diffusion models.
\newblock 2023.

\bibitem[Hu et~al.(2022)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen]{hu2022lora}
Hu, E.~J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.
\newblock Lo{RA}: Low-rank adaptation of large language models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=nZeVKeeFYf9}.

\bibitem[Ilharco et~al.(2021)Ilharco, Wortsman, Wightman, Gordon, Carlini, Taori, Dave, Shankar, Namkoong, Miller, Hajishirzi, Farhadi, and Schmidt]{ilharco_gabriel_2021_5143773}
Ilharco, G., Wortsman, M., Wightman, R., Gordon, C., Carlini, N., Taori, R., Dave, A., Shankar, V., Namkoong, H., Miller, J., Hajishirzi, H., Farhadi, A., and Schmidt, L.
\newblock Openclip, July 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.5143773}.
\newblock If you use this software, please cite it as below.

\bibitem[Ittyerah \& Marks(2007)Ittyerah and Marks]{ittyerah2007memory}
Ittyerah, M. and Marks, L.~E.
\newblock Memory for curvature of objects: Haptic touch vs. vision.
\newblock \emph{British Journal of Psychology}, 98\penalty0 (4):\penalty0 589--610, 2007.

\bibitem[Johansson \& Flanagan(2009)Johansson and Flanagan]{johansson2009coding}
Johansson, R.~S. and Flanagan, J.~R.
\newblock Coding and use of tactile signals from the fingertips in object manipulation tasks.
\newblock \emph{Nature Reviews Neuroscience}, 10\penalty0 (5):\penalty0 345--359, 2009.

\bibitem[Jones et~al.(2005)Jones, Bokinsky, Tretter, and Negishi]{jones2005comparison}
Jones, M.~G., Bokinsky, A., Tretter, T., and Negishi, A.
\newblock A comparison of learning with haptic and visual modalities.
\newblock 2005.

\bibitem[Kampouris et~al.(2016)Kampouris, Mariolis, Peleka, Skartados, Kargakos, Triantafyllou, and Malassiotis]{kampouris2016multi}
Kampouris, C., Mariolis, I., Peleka, G., Skartados, E., Kargakos, A., Triantafyllou, D., and Malassiotis, S.
\newblock Multi-sensorial and explorative recognition of garments and their material properties in unconstrained environment.
\newblock In \emph{2016 IEEE international conference on robotics and automation (ICRA)}, pp.\  1656--1663. IEEE, 2016.

\bibitem[Kerr et~al.(2023)Kerr, Huang, Wilcox, Hoque, Ichnowski, Calandra, and Goldberg]{kerr2023selfsupervised}
Kerr, J., Huang, H., Wilcox, A., Hoque, R., Ichnowski, J., Calandra, R., and Goldberg, K.
\newblock Self-supervised visuo-tactile pretraining to locate and follow garment features, 2023.

\bibitem[Klatzky \& Lederman(2003)Klatzky and Lederman]{klatzky2003skin}
Klatzky, R.~L. and Lederman, S.~J.
\newblock The skin and its receptors 148 pathways to cortex and major cortical areas.
\newblock \emph{Handbook of psychology, experimental psychology}, 4:\penalty0 147, 2003.

\bibitem[Lambeta et~al.(2020)Lambeta, Chou, Tian, Yang, Maloon, Most, Stroud, Santos, Byagowi, Kammerer, Jayaraman, and Calandra]{digit}
Lambeta, M., Chou, P.-W., Tian, S., Yang, B., Maloon, B., Most, V.~R., Stroud, D., Santos, R., Byagowi, A., Kammerer, G., Jayaraman, D., and Calandra, R.
\newblock Digit: A novel design for a low-cost compact high-resolution tactile sensor with application to in-hand manipulation.
\newblock \emph{IEEE Robotics and Automation Letters}, 5\penalty0 (3):\penalty0 3838--3845, 2020.
\newblock \doi{10.1109/LRA.2020.2977257}.

\bibitem[Lee et~al.(2013)]{lee2013pseudo}
Lee, D.-H. et~al.
\newblock Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks.
\newblock In \emph{Workshop on challenges in representation learning, ICML}, volume~3, pp.\  896. Atlanta, 2013.

\bibitem[Li et~al.(2023{\natexlab{a}})Li, Li, Savarese, and Hoi]{li2023blip2}
Li, J., Li, D., Savarese, S., and Hoi, S.
\newblock Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, 2023{\natexlab{a}}.

\bibitem[Li \& Adelson(2013)Li and Adelson]{li2013sensing}
Li, R. and Adelson, E.~H.
\newblock Sensing and recognizing surface textures using a gelsight sensor.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}, pp.\  1241--1247, 2013.

\bibitem[Li et~al.(2019)Li, Zhu, Tedrake, and Torralba]{li2019connecting}
Li, Y., Zhu, J.-Y., Tedrake, R., and Torralba, A.
\newblock Connecting touch and vision via cross-modal prediction, 2019.

\bibitem[Li et~al.(2023{\natexlab{b}})Li, Fan, Hu, Feichtenhofer, and He]{li2023scaling}
Li, Y., Fan, H., Hu, R., Feichtenhofer, C., and He, K.
\newblock Scaling language-image pre-training via masking.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  23390--23400, 2023{\natexlab{b}}.

\bibitem[Lin et~al.(2023)Lin, Liu, Zhang, Gao, Qiu, Xiao, Qiu, Lin, Shao, Chen, Han, Huang, Zhang, He, Li, and Qiao]{lin2023sphinx}
Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao, W., Chen, K., Han, J., Huang, S., Zhang, Y., He, X., Li, H., and Qiao, Y.
\newblock Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Li, Li, and Lee]{liu2023improvedllava}
Liu, H., Li, C., Li, Y., and Lee, Y.~J.
\newblock Improved baselines with visual instruction tuning, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Li, Wu, and Lee]{liu2023llava}
Liu, H., Li, C., Wu, Q., and Lee, Y.~J.
\newblock Visual instruction tuning.
\newblock In \emph{NeurIPS}, 2023{\natexlab{b}}.

\bibitem[Loshchilov \& Hutter(2017{\natexlab{a}})Loshchilov and Hutter]{loshchilov2016sgdr}
Loshchilov, I. and Hutter, F.
\newblock Sgdr: Stochastic gradient descent with warm restarts.
\newblock 2017{\natexlab{a}}.

\bibitem[Loshchilov \& Hutter(2017{\natexlab{b}})Loshchilov and Hutter]{loshchilov2017decoupled}
Loshchilov, I. and Hutter, F.
\newblock Decoupled weight decay regularization.
\newblock \emph{arXiv preprint arXiv:1711.05101}, 2017{\natexlab{b}}.

\bibitem[Lu et~al.(2023)Lu, Clark, Lee, Zhang, Khosla, Marten, Hoiem, and Kembhavi]{lu2023unifiedio}
Lu, J., Clark, C., Lee, S., Zhang, Z., Khosla, S., Marten, R., Hoiem, D., and Kembhavi, A.
\newblock Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action, 2023.

\bibitem[McLachlan(1975)]{mclachlan1975iterative}
McLachlan, G.~J.
\newblock Iterative reclassification procedure for constructing an asymptotically optimal rule of allocation in discriminant analysis.
\newblock \emph{Journal of the American Statistical Association}, 70\penalty0 (350):\penalty0 365--369, 1975.

\bibitem[Miller et~al.(2018)Miller, Schmidt, Blankenburg, and Pulverm{\"u}ller]{miller2018verbal}
Miller, T.~M., Schmidt, T.~T., Blankenburg, F., and Pulverm{\"u}ller, F.
\newblock Verbal labels facilitate tactile perception.
\newblock \emph{Cognition}, 171:\penalty0 172--179, 2018.

\bibitem[Moon et~al.(2023)Moon, Madotto, Lin, Nagarajan, Smith, Jain, Yeh, Murugesan, Heidari, Liu, Srinet, Damavandi, and Kumar]{moon2023anymal}
Moon, S., Madotto, A., Lin, Z., Nagarajan, T., Smith, M., Jain, S., Yeh, C.-F., Murugesan, P., Heidari, P., Liu, Y., Srinet, K., Damavandi, B., and Kumar, A.
\newblock Anymal: An efficient and scalable any-modality augmented language model, 2023.

\bibitem[Ojala et~al.(2002)Ojala, Pietikainen, and Maenpaa]{ojala2002multiresolution}
Ojala, T., Pietikainen, M., and Maenpaa, T.
\newblock Multiresolution gray-scale and rotation invariant texture classification with local binary patterns.
\newblock \emph{IEEE Transactions on pattern analysis and machine intelligence}, 24\penalty0 (7):\penalty0 971--987, 2002.

\bibitem[OpenAI et~al.(2023)OpenAI, :, Achiam, Adler, Agarwal, Ahmad, Akkaya, Aleman, Almeida, Altenschmidt, Altman, Anadkat, Avila, Babuschkin, Balaji, Balcom, Baltescu, Bao, Bavarian, Belgum, Bello, Berdine, Bernadett-Shapiro, Berner, Bogdonoff, Boiko, Boyd, Brakman, Brockman, Brooks, Brundage, Button, Cai, Campbell, Cann, Carey, Carlson, Carmichael, Chan, Chang, Chantzis, Chen, Chen, Chen, Chen, Chen, Chess, Cho, Chu, Chung, Cummings, Currier, Dai, Decareaux, Degry, Deutsch, Deville, Dhar, Dohan, Dowling, Dunning, Ecoffet, Eleti, Eloundou, Farhi, Fedus, Felix, Fishman, Forte, Fulford, Gao, Georges, Gibson, Goel, Gogineni, Goh, Gontijo-Lopes, Gordon, Grafstein, Gray, Greene, Gross, Gu, Guo, Hallacy, Han, Harris, He, Heaton, Heidecke, Hesse, Hickey, Hickey, Hoeschele, Houghton, Hsu, Hu, Hu, Huizinga, Jain, Jain, Jang, Jiang, Jiang, Jin, Jin, Jomoto, Jonn, Jun, Kaftan, Łukasz Kaiser, Kamali, Kanitscheider, Keskar, Khan, Kilpatrick, Kim, Kim, Kim, Kirchner, Kiros, Knight, Kokotajlo, Łukasz Kondraciuk, Kondrich, Konstantinidis, Kosic, Krueger, Kuo, Lampe, Lan, Lee, Leike, Leung, Levy, Li, Lim, Lin, Lin, Litwin, Lopez, Lowe, Lue, Makanju, Malfacini, Manning, Markov, Markovski, Martin, Mayer, Mayne, McGrew, McKinney, McLeavey, McMillan, McNeil, Medina, Mehta, Menick, Metz, Mishchenko, Mishkin, Monaco, Morikawa, Mossing, Mu, Murati, Murk, Mély, Nair, Nakano, Nayak, Neelakantan, Ngo, Noh, Ouyang, O'Keefe, Pachocki, Paino, Palermo, Pantuliano, Parascandolo, Parish, Parparita, Passos, Pavlov, Peng, Perelman, de~Avila Belbute~Peres, Petrov, de~Oliveira~Pinto, Michael, Pokorny, Pokrass, Pong, Powell, Power, Power, Proehl, Puri, Radford, Rae, Ramesh, Raymond, Real, Rimbach, Ross, Rotsted, Roussez, Ryder, Saltarelli, Sanders, Santurkar, Sastry, Schmidt, Schnurr, Schulman, Selsam, Sheppard, Sherbakov, Shieh, Shoker, Shyam, Sidor, Sigler, Simens, Sitkin, Slama, Sohl, Sokolowsky, Song, Staudacher, Such, Summers, Sutskever, Tang, Tezak, Thompson, Tillet, Tootoonchian, Tseng, Tuggle, Turley, Tworek, Uribe, Vallone, Vijayvergiya, Voss, Wainwright, Wang, Wang, Wang, Ward, Wei, Weinmann, Welihinda, Welinder, Weng, Weng, Wiethoff, Willner, Winter, Wolrich, Wong, Workman, Wu, Wu, Wu, Xiao, Xu, Yoo, Yu, Yuan, Zaremba, Zellers, Zhang, Zhang, Zhao, Zheng, Zhuang, Zhuk, and Zoph]{openai2023gpt4}
OpenAI, :, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.~L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H.~W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.~P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S.~S., Guo, Y., Hallacy, C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N.~S., Khan, T., Kilpatrick, L., Kim, J.~W., Kim, C., Kim, Y., Kirchner, H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C.~M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S.~M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R., Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O'Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de~Avila Belbute~Peres, F., Petrov, M., de~Oliveira~Pinto, H.~P., Michael, Pokorny, Pokrass, M., Pong, V., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F.~P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F.~C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang, J.~J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B.
\newblock Gpt-4 technical report, 2023.

\bibitem[Pacchierotti et~al.(2017)Pacchierotti, Sinclair, Solazzi, Frisoli, Hayward, and Prattichizzo]{pacchierotti2017}
Pacchierotti, C., Sinclair, S., Solazzi, M., Frisoli, A., Hayward, V., and Prattichizzo, D.
\newblock Wearable haptic systems for the fingertip and the hand: Taxonomy, review, and perspectives.
\newblock \emph{IEEE Transactions on Haptics}, 10\penalty0 (4):\penalty0 580--600, 2017.
\newblock \doi{10.1109/TOH.2017.2689006}.

\bibitem[Qi et~al.(2023)Qi, Yi, Ma, Suresh, Lambeta, Calandra, and Malik]{qi2023general}
Qi, H., Yi, B., Ma, Y., Suresh, S., Lambeta, M., Calandra, R., and Malik, J.
\newblock {General In-Hand Object Rotation with Vision and Touch}.
\newblock In \emph{Conference on Robot Learning (CoRL)}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, and Sutskever]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I.
\newblock Learning transferable visual models from natural language supervision, 2021.

\bibitem[Radosavovic et~al.(2023)Radosavovic, Shi, Fu, Goldberg, Darrell, and Malik]{radosavovic2023robot}
Radosavovic, I., Shi, B., Fu, L., Goldberg, K., Darrell, T., and Malik, J.
\newblock Robot learning with sensorimotor pre-training.
\newblock \emph{arXiv preprint arXiv:2306.10007}, 2023.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov, Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, Eccles, Bruce, Razavi, Edwards, Heess, Chen, Hadsell, Vinyals, Bordbar, and de~Freitas]{reed2022generalist}
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.~G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.~T., Eccles, T., Bruce, J., Razavi, A., Edwards, A., Heess, N., Chen, Y., Hadsell, R., Vinyals, O., Bordbar, M., and de~Freitas, N.
\newblock A generalist agent, 2022.

\bibitem[Rombach et~al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer]{rombach2022highresolution}
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B.
\newblock High-resolution image synthesis with latent diffusion models, 2022.

\bibitem[Rosenberg et~al.(2005)Rosenberg, Hebert, and Schneiderman]{rosenberg2005semi}
Rosenberg, C., Hebert, M., and Schneiderman, H.
\newblock Semi-supervised self-training of object detection models.
\newblock 2005.

\bibitem[Schmidt et~al.(2019)Schmidt, Miller, Blankenburg, and Pulverm{\"u}ller]{schmidt2019neuronal}
Schmidt, T.~T., Miller, T.~M., Blankenburg, F., and Pulverm{\"u}ller, F.
\newblock Neuronal correlates of label facilitated tactile perception.
\newblock \emph{Scientific Reports}, 9\penalty0 (1):\penalty0 1606, 2019.

\bibitem[Sferrazza \& D’Andrea(2019)Sferrazza and D’Andrea]{sferrazza2019design}
Sferrazza, C. and D’Andrea, R.
\newblock Design, motivation and evaluation of a full-resolution optical tactile sensor.
\newblock \emph{Sensors}, 19\penalty0 (4):\penalty0 928, 2019.

\bibitem[Shimonomura(2019)]{shimonomura2019tactile}
Shimonomura, K.
\newblock Tactile image sensors employing camera: A review.
\newblock \emph{Sensors}, 19\penalty0 (18):\penalty0 3933, 2019.

\bibitem[Sohn et~al.(2020)Sohn, Berthelot, Li, Zhang, Carlini, Cubuk, Kurakin, Zhang, and Raffel]{sohn2020fixmatch}
Sohn, K., Berthelot, D., Li, C.-L., Zhang, Z., Carlini, N., Cubuk, E.~D., Kurakin, A., Zhang, H., and Raffel, C.
\newblock Fixmatch: Simplifying semi-supervised learning with consistency and confidence.
\newblock \emph{arXiv preprint arXiv:2001.07685}, 2020.

\bibitem[Speed et~al.(2021)Speed, Croijmans, Dolscheid, and Majid]{speed2021crossmodal}
Speed, L.~J., Croijmans, I., Dolscheid, S., and Majid, A.
\newblock Crossmodal associations with olfactory, auditory, and tactile stimuli in children and adults.
\newblock \emph{i-Perception}, 12\penalty0 (6):\penalty0 20416695211048513, 2021.

\bibitem[Stone \& Gonzalez(2015)Stone and Gonzalez]{stone2015contributions}
Stone, K.~D. and Gonzalez, C.~L.
\newblock The contributions of vision and haptics to reaching and grasping.
\newblock \emph{Frontiers in psychology}, 6:\penalty0 1403, 2015.

\bibitem[Su et~al.(2023)Su, Lan, Li, Xu, Wang, and Cai]{su2023pandagpt}
Su, Y., Lan, T., Li, H., Xu, J., Wang, Y., and Cai, D.
\newblock Pandagpt: One model to instruction-follow them all, 2023.

\bibitem[Sun et~al.(2023)Sun, Cui, Zhang, Zhang, Yu, Luo, Wang, Rao, Liu, Huang, and Wang]{sun2023generative}
Sun, Q., Cui, Y., Zhang, X., Zhang, F., Yu, Q., Luo, Z., Wang, Y., Rao, Y., Liu, J., Huang, T., and Wang, X.
\newblock Generative multimodal models are in-context learners, 2023.

\bibitem[Suresh et~al.(2022)Suresh, Si, Anderson, Kaess, and Mukadam]{suresh2022midastouch}
Suresh, S., Si, Z., Anderson, S., Kaess, M., and Mukadam, M.
\newblock Midastouch: Monte-carlo inference over distributions across sliding touch.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022.
\newblock URL \url{https://openreview.net/forum?id=JWROnOf4w-K}.

\bibitem[Tang et~al.(2023)Tang, Yang, Khademi, Liu, Zhu, and Bansal]{tang2023codi2}
Tang, Z., Yang, Z., Khademi, M., Liu, Y., Zhu, C., and Bansal, M.
\newblock Codi-2: In-context, interleaved, and interactive any-to-any generation, 2023.

\bibitem[Taori et~al.(2023)Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang, and Hashimoto]{alpaca}
Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., and Hashimoto, T.~B.
\newblock Stanford alpaca: An instruction-following llama model.
\newblock \url{https://github.com/tatsu-lab/stanford_alpaca}, 2023.

\bibitem[Touvron et~al.(2023)Touvron, Martin, Stone, Albert, Almahairi, Babaei, Bashlykov, Batra, Bhargava, Bhosale, et~al.]{touvron2023llama}
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., et~al.
\newblock Llama 2: Open foundation and fine-tuned chat models.
\newblock \emph{arXiv preprint arXiv:2307.09288}, 2023.

\bibitem[Turk(2014)]{turk2014multimodal}
Turk, M.
\newblock Multimodal interaction: A review.
\newblock \emph{Pattern recognition letters}, 36:\penalty0 189--195, 2014.

\bibitem[Wang et~al.(2020)Wang, Lian, Miao, Liu, and Yu]{wang2020long}
Wang, X., Lian, L., Miao, Z., Liu, Z., and Yu, S.~X.
\newblock Long-tailed recognition by routing diverse distribution-aware experts.
\newblock \emph{arXiv preprint arXiv:2010.01809}, 2020.

\bibitem[Wang et~al.(2022{\natexlab{a}})Wang, Wu, Lian, and Yu]{wang2022debiased}
Wang, X., Wu, Z., Lian, L., and Yu, S.~X.
\newblock Debiased learning from naturally imbalanced pseudo-labels.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition}, pp.\  14647--14657, 2022{\natexlab{a}}.

\bibitem[Wang et~al.(2023)Wang, Girdhar, Yu, and Misra]{wang2023cut}
Wang, X., Girdhar, R., Yu, S.~X., and Misra, I.
\newblock Cut and learn for unsupervised object detection and instance segmentation, 2023.

\bibitem[Wang et~al.(2022{\natexlab{b}})Wang, Kordi, Mishra, Liu, Smith, Khashabi, and Hajishirzi]{selfinstruct}
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N.~A., Khashabi, D., and Hajishirzi, H.
\newblock Self-instruct: Aligning language model with self generated instructions, 2022{\natexlab{b}}.

\bibitem[Wu et~al.(2023)Wu, Fei, Qu, Ji, and Chua]{wu2023nextgpt}
Wu, S., Fei, H., Qu, L., Ji, W., and Chua, T.-S.
\newblock Next-gpt: Any-to-any multimodal llm.
\newblock \emph{CoRR}, abs/2309.05519, 2023.

\bibitem[Yamaguchi \& Atkeson(2016)Yamaguchi and Atkeson]{fingervision}
Yamaguchi, A. and Atkeson, C.~G.
\newblock Combining finger vision and optical tactile sensing: Reducing and handling errors while cutting vegetables.
\newblock In \emph{2016 IEEE-RAS 16th International Conference on Humanoid Robots (Humanoids)}, pp.\  1045--1051, 2016.
\newblock \doi{10.1109/HUMANOIDS.2016.7803400}.

\bibitem[Yang et~al.(2022)Yang, Ma, Zhang, Zhu, Yuan, and Owens]{yang2022touch}
Yang, F., Ma, C., Zhang, J., Zhu, J., Yuan, W., and Owens, A.
\newblock Touch and go: Learning from human-collected vision and touch.
\newblock In \emph{Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track}, 2022.

\bibitem[Yang et~al.(2024)Yang, Feng, Chen, Park, Wang, Dou, Zeng, Chen, Gangopadhyay, Owens, et~al.]{yang2024binding}
Yang, F., Feng, C., Chen, Z., Park, H., Wang, D., Dou, Y., Zeng, Z., Chen, X., Gangopadhyay, R., Owens, A., et~al.
\newblock Binding touch to everything: Learning unified multimodal tactile representations.
\newblock \emph{arXiv preprint arXiv:2401.18084}, 2024.

\bibitem[Yuan et~al.(2017)Yuan, Dong, and Adelson]{yuan2017gelsight}
Yuan, W., Dong, S., and Adelson, E.~H.
\newblock Gelsight: High-resolution robot tactile sensors for estimating geometry and force.
\newblock \emph{Sensors}, 17\penalty0 (12):\penalty0 2762, 2017.

\bibitem[Yuan et~al.(2018)Yuan, Mo, Wang, and Adelson]{yuan2018active}
Yuan, W., Mo, Y., Wang, S., and Adelson, E.~H.
\newblock Active clothing material perception using tactile sensing and deep learning.
\newblock In \emph{2018 IEEE International Conference on Robotics and Automation (ICRA)}, pp.\  4842--4849. IEEE, 2018.

\bibitem[Zhang \& Demiris(2023)Zhang and Demiris]{10185075}
Zhang, F. and Demiris, Y.
\newblock Visual-tactile learning of garment unfolding for robot-assisted dressing.
\newblock \emph{IEEE Robotics and Automation Letters}, 8\penalty0 (9):\penalty0 5512--5519, 2023.
\newblock \doi{10.1109/LRA.2023.3296371}.

\bibitem[Zhang et~al.(2021)Zhang, Guo, Zhang, Li, Miao, Cui, Qiao, Gao, and Li]{zhang2021pointclip}
Zhang, R., Guo, Z., Zhang, W., Li, K., Miao, X., Cui, B., Qiao, Y., Gao, P., and Li, H.
\newblock Pointclip: Point cloud understanding by clip, 2021.

\bibitem[Zhang et~al.(2023)Zhang, Han, Liu, Gao, Zhou, Hu, Yan, Lu, Li, and Qiao]{zhang2023llamaadapter}
Zhang, R., Han, J., Liu, C., Gao, P., Zhou, A., Hu, X., Yan, S., Lu, P., Li, H., and Qiao, Y.
\newblock Llama-adapter: Efficient finetuning of language models with zero-init attention.
\newblock \emph{arXiv preprint arXiv:2303.16199}, 2023.

\bibitem[Zhong et~al.(2022)Zhong, Albini, Jones, Maiolino, and Posner]{zhong2022touching}
Zhong, S., Albini, A., Jones, O.~P., Maiolino, P., and Posner, I.
\newblock Touching a ne{RF}: Leveraging neural radiance fields for tactile sensory data generation.
\newblock In \emph{6th Annual Conference on Robot Learning}, 2022.
\newblock URL \url{https://openreview.net/forum?id=No3mbanRlZJ}.

\bibitem[Zhu et~al.(2023)Zhu, Chen, Shen, Li, and Elhoseiny]{zhu2023minigpt}
Zhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.
\newblock Minigpt-4: Enhancing vision-language understanding with advanced large language models.
\newblock \emph{arXiv preprint arXiv:2304.10592}, 2023.

\end{thebibliography}
