\begin{thebibliography}{30}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2020)Agarwal, Kakade, and Yang]{agarwal2020model}
Alekh Agarwal, Sham Kakade, and Lin~F Yang.
\newblock \href{https://arxiv.org/pdf/1906.03804.pdf}{{Model-based
  reinforcement learning with a generative model is minimax optimal}}.
\newblock In \emph{Conference on Learning Theory}, 2020.

\bibitem[Azar et~al.(2012)Azar, Munos, and Kappen]{azar2012on}
Mohammad~Gheshlaghi Azar, R{\'{e}}mi Munos, and Bert Kappen.
\newblock \href{https://arxiv.org/pdf/1206.6461.pdf}{{On the sample complexity
  of reinforcement learning with a generative model}}.
\newblock In \emph{International Conference on Machine Learning}, 2012.

\bibitem[Azar et~al.(2013)Azar, Munos, and Kappen]{azar2013minimax}
Mohammad~Gheshlaghi Azar, R{\'{e}}mi Munos, and Hilbert~J. Kappen.
\newblock \href{https://hal.archives-ouvertes.fr/hal-00831875}{{Minimax PAC
  bounds on the sample complexity of reinforcement learning with a generative
  model}}.
\newblock \emph{Machine Learning}, 91\penalty0 (3):\penalty0 325--349, 2013.

\bibitem[Azar et~al.(2017)Azar, Osband, and Munos]{azar2017minimax}
Mohammad~Gheshlaghi Azar, Ian Osband, and R{\'{e}}mi Munos.
\newblock \href{https://arxiv.org/pdf/1703.05449.pdf}{{Minimax regret bounds
  for reinforcement learning}}.
\newblock In \emph{International Conference on Machine Learning}, 2017.

\bibitem[Bartlett and Tewari(2009)]{bartlett2009regal}
Peter~L. Bartlett and Ambuj Tewari.
\newblock \href{https://arxiv.org/pdf/1205.2661.pdf}{{REGAL: A regularization
  based algorithm for reinforcement learning in weakly communicating MDPs}}.
\newblock In \emph{Uncertainty in Artificial Intelligence}, 2009.

\bibitem[Boucheron et~al.(2013)Boucheron, Lugosi, and
  Massart]{boucheron2013concentration}
St{\'{e}}phane Boucheron, G{\'{a}}bor Lugosi, and Pascal Massart.
\newblock \href{https://www.hse.ru/data/2016/11/24/1113029206/Concentration
  inequalities.pdf}{\emph{{Concentration inequalities}}}.
\newblock Oxford University Press, 2013.

\bibitem[Cover and Thomas(2006)]{cover2006elements}
Thomas~M. Cover and Joy~A. Thomas.
\newblock
  \href{https://www.amazon.com/Elements-Information-Theory-Telecommunications-Processing/dp/0471241954}{\emph{{Elements
  of information theory}}}.
\newblock John Wiley {\&} Sons, 2006.

\bibitem[Dann and Brunskill(2015)]{dann2015sample}
Christoph Dann and Emma Brunskill.
\newblock \href{https://arxiv.org/pdf/1510.08906.pdf}{{Sample complexity of
  episodic fixed-horizon reinforcement learning}}.
\newblock In \emph{Neural Information Processing Systems}, 2015.

\bibitem[Dann et~al.(2017)Dann, Lattimore, and Brunskill]{dann2017unifying}
Christoph Dann, Tor Lattimore, and Emma Brunskill.
\newblock \href{https://arxiv.org/pdf/1703.07710.pdf}{{Unifying {PAC} and
  regret: Uniform {PAC} bounds for episodic reinforcement learning}}.
\newblock In \emph{Neural Information Processing Systems}, 2017.

\bibitem[Dann et~al.(2019)Dann, Li, Wei, and Brunskill]{dann2019policy}
Christoph Dann, Lihong Li, Wei Wei, and Emma Brunskill.
\newblock \href{https://arxiv.org/pdf/1811.03056.pdf}{{Policy certificates:
  Towards accountable reinforcement learning}}.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[de~la Pe{\~{n}}a et~al.(2004)de~la Pe{\~{n}}a, Klass, and
  Lai]{de2004self}
Victor~H. de~la Pe{\~{n}}a, Michael~J. Klass, and Tze~Leung Lai.
\newblock \href{https://arxiv.org/pdf/math/0410102.pdf}{{Self-normalized
  processes: {E}xponential inequalities, moment bounds and iterated logarithm
  laws}}.
\newblock \emph{Annals of probability}, 32:\penalty0 1902--1933, 2004.

\bibitem[Domingues et~al.(2020)Domingues, M{\'e}nard, Pirotta, Kaufmann, and
  Valko]{domingues2020regret}
Omar~Darwiche Domingues, Pierre M{\'e}nard, Matteo Pirotta, Emilie Kaufmann,
  and Michal Valko.
\newblock \href{http://arxiv.org/abs/2004.05599}{Regret bounds for kernel-based
  reinforcement learning}.
\newblock \emph{arXiv preprint arXiv:2004.05599}, 2020.

\bibitem[Durrett(2010)]{durrett2010probability}
Rick Durrett.
\newblock
  \href{https://services.math.duke.edu/{~}rtd/PTE/PTE5{\_}011119.pdf}{\emph{{Probability:
  Theory and Examples}}}.
\newblock Cambridge Series in Statistical and Probabilistic Mathematics.
  Cambridge University Press, 4 edition, 2010.

\bibitem[Even-Dar et~al.(2006)Even-Dar, Mannor, and Mansour]{even2006action}
Eyal Even-Dar, Shie Mannor, and Yishay Mansour.
\newblock
  \href{https://jmlr.csail.mit.edu/papers/volume7/evendar06a/evendar06a.pdf}{{Action
  elimination and stopping conditions for the multi-armed bandit and
  reinforcement learning problems}}.
\newblock \emph{Journal of Machine Learning Research}, 7:\penalty0 1079--1105,
  2006.

\bibitem[Fiechter(1994)]{fiechter1994efficient}
Claude-Nicolas Fiechter.
\newblock
  \href{http://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=7F5F8FCD1AA7ED07356410DDD5B384FE?doi=10.1.1.49.8652\&rep=rep1\&type=pdf}{{Efficient
  reinforcement learning}}.
\newblock In \emph{Conference on Learning Theory}, 1994.

\bibitem[Garivier et~al.(2019)Garivier, M{\'{e}}nard, and
  Stoltz]{garivier2019explore}
Aur{\'{e}}lien Garivier, Pierre M{\'{e}}nard, and Gilles Stoltz.
\newblock \href{https://arxiv.org/pdf/1602.07182.pdf}{{Explore first, exploit
  next: The true shape of regret in bandit problems}}.
\newblock \emph{Mathematics of Operations Research}, 44\penalty0 (2):\penalty0
  377--399, 2019.

\bibitem[Hazan et~al.(2019)Hazan, Kakade, Singh, and Soest]{hazan2019provably}
Elad Hazan, Sham Kakade, Karan Singh, and Abby~Van Soest.
\newblock \href{https://arxiv.org/pdf/1812.02690.pdf}{{Provably efficient
  maximum entropy exploration}}.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Jaksch et~al.(2010)Jaksch, Ortner, and Auer]{jaksch2010near}
Thomas Jaksch, Ronald Ortner, and Peter Auer.
\newblock
  \href{http://www.jmlr.org/papers/volume11/jaksch10a/jaksch10a.pdf}{{Near-optimal
  regret bounds for reinforcement learning}}.
\newblock \emph{Journal of Machine Learning Research}, 99:\penalty0 1563--1600,
  2010.

\bibitem[Jin et~al.(2018)Jin, Allen-Zhu, Bubeck, and Jordan]{jin2018is}
Chi Jin, Zeyuan Allen-Zhu, S{\'{e}}bastien Bubeck, and Michael~I. Jordan.
\newblock \href{https://arxiv.org/pdf/1807.03765.pdf}{{Is Q-learning provably
  efficient?}}
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Jin et~al.(2020)Jin, Krishnamurthy, Simchowitz, and
  Yu]{jin2020reward-free}
Chi Jin, Akshay Krishnamurthy, Max Simchowitz, and Tiancheng Yu.
\newblock \href{http://arxiv.org/abs/2002.02794}{{Reward-free exploration for
  reinforcement learning}}.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Jonsson et~al.(2020)Jonsson, Kaufmann, M{\'{e}}nard, Domingues,
  Leurent, and Valko]{jonsson2020planning}
Anders Jonsson, Emilie Kaufmann, Pierre M{\'{e}}nard, Omar~Darwiche Domingues,
  Edouard Leurent, and Michal Valko.
\newblock \href{http://arxiv.org/abs/2006.05879}{{Planning in markov decision
  processes with gap-dependent sample complexity}}.
\newblock \emph{arXiv preprint arXiv:2006.05879},  2020.

\bibitem[Kakade(2003)]{kakade2013on}
Sham Kakade.
\newblock
  \href{https://homes.cs.washington.edu/~sham/papers/thesis/sham_thesis.pdf}{\emph{{On
  the sample complexity of reinforcement learning}}}.
\newblock PhD thesis, University College London, 2003.

\bibitem[Kaufmann et~al.(2020)Kaufmann, M{\'{e}}nard, Domingues, Jonsson,
  Leurent, and Valko]{kaufmann2020adaptive}
Emilie Kaufmann, Pierre M{\'{e}}nard, Omar~Darwiche Domingues, Anders Jonsson,
  Edouard Leurent, and Michal Valko.
\newblock \href{https://arxiv.org/pdf/2006.06294.pdf}{{Adaptive reward-free
  exploration}}.
\newblock \emph{arXiv preprint arXiv:2006.06294}, 2020.

\bibitem[Kearns and Singh(1998)]{kearns1998finite-sample}
Michael~J. Kearns and Satinder~P. Singh.
\newblock
  \href{http://papers.neurips.cc/paper/1531-finite-sample-convergence-rates-for-q-learning-and-indirect-algorithms.pdf}{{Finite-sample
  convergence rates for Q-learning and indirect algorithms}}.
\newblock In \emph{Neural Information Processing Systems}, 1998.

\bibitem[Puterman(1994)]{puterman1994markov}
Martin~L. Puterman.
\newblock
  \href{https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316887}{\emph{{Markov
  decision processes: Discrete stochastic dynamic programming}}}.
\newblock John Wiley {\&} Sons, New York, NY, 1994.

\bibitem[Sidford et~al.(2018)Sidford, Wang, Wu, Yang, and Ye]{sidford2018near}
Aaron Sidford, Mengdi Wang, Xian Wu, Lin~F. Yang, and Yinyu Ye.
\newblock \href{https://arxiv.org/pdf/1806.01492.pdf}{{Near-optimal time and
  sample complexities for solving discounted Markov decision process with a
  generative model}}.
\newblock In \emph{Neural Information Processing Systems}, 2018.

\bibitem[Talebi and Maillard(2018)]{talebi2018variance}
Mohammad~Sadegh Talebi and Odalric-Ambrym Maillard.
\newblock \href{https://arxiv.org/pdf/1803.01626.pdf}{{Variance-aware regret
  bounds for undiscounted reinforcement learning in MDPs}}.
\newblock In \emph{Algorithmic Learning Theory}, 2018.

\bibitem[Wang et~al.(2020)Wang, Du, Yang, and Salakhutdinov]{wang2020on}
Ruosong Wang, Simon~S Du, Lin~F Yang, and Ruslan Salakhutdinov.
\newblock \href{https://arxiv.org/pdf/2006.11274.pdf}{{On reward-free
  reinforcement learning with linear function approximation}}.
\newblock \emph{arXiv preprint arXiv:2006.11274}, 2020.

\bibitem[Zanette and Brunskill(2019)]{zanette2019tighter}
Andrea Zanette and Emma Brunskill.
\newblock \href{https://arxiv.org/pdf/1901.00210.pdf}{{Tighter
  problem-dependent regret bounds in reinforcement learning without domain
  knowledge using value function bounds}}.
\newblock In \emph{International Conference on Machine Learning}, 2019.

\bibitem[Zhang et~al.(2020)Zhang, Ma, and Singla]{zhang2020task-agnostic}
Xuezhou Zhang, Yuzhe Ma, and Adish Singla.
\newblock \href{https://arxiv.org/pdf/2006.09497.pdf}{{Task-agnostic
  exploration in reinforcement learning}}.
\newblock \emph{arXiv preprint: arXiv:2006.09497}, 2020.

\end{thebibliography}
