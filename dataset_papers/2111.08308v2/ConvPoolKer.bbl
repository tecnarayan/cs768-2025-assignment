\newcommand{\etalchar}[1]{$^{#1}$}
\providecommand{\bysame}{\leavevmode\hbox to3em{\hrulefill}\thinspace}
\providecommand{\MR}{\relax\ifhmode\unskip\space\fi MR }
% \MRhref is called by the amsart/book/proc definition of \MR.
\providecommand{\MRhref}[2]{%
  \href{http://www.ams.org/mathscinet-getitem?mr=#1}{#2}
}
\providecommand{\href}[2]{#2}
\begin{thebibliography}{GMMM21}

\bibitem[ADH{\etalchar{+}}19]{arora2019exact}
Sanjeev Arora, Simon~S Du, Wei Hu, Zhiyuan Li, Russ Salakhutdinov, and Ruosong
  Wang, \emph{On exact computation with an infinitely wide neural net},
  Advances in Neural Information Processing Systems, 2019, pp.~8139--8148.

\bibitem[AZLS19]{allen2018convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song, \emph{On the convergence rate of
  training recurrent neural networks}, Advances in Neural Information
  Processing Systems, 2019, pp.~6676--6688.

\bibitem[Bac21]{bach2021learning}
Francis Bach, \emph{Learning theory from first principles}, 2021.

\bibitem[BBL05]{boucheron2005theory}
St{\'e}phane Boucheron, Olivier Bousquet, and G{\'a}bor Lugosi, \emph{Theory of
  classification: A survey of some recent advances}, ESAIM: probability and
  statistics \textbf{9} (2005), 323--375.

\bibitem[Bec75]{beckner1975inequalities}
William Beckner, \emph{{Inequalities in Fourier analysis}}, Annals of
  Mathematics (1975), 159--182.

\bibitem[Bec92]{beckner1992sobolev}
\bysame, \emph{{Sobolev inequalities, the Poisson semigroup, and analysis on
  the sphere $S^n$}}, Proceedings of the National Academy of Sciences
  \textbf{89} (1992), no.~11, 4816--4819.

\bibitem[Bie21]{bietti2021approximation}
Alberto Bietti, \emph{Approximation and learning with deep convolutional
  models: a kernel perspective}, arXiv preprint arXiv:2102.10032 (2021).

\bibitem[Bon70]{bonami1970etude}
Aline Bonami, \emph{{Etude des coefficients de Fourier des fonctions de
  $L^p(G)$}}, Annales de l'institut Fourier, vol.~20, 1970, pp.~335--402.

\bibitem[BPL10]{boureau2010theoretical}
Y-Lan Boureau, Jean Ponce, and Yann LeCun, \emph{A theoretical analysis of
  feature pooling in visual recognition}, Proceedings of the 27th international
  conference on machine learning (ICML-10), 2010, pp.~111--118.

\bibitem[BVB21]{bietti2021sample}
Alberto Bietti, Luca Venturi, and Joan Bruna, \emph{On the sample complexity of
  learning with geometric stability}, arXiv preprint arXiv:2106.07148 (2021).

\bibitem[CBP21]{canatar2021spectral}
Abdulkadir Canatar, Blake Bordelon, and Cengiz Pehlevan, \emph{Spectral bias
  and task-model alignment explain generalization in kernel regression and
  infinitely wide neural networks}, Nature communications \textbf{12} (2021),
  no.~1, 1--12.

\bibitem[CDV07]{caponnetto2007optimal}
Andrea Caponnetto and Ernesto De~Vito, \emph{Optimal rates for the regularized
  least-squares algorithm}, Foundations of Computational Mathematics \textbf{7}
  (2007), no.~3, 331--368.

\bibitem[CLKZ21]{cui2021generalization}
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborov{\'a},
  \emph{Generalization error rates in kernel regression: The crossover from the
  noiseless to noisy regime}, arXiv preprint arXiv:2105.15004 (2021).

\bibitem[CNL11]{coates2011analysis}
Adam Coates, Andrew Ng, and Honglak Lee, \emph{An analysis of single-layer
  networks in unsupervised feature learning}, Proceedings of the Fourteenth
  International Conference on Artificial Intelligence and Statistics (Fort
  Lauderdale, FL, USA) (Geoffrey Gordon, David Dunson, and Miroslav Dud√≠k,
  eds.), Proceedings of Machine Learning Research, vol.~15, PMLR, 11--13 Apr
  2011, pp.~215--223.

\bibitem[COB19]{chizat2019lazy}
Lenaic Chizat, Edouard Oyallon, and Francis Bach, \emph{On lazy training in
  differentiable programming}, Advances in Neural Information Processing
  Systems, 2019, pp.~2933--2943.

\bibitem[CS16a]{cohen2016convolutional}
Nadav Cohen and Amnon Shashua, \emph{Convolutional rectifier networks as
  generalized tensor decompositions}, International Conference on Machine
  Learning, PMLR, 2016, pp.~955--963.

\bibitem[CS16b]{cohen2016inductive}
\bysame, \emph{Inductive bias of deep convolutional networks through pooling
  geometry}, arXiv preprint arXiv:1605.06743 (2016).

\bibitem[DFS16]{daniely2016toward}
Amit Daniely, Roy Frostig, and Yoram Singer, \emph{Toward deeper understanding
  of neural networks: The power of initialization and a dual view on
  expressivity}, Advances in Neural Information Processing Systems, 2016,
  pp.~2253--2261.

\bibitem[DLL{\etalchar{+}}19]{du2018gradient2}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai, \emph{Gradient
  descent finds global minima of deep neural networks}, Proceedings of the 36th
  International Conference on Machine Learning, Proceedings of Machine Learning
  Research, vol.~97, PMLR, 09--15 Jun 2019, pp.~1675--1685.

\bibitem[DZPS19]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh, \emph{Gradient descent
  provably optimizes over-parameterized neural networks}, International
  Conference on Learning Representations, 2019.

\bibitem[EK10]{el2010spectrum}
Noureddine El~Karoui, \emph{The spectrum of kernel random matrices}, The Annals
  of Statistics \textbf{38} (2010), no.~1, 1--50.

\bibitem[FCW21]{favero2021locality}
Alessandro Favero, Francesco Cagnetta, and Matthieu Wyart, \emph{Locality
  defeats the curse of dimensionality in convolutional teacher-student
  scenarios}, arXiv preprint arXiv:2106.08619 (2021).

\bibitem[GMMM20]{ghorbani2020neural}
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari,
  \emph{When do neural networks outperform kernel methods?}, Advances in Neural
  Information Processing Systems \textbf{33} (2020).

\bibitem[GMMM21]{ghorbani2021linearized}
\bysame, \emph{Linearized two-layers neural networks in high dimension}, The
  Annals of Statistics \textbf{49} (2021), no.~2, 1029--1054.

\bibitem[Gro75]{gross1975logarithmic}
Leonard Gross, \emph{Logarithmic sobolev inequalities}, American Journal of
  Mathematics \textbf{97} (1975), no.~4, 1061--1083.

\bibitem[HBM07]{harchaoui2007testing}
Zaid Harchaoui, Francis~R Bach, and Eric Moulines, \emph{Testing for
  homogeneity with kernel fisher discriminant analysis.}, NIPS, Citeseer, 2007,
  pp.~609--616.

\bibitem[HZRS16]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, \emph{Deep residual
  learning for image recognition}, Proceedings of the IEEE conference on
  computer vision and pattern recognition, 2016, pp.~770--778.

\bibitem[JGH18]{jacot2018neural}
Arthur Jacot, Franck Gabriel, and Cl{\'e}ment Hongler, \emph{Neural tangent
  kernel: Convergence and generalization in neural networks}, Advances in
  Neural Information Processing Systems, 2018, pp.~8580--8589.

\bibitem[J{\c{S}}S{\etalchar{+}}20]{jacot2020kernel}
Arthur Jacot, Berfin {\c{S}}im{\c{s}}ek, Francesco Spadaro, Cl{\'e}ment
  Hongler, and Franck Gabriel, \emph{Kernel alignment risk estimator: Risk
  prediction from training data}, arXiv preprint arXiv:2006.09796 (2020).

\bibitem[KSH12]{krizhevsky2012imagenet}
Alex Krizhevsky, Ilya Sutskever, and Geoffrey~E Hinton, \emph{Imagenet
  classification with deep convolutional neural networks}, Advances in Neural
  Information Processing Systems, 2012, pp.~1097--1105.

\bibitem[LBH15]{lecun2015deep}
Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, \emph{Deep learning}, Nature
  \textbf{521} (2015), no.~7553, 436--444.

\bibitem[LR{\etalchar{+}}20]{liang2020just}
Tengyuan Liang, Alexander Rakhlin, et~al., \emph{Just interpolate: Kernel
  ``ridgeless'' regression can generalize}, Annals of Statistics \textbf{48}
  (2020), no.~3, 1329--1347.

\bibitem[LWY{\etalchar{+}}19]{li2019enhanced}
Zhiyuan Li, Ruosong Wang, Dingli Yu, Simon~S Du, Wei Hu, Ruslan Salakhutdinov,
  and Sanjeev Arora, \emph{Enhanced convolutional neural tangent kernels},
  arXiv preprint arXiv:1911.00809 (2019).

\bibitem[LZA20]{li2020convolutional}
Zhiyuan Li, Yi~Zhang, and Sanjeev Arora, \emph{Why are convolutional nets more
  sample-efficient than fully-connected nets?}, arXiv preprint arXiv:2010.08515
  (2020).

\bibitem[Mai16]{mairal2016end}
Julien Mairal, \emph{End-to-end kernel learning with supervised convolutional
  kernel networks}, arXiv preprint arXiv:1605.06265 (2016).

\bibitem[MKHS14]{mairal2014convolutional}
Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid,
  \emph{Convolutional kernel networks}, arXiv preprint arXiv:1406.3332 (2014).

\bibitem[MMM21a]{mei2021generalization}
Song Mei, Theodor Misiakiewicz, and Andrea Montanari, \emph{Generalization
  error of random features and kernel methods: hypercontractivity and kernel
  matrix concentration}, arXiv preprint arXiv:2101.10588 (2021).

\bibitem[MMM21b]{mei2021learning}
\bysame, \emph{Learning with invariances in random features and kernel models},
  arXiv preprint arXiv:2102.13219 (2021).

\bibitem[MP16]{mhaskar2016deep}
Hrushikesh~N Mhaskar and Tomaso Poggio, \emph{Deep vs. shallow networks: An
  approximation theory perspective}, Analysis and Applications \textbf{14}
  (2016), no.~06, 829--848.

\bibitem[MSS20]{malach2020computational}
Eran Malach and Shai Shalev-Shwartz, \emph{Computational separation between
  convolutional and fully-connected networks}, arXiv preprint arXiv:2010.01369
  (2020).

\bibitem[O'D14]{o2014analysis}
Ryan O'Donnell, \emph{Analysis of boolean functions}, Cambridge University
  Press, 2014.

\bibitem[RV13]{rudelson2013hanson}
Mark Rudelson and Roman Vershynin, \emph{Hanson-wright inequality and
  sub-gaussian concentration}, Electronic Communications in Probability
  \textbf{18} (2013).

\bibitem[SFG{\etalchar{+}}20]{shankar2020neural}
Vaishaal Shankar, Alex Fang, Wenshuo Guo, Sara Fridovich-Keil, Jonathan
  Ragan-Kelley, Ludwig Schmidt, and Benjamin Recht, \emph{Neural kernels
  without tangents}, International Conference on Machine Learning, PMLR, 2020,
  pp.~8614--8623.

\bibitem[SH20]{scetbon2020harmonic}
Meyer Scetbon and Zaid Harchaoui, \emph{Harmonic decompositions of
  convolutional networks}, International Conference on Machine Learning, PMLR,
  2020, pp.~8522--8532.

\bibitem[SSBD14]{shalev2014understanding}
Shai Shalev-Shwartz and Shai Ben-David, \emph{Understanding machine learning:
  From theory to algorithms}, Cambridge University Press, 2014.

\bibitem[TABO21]{thiry2021unreasonable}
Louis Thiry, Michael Arbel, Eugene Belilovsky, and Edouard Oyallon, \emph{The
  unreasonable effectiveness of patches in deep convolutional kernels methods},
  arXiv preprint arXiv:2101.07528 (2021).

\bibitem[Wai19]{wainwright2019high}
Martin~J Wainwright, \emph{High-dimensional statistics: A non-asymptotic
  viewpoint}, vol.~48, Cambridge University Press, 2019.

\bibitem[Xia21]{xiao2021eigenspace}
Lechao Xiao, \emph{Eigenspace restructuring: a principle of space and frequency
  in neural networks}, arXiv preprint arXiv:2112.05611 (2021).

\bibitem[ZCZG18]{zou2018stochastic}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu, \emph{Stochastic gradient
  descent optimizes over-parameterized deep relu networks}, arXiv:1811.08888
  (2018).

\end{thebibliography}
