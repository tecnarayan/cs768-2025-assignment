@inproceedings{NIPS2014_5ca3e9b1,
 author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Generative Adversarial Nets},
 url = {https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf},
 volume = {27},
 year = {2014}
}



@inproceedings{friedler2019comparative,
author = {Friedler, Sorelle A. and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P. and Roth, Derek},
title = {A Comparative Study of Fairness-Enhancing Interventions in Machine Learning},
year = {2019},
isbn = {9781450361255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287560.3287589},
doi = {10.1145/3287560.3287589},
booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
pages = {329–338},
numpages = {10},
keywords = {benchmarks, Fairness-aware machine learning},
location = {Atlanta, GA, USA},
series = {FAT* '19}
}

@inproceedings{xu2019modeling,
  title={Modeling Tabular data using Conditional GAN},
  author={Xu, Lei and Skoularidou, Maria and Cuesta-Infante, Alfredo and Veeramachaneni, Kalyan},
  booktitle={Advances in Neural Information Processing Systems},
  year={2019}
}

@article{fabris2022algorithmic,
  title={Algorithmic Fairness Datasets: the Story so Far},
  author={Fabris, Alessandro and Messina, Stefano and Silvello, Gianmaria and Susto, Gian Antonio},
  journal={arXiv preprint arXiv:2202.01711},
  year={2022}
}


@article{borji2022evaluation,
  title = {Pros and cons of GAN evaluation measures: New   developments},
  journal = {Computer Vision and Image Understanding},
  volume = {215},
  pages = {103329},
  year = {2022},
  issn = {1077-3142},
  doi = {https://doi.org/10.1016/j.cviu.2021.103329},
  url = {https://www.sciencedirect.com/science/article/pii/S1077314221001685},
  author = {Ali Borji},
  keywords = {GAN evaluation, Generative modeling, Deepfakes}
}

@InProceedings{zhao2021effective,
  title = {CTAB-GAN: Effective Table Data Synthesizing},
  author = {Zhao, Zilong and Kunar, Aditya and Birke, Robert and Chen, Lydia Y.},
  booktitle = {Proceedings of The 13th Asian Conference on Machine Learning},
  pages = {97--112},
  year = {2021},
  editor = {Balasubramanian, Vineeth N. and Tsang, Ivor},
  volume = {157},
  series = {Proceedings of Machine Learning Research},
  month = {17--19 Nov},
  publisher = {PMLR},
  pdf = {https://proceedings.mlr.press/v157/zhao21a/zhao21a.pdf},
  url = {https://proceedings.mlr.press/v157/zhao21a.html}
}

@InProceedings{ding2021retiring,
  title={Retiring Adult: New Datasets For Fair Machine Learning},
  author={Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
  booktitle={Advances in Neural Information Processing Systems},
  year={2021}
}

@inproceedings{bao2021compaslicated,
  author    = {Michelle Bao and
               Angela Zhou and
               Samantha Zottola and
               Brian Brubach and
               Brian Brubach and
               Sarah Desmarais and
               Aaron Horowitz and
               Kristian Lum and
               Suresh Venkatasubramanian},
  editor    = {Joaquin Vanschoren and
               Sai{-}Kit Yeung},
  title     = {It's COMPASlicated: The Messy Relationship between {RAI} Datasets
               and Algorithmic Fairness Benchmarks},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on
               Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
               2021, virtual},
  year      = {2021},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/92cc227532d17e56e07902b254dfad10-Abstract-round1.html},
  timestamp = {Thu, 05 May 2022 16:53:59 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/BaoZZBBDHLV21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{koch2021reduced,
  author    = {Bernard Koch and
               Emily Denton and
               Alex Hanna and
               Jacob G. Foster},
  editor    = {Joaquin Vanschoren and
               Sai{-}Kit Yeung},
  title     = {Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning
               Research},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on
               Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
               2021, virtual},
  year      = {2021},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/3b8a614226a953a8cd9526fca6fe9ba5-Abstract-round2.html},
  timestamp = {Thu, 05 May 2022 16:53:59 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/KochDHF21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{mehrabi2021survey,
  title={A survey on bias and fairness in machine learning},
  author={Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  journal={ACM Computing Surveys (CSUR)},
  volume={54},
  number={6},
  pages={1--35},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{kohavi1996scaling,
  author = {Kohavi, Ron},
  title = {Scaling up the Accuracy of Naive-Bayes Classifiers: A Decision-Tree Hybrid},
  year = {1996},
  publisher = {AAAI Press},
  booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
  pages = {202–207},
  numpages = {6},
  location = {Portland, Oregon},
  series = {KDD'96}
}


@inproceedings{quy2022survey,
  author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and   Ntoutsi, Eirini},
  year = {2022},
  month = {03},
  pages = {},
  title = {A survey on datasets for fairness‐aware machine learning},
  booktitle = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
  doi = {10.1002/widm.1452}
}

@inproceedings{barenstein2019propublica,
  author = {{Barenstein}, Matias},
  title = "{ProPublica's COMPAS Data Revisited}",
  journal = {arXiv e-prints},
  year = "2019",
  month = "Jun",
  eid = {arXiv:1906.04711},
  pages = {arXiv:1906.04711},
  archivePrefix = {arXiv},
  eprint = {1906.04711},
  primaryClass = {econ.GN},
  adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190604711B},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{kato2012,
author = {Mivule, Kato},
year = {2012},
month = {07},
pages = {},
title = {Utilizing Noise Addition for Data Privacy, an Overview},
booktitle = {Proceedings of the  International Conference on Information and Knowledge Engineering (IKE 2012)},
doi = {10.13140/2.1.4629.2482}
}


@misc{dua2017uci,
  author = "Dua, Dheeru and Graff, Casey",
  year = "2017",
  title = "{UCI} Machine Learning Repository",
  url = "http://archive.ics.uci.edu/ml",
  institution = "University of California, Irvine, School of Information and Computer   Sciences"
}

@misc{angwin2016machine,
  title = {Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks},
  howpublished = {\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}},
  journal = {ProPublica},
  author = {Angwin, Julia and Larson, Jeff and Kirchner, Lauren and Mattu, Surya},
  year = {2016},
  month = {May},
  note = {Accessed: 2022-05-16},
  urldate = {2022-05-16},
}

@inproceedings{gromping2019south,
  author = "Grömping, Ulrike",
  title = "South German Credit Data: Correcting a Widely Used Data Set", 
  year = "2019", 
  booktitle = "Reports in Mathematics, Physics and Chemistry",
  institution = "Department II, Beuth University of Applied Sciences Berlin"
}

@inproceedings{zhang2021index,
  author = {Zhang, Daniel and Mishra, Saurabh and Brynjolfsson, Erik and Etchemendy, John and Ganguli, Deep and Grosz, Barbara and Lyons, Terah and Manyika, James and Niebles, Juan and Sellitto, Michael and Shoham, Yoav and Clark, Jack and Perrault, Raymond},
  year = {2021},
  month = {March},
  title = {The AI Index 2021 Annual Report},
  booktitle = {AI Index Steering Committee, Human-Centered AI Institute, Stanford University, Stanford, CA}
}

@article{kroger2021how,
  title={How Data Can Be Used Against People: A Classification of Personal Data Misuses},
  author={Jacob Leon Kr{\"o}ger and Milagros Miceli and Florian M{\"u}ller},
  journal={SSRN Electronic Journal},
  year={2021}
}


@inproceedings{jordon2018pate,
  title={PATE-GAN: Generating synthetic data with differential privacy guarantees},
  author={Jordon, James and Yoon, Jinsung and Van Der Schaar, Mihaela},
  booktitle={International conference on learning representations},
  year={2018}
}

@inproceedings{dwork2006calibrating,
  title={Calibrating noise to sensitivity in private data analysis},
  author={Dwork, Cynthia and McSherry, Frank and Nissim, Kobbi and Smith, Adam},
  booktitle={Theory of cryptography conference},
  pages={265--284},
  year={2006},
  organization={Springer}
}


@inproceedings{papernot2017semi,
  author = {Nicolas Papernot and Mart{\'{\i}}n Abadi and {\'{U}}lfar Erlingsson and Ian J. Goodfellow and Kunal Talwar},
  title = {Semi-supervised Knowledge Transfer for Deep Learning from Private Training Data},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year = {2017},
  url = {https://openreview.net/forum?id=HkwoSDPgg},
  timestamp = {Thu, 04 Apr 2019 13:20:08 +0200},
  biburl = {https://dblp.org/rec/conf/iclr/PapernotAEGT17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2018differentially,
  author = {Qingrong Chen and Chong Xiang and Minhui Xue and Bo Li and Nikita Borisov and Dali Kaafar and Haojin Zhu},
  title = {Differentially Private Data Generative Models},
  journal = {CoRR},
  volume = {abs/1812.02274},
  year = {2018},
  url = {http://arxiv.org/abs/1812.02274},
  eprinttype = {arXiv},
  eprint = {1812.02274},
  timestamp = {Mon, 09 Aug 2021 08:23:49 +0200},
  biburl = {https://dblp.org/rec/journals/corr/abs-1812-02274.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{gebru2018datasheets,
author = {Gebru, Timnit and Morgenstern, Jamie and Vecchione, Briana and Wortman Vaughan, Jennifer and Wallach, Hanna and Daumé III, Hal and Crawford, Kate},
title = {Datasheets for Datasets},
year = {2018},
month = {March},
abstract = {The machine learning community currently has no standardized process for documenting datasets, which can lead to severe consequences in high-stakes domains. To address this gap, we propose datasheets for datasets. In the electronics industry, every component, no matter how simple or complex, is accompanied with a datasheet that describes its operating characteristics, test results, recommended uses, and other information. By analogy, we propose that every dataset be accompanied with a datasheet that documents its motivation, composition, collection process, recommended uses, and so on. Datasheets for datasets will facilitate better communication between dataset creators and dataset consumers, and encourage the machine learning community to prioritize transparency and accountability.},
url = {https://www.microsoft.com/en-us/research/publication/datasheets-for-datasets/},
}

@inproceedings{ke2017lightgbm,
 author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {LightGBM: A Highly Efficient Gradient Boosting Decision Tree},
 url = {https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf},
 volume = {30},
 year = {2017}
}

@article{luccioni2022framework,
  url = {https://arxiv.org/abs/2111.04424},
  author = {Luccioni, Alexandra Sasha and Corry, Frances and Sridharan, Hamsini and Ananny, Mike and Schultz, Jason and Crawford, Kate},
  title = {A Framework for Deprecating Datasets: Standardizing Documentation, Identification, and Communication},
  publisher = {arXiv},
  year = {2022}
}

@inproceedings{zafar2017fairness,
  title={Fairness Constraints: Mechanisms for Fair Classification},
  author={Muhammad Bilal Zafar and Isabel Valera and Manuel Gomez-Rodriguez and Krishna P. Gummadi},
  booktitle={AISTATS},
  year={2017}
}


% strategic classification
@inproceedings{strategic-classification,
  author    = {Nilesh N. Dalvi and
               Pedro M. Domingos and
               Mausam and
               Sumit K. Sanghai and
               Deepak Verma},
  editor    = {Won Kim and
               Ron Kohavi and
               Johannes Gehrke and
               William DuMouchel},
  title     = {Adversarial classification},
  booktitle = {Proceedings of the Tenth {ACM} {SIGKDD} International Conference on
               Knowledge Discovery and Data Mining, Seattle, Washington, USA, August
               22-25, 2004},
  pages     = {99--108},
  publisher = {{ACM}},
  year      = {2004},
  url       = {https://doi.org/10.1145/1014052.1014066},
  doi       = {10.1145/1014052.1014066},
  timestamp = {Tue, 06 Nov 2018 16:59:36 +0100},
  biburl    = {https://dblp.org/rec/conf/kdd/DalviDMSV04.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

% performative prediction
@inproceedings{performative-prediction,
  author    = {Juan C. Perdomo and
               Tijana Zrnic and
               Celestine Mendler{-}D{\"{u}}nner and
               Moritz Hardt},
  title     = {Performative Prediction},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning,
               {ICML} 2020, 13-18 July 2020, Virtual Event},
  series    = {Proceedings of Machine Learning Research},
  volume    = {119},
  pages     = {7599--7609},
  publisher = {{PMLR}},
  year      = {2020},
  url       = {http://proceedings.mlr.press/v119/perdomo20a.html},
  timestamp = {Tue, 15 Dec 2020 17:40:19 +0100},
  biburl    = {https://dblp.org/rec/conf/icml/PerdomoZMH20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


%%% intro refs on fairness

@article{exacerbate-inequities-howard2018ugly,
  title={The ugly truth about ourselves and our robot creations: the problem of bias and social inequity},
  author={Howard, Ayanna and Borenstein, Jason},
  journal={Science and engineering ethics},
  volume={24},
  number={5},
  pages={1521--1536},
  year={2018},
  publisher={Springer}
}

@misc{exacerbate-inequities-kirchner2016machine,
  title     = {Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks},
  howpublished = {\url{https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing}},
  journal   = {ProPublica},
  author    = {Angwin, Julia and Larson, Jeff and Kirchner, Lauren and Mattu, Surya},
  year      = {2016},
  month     = {May},
  urldate   = {2020-01-09},
}

@book{exacerbate-inequities-o2016weapons,
  title={Weapons of math destruction: How big data increases inequality and threatens democracy},
  author={O'Neil, Cathy},
  year={2016},
  publisher={Crown}
}

@article{measure-barocas,
  title={Fairness in machine learning},
  author={Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  journal={{NIPS} tutorial},
  volume={1},
  pages={2017},
  year={2017}
}

@inproceedings{empirical-friedler2019comparative,
  title={A comparative study of fairness-enhancing interventions in machine learning},
  author={Friedler, Sorelle A and Scheidegger, Carlos and Venkatasubramanian, Suresh and Choudhary, Sonam and Hamilton, Evan P and Roth, Derek},
  booktitle={Proceedings of the conference on fairness, accountability, and transparency},
  pages={329--338},
  year={2019}
}

@article{empirical-lamba2021empirical,
  title={An Empirical Comparison of Bias Reduction Methods on Real-World Problems in High-Stakes Policy Settings},
  author={Lamba, Hemank and Rodolfa, Kit T and Ghani, Rayid},
  journal={ACM SIGKDD Explorations Newsletter},
  volume={23},
  number={1},
  pages={69--85},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inbook{mlstory,
  author = {Moritz Hardt and Benjamin Recht},
  title = {Patterns, predictions, and actions: Foundations of machine learning},
  year = {2022},
  chapter = {8},
  publisher = {Princeton University Press}
}

% importance of datasets

@article{importantData1,
  author    = {Alon Y. Halevy and
               Peter Norvig and
               Fernando Pereira},
  title     = {The Unreasonable Effectiveness of Data},
  journal   = {{IEEE} Intell. Syst.},
  volume    = {24},
  number    = {2},
  pages     = {8--12},
  year      = {2009},
  url       = {https://doi.org/10.1109/MIS.2009.36},
  doi       = {10.1109/MIS.2009.36},
  timestamp = {Fri, 06 Mar 2020 22:01:19 +0100},
  biburl    = {https://dblp.org/rec/journals/expert/HalevyNP09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{importantData2,
  author    = {Chen Sun and
               Abhinav Shrivastava and
               Saurabh Singh and
               Abhinav Gupta},
  title     = {Revisiting Unreasonable Effectiveness of Data in Deep Learning Era},
  booktitle = {{IEEE} International Conference on Computer Vision, {ICCV} 2017, Venice,
               Italy, October 22-29, 2017},
  pages     = {843--852},
  publisher = {{IEEE} Computer Society},
  year      = {2017},
  url       = {https://doi.org/10.1109/ICCV.2017.97},
  doi       = {10.1109/ICCV.2017.97},
  timestamp = {Wed, 16 Oct 2019 14:14:51 +0200},
  biburl    = {https://dblp.org/rec/conf/iccv/SunSSG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{importantDataBench,
  author    = {Ravit Dotan and
               Smitha Milli},
  editor    = {Mireille Hildebrandt and
               Carlos Castillo and
               L. Elisa Celis and
               Salvatore Ruggieri and
               Linnet Taylor and
               Gabriela Zanfir{-}Fortuna},
  title     = {Value-laden disciplinary shifts in machine learning},
  booktitle = {FAT* '20: Conference on Fairness, Accountability, and Transparency,
               Barcelona, Spain, January 27-30, 2020},
  pages     = {294},
  publisher = {{ACM}},
  year      = {2020},
  url       = {https://doi.org/10.1145/3351095.3373157},
  doi       = {10.1145/3351095.3373157},
  timestamp = {Thu, 26 Aug 2021 22:19:24 +0200},
  biburl    = {https://dblp.org/rec/conf/fat/DotanM20.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

%%% BANK ACCOUNT BASIC RIGHT
@article{basic_account_EU,
  title={{Directive 2014/92/EU of the European Parliament and of the Council of 23 July 2014 on the comparability of fees related to payment accounts, payment account switching and access to payment accounts with basic features}},
  author={{European Parliament and Council}},
  journal={Official Journal of the European Union (OJ)},
  volume={57},
  pages={214},
  year={2014}
}

%% GDPR
@article{GDPR,
  title={{Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46}},
  author={{European Parliament and Council}},
  journal={Official Journal of the European Union (OJ)},
  volume={59},
  pages={1},
  year={2016}
}

@inproceedings{shifts,
  author    = {Andrey Malinin and
               Neil Band and
               Yarin Gal and
               Mark J. F. Gales and
               Alexander Ganshin and
               German Chesnokov and
               Alexey Noskov and
               Andrey Ploskonosov and
               Liudmila Prokhorenkova and
               Ivan Provilkov and
               Vatsal Raina and
               Vyas Raina and
               Denis Roginskiy and
               Mariya Shmatova and
               Panagiotis Tigas and
               Boris Yangel},
  editor    = {Joaquin Vanschoren and
               Sai{-}Kit Yeung},
  title     = {Shifts: {A} Dataset of Real Distributional Shift Across Multiple Large-Scale
               Tasks},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on
               Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December
               2021, virtual},
  year      = {2021},
  url       = {https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/ad61ab143223efbc24c7d2583be69251-Abstract-round2.html},
  timestamp = {Thu, 05 May 2022 16:53:59 +0200},
  biburl    = {https://dblp.org/rec/conf/nips/MalininBGGGCNPP21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{discontents,
  author    = {Amandalynne Paullada and
               Inioluwa Deborah Raji and
               Emily M. Bender and
               Emily Denton and
               Alex Hanna},
  title     = {Data and its (dis)contents: {A} survey of dataset development and
               use in machine learning research},
  journal   = {Patterns},
  volume    = {2},
  number    = {11},
  pages     = {100336},
  year      = {2021},
  url       = {https://doi.org/10.1016/j.patter.2021.100336},
  doi       = {10.1016/j.patter.2021.100336},
  timestamp = {Fri, 10 Dec 2021 18:59:30 +0100},
  biburl    = {https://dblp.org/rec/journals/patterns/PaulladaRBDH21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{lequySurveyDatasetsFairnessaware2022,
  title = {A Survey on Datasets for Fairness-Aware Machine Learning},
  author = {Le Quy, Tai and Roy, Arjun and Iosifidis, Vasileios and Zhang, Wenbin and Ntoutsi, Eirini},
  year = {2022},
  journal = {WIREs Data Mining and Knowledge Discovery},
  volume = {12},
  number = {3},
  pages = {e1452},
  issn = {1942-4795},
  doi = {10.1002/widm.1452},
  abstract = {As decision-making increasingly relies on machine learning (ML) and (big) data, the issue of fairness in data-driven artificial intelligence systems is receiving increasing attention from both research and industry. A large variety of fairness-aware ML solutions have been proposed which involve fairness-related interventions in the data, learning algorithms, and/or model outputs. However, a vital part of proposing new approaches is evaluating them empirically on benchmark datasets that represent realistic and diverse settings. Therefore, in this paper, we overview real-world datasets used for fairness-aware ML. We focus on tabular data as the most common data representation for fairness-aware ML. We start our analysis by identifying relationships between the different attributes, particularly with respect to protected attributes and class attribute, using a Bayesian network. For a deeper understanding of bias in the datasets, we investigate interesting relationships using exploratory analysis. This article is categorized under: Commercial, Legal, and Ethical Issues {$>$} Fairness in Data Mining Fundamental Concepts of Data and Knowledge {$>$} Data Concepts Technologies {$>$} Data Preprocessing},
  langid = {english},
  keywords = {benchmark datasets,bias,datasets for fairness,discrimination,fairness-aware machine learning},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1452},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FKLZSR2Z/Le Quy et al. - 2022 - A survey on datasets for fairness-aware machine le.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/L6X9YGR2/widm.html}
}


@inproceedings{dalpozzoloCalibratingProbabilityUndersampling2015,
  title = {Calibrating {{Probability}} with {{Undersampling}} for {{Unbalanced Classification}}},
  booktitle = {2015 {{IEEE Symposium Series}} on {{Computational Intelligence}} ({{SSCI}})},
  author = {Dal Pozzolo, Andrea and Caelen, Olivier and Johnson, Reid and Bontempi, Gianluca},
  year = {2015},
  month = dec,
  address = {{Cape Town, South Africa}},
  doi = {10.1109/SSCI.2015.33},
  abstract = {Undersampling is a popular technique for unbalanced datasets to reduce the skew in class distributions. However, it is well-known that undersampling one class modifies the priors of the training set and consequently biases the posterior probabilities of a classifier [9]. In this paper, we study analytically and experimentally how undersampling affects the posterior probability of a machine learning model. We formalize the problem of undersampling and explore the relationship between conditional probability in the presence and absence of undersampling. Although the bias due to undersampling does not affect the ranking order returned by the posterior probability, it significantly impacts the classification accuracy and probability calibration. We use Bayes Minimum Risk theory to find the correct classification threshold and show how to adjust it after undersampling. Experiments on several real-world unbalanced datasets validate our results.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/LZXPHU5Q/Dal Pozzolo et al. - 2015 - Calibrating Probability with Undersampling for Unb.pdf}
}

@inproceedings{lopez-rojasPAYSIMFINANCIALMOBILE2016,
  title = {{{PAYSIM}}: {{A FINANCIAL MOBILE MONEY SIMULATOR FOR FRAUD DETECTION}}},
  shorttitle = {{{PAYSIM}}},
  booktitle = {28th {{European Modeling}} and {{Simulation Symposium}} 2016 ({{EMSS}} 2016)},
  author = {{Lopez-Rojas}, Edgar Alonso and Elmir, Ahmad and Axelsson, Stefan},
  year = {2016},
  month = sep,
  address = {{Larnaca, Cyprus}},
  abstract = {The lack of legitimate datasets on mobile money transactions to perform research on in the domain of fraud detection is a big problem today in the scientific community. Part of the problem is the intrinsic private nature of financial transactions, that leads to no public available data sets. This will leave the researchers with the burden of first harnessing the dataset before performing the actual research on it. This paper propose an approach to such a problem that we named the PaySim simulator. PaySim is a financial simulator that simulates mobile money transactions based on an original dataset. In this paper, we present a solution to ultimately yield the possibility to simulate mobile money transactions in such a way that they become similar to the original dataset. With technology frameworks such as Agent-Based simulation techniques, and the application of mathematical statistics, we show in this paper that the simulated data can be as prudent as the original dataset for research.},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QD2BU2UG/Lopez-Rojas et al. - 2016 - PAYSIM A FINANCIAL MOBILE MONEY SIMULATOR FOR FRA.pdf}
}

@book{fairmlbook,
  title = {Fairness and Machine Learning},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {fairmlbook.org},
  note = {\url{http://www.fairmlbook.org}},
  year = {2019}
}

@inproceedings{corbett-daviesAlgorithmicDecisionMaking2017,
  title = {Algorithmic {{Decision Making}} and the {{Cost}} of {{Fairness}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {{Corbett-Davies}, Sam and Pierson, Emma and Feller, Avi and Goel, Sharad and Huq, Aziz},
  year = {2017},
  month = aug,
  series = {{{KDD}} '17},
  pages = {797--806},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3097983.3098095},
  abstract = {Algorithms are now regularly used to decide whether defendants awaiting trial are too dangerous to be released back into the community. In some cases, black defendants are substantially more likely than white defendants to be incorrectly classified as high risk. To mitigate such disparities, several techniques have recently been proposed to achieve algorithmic fairness. Here we reformulate algorithmic fairness as constrained optimization: the objective is to maximize public safety while satisfying formal fairness constraints designed to reduce racial disparities. We show that for several past definitions of fairness, the optimal algorithms that result require detaining defendants above race-specific risk thresholds. We further show that the optimal unconstrained algorithm requires applying a single, uniform threshold to all defendants. The unconstrained algorithm thus maximizes public safety while also satisfying one important understanding of equality: that all individuals are held to the same standard, irrespective of race. Because the optimal constrained and unconstrained algorithms generally differ, there is tension between improving public safety and satisfying prevailing notions of algorithmic fairness. By examining data from Broward County, Florida, we show that this trade-off can be large in practice. We focus on algorithms for pretrial release decisions, but the principles we discuss apply to other domains, and also to human decision makers carrying out structured decision rules.},
  isbn = {978-1-4503-4887-4},
  keywords = {algorithmic fairness,discrimination,disparate impact,pretrial detention,risk assessment},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/FQU6SPE4/Corbett-Davies et al. - 2017 - Algorithmic Decision Making and the Cost of Fairne.pdf}
}

@misc{vadhanOpenDPOpenSourceSuite2019a,
  title = {{{OpenDP}} : {{An Open-Source Suite}} of {{Differential Privacy Tools}}},
  shorttitle = {{{OpenDP}}},
  author = {Vadhan, S. and Crosas, M. and Honaker, James},
  year = {2019},
  abstract = {This platform, OpenDP, aims for this platform to become the standard body of trusted and open-source implementations of differentially private algorithms for statistical analysis and machine learning on sensitive data, and a pathway that rapidly brings the newest algorithmic developments to a wide array of practitioners. Project Goal We propose to lead a community effort to build a system of tools for enabling privacy-protective analysis of sensitive personal data, focused on an open-source library of algorithms for generating differentially private statistical releases. We aim for this platform, OpenDP, to become the standard body of trusted and open-source implementations of differentially private algorithms for statistical analysis and machine learning on sensitive data, and a pathway that rapidly brings the newest algorithmic developments to a wide array of practitioners.},
  howpublished = {https://www.semanticscholar.org/paper/OpenDP-\%3A-An-Open-Source-Suite-of-Differential-Tools-Vadhan-Crosas/853f3d2673078c0fe7d16b04c7eae307905eb22b},
  langid = {english},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/5JTBV38X/853f3d2673078c0fe7d16b04c7eae307905eb22b.html}
}


@misc{xieDifferentiallyPrivateGenerative2018,
  title = {Differentially {{Private Generative Adversarial Network}}},
  author = {Xie, Liyang and Lin, Kaixiang and Wang, Shu and Wang, Fei and Zhou, Jiayu},
  year = {2018},
  month = feb,
  number = {arXiv:1802.06739},
  eprint = {1802.06739},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1802.06739},
  abstract = {Generative Adversarial Network (GAN) and its variants have recently attracted intensive research interests due to their elegant theoretical foundation and excellent empirical performance as generative models. These tools provide a promising direction in the studies where data availability is limited. One common issue in GANs is that the density of the learned generative distribution could concentrate on the training data points, meaning that they can easily remember training samples due to the high model complexity of deep networks. This becomes a major concern when GANs are applied to private or sensitive data such as patient medical records, and the concentration of distribution may divulge critical patient information. To address this issue, in this paper we propose a differentially private GAN (DPGAN) model, in which we achieve differential privacy in GANs by adding carefully designed noise to gradients during the learning procedure. We provide rigorous proof for the privacy guarantee, as well as comprehensive empirical evidence to support our analysis, where we demonstrate that our method can generate high quality data points at a reasonable privacy level.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JG32WYH6/Xie et al. - 2018 - Differentially Private Generative Adversarial Netw.pdf}
}

@misc{rosenblattDifferentiallyPrivateSynthetic2020,
  title = {Differentially {{Private Synthetic Data}}: {{Applied Evaluations}} and {{Enhancements}}},
  shorttitle = {Differentially {{Private Synthetic Data}}},
  author = {Rosenblatt, Lucas and Liu, Xiaoyan and Pouyanfar, Samira and {de Leon}, Eduardo and Desai, Anuj and Allen, Joshua},
  year = {2020},
  month = nov,
  number = {arXiv:2011.05537},
  eprint = {2011.05537},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.05537},
  abstract = {Machine learning practitioners frequently seek to leverage the most informative available data, without violating the data owner's privacy, when building predictive models. Differentially private data synthesis protects personal details from exposure, and allows for the training of differentially private machine learning models on privately generated datasets. But how can we effectively assess the efficacy of differentially private synthetic data? In this paper, we survey four differentially private generative adversarial networks for data synthesis. We evaluate each of them at scale on five standard tabular datasets, and in two applied industry scenarios. We benchmark with novel metrics from recent literature and other standard machine learning tools. Our results suggest some synthesizers are more applicable for different privacy budgets, and we further demonstrate complicating domain-based tradeoffs in selecting an approach. We offer experimental learning on applied machine learning scenarios with private internal data to researchers and practioners alike. In addition, we propose QUAIL, an ensemble-based modeling approach to generating synthetic data. We examine QUAIL's tradeoffs, and note circumstances in which it outperforms baseline differentially private supervised learning models under the same budget constraint.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/QI2DVSCP/Rosenblatt et al. - 2020 - Differentially Private Synthetic Data Applied Eva.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JKPHVKRK/2011.html}
}

@inproceedings{dworkDifferentialPrivacySurvey2008,
  title = {Differential {{Privacy}}: {{A Survey}} of {{Results}}},
  shorttitle = {Differential {{Privacy}}},
  booktitle = {Theory and {{Applications}} of {{Models}} of {{Computation}}},
  author = {Dwork, Cynthia},
  editor = {Agrawal, Manindra and Du, Dingzhu and Duan, Zhenhua and Li, Angsheng},
  year = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {1--19},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-79228-4_1},
  abstract = {Over the past five years a new approach to privacy-preserving data analysis has born fruit [13, 18, 7, 19, 5, 37, 35, 8, 32]. This approach differs from much (but not all!) of the related literature in the statistics, databases, theory, and cryptography communities, in that a formal and ad omnia privacy guarantee is defined, and the data analysis techniques presented are rigorously proved to satisfy the guarantee. The key privacy guarantee that has emerged is differential privacy. Roughly speaking, this ensures that (almost, and quantifiably) no risk is incurred by joining a statistical database.},
  isbn = {978-3-540-79228-4},
  langid = {english},
  keywords = {Differential Privacy,Privacy Mechanism,Statistical Database,Statistical Query,True Answer}
}

@misc{taoBenchmarkingDifferentiallyPrivate2022,
  title = {Benchmarking {{Differentially Private Synthetic Data Generation Algorithms}}},
  author = {Tao, Yuchao and McKenna, Ryan and Hay, Michael and Machanavajjhala, Ashwin and Miklau, Gerome},
  year = {2022},
  month = feb,
  number = {arXiv:2112.09238},
  eprint = {2112.09238},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.09238},
  abstract = {This work presents a systematic benchmark of differentially private synthetic data generation algorithms that can generate tabular data. Utility of the synthetic data is evaluated by measuring whether the synthetic data preserve the distribution of individual and pairs of attributes, pairwise correlation as well as on the accuracy of an ML classification model. In a comprehensive empirical evaluation we identify the top performing algorithms and those that consistently fail to beat baseline approaches.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Cryptography and Security},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/2QYQWV22/Tao et al. - 2022 - Benchmarking Differentially Private Synthetic Data.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/6H35TVPT/2112.html}
}

@inproceedings{pombalPrisonersTheirOwn2022,
  title = {Prisoners of {{Their Own Devices}}: {{How Models Induce Data Bias}} in {{Performative Prediction}}},
  booktitle = {{{ICML}} 2022 {{Workshop}} on {{Responsible Decision Making}} in {{Dynamic Environments}}},
  author = {Pombal, Jos{\'e} and Saleiro, Pedro and Figueiredo, M{\'a}rio A. T. and Bizarro, Pedro},
  year = {2022},
  month = aug,
  eprint = {2206.13183},
  eprinttype = {arxiv},
  primaryclass = {cs},
  doi = {10.48550/arXiv.2206.13183},
  abstract = {The unparalleled ability of machine learning algorithms to learn patterns from data also enables them to incorporate biases embedded within. A biased model can then make decisions that disproportionately harm certain groups in society. Much work has been devoted to measuring unfairness in static ML environments, but not in dynamic, performative prediction ones, in which most real-world use cases operate. In the latter, the predictive model itself plays a pivotal role in shaping the distribution of the data. However, little attention has been heeded to relating unfairness to these interactions. Thus, to further the understanding of unfairness in these settings, we propose a taxonomy to characterize bias in the data, and study cases where it is shaped by model behaviour. Using a real-world account opening fraud detection case study as an example, we study the dangers to both performance and fairness of two typical biases in performative prediction: distribution shifts, and the problem of selective labels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/SU7W5IBR/Pombal et al. - 2022 - Prisoners of Their Own Devices How Models Induce .pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/GNSRID5A/2206.html}
}

@inproceedings{pombalUnderstandingUnfairnessFraud2022,
  title = {Understanding {{Unfairness}} in {{Fraud Detection}} through {{Model}} and {{Data Bias Interactions}}},
  booktitle = {{{KDD}} 2022 {{Workshop}} on {{Machine Learning}} in {{Finance}}},
  author = {Pombal, Jos{\'e} and Cruz, Andr{\'e} F. and Bravo, Jo{\~a}o and Saleiro, Pedro and Figueiredo, M{\'a}rio A. T. and Bizarro, Pedro},
  year = {2022},
  month = jul,
  eprint = {2207.06273},
  eprinttype = {arxiv},
  primaryclass = {cs, q-fin},
  doi = {10.48550/arXiv.2207.06273},
  abstract = {In recent years, machine learning algorithms have become ubiquitous in a multitude of high-stakes decision-making applications. The unparalleled ability of machine learning algorithms to learn patterns from data also enables them to incorporate biases embedded within. A biased model can then make decisions that disproportionately harm certain groups in society -- limiting their access to financial services, for example. The awareness of this problem has given rise to the field of Fair ML, which focuses on studying, measuring, and mitigating unfairness in algorithmic prediction, with respect to a set of protected groups (e.g., race or gender). However, the underlying causes for algorithmic unfairness still remain elusive, with researchers divided between blaming either the ML algorithms or the data they are trained on. In this work, we maintain that algorithmic unfairness stems from interactions between models and biases in the data, rather than from isolated contributions of either of them. To this end, we propose a taxonomy to characterize data bias and we study a set of hypotheses regarding the fairness-accuracy trade-offs that fairness-blind ML algorithms exhibit under different data bias settings. On our real-world account-opening fraud use case, we find that each setting entails specific trade-offs, affecting fairness in expected value and variance -- the latter often going unnoticed. Moreover, we show how algorithms compare differently in terms of accuracy and fairness, depending on the biases affecting the data. Finally, we note that under specific data bias conditions, simple pre-processing interventions can successfully balance group-wise error rates, while the same techniques fail in more complex settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Finance - Statistical Finance},
  file = {/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/XXFUGN4B/Pombal et al. - 2022 - Understanding Unfairness in Fraud Detection throug.pdf;/home/jose.pombal/snap/zotero-snap/common/Zotero/storage/JQZGR2RY/2207.html}
}
