\begin{thebibliography}{66}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2022)Ahn, Brohan, Brown, Chebotar, Cortes, David, Finn, Fu,
  Gopalakrishnan, Hausman, et~al.]{ahn2022can}
Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C.,
  Fu, C., Gopalakrishnan, K., Hausman, K., et~al.
\newblock Do as i can, not as i say: Grounding language in robotic affordances.
\newblock \emph{arXiv preprint arXiv:2204.01691}, 2022.

\bibitem[Alayrac et~al.(2022)Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc,
  Mensch, Millican, Reynolds, et~al.]{alayrac2022flamingo}
Alayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc,
  K., Mensch, A., Millican, K., Reynolds, M., et~al.
\newblock Flamingo: a visual language model for few-shot learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 23716--23736, 2022.

\bibitem[Austin et~al.(2021)Austin, Odena, Nye, Bosma, Michalewski, Dohan,
  Jiang, Cai, Terry, Le, et~al.]{austin2021program}
Austin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang,
  E., Cai, C., Terry, M., Le, Q., et~al.
\newblock Program synthesis with large language models.
\newblock \emph{arXiv preprint arXiv:2108.07732}, 2021.

\bibitem[Brohan et~al.(2023)Brohan, Brown, Carbajal, Chebotar, Chen,
  Choromanski, Ding, Driess, Dubey, Finn, et~al.]{brohan2023rt}
Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K.,
  Ding, T., Driess, D., Dubey, A., Finn, C., et~al.
\newblock Rt-2: Vision-language-action models transfer web knowledge to robotic
  control.
\newblock \emph{arXiv preprint arXiv:2307.15818}, 2023.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.~D., Dhariwal, P.,
  Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Cai et~al.(2023)Cai, Liu, Mustikovela, Meyer, Chai, Park, and
  Lee]{cai2023making}
Cai, M., Liu, H., Mustikovela, S.~K., Meyer, G.~P., Chai, Y., Park, D., and
  Lee, Y.~J.
\newblock Making large multimodal models understand arbitrary visual prompts.
\newblock \emph{arXiv preprint arXiv:2312.00784}, 2023.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Xia, Ichter, Rao, Gopalakrishnan,
  Ryoo, Stone, and Kappler]{chen2023open}
Chen, B., Xia, F., Ichter, B., Rao, K., Gopalakrishnan, K., Ryoo, M.~S., Stone,
  A., and Kappler, D.
\newblock Open-vocabulary queryable scene representations for real world
  planning.
\newblock In \emph{2023 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  11509--11522. IEEE, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2024)Chen, Xu, Kirmani, Ichter, Driess, Florence, Sadigh,
  Guibas, and Xia]{chen2024spatialvlm}
Chen, B., Xu, Z., Kirmani, S., Ichter, B., Driess, D., Florence, P., Sadigh,
  D., Guibas, L., and Xia, F.
\newblock Spatialvlm: Endowing vision-language models with spatial reasoning
  capabilities.
\newblock \emph{arXiv preprint arXiv:2401.12168}, 2024.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Djolonga, Padlewski, Mustafa,
  Changpinyo, Wu, Ruiz, Goodman, Wang, Tay, et~al.]{chen2023pali}
Chen, X., Djolonga, J., Padlewski, P., Mustafa, B., Changpinyo, S., Wu, J.,
  Ruiz, C.~R., Goodman, S., Wang, X., Tay, Y., et~al.
\newblock Pali-x: On scaling up a multilingual vision and language model.
\newblock \emph{arXiv preprint arXiv:2305.18565}, 2023{\natexlab{b}}.

\bibitem[Cui et~al.(2022)Cui, Niekum, Gupta, Kumar, and Rajeswaran]{cui2022can}
Cui, Y., Niekum, S., Gupta, A., Kumar, V., and Rajeswaran, A.
\newblock Can foundation models perform zero-shot task specification for robot
  manipulation?
\newblock In \emph{Learning for Dynamics and Control Conference}, pp.\
  893--905. PMLR, 2022.

\bibitem[De~Boer et~al.(2005)De~Boer, Kroese, Mannor, and
  Rubinstein]{de2005tutorial}
De~Boer, P.-T., Kroese, D.~P., Mannor, S., and Rubinstein, R.~Y.
\newblock A tutorial on the cross-entropy method.
\newblock \emph{Annals of operations research}, 134:\penalty0 19--67, 2005.

\bibitem[Dorbala et~al.(2022)Dorbala, Sigurdsson, Piramuthu, Thomason, and
  Sukhatme]{dorbala2022clip}
Dorbala, V.~S., Sigurdsson, G., Piramuthu, R., Thomason, J., and Sukhatme,
  G.~S.
\newblock Clip-nav: Using clip for zero-shot vision-and-language navigation.
\newblock \emph{arXiv preprint arXiv:2211.16649}, 2022.

\bibitem[Firoozi et~al.(2023)Firoozi, Tucker, Tian, Majumdar, Sun, Liu, Zhu,
  Song, Kapoor, Hausman, et~al.]{firoozi2023foundation}
Firoozi, R., Tucker, J., Tian, S., Majumdar, A., Sun, J., Liu, W., Zhu, Y.,
  Song, S., Kapoor, A., Hausman, K., et~al.
\newblock Foundation models in robotics: Applications, challenges, and the
  future.
\newblock \emph{arXiv preprint arXiv:2312.07843}, 2023.

\bibitem[Gadre et~al.(2023)Gadre, Wortsman, Ilharco, Schmidt, and
  Song]{gadre2023cows}
Gadre, S.~Y., Wortsman, M., Ilharco, G., Schmidt, L., and Song, S.
\newblock Cows on pasture: Baselines and benchmarks for language-driven
  zero-shot object navigation.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pp.\  23171--23181, 2023.

\bibitem[Gao et~al.(2023)Gao, Sarkar, Xia, Xiao, Wu, Ichter, Majumdar, and
  Sadigh]{gao2023physically}
Gao, J., Sarkar, B., Xia, F., Xiao, T., Wu, J., Ichter, B., Majumdar, A., and
  Sadigh, D.
\newblock Physically grounded vision-language models for robotic manipulation.
\newblock \emph{arXiv preprint arXiv:2309.02561}, 2023.

\bibitem[Gemini et~al.(2023)Gemini, Anil, Borgeaud, Wu, Alayrac, Yu, Soricut,
  Schalkwyk, Dai, Hauth, et~al.]{team2023gemini}
Gemini, T., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut,
  R., Schalkwyk, J., Dai, A.~M., Hauth, A., et~al.
\newblock Gemini: a family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.

\bibitem[Gemini~Team(2023)]{gemini2023gemini}
Gemini~Team, G.
\newblock Gemini: A family of highly capable multimodal models.
\newblock Technical report, Google, 2023.
\newblock URL
  \url{https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf}.

\bibitem[Gu et~al.(2023)Gu, Kirmani, Wohlhart, Lu, Arenas, Rao, Yu, Fu,
  Gopalakrishnan, Xu, et~al.]{gu2023rt}
Gu, J., Kirmani, S., Wohlhart, P., Lu, Y., Arenas, M.~G., Rao, K., Yu, W., Fu,
  C., Gopalakrishnan, K., Xu, Z., et~al.
\newblock Rt-trajectory: Robotic task generalization via hindsight trajectory
  sketches.
\newblock \emph{arXiv preprint arXiv:2311.01977}, 2023.

\bibitem[Hu et~al.(2023)Hu, Xie, Jain, Francis, Patrikar, Keetha, Kim, Xie,
  Zhang, Zhao, et~al.]{hu2023toward}
Hu, Y., Xie, Q., Jain, V., Francis, J., Patrikar, J., Keetha, N., Kim, S., Xie,
  Y., Zhang, T., Zhao, Z., et~al.
\newblock Toward general-purpose robots via foundation models: A survey and
  meta-analysis.
\newblock \emph{arXiv preprint arXiv:2312.08782}, 2023.

\bibitem[Huang et~al.(2023{\natexlab{a}})Huang, Mees, Zeng, and
  Burgard]{huang2023visual}
Huang, C., Mees, O., Zeng, A., and Burgard, W.
\newblock Visual language maps for robot navigation.
\newblock In \emph{2023 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  10608--10615. IEEE, 2023{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{a}})Huang, Abbeel, Pathak, and
  Mordatch]{huang2022language}
Huang, W., Abbeel, P., Pathak, D., and Mordatch, I.
\newblock Language models as zero-shot planners: Extracting actionable
  knowledge for embodied agents.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  9118--9147. PMLR, 2022{\natexlab{a}}.

\bibitem[Huang et~al.(2022{\natexlab{b}})Huang, Xia, Xiao, Chan, Liang,
  Florence, Zeng, Tompson, Mordatch, Chebotar, et~al.]{huang2022inner}
Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A.,
  Tompson, J., Mordatch, I., Chebotar, Y., et~al.
\newblock Inner monologue: Embodied reasoning through planning with language
  models.
\newblock \emph{arXiv preprint arXiv:2207.05608}, 2022{\natexlab{b}}.

\bibitem[Huang et~al.(2023{\natexlab{b}})Huang, Wang, Zhang, Li, Wu, and
  Fei-Fei]{huang2023voxposer}
Huang, W., Wang, C., Zhang, R., Li, Y., Wu, J., and Fei-Fei, L.
\newblock Voxposer: Composable 3d value maps for robotic manipulation with
  language models.
\newblock \emph{arXiv preprint arXiv:2307.05973}, 2023{\natexlab{b}}.

\bibitem[Itseez(2015)]{itseez2015opencv}
Itseez.
\newblock Open source computer vision library.
\newblock \url{https://github.com/itseez/opencv}, 2015.

\bibitem[Jiang et~al.(2022)Jiang, Gupta, Zhang, Wang, Dou, Chen, Fei-Fei,
  Anandkumar, Zhu, and Fan]{jiang2022vima}
Jiang, Y., Gupta, A., Zhang, Z., Wang, G., Dou, Y., Chen, Y., Fei-Fei, L.,
  Anandkumar, A., Zhu, Y., and Fan, L.
\newblock Vima: General robot manipulation with multimodal prompts.
\newblock \emph{arXiv}, 2022.

\bibitem[Koh et~al.(2024)Koh, Lo, Jang, Duvvur, Lim, Huang, Neubig, Zhou,
  Salakhutdinov, and Fried]{koh2024visualwebarena}
Koh, J.~Y., Lo, R., Jang, L., Duvvur, V., Lim, M.~C., Huang, P.-Y., Neubig, G.,
  Zhou, S., Salakhutdinov, R., and Fried, D.
\newblock Visualwebarena: Evaluating multimodal agents on realistic visual web
  tasks.
\newblock \emph{arXiv preprint arXiv:2401.13649}, 2024.

\bibitem[Kojima et~al.(2022)Kojima, Gu, Reid, Matsuo, and
  Iwasawa]{kojima2022large}
Kojima, T., Gu, S.~S., Reid, M., Matsuo, Y., and Iwasawa, Y.
\newblock Large language models are zero-shot reasoners.
\newblock \emph{Advances in neural information processing systems},
  35:\penalty0 22199--22213, 2022.

\bibitem[Lester et~al.(2021)Lester, Al-Rfou, and Constant]{lester2021power}
Lester, B., Al-Rfou, R., and Constant, N.
\newblock The power of scale for parameter-efficient prompt tuning.
\newblock \emph{arXiv preprint arXiv:2104.08691}, 2021.

\bibitem[Li \& Liang(2021)Li and Liang]{li2021prefix}
Li, X.~L. and Liang, P.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Liang et~al.(2023)Liang, Huang, Xia, Xu, Hausman, Ichter, Florence,
  and Zeng]{liang2023code}
Liang, J., Huang, W., Xia, F., Xu, P., Hausman, K., Ichter, B., Florence, P.,
  and Zeng, A.
\newblock Code as policies: Language model programs for embodied control.
\newblock In \emph{2023 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  9493--9500. IEEE, 2023.

\bibitem[Lin et~al.(2023)Lin, Agia, Migimatsu, Pavone, and
  Bohg]{lin2023text2motion}
Lin, K., Agia, C., Migimatsu, T., Pavone, M., and Bohg, J.
\newblock Text2motion: From natural language instructions to feasible plans.
\newblock \emph{arXiv preprint arXiv:2303.12153}, 2023.

\bibitem[Liu et~al.(2023{\natexlab{a}})Liu, Jiang, Zhang, Liu, Zhang, Biswas,
  and Stone]{liu2023llm+}
Liu, B., Jiang, Y., Zhang, X., Liu, Q., Zhang, S., Biswas, J., and Stone, P.
\newblock Llm+ p: Empowering large language models with optimal planning
  proficiency.
\newblock \emph{arXiv preprint arXiv:2304.11477}, 2023{\natexlab{a}}.

\bibitem[Liu et~al.(2023{\natexlab{b}})Liu, Dong, Zhang, Luo, Gao, Huang, Gong,
  and Wang]{liu20233daxiesprompts}
Liu, D., Dong, X., Zhang, R., Luo, X., Gao, P., Huang, X., Gong, Y., and Wang,
  Z.
\newblock 3daxiesprompts: Unleashing the 3d spatial task capabilities of
  gpt-4v.
\newblock \emph{arXiv preprint arXiv:2312.09738}, 2023{\natexlab{b}}.

\bibitem[Liu et~al.(2023{\natexlab{c}})Liu, Bahety, and Song]{liu2023reflect}
Liu, Z., Bahety, A., and Song, S.
\newblock Reflect: Summarizing robot experiences for failure explanation and
  correction.
\newblock \emph{arXiv preprint arXiv:2306.15724}, 2023{\natexlab{c}}.

\bibitem[Ma et~al.(2023)Ma, Liang, Wang, Huang, Bastani, Jayaraman, Zhu, Fan,
  and Anandkumar]{ma2023eureka}
Ma, Y.~J., Liang, W., Wang, G., Huang, D.-A., Bastani, O., Jayaraman, D., Zhu,
  Y., Fan, L., and Anandkumar, A.
\newblock Eureka: Human-level reward design via coding large language models.
\newblock \emph{arXiv preprint arXiv:2310.12931}, 2023.

\bibitem[Mirchandani et~al.(2023)Mirchandani, Xia, Florence, Ichter, Driess,
  Arenas, Rao, Sadigh, and Zeng]{mirchandani2023large}
Mirchandani, S., Xia, F., Florence, P., Ichter, B., Driess, D., Arenas, M.~G.,
  Rao, K., Sadigh, D., and Zeng, A.
\newblock Large language models as general pattern machines.
\newblock \emph{arXiv preprint arXiv:2307.04721}, 2023.

\bibitem[OpenAI(2023)]{openai2023gpt4v}
OpenAI.
\newblock Gpt-4v(ision) system card.
\newblock Technical report, OpenAI, 2023.
\newblock URL \url{https://openai.com/research/gpt-4v-system-card}.

\bibitem[Padalkar et~al.(2023)Padalkar, Pooley, Jain, Bewley, Herzog, Irpan,
  Khazatsky, Rai, Singh, Brohan, et~al.]{padalkar2023open}
Padalkar, A., Pooley, A., Jain, A., Bewley, A., Herzog, A., Irpan, A.,
  Khazatsky, A., Rai, A., Singh, A., Brohan, A., et~al.
\newblock Open x-embodiment: Robotic learning datasets and rt-x models.
\newblock \emph{arXiv preprint arXiv:2310.08864}, 2023.

\bibitem[Pryzant et~al.(2023)Pryzant, Iter, Li, Lee, Zhu, and
  Zeng]{pryzant2023automatic}
Pryzant, R., Iter, D., Li, J., Lee, Y.~T., Zhu, C., and Zeng, M.
\newblock Automatic prompt optimization with" gradient descent" and beam
  search.
\newblock \emph{arXiv preprint arXiv:2305.03495}, 2023.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Radford, A., Kim, J.~W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
  G., Askell, A., Mishkin, P., Clark, J., et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International conference on machine learning}, pp.\
  8748--8763. PMLR, 2021.

\bibitem[Raman et~al.(2022)Raman, Cohen, Rosen, Idrees, Paulius, and
  Tellex]{raman2022planning}
Raman, S.~S., Cohen, V., Rosen, E., Idrees, I., Paulius, D., and Tellex, S.
\newblock Planning with large language models via corrective re-prompting.
\newblock In \emph{NeurIPS 2022 Foundation Models for Decision Making
  Workshop}, 2022.

\bibitem[Reed et~al.(2022)Reed, Zolna, Parisotto, Colmenarejo, Novikov,
  Barth-Maron, Gimenez, Sulsky, Kay, Springenberg, et~al.]{reed2022generalist}
Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S.~G., Novikov, A.,
  Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J.~T.,
  et~al.
\newblock A generalist agent.
\newblock \emph{arXiv preprint arXiv:2205.06175}, 2022.

\bibitem[Shah et~al.(2023{\natexlab{a}})Shah, Equi, Osi{\'n}ski, Xia, Ichter,
  and Levine]{shah2023navigation}
Shah, D., Equi, M.~R., Osi{\'n}ski, B., Xia, F., Ichter, B., and Levine, S.
\newblock Navigation with large language models: Semantic guesswork as a
  heuristic for planning.
\newblock In \emph{Conference on Robot Learning}, pp.\  2683--2699. PMLR,
  2023{\natexlab{a}}.

\bibitem[Shah et~al.(2023{\natexlab{b}})Shah, Osi{\'n}ski, Levine,
  et~al.]{shah2023lm}
Shah, D., Osi{\'n}ski, B., Levine, S., et~al.
\newblock Lm-nav: Robotic navigation with large pre-trained models of language,
  vision, and action.
\newblock In \emph{Conference on Robot Learning}, pp.\  492--504. PMLR,
  2023{\natexlab{b}}.

\bibitem[Shridhar et~al.(2022)Shridhar, Manuelli, and Fox]{shridhar2022cliport}
Shridhar, M., Manuelli, L., and Fox, D.
\newblock Cliport: What and where pathways for robotic manipulation.
\newblock In \emph{Conference on Robot Learning}, pp.\  894--906. PMLR, 2022.

\bibitem[Shtedritski et~al.(2023)Shtedritski, Rupprecht, and
  Vedaldi]{shtedritski2023does}
Shtedritski, A., Rupprecht, C., and Vedaldi, A.
\newblock What does clip know about a red circle? visual prompt engineering for
  vlms.
\newblock \emph{arXiv preprint arXiv:2304.06712}, 2023.

\bibitem[Silver et~al.(2023)Silver, Dan, Srinivas, Tenenbaum, Kaelbling, and
  Katz]{silver2023generalized}
Silver, T., Dan, S., Srinivas, K., Tenenbaum, J.~B., Kaelbling, L.~P., and
  Katz, M.
\newblock Generalized planning in pddl domains with pretrained large language
  models.
\newblock \emph{arXiv preprint arXiv:2305.11014}, 2023.

\bibitem[Singh et~al.(2023)Singh, Blukis, Mousavian, Goyal, Xu, Tremblay, Fox,
  Thomason, and Garg]{singh2023progprompt}
Singh, I., Blukis, V., Mousavian, A., Goyal, A., Xu, D., Tremblay, J., Fox, D.,
  Thomason, J., and Garg, A.
\newblock Progprompt: Generating situated robot task plans using large language
  models.
\newblock In \emph{2023 IEEE International Conference on Robotics and
  Automation (ICRA)}, pp.\  11523--11530. IEEE, 2023.

\bibitem[Wang et~al.(2023{\natexlab{a}})Wang, Xie, Jiang, Mandlekar, Xiao, Zhu,
  Fan, and Anandkumar]{wang2023voyager}
Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and
  Anandkumar, A.
\newblock Voyager: An open-ended embodied agent with large language models.
\newblock \emph{arXiv preprint arXiv:2305.16291}, 2023{\natexlab{a}}.

\bibitem[Wang et~al.(2023{\natexlab{b}})Wang, Zhang, Chen, and
  Sreenath]{wang2023prompt}
Wang, Y.-J., Zhang, B., Chen, J., and Sreenath, K.
\newblock Prompt a robot to walk with large language models.
\newblock \emph{arXiv preprint arXiv:2309.09969}, 2023{\natexlab{b}}.

\bibitem[Wang et~al.(2023{\natexlab{c}})Wang, Cai, Liu, Ma, and
  Liang]{wang2023describe}
Wang, Z., Cai, S., Liu, A., Ma, X., and Liang, Y.
\newblock Describe, explain, plan and select: Interactive planning with large
  language models enables open-world multi-task agents.
\newblock \emph{arXiv preprint arXiv:2302.01560}, 2023{\natexlab{c}}.

\bibitem[Wei et~al.(2022)Wei, Wang, Schuurmans, Bosma, Xia, Chi, Le, Zhou,
  et~al.]{wei2022chain}
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.~V.,
  Zhou, D., et~al.
\newblock Chain-of-thought prompting elicits reasoning in large language
  models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 24824--24837, 2022.

\bibitem[Wen et~al.(2023)Wen, Yang, Fu, Wang, Cai, Li, Ma, Li, Xu, Shang,
  et~al.]{wen2023road}
Wen, L., Yang, X., Fu, D., Wang, X., Cai, P., Li, X., Ma, T., Li, Y., Xu, L.,
  Shang, D., et~al.
\newblock On the road with gpt-4v (ision): Early explorations of
  visual-language model on autonomous driving.
\newblock \emph{arXiv preprint arXiv:2311.05332}, 2023.

\bibitem[Wu et~al.(2023)Wu, Antonova, Kan, Lepert, Zeng, Song, Bohg,
  Rusinkiewicz, and Funkhouser]{wu2023tidybot}
Wu, J., Antonova, R., Kan, A., Lepert, M., Zeng, A., Song, S., Bohg, J.,
  Rusinkiewicz, S., and Funkhouser, T.
\newblock Tidybot: Personalized robot assistance with large language models.
\newblock \emph{arXiv preprint arXiv:2305.05658}, 2023.

\bibitem[Xu et~al.(2022)Xu, Chen, Du, Shao, Wang, Li, and Yang]{xu2022gps}
Xu, H., Chen, Y., Du, Y., Shao, N., Wang, Y., Li, H., and Yang, Z.
\newblock Gps: Genetic prompt search for efficient few-shot learning.
\newblock \emph{arXiv preprint arXiv:2210.17041}, 2022.

\bibitem[Xu et~al.(2023)Xu, Zhou, Yan, Gu, Arnab, Sun, Wang, and
  Schmid]{xu2023pixel}
Xu, J., Zhou, X., Yan, S., Gu, X., Arnab, A., Sun, C., Wang, X., and Schmid, C.
\newblock Pixel aligned language models.
\newblock \emph{arXiv preprint arXiv:2312.09237}, 2023.

\bibitem[Yan et~al.(2023)Yan, Yang, Zhu, Lin, Li, Wang, Yang, Zhong, McAuley,
  Gao, et~al.]{yan2023gpt}
Yan, A., Yang, Z., Zhu, W., Lin, K., Li, L., Wang, J., Yang, J., Zhong, Y.,
  McAuley, J., Gao, J., et~al.
\newblock Gpt-4v in wonderland: Large multimodal models for zero-shot
  smartphone gui navigation.
\newblock \emph{arXiv preprint arXiv:2311.07562}, 2023.

\bibitem[Yang et~al.(2023{\natexlab{a}})Yang, Wang, Lu, Liu, Le, Zhou, and
  Chen]{yang2023large}
Yang, C., Wang, X., Lu, Y., Liu, H., Le, Q.~V., Zhou, D., and Chen, X.
\newblock Large language models as optimizers.
\newblock \emph{arXiv preprint arXiv:2309.03409}, 2023{\natexlab{a}}.

\bibitem[Yang et~al.(2023{\natexlab{b}})Yang, Zhang, Li, Zou, Li, and
  Gao]{yang2023set}
Yang, J., Zhang, H., Li, F., Zou, X., Li, C., and Gao, J.
\newblock Set-of-mark prompting unleashes extraordinary visual grounding in
  gpt-4v.
\newblock \emph{arXiv preprint arXiv:2310.11441}, 2023{\natexlab{b}}.

\bibitem[Yang et~al.(2023{\natexlab{c}})Yang, Li, Lin, Wang, Lin, Liu, and
  Wang]{yang2023dawn}
Yang, Z., Li, L., Lin, K., Wang, J., Lin, C.-C., Liu, Z., and Wang, L.
\newblock The dawn of lmms: Preliminary explorations with gpt-4v (ision).
\newblock \emph{arXiv preprint arXiv:2309.17421}, 9\penalty0 (1):\penalty0 1,
  2023{\natexlab{c}}.

\bibitem[Yu et~al.(2016)Yu, Poirson, Yang, Berg, and Berg]{yu2016modeling}
Yu, L., Poirson, P., Yang, S., Berg, A.~C., and Berg, T.~L.
\newblock Modeling context in referring expressions.
\newblock In \emph{Computer Vision--ECCV 2016: 14th European Conference,
  Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pp.\  69--85. Springer, 2016.

\bibitem[Yu et~al.(2023)Yu, Gileadi, Fu, Kirmani, Lee, Arenas, Chiang, Erez,
  Hasenclever, Humplik, et~al.]{yu2023language}
Yu, W., Gileadi, N., Fu, C., Kirmani, S., Lee, K.-H., Arenas, M.~G., Chiang,
  H.-T.~L., Erez, T., Hasenclever, L., Humplik, J., et~al.
\newblock Language to rewards for robotic skill synthesis.
\newblock \emph{arXiv preprint arXiv:2306.08647}, 2023.

\bibitem[Zeng et~al.(2021)Zeng, Florence, Tompson, Welker, Chien, Attarian,
  Armstrong, Krasin, Duong, Sindhwani, et~al.]{zeng2021transporter}
Zeng, A., Florence, P., Tompson, J., Welker, S., Chien, J., Attarian, M.,
  Armstrong, T., Krasin, I., Duong, D., Sindhwani, V., et~al.
\newblock Transporter networks: Rearranging the visual world for robotic
  manipulation.
\newblock In \emph{Conference on Robot Learning}, pp.\  726--747. PMLR, 2021.

\bibitem[Zeng et~al.(2022)Zeng, Attarian, Ichter, Choromanski, Wong, Welker,
  Tombari, Purohit, Ryoo, Sindhwani, et~al.]{zeng2022socratic}
Zeng, A., Attarian, M., Ichter, B., Choromanski, K., Wong, A., Welker, S.,
  Tombari, F., Purohit, A., Ryoo, M., Sindhwani, V., et~al.
\newblock Socratic models: Composing zero-shot multimodal reasoning with
  language.
\newblock \emph{arXiv preprint arXiv:2204.00598}, 2022.

\bibitem[Zheng et~al.(2024)Zheng, Gou, Kil, Sun, and Su]{zheng2024gpt}
Zheng, B., Gou, B., Kil, J., Sun, H., and Su, Y.
\newblock Gpt-4v (ision) is a generalist web agent, if grounded.
\newblock \emph{arXiv preprint arXiv:2401.01614}, 2024.

\bibitem[Zhou et~al.(2022)Zhou, Muresanu, Han, Paster, Pitis, Chan, and
  Ba]{zhou2022large}
Zhou, Y., Muresanu, A.~I., Han, Z., Paster, K., Pitis, S., Chan, H., and Ba, J.
\newblock Large language models are human-level prompt engineers.
\newblock \emph{arXiv preprint arXiv:2211.01910}, 2022.

\end{thebibliography}
