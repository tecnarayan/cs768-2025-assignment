\begin{thebibliography}{10}

\bibitem{abel2019state}
David Abel, Dilip Arumugam, Kavosh Asadi, Yuu Jinnai, Michael~L Littman, and
  Lawson~LS Wong.
\newblock State abstraction as compression in apprenticeship learning.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~33, pages 3134--3142, 2019.

\bibitem{abel2016near}
David Abel, David Hershkowitz, and Michael Littman.
\newblock Near optimal behavior via approximate state abstraction.
\newblock In {\em International Conference on Machine Learning}, pages
  2915--2923. PMLR, 2016.

\bibitem{abel2020value}
David Abel, Nate Umbanhowar, Khimya Khetarpal, Dilip Arumugam, Doina Precup,
  and Michael Littman.
\newblock Value preserving state-action abstractions.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1639--1650. PMLR, 2020.

\bibitem{agarwal2020contrastive}
Rishabh Agarwal, Marlos~C Machado, Pablo~Samuel Castro, and Marc~G Bellemare.
\newblock Contrastive behavioral similarity embeddings for generalization in
  reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{agarwal2021deep}
Rishabh Agarwal, Max Schwarzer, Pablo~Samuel Castro, Aaron~C Courville, and
  Marc Bellemare.
\newblock Deep reinforcement learning at the edge of the statistical precipice.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{anand2019unsupervised}
Ankesh Anand, Evan Racah, Sherjil Ozair, Yoshua Bengio, Marc-Alexandre
  C{\^o}t{\'e}, and R~Devon Hjelm.
\newblock Unsupervised state representation learning in {A}tari.
\newblock {\em Advances in Neural Information Processing Systems},
  32:8769--8782, 2019.

\bibitem{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem{barth2018distributed}
Gabriel Barth-Maron, Matthew~W Hoffman, David Budden, Will Dabney, Dan Horgan,
  Dhruva Tb, Alistair Muldal, Nicolas Heess, and Timothy Lillicrap.
\newblock Distributed distributional deterministic policy gradients.
\newblock {\em arXiv preprint arXiv:1804.08617}, 2018.

\bibitem{bellman1957dynamic}
Richard Bellman.
\newblock {\em Dynamic Programming}.
\newblock Princeton University Press, 1957.

\bibitem{bertsekas2012dynamic}
Dimitri Bertsekas.
\newblock {\em Dynamic programming and optimal control: Volume I}, volume~1.
\newblock Athena scientific, 2012.

\bibitem{biza2021learning}
O~Biza, R~Platt, JW~van~de Meent, and L~Wong.
\newblock Learning discrete state abstractions with deep variational inference.
\newblock {\em Advances in Approximate Bayesian Inference}, 2021.

\bibitem{biza2019online}
Ondrej Biza and Robert Platt.
\newblock Online abstraction with mdp homomorphisms for deep learning.
\newblock In {\em Proceedings of the 18th International Conference on
  Autonomous Agents and MultiAgent Systems}, 2019.

\bibitem{blute1997bisimulation}
Richard Blute, Jos{\'e}e Desharnais, Abbas Edalat, and Prakash Panangaden.
\newblock Bisimulation for labelled markov processes.
\newblock In {\em Proceedings of Twelfth Annual IEEE Symposium on Logic in
  Computer Science}, pages 149--158. IEEE, 1997.

\bibitem{bogachev2007measure}
Vladimir~Igorevich Bogachev and Maria Aparecida~Soares Ruas.
\newblock {\em Measure theory}, volume~1.
\newblock Springer, 2007.

\bibitem{caselles2019symmetry}
Hugo Caselles-Dupr{\'e}, Michael Garcia~Ortiz, and David Filliat.
\newblock Symmetry-based disentangled representation learning requires
  interaction with environments.
\newblock {\em Advances in Neural Information Processing Systems},
  32:4606--4615, 2019.

\bibitem{castro2020scalable}
Pablo~Samuel Castro.
\newblock Scalable methods for computing state similarity in deterministic
  markov decision processes.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~34, pages 10069--10076, 2020.

\bibitem{castro2021mico}
Pablo~Samuel Castro, Tyler Kastner, Prakash Panangaden, and Mark Rowland.
\newblock Mico: Improved representations via sampling-based state similarity
  for {M}arkov decision processes.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{castro2010using}
Pablo~Samuel Castro and Doina Precup.
\newblock Using bisimulation for policy transfer in {MDP}s.
\newblock In {\em Twenty-Fourth AAAI Conference on Artificial Intelligence},
  2010.

\bibitem{castro2011automatic}
Pablo~Samuel Castro and Doina Precup.
\newblock Automatic construction of temporally extended actions for {MDP}s
  using bisimulation metrics.
\newblock In {\em European Workshop on Reinforcement Learning}, pages 140--152.
  Springer, 2011.

\bibitem{chandak2019learning}
Yash Chandak, Georgios Theocharous, James Kostas, Scott Jordan, and Philip
  Thomas.
\newblock Learning action representations for reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  941--950. PMLR, 2019.

\bibitem{cranmer2020lagrangian}
Miles Cranmer, Sam Greydanus, Stephan Hoyer, Peter Battaglia, David Spergel,
  and Shirley Ho.
\newblock Lagrangian neural networks.
\newblock {\em arXiv preprint arXiv:2003.04630}, 2020.

\bibitem{Desharnais02}
J.~Desharnais, A.~Edalat, and P.~Panangaden.
\newblock Bisimulation for labeled {Markov} processes.
\newblock {\em Information and Computation}, 179(2):163--193, Dec 2002.

\bibitem{Desharnais99b}
J.~Desharnais, V.~Gupta, R.~Jagadeesan, and P.~Panangaden.
\newblock Metrics for labeled {Markov} systems.
\newblock In {\em Proceedings of CONCUR99}, number 1664 in Lecture Notes in
  Computer Science. Springer-Verlag, 1999.

\bibitem{dulac2015deep}
Gabriel Dulac-Arnold, Richard Evans, Hado van Hasselt, Peter Sunehag, Timothy
  Lillicrap, Jonathan Hunt, Timothy Mann, Theophane Weber, Thomas Degris, and
  Ben Coppin.
\newblock Deep reinforcement learning in large discrete action spaces.
\newblock {\em arXiv preprint arXiv:1512.07679}, 2015.

\bibitem{eslami2018neural}
SM~Ali Eslami, Danilo~Jimenez Rezende, Frederic Besse, Fabio Viola, Ari~S
  Morcos, Marta Garnelo, Avraham Ruderman, Andrei~A Rusu, Ivo Danihelka, Karol
  Gregor, et~al.
\newblock Neural scene representation and rendering.
\newblock {\em Science}, 360(6394):1204--1210, 2018.

\bibitem{fan2021secant}
Linxi Fan, Guanzhi Wang, De-An Huang, Zhiding Yu, Li~Fei-Fei, Yuke Zhu, and
  Animashree Anandkumar.
\newblock Secant: Self-expert cloning for zero-shot generalization of visual
  policies.
\newblock In {\em International Conference on Machine Learning}, pages
  3088--3099. PMLR, 2021.

\bibitem{ferns2006methods}
Norm Ferns, Pablo~Samuel Castro, Doina Precup, and Prakash Panangaden.
\newblock Methods for computing state similarity in {M}arkov decision
  processes.
\newblock In {\em Proceedings of the Twenty-Second Conference on Uncertainty in
  Artificial Intelligence}, pages 174--181, 2006.

\bibitem{ferns2005metrics}
Norm Ferns, Prakash Panangaden, and Doina Precup.
\newblock Metrics for {M}arkov decision processes with infinite state spaces.
\newblock In {\em Proceedings of the Twenty-First Conference on Uncertainty in
  Artificial Intelligence}, pages 201--208, 2005.

\bibitem{ferns2011bisimulation}
Norm Ferns, Prakash Panangaden, and Doina Precup.
\newblock Bisimulation metrics for continuous {M}arkov decision processes.
\newblock {\em SIAM Journal on Computing}, 40(6):1662--1714, 2011.

\bibitem{fujimoto2018addressing}
Scott Fujimoto, Herke Hoof, and David Meger.
\newblock Addressing function approximation error in actor-critic methods.
\newblock In {\em International Conference on Machine Learning}, pages
  1587--1596. PMLR, 2018.

\bibitem{gelada2019deepmdp}
Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc~G Bellemare.
\newblock Deepmdp: Learning continuous latent space models for representation
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  2170--2179. PMLR, 2019.

\bibitem{givan2003equivalence}
Robert Givan, Thomas Dean, and Matthew Greig.
\newblock Equivalence notions and model minimization in {M}arkov decision
  processes.
\newblock {\em Artificial Intelligence}, 147(1-2):163--223, 2003.

\bibitem{greydanus2019hamiltonian}
Samuel Greydanus, Misko Dzamba, and Jason Yosinski.
\newblock Hamiltonian neural networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{grimm2021proper}
Christopher Grimm, Andr{\'e} Barreto, Gregory Farquhar, David Silver, and
  Satinder Singh.
\newblock Proper value equivalence.
\newblock {\em arXiv preprint arXiv:2106.10316}, 2021.

\bibitem{grimm2020value}
Christopher Grimm, Andr{\'e} Barreto, Satinder Singh, and David Silver.
\newblock The value equivalence principle for model-based reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:2011.03506}, 2020.

\bibitem{ha2018world}
David Ha and J{\"u}rgen Schmidhuber.
\newblock World models.
\newblock {\em arXiv e-prints}, pages arXiv--1803, 2018.

\bibitem{haarnoja2018soft}
Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.
\newblock Soft actor-critic: Off-policy maximum entropy deep reinforcement
  learning with a stochastic actor.
\newblock In {\em International conference on machine learning}, pages
  1861--1870. PMLR, 2018.

\bibitem{hafner2019dream}
Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi.
\newblock Dream to control: Learning behaviors by latent imagination.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{hafner2019learning}
Danijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas, David Ha,
  Honglak Lee, and James Davidson.
\newblock Learning latent dynamics for planning from pixels.
\newblock In {\em International Conference on Machine Learning}, pages
  2555--2565. PMLR, 2019.

\bibitem{hansen2020self}
Nicklas Hansen, Rishabh Jangir, Yu~Sun, Guillem Aleny{\`a}, Pieter Abbeel,
  Alexei~A Efros, Lerrel Pinto, and Xiaolong Wang.
\newblock Self-supervised policy adaptation during deployment.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{hansen2021generalization}
Nicklas Hansen and Xiaolong Wang.
\newblock Generalization in reinforcement learning by soft data augmentation.
\newblock In {\em 2021 IEEE International Conference on Robotics and Automation
  (ICRA)}, pages 13611--13617. IEEE, 2021.

\bibitem{higgins2018towards}
Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey,
  Danilo Rezende, and Alexander Lerchner.
\newblock Towards a definition of disentangled representations.
\newblock {\em arXiv preprint arXiv:1812.02230}, 2018.

\bibitem{higgins2021symetric}
Irina Higgins, Peter Wirnsberger, Andrew Jaegle, and Aleksandar Botev.
\newblock Symetric: Measuring the quality of learnt hamiltonian dynamics
  inferred from vision.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{hjelm2018learning}
R~Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Phil
  Bachman, Adam Trischler, and Yoshua Bengio.
\newblock Learning deep representations by mutual information estimation and
  maximization.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{horgan2018distributed}
Dan Horgan, John Quan, David Budden, Gabriel Barth-Maron, Matteo Hessel, Hado
  van Hasselt, and David Silver.
\newblock Distributed prioritized experience replay.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{jaderberg2016reinforcement}
Max Jaderberg, Volodymyr Mnih, Wojciech~Marian Czarnecki, Tom Schaul, Joel~Z
  Leibo, David Silver, and Koray Kavukcuoglu.
\newblock Reinforcement learning with unsupervised auxiliary tasks.
\newblock {\em arXiv preprint arXiv:1611.05397}, 2016.

\bibitem{jonschkowski2015learning}
Rico Jonschkowski and Oliver Brock.
\newblock Learning state representations with robotic priors.
\newblock {\em Autonomous Robots}, 39(3):407--428, 2015.

\bibitem{kemertas2021towards}
Mete Kemertas and Tristan Aumentado-Armstrong.
\newblock Towards robust bisimulation metric learning.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{lang2012differential}
Serge Lang.
\newblock {\em Differential and {R}iemannian manifolds}, volume 160.
\newblock Springer Science \& Business Media, 2012.

\bibitem{larsen1991bisimulation}
Kim~G Larsen and Arne Skou.
\newblock Bisimulation through probabilistic testing.
\newblock {\em Information and computation}, 94(1):1--28, 1991.

\bibitem{lee2019network}
Kimin Lee, Kibok Lee, Jinwoo Shin, and Honglak Lee.
\newblock Network randomization: A simple technique for generalization in deep
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:1910.05396}, 2019.

\bibitem{li2006towards}
Lihong Li, Thomas~J Walsh, and Michael~L Littman.
\newblock Towards a unified theory of state abstraction for {MDP}s.
\newblock {\em ISAIM}, 4:5, 2006.

\bibitem{QingLi2021continuousbenchmark}
Qing Li.
\newblock Continuous control benchmark of {DeepMind} control suite and
  {MuJoCo}.
\newblock \url{https://github.com/LQNew/Continuous_Control_Benchmark}, 2021.

\bibitem{lillicrap2015continuous}
Timothy~P Lillicrap, Jonathan~J Hunt, Alexander Pritzel, Nicolas Heess, Tom
  Erez, Yuval Tassa, David Silver, and Daan Wierstra.
\newblock Continuous control with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1509.02971}, 2015.

\bibitem{liu2021aps}
Hao Liu and Pieter Abbeel.
\newblock Aps: Active pretraining with successor features.
\newblock In {\em International Conference on Machine Learning}, pages
  6736--6747. PMLR, 2021.

\bibitem{liu2019self}
Shikun Liu, Andrew Davison, and Edward Johns.
\newblock Self-supervised generalisation with meta auxiliary learning.
\newblock {\em Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem{lyle2021effect}
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dabney.
\newblock On the effect of auxiliary tasks on representation dynamics.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1--9. PMLR, 2021.

\bibitem{mahajan2017symmetry}
Anuj Mahajan and Theja Tulabandhula.
\newblock Symmetry learning for function approximation in reinforcement
  learning.
\newblock {\em arXiv preprint arXiv:1706.02999}, 2017.

\bibitem{milner1989communication}
Robin Milner.
\newblock {\em Communication and concurrency}, volume~84.
\newblock Prentice hall Englewood Cliffs, 1989.

\bibitem{mnih2013playing}
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
  Antonoglou, Daan Wierstra, and Martin Riedmiller.
\newblock Playing {A}tari with deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1312.5602}, 2013.

\bibitem{mondal2022eqr}
Arnab~Kumar Mondal, Vineet Jain, Kaleem Siddiqi, and Siamak Ravanbakhsh.
\newblock Eqr: Equivariant representations for data-efficient reinforcement
  learning.
\newblock In {\em International Conference on Machine Learning}, pages
  15908--15926. PMLR, 2022.

\bibitem{narayanamurthy2008hardness}
Shravan~Matthur Narayanamurthy and Balaraman Ravindran.
\newblock On the hardness of finding symmetries in markov decision processes.
\newblock In {\em Proceedings of the 25th international conference on Machine
  learning}, pages 688--695, 2008.

\bibitem{pardo2020tonic}
Fabio Pardo.
\newblock Tonic: A deep reinforcement learning library for fast prototyping and
  benchmarking.
\newblock {\em arXiv preprint arXiv:2011.07537}, 2020.

\bibitem{park2022learning}
Jung~Yeon Park, Ondrej Biza, Linfeng Zhao, Jan~Willem van~de Meent, and Robin
  Walters.
\newblock Learning symmetric embeddings for equivariant world models.
\newblock {\em arXiv preprint arXiv:2204.11371}, 2022.

\bibitem{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems},
  32:8026--8037, 2019.

\bibitem{quessard2020learning}
Robin Quessard, Thomas~D Barrett, and William~R Clements.
\newblock Learning group structure and disentangled representations of
  dynamical environments.
\newblock {\em arXiv preprint arXiv:2002.06991}, 2020.

\bibitem{rajendran2009learning}
Srividhya Rajendran and Manfred Huber.
\newblock Learning to generalize and reuse skills using approximate partial
  policy homomorphisms.
\newblock In {\em 2009 IEEE International Conference on Systems, Man and
  Cybernetics}, pages 2239--2244. IEEE, 2009.

\bibitem{ravindran2004algebraic}
Balaraman Ravindran.
\newblock {\em An algebraic approach to abstraction in reinforcement learning}.
\newblock University of Massachusetts Amherst, 2004.

\bibitem{ravindran2001symmetries}
Balaraman Ravindran and Andrew~G Barto.
\newblock Symmetries and model minimization in markov decision processes, 2001.

\bibitem{ravindran2003relativized}
Balaraman Ravindran and Andrew~G Barto.
\newblock Relativized options: Choosing the right transformation.
\newblock In {\em Proceedings of the 20th International Conference on Machine
  Learning (ICML-03)}, pages 608--615, 2003.

\bibitem{ravindran2004approximate}
Balaraman Ravindran and Andrew~G Barto.
\newblock Approximate homomorphisms: A framework for non-exact minimization in
  {M}arkov {D}ecision {P}rocesses, 2004.

\bibitem{sallans2004reinforcement}
Brian Sallans and Geoffrey~E Hinton.
\newblock Reinforcement learning with factored states and actions.
\newblock {\em The Journal of Machine Learning Research}, 5:1063--1088, 2004.

\bibitem{sharma2017learning}
Sahil Sharma, Aravind Suresh, Rahul Ramesh, and Balaraman Ravindran.
\newblock Learning to factor policies and action-value functions: Factored
  action space representations for deep reinforcement learning.
\newblock {\em arXiv preprint arXiv:1705.07269}, 2017.

\bibitem{silver2014deterministic}
David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and
  Martin Riedmiller.
\newblock Deterministic policy gradient algorithms.
\newblock In {\em International conference on machine learning}, pages
  387--395. PMLR, 2014.

\bibitem{sinha2021s4rl}
Samarth Sinha, Ajay Mandlekar, and Animesh Garg.
\newblock S4rl: Surprisingly simple self-supervision for offline reinforcement
  learning in robotics.
\newblock In {\em 5th Annual Conference on Robot Learning}, 2021.

\bibitem{soni2006using}
Vishal Soni and Satinder Singh.
\newblock Using homomorphisms to transfer options across continuous
  reinforcement learning domains.
\newblock In {\em AAAI}, volume~6, pages 494--499, 2006.

\bibitem{sorg2009transfer}
Jonathan Sorg and Satinder Singh.
\newblock Transfer via soft homomorphisms.
\newblock In {\em Proceedings of The 8th International Conference on Autonomous
  Agents and Multiagent Systems-Volume 2}, pages 741--748, 2009.

\bibitem{spivak2018calculus}
Michael Spivak.
\newblock {\em Calculus on manifolds: a modern approach to classical theorems
  of advanced calculus}.
\newblock CRC press, 2018.

\bibitem{stooke2021decoupling}
Adam Stooke, Kimin Lee, Pieter Abbeel, and Michael Laskin.
\newblock Decoupling representation learning from reinforcement learning.
\newblock In {\em International Conference on Machine Learning}, pages
  9870--9879. PMLR, 2021.

\bibitem{sutton2018reinforcement}
Richard~S Sutton and Andrew~G Barto.
\newblock {\em Reinforcement learning: An introduction}.
\newblock MIT press, 2018.

\bibitem{sutton2000policy}
Richard~S Sutton, David~A McAllester, Satinder~P Singh, and Yishay Mansour.
\newblock Policy gradient methods for reinforcement learning with function
  approximation.
\newblock In {\em Advances in neural information processing systems}, pages
  1057--1063, 2000.

\bibitem{sutton1999between}
Richard~S Sutton, Doina Precup, and Satinder Singh.
\newblock Between {MDP}s and semi-{MDP}s: A framework for temporal abstraction
  in reinforcement learning.
\newblock {\em Artificial intelligence}, 112(1-2):181--211, 1999.

\bibitem{taylor2008bounding}
Jonathan Taylor, Doina Precup, and Prakash Panagaden.
\newblock Bounding performance loss in approximate mdp homomorphisms.
\newblock {\em Advances in Neural Information Processing Systems},
  21:1649--1656, 2008.

\bibitem{tennenholtz2019natural}
Guy Tennenholtz and Shie Mannor.
\newblock The natural language of actions.
\newblock In {\em International Conference on Machine Learning}, pages
  6196--6205. PMLR, 2019.

\bibitem{thomas2017independently}
Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati, Philippe
  Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, and Yoshua Bengio.
\newblock Independently controllable factors.
\newblock {\em arXiv preprint arXiv:1708.01289}, 2017.

\bibitem{todorov2012mujoco}
Emanuel Todorov, Tom Erez, and Yuval Tassa.
\newblock Mujoco: A physics engine for model-based control.
\newblock In {\em 2012 IEEE/RSJ International Conference on Intelligent Robots
  and Systems}, pages 5026--5033. IEEE, 2012.

\bibitem{van2020plannable}
Elise van~der Pol, Thomas Kipf, Frans~A Oliehoek, and Max Welling.
\newblock Plannable approximations to mdp homomorphisms: Equivariance under
  actions.
\newblock In {\em Proceedings of the 19th International Conference on
  Autonomous Agents and MultiAgent Systems}, pages 1431--1439, 2020.

\bibitem{van2021multi}
Elise van~der Pol, Herke van Hoof, Frans~A Oliehoek, and Max Welling.
\newblock Multi-agent {MDP} homomorphic networks.
\newblock {\em arXiv preprint arXiv:2110.04495}, 2021.

\bibitem{van2020mdp}
Elise van~der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, and Max
  Welling.
\newblock Mdp homomorphic networks: Group symmetries in reinforcement learning.
\newblock {\em Advances in Neural Information Processing Systems}, 33, 2020.

\bibitem{wang2021so2}
Dian Wang, Robin Walters, and Robert Platt.
\newblock So(2)-equivariant reinforcement learning.
\newblock In {\em International Conference on Learning Representations}, 2021.

\bibitem{whitney2019dynamics}
William Whitney, Rajat Agarwal, Kyunghyun Cho, and Abhinav Gupta.
\newblock Dynamics-aware embeddings.
\newblock In {\em International Conference on Learning Representations}, 2019.

\bibitem{williams1992simple}
Ronald~J Williams.
\newblock Simple statistical gradient-following algorithms for connectionist
  reinforcement learning.
\newblock {\em Machine learning}, 8(3):229--256, 1992.

\bibitem{wolfe2006decision}
Alicia~P Wolfe and Andrew~G Barto.
\newblock Decision tree methods for finding reusable mdp homomorphisms.
\newblock In {\em PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL
  INTELLIGENCE}, volume~21, page 530. Menlo Park, CA; Cambridge, MA; London;
  AAAI Press; MIT Press; 1999, 2006.

\bibitem{wolfe2006defining}
Alicia~Peregrin Wolfe and Andrew~G Barto.
\newblock Defining object types and options using mdp homomorphisms.
\newblock In {\em Proceedings of the ICML-06 Workshop on Structural Knowledge
  Transfer for Machine Learning}. Citeseer, 2006.

\bibitem{yarats2021mastering}
Denis Yarats, Rob Fergus, Alessandro Lazaric, and Lerrel Pinto.
\newblock Mastering visual continuous control: Improved data-augmented
  reinforcement learning.
\newblock {\em arXiv preprint arXiv:2107.09645}, 2021.

\bibitem{yarats2020image}
Denis Yarats, Ilya Kostrikov, and Rob Fergus.
\newblock Image augmentation is all you need: Regularizing deep reinforcement
  learning from pixels.
\newblock In {\em International Conference on Learning Representations}, 2020.

\bibitem{yarats2021improving}
Denis Yarats, Amy Zhang, Ilya Kostrikov, Brandon Amos, Joelle Pineau, and Rob
  Fergus.
\newblock Improving sample efficiency in model-free reinforcement learning from
  images.
\newblock In {\em Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~35, pages 10674--10681, 2021.

\bibitem{zhang2020learning}
Amy Zhang, Rowan~Thomas McAllister, Roberto Calandra, Yarin Gal, and Sergey
  Levine.
\newblock Learning invariant representations for reinforcement learning without
  reconstruction.
\newblock In {\em International Conference on Learning Representations}, 2020.

\end{thebibliography}
