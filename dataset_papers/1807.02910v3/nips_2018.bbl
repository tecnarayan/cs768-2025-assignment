\begin{thebibliography}{10}

\bibitem{baehrens2010explain}
David Baehrens, Timon Schroeter, Stefan Harmeling, Motoaki Kawanabe, Katja
  Hansen, and Klaus-Robert M{\~A}{\v{z}}ller.
\newblock How to explain individual classification decisions.
\newblock {\em Journal of Machine Learning Research}, 11(Jun):1803--1831, 2010.

\bibitem{bien2011prototype}
Jacob Bien and Robert Tibshirani.
\newblock Prototype selection for interpretable classification.
\newblock {\em The Annals of Applied Statistics}, pages 2403--2424, 2011.

\bibitem{bloniarz2016supervised}
Adam Bloniarz, Ameet Talwalkar, Bin Yu, and Christopher Wu.
\newblock Supervised neighborhoods for distributed nonparametric regression.
\newblock In {\em Artificial Intelligence and Statistics}, pages 1450--1459,
  2016.

\bibitem{breiman2001random}
Leo Breiman.
\newblock Random forests.
\newblock {\em Machine learning}, 45(1):5--32, 2001.

\bibitem{chatterjee1986influential}
Samprit Chatterjee and Ali~S Hadi.
\newblock Influential observations, high leverage points, and outliers in
  linear regression.
\newblock {\em Statistical Science}, pages 379--393, 1986.

\bibitem{cook1980characterizations}
R~Dennis Cook and Sanford Weisberg.
\newblock Characterizations of an empirical influence function for detecting
  influential cases in regression.
\newblock {\em Technometrics}, 22(4):495--508, 1980.

\bibitem{Dua:2017}
Dua Dheeru and Efi Karra~Taniskidou.
\newblock {UCI} machine learning repository, 2017.

\bibitem{doshi2017towards}
Finale Doshi-Velez and Been Kim.
\newblock Towards a rigorous science of interpretable machine learning.
\newblock 2017.

\bibitem{friedman2001greedy}
Jerome~H Friedman.
\newblock Greedy function approximation: a gradient boosting machine.
\newblock {\em Annals of statistics}, pages 1189--1232, 2001.

\bibitem{kaufman2012leakage}
Shachar Kaufman, Saharon Rosset, Claudia Perlich, and Ori Stitelman.
\newblock Leakage in data mining: Formulation, detection, and avoidance.
\newblock {\em ACM Transactions on Knowledge Discovery from Data (TKDD)},
  6(4):15, 2012.

\bibitem{kazemitabar2017variable}
Jalil Kazemitabar, Arash Amini, Adam Bloniarz, and Ameet~S Talwalkar.
\newblock Variable importance using decision trees.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  425--434, 2017.

\bibitem{koh2017understanding}
Pang~Wei Koh and Percy Liang.
\newblock Understanding black-box predictions via influence functions.
\newblock {\em arXiv preprint arXiv:1703.04730}, 2017.

\bibitem{lakkaraju2016interpretable}
Himabindu Lakkaraju, Stephen~H Bach, and Jure Leskovec.
\newblock Interpretable decision sets: A joint framework for description and
  prediction.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 1675--1684. ACM, 2016.

\bibitem{lin2006random}
Yi~Lin and Yongho Jeon.
\newblock Random forests and adaptive nearest neighbors.
\newblock {\em Journal of the American Statistical Association},
  101(474):578--590, 2006.

\bibitem{lipton2016mythos}
Zachary~C Lipton.
\newblock The mythos of model interpretability.
\newblock {\em arXiv preprint arXiv:1606.03490}, 2016.

\bibitem{lundberg2017unified}
Scott~M Lundberg and Su-In Lee.
\newblock A unified approach to interpreting model predictions.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  4768--4777, 2017.

\bibitem{ribeiro2016should}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Why should i trust you?: Explaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, pages 1135--1144. ACM, 2016.

\bibitem{ribeiro2018anchors}
Marco~Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
\newblock Anchors: High-precision model-agnostic explanations.
\newblock AAAI, 2018.

\bibitem{scornet2016random}
Erwan Scornet.
\newblock Random forests and kernel methods.
\newblock {\em IEEE Transactions on Information Theory}, 62(3):1485--1500,
  2016.

\end{thebibliography}
