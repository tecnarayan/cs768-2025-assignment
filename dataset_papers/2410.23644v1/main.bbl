\begin{thebibliography}{29}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ben-David and Urner(2014)]{ben2014domain}
Shai Ben-David and Ruth Urner.
\newblock Domain adaptation--can quantity compensate for quality?
\newblock \emph{Annals of Mathematics and Artificial Intelligence}, 70\penalty0 (3):\penalty0 185--202, 2014.

\bibitem[Beygelzimer et~al.(2006)Beygelzimer, Kakade, and Langford]{beygelzimer2006cover}
Alina Beygelzimer, Sham Kakade, and John Langford.
\newblock Cover trees for nearest neighbor.
\newblock In \emph{Proceedings of the 23rd international conference on Machine learning}, pages 97--104, 2006.

\bibitem[Blanchard(2022)]{blanchard2022universal}
Moise Blanchard.
\newblock Universal online learning: An optimistically universal learning rule.
\newblock In \emph{Conference on Learning Theory}, pages 1077--1125. PMLR, 2022.

\bibitem[Blanchard and Cosson(2022)]{blanchard2022bounded}
Moise Blanchard and Romain Cosson.
\newblock Universal online learning with bounded loss: Reduction to binary classification.
\newblock In \emph{Conference on Learning Theory}, pages 479--495. PMLR, 2022.

\bibitem[Block et~al.(2022)Block, Dagan, Golowich, and Rakhlin]{block2022smoothed}
Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin.
\newblock Smoothed online learning is as easy as statistical learning.
\newblock In \emph{Conference on Learning Theory}, pages 1716--1786. PMLR, 2022.

\bibitem[C{\'e}rou and Guyader(2006)]{cerou2006nearest}
Fr{\'e}d{\'e}ric C{\'e}rou and Arnaud Guyader.
\newblock Nearest neighbor classification in infinite dimension.
\newblock \emph{ESAIM: Probability and Statistics}, 10:\penalty0 340--355, 2006.

\bibitem[Chaudhuri and Dasgupta(2014)]{chaudhuri2014rates}
Kamalika Chaudhuri and Sanjoy Dasgupta.
\newblock Rates of convergence for nearest neighbor classification.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Cover and Hart(1967)]{cover1967nearest}
Thomas Cover and Peter Hart.
\newblock Nearest neighbor pattern classification.
\newblock \emph{IEEE transactions on information theory}, 13\penalty0 (1):\penalty0 21--27, 1967.

\bibitem[Dasgupta(2012)]{dasgupta2012consistency}
Sanjoy Dasgupta.
\newblock Consistency of nearest neighbor classification under selective sampling.
\newblock In \emph{Conference on Learning Theory}, pages 18--1. JMLR Workshop and Conference Proceedings, 2012.

\bibitem[Dasgupta and Kpotufe(2021)]{dasgupta2020nearest}
Sanjoy Dasgupta and Samory Kpotufe.
\newblock \emph{Nearest Neighbor Classification and Search}, page 403â€“423.
\newblock Cambridge University Press, 2021.
\newblock \doi{10.1017/9781108637435.024}.

\bibitem[Devroye et~al.(1994)Devroye, Gyorfi, Krzyzak, and Lugosi]{devroye1994strong}
Luc Devroye, Laszlo Gyorfi, Adam Krzyzak, and G{\'a}bor Lugosi.
\newblock On the strong universal consistency of nearest neighbor regression function estimates.
\newblock \emph{The Annals of Statistics}, 22\penalty0 (3):\penalty0 1371--1385, 1994.

\bibitem[Devroye et~al.(2013)Devroye, Gy{\"o}rfi, and Lugosi]{devroye2013probabilistic}
Luc Devroye, L{\'a}szl{\'o} Gy{\"o}rfi, and G{\'a}bor Lugosi.
\newblock \emph{A probabilistic theory of pattern recognition}, volume~31.
\newblock Springer Science \& Business Media, 2013.

\bibitem[Durrett(2019)]{durrett2019probability}
Rick Durrett.
\newblock \emph{Probability: theory and examples}, volume~49.
\newblock Cambridge university press, 2019.

\bibitem[Fix and Hodges(1951)]{fix1951discriminatory}
Evelyn Fix and Joseph~Lawson Hodges.
\newblock Discriminatory analysis, nonparametric discrimination.
\newblock \emph{USAF School of Aviation Medicine, Randolph Field, Texas, Project 21-49-004, Report 4, Contract AD41(128)-31}, 1951.

\bibitem[Gromov et~al.(1999)Gromov, Katz, Pansu, and Semmes]{gromov1999metric}
Mikhael Gromov, Misha Katz, Pierre Pansu, and Stephen Semmes.
\newblock \emph{Metric structures for Riemannian and non-Riemannian spaces}, volume 152.
\newblock Springer, 1999.

\bibitem[Haghtalab et~al.(2020)Haghtalab, Roughgarden, and Shetty]{haghtalab2020smoothed}
Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty.
\newblock Smoothed analysis of online and differentially private learning.
\newblock \emph{Advances in Neural Information Processing Systems}, 33:\penalty0 9203--9215, 2020.

\bibitem[Haghtalab et~al.(2022)Haghtalab, Roughgarden, and Shetty]{haghtalab2022smoothed}
Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty.
\newblock Smoothed analysis with adaptive adversaries.
\newblock In \emph{2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS)}, pages 942--953. IEEE, 2022.

\bibitem[Hanneke(2021)]{hanneke2021learning}
Steve Hanneke.
\newblock Learning whenever learning is possible: Universal learning under general stochastic processes.
\newblock \emph{Journal of Machine Learning Research}, 22\penalty0 (130):\penalty0 1--116, 2021.

\bibitem[Hanneke et~al.(2021)Hanneke, Livni, and Moran]{hanneke2021online}
Steve Hanneke, Roi Livni, and Shay Moran.
\newblock Online learning with simple predictors and a combinatorial characterization of minimax in 0/1 games.
\newblock In \emph{Conference on Learning Theory}, pages 2289--2314. PMLR, 2021.

\bibitem[Heinonen(2001)]{heinonen2001lectures}
Juha Heinonen.
\newblock \emph{Lectures on analysis on metric spaces}.
\newblock Springer Science \& Business Media, 2001.

\bibitem[Holst and Irle(2001)]{holst2001nearest}
M~Holst and A~Irle.
\newblock Nearest neighbor classification with dependent training sequences.
\newblock \emph{Annals of statistics}, pages 1424--1442, 2001.

\bibitem[Hyt{\"o}nen(2010)]{hytonen2010framework}
Tuomas Hyt{\"o}nen.
\newblock A framework for non-homogeneous analysis on metric spaces, and the rbmo space of tolsa.
\newblock \emph{Publicacions Matematiques}, pages 485--504, 2010.

\bibitem[Kulkarni and Posner(1995)]{kulkarni1995rates}
Sanjeev~R Kulkarni and Steven~E Posner.
\newblock Rates of convergence of nearest neighbor estimation under arbitrary sampling.
\newblock \emph{IEEE Transactions on Information Theory}, 41\penalty0 (4):\penalty0 1028--1039, 1995.

\bibitem[Rakhlin et~al.(2011)Rakhlin, Sridharan, and Tewari]{rakhlin2011online}
Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari.
\newblock Online learning: Stochastic and constrained adversaries.
\newblock \emph{arXiv preprint arXiv:1104.5070}, 2011.

\bibitem[Roughgarden(2021)]{roughgarden2021beyond}
Tim Roughgarden.
\newblock \emph{Beyond the worst-case analysis of algorithms}.
\newblock Cambridge University Press, 2021.

\bibitem[Ryabko(2006)]{ryabko2006pattern}
Daniil Ryabko.
\newblock Pattern recognition for conditionally independent data.
\newblock \emph{Journal of Machine Learning Research}, 7\penalty0 (4), 2006.

\bibitem[Spielman and Teng(2004)]{spielman2004smoothed}
Daniel~A Spielman and Shang-Hua Teng.
\newblock Smoothed analysis of algorithms: Why the simplex algorithm usually takes polynomial time.
\newblock \emph{Journal of the ACM (JACM)}, 51\penalty0 (3):\penalty0 385--463, 2004.

\bibitem[Stone(1977)]{stone1977consistent}
Charles~J Stone.
\newblock Consistent nonparametric regression.
\newblock \emph{The annals of statistics}, pages 595--620, 1977.

\bibitem[Talagrand(2008)]{talagrand2008maharam}
Michel Talagrand.
\newblock Maharam's problem.
\newblock \emph{Annals of mathematics}, pages 981--1009, 2008.

\end{thebibliography}
