\begin{thebibliography}{44}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bian et~al.(2017)Bian, Buhmann, Krause, and Tschiatschek]{Bian2017a}
A.~A. Bian, J.~M. Buhmann, A.~Krause, and S.~Tschiatschek.
\newblock Guarantees for greedy maximization of non-submodular functions with
  applications.
\newblock In \emph{Proceedings of the 34th International Conference on Machine
  Learning-Volume 70}, pages 498--507. JMLR. org, 2017.

\bibitem[Blalock et~al.(2020)Blalock, Ortiz, Frankle, and Guttag]{Blalock2020}
D.~Blalock, J.~J.~G. Ortiz, J.~Frankle, and J.~Guttag.
\newblock What is the state of neural network pruning?
\newblock \emph{arXiv preprint arXiv:2003.03033}, 2020.

\bibitem[Bucila et~al.(2006)Bucila, Caruana, and
  Niculescu-Mizil]{bucila2006model}
C.~Bucila, R.~Caruana, and A.~Niculescu-Mizil.
\newblock Model compression.
\newblock In \emph{Proceedings of the 12th ACM SIGKDD International Conference
  on Knowledge Discovery and Data Mining}, KDD '06, page 535–541, New York,
  NY, USA, 2006. Association for Computing Machinery.
\newblock ISBN 1595933395.
\newblock \doi{10.1145/1150402.1150464}.
\newblock URL \url{https://doi.org/10.1145/1150402.1150464}.

\bibitem[Buschjäger et~al.(2020)Buschjäger, Honysz, and
  Morik]{Buschjaeger2020}
S.~Buschjäger, P.-J. Honysz, and K.~Morik.
\newblock Very fast streaming submodular function maximization, 2020.

\bibitem[Collins and Kohli(2014)]{collins2014memory}
M.~D. Collins and P.~Kohli.
\newblock Memory bounded deep convolutional networks.
\newblock \emph{arXiv preprint arXiv:1412.1442}, 2014.

\bibitem[Courbariaux et~al.(2015)Courbariaux, Bengio, and
  David]{courbariaux2015binaryconnect}
M.~Courbariaux, Y.~Bengio, and J.-P. David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In \emph{Advances in neural information processing systems}, pages
  3123--3131, 2015.

\bibitem[Das and Kempe(2011)]{Das2011}
A.~Das and D.~Kempe.
\newblock Submodular meets spectral: Greedy algorithms for subset selection,
  sparse approximation and dictionary selection.
\newblock \emph{arXiv preprint arXiv:1102.3975}, 2011.

\bibitem[Denil et~al.(2013)Denil, Shakibi, Dinh, Ranzato, and
  de~Freitas]{Denil2013Predicting}
M.~Denil, B.~Shakibi, L.~Dinh, M.~A. Ranzato, and N.~de~Freitas.
\newblock Predicting parameters in deep learning.
\newblock In C.~J.~C. Burges, L.~Bottou, M.~Welling, Z.~Ghahramani, and K.~Q.
  Weinberger, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~26. Curran Associates, Inc., 2013.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2013/file/7fec306d1e665bc9c748b5d2b99a6e97-Paper.pdf}.

\bibitem[El~Halabi et~al.(2018)El~Halabi, Bach, and Cevher]{ElHalabi2018}
M.~El~Halabi, F.~Bach, and V.~Cevher.
\newblock Combinatorial penalties: Structure preserved by convex relaxations.
\newblock \emph{Proceedings of the 21st International Conference on Artificial
  Intelligence and Statistics}, 2018.

\bibitem[Elenberg et~al.(2016)Elenberg, Khanna, Dimakis, and
  Negahban]{Elenberg2016}
E.~R. Elenberg, R.~Khanna, A.~G. Dimakis, and S.~Negahban.
\newblock Restricted strong convexity implies weak submodularity.
\newblock \emph{arXiv preprint arXiv:1612.00804}, 2016.

\bibitem[Gong et~al.(2014)Gong, Liu, Yang, and Bourdev]{gong2014compressing}
Y.~Gong, L.~Liu, M.~Yang, and L.~Bourdev.
\newblock Compressing deep convolutional networks using vector quantization.
\newblock \emph{arXiv preprint arXiv:1412.6115}, 2014.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{He2016}
K.~He, X.~Zhang, S.~Ren, and J.~Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[He et~al.(2014)He, Fan, Qian, Tan, and Yu]{He2014}
T.~He, Y.~Fan, Y.~Qian, T.~Tan, and K.~Yu.
\newblock Reshaping deep neural network for fast decoding by node-pruning.
\newblock In \emph{2014 IEEE International Conference on Acoustics, Speech and
  Signal Processing (ICASSP)}, pages 245--249. IEEE, 2014.

\bibitem[He et~al.(2017)He, Zhang, and Sun]{He2017}
Y.~He, X.~Zhang, and J.~Sun.
\newblock Channel pruning for accelerating very deep neural networks.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 1389--1397, 2017.

\bibitem[Hinton et~al.(2015)Hinton, Vinyals, and Dean]{hinton2015distilling}
G.~Hinton, O.~Vinyals, and J.~Dean.
\newblock Distilling the knowledge in a neural network.
\newblock \emph{Neural Information Processing Systems (NeurIPS) Workshops},
  2015.

\bibitem[Hoefler et~al.(2021)Hoefler, Alistarh, Ben-Nun, Dryden, and
  Peste]{Hoefler2021}
T.~Hoefler, D.~Alistarh, T.~Ben-Nun, N.~Dryden, and A.~Peste.
\newblock Sparsity in deep learning: Pruning and growth for efficient inference
  and training in neural networks.
\newblock \emph{arXiv preprint arXiv:2102.00554}, 2021.

\bibitem[Krizhevsky et~al.(2009)Krizhevsky, Hinton, et~al.]{Krizhevsky2009}
A.~Krizhevsky, G.~Hinton, et~al.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kuzmin et~al.(2019)Kuzmin, Nagel, Pitre, Pendyam, Blankevoort, and
  Welling]{Kuzmin2019}
A.~Kuzmin, M.~Nagel, S.~Pitre, S.~Pendyam, T.~Blankevoort, and M.~Welling.
\newblock Taxonomy and evaluation of structured compression of convolutional
  neural networks.
\newblock \emph{arXiv preprint arXiv:1912.09802}, 2019.

\bibitem[Lebedev et~al.(2015)Lebedev, Ganin, Rakhuba, Oseledets, and
  Lempitsky]{lebedev2015speeding}
V.~Lebedev, Y.~Ganin, M.~Rakhuba, I.~V. Oseledets, and V.~S. Lempitsky.
\newblock Speeding-up convolutional neural networks using fine-tuned
  cp-decomposition.
\newblock In \emph{International Conference on Learning Representations}, 2015.

\bibitem[LeCun et~al.(1989)LeCun, Boser, Denker, Henderson, Howard, Hubbard,
  and Jackel]{LeCun1989}
Y.~LeCun, B.~Boser, J.~S. Denker, D.~Henderson, R.~E. Howard, W.~Hubbard, and
  L.~D. Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock \emph{Neural computation}, 1\penalty0 (4):\penalty0 541--551, 1989.

\bibitem[Lecun et~al.(1998)Lecun, Cortes, and Burges]{Lecun1998}
Y.~Lecun, C.~Cortes, and C.~Burges.
\newblock The mnist databaseof handwritten digits, 1998.

\bibitem[Lehmann et~al.(2006)Lehmann, Lehmann, and Nisan]{Lehmann2006}
B.~Lehmann, D.~Lehmann, and N.~Nisan.
\newblock Combinatorial auctions with decreasing marginal utilities.
\newblock \emph{Games and Economic Behavior}, 55\penalty0 (2):\penalty0
  270--296, 2006.

\bibitem[Li et~al.(2017)Li, Kadav, Durdanovic, Samet, and Graf]{Li2017}
H.~Li, A.~Kadav, I.~Durdanovic, H.~Samet, and H.~P. Graf.
\newblock Pruning filters for efficient convnets.
\newblock In \emph{5th International Conference on Learning Representations,
  {ICLR} 2017, Toulon, France, April 24-26, 2017, Conference Track
  Proceedings}. OpenReview.net, 2017.
\newblock URL \url{https://openreview.net/forum?id=rJqFGTslg}.

\bibitem[Li et~al.(2022)Li, Feldman, Kazemi, and Karbasi]{Li2022}
W.~Li, M.~Feldman, E.~Kazemi, and A.~Karbasi.
\newblock Submodular maximization in clean linear time, 2022.
\newblock URL \url{https://arxiv.org/abs/2006.09327}.

\bibitem[Liebenwein et~al.(2020)Liebenwein, Baykal, Lang, Feldman, and
  Rus]{Liebenwein2020}
L.~Liebenwein, C.~Baykal, H.~Lang, D.~Feldman, and D.~Rus.
\newblock Provable filter pruning for efficient neural networks.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=BJxkOlSYDH}.

\bibitem[Luo et~al.(2017)Luo, Wu, and Lin]{luo2017}
J.-H. Luo, J.~Wu, and W.~Lin.
\newblock Thinet: A filter level pruning method for deep neural network
  compression.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 5058--5066, 2017.

\bibitem[Mariet and Sra(2015)]{Mariet2015}
Z.~Mariet and S.~Sra.
\newblock Diversity networks: Neural network compression using determinantal
  point processes.
\newblock \emph{arXiv preprint arXiv:1511.05077}, 2015.

\bibitem[McGuffie and Newhouse(2020)]{McGuffie2020}
K.~McGuffie and A.~Newhouse.
\newblock The radicalization risks of gpt-3 and advanced neural language
  models.
\newblock \emph{arXiv preprint arXiv:2009.06807}, 2020.

\bibitem[Mirzasoleiman et~al.(2015)Mirzasoleiman, Badanidiyuru, Karbasi,
  Vondr{\'a}k, and Krause]{Mirzasoleiman2015}
B.~Mirzasoleiman, A.~Badanidiyuru, A.~Karbasi, J.~Vondr{\'a}k, and A.~Krause.
\newblock Lazier than lazy greedy.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~29, 2015.

\bibitem[Molchanov et~al.(2017)Molchanov, Tyree, Karras, Aila, and
  Kautz]{Molchanov2017}
P.~Molchanov, S.~Tyree, T.~Karras, T.~Aila, and J.~Kautz.
\newblock Pruning convolutional neural networks for resource efficient
  inference.
\newblock \emph{ICLR}, 2017.

\bibitem[Mussay et~al.(2020)Mussay, Osadchy, Braverman, Zhou, and
  Feldman]{Mussay2020}
B.~Mussay, M.~Osadchy, V.~Braverman, S.~Zhou, and D.~Feldman.
\newblock Data-independent neural pruning via coresets.
\newblock In \emph{International Conference on Learning Representations}, 2020.
\newblock URL \url{https://openreview.net/forum?id=H1gmHaEKwB}.

\bibitem[Mussay et~al.(2021)Mussay, Feldman, Zhou, Braverman, and
  Osadchy]{Mussay2021}
B.~Mussay, D.~Feldman, S.~Zhou, V.~Braverman, and M.~Osadchy.
\newblock Data-independent structured pruning of neural networks via coresets.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  pages 1--13, 2021.
\newblock \doi{10.1109/TNNLS.2021.3088587}.

\bibitem[Natarajan(1995)]{Natarajan1995}
B.~K. Natarajan.
\newblock Sparse approximate solutions to linear systems.
\newblock \emph{SIAM journal on computing}, 24\penalty0 (2):\penalty0 227--234,
  1995.

\bibitem[Nemhauser et~al.(1978)Nemhauser, Wolsey, and Fisher]{Nemhauser1978}
G.~Nemhauser, L.~Wolsey, and M.~Fisher.
\newblock An analysis of approximations for maximizing submodular set functions
  --- {I}.
\newblock \emph{Mathematical Programming}, 14\penalty0 (1):\penalty0 265--294,
  1978.

\bibitem[Paszke et~al.(2017)Paszke, Gross, Chintala, Chanan, Yang, DeVito, Lin,
  Desmaison, Antiga, and Lerer]{Paszke2017}
A.~Paszke, S.~Gross, S.~Chintala, G.~Chanan, E.~Yang, Z.~DeVito, Z.~Lin,
  A.~Desmaison, L.~Antiga, and A.~Lerer.
\newblock Automatic differentiation in pytorch.
\newblock 2017.

\bibitem[Phan(2021)]{Phan2021}
H.~Phan.
\newblock huyvnphan/pytorch\_cifar10, Jan. 2021.
\newblock URL \url{https://doi.org/10.5281/zenodo.4431043}.

\bibitem[Simonyan and Zisserman(2015)]{Simonyan2015}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In Y.~Bengio and Y.~LeCun, editors, \emph{3rd International
  Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA, May
  7-9, 2015, Conference Track Proceedings}, 2015.
\newblock URL \url{http://arxiv.org/abs/1409.1556}.

\bibitem[Srinivas and Babu(2015)]{Srinivas2015}
S.~Srinivas and R.~V. Babu.
\newblock Data-free parameter pruning for deep neural networks.
\newblock In \emph{Proceedings of the British Machine Vision Conference
  (BMVC)}, pages 31.1--31.12. BMVA Press, September 2015.

\bibitem[Su et~al.(2018)Su, Li, Bhattacharjee, and Huang]{su2018tensorial}
J.~Su, J.~Li, B.~Bhattacharjee, and F.~Huang.
\newblock Tensorial neural networks: Generalization of neural networks and
  application to model compression.
\newblock \emph{arXiv preprint arXiv:1805.10352}, 2018.

\bibitem[Sviridenko et~al.(2017)Sviridenko, Vondr{\'a}k, and
  Ward]{Sviridenko2017}
M.~Sviridenko, J.~Vondr{\'a}k, and J.~Ward.
\newblock Optimal approximation for submodular and supermodular optimization
  with bounded curvature.
\newblock \emph{Mathematics of Operations Research}, 42\penalty0 (4):\penalty0
  1197--1218, 2017.

\bibitem[Voita et~al.(2019)Voita, Talbot, Moiseev, Sennrich, and
  Titov]{voita2019analyzing}
E.~Voita, D.~Talbot, F.~Moiseev, R.~Sennrich, and I.~Titov.
\newblock Analyzing multi-head self-attention: Specialized heads do the heavy
  lifting, the rest can be pruned.
\newblock In \emph{Proceedings of the 57th Annual Meeting of the Association
  for Computational Linguistics}, pages 5797--5808, 2019.

\bibitem[Ye et~al.(2020{\natexlab{a}})Ye, Gong, Nie, Zhou, Klivans, and
  Liu]{Ye2020}
M.~Ye, C.~Gong, L.~Nie, D.~Zhou, A.~Klivans, and Q.~Liu.
\newblock Good subnetworks provably exist: Pruning via greedy forward
  selection.
\newblock \emph{ICML}, 2020{\natexlab{a}}.

\bibitem[Ye et~al.(2020{\natexlab{b}})Ye, Wu, and Liu]{Ye2020b}
M.~Ye, L.~Wu, and Q.~Liu.
\newblock Greedy optimization provably wins the lottery: Logarithmic number of
  winning tickets is enough.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 16409--16420, 2020{\natexlab{b}}.

\bibitem[Zhuang et~al.(2018)Zhuang, Tan, Zhuang, Liu, Guo, Wu, Huang, and
  Zhu]{Zhuang2018}
Z.~Zhuang, M.~Tan, B.~Zhuang, J.~Liu, Y.~Guo, Q.~Wu, J.~Huang, and J.~Zhu.
\newblock Discrimination-aware channel pruning for deep neural networks.
\newblock In S.~Bengio, H.~Wallach, H.~Larochelle, K.~Grauman, N.~Cesa-Bianchi,
  and R.~Garnett, editors, \emph{Advances in Neural Information Processing
  Systems}, volume~31. Curran Associates, Inc., 2018.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2018/file/55a7cf9c71f1c9c495413f934dd1a158-Paper.pdf}.

\end{thebibliography}
