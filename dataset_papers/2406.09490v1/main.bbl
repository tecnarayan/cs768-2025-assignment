\begin{thebibliography}{10}

\bibitem{wandb}
{\sc Biewald, L.}
\newblock Experiment tracking with weights and biases, 2020.
\newblock Software available from wandb.com.

\bibitem{BoydstunAmberE.2013Mtn:}
{\sc Boydstun, A.~E.}
\newblock {\em Making the news : politics, the media, and agenda setting}.
\newblock The University of Chicago Press, Chicago ; London, 2013.

\bibitem{bryan2023}
{\sc Bryan, T., Carlson, J., Arora, A., and Dell, M.}
\newblock Efficientocr: An extensible, open-source package for efficiently digitizing world knowledge”.
\newblock {\em Empirical Methods on Natural Language Processing (Systems Demonstrations Track)\/} (2023).

\bibitem{sklearn_api}
{\sc Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V., Prettenhofer, P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J., Joly, A., Holt, B., and Varoquaux, G.}
\newblock {API} design for machine learning software: experiences from the scikit-learn project.
\newblock In {\em ECML PKDD Workshop: Languages for Data Mining and Machine Learning\/} (2013), pp.~108--122.

\bibitem{carlson2023}
{\sc Carlson, J., Bryan, T., and Dell, M.}
\newblock Efficient ocr for building a diverse digital history.
\newblock {\em Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics\/} (forthcoming).

\bibitem{dell2023american}
{\sc Dell, M., Carlson, J., Bryan, T., Silcock, E., Arora, A., Shen, Z., D'Amico-Wong, L., Le, Q., Querubin, P., and Heldring, L.}
\newblock American stories: A large-scale structured text dataset of historical us newspapers.
\newblock {\em Advances in Neural Information and Processing Systems, Datasets and Benchmarks\/} (2023).

\bibitem{franklin2024news}
{\sc Franklin, B., Silcock, E., Arora, A., Bryan, T., and Dell, M.}
\newblock News deja vu: Connecting past and present with semantic search.
\newblock In {\em Proceedings of workshop for NLP and CSS, NAACL\/} (2024).

\bibitem{gebru2021datasheets}
{\sc Gebru, T., Morgenstern, J., Vecchione, B., Vaughan, J.~W., Wallach, H., au2, H. D.~I., and Crawford, K.}
\newblock Datasheets for datasets, 2021.

\bibitem{guarneri2017newsprint}
{\sc Guarneri, J.}
\newblock {\em Newsprint Metropolis}.
\newblock University of Chicago Press, 2017.

\bibitem{he2017mask}
{\sc He, K., Gkioxari, G., Doll{\'a}r, P., and Girshick, R.}
\newblock Mask r-cnn.
\newblock In {\em Proceedings of the IEEE international conference on computer vision\/} (2017), pp.~2961--2969.

\bibitem{johnson2019billion}
{\sc Johnson, J., Douze, M., and J{\'e}gou, H.}
\newblock Billion-scale similarity search with gpus.
\newblock {\em IEEE Transactions on Big Data 7}, 3 (2019), 535--547.

\bibitem{kandpal2022deduplicating}
{\sc Kandpal, N., Wallace, E., and Raffel, C.}
\newblock Deduplicating training data mitigates privacy risks in language models.
\newblock {\em arXiv preprint arXiv:2202.06539\/} (2022).

\bibitem{lee2022language}
{\sc Lee, J., Le, T., Chen, J., and Lee, D.}
\newblock Do language models plagiarize?
\newblock {\em arXiv preprint arXiv:2203.07618\/} (2022).

\bibitem{lee2021deduplicating}
{\sc Lee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., and Carlini, N.}
\newblock Deduplicating training data makes language models better.
\newblock {\em arXiv preprint arXiv:2107.06499\/} (2021).

\bibitem{liu2019roberta}
{\sc Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.}
\newblock Roberta: A robustly optimized bert pretraining approach.
\newblock {\em arXiv preprint arXiv:1907.11692\/} (2019).

\bibitem{copyright}
{\sc Ockerbloom, J.~M.}
\newblock Newspaper copyrights, notices, and renewals, 2019.

\bibitem{pytorch_lib}
{\sc Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S.}
\newblock {PyTorch: An Imperative Style, High-Performance Deep Learning Library}.
\newblock In {\em Advances in Neural Information Processing Systems 32\/} (2019), H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d'Alché Buc, E.~Fox, and R.~Garnett, Eds., Curran Associates, Inc., pp.~8024--8035.

\bibitem{reimers2019sentence}
{\sc Reimers, N., and Gurevych, I.}
\newblock Sentence-bert: Sentence embeddings using siamese bert-networks.
\newblock {\em arXiv preprint arXiv:1908.10084\/} (2019).

\bibitem{shen2022olala}
{\sc Shen, Z., Zhao, J., Yu, Y., Li, W., and Dell, M.}
\newblock Olala: object-level active learning based layout annotation.
\newblock {\em EMNLP Computational Social Science Workshop\/} (2022).

\bibitem{silcock2023massive}
{\sc Silcock, E., Arora, A., and Dell, M.}
\newblock A massive scale semantic similarity dataset of historical english.
\newblock {\em Advances in Neural Information and Processing Systems, Datasets and Benchmarks\/} (2023).

\bibitem{silcock2022noise}
{\sc Silcock, E., D'Amico-Wong, L., Yang, J., and Dell, M.}
\newblock Noise-robust de-duplication at scale.
\newblock In {\em The Eleventh International Conference on Learning Representations\/} (2022).

\bibitem{smith2015computational}
{\sc Smith, D.~A., Cordell, R., and Mullen, A.}
\newblock Computational methods for uncovering reprinted texts in antebellum newspapers.
\newblock {\em American Literary History 27}, 3 (2015), E1--E15.

\bibitem{song2020mpnet}
{\sc Song, K., Tan, X., Qin, T., Lu, J., and Liu, T.-Y.}
\newblock Mpnet: Masked and permuted pre-training for language understanding.
\newblock {\em Advances in Neural Information Processing Systems 33\/} (2020), 16857--16867.

\bibitem{yolov8}
{\sc Ultalytics}.
\newblock Yolo v8 github repository.
\newblock \url{https://github.com/ultralytics/ultralytics}, 2023.

\bibitem{hugging_face_tf}
{\sc Von~Platen, P.}
\newblock Transformer-based encoder-decoder models, 2020.

\bibitem{wu2019scalable}
{\sc Wu, L., Petroni, F., Josifoski, M., Riedel, S., and Zettlemoyer, L.}
\newblock Scalable zero-shot entity linking with dense entity retrieval.
\newblock {\em arXiv preprint arXiv:1911.03814\/} (2019).

\bibitem{yamada2022global}
{\sc Yamada, I., Washio, K., Shindo, H., and Matsumoto, Y.}
\newblock Global entity disambiguation with bert.
\newblock In {\em Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\/} (2022), pp.~3264--3271.

\end{thebibliography}
