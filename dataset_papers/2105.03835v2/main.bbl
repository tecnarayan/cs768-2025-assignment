\begin{thebibliography}{47}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ackerson \& Fu(1970)Ackerson and Fu]{ackerson1970state}
Ackerson, G. and Fu, K.
\newblock On state estimation in switching environments.
\newblock \emph{IEEE transactions on automatic control}, 15\penalty0
  (1):\penalty0 10--17, 1970.

\bibitem[Arlot et~al.(2019)Arlot, Celisse, and Harchaoui]{arlot2019kernel}
Arlot, S., Celisse, A., and Harchaoui, Z.
\newblock A kernel multiple change-point algorithm via model selection.
\newblock \emph{Journal of Machine Learning Research}, 20\penalty0
  (162):\penalty0 1--56, 2019.

\bibitem[Bai et~al.(2000)]{bai2000vector}
Bai, J. et~al.
\newblock Vector autoregressive models with structural changes in regression
  coefficients and in variance-covariance matrices.
\newblock Technical report, China Economics and Management Academy, Central
  University of Finance and Economics, 2000.

\bibitem[Bengio et~al.(2015)Bengio, Vinyals, Jaitly, and
  Shazeer]{bengio2015scheduled}
Bengio, S., Vinyals, O., Jaitly, N., and Shazeer, N.
\newblock Scheduled sampling for sequence prediction with recurrent neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 28: Annual
  Conference on Neural Information Processing Systems 2015, December 7-12,
  2015, Montreal, Quebec, Canada}, pp.\  1171--1179, 2015.

\bibitem[Brouwer et~al.(2019)Brouwer, Simm, Arany, and Moreau]{de2019gru}
Brouwer, E.~D., Simm, J., Arany, A., and Moreau, Y.
\newblock {GRU}-{ODE}-{B}ayes: Continuous modeling of sporadically-observed
  time series.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  7377--7388, 2019.

\bibitem[Calvo et~al.(2003)Calvo, Montijano, and Rández]{calvo2003}
Calvo, M., Montijano, J., and Rández, L.
\newblock On the solution of discontinuous {IVP}s by adaptive {R}unge–{K}utta
  codes.
\newblock \emph{Numerical Algorithms}, 33, 2003.
\newblock \doi{10.1023/A:1025507920426}.

\bibitem[Calvo et~al.(2008)Calvo, Montijano, and Rández]{calvo2008}
Calvo, M., Montijano, J., and Rández, L.
\newblock The numerical solution of discontinuous {IVP}s by {R}unge-{K}utta
  codes: A review.
\newblock \emph{SeMA Journal}, 44, 2008.

\bibitem[Chen et~al.(2020)Chen, Amos, and Nickel]{chen2020learning}
Chen, R.~T., Amos, B., and Nickel, M.
\newblock Learning neural event functions for ordinary differential equations.
\newblock \emph{arXiv preprint arXiv:2011.03902}, 2020.

\bibitem[Chen et~al.(2018)Chen, Rubanova, Bettencourt, and
  Duvenaud]{chen2018neural}
Chen, T.~Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D.
\newblock Neural ordinary differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems 31: Annual
  Conference on Neural Information Processing Systems 2018, NeurIPS 2018,
  December 3-8, 2018, Montr{\'{e}}al, Canada}, pp.\  6572--6583, 2018.

\bibitem[Cho et~al.(2014)Cho, van Merri{\"e}nboer, Gulcehre, Bahdanau,
  Bougares, Schwenk, and Bengio]{cho2014learning}
Cho, K., van Merri{\"e}nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F.,
  Schwenk, H., and Bengio, Y.
\newblock Learning phrase representations using {RNN} encoder{--}decoder for
  statistical machine translation.
\newblock In \emph{Proceedings of the 2014 Conference on Empirical Methods in
  Natural Language Processing ({EMNLP})}, pp.\  1724--1734, Doha, Qatar, 2014.
  Association for Computational Linguistics.
\newblock \doi{10.3115/v1/D14-1179}.

\bibitem[Cranmer et~al.(2020)Cranmer, Brehmer, and Louppe]{cranmer2020frontier}
Cranmer, K., Brehmer, J., and Louppe, G.
\newblock The frontier of simulation-based inference.
\newblock \emph{Proceedings of the National Academy of Sciences}, 117\penalty0
  (48):\penalty0 30055--30062, 2020.

\bibitem[Dong et~al.(2020)Dong, Seybold, Murphy, and Bui]{dong2020collapsed}
Dong, Z., Seybold, B.~A., Murphy, K., and Bui, H.~H.
\newblock Collapsed amortized variational inference for switching nonlinear
  dynamical systems.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  2638--2647. {PMLR},
  2020.

\bibitem[Dua \& Graff(2017)Dua and Graff]{Dua:2019}
Dua, D. and Graff, C.
\newblock {UCI} machine learning repository, 2017.

\bibitem[Dupont et~al.(2019)Dupont, Doucet, and Teh]{dupont2019augmented}
Dupont, E., Doucet, A., and Teh, Y.~W.
\newblock Augmented neural {ODE}s.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  3134--3144, 2019.

\bibitem[Finlay et~al.(2020)Finlay, Jacobsen, Nurbekyan, and
  Oberman]{finlay2020train}
Finlay, C., Jacobsen, J., Nurbekyan, L., and Oberman, A.~M.
\newblock How to train your {N}eural {ODE:} the world of jacobian and kinetic
  regularization.
\newblock In \emph{Proceedings of the 37th International Conference on Machine
  Learning, {ICML} 2020, 13-18 July 2020, Virtual Event}, volume 119 of
  \emph{Proceedings of Machine Learning Research}, pp.\  3154--3164. {PMLR},
  2020.

\bibitem[Fisher(1923)]{fisher1923xxi}
Fisher, R.~A.
\newblock {XXI}.—on the dominance ratio.
\newblock \emph{Proceedings of the royal society of Edinburgh}, 42:\penalty0
  321--341, 1923.

\bibitem[Fraccaro et~al.(2017)Fraccaro, Kamronn, Paquet, and
  Winther]{fraccaro2017disentangled}
Fraccaro, M., Kamronn, S., Paquet, U., and Winther, O.
\newblock A disentangled recognition and nonlinear dynamics model for
  unsupervised learning.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pp.\  3601--3610, 2017.

\bibitem[Fu et~al.(2019)Fu, Li, Liu, Gao, Celikyilmaz, and
  Carin]{fu2019cyclical}
Fu, H., Li, C., Liu, X., Gao, J., Celikyilmaz, A., and Carin, L.
\newblock Cyclical annealing schedule: A simple approach to mitigating {KL}
  vanishing.
\newblock In \emph{Proceedings of the 2019 Conference of the North {A}merican
  Chapter of the Association for Computational Linguistics: Human Language
  Technologies, Volume 1 (Long and Short Papers)}, pp.\  240--250, Minneapolis,
  Minnesota, 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/N19-1021}.

\bibitem[{Jackson} et~al.(2005){Jackson}, {Scargle}, {Barnes}, {Arabhi}, {Alt},
  {Gioumousis}, {Gwin}, {Sangtrakulcharoen}, {Tan}, and {Tun Tao
  Tsai}]{1381461}
{Jackson}, B., {Scargle}, J.~D., {Barnes}, D., {Arabhi}, S., {Alt}, A.,
  {Gioumousis}, P., {Gwin}, E., {Sangtrakulcharoen}, P., {Tan}, L., and {Tun
  Tao Tsai}.
\newblock An algorithm for optimal partitioning of data on an interval.
\newblock \emph{IEEE Signal Processing Letters}, 12\penalty0 (2):\penalty0
  105--108, 2005.
\newblock \doi{10.1109/LSP.2001.838216}.

\bibitem[Jia \& Benson(2019)Jia and Benson]{jia2019neural}
Jia, J. and Benson, A.~R.
\newblock Neural jump stochastic differential equations.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  9843--9854, 2019.

\bibitem[Johnson et~al.(2016)Johnson, Duvenaud, Wiltschko, Adams, and
  Datta]{johnson2016composing}
Johnson, M.~J., Duvenaud, D., Wiltschko, A.~B., Adams, R.~P., and Datta, S.~R.
\newblock Composing graphical models with neural networks for structured
  representations and fast inference.
\newblock In \emph{Advances in Neural Information Processing Systems 29: Annual
  Conference on Neural Information Processing Systems 2016, December 5-10,
  2016, Barcelona, Spain}, pp.\  2946--2954, 2016.

\bibitem[Kelly et~al.(2020)Kelly, Bettencourt, Johnson, and
  Duvenaud]{kelly2020learning}
Kelly, J., Bettencourt, J., Johnson, M.~J., and Duvenaud, D.
\newblock Learning differential equations that are easy to solve.
\newblock In \emph{Advances in Neural Information Processing Systems 33: Annual
  Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
  December 6-12, 2020, virtual}, 2020.

\bibitem[Kidger et~al.(2020)Kidger, Chen, and Lyons]{kidger2020hey}
Kidger, P., Chen, R.~T., and Lyons, T.
\newblock ``{H}ey, that's not an {ODE}": Faster {ODE} adjoints with 12 lines of
  code.
\newblock \emph{arXiv preprint arXiv:2009.09457}, 2020.

\bibitem[Killick et~al.(2012)Killick, Fearnhead, and
  Eckley]{killick2012optimal}
Killick, R., Fearnhead, P., and Eckley, I.~A.
\newblock Optimal detection of changepoints with a linear computational cost.
\newblock \emph{Journal of the American Statistical Association}, 107\penalty0
  (500):\penalty0 1590--1598, 2012.

\bibitem[Kingma \& Ba(2015)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock In \emph{3rd International Conference on Learning Representations,
  {ICLR} 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track
  Proceedings}, 2015.

\bibitem[Kingma \& Welling(2014)Kingma and Welling]{kingma2013auto}
Kingma, D.~P. and Welling, M.
\newblock Auto-encoding variational {B}ayes.
\newblock In \emph{2nd International Conference on Learning Representations,
  {ICLR} 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
  Proceedings}, 2014.

\bibitem[Kipf et~al.(2019)Kipf, Li, Dai, Zambaldi, Sanchez{-}Gonzalez,
  Grefenstette, Kohli, and Battaglia]{kipf2019compile}
Kipf, T., Li, Y., Dai, H., Zambaldi, V.~F., Sanchez{-}Gonzalez, A.,
  Grefenstette, E., Kohli, P., and Battaglia, P.~W.
\newblock Comp{ILE}: Compositional imitation learning and execution.
\newblock In \emph{Proceedings of the 36th International Conference on Machine
  Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pp.\
  3418--3428. {PMLR}, 2019.

\bibitem[Lavielle \& Teyssiere(2006)Lavielle and
  Teyssiere]{lavielle2006detection}
Lavielle, M. and Teyssiere, G.
\newblock Detection of multiple change-points in multivariate time series.
\newblock \emph{Lithuanian Mathematical Journal}, 46\penalty0 (3):\penalty0
  287--306, 2006.

\bibitem[Lee et~al.(2018)Lee, Ortiz, Ko, and Lee]{lee2018time}
Lee, W.-H., Ortiz, J., Ko, B., and Lee, R.
\newblock Time series segmentation through automatic feature learning.
\newblock \emph{arXiv preprint arXiv:1801.05394}, 2018.

\bibitem[Li et~al.(2020)Li, Wong, Chen, and Duvenaud]{li2020scalable}
Li, X., Wong, T.~L., Chen, R. T.~Q., and Duvenaud, D.
\newblock Scalable gradients for stochastic differential equations.
\newblock In \emph{The 23rd International Conference on Artificial Intelligence
  and Statistics, {AISTATS} 2020, 26-28 August 2020, Online [Palermo, Sicily,
  Italy]}, volume 108 of \emph{Proceedings of Machine Learning Research}, pp.\
  3870--3882. {PMLR}, 2020.

\bibitem[Linderman et~al.(2017)Linderman, Johnson, Miller, Adams, Blei, and
  Paninski]{linderman2017bayesian}
Linderman, S., Johnson, M., Miller, A., Adams, R., Blei, D., and Paninski, L.
\newblock Bayesian learning and inference in recurrent switching linear
  dynamical systems.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  914--922.
  PMLR, 2017.

\bibitem[MacKay(1992)]{mackay1992bayesian}
MacKay, D.~J.
\newblock Bayesian interpolation.
\newblock \emph{Neural computation}, 4\penalty0 (3):\penalty0 415--447, 1992.

\bibitem[Mei \& Eisner(2017)Mei and Eisner]{mei2017neural}
Mei, H. and Eisner, J.
\newblock The neural {H}awkes process: A neurally self-modulating multivariate
  point process.
\newblock In \emph{Advances in Neural Information Processing Systems 30: Annual
  Conference on Neural Information Processing Systems 2017, December 4-9, 2017,
  Long Beach, CA, {USA}}, pp.\  6754--6764, 2017.

\bibitem[Nakamura et~al.(2017)Nakamura, Nagai, Mochihashi, Kobayashi, Asoh, and
  Kaneko]{nakamura2017segmenting}
Nakamura, T., Nagai, T., Mochihashi, D., Kobayashi, I., Asoh, H., and Kaneko,
  M.
\newblock Segmenting continuous motions with hidden semi-{M}arkov models and
  {G}aussian processes.
\newblock \emph{Frontiers in neurorobotics}, 11:\penalty0 67, 2017.

\bibitem[Norcliffe et~al.(2021)Norcliffe, Bodnar, Day, Moss, and
  Lio]{norcliffe2021neural}
Norcliffe, A., Bodnar, C., Day, B., Moss, J., and Lio, P.
\newblock Neural {ODE} processes.
\newblock \emph{arXiv preprint arXiv:2103.12413}, 2021.

\bibitem[Rackauckas et~al.(2020)Rackauckas, Ma, Martensen, Warner, Zubov,
  Supekar, Skinner, and Ramadhan]{rackauckas2020universal}
Rackauckas, C., Ma, Y., Martensen, J., Warner, C., Zubov, K., Supekar, R.,
  Skinner, D., and Ramadhan, A.
\newblock Universal differential equations for scientific machine learning.
\newblock \emph{arXiv preprint arXiv:2001.04385}, 2020.

\bibitem[Rand(1971)]{rand1971objective}
Rand, W.~M.
\newblock Objective criteria for the evaluation of clustering methods.
\newblock \emph{Journal of the American Statistical association}, 66\penalty0
  (336):\penalty0 846--850, 1971.

\bibitem[Rockafellar \& Wets(2009)Rockafellar and
  Wets]{rockafellar2009variational}
Rockafellar, R.~T. and Wets, R. J.-B.
\newblock \emph{Variational analysis}, volume 317.
\newblock Springer Science \& Business Media, 2009.

\bibitem[Rubanova et~al.(2019)Rubanova, Chen, and Duvenaud]{rubanova2019latent}
Rubanova, Y., Chen, R. T.~Q., and Duvenaud, D.~K.
\newblock Latent ordinary differential equations for irregularly-sampled time
  series.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~32, 2019.

\bibitem[Schwarz et~al.(1978)]{schwarz1978estimating}
Schwarz, G. et~al.
\newblock Estimating the dimension of a model.
\newblock \emph{The annals of statistics}, 6\penalty0 (2):\penalty0 461--464,
  1978.

\bibitem[Stewart(2011)]{stewart2011}
Stewart, D.
\newblock \emph{Dynamics with Inequalities}, chapter~8, pp.\  283--306.
\newblock Society for Industrial and Applied Mathematics, 2011.
\newblock \doi{10.1137/1.9781611970715.ch8}.

\bibitem[Supratak et~al.(2017)Supratak, Dong, Wu, and
  Guo]{supratak2017deepsleepnet}
Supratak, A., Dong, H., Wu, C., and Guo, Y.
\newblock Deep{S}leep{N}et: A model for automatic sleep stage scoring based on
  raw single-channel {EEG}.
\newblock \emph{IEEE Transactions on Neural Systems and Rehabilitation
  Engineering}, 25\penalty0 (11):\penalty0 1998--2008, 2017.

\bibitem[Truong et~al.(2020)Truong, Oudre, and Vayatis]{truong2020selective}
Truong, C., Oudre, L., and Vayatis, N.
\newblock Selective review of offline change point detection methods.
\newblock \emph{Signal Processing}, 167:\penalty0 107299, 2020.

\bibitem[Van Der~Schaft \& Schumacher(2000)Van Der~Schaft and
  Schumacher]{van2000introduction}
Van Der~Schaft, A.~J. and Schumacher, J.~M.
\newblock \emph{An introduction to hybrid dynamical systems}, volume 251.
\newblock Springer London, 2000.

\bibitem[Watanabe(2013)]{watanabe2013widely}
Watanabe, S.
\newblock A widely applicable {B}ayesian information criterion.
\newblock \emph{Journal of Machine Learning Research}, 14\penalty0
  (Mar):\penalty0 867--897, 2013.

\bibitem[Wright(1931)]{wright1931evolution}
Wright, S.
\newblock Evolution in {M}endelian populations.
\newblock \emph{Genetics}, 16\penalty0 (2):\penalty0 97, 1931.

\bibitem[Yildiz et~al.(2019)Yildiz, Heinonen, and
  L{\"{a}}hdesm{\"{a}}ki]{yildiz2019ode}
Yildiz, C., Heinonen, M., and L{\"{a}}hdesm{\"{a}}ki, H.
\newblock {ODE2VAE:} deep generative second order {ODE}s with {B}ayesian neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems 32: Annual
  Conference on Neural Information Processing Systems 2019, NeurIPS 2019,
  December 8-14, 2019, Vancouver, BC, Canada}, pp.\  13412--13421, 2019.

\end{thebibliography}
