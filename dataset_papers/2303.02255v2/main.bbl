\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Bartlett et~al.(2020)Bartlett, Long, Lugosi, and
  Tsigler]{bartlett2020benign}
Bartlett, P.~L., Long, P.~M., Lugosi, G., and Tsigler, A.
\newblock Benign overfitting in linear regression.
\newblock \emph{Proceedings of the National Academy of Sciences}, 2020.

\bibitem[Bishop \& Nasrabadi(2006)Bishop and Nasrabadi]{bishop2006pattern}
Bishop, C.~M. and Nasrabadi, N.~M.
\newblock \emph{Pattern recognition and machine learning}, volume~4.
\newblock Springer, 2006.

\bibitem[Cheng \& Montanari(2022)Cheng and Montanari]{cheng2022dimension}
Cheng, C. and Montanari, A.
\newblock Dimension free ridge regression.
\newblock \emph{arXiv preprint arXiv:2210.08571}, 2022.

\bibitem[Diakonikolas et~al.(2020)Diakonikolas, Goel, Karmalkar, Klivans, and
  Soltanolkotabi]{diakonikolas2020approximation}
Diakonikolas, I., Goel, S., Karmalkar, S., Klivans, A.~R., and Soltanolkotabi,
  M.
\newblock Approximation schemes for relu regression.
\newblock In \emph{Conference on Learning Theory}, pp.\  1452--1485. PMLR,
  2020.

\bibitem[Diakonikolas et~al.(2022)Diakonikolas, Kontonis, Tzamos, and
  Zarifis]{diakonikolas2022learning}
Diakonikolas, I., Kontonis, V., Tzamos, C., and Zarifis, N.
\newblock Learning a single neuron with adversarial label noise via gradient
  descent.
\newblock In \emph{Conference on Learning Theory}, pp.\  4313--4361. PMLR,
  2022.

\bibitem[Du et~al.(2017)Du, Lee, and Tian]{du2017convolutional}
Du, S.~S., Lee, J.~D., and Tian, Y.
\newblock When is a convolutional filter easy to learn?
\newblock \emph{arXiv preprint arXiv:1709.06129}, 2017.

\bibitem[Foster et~al.(2018)Foster, Sekhari, and Sridharan]{foster2018uniform}
Foster, D.~J., Sekhari, A., and Sridharan, K.
\newblock Uniform convergence of gradients for non-convex learning and
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Frei et~al.(2020)Frei, Cao, and Gu]{frei2020agnostic}
Frei, S., Cao, Y., and Gu, Q.
\newblock Agnostic learning of a single neuron with gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5417--5428, 2020.

\bibitem[Ge et~al.(2019)Ge, Kakade, Kidambi, and Netrapalli]{ge2019step}
Ge, R., Kakade, S.~M., Kidambi, R., and Netrapalli, P.
\newblock The step decay schedule: A near optimal, geometrically decaying
  learning rate procedure for least squares.
\newblock \emph{arXiv preprint arXiv:1904.12838}, 2019.

\bibitem[Goel et~al.(2019)Goel, Karmalkar, and Klivans]{goel2019time}
Goel, S., Karmalkar, S., and Klivans, A.
\newblock Time/accuracy tradeoffs for learning a relu with respect to gaussian
  marginals.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Goel et~al.(2020)Goel, Klivans, Manurangsi, and
  Reichman]{Goel2020TightHR}
Goel, S., Klivans, A.~R., Manurangsi, P., and Reichman, D.
\newblock Tight hardness results for training depth-2 relu networks.
\newblock In \emph{Information Technology Convergence and Services}, 2020.

\bibitem[Jain et~al.(2017)Jain, Netrapalli, Kakade, Kidambi, and
  Sidford]{jain2017parallelizing}
Jain, P., Netrapalli, P., Kakade, S.~M., Kidambi, R., and Sidford, A.
\newblock Parallelizing stochastic gradient descent for least squares
  regression: mini-batching, averaging, and model misspecification.
\newblock \emph{The Journal of Machine Learning Research}, 18\penalty0
  (1):\penalty0 8258--8299, 2017.

\bibitem[Kakade et~al.(2011)Kakade, Kanade, Shamir, and
  Kalai]{kakade2011efficient}
Kakade, S.~M., Kanade, V., Shamir, O., and Kalai, A.
\newblock Efficient learning of generalized linear and single index models with
  isotonic regression.
\newblock \emph{Advances in Neural Information Processing Systems}, 24, 2011.

\bibitem[Kalai \& Sastry(2009)Kalai and Sastry]{kalai2009isotron}
Kalai, A.~T. and Sastry, R.
\newblock The isotron algorithm: High-dimensional isotonic regression.
\newblock In \emph{COLT}, 2009.

\bibitem[Koehler et~al.(2021)Koehler, Zhou, Sutherland, and
  Srebro]{koehler2021uniform}
Koehler, F., Zhou, L., Sutherland, D.~J., and Srebro, N.
\newblock Uniform convergence of interpolators: Gaussian width, norm bounds and
  benign overfitting.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 20657--20668, 2021.

\bibitem[Mei et~al.(2018)Mei, Bai, and Montanari]{mei2018landscape}
Mei, S., Bai, Y., and Montanari, A.
\newblock The landscape of empirical risk for nonconvex losses.
\newblock \emph{The Annals of Statistics}, 46\penalty0 (6A):\penalty0
  2747--2774, 2018.

\bibitem[Soltanolkotabi(2017)]{soltanolkotabi2017learning}
Soltanolkotabi, M.
\newblock Learning relus via gradient descent.
\newblock \emph{Advances in neural information processing systems}, 30, 2017.

\bibitem[Tsigler \& Bartlett(2020)Tsigler and Bartlett]{tsigler2020benign}
Tsigler, A. and Bartlett, P.~L.
\newblock Benign overfitting in ridge regression.
\newblock \emph{arXiv preprint arXiv:2009.14286}, 2020.

\bibitem[Wu et~al.(2022{\natexlab{a}})Wu, Zou, Braverman, Gu, and
  Kakade]{wu2022iterate}
Wu, J., Zou, D., Braverman, V., Gu, Q., and Kakade, S.~M.
\newblock Last iterate risk bounds of sgd with decaying stepsize for
  overparameterized linear regression.
\newblock \emph{The 39th International Conference on Machine Learning},
  2022{\natexlab{a}}.

\bibitem[Wu et~al.(2022{\natexlab{b}})Wu, Zou, Braverman, Gu, and
  Kakade]{wu2022power}
Wu, J., Zou, D., Braverman, V., Gu, Q., and Kakade, S.~M.
\newblock The power and limitation of pretraining-finetuning for linear
  regression under covariate shift.
\newblock \emph{The 36th Conference on Neural Information Processing Systems},
  2022{\natexlab{b}}.

\bibitem[Yehudai \& Shamir(2020)Yehudai and Shamir]{yehudai2020learning}
Yehudai, G. and Shamir, O.
\newblock Learning a single neuron with gradient methods.
\newblock In \emph{Conference on Learning Theory}, pp.\  3756--3786. PMLR,
  2020.

\bibitem[Zhou et~al.(2020)Zhou, Sutherland, and Srebro]{zhou2020uniform}
Zhou, L., Sutherland, D.~J., and Srebro, N.
\newblock On uniform convergence and low-norm interpolation learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6867--6877, 2020.

\bibitem[Zhou et~al.(2021)Zhou, Koehler, Sutherland, and
  Srebro]{zhou2021optimistic}
Zhou, L., Koehler, F., Sutherland, D.~J., and Srebro, N.
\newblock Optimistic rates: A unifying theory for interpolation learning and
  regularization in linear regression.
\newblock \emph{arXiv preprint arXiv:2112.04470}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Koehler, Sur, Sutherland, and
  Srebro]{zhou2022non}
Zhou, L., Koehler, F., Sur, P., Sutherland, D.~J., and Srebro, N.
\newblock A non-asymptotic moreau envelope theory for high-dimensional
  generalized linear models.
\newblock \emph{arXiv preprint arXiv:2210.12082}, 2022.

\bibitem[Zou et~al.(2021{\natexlab{a}})Zou, Wu, Braverman, Gu, Foster, and
  Kakade]{zou2021benefits}
Zou, D., Wu, J., Braverman, V., Gu, Q., Foster, D.~P., and Kakade, S.
\newblock The benefits of implicit regularization from sgd in least squares
  problems.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 5456--5468, 2021{\natexlab{a}}.

\bibitem[Zou et~al.(2021{\natexlab{b}})Zou, Wu, Braverman, Gu, and
  Kakade]{zou2021benign}
Zou, D., Wu, J., Braverman, V., Gu, Q., and Kakade, S.
\newblock Benign overfitting of constant-stepsize sgd for linear regression.
\newblock In \emph{Conference on Learning Theory}, pp.\  4633--4635. PMLR,
  2021{\natexlab{b}}.

\end{thebibliography}
