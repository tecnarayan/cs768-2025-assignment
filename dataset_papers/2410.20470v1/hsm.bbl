\begin{thebibliography}{10}

\bibitem{abramson2024accurate}
J.~Abramson, J.~Adler, J.~Dunger, R.~Evans, T.~Green, A.~Pritzel, O.~Ronneberger, L.~Willmore, A.~J. Ballard, J.~Bambrick, et~al.
\newblock Accurate structure prediction of biomolecular interactions with alphafold 3.
\newblock {\em Nature}, pages 1--3, 2024.

\bibitem{akhound2024iterated}
T.~Akhound-Sadegh, J.~Rector-Brooks, A.~J. Bose, S.~Mittal, P.~Lemos, C.-H. Liu, M.~Sendera, S.~Ravanbakhsh, G.~Gidel, Y.~Bengio, et~al.
\newblock Iterated denoising energy matching for sampling from boltzmann densities.
\newblock {\em arXiv preprint arXiv:2402.06121}, 2024.

\bibitem{albergo2023stochastic}
M.~S. Albergo, N.~M. Boffi, and E.~Vanden-Eijnden.
\newblock Stochastic interpolants: A unifying framework for flows and diffusions.
\newblock {\em arXiv preprint arXiv:2303.08797}, 2023.

\bibitem{arnol2013mathematical}
V.~I. Arnol'd.
\newblock {\em Mathematical methods of classical mechanics}, volume~60.
\newblock Springer Science \& Business Media, 2013.

\bibitem{bou2018geometric}
N.~Bou-Rabee and J.~M. Sanz-Serna.
\newblock Geometric integrators and the hamiltonian monte carlo method.
\newblock {\em Acta Numerica}, 27:113--206, 2018.

\bibitem{chen2020vflow}
J.~Chen, C.~Lu, B.~Chenli, J.~Zhu, and T.~Tian.
\newblock Vflow: More expressive generative flows with variational data augmentation.
\newblock In {\em International Conference on Machine Learning}, pages 1660--1669. PMLR, 2020.

\bibitem{chen2018neural}
R.~T. Chen, Y.~Rubanova, J.~Bettencourt, and D.~K. Duvenaud.
\newblock Neural ordinary differential equations.
\newblock {\em Advances in neural information processing systems}, 31, 2018.

\bibitem{chen2014stochastic}
T.~Chen, E.~Fox, and C.~Guestrin.
\newblock Stochastic gradient hamiltonian monte carlo.
\newblock In {\em International conference on machine learning}, pages 1683--1691. PMLR, 2014.

\bibitem{chen2023generativeagm}
T.~Chen, J.~Gu, L.~Dinh, E.~A. Theodorou, J.~Susskind, and S.~Zhai.
\newblock Generative modeling with phase stochastic bridges.
\newblock {\em arXiv preprint arXiv:2310.07805}, 2023.

\bibitem{chi2023diffusion}
C.~Chi, S.~Feng, Y.~Du, Z.~Xu, E.~Cousineau, B.~Burchfiel, and S.~Song.
\newblock Diffusion policy: Visuomotor policy learning via action diffusion.
\newblock {\em arXiv preprint arXiv:2303.04137}, 2023.

\bibitem{corso2022diffdock}
G.~Corso, H.~St{\"a}rk, B.~Jing, R.~Barzilay, and T.~Jaakkola.
\newblock Diffdock: Diffusion steps, twists, and turns for molecular docking.
\newblock {\em arXiv preprint arXiv:2210.01776}, 2022.

\bibitem{de2024target}
V.~De~Bortoli, M.~Hutchinson, P.~Wirnsberger, and A.~Doucet.
\newblock Target score matching.
\newblock {\em arXiv preprint arXiv:2402.08667}, 2024.

\bibitem{dockhorn2021score}
T.~Dockhorn, A.~Vahdat, and K.~Kreis.
\newblock Score-based generative modeling with critically-damped langevin diffusion.
\newblock {\em arXiv preprint arXiv:2112.07068}, 2021.

\bibitem{duane1987hybrid}
S.~Duane, A.~D. Kennedy, B.~J. Pendleton, and D.~Roweth.
\newblock Hybrid monte carlo.
\newblock {\em Physics letters B}, 195(2):216--222, 1987.

\bibitem{foreman2021deep}
S.~Foreman, X.-Y. Jin, and J.~C. Osborn.
\newblock Deep learning hamiltonian monte carlo.
\newblock {\em arXiv preprint arXiv:2105.03418}, 2021.

\bibitem{gorham2017measuring}
J.~Gorham and L.~Mackey.
\newblock Measuring sample quality with kernels.
\newblock In {\em International Conference on Machine Learning}, pages 1292--1301. PMLR, 2017.

\bibitem{grathwohl2020learning}
W.~Grathwohl, K.-C. Wang, J.-H. Jacobsen, D.~Duvenaud, and R.~Zemel.
\newblock Learning the stein discrepancy for training and evaluating energy-based models without sampling.
\newblock In {\em International Conference on Machine Learning}, pages 3732--3747. PMLR, 2020.

\bibitem{greydanus2019hamiltonian}
S.~Greydanus, M.~Dzamba, and J.~Yosinski.
\newblock Hamiltonian neural networks.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{gulrajani2017improved}
I.~Gulrajani, F.~Ahmed, M.~Arjovsky, V.~Dumoulin, and A.~C. Courville.
\newblock Improved training of wasserstein gans.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{heusel2017gans}
M.~Heusel, H.~Ramsauer, T.~Unterthiner, B.~Nessler, and S.~Hochreiter.
\newblock Gans trained by a two time-scale update rule converge to a local nash equilibrium.
\newblock {\em Advances in neural information processing systems}, 30, 2017.

\bibitem{ho2020denoising}
J.~Ho, A.~Jain, and P.~Abbeel.
\newblock Denoising diffusion probabilistic models.
\newblock {\em Advances in neural information processing systems}, 33:6840--6851, 2020.

\bibitem{hoffman2019neutra}
M.~Hoffman, P.~Sountsov, J.~V. Dillon, I.~Langmore, D.~Tran, and S.~Vasudevan.
\newblock Neutra-lizing bad geometry in hamiltonian monte carlo using neural transport.
\newblock {\em arXiv preprint arXiv:1903.03704}, 2019.

\bibitem{hoffman2014no}
M.~D. Hoffman, A.~Gelman, et~al.
\newblock The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo.
\newblock {\em J. Mach. Learn. Res.}, 15(1):1593--1623, 2014.

\bibitem{hutchinson1989stochastic}
M.~F. Hutchinson.
\newblock A stochastic estimator of the trace of the influence matrix for laplacian smoothing splines.
\newblock {\em Communications in Statistics-Simulation and Computation}, 18(3):1059--1076, 1989.

\bibitem{hyvarinen2005estimation}
A.~Hyv{\"a}rinen and P.~Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock {\em Journal of Machine Learning Research}, 6(4), 2005.

\bibitem{karras2022elucidating}
T.~Karras, M.~Aittala, T.~Aila, and S.~Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock {\em Advances in Neural Information Processing Systems}, 35:26565--26577, 2022.

\bibitem{Karras2020TrainingGA}
T.~Karras, M.~Aittala, J.~Hellsten, S.~Laine, J.~Lehtinen, and T.~Aila.
\newblock Training generative adversarial networks with limited data.
\newblock {\em ArXiv}, abs/2006.06676, 2020.

\bibitem{karras2020analyzing}
T.~Karras, S.~Laine, M.~Aittala, J.~Hellsten, J.~Lehtinen, and T.~Aila.
\newblock Analyzing and improving the image quality of stylegan.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 8110--8119, 2020.

\bibitem{koster2009estimating}
U.~K{\"o}ster, J.~T. Lindgren, and A.~Hyv{\"a}rinen.
\newblock Estimating markov random field potentials for natural images.
\newblock In {\em International Conference on Independent Component Analysis and Signal Separation}, pages 515--522. Springer, 2009.

\bibitem{levy2017generalizing}
D.~Levy, M.~D. Hoffman, and J.~Sohl-Dickstein.
\newblock Generalizing hamiltonian monte carlo with neural networks.
\newblock {\em arXiv preprint arXiv:1711.09268}, 2017.

\bibitem{lipman2022flow}
Y.~Lipman, R.~T. Chen, H.~Ben-Hamu, M.~Nickel, and M.~Le.
\newblock Flow matching for generative modeling.
\newblock {\em arXiv preprint arXiv:2210.02747}, 2022.

\bibitem{liu2022flow}
X.~Liu, C.~Gong, and Q.~Liu.
\newblock Flow straight and fast: Learning to generate and transfer data with rectified flow.
\newblock {\em arXiv preprint arXiv:2209.03003}, 2022.

\bibitem{marsden2013introduction}
J.~E. Marsden and T.~S. Ratiu.
\newblock {\em Introduction to mechanics and symmetry: a basic exposition of classical mechanical systems}, volume~17.
\newblock Springer Science \& Business Media, 2013.

\bibitem{Martens2012EstimatingTH}
J.~Martens, I.~Sutskever, and K.~Swersky.
\newblock Estimating the hessian by back-propagating curvature.
\newblock In {\em International Conference on Machine Learning}, 2012.

\bibitem{miyato2018spectral}
T.~Miyato, T.~Kataoka, M.~Koyama, and Y.~Yoshida.
\newblock Spectral normalization for generative adversarial networks.
\newblock {\em arXiv preprint arXiv:1802.05957}, 2018.

\bibitem{neal2011mcmc}
R.~M. Neal et~al.
\newblock Mcmc using hamiltonian dynamics.
\newblock {\em Handbook of markov chain monte carlo}, 2(11):2, 2011.

\bibitem{oksendal2003stochastic}
B.~{\O}ksendal and B.~{\O}ksendal.
\newblock {\em Stochastic differential equations}.
\newblock Springer, 2003.

\bibitem{paszke2019pytorch}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen, Z.~Lin, N.~Gimelshein, L.~Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{rombach2022high}
R.~Rombach, A.~Blattmann, D.~Lorenz, P.~Esser, and B.~Ommer.
\newblock High-resolution image synthesis with latent diffusion models.
\newblock In {\em Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, pages 10684--10695, 2022.

\bibitem{shaul2023bespoke}
N.~Shaul, J.~Perez, R.~T. Chen, A.~Thabet, A.~Pumarola, and Y.~Lipman.
\newblock Bespoke solvers for generative flow models.
\newblock {\em arXiv preprint arXiv:2310.19075}, 2023.

\bibitem{sohl2015deep}
J.~Sohl-Dickstein, E.~Weiss, N.~Maheswaranathan, and S.~Ganguli.
\newblock Deep unsupervised learning using nonequilibrium thermodynamics.
\newblock In {\em International conference on machine learning}, pages 2256--2265. PMLR, 2015.

\bibitem{song2019generative}
Y.~Song and S.~Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock {\em Advances in neural information processing systems}, 32, 2019.

\bibitem{song2020sliced}
Y.~Song, S.~Garg, J.~Shi, and S.~Ermon.
\newblock Sliced score matching: A scalable approach to density and score estimation.
\newblock In {\em Uncertainty in Artificial Intelligence}, pages 574--584. PMLR, 2020.

\bibitem{song2021train}
Y.~Song and D.~P. Kingma.
\newblock How to train your energy-based models.
\newblock {\em arXiv preprint arXiv:2101.03288}, 2021.

\bibitem{song2020score}
Y.~Song, J.~Sohl-Dickstein, D.~P. Kingma, A.~Kumar, S.~Ermon, and B.~Poole.
\newblock Score-based generative modeling through stochastic differential equations.
\newblock {\em arXiv preprint arXiv:2011.13456}, 2020.

\bibitem{stein1972bound}
C.~Stein.
\newblock A bound for the error in the normal approximation to the distribution of a sum of dependent random variables.
\newblock In {\em Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability, Volume 2: Probability Theory}, volume~6, pages 583--603. University of California Press, 1972.

\bibitem{Vahdat2021ScorebasedGM}
A.~Vahdat, K.~Kreis, and J.~Kautz.
\newblock Score-based generative modeling in latent space.
\newblock In {\em Neural Information Processing Systems}, 2021.

\bibitem{vincent2011connection}
P.~Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock {\em Neural computation}, 23(7):1661--1674, 2011.

\bibitem{watson2023novo}
J.~L. Watson, D.~Juergens, N.~R. Bennett, B.~L. Trippe, J.~Yim, H.~E. Eisenach, W.~Ahern, A.~J. Borst, R.~J. Ragotte, L.~F. Milles, et~al.
\newblock De novo design of protein structure and function with rfdiffusion.
\newblock {\em Nature}, 620(7976):1089--1100, 2023.

\bibitem{Xu2022PoissonFG}
Y.~Xu, Z.~Liu, M.~Tegmark, and T.~Jaakkola.
\newblock Poisson flow generative models.
\newblock {\em ArXiv}, abs/2209.11178, 2022.

\bibitem{Xu2023PFGMUT}
Y.~Xu, Z.~Liu, Y.~Tian, S.~Tong, M.~Tegmark, and T.~Jaakkola.
\newblock Pfgm++: Unlocking the potential of physics-inspired generative models.
\newblock In {\em International Conference on Machine Learning}, 2023.

\bibitem{xu2023stable}
Y.~Xu, S.~Tong, and T.~Jaakkola.
\newblock Stable target field for reduced variance score estimation in diffusion models.
\newblock {\em arXiv preprint arXiv:2302.00670}, 2023.

\end{thebibliography}
