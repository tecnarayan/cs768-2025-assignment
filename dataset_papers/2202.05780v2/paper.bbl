\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ba et~al.(2016)Ba, Hinton, Mnih, Leibo, and Ionescu]{ba2016using}
Ba, J., Hinton, G.~E., Mnih, V., Leibo, J.~Z., and Ionescu, C.
\newblock Using fast weights to attend to the recent past.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pp.\  4331--4339, Barcelona, Spain, December 2016.

\bibitem[Bellemare et~al.(2016)Bellemare, Ostrovski, Guez, Thomas, and
  Munos]{BellemareOGTM16}
Bellemare, M.~G., Ostrovski, G., Guez, A., Thomas, P.~S., and Munos, R.
\newblock Increasing the action gap: New operators for reinforcement learning.
\newblock In \emph{Proc. {AAAI} Conf. on Artificial Intelligence}, pp.\
  1476--1483, Phoenix, {AZ}, {USA}, February 2016. {AAAI} Press.

\bibitem[Chen et~al.(2021)Chen, Liu, Xu, Darrell, and Wang]{chen2021meta}
Chen, Y., Liu, Z., Xu, H., Darrell, T., and Wang, X.
\newblock Meta-baseline: exploring simple meta-learning for few-shot learning.
\newblock In \emph{Proc. IEEE Int. Conf. on Computer Vision (ICCV)}, pp.\
  9062--9071, Virtual only, March 2021.

\bibitem[Choromanski et~al.(2021)Choromanski, Likhosherstov, Dohan, Song, Gane,
  Sarlos, Hawkins, Davis, Mohiuddin, Kaiser, et~al.]{choromanski2020rethinking}
Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T.,
  Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et~al.
\newblock Rethinking attention with performers.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Virtual
  only, 2021.

\bibitem[Cobbe et~al.(2020)Cobbe, Hesse, Hilton, and Schulman]{CobbeHHS20}
Cobbe, K., Hesse, C., Hilton, J., and Schulman, J.
\newblock Leveraging procedural generation to benchmark reinforcement learning.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  2048--2056, Virtual only, July 2020.

\bibitem[Cotter \& Conwell(1990)Cotter and Conwell]{cotter1990fixed}
Cotter, N.~E. and Conwell, P.~R.
\newblock Fixed-weight networks can learn.
\newblock In \emph{Proc. Int. Joint Conf. on Neural Networks (IJCNN)}, pp.\
  553--559, San Diego, {CA}, {USA}, June 1990.

\bibitem[Cotter \& Conwell(1991)Cotter and Conwell]{cotter1991learning}
Cotter, N.~E. and Conwell, P.~R.
\newblock Learning algorithms and fixed dynamics.
\newblock In \emph{Proc. Int. Joint Conf. on Neural Networks (IJCNN)}, pp.\
  799--801, Seattle, {WA}, {USA}, July 1991.

\bibitem[Csord{\'a}s et~al.(2022)Csord{\'a}s, Irie, and
  Schmidhuber]{csordas2021neural}
Csord{\'a}s, R., Irie, K., and Schmidhuber, J.
\newblock The neural data router: Adaptive control flow in transformers
  improves systematic generalization.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Virtual
  only, April 2022.

\bibitem[Deleu et~al.(2019)Deleu, W{\"u}rfl, Samiei, Cohen, and
  Bengio]{deleu2019torchmeta}
Deleu, T., W{\"u}rfl, T., Samiei, M., Cohen, J.~P., and Bengio, Y.
\newblock Torchmeta: A meta-learning library for {P}y{T}orch.
\newblock \emph{Preprint arXiv:1909.06576}, 2019.

\bibitem[Duan et~al.(2016)Duan, Schulman, Chen, Bartlett, Sutskever, and
  Abbeel]{duan2016rl}
Duan, Y., Schulman, J., Chen, X., Bartlett, P.~L., Sutskever, I., and Abbeel,
  P.
\newblock {RL}$^2$: Fast reinforcement learning via slow reinforcement
  learning.
\newblock \emph{Preprint arXiv:1611.02779}, 2016.

\bibitem[Eagleman(2020)]{eagleman2020livewired}
Eagleman, D.
\newblock \emph{Livewired: The inside story of the ever-changing brain}.
\newblock 2020.

\bibitem[Eden et~al.(2013)Eden, Moor, Soraker, and Steinhart]{Eden2013}
Eden, A.~H., Moor, J.~H., Soraker, J.~H., and Steinhart, E.
\newblock \emph{Singularity Hypotheses: A Scientific and Philosophical
  Assessment}.
\newblock Springer, 2013.

\bibitem[Espeholt et~al.(2018)Espeholt, Soyer, Munos, Simonyan, Mnih, Ward,
  Doron, Firoiu, Harley, Dunning, Legg, and Kavukcuoglu]{EspeholtSMSMWDF18}
Espeholt, L., Soyer, H., Munos, R., Simonyan, K., Mnih, V., Ward, T., Doron,
  Y., Firoiu, V., Harley, T., Dunning, I., Legg, S., and Kavukcuoglu, K.
\newblock {IMPALA:} scalable distributed deep-{RL} with importance weighted
  actor-learner architectures.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  1406--1415, Stockholm, Sweden, July 2018.

\bibitem[Finn \& Levine(2018)Finn and Levine]{FinnL18}
Finn, C. and Levine, S.
\newblock Meta-learning and universality: Deep representations and gradient
  descent can approximate any learning algorithm.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Vancouver,
  Canada, April 2018.

\bibitem[Finn et~al.(2017)Finn, Abbeel, and Levine]{FinnAL17}
Finn, C., Abbeel, P., and Levine, S.
\newblock Model-agnostic meta-learning for fast adaptation of deep networks.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  1126--1135, Sydney, Australia, August 2017.

\bibitem[Fodor et~al.(1988)Fodor, Pylyshyn, et~al.]{fodor1988connectionism}
Fodor, J.~A., Pylyshyn, Z.~W., et~al.
\newblock Connectionism and cognitive architecture: A critical analysis.
\newblock \emph{Cognition}, 28\penalty0 (1-2):\penalty0 3--71, 1988.

\bibitem[Gers et~al.(2000)Gers, Schmidhuber, and Cummins]{gers2000learning}
Gers, F.~A., Schmidhuber, J., and Cummins, F.
\newblock Learning to forget: Continual prediction with {LSTM}.
\newblock \emph{Neural computation}, 12\penalty0 (10):\penalty0 2451--2471,
  2000.

\bibitem[Good(1965)]{good1965}
Good, I.
\newblock Speculations concerning the first ultraintelligent machine.
\newblock \emph{Advances in Computers}, pp.\  31--88, 1965.

\bibitem[Hanson(1990)]{hanson1990stochastic}
Hanson, S.~J.
\newblock A stochastic version of the delta rule.
\newblock \emph{Physica D: Nonlinear Phenomena}, 42\penalty0 (1-3):\penalty0
  265--272, 1990.

\bibitem[Hochreiter \& Schmidhuber(1997)Hochreiter and
  Schmidhuber]{hochreiter1997long}
Hochreiter, S. and Schmidhuber, J.
\newblock Long short-term memory.
\newblock \emph{Neural computation}, 9\penalty0 (8):\penalty0 1735--1780, 1997.

\bibitem[Hochreiter et~al.(2001)Hochreiter, Younger, and
  Conwell]{hochreiter2001learning}
Hochreiter, S., Younger, A.~S., and Conwell, P.~R.
\newblock Learning to learn using gradient descent.
\newblock In \emph{Proc. Int. Conf. on Artificial Neural Networks ({ICANN})},
  volume 2130, pp.\  87--94, Vienna, Austria, August 2001.

\bibitem[Hubinger et~al.(2019)Hubinger, van Merwijk, Mikulik, Skalse, and
  Garrabrant]{hubinger2019risks}
Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S.
\newblock Risks from learned optimization in advanced machine learning systems.
\newblock \emph{Preprint arXiv:1906.01820}, 2019.

\bibitem[Irie et~al.(2018)Irie, Kumar, Nirschl, and Liao]{irie18:radmm}
Irie, K., Kumar, S., Nirschl, M., and Liao, H.
\newblock {RADMM}: Recurrent adaptive mixture model with applications to domain
  robust language modeling.
\newblock In \emph{Proc. {IEEE} Int. Conf. on Acoustics, Speech and Signal
  Processing (ICASSP)}, pp.\  6079--6083, Calgary, Canada, April 2018.

\bibitem[Irie et~al.(2021)Irie, Schlag, Csord\'as, and
  Schmidhuber]{irie2021going}
Irie, K., Schlag, I., Csord\'as, R., and Schmidhuber, J.
\newblock Going beyond linear transformers with recurrent fast weight
  programmers.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, 2021.

\bibitem[Katharopoulos et~al.(2020)Katharopoulos, Vyas, Pappas, and
  Fleuret]{katharopoulos2020transformers}
Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F.
\newblock Transformers are {RNN}s: Fast autoregressive transformers with linear
  attention.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Virtual only,
  July 2020.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{Preprint arXiv:1412.6980}, 2014.

\bibitem[Kirsch \& Schmidhuber(2021)Kirsch and Schmidhuber]{kirsch2020meta}
Kirsch, L. and Schmidhuber, J.
\newblock Meta-learning backpropagation and improving it.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, 2021.

\bibitem[Krause et~al.(2018)Krause, Kahembwe, Murray, and Renals]{KrauseK0R18}
Krause, B., Kahembwe, E., Murray, I., and Renals, S.
\newblock Dynamic evaluation of neural sequence models.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  2771--2780, Stockholm, Sweden, July 2018.

\bibitem[Krizhevsky(2009)]{krizhevsky}
Krizhevsky, A.
\newblock Learning multiple layers of features from tiny images.
\newblock Master's thesis, Computer Science Department, University of Toronto,
  2009.

\bibitem[K{\"u}ttler et~al.(2019)K{\"u}ttler, Nardelli, Lavril, Selvatici,
  Sivakumar, Rockt{\"a}schel, and Grefenstette]{kuttler2019torchbeast}
K{\"u}ttler, H., Nardelli, N., Lavril, T., Selvatici, M., Sivakumar, V.,
  Rockt{\"a}schel, T., and Grefenstette, E.
\newblock Torchbeast: A {P}y{T}orch platform for distributed {RL}.
\newblock \emph{Preprint arXiv:1910.03552}, 2019.

\bibitem[Lake et~al.(2015)Lake, Salakhutdinov, and Tenenbaum]{lake2015human}
Lake, B.~M., Salakhutdinov, R., and Tenenbaum, J.~B.
\newblock Human-level concept learning through probabilistic program induction.
\newblock \emph{Science}, 350\penalty0 (6266):\penalty0 1332--1338, 2015.

\bibitem[Lazaridou et~al.(2021)Lazaridou, Kuncoro, Gribovskaya, Agrawal, Liska,
  Terzi, Gimenez, d'Autume, Ruder, Yogatama, et~al.]{lazaridou2021pitfalls}
Lazaridou, A., Kuncoro, A., Gribovskaya, E., Agrawal, D., Liska, A., Terzi, T.,
  Gimenez, M., d'Autume, C. d.~M., Ruder, S., Yogatama, D., et~al.
\newblock Pitfalls of static language modelling.
\newblock \emph{Preprint arXiv:2102.01951}, 2021.

\bibitem[Lin et~al.(2021)Lin, Shi, Pathak, and Ramanan]{lin2021clear}
Lin, Z., Shi, J., Pathak, D., and Ramanan, D.
\newblock The {CLEAR} {B}enchmark: {C}ontinual {LEA}rning on {R}eal-{W}orld
  {I}magery.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS), Track on Datasets and Benchmarks}, Virtual only, December 2021.

\bibitem[Miconi et~al.(2018)Miconi, Stanley, and Clune]{miconi18a}
Miconi, T., Stanley, K., and Clune, J.
\newblock Differentiable plasticity: training plastic neural networks with
  backpropagation.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  3559--3568, Stockholm, Sweden, July 2018.

\bibitem[Miconi et~al.(2019)Miconi, Rawal, Clune, and
  Stanley]{miconi2018backpropamine}
Miconi, T., Rawal, A., Clune, J., and Stanley, K.~O.
\newblock Backpropamine: training self-modifying neural networks with
  differentiable neuromodulated plasticity.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, New Orleans,
  LA, USA, May 2019.

\bibitem[Mikolov et~al.(2010)Mikolov, Karafi{\'a}t, Burget, Cernock{\'y}, and
  Khudanpur]{mikolov2010recurrent}
Mikolov, T., Karafi{\'a}t, M., Burget, L., Cernock{\'y}, J., and Khudanpur, S.
\newblock Recurrent neural network based language model.
\newblock In \emph{Proc. Interspeech}, pp.\  1045--1048, Makuhari, Japan,
  September 2010.

\bibitem[Mishra et~al.(2018)Mishra, Rohaninejad, Chen, and Abbeel]{mishra2018a}
Mishra, N., Rohaninejad, M., Chen, X., and Abbeel, P.
\newblock A simple neural attentive meta-learner.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Vancouver,
  Cananda, 2018.

\bibitem[Munkhdalai \& Trischler(2018)Munkhdalai and
  Trischler]{munkhdalai2018metalearning}
Munkhdalai, T. and Trischler, A.
\newblock Metalearning with hebbian fast weights.
\newblock \emph{Preprint arXiv:1807.05076}, 2018.

\bibitem[Munkhdalai \& Yu(2017)Munkhdalai and Yu]{MunkhdalaiY17}
Munkhdalai, T. and Yu, H.
\newblock Meta networks.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  2554--2563, Sydney, Australia, August 2017.

\bibitem[Munkhdalai et~al.(2019)Munkhdalai, Sordoni, Wang, and
  Trischler]{MunkhdalaiSWT19}
Munkhdalai, T., Sordoni, A., Wang, T., and Trischler, A.
\newblock Metalearned neural memory.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  13310--13321, Vancouver, Canada, December 2019.

\bibitem[Najarro \& Risi(2020)Najarro and Risi]{NajarroR20}
Najarro, E. and Risi, S.
\newblock Meta-learning through hebbian plasticity in random networks.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, December 2020.

\bibitem[Nivel \& Th{\'o}risson(2009)Nivel and Th{\'o}risson]{nivel2009self}
Nivel, E. and Th{\'o}risson, K.~R.
\newblock Self-programming: Operationalizing autonomy.
\newblock In \emph{Proc. Conf. on Artificial General Intelligence (AGI)}, pp.\
  150--155, Arlington, VA, USA, March 2009.

\bibitem[Oreshkin et~al.(2018)Oreshkin, L{\'{o}}pez, and Lacoste]{OreshkinLL18}
Oreshkin, B.~N., L{\'{o}}pez, P.~R., and Lacoste, A.
\newblock {TADAM:} task dependent adaptive metric for improved few-shot
  learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, pp.\  719--729, Montr{\'{e}}al, Canada, December 2018.

\bibitem[Peng et~al.(2021)Peng, Pappas, Yogatama, Schwartz, Smith, and
  Kong]{peng2021random}
Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N.~A., and Kong, L.
\newblock Random feature attention.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Virtual
  only, 2021.

\bibitem[Ravi \& Larochelle(2017)Ravi and Larochelle]{RaviL17}
Ravi, S. and Larochelle, H.
\newblock Optimization as a model for few-shot learning.
\newblock In \emph{Int. Conf. on Learning Representations (ICLR)}, Toulon,
  France, April 2017.

\bibitem[Sandler et~al.(2021)Sandler, Vladymyrov, Zhmoginov, Miller, Madams,
  Jackson, and y~Arcas]{sandler21}
Sandler, M., Vladymyrov, M., Zhmoginov, A., Miller, N., Madams, T., Jackson,
  A., and y~Arcas, B.~A.
\newblock Meta-learning bidirectional update rules.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  9288--9300, Virtual only, July 2021.

\bibitem[Santoro et~al.(2016)Santoro, Bartunov, Botvinick, Wierstra, and
  Lillicrap]{SantoroBBWL16}
Santoro, A., Bartunov, S., Botvinick, M., Wierstra, D., and Lillicrap, T.~P.
\newblock Meta-learning with memory-augmented neural networks.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  1842--1850, New York City, {NY}, {USA}, June 2016.

\bibitem[Schlag et~al.(2021)Schlag, Irie, and Schmidhuber]{schlag2021linear}
Schlag, I., Irie, K., and Schmidhuber, J.
\newblock Linear {T}ransformers are secretly fast weight programmers.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, Virtual only,
  July 2021.

\bibitem[Schmidgall(2020)]{Schmidgall20}
Schmidgall, S.
\newblock Adaptive reinforcement learning through evolving self-modifying
  neural networks.
\newblock In \emph{Proc. Genetic and Evolutionary Computation Conference
  {(GECCO)}, Companion Volume}, pp.\  89--90, Canc{\'{u}}n, Mexico, July 2020.

\bibitem[Schmidhuber(1987)]{Schmidhuber:87long}
Schmidhuber, J.
\newblock {Evolutionary principles in self-referential learning, or on learning
  how to learn: the meta-meta-... hook. Institut f\"{u}r Informatik, Technische
  Universit\"{a}t M\"{u}nchen}, 1987.
\newblock http://www.idsia.ch/\~{ }juergen/diploma.html.

\bibitem[Schmidhuber(1990)]{Schmidhuber:90diffenglish}
Schmidhuber, J.
\newblock Making the world differentiable: On using fully recurrent
  self-supervised neural networks for dynamic reinforcement learning and
  planning in non-stationary environments.
\newblock Technical Report FKI-126-90,
  \url{http://people.idsia.ch/~juergen/FKI-126-90_(revised)bw_ocr.pdf}, Tech.
  Univ. Munich, 1990.

\bibitem[Schmidhuber(1991)]{Schmidhuber:91fastweights}
Schmidhuber, J.
\newblock Learning to control fast-weight memories: An alternative to recurrent
  nets.
\newblock Technical Report FKI-147-91, Institut f\"{u}r Informatik, Technische
  Universit\"{a}t M\"{u}nchen, March 1991.

\bibitem[Schmidhuber(1992{\natexlab{a}})]{Schmidhuber:92selfref}
Schmidhuber, J.
\newblock Steps towards ``self-referential'' learning.
\newblock Technical Report CU-CS-627-92, Dept. of Comp. Sci., University of
  Colorado at Boulder, November 1992{\natexlab{a}}.

\bibitem[Schmidhuber(1992{\natexlab{b}})]{schmidhuber1992learning}
Schmidhuber, J.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock \emph{Neural Computation}, 4\penalty0 (1):\penalty0 131--139,
  1992{\natexlab{b}}.

\bibitem[Schmidhuber(1993{\natexlab{a}})]{Schmidhuber:93selfrefann}
Schmidhuber, J.
\newblock An introspective network that can learn to run its own weight change
  algorithm.
\newblock In \emph{Proc. {IEE} Int. Conf. on Artificial Neural Networks}, pp.\
  191--195, Brighton, {UK}, May 1993{\natexlab{a}}.

\bibitem[Schmidhuber(1993{\natexlab{b}})]{Schmidhuber:93selfreficann}
Schmidhuber, J.
\newblock A self-referential weight matrix.
\newblock In \emph{Proc. Int. Conf. on Artificial Neural Networks (ICANN)},
  pp.\  446--451, Amsterdam, Netherlands, September 1993{\natexlab{b}}.

\bibitem[Schmidhuber(1993{\natexlab{c}})]{Schmidhuber:93selfrefieee}
Schmidhuber, J.
\newblock A neural network that embeds its own meta-levels.
\newblock In \emph{Proc. {IEEE} Int. Conf. on Neural Networks (ICNN)}, San
  Francisco, {CA}, {USA}, March 1993{\natexlab{c}}.

\bibitem[Schmidhuber(1993{\natexlab{d}})]{schmidhuber1993reducing}
Schmidhuber, J.
\newblock Reducing the ratio between learning complexity and number of time
  varying variables in fully recurrent nets.
\newblock In \emph{International Conference on Artificial Neural Networks
  (ICANN)}, pp.\  460--463, Amsterdam, Netherlands, September
  1993{\natexlab{d}}.

\bibitem[Schmidhuber(2006)]{Schmidhuber:04agi}
Schmidhuber, J.
\newblock G\"{o}del machines: Fully self-referential optimal universal
  self-improvers.
\newblock In \emph{Artificial General Intelligence}. Springer, 2006.

\bibitem[Solomonoff(1964)]{solomonoff1964formal}
Solomonoff, R.~J.
\newblock A formal theory of inductive inference. {P}art {I}.
\newblock \emph{Information and control}, 7\penalty0 (1):\penalty0 1--22, 1964.

\bibitem[Srivastava et~al.(2014)Srivastava, Hinton, Krizhevsky, Sutskever, and
  Salakhutdinov]{srivastava2014dropout}
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov,
  R.
\newblock Dropout: a simple way to prevent neural networks from overfitting.
\newblock \emph{The Journal of Machine Learning Research}, 15\penalty0
  (1):\penalty0 1929--1958, 2014.

\bibitem[Steunebrink et~al.(2016)Steunebrink, Th{\'{o}}risson, and
  Schmidhuber]{SteunebrinkTS16}
Steunebrink, B.~R., Th{\'{o}}risson, K.~R., and Schmidhuber, J.
\newblock Growing recursive self-improvers.
\newblock In \emph{Proc. Conf. on Artificial General Intelligence (AGI)}, pp.\
  129--139, New York, NY, USA, July 2016.

\bibitem[Tolstikhin et~al.(2021)Tolstikhin, Houlsby, Kolesnikov, Beyer, Zhai,
  Unterthiner, Yung, Steiner, Keysers, Uszkoreit, et~al.]{tolstikhin2021mlp}
Tolstikhin, I.~O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X.,
  Unterthiner, T., Yung, J., Steiner, A., Keysers, D., Uszkoreit, J., et~al.
\newblock {MLP}-mixer: An all-{MLP} architecture for vision.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NeurIPS)}, Virtual only, December 2021.

\bibitem[Tompson et~al.(2015)Tompson, Goroshin, Jain, LeCun, and
  Bregler]{TompsonGJLB15}
Tompson, J., Goroshin, R., Jain, A., LeCun, Y., and Bregler, C.
\newblock Efficient object localization using convolutional networks.
\newblock In \emph{Proc. {IEEE} Conf. on Computer Vision and Pattern
  Recognition (CVPR)}, pp.\  648--656, Boston, {MA}, {USA}, June 2015.

\bibitem[Vaswani et~al.(2017)Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez,
  Kaiser, and Polosukhin]{trafo}
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.~N.,
  Kaiser, {\L}., and Polosukhin, I.
\newblock Attention is all you need.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pp.\  5998--6008, Long Beach, {CA}, {USA}, December 2017.

\bibitem[Vinyals et~al.(2016)Vinyals, Blundell, Lillicrap, Kavukcuoglu, and
  Wierstra]{VinyalsBLKW16}
Vinyals, O., Blundell, C., Lillicrap, T., Kavukcuoglu, K., and Wierstra, D.
\newblock Matching networks for one shot learning.
\newblock In \emph{Proc. Advances in Neural Information Processing Systems
  (NIPS)}, pp.\  3630--3638, Barcelona, Spain, December 2016.

\bibitem[Waibel et~al.(1989)Waibel, Hanazawa, Hinton, Shikano, and
  Lang]{waibel1989phoneme}
Waibel, A., Hanazawa, T., Hinton, G., Shikano, K., and Lang, K.~J.
\newblock Phoneme recognition using time-delay neural networks.
\newblock \emph{IEEE Transactions on Acoustics, Speech, and Signal Processing},
  37\penalty0 (3):\penalty0 328--339, 1989.

\bibitem[Wang et~al.(2017)Wang, Kurth{-}Nelson, Soyer, Leibo, Tirumala, Munos,
  Blundell, Kumaran, and Botvinick]{wang2016learning}
Wang, J., Kurth{-}Nelson, Z., Soyer, H., Leibo, J.~Z., Tirumala, D., Munos, R.,
  Blundell, C., Kumaran, D., and Botvinick, M.~M.
\newblock Learning to reinforcement learn.
\newblock In \emph{Proc. Annual Meeting of the Cognitive Science Society
  (CogSci)}, London, {UK}, July 2017.

\bibitem[Wang(2007)]{wang2007logic}
Wang, P.
\newblock The logic of intelligence.
\newblock In \emph{Artificial general intelligence}, pp.\  31--62. Springer,
  2007.

\bibitem[Wang et~al.(2018)Wang, Li, and Hammer]{wang2018self}
Wang, P., Li, X., and Hammer, P.
\newblock Self in {NARS}, an {AGI} system.
\newblock \emph{Frontiers in Robotics and AI}, 5:\penalty0 20, 2018.

\bibitem[Widrow \& Hoff(1960)Widrow and Hoff]{widrow1960adaptive}
Widrow, B. and Hoff, M.~E.
\newblock Adaptive switching circuits.
\newblock In \emph{Proc. {IRE} WESCON Convention Record}, pp.\  96--104, Los
  Angeles, {CA}, {USA}, August 1960.

\bibitem[Williams \& Peng(1990)Williams and Peng]{williams1990efficient}
Williams, R.~J. and Peng, J.
\newblock An efficient gradient-based algorithm for on-line training of
  recurrent network trajectories.
\newblock \emph{Neural computation}, 2\penalty0 (4):\penalty0 490--501, 1990.

\bibitem[Yap et~al.(2021)Yap, Ritter, and Barber]{yapRB21}
Yap, P.~C., Ritter, H., and Barber, D.
\newblock Addressing catastrophic forgetting in few-shot problems.
\newblock In \emph{Proc. Int. Conf. on Machine Learning (ICML)}, pp.\
  11909--11919, Virtual only, July 2021.

\bibitem[Zhmoginov et~al.(2022)Zhmoginov, Sandler, and
  Vladymyrov]{zhmoginov2022hypertransformer}
Zhmoginov, A., Sandler, M., and Vladymyrov, M.
\newblock Hypertransformer: Model generation for supervised and semi-supervised
  few-shot learning.
\newblock \emph{Preprint arXiv:2201.04182}, 2022.

\end{thebibliography}
