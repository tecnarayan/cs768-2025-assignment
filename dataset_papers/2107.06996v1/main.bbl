\begin{thebibliography}{49}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Adamic \& Glance(2005)Adamic and Glance]{adamic2005political}
Adamic, L.~A. and Glance, N.
\newblock The political blogosphere and the 2004 us election: divided they
  blog.
\newblock In \emph{Proceedings of the 3rd international workshop on Link
  discovery}, pp.\  36--43, 2005.

\bibitem[Aviles-Rivero et~al.(2019)Aviles-Rivero, Papadakis, Li, Alsaleh, Tan,
  and Schonlieb]{aviles2019labelled}
Aviles-Rivero, A.~I., Papadakis, N., Li, R., Alsaleh, S.~M., Tan, R.~T., and
  Schonlieb, C.-B.
\newblock When labelled data hurts: Deep semi-supervised classification with
  the graph 1-laplacian.
\newblock \emph{arXiv preprint arXiv:1906.08635}, 2019.

\bibitem[Bauschke \& Combettes(2011)Bauschke and Combettes]{10.5555/2028633}
Bauschke, H.~H. and Combettes, P.~L.
\newblock \emph{Convex Analysis and Monotone Operator Theory in Hilbert
  Spaces}.
\newblock Springer Publishing Company, Incorporated, 1st edition, 2011.
\newblock ISBN 1441994661.

\bibitem[Bresson et~al.(2013{\natexlab{a}})Bresson, Laurent, Uminsky, and von
  Brecht]{bresson2013adaptive}
Bresson, X., Laurent, T., Uminsky, D., and von Brecht, J.~H.
\newblock An adaptive total variation algorithm for computing the balanced cut
  of a graph, 2013{\natexlab{a}}.

\bibitem[Bresson et~al.(2013{\natexlab{b}})Bresson, Laurent, Uminsky, and
  Von~Brecht]{bresson2013multiclass}
Bresson, X., Laurent, T., Uminsky, D., and Von~Brecht, J.~H.
\newblock Multiclass total variation clustering.
\newblock \emph{arXiv preprint arXiv:1306.1185}, 2013{\natexlab{b}}.

\bibitem[B{\"u}hler \& Hein(2009)B{\"u}hler and Hein]{buhler2009spectral}
B{\"u}hler, T. and Hein, M.
\newblock Spectral clustering based on the graph p-laplacian.
\newblock In \emph{Proceedings of the 26th Annual International Conference on
  Machine Learning}, pp.\  81--88, 2009.

\bibitem[Chen et~al.(2013)Chen, Huang, and Zhang]{chen2013primal}
Chen, P., Huang, J., and Zhang, X.
\newblock A primal--dual fixed point algorithm for convex separable
  minimization with applications to image restoration.
\newblock \emph{Inverse Problems}, 29\penalty0 (2):\penalty0 025011, 2013.

\bibitem[Chen et~al.(2020)Chen, Eldar, and Zhao]{chen2020graph}
Chen, S., Eldar, Y.~C., and Zhao, L.
\newblock Graph unrolling networks: Interpretable neural networks for graph
  signal denoising, 2020.

\bibitem[Chung \& Graham(1997)Chung and Graham]{chung1997spectral}
Chung, F.~R. and Graham, F.~C.
\newblock \emph{Spectral graph theory}.
\newblock Number~92. American Mathematical Soc., 1997.

\bibitem[Defferrard et~al.(2016)Defferrard, Bresson, and
  Vandergheynst]{defferrard2016convolutional}
Defferrard, M., Bresson, X., and Vandergheynst, P.
\newblock Convolutional neural networks on graphs with fast localized spectral
  filtering.
\newblock In \emph{Proceedings of the 30th International Conference on Neural
  Information Processing Systems}, pp.\  3844--3852, 2016.

\bibitem[Elad(2010)]{elad2010sparse}
Elad, M.
\newblock \emph{Sparse and redundant representations: from theory to
  applications in signal and image processing}.
\newblock Springer Science \& Business Media, 2010.

\bibitem[Entezari et~al.(2020)Entezari, Al-Sayouri, Darvishzadeh, and
  Papalexakis]{all-you-need-is-low-rank}
Entezari, N., Al-Sayouri, S.~A., Darvishzadeh, A., and Papalexakis, E.~E.
\newblock All you need is low (rank) defending against adversarial attacks on
  graphs.
\newblock In \emph{WSDM}, 2020.

\bibitem[Gilmer et~al.(2017)Gilmer, Schoenholz, Riley, Vinyals, and
  Dahl]{gilmer2017neural}
Gilmer, J., Schoenholz, S.~S., Riley, P.~F., Vinyals, O., and Dahl, G.~E.
\newblock Neural message passing for quantum chemistry.
\newblock In \emph{International Conference on Machine Learning}, pp.\
  1263--1272. PMLR, 2017.

\bibitem[Hamilton et~al.(2017)Hamilton, Ying, and
  Leskovec]{hamilton2017inductive}
Hamilton, W.~L., Ying, R., and Leskovec, J.
\newblock Inductive representation learning on large graphs.
\newblock \emph{arXiv preprint arXiv:1706.02216}, 2017.

\bibitem[Hastie et~al.(2015)Hastie, Tibshirani, and Wainwright]{lassobook2015}
Hastie, T., Tibshirani, R., and Wainwright, M.
\newblock \emph{Statistical Learning with Sparsity: The Lasso and
  Generalizations}.
\newblock Chapman Hall/CRC, 2015.
\newblock ISBN 1498712169.

\bibitem[Jin et~al.(2020)Jin, Ma, Liu, Tang, Wang, and Tang]{jin2020graph}
Jin, W., Ma, Y., Liu, X., Tang, X., Wang, S., and Tang, J.
\newblock Graph structure learning for robust graph neural networks.
\newblock In \emph{Proceedings of the 26th ACM SIGKDD International Conference
  on Knowledge Discovery \& Data Mining}, pp.\  66--74, 2020.

\bibitem[Jung et~al.(2016)Jung, Hero~III, Mara, and Jahromi]{jung2016semi}
Jung, A., Hero~III, A.~O., Mara, A., and Jahromi, S.
\newblock Semi-supervised learning via sparse label propagation.
\newblock \emph{arXiv preprint arXiv:1612.01414}, 2016.

\bibitem[{Jung} et~al.(2019){Jung}, {Hero, III}, {Mara}, {Jahromi},
  {Heimowitz}, and {Eldar}]{semi_tv_min}
{Jung}, A., {Hero, III}, A.~O., {Mara}, A.~C., {Jahromi}, S., {Heimowitz}, A.,
  and {Eldar}, Y.~C.
\newblock Semi-supervised learning in network-structured data via total
  variation minimization.
\newblock \emph{IEEE Transactions on Signal Processing}, 67\penalty0
  (24):\penalty0 6256--6269, 2019.
\newblock \doi{10.1109/TSP.2019.2953593}.

\bibitem[Kim et~al.(2009)Kim, Koh, Boyd, and Gorinevsky]{kim2009ell_1}
Kim, S.-J., Koh, K., Boyd, S., and Gorinevsky, D.
\newblock $\ell_1$ trend filtering.
\newblock \emph{SIAM review}, 51\penalty0 (2):\penalty0 339--360, 2009.

\bibitem[Kingma \& Ba(2014)Kingma and Ba]{kingma2014adam}
Kingma, D.~P. and Ba, J.
\newblock Adam: A method for stochastic optimization.
\newblock \emph{arXiv preprint arXiv:1412.6980}, 2014.

\bibitem[Kipf \& Welling(2016)Kipf and Welling]{kipf2016semi}
Kipf, T.~N. and Welling, M.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock \emph{arXiv preprint arXiv:1609.02907}, 2016.

\bibitem[Klicpera et~al.(2018)Klicpera, Bojchevski, and
  G{\"u}nnemann]{klicpera2018predict}
Klicpera, J., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Predict then propagate: Graph neural networks meet personalized
  pagerank.
\newblock \emph{arXiv preprint arXiv:1810.05997}, 2018.

\bibitem[Li et~al.(2020)Li, Jin, Xu, and Tang]{li2020deeprobust}
Li, Y., Jin, W., Xu, H., and Tang, J.
\newblock Deeprobust: A pytorch library for adversarial attacks and defenses,
  2020.

\bibitem[Li \& Yan(2017)Li and Yan]{li2017primal}
Li, Z. and Yan, M.
\newblock A primal-dual algorithm with optimal stepsizes and its application in
  decentralized consensus optimization.
\newblock \emph{arXiv preprint arXiv:1711.06785}, 2017.

\bibitem[Loris \& Verhoeven(2011)Loris and Verhoeven]{loris2011generalization}
Loris, I. and Verhoeven, C.
\newblock On a generalization of the iterative soft-thresholding algorithm for
  the case of non-separable penalty.
\newblock \emph{Inverse Problems}, 27\penalty0 (12):\penalty0 125007, 2011.

\bibitem[Ma \& Tang(2020)Ma and Tang]{ma2020deep}
Ma, Y. and Tang, J.
\newblock \emph{Deep Learning on Graphs}.
\newblock Cambridge University Press, 2020.

\bibitem[Ma et~al.(2020)Ma, Liu, Zhao, Liu, Tang, and Shah]{ma2020unified}
Ma, Y., Liu, X., Zhao, T., Liu, Y., Tang, J., and Shah, N.
\newblock A unified view on graph neural networks as graph signal denoising.
\newblock \emph{arXiv preprint arXiv:2010.01777}, 2020.

\bibitem[Nie et~al.(2011)Nie, Wang, Huang, and Ding]{nie2011unsupervised}
Nie, F., Wang, H., Huang, H., and Ding, C.
\newblock Unsupervised and semi-supervised learning via $\ell_1$-norm graph.
\newblock In \emph{2011 International Conference on Computer Vision}, pp.\
  2268--2273. IEEE, 2011.

\bibitem[Nt \& Maehara(2019)Nt and Maehara]{nt2019revisiting}
Nt, H. and Maehara, T.
\newblock Revisiting graph neural networks: All we have is low-pass filters.
\newblock \emph{arXiv preprint arXiv:1905.09550}, 2019.

\bibitem[Pan et~al.(2020)Pan, Shiji, and Gao]{pan2020_unified}
Pan, X., Shiji, S., and Gao, H.
\newblock A unified framework for convolution-based graph neural networks.
\newblock \emph{https://openreview.net/forum?id=zUMD--Fb9Bt}, 2020.

\bibitem[Rudin et~al.(1992)Rudin, Osher, and Fatemi]{rudin1992nonlinear}
Rudin, L.~I., Osher, S., and Fatemi, E.
\newblock Nonlinear total variation based noise removal algorithms.
\newblock \emph{Physica D: nonlinear phenomena}, 60\penalty0 (1-4):\penalty0
  259--268, 1992.

\bibitem[Scarselli et~al.(2008)Scarselli, Gori, Tsoi, Hagenbuchner, and
  Monfardini]{scarselli2008graph}
Scarselli, F., Gori, M., Tsoi, A.~C., Hagenbuchner, M., and Monfardini, G.
\newblock The graph neural network model.
\newblock \emph{IEEE transactions on neural networks}, 20\penalty0
  (1):\penalty0 61--80, 2008.

\bibitem[Sen et~al.(2008)Sen, Namata, Bilgic, Getoor, Galligher, and
  Eliassi-Rad]{sen2008collective}
Sen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B., and Eliassi-Rad, T.
\newblock Collective classification in network data.
\newblock \emph{AI magazine}, 29\penalty0 (3):\penalty0 93--93, 2008.

\bibitem[Sharpnack et~al.(2012)Sharpnack, Singh, and
  Rinaldo]{sharpnack2012sparsistency}
Sharpnack, J., Singh, A., and Rinaldo, A.
\newblock Sparsistency of the edge lasso over graphs.
\newblock In \emph{Artificial Intelligence and Statistics}, pp.\  1028--1036.
  PMLR, 2012.

\bibitem[Shchur et~al.(2018)Shchur, Mumme, Bojchevski, and
  G{\"u}nnemann]{shchur2018pitfalls}
Shchur, O., Mumme, M., Bojchevski, A., and G{\"u}nnemann, S.
\newblock Pitfalls of graph neural network evaluation.
\newblock \emph{arXiv preprint arXiv:1811.05868}, 2018.

\bibitem[Szlam \& Bresson(2010)Szlam and Bresson]{szlam2010total}
Szlam, A. and Bresson, X.
\newblock Total variation, cheeger cuts.
\newblock In \emph{ICML}, 2010.

\bibitem[Tibshirani et~al.(2005)Tibshirani, Saunders, Rosset, Zhu, and
  Knight]{tibshirani2005sparsity}
Tibshirani, R., Saunders, M., Rosset, S., Zhu, J., and Knight, K.
\newblock Sparsity and smoothness via the fused lasso.
\newblock \emph{Journal of the Royal Statistical Society: Series B (Statistical
  Methodology)}, 67\penalty0 (1):\penalty0 91--108, 2005.

\bibitem[Tibshirani et~al.(2014)]{tibshirani2014adaptive}
Tibshirani, R.~J. et~al.
\newblock Adaptive piecewise polynomial estimation via trend filtering.
\newblock \emph{Annals of statistics}, 42\penalty0 (1):\penalty0 285--323,
  2014.

\bibitem[Varma et~al.(2019)Varma, Lee, Kova{\v{c}}evi{\'c}, and
  Chi]{varma2019vector}
Varma, R., Lee, H., Kova{\v{c}}evi{\'c}, J., and Chi, Y.
\newblock Vector-valued graph trend filtering with non-convex penalties.
\newblock \emph{IEEE Transactions on Signal and Information Processing over
  Networks}, 6:\penalty0 48--62, 2019.

\bibitem[Veli{\v{c}}kovi{\'c} et~al.(2017)Veli{\v{c}}kovi{\'c}, Cucurull,
  Casanova, Romero, Lio, and Bengio]{velivckovic2017graph}
Veli{\v{c}}kovi{\'c}, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., and
  Bengio, Y.
\newblock Graph attention networks.
\newblock \emph{arXiv preprint arXiv:1710.10903}, 2017.

\bibitem[Wang et~al.(2016)Wang, Sharpnack, Smola, and
  Tibshirani]{wang2016trend}
Wang, Y.-X., Sharpnack, J., Smola, A.~J., and Tibshirani, R.~J.
\newblock Trend filtering on graphs.
\newblock \emph{Journal of Machine Learning Research}, 17:\penalty0 1--41,
  2016.

\bibitem[Wu et~al.(2019)Wu, Souza, Zhang, Fifty, Yu, and
  Weinberger]{wu2019simplifying}
Wu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Weinberger, K.
\newblock Simplifying graph convolutional networks.
\newblock In \emph{International conference on machine learning}, pp.\
  6861--6871. PMLR, 2019.

\bibitem[Zhao \& Akoglu(2019)Zhao and Akoglu]{zhao2019pairnorm}
Zhao, L. and Akoglu, L.
\newblock Pairnorm: Tackling oversmoothing in gnns.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Zhou et~al.(2004)Zhou, Bousquet, Lal, Weston, and
  Sch{\"o}lkopf]{zhou2004learning}
Zhou, D., Bousquet, O., Lal, T.~N., Weston, J., and Sch{\"o}lkopf, B.
\newblock Learning with local and global consistency.
\newblock 2004.

\bibitem[Zhu et~al.(2021)Zhu, Wang, Shi, Ji, and Cui]{zhu2021interpreting}
Zhu, M., Wang, X., Shi, C., Ji, H., and Cui, P.
\newblock Interpreting and unifying graph neural networks with an optimization
  framework, 2021.

\bibitem[Zhu \& Ghahramani()Zhu and Ghahramani]{zhu2002learning}
Zhu, X. and Ghahramani, Z.
\newblock Learning from labeled and unlabeled data with label propagation.

\bibitem[Zhu(2005)]{zhu2005semi}
Zhu, X.~J.
\newblock Semi-supervised learning literature survey.
\newblock 2005.

\bibitem[Z{\"u}gner \& G{\"u}nnemann(2019)Z{\"u}gner and
  G{\"u}nnemann]{mettack}
Z{\"u}gner, D. and G{\"u}nnemann, S.
\newblock Adversarial attacks on graph neural networks via meta learning.
\newblock \emph{arXiv preprint arXiv:1902.08412}, 2019.

\bibitem[Z{\"u}gner et~al.(2018)Z{\"u}gner, Akbarnejad, and
  G{\"u}nnemann]{nettack}
Z{\"u}gner, D., Akbarnejad, A., and G{\"u}nnemann, S.
\newblock Adversarial attacks on neural networks for graph data.
\newblock In \emph{KDD}. ACM, 2018.

\end{thebibliography}
