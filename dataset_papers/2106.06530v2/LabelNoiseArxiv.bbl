\begin{thebibliography}{32}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, Salakhutdinov, and
  Wang]{arora2019exact}
S.~Arora, S.~S. Du, W.~Hu, Z.~Li, R.~Salakhutdinov, and R.~Wang.
\newblock On exact computation with an infinitely wide neural net.
\newblock \emph{arXiv preprint arXiv:1904.11955}, 2019.

\bibitem[Biewald(2020)]{wandb}
L.~Biewald.
\newblock Experiment tracking with weights and biases, 2020.
\newblock URL \url{https://www.wandb.com/}.
\newblock Software available from wandb.com.

\bibitem[Blanc et~al.(2019)Blanc, Gupta, Valiant, and
  Valiant]{blanc2019implicit}
G.~Blanc, N.~Gupta, G.~Valiant, and P.~Valiant.
\newblock Implicit regularization for deep neural networks driven by an
  ornstein-uhlenbeck like process.
\newblock \emph{arXiv preprint arXiv:1904.09080}, 2019.

\bibitem[Cohen et~al.(2021)Cohen, Kaur, Li, Kolter, and
  Talwalkar]{cohen2021gradient}
J.~M. Cohen, S.~Kaur, Y.~Li, J.~Z. Kolter, and A.~Talwalkar.
\newblock Gradient descent on neural networks typically occurs at the edge of
  stability, 2021.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
S.~S. Du, J.~D. Lee, H.~Li, L.~Wang, and X.~Zhai.
\newblock Gradient descent finds global minima of deep neural networks, 2019.

\bibitem[{Falcon et al.}(2019)]{pytorchlightning2019}
W.~{Falcon et al.}
\newblock Pytorch lightning.
\newblock \emph{GitHub. Note:
  https://github.com/PyTorchLightning/pytorch-lightning}, 3, 2019.

\bibitem[Foret et~al.(2020)Foret, Kleiner, Mobahi, and
  Neyshabur]{foret2020sharpness}
P.~Foret, A.~Kleiner, H.~Mobahi, and B.~Neyshabur.
\newblock Sharpness-aware minimization for efficiently improving
  generalization.
\newblock \emph{arXiv preprint arXiv:2010.01412}, 2020.

\bibitem[Ge et~al.(2015)Ge, Huang, Jin, and Yuan]{ge2015escaping}
R.~Ge, F.~Huang, C.~Jin, and Y.~Yuan.
\newblock Escaping from saddle pointsâ€”online stochastic gradient for tensor
  decomposition.
\newblock In \emph{Conference on Learning Theory}, pages 797--842, 2015.

\bibitem[Goyal et~al.(2017)Goyal, Doll{\'a}r, Girshick, Noordhuis, Wesolowski,
  Kyrola, Tulloch, Jia, and He]{goyal2017accurate}
P.~Goyal, P.~Doll{\'a}r, R.~Girshick, P.~Noordhuis, L.~Wesolowski, A.~Kyrola,
  A.~Tulloch, Y.~Jia, and K.~He.
\newblock Accurate, large minibatch sgd: Training imagenet in 1 hour.
\newblock \emph{arXiv preprint arXiv:1706.02677}, 2017.

\bibitem[Gunasekar et~al.(2017)Gunasekar, Woodworth, Bhojanapalli, Neyshabur,
  and Srebro]{gunasekar2017implicit}
S.~Gunasekar, B.~E. Woodworth, S.~Bhojanapalli, B.~Neyshabur, and N.~Srebro.
\newblock Implicit regularization in matrix factorization.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  6151--6159, 2017.

\bibitem[Gunasekar et~al.(2018)Gunasekar, Lee, Soudry, and
  Srebro]{gunasekar2018characterizing}
S.~Gunasekar, J.~Lee, D.~Soudry, and N.~Srebro.
\newblock Characterizing implicit bias in terms of optimization geometry.
\newblock \emph{arXiv preprint arXiv:1802.08246}, 2018.

\bibitem[HaoChen et~al.(2020)HaoChen, Wei, Lee, and Ma]{haochen2020shape}
J.~Z. HaoChen, C.~Wei, J.~D. Lee, and T.~Ma.
\newblock Shape matters: Understanding the implicit bias of the noise
  covariance.
\newblock \emph{arXiv preprint arXiv:2006.08680}, 2020.

\bibitem[Hendrycks and Gimpel(2020)]{hendrycks2020gaussian}
D.~Hendrycks and K.~Gimpel.
\newblock Gaussian error linear units (gelus), 2020.

\bibitem[Jacot et~al.(2018)Jacot, Gabriel, and Hongler]{jacot2018neural}
A.~Jacot, F.~Gabriel, and C.~Hongler.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in neural information processing systems}, pages
  8571--8580, 2018.

\bibitem[Jin et~al.(2019)Jin, Netrapalli, Ge, Kakade, and
  Jordan]{jin2019stochastic}
C.~Jin, P.~Netrapalli, R.~Ge, S.~M. Kakade, and M.~I. Jordan.
\newblock Stochastic gradient descent escapes saddle points efficiently.
\newblock \emph{arXiv preprint arXiv:1902.04811}, 2019.

\bibitem[Keskar et~al.(2016)Keskar, Mudigere, Nocedal, Smelyanskiy, and
  Tang]{keskar2016large}
N.~S. Keskar, D.~Mudigere, J.~Nocedal, M.~Smelyanskiy, and P.~T.~P. Tang.
\newblock On large-batch training for deep learning: Generalization gap and
  sharp minima.
\newblock \emph{arXiv preprint arXiv:1609.04836}, 2016.

\bibitem[Krizhevsky(2009)]{Krizhevsky09learningmultiple}
A.~Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock Technical report, 2009.

\bibitem[LeCun et~al.(2012)LeCun, Bottou, Orr, and
  M{\"u}ller]{lecun2012efficient}
Y.~A. LeCun, L.~Bottou, G.~B. Orr, and K.-R. M{\"u}ller.
\newblock Efficient backprop.
\newblock In \emph{Neural networks: Tricks of the trade}, pages 9--48.
  Springer, 2012.

\bibitem[Li et~al.(2017)Li, Ma, and Zhang]{li2017algorithmic}
Y.~Li, T.~Ma, and H.~Zhang.
\newblock Algorithmic regularization in over-parameterized matrix sensing and
  neural networks with quadratic activations.
\newblock \emph{arXiv preprint arXiv:1712.09203}, 2017.

\bibitem[Li et~al.(2019)Li, Wei, and Ma]{li2019towards}
Y.~Li, C.~Wei, and T.~Ma.
\newblock Towards explaining the regularization effect of initial large
  learning rate in training neural networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  11669--11680, 2019.

\bibitem[Liu et~al.(2019)Liu, Papailiopoulos, and Achlioptas]{liu2019bad}
S.~Liu, D.~Papailiopoulos, and D.~Achlioptas.
\newblock Bad global minima exist and sgd can reach them.
\newblock \emph{arXiv preprint arXiv:1906.02613}, 2019.

\bibitem[Mianjy et~al.(2018)Mianjy, Arora, and Vidal]{mianjy2018implicit}
P.~Mianjy, R.~Arora, and R.~Vidal.
\newblock On the implicit bias of dropout.
\newblock \emph{arXiv preprint arXiv:1806.09777}, 2018.

\bibitem[Moroshko et~al.(2020)Moroshko, Gunasekar, Woodworth, Lee, Srebro, and
  Soudry]{moroshko2020implicit}
E.~Moroshko, S.~Gunasekar, B.~Woodworth, J.~D. Lee, N.~Srebro, and D.~Soudry.
\newblock Implicit bias in deep linear classification: Initialization scale vs
  training accuracy.
\newblock \emph{Neural Information Processing Systems (NeurIPS)}, 2020.

\bibitem[Paszke et~al.(2019)Paszke, Gross, Massa, Lerer, Bradbury, Chanan,
  Killeen, Lin, Gimelshein, Antiga, Desmaison, Kopf, Yang, DeVito, Raison,
  Tejani, Chilamkurthy, Steiner, Fang, Bai, and Chintala]{pytorch2019}
A.~Paszke, S.~Gross, F.~Massa, A.~Lerer, J.~Bradbury, G.~Chanan, T.~Killeen,
  Z.~Lin, N.~Gimelshein, L.~Antiga, A.~Desmaison, A.~Kopf, E.~Yang, Z.~DeVito,
  M.~Raison, A.~Tejani, S.~Chilamkurthy, B.~Steiner, L.~Fang, J.~Bai, and
  S.~Chintala.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock In H.~Wallach, H.~Larochelle, A.~Beygelzimer, F.~d\textquotesingle
  Alch\'{e}-Buc, E.~Fox, and R.~Garnett, editors, \emph{Advances in Neural
  Information Processing Systems 32}, pages 8024--8035. Curran Associates,
  Inc., 2019.
\newblock URL
  \url{http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}.

\bibitem[Shallue et~al.(2018)Shallue, Lee, Antognini, Sohl-Dickstein, Frostig,
  and Dahl]{shallue2018measuring}
C.~J. Shallue, J.~Lee, J.~Antognini, J.~Sohl-Dickstein, R.~Frostig, and G.~E.
  Dahl.
\newblock Measuring the effects of data parallelism on neural network training.
\newblock \emph{arXiv preprint arXiv:1811.03600}, 2018.

\bibitem[Smith et~al.(2017)Smith, Kindermans, Ying, and Le]{smith2017don}
S.~L. Smith, P.-J. Kindermans, C.~Ying, and Q.~V. Le.
\newblock Don't decay the learning rate, increase the batch size.
\newblock \emph{arXiv preprint arXiv:1711.00489}, 2017.

\bibitem[Soudry et~al.(2018)Soudry, Hoffer, Nacson, Gunasekar, and
  Srebro]{soudry2018implicit}
D.~Soudry, E.~Hoffer, M.~S. Nacson, S.~Gunasekar, and N.~Srebro.
\newblock The implicit bias of gradient descent on separable data.
\newblock \emph{The Journal of Machine Learning Research}, 19\penalty0
  (1):\penalty0 2822--2878, 2018.

\bibitem[Szegedy et~al.(2016)Szegedy, Vanhoucke, Ioffe, Shlens, and
  Wojna]{szegedy2016rethinking}
C.~Szegedy, V.~Vanhoucke, S.~Ioffe, J.~Shlens, and Z.~Wojna.
\newblock Rethinking the inception architecture for computer vision.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 2818--2826, 2016.

\bibitem[Vaskevicius et~al.(2019)Vaskevicius, Kanade, and
  Rebeschini]{vaskevicius2019implicit}
T.~Vaskevicius, V.~Kanade, and P.~Rebeschini.
\newblock Implicit regularization for optimal sparse recovery.
\newblock In \emph{Advances in Neural Information Processing Systems}, pages
  2968--2979, 2019.

\bibitem[Wei and Ma(2019)]{wei2019improved}
C.~Wei and T.~Ma.
\newblock Improved sample complexities for deep networks and robust
  classification via an all-layer margin.
\newblock \emph{arXiv preprint arXiv:1910.04284}, 2019.

\bibitem[Wen et~al.(2019)Wen, Luk, Gazeau, Zhang, Chan, and
  Ba]{wen2019interplay}
Y.~Wen, K.~Luk, M.~Gazeau, G.~Zhang, H.~Chan, and J.~Ba.
\newblock Interplay between optimization and generalization of stochastic
  gradient descent with covariance noise.
\newblock \emph{arXiv preprint arXiv:1902.08234}, 2019.

\bibitem[Woodworth et~al.(2020)Woodworth, Gunasekar, Lee, Moroshko, Savarese,
  Golan, Soudry, and Srebro]{woodworth2020kernel}
B.~Woodworth, S.~Gunasekar, J.~D. Lee, E.~Moroshko, P.~Savarese, I.~Golan,
  D.~Soudry, and N.~Srebro.
\newblock Kernel and rich regimes in overparametrized models.
\newblock \emph{arXiv preprint arXiv:2002.09277}, 2020.

\end{thebibliography}
