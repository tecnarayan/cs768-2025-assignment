\begin{thebibliography}{58}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Allen-Zhu et~al.(2019{\natexlab{a}})Allen-Zhu, Li, and
  Song]{allen2019convergence}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock A convergence theory for deep learning via over-parameterization.
\newblock In \emph{International conference on machine learning}, pages
  242--252. PMLR, 2019{\natexlab{a}}.

\bibitem[Allen-Zhu et~al.(2019{\natexlab{b}})Allen-Zhu, Li, and
  Song]{allen2019convergencerecurr}
Zeyuan Allen-Zhu, Yuanzhi Li, and Zhao Song.
\newblock On the convergence rate of training recurrent neural networks.
\newblock \emph{Advances in neural information processing systems}, 32,
  2019{\natexlab{b}}.

\bibitem[Anand and Achim(2022)]{anand2022protein}
Namrata Anand and Tudor Achim.
\newblock Protein structure and sequence generation with equivariant denoising
  diffusion probabilistic models.
\newblock \emph{arXiv preprint arXiv:2205.15019}, 2022.

\bibitem[Anderson(1982)]{anderson1982reverse}
Brian~DO Anderson.
\newblock Reverse-time diffusion equation models.
\newblock \emph{Stochastic Processes and their Applications}, 12\penalty0
  (3):\penalty0 313--326, 1982.

\bibitem[Arora et~al.(2019)Arora, Du, Hu, Li, and Wang]{arora2019fine}
Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang.
\newblock Fine-grained analysis of optimization and generalization for
  overparameterized two-layer neural networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  322--332. PMLR, 2019.

\bibitem[Austin et~al.(2021)Austin, Johnson, Ho, Tarlow, and Van
  Den~Berg]{austin2021structured}
Jacob Austin, Daniel~D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van
  Den~Berg.
\newblock Structured denoising diffusion models in discrete state-spaces.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 17981--17993, 2021.

\bibitem[Baranchuk et~al.(2022)Baranchuk, Voynov, Rubachev, Khrulkov, and
  Babenko]{baranchuk2022labelefficient}
Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem
  Babenko.
\newblock Label-efficient semantic segmentation with diffusion models.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=SlxSY2UZQT}.

\bibitem[Benton et~al.(2024)Benton, De~Bortoli, Doucet, and
  Deligiannidis]{benton2024nearly}
Joe Benton, Valentin De~Bortoli, Arnaud Doucet, and George Deligiannidis.
\newblock Nearly d-linear convergence bounds for diffusion models via
  stochastic localization.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.

\bibitem[Block et~al.(2020)Block, Mroueh, and Rakhlin]{block2020generative}
Adam Block, Youssef Mroueh, and Alexander Rakhlin.
\newblock Generative modeling with denoising auto-encoders and langevin
  sampling.
\newblock \emph{arXiv preprint arXiv:2002.00107}, 2020.

\bibitem[Cai et~al.(2019)Cai, Yang, Lee, and Wang]{cai2019neural}
Qi~Cai, Zhuoran Yang, Jason~D Lee, and Zhaoran Wang.
\newblock Neural temporal-difference learning converges to global optima.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Cao et~al.(2024)Cao, Tan, Gao, Xu, Chen, Heng, and Li]{cao2024survey}
Hanqun Cao, Cheng Tan, Zhangyang Gao, Yilun Xu, Guangyong Chen, Pheng-Ann Heng,
  and Stan~Z Li.
\newblock A survey on generative diffusion models.
\newblock \emph{IEEE Transactions on Knowledge and Data Engineering}, 2024.

\bibitem[Chen et~al.(2023{\natexlab{a}})Chen, Lee, and Lu]{chen2023improved}
Hongrui Chen, Holden Lee, and Jianfeng Lu.
\newblock Improved analysis of score-based generative modeling: User-friendly
  bounds under minimal smoothness assumptions.
\newblock In \emph{International Conference on Machine Learning}, pages
  4735--4763. PMLR, 2023{\natexlab{a}}.

\bibitem[Chen et~al.(2023{\natexlab{b}})Chen, Huang, Zhao, and
  Wang]{chen2023score}
Minshuo Chen, Kaixuan Huang, Tuo Zhao, and Mengdi Wang.
\newblock Score approximation, estimation and distribution recovery of
  diffusion models on low-dimensional data.
\newblock In \emph{International Conference on Machine Learning}, pages
  4672--4712. PMLR, 2023{\natexlab{b}}.

\bibitem[Chen et~al.(2024)Chen, Mei, Fan, and Wang]{chen2024overview}
Minshuo Chen, Song Mei, Jianqing Fan, and Mengdi Wang.
\newblock An overview of diffusion models: Applications, guided generation,
  statistical rates and optimization.
\newblock \emph{arXiv preprint arXiv:2404.07771}, 2024.

\bibitem[Chen et~al.(2021)Chen, Zhang, Zen, Weiss, Norouzi, and
  Chan]{chen2021wavegrad}
Nanxin Chen, Yu~Zhang, Heiga Zen, Ron~J Weiss, Mohammad Norouzi, and William
  Chan.
\newblock Wavegrad: Estimating gradients for waveform generation.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=NsMLjcFaO8O}.

\bibitem[Chen et~al.(2023{\natexlab{c}})Chen, Chewi, Li, Li, Salim, and
  Zhang]{chen2022sampling}
Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru~R Zhang.
\newblock Sampling is as easy as learning the score: theory for diffusion
  models with minimal data assumptions.
\newblock \emph{ICLR}, 2023{\natexlab{c}}.

\bibitem[Chung and Ye(2022)]{chung2022score}
Hyungjin Chung and Jong~Chul Ye.
\newblock Score-based diffusion models for accelerated mri.
\newblock \emph{Medical image analysis}, 80:\penalty0 102479, 2022.

\bibitem[Conforti et~al.(2023)Conforti, Durmus, and Silveri]{conforti2023score}
Giovanni Conforti, Alain Durmus, and Marta~Gentiloni Silveri.
\newblock Score diffusion models without early stopping: finite fisher
  information is all you need.
\newblock \emph{arXiv preprint arXiv:2308.12240}, 2023.

\bibitem[De~Bortoli(2022)]{de2022convergence}
Valentin De~Bortoli.
\newblock Convergence of denoising diffusion models under the manifold
  hypothesis.
\newblock \emph{TMLR}, 2022.

\bibitem[Dhariwal and Nichol(2021)]{dhariwal2021diffusion}
Prafulla Dhariwal and Alexander Nichol.
\newblock Diffusion models beat gans on image synthesis.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 8780--8794, 2021.

\bibitem[Du et~al.(2019)Du, Lee, Li, Wang, and Zhai]{du2019gradient}
Simon Du, Jason Lee, Haochuan Li, Liwei Wang, and Xiyu Zhai.
\newblock Gradient descent finds global minima of deep neural networks.
\newblock In \emph{International conference on machine learning}, pages
  1675--1685. PMLR, 2019.

\bibitem[Du et~al.(2018)Du, Zhai, Poczos, and Singh]{du2018gradient}
Simon~S Du, Xiyu Zhai, Barnabas Poczos, and Aarti Singh.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock \emph{arXiv preprint arXiv:1810.02054}, 2018.

\bibitem[Duan et~al.(2023)Duan, Du, Jia, and Kulik]{duan2023accurate}
Chenru Duan, Yuanqi Du, Haojun Jia, and Heather~J Kulik.
\newblock Accurate transition state generation with an object-aware equivariant
  elementary reaction diffusion model.
\newblock \emph{Nature Computational Science}, 3\penalty0 (12):\penalty0
  1045--1055, 2023.

\bibitem[Gao and Zhu(2024)]{gao2024convergence}
Xuefeng Gao and Lingjiong Zhu.
\newblock Convergence analysis for general probability flow odes of diffusion
  models in wasserstein distances.
\newblock \emph{arXiv preprint arXiv:2401.17958}, 2024.

\bibitem[Han et~al.(2024)Han, Razaviyayn, and Xu]{han2024neural}
Yinbin Han, Meisam Razaviyayn, and Renyuan Xu.
\newblock Neural network-based score estimation in diffusion models:
  Optimization and generalization.
\newblock In \emph{The Twelfth International Conference on Learning
  Representations}, 2024.
\newblock URL \url{https://openreview.net/forum?id=h8GeqOxtd4}.

\bibitem[He et~al.(2024)He, Rojas, and Tao]{he2024zeroth}
Ye~He, Kevin Rojas, and Molei Tao.
\newblock Zeroth-order sampling methods for non-log-concave distributions:
  Alleviating metastability by denoising diffusion.
\newblock \emph{arXiv preprint arXiv:2402.17886}, 2024.

\bibitem[Ho et~al.(2022{\natexlab{a}})Ho, Saharia, Chan, Fleet, Norouzi, and
  Salimans]{ho2022cascaded}
Jonathan Ho, Chitwan Saharia, William Chan, David~J Fleet, Mohammad Norouzi,
  and Tim Salimans.
\newblock Cascaded diffusion models for high fidelity image generation.
\newblock \emph{Journal of Machine Learning Research}, 23\penalty0
  (47):\penalty0 1--33, 2022{\natexlab{a}}.

\bibitem[Ho et~al.(2022{\natexlab{b}})Ho, Salimans, Gritsenko, Chan, Norouzi,
  and Fleet]{ho2022video}
Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi,
  and David~J Fleet.
\newblock Video diffusion models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 8633--8646, 2022{\natexlab{b}}.

\bibitem[Hyv{\"a}rinen and Dayan(2005)]{hyvarinen2005estimation}
Aapo Hyv{\"a}rinen and Peter Dayan.
\newblock Estimation of non-normalized statistical models by score matching.
\newblock \emph{Journal of Machine Learning Research}, 6\penalty0 (4), 2005.

\bibitem[Karras et~al.(2022)Karras, Aittala, Aila, and
  Laine]{karras2022elucidating}
Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
\newblock Elucidating the design space of diffusion-based generative models.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 26565--26577, 2022.

\bibitem[Karras et~al.(2023)Karras, Aittala, Lehtinen, Hellsten, Aila, and
  Laine]{karras2023analyzing}
Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and
  Samuli Laine.
\newblock Analyzing and improving the training dynamics of diffusion models.
\newblock \emph{arXiv preprint arXiv:2312.02696}, 2023.

\bibitem[Lee et~al.(2022)Lee, Lu, and Tan]{lee2022convergence}
Holden Lee, Jianfeng Lu, and Yixin Tan.
\newblock Convergence for score-based generative modeling with polynomial
  complexity.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 22870--22882, 2022.

\bibitem[Lee and Kim(2014)]{lee2014concise}
Yongjae Lee and Woo~Chang Kim.
\newblock Concise formulas for the surface area of the intersection of two
  hyperspherical caps.
\newblock \emph{KAIST Technical Report}, 2014.

\bibitem[Li et~al.(2022)Li, Thickstun, Gulrajani, Liang, and
  Hashimoto]{li2022diffusion}
Xiang Li, John Thickstun, Ishaan Gulrajani, Percy~S Liang, and Tatsunori~B
  Hashimoto.
\newblock Diffusion-lm improves controllable text generation.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 4328--4343, 2022.

\bibitem[Li and Liang(2018)]{li2018learning}
Yuanzhi Li and Yingyu Liang.
\newblock Learning overparameterized neural networks via stochastic gradient
  descent on structured data.
\newblock \emph{Advances in neural information processing systems}, 31, 2018.

\bibitem[Liu et~al.(2022)Liu, Pan, and Tao]{liu2022provable}
Xin Liu, Zhisong Pan, and Wei Tao.
\newblock Provable convergence of nesterov’s accelerated gradient method for
  over-parameterized neural networks.
\newblock \emph{Knowledge-Based Systems}, 251:\penalty0 109277, 2022.

\bibitem[Lou et~al.(2023)Lou, Meng, and Ermon]{lou2023discrete}
Aaron Lou, Chenlin Meng, and Stefano Ermon.
\newblock Discrete diffusion language modeling by estimating the ratios of the
  data distribution.
\newblock \emph{arXiv preprint arXiv:2310.16834}, 2023.

\bibitem[Meng et~al.(2022)Meng, He, Song, Song, Wu, Zhu, and
  Ermon]{meng2022sdedit}
Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and
  Stefano Ermon.
\newblock {SDE}dit: Guided image synthesis and editing with stochastic
  differential equations.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=aBsCjcPu_tE}.

\bibitem[Oko et~al.(2023)Oko, Akiyama, and Suzuki]{oko2023diffusion}
Kazusato Oko, Shunta Akiyama, and Taiji Suzuki.
\newblock Diffusion models are minimax optimal distribution estimators.
\newblock In \emph{International Conference on Machine Learning}, pages
  26517--26582. PMLR, 2023.

\bibitem[Peebles and Xie(2023)]{peebles2023scalable}
William Peebles and Saining Xie.
\newblock Scalable diffusion models with transformers.
\newblock In \emph{Proceedings of the IEEE/CVF International Conference on
  Computer Vision}, pages 4195--4205, 2023.

\bibitem[Ramesh et~al.(2022)Ramesh, Dhariwal, Nichol, Chu, and
  Chen]{ramesh2022hierarchical}
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.
\newblock Hierarchical text-conditional image generation with clip latents.
\newblock \emph{arXiv preprint arXiv:2204.06125}, 1\penalty0 (2):\penalty0 3,
  2022.

\bibitem[Ronneberger et~al.(2015)Ronneberger, Fischer, and
  Brox]{ronneberger2015u}
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.
\newblock U-net: Convolutional networks for biomedical image segmentation.
\newblock In \emph{Medical image computing and computer-assisted
  intervention--MICCAI 2015: 18th international conference, Munich, Germany,
  October 5-9, 2015, proceedings, part III 18}, pages 234--241. Springer, 2015.

\bibitem[Schneuing et~al.(2022)Schneuing, Du, Harris, Jamasb, Igashov, Du,
  Blundell, Li{\'o}, Gomes, Welling, et~al.]{schneuing2022structure}
Arne Schneuing, Yuanqi Du, Charles Harris, Arian Jamasb, Ilia Igashov, Weitao
  Du, Tom Blundell, Pietro Li{\'o}, Carla Gomes, Max Welling, et~al.
\newblock Structure-based drug design with equivariant diffusion models.
\newblock \emph{arXiv preprint arXiv:2210.13695}, 2022.

\bibitem[Shah et~al.(2023)Shah, Chen, and Klivans]{shah2023learning}
Kulin Shah, Sitan Chen, and Adam Klivans.
\newblock Learning mixtures of gaussians using the ddpm objective.
\newblock \emph{Advances in Neural Information Processing Systems},
  36:\penalty0 19636--19649, 2023.

\bibitem[Song and Ermon(2019)]{song2019generative}
Yang Song and Stefano Ermon.
\newblock Generative modeling by estimating gradients of the data distribution.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Song et~al.(2021)Song, Sohl-Dickstein, Kingma, Kumar, Ermon, and
  Poole]{song2020score}
Yang Song, Jascha Sohl-Dickstein, Diederik~P Kingma, Abhishek Kumar, Stefano
  Ermon, and Ben Poole.
\newblock Score-based generative modeling through stochastic differential
  equations.
\newblock \emph{International Conference on Learning Representations}, 2021.

\bibitem[Song and Yang(2019)]{song2019quadratic}
Zhao Song and Xin Yang.
\newblock Quadratic suffices for over-parametrization via matrix chernoff
  bound.
\newblock \emph{arXiv preprint arXiv:1906.03593}, 2019.

\bibitem[Vincent(2011)]{vincent2011connection}
Pascal Vincent.
\newblock A connection between score matching and denoising autoencoders.
\newblock \emph{Neural computation}, 23\penalty0 (7):\penalty0 1661--1674,
  2011.

\bibitem[Watson et~al.(2023)Watson, Juergens, Bennett, Trippe, Yim, Eisenach,
  Ahern, Borst, Ragotte, Milles, et~al.]{watson2023novo}
Joseph~L Watson, David Juergens, Nathaniel~R Bennett, Brian~L Trippe, Jason
  Yim, Helen~E Eisenach, Woody Ahern, Andrew~J Borst, Robert~J Ragotte, Lukas~F
  Milles, et~al.
\newblock De novo design of protein structure and function with rfdiffusion.
\newblock \emph{Nature}, 620\penalty0 (7976):\penalty0 1089--1100, 2023.

\bibitem[Wibisono et~al.(2024)Wibisono, Wu, and Yang]{wibisono2024optimal}
Andre Wibisono, Yihong Wu, and Kaylee~Yingxi Yang.
\newblock Optimal score estimation via empirical bayes smoothing.
\newblock \emph{arXiv preprint arXiv:2402.07747}, 2024.

\bibitem[Wu et~al.(2023)Wu, FU, Fang, Zhang, Yang, Xiong, Liu, and
  Xu]{wu2023medsegdiff}
Junde Wu, RAO FU, Huihui Fang, Yu~Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu,
  and Yanwu Xu.
\newblock Medsegdiff: Medical image segmentation with diffusion probabilistic
  model.
\newblock In \emph{Medical Imaging with Deep Learning}, 2023.
\newblock URL \url{https://openreview.net/forum?id=Jdw-cm2jG9}.

\bibitem[Yang and Wibisono(2022)]{yang2022convergence}
Kaylee~Yingxi Yang and Andre Wibisono.
\newblock Convergence in {KL} and {R}\'enyi divergence of the unadjusted
  langevin algorithm using estimated score.
\newblock \emph{NeurIPS Workshop on Score-Based Methods}, 2022.

\bibitem[Yang et~al.(2023)Yang, Zhang, Song, Hong, Xu, Zhao, Zhang, Cui, and
  Yang]{yang2023diffusion}
Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao
  Zhang, Bin Cui, and Ming-Hsuan Yang.
\newblock Diffusion models: A comprehensive survey of methods and applications.
\newblock \emph{ACM Computing Surveys}, 56\penalty0 (4):\penalty0 1--39, 2023.

\bibitem[Yang et~al.(2024)Yang, Wang, Jiang, and Li]{yangconvergence}
Ruofeng Yang, Zhijie Wang, Bo~Jiang, and Shuai Li.
\newblock The convergence of variance exploding diffusion models under the
  manifold hypothesis.
\newblock \emph{OpenReview}, 2024.

\bibitem[Yoon et~al.(2021)Yoon, Hwang, and Lee]{yoon2021adversarial}
Jongmin Yoon, Sung~Ju Hwang, and Juho Lee.
\newblock Adversarial purification with score-based generative models.
\newblock In \emph{International Conference on Machine Learning}, pages
  12062--12072. PMLR, 2021.

\bibitem[Zhu et~al.(2024)Zhu, Chen, Theodorou, Chen, and Tao]{zhu2024quantum}
Yuchen Zhu, Tianrong Chen, Evangelos~A Theodorou, Xie Chen, and Molei Tao.
\newblock Quantum state generation with structure-preserving diffusion model.
\newblock \emph{arXiv preprint arXiv:2404.06336}, 2024.

\bibitem[Zou and Gu(2019)]{zou2019improved}
Difan Zou and Quanquan Gu.
\newblock An improved analysis of training over-parameterized deep neural
  networks.
\newblock \emph{Advances in neural information processing systems}, 32, 2019.

\bibitem[Zou et~al.(2020)Zou, Cao, Zhou, and Gu]{zou2020gradient}
Difan Zou, Yuan Cao, Dongruo Zhou, and Quanquan Gu.
\newblock Gradient descent optimizes over-parameterized deep relu networks.
\newblock \emph{Machine learning}, 109:\penalty0 467--492, 2020.

\end{thebibliography}
