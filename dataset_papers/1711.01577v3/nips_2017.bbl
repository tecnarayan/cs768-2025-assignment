\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Appleyard \bgroup \em et al.\egroup
  }{2016}]{appleyard2016optimizing}
Jeremy Appleyard, Tomas Kocisky, and Phil Blunsom.
\newblock Optimizing performance of recurrent neural networks on gpus.
\newblock {\em arXiv preprint arXiv:1604.01946}, 2016.

\bibitem[\protect\citeauthoryear{Arjovsky \bgroup \em et al.\egroup
  }{2016}]{arjovsky2016unitary}
Martin Arjovsky, Amar Shah, and Yoshua Bengio.
\newblock Unitary evolution recurrent neural networks.
\newblock In {\em ICML}, 2016.

\bibitem[\protect\citeauthoryear{Ba \bgroup \em et al.\egroup
  }{2016}]{ba2016layer}
Jimmy~Lei Ba, Jamie~Ryan Kiros, and Geoffrey~E Hinton.
\newblock Layer normalization.
\newblock {\em arXiv preprint arXiv:1607.06450}, 2016.

\bibitem[\protect\citeauthoryear{Bengio \bgroup \em et al.\egroup
  }{1994}]{bengio1994learning}
Yoshua Bengio, Patrice Simard, and Paolo Frasconi.
\newblock Learning long-term dependencies with gradient descent is difficult.
\newblock {\em IEEE TNN}, 5(2):157--166, 1994.

\bibitem[\protect\citeauthoryear{Bengio}{2009}]{bengio2009learning}
Yoshua Bengio.
\newblock Learning deep architectures for ai.
\newblock {\em Foundations and trends{\textregistered} in Machine Learning},
  2009.

\bibitem[\protect\citeauthoryear{Bertinetto \bgroup \em et al.\egroup
  }{2016}]{bertinetto2016learning}
Luca Bertinetto, Jo{\~a}o~F Henriques, Jack Valmadre, Philip Torr, and Andrea
  Vedaldi.
\newblock Learning feed-forward one-shot learners.
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Bradbury \bgroup \em et al.\egroup
  }{2017}]{bradbury2016quasi}
James Bradbury, Stephen Merity, Caiming Xiong, and Richard Socher.
\newblock Quasi-recurrent neural networks.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{Chang \bgroup \em et al.\egroup
  }{2017}]{chang2017dilated}
Shiyu Chang, Yang Zhang, Wei Han, Mo~Yu, Xiaoxiao Guo, Wei Tan, Xiaodong Cui,
  Michael Witbrock, Mark Hasegawa-Johnson, and Thomas Huang.
\newblock Dilated recurrent neural networks.
\newblock In {\em NIPS}, 2017.

\bibitem[\protect\citeauthoryear{Chen \bgroup \em et al.\egroup
  }{2016}]{chen2016combining}
Jianxu Chen, Lin Yang, Yizhe Zhang, Mark Alber, and Danny~Z Chen.
\newblock Combining fully convolutional and recurrent neural networks for 3d
  biomedical image segmentation.
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Chung \bgroup \em et al.\egroup
  }{2015}]{chung2015gated}
Junyoung Chung, Caglar Gulcehre, Kyunghyun Cho, and Yoshua Bengio.
\newblock Gated feedback recurrent neural networks.
\newblock In {\em ICML}, 2015.

\bibitem[\protect\citeauthoryear{Chung \bgroup \em et al.\egroup
  }{2017}]{chung2016hierarchical}
Junyoung Chung, Sungjin Ahn, and Yoshua Bengio.
\newblock Hierarchical multiscale recurrent neural networks.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{Collobert \bgroup \em et al.\egroup
  }{2011}]{collobert2011torch7}
Ronan Collobert, Koray Kavukcuoglu, and Cl{\'e}ment Farabet.
\newblock Torch7: A matlab-like environment for machine learning.
\newblock In {\em NIPS Workshop}, 2011.

\bibitem[\protect\citeauthoryear{Cooijmans \bgroup \em et al.\egroup
  }{2017}]{cooijmans2016recurrent}
Tim Cooijmans, Nicolas Ballas, C{\'e}sar Laurent, and Aaron Courville.
\newblock Recurrent batch normalization.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{De~Brabandere \bgroup \em et al.\egroup
  }{2016}]{de2016dynamic}
Bert De~Brabandere, Xu~Jia, Tinne Tuytelaars, and Luc Van~Gool.
\newblock Dynamic filter networks.
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Denil \bgroup \em et al.\egroup
  }{2013}]{denil2013predicting}
Misha Denil, Babak Shakibi, Laurent Dinh, Nando de~Freitas, et~al.
\newblock Predicting parameters in deep learning.
\newblock In {\em NIPS}, 2013.

\bibitem[\protect\citeauthoryear{Diamos \bgroup \em et al.\egroup
  }{2016}]{diamos2016persistent}
Greg Diamos, Shubho Sengupta, Bryan Catanzaro, Mike Chrzanowski, Adam Coates,
  Erich Elsen, Jesse Engel, Awni Hannun, and Sanjeev Satheesh.
\newblock Persistent rnns: Stashing recurrent weights on-chip.
\newblock In {\em ICML}, 2016.

\bibitem[\protect\citeauthoryear{Elman}{1990}]{elman1990finding}
Jeffrey~L Elman.
\newblock Finding structure in time.
\newblock {\em Cognitive science}, 14(2):179--211, 1990.

\bibitem[\protect\citeauthoryear{Garipov \bgroup \em et al.\egroup
  }{2016}]{garipov2016ultimate}
Timur Garipov, Dmitry Podoprikhin, Alexander Novikov, and Dmitry Vetrov.
\newblock Ultimate tensorization: compressing convolutional and fc layers
  alike.
\newblock In {\em NIPS Workshop}, 2016.

\bibitem[\protect\citeauthoryear{Gers \bgroup \em et al.\egroup
  }{2000}]{gers2000learning}
Felix~A Gers, J{\"u}rgen Schmidhuber, and Fred Cummins.
\newblock Learning to forget: Continual prediction with lstm.
\newblock {\em Neural computation}, 12(10):2451--2471, 2000.

\bibitem[\protect\citeauthoryear{Graves \bgroup \em et al.\egroup
  }{2013}]{graves2013speech}
Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton.
\newblock Speech recognition with deep recurrent neural networks.
\newblock In {\em ICASSP}, 2013.

\bibitem[\protect\citeauthoryear{Graves}{2013}]{graves2013generating}
Alex Graves.
\newblock Generating sequences with recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1308.0850}, 2013.

\bibitem[\protect\citeauthoryear{Graves}{2016}]{graves2016adaptive}
Alex Graves.
\newblock Adaptive computation time for recurrent neural networks.
\newblock {\em arXiv preprint arXiv:1603.08983}, 2016.

\bibitem[\protect\citeauthoryear{Ha \bgroup \em et al.\egroup
  }{2017}]{ha2016hypernetworks}
David Ha, Andrew Dai, and Quoc~V Le.
\newblock Hypernetworks.
\newblock In {\em ICLR}, 2017.

\bibitem[\protect\citeauthoryear{Hochreiter and
  Schmidhuber}{1997}]{hochreiter1997long}
Sepp Hochreiter and J{\"u}rgen Schmidhuber.
\newblock Long short-term memory.
\newblock {\em Neural computation}, 9(8):1735--1780, 1997.

\bibitem[\protect\citeauthoryear{Hutter}{2012}]{hutterhuman}
Marcus Hutter.
\newblock The human knowledge compression contest.
\newblock {\em URL http://prize.hutter1.net}, 2012.

\bibitem[\protect\citeauthoryear{Irsoy and Cardie}{2015}]{irsoy2014modeling}
Ozan Irsoy and Claire Cardie.
\newblock Modeling compositionality with multiplicative recurrent neural
  networks.
\newblock In {\em ICLR}, 2015.

\bibitem[\protect\citeauthoryear{Jozefowicz \bgroup \em et al.\egroup
  }{2015}]{jozefowicz2015empirical}
Rafal Jozefowicz, Wojciech Zaremba, and Ilya Sutskever.
\newblock An empirical exploration of recurrent network architectures.
\newblock In {\em ICML}, 2015.

\bibitem[\protect\citeauthoryear{Kaiser and Bengio}{2016}]{kaiser2016can}
{\L}ukasz Kaiser and Samy Bengio.
\newblock Can active memory replace attention?
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Kaiser and Sutskever}{2016}]{kaiser2015neural}
{\L}ukasz Kaiser and Ilya Sutskever.
\newblock Neural gpus learn algorithms.
\newblock In {\em ICLR}, 2016.

\bibitem[\protect\citeauthoryear{Kalchbrenner \bgroup \em et al.\egroup
  }{2016}]{kalchbrenner2015grid}
Nal Kalchbrenner, Ivo Danihelka, and Alex Graves.
\newblock Grid long short-term memory.
\newblock In {\em ICLR}, 2016.

\bibitem[\protect\citeauthoryear{Kingma and Ba}{2015}]{kingma2014adam}
Diederik Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization.
\newblock In {\em ICLR}, 2015.

\bibitem[\protect\citeauthoryear{Krause \bgroup \em et al.\egroup
  }{2017}]{krause2016multiplicative}
Ben Krause, Liang Lu, Iain Murray, and Steve Renals.
\newblock Multiplicative lstm for sequence modelling.
\newblock In {\em ICLR Workshop}, 2017.

\bibitem[\protect\citeauthoryear{Le \bgroup \em et al.\egroup
  }{2015}]{le2015simple}
Quoc~V Le, Navdeep Jaitly, and Geoffrey~E Hinton.
\newblock A simple way to initialize recurrent networks of rectified linear
  units.
\newblock {\em arXiv preprint arXiv:1504.00941}, 2015.

\bibitem[\protect\citeauthoryear{LeCun \bgroup \em et al.\egroup
  }{1989}]{lecun1989backpropagation}
Yann LeCun, Bernhard Boser, John~S Denker, Donnie Henderson, Richard~E Howard,
  Wayne Hubbard, and Lawrence~D Jackel.
\newblock Backpropagation applied to handwritten zip code recognition.
\newblock {\em Neural computation}, 1(4):541--551, 1989.

\bibitem[\protect\citeauthoryear{LeCun \bgroup \em et al.\egroup
  }{1998}]{lecun1998gradient}
Yann LeCun, L{\'e}on Bottou, Yoshua Bengio, and Patrick Haffner.
\newblock Gradient-based learning applied to document recognition.
\newblock {\em Proceedings of the IEEE}, 86(11):2278--2324, 1998.

\bibitem[\protect\citeauthoryear{Lei and Zhang}{2017}]{lei2017training}
Tao Lei and Yu~Zhang.
\newblock Training rnns as fast as cnns.
\newblock {\em arXiv preprint arXiv:1709.02755}, 2017.

\bibitem[\protect\citeauthoryear{Leifert \bgroup \em et al.\egroup
  }{2016}]{leifert2016cells}
Gundram Leifert, Tobias Strau{\ss}, Tobias Gr{\"u}ning, Welf Wustlich, and
  Roger Labahn.
\newblock Cells in multidimensional recurrent neural networks.
\newblock {\em JMLR}, 17(1):3313--3349, 2016.

\bibitem[\protect\citeauthoryear{Mujika \bgroup \em et al.\egroup
  }{2017}]{mujika2017fast}
Asier Mujika, Florian Meier, and Angelika Steger.
\newblock Fast-slow recurrent neural networks.
\newblock In {\em NIPS}, 2017.

\bibitem[\protect\citeauthoryear{Novikov \bgroup \em et al.\egroup
  }{2015}]{novikov2015tensorizing}
Alexander Novikov, Dmitrii Podoprikhin, Anton Osokin, and Dmitry~P Vetrov.
\newblock Tensorizing neural networks.
\newblock In {\em NIPS}, 2015.

\bibitem[\protect\citeauthoryear{Oord \bgroup \em et al.\egroup
  }{2016}]{oord2016wavenet}
Aaron van~den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,
  Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu.
\newblock Wavenet: A generative model for raw audio.
\newblock {\em arXiv preprint arXiv:1609.03499}, 2016.

\bibitem[\protect\citeauthoryear{Patraucean \bgroup \em et al.\egroup
  }{2016}]{patraucean2015spatio}
Viorica Patraucean, Ankur Handa, and Roberto Cipolla.
\newblock Spatio-temporal video autoencoder with differentiable memory.
\newblock In {\em ICLR Workshop}, 2016.

\bibitem[\protect\citeauthoryear{Romera-Paredes and
  Torr}{2016}]{romera2016recurrent}
Bernardino Romera-Paredes and Philip Hilaire~Sean Torr.
\newblock Recurrent instance segmentation.
\newblock In {\em ECCV}, 2016.

\bibitem[\protect\citeauthoryear{Rumelhart \bgroup \em et al.\egroup
  }{1986}]{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536, 1986.

\bibitem[\protect\citeauthoryear{Schmidhuber}{1992}]{schmidhuber1992learning2}
J{\"u}rgen Schmidhuber.
\newblock Learning to control fast-weight memories: An alternative to dynamic
  recurrent networks.
\newblock {\em Neural Computation}, 4(1):131--139, 1992.

\bibitem[\protect\citeauthoryear{Stollenga \bgroup \em et al.\egroup
  }{2015}]{stollenga2015parallel}
Marijn~F Stollenga, Wonmin Byeon, Marcus Liwicki, and Juergen Schmidhuber.
\newblock Parallel multi-dimensional lstm, with application to fast biomedical
  volumetric image segmentation.
\newblock In {\em NIPS}, 2015.

\bibitem[\protect\citeauthoryear{Sutskever \bgroup \em et al.\egroup
  }{2011}]{sutskever2011generating}
Ilya Sutskever, James Martens, and Geoffrey~E Hinton.
\newblock Generating text with recurrent neural networks.
\newblock In {\em ICML}, 2011.

\bibitem[\protect\citeauthoryear{Taylor and Hinton}{2009}]{taylor2009factored}
Graham~W Taylor and Geoffrey~E Hinton.
\newblock Factored conditional restricted boltzmann machines for modeling
  motion style.
\newblock In {\em ICML}, 2009.

\bibitem[\protect\citeauthoryear{van~den Oord \bgroup \em et al.\egroup
  }{2016}]{van2016pixel}
Aaron van~den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.
\newblock Pixel recurrent neural networks.
\newblock In {\em ICML}, 2016.

\bibitem[\protect\citeauthoryear{Wisdom \bgroup \em et al.\egroup
  }{2016}]{wisdom2016full}
Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le~Roux, and Les Atlas.
\newblock Full-capacity unitary recurrent neural networks.
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Wu \bgroup \em et al.\egroup
  }{2016a}]{wu2016deep}
Lin Wu, Chunhua Shen, and Anton van~den Hengel.
\newblock Deep recurrent convolutional networks for video-based person
  re-identification: An end-to-end approach.
\newblock {\em arXiv preprint arXiv:1606.01609}, 2016.

\bibitem[\protect\citeauthoryear{Wu \bgroup \em et al.\egroup
  }{2016b}]{wu2016multiplicative}
Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan Salakhutdinov.
\newblock On multiplicative integration with recurrent neural networks.
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Xingjian \bgroup \em et al.\egroup
  }{2015}]{xingjian2015convolutional}
SHI Xingjian, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and
  Wang-chun Woo.
\newblock Convolutional lstm network: A machine learning approach for
  precipitation nowcasting.
\newblock In {\em NIPS}, 2015.

\bibitem[\protect\citeauthoryear{Zhang \bgroup \em et al.\egroup
  }{2016}]{zhang2016architectural}
Saizheng Zhang, Yuhuai Wu, Tong Che, Zhouhan Lin, Roland Memisevic, Ruslan~R
  Salakhutdinov, and Yoshua Bengio.
\newblock Architectural complexity measures of recurrent neural networks.
\newblock In {\em NIPS}, 2016.

\bibitem[\protect\citeauthoryear{Zilly \bgroup \em et al.\egroup
  }{2017}]{zilly2016recurrent}
Julian~Georg Zilly, Rupesh~Kumar Srivastava, Jan Koutn{\'\i}k, and J{\"u}rgen
  Schmidhuber.
\newblock Recurrent highway networks.
\newblock In {\em ICML}, 2017.

\end{thebibliography}
