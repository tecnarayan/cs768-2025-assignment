\begin{thebibliography}{10}

\bibitem{devlin2018bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock {\em arXiv preprint arXiv:1810.04805}, 2018.

\bibitem{fedus2021switch}
William Fedus, Barret Zoph, and Noam Shazeer.
\newblock Switch transformers: Scaling to trillion parameter models with simple
  and efficient sparsity.
\newblock {\em arXiv preprint arXiv:2101.03961}, 2021.

\bibitem{chakrabarti2019backprop}
Ayan Chakrabarti and Benjamin Moseley.
\newblock Backprop with approximate activations for memory-efficient network
  training.
\newblock {\em arXiv preprint arXiv:1901.07988}, 2019.

\bibitem{fu2020don}
Fangcheng Fu, Yuzheng Hu, Yihan He, Jiawei Jiang, Yingxia Shao, Ce~Zhang, and
  Bin Cui.
\newblock Don’t waste your bits! squeeze activations and gradients for deep
  neural networks via tinyscript.
\newblock In {\em International Conference on Machine Learning}, pages
  3304--3314. PMLR, 2020.

\bibitem{chen2021actnn}
Jianfei Chen, Lianmin Zheng, Zhewei Yao, Dequan Wang, Ion Stoica, Michael~W
  Mahoney, and Joseph~E Gonzalez.
\newblock Actnn: Reducing training memory footprint via 2-bit activation
  compressed training.
\newblock In {\em International Conference on Machine Learning}, 2021.

\bibitem{evans2021ac}
R~David Evans and Tor Aamodt.
\newblock {AC-GC}: Lossy activation compression with guaranteed convergence.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{pan2021mesa}
Zizheng Pan, Peng Chen, Haoyu He, Jing Liu, Jianfei Cai, and Bohan Zhuang.
\newblock Mesa: A memory-saving training framework for transformers.
\newblock {\em arXiv preprint arXiv:2111.11124}, 2021.

\bibitem{evans2020jpeg}
R~David Evans, Lufei Liu, and Tor~M Aamodt.
\newblock Jpeg-act: accelerating deep learning via transform-based lossy
  compression.
\newblock In {\em 2020 ACM/IEEE 47th Annual International Symposium on Computer
  Architecture (ISCA)}, pages 860--873. IEEE, 2020.

\bibitem{jin2021novel}
Sian Jin, Guanpeng Li, Shuaiwen~Leon Song, and Dingwen Tao.
\newblock A novel memory-efficient deep learning training framework via
  error-bounded lossy compression.
\newblock In {\em 26th ACM SIGPLAN Symposium on Principles and Practice of
  Parallel Programming}, pages 485--487, 2021.

\bibitem{anonymous2022exact}
Anonymous.
\newblock {EXACT}: Scalable graph neural networks training via extreme
  activation compression.
\newblock In {\em Submitted to The Tenth International Conference on Learning
  Representations}, 2022.
\newblock under review.

\bibitem{vaswani2017attention}
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan~N Gomez, {\L}ukasz Kaiser, and Illia Polosukhin.
\newblock Attention is all you need.
\newblock In {\em Advances in neural information processing systems}, pages
  5998--6008, 2017.

\bibitem{kipf2016semi}
Thomas~N Kipf and Max Welling.
\newblock Semi-supervised classification with graph convolutional networks.
\newblock {\em arXiv preprint arXiv:1609.02907}, 2016.

\bibitem{micikevicius2018mixed}
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen,
  David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh
  Venkatesh, and Hao Wu.
\newblock Mixed precision training.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wu2018training}
Shuang Wu, Guoqi Li, Feng Chen, and Luping Shi.
\newblock Training and inference with integers in deep neural networks.
\newblock In {\em International Conference on Learning Representations}, 2018.

\bibitem{wang2018training}
Naigang Wang, Jungwook Choi, Daniel Brand, Chia-Yu Chen, and Kailash
  Gopalakrishnan.
\newblock Training deep neural networks with 8-bit floating point numbers.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  7675--7684, 2018.

\bibitem{banner2018scalable}
Ron Banner, Itay Hubara, Elad Hoffer, and Daniel Soudry.
\newblock Scalable methods for 8-bit training of neural networks.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  5145--5153, 2018.

\bibitem{chen2020statistical}
Jianfei Chen, Yu~Gai, Zhewei Yao, Michael~W Mahoney, and Joseph~E Gonzalez.
\newblock A statistical framework for low-bitwidth training of deep neural
  networks.
\newblock In {\em Advances in neural information processing systems}, 2020.

\bibitem{sun2020ultra}
Xiao Sun, Naigang Wang, Chia-Yu Chen, Jiamin Ni, Ankur Agrawal, Xiaodong Cui,
  Swagath Venkataramani, Kaoutar El~Maghraoui, Vijayalakshmi~Viji Srinivasan,
  and Kailash Gopalakrishnan.
\newblock Ultra-low precision 4-bit training of deep neural networks.
\newblock In {\em Advances in Neural Information Processing Systems},
  volume~33, 2020.

\bibitem{chen2016training}
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin.
\newblock Training deep nets with sublinear memory cost.
\newblock {\em arXiv preprint arXiv:1604.06174}, 2016.

\bibitem{jain2019checkmate}
Paras Jain, Ajay Jain, Aniruddha Nrusimha, Amir Gholami, Pieter Abbeel, Kurt
  Keutzer, Ion Stoica, and Joseph~E Gonzalez.
\newblock Checkmate: Breaking the memory wall with optimal tensor
  rematerialization.
\newblock {\em arXiv preprint arXiv:1910.02653}, 2019.

\bibitem{kirisame2020dynamic}
Marisa Kirisame, Steven Lyubomirsky, Altan Haan, Jennifer Brennan, Mike He,
  Jared Roesch, Tianqi Chen, and Zachary Tatlock.
\newblock Dynamic tensor rematerialization.
\newblock {\em arXiv preprint arXiv:2006.09616}, 2020.

\bibitem{huang2020swapadvisor}
Chien-Chin Huang, Gu~Jin, and Jinyang Li.
\newblock Swapadvisor: Pushing deep learning beyond the gpu memory limit via
  smart swapping.
\newblock In {\em Twenty-Fifth International Conference on Architectural
  Support for Programming Languages and Operating Systems}, pages 1341--1355,
  2020.

\bibitem{wang2018superneurons}
Linnan Wang, Jinmian Ye, Yiyang Zhao, Wei Wu, Ang Li, Shuaiwen~Leon Song,
  Zenglin Xu, and Tim Kraska.
\newblock Superneurons: Dynamic {GPU} memory management for training deep
  neural networks.
\newblock In {\em 23rd ACM SIGPLAN symposium on principles and practice of
  parallel programming}, pages 41--53, 2018.

\bibitem{peng2020capuchin}
Xuan Peng, Xuanhua Shi, Hulin Dai, Hai Jin, Weiliang Ma, Qian Xiong, Fan Yang,
  and Xuehai Qian.
\newblock Capuchin: Tensor-based gpu memory management for deep learning.
\newblock In {\em Twenty-Fifth International Conference on Architectural
  Support for Programming Languages and Operating Systems}, pages 891--905,
  2020.

\bibitem{beaumont2021efficient}
Olivier Beaumont, Lionel Eyraud-Dubois, and Alena Shilova.
\newblock Efficient combination of rematerialization and offloading for
  training dnns.
\newblock {\em Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem{bottou2010large}
L{\'e}on Bottou.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In {\em Proceedings of COMPSTAT'2010}, pages 177--186. Springer,
  2010.

\bibitem{bottou2018optimization}
L{\'e}on Bottou, Frank~E Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning.
\newblock {\em SIAM Review}, 60(2):223--311, 2018.

\bibitem{courbariaux2015binaryconnect}
Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David.
\newblock Binaryconnect: Training deep neural networks with binary weights
  during propagations.
\newblock In {\em Advances in neural information processing systems}, pages
  3123--3131, 2015.

\bibitem{rabe2021self}
Markus~N Rabe and Charles Staats.
\newblock Self-attention does not need $o(n^2)$ memory.
\newblock {\em arXiv preprint arXiv:2112.05682}, 2021.

\bibitem{simonyan2014very}
K.~Simonyan and A.~Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock In {\em International Conference on Learning Representations}, 2015.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em IEEE conference on computer vision and pattern recognition},
  pages 770--778, 2016.

\bibitem{liu2021Swin}
Ze~Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and
  Baining Guo.
\newblock Swin transformer: Hierarchical vision transformer using shifted
  windows.
\newblock {\em International Conference on Computer Vision (ICCV)}, 2021.

\bibitem{imagenet_cvpr09}
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li~Fei-Fei.
\newblock Imagenet: A large-scale hierarchical image database.
\newblock In {\em IEEE conference on computer vision and pattern recognition},
  pages 248--255. Ieee, 2009.

\bibitem{lin2017focal}
Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll{\'a}r.
\newblock Focal loss for dense object detection.
\newblock In {\em International Conference on Computer Vision (ICCV)}, pages
  2980--2988, 2017.

\bibitem{ren2015faster}
Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.
\newblock Faster {R-CNN}: Towards real-time object detection with region
  proposal networks.
\newblock {\em Advances in neural information processing systems}, 28:91--99,
  2015.

\bibitem{lin2014microsoft}
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
  Ramanan, Piotr Doll{\'a}r, and C~Lawrence Zitnick.
\newblock Microsoft coco: Common objects in context.
\newblock In {\em European conference on computer vision}, pages 740--755.
  Springer, 2014.

\bibitem{ren2021zero}
Jie Ren, Samyam Rajbhandari, Reza~Yazdani Aminabadi, Olatunji Ruwase, Shuangyan
  Yang, Minjia Zhang, Dong Li, and Yuxiong He.
\newblock Zero-offload: Democratizing billion-scale model training.
\newblock {\em arXiv preprint arXiv:2101.06840}, 2021.

\bibitem{wolf-etal-2020-transformers}
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
  Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe
  Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien
  Plu, Canwen Xu, Teven~Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest,
  and Alexander~M. Rush.
\newblock Transformers: State-of-the-art natural language processing.
\newblock In {\em 2020 Conference on Empirical Methods in Natural Language
  Processing: System Demonstrations}, pages 38--45, Online, October 2020.
  Association for Computational Linguistics.

\bibitem{zeng2019graphsaint}
Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor
  Prasanna.
\newblock Graphsaint: Graph sampling based inductive learning method.
\newblock {\em arXiv preprint arXiv:1907.04931}, 2019.

\bibitem{hu2020open}
Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu,
  Michele Catasta, and Jure Leskovec.
\newblock Open graph benchmark: Datasets for machine learning on graphs.
\newblock {\em arXiv preprint arXiv:2005.00687}, 2020.

\bibitem{velivckovic2017graph}
Petar Veli{\v{c}}kovi{\'c}, Guillem Cucurull, Arantxa Casanova, Adriana Romero,
  Pietro Lio, and Yoshua Bengio.
\newblock Graph attention networks.
\newblock {\em arXiv preprint arXiv:1710.10903}, 2017.

\bibitem{chen2020simple_gcnii}
Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li.
\newblock Simple and deep graph convolutional networks.
\newblock In {\em International Conference on Machine Learning}, pages
  1725--1735. PMLR, 2020.

\bibitem{cen2021cogdl}
Yukuo Cen, Zhenyu Hou, Yan Wang, Qibin Chen, Yizhen Luo, Xingcheng Yao, Aohan
  Zeng, Shiguang Guo, Peng Zhang, Guohao Dai, Yu~Wang, Chang Zhou, Hongxia
  Yang, and Jie Tang.
\newblock Cogdl: Toolkit for deep learning on graphs.
\newblock {\em arXiv preprint arXiv:2103.00959}, 2021.

\bibitem{wang2018glue}
Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel
  Bowman.
\newblock Glue: A multi-task benchmark and analysis platform for natural
  language understanding.
\newblock In {\em 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting
  Neural Networks for NLP}, pages 353--355, 2018.

\bibitem{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock Bert: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock In {\em 2019 Conference of the North American Chapter of the
  Association for Computational Linguistics: Human Language Technologies,
  Volume 1 (Long and Short Papers)}, pages 4171--4186, 2019.

\end{thebibliography}
