\begin{thebibliography}{26}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Abbasi-Yadkori et~al.(2011)Abbasi-Yadkori, P{\'a}l, and
  Szepesv{\'a}ri]{abbasi2011improved}
Yasin Abbasi-Yadkori, D{\'a}vid P{\'a}l, and Csaba Szepesv{\'a}ri.
\newblock Improved algorithms for linear stochastic bandits.
\newblock \emph{Advances in neural information processing systems}, 24, 2011.

\bibitem[Abbasi-Yadkori et~al.(2019)Abbasi-Yadkori, Bartlett, Bhatia, Lazic,
  Szepesvari, and Weisz]{abbasi2019politex}
Yasin Abbasi-Yadkori, Peter Bartlett, Kush Bhatia, Nevena Lazic, Csaba
  Szepesvari, and Gell{\'e}rt Weisz.
\newblock Politex: Regret bounds for policy iteration using expert prediction.
\newblock In \emph{International Conference on Machine Learning}, pages
  3692--3702. PMLR, 2019.

\bibitem[Agarwal et~al.(2020)Agarwal, Henaff, Kakade, and Sun]{agarwal2020pc}
Alekh Agarwal, Mikael Henaff, Sham Kakade, and Wen Sun.
\newblock Pc-pg: Policy cover directed exploration for provable policy gradient
  learning.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 13399--13412, 2020.

\bibitem[Agarwal et~al.(2021)Agarwal, Kakade, Lee, and
  Mahajan]{agarwal2021theory}
Alekh Agarwal, Sham~M Kakade, Jason~D Lee, and Gaurav Mahajan.
\newblock On the theory of policy gradient methods: Optimality, approximation,
  and distribution shift.
\newblock \emph{J. Mach. Learn. Res.}, 22\penalty0 (98):\penalty0 1--76, 2021.

\bibitem[Agarwal et~al.(2022)Agarwal, Jin, and Zhang]{agarwal2022vo}
Alekh Agarwal, Yujia Jin, and Tong Zhang.
\newblock {VO}$q${L}: Towards optimal regret in model-free {RL} with nonlinear
  function approximation.
\newblock \emph{arXiv preprint arXiv:2212.06069}, 2022.

\bibitem[Berner et~al.(2019)Berner, Brockman, Chan, Cheung, Debiak, Dennison,
  Farhi, Fischer, Hashme, Hesse, J{\'{o}}zefowicz, Gray, Olsson, Pachocki,
  Petrov, de~Oliveira~Pinto, Raiman, Salimans, Schlatter, Schneider, Sidor,
  Sutskever, Tang, Wolski, and Zhang]{berner2019dota}
Christopher Berner, Greg Brockman, Brooke Chan, Vicki Cheung, Przemyslaw
  Debiak, Christy Dennison, David Farhi, Quirin Fischer, Shariq Hashme,
  Christopher Hesse, Rafal J{\'{o}}zefowicz, Scott Gray, Catherine Olsson,
  Jakub Pachocki, Michael Petrov, Henrique~Pond{\'{e}} de~Oliveira~Pinto,
  Jonathan Raiman, Tim Salimans, Jeremy Schlatter, Jonas Schneider, Szymon
  Sidor, Ilya Sutskever, Jie Tang, Filip Wolski, and Susan Zhang.
\newblock Dota 2 with large scale deep reinforcement learning.
\newblock \emph{arXiv preprint arXiv:1912.06680}, 2019.

\bibitem[Bhandari and Russo(2019)]{bhandari2019global}
Jalaj Bhandari and Daniel Russo.
\newblock Global optimality guarantees for policy gradient methods.
\newblock \emph{arXiv preprint arXiv:1906.01786}, 2019.

\bibitem[Cai et~al.(2020)Cai, Yang, Jin, and Wang]{cai2020provably}
Qi~Cai, Zhuoran Yang, Chi Jin, and Zhaoran Wang.
\newblock Provably efficient exploration in policy optimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1283--1294. PMLR, 2020.

\bibitem[Finn et~al.(2016)Finn, Levine, and Abbeel]{finn2016guided}
Chelsea Finn, Sergey Levine, and Pieter Abbeel.
\newblock Guided cost learning: Deep inverse optimal control via policy
  optimization.
\newblock In \emph{International conference on machine learning}, pages 49--58.
  PMLR, 2016.

\bibitem[He et~al.(2022)He, Zhao, Zhou, and Gu]{he2022nearly}
Jiafan He, Heyang Zhao, Dongruo Zhou, and Quanquan Gu.
\newblock Nearly minimax optimal reinforcement learning for linear {M}arkov
  decision processes.
\newblock \emph{arXiv preprint arXiv:2212.06132}, 2022.

\bibitem[Jin et~al.(2020)Jin, Yang, Wang, and Jordan]{jin2020provably}
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael~I Jordan.
\newblock Provably efficient reinforcement learning with linear function
  approximation.
\newblock In \emph{Conference on Learning Theory}, pages 2137--2143. PMLR,
  2020.

\bibitem[Jin et~al.(2021)Jin, Liu, and Miryoosefi]{jin2021bellman}
Chi Jin, Qinghua Liu, and Sobhan Miryoosefi.
\newblock {B}ellman eluder dimension: New rich classes of {RL} problems, and
  sample-efficient algorithms.
\newblock \emph{Advances in neural information processing systems},
  34:\penalty0 13406--13418, 2021.

\bibitem[Kakade(2001)]{kakade2001natural}
Sham~M Kakade.
\newblock A natural policy gradient.
\newblock \emph{Advances in neural information processing systems}, 14, 2001.

\bibitem[Liu et~al.(2019)Liu, Cai, Yang, and Wang]{liu2019neural}
Boyi Liu, Qi~Cai, Zhuoran Yang, and Zhaoran Wang.
\newblock Neural proximal/trust region policy optimization attains globally
  optimal policy.
\newblock \emph{arXiv preprint arXiv:1906.10306}, 2019.

\bibitem[Neu et~al.(2017)Neu, Jonsson, and G{\'o}mez]{neu2017unified}
Gergely Neu, Anders Jonsson, and Vicen{\c{c}} G{\'o}mez.
\newblock A unified view of entropy-regularized {M}arkov decision processes.
\newblock \emph{arXiv preprint arXiv:1705.07798}, 2017.

\bibitem[OpenAI(2022)]{chatgpt}
OpenAI.
\newblock {ChatGPT}: Optimizing language models for dialogue, 2022.
\newblock URL \url{https://openai.com/blog/chatgpt/}.

\bibitem[Russo and Van~Roy(2013)]{russo2013eluder}
Daniel Russo and Benjamin Van~Roy.
\newblock Eluder dimension and the sample complexity of optimistic exploration.
\newblock \emph{Advances in Neural Information Processing Systems}, 26, 2013.

\bibitem[Schulman et~al.(2015)Schulman, Levine, Abbeel, Jordan, and
  Moritz]{schulman2015trust}
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp
  Moritz.
\newblock Trust region policy optimization.
\newblock In \emph{International conference on machine learning}, pages
  1889--1897. PMLR, 2015.

\bibitem[Schulman et~al.(2017)Schulman, Wolski, Dhariwal, Radford, and
  Klimov]{schulman2017proximal}
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.
\newblock Proximal policy optimization algorithms.
\newblock \emph{arXiv preprint arXiv:1707.06347}, 2017.

\bibitem[Shani et~al.(2020)Shani, Efroni, Rosenberg, and
  Mannor]{shani2020optimistic}
Lior Shani, Yonathan Efroni, Aviv Rosenberg, and Shie Mannor.
\newblock Optimistic policy optimization with bandit feedback.
\newblock In \emph{International Conference on Machine Learning}, pages
  8604--8613. PMLR, 2020.

\bibitem[Wagenmaker et~al.(2022)Wagenmaker, Chen, Simchowitz, Du, and
  Jamieson]{wagenmaker2022reward}
Andrew~J Wagenmaker, Yifang Chen, Max Simchowitz, Simon Du, and Kevin Jamieson.
\newblock Reward-free {RL} is no harder than reward-aware {RL} in linear
  {M}arkov decision processes.
\newblock In \emph{International Conference on Machine Learning}, pages
  22430--22456. PMLR, 2022.

\bibitem[Wang et~al.(2020)Wang, Salakhutdinov, and Yang]{wang2020reinforcement}
Ruosong Wang, Russ~R Salakhutdinov, and Lin Yang.
\newblock Reinforcement learning with general value function approximation:
  Provably efficient approach via bounded eluder dimension.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 6123--6135, 2020.

\bibitem[Wu et~al.(2022)Wu, Yang, Zhong, Wang, Du, and Jiao]{wu2022nearly}
Tianhao Wu, Yunchang Yang, Han Zhong, Liwei Wang, Simon Du, and Jiantao Jiao.
\newblock Nearly optimal policy optimization with stable at any time guarantee.
\newblock In \emph{International Conference on Machine Learning}, pages
  24243--24265. PMLR, 2022.

\bibitem[Zanette et~al.(2020)Zanette, Lazaric, Kochenderfer, and
  Brunskill]{zanette2020learning}
Andrea Zanette, Alessandro Lazaric, Mykel Kochenderfer, and Emma Brunskill.
\newblock Learning near optimal policies with low inherent {B}ellman error.
\newblock In \emph{International Conference on Machine Learning}, pages
  10978--10989. PMLR, 2020.

\bibitem[Zanette et~al.(2021)Zanette, Cheng, and
  Agarwal]{zanette2021cautiously}
Andrea Zanette, Ching-An Cheng, and Alekh Agarwal.
\newblock Cautiously optimistic policy optimization and exploration with linear
  function approximation.
\newblock In \emph{Conference on Learning Theory}, pages 4473--4525. PMLR,
  2021.

\bibitem[Zhou et~al.(2021)Zhou, Gu, and Szepesvari]{zhou2021nearly}
Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari.
\newblock Nearly minimax optimal reinforcement learning for linear mixture
  {M}arkov decision processes.
\newblock In \emph{Conference on Learning Theory}, pages 4532--4576. PMLR,
  2021.

\end{thebibliography}
