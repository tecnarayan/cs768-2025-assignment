\begin{thebibliography}{10}

\bibitem[Abraham, 2004]{abraham2004optimal}
Ittai Abraham, Yonatan Amit, and Danny Dolev.
\newblock Optimal resilience asynchronous approximate agreement.
\newblock In {\em International Conference on Principles of Distributed
  Systems}, pages 229--239. Springer, 2004.

\bibitem[Alistarh et al., 2018]{alistarh2018byzantine}
Dan Alistarh, Zeyuan Allen-Zhu, and Jerry Li.
\newblock Byzantine stochastic gradient descent.
\newblock In {\em Neural Information Processing Systems}, pages 4613--4623,
  2018.

\bibitem[Baruch et al., 2019]{baruch2019little}
Gilad Baruch, Moran Baruch, and Yoav Goldberg.
\newblock A little is enough: Circumventing defenses for distributed learning.
\newblock In {\em Neural Information Processing Systems}, pages 8635--8645,
  2019.

\bibitem[Blanchard et al., 2017]{krum}
Peva Blanchard, El{-}Mahdi El{-}Mhamdi, Rachid Guerraoui, and Julien Stainer.
\newblock Machine learning with adversaries: {B}yzantine tolerant gradient
  descent.
\newblock In {\em Neural Information Processing Systems}, pages 118--128, 2017.

\bibitem[Bracha, 1987]{BRACHA1987130}
Gabriel Bracha.
\newblock Asynchronous byzantine agreement protocols.
\newblock {\em Information and Computation}, 75(2):130--143, 1987.

\bibitem[Bradshaw and Howard, 2019]{bradshaw19}
Samantha Bradshaw and Philip~N Howard.
\newblock {\em The global disinformation order: 2019 global inventory of
  organised social media manipulation}.
\newblock Project on Computational Propaganda, 2019.

\bibitem[Castro et al., 1999]{castro1999practical}
Miguel Castro, Barbara Liskov, et~al.
\newblock Practical {B}yzantine fault tolerance.
\newblock In {\em Operating Systems Design and Implementation}, volume~99,
  pages 173--186, 1999.

\bibitem[Chen et al., 2018]{chen2018draco}
Lingjiao Chen, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos.
\newblock Draco: Byzantine-resilient distributed training via redundant
  gradients.
\newblock In {\em International Conference on Machine Learning}, pages
  902--911, 2018.

\bibitem[Damaskinos et al., 2018]{kardam}
Georgios Damaskinos, El{-}Mahdi El{-}Mhamdi, Rachid Guerraoui, Rhicheek Patra,
  and Mahsa Taziki.
\newblock Asynchronous {B}yzantine machine learning (the case of {SGD}).
\newblock In {\em Neural Information Processing Systems}, volume~80, pages
  1145--1154, 2018.

\bibitem[Dinh et al., 2020]{DinhTN20}
Canh~T. Dinh, Nguyen~H. Tran, and Tuan~Dung Nguyen.
\newblock Personalized federated learning with moreau envelopes.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\bibitem[Dolev et al., 1986]{dolev1986reaching}
Danny Dolev, Nancy~A Lynch, Shlomit~S Pinter, Eugene~W Stark, and William~E
  Weihl.
\newblock Reaching approximate agreement in the presence of faults.
\newblock {\em Journal of the ACM}, 33(3):499--516, 1986.

\bibitem[El-Mhamdi et al., 2020]{podc2020}
El{-}Mahdi El{-}Mhamdi, Rachid Guerraoui, Arsany Guirguis, L{\^e}~Nguy{\^e}n
  Hoang, and S{\'e}bastien Rouault.
\newblock Genuinely distributed {B}yzantine machine learning.
\newblock In {\em Principles of Distributed Computing}, pages 355--364, 2020.

\bibitem[El-Mhamdi et al., 2018]{bulyanPaper}
El{-}Mahdi El{-}Mhamdi, Rachid Guerraoui, and S{\'e}bastien Rouault.
\newblock The hidden vulnerability of distributed learning in {B}yzantium.
\newblock In {\em International Conference on Machine Learning}, volume~80,
  pages 3521--3530, 2018.

\bibitem[Fallah et al., 2020]{FallahMO20}
Alireza Fallah, Aryan Mokhtari, and Asuman~E. Ozdaglar.
\newblock Personalized federated learning with theoretical guarantees: {A}
  model-agnostic meta-learning approach.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\bibitem[Farhadkhani et al., 2021]{FarhadkhaniGH21}
Sadegh Farhadkhani, Rachid Guerraoui, and L{\^{e}}~Nguy{\^{e}}n Hoang.
\newblock Strategyproof learning: Building trustworthy user-generated datasets.
\newblock {\em CoRR}, abs/2106.02398, 2021.

\bibitem[Fekete, 1986]{fekete1986asymptotically}
Alan~David Fekete.
\newblock Asymptotically optimal algorithms for approximate agreement.
\newblock In {\em Principles of Distributed Computing}, pages 73--87, 1986.

\bibitem[Fekete, 1987]{Fekete87}
Alan~David Fekete.
\newblock Asynchronous approximate agreement.
\newblock In {\em Principles of Distributed Computing}, pages 64--76, 1987.

\bibitem[Fischer et al., 1985]{fischer1985impossibility}
Michael~J Fischer, Nancy~A Lynch, and Michael~S Paterson.
\newblock Impossibility of distributed consensus with one faulty process.
\newblock {\em Journal of the ACM}, 32(2):374--382, 1985.

\bibitem[Grid5000, 2019]{g5k}
Grid5000.
\newblock Grid5000.
\newblock \url{https://www.grid5000.fr/}, 2019.

\bibitem[Guerraoui et al., 2021]{guerraoui2021garfield}
Rachid Guerraoui, Arsany Guirguis, J{\'{e}}r{\'{e}}my Plassmann, Anton Ragot,
  and S{\'{e}}bastien Rouault.
\newblock {GARFIELD:} system support for byzantine machine learning (regular
  paper).
\newblock In {\em 51st Annual {IEEE/IFIP} International Conference on
  Dependable Systems and Networks, {DSN} 2021, Taipei, Taiwan, June 21-24,
  2021}, pages 39--51. {IEEE}, 2021.

\bibitem[Guirguis, 2021]{code}
Arsany Guirguis.
\newblock \learn{} source code.
\newblock \url{https://github.com/LPD-EPFL/garfield/tree/decentralized}, 2021.

\bibitem[Guo et al., 2020]{guo2020towards}
Shangwei Guo, Tianwei Zhang, Xiaofei Xie, Lei Ma, Tao Xiang, and Yang Liu.
\newblock Towards byzantine-resilient learning in decentralized systems.
\newblock {\em arXiv preprint arXiv:2002.08569}, 2020.

\bibitem[Hanzely et al., 2020]{HanzelyHHR20}
Filip Hanzely, Slavom{\'{\i}}r Hanzely, Samuel Horv{\'{a}}th, and Peter
  Richt{\'{a}}rik.
\newblock Lower bounds and optimal algorithms for personalized federated
  learning.
\newblock In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell,
  Maria{-}Florina Balcan, and Hsuan{-}Tien Lin, editors, {\em Advances in
  Neural Information Processing Systems 33: Annual Conference on Neural
  Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
  virtual}, 2020.

\bibitem[He et al., 2016]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Konevcny et al., 2016]{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Daniel Ramage, and Peter
  Richt{\'a}rik.
\newblock Federated optimization: distributed machine learning for on-device
  intelligence.
\newblock {\em arXiv preprint arXiv:1610.02527}, 2016.

\bibitem[Krizhevsky, 2009]{cifar}
Alex Krizhevsky.
\newblock Cifar dataset.
\newblock {\url{https://www.cs.toronto.edu/~kriz/cifar.html}}, 2009.

\bibitem[Lamport et al., 1982]{lamport1982Byzantine}
Leslie Lamport, Robert Shostak, and Marshall Pease.
\newblock The {B}yzantine generals problem.
\newblock {\em Transactions on Programming Languages and Systems},
  4(3):382--401, 1982.

\bibitem[LeCun, 1998]{mnist}
Yann Le{C}un.
\newblock Mnist dataset.
\newblock {\url{http://yann.lecun.com/exdb/mnist/}}, 1998.

\bibitem[Li et al., 2014]{li2014scaling}
Mu~Li, David~G Andersen, Jun~Woo Park, Alexander~J Smola, Amr Ahmed, Vanja
  Josifovski, James Long, Eugene~J Shekita, and Bor-Yiing Su.
\newblock Scaling distributed machine learning with the parameter server.
\newblock In {\em Operating Systems Design and Implementation}, volume~1,
  page~3, 2014.

\bibitem[Mendes and Herlihy, 2013]{MendesHerlihy13}
Hammurabi Mendes and Maurice Herlihy.
\newblock Multidimensional approximate agreement in byzantine asynchronous
  systems.
\newblock In {\em Symposium on Theory of computing}, pages 391--400, 2013.

\bibitem[Neudert et al., 2019]{neudert2019}
Lisa-Maria Neudert, Philip Howard, and Bence Kollanyi.
\newblock Sourcing and automation of political news and information during
  three european elections.
\newblock {\em Social Media+ Society}, 5(3):2056305119863147, 2019.

\bibitem[Paszke et al., 2019]{paszke2019pytorch}
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
  Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et~al.
\newblock Pytorch: An imperative style, high-performance deep learning library.
\newblock {\em arXiv preprint arXiv:1912.01703}, 2019.

\bibitem[Rajput et al., 2019]{rajput2019detox}
Shashank Rajput, Hongyi Wang, Zachary Charles, and Dimitris Papailiopoulos.
\newblock Detox: A redundancy-based framework for faster and more robust
  gradient aggregation.
\newblock In {\em Neural Information Processing Systems}, pages 10320--10330,
  2019.

\bibitem[Rousseeuw, 1985]{rousseeuw1985multivariate}
Peter~J Rousseeuw.
\newblock Multivariate estimation with high breakdown point.
\newblock {\em Mathematical Statistics and Applications}, 8:283--297, 1985.

\bibitem[Rumelhart et al., 1986]{rumelhart1986learning}
David~E Rumelhart, Geoffrey~E Hinton, and Ronald~J Williams.
\newblock Learning representations by back-propagating errors.
\newblock {\em Nature}, 323(6088):533--536, 1986.

\bibitem[Xie et al., 2018]{xie2018generalized}
Cong Xie, Oluwasanmi Koyejo, and Indranil Gupta.
\newblock Generalized {B}yzantine-tolerant {SGD}.
\newblock {\em arXiv preprint arXiv:1802.10116}, 2018.

\bibitem[Yang and Bajwa, 2019]{yang2019bridge}
Zhixiong Yang and Waheed~U Bajwa.
\newblock Bridge: Byzantine-resilient decentralized gradient descent.
\newblock {\em arXiv preprint arXiv:1908.08098}, 2019.

\bibitem[Yang and Bajwa, 2019]{yang2019byrdie}
Zhixiong Yang and Waheed~U Bajwa.
\newblock Byrdie: Byzantine-resilient distributed coordinate descent for
  decentralized learning.
\newblock {\em Transactions on Signal and Information Processing over
  Networks}, 5(4):611--627, 2019.

\end{thebibliography}
