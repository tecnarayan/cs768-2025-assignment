\begin{thebibliography}{74}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahmed et~al.(2021)Ahmed, Bengio, van Seijen, and
  Courville]{Ahmed2021SystematicGW}
Faruk Ahmed, Yoshua Bengio, Harm van Seijen, and Aaron~C. Courville.
\newblock Systematic generalisation with group invariant predictions.
\newblock In \emph{ICLR}, 2021.

\bibitem[Arjovsky et~al.(2019)Arjovsky, Bottou, Gulrajani, and
  Lopez-Paz]{arjovsky2019invariant}
Martin Arjovsky, L{\'e}on Bottou, Ishaan Gulrajani, and David Lopez-Paz.
\newblock Invariant risk minimization.
\newblock \emph{arXiv preprint arXiv:1907.02893}, 2019.

\bibitem[Bahng et~al.(2022)Bahng, Jahanian, Sankaranarayanan, and
  Isola]{bahng2022visual}
Hyojin Bahng, Ali Jahanian, Swami Sankaranarayanan, and Phillip Isola.
\newblock Visual prompting: Modifying pixel space to adapt pre-trained models.
\newblock \emph{arXiv preprint arXiv:2203.17274}, 2022.

\bibitem[Beery et~al.(2018)Beery, Van~Horn, and Perona]{beery2018recognition}
Sara Beery, Grant Van~Horn, and Pietro Perona.
\newblock Recognition in terra incognita.
\newblock In \emph{Proceedings of the European Conference on Computer Vision
  (ECCV)}, pages 456--473, 2018.

\bibitem[Berg et~al.(2022)Berg, Hall, Bhalgat, Yang, Kirk, Shtedritski, and
  Bain]{berg2022prompt}
Hugo Berg, Siobhan~Mackenzie Hall, Yash Bhalgat, Wonsuk Yang, Hannah~Rose Kirk,
  Aleksandar Shtedritski, and Max Bain.
\newblock A prompt array keeps the bias away: Debiasing vision-language models
  with adversarial learning.
\newblock \emph{arXiv preprint arXiv:2203.11933}, 2022.

\bibitem[Black et~al.(2021)Black, Gao, Wang, Leahy, and Biderman]{gpt-neo}
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman.
\newblock {GPT-Neo: Large Scale Autoregressive Language Modeling with
  Mesh-Tensorflow}.
\newblock March 2021.
\newblock \doi{10.5281/zenodo.5297715}.
\newblock URL \url{https://doi.org/10.5281/zenodo.5297715}.
\newblock {If you use this software, please cite it using these metadata.}

\bibitem[Blodgett et~al.(2016)Blodgett, Green, and
  O'Connor]{blodgett2016demographic}
Su~Lin Blodgett, Lisa Green, and Brendan O'Connor.
\newblock Demographic dialectal variation in social media: A case study of
  african-american english.
\newblock \emph{arXiv preprint arXiv:1608.08868}, 2016.

\bibitem[Bommasani et~al.(2021)Bommasani, Hudson, Adeli, Altman, Arora, von
  Arx, Bernstein, Bohg, Bosselut, Brunskill,
  et~al.]{bommasani2021opportunities}
Rishi Bommasani, Drew~A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
  von Arx, Michael~S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
  Brunskill, et~al.
\newblock On the opportunities and risks of foundation models.
\newblock \emph{arXiv preprint arXiv:2108.07258}, 2021.

\bibitem[Borkan et~al.(2019)Borkan, Dixon, Sorensen, Thain, and
  Vasserman]{DBLP:journals/corr/abs-1903-04561}
Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman.
\newblock Nuanced metrics for measuring unintended bias with real data for text
  classification.
\newblock \emph{CoRR}, abs/1903.04561, 2019.
\newblock URL \url{http://arxiv.org/abs/1903.04561}.

\bibitem[Brown et~al.(2020)Brown, Mann, Ryder, Subbiah, Kaplan, Dhariwal,
  Neelakantan, Shyam, Sastry, Askell, et~al.]{brown2020language}
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared~D Kaplan, Prafulla
  Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
  et~al.
\newblock Language models are few-shot learners.
\newblock \emph{Advances in neural information processing systems},
  33:\penalty0 1877--1901, 2020.

\bibitem[Buolamwini and Gebru(2018)]{buolamwini2018gender}
Joy Buolamwini and Timnit Gebru.
\newblock Gender shades: Intersectional accuracy disparities in commercial
  gender classification.
\newblock In \emph{Conference on fairness, accountability and transparency},
  pages 77--91. PMLR, 2018.

\bibitem[Byrd and Lipton(2019)]{byrd2019effect}
Jonathon Byrd and Zachary Lipton.
\newblock What is the effect of importance weighting in deep learning?
\newblock In \emph{International Conference on Machine Learning}, pages
  872--881. PMLR, 2019.

\bibitem[Chen et~al.(2020)Chen, Kornblith, Norouzi, and Hinton]{chen2020simple}
Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton.
\newblock A simple framework for contrastive learning of visual
  representations.
\newblock In \emph{International conference on machine learning}, pages
  1597--1607. PMLR, 2020.

\bibitem[Christie et~al.(2018)Christie, Fendley, Wilson, and
  Mukherjee]{christie2018functional}
Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee.
\newblock Functional map of the world.
\newblock In \emph{Proceedings of the IEEE Conference on Computer Vision and
  Pattern Recognition}, pages 6172--6180, 2018.

\bibitem[Creager et~al.(2021)Creager, Jacobsen, and
  Zemel]{creager2021environment}
Elliot Creager, J{\"o}rn-Henrik Jacobsen, and Richard Zemel.
\newblock Environment inference for invariant learning.
\newblock In \emph{International Conference on Machine Learning}, pages
  2189--2200. PMLR, 2021.

\bibitem[Cui et~al.(2019)Cui, Jia, Lin, Song, and Belongie]{cui2019class}
Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie.
\newblock Class-balanced loss based on effective number of samples.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 9268--9277, 2019.

\bibitem[Desai and Johnson(2021)]{desai2021virtex}
Karan Desai and Justin Johnson.
\newblock Virtex: Learning visual representations from textual annotations.
\newblock In \emph{Proceedings of the IEEE/CVF Conference on Computer Vision
  and Pattern Recognition}, pages 11162--11173, 2021.

\bibitem[Fazlyab et~al.(2019)Fazlyab, Robey, Hassani, Morari, and
  Pappas]{fazlyab2019efficient}
Mahyar Fazlyab, Alexander Robey, Hamed Hassani, Manfred Morari, and George
  Pappas.
\newblock Efficient and accurate estimation of lipschitz constants for deep
  neural networks.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[F{\"u}rst et~al.(2021)F{\"u}rst, Rumetshofer, Tran, Ramsauer, Tang,
  Lehner, Kreil, Kopp, Klambauer, Bitto-Nemling, et~al.]{furst2021cloob}
Andreas F{\"u}rst, Elisabeth Rumetshofer, Viet Tran, Hubert Ramsauer, Fei Tang,
  Johannes Lehner, David Kreil, Michael Kopp, G{\"u}nter Klambauer, Angela
  Bitto-Nemling, et~al.
\newblock Cloob: Modern hopfield networks with infoloob outperform clip.
\newblock \emph{arXiv preprint arXiv:2110.11316}, 2021.

\bibitem[Gao et~al.(2020)Gao, Biderman, Black, Golding, Hoppe, Foster, Phang,
  He, Thite, Nabeshima, et~al.]{gao2020pile}
Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles
  Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et~al.
\newblock The pile: An 800gb dataset of diverse text for language modeling.
\newblock \emph{arXiv preprint arXiv:2101.00027}, 2020.

\bibitem[Gao et~al.(2021)Gao, Geng, Zhang, Ma, Fang, Zhang, Li, and
  Qiao]{gao2021clip}
Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang,
  Hongsheng Li, and Yu~Qiao.
\newblock Clip-adapter: Better vision-language models with feature adapters.
\newblock \emph{arXiv preprint arXiv:2110.04544}, 2021.

\bibitem[Ge et~al.(2021)Ge, Mishra, Li, Wang, and Jacobs]{ge2021robust}
Songwei Ge, Shlok Mishra, Chun-Liang Li, Haohan Wang, and David Jacobs.
\newblock Robust contrastive learning using negative samples with diminished
  semantics.
\newblock \emph{Advances in Neural Information Processing Systems}, 34, 2021.

\bibitem[Gouk et~al.(2021)Gouk, Frank, Pfahringer, and
  Cree]{gouk2021regularisation}
Henry Gouk, Eibe Frank, Bernhard Pfahringer, and Michael~J Cree.
\newblock Regularisation of neural networks by enforcing lipschitz continuity.
\newblock \emph{Machine Learning}, 110\penalty0 (2):\penalty0 393--416, 2021.

\bibitem[Hashimoto et~al.(2018)Hashimoto, Srivastava, Namkoong, and
  Liang]{hashimoto2018fairness}
Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang.
\newblock Fairness without demographics in repeated loss minimization.
\newblock In \emph{International Conference on Machine Learning}, pages
  1929--1938. PMLR, 2018.

\bibitem[He and Garcia(2009)]{he2009learning}
Haibo He and Edwardo~A Garcia.
\newblock Learning from imbalanced data.
\newblock \emph{IEEE Transactions on knowledge and data engineering},
  21\penalty0 (9):\penalty0 1263--1284, 2009.

\bibitem[He et~al.(2016)He, Zhang, Ren, and Sun]{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In \emph{Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem[Houlsby et~al.(2019)Houlsby, Giurgiu, Jastrzebski, Morrone,
  De~Laroussilhe, Gesmundo, Attariyan, and Gelly]{pmlr-v97-houlsby19a}
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin
  De~Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly.
\newblock Parameter-efficient transfer learning for {NLP}.
\newblock In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,
  \emph{Proceedings of the 36th International Conference on Machine Learning},
  volume~97 of \emph{Proceedings of Machine Learning Research}, pages
  2790--2799. PMLR, 09--15 Jun 2019.
\newblock URL \url{https://proceedings.mlr.press/v97/houlsby19a.html}.

\bibitem[Idrissi et~al.(2021)Idrissi, Arjovsky, Pezeshki, and
  Lopez-Paz]{idrissi2021simple}
Badr~Youbi Idrissi, Martin Arjovsky, Mohammad Pezeshki, and David Lopez-Paz.
\newblock Simple data balancing achieves competitive worst-group-accuracy.
\newblock \emph{arXiv preprint arXiv:2110.14503}, 2021.

\bibitem[Ioffe and Szegedy(2015)]{ioffe2015batch}
Sergey Ioffe and Christian Szegedy.
\newblock Batch normalization: Accelerating deep network training by reducing
  internal covariate shift.
\newblock In \emph{International conference on machine learning}, pages
  448--456. PMLR, 2015.

\bibitem[Jia et~al.(2021)Jia, Yang, Xia, Chen, Parekh, Pham, Le, Sung, Li, and
  Duerig]{jia2021scaling}
Chao Jia, Yinfei Yang, Ye~Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le,
  Yun-Hsuan Sung, Zhen Li, and Tom Duerig.
\newblock Scaling up visual and vision-language representation learning with
  noisy text supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  4904--4916. PMLR, 2021.

\bibitem[Jordan and Dimakis(2020)]{jordan2020exactly}
Matt Jordan and Alexandros~G Dimakis.
\newblock Exactly computing the local lipschitz constant of relu networks.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 7344--7353, 2020.

\bibitem[Khosla et~al.(2020)Khosla, Teterwak, Wang, Sarna, Tian, Isola,
  Maschinot, Liu, and Krishnan]{Khosla2020Supcon}
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
  Isola, Aaron Maschinot, Ce~Liu, and Dilip Krishnan.
\newblock Supervised contrastive learning.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 18661--18673, 2020.

\bibitem[Kirichenko et~al.(2022)Kirichenko, Izmailov, and
  Wilson]{kirichenko2022last}
Polina Kirichenko, Pavel Izmailov, and Andrew~Gordon Wilson.
\newblock Last layer re-training is sufficient for robustness to spurious
  correlations.
\newblock \emph{arXiv preprint arXiv:2204.02937}, 2022.

\bibitem[Koh et~al.(2021)Koh, Sagawa, Xie, Zhang, Balsubramani, Hu, Yasunaga,
  Phillips, Gao, Lee, et~al.]{koh2021wilds}
Pang~Wei Koh, Shiori Sagawa, Sang~Michael Xie, Marvin Zhang, Akshay
  Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard~Lanas Phillips, Irena
  Gao, Tony Lee, et~al.
\newblock Wilds: A benchmark of in-the-wild distribution shifts.
\newblock In \emph{International Conference on Machine Learning}, pages
  5637--5664. PMLR, 2021.

\bibitem[Krizhevsky(2009)]{Krizhevsky2009LearningML}
Alex Krizhevsky.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem[Kumar et~al.(2022)Kumar, Raghunathan, Jones, Ma, and
  Liang]{kumar2022fine}
Ananya Kumar, Aditi Raghunathan, Robbie Jones, Tengyu Ma, and Percy Liang.
\newblock Fine-tuning can distort pretrained features and underperform
  out-of-distribution.
\newblock \emph{arXiv preprint arXiv:2202.10054}, 2022.

\bibitem[Levine et~al.(2022)Levine, Dalmedigos, Ram, Zeldes, Jannai, Muhlgay,
  Osin, Lieber, Lenz, Shalev-Shwartz, et~al.]{levine2022standing}
Yoav Levine, Itay Dalmedigos, Ori Ram, Yoel Zeldes, Daniel Jannai, Dor Muhlgay,
  Yoni Osin, Opher Lieber, Barak Lenz, Shai Shalev-Shwartz, et~al.
\newblock Standing on the shoulders of giant frozen language models.
\newblock \emph{arXiv preprint arXiv:2204.10019}, 2022.

\bibitem[Li and Liang(2021)]{li2021prefix}
Xiang~Lisa Li and Percy Liang.
\newblock Prefix-tuning: Optimizing continuous prompts for generation.
\newblock \emph{arXiv preprint arXiv:2101.00190}, 2021.

\bibitem[Li et~al.(2021)Li, Liang, Zhao, Cui, Ouyang, Shao, Yu, and
  Yan]{li2021supervision}
Yangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao,
  Fengwei Yu, and Junjie Yan.
\newblock Supervision exists everywhere: A data efficient contrastive
  language-image pre-training paradigm.
\newblock \emph{arXiv preprint arXiv:2110.05208}, 2021.

\bibitem[Liu et~al.(2021)Liu, Haghgoo, Chen, Raghunathan, Koh, Sagawa, Liang,
  and Finn]{liu2021just}
Evan~Z Liu, Behzad Haghgoo, Annie~S Chen, Aditi Raghunathan, Pang~Wei Koh,
  Shiori Sagawa, Percy Liang, and Chelsea Finn.
\newblock Just train twice: Improving group robustness without training group
  information.
\newblock In \emph{International Conference on Machine Learning}, pages
  6781--6792. PMLR, 2021.

\bibitem[Liu et~al.(2015)Liu, Luo, Wang, and Tang]{liu2015deep}
Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
\newblock Deep learning face attributes in the wild.
\newblock In \emph{Proceedings of the IEEE international conference on computer
  vision}, pages 3730--3738, 2015.

\bibitem[Lu et~al.(2020)Lu, Nott, Olson, Todeschini, Vahabi, Carmon, and
  Schmidt]{lu2020harder}
Shangyun Lu, Bradley Nott, Aaron Olson, Alberto Todeschini, Hossein Vahabi,
  Yair Carmon, and Ludwig Schmidt.
\newblock Harder or different? a closer look at distribution shift in dataset
  reproduction.
\newblock In \emph{ICML Workshop on Uncertainty and Robustness in Deep
  Learning}, 2020.

\bibitem[McInnes et~al.(2018)McInnes, Healy, and Melville]{mcinnes2018umap}
Leland McInnes, John Healy, and James Melville.
\newblock Umap: Uniform manifold approximation and projection for dimension
  reduction.
\newblock \emph{arXiv preprint arXiv:1802.03426}, 2018.

\bibitem[Mu et~al.(2021)Mu, Kirillov, Wagner, and Xie]{mu2021slip}
Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie.
\newblock Slip: Self-supervision meets language-image pre-training.
\newblock \emph{arXiv preprint arXiv:2112.12750}, 2021.

\bibitem[Nam et~al.(2020)Nam, Cha, Ahn, Lee, and Shin]{nam2020learning}
Junhyun Nam, Hyuntak Cha, Sungsoo Ahn, Jaeho Lee, and Jinwoo Shin.
\newblock Learning from failure: De-biasing classifier from biased classifier.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 20673--20684, 2020.

\bibitem[Nam et~al.(2022)Nam, Kim, Lee, and Shin]{nam2022spread}
Junhyun Nam, Jaehyung Kim, Jaeho Lee, and Jinwoo Shin.
\newblock Spread spurious attribute: Improving worst-group accuracy with
  spurious attribute estimation.
\newblock \emph{arXiv preprint arXiv:2204.02070}, 2022.

\bibitem[Neyshabur et~al.(2017)Neyshabur, Bhojanapalli, and
  Srebro]{neyshabur2017pac}
Behnam Neyshabur, Srinadh Bhojanapalli, and Nathan Srebro.
\newblock A pac-bayesian approach to spectrally-normalized margin bounds for
  neural networks.
\newblock \emph{arXiv preprint arXiv:1707.09564}, 2017.

\bibitem[Ni et~al.(2019)Ni, Li, and McAuley]{ni-etal-2019-justifying}
Jianmo Ni, Jiacheng Li, and Julian McAuley.
\newblock Justifying recommendations using distantly-labeled reviews and
  fine-grained aspects.
\newblock In \emph{Proceedings of the 2019 Conference on Empirical Methods in
  Natural Language Processing and the 9th International Joint Conference on
  Natural Language Processing (EMNLP-IJCNLP)}, pages 188--197, Hong Kong,
  China, November 2019. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/D19-1018}.
\newblock URL \url{https://aclanthology.org/D19-1018}.

\bibitem[Oakden-Rayner et~al.(2020)Oakden-Rayner, Dunnmon, Carneiro, and
  R{\'e}]{oakden2020hidden}
Luke Oakden-Rayner, Jared Dunnmon, Gustavo Carneiro, and Christopher R{\'e}.
\newblock Hidden stratification causes clinically meaningful failures in
  machine learning for medical imaging.
\newblock In \emph{Proceedings of the ACM conference on health, inference, and
  learning}, pages 151--159, 2020.

\bibitem[Oord et~al.(2018)Oord, Li, and Vinyals]{oord2018representation}
Aaron van~den Oord, Yazhe Li, and Oriol Vinyals.
\newblock Representation learning with contrastive predictive coding.
\newblock \emph{arXiv preprint arXiv:1807.03748}, 2018.

\bibitem[Pfeiffer et~al.(2020)Pfeiffer, R{\"u}ckl{\'e}, Poth, Kamath,
  Vuli{\'c}, Ruder, Cho, and Gurevych]{pfeiffer-etal-2020-adapterhub}
Jonas Pfeiffer, Andreas R{\"u}ckl{\'e}, Clifton Poth, Aishwarya Kamath, Ivan
  Vuli{\'c}, Sebastian Ruder, Kyunghyun Cho, and Iryna Gurevych.
\newblock {A}dapter{H}ub: A framework for adapting transformers.
\newblock In \emph{Proceedings of the 2020 Conference on Empirical Methods in
  Natural Language Processing: System Demonstrations}, pages 46--54, Online,
  October 2020. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2020.emnlp-demos.7}.
\newblock URL \url{https://aclanthology.org/2020.emnlp-demos.7}.

\bibitem[Pfeiffer et~al.(2021)Pfeiffer, Kamath, R{\"u}ckl{\'e}, Cho, and
  Gurevych]{pfeiffer-etal-2021-adapterfusion}
Jonas Pfeiffer, Aishwarya Kamath, Andreas R{\"u}ckl{\'e}, Kyunghyun Cho, and
  Iryna Gurevych.
\newblock {A}dapter{F}usion: Non-destructive task composition for transfer
  learning.
\newblock In \emph{Proceedings of the 16th Conference of the European Chapter
  of the Association for Computational Linguistics: Main Volume}, pages
  487--503, Online, April 2021. Association for Computational Linguistics.
\newblock \doi{10.18653/v1/2021.eacl-main.39}.
\newblock URL \url{https://aclanthology.org/2021.eacl-main.39}.

\bibitem[Radford et~al.(2021)Radford, Kim, Hallacy, Ramesh, Goh, Agarwal,
  Sastry, Askell, Mishkin, Clark, et~al.]{radford2021learning}
Alec Radford, Jong~Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
  Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
  et~al.
\newblock Learning transferable visual models from natural language
  supervision.
\newblock In \emph{International Conference on Machine Learning}, pages
  8748--8763. PMLR, 2021.

\bibitem[Rebuffi et~al.(2017)Rebuffi, Bilen, and Vedaldi]{Rebuffi17}
S-A Rebuffi, H.~Bilen, and A.~Vedaldi.
\newblock Learning multiple visual domains with residual adapters.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[Recht et~al.(2018)Recht, Roelofs, Schmidt, and
  Shankar]{recht2018cifar}
Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar.
\newblock Do cifar-10 classifiers generalize to cifar-10?
\newblock \emph{arXiv preprint arXiv:1806.00451}, 2018.

\bibitem[Robinson et~al.(2021)Robinson, Chuang, Sra, and
  Jegelka]{Robinson2020ContrastiveLW}
J.~Robinson, Ching-Yao Chuang, S.~Sra, and S.~Jegelka.
\newblock Contrastive learning with hard negative samples.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Sagawa et~al.(2019)Sagawa, Koh, Hashimoto, and
  Liang]{sagawa2019distributionally}
Shiori Sagawa, Pang~Wei Koh, Tatsunori~B Hashimoto, and Percy Liang.
\newblock Distributionally robust neural networks for group shifts: On the
  importance of regularization for worst-case generalization.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Sagawa et~al.(2020)Sagawa, Raghunathan, Koh, and
  Liang]{sagawa2020investigation}
Shiori Sagawa, Aditi Raghunathan, Pang~Wei Koh, and Percy Liang.
\newblock An investigation of why overparameterization exacerbates spurious
  correlations.
\newblock In \emph{International Conference on Machine Learning}, pages
  8346--8356. PMLR, 2020.

\bibitem[Santurkar et~al.(2020)Santurkar, Tsipras, and
  Madry]{santurkar2020breeds}
Shibani Santurkar, Dimitris Tsipras, and Aleksander Madry.
\newblock Breeds: Benchmarks for subpopulation shift.
\newblock \emph{arXiv preprint arXiv:2008.04859}, 2020.

\bibitem[Shimodaira(2000)]{shimodaira2000improving}
Hidetoshi Shimodaira.
\newblock Improving predictive inference under covariate shift by weighting the
  log-likelihood function.
\newblock \emph{Journal of statistical planning and inference}, 90\penalty0
  (2):\penalty0 227--244, 2000.

\bibitem[Singh et~al.(2021)Singh, Hu, Goswami, Couairon, Galuba, Rohrbach, and
  Kiela]{singh2021flava}
Amanpreet Singh, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech
  Galuba, Marcus Rohrbach, and Douwe Kiela.
\newblock Flava: A foundational language and vision alignment model.
\newblock \emph{arXiv preprint arXiv:2112.04482}, 2021.

\bibitem[Singla et~al.(2022)Singla, Moayeri, and Feizi]{singla2022core}
Sahil Singla, Mazda Moayeri, and Soheil Feizi.
\newblock Core risk minimization using salient imagenet.
\newblock \emph{arXiv preprint arXiv:2203.15566}, 2022.

\bibitem[Sohoni et~al.(2020)Sohoni, Dunnmon, Angus, Gu, and
  R\'{e}]{sohoni2020no}
Nimit Sohoni, Jared Dunnmon, Geoffrey Angus, Albert Gu, and Christopher R\'{e}.
\newblock No subclass left behind: Fine-grained robustness in coarse-grained
  classification problems.
\newblock In \emph{Advances in Neural Information Processing Systems},
  volume~33, pages 19339--19352, 2020.

\bibitem[Taghanaki et~al.(2021)Taghanaki, Choi, Khasahmadi, and
  Goyal]{taghanaki2021robust}
Saeid~Asgari Taghanaki, Kristy Choi, Amir Khasahmadi, and Anirudh Goyal.
\newblock Robust representation learning via perceptual similarity metrics.
\newblock \emph{arXiv preprint arXiv:2106.06620}, 2021.

\bibitem[Virmaux and Scaman(2018)]{virmaux2018lipschitz}
Aladin Virmaux and Kevin Scaman.
\newblock Lipschitz regularity of deep neural networks: analysis and efficient
  estimation.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Welinder et~al.(2010)Welinder, Branson, Mita, Wah, Schroff, Belongie,
  and Perona]{WelinderEtal2010}
P.~Welinder, S.~Branson, T.~Mita, C.~Wah, F.~Schroff, S.~Belongie, and
  P.~Perona.
\newblock {Caltech-UCSD Birds 200}.
\newblock Technical Report CNS-TR-2010-001, California Institute of Technology,
  2010.

\bibitem[Wortsman et~al.(2021)Wortsman, Ilharco, Li, Kim, Hajishirzi, Farhadi,
  Namkoong, and Schmidt]{wortsman2021robust}
Mitchell Wortsman, Gabriel Ilharco, Mike Li, Jong~Wook Kim, Hannaneh
  Hajishirzi, Ali Farhadi, Hongseok Namkoong, and Ludwig Schmidt.
\newblock Robust fine-tuning of zero-shot models.
\newblock \emph{arXiv preprint arXiv:2109.01903}, 2021.

\bibitem[Yao et~al.(2021)Yao, Zhang, Zhang, Liu, Chua, and Sun]{yao2021cpt}
Yuan Yao, Ao~Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong
  Sun.
\newblock Cpt: Colorful prompt tuning for pre-trained vision-language models.
\newblock \emph{arXiv preprint arXiv:2109.11797}, 2021.

\bibitem[Yoshida and Miyato(2017)]{yoshida2017spectral}
Yuichi Yoshida and Takeru Miyato.
\newblock Spectral norm regularization for improving the generalizability of
  deep learning.
\newblock \emph{arXiv preprint arXiv:1705.10941}, 2017.

\bibitem[Zhang et~al.(2022)Zhang, Sohoni, Zhang, Finn, and
  R{\'e}]{zhang2022correct}
Michael Zhang, Nimit~S Sohoni, Hongyang~R Zhang, Chelsea Finn, and Christopher
  R{\'e}.
\newblock Correct-n-contrast: A contrastive approach for improving robustness
  to spurious correlations.
\newblock \emph{arXiv preprint arXiv:2203.01517}, 2022.

\bibitem[Zhang et~al.(2021)Zhang, Fang, Gao, Zhang, Li, Dai, Qiao, and
  Li]{zhang2021tip}
Renrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai,
  Yu~Qiao, and Hongsheng Li.
\newblock Tip-adapter: Training-free clip-adapter for better vision-language
  modeling.
\newblock \emph{arXiv preprint arXiv:2111.03930}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Jiang, Miura, Manning, and
  Langlotz]{zhang2020contrastive}
Yuhao Zhang, Hang Jiang, Yasuhide Miura, Christopher~D Manning, and Curtis~P
  Langlotz.
\newblock Contrastive learning of medical visual representations from paired
  images and text.
\newblock \emph{arXiv preprint arXiv:2010.00747}, 2020.

\bibitem[Zhou et~al.(2021)Zhou, Yang, Loy, and Liu]{zhou2021learning}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Learning to prompt for vision-language models.
\newblock \emph{arXiv preprint arXiv:2109.01134}, 2021.

\bibitem[Zhou et~al.(2022)Zhou, Yang, Loy, and Liu]{zhou2022conditional}
Kaiyang Zhou, Jingkang Yang, Chen~Change Loy, and Ziwei Liu.
\newblock Conditional prompt learning for vision-language models.
\newblock \emph{arXiv preprint arXiv:2203.05557}, 2022.

\end{thebibliography}
