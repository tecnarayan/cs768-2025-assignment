\begin{thebibliography}{80}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahn et~al.(2022)Ahn, Jain, Ji, Kale, Netrapalli, and
  Shamir]{ahn2022reproducibility}
Kwangjun Ahn, Prateek Jain, Ziwei Ji, Satyen Kale, Praneeth Netrapalli, and
  Gil~I Shamir.
\newblock Reproducibility in optimization: Theoretical framework and limits.
\newblock \emph{Advances in Neural Information Processing Systems}, 35, 2022.

\bibitem[Allen-Zhu(2018)]{allen2018make}
Zeyuan Allen-Zhu.
\newblock How to make the gradients small stochastically: Even faster convex
  and nonconvex sgd.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Allen-Zhu and Hazan(2016)]{allen2016optimal}
Zeyuan Allen-Zhu and Elad Hazan.
\newblock Optimal black-box reductions between optimization objectives.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Arjovsky and Bottou(2017)]{arjovsky2017towards}
Martin Arjovsky and Leon Bottou.
\newblock Towards principled methods for training generative adversarial
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Arjovsky et~al.(2017)Arjovsky, Chintala, and
  Bottou]{arjovsky2017wasserstein}
Martin Arjovsky, Soumith Chintala, and L{\'e}on Bottou.
\newblock Wasserstein generative adversarial networks.
\newblock In \emph{International Conference on Machine Learning}, pages
  214--223. PMLR, 2017.

\bibitem[Attia and Koren(2021)]{attia2021algorithmic}
Amit Attia and Tomer Koren.
\newblock Algorithmic instabilities of accelerated gradient descent.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1204--1214, 2021.

\bibitem[Attia and Koren(2022)]{attia2022uniform}
Amit Attia and Tomer Koren.
\newblock Uniform stability for first-order empirical risk minimization.
\newblock In \emph{Conference on Learning Theory}, pages 3313--3332. PMLR,
  2022.

\bibitem[Attouch et~al.(2018)Attouch, Chbani, and Riahi]{attouch2018combining}
Hedy Attouch, Zaki Chbani, and Hassan Riahi.
\newblock Combining fast inertial dynamics for convex optimization with
  {T}ikhonov regularization.
\newblock \emph{Journal of Mathematical Analysis and Applications},
  457\penalty0 (2):\penalty0 1065--1094, 2018.

\bibitem[Bailey and Piliouras(2018)]{bailey2018multiplicative}
James~P Bailey and Georgios Piliouras.
\newblock Multiplicative weights update in zero-sum games.
\newblock In \emph{Proceedings of the ACM Conference on Economics and
  Computation}, pages 321--338, 2018.

\bibitem[Baker(2016)]{baker20161}
Monya Baker.
\newblock 1,500 scientists lift the lid on reproducibility.
\newblock \emph{Nature}, 533:\penalty0 452â€“454, 2016.

\bibitem[Bassily et~al.(2020)Bassily, Feldman, Guzm{\'a}n, and
  Talwar]{bassily2020stability}
Raef Bassily, Vitaly Feldman, Crist{\'o}bal Guzm{\'a}n, and Kunal Talwar.
\newblock Stability of stochastic gradient descent on nonsmooth convex losses.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 4381--4391, 2020.

\bibitem[Bauschke et~al.(2011)Bauschke, Combettes, et~al.]{bauschke2011convex}
Heinz~H Bauschke, Patrick~L Combettes, et~al.
\newblock \emph{Convex analysis and monotone operator theory in Hilbert
  spaces}, volume 408.
\newblock Springer, 2011.

\bibitem[Bellman(2008)]{bellman2008stability}
Richard Bellman.
\newblock \emph{Stability theory of differential equations}.
\newblock Courier Corporation, 2008.

\bibitem[Beznosikov et~al.(2020)Beznosikov, Horv{\'a}th, Richt{\'a}rik, and
  Safaryan]{beznosikov2020biased}
Aleksandr Beznosikov, Samuel Horv{\'a}th, Peter Richt{\'a}rik, and Mher
  Safaryan.
\newblock On biased compression for distributed learning.
\newblock \emph{arXiv preprint arXiv:2002.12410}, 2020.

\bibitem[Blanchard et~al.(2017)Blanchard, El~Mhamdi, Guerraoui, and
  Stainer]{blanchard2017machine}
Peva Blanchard, El~Mahdi El~Mhamdi, Rachid Guerraoui, and Julien Stainer.
\newblock Machine learning with adversaries: Byzantine tolerant gradient
  descent.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Boob and Guzm{\'a}n(2023)]{boob2023optimal}
Digvijay Boob and Crist{\'o}bal Guzm{\'a}n.
\newblock Optimal algorithms for differentially private stochastic monotone
  variational inequalities and saddle-point problems.
\newblock \emph{Mathematical Programming}, pages 1--43, 2023.

\bibitem[Bousquet and Elisseeff(2002)]{bousquet2002stability}
Olivier Bousquet and Andr{\'e} Elisseeff.
\newblock Stability and generalization.
\newblock \emph{The Journal of Machine Learning Research}, 2:\penalty0
  499--526, 2002.

\bibitem[Bouthillier et~al.(2019)Bouthillier, Laurent, and
  Vincent]{bouthillier2019unreproducible}
Xavier Bouthillier, C{\'e}sar Laurent, and Pascal Vincent.
\newblock Unreproducible research is reproducible.
\newblock In \emph{International Conference on Machine Learning}, pages
  725--734. PMLR, 2019.

\bibitem[Brock et~al.(2019)Brock, Donahue, and Simonyan]{brock2018large}
Andrew Brock, Jeff Donahue, and Karen Simonyan.
\newblock Large scale {GAN} training for high fidelity natural image synthesis.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[Bubeck(2015)]{bubeck2015convex}
S{\'e}bastien Bubeck.
\newblock Convex optimization: Algorithms and complexity.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  8\penalty0 (3-4):\penalty0 231--357, 2015.

\bibitem[Bun et~al.(2023)Bun, Gaboardi, Hopkins, Impagliazzo, Lei, Pitassi,
  Sivakumar, and Sorrell]{bun2023stability}
Mark Bun, Marco Gaboardi, Max Hopkins, Russell Impagliazzo, Rex Lei, Toniann
  Pitassi, Satchit Sivakumar, and Jessica Sorrell.
\newblock Stability is stable: Connections between replicability, privacy, and
  adaptive generalization.
\newblock In \emph{Proceedings of the 55th Annual ACM Symposium on Theory of
  Computing}, pages 520--527, 2023.

\bibitem[Chavdarova et~al.(2021)Chavdarova, Pagliardini, Stich, Fleuret, and
  Jaggi]{chavdarova2021taming}
Tatjana Chavdarova, Matteo Pagliardini, Sebastian~U Stich, Fran{\c{c}}ois
  Fleuret, and Martin Jaggi.
\newblock Taming {GAN}s with lookahead-minmax.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Che et~al.(2017)Che, Li, Jacob, Bengio, and Li]{che2017mode}
Tong Che, Yanran Li, Athul Jacob, Yoshua Bengio, and Wenjie Li.
\newblock Mode regularized generative adversarial networks.
\newblock In \emph{International Conference on Learning Representations}, 2017.

\bibitem[Chen et~al.(2018)Chen, Jin, and Yu]{chen2018stability}
Yuansi Chen, Chi Jin, and Bin Yu.
\newblock Stability and convergence trade-off of iterative optimization
  algorithms.
\newblock \emph{arXiv preprint arXiv:1804.01619}, 2018.

\bibitem[Dai et~al.(2018)Dai, Shaw, Li, Xiao, He, Liu, Chen, and
  Song]{dai2018sbeed}
Bo~Dai, Albert Shaw, Lihong Li, Lin Xiao, Niao He, Zhen Liu, Jianshu Chen, and
  Le~Song.
\newblock {SBEED}: Convergent reinforcement learning with nonlinear function
  approximation.
\newblock In \emph{International Conference on Machine Learning}, pages
  1125--1134. PMLR, 2018.

\bibitem[d'Aspremont(2008)]{d2008smooth}
Alexandre d'Aspremont.
\newblock Smooth optimization with approximate gradient.
\newblock \emph{SIAM Journal on Optimization}, 19\penalty0 (3):\penalty0
  1171--1183, 2008.

\bibitem[Devolder et~al.(2013)Devolder, Glineur, Nesterov,
  et~al.]{devolder2013first}
Olivier Devolder, Fran{\c{c}}ois Glineur, Yurii Nesterov, et~al.
\newblock First-order methods with inexact oracle: the strongly-convex case.
\newblock \emph{CORE Discussion Papers}, 2013016:\penalty0 47, 2013.

\bibitem[Devolder et~al.(2014)Devolder, Glineur, and
  Nesterov]{devolder2014first}
Olivier Devolder, Fran{\c{c}}ois Glineur, and Yurii Nesterov.
\newblock First-order methods of smooth convex optimization with inexact
  oracle.
\newblock \emph{Mathematical Programming}, 146:\penalty0 37--75, 2014.

\bibitem[Dwork and Roth(2014)]{dwork2014algorithmic}
Cynthia Dwork and Aaron Roth.
\newblock The algorithmic foundations of differential privacy.
\newblock \emph{Foundations and Trends{\textregistered} in Theoretical Computer
  Science}, 9\penalty0 (3--4):\penalty0 211--407, 2014.

\bibitem[Esfandiari et~al.(2023{\natexlab{a}})Esfandiari, Kalavasis, Karbasi,
  Krause, Mirrokni, and Velegkas]{esfandiari2023bandits}
Hossein Esfandiari, Alkis Kalavasis, Amin Karbasi, Andreas Krause, Vahab
  Mirrokni, and Grigoris Velegkas.
\newblock Replicable bandits.
\newblock In \emph{International Conference on Learning Representations},
  2023{\natexlab{a}}.

\bibitem[Esfandiari et~al.(2023{\natexlab{b}})Esfandiari, Karbasi, Mirrokni,
  Velegkas, and Zhou]{esfandiari2023clustering}
Hossein Esfandiari, Amin Karbasi, Vahab Mirrokni, Grigoris Velegkas, and Felix
  Zhou.
\newblock Replicable clustering.
\newblock \emph{arXiv preprint arXiv:2302.10359}, 2023{\natexlab{b}}.

\bibitem[Facchinei and Pang(2003)]{facchinei2003finite}
Francisco Facchinei and Jong-Shi Pang.
\newblock \emph{Finite-dimensional variational inequalities and complementarity
  problems}.
\newblock Springer, 2003.

\bibitem[Farnia and Ozdaglar(2021)]{farnia2021train}
Farzan Farnia and Asuman Ozdaglar.
\newblock Train simultaneously, generalize better: Stability of gradient-based
  minimax learners.
\newblock In \emph{International Conference on Machine Learning}, pages
  3174--3185. PMLR, 2021.

\bibitem[Feldman et~al.(2020)Feldman, Koren, and Talwar]{feldman2020private}
Vitaly Feldman, Tomer Koren, and Kunal Talwar.
\newblock Private stochastic convex optimization: optimal rates in linear time.
\newblock In \emph{Proceedings of the 52nd Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 439--449, 2020.

\bibitem[Gidel et~al.(2019{\natexlab{a}})Gidel, Berard, Vignoud, Vincent, and
  Lacoste-Julien]{gidel2018a}
Gauthier Gidel, Hugo Berard, GaÃ«tan Vignoud, Pascal Vincent, and Simon
  Lacoste-Julien.
\newblock A variational inequality perspective on generative adversarial
  networks.
\newblock In \emph{International Conference on Learning Representations},
  2019{\natexlab{a}}.

\bibitem[Gidel et~al.(2019{\natexlab{b}})Gidel, Hemmat, Pezeshki, Le~Priol,
  Huang, Lacoste-Julien, and Mitliagkas]{gidel2019negative}
Gauthier Gidel, Reyhane~Askari Hemmat, Mohammad Pezeshki, R{\'e}mi Le~Priol,
  Gabriel Huang, Simon Lacoste-Julien, and Ioannis Mitliagkas.
\newblock Negative momentum for improved game dynamics.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1802--1811. PMLR, 2019{\natexlab{b}}.

\bibitem[Goodfellow et~al.(2014)Goodfellow, Pouget-Abadie, Mirza, Xu,
  Warde-Farley, Ozair, Courville, and Bengio]{goodfellow2014generative}
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
  Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
\newblock Generative adversarial nets.
\newblock \emph{Advances in Neural Information Processing Systems}, 27, 2014.

\bibitem[Goodman et~al.(2016)Goodman, Fanelli, and Ioannidis]{goodman2016does}
Steven~N Goodman, Daniele Fanelli, and John~PA Ioannidis.
\newblock What does research reproducibility mean?
\newblock \emph{Science Translational Medicine}, 8\penalty0 (341):\penalty0
  341ps12--341ps12, 2016.

\bibitem[Gorbunov et~al.(2023)Gorbunov, Taylor, Horv{\'a}th, and
  Gidel]{gorbunov2023convergence}
Eduard Gorbunov, Adrien Taylor, Samuel Horv{\'a}th, and Gauthier Gidel.
\newblock Convergence of proximal point and extragradient-based methods beyond
  monotonicity: the case of negative comonotonicity.
\newblock In \emph{International Conference on Machine Learning}, pages
  11614--11641. PMLR, 2023.

\bibitem[Gundersen and Kjensmo(2018)]{gundersen2018state}
Odd~Erik Gundersen and Sigbj{\o}rn Kjensmo.
\newblock State of the art: Reproducibility in artificial intelligence.
\newblock In \emph{Proceedings of the AAAI Conference on Artificial
  Intelligence}, volume~32, 2018.

\bibitem[Haibe-Kains et~al.(2020)Haibe-Kains, Adam, Hosny, Khodakarami,
  Waldron, Wang, McIntosh, Goldenberg, Kundaje, Greene,
  et~al.]{haibe2020transparency}
Benjamin Haibe-Kains, George~Alexandru Adam, Ahmed Hosny, Farnoosh Khodakarami,
  Levi Waldron, Bo~Wang, Chris McIntosh, Anna Goldenberg, Anshul Kundaje,
  Casey~S Greene, et~al.
\newblock Transparency and reproducibility in artificial intelligence.
\newblock \emph{Nature}, 586\penalty0 (7829):\penalty0 14--16, 2020.

\bibitem[Hardt et~al.(2016)Hardt, Recht, and Singer]{hardt2016train}
Moritz Hardt, Ben Recht, and Yoram Singer.
\newblock Train faster, generalize better: Stability of stochastic gradient
  descent.
\newblock In \emph{International Conference on Machine Learning}, pages
  1225--1234. PMLR, 2016.

\bibitem[Henderson et~al.(2018)Henderson, Islam, Bachman, Pineau, Precup, and
  Meger]{henderson2018deep}
Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup,
  and David Meger.
\newblock Deep reinforcement learning that matters.
\newblock In \emph{Proceedings of the AAAI conference on artificial
  intelligence}, volume~32, 2018.

\bibitem[Hu et~al.(2020)Hu, Zhang, Chen, and He]{hu2020biased}
Yifan Hu, Siqi Zhang, Xin Chen, and Niao He.
\newblock Biased stochastic first-order methods for conditional stochastic
  optimization and applications in meta learning.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 2759--2770, 2020.

\bibitem[Impagliazzo et~al.(2022)Impagliazzo, Lei, Pitassi, and
  Sorrell]{impagliazzo2022reproducibility}
Russell Impagliazzo, Rex Lei, Toniann Pitassi, and Jessica Sorrell.
\newblock Reproducibility in learning.
\newblock In \emph{Proceedings of the 54th Annual ACM SIGACT Symposium on
  Theory of Computing}, pages 818--831, 2022.

\bibitem[Juditsky et~al.(2011)Juditsky, Nemirovski, and
  Tauvel]{juditsky2011solving}
Anatoli Juditsky, Arkadi Nemirovski, and Claire Tauvel.
\newblock Solving variational inequalities with stochastic mirror-prox
  algorithm.
\newblock \emph{Stochastic Systems}, 1\penalty0 (1):\penalty0 17--58, 2011.

\bibitem[Kalavasis et~al.(2023)Kalavasis, Karbasi, Moran, and
  Velegkas]{kalavasis2023statistical}
Alkis Kalavasis, Amin Karbasi, Shay Moran, and Grigoris Velegkas.
\newblock Statistical indistinguishability of learning algorithms.
\newblock In \emph{International Conference on Machine Learning}, volume 202,
  pages 15586--15622. PMLR, 2023.

\bibitem[Korpelevich(1976)]{korpelevich1976extragradient}
Galina~M Korpelevich.
\newblock The extragradient method for finding saddle points and other
  problems.
\newblock \emph{Matecon}, 12:\penalty0 747--756, 1976.

\bibitem[Lei et~al.(2021)Lei, Yang, Yang, and Ying]{lei2021stability}
Yunwen Lei, Zhenhuan Yang, Tianbao Yang, and Yiming Ying.
\newblock Stability and generalization of stochastic gradient methods for
  minimax problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  6175--6186. PMLR, 2021.

\bibitem[Li et~al.(2021)Li, Tian, Zhang, and Jadbabaie]{li2021complexity}
Haochuan Li, Yi~Tian, Jingzhao Zhang, and Ali Jadbabaie.
\newblock Complexity lower bounds for nonconvex-strongly-concave min-max
  optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  34:\penalty0 1792--1804, 2021.

\bibitem[Lin et~al.(2015)Lin, Mairal, and Harchaoui]{lin2015universal}
Hongzhou Lin, Julien Mairal, and Zaid Harchaoui.
\newblock A universal catalyst for first-order optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 28, 2015.

\bibitem[Lin et~al.(2020)Lin, Jin, and Jordan]{lin2020gradient}
Tianyi Lin, Chi Jin, and Michael Jordan.
\newblock On gradient descent ascent for nonconvex-concave minimax problems.
\newblock In \emph{International Conference on Machine Learning}, pages
  6083--6093. PMLR, 2020.

\bibitem[Lucic et~al.(2018)Lucic, Kurach, Michalski, Gelly, and
  Bousquet]{lucic2018gans}
Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain Gelly, and Olivier
  Bousquet.
\newblock Are {GAN}s created equal? {A} large-scale study.
\newblock \emph{Advances in Neural Information Processing Systems}, 31, 2018.

\bibitem[Madry et~al.(2018)Madry, Makelov, Schmidt, Tsipras, and
  Vladu]{madry2018towards}
Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
  Adrian Vladu.
\newblock Towards deep learning models resistant to adversarial attacks.
\newblock In \emph{International Conference on Learning Representations}, 2018.

\bibitem[Mertikopoulos et~al.(2018)Mertikopoulos, Papadimitriou, and
  Piliouras]{mertikopoulos2018cycles}
Panayotis Mertikopoulos, Christos Papadimitriou, and Georgios Piliouras.
\newblock Cycles in adversarial regularized learning.
\newblock In \emph{Proceedings of the 29th Annual ACM-SIAM Symposium on
  Discrete Algorithms}, pages 2703--2717. SIAM, 2018.

\bibitem[Mescheder et~al.(2017)Mescheder, Nowozin, and
  Geiger]{mescheder2017numerics}
Lars Mescheder, Sebastian Nowozin, and Andreas Geiger.
\newblock The numerics of {GAN}s.
\newblock \emph{Advances in Neural Information Processing Systems}, 30, 2017.

\bibitem[Mokhtari et~al.(2020{\natexlab{a}})Mokhtari, Ozdaglar, and
  Pattathil]{mokhtari2020unified}
Aryan Mokhtari, Asuman Ozdaglar, and Sarath Pattathil.
\newblock A unified analysis of extra-gradient and optimistic gradient methods
  for saddle point problems: Proximal point approach.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 1497--1507. PMLR, 2020{\natexlab{a}}.

\bibitem[Mokhtari et~al.(2020{\natexlab{b}})Mokhtari, Ozdaglar, and
  Pattathil]{mokhtari2020convergence}
Aryan Mokhtari, Asuman~E Ozdaglar, and Sarath Pattathil.
\newblock Convergence rate of ${O}(1/k)$ for optimistic gradient and
  extragradient methods in smooth convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 30\penalty0 (4):\penalty0
  3230--3251, 2020{\natexlab{b}}.

\bibitem[Nemirovski(2004)]{nemirovski2004prox}
Arkadi Nemirovski.
\newblock Prox-method with rate of convergence ${O}(1/t)$ for variational
  inequalities with lipschitz continuous monotone operators and smooth
  convex-concave saddle point problems.
\newblock \emph{SIAM Journal on Optimization}, 15\penalty0 (1):\penalty0
  229--251, 2004.

\bibitem[Nesterov(1983)]{nesterov1983method}
Yurii Nesterov.
\newblock A method for solving a convex programming problem with convergence
  rate ${O} (1/k^2)$.
\newblock In \emph{Soviet Mathematics. Doklady}, volume~27, pages 367--372,
  1983.

\bibitem[Nesterov(2003)]{nesterov2003introductory}
Yurii Nesterov.
\newblock \emph{Introductory lectures on convex optimization: A basic course},
  volume~87.
\newblock Springer Science \& Business Media, 2003.

\bibitem[Nesterov and Spokoiny(2017)]{nesterov2017random}
Yurii Nesterov and Vladimir Spokoiny.
\newblock Random gradient-free minimization of convex functions.
\newblock \emph{Foundations of Computational Mathematics}, 17:\penalty0
  527--566, 2017.

\bibitem[Ouyang and Xu(2021)]{ouyang2021lower}
Yuyuan Ouyang and Yangyang Xu.
\newblock Lower complexity bounds of first-order methods for convex-concave
  bilinear saddle-point problems.
\newblock \emph{Mathematical Programming}, 185\penalty0 (1-2):\penalty0 1--35,
  2021.

\bibitem[Pineau et~al.(2021)Pineau, Vincent-Lamarre, Sinha, Larivi{\`e}re,
  Beygelzimer, d'Alch{\'e} Buc, Fox, and Larochelle]{pineau2021improving}
Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, Vincent Larivi{\`e}re,
  Alina Beygelzimer, Florence d'Alch{\'e} Buc, Emily Fox, and Hugo Larochelle.
\newblock Improving reproducibility in machine learning research ({A} report
  from the {NeurIPS} 2019 reproducibility program).
\newblock \emph{The Journal of Machine Learning Research}, 22\penalty0
  (1):\penalty0 7459--7478, 2021.

\bibitem[Rockafellar(1970)]{rockafellar1970monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators associated with saddle-functions and minimax
  problems.
\newblock \emph{Nonlinear Functional Analysis}, 18\penalty0 (part 1):\penalty0
  397--407, 1970.

\bibitem[Rockafellar(1976)]{rockafellar1976monotone}
R~Tyrrell Rockafellar.
\newblock Monotone operators and the proximal point algorithm.
\newblock \emph{SIAM Journal on Control and Optimization}, 14\penalty0
  (5):\penalty0 877--898, 1976.

\bibitem[Salimans et~al.(2016)Salimans, Goodfellow, Zaremba, Cheung, Radford,
  and Chen]{salimans2016improved}
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and
  Xi~Chen.
\newblock Improved techniques for training {GAN}s.
\newblock \emph{Advances in Neural Information Processing Systems}, 29, 2016.

\bibitem[Shalev-Shwartz et~al.(2009)Shalev-Shwartz, Shamir, Srebro, and
  Sridharan]{shalev2009stochastic}
Shai Shalev-Shwartz, Ohad Shamir, Nathan Srebro, and Karthik Sridharan.
\newblock Stochastic convex optimization.
\newblock In \emph{Conference on Learning Theory}, volume~2, page~5, 2009.

\bibitem[Shamir and Coviello(2020)]{shamir2020anti}
Gil~I Shamir and Lorenzo Coviello.
\newblock Anti-distillation: Improving reproducibility of deep networks.
\newblock \emph{arXiv preprint arXiv:2010.09923}, 2020.

\bibitem[Stonyakin et~al.(2022)Stonyakin, Gasnikov, Dvurechensky, Titov, and
  Alkousa]{stonyakin2022generalized}
Fedor Stonyakin, Alexander Gasnikov, Pavel Dvurechensky, Alexander Titov, and
  Mohammad Alkousa.
\newblock Generalized mirror prox algorithm for monotone variational
  inequalities: Universality and inexact oracle.
\newblock \emph{Journal of Optimization Theory and Applications}, 194\penalty0
  (3):\penalty0 988--1013, 2022.

\bibitem[Thekumparampil et~al.(2019)Thekumparampil, Jain, Netrapalli, and
  Oh]{thekumparampil2019efficient}
Kiran~K Thekumparampil, Prateek Jain, Praneeth Netrapalli, and Sewoong Oh.
\newblock Efficient algorithms for smooth minimax optimization.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Tseng(1995)]{tseng1995linear}
Paul Tseng.
\newblock On linear convergence of iterative methods for the variational
  inequality problem.
\newblock \emph{Journal of Computational and Applied Mathematics}, 60\penalty0
  (1-2):\penalty0 237--252, 1995.

\bibitem[Von~Neumann(1959)]{von1959theory}
John Von~Neumann.
\newblock On the theory of games of strategy.
\newblock \emph{Contributions to the Theory of Games}, 4:\penalty0 13--42,
  1959.

\bibitem[Yang et~al.(2020)Yang, Zhang, Kiyavash, and He]{yang2020catalyst}
Junchi Yang, Siqi Zhang, Negar Kiyavash, and Niao He.
\newblock A catalyst framework for minimax optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  33:\penalty0 5667--5678, 2020.

\bibitem[Yoon and Ryu(2021)]{yoon2021accelerated}
TaeHo Yoon and Ernest~K Ryu.
\newblock Accelerated algorithms for smooth convex-concave minimax problems
  with ${O} (1/k^2)$ rate on squared gradient norm.
\newblock In \emph{International Conference on Machine Learning}, pages
  12098--12109. PMLR, 2021.

\bibitem[Zhang et~al.(2021{\natexlab{a}})Zhang, Hong, Wang, and
  Zhang]{zhang2021generalization}
Junyu Zhang, Mingyi Hong, Mengdi Wang, and Shuzhong Zhang.
\newblock Generalization bounds for stochastic saddle point problems.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, pages 568--576. PMLR, 2021{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{a}})Zhang, Hong, and
  Zhang]{zhang2022lower}
Junyu Zhang, Mingyi Hong, and Shuzhong Zhang.
\newblock On lower iteration complexity bounds for the convex concave saddle
  point problems.
\newblock \emph{Mathematical Programming}, 194\penalty0 (1-2):\penalty0
  901--935, 2022{\natexlab{a}}.

\bibitem[Zhang et~al.(2022{\natexlab{b}})Zhang, Thekumparampil, Oh, and
  He]{zhang2022bring}
Liang Zhang, Kiran~K Thekumparampil, Sewoong Oh, and Niao He.
\newblock Bring your own algorithm for optimal differentially private
  stochastic minimax optimization.
\newblock \emph{Advances in Neural Information Processing Systems},
  35:\penalty0 35174--35187, 2022{\natexlab{b}}.

\bibitem[Zhang et~al.(2019)Zhang, Lucas, Ba, and Hinton]{zhang2019lookahead}
Michael Zhang, James Lucas, Jimmy Ba, and Geoffrey~E Hinton.
\newblock Lookahead optimizer: $k$ steps forward, 1 step back.
\newblock \emph{Advances in Neural Information Processing Systems}, 32, 2019.

\bibitem[Zhang et~al.(2021{\natexlab{b}})Zhang, Yang, Guzm{\'a}n, Kiyavash, and
  He]{zhang2021complexity}
Siqi Zhang, Junchi Yang, Crist{\'o}bal Guzm{\'a}n, Negar Kiyavash, and Niao He.
\newblock The complexity of nonconvex-strongly-concave minimax optimization.
\newblock In \emph{Uncertainty in Artificial Intelligence}, pages 482--492.
  PMLR, 2021{\natexlab{b}}.

\end{thebibliography}
