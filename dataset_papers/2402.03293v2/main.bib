@article{rumelhart1986learning,
  author  = {David E. Rumelhart and Geoffrey E. Hinton and Ronald J. Williams},
  title   = {Learning representations by back-propagating errors},
  journal = {Nature},
  year    = {1986},
  doi     = {10.1038/323533a0},
  url     = {https://doi.org/10.1038/323533a0}
}

@inproceedings{wang2013variance,
  title     = {Variance reduction for stochastic gradient optimization},
  author    = {Wang, Chong and Chen, Xi and Smola, Alexander J and Xing, Eric P},
  booktitle = {NIPS},
  year      = {2013},
  url       = {https://papers.nips.cc/paper_files/paper/2013/hash/9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html}
}

@book{nesterov1998introductory,
  title     = {Introductory Lectures on Convex Optimization: A Basic Course},
  author    = {Nesterov, Yurii},
  year      = {1998},
  publisher = {Springer},
  url       = {https://doi.org/10.1007/978-1-4419-8853-9}
}

@inproceedings{dettmers20218bit,
  title     = {8-bit Optimizers via Block-wise Quantization},
  author    = {Tim Dettmers and M. Lewis and Sam Shleifer and Luke Zettlemoyer},
  booktitle = {ICLR},
  year      = {2021},
  url       = {https://openreview.net/forum?id=shpkpVXzo3h}
}

@article{chen2016training,
  title   = {Training deep nets with sublinear memory cost},
  author  = {Chen, Tianqi and Xu, Bing and Zhang, Chiyuan and Guestrin, Carlos},
  journal = {arXiv preprint arXiv:1604.06174},
  year    = {2016},
  url     = {https://arxiv.org/abs/1604.06174}
}

@article{radford2019language,
  title   = {Language models are unsupervised multitask learners},
  author  = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal = {OpenAI blog},
  year    = {2019},
  url     = {https://openai.com/research/better-language-models}
}

@article{raffel2020exploring,
  title   = {Exploring the limits of transfer learning with a unified text-to-text transformer},
  author  = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal = {JMLR},
  volume  = {21},
  number  = {1},
  pages   = {5485--5551},
  year    = {2020},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@inproceedings{dettmers2023qlora,
  title     = {{QL}o{RA}: Efficient Finetuning of Quantized {LLM}s},
  author    = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
  booktitle = {NeurIPS},
  year      = {2023},
  url       = {https://openreview.net/forum?id=OUIFPHEgJU}
}

@inproceedings{jelassi2022towards,
  title     = {Towards understanding how momentum improves generalization in deep learning},
  author    = {Jelassi, Samy and Li, Yuanzhi},
  booktitle = {ICML},
  pages     = {9965--10040},
  year      = {2022},
  url       = {https://openreview.net/forum?id=lf0W6tcWmh-}
}

@inproceedings{smith2018don,
  title     = {Don't Decay the Learning Rate, Increase the Batch Size},
  author    = {Smith, Samuel L and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V},
  booktitle = {ICLR},
  year      = {2018},
  url       = {https://openreview.net/forum?id=B1Yy1BxCZ}
}

@article{matouvsek2008variants,
  title   = {On variants of the {J}ohnson--{L}indenstrauss lemma},
  author  = {Matou{\v{s}}ek, Ji{\v{r}}{\'\i}},
  journal = {Random Structures \& Algorithms},
  volume  = {33},
  number  = {2},
  pages   = {142--156},
  year    = {2008},
  url     = {https://doi.org/10.1002/rsa.20218}
}

@article{dasgupta2003elementary,
  title   = {An elementary proof of a theorem of {J}ohnson and {L}indenstrauss},
  author  = {Dasgupta, Sanjoy and Gupta, Anupam},
  journal = {Random Structures \& Algorithms},
  volume  = {22},
  number  = {1},
  pages   = {60--65},
  year    = {2003},
  url     = {https://doi.org/10.1002/rsa.10073}
}

@article{goh2017why,
  author  = {Goh, Gabriel},
  title   = {Why Momentum Really Works},
  journal = {Distill},
  year    = {2017},
  url     = {http://distill.pub/2017/momentum}
}


@inproceedings{cutkosky2019momentum,
  title   = {Momentum-based variance reduction in non-convex {SGD}},
  author  = {Cutkosky, Ashok and Orabona, Francesco},
  booktitle = {NeurIPS},
  year    = {2019},
  url     = {https://papers.nips.cc/paper_files/paper/2019/hash/b8002139cdde66b87638f7f91d169d96-Abstract.html}
}

@article{duchi2011adaptive,
  title   = {Adaptive subgradient methods for online learning and stochastic optimization},
  author  = {Duchi, John and Hazan, Elad and Singer, Yoram},
  journal = {JMLR},
  volume  = {12},
  number  = {7},
  year    = {2011},
  url     = {https://jmlr.org/papers/v12/duchi11a.html}
}

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and Jimmy Ba},
  editor    = {Yoshua Bengio and Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {ICLR},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980}
}

@inproceedings{li2021prefix,
  title     = {Prefix-Tuning: Optimizing Continuous Prompts for Generation},
  author    = {Li, Xiang Lisa and Liang, Percy},
  booktitle = {ACL-IJCNLP},
  volume    = {1},
  pages     = {4582--4597},
  year      = {2021},
  url       = {https://aclanthology.org/2021.acl-long.353}
}

@inproceedings{liberty2013simple,
  title     = {Simple and deterministic matrix sketching},
  author    = {Liberty, Edo},
  booktitle = {KDD},
  pages     = {581--588},
  year      = {2013},
  url       = {https://doi.org/10.1145/2487575.2487623}
}

@article{shlens2014tutorial,
  title   = {A Tutorial on Principal Component Analysis},
  author  = {Jonathon Shlens},
  year    = {2014},
  journal = {arXiv preprint arXiv: 1404.1100},
  url     = {https://arxiv.org/abs/1404.1100}
}

@article{lialin2023stack,
  title   = {Stack More Layers Differently: High-Rank Training Through Low-Rank Updates},
  author  = {Vladislav Lialin and Namrata Shivagunde and Sherin Muckatira and Anna Rumshisky},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.05695},
  url     = {https://arxiv.org/abs/2307.05695}
}

@inproceedings{houlsby2019parameterefficient,
  title   = {Parameter-Efficient Transfer Learning for {NLP}},
  author  = {N. Houlsby and A. Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and S. Gelly},
  booktitle = {ICML},
  year    = {2019},
  url     = {https://proceedings.mlr.press/v97/houlsby19a.html},
  pages   = {2790--2799}
}


@misc{deepmind2020jax,
  title  = {The {D}eep{M}ind {JAX} {E}cosystem},
  author = {DeepMind and Babuschkin, Igor and Baumli, Kate and Bell, Alison and Bhupatiraju, Surya and Bruce, Jake and Buchlovsky, Peter and Budden, David and Cai, Trevor and Clark, Aidan and Danihelka, Ivo and Dedieu, Antoine and Fantacci, Claudio and Godwin, Jonathan and Jones, Chris and Hemsley, Ross and Hennigan, Tom and Hessel, Matteo and Hou, Shaobo and Kapturowski, Steven and Keck, Thomas and Kemaev, Iurii and King, Michael and Kunesch, Markus and Martens, Lena and Merzic, Hamza and Mikulik, Vladimir and Norman, Tamara and Papamakarios, George and Quan, John and Ring, Roman and Ruiz, Francisco and Sanchez, Alvaro and Sartran, Laurent and Schneider, Rosalia and Sezener, Eren and Spencer, Stephen and Srinivasan, Srivatsan and Stanojevi\'{c}, Milo\v{s} and Stokowiec, Wojciech and Wang, Luyu and Zhou, Guangyao and Viola, Fabio},
  url    = {http://github.com/google-deepmind},
  year   = {2020}
}

@article{finesso2006nonnegative,
  title   = {Nonnegative matrix factorization and {I}-divergence alternating minimization},
  author  = {Finesso, Lorenzo and Spreij, Peter},
  journal = {Linear Algebra and its Applications},
  volume  = {416},
  number  = {2-3},
  pages   = {270--287},
  year    = {2006},
  url     = {https://doi.org/10.1016/j.laa.2005.11.012}
}

@article{hoffmann2022training,
  title   = {Training compute-optimal large language models},
  author  = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
  journal = {arXiv preprint arXiv:2203.15556},
  year    = {2022},
  url     = {https://arxiv.org/abs/2203.15556}
}

@inproceedings{dasgupta2000experiments,
  title     = {Experiments with random projection},
  author    = {Dasgupta, Sanjoy},
  booktitle = {UAI},
  pages     = {143--151},
  year      = {2000},
  url       = {https://dl.acm.org/doi/10.5555/647234.719759}
}

@inproceedings{loshchilov2018decoupled,
  title     = {Decoupled Weight Decay Regularization},
  author    = {Ilya Loshchilov and Frank Hutter},
  booktitle = {ICLR},
  year      = {2019},
  url       = {https://openreview.net/forum?id=Bkg6RiCqY7}
}

@inproceedings{loshchilov2017sgdr,
  title     = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
  author    = {Ilya Loshchilov and Frank Hutter},
  booktitle = {ICLR},
  year      = {2017},
  url       = {https://openreview.net/forum?id=Skq89Scxx}
}

@inproceedings{li2020rethinking,
  title     = {Rethinking the Hyperparameters for Fine-tuning},
  author    = {Hao Li and Pratik Chaudhari and Hao Yang and Michael Lam and Avinash Ravichandran and Rahul Bhotika and Stefano Soatto},
  booktitle = {ICLR},
  year      = {2020},
  url       = {https://openreview.net/forum?id=B1g8VkHFPH}
}

@inproceedings{bingham2001random,
  title     = {Random projection in dimensionality reduction: applications to image and text data},
  author    = {Bingham, Ella and Mannila, Heikki},
  booktitle = {KDD},
  pages     = {245--250},
  year      = {2001},
  url       = {https://dl.acm.org/doi/10.1145/502512.502546}
}

@inproceedings{lin2004rouge,
  title     = {{ROUGE}: A Package for Automatic Evaluation of Summaries},
  author    = {Lin, Chin-Yew},
  booktitle = {Text Summarization Branches Out},
  year      = {2004},
  url       = {https://aclanthology.org/W04-1013},
  pages     = {74--81}
}

@inproceedings{feinberg2023sketchy,
  title     = {Sketchy: Memory-efficient Adaptive Regularization with Frequent Directions},
  author    = {Vladimir Feinberg and Xinyi Chen and Y. Jennifer Sun and Rohan Anil and Elad Hazan},
  booktitle = {NeurIPS},
  year      = {2023},
  url       = {https://openreview.net/forum?id=DeZst6dKyi}
}

'@inproceedings{lin2020exploring,
  title     = {Exploring Versatile Generative Language Model Via Parameter-Efficient Transfer Learning},
  author    = {Lin, Zhaojiang and Madotto, Andrea and Fung, Pascale},
  booktitle = {EMNLP Findings},
  pages     = {441--459},
  year      = {2020},
  url       = {https://aclanthology.org/2020.findings-emnlp.41}
}

@inproceedings{cettolo2017overview,
  title     = {Overview of the {IWSLT} 2017 Evaluation Campaign},
  author    = {Cettolo, Mauro  and
               Federico, Marcello  and
               Bentivogli, Luisa  and
               Niehues, Jan  and
               St{\"u}ker, Sebastian  and
               Sudoh, Katsuhito  and
               Yoshino, Koichiro  and
               Federmann, Christian},
  booktitle = {IWSLT},
  year      = {2017},
  url       = {https://aclanthology.org/2017.iwslt-1.1},
  pages     = {2--14}
}

@inproceedings{narayan2018don,
  title     = {Donâ€™t Give Me the Details, Just the Summary! {T}opic-Aware Convolutional Neural Networks for Extreme Summarization},
  author    = {Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  booktitle = {EMNLP},
  pages     = {1797--1807},
  year      = {2018},
  url       = {https://aclanthology.org/D18-1206}
}

@inproceedings{post2018call,
  title     = {A Call for Clarity in Reporting {BLEU} Scores},
  author    = {Post, Matt},
  booktitle = {WMT},
  year      = {2018},
  url       = {https://aclanthology.org/W18-6319},
  pages     = {186--191}
}

@misc{jax2018github,
  author  = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title   = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url     = {http://github.com/google/jax},
  version = {0.3.13},
  year    = {2018}
}

@article{rae2021scaling,
  title   = {Scaling language models: Methods, analysis \& insights from training {G}opher},
  author  = {Rae, Jack W and Borgeaud, Sebastian and Cai, Trevor and Millican, Katie and Hoffmann, Jordan and Song, Francis and Aslanides, John and Henderson, Sarah and Ring, Roman and Young, Susannah and others},
  journal = {arXiv preprint arXiv:2112.11446},
  year    = {2021},
  url     = {https://arxiv.org/abs/2112.11446}
}

@article{hinton2012neural,
  title   = {Neural networks for machine learning lecture 6a overview of mini-batch gradient descent},
  author  = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
  journal = {Coursera},
  year    = {2012},
  url     = {https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf}
}

@article{laurent2000adaptive,
  title   = {Adaptive estimation of a quadratic functional by model selection},
  author  = {Laurent, Beatrice and Massart, Pascal},
  journal = {Annals of statistics},
  volume  = {28},
  number  = {5},
  pages   = {1302--1338},
  year    = {2000},
  url     = {https://www.jstor.org/stable/2674095}
}

@article{xiao2017fashion,
  author  = {Han Xiao and Kashif Rasul and Roland Vollgraf},
  title   = {Fashion-{MNIST}: a Novel Image Dataset for Benchmarking Machine Learning Algorithms},
  year    = {2017},
  journal = {arXiv preprint arXiv:1708.07747},
  url     = {https://arxiv.org/abs/1708.07747}
}

@inproceedings{indyk1998approximate,
  title     = {Approximate nearest neighbors: towards removing the curse of dimensionality},
  author    = {Indyk, Piotr and Motwani, Rajeev},
  booktitle = {STOC},
  pages     = {604--613},
  year      = {1998},
  url       = {https://dl.acm.org/doi/10.1145/276698.276876}
}

@inproceedings{zaken2022bitfit,
  title     = {Bit{F}it: Simple Parameter-efficient Fine-tuning for Transformer-based Masked Language-models},
  author    = {Zaken, Elad Ben and Goldberg, Yoav and Ravfogel, Shauli},
  booktitle = {ACL},
  pages     = {1--9},
  volume    = {2},
  year      = {2022},
  url       = {https://aclanthology.org/2022.acl-short.1}
}

@inproceedings{brown2020language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {NeurIPS},
  pages     = {1877--1901},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  year      = {2020}
}

@article{masters2018revisiting,
  title   = {Revisiting small batch training for deep neural networks},
  author  = {Masters, Dominic and Luschi, Carlo},
  journal = {arXiv preprint arXiv:1804.07612},
  year    = {2018},
  url     = {https://arxiv.org/abs/1804.07612}
}

@article{wu2021recursively,
  title   = {Recursively summarizing books with human feedback},
  author  = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M and Stiennon, Nisan and Lowe, Ryan and Leike, Jan and Christiano, Paul},
  journal = {arXiv preprint arXiv:2109.10862},
  year    = {2021},
  url     = {https://arxiv.org/abs/2109.10862}
}

@inproceedings{vaswani2017attention,
  title   = {Attention is all you need},
  author  = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle = {NIPS},
  year    = {2017},
  url     = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html}
}

@inproceedings{malladi2023finetuning,
  title     = {Fine-Tuning Language Models with Just Forward Passes},
  author    = {Sadhika Malladi and Tianyu Gao and Eshaan Nichani and Alex Damian and Jason D. Lee and Danqi Chen and Sanjeev Arora},
  booktitle = {NeurIPS},
  year      = {2023},
  url       = {https://openreview.net/forum?id=Vota6rFhBQ}
}

@inproceedings{
micikevicius2018mixed,
title={Mixed Precision Training},
author={Paulius Micikevicius and Sharan Narang and Jonah Alben and Gregory Diamos and Erich Elsen and David Garcia and Boris Ginsburg and Michael Houston and Oleksii Kuchaiev and Ganesh Venkatesh and Hao Wu},
booktitle={ICLR},
year={2018},
url={https://openreview.net/forum?id=r1gs9JgRZ},
}

@inproceedings{oktay2020randomized,
  title     = {Randomized Automatic Differentiation},
  author    = {Oktay, Deniz and McGreivy, Nick and Aduol, Joshua and Beatson, Alex and Adams, Ryan P},
  booktitle = {ICLR},
  year      = {2020},
  url = {https://openreview.net/forum?id=xpx9zj7CUlY}
}

@inproceedings{rombach2022high,
  title     = {High-resolution image synthesis with latent diffusion models},
  author    = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle = {CVPR},
  pages     = {10684--10695},
  year      = {2022},
  url       = {https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html}
}

@inproceedings{shazeer2018adafactor,
  title     = {Adafactor: Adaptive learning rates with sublinear memory cost},
  author    = {Shazeer, Noam and Stern, Mitchell},
  booktitle = {ICML},
  pages     = {4596--4604},
  year      = {2018},
  url       = {https://proceedings.mlr.press/v80/shazeer18a.html}
}

@inproceedings{hu2022lora,
  title     = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
  author    = {Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
  booktitle = {ICLR},
  year      = {2022},
  url       = {https://openreview.net/forum?id=nZeVKeeFYf9}
}


@article{dosovitskiy2020image,
  title     = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and G. Heigold and S. Gelly and Jakob Uszkoreit and N. Houlsby},
  journal   = {ICML},
  year      = {2020},
  url = {https://openreview.net/forum?id=YicbFdNTTy}
}

@article{krizhevsky2009learning,
  title   = {Learning multiple layers of features from tiny images},
  author  = {Krizhevsky, Alex and Hinton, Geoffrey and others},
  year    = {2009},
  journal = {Technical report},
  url     = {https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf}
}

@article{zhao2024galore,
  title   = {Ga{L}ore: Memory-Efficient {LLM} Training by Gradient Low-Rank Projection},
  author  = {Jiawei Zhao and Zhenyu Zhang and Beidi Chen and Zhangyang Wang and Anima Anandkumar and Yuandong Tian},
  year    = {2024},
  journal = {arXiv preprint arXiv: 2403.03507},
  url = {https://arxiv.org/abs/2403.03507}
}

@article{lv2023full,
  title   = {Full Parameter Fine-tuning for Large Language Models with Limited Resources},
  author  = {Kai Lv and Yuqing Yang and Tengxiao Liu and Qinghui Gao and Qipeng Guo and Xipeng Qiu},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2306.09782},
  url     = {https://arxiv.org/abs/2306.09782}
}

@inproceedings{rothchild2020fetchsgd,
  title     = {{F}etch{SGD}: Communication-Efficient Federated Learning with Sketching},
  author    = {Rothchild, Daniel and Panda, Ashwinee and Ullah, Enayat and Ivkin, Nikita and Stoica, Ion and Braverman, Vladimir and Gonzalez, Joseph and Arora, Raman},
  booktitle = {ICML},
  pages     = {8253-8265},
  year      = {2020},
  url       = {https://proceedings.mlr.press/v119/rothchild20a.html},
}

@article{touvron2023llama,
  title   = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author  = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year    = {2023},
  journal = {arXiv preprint arXiv: 2307.09288},
  url={https://arxiv.org/abs/2307.09288}
}