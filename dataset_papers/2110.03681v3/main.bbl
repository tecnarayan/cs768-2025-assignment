\begin{thebibliography}{50}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[{Aji} \& {Heafield}(2017){Aji} and {Heafield}]{aji2017sparse}
{Aji}, A.~F. and {Heafield}, K.
\newblock Sparse communication for distributed gradient descent.
\newblock In \emph{Proceedings of the 2017 Conference on Empirical Methods in
  Natural Language Processing}, 2017.

\bibitem[{Alemohammad} et~al.(2021){Alemohammad}, {Wang}, {Balestriero}, and
  {Baraniuk}]{alemohammad2021the}
{Alemohammad}, S., {Wang}, Z., {Balestriero}, R., and {Baraniuk}, R.
\newblock The recurrent neural tangent kernel.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Alistarh et~al.(2017)Alistarh, Grubic, Li, Tomioka, and
  Vojnovic]{alistarh2017qsgd}
Alistarh, D., Grubic, D., Li, J., Tomioka, R., and Vojnovic, M.
\newblock {QSGD}: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock 2017.

\bibitem[{Alistarh} et~al.(2018){Alistarh}, {Hoefler}, {Johansson},
  {Konstantinov}, {Khirirat}, and {Renggli}]{alistarh2018the}
{Alistarh}, D., {Hoefler}, T., {Johansson}, M., {Konstantinov}, N., {Khirirat},
  S., and {Renggli}, C.
\newblock The convergence of sparsified gradient methods.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[{Arora} et~al.(2019){Arora}, {Du}, {Hu}, {Li}, {Salakhutdinov}, and
  {Wang}]{arora2019on}
{Arora}, S., {Du}, S.~S., {Hu}, W., {Li}, Z., {Salakhutdinov}, R., and {Wang},
  R.
\newblock On exact computation with an infinitely wide neural net.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[Caldas et~al.(2018)Caldas, Duddu, Wu, Li, Kone{\v{c}}n{\`y}, McMahan,
  Smith, and Talwalkar]{caldas2018leaf}
Caldas, S., Duddu, S. M.~K., Wu, P., Li, T., Kone{\v{c}}n{\`y}, J., McMahan,
  H.~B., Smith, V., and Talwalkar, A.
\newblock Leaf: A benchmark for federated settings.
\newblock \emph{arXiv preprint arXiv:1812.01097}, 2018.

\bibitem[Chen \& Chao(2021)Chen and Chao]{chen2021fedbe}
Chen, H.-Y. and Chao, W.-L.
\newblock Fed{BE}: Making {B}ayesian model ensemble applicable to federated
  learning.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[{Chen} et~al.(2020){Chen}, {Cao}, {Gu}, and {Zhang}]{chen2020a}
{Chen}, Z., {Cao}, Y., {Gu}, Q., and {Zhang}, T.
\newblock A generalized neural tangent kernel analysis for two-layer neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Cheng et~al.(2021)Cheng, Eykholt, Gu, Jamjoom, Jayaram, Valdez, and
  Verma]{cheng2021separation}
Cheng, P.-C., Eykholt, K., Gu, Z., Jamjoom, H., Jayaram, K., Valdez, E., and
  Verma, A.
\newblock Separation of powers in federated learning.
\newblock \emph{arXiv preprint arXiv:2105.09400}, 2021.

\bibitem[{Du} et~al.(2019){Du}, {Zhai}, {Poczos}, and {Singh}]{du2019gradient}
{Du}, S.~S., {Zhai}, X., {Poczos}, B., and {Singh}, A.
\newblock Gradient descent provably optimizes over-parameterized neural
  networks.
\newblock In \emph{International Conference on Learning Representations}, 2019.

\bibitem[{Dukler} et~al.(2020){Dukler}, {Montufar}, and
  {Gu}]{dukler2020optimization}
{Dukler}, Y., {Montufar}, G., and {Gu}, Q.
\newblock Optimization theory for {R}e{LU} neural networks trained with
  normalization layers.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[{Girgis} et~al.(2021){Girgis}, {Data}, {Diggavi}, {Kairouz}, and
  {Suresh}]{girgis2021shuffled}
{Girgis}, A.~M., {Data}, D., {Diggavi}, S.~N., {Kairouz}, P., and {Suresh},
  A.~T.
\newblock Shuffled model of differential privacy in federated learning.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem[{Gonzalez} \& {Woods}(2014){Gonzalez} and
  {Woods}]{gonzalez2014digital}
{Gonzalez}, R.~C. and {Woods}, R.~E.
\newblock \emph{Digital {I}mage {P}rocessing, 3rd {E}dition}.
\newblock 2014.

\bibitem[Good(1976)]{good1976application}
Good, I.~J.
\newblock On the application of symmetric {D}irichlet distributions and their
  mixtures to contingency tables.
\newblock \emph{The Annals of Statistics}, 4\penalty0 (6):\penalty0 1159--1189,
  1976.

\bibitem[Haddadpour et~al.(2021)Haddadpour, Kamani, Mokhtari, and
  Mahdavi]{haddadpour2021federated}
Haddadpour, F., Kamani, M.~M., Mokhtari, A., and Mahdavi, M.
\newblock Federated learning with compression: Unified analysis and sharp
  guarantees.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2021.

\bibitem[{He} et~al.(2016){He}, {Zhang}, {Ren}, and {Sun}]{he2016deep}
{He}, K., {Zhang}, X., {Ren}, S., and {Sun}, J.
\newblock Deep residual learning for image recognition.
\newblock In \emph{International Conference on Computer Vision and Pattern
  Recognition}, 2016.

\bibitem[Hsu et~al.(2019)Hsu, Qi, and Brown]{hsu2019measuring}
Hsu, T.-M.~H., Qi, H., and Brown, M.
\newblock Measuring the effects of non-identical data distribution for
  federated visual classification.
\newblock \emph{arXiv preprint arXiv:1909.06335}, 2019.

\bibitem[Huang et~al.(2021)Huang, Li, Song, and Yang]{huang2021fl}
Huang, B., Li, X., Song, Z., and Yang, X.
\newblock {FL-NTK}: A neural tangent kernel-based framework for federated
  learning convergence analysis.
\newblock \emph{arXiv preprint arXiv:2105.05001}, 2021.

\bibitem[{Jacot} et~al.(2018){Jacot}, {Gabriel}, and
  {Hongler}]{jacot2018neural}
{Jacot}, A., {Gabriel}, F., and {Hongler}, C.
\newblock Neural tangent kernel: Convergence and generalization in neural
  networks.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2018.

\bibitem[Kairouz et~al.(2021)Kairouz, McMahan, Avent, Bellet, Bennis,
  et~al.]{kairouz2021advances}
Kairouz, P., McMahan, H.~B., Avent, B., Bellet, A., Bennis, M., et~al.
\newblock Advances and open problems in federated learning.
\newblock \emph{Foundations and Trends in Machine Learning}, 2021.

\bibitem[Karimireddy et~al.(2020)Karimireddy, Kale, Mohri, Reddi, Stich, and
  Suresh]{karimireddy2020scaffold}
Karimireddy, S.~P., Kale, S., Mohri, M., Reddi, S., Stich, S., and Suresh,
  A.~T.
\newblock Scaffold: Stochastic controlled averaging for federated learning.
\newblock In \emph{International Conference on Machine Learning}, 2020.

\bibitem[Kolda \& Bader(2009)Kolda and Bader]{kolda2009tensor}
Kolda, T.~G. and Bader, B.~W.
\newblock Tensor decompositions and applications.
\newblock \emph{SIAM {R}eview}, 51\penalty0 (3):\penalty0 455--500, 2009.

\bibitem[{Krizhevsky}(2009)]{krizhevsky2009learning}
{Krizhevsky}, A.
\newblock Learning multiple layers of features from tiny images.
\newblock \emph{Master thesis, Dept. of Comput. Sci., Univ. of Toronto,
  Toronto, Canada}, 2009.

\bibitem[LeCun et~al.(1998)LeCun, Bottou, Bengio, and
  Haffner]{lecun1998gradient}
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P.
\newblock Gradient-based learning applied to document recognition.
\newblock \emph{Proceedings of the IEEE}, 86\penalty0 (11):\penalty0
  2278--2324, 1998.

\bibitem[Lee et~al.(2019)Lee, Xiao, Schoenholz, Bahri, Novak, Sohl-Dickstein,
  and Pennington]{lee2019wide}
Lee, J., Xiao, L., Schoenholz, S., Bahri, Y., Novak, R., Sohl-Dickstein, J.,
  and Pennington, J.
\newblock Wide neural networks of any depth evolve as linear models under
  gradient descent.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\bibitem[{Lee} et~al.(2020){Lee}, {Schoenholz}, {Pennington}, {Adlam}, {Xiao},
  {Novak}, and {Sohl-Dickstein}]{lee2020finite}
{Lee}, J., {Schoenholz}, S.~S., {Pennington}, J., {Adlam}, B., {Xiao}, L.,
  {Novak}, R., and {Sohl-Dickstein}, J.
\newblock Finite versus infinite neural networks: An empirical study.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Li et~al.(2020)Li, Sahu, Talwalkar, and Smith]{li2020flsurvey}
Li, T., Sahu, A.~K., Talwalkar, A., and Smith, V.
\newblock Federated learning: Challenges, methods, and future directions.
\newblock \emph{IEEE Signal Processing Magazine}, 37\penalty0 (3):\penalty0
  50--60, 2020.

\bibitem[{Li} et~al.(2020){Li}, {Sahu}, {Zaheer}, {Sanjabi}, {Talwalkar}, and
  {Smith}]{li2020federated}
{Li}, T., {Sahu}, A.~K., {Zaheer}, M., {Sanjabi}, M., {Talwalkar}, A., and
  {Smith}, V.
\newblock Federated optimization in heterogeneous networks.
\newblock 2020.

\bibitem[{Li} et~al.(2021){Li}, {Jiang}, {Zhang}, {Kamp}, and
  {Dou}]{li2021fedbn}
{Li}, X., {Jiang}, M., {Zhang}, X., {Kamp}, M., and {Dou}, Q.
\newblock Fed{BN}: Federated learning on non-iid features via local batch
  normalization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Liang et~al.(2019)Liang, Liu, Ziyin, Allen, Auerbach, Brent,
  Salakhutdinov, and Morency]{liang2019think}
Liang, P.~P., Liu, T., Ziyin, L., Allen, N.~B., Auerbach, R.~P., Brent, D.,
  Salakhutdinov, R., and Morency, L.-P.
\newblock Think locally, act globally: Federated learning with local and global
  representations.
\newblock \emph{NeurIPS Workshop on Federated Learning}, 2019.

\bibitem[McMahan et~al.(2017)McMahan, Moore, Ramage, Hampson, and
  y~Arcas]{mcmahan2017communication}
McMahan, B., Moore, E., Ramage, D., Hampson, S., and y~Arcas, B.~A.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock In \emph{Artificial Intelligence and Statistics}, 2017.

\bibitem[Mohri et~al.(2018)Mohri, Rostamizadeh, and
  Talwalkar]{mohri2018foundations}
Mohri, M., Rostamizadeh, A., and Talwalkar, A.
\newblock \emph{Foundations of machine learning}.
\newblock MIT press, 2018.

\bibitem[{Nakkiran} et~al.(2020){Nakkiran}, {Kaplun}, {Bansal}, {Yang},
  {Barak}, and {Sutskever}]{nakkiran2020deep}
{Nakkiran}, P., {Kaplun}, G., {Bansal}, Y., {Yang}, T., {Barak}, B., and
  {Sutskever}, I.
\newblock Deep double descent: Where bigger models and more data hurt.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[Nasr et~al.(2018)Nasr, Shokri, and Houmansadr]{nasr2018machine}
Nasr, M., Shokri, R., and Houmansadr, A.
\newblock Machine learning with membership privacy using adversarial
  regularization.
\newblock In \emph{ACM SIGSAC Conference on Computer and Communications
  Security}, pp.\  634--646, 2018.

\bibitem[{Reddi} et~al.(2021){Reddi}, {Charles}, {Zaheer}, {Garrett}, {Rush},
  {Konečný}, {Kumar}, and {McMahan}]{reddi2021adaptive}
{Reddi}, S.~J., {Charles}, Z., {Zaheer}, M., {Garrett}, Z., {Rush}, K.,
  {Konečný}, J., {Kumar}, S., and {McMahan}, H.~B.
\newblock Adaptive federated optimization.
\newblock In \emph{International Conference on Learning Representations}, 2021.

\bibitem[Reisizadeh et~al.(2020)Reisizadeh, Mokhtari, Hassani, Jadbabaie, and
  Pedarsani]{reisizadeh2020fedpaq}
Reisizadeh, A., Mokhtari, A., Hassani, H., Jadbabaie, A., and Pedarsani, R.
\newblock Fed{PAQ}: A communication-efficient federated learning method with
  periodic averaging and quantization.
\newblock In \emph{International Conference on Artificial Intelligence and
  Statistics}, 2020.

\bibitem[Sattler et~al.(2019)Sattler, Wiedemann, M{\"u}ller, and
  Samek]{sattler2019robust}
Sattler, F., Wiedemann, S., M{\"u}ller, K.-R., and Samek, W.
\newblock Robust and communication-efficient federated learning from non-iid
  data.
\newblock \emph{IEEE Transactions on Neural Networks and Learning Systems},
  31\penalty0 (9):\penalty0 3400--3413, 2019.

\bibitem[Seo et~al.(2020)Seo, Park, Oh, Bennis, and Kim]{seo2020federated}
Seo, H., Park, J., Oh, S., Bennis, M., and Kim, S.-L.
\newblock Federated knowledge distillation.
\newblock \emph{arXiv preprint arXiv:2011.02367}, 2020.

\bibitem[{Smith} et~al.(2017){Smith}, {Chiang}, {Sanjabi}, and
  {Talwalkar}]{smith2017federated}
{Smith}, V., {Chiang}, C.-K., {Sanjabi}, M., and {Talwalkar}, A.
\newblock Federated multi-task learning.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2017.

\bibitem[{Su} et~al.(2021){Su}, {Xu}, and {Yang}]{su2021achieving}
{Su}, L., {Xu}, J., and {Yang}, P.
\newblock Achieving statistical optimality of federated learning: Beyond
  stationary points.
\newblock \emph{arXiv preprint arXiv:2106.15216}, 2021.

\bibitem[Tran et~al.(2019)Tran, Bao, Zomaya, Nguyen, and
  Hong]{tran2019federated}
Tran, N.~H., Bao, W., Zomaya, A., Nguyen, M.~N., and Hong, C.~S.
\newblock Federated learning over wireless networks: Optimization model design
  and analysis.
\newblock In \emph{IEEE Conference on Computer Communications}, pp.\
  1387--1395. IEEE, 2019.

\bibitem[Van~Oorschot(2020)]{van2020computer}
Van~Oorschot, P.~C.
\newblock \emph{Computer Security and the Internet: Tools and Jewels}.
\newblock Springer Nature, 2020.

\bibitem[Wang et~al.(2020)Wang, Yurochkin, Sun, Papailiopoulos, and
  Khazaeni]{wang2020federated}
Wang, H., Yurochkin, M., Sun, Y., Papailiopoulos, D., and Khazaeni, Y.
\newblock Federated learning with matched averaging.
\newblock In \emph{International Conference on Learning Representations}, 2020.

\bibitem[{Wang} et~al.(2020){Wang}, {Liu}, {Liang}, {Joshi}, and
  {Poor}]{wang2020tackling}
{Wang}, J., {Liu}, Q., {Liang}, H., {Joshi}, G., and {Poor}, H.~V.
\newblock Tackling the objective inconsistency problem in heterogeneous
  federated optimization.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2020.

\bibitem[Wang et~al.(2021)Wang, Charles, Xu, Joshi, McMahan, Al-Shedivat,
  Andrew, Avestimehr, Daly, Data, et~al.]{wang2021field}
Wang, J., Charles, Z., Xu, Z., Joshi, G., McMahan, H.~B., Al-Shedivat, M.,
  Andrew, G., Avestimehr, S., Daly, K., Data, D., et~al.
\newblock A field guide to federated optimization.
\newblock \emph{arXiv preprint arXiv:2107.06917}, 2021.

\bibitem[Xiao et~al.(2017)Xiao, Rasul, and Vollgraf]{xiao2017fashion}
Xiao, H., Rasul, K., and Vollgraf, R.
\newblock Fashion-{MNIST}: a novel image dataset for benchmarking machine
  learning algorithms.
\newblock \emph{arXiv preprint arXiv:1708.07747}, 2017.

\bibitem[{Yang} \& {Littwin}(2021){Yang} and {Littwin}]{yang2021tensor}
{Yang}, G. and {Littwin}, E.
\newblock Tensor programs {II}b: Architectural universality of neural tangent
  kernel training dynamics.
\newblock In \emph{International Conference on Machine Learning}, 2021.

\bibitem[Zhang et~al.(2020)Zhang, Jia, Pei, Wang, Li, and
  Song]{zhang2020secret}
Zhang, Y., Jia, R., Pei, H., Wang, W., Li, B., and Song, D.
\newblock The secret revealer: Generative model-inversion attacks against deep
  neural networks.
\newblock In \emph{IEEE/CVF Conference on Computer Vision and Pattern
  Recognition}, pp.\  253--261, 2020.

\bibitem[Zhao et~al.(2018)Zhao, Li, Lai, Suda, Civin, and
  Chandra]{zhao2018federated}
Zhao, Y., Li, M., Lai, L., Suda, N., Civin, D., and Chandra, V.
\newblock Federated learning with non-iid data.
\newblock \emph{arXiv preprint arXiv:1806.00582}, 2018.

\bibitem[{Zhu} et~al.(2019){Zhu}, {Liu}, and {Han}]{zhu2019deep}
{Zhu}, L., {Liu}, Z., and {Han}, S.
\newblock Deep leakage from gradients.
\newblock In \emph{Advances in Neural Information Processing Systems}, 2019.

\end{thebibliography}
