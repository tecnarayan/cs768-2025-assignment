\begin{thebibliography}{10}

\bibitem{abadi2016tensorflow}
Mart{\'\i}n Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey
  Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, et~al.
\newblock {TensorFlow}: A system for large-scale machine learning.
\newblock In {\em OSDI}, volume~16, pages 265--283, 2016.

\bibitem{aji2017sparse}
Alham~Fikri Aji and Kenneth Heafield.
\newblock Sparse communication for distributed gradient descent.
\newblock {\em arXiv preprint arXiv:1704.05021}, 2017.

\bibitem{alexander1987transient}
S~Alexander.
\newblock Transient weight misadjustment properties for the finite precision
  {LMS} algorithm.
\newblock {\em IEEE Transactions on Acoustics, Speech, and Signal Processing},
  35(9):1250--1258, 1987.

\bibitem{alistarh2017qsgd}
Dan Alistarh, Demjan Grubic, Jerry Li, Ryota Tomioka, and Milan Vojnovic.
\newblock Qsgd: Communication-efficient {SGD} via gradient quantization and
  encoding.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1707--1718, 2017.

\bibitem{bermudez1996nonlinear}
Jos{\'e} Carlos~M Bermudez and Neil~J Bershad.
\newblock A nonlinear analytical model for the quantized {LMS} algorithm-the
  arbitrary step size case.
\newblock {\em IEEE Transactions on Signal Processing}, 44(5):1175--1183, 1996.

\bibitem{bernstein2018signsgd}
Jeremy Bernstein, Yu-Xiang Wang, Kamyar Azizzadenesheli, and Anima Anandkumar.
\newblock {signSGD:} compressed optimisation for non-convex problems.
\newblock {\em arXiv preprint arXiv:1802.04434}, 2018.

\bibitem{bubeck2015convex}
S{\'e}bastien Bubeck et~al.
\newblock Convex optimization: Algorithms and complexity.
\newblock {\em Foundations and Trends{\textregistered} in Machine Learning},
  8(3-4):231--357, 2015.

\bibitem{chandrasekaran2012convex}
Venkat Chandrasekaran, Benjamin Recht, Pablo~A Parrilo, and Alan~S Willsky.
\newblock The convex geometry of linear inverse problems.
\newblock {\em Foundations of Computational mathematics}, 12(6):805--849, 2012.

\bibitem{chen2017adacomp}
Chia-Yu Chen, Jungwook Choi, Daniel Brand, Ankur Agrawal, Wei Zhang, and
  Kailash Gopalakrishnan.
\newblock Adacomp: Adaptive residual gradient compression for data-parallel
  distributed training.
\newblock {\em arXiv preprint arXiv:1712.02679}, 2017.

\bibitem{chen2016revisiting}
Jianmin Chen, Xinghao Pan, Rajat Monga, Samy Bengio, and Rafal Jozefowicz.
\newblock Revisiting distributed synchronous {SGD}.
\newblock {\em arXiv preprint arXiv:1604.00981}, 2016.

\bibitem{chen2015mxnet}
Tianqi Chen, Mu~Li, Yutian Li, Min Lin, Naiyan Wang, Minjie Wang, Tianjun Xiao,
  Bing Xu, Chiyuan Zhang, and Zheng Zhang.
\newblock Mxnet: A flexible and efficient machine learning library for
  heterogeneous distributed systems.
\newblock {\em arXiv preprint arXiv:1512.01274}, 2015.

\bibitem{cotter2011better}
Andrew Cotter, Ohad Shamir, Nati Srebro, and Karthik Sridharan.
\newblock Better mini-batch algorithms via accelerated gradient methods.
\newblock In {\em Advances in neural information processing systems}, pages
  1647--1655, 2011.

\bibitem{dalcin2011parallel}
Lisandro~D Dalcin, Rodrigo~R Paz, Pablo~A Kler, and Alejandro Cosimo.
\newblock Parallel distributed computing using python.
\newblock {\em Advances in Water Resources}, 34(9):1124--1139, 2011.

\bibitem{de2016big}
Soham De, Abhay Yadav, David Jacobs, and Tom Goldstein.
\newblock {Big Batch SGD}: Automated inference using adaptive batch sizes.
\newblock {\em arXiv preprint arXiv:1610.05792}, 2016.

\bibitem{de2017understanding}
Christopher De~Sa, Matthew Feldman, Christopher R{\'e}, and Kunle Olukotun.
\newblock Understanding and optimizing asynchronous low-precision stochastic
  gradient descent.
\newblock In {\em Proceedings of the 44th Annual International Symposium on
  Computer Architecture}, pages 561--574. ACM, 2017.

\bibitem{de2018high}
Christopher De~Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher~R
  Aberger, Kunle Olukotun, and Christopher R{\'e}.
\newblock High-accuracy low-precision training.
\newblock {\em arXiv preprint arXiv:1803.03383}, 2018.

\bibitem{de2015global}
Christopher De~Sa, Christopher Re, and Kunle Olukotun.
\newblock Global convergence of stochastic gradient descent for some non-convex
  matrix problems.
\newblock In {\em International Conference on Machine Learning}, pages
  2332--2341, 2015.

\bibitem{de2015taming}
Christopher~M De~Sa, Ce~Zhang, Kunle Olukotun, and Christopher R{\'e}.
\newblock Taming the wild: A unified analysis of hogwild-style algorithms.
\newblock In {\em Advances in neural information processing systems}, pages
  2674--2682, 2015.

\bibitem{dean2012large}
Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao,
  Andrew Senior, Paul Tucker, Ke~Yang, Quoc~V Le, et~al.
\newblock Large scale distributed deep networks.
\newblock In {\em Advances in neural information processing systems}, pages
  1223--1231, 2012.

\bibitem{duchi2015asynchronous}
John~C Duchi, Sorathan Chaturapruek, and Christopher R{\'e}.
\newblock Asynchronous stochastic convex optimization.
\newblock {\em arXiv preprint arXiv:1508.00882}, 2015.

\bibitem{ghadimi2013stochastic}
Saeed Ghadimi and Guanghui Lan.
\newblock Stochastic first-and zeroth-order methods for nonconvex stochastic
  programming.
\newblock {\em SIAM Journal on Optimization}, 23(4):2341--2368, 2013.

\bibitem{gitlin1973design}
R~Gitlin, J~Mazo, and M~Taylor.
\newblock On the design of gradient algorithms for digitally implemented
  adaptive filters.
\newblock {\em IEEE Transactions on Circuit Theory}, 20(2):125--136, 1973.

\bibitem{grubic2018synchronous}
Demjan Grubic, Leo Tam, Dan Alistarh, and Ce~Zhang.
\newblock Synchronous multi-{GPU} deep learning with low-precision
  communication: An experimental study.
\newblock 2018.

\bibitem{he2016deep}
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778, 2016.

\bibitem{huang2017densely}
Gao Huang, Zhuang Liu, Kilian~Q Weinberger, and Laurens van~der Maaten.
\newblock Densely connected convolutional networks.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, volume~1, page~3, 2017.

\bibitem{jaderberg2014speeding}
Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman.
\newblock Speeding up convolutional neural networks with low rank expansions.
\newblock {\em arXiv preprint arXiv:1405.3866}, 2014.

\bibitem{jaggi2014communication}
Martin Jaggi, Virginia Smith, Martin Tak{\'a}c, Jonathan Terhorst, Sanjay
  Krishnan, Thomas Hofmann, and Michael~I Jordan.
\newblock Communication-efficient distributed dual coordinate ascent.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  3068--3076, 2014.

\bibitem{caffe2}
Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev, Jonathan Long, Ross
  Girshick, Sergio Guadarrama, and Trevor Darrell.
\newblock Caffe: Convolutional architecture for fast feature embedding.
\newblock {\em arXiv preprint arXiv:1408.5093}, 2014.

\bibitem{karimi2016linear}
Hamed Karimi, Julie Nutini, and Mark Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the {P}olyak-{\l}ojasiewicz condition.
\newblock In {\em Joint European Conference on Machine Learning and Knowledge
  Discovery in Databases}, pages 795--811. Springer, 2016.

\bibitem{konevcny2016federated}
Jakub Kone{\v{c}}n{\`y}, H~Brendan McMahan, Felix~X Yu, Peter Richt{\'a}rik,
  Ananda~Theertha Suresh, and Dave Bacon.
\newblock Federated learning: Strategies for improving communication
  efficiency.
\newblock {\em arXiv preprint arXiv:1610.05492}, 2016.

\bibitem{konevcny2016randomized}
Jakub Kone{\v{c}}n{\`y} and Peter Richt{\'a}rik.
\newblock Randomized distributed mean estimation: Accuracy vs communication.
\newblock {\em arXiv preprint arXiv:1611.07555}, 2016.

\bibitem{krizhevsky2009learning}
Alex Krizhevsky and Geoffrey Hinton.
\newblock Learning multiple layers of features from tiny images.
\newblock 2009.

\bibitem{leblond2016asaga}
R{\'e}mi Leblond, Fabian Pedregosa, and Simon Lacoste-Julien.
\newblock {ASAGA}: asynchronous parallel {SAGA}.
\newblock {\em arXiv preprint arXiv:1606.04809}, 2016.

\bibitem{lin2017deep}
Yujun Lin, Song Han, Huizi Mao, Yu~Wang, and William~J Dally.
\newblock Deep gradient compression: Reducing the communication bandwidth for
  distributed training.
\newblock {\em arXiv preprint arXiv:1712.01887}, 2017.

\bibitem{liu2015asynchronous}
Ji~Liu, Stephen~J Wright, Christopher R{\'e}, Victor Bittorf, and Srikrishna
  Sridhar.
\newblock An asynchronous parallel stochastic coordinate descent algorithm.
\newblock {\em The Journal of Machine Learning Research}, 16(1):285--322, 2015.

\bibitem{mania2015perturbed}
Horia Mania, Xinghao Pan, Dimitris Papailiopoulos, Benjamin Recht, Kannan
  Ramchandran, and Michael~I Jordan.
\newblock Perturbed iterate analysis for asynchronous stochastic optimization.
\newblock {\em arXiv preprint arXiv:1507.06970}, 2015.

\bibitem{mcmahan2016communication}
H~Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, et~al.
\newblock Communication-efficient learning of deep networks from decentralized
  data.
\newblock {\em arXiv preprint arXiv:1602.05629}, 2016.

\bibitem{netzer2011reading}
Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo~Wu, and Andrew~Y
  Ng.
\newblock Reading digits in natural images with unsupervised feature learning.
\newblock In {\em NIPS workshop on deep learning and unsupervised feature
  learning}, volume 2011, page~5, 2011.

\bibitem{paszke2017automatic}
Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary
  DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.
\newblock Automatic differentiation in {PyTorch}.
\newblock 2017.

\bibitem{qi17paleo}
Hang Qi, Evan~R. Sparks, and Ameet Talwalkar.
\newblock Paleo: A performance model for deep neural networks.
\newblock In {\em Proceedings of the International Conference on Learning
  Representations}, 2017.

\bibitem{rastegari2016xnor}
Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi.
\newblock Xnor-net: Imagenet classification using binary convolutional neural
  networks.
\newblock In {\em European Conference on Computer Vision}, pages 525--542.
  Springer, 2016.

\bibitem{recht2011hogwild}
Benjamin Recht, Christopher Re, Stephen Wright, and Feng Niu.
\newblock Hogwild: A lock-free approach to parallelizing stochastic gradient
  descent.
\newblock In {\em Advances in neural information processing systems}, pages
  693--701, 2011.

\bibitem{reddi2016stochastic}
Sashank~J Reddi, Ahmed Hefny, Suvrit Sra, Barnabas Poczos, and Alex Smola.
\newblock Stochastic variance reduction for nonconvex optimization.
\newblock In {\em International conference on machine learning}, pages
  314--323, 2016.

\bibitem{renggli2018sparcml}
C{\`e}dric Renggli, Dan Alistarh, and Torsten Hoefler.
\newblock {SparCML:} high-performance sparse communication for machine
  learning.
\newblock {\em arXiv preprint arXiv:1802.08021}, 2018.

\bibitem{sainath2013low}
Tara~N Sainath, Brian Kingsbury, Vikas Sindhwani, Ebru Arisoy, and Bhuvana
  Ramabhadran.
\newblock Low-rank matrix factorization for deep neural network training with
  high-dimensional output targets.
\newblock In {\em Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
  International Conference on}, pages 6655--6659. IEEE, 2013.

\bibitem{seide20141}
Frank Seide, Hao Fu, Jasha Droppo, Gang Li, and Dong Yu.
\newblock 1-bit stochastic gradient descent and its application to
  data-parallel distributed training of speech dnns.
\newblock In {\em Fifteenth Annual Conference of the International Speech
  Communication Association}, 2014.

\bibitem{simonyan2014very}
Karen Simonyan and Andrew Zisserman.
\newblock Very deep convolutional networks for large-scale image recognition.
\newblock {\em arXiv preprint arXiv:1409.1556}, 2014.

\bibitem{strom2015scalable}
Nikko Strom.
\newblock Scalable distributed {DNN} training using commodity gpu cloud
  computing.
\newblock In {\em Sixteenth Annual Conference of the International Speech
  Communication Association}, 2015.

\bibitem{suresh2016distributed}
Ananda~Theertha Suresh, Felix~X Yu, Sanjiv Kumar, and H~Brendan McMahan.
\newblock Distributed mean estimation with limited communication.
\newblock {\em arXiv preprint arXiv:1611.00429}, 2016.

\bibitem{tsuzuku2018variance}
Yusuke Tsuzuku, Hiroto Imachi, and Takuya Akiba.
\newblock Variance-based gradient compression for efficient distributed deep
  learning.
\newblock {\em arXiv preprint arXiv:1802.06058}, 2018.

\bibitem{wangni2017gradient}
Jianqiao Wangni, Jialei Wang, Ji~Liu, and Tong Zhang.
\newblock Gradient sparsification for communication-efficient distributed
  optimization.
\newblock {\em arXiv preprint arXiv:1710.09854}, 2017.

\bibitem{wen2017terngrad}
Wei Wen, Cong Xu, Feng Yan, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li.
\newblock Terngrad: Ternary gradients to reduce communication in distributed
  deep learning.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  1508--1518, 2017.

\bibitem{wiesler2014mean}
Simon Wiesler, Alexander Richard, Ralf Schluter, and Hermann Ney.
\newblock Mean-normalized stochastic gradient for large-scale deep learning.
\newblock In {\em Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
  International Conference on}, pages 180--184. IEEE, 2014.

\bibitem{xue2013restructuring}
Jian Xue, Jinyu Li, and Yifan Gong.
\newblock Restructuring of deep neural network acoustic models with singular
  value decomposition.
\newblock In {\em Interspeech}, pages 2365--2369, 2013.

\bibitem{yin2018gradient}
Dong Yin, Ashwin Pananjady, Max Lam, Dimitris Papailiopoulos, Kannan
  Ramchandran, and Peter Bartlett.
\newblock Gradient diversity: a key ingredient for scalable distributed
  learning.
\newblock In {\em International Conference on Artificial Intelligence and
  Statistics}, pages 1998--2007, 2018.

\bibitem{zhang2017zipml}
Hantian Zhang, Jerry Li, Kaan Kara, Dan Alistarh, Ji~Liu, and Ce~Zhang.
\newblock Zipml: Training linear models with end-to-end low precision, and a
  little bit of deep learning.
\newblock In {\em International Conference on Machine Learning}, pages
  4035--4043, 2017.

\bibitem{zhou2016dorefa}
Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He~Wen, and Yuheng Zou.
\newblock {DoReFa-Net:} training low bitwidth convolutional neural networks
  with low bitwidth gradients.
\newblock {\em arXiv preprint arXiv:1606.06160}, 2016.

\end{thebibliography}
